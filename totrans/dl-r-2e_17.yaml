- en: 14 Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*This chapter covers*'
  prefs: []
  type: TYPE_NORMAL
- en: Important takeaways from this book
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The limitations of deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Possible future directions for deep learning, machine learning, and AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resources for further learning and applying your skills in practice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You’ve almost reached the end of this book. This last chapter will summarize
    and review core concepts while also expanding your horizons beyond what you’ve
    learned so far. Becoming an effective AI practitioner is a journey, and finishing
    this book is merely your first step on it. I want to make sure you realize this
    and are properly equipped to take the next steps of this journey on your own.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll start with a bird’s-eye view of what you should take away from this book.
    This should refresh your memory regarding some of the concepts you’ve learned.
    Next, I’ll present an overview of some key limitations of deep learning. To use
    a tool appropriately, you should not only understand what it *can* do but also
    be aware of what it *can’t* do. Finally, I’ll offer some speculative thoughts
    about the future evolution of deep learning, machine learning, and AI. This should
    be especially interesting to you if you’d like to get into fundamental research.
    The chapter ends with a short list of resources and strategies for further learning
    about machine learning and staying up to date with new advances.
  prefs: []
  type: TYPE_NORMAL
- en: 14.1 Key concepts in review
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section briefly synthesizes key takeaways from this book. If you ever need
    a quick refresher to help you recall what you’ve learned, you can read these few
    pages.
  prefs: []
  type: TYPE_NORMAL
- en: 14.1.1 Various approaches to AI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First of all, deep learning isn’t synonymous with AI, or even with machine
    learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Artificial intelligence* (AI) is an ancient, broad field that can generally
    be understood as “all attempts to automate human cognitive processes.” This can
    range from the very basic, such as an Excel spreadsheet, to the very advanced,
    like a humanoid robot that can walk and talk.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Machine learning* is a specific subfield of AI that aims at automatically
    developing programs (called *models*) purely from exposure to training data. This
    process of turning data into a program is called *learning*. Although machine
    learning has been around for a long time, it started to take off only in the 1990s,
    before becoming the dominant form of AI in the 2000s.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deep learning* is one of many branches of machine learning, where the models
    are long chains of geometric transformations, applied one after the other. These
    operations are structured into modules called *layers*: deep learning models are
    typically stacks of layers—or, more generally, graphs of layers. These layers
    are parameterized by *weights*, which are the parameters learned during training.
    The *knowledge* of a model is stored in its weights, and the process of learning
    consists of finding “good values” for these weights—values that minimize a *loss
    function*. Because the chain of geometric transformations considered is differentiable,
    updating the weights to minimize the loss function is done efficiently via *gradient
    descent*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even though deep learning is just one among many approaches to machine learning,
    it isn’t on an equal footing with the others. Deep learning is a breakout success.
    Here’s why.
  prefs: []
  type: TYPE_NORMAL
- en: 14.1.2 What makes deep learning special within the field of machine learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the span of only a few years, deep learning has achieved tremendous breakthroughs
    across a wide range of tasks that have been historically perceived as extremely
    difficult for computers, especially in the area of machine perception: extracting
    useful information from images, videos, sound, and more. Given sufficient training
    data (in particular, training data appropriately labeled by humans), deep learning
    makes it possible to extract from perceptual data almost anything a human could.
    Hence, it’s sometimes said that deep learning has “solved perception”— although
    that’s true only for a fairly narrow definition of perception.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Due to its unprecedented technical successes, deep learning has singlehandedly
    brought about the third and by far the largest *AI summer*: a period of intense
    interest, investment, and hype in the field of AI. As this book is being written,
    we’re in the middle of it. Whether this period will end in the near future, and
    what happens after it ends, are topics of debate. One thing is certain: in stark
    contrast with previous AI summers, deep learning has provided enormous business
    value to both large and small technology companies, enabling human-level speech
    recognition, smart assistants, human-level image classification, vastly improved
    machine translation, and more. The hype may (and likely will) recede, but the
    sustained economic and technological impact of deep learning will remain. In that
    sense, deep learning could be analogous to the internet: it may be overly hyped
    for a few years, but in the longer term, it will still be a major revolution that
    will transform our economy and our lives.'
  prefs: []
  type: TYPE_NORMAL
- en: I’m particularly optimistic about deep learning, because even if we were to
    make no further technological progress in the next decade, deploying existing
    algorithms to every applicable problem would be a game changer for most industries.
    Deep learning is nothing short of a revolution, and progress is currently happening
    at an incredibly fast rate, due to an exponential investment in resources and
    headcount. From where I stand, the future looks bright, although short-term expectations
    are somewhat overoptimistic; deploying deep learning to the full extent of its
    potential will likely take multiple decades.
  prefs: []
  type: TYPE_NORMAL
- en: 14.1.3 How to think about deep learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The most surprising thing about deep learning is how simple it is. Ten years
    ago, no one expected that we would achieve such amazing results on machine-perception
    problems by using simple parametric models trained with gradient descent. Now
    it turns out that all you need is sufficiently large parametric models trained
    with gradient descent on sufficiently many examples. As Feynman once said about
    the universe, “It’s not complicated, it’s just a lot of it.^([1](#Rendnote1))
  prefs: []
  type: TYPE_NORMAL
- en: In deep learning, everything is a vector—that is to say, everything is a *point*
    in a *geometric space*. Model inputs (text, images, and so on) and targets are
    first *vectorized*— turned into an initial input vector space and target vector
    space. Each layer in a deep learning model operates one simple geometric transformation
    on the data that goes through it. Together, the chain of layers in the model forms
    one complex geometric transformation, broken down into a series of simple ones.
    This complex transformation attempts to map the input space to the target space,
    one point at a time. This transformation is parameterized by the weights of the
    layers, which are iteratively updated based on how well the model is currently
    performing. A key characteristic of this geometric transformation is that it must
    be *differentiable*, which is required for us to be able to learn its parameters
    via gradient descent. Intuitively, this means the geometric morphing from inputs
    to outputs must be smooth and continuous—a significant constraint.
  prefs: []
  type: TYPE_NORMAL
- en: 'The entire process of applying this complex geometric transformation to the
    input data can be visualized in 3D by imagining a person trying to uncrumple a
    paper ball: the crumpled paper ball is the manifold of the input data that the
    model starts with. Each movement operated by the person on the paper ball is similar
    to a simple geometric transformation operated by one layer. The full uncrumpling
    gesture sequence is the complex transformation of the entire model. Deep learning
    models are mathematical machines for uncrumpling complicated manifolds of high-dimensional
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s the magic of deep learning: turning meaning into vectors, then into
    geometric spaces, and then incrementally learning complex geometric transformations
    that map one space to another. All you need are spaces of sufficiently high dimensionality
    to capture the full scope of the relationships found in the original data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The whole process hinges on a single core idea: *that meaning is derived from
    the pairwise relationship between things* (between words in a language, between
    pixels in an image, and so on) and that *these relationships can be captured by
    a distance function*. But note that whether the brain also implements meaning
    via geometric spaces is an entirely separate question. Vector spaces are efficient
    to work with from a computational standpoint, but different data structures for
    intelligence can easily be envisioned—in particular, graphs. Neural networks initially
    emerged from the idea of using graphs as a way to encode meaning, which is why
    they’re named *neural networks*; the surrounding field of research used to be
    called *connectionism*. Nowadays the name “neural network” exists purely for historical
    reasons—it’s an extremely misleading name because they’re neither neural nor networks.
    In particular, neural networks have hardly anything to do with the brain. A more
    appropriate name would have been *layered representations learning* or *hierarchical
    representations learning*, or maybe even *deep differentiable models* or *chained
    geometric transforms*, to emphasize the fact that continuous geometric space manipulation
    is at their core.'
  prefs: []
  type: TYPE_NORMAL
- en: 14.1.4 Key enabling technologies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The technological revolution that’s currently unfolding didn’t start with any
    single breakthrough invention. Rather, like any other revolution, it’s the product
    of a vast accumulation of enabling factors—gradual at first, and then sudden.
    In the case of deep learning, we can point out the following key factors:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Incremental algorithmic innovations*—These first began appearing slowly over
    the span of two decades (starting with backpropagation), and then were developed
    increasingly faster as more research effort was poured into deep learning after
    2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The availability of large amounts of perceptual data*—This was a requirement
    in order to realize that sufficiently large models trained on sufficiently large
    data are all we need. This is, in turn, a byproduct of the rise of the consumer
    internet and Moore’s law applied to storage media.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The availability of fast, highly parallel computation hardware at a low price*—Especially
    the GPUs produced by NVIDIA—first gaming GPUs and then chips designed from the
    ground up for deep learning. Early on, NVIDIA CEO Jensen Huang took note of the
    deep learning boom and decided to bet the company’s future on it, which paid off
    in a big way.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A complex stack of software layers that makes this computational power available
    to humans*—The CUDA language, frameworks like TensorFlow that do automatic differentiation,
    and Keras, which makes deep learning accessible to most people.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the future, deep learning will not be used only by specialists—researchers,
    graduate students, and engineers with an academic profile—it will be a tool in
    the toolbox of every developer, much like web technology today. Everyone needs
    to build intelligent apps: just as every business today needs a website, every
    product will need to intelligently make sense of user-generated data. Bringing
    about this future will require us to build tools that make deep learning radically
    easy to use and accessible to anyone with basic coding abilities. Keras has been
    the first major step in that direction.'
  prefs: []
  type: TYPE_NORMAL
- en: 14.1.5 The universal machine learning workflow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Having access to an extremely powerful tool for creating models that map any
    input space to any target space is great, but the difficult part of the machine
    learning work-flow is often everything that comes before designing and training
    such models (and, for production models, what comes after, as well). Understanding
    the problem domain so as to be able to determine what to attempt to predict, given
    what data, and how to measure success, is a prerequisite for any successful application
    of machine learning, and it isn’t something that advanced tools like Keras and
    TensorFlow can help you with. As a reminder, here’s a quick summary of the typical
    machine learning workflow as described in chapter 6:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1** Define the problem. What data is available, and what are you trying to
    predict? Will you need to collect more data or hire people to manually label a
    dataset?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**2** Identify a way to reliably measure success on your goal. For simple tasks,
    this may be prediction accuracy, but in many cases, it will require sophisticated,
    domain-specific metrics.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**3** Prepare the validation process that you’ll use to evaluate your models.
    In particular, you should define a training set, a validation set, and a test
    set. The validation and test set labels shouldn’t leak into the training data:
    for instance, with temporal prediction, the validation and test data should be
    posterior to the training data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**4** Vectorize the data by turning it into vectors and preprocessing it in
    a way that makes it more easily approachable by a neural network (normalization
    and so on).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**5** Develop a first model that beats a trivial common-sense baseline, thus
    demonstrating that machine learning can work on your problem. This may not always
    be the case!'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**6** Gradually refine your model architecture by tuning hyperparameters and
    adding regularization. Make changes based on performance on the validation data
    only, not the test data or the training data. Remember that you should get your
    model to overfit (thus identifying a model capacity level that’s greater than
    you need) and only then begin to add regularization or downsize your model. Beware
    of validation-set overfitting when tuning hyperparameters—your hyper-parameters
    may end up being overspecialized to the validation set. Avoiding this is the purpose
    of having a separate test set.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**7** Deploy your final model in production—as a web API, as part of a JavaScript
    or C++ application, on an embedded device, and so on. Keep monitoring its performance
    on real-world data, and use your findings to refine the next iteration of the
    model!'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 14.1.6 Key network architectures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The four families of network architectures that you should be familiar with
    are *densely connected networks, convolutional networks, recurrent networks*,
    and *Transformers*. Each type of model is meant for a specific input modality.
    A network architecture encodes *assumptions* about the structure of the data:
    a *hypothesis space* within which the search for a good model will proceed. Whether
    a given architecture will work on a given problem depends entirely on the match
    between the structure of the data and the assumptions of the network architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: 'These different network types can easily be combined to achieve larger multi-modal
    models, much as you combine LEGO bricks. In a way, deep learning layers are LEGO
    bricks for information processing. Here’s a quick overview of the mapping between
    input modalities and appropriate network architectures:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Vector data*—Densely connected models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Image data*—2D convnets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Sequence data*—RNNs for time series, or Transformers for discrete sequences
    (such as sequences of words). 1D convnets can also be used for translation-invariant,
    continuous sequence data, such as birdsong waveforms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Video data*—Either 3D convnets (if you need to capture motion effects), or
    a combination of a frame-level 2D convnet for feature extraction followed by a
    sequence-processing model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Volumetric data*—3D convnets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let’s quickly review the specificities of each network architecture.
  prefs: []
  type: TYPE_NORMAL
- en: DENSELY CONNECTED NETWORKS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A densely connected network is a stack of Dense layers meant to process vector
    data (where each sample is a vector of numerical or categorical attributes). Such
    networks assume no specific structure in the input features: they’re called *densely
    connected* because the units of a Dense layer are connected to every other unit.
    The layer attempts to map relationships between any two input features; this is
    unlike a 2D convolution layer, for instance, which looks only at *local* relationships.'
  prefs: []
  type: TYPE_NORMAL
- en: Densely connected networks are most commonly used for categorical data (e.g.,
    where the input features are lists of attributes), such as the Boston housing
    price data-set used in chapter 4\. They’re also used as the final classification
    or regression stage of most networks. For instance, the convnets covered in chapter
    8 typically end with one or two Dense layers, and so do the recurrent networks
    in chapter 10.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember, to perform *binary classification*, end your stack of layers with
    a Dense layer with a single unit and a sigmoid activation, and use binary_crossentropy
    as the loss. Your targets should be either 0 or 1:'
  prefs: []
  type: TYPE_NORMAL
- en: inputs <- layer_input(shape = c(num_inputs_features))
  prefs: []
  type: TYPE_NORMAL
- en: outputs <- inputs %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(32, activation = "relu") %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(32, activation = "relu") %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(1, activation = "sigmoid")
  prefs: []
  type: TYPE_NORMAL
- en: model <- keras_model(inputs, outputs)
  prefs: []
  type: TYPE_NORMAL
- en: model %>% compile(optimizer = "rmsprop", loss = "binary_crossentropy")
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform *single-label categorical classification* (where each sample has
    exactly one class, no more), end your stack of layers with a Dense layer with
    a number of units equal to the number of classes, and a softmax activation. If
    your targets are one-hot encoded, use categorical_crossentropy as the loss; if
    they’re integers, use sparse_categorical_ crossentropy:'
  prefs: []
  type: TYPE_NORMAL
- en: inputs <- layer_input(shape = c(num_inputs_features))
  prefs: []
  type: TYPE_NORMAL
- en: outputs <- inputs %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(32, activation = "relu") %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(32, activation = "relu") %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(num_classes, activation = "softmax")
  prefs: []
  type: TYPE_NORMAL
- en: model <- keras_model(inputs, outputs)
  prefs: []
  type: TYPE_NORMAL
- en: model %>% compile(optimizer = "rmsprop", loss = "categorical_crossentropy")
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform *multilabel categorical classification* (where each sample can have
    several classes), end your stack of layers with a Dense layer with a number of
    units equal to the number of classes, and a sigmoid activation, and use binary_crossentropy
    as the loss. Your targets should be multi-hot encoded:'
  prefs: []
  type: TYPE_NORMAL
- en: inputs <- layer_input(shape = c(num_inputs_features))
  prefs: []
  type: TYPE_NORMAL
- en: outputs <- inputs %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(32, activation = "relu") %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(32, activation = "relu") %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(num_classes, activation = "sigmoid")
  prefs: []
  type: TYPE_NORMAL
- en: model <- keras_model(inputs, outputs)
  prefs: []
  type: TYPE_NORMAL
- en: model %>% compile(optimizer = "rmsprop", loss = "binary_crossentropy")
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform *regression* toward a vector of continuous values, end your stack
    of layers with a Dense layer with a number of units equal to the number of values
    you’re trying to predict (often a single one, such as the price of a house), and
    no activation. Various losses can be used for regression—most commonly mean_squared_error
    (MSE):'
  prefs: []
  type: TYPE_NORMAL
- en: inputs <- layer_input(shape = c(num_inputs_features))
  prefs: []
  type: TYPE_NORMAL
- en: outputs <- inputs %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(32, activation = "relu") %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(32, activation = "relu") %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(num_values)
  prefs: []
  type: TYPE_NORMAL
- en: model <- keras_model(inputs, outputs)
  prefs: []
  type: TYPE_NORMAL
- en: model %>% compile(optimizer = "rmsprop", loss = "mse")
  prefs: []
  type: TYPE_NORMAL
- en: CONVNETS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Convolution layers look at spatially local patterns by applying the same geometric
    transformation to different spatial locations (*patches*) in an input tensor.
    This results in representations that are *translation invariant*, making convolution
    layers highly data efficient and modular. This idea is applicable to spaces of
    any dimensionality: 1D (continuous sequences), 2D (images), 3D (volumes), and
    so on. You can use the Conv1D layer to process sequences, the Conv2D layer to
    process images, and the Conv3D layers to process volumes. As a leaner, more efficient
    alternative to convolution layers, you can also use *depthwise separable convolution*
    layers, such as SeparableConv2D.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Convnets*, or *convolutional networks*, consist of stacks of convolution and
    max-pooling layers. The pooling layers let you spatially downsample the data,
    which is required to keep feature maps to a reasonable size as the number of features
    grows, and to allow subsequent convolution layers to “see” a greater spatial extent
    of the inputs. Convnets are often ended with either a Flatten operation or a global-pooling
    layer, turning spatial feature maps into vectors, followed by Dense layers to
    achieve classification or regression.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a typical image classification network (categorical classification,
    in this case), leveraging SeparableConv2D layers:'
  prefs: []
  type: TYPE_NORMAL
- en: inputs <- layer_input(shape = c(height, width, channels))
  prefs: []
  type: TYPE_NORMAL
- en: outputs <- inputs %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_separable_conv_2d(32, 3, activation = "relu") %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_separable_conv_2d(64, 3, activation = "relu") %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_max_pooling_2d(2) %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_separable_conv_2d(64, 3, activation = "relu") %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_separable_conv_2d(128, 3, activation = "relu") %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_max_pooling_2d(2) %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_separable_conv_2d(64, 3, activation = "relu") %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_separable_conv_2d(128, 3, activation = "relu") %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_global_average_pooling_2d() %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(32, activation = "relu") %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(num_classes, activation = "softmax")
  prefs: []
  type: TYPE_NORMAL
- en: model <- keras_model(inputs, outputs)
  prefs: []
  type: TYPE_NORMAL
- en: model %>% compile(optimizer = "rmsprop", loss = "categorical_crossentropy")
  prefs: []
  type: TYPE_NORMAL
- en: When building a very deep convnet, it’s common to add *batch normalization*
    layers as well as *residual connections*—two architecture patterns that help gradient
    information flow smoothly through the network.
  prefs: []
  type: TYPE_NORMAL
- en: RNNS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Recurrent neural networks* (RNNs) work by processing sequences of inputs one
    time step at a time, and maintaining a state throughout (a state is typically
    a vector or set of vectors). They should be used preferentially over 1D convnets
    in the case of sequences where patterns of interest aren’t invariant by temporal
    translation (e.g., time-series data where the recent past is more important than
    the distant past).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Three RNN layers are available in Keras: SimpleRNN, GRU, and LSTM. For most
    practical purposes, you should use either GRU or LSTM. LSTM is the more powerful
    of the two but is also more expensive; you can think of GRU as a simpler, cheaper
    alternative to it.'
  prefs: []
  type: TYPE_NORMAL
- en: To stack multiple RNN layers on top of each other, each layer prior to the last
    layer in the stack should return the full sequence of its outputs (each input
    time step will correspond to an output time step). If you aren’t stacking any
    further RNN layers, it’s common to return only the last output, which contains
    information about the entire sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is a single RNN layer for binary classification of vector sequences:'
  prefs: []
  type: TYPE_NORMAL
- en: inputs <- layer_input(shape = c(num_timesteps, num_features))
  prefs: []
  type: TYPE_NORMAL
- en: outputs <- inputs %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_lstm(32) %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(num_classes, activation = "sigmoid")
  prefs: []
  type: TYPE_NORMAL
- en: model <- keras_model(inputs, outputs)
  prefs: []
  type: TYPE_NORMAL
- en: model %>% compile(optimizer = "rmsprop", loss = "binary_crossentropy")
  prefs: []
  type: TYPE_NORMAL
- en: 'And this is a stacked RNN for binary classification of vector sequences:'
  prefs: []
  type: TYPE_NORMAL
- en: inputs <- layer_input(shape = c(num_timesteps, num_features))
  prefs: []
  type: TYPE_NORMAL
- en: outputs <- inputs %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_lstm(32, return_sequences = TRUE) %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_lstm(32, return_sequences = TRUE) %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_lstm(32) %>% layer_dense(num_classes, activation = "sigmoid")
  prefs: []
  type: TYPE_NORMAL
- en: model <- keras_model(inputs, outputs)
  prefs: []
  type: TYPE_NORMAL
- en: model %>% compile(optimizer = "rmsprop", loss = "binary_crossentropy")
  prefs: []
  type: TYPE_NORMAL
- en: TRANSFORMERS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A Transformer looks at a set of vectors (such as word vectors), and leverages
    *neural attention* to transform each vector into a representation that is aware
    of the *context* provided by the other vectors in the set. When the set in question
    is an ordered sequence, you can also leverage *positional encoding* to create
    Transformers that can take into account both global context and word order, capable
    of processing long text paragraphs much more effectively than RNNs or 1D convnets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Transformers can be used for any set-processing or sequence-processing task,
    including text classification, but they excel especially at *sequence-to-sequence
    learning*, such as translating paragraphs in a source language into a target language.
    A sequence-to-sequence Transformer is made up of two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: A TransformerEncoder that turns an input vector sequence into a context-aware,
    order-aware output vector sequence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A TransformerDecoder that takes the output of the TransformerEncoder, as well
    as a target sequence, and predicts what should come next in the target sequenc
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you’re only processing a single sequence (or set) of vectors, you’d be only
    using the TransformerEncoder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is a sequence-to-sequence Transformer for mapping a source sequence
    to a target sequence (this setup could be used for machine translation or question
    answering, for instance):'
  prefs: []
  type: TYPE_NORMAL
- en: encoder_inputs <- layer_input(shape = c(sequence_length),➊
  prefs: []
  type: TYPE_NORMAL
- en: dtype = "int64")
  prefs: []
  type: TYPE_NORMAL
- en: encoder_outputs <- encoder_inputs %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_positional_embedding(sequence_length, vocab_size, embed_dim) %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_transformer_encoder(embed_dim, dense_dim, num_heads)
  prefs: []
  type: TYPE_NORMAL
- en: decoder <- layer_transformer_decoder(NULL, embed_dim, dense_dim, num_heads)
  prefs: []
  type: TYPE_NORMAL
- en: decoder_inputs <- layer_input(shape = c(NA),➋
  prefs: []
  type: TYPE_NORMAL
- en: dtype = "int64")
  prefs: []
  type: TYPE_NORMAL
- en: decoder_outputs <- decoder_inputs %>%➌
  prefs: []
  type: TYPE_NORMAL
- en: layer_positional_embedding(sequence_length, vocab_size, embed_dim) %>%
  prefs: []
  type: TYPE_NORMAL
- en: decoder(., encoder_outputs) %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(vocab_size, activation = "softmax")
  prefs: []
  type: TYPE_NORMAL
- en: transformer <- keras_model(list(encoder_inputs, decoder_inputs),
  prefs: []
  type: TYPE_NORMAL
- en: decoder_outputs)
  prefs: []
  type: TYPE_NORMAL
- en: transformer %>%
  prefs: []
  type: TYPE_NORMAL
- en: compile(optimizer = "rmsprop", loss = "categorical_crossentropy")
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Source sequence**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Target sequence so far**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Target sequence one step in the future**
  prefs: []
  type: TYPE_NORMAL
- en: 'And this is a lone TransformerEncoder for binary classification of integer
    sequences:'
  prefs: []
  type: TYPE_NORMAL
- en: inputs <- layer_input(shape = c(sequence_length), dtype = "int64")
  prefs: []
  type: TYPE_NORMAL
- en: outputs <- inputs %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_positional_embedding(sequence_length, vocab_size, embed_dim) %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_transformer_encoder(embed_dim, dense_dim, num_heads) %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_global_max_pooling_1d() %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(1, activation = "sigmoid")
  prefs: []
  type: TYPE_NORMAL
- en: model <- keras_model(inputs, outputs)
  prefs: []
  type: TYPE_NORMAL
- en: model %>% compile(optimizer = "rmsprop", loss = "binary_crossentropy")
  prefs: []
  type: TYPE_NORMAL
- en: Full implementations of the TransformerEncoder, the TransformerDecoder, and
    the PositionalEmbedding layers are provided in chapter 11.
  prefs: []
  type: TYPE_NORMAL
- en: 14.1.7 The space of possibilities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'What will you build with these techniques? Remember, building deep learning
    models is like playing with LEGO bricks: layers can be plugged together to map
    essentially anything to anything, given that you have appropriate training data
    available and that the mapping is achievable via a continuous geometric transformation
    of reasonable complexity. The space of possibilities is infinite. This section
    offers a few examples to inspire you to think beyond the basic classification
    and regression tasks that have traditionally been the bread and butter of machine
    learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'I’ve sorted my suggested applications by input and output modalities in the
    following list. Note that quite a few of them stretch the limits of what is possible,
    although a model could be trained on all of these tasks—in some cases, such a
    model probably wouldn’t generalize far from its training data. Sections 14.2 through
    14.4 will address how these limitations could be lifted in the future:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mapping vector data to vector data:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Predictive health care*—Mapping patient medical records to predictions of
    patient outcomes'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Behavioral targeting*—Mapping a set of website attributes with data on how
    long a user will spend on the website'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Product quality control*—Mapping a set of attributes relative to an instance
    of a manufactured product with the probability that the product will fail by next
    year'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mapping image data to vector data:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Medical assistant*—Mapping slides of medical images to a prediction about
    the presence of a tumor'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Self-driving vehicle*—Mapping car dashcam video frames to steering wheel angle
    commands and gas and braking commands'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Board game AI*—Mapping Go or chess boards to the next player move'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Diet helper*—Mapping pictures of a dish to its calorie count'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Age prediction*—Mapping selfies to the age of the person'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mapping time-series data to vector data:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Weather prediction*—Mapping time series of weather data in a grid of locations
    to the temperature in a specific place one week later'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Brain-computer interfaces*—Mapping time series of magnetoencephalogram (MEG)
    data to computer commands'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Behavioral targeting*—Mapping time series of user interactions on a website
    to the probability that a user will buy something'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mapping text to text:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Machine translation*—Mapping a paragraph in one language to a translated version
    in a different language'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Smart reply*—Mapping emails to possible one-line replies'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Question answering*—Mapping general-knowledge questions to answers'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Summarization*—Mapping a long article to a short summary of the article'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mapping images to text:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Text transcription*—Mapping images that contain a text element to the corresponding
    text string'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Captioning*—Mapping images to short captions describing the contents of the
    images'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mapping text to images:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Conditioned image generation*—Mapping a short text description to images matching
    the description'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Logo generation/selection*—Mapping the name and description of a company to
    a logo suggestion'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mapping images to images:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Super-resolution*—Mapping downsized images to higher-resolution versions of
    the same images'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Visual depth sensing*—Mapping images of indoor environments to maps of depth
    predictions'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mapping images and text to text:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Visual QA*—Mapping images and natural language questions about the contents
    of images to natural language answers'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Mapping video and text to text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Video QA*—Mapping short videos and natural language questions about the contents
    of videos to natural language answers'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Almost* anything is possible, but not quite *anything*. You’ll see in the
    next section what we *can’t* do with deep learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 14.2 The limitations of deep learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The space of applications that can be implemented with deep learning is infinite.
    And yet, many applications remain completely out of reach for current deep learning
    techniques—even given vast amounts of human-annotated data. Say, for instance,
    that you could assemble a dataset of hundreds of thousands—even millions—of English-language
    descriptions of the features of a software product, written by a product manager,
    as well as the corresponding source code developed by a team of engineers to meet
    these requirements. Even with this data, you could not train a deep learning model
    to read a product description and generate the appropriate codebase. That’s just
    one example among many. In general, anything that requires reasoning—like programming
    or applying the scientific method—long-term planning—and algorithmic data manipulation
    is out of reach for deep learning models, no matter how much data you throw at
    them. Even learning a simple sorting algorithm with a deep neural network is tremendously
    difficult.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is because a deep learning model is just *a chain of simple, continuous
    geometric transformations* mapping one vector space into another. All it can do
    is map one data manifold X into another manifold Y, assuming the existence of
    a learnable continuous transform from X to Y. A deep learning model can be interpreted
    as a kind of program, but, inversely, *most programs can’t be expressed as deep
    learning models*. For most tasks, either there exists no corresponding neural
    network of reasonable size that solves the task, or, even if one exists, it may
    not be *learnable*: the corresponding geometric transform may be far too complex,
    or there may not be appropriate data available to learn it.'
  prefs: []
  type: TYPE_NORMAL
- en: Scaling up current deep learning techniques by stacking more layers and using
    more training data can only superficially palliate some of these issues. It won’t
    solve the more fundamental problems that deep learning models are limited in what
    they can represent and that most of the programs you may wish to learn can’t be
    expressed as a continuous geometric morphing of a data manifold.
  prefs: []
  type: TYPE_NORMAL
- en: 14.2.1 The risk of anthropomorphizing machine learning models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One real risk with contemporary AI is misinterpreting what deep learning models
    do and overestimating their abilities. A fundamental feature of humans is our
    *theory of mind*: our tendency to project intentions, beliefs, and knowledge on
    the things around us. Drawing a smiley face on a rock suddenly makes it “happy”
    in our minds. Applied to deep learning, this means that, for instance, when we’re
    able to somewhat successfully train a model to generate captions to describe pictures,
    we’re led to believe that the model “understands” the contents of the pictures
    and the captions it generates. Then we’re surprised when any slight departure
    from the sort of images present in the training data causes the model to generate
    completely absurd captions (see [figure 14.1](#fig14-1)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0486-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 14.1 Failure of an image-captioning system based on deep learning**'
  prefs: []
  type: TYPE_NORMAL
- en: In particular, this is highlighted by *adversarial examples*, which are samples
    fed to a deep learning network that are designed to trick the model into misclassifying
    them. You’re already aware that, for instance, it’s possible to do gradient ascent
    in input space to generate inputs that maximize the activation of some convnet
    filter—this is the basis of the filter-visualization technique introduced in chapter
    9, as well as the DeepDream algorithm from chapter 12\. Similarly, through gradient
    ascent, you can slightly modify an image to maximize the class prediction for
    a given class. By taking a picture of a panda and adding to it a gibbon gradient,
    we can get a neural network to classify the panda as a gibbon (see [figure 14.2](#fig14-2)).
    This evidences both the brittleness of these models and the deep difference between
    their input-to-output mapping and our human perception.
  prefs: []
  type: TYPE_NORMAL
- en: In short, deep learning models don’t have any understanding of their input—at
    least not in a human sense. Our own understanding of images, sounds, and language
    is grounded in our sensorimotor experience as humans. Machine learning models
    have no access to such experiences and thus can’t understand their inputs in a
    human-relatable way. By annotating large numbers of training examples to feed
    into our models, we get them to learn a geometric transform that maps data to
    human concepts on a specific set of examples, but this mapping is a simplistic
    sketch of the original model in our minds—the one developed from our experience
    as embodied agents. It’s like a dim image in a mirror (see [figure 14.3](#fig14-1)).
    The models you create will take any shortcut available to fit their training data.
    For instance, image models tend to rely more on local textures than on a global
    understanding of the input images—a model trained on a dataset that features both
    leopards and sofas is likely to classify a leopard-pattern sofa as an actual leopard.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0487-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 14.2 An adversarial example: Imperceptible changes in an image can
    upend a model’s classification of an image.**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0487-02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 14.3 Current machine learning models: Like a dim image in a mirror**'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a machine learning practitioner, always be mindful of this, and never fall
    into the trap of believing that neural networks understand the tasks they perform—they
    don’t, at least not in a way that would make sense to us. They were trained on
    a different, far narrower task than the one we wanted to teach them: that of mapping
    training inputs to training targets, point by point. Show them anything that deviates
    from their training data, and they will break in absurd ways.'
  prefs: []
  type: TYPE_NORMAL
- en: 14.2.2 Automatons vs. intelligent agents
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Fundamental differences exist between the straightforward geometric morphing
    from input to output that deep learning models do and the way humans think and
    learn. It isn’t just the fact that humans learn by themselves from embodied experience
    instead of being presented with explicit training examples. The human brain is
    an entirely different beast compared to a differentiable parametric function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s zoom out a little bit and ask, “what’s the purpose of intelligence?”
    Why did it arise in the first place? We can only speculate, but we can make fairly
    informed speculations. We can start by looking at brains—the organ that produces
    intelligence. Brains are an evolutionary adaption—a mechanism developed incrementally
    over hundreds of millions of years, via random trial and error, guided by natural
    selection— that dramatically expanded the ability of organisms to adapt to their
    environment. Brains originally appeared more than half a billion years ago as
    a way to *store and execute behavioral programs*. “Behavioral programs” are just
    sets of instructions that make an organism reactive to its environment: “if this
    happens, then do that.” They link the organism’s sensory inputs to its motor controls.
    In the beginning, brains would have served to hardcode behavioral programs (as
    neural connectivity patterns), which would allow an organism to react appropriately
    to its sensory input. This is the way insect brains still work—flies, ants, *C.
    elegans* (see [figure 14.4](#fig14-1)), and so on. Because the original “source
    code” of these programs was DNA, which would be decoded as neural connectivity
    patterns, evolution was suddenly able to *search over behavior space* in a largely
    unbounded way—a major evolutionary shift.'
  prefs: []
  type: TYPE_NORMAL
- en: Evolution was the programmer, and brains were computers carefully executing
    the code evolution gave them. Because neural connectivity is a very general computing
    substrate, the sensorimotor space of all brain-enabled species could suddenly
    start undergoing a dramatic expansion. Eyes, ears, mandibles, four legs, 24 legs—as
    long as you have a brain, evolution will kindly figure out for you behavioral
    programs that make good use of these. Brains can handle any modality—or combination
    of modalities—you throw at them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, mind you, these early brains weren’t exactly intelligent per se. They
    were very much *automatons*: they would merely execute behavioral programs hardcoded
    in the organism’s DNA. They could only be described as intelligent in the same
    sense that a thermostat is “intelligent.” Or a list-sorting program. Or… a trained
    deep neural network (of the artificial kind). This is an important distinction,
    so let’s look at it carefully: what’s the difference between automatons and actual
    intelligent agents?'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0489-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 14.4 The brain network of the *C. elegans* worm: A behavioral automaton
    “programmed” by natural evolution. Figure created by Emma Towlson (from Yan et
    al., “Network Control Principles Predict Neuron Function in the *Caenorhabditis
    elegans* Connectome,” *Nature*, Oct. 2017).**'
  prefs: []
  type: TYPE_NORMAL
- en: 14.2.3 Local generalization vs. extreme generalization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Seventeenth-century French philosopher and scientist René Descartes wrote in
    1637 an illuminating comment that perfectly captures this distinction, long before
    the rise of AI, and in fact, before the first mechanical computer (which his colleague
    Pascal would create five years later). Descartes tells us, in reference to automatons,
  prefs: []
  type: TYPE_NORMAL
- en: '*Even though such machines might do some things as well as we do them, or perhaps
    even better, they would inevitably fail in others, which would reveal they were
    acting not through understanding, but only from the disposition of their organs.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: René Descartes, *Discourse on the Method* (1637)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: There it is. Intelligence is characterized by *understanding*, and understanding
    is evidenced by *generalization*—the ability to handle whatever novel situation
    may arise. How do you tell the difference between a student that has memorized
    the past three years of exam questions but has no understanding of the subject,
    and a student who actually understands the material? You give them a brand-new
    problem. An automaton is static, crafted to accomplish specific things in a specific
    context—”if this, then that”—while an intelligent agent can adapt on the fly to
    novel, unexpected situations. When an automaton is exposed to something that doesn’t
    match what it is “programmed” to do (whether we’re talking about human-written
    programs, evolution-generated programs, or the implicit programming process of
    fitting a model on a training dataset), it will fail. Meanwhile, intelligent agents,
    like humans, will use their understanding to find a way forward.
  prefs: []
  type: TYPE_NORMAL
- en: 'Humans are capable of far more than mapping immediate stimuli to immediate
    responses, as a deep net, or an insect, would. We maintain complex, abstract models
    of our current situation, of ourselves, and of other people, and we can use these
    models to anticipate different possible futures and perform long-term planning.
    You can merge together known concepts to represent something you’ve never experienced
    before— like imagining what you’d do if you won the lottery, or picturing how
    your friend would react if you discreetly replaced her keys with exact copies
    made of elastic rubber. This ability to handle novelty and what-ifs, to expand
    our mental model space far beyond what we can experience directly—to leverage
    *abstraction* and *reasoning*—is the defining characteristic of human cognition.
    I call it *extreme generalization*: an ability to adapt to novel, never-before-experienced
    situations using little data or even no new data at all. This capability is key
    to the intelligence displayed by humans and advanced animals.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This stands in sharp contrast with what automaton-like systems do. A very rigid
    automaton wouldn’t feature any generalization at all—it would be incapable of
    handling anything that it wasn’t precisely told about in advance. A hash table
    or a basic question-answering program implemented as hardcoded if-then-else statements
    would fall into this category. Deep nets do slightly better: they can successfully
    process inputs that deviate a bit from what they’re familiar with, which is precisely
    what makes them useful. Our cats vs. dogs model from chapter 8 could classify
    cat or dog pictures it had not seen before, as long as they were close enough
    to what it was trained on. However, deep nets are limited to what I call *local
    generalization* (see [figure 14.5](#fig14-5)): the mapping from inputs to outputs
    performed by a deep net quickly stops making sense as inputs start deviating from
    what the net saw at training time. Deep nets can only generalize to *known unknowns*—to
    factors of variation that were anticipated during model development and that are
    extensively featured in the training data, such as different camera angles or
    lighting conditions for pet pictures. That’s because deep nets generalize via
    interpolation on a manifold (remember chapter 5): any factor of variation in their
    input space needs to be captured by the manifold they learn. That’s why basic
    data augmentation is so helpful in improving deep net generalization. Unlike humans,
    these models have no ability to improvise in the face of situations for which
    little or no data is available (like winning the lottery or being handed rubber
    keys) that only share abstract commonalities with past situations.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0490-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 14.5 Local generalization vs. extreme generalization**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider, for instance, the problem of learning the appropriate launch parameters
    to get a rocket to land on the moon. If you used a deep net for this task and
    trained it using supervised learning or reinforcement learning, you’d have to
    feed it tens of thousands or even millions of launch trials: you’d need to expose
    it to a *dense sampling* of the input space, for it to learn a reliable mapping
    from input space to output space. In contrast, as humans, we can use our power
    of abstraction to come up with physical models—rocket science—and derive an exact
    solution that will land the rocket on the moon in one or a few trials. Similarly,
    if you developed a deep net controlling a human body, and you wanted it to learn
    to safely navigate a city without getting hit by cars, the net would have to die
    many thousands of times in various situations until it could infer that cars are
    dangerous and develop appropriate avoidance behaviors. Dropped into a new city,
    the net would have to relearn most of what it knows. On the other hand, humans
    are able to learn safe behaviors without having to die even once—again, thanks
    to our power of abstract modeling of novel situations.'
  prefs: []
  type: TYPE_NORMAL
- en: 14.2.4 The purpose of intelligence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This distinction between highly adaptable intelligent agents and rigid automatons
    leads us back to brain evolution. Why did brains—originally a mere medium for
    natural evolution to develop behavioral automatons—eventually turn intelligent?
    Like every significant evolutionary milestone, it happened because natural selection
    constraints encouraged it to happen.
  prefs: []
  type: TYPE_NORMAL
- en: 'Brains are responsible for behavior generation. If the set of situations an
    organism had to face was mostly static and known in advance, behavior generation
    would be an easy problem: evolution would just figure out the correct behaviors
    via random trial and error and hardcode them into the organism’s DNA. This first
    stage of brain evolution— brains as automatons—would already be optimal. However,
    crucially, as organism complexity—and alongside it, environmental complexity—kept
    increasing, the situations that animals had to deal with became much more dynamic
    and more unpredictable. A day in your life, if you look closely, is unlike any
    day you’ve ever experienced, and unlike any day ever experienced by any of your
    evolutionary ancestors. You need to be able to face unknown and surprising situations
    constantly. There is no way for evolution to find and hardcode as DNA the sequence
    of behaviors you’ve been executing to successfully navigate your day since you
    woke up a few hours ago. It has to be generated on the fly, every day.'
  prefs: []
  type: TYPE_NORMAL
- en: The brain, as a good behavior-generation engine, simply adapted to fit this
    need. It optimized for adaptability and generality, rather than merely optimizing
    for fitness to a fixed set of situations. This shift likely occurred multiple
    times throughout evolutionary history, resulting in highly intelligent animals
    in very distant evolutionary branches—apes, octopuses, ravens, and more. Intelligence
    is an answer to challenges presented by complex, dynamic ecosystems.
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s the nature of intelligence: it is the ability to efficiently leverage
    the information at your disposal to produce successful behavior in the face of
    an uncertain, ever-changing future. What Descartes calls “understanding” is the
    key to this remarkable capability: the power to mine your past experience to develop
    modular, reusable abstractions that can be quickly repurposed to handle novel
    situations and achieve extreme generalization.'
  prefs: []
  type: TYPE_NORMAL
- en: 14.2.5 Climbing the spectrum of generalization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As a crude caricature, you could summarize the evolutionary history of biological
    intelligence as a slow climb up the *spectrum of generalization*. It started with
    automaton-like brains that could perform only local generalization. Over time,
    evolution started producing organisms capable of increasingly broader generalization
    that could thrive in evermore complex and variable environments. Eventually, in
    the past few millions of years— an instant in evolutionary terms—certain hominid
    species started trending toward an implementation of biological intelligence capable
    of extreme generalization, precipitating the start of the Anthropocene and forever
    changing the history of life on earth.
  prefs: []
  type: TYPE_NORMAL
- en: The progress of AI over the past 70 years bears striking similarities to this
    evolution. Early AI systems were pure automatons, like the ELIZA chat program
    from the 1960s, or SHRDLU,^([2](#Rendnote2)) a 1970 AI capable of manipulating
    simple objects from natural language commands. In the 1990s and 2000s, we saw
    the rise of machine learning systems capable of local generalization, which could
    deal with some level of uncertainty and novelty. In the 2010s, deep learning further
    expanded the local-generalization power of these systems by enabling engineers
    to leverage much larger datasets and much more expressive models.
  prefs: []
  type: TYPE_NORMAL
- en: Today, we may be on the cusp of the next evolutionary step. There is increasing
    interest in systems that could achieve *broad generalization*, which I define
    as the ability to deal with *unknown unknowns* within a single broad domain of
    tasks (including situations the system was not trained to handle and that its
    creators could not have anticipated), for instance, a self-driving car capable
    of safely dealing with any situation you throw at it, or a domestic robot that
    could pass the “Woz test of intelligence”—entering a random kitchen and making
    a cup of coffee.^([3](#Rendnote3)) By combining deep learning and painstakingly
    handcrafted abstract models of the world, we’re already making visible progress
    toward these goals.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, for the time being, AI remains limited to *cognitive automation*:
    the “intelligence” label in “artificial intelligence” is a category error. It
    would be more accurate to call our field “artificial cognition,” with “cognitive
    automation” and “artificial intelligence” being two nearly independent subfields
    within it. In this subdivision, “artificial intelligence” would be a greenfield
    where almost everything remains to be discovered.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, I don’t mean to diminish the achievements of deep learning. Cognitive
    automation is incredibly useful, and the way deep learning models are capable
    of automating tasks from exposure to data alone represents an especially powerful
    form of cognitive automation, far more practical and versatile than explicit programming.
    Doing this well is a game-changer for essentially every industry. But it’s still
    a long way from human (or animal) intelligence. Our models, so far, can perform
    only local generalization: they map space X to space Y via a smooth geometric
    transform learned from a dense sampling of X-to-Y data points, and any disruption
    within spaces X or Y invalidates this mapping. They can generalize only to new
    situations that stay similar to past data, whereas human cognition is capable
    of extreme generalization, quickly adapting to radically novel situations and
    planning for long-term future situations.'
  prefs: []
  type: TYPE_NORMAL
- en: 14.3 Setting the course toward greater generality in AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To lift some of the limitations we have discussed and create AI that can compete
    with human brains, we need to move away from straightforward input-to-output mappings
    and on to *reasoning* and *abstraction*. In the following couple of sections,
    we’ll take a look at what the road ahead may look like.
  prefs: []
  type: TYPE_NORMAL
- en: '14.3.1 On the importance of setting the right objective: The shortcut rule'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Biological intelligence was the answer to a question asked by nature. Likewise,
    if we want to develop true artificial intelligence, first, we need to be asking
    the right questions.
  prefs: []
  type: TYPE_NORMAL
- en: 'An effect you see constantly in systems design is the *shortcut rule*: if you
    focus on optimizing one success metric, you will achieve your goal, but at the
    expense of everything in the system that wasn’t covered by your success metric.
    You end up taking every available shortcut toward the goal. Your creations are
    shaped by the incentives you give yourself.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You see this often in machine learning competitions. In 2009, Netflix ran a
    challenge that promised a $1 million prize to the team that achieved the highest
    score on a movie-recommendation task. It ended up never using the system created
    by the winning team, because it was way too complex and compute intensive. The
    winners had optimized for prediction accuracy alone—what they were incentivized
    to achieve—at the expense of every other desirable characteristic of the system:
    inference cost, maintainability, and explainability. The shortcut rule holds true
    in most Kaggle competitions as well: the models produced by Kaggle winners can
    rarely, if ever, be used in production.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The shortcut rule has been everywhere in AI over the past few decades. In the
    1970s, psychologist and computer science pioneer Allen Newell, concerned that
    his field wasn’t making any meaningful progress toward a proper theory of cognition,
    proposed a new grand goal for AI: chess-playing. The rationale was that playing
    chess, in humans, seemed to involve—perhaps even require—capabilities such as
    perception, reasoning and analysis, memory, study from books, and so on. Surely,
    if we could build a chess-playing machine, it would have to feature these attributes
    as well. Right?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Over two decades later, the dream came true: in 1997, IBM’s Deep Blue beat
    Gary Kasparov, the best chess player in the world. Researchers had then to contend
    with the fact that creating a chess-champion AI had taught them little about human
    intelligence. The Alpha-Beta algorithm at the heart of Deep Blue wasn’t a model
    of the human brain and couldn’t generalize to tasks other than similar board games.
    It turned out it was easier to build an AI that could only play chess than to
    build an artificial mind—so that’s the shortcut researchers took.'
  prefs: []
  type: TYPE_NORMAL
- en: So far, the driving success metric of the field of AI has been to solve specific
    tasks, from chess to Go, from MNIST classification to ImageNet, from Atari arcade
    games to *StarCraft* and *Dota 2*. Consequently, the history of the field has
    been defined by a series of “successes” where we figured out how to solve these
    tasks *without featuring any intelligence*.
  prefs: []
  type: TYPE_NORMAL
- en: If that sounds like a surprising statement, keep in mind that human-like intelligence
    isn’t characterized by skill at any particular task—rather, it is the ability
    to adapt to novelty, to efficiently acquire new skills and master never-seen-before
    tasks. By fixing the task, you make it possible to provide an arbitrarily precise
    description of what needs to be done, either via hardcoding human-provided knowledge
    or by supplying humongous amounts of data. You make it possible for engineers
    to “buy” more skill for their AI by just adding data or adding hardcoded knowledge,
    without increasing the generalization power of the AI (see [figure 14.6](#fig14-6)).
    If you have near-infinite training data, even a very crude algorithm like nearest-neighbor
    search can play video games with superhuman skill. Likewise if you have a near-infinite
    amount of human-written if-then-else statements. That is, until you make a small
    change to the rules of the game—the kind a human could adapt to instantly—which
    will require the nonintelli-gent system to be retrained or rebuilt from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0494-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 14.6 A low-generalization system can achieve arbitrary skill at a
    fixed task given unlimited task-specific information.**'
  prefs: []
  type: TYPE_NORMAL
- en: In short, by fixing the task, you remove the need to handle uncertainty and
    novelty, and because the nature of intelligence is the ability to handle uncertainty
    and novelty, you’re effectively removing the need for intelligence. And because
    it’s always easier to find a non-intelligent solution to a specific task than
    to solve the general problem of intelligence, that’s the shortcut you will take
    100% of the time. Humans can use their general intelligence to acquire skills
    at any new task, but in reverse, there is no path from a collection of task-specific
    skills to general intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: 14.3.2 A new target
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To make artificial intelligence actually intelligent, and give it the ability
    to deal with the incredible variability and ever-changing nature of the real world,
    we first need to move away from seeking to achieve *task-specific skill* and,
    instead, start targeting generalization power itself. We need new metrics of progress
    that will help us develop increasingly intelligent systems, metrics that will
    point in the right direction and that will give us an actionable feedback signal.
    As long as we set our goal to be “create a model that solves task X,” the shortcut
    rule will apply, and we’ll end up with a model that does X, period.
  prefs: []
  type: TYPE_NORMAL
- en: 'In my view, intelligence can be precisely quantified as an *efficiency ratio*:
    the conversion ratio between the *amount of relevant information* you have available
    about the world (which could be either *past experience* or innate *prior knowledge*)
    and your *future operating area*, the set of novel situations where you will be
    able to produce appropriate behavior (you can view this as your *skill set*).
    A more intelligent agent will be able to handle a broader set of future tasks
    and situations using a smaller amount of past experience. To measure such a ratio,
    you just need to fix the information available to your system—its experience and
    its prior knowledge—and measure its performance on a set of reference situations
    or tasks that are known to be sufficiently different from what the system has
    had access to. Trying to maximize this ratio should lead you toward intelligence.
    Crucially, to avoid cheating, you’re going to need to make sure you test the system
    only on tasks it wasn’t programmed or trained to handle—in fact, you need tasks
    that the *creators of the system could not have anticipated*.'
  prefs: []
  type: TYPE_NORMAL
- en: In 2018 and 2019, I developed a benchmark dataset called the *Abstraction and
    Reasoning Corpus* (ARC)^([4](#Rendnote4)) that seeks to capture this definition
    of intelligence. ARC is meant to be approachable by both machines and humans,
    and it looks very similar to human IQ tests, such as Raven’s progressive matrices.
    At test time, you’ll see a series of “tasks.” Each task is explained via three
    or four “examples” that take the form of an input grid and a corresponding output
    grid (see [figure 14.7](#fig14-7)). You’ll then be given a brand-new input grid,
    and you’ll have three tries to produce the correct output grid before moving on
    to the next task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Compared to IQ tests, two things are unique about ARC. First, ARC seeks to
    measure generalization power, by only testing you on tasks you’ve never seen before.
    That means that ARC is *a game you can’t practice for*, at least in theory: the
    tasks you will be tested on will have their own unique logic that you will have
    to understand on the fly. You can’t just memorize specific strategies from past
    tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0496-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 14.7 An ARC task: The nature of the task is demonstrated by a couple
    of input-output pair examples. Provided with a new input, you must construct the
    corresponding output.**'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, ARC tries to control for the *prior knowledge* that you bring to
    the test. You never approach a new problem entirely from scratch—you bring to
    it preexisting skills and information. ARC makes the assumption that all test
    takers should start from the set of knowledge priors, called “Core Knowledge priors,”
    that represent the “knowledge systems” that humans are born with. Unlike an IQ
    test, ARC tasks will never involve acquired knowledge, like English sentences,
    for instance.
  prefs: []
  type: TYPE_NORMAL
- en: Unsurprisingly, deep-learning-based methods (including models trained on extremely
    large amounts of external data, like GPT-3) have proven entirely unable to solve
    ARC tasks, because these tasks are non-interpolative and thus are a poor fit for
    curve-fitting. Meanwhile, average humans have no issue solving these tasks on
    the first try, without any practice. When you see a situation like this, where
    humans as young as five are able to naturally perform something that seems to
    be completely out of reach for modern AI technology, that’s a clear signal that
    something interesting is going on—that we’re missing something.
  prefs: []
  type: TYPE_NORMAL
- en: 'What would it take to solve ARC? Hopefully, this challenge will get you thinking.
    That’s the entire point of ARC: to give you a goal of a different kind that will
    nudge you in a new direction—hopefully a productive direction. Now let’s take
    a quick look at the key ingredients you’re going to need if you want to answer
    the call.'
  prefs: []
  type: TYPE_NORMAL
- en: '14.4 Implementing intelligence: The missing ingredients'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, you’ve learned that there’s a lot more to intelligence than the sort
    of latent manifold interpolation that deep learning does. But what, then, do we
    need to start building real intelligence? What are the core pieces that are currently
    eluding us?
  prefs: []
  type: TYPE_NORMAL
- en: 14.4.1 Intelligence as sensitivity to abstract analogies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Intelligence is the ability to use your past experience (and innate prior knowledge)
    to face novel, unexpected future situations. If the future you had to face was
    *truly novel*— sharing no common ground with anything you’ve seen before—you’d
    be unable to react to it, no matter how intelligent you were.
  prefs: []
  type: TYPE_NORMAL
- en: Intelligence works because nothing is ever truly without precedent. When we
    encounter something new, we’re able to make sense of it by drawing analogies to
    our past experience, by articulating it in terms of the abstract concepts we’ve
    collected over time. A person from the 17th century seeing a jet plane for the
    first time might describe it as a large, loud metal bird that doesn’t flap its
    wings. A car? That’s a horseless carriage. If you’re trying to teach physics to
    a grade schooler, you can explain how electricity is like water in a pipe, or
    how space-time is like a rubber sheet getting distorted by heavy objects.
  prefs: []
  type: TYPE_NORMAL
- en: Besides such clear-cut, explicit analogies, we’re constantly making smaller,
    implicit analogies, every second, with every thought. Analogies are how we navigate
    life. Going to a new supermarket? You’ll find your way by relating it to similar
    stores you’ve been to. Talking to someone new? They’ll remind you of a few people
    you’ve met before. Even seemingly random patterns, like the shape of clouds, instantly
    evoke in us vivid images—an elephant, a ship, a fish.
  prefs: []
  type: TYPE_NORMAL
- en: 'These analogies aren’t just in our minds, either: physical reality itself is
    full of isomorphisms. Electromagnetism is analogous to gravity. Animals are all
    structurally similar to each other, due to shared origins. Silica crystals are
    similar to ice crystals. And so on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'I call this the *kaleidoscope hypothesis*: our experience of the world seems
    to feature incredible complexity and never-ending novelty, but everything in this
    sea of complexity is similar to everything else. The number of *unique atoms of
    meaning* that you need to describe the universe you live in is relatively small,
    and everything around you is a recombination of these atoms, a few seeds, endless
    variation—much like what goes on inside a kaleidoscope, where a few glass beads
    are reflected by a system of mirrors to produce rich, seemingly ever-changing
    patterns (see [figure 14.8](#fig14-8)).'
  prefs: []
  type: TYPE_NORMAL
- en: Generalization power—intelligence—is the ability to mine your experience to
    identify these atoms of meaning that can seemingly be reused across many different
    situations. Once extracted, they’re called *abstractions*. Whenever you encounter
    a new situation, you make sense of it via your accumulated collection of abstractions.
    How do you identify reusable atoms of meaning? Simply by noticing when two things
    are similar—by noticing analogies. If something is repeated twice, then both instances
    must have a single origin, like in a kaleidoscope. Abstraction is the engine of
    intelligence, and analogy-making is the engine that produces abstraction.
  prefs: []
  type: TYPE_NORMAL
- en: In short, intelligence is literally sensitivity to abstract analogies, and that’s
    in fact all there is to it. If you have a high sensitivity to analogies, you will
    extract powerful abstractions from little experience, and you will be able to
    use these abstractions to operate in a maximally large area of future experience
    space. You will be maximally efficient in converting past experience into the
    ability to handle future novelty.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0498-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 14.8 A kaleidoscope produces rich (yet repetitive) patterns from just
    a few beads of colored glass.**'
  prefs: []
  type: TYPE_NORMAL
- en: 14.4.2 The two poles of abstraction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If intelligence is sensitivity to analogies, then developing artificial intelligence
    should start with spelling out a step-by-step algorithm for analogy-making. Analogy-making
    starts with *comparing things to one another*. Crucially, there are *two distinct
    ways* to compare things, from which arise two different kinds of abstraction,
    two modes of thinking, each better suited to a different kind of problem. Together,
    these two poles of abstraction form the basis for all of our thoughts.
  prefs: []
  type: TYPE_NORMAL
- en: The first way to relate things to each other is *similarity comparison*, which
    gives rise to *value-centric analogies*. The second way is *exact structural match*,
    which gives rise to *program-centric analogies* (or structure-centric analogies).
    In both cases, you start from *instances* of a thing, and you merge together related
    instances to produce an *abstraction* that captures the common elements of the
    underlying instances. What varies is how you tell that two instances are related,
    and how you merge instances into abstractions. Let’s take a close look at each
    type.
  prefs: []
  type: TYPE_NORMAL
- en: VALUE-CENTRIC ANALOGY
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s say you come across a number of different beetles in your backyard, belonging
    to multiple species. You’ll notice similarities between them. Some will be more
    similar to one another, and some will be less similar: the notion of similarity
    is implicitly a smooth, continuous *distance function* that defines a latent manifold
    where your instances live. Once you’ve seen enough beetles, you can start clustering
    more similar instances together and merging them into a set of *prototypes* that
    captures the shared visual features of each cluster (see [figure 14.9](#fig14-9)).
    This prototype is abstract: it doesn’t look like any specific instance you’ve
    seen, though it encodes properties that are common across all of them. When you
    encounter a new beetle, you won’t need to compare it to every single beetle you’ve
    seen before to know what to do with it. You can simply compare it to your handful
    of prototypes, so as to find the closest prototype— the beetle’s *category*—and
    use it to make useful predictions: is the beetle likely to bite you? Will it eat
    your apples?'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0499-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 14.9 Value-centric analogy relates instances via a continuous notion
    of similarity to obtain abstract prototypes.**'
  prefs: []
  type: TYPE_NORMAL
- en: Does this sound familiar? It’s pretty much a description of what unsupervised
    machine learning (such as the *K*-means clustering algorithm) does. In general,
    all of modern machine learning, unsupervised or not, works by learning latent
    manifolds that describe a space of instances encoded via prototypes. (Remember
    the convnet features you visualized in chapter 9? They were visual prototypes.)
    Value-centric analogy is the kind of analogy-making that enables deep learning
    models to perform local generalization.
  prefs: []
  type: TYPE_NORMAL
- en: It’s also what many of your own cognitive abilities run on. As a human, you
    perform value-centric analogies all the time. It’s the type of abstraction that
    underlies *pattern recognition, perception*, and *intuition*. If you can do a
    task without thinking about it, you’re relying heavily on value-centric analogies.
    If you’re watching a movie and you start subconsciously categorizing the different
    characters into “types,” that’s value-centric abstraction.
  prefs: []
  type: TYPE_NORMAL
- en: PROGRAM-CENTRIC ANALOGY
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Crucially, there’s more to cognition than the kind of immediate, approximative,
    intuitive categorization that value-centric analogy enables. There’s another type
    of abstraction-generation mechanism that’s slower, exact, deliberate: program-centric
    (or structure-centric) analogy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In software engineering, you often write different functions or classes that
    seem to have a lot in common. When you notice these redundancies, you start asking,
    “could there be a more abstract function that performs the same job, that could
    be reused twice? Could there be an abstract base class that both of my classes
    could inherit from?” The definition of abstraction you’re using here corresponds
    to program-centric analogy. You’re not trying to compare your classes and functions
    by *how similar* they look the way you’d compare two human faces, via an implicit
    distance function. Rather, you’re interested in whether there are *parts* of them
    that have *exactly the same structure*. You’re looking for what is called a *subgraph
    isomorphism* (see [figure 14.10](#fig14-10)): programs can be represented as graphs
    of operators, and you’re trying to find subgraphs (program subsets) that are exactly
    shared across your different programs.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0500-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 14.10 Program-centric analogy identifies and isolates isomorphic substructures
    across different instances.**'
  prefs: []
  type: TYPE_NORMAL
- en: This kind of analogy-making via exact structural match within different discrete
    structures isn’t at all exclusive to specialized fields like computer science
    or mathematics— you’re constantly using it without noticing. It underlies *reasoning,
    planning*, and the general concept of *rigor* (as opposed to intuition). Any time
    you’re thinking about objects connected to each other by a discrete network of
    relationships (rather than a continuous similarity function), you’re leveraging
    program-centric analogies.
  prefs: []
  type: TYPE_NORMAL
- en: COGNITION AS A COMBINATION OF BOTH KINDS OF ABSTRACTION
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s compare these two poles of abstraction side by side (see [table 14.1](#tab14-1)).
  prefs: []
  type: TYPE_NORMAL
- en: Table 14.1 The two poles of abstraction
  prefs: []
  type: TYPE_NORMAL
- en: '| Value-centric abstraction | Program-centric abstraction |'
  prefs: []
  type: TYPE_TB
- en: '| Relates things by distance | Relates things by exact structural match |'
  prefs: []
  type: TYPE_TB
- en: '| Continuous, grounded in geometry | Discrete, grounded in topology |'
  prefs: []
  type: TYPE_TB
- en: '| Produces abstractions by “averaging” instances into “prototypes” | Produces
    abstractions by isolating isomorphic substructures across instances |'
  prefs: []
  type: TYPE_TB
- en: '| Underlies perception and intuition | Underlies reasoning and planning |'
  prefs: []
  type: TYPE_TB
- en: '| Immediate, fuzzy, approximative | Slow, exact, rigorous |'
  prefs: []
  type: TYPE_TB
- en: '| Requires a lot of experience to produce reliable results | Experience efficient;
    can operate on as few as two instances |'
  prefs: []
  type: TYPE_TB
- en: 14.4.3 The two poles of abstraction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Everything we do, everything we think, is a combination of these two types of
    abstraction. You’d be hard-pressed to find tasks that involve only one of the
    two. Even a seemingly “pure perception” task, like recognizing objects in a scene,
    involves a fair amount of implicit reasoning about the relationships between the
    objects you’re looking at. And even a seemingly “pure reasoning” task, like finding
    the proof of a mathematical theorem, involves a good amount of intuition. When
    a mathematician puts their pen to the paper, they’ve already got a fuzzy vision
    of the direction in which they’re going. The discrete reasoning steps they take
    to get to the destination are guided by high-level intuition.
  prefs: []
  type: TYPE_NORMAL
- en: These two poles are complementary, and it’s their interleaving that enables
    extreme generalization. No mind could be complete without both of them.
  prefs: []
  type: TYPE_NORMAL
- en: 14.4.4 The missing half of the picture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'By this point, you should start seeing what’s missing from modern deep learning:
    it’s very good at encoding value-centric abstraction, but it has basically no
    ability to generate program-centric abstraction. Human-like intelligence is a
    tight interleaving of both types, so we’re literally missing half of what we need—arguably
    the most important half.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, here’s a caveat. So far, I’ve presented each type of abstraction as entirely
    separate from the other—opposite, even. In practice, however, they’re more of
    a spectrum: to an extent, you could do reasoning by embedding discrete programs
    in continuous manifolds, just like you may fit a polynomial function through any
    set of discrete points, as long as you have enough coefficients. And inversely,
    you could use discrete programs to emulate continuous distance functions—after
    all, when you’re doing linear algebra on a computer, you’re working with continuous
    spaces, entirely via discrete programs that operate on ones and zeros.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there are clearly types of problems that are better suited to one
    or the other. Try to train a deep learning model to sort a list of five numbers,
    for instance. With the right architecture, it’s not impossible, but it’s an exercise
    in frustration. You’ll need a massive amount of training data to make it happen,
    and even then, the model will still make occasional mistakes when presented with
    new numbers. And if you want to start sorting lists of 10 numbers instead, you’ll
    need to completely retrain the model on even more data. Meanwhile, writing a sorting
    algorithm in R takes just a few lines, and the resulting program, once validated
    on a couple more examples, will work every time on lists of any size. That’s pretty
    strong generalization: going from a couple of demonstration examples and test
    examples to a program that can successfully process literally any list of numbers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In reverse, perception problems are a terrible fit for discrete reasoning processes.
    Try to write a pure-R program to classify MNIST digits without using any machine
    learning technique: you’re in for a ride. You’ll find yourself painstakingly coding
    functions that can detect the number of closed loops in a digit, the coordinates
    of the center of mass of a digit, and so on. After thousands of lines of code,
    you might achieve… 90% test accuracy. In this case, fitting a parametric model
    is much simpler; it can better utilize the large amount of data that’s available,
    and it achieves much more robust results. If you have lots of data and you’re
    faced with a problem where the manifold hypothesis applies, go with deep learning.'
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, it’s unlikely that we’ll see the rise of an approach that would
    reduce reasoning problems to manifold interpolation, or that would reduce perception
    problems to discrete reasoning. The way forward in AI is to develop a unified
    framework that incorporates *both* types of abstract analogy-making. Let’s examine
    what that might look like.
  prefs: []
  type: TYPE_NORMAL
- en: 14.5 The future of deep learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given what we know of how deep nets work, their limitations, and what they’re
    currently missing, can we predict where things are headed in the medium term?
    Following are some purely personal thoughts. Note that I don’t have a crystal
    ball, so a lot of what I anticipate may fail to become reality. I’m sharing these
    predictions not because I expect them to be proven completely right in the future
    but because they’re interesting and actionable in the present.
  prefs: []
  type: TYPE_NORMAL
- en: 'At a high level, these are the main directions in which I see promise:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Models closer to general-purpose computer programs*, built on top of far richer
    primitives than the current differentiable layers. This is how we’ll get to reasoning
    and abstraction, the lack of which is the fundamental weakness of current models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A fusion between deep learning and discrete search over program spaces*, with
    the former providing perception and intuition capabilities, and the latter providing
    reasoning and planning capabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Greater, systematic reuse of previously learned features and architectures*,
    such as meta-learning systems using reusable and modular program subroutines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, note that these considerations aren’t specific to the sort of
    supervised learning that has been the bread and butter of deep learning so far—rather,
    they’re applicable to any form of machine learning, including unsupervised, self-supervised,
    and reinforcement learning. It isn’t fundamentally important where your labels
    come from or what your training loop looks like; these different branches of machine
    learning are different facets of the same construct. Let’s dive in.
  prefs: []
  type: TYPE_NORMAL
- en: 14.5.1 Models as programs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As noted in the previous section, a necessary transformational development
    that we can expect in the field of machine learning is a move away from models
    that perform purely *pattern recognition* and can achieve only *local generalization*,
    toward models capable of abstraction and reasoning that can achieve *extreme generalization*.
    Current AI programs that are capable of basic forms of reasoning are all hardcoded
    by human programmers: for instance, software that relies on search algorithms,
    graph manipulation, and formal logic.'
  prefs: []
  type: TYPE_NORMAL
- en: 'That may be about to change, thanks to *program synthesis*—a field that is
    very niche today, but I expect to take off in a big way over the next few decades.
    Program synthesis consists of automatically generating simple programs by using
    a search algorithm (possibly genetic search, as in *genetic programming*) to explore
    a large space of possible programs (see [figure 14.11](#fig14-11)). The search
    stops when a program is found that matches the required specifications, often
    provided as a set of input-output pairs. This is highly reminiscent of machine
    learning: given training data provided as input-output pairs, we find a program
    that matches inputs to outputs and can generalize to new inputs. The difference
    is that instead of learning parameter values in a hardcoded program (a neural
    network), we generate source code via a discrete search process (see [table 14.2](#tab14-2)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0503-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 14.11 A schematic view of program synthesis: Given a program specification
    and a set of building blocks, a search process assembles the building blocks into
    candidate programs, which are then tested against the specification. The search
    continues until a valid program is found.**'
  prefs: []
  type: TYPE_NORMAL
- en: Table 14.2 Machine learning vs. program synthesis
  prefs: []
  type: TYPE_NORMAL
- en: '| Machine learning | Program synthesis |'
  prefs: []
  type: TYPE_TB
- en: '| Model: differentiable parametric function | Model: graph of operators from
    a programming language |'
  prefs: []
  type: TYPE_TB
- en: '| Engine: gradient descent | Engine: discrete search (such as genetic search)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Requires a lot of data to produce reliable results | Data efficient; can
    work with a couple of training examples |'
  prefs: []
  type: TYPE_TB
- en: 14.5.2 Machine learning vs. program synthesis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Program synthesis is how we’re going to add program-centric abstraction capabilities
    to our AI systems. It’s the missing piece of the puzzle. I mentioned earlier that
    deep learning techniques were entirely unusable on ARC, a reasoning-focused intelligence
    test. Meanwhile, very crude program-synthesis approaches are already producing
    very promising results on this benchmark.
  prefs: []
  type: TYPE_NORMAL
- en: 14.5.3 Blending together deep learning and program synthesis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Of course, deep learning isn’t going anywhere. Program synthesis isn’t its
    replacement; it is its complement. It’s the hemisphere that has been so far missing
    from our artificial brains. We’re going to be leveraging both, in combination.
    There are two major ways this will take place:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1** Developing systems that integrate both deep learning modules and discrete
    algorithmic modules'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**2** Using deep learning to make the program search process itself more efficient'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s review each of these possible avenues.
  prefs: []
  type: TYPE_NORMAL
- en: INTEGRATING DEEP LEARNING MODULES AND ALGORITHMIC MODULES INTO HYBRID SYSTEMS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Today, the most powerful AI systems are hybrid: they leverage both deep learning
    models and handcrafted symbol-manipulation programs. In DeepMind’s AlphaGo, for
    example, most of the intelligence on display is designed and hardcoded by human
    programmers (such as Monte Carlo Tree Search). Learning from data happens only
    in specialized submodules (value networks and policy networks). Or consider autonomous
    vehicles: a self-driving car is able to handle a large variety of situations because
    it maintains a model of the world around it—a literal 3D model—full of assumptions
    hardcoded by human engineers. This model is constantly updated via deep learning
    perception modules that interface it with the surroundings of the car.'
  prefs: []
  type: TYPE_NORMAL
- en: For both of these systems—AlphaGo and self-driving vehicles—the combination
    of human-created discrete programs and learned continuous models is what unlocks
    a level of performance that would be impossible with either approach in isolation,
    such as an end-to-end deep net or a piece of software without ML elements. So
    far, the discrete algorithmic elements of such hybrid systems are painstakingly
    hardcoded by human engineers. But in the future, such systems may be fully learned,
    with no human involvement.
  prefs: []
  type: TYPE_NORMAL
- en: 'What will this look like? Consider a well-known type of network: RNNs. It’s
    important to note that RNNs have slightly fewer limitations than feed-forward
    networks. That’s because RNNs are a bit more than mere geometric transformations:
    they’re geometric transformations *repeatedly applied inside a* for *loop*. The
    temporal for loop is itself hard-coded by human developers: it’s a built-in assumption
    of the network. Naturally, RNNs are still extremely limited in what they can represent,
    primarily because each step they perform is a differentiable geometric transformation,
    and they carry information from step to step via points in a continuous geometric
    space (state vectors). Now imagine a neural network that’s augmented in a similar
    way with programming primitives, but instead of a single hardcoded for loop with
    hardcoded continuous-space memory, the network includes a large set of programming
    primitives that the model is free to manipulate to expand its processing function,
    such as if branches, while statements, variable creation, disk storage for long-term
    memory, sorting operators, and advanced data structures (such as lists, graphs,
    and hash tables). The space of programs that such a network could represent would
    be far broader than what can be represented with current deep learning models,
    and some of these programs could achieve superior generalization power. Importantly,
    such programs will not be differentiable end-to-end, though specific modules will
    remain differentiable and thus will need to be generated via a combination of
    discrete program search and gradient descent.'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll move away from having, on one hand, hardcoded algorithmic intelligence
    (handcrafted software) and, on the other hand, learned geometric intelligence
    (deep learning). Instead, we’ll have a blend of formal algorithmic modules that
    provide reasoning and abstraction capabilities, and geometric modules that provide
    informal intuition and pattern-recognition capabilities (see [figure 14.12](#fig14-12)).
    The entire system will be learned with little or no human involvement. This should
    dramatically expand the scope of problems that can be solved with machine learning—the
    space of programs that we can generate automatically, given appropriate training
    data. Systems like AlphaGo—or even RNNs—can be seen as a prehistoric ancestor
    of such hybrid algorithmic-geometric models.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0505-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 14.12 A learned program relying on both geometric primitives (pattern
    recognition, intuition) and algorithmic primitives (reasoning, search, memory)**'
  prefs: []
  type: TYPE_NORMAL
- en: USING DEEP LEARNING TO GUIDE PROGRAM SEARCH
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Today, program synthesis faces a major obstacle: it’s tremendously inefficient.
    To caricature, program synthesis works by trying every possible program in a search
    space until it finds one that matches the specification provided. As the complexity
    of the program specification increases, or as the vocabulary of primitives used
    to write programs expands, the program search process runs into what’s known as
    *combinatorial explosion*, where the set of possible programs to consider grows
    very fast—in fact, much faster than merely exponentially fast. As a result, today,
    program synthesis can be used to generate only very short programs. You’re not
    going to be generating a new OS for your computer anytime soon.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To move forward, we’re going to need to make program synthesis efficient by
    bringing it closer to the way humans write software. When you open your editor
    to code up a script, you’re not thinking about every possible program you could
    potentially write. You have in mind only a handful of possible approaches: you
    can use your understanding of the problem and your past experience to drastically
    cut through the space of possible options to consider.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep learning can help program synthesis do the same: although each specific
    program we’d like to generate might be a fundamentally discrete object that performs
    non-interpolative data manipulation, evidence so far indicates that *the space
    of all useful programs* may look a lot like a continuous manifold. That means
    that a deep learning model that has been trained on millions of successful program-generation
    episodes might start to develop solid *intuition* about the *path through program
    space* that the search process should take to go from a specification to the corresponding
    program— just like a software engineer might have immediate intuition about the
    overall architecture of the script they’re about to write, about the intermediate
    functions and classes they should use as stepping-stones on the way to the goal.'
  prefs: []
  type: TYPE_NORMAL
- en: Remember that human reasoning is heavily guided by value-centric abstraction,
    that is, by pattern recognition and intuition. Program synthesis should be, too.
    I expect the general approach of guiding program search via learned heuristics
    to see increasing research interest over the next 10 to 20 years.
  prefs: []
  type: TYPE_NORMAL
- en: 14.5.4 Lifelong learning and modular subroutine reuse
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If models become more complex and are built on top of richer algorithmic primitives,
    this increased complexity will require higher reuse between tasks, rather than
    training a new model from scratch every time we have a new task or a new dataset.
    Many datasets don’t contain enough information for us to develop a new, complex
    model from scratch, and it will be necessary to use information from previously
    encountered datasets (much as you don’t learn English from scratch every time
    you open a new book—that would be impossible). Training models from scratch on
    every new task is also inefficient due to the large overlap between the current
    tasks and previously encountered tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'A remarkable observation has been made repeatedly in recent years: training
    the *same* model to do several loosely connected tasks at the same time results
    in a model that’s *better at each task*. For instance, training the same neural
    machine-translation model to perform both English-to-German translation and French-to-Italian
    translation will result in a model that’s better at each language pair. Similarly,
    training an image-classification model jointly with an image-segmentation model,
    sharing the same convolutional base, results in a model that’s better at both
    tasks. This is fairly intuitive: there’s always *some* information overlap between
    seemingly disconnected tasks, and a joint model has access to a greater amount
    of information about each individual task than a model trained on that specific
    task only.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, when it comes to model reuse across tasks, we use pretrained weights
    for models that perform common functions, such as visual feature extraction. You
    saw this in action in chapter 9\. In the future, I expect a generalized version
    of this to be commonplace: we’ll use not only previously learned features (submodel
    weights) but also model architectures and training procedures. As models become
    more like programs, we’ll begin to reuse *program subroutines* like the functions
    and classes found in human programming languages.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Think of the process of software development today: once an engineer solves
    a specific problem (HTTP queries, for instance), they package it as an abstract,
    reusable library. Engineers who face a similar problem in the future will be able
    to search for existing packages, download one, and use it in their own project.
    In a similar way, in the future, meta-learning systems will be able to assemble
    new programs by sifting through a global library of high-level reusable blocks.
    When the system finds itself developing similar program subroutines for several
    different tasks, it can come up with an *abstract*, reusable version of the subroutine
    and store it in the global library (see [figure 14.13](#fig14-1)). These subroutines
    can be either geometric (deep learning modules with pretrained representations)
    or algorithmic (closer to the libraries that contemporary software engineers manipulate).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0507-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 14.13 A meta-learner capable of quickly developing task-specific models
    using reusable primitives (both algorithmic and geometric), thus achieving extreme
    generalization**'
  prefs: []
  type: TYPE_NORMAL
- en: 14.5.5 The long-term vision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In short, here’s my long-term vision for machine learning:'
  prefs: []
  type: TYPE_NORMAL
- en: Models will be more like programs and will have capabilities that go far beyond
    the continuous geometric transformations of the input data we currently work with.
    These programs will arguably be much closer to the abstract mental models that
    humans maintain about their surroundings and themselves, and they will be capable
    of stronger generalization due to their rich algorithmic nature.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In particular, models will blend *algorithmic modules* providing formal reasoning,
    search, and abstraction capabilities with *geometric modules* providing informal
    intuition and pattern-recognition capabilities. This will achieve a blend of value-centric
    and program-centric abstraction. AlphaGo or self-driving cars (systems that require
    a lot of manual software engineering and human-made design decisions) provide
    an early example of what such a blend of symbolic and geometric AI could look
    like.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Such models will be *grown* automatically rather than hardcoded by human engineers,
    using modular parts stored in a global library of reusable subroutines—a library
    evolved by learning high-performing models on thousands of previous tasks and
    datasets. As frequent problem-solving patterns are identified by the meta-learning
    system, they will be turned into reusable subroutines—much like functions and
    classes in software engineering—and added to the global library.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The process that searches over possible combinations of subroutines to grow
    new models will be a discrete search process (program synthesis), but it will
    be heavily guided by a form of *program-space intuition* provided by deep learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This global subroutine library and associated model-growing system will be
    able to achieve some form of humanlike *extreme generalization*: given a new task
    or situation, the system will be able to assemble a new working model appropriate
    for the task using very little data, thanks to rich programlike primitives that
    generalize well and extensive experience with similar tasks. In the same way,
    humans can quickly learn to play a complex new video game if they have experience
    with many previous games, because the models derived from this previous experience
    are abstract and programlike, rather than a basic mapping between stimuli and
    action.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As such, this perpetually learning model-growing system can be interpreted
    as an *artificial general intelligence* (AGI). But don’t expect any singularitarian
    robot apocalypse to ensue: that’s pure fantasy, coming from a long series of profound
    misunderstandings of both intelligence and technology. Such a critique, however,
    doesn’t belong in this book'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 14.6 Staying up-to-date in a fast-moving field
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As final parting words, I want to give you some pointers about how to keep learning
    and updating your knowledge and skills after you’ve turned the last page of this
    book. The field of modern deep learning, as we know it today, is only a few years
    old, despite a long, slow prehistory stretching back decades. With an exponential
    increase in financial resources and research headcount since 2013, the field as
    a whole is now moving at a frenetic pace. What you’ve learned in this book won’t
    stay relevant forever, and it isn’t all you’ll need for the rest of your career.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, there are plenty of free online resources that you can use to stay
    up to date and expand your horizons. Here are a few.
  prefs: []
  type: TYPE_NORMAL
- en: 14.6.1 Practice on real-world problems using Kaggle
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An effective way to acquire real-world experience is to try your hand at machine
    learning competitions on Kaggle ([https://kaggle.com](https://www.kaggle.com)).
    The only real way to learn is through practice and actual coding—that’s the philosophy
    of this book, and Kaggle competitions are the natural continuation of this. On
    Kaggle, you’ll find an array of constantly renewed data science competitions,
    many of which involve deep learning, prepared by companies interested in obtaining
    novel solutions to some of their most challenging machine learning problems. Fairly
    large monetary prizes are offered to top entrants.
  prefs: []
  type: TYPE_NORMAL
- en: Most competitions are won using either the XGBoost library (for shallow machine
    learning) or Keras (for deep learning), so you’ll fit right in! By participating
    in a few competitions, maybe as part of a team, you’ll become more familiar with
    the practical side of some of the advanced best practices described in this book,
    especially hyper-parameter tuning, avoiding validation set overfitting, and model
    ensembling.
  prefs: []
  type: TYPE_NORMAL
- en: 14.6.2 Read about the latest developments on arXiv
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Deep learning research, in contrast with some other scientific fields, takes
    place completely in the open. Papers are made publicly and freely accessible as
    soon as they’re finalized, and a lot of related software is open source. arXiv
    ([https://arxiv.org](https://www.arxiv.org))—pronounced “archive” (the X stands
    for the Greek *chi*)—is an open-access preprint server for physics, mathematics,
    and computer science research papers. It has become the de facto way to stay up-to-date
    on the bleeding edge of machine learning and deep learning. The large majority
    of deep learning researchers upload any paper they write to arXiv shortly after
    completion. This allows them to plant a flag and claim a specific finding without
    waiting for a conference acceptance (which takes months), which is necessary given
    the fast pace of research and the intense competition in the field. It also allows
    the field to move extremely fast: all new findings are immediately available for
    all to see and to build on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An important downside is that the sheer quantity of new papers posted every
    day on arXiv makes it impossible to even skim them all, and the fact that they
    aren’t peer-reviewed makes it difficult to identify those that are both important
    and high quality. It’s challenging, and becoming increasingly more so, to find
    the signal in the noise. But some tools can help: in particular, you can use Google
    Scholar ([https://scholar.google.com](https://www.scholar.google.com)) to keep
    track of publications by your favorite authors.'
  prefs: []
  type: TYPE_NORMAL
- en: 14.6.3 Explore the Keras ecosystem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With over one million users as of late 2021 and still growing, Keras has a
    large ecosystem of tutorials, guides, and related open source projects:'
  prefs: []
  type: TYPE_NORMAL
- en: Your main reference for working with Keras in R is the online documentation
    at [https://keras.rstudio.com](https://www.keras.rstudio.com) and [https://tensorflow.rstudio.com](https://www.tensorflow.rstudio.com).
    In particular, you’ll find extensive developer guides at [http://tensorflow.rstudio.com/guides](http://www.tensorflow.rstudio.com/guides),
    dozens of high-quality Keras code examples at [http://tensorflow.rstudio.com/](http://www.tensorflow.rstudio.com/)
    examples, and many tutorials at [http://tensorflow.rstudio.com/tutorials](http://www.tensorflow.rstudio.com/tutorials).
    Make sure to check them out!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Don’t hesitate to also consult the Python documentation for Keras and Tensor-Flow,
    available at [https://www.tensorflow.org/api_docs/python/tf](https://www.tensorflow.org/api_docs/python/tf)
    and [https://keras.io/](https://www.keras.io/), even if you don’t know Python.
    Almost everything there you can read, understand, and apply to the R interface,
    all without any knowledge of Python. (If you do encounter some perplexing Python
    syntax, be sure to consult the appendix.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The R source code for Keras and Tensorflow can be found at [https://github.com/rstudio/keras](https://www.github.com/rstudio/keras)
    and [https://github.com/rstudio/tensorflow](https://www.github.com/rstudio/tensorflow).
    The Python and C++ sources are available at [https://github.com/keras-team/keras](https://www.github.com/keras-team/keras)
    and [https://github.com/tensorflow/tensorflow](https://www.github.com/tensorflow/tensorflow).
    All are open source.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can ask for help and join deep learning discussions in a few places:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Machine Learning section of Rstudio community: [https://community.rstudio.com/c/ml/15](https://www.community.rstudio.com/c/ml/15)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stack overflow: [https://stackoverflow.com](https://www.stackoverflow.com)
    (Be sure to tag your question with both R and Keras.)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The (Python) Keras mailing list: keras-users@googlegroups.com.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can follow me (François) on Twitter: @fchollet.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 14.7 Final words
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is the end of *Deep Learning with R, Second Edition*. I hope you’ve learned
    a thing or two about machine learning, deep learning, Keras, and maybe even cognition
    in general. Learning is a lifelong journey, especially in the field of AI, where
    we have far more unknowns on our hands than certitudes. So please go on learning,
    questioning, and researching. Never stop! Because even given the progress made
    so far, most of the fundamental questions in AI remain unanswered. Many haven’t
    even been properly asked yet.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](#endnote1)) Richard Feynman, interview, “The World from Another Point
    of View,” Yorkshire Television, 1972.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ^([2](#endnote2)) Terry Winograd, “Procedures as a Representation for Data in
    a Computer Program for Understanding Natural Language” (1971).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '^([3](#endnote3)) Fast Company, “Wozniak: Could a Computer Make a Cup of Coffee?”
    (March 2010), [http://mng.bz/pJMP](http://mng.bz/pJMP).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ^([4](#endnote4)) François Chollet, “On the Measure of Intelligence” (2019),
    [https://arxiv.org/abs/1911.01547](https://arxiv.org/abs/1911.01547).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
