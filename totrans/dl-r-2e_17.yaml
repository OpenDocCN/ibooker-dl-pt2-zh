- en: 14 Conclusions
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*This chapter covers*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Important takeaways from this book
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The limitations of deep learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Possible future directions for deep learning, machine learning, and AI
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resources for further learning and applying your skills in practice
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You’ve almost reached the end of this book. This last chapter will summarize
    and review core concepts while also expanding your horizons beyond what you’ve
    learned so far. Becoming an effective AI practitioner is a journey, and finishing
    this book is merely your first step on it. I want to make sure you realize this
    and are properly equipped to take the next steps of this journey on your own.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: We’ll start with a bird’s-eye view of what you should take away from this book.
    This should refresh your memory regarding some of the concepts you’ve learned.
    Next, I’ll present an overview of some key limitations of deep learning. To use
    a tool appropriately, you should not only understand what it *can* do but also
    be aware of what it *can’t* do. Finally, I’ll offer some speculative thoughts
    about the future evolution of deep learning, machine learning, and AI. This should
    be especially interesting to you if you’d like to get into fundamental research.
    The chapter ends with a short list of resources and strategies for further learning
    about machine learning and staying up to date with new advances.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 14.1 Key concepts in review
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section briefly synthesizes key takeaways from this book. If you ever need
    a quick refresher to help you recall what you’ve learned, you can read these few
    pages.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 14.1.1 Various approaches to AI
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First of all, deep learning isn’t synonymous with AI, or even with machine
    learning:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '*Artificial intelligence* (AI) is an ancient, broad field that can generally
    be understood as “all attempts to automate human cognitive processes.” This can
    range from the very basic, such as an Excel spreadsheet, to the very advanced,
    like a humanoid robot that can walk and talk.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Machine learning* is a specific subfield of AI that aims at automatically
    developing programs (called *models*) purely from exposure to training data. This
    process of turning data into a program is called *learning*. Although machine
    learning has been around for a long time, it started to take off only in the 1990s,
    before becoming the dominant form of AI in the 2000s.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deep learning* is one of many branches of machine learning, where the models
    are long chains of geometric transformations, applied one after the other. These
    operations are structured into modules called *layers*: deep learning models are
    typically stacks of layers—or, more generally, graphs of layers. These layers
    are parameterized by *weights*, which are the parameters learned during training.
    The *knowledge* of a model is stored in its weights, and the process of learning
    consists of finding “good values” for these weights—values that minimize a *loss
    function*. Because the chain of geometric transformations considered is differentiable,
    updating the weights to minimize the loss function is done efficiently via *gradient
    descent*.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*深度学习*是机器学习的众多分支之一，其中模型是长链的几何变换，依次应用。这些操作被结构化为称为*层*的模块：深度学习模型通常是层的堆叠——或者更普遍地，层的图形。这些层由*权重*参数化，在训练期间学习。模型的*p学习是存储在其权重中的知识，并且学习过程包括找到这些权重的“好值”——最小化*损失函数*的值。由于考虑的几何变换链是可微的，通过*梯度下降*有效地更新权重以最小化损失函数。'
- en: Even though deep learning is just one among many approaches to machine learning,
    it isn’t on an equal footing with the others. Deep learning is a breakout success.
    Here’s why.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 即使深度学习只是机器学习中的一种方法，它也与其他方法不平等。深度学习是突破性的成功。以下是原因。
- en: 14.1.2 What makes deep learning special within the field of machine learning
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.1.2 深度学习在机器学习领域中所特殊之处
- en: 'In the span of only a few years, deep learning has achieved tremendous breakthroughs
    across a wide range of tasks that have been historically perceived as extremely
    difficult for computers, especially in the area of machine perception: extracting
    useful information from images, videos, sound, and more. Given sufficient training
    data (in particular, training data appropriately labeled by humans), deep learning
    makes it possible to extract from perceptual data almost anything a human could.
    Hence, it’s sometimes said that deep learning has “solved perception”— although
    that’s true only for a fairly narrow definition of perception.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅在几年的时间内，深度学习已经在历史上被认为是计算机极为困难的一系列任务中取得了巨大的突破，尤其是在机器感知的领域：从图像、视频、声音等感知数据中提取有用信息。在充足的训练数据（特别是由人类正确标注的训练数据）的情况下，深度学习可以从感知数据中提取出几乎任何一个人可能做出的内容。因此，有时被称为深度学习“解决了感知问题”——尽管这仅针对感知的一个相当狭窄的定义而言。
- en: 'Due to its unprecedented technical successes, deep learning has singlehandedly
    brought about the third and by far the largest *AI summer*: a period of intense
    interest, investment, and hype in the field of AI. As this book is being written,
    we’re in the middle of it. Whether this period will end in the near future, and
    what happens after it ends, are topics of debate. One thing is certain: in stark
    contrast with previous AI summers, deep learning has provided enormous business
    value to both large and small technology companies, enabling human-level speech
    recognition, smart assistants, human-level image classification, vastly improved
    machine translation, and more. The hype may (and likely will) recede, but the
    sustained economic and technological impact of deep learning will remain. In that
    sense, deep learning could be analogous to the internet: it may be overly hyped
    for a few years, but in the longer term, it will still be a major revolution that
    will transform our economy and our lives.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其前所未有的技术成功，深度学习独自引发了第三个、迄今为止最大的“人工智能夏令营”：一段对人工智能领域充满了浓厚兴趣、投资和炒作的时期。在本书撰写时，我们正处于其中。这一时期是否会在不久的将来结束，以及结束后会发生什么，都是有争议的话题。有一件事情是肯定的：与以往的人工智能夏令营形成鲜明反差的是，深度学习对大型和小型科技公司都带来了巨大的商业价值，实现了人类级别的语音识别、智能助手、人类级别的图像分类、大幅改进的机器翻译等。炒作可能（并很可能会）消退，但深度学习的可持续经济和技术影响将会保持存在。在这个意义上，深度学习可能类似于互联网：它可能在短时间内被过度炒作，但从更长期看，它仍将是一场重大的革命，将改变我们的经济和生活。
- en: I’m particularly optimistic about deep learning, because even if we were to
    make no further technological progress in the next decade, deploying existing
    algorithms to every applicable problem would be a game changer for most industries.
    Deep learning is nothing short of a revolution, and progress is currently happening
    at an incredibly fast rate, due to an exponential investment in resources and
    headcount. From where I stand, the future looks bright, although short-term expectations
    are somewhat overoptimistic; deploying deep learning to the full extent of its
    potential will likely take multiple decades.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我对深度学习特别乐观，因为即使在未来十年内我们不再取得技术上的进步，将现有算法应用到每个适用的问题上都将改变大多数行业的游戏规则。深度学习无异于一场革命，而目前的进展正以令人难以置信的速度发生，这要归功于对资源和人力的指数级投资。站在我现在的位置看，未来看起来很光明，尽管短期内的期望有些过于乐观；要充分发挥深度学习的潜力可能需要数十年时间。
- en: 14.1.3 How to think about deep learning
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.1.3 如何思考深度学习
- en: The most surprising thing about deep learning is how simple it is. Ten years
    ago, no one expected that we would achieve such amazing results on machine-perception
    problems by using simple parametric models trained with gradient descent. Now
    it turns out that all you need is sufficiently large parametric models trained
    with gradient descent on sufficiently many examples. As Feynman once said about
    the universe, “It’s not complicated, it’s just a lot of it.^([1](#Rendnote1))
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 关于深度学习最令人惊讶的是它有多简单。十年前，没有人预料到我们会通过使用简单的参数模型，使用梯度下降训练的方式，在机器感知问题上取得如此惊人的结果。现在，事实证明你所需要的一切只是足够大的参数模型，以及在足够多的示例上使用梯度下降进行训练。正如费曼曾经说过关于宇宙的，“它并不复杂，只是很多而已。”^([1](#Rendnote1))
- en: In deep learning, everything is a vector—that is to say, everything is a *point*
    in a *geometric space*. Model inputs (text, images, and so on) and targets are
    first *vectorized*— turned into an initial input vector space and target vector
    space. Each layer in a deep learning model operates one simple geometric transformation
    on the data that goes through it. Together, the chain of layers in the model forms
    one complex geometric transformation, broken down into a series of simple ones.
    This complex transformation attempts to map the input space to the target space,
    one point at a time. This transformation is parameterized by the weights of the
    layers, which are iteratively updated based on how well the model is currently
    performing. A key characteristic of this geometric transformation is that it must
    be *differentiable*, which is required for us to be able to learn its parameters
    via gradient descent. Intuitively, this means the geometric morphing from inputs
    to outputs must be smooth and continuous—a significant constraint.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，一切都是向量——也就是说，一切都是*几何空间*中的*点*。模型输入（文本、图像等）和目标首先被*向量化*——转换为初始输入向量空间和目标向量空间。深度学习模型中的每一层对通过它的数据进行一次简单的几何变换。模型中层的链条一起形成一个复杂的几何变换，分解为一系列简单的变换。这个复杂的变换试图将输入空间一点一点地映射到目标空间。这个变换由层的权重参数化，这些权重根据模型当前的性能进行迭代更新。这个几何变换的一个关键特性是它必须是*可微分*的，这是我们能够通过梯度下降来学习其参数的必要条件。直觉上，这意味着从输入到输出的几何变形必须是平滑和连续的——这是一个重要的约束。
- en: 'The entire process of applying this complex geometric transformation to the
    input data can be visualized in 3D by imagining a person trying to uncrumple a
    paper ball: the crumpled paper ball is the manifold of the input data that the
    model starts with. Each movement operated by the person on the paper ball is similar
    to a simple geometric transformation operated by one layer. The full uncrumpling
    gesture sequence is the complex transformation of the entire model. Deep learning
    models are mathematical machines for uncrumpling complicated manifolds of high-dimensional
    data.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 将这个复杂的几何变换应用到输入数据的整个过程可以在3D中可视化，就好像一个人试图展开一个纸球：皱巴巴的纸球就是模型初始的输入数据的流形。人对纸球施加的每一个移动都类似于一个简单的几何变换，由一个层操作。完整的展开手势序列是整个模型的复杂变换。深度学习模型是用于展开高维数据复杂流形的数学机器。
- en: 'That’s the magic of deep learning: turning meaning into vectors, then into
    geometric spaces, and then incrementally learning complex geometric transformations
    that map one space to another. All you need are spaces of sufficiently high dimensionality
    to capture the full scope of the relationships found in the original data.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是深度学习的魔力：将意义转化为向量，然后转化为几何空间，然后逐渐学习将一个空间映射到另一个空间的复杂几何变换。你所需要的只是足够高维度的空间来捕捉原始数据中发现的关系的全部范围。
- en: 'The whole process hinges on a single core idea: *that meaning is derived from
    the pairwise relationship between things* (between words in a language, between
    pixels in an image, and so on) and that *these relationships can be captured by
    a distance function*. But note that whether the brain also implements meaning
    via geometric spaces is an entirely separate question. Vector spaces are efficient
    to work with from a computational standpoint, but different data structures for
    intelligence can easily be envisioned—in particular, graphs. Neural networks initially
    emerged from the idea of using graphs as a way to encode meaning, which is why
    they’re named *neural networks*; the surrounding field of research used to be
    called *connectionism*. Nowadays the name “neural network” exists purely for historical
    reasons—it’s an extremely misleading name because they’re neither neural nor networks.
    In particular, neural networks have hardly anything to do with the brain. A more
    appropriate name would have been *layered representations learning* or *hierarchical
    representations learning*, or maybe even *deep differentiable models* or *chained
    geometric transforms*, to emphasize the fact that continuous geometric space manipulation
    is at their core.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 整个过程的关键思想在于：*意义来源于事物之间的成对关系*（在语言中的单词之间，在图像中的像素之间等），*这些关系可以用距离函数来捕捉*。但请注意，大脑是否也通过几何空间实现了意义是一个完全独立的问题。从计算的角度来看，向量空间是有效的，但是可以很容易地构想出不同的智能数据结构——特别是图形。神经网络最初是从使用图形作为编码意义的一种方式的想法中产生的，这就是为什么它们被命名为*神经网络*；周围的研究领域曾经被称为*连接主义*。现在“神经网络”的名字纯粹是出于历史原因——这是一个极其误导性的名字，因为它们既不是神经也不是网络。特别是神经网络与大脑几乎毫无关系。一个更合适的名称可能应该是*层次表示学习*或*深度可微模型*或*链式几何变换*，以强调连续几何空间操作是它们的核心。
- en: 14.1.4 Key enabling technologies
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.1.4 关键技术
- en: 'The technological revolution that’s currently unfolding didn’t start with any
    single breakthrough invention. Rather, like any other revolution, it’s the product
    of a vast accumulation of enabling factors—gradual at first, and then sudden.
    In the case of deep learning, we can point out the following key factors:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当前正在展开的技术革命并不是始于任何单一的突破性发明。相反，就像任何其他革命一样，它是巨大的 enabling 因素的积累的产物——一开始是渐进的，然后是突然的。在深度学习的情况下，我们可以指出以下关键因素：
- en: '*Incremental algorithmic innovations*—These first began appearing slowly over
    the span of two decades (starting with backpropagation), and then were developed
    increasingly faster as more research effort was poured into deep learning after
    2012.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*渐进的算法创新*——这些创新最初在两个十年内缓慢出现（从反向传播开始），然后随着在2012年之后更多的研究工作投入到深度学习中而日益加快发展。'
- en: '*The availability of large amounts of perceptual data*—This was a requirement
    in order to realize that sufficiently large models trained on sufficiently large
    data are all we need. This is, in turn, a byproduct of the rise of the consumer
    internet and Moore’s law applied to storage media.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*大量感知数据的可用性*——这是一个前提，以便意识到在足够大的数据上训练的足够大的模型是我们所需要的。这又是消费互联网的崛起和摩尔定律应用于存储介质的副产品。'
- en: '*The availability of fast, highly parallel computation hardware at a low price*—Especially
    the GPUs produced by NVIDIA—first gaming GPUs and then chips designed from the
    ground up for deep learning. Early on, NVIDIA CEO Jensen Huang took note of the
    deep learning boom and decided to bet the company’s future on it, which paid off
    in a big way.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*价格低廉的快速、高度并行计算硬件的可用性*——尤其是由NVIDIA生产的GPU——首先是游戏GPU，然后是从头开始设计用于深度学习的芯片。早在初期，NVIDIA首席执行官黄仁勋注意到了深度学习的兴起，并决定把公司的未来押注在这一点上，这种决定带来了巨大的回报。'
- en: '*A complex stack of software layers that makes this computational power available
    to humans*—The CUDA language, frameworks like TensorFlow that do automatic differentiation,
    and Keras, which makes deep learning accessible to most people.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the future, deep learning will not be used only by specialists—researchers,
    graduate students, and engineers with an academic profile—it will be a tool in
    the toolbox of every developer, much like web technology today. Everyone needs
    to build intelligent apps: just as every business today needs a website, every
    product will need to intelligently make sense of user-generated data. Bringing
    about this future will require us to build tools that make deep learning radically
    easy to use and accessible to anyone with basic coding abilities. Keras has been
    the first major step in that direction.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 14.1.5 The universal machine learning workflow
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Having access to an extremely powerful tool for creating models that map any
    input space to any target space is great, but the difficult part of the machine
    learning work-flow is often everything that comes before designing and training
    such models (and, for production models, what comes after, as well). Understanding
    the problem domain so as to be able to determine what to attempt to predict, given
    what data, and how to measure success, is a prerequisite for any successful application
    of machine learning, and it isn’t something that advanced tools like Keras and
    TensorFlow can help you with. As a reminder, here’s a quick summary of the typical
    machine learning workflow as described in chapter 6:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '**1** Define the problem. What data is available, and what are you trying to
    predict? Will you need to collect more data or hire people to manually label a
    dataset?'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**2** Identify a way to reliably measure success on your goal. For simple tasks,
    this may be prediction accuracy, but in many cases, it will require sophisticated,
    domain-specific metrics.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**3** Prepare the validation process that you’ll use to evaluate your models.
    In particular, you should define a training set, a validation set, and a test
    set. The validation and test set labels shouldn’t leak into the training data:
    for instance, with temporal prediction, the validation and test data should be
    posterior to the training data.'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**4** Vectorize the data by turning it into vectors and preprocessing it in
    a way that makes it more easily approachable by a neural network (normalization
    and so on).'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**5** Develop a first model that beats a trivial common-sense baseline, thus
    demonstrating that machine learning can work on your problem. This may not always
    be the case!'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**6** Gradually refine your model architecture by tuning hyperparameters and
    adding regularization. Make changes based on performance on the validation data
    only, not the test data or the training data. Remember that you should get your
    model to overfit (thus identifying a model capacity level that’s greater than
    you need) and only then begin to add regularization or downsize your model. Beware
    of validation-set overfitting when tuning hyperparameters—your hyper-parameters
    may end up being overspecialized to the validation set. Avoiding this is the purpose
    of having a separate test set.'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**6** 通过调整超参数和添加正则化逐渐完善模型架构。基于验证数据的表现进行更改，而不是测试数据或训练数据。记住，你应该使你的模型过度拟合（因此识别出一个比你需要的模型容量水平更高的水平），然后才开始添加正则化或减小模型尺寸。在调整超参数时要小心验证集的过拟合
    - 你的超参数可能最终被过于专门化于验证集。避免这种情况是拥有一个单独的测试集的目的。'
- en: '**7** Deploy your final model in production—as a web API, as part of a JavaScript
    or C++ application, on an embedded device, and so on. Keep monitoring its performance
    on real-world data, and use your findings to refine the next iteration of the
    model!'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**7** 将最终模型部署到生产环境 - 作为web API、JavaScript或C++应用的一部分、嵌入式设备等等。继续监控其在真实世界数据上的性能，并利用您的发现来完善模型的下一个迭代版本！'
- en: 14.1.6 Key network architectures
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.1.6 关键的网络架构
- en: 'The four families of network architectures that you should be familiar with
    are *densely connected networks, convolutional networks, recurrent networks*,
    and *Transformers*. Each type of model is meant for a specific input modality.
    A network architecture encodes *assumptions* about the structure of the data:
    a *hypothesis space* within which the search for a good model will proceed. Whether
    a given architecture will work on a given problem depends entirely on the match
    between the structure of the data and the assumptions of the network architecture.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该熟悉的四个网络架构家族分别是*密集连接网络、卷积网络、循环网络*和*Transformer*。每种模型类型针对特定的输入模态。网络架构对数据结构有*假设*：一个良好模型搜索将进行的*假设空间*。给定架构是否适用于给定问题完全取决于数据结构与网络架构假设之间的匹配。
- en: 'These different network types can easily be combined to achieve larger multi-modal
    models, much as you combine LEGO bricks. In a way, deep learning layers are LEGO
    bricks for information processing. Here’s a quick overview of the mapping between
    input modalities and appropriate network architectures:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这些不同类型的网络可以轻松地组合在一起，以实现更大的多模型，就像组合乐高积木一样。从某种意义上说，深度学习层是信息处理的乐高积木。以下是输入模态和适当的网络架构之间的快速映射概述：
- en: '*Vector data*—Densely connected models.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*矢量数据* - 密集连接模型。'
- en: '*Image data*—2D convnets.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图像数据* - 2D 卷积神经网络。'
- en: '*Sequence data*—RNNs for time series, or Transformers for discrete sequences
    (such as sequences of words). 1D convnets can also be used for translation-invariant,
    continuous sequence data, such as birdsong waveforms.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*序列数据* - 用于时间序列的RNN，或用于离散序列（例如单词序列）的Transformer。也可以使用 1D 卷积神经网络来处理翻译不变的连续序列数据，例如鸟鸣波形。'
- en: '*Video data*—Either 3D convnets (if you need to capture motion effects), or
    a combination of a frame-level 2D convnet for feature extraction followed by a
    sequence-processing model.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*视频数据* - 3D 卷积神经网络（如果需要捕捉运动效果），或者是一个用于特征提取的帧级 2D 卷积神经网络，后跟一个序列处理模型的组合。'
- en: '*Volumetric data*—3D convnets.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*体积数据* - 3D 卷积神经网络。'
- en: Now let’s quickly review the specificities of each network architecture.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们快速复习每种网络架构的特点。
- en: DENSELY CONNECTED NETWORKS
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 密集连接网络
- en: 'A densely connected network is a stack of Dense layers meant to process vector
    data (where each sample is a vector of numerical or categorical attributes). Such
    networks assume no specific structure in the input features: they’re called *densely
    connected* because the units of a Dense layer are connected to every other unit.
    The layer attempts to map relationships between any two input features; this is
    unlike a 2D convolution layer, for instance, which looks only at *local* relationships.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 密集连接网络是一堆用于处理矢量数据的密集层（其中每个样本都是数字或分类属性的矢量）。这些网络假设输入特征没有特定的结构：它们被称为*密集连接*，因为密集层的单元与每个其他单元相连。该层试图映射任何两个输入特征之间的关系；这与例如只关注*局部*关系的
    2D 卷积层不同。
- en: Densely connected networks are most commonly used for categorical data (e.g.,
    where the input features are lists of attributes), such as the Boston housing
    price data-set used in chapter 4\. They’re also used as the final classification
    or regression stage of most networks. For instance, the convnets covered in chapter
    8 typically end with one or two Dense layers, and so do the recurrent networks
    in chapter 10.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember, to perform *binary classification*, end your stack of layers with
    a Dense layer with a single unit and a sigmoid activation, and use binary_crossentropy
    as the loss. Your targets should be either 0 or 1:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: inputs <- layer_input(shape = c(num_inputs_features))
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: outputs <- inputs %>%
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(32, activation = "relu") %>%
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(32, activation = "relu") %>%
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(1, activation = "sigmoid")
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: model <- keras_model(inputs, outputs)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: model %>% compile(optimizer = "rmsprop", loss = "binary_crossentropy")
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform *single-label categorical classification* (where each sample has
    exactly one class, no more), end your stack of layers with a Dense layer with
    a number of units equal to the number of classes, and a softmax activation. If
    your targets are one-hot encoded, use categorical_crossentropy as the loss; if
    they’re integers, use sparse_categorical_ crossentropy:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: inputs <- layer_input(shape = c(num_inputs_features))
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: outputs <- inputs %>%
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(32, activation = "relu") %>%
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(32, activation = "relu") %>%
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(num_classes, activation = "softmax")
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: model <- keras_model(inputs, outputs)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: model %>% compile(optimizer = "rmsprop", loss = "categorical_crossentropy")
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform *multilabel categorical classification* (where each sample can have
    several classes), end your stack of layers with a Dense layer with a number of
    units equal to the number of classes, and a sigmoid activation, and use binary_crossentropy
    as the loss. Your targets should be multi-hot encoded:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: inputs <- layer_input(shape = c(num_inputs_features))
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: outputs <- inputs %>%
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(32, activation = "relu") %>%
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(32, activation = "relu") %>%
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(num_classes, activation = "sigmoid")
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: model <- keras_model(inputs, outputs)
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: model %>% compile(optimizer = "rmsprop", loss = "binary_crossentropy")
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform *regression* toward a vector of continuous values, end your stack
    of layers with a Dense layer with a number of units equal to the number of values
    you’re trying to predict (often a single one, such as the price of a house), and
    no activation. Various losses can be used for regression—most commonly mean_squared_error
    (MSE):'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: inputs <- layer_input(shape = c(num_inputs_features))
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: outputs <- inputs %>%
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(32, activation = "relu") %>%
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(32, activation = "relu") %>%
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(num_values)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: model <- keras_model(inputs, outputs)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: model %>% compile(optimizer = "rmsprop", loss = "mse")
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: CONVNETS
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Convolution layers look at spatially local patterns by applying the same geometric
    transformation to different spatial locations (*patches*) in an input tensor.
    This results in representations that are *translation invariant*, making convolution
    layers highly data efficient and modular. This idea is applicable to spaces of
    any dimensionality: 1D (continuous sequences), 2D (images), 3D (volumes), and
    so on. You can use the Conv1D layer to process sequences, the Conv2D layer to
    process images, and the Conv3D layers to process volumes. As a leaner, more efficient
    alternative to convolution layers, you can also use *depthwise separable convolution*
    layers, such as SeparableConv2D.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '*Convnets*, or *convolutional networks*, consist of stacks of convolution and
    max-pooling layers. The pooling layers let you spatially downsample the data,
    which is required to keep feature maps to a reasonable size as the number of features
    grows, and to allow subsequent convolution layers to “see” a greater spatial extent
    of the inputs. Convnets are often ended with either a Flatten operation or a global-pooling
    layer, turning spatial feature maps into vectors, followed by Dense layers to
    achieve classification or regression.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a typical image classification network (categorical classification,
    in this case), leveraging SeparableConv2D layers:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: inputs <- layer_input(shape = c(height, width, channels))
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: outputs <- inputs %>%
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: layer_separable_conv_2d(32, 3, activation = "relu") %>%
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: layer_separable_conv_2d(64, 3, activation = "relu") %>%
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: layer_max_pooling_2d(2) %>%
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: layer_separable_conv_2d(64, 3, activation = "relu") %>%
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: layer_separable_conv_2d(128, 3, activation = "relu") %>%
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: layer_max_pooling_2d(2) %>%
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: layer_separable_conv_2d(64, 3, activation = "relu") %>%
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: layer_separable_conv_2d(128, 3, activation = "relu") %>%
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: layer_global_average_pooling_2d() %>%
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(32, activation = "relu") %>%
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(num_classes, activation = "softmax")
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: model <- keras_model(inputs, outputs)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: model %>% compile(optimizer = "rmsprop", loss = "categorical_crossentropy")
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: When building a very deep convnet, it’s common to add *batch normalization*
    layers as well as *residual connections*—two architecture patterns that help gradient
    information flow smoothly through the network.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: RNNS
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Recurrent neural networks* (RNNs) work by processing sequences of inputs one
    time step at a time, and maintaining a state throughout (a state is typically
    a vector or set of vectors). They should be used preferentially over 1D convnets
    in the case of sequences where patterns of interest aren’t invariant by temporal
    translation (e.g., time-series data where the recent past is more important than
    the distant past).'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 'Three RNN layers are available in Keras: SimpleRNN, GRU, and LSTM. For most
    practical purposes, you should use either GRU or LSTM. LSTM is the more powerful
    of the two but is also more expensive; you can think of GRU as a simpler, cheaper
    alternative to it.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: To stack multiple RNN layers on top of each other, each layer prior to the last
    layer in the stack should return the full sequence of its outputs (each input
    time step will correspond to an output time step). If you aren’t stacking any
    further RNN layers, it’s common to return only the last output, which contains
    information about the entire sequence.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is a single RNN layer for binary classification of vector sequences:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: inputs <- layer_input(shape = c(num_timesteps, num_features))
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: outputs <- inputs %>%
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: layer_lstm(32) %>%
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(num_classes, activation = "sigmoid")
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: model <- keras_model(inputs, outputs)
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: model %>% compile(optimizer = "rmsprop", loss = "binary_crossentropy")
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: 'And this is a stacked RNN for binary classification of vector sequences:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: inputs <- layer_input(shape = c(num_timesteps, num_features))
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: outputs <- inputs %>%
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: layer_lstm(32, return_sequences = TRUE) %>%
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: layer_lstm(32, return_sequences = TRUE) %>%
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: layer_lstm(32) %>% layer_dense(num_classes, activation = "sigmoid")
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: model <- keras_model(inputs, outputs)
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: model %>% compile(optimizer = "rmsprop", loss = "binary_crossentropy")
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: TRANSFORMERS
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A Transformer looks at a set of vectors (such as word vectors), and leverages
    *neural attention* to transform each vector into a representation that is aware
    of the *context* provided by the other vectors in the set. When the set in question
    is an ordered sequence, you can also leverage *positional encoding* to create
    Transformers that can take into account both global context and word order, capable
    of processing long text paragraphs much more effectively than RNNs or 1D convnets.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'Transformers can be used for any set-processing or sequence-processing task,
    including text classification, but they excel especially at *sequence-to-sequence
    learning*, such as translating paragraphs in a source language into a target language.
    A sequence-to-sequence Transformer is made up of two parts:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: A TransformerEncoder that turns an input vector sequence into a context-aware,
    order-aware output vector sequence
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A TransformerDecoder that takes the output of the TransformerEncoder, as well
    as a target sequence, and predicts what should come next in the target sequenc
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you’re only processing a single sequence (or set) of vectors, you’d be only
    using the TransformerEncoder.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is a sequence-to-sequence Transformer for mapping a source sequence
    to a target sequence (this setup could be used for machine translation or question
    answering, for instance):'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: encoder_inputs <- layer_input(shape = c(sequence_length),➊
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: dtype = "int64")
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: encoder_outputs <- encoder_inputs %>%
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: layer_positional_embedding(sequence_length, vocab_size, embed_dim) %>%
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: layer_transformer_encoder(embed_dim, dense_dim, num_heads)
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: decoder <- layer_transformer_decoder(NULL, embed_dim, dense_dim, num_heads)
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: decoder_inputs <- layer_input(shape = c(NA),➋
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: dtype = "int64")
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: decoder_outputs <- decoder_inputs %>%➌
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: layer_positional_embedding(sequence_length, vocab_size, embed_dim) %>%
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: decoder(., encoder_outputs) %>%
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(vocab_size, activation = "softmax")
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: transformer <- keras_model(list(encoder_inputs, decoder_inputs),
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: decoder_outputs)
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: transformer %>%
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: compile(optimizer = "rmsprop", loss = "categorical_crossentropy")
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Source sequence**
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Target sequence so far**
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Target sequence one step in the future**
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'And this is a lone TransformerEncoder for binary classification of integer
    sequences:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: inputs <- layer_input(shape = c(sequence_length), dtype = "int64")
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: outputs <- inputs %>%
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: layer_positional_embedding(sequence_length, vocab_size, embed_dim) %>%
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: layer_transformer_encoder(embed_dim, dense_dim, num_heads) %>%
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: layer_global_max_pooling_1d() %>%
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(1, activation = "sigmoid")
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: model <- keras_model(inputs, outputs)
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: model %>% compile(optimizer = "rmsprop", loss = "binary_crossentropy")
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Full implementations of the TransformerEncoder, the TransformerDecoder, and
    the PositionalEmbedding layers are provided in chapter 11.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 14.1.7 The space of possibilities
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'What will you build with these techniques? Remember, building deep learning
    models is like playing with LEGO bricks: layers can be plugged together to map
    essentially anything to anything, given that you have appropriate training data
    available and that the mapping is achievable via a continuous geometric transformation
    of reasonable complexity. The space of possibilities is infinite. This section
    offers a few examples to inspire you to think beyond the basic classification
    and regression tasks that have traditionally been the bread and butter of machine
    learning.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'I’ve sorted my suggested applications by input and output modalities in the
    following list. Note that quite a few of them stretch the limits of what is possible,
    although a model could be trained on all of these tasks—in some cases, such a
    model probably wouldn’t generalize far from its training data. Sections 14.2 through
    14.4 will address how these limitations could be lifted in the future:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'Mapping vector data to vector data:'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Predictive health care*—Mapping patient medical records to predictions of
    patient outcomes'
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Behavioral targeting*—Mapping a set of website attributes with data on how
    long a user will spend on the website'
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Product quality control*—Mapping a set of attributes relative to an instance
    of a manufactured product with the probability that the product will fail by next
    year'
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mapping image data to vector data:'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Medical assistant*—Mapping slides of medical images to a prediction about
    the presence of a tumor'
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Self-driving vehicle*—Mapping car dashcam video frames to steering wheel angle
    commands and gas and braking commands'
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Board game AI*—Mapping Go or chess boards to the next player move'
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Diet helper*—Mapping pictures of a dish to its calorie count'
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Age prediction*—Mapping selfies to the age of the person'
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mapping time-series data to vector data:'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Weather prediction*—Mapping time series of weather data in a grid of locations
    to the temperature in a specific place one week later'
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Brain-computer interfaces*—Mapping time series of magnetoencephalogram (MEG)
    data to computer commands'
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Behavioral targeting*—Mapping time series of user interactions on a website
    to the probability that a user will buy something'
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mapping text to text:'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Machine translation*—Mapping a paragraph in one language to a translated version
    in a different language'
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Smart reply*—Mapping emails to possible one-line replies'
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Question answering*—Mapping general-knowledge questions to answers'
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Summarization*—Mapping a long article to a short summary of the article'
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mapping images to text:'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Text transcription*—Mapping images that contain a text element to the corresponding
    text string'
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Captioning*—Mapping images to short captions describing the contents of the
    images'
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mapping text to images:'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Conditioned image generation*—Mapping a short text description to images matching
    the description'
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Logo generation/selection*—Mapping the name and description of a company to
    a logo suggestion'
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mapping images to images:'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Super-resolution*—Mapping downsized images to higher-resolution versions of
    the same images'
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Visual depth sensing*—Mapping images of indoor environments to maps of depth
    predictions'
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mapping images and text to text:'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Visual QA*—Mapping images and natural language questions about the contents
    of images to natural language answers'
  id: totrans-193
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Mapping video and text to text
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Video QA*—Mapping short videos and natural language questions about the contents
    of videos to natural language answers'
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Almost* anything is possible, but not quite *anything*. You’ll see in the
    next section what we *can’t* do with deep learning.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 14.2 The limitations of deep learning
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The space of applications that can be implemented with deep learning is infinite.
    And yet, many applications remain completely out of reach for current deep learning
    techniques—even given vast amounts of human-annotated data. Say, for instance,
    that you could assemble a dataset of hundreds of thousands—even millions—of English-language
    descriptions of the features of a software product, written by a product manager,
    as well as the corresponding source code developed by a team of engineers to meet
    these requirements. Even with this data, you could not train a deep learning model
    to read a product description and generate the appropriate codebase. That’s just
    one example among many. In general, anything that requires reasoning—like programming
    or applying the scientific method—long-term planning—and algorithmic data manipulation
    is out of reach for deep learning models, no matter how much data you throw at
    them. Even learning a simple sorting algorithm with a deep neural network is tremendously
    difficult.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 'This is because a deep learning model is just *a chain of simple, continuous
    geometric transformations* mapping one vector space into another. All it can do
    is map one data manifold X into another manifold Y, assuming the existence of
    a learnable continuous transform from X to Y. A deep learning model can be interpreted
    as a kind of program, but, inversely, *most programs can’t be expressed as deep
    learning models*. For most tasks, either there exists no corresponding neural
    network of reasonable size that solves the task, or, even if one exists, it may
    not be *learnable*: the corresponding geometric transform may be far too complex,
    or there may not be appropriate data available to learn it.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Scaling up current deep learning techniques by stacking more layers and using
    more training data can only superficially palliate some of these issues. It won’t
    solve the more fundamental problems that deep learning models are limited in what
    they can represent and that most of the programs you may wish to learn can’t be
    expressed as a continuous geometric morphing of a data manifold.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 14.2.1 The risk of anthropomorphizing machine learning models
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One real risk with contemporary AI is misinterpreting what deep learning models
    do and overestimating their abilities. A fundamental feature of humans is our
    *theory of mind*: our tendency to project intentions, beliefs, and knowledge on
    the things around us. Drawing a smiley face on a rock suddenly makes it “happy”
    in our minds. Applied to deep learning, this means that, for instance, when we’re
    able to somewhat successfully train a model to generate captions to describe pictures,
    we’re led to believe that the model “understands” the contents of the pictures
    and the captions it generates. Then we’re surprised when any slight departure
    from the sort of images present in the training data causes the model to generate
    completely absurd captions (see [figure 14.1](#fig14-1)).'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0486-01.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
- en: '**Figure 14.1 Failure of an image-captioning system based on deep learning**'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: In particular, this is highlighted by *adversarial examples*, which are samples
    fed to a deep learning network that are designed to trick the model into misclassifying
    them. You’re already aware that, for instance, it’s possible to do gradient ascent
    in input space to generate inputs that maximize the activation of some convnet
    filter—this is the basis of the filter-visualization technique introduced in chapter
    9, as well as the DeepDream algorithm from chapter 12\. Similarly, through gradient
    ascent, you can slightly modify an image to maximize the class prediction for
    a given class. By taking a picture of a panda and adding to it a gibbon gradient,
    we can get a neural network to classify the panda as a gibbon (see [figure 14.2](#fig14-2)).
    This evidences both the brittleness of these models and the deep difference between
    their input-to-output mapping and our human perception.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: In short, deep learning models don’t have any understanding of their input—at
    least not in a human sense. Our own understanding of images, sounds, and language
    is grounded in our sensorimotor experience as humans. Machine learning models
    have no access to such experiences and thus can’t understand their inputs in a
    human-relatable way. By annotating large numbers of training examples to feed
    into our models, we get them to learn a geometric transform that maps data to
    human concepts on a specific set of examples, but this mapping is a simplistic
    sketch of the original model in our minds—the one developed from our experience
    as embodied agents. It’s like a dim image in a mirror (see [figure 14.3](#fig14-1)).
    The models you create will take any shortcut available to fit their training data.
    For instance, image models tend to rely more on local textures than on a global
    understanding of the input images—a model trained on a dataset that features both
    leopards and sofas is likely to classify a leopard-pattern sofa as an actual leopard.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0487-01.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
- en: '**Figure 14.2 An adversarial example: Imperceptible changes in an image can
    upend a model’s classification of an image.**'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0487-02.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
- en: '**Figure 14.3 Current machine learning models: Like a dim image in a mirror**'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'As a machine learning practitioner, always be mindful of this, and never fall
    into the trap of believing that neural networks understand the tasks they perform—they
    don’t, at least not in a way that would make sense to us. They were trained on
    a different, far narrower task than the one we wanted to teach them: that of mapping
    training inputs to training targets, point by point. Show them anything that deviates
    from their training data, and they will break in absurd ways.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 14.2.2 Automatons vs. intelligent agents
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Fundamental differences exist between the straightforward geometric morphing
    from input to output that deep learning models do and the way humans think and
    learn. It isn’t just the fact that humans learn by themselves from embodied experience
    instead of being presented with explicit training examples. The human brain is
    an entirely different beast compared to a differentiable parametric function.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s zoom out a little bit and ask, “what’s the purpose of intelligence?”
    Why did it arise in the first place? We can only speculate, but we can make fairly
    informed speculations. We can start by looking at brains—the organ that produces
    intelligence. Brains are an evolutionary adaption—a mechanism developed incrementally
    over hundreds of millions of years, via random trial and error, guided by natural
    selection— that dramatically expanded the ability of organisms to adapt to their
    environment. Brains originally appeared more than half a billion years ago as
    a way to *store and execute behavioral programs*. “Behavioral programs” are just
    sets of instructions that make an organism reactive to its environment: “if this
    happens, then do that.” They link the organism’s sensory inputs to its motor controls.
    In the beginning, brains would have served to hardcode behavioral programs (as
    neural connectivity patterns), which would allow an organism to react appropriately
    to its sensory input. This is the way insect brains still work—flies, ants, *C.
    elegans* (see [figure 14.4](#fig14-1)), and so on. Because the original “source
    code” of these programs was DNA, which would be decoded as neural connectivity
    patterns, evolution was suddenly able to *search over behavior space* in a largely
    unbounded way—a major evolutionary shift.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Evolution was the programmer, and brains were computers carefully executing
    the code evolution gave them. Because neural connectivity is a very general computing
    substrate, the sensorimotor space of all brain-enabled species could suddenly
    start undergoing a dramatic expansion. Eyes, ears, mandibles, four legs, 24 legs—as
    long as you have a brain, evolution will kindly figure out for you behavioral
    programs that make good use of these. Brains can handle any modality—or combination
    of modalities—you throw at them.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, mind you, these early brains weren’t exactly intelligent per se. They
    were very much *automatons*: they would merely execute behavioral programs hardcoded
    in the organism’s DNA. They could only be described as intelligent in the same
    sense that a thermostat is “intelligent.” Or a list-sorting program. Or… a trained
    deep neural network (of the artificial kind). This is an important distinction,
    so let’s look at it carefully: what’s the difference between automatons and actual
    intelligent agents?'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0489-01.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
- en: '**Figure 14.4 The brain network of the *C. elegans* worm: A behavioral automaton
    “programmed” by natural evolution. Figure created by Emma Towlson (from Yan et
    al., “Network Control Principles Predict Neuron Function in the *Caenorhabditis
    elegans* Connectome,” *Nature*, Oct. 2017).**'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 14.2.3 Local generalization vs. extreme generalization
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Seventeenth-century French philosopher and scientist René Descartes wrote in
    1637 an illuminating comment that perfectly captures this distinction, long before
    the rise of AI, and in fact, before the first mechanical computer (which his colleague
    Pascal would create five years later). Descartes tells us, in reference to automatons,
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '*Even though such machines might do some things as well as we do them, or perhaps
    even better, they would inevitably fail in others, which would reveal they were
    acting not through understanding, but only from the disposition of their organs.*'
  id: totrans-221
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-222
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: René Descartes, *Discourse on the Method* (1637)
  id: totrans-223
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: There it is. Intelligence is characterized by *understanding*, and understanding
    is evidenced by *generalization*—the ability to handle whatever novel situation
    may arise. How do you tell the difference between a student that has memorized
    the past three years of exam questions but has no understanding of the subject,
    and a student who actually understands the material? You give them a brand-new
    problem. An automaton is static, crafted to accomplish specific things in a specific
    context—”if this, then that”—while an intelligent agent can adapt on the fly to
    novel, unexpected situations. When an automaton is exposed to something that doesn’t
    match what it is “programmed” to do (whether we’re talking about human-written
    programs, evolution-generated programs, or the implicit programming process of
    fitting a model on a training dataset), it will fail. Meanwhile, intelligent agents,
    like humans, will use their understanding to find a way forward.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 'Humans are capable of far more than mapping immediate stimuli to immediate
    responses, as a deep net, or an insect, would. We maintain complex, abstract models
    of our current situation, of ourselves, and of other people, and we can use these
    models to anticipate different possible futures and perform long-term planning.
    You can merge together known concepts to represent something you’ve never experienced
    before— like imagining what you’d do if you won the lottery, or picturing how
    your friend would react if you discreetly replaced her keys with exact copies
    made of elastic rubber. This ability to handle novelty and what-ifs, to expand
    our mental model space far beyond what we can experience directly—to leverage
    *abstraction* and *reasoning*—is the defining characteristic of human cognition.
    I call it *extreme generalization*: an ability to adapt to novel, never-before-experienced
    situations using little data or even no new data at all. This capability is key
    to the intelligence displayed by humans and advanced animals.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: 'This stands in sharp contrast with what automaton-like systems do. A very rigid
    automaton wouldn’t feature any generalization at all—it would be incapable of
    handling anything that it wasn’t precisely told about in advance. A hash table
    or a basic question-answering program implemented as hardcoded if-then-else statements
    would fall into this category. Deep nets do slightly better: they can successfully
    process inputs that deviate a bit from what they’re familiar with, which is precisely
    what makes them useful. Our cats vs. dogs model from chapter 8 could classify
    cat or dog pictures it had not seen before, as long as they were close enough
    to what it was trained on. However, deep nets are limited to what I call *local
    generalization* (see [figure 14.5](#fig14-5)): the mapping from inputs to outputs
    performed by a deep net quickly stops making sense as inputs start deviating from
    what the net saw at training time. Deep nets can only generalize to *known unknowns*—to
    factors of variation that were anticipated during model development and that are
    extensively featured in the training data, such as different camera angles or
    lighting conditions for pet pictures. That’s because deep nets generalize via
    interpolation on a manifold (remember chapter 5): any factor of variation in their
    input space needs to be captured by the manifold they learn. That’s why basic
    data augmentation is so helpful in improving deep net generalization. Unlike humans,
    these models have no ability to improvise in the face of situations for which
    little or no data is available (like winning the lottery or being handed rubber
    keys) that only share abstract commonalities with past situations.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0490-01.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
- en: '**Figure 14.5 Local generalization vs. extreme generalization**'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider, for instance, the problem of learning the appropriate launch parameters
    to get a rocket to land on the moon. If you used a deep net for this task and
    trained it using supervised learning or reinforcement learning, you’d have to
    feed it tens of thousands or even millions of launch trials: you’d need to expose
    it to a *dense sampling* of the input space, for it to learn a reliable mapping
    from input space to output space. In contrast, as humans, we can use our power
    of abstraction to come up with physical models—rocket science—and derive an exact
    solution that will land the rocket on the moon in one or a few trials. Similarly,
    if you developed a deep net controlling a human body, and you wanted it to learn
    to safely navigate a city without getting hit by cars, the net would have to die
    many thousands of times in various situations until it could infer that cars are
    dangerous and develop appropriate avoidance behaviors. Dropped into a new city,
    the net would have to relearn most of what it knows. On the other hand, humans
    are able to learn safe behaviors without having to die even once—again, thanks
    to our power of abstract modeling of novel situations.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 14.2.4 The purpose of intelligence
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This distinction between highly adaptable intelligent agents and rigid automatons
    leads us back to brain evolution. Why did brains—originally a mere medium for
    natural evolution to develop behavioral automatons—eventually turn intelligent?
    Like every significant evolutionary milestone, it happened because natural selection
    constraints encouraged it to happen.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 'Brains are responsible for behavior generation. If the set of situations an
    organism had to face was mostly static and known in advance, behavior generation
    would be an easy problem: evolution would just figure out the correct behaviors
    via random trial and error and hardcode them into the organism’s DNA. This first
    stage of brain evolution— brains as automatons—would already be optimal. However,
    crucially, as organism complexity—and alongside it, environmental complexity—kept
    increasing, the situations that animals had to deal with became much more dynamic
    and more unpredictable. A day in your life, if you look closely, is unlike any
    day you’ve ever experienced, and unlike any day ever experienced by any of your
    evolutionary ancestors. You need to be able to face unknown and surprising situations
    constantly. There is no way for evolution to find and hardcode as DNA the sequence
    of behaviors you’ve been executing to successfully navigate your day since you
    woke up a few hours ago. It has to be generated on the fly, every day.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: The brain, as a good behavior-generation engine, simply adapted to fit this
    need. It optimized for adaptability and generality, rather than merely optimizing
    for fitness to a fixed set of situations. This shift likely occurred multiple
    times throughout evolutionary history, resulting in highly intelligent animals
    in very distant evolutionary branches—apes, octopuses, ravens, and more. Intelligence
    is an answer to challenges presented by complex, dynamic ecosystems.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s the nature of intelligence: it is the ability to efficiently leverage
    the information at your disposal to produce successful behavior in the face of
    an uncertain, ever-changing future. What Descartes calls “understanding” is the
    key to this remarkable capability: the power to mine your past experience to develop
    modular, reusable abstractions that can be quickly repurposed to handle novel
    situations and achieve extreme generalization.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 14.2.5 Climbing the spectrum of generalization
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As a crude caricature, you could summarize the evolutionary history of biological
    intelligence as a slow climb up the *spectrum of generalization*. It started with
    automaton-like brains that could perform only local generalization. Over time,
    evolution started producing organisms capable of increasingly broader generalization
    that could thrive in evermore complex and variable environments. Eventually, in
    the past few millions of years— an instant in evolutionary terms—certain hominid
    species started trending toward an implementation of biological intelligence capable
    of extreme generalization, precipitating the start of the Anthropocene and forever
    changing the history of life on earth.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: The progress of AI over the past 70 years bears striking similarities to this
    evolution. Early AI systems were pure automatons, like the ELIZA chat program
    from the 1960s, or SHRDLU,^([2](#Rendnote2)) a 1970 AI capable of manipulating
    simple objects from natural language commands. In the 1990s and 2000s, we saw
    the rise of machine learning systems capable of local generalization, which could
    deal with some level of uncertainty and novelty. In the 2010s, deep learning further
    expanded the local-generalization power of these systems by enabling engineers
    to leverage much larger datasets and much more expressive models.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Today, we may be on the cusp of the next evolutionary step. There is increasing
    interest in systems that could achieve *broad generalization*, which I define
    as the ability to deal with *unknown unknowns* within a single broad domain of
    tasks (including situations the system was not trained to handle and that its
    creators could not have anticipated), for instance, a self-driving car capable
    of safely dealing with any situation you throw at it, or a domestic robot that
    could pass the “Woz test of intelligence”—entering a random kitchen and making
    a cup of coffee.^([3](#Rendnote3)) By combining deep learning and painstakingly
    handcrafted abstract models of the world, we’re already making visible progress
    toward these goals.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'However, for the time being, AI remains limited to *cognitive automation*:
    the “intelligence” label in “artificial intelligence” is a category error. It
    would be more accurate to call our field “artificial cognition,” with “cognitive
    automation” and “artificial intelligence” being two nearly independent subfields
    within it. In this subdivision, “artificial intelligence” would be a greenfield
    where almost everything remains to be discovered.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, I don’t mean to diminish the achievements of deep learning. Cognitive
    automation is incredibly useful, and the way deep learning models are capable
    of automating tasks from exposure to data alone represents an especially powerful
    form of cognitive automation, far more practical and versatile than explicit programming.
    Doing this well is a game-changer for essentially every industry. But it’s still
    a long way from human (or animal) intelligence. Our models, so far, can perform
    only local generalization: they map space X to space Y via a smooth geometric
    transform learned from a dense sampling of X-to-Y data points, and any disruption
    within spaces X or Y invalidates this mapping. They can generalize only to new
    situations that stay similar to past data, whereas human cognition is capable
    of extreme generalization, quickly adapting to radically novel situations and
    planning for long-term future situations.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 14.3 Setting the course toward greater generality in AI
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To lift some of the limitations we have discussed and create AI that can compete
    with human brains, we need to move away from straightforward input-to-output mappings
    and on to *reasoning* and *abstraction*. In the following couple of sections,
    we’ll take a look at what the road ahead may look like.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '14.3.1 On the importance of setting the right objective: The shortcut rule'
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Biological intelligence was the answer to a question asked by nature. Likewise,
    if we want to develop true artificial intelligence, first, we need to be asking
    the right questions.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 'An effect you see constantly in systems design is the *shortcut rule*: if you
    focus on optimizing one success metric, you will achieve your goal, but at the
    expense of everything in the system that wasn’t covered by your success metric.
    You end up taking every available shortcut toward the goal. Your creations are
    shaped by the incentives you give yourself.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: 'You see this often in machine learning competitions. In 2009, Netflix ran a
    challenge that promised a $1 million prize to the team that achieved the highest
    score on a movie-recommendation task. It ended up never using the system created
    by the winning team, because it was way too complex and compute intensive. The
    winners had optimized for prediction accuracy alone—what they were incentivized
    to achieve—at the expense of every other desirable characteristic of the system:
    inference cost, maintainability, and explainability. The shortcut rule holds true
    in most Kaggle competitions as well: the models produced by Kaggle winners can
    rarely, if ever, be used in production.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 'The shortcut rule has been everywhere in AI over the past few decades. In the
    1970s, psychologist and computer science pioneer Allen Newell, concerned that
    his field wasn’t making any meaningful progress toward a proper theory of cognition,
    proposed a new grand goal for AI: chess-playing. The rationale was that playing
    chess, in humans, seemed to involve—perhaps even require—capabilities such as
    perception, reasoning and analysis, memory, study from books, and so on. Surely,
    if we could build a chess-playing machine, it would have to feature these attributes
    as well. Right?'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: 'Over two decades later, the dream came true: in 1997, IBM’s Deep Blue beat
    Gary Kasparov, the best chess player in the world. Researchers had then to contend
    with the fact that creating a chess-champion AI had taught them little about human
    intelligence. The Alpha-Beta algorithm at the heart of Deep Blue wasn’t a model
    of the human brain and couldn’t generalize to tasks other than similar board games.
    It turned out it was easier to build an AI that could only play chess than to
    build an artificial mind—so that’s the shortcut researchers took.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: So far, the driving success metric of the field of AI has been to solve specific
    tasks, from chess to Go, from MNIST classification to ImageNet, from Atari arcade
    games to *StarCraft* and *Dota 2*. Consequently, the history of the field has
    been defined by a series of “successes” where we figured out how to solve these
    tasks *without featuring any intelligence*.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: If that sounds like a surprising statement, keep in mind that human-like intelligence
    isn’t characterized by skill at any particular task—rather, it is the ability
    to adapt to novelty, to efficiently acquire new skills and master never-seen-before
    tasks. By fixing the task, you make it possible to provide an arbitrarily precise
    description of what needs to be done, either via hardcoding human-provided knowledge
    or by supplying humongous amounts of data. You make it possible for engineers
    to “buy” more skill for their AI by just adding data or adding hardcoded knowledge,
    without increasing the generalization power of the AI (see [figure 14.6](#fig14-6)).
    If you have near-infinite training data, even a very crude algorithm like nearest-neighbor
    search can play video games with superhuman skill. Likewise if you have a near-infinite
    amount of human-written if-then-else statements. That is, until you make a small
    change to the rules of the game—the kind a human could adapt to instantly—which
    will require the nonintelli-gent system to be retrained or rebuilt from scratch.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0494-01.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
- en: '**Figure 14.6 A low-generalization system can achieve arbitrary skill at a
    fixed task given unlimited task-specific information.**'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: In short, by fixing the task, you remove the need to handle uncertainty and
    novelty, and because the nature of intelligence is the ability to handle uncertainty
    and novelty, you’re effectively removing the need for intelligence. And because
    it’s always easier to find a non-intelligent solution to a specific task than
    to solve the general problem of intelligence, that’s the shortcut you will take
    100% of the time. Humans can use their general intelligence to acquire skills
    at any new task, but in reverse, there is no path from a collection of task-specific
    skills to general intelligence.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: 14.3.2 A new target
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To make artificial intelligence actually intelligent, and give it the ability
    to deal with the incredible variability and ever-changing nature of the real world,
    we first need to move away from seeking to achieve *task-specific skill* and,
    instead, start targeting generalization power itself. We need new metrics of progress
    that will help us develop increasingly intelligent systems, metrics that will
    point in the right direction and that will give us an actionable feedback signal.
    As long as we set our goal to be “create a model that solves task X,” the shortcut
    rule will apply, and we’ll end up with a model that does X, period.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'In my view, intelligence can be precisely quantified as an *efficiency ratio*:
    the conversion ratio between the *amount of relevant information* you have available
    about the world (which could be either *past experience* or innate *prior knowledge*)
    and your *future operating area*, the set of novel situations where you will be
    able to produce appropriate behavior (you can view this as your *skill set*).
    A more intelligent agent will be able to handle a broader set of future tasks
    and situations using a smaller amount of past experience. To measure such a ratio,
    you just need to fix the information available to your system—its experience and
    its prior knowledge—and measure its performance on a set of reference situations
    or tasks that are known to be sufficiently different from what the system has
    had access to. Trying to maximize this ratio should lead you toward intelligence.
    Crucially, to avoid cheating, you’re going to need to make sure you test the system
    only on tasks it wasn’t programmed or trained to handle—in fact, you need tasks
    that the *creators of the system could not have anticipated*.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: In 2018 and 2019, I developed a benchmark dataset called the *Abstraction and
    Reasoning Corpus* (ARC)^([4](#Rendnote4)) that seeks to capture this definition
    of intelligence. ARC is meant to be approachable by both machines and humans,
    and it looks very similar to human IQ tests, such as Raven’s progressive matrices.
    At test time, you’ll see a series of “tasks.” Each task is explained via three
    or four “examples” that take the form of an input grid and a corresponding output
    grid (see [figure 14.7](#fig14-7)). You’ll then be given a brand-new input grid,
    and you’ll have three tries to produce the correct output grid before moving on
    to the next task.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: 'Compared to IQ tests, two things are unique about ARC. First, ARC seeks to
    measure generalization power, by only testing you on tasks you’ve never seen before.
    That means that ARC is *a game you can’t practice for*, at least in theory: the
    tasks you will be tested on will have their own unique logic that you will have
    to understand on the fly. You can’t just memorize specific strategies from past
    tasks.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0496-01.jpg)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
- en: '**Figure 14.7 An ARC task: The nature of the task is demonstrated by a couple
    of input-output pair examples. Provided with a new input, you must construct the
    corresponding output.**'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: In addition, ARC tries to control for the *prior knowledge* that you bring to
    the test. You never approach a new problem entirely from scratch—you bring to
    it preexisting skills and information. ARC makes the assumption that all test
    takers should start from the set of knowledge priors, called “Core Knowledge priors,”
    that represent the “knowledge systems” that humans are born with. Unlike an IQ
    test, ARC tasks will never involve acquired knowledge, like English sentences,
    for instance.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Unsurprisingly, deep-learning-based methods (including models trained on extremely
    large amounts of external data, like GPT-3) have proven entirely unable to solve
    ARC tasks, because these tasks are non-interpolative and thus are a poor fit for
    curve-fitting. Meanwhile, average humans have no issue solving these tasks on
    the first try, without any practice. When you see a situation like this, where
    humans as young as five are able to naturally perform something that seems to
    be completely out of reach for modern AI technology, that’s a clear signal that
    something interesting is going on—that we’re missing something.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 'What would it take to solve ARC? Hopefully, this challenge will get you thinking.
    That’s the entire point of ARC: to give you a goal of a different kind that will
    nudge you in a new direction—hopefully a productive direction. Now let’s take
    a quick look at the key ingredients you’re going to need if you want to answer
    the call.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '14.4 Implementing intelligence: The missing ingredients'
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, you’ve learned that there’s a lot more to intelligence than the sort
    of latent manifold interpolation that deep learning does. But what, then, do we
    need to start building real intelligence? What are the core pieces that are currently
    eluding us?
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 14.4.1 Intelligence as sensitivity to abstract analogies
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Intelligence is the ability to use your past experience (and innate prior knowledge)
    to face novel, unexpected future situations. If the future you had to face was
    *truly novel*— sharing no common ground with anything you’ve seen before—you’d
    be unable to react to it, no matter how intelligent you were.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Intelligence works because nothing is ever truly without precedent. When we
    encounter something new, we’re able to make sense of it by drawing analogies to
    our past experience, by articulating it in terms of the abstract concepts we’ve
    collected over time. A person from the 17th century seeing a jet plane for the
    first time might describe it as a large, loud metal bird that doesn’t flap its
    wings. A car? That’s a horseless carriage. If you’re trying to teach physics to
    a grade schooler, you can explain how electricity is like water in a pipe, or
    how space-time is like a rubber sheet getting distorted by heavy objects.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: Besides such clear-cut, explicit analogies, we’re constantly making smaller,
    implicit analogies, every second, with every thought. Analogies are how we navigate
    life. Going to a new supermarket? You’ll find your way by relating it to similar
    stores you’ve been to. Talking to someone new? They’ll remind you of a few people
    you’ve met before. Even seemingly random patterns, like the shape of clouds, instantly
    evoke in us vivid images—an elephant, a ship, a fish.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: 'These analogies aren’t just in our minds, either: physical reality itself is
    full of isomorphisms. Electromagnetism is analogous to gravity. Animals are all
    structurally similar to each other, due to shared origins. Silica crystals are
    similar to ice crystals. And so on.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: 'I call this the *kaleidoscope hypothesis*: our experience of the world seems
    to feature incredible complexity and never-ending novelty, but everything in this
    sea of complexity is similar to everything else. The number of *unique atoms of
    meaning* that you need to describe the universe you live in is relatively small,
    and everything around you is a recombination of these atoms, a few seeds, endless
    variation—much like what goes on inside a kaleidoscope, where a few glass beads
    are reflected by a system of mirrors to produce rich, seemingly ever-changing
    patterns (see [figure 14.8](#fig14-8)).'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Generalization power—intelligence—is the ability to mine your experience to
    identify these atoms of meaning that can seemingly be reused across many different
    situations. Once extracted, they’re called *abstractions*. Whenever you encounter
    a new situation, you make sense of it via your accumulated collection of abstractions.
    How do you identify reusable atoms of meaning? Simply by noticing when two things
    are similar—by noticing analogies. If something is repeated twice, then both instances
    must have a single origin, like in a kaleidoscope. Abstraction is the engine of
    intelligence, and analogy-making is the engine that produces abstraction.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: In short, intelligence is literally sensitivity to abstract analogies, and that’s
    in fact all there is to it. If you have a high sensitivity to analogies, you will
    extract powerful abstractions from little experience, and you will be able to
    use these abstractions to operate in a maximally large area of future experience
    space. You will be maximally efficient in converting past experience into the
    ability to handle future novelty.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0498-01.jpg)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
- en: '**Figure 14.8 A kaleidoscope produces rich (yet repetitive) patterns from just
    a few beads of colored glass.**'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: 14.4.2 The two poles of abstraction
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If intelligence is sensitivity to analogies, then developing artificial intelligence
    should start with spelling out a step-by-step algorithm for analogy-making. Analogy-making
    starts with *comparing things to one another*. Crucially, there are *two distinct
    ways* to compare things, from which arise two different kinds of abstraction,
    two modes of thinking, each better suited to a different kind of problem. Together,
    these two poles of abstraction form the basis for all of our thoughts.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: The first way to relate things to each other is *similarity comparison*, which
    gives rise to *value-centric analogies*. The second way is *exact structural match*,
    which gives rise to *program-centric analogies* (or structure-centric analogies).
    In both cases, you start from *instances* of a thing, and you merge together related
    instances to produce an *abstraction* that captures the common elements of the
    underlying instances. What varies is how you tell that two instances are related,
    and how you merge instances into abstractions. Let’s take a close look at each
    type.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: VALUE-CENTRIC ANALOGY
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s say you come across a number of different beetles in your backyard, belonging
    to multiple species. You’ll notice similarities between them. Some will be more
    similar to one another, and some will be less similar: the notion of similarity
    is implicitly a smooth, continuous *distance function* that defines a latent manifold
    where your instances live. Once you’ve seen enough beetles, you can start clustering
    more similar instances together and merging them into a set of *prototypes* that
    captures the shared visual features of each cluster (see [figure 14.9](#fig14-9)).
    This prototype is abstract: it doesn’t look like any specific instance you’ve
    seen, though it encodes properties that are common across all of them. When you
    encounter a new beetle, you won’t need to compare it to every single beetle you’ve
    seen before to know what to do with it. You can simply compare it to your handful
    of prototypes, so as to find the closest prototype— the beetle’s *category*—and
    use it to make useful predictions: is the beetle likely to bite you? Will it eat
    your apples?'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0499-01.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
- en: '**Figure 14.9 Value-centric analogy relates instances via a continuous notion
    of similarity to obtain abstract prototypes.**'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Does this sound familiar? It’s pretty much a description of what unsupervised
    machine learning (such as the *K*-means clustering algorithm) does. In general,
    all of modern machine learning, unsupervised or not, works by learning latent
    manifolds that describe a space of instances encoded via prototypes. (Remember
    the convnet features you visualized in chapter 9? They were visual prototypes.)
    Value-centric analogy is the kind of analogy-making that enables deep learning
    models to perform local generalization.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: It’s also what many of your own cognitive abilities run on. As a human, you
    perform value-centric analogies all the time. It’s the type of abstraction that
    underlies *pattern recognition, perception*, and *intuition*. If you can do a
    task without thinking about it, you’re relying heavily on value-centric analogies.
    If you’re watching a movie and you start subconsciously categorizing the different
    characters into “types,” that’s value-centric abstraction.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: PROGRAM-CENTRIC ANALOGY
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Crucially, there’s more to cognition than the kind of immediate, approximative,
    intuitive categorization that value-centric analogy enables. There’s another type
    of abstraction-generation mechanism that’s slower, exact, deliberate: program-centric
    (or structure-centric) analogy.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 'In software engineering, you often write different functions or classes that
    seem to have a lot in common. When you notice these redundancies, you start asking,
    “could there be a more abstract function that performs the same job, that could
    be reused twice? Could there be an abstract base class that both of my classes
    could inherit from?” The definition of abstraction you’re using here corresponds
    to program-centric analogy. You’re not trying to compare your classes and functions
    by *how similar* they look the way you’d compare two human faces, via an implicit
    distance function. Rather, you’re interested in whether there are *parts* of them
    that have *exactly the same structure*. You’re looking for what is called a *subgraph
    isomorphism* (see [figure 14.10](#fig14-10)): programs can be represented as graphs
    of operators, and you’re trying to find subgraphs (program subsets) that are exactly
    shared across your different programs.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0500-01.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
- en: '**Figure 14.10 Program-centric analogy identifies and isolates isomorphic substructures
    across different instances.**'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: This kind of analogy-making via exact structural match within different discrete
    structures isn’t at all exclusive to specialized fields like computer science
    or mathematics— you’re constantly using it without noticing. It underlies *reasoning,
    planning*, and the general concept of *rigor* (as opposed to intuition). Any time
    you’re thinking about objects connected to each other by a discrete network of
    relationships (rather than a continuous similarity function), you’re leveraging
    program-centric analogies.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: COGNITION AS A COMBINATION OF BOTH KINDS OF ABSTRACTION
  id: totrans-291
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s compare these two poles of abstraction side by side (see [table 14.1](#tab14-1)).
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: Table 14.1 The two poles of abstraction
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '| Value-centric abstraction | Program-centric abstraction |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
- en: '| Relates things by distance | Relates things by exact structural match |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
- en: '| Continuous, grounded in geometry | Discrete, grounded in topology |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
- en: '| Produces abstractions by “averaging” instances into “prototypes” | Produces
    abstractions by isolating isomorphic substructures across instances |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
- en: '| Underlies perception and intuition | Underlies reasoning and planning |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
- en: '| Immediate, fuzzy, approximative | Slow, exact, rigorous |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
- en: '| Requires a lot of experience to produce reliable results | Experience efficient;
    can operate on as few as two instances |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
- en: 14.4.3 The two poles of abstraction
  id: totrans-301
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Everything we do, everything we think, is a combination of these two types of
    abstraction. You’d be hard-pressed to find tasks that involve only one of the
    two. Even a seemingly “pure perception” task, like recognizing objects in a scene,
    involves a fair amount of implicit reasoning about the relationships between the
    objects you’re looking at. And even a seemingly “pure reasoning” task, like finding
    the proof of a mathematical theorem, involves a good amount of intuition. When
    a mathematician puts their pen to the paper, they’ve already got a fuzzy vision
    of the direction in which they’re going. The discrete reasoning steps they take
    to get to the destination are guided by high-level intuition.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: These two poles are complementary, and it’s their interleaving that enables
    extreme generalization. No mind could be complete without both of them.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: 14.4.4 The missing half of the picture
  id: totrans-304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'By this point, you should start seeing what’s missing from modern deep learning:
    it’s very good at encoding value-centric abstraction, but it has basically no
    ability to generate program-centric abstraction. Human-like intelligence is a
    tight interleaving of both types, so we’re literally missing half of what we need—arguably
    the most important half.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, here’s a caveat. So far, I’ve presented each type of abstraction as entirely
    separate from the other—opposite, even. In practice, however, they’re more of
    a spectrum: to an extent, you could do reasoning by embedding discrete programs
    in continuous manifolds, just like you may fit a polynomial function through any
    set of discrete points, as long as you have enough coefficients. And inversely,
    you could use discrete programs to emulate continuous distance functions—after
    all, when you’re doing linear algebra on a computer, you’re working with continuous
    spaces, entirely via discrete programs that operate on ones and zeros.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there are clearly types of problems that are better suited to one
    or the other. Try to train a deep learning model to sort a list of five numbers,
    for instance. With the right architecture, it’s not impossible, but it’s an exercise
    in frustration. You’ll need a massive amount of training data to make it happen,
    and even then, the model will still make occasional mistakes when presented with
    new numbers. And if you want to start sorting lists of 10 numbers instead, you’ll
    need to completely retrain the model on even more data. Meanwhile, writing a sorting
    algorithm in R takes just a few lines, and the resulting program, once validated
    on a couple more examples, will work every time on lists of any size. That’s pretty
    strong generalization: going from a couple of demonstration examples and test
    examples to a program that can successfully process literally any list of numbers.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: 'In reverse, perception problems are a terrible fit for discrete reasoning processes.
    Try to write a pure-R program to classify MNIST digits without using any machine
    learning technique: you’re in for a ride. You’ll find yourself painstakingly coding
    functions that can detect the number of closed loops in a digit, the coordinates
    of the center of mass of a digit, and so on. After thousands of lines of code,
    you might achieve… 90% test accuracy. In this case, fitting a parametric model
    is much simpler; it can better utilize the large amount of data that’s available,
    and it achieves much more robust results. If you have lots of data and you’re
    faced with a problem where the manifold hypothesis applies, go with deep learning.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, it’s unlikely that we’ll see the rise of an approach that would
    reduce reasoning problems to manifold interpolation, or that would reduce perception
    problems to discrete reasoning. The way forward in AI is to develop a unified
    framework that incorporates *both* types of abstract analogy-making. Let’s examine
    what that might look like.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: 14.5 The future of deep learning
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given what we know of how deep nets work, their limitations, and what they’re
    currently missing, can we predict where things are headed in the medium term?
    Following are some purely personal thoughts. Note that I don’t have a crystal
    ball, so a lot of what I anticipate may fail to become reality. I’m sharing these
    predictions not because I expect them to be proven completely right in the future
    but because they’re interesting and actionable in the present.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: 'At a high level, these are the main directions in which I see promise:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '*Models closer to general-purpose computer programs*, built on top of far richer
    primitives than the current differentiable layers. This is how we’ll get to reasoning
    and abstraction, the lack of which is the fundamental weakness of current models.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A fusion between deep learning and discrete search over program spaces*, with
    the former providing perception and intuition capabilities, and the latter providing
    reasoning and planning capabilities.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Greater, systematic reuse of previously learned features and architectures*,
    such as meta-learning systems using reusable and modular program subroutines.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, note that these considerations aren’t specific to the sort of
    supervised learning that has been the bread and butter of deep learning so far—rather,
    they’re applicable to any form of machine learning, including unsupervised, self-supervised,
    and reinforcement learning. It isn’t fundamentally important where your labels
    come from or what your training loop looks like; these different branches of machine
    learning are different facets of the same construct. Let’s dive in.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: 14.5.1 Models as programs
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As noted in the previous section, a necessary transformational development
    that we can expect in the field of machine learning is a move away from models
    that perform purely *pattern recognition* and can achieve only *local generalization*,
    toward models capable of abstraction and reasoning that can achieve *extreme generalization*.
    Current AI programs that are capable of basic forms of reasoning are all hardcoded
    by human programmers: for instance, software that relies on search algorithms,
    graph manipulation, and formal logic.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: 'That may be about to change, thanks to *program synthesis*—a field that is
    very niche today, but I expect to take off in a big way over the next few decades.
    Program synthesis consists of automatically generating simple programs by using
    a search algorithm (possibly genetic search, as in *genetic programming*) to explore
    a large space of possible programs (see [figure 14.11](#fig14-11)). The search
    stops when a program is found that matches the required specifications, often
    provided as a set of input-output pairs. This is highly reminiscent of machine
    learning: given training data provided as input-output pairs, we find a program
    that matches inputs to outputs and can generalize to new inputs. The difference
    is that instead of learning parameter values in a hardcoded program (a neural
    network), we generate source code via a discrete search process (see [table 14.2](#tab14-2)).'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0503-01.jpg)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
- en: '**Figure 14.11 A schematic view of program synthesis: Given a program specification
    and a set of building blocks, a search process assembles the building blocks into
    candidate programs, which are then tested against the specification. The search
    continues until a valid program is found.**'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: Table 14.2 Machine learning vs. program synthesis
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '| Machine learning | Program synthesis |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
- en: '| Model: differentiable parametric function | Model: graph of operators from
    a programming language |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
- en: '| Engine: gradient descent | Engine: discrete search (such as genetic search)
    |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
- en: '| Requires a lot of data to produce reliable results | Data efficient; can
    work with a couple of training examples |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
- en: 14.5.2 Machine learning vs. program synthesis
  id: totrans-327
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Program synthesis is how we’re going to add program-centric abstraction capabilities
    to our AI systems. It’s the missing piece of the puzzle. I mentioned earlier that
    deep learning techniques were entirely unusable on ARC, a reasoning-focused intelligence
    test. Meanwhile, very crude program-synthesis approaches are already producing
    very promising results on this benchmark.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: 14.5.3 Blending together deep learning and program synthesis
  id: totrans-329
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Of course, deep learning isn’t going anywhere. Program synthesis isn’t its
    replacement; it is its complement. It’s the hemisphere that has been so far missing
    from our artificial brains. We’re going to be leveraging both, in combination.
    There are two major ways this will take place:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: '**1** Developing systems that integrate both deep learning modules and discrete
    algorithmic modules'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**2** Using deep learning to make the program search process itself more efficient'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s review each of these possible avenues.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: INTEGRATING DEEP LEARNING MODULES AND ALGORITHMIC MODULES INTO HYBRID SYSTEMS
  id: totrans-334
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Today, the most powerful AI systems are hybrid: they leverage both deep learning
    models and handcrafted symbol-manipulation programs. In DeepMind’s AlphaGo, for
    example, most of the intelligence on display is designed and hardcoded by human
    programmers (such as Monte Carlo Tree Search). Learning from data happens only
    in specialized submodules (value networks and policy networks). Or consider autonomous
    vehicles: a self-driving car is able to handle a large variety of situations because
    it maintains a model of the world around it—a literal 3D model—full of assumptions
    hardcoded by human engineers. This model is constantly updated via deep learning
    perception modules that interface it with the surroundings of the car.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: For both of these systems—AlphaGo and self-driving vehicles—the combination
    of human-created discrete programs and learned continuous models is what unlocks
    a level of performance that would be impossible with either approach in isolation,
    such as an end-to-end deep net or a piece of software without ML elements. So
    far, the discrete algorithmic elements of such hybrid systems are painstakingly
    hardcoded by human engineers. But in the future, such systems may be fully learned,
    with no human involvement.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: 'What will this look like? Consider a well-known type of network: RNNs. It’s
    important to note that RNNs have slightly fewer limitations than feed-forward
    networks. That’s because RNNs are a bit more than mere geometric transformations:
    they’re geometric transformations *repeatedly applied inside a* for *loop*. The
    temporal for loop is itself hard-coded by human developers: it’s a built-in assumption
    of the network. Naturally, RNNs are still extremely limited in what they can represent,
    primarily because each step they perform is a differentiable geometric transformation,
    and they carry information from step to step via points in a continuous geometric
    space (state vectors). Now imagine a neural network that’s augmented in a similar
    way with programming primitives, but instead of a single hardcoded for loop with
    hardcoded continuous-space memory, the network includes a large set of programming
    primitives that the model is free to manipulate to expand its processing function,
    such as if branches, while statements, variable creation, disk storage for long-term
    memory, sorting operators, and advanced data structures (such as lists, graphs,
    and hash tables). The space of programs that such a network could represent would
    be far broader than what can be represented with current deep learning models,
    and some of these programs could achieve superior generalization power. Importantly,
    such programs will not be differentiable end-to-end, though specific modules will
    remain differentiable and thus will need to be generated via a combination of
    discrete program search and gradient descent.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: We’ll move away from having, on one hand, hardcoded algorithmic intelligence
    (handcrafted software) and, on the other hand, learned geometric intelligence
    (deep learning). Instead, we’ll have a blend of formal algorithmic modules that
    provide reasoning and abstraction capabilities, and geometric modules that provide
    informal intuition and pattern-recognition capabilities (see [figure 14.12](#fig14-12)).
    The entire system will be learned with little or no human involvement. This should
    dramatically expand the scope of problems that can be solved with machine learning—the
    space of programs that we can generate automatically, given appropriate training
    data. Systems like AlphaGo—or even RNNs—can be seen as a prehistoric ancestor
    of such hybrid algorithmic-geometric models.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0505-01.jpg)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
- en: '**Figure 14.12 A learned program relying on both geometric primitives (pattern
    recognition, intuition) and algorithmic primitives (reasoning, search, memory)**'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: USING DEEP LEARNING TO GUIDE PROGRAM SEARCH
  id: totrans-341
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Today, program synthesis faces a major obstacle: it’s tremendously inefficient.
    To caricature, program synthesis works by trying every possible program in a search
    space until it finds one that matches the specification provided. As the complexity
    of the program specification increases, or as the vocabulary of primitives used
    to write programs expands, the program search process runs into what’s known as
    *combinatorial explosion*, where the set of possible programs to consider grows
    very fast—in fact, much faster than merely exponentially fast. As a result, today,
    program synthesis can be used to generate only very short programs. You’re not
    going to be generating a new OS for your computer anytime soon.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: 'To move forward, we’re going to need to make program synthesis efficient by
    bringing it closer to the way humans write software. When you open your editor
    to code up a script, you’re not thinking about every possible program you could
    potentially write. You have in mind only a handful of possible approaches: you
    can use your understanding of the problem and your past experience to drastically
    cut through the space of possible options to consider.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep learning can help program synthesis do the same: although each specific
    program we’d like to generate might be a fundamentally discrete object that performs
    non-interpolative data manipulation, evidence so far indicates that *the space
    of all useful programs* may look a lot like a continuous manifold. That means
    that a deep learning model that has been trained on millions of successful program-generation
    episodes might start to develop solid *intuition* about the *path through program
    space* that the search process should take to go from a specification to the corresponding
    program— just like a software engineer might have immediate intuition about the
    overall architecture of the script they’re about to write, about the intermediate
    functions and classes they should use as stepping-stones on the way to the goal.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: Remember that human reasoning is heavily guided by value-centric abstraction,
    that is, by pattern recognition and intuition. Program synthesis should be, too.
    I expect the general approach of guiding program search via learned heuristics
    to see increasing research interest over the next 10 to 20 years.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: 14.5.4 Lifelong learning and modular subroutine reuse
  id: totrans-346
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If models become more complex and are built on top of richer algorithmic primitives,
    this increased complexity will require higher reuse between tasks, rather than
    training a new model from scratch every time we have a new task or a new dataset.
    Many datasets don’t contain enough information for us to develop a new, complex
    model from scratch, and it will be necessary to use information from previously
    encountered datasets (much as you don’t learn English from scratch every time
    you open a new book—that would be impossible). Training models from scratch on
    every new task is also inefficient due to the large overlap between the current
    tasks and previously encountered tasks.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: 'A remarkable observation has been made repeatedly in recent years: training
    the *same* model to do several loosely connected tasks at the same time results
    in a model that’s *better at each task*. For instance, training the same neural
    machine-translation model to perform both English-to-German translation and French-to-Italian
    translation will result in a model that’s better at each language pair. Similarly,
    training an image-classification model jointly with an image-segmentation model,
    sharing the same convolutional base, results in a model that’s better at both
    tasks. This is fairly intuitive: there’s always *some* information overlap between
    seemingly disconnected tasks, and a joint model has access to a greater amount
    of information about each individual task than a model trained on that specific
    task only.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, when it comes to model reuse across tasks, we use pretrained weights
    for models that perform common functions, such as visual feature extraction. You
    saw this in action in chapter 9\. In the future, I expect a generalized version
    of this to be commonplace: we’ll use not only previously learned features (submodel
    weights) but also model architectures and training procedures. As models become
    more like programs, we’ll begin to reuse *program subroutines* like the functions
    and classes found in human programming languages.'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: 'Think of the process of software development today: once an engineer solves
    a specific problem (HTTP queries, for instance), they package it as an abstract,
    reusable library. Engineers who face a similar problem in the future will be able
    to search for existing packages, download one, and use it in their own project.
    In a similar way, in the future, meta-learning systems will be able to assemble
    new programs by sifting through a global library of high-level reusable blocks.
    When the system finds itself developing similar program subroutines for several
    different tasks, it can come up with an *abstract*, reusable version of the subroutine
    and store it in the global library (see [figure 14.13](#fig14-1)). These subroutines
    can be either geometric (deep learning modules with pretrained representations)
    or algorithmic (closer to the libraries that contemporary software engineers manipulate).'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0507-01.jpg)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
- en: '**Figure 14.13 A meta-learner capable of quickly developing task-specific models
    using reusable primitives (both algorithmic and geometric), thus achieving extreme
    generalization**'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: 14.5.5 The long-term vision
  id: totrans-353
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In short, here’s my long-term vision for machine learning:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: Models will be more like programs and will have capabilities that go far beyond
    the continuous geometric transformations of the input data we currently work with.
    These programs will arguably be much closer to the abstract mental models that
    humans maintain about their surroundings and themselves, and they will be capable
    of stronger generalization due to their rich algorithmic nature.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In particular, models will blend *algorithmic modules* providing formal reasoning,
    search, and abstraction capabilities with *geometric modules* providing informal
    intuition and pattern-recognition capabilities. This will achieve a blend of value-centric
    and program-centric abstraction. AlphaGo or self-driving cars (systems that require
    a lot of manual software engineering and human-made design decisions) provide
    an early example of what such a blend of symbolic and geometric AI could look
    like.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Such models will be *grown* automatically rather than hardcoded by human engineers,
    using modular parts stored in a global library of reusable subroutines—a library
    evolved by learning high-performing models on thousands of previous tasks and
    datasets. As frequent problem-solving patterns are identified by the meta-learning
    system, they will be turned into reusable subroutines—much like functions and
    classes in software engineering—and added to the global library.
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The process that searches over possible combinations of subroutines to grow
    new models will be a discrete search process (program synthesis), but it will
    be heavily guided by a form of *program-space intuition* provided by deep learning.
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This global subroutine library and associated model-growing system will be
    able to achieve some form of humanlike *extreme generalization*: given a new task
    or situation, the system will be able to assemble a new working model appropriate
    for the task using very little data, thanks to rich programlike primitives that
    generalize well and extensive experience with similar tasks. In the same way,
    humans can quickly learn to play a complex new video game if they have experience
    with many previous games, because the models derived from this previous experience
    are abstract and programlike, rather than a basic mapping between stimuli and
    action.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As such, this perpetually learning model-growing system can be interpreted
    as an *artificial general intelligence* (AGI). But don’t expect any singularitarian
    robot apocalypse to ensue: that’s pure fantasy, coming from a long series of profound
    misunderstandings of both intelligence and technology. Such a critique, however,
    doesn’t belong in this book'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 14.6 Staying up-to-date in a fast-moving field
  id: totrans-361
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As final parting words, I want to give you some pointers about how to keep learning
    and updating your knowledge and skills after you’ve turned the last page of this
    book. The field of modern deep learning, as we know it today, is only a few years
    old, despite a long, slow prehistory stretching back decades. With an exponential
    increase in financial resources and research headcount since 2013, the field as
    a whole is now moving at a frenetic pace. What you’ve learned in this book won’t
    stay relevant forever, and it isn’t all you’ll need for the rest of your career.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, there are plenty of free online resources that you can use to stay
    up to date and expand your horizons. Here are a few.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: 14.6.1 Practice on real-world problems using Kaggle
  id: totrans-364
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An effective way to acquire real-world experience is to try your hand at machine
    learning competitions on Kaggle ([https://kaggle.com](https://www.kaggle.com)).
    The only real way to learn is through practice and actual coding—that’s the philosophy
    of this book, and Kaggle competitions are the natural continuation of this. On
    Kaggle, you’ll find an array of constantly renewed data science competitions,
    many of which involve deep learning, prepared by companies interested in obtaining
    novel solutions to some of their most challenging machine learning problems. Fairly
    large monetary prizes are offered to top entrants.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: Most competitions are won using either the XGBoost library (for shallow machine
    learning) or Keras (for deep learning), so you’ll fit right in! By participating
    in a few competitions, maybe as part of a team, you’ll become more familiar with
    the practical side of some of the advanced best practices described in this book,
    especially hyper-parameter tuning, avoiding validation set overfitting, and model
    ensembling.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: 14.6.2 Read about the latest developments on arXiv
  id: totrans-367
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Deep learning research, in contrast with some other scientific fields, takes
    place completely in the open. Papers are made publicly and freely accessible as
    soon as they’re finalized, and a lot of related software is open source. arXiv
    ([https://arxiv.org](https://www.arxiv.org))—pronounced “archive” (the X stands
    for the Greek *chi*)—is an open-access preprint server for physics, mathematics,
    and computer science research papers. It has become the de facto way to stay up-to-date
    on the bleeding edge of machine learning and deep learning. The large majority
    of deep learning researchers upload any paper they write to arXiv shortly after
    completion. This allows them to plant a flag and claim a specific finding without
    waiting for a conference acceptance (which takes months), which is necessary given
    the fast pace of research and the intense competition in the field. It also allows
    the field to move extremely fast: all new findings are immediately available for
    all to see and to build on.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: 'An important downside is that the sheer quantity of new papers posted every
    day on arXiv makes it impossible to even skim them all, and the fact that they
    aren’t peer-reviewed makes it difficult to identify those that are both important
    and high quality. It’s challenging, and becoming increasingly more so, to find
    the signal in the noise. But some tools can help: in particular, you can use Google
    Scholar ([https://scholar.google.com](https://www.scholar.google.com)) to keep
    track of publications by your favorite authors.'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: 14.6.3 Explore the Keras ecosystem
  id: totrans-370
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With over one million users as of late 2021 and still growing, Keras has a
    large ecosystem of tutorials, guides, and related open source projects:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: Your main reference for working with Keras in R is the online documentation
    at [https://keras.rstudio.com](https://www.keras.rstudio.com) and [https://tensorflow.rstudio.com](https://www.tensorflow.rstudio.com).
    In particular, you’ll find extensive developer guides at [http://tensorflow.rstudio.com/guides](http://www.tensorflow.rstudio.com/guides),
    dozens of high-quality Keras code examples at [http://tensorflow.rstudio.com/](http://www.tensorflow.rstudio.com/)
    examples, and many tutorials at [http://tensorflow.rstudio.com/tutorials](http://www.tensorflow.rstudio.com/tutorials).
    Make sure to check them out!
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Don’t hesitate to also consult the Python documentation for Keras and Tensor-Flow,
    available at [https://www.tensorflow.org/api_docs/python/tf](https://www.tensorflow.org/api_docs/python/tf)
    and [https://keras.io/](https://www.keras.io/), even if you don’t know Python.
    Almost everything there you can read, understand, and apply to the R interface,
    all without any knowledge of Python. (If you do encounter some perplexing Python
    syntax, be sure to consult the appendix.)
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The R source code for Keras and Tensorflow can be found at [https://github.com/rstudio/keras](https://www.github.com/rstudio/keras)
    and [https://github.com/rstudio/tensorflow](https://www.github.com/rstudio/tensorflow).
    The Python and C++ sources are available at [https://github.com/keras-team/keras](https://www.github.com/keras-team/keras)
    and [https://github.com/tensorflow/tensorflow](https://www.github.com/tensorflow/tensorflow).
    All are open source.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can ask for help and join deep learning discussions in a few places:'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Machine Learning section of Rstudio community: [https://community.rstudio.com/c/ml/15](https://www.community.rstudio.com/c/ml/15)'
  id: totrans-376
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stack overflow: [https://stackoverflow.com](https://www.stackoverflow.com)
    (Be sure to tag your question with both R and Keras.)'
  id: totrans-377
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The (Python) Keras mailing list: keras-users@googlegroups.com.'
  id: totrans-378
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can follow me (François) on Twitter: @fchollet.'
  id: totrans-379
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 14.7 Final words
  id: totrans-380
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is the end of *Deep Learning with R, Second Edition*. I hope you’ve learned
    a thing or two about machine learning, deep learning, Keras, and maybe even cognition
    in general. Learning is a lifelong journey, especially in the field of AI, where
    we have far more unknowns on our hands than certitudes. So please go on learning,
    questioning, and researching. Never stop! Because even given the progress made
    so far, most of the fundamental questions in AI remain unanswered. Many haven’t
    even been properly asked yet.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](#endnote1)) Richard Feynman, interview, “The World from Another Point
    of View,” Yorkshire Television, 1972.
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ^([2](#endnote2)) Terry Winograd, “Procedures as a Representation for Data in
    a Computer Program for Understanding Natural Language” (1971).
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '^([3](#endnote3)) Fast Company, “Wozniak: Could a Computer Make a Cup of Coffee?”
    (March 2010), [http://mng.bz/pJMP](http://mng.bz/pJMP).'
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ^([4](#endnote4)) François Chollet, “On the Measure of Intelligence” (2019),
    [https://arxiv.org/abs/1911.01547](https://arxiv.org/abs/1911.01547).
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
