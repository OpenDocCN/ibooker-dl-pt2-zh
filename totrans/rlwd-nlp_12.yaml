- en: 9 Transfer learning with pretrained language models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9 使用预训练语言模型进行迁移学习
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容包括
- en: Using transfer learning to leverage knowledge from unlabeled textual data
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用无标签文本数据的知识进行迁移学习
- en: Using self-supervised learning to pretrain large language models such as BERT
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自监督学习对大型语言模型进行预训练，如 BERT
- en: Building a sentiment analyzer with BERT and the Hugging Face Transformers library
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 BERT 和 Hugging Face Transformers 库构建情感分析器
- en: Building a natural language inference model with BERT and AllenNLP
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 BERT 和 AllenNLP 构建自然语言推断模型
- en: The year 2018 is often called “an inflection point” in the history of NLP. A
    prominent NLP researcher, Sebastian Ruder ([https://ruder.io/nlp-imagenet/](https://ruder.io/nlp-imagenet/)),
    dubbed this change “NLP’s ImageNet moment,” where he used the name of a popular
    computer vision dataset and powerful models pretrained on it, pointing out that
    similar changes were underway in the NLP community as well. Powerful pretrained
    language models such as ELMo, BERT, and GPT-2 achieved state-of-the-art performance
    in many NLP tasks and completely changed how we build NLP models within months.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年通常被称为自然语言处理历史上的“拐点”。一位著名的 NLP 研究者，Sebastian Ruder（[https://ruder.io/nlp-imagenet/](https://ruder.io/nlp-imagenet/)）将这一变化称为“NLP
    的 ImageNet 时刻”，他使用了一个流行的计算机视觉数据集的名称以及在其上进行预训练的强大模型，指出 NLP 社区正在进行类似的变革。强大的预训练语言模型，如
    ELMo、BERT 和 GPT-2，在许多 NLP 任务上实现了最先进的性能，并在几个月内彻底改变了我们构建 NLP 模型的方式。
- en: One important concept underlying these powerful pretrained language models is
    *transfer learning*, a technique for improving the performance of one task using
    a model trained on another task. In this chapter, we’ll first introduce the concept,
    then move on to introducing BERT, the most popular pretrained language model proposed
    for NLP. We’ll cover how BERT is designed and pretrained, as well as how to use
    the model for downstream NLP tasks including sentiment analysis and natural language
    inference. We’ll also touch on other popular pretrained models including ELMo
    and RoBERTa.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这些强大的预训练语言模型背后的一个重要概念是*迁移学习*，一种利用在另一个任务上训练的模型来改善一个任务性能的技术。在本章中，我们首先介绍这个概念，然后介绍
    BERT，这是为 NLP 提出的最流行的预训练语言模型。我们将介绍 BERT 的设计和预训练，以及如何将该模型用于下游 NLP 任务，包括情感分析和自然语言推断。我们还将涉及其他流行的预训练模型，包括
    ELMo 和 RoBERTa。
- en: 9.1 Transfer learning
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 迁移学习
- en: We start this chapter by introducing *transfer learning*, a powerful machine
    learning concept fundamental to many pretrained language models (PLMs) in this
    chapter.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从介绍*迁移学习*开始这一章，这是本章中许多预训练语言模型（PLM）的基本机器学习概念。
- en: 9.1.1 Traditional machine learning
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.1 传统机器学习
- en: In traditional machine learning, before the advent of pretrained language models,
    NLP models were trained on a per-task basis, and they were useful only for the
    type of the task they were trained for (figure 9.1). For example, if you wanted
    a sentiment analysis model, you needed to use a dataset annotated with the desired
    output (e.g., negative, neutral, and positive labels), and the trained model was
    useful only for sentiment analysis. If you needed to build another model for part-of-speech
    (POS) tagging (an NLP task to identify the part of speech of words; see section
    5.2 for a review), you needed to do this all over again by collecting training
    data and training a POS tagging model from scratch. You could not “reuse” your
    sentiment analysis model for POS tagging, no matter how good your model was, because
    these two were trained for two fundamentally different tasks. However, these tasks
    both operated on the same language and all this seemed wasteful. For example,
    knowing that “wonderful,” “awesome,” and “great” are all adjectives that have
    positive meaning would help both sentiment analysis and part-of-speech tagging.
    Under the traditional machine learning paradigm, not only did we need to prepare
    training data large enough to teach “common sense” like this to the model, but
    individual NLP models also needed to learn such facts about the language from
    scratch, solely from the given data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统机器学习中，在预训练语言模型出现之前，NLP 模型是根据任务进行训练的，它们仅对它们所训练的任务类型有用（图 9.1）。例如，如果你想要一个情感分析模型，你需要使用带有所需输出的数据集（例如，负面、中性和正面标签），而训练好的模型仅对情感分析有用。如果你需要构建另一个用于词性标注（一种
    NLP 任务，用于识别单词的词性；请参阅第 5.2 节进行回顾）的模型，你需要通过收集训练数据并从头开始训练一个词性标注模型来完成。无论你的模型有多好，你都不能将情感分析模型“重用”于词性标注，因为这两者是为两个根本不同的任务而训练的。然而，这些任务都在同一语言上操作，这一切似乎都是浪费的。例如，知道
    “wonderful”，“awesome” 和 “great” 都是具有积极意义的形容词，这将有助于情感分析和词性标注。在传统机器学习范式下，我们不仅需要准备足够大的训练数据来向模型教授这种“常识”，而且个别
    NLP 模型还需要从给定的数据中学习关于语言的这些事实。
- en: '![CH09_F01_Hagiwara](../Images/CH09_F01_Hagiwara.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F01_Hagiwara](../Images/CH09_F01_Hagiwara.png)'
- en: Figure 9.1 In traditional machine learning, each trained model was used for
    just one task.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 在传统机器学习中，每个训练好的模型仅用于一个任务。
- en: 9.1.2 Word embeddings
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.2 词嵌入
- en: At this point, you may realize this sounds somewhat familiar. Recall our discussion
    in section 3.1 on word embeddings and why they are important. To recap, word embeddings
    are vector representations of words that are learned so that semantically similar
    words share similar representations. As a result, vectors for “dog” and “cat,”
    for example, end up being located in a close proximity in a high-dimensional space.
    These representations are trained on an independent, large textual corpus without
    any training signals, using algorithms such as Skip-gram and CBOW, often collectively
    called *Word2vec* (section 3.4).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，你可能会意识到这听起来有些眼熟。回想一下我们在第 3.1 节关于词嵌入以及它们为什么重要的讨论。简而言之，词嵌入是单词的向量表示，这些向量是通过学习得到的，以便语义上相似的单词具有相似的表示。因此，例如，“dog”
    和 “cat” 的向量最终会位于高维空间中的接近位置。这些表示是在一个独立的大型文本语料库上进行训练的，没有任何训练信号，使用诸如 Skip-gram 和
    CBOW 等算法，通常统称为 *Word2vec*（第 3.4 节）。
- en: After these word embeddings are trained, downstream NLP tasks can use them as
    the input to their models (which are often neural networks, but not necessarily).
    Because these embeddings already capture semantic relationship between words (e.g.,
    dogs and cats are both animals), these tasks no longer need to learn how the language
    works from scratch, which gives them the upper hand in the task they are trying
    to solve. The model can now focus on learning higher-level concepts that cannot
    be captured by word embeddings (e.g., phrases, syntax, and semantics) and the
    task-specific patterns learned from the given annotated data. This is why using
    word embeddings gives a performance boost to many NLP models.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些词嵌入训练之后，下游 NLP 任务可以将它们作为模型的输入（通常是神经网络，但不一定）。因为这些嵌入已经捕捉到单词之间的语义关系（例如，dogs
    和 cats 都是动物），所以这些任务不再需要从头学习语言是如何工作的，这使它们在试图解决的任务中占据了上风。模型现在可以专注于学习无法被词嵌入捕捉到的更高级别概念（例如，短语、句法和语义）以及从给定的注释数据中学到的任务特定模式。这就是为什么使用词嵌入会给许多
    NLP 模型带来性能提升的原因。
- en: In chapter 3, we likened this to teaching a baby (= an NLP model) how to dance.
    By letting babies learn how to walk steadily first (= training word embeddings),
    dance teachers (= task-specific datasets and training objectives) can focus on
    teaching specific dance moves without worrying whether babies can even stand and
    walk properly. This “phased training” approach makes everything easier if you
    want to teach another skill to the baby (e.g., teaching martial arts) because
    they already have a good grasp of the fundamental skill (walking).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 3 章中，我们将这比作是教一个婴儿（= 一个自然语言处理模型）如何跳舞。通过让婴儿先学会稳步行走（= 训练词嵌入），舞蹈老师（= 任务特定数据集和训练目标）可以专注于教授具体的舞步，而不必担心婴儿是否能够站立和行走。这种“分阶段训练”方法使得如果你想教婴儿另一种技能（例如，教授武术），一切都变得更容易，因为他们已经对基本技能（行走）有了很好的掌握。
- en: The beauty of all this is that word embeddings can be learned independently
    of the downstream tasks. These word embeddings are *pretrained*, meaning their
    training happens before the training of downstream NLP tasks. Using the dancing
    baby analogy, dance teachers can safely assume that all the incoming dance students
    have already learned how to stand and walk properly. Pretrained word embeddings
    created by the developers of the algorithm are often freely available, and anyone
    can download and integrate them into their NLP applications. This process is illustrated
    in figure 9.2.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这一切的美妙之处在于，词嵌入可以独立于下游任务进行学习。这些词嵌入是*预训练*的，这意味着它们的训练发生在下游自然语言处理任务的训练之前。使用跳舞婴儿的类比，舞蹈老师可以安全地假设所有即将到来的舞蹈学生都已经学会了如何正确站立和行走。由算法开发者创建的预训练词嵌入通常是免费提供的，任何人都可以下载并将其集成到他们的自然语言处理应用程序中。这个过程在图
    9.2 中有所说明。
- en: '![CH09_F02_Hagiwara](../Images/CH09_F02_Hagiwara.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F02_Hagiwara](../Images/CH09_F02_Hagiwara.png)'
- en: Figure 9.2 Leveraging word embeddings helps build a better NLP model.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 利用词嵌入有助于构建更好的自然语言处理模型。
- en: 9.1.3 What is transfer learning?
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.3 什么是迁移学习？
- en: If you generalize what you did with word embeddings earlier, you took the outcome
    of one task (i.e., predicting word cooccurrence with embeddings) and transferred
    the knowledge gleaned from it to another one (i.e., sentiment analysis, or any
    other NLP tasks). In machine learning, this process is called *transfer learning*,
    which is a collection of related techniques to improve the performance of a machine
    learning model in a task using data and/or models trained in a different task.
    Transfer learning always consists of two or more steps—a machine learning model
    is first trained for one task (called *pretraining*), which is then adjusted and
    used in another (called *adaptation*). If the same model is used for both tasks,
    the second step is called *fine-tuning*, because you are tuning the same model
    slightly but for a different task. See figure 9.3 for an illustration of transfer
    learning in NLP.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你总结一下之前对词嵌入所做的事情，你会发现你将一个任务的结果（即，用嵌入预测词共现）并将从中获得的知识转移到另一个任务（即，情感分析，或任何其他自然语言处理任务）。在机器学习中，这个过程被称为*迁移学习*，这是一系列相关的技术，用于通过在不同任务上训练的数据和/或模型来提高机器学习模型在某一任务中的性能。迁移学习总是由两个或多个步骤组成—首先为一个任务训练一个机器学习模型（称为*预训练*），然后调整并在另一个任务中使用它（称为*适应*）。如果同一个模型用于两个任务，第二步称为*微调*，因为你稍微调整了同一个模型，但是用于不同的任务。请参见图
    9.3，以了解自然语言处理中迁移学习的示意图。
- en: '![CH09_F03_Hagiwara](../Images/CH09_F03_Hagiwara.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F03_Hagiwara](../Images/CH09_F03_Hagiwara.png)'
- en: Figure 9.3 Leveraging transfer learning helps build a better NLP model.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3 利用迁移学习有助于构建更好的自然语言处理模型。
- en: Transfer learning has become the dominant way for building high-quality NLP
    models in the past few years for two main reasons. Firstly, thanks to powerful
    neural network models such as the Transformer and self-supervised learning (see
    section 9.2.2), it became possible to bootstrap high-quality embeddings from an
    almost unlimited amount of natural language text. These embeddings take into account
    the structure, context, and semantics of natural language text to a great extent.
    Secondly, thanks to transfer learning, anyone can incorporate these powerful pretrained
    language models into their NLP applications, even without access to a lot of textual
    resources, such as web-scale corpora, or compute resources, such as powerful GPUs.
    The advent of these new technologies (the Transformer, self-supervised learning,
    pretrained language models, and transfer learning) moved the field of NLP to a
    completely new stage and pushed the performance of many NLP tasks to a near-human
    level. In the following subsections, we’ll see transfer learning in action by
    actually building NLP models while leveraging PLMs such as BERT.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 过去几年中，迁移学习已成为构建高质量自然语言处理模型的主要方法，原因有两个。首先，由于强大的神经网络模型如Transformer和自监督学习（见第9.2.2节），几乎可以从几乎无限量的自然语言文本中引导出高质量的嵌入。这些嵌入在很大程度上考虑了自然语言文本的结构、上下文和语义。其次，由于迁移学习，任何人都可以将这些强大的预训练语言模型整合到他们的自然语言处理应用程序中，即使没有访问大量的文本资源，如网络规模语料库，或计算资源，如强大的GPU。这些新技术的出现（Transformer、自监督学习、预训练语言模型和迁移学习）将自然语言处理领域推向了一个全新的阶段，并将许多自然语言处理任务的性能推向了接近人类水平。在接下来的子节中，我们将看到迁移学习在实际构建自然语言处理模型时的应用，同时利用诸如BERT等预训练语言模型。
- en: Note that the concept called *domain adaptation* is closely related to transfer
    learning. Domain adaptation is a technique where you train a machine learning
    model in one domain (e.g., news) and adapt it to another domain (e.g., social
    media), but these are for the *same task* (e.g., text classification). On the
    other hand, the transfer learning we cover in this chapter is applied to *different
    tasks* (e.g., language modeling versus text classification). You can achieve the
    same effect using the transfer learning paradigm covered in this chapter, and
    we do not specifically cover domain adaptation as a separate topic. Interested
    readers can learn more about domain adaptation from a recent review paper.[¹](#pgfId-1111602)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，所谓的*领域自适应*概念与迁移学习密切相关。领域自适应是一种技术，你在一个领域（例如，新闻）训练一个机器学习模型，然后将其调整到另一个领域（例如，社交媒体），但这些领域属于*相同任务*（例如，文本分类）。另一方面，在本章中涵盖的迁移学习应用于*不同任务*（例如，语言建模与文本分类）。你可以利用本章介绍的迁移学习范式来实现相同的效果，我们不会专门涵盖领域自适应作为一个单独的主题。有兴趣的读者可以从最近的一篇评论性文章中了解更多关于领域自适应的信息。[¹](#pgfId-1111602)
- en: 9.2 BERT
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2 BERT
- en: In this section, we will cover BERT in detail. BERT (Bidirectional Encoder Representations
    from Transformers)[²](#pgfId-1111610) is by far the most popular and most influential
    pretrained language model to date that revolutionized how people train and build
    NLP models. We will first introduce *contextualized embeddings* and why they are
    important, then move on to explaining self-supervised learning, which is an important
    concept in pretraining language models. We’ll cover two self-supervised tasks
    used for pretraining BERT, namely, masked language models and next-sentence prediction,
    and cover ways to adapt BERT for your applications.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将详细介绍BERT。BERT（双向编码器表示转换器）[²](#pgfId-1111610)是迄今为止最流行和最具影响力的预训练语言模型，彻底改变了人们训练和构建自然语言处理模型的方式。我们将首先介绍*上下文化嵌入*及其重要性，然后讨论自监督学习，这是预训练语言模型中的一个重要概念。我们将涵盖BERT用于预训练的两个自监督任务，即，掩码语言模型和下一个句子预测，并介绍如何将BERT调整到你的应用程序中。
- en: 9.2.1 Limitations of word embeddings
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.1 词嵌入的局限性
- en: 'Word embeddings are a powerful concept that can give your application a boost
    in the performance, although they are not without limitation. One obvious issue
    is that they cannot take context into account. Words you see in natural language
    are often polysemous, meaning they may have more than one meaning, depending on
    their context. However, because word embeddings are trained per token type, all
    the different meanings are compressed into a single vector. For example, training
    a single vector for “dog” or “apple” cannot deal with the fact that “hot dog”
    or “Big Apple” are not a type of animal or fruit, respectively. As another example,
    consider what “play” means in these sentences: “They played games,” “I play Chopin,”
    “We play baseball,” and “Hamlet is a play by Shakespeare” (these sentences are
    all from Tatoeba.org). These occurrences of “play” have different meanings, and
    assigning a single vector wouldn’t help much in downstream NLP tasks (e.g., in
    classifying the topic into sports, music, and art).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 单词嵌入是一个强大的概念，可以提高应用程序的性能，尽管它们也有限制。一个明显的问题是它们无法考虑上下文。在自然语言中看到的单词通常是多义的，意味着它们可能根据上下文有多个含义。然而，由于单词嵌入是按标记类型训练的，所有不同的含义都被压缩成一个单一的向量。例如，为“dog”或“apple”训练一个单一的向量无法处理“热狗”或“大苹果”分别不是动物或水果这一事实。再举一个例子，考虑这些句子中“play”的含义：“They
    played games,” “I play Chopin,” “We play baseball,” 和 “Hamlet is a play by Shakespeare”（这些句子都来自
    Tatoeba.org）。这些“play”的出现有不同的含义，分配一个单一的向量在下游的 NLP 任务中并不会有太大帮助（例如在将主题分类为体育、音乐和艺术方面）。
- en: Due to this limitation, NLP researchers started exploring ways to transform
    the entire sentence into a series of vectors that consider the context, called
    *contextualized embeddings* or simply *contextualization*. With these representations,
    all the occurrences of “play” in the previous examples would have different vectors
    assigned, helping downstream tasks disambiguate different uses of the word. Notable
    milestones in contextualized embeddings include CoVe[³](#pgfId-1111623) and ELMo
    (section 9.3.1), although the biggest breakthrough was achieved by BERT, a Transformer-based
    pretrained language model, which is the focus of this section.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个限制，自然语言处理（NLP）研究人员开始探索将整个句子转换为一系列考虑上下文的向量的方法，称为*上下文化嵌入*或简称为*上下文化*。有了这些表示，前面示例中“play”的所有出现将被分配不同的向量，帮助下游任务区分单词的不同用法。上下文化嵌入的重要里程碑包括
    CoVe[³](#pgfId-1111623) 和 ELMo（第9.3.1节），尽管最大的突破是由 BERT 实现的，这是一个基于 Transformer
    的预训练语言模型，是本节的重点。
- en: 'We learned the Transformer uses a mechanism called *self-attention* to gradually
    transform the input sequence by summarizing it. The core idea of BERT is simple:
    it uses the Transformer (the Transformer encoder, to be precise) to transform
    the input into contextualized embeddings. The Transformer transforms the input
    through a series of layers by gradually summarizing the input. Similarly, BERT
    contextualizes the input through a series of Transformer encoder layers. This
    is illustrated in figure 9.4.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学习到 Transformer 使用一种称为*自注意力*的机制逐渐转换输入序列来总结它。BERT的核心思想很简单：它使用 Transformer（准确地说是
    Transformer 编码器）将输入转换为上下文化嵌入。Transformer通过一系列层逐渐摘要输入。同样，BERT通过一系列Transformer编码器层对输入进行上下文化处理。这在图9.4中有所说明。
- en: '![CH09_F04_Hagiwara](../Images/CH09_F04_Hagiwara.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F04_Hagiwara](../Images/CH09_F04_Hagiwara.png)'
- en: Figure 9.4 BERT processes input through attention layers to produce contextualized
    embeddings.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 BERT通过注意力层处理输入以生成上下文化嵌入。
- en: Because BERT is based on the Transformer architecture, it inherits all the strengths
    of the Transformer. Its self-attention mechanism enables it to “random access”
    over the input and capture long-term dependencies among input tokens. Unlike traditional
    language models (such as the one based on LSTM that we covered in section 5.5)
    that can make predictions in only one direction, the Transformer can take into
    account the context in both directions. Using the sentence “Hamlet is a play by
    Shakespeare” as an example, the contextualized embedding for “play” can incorporate
    the information from both “Hamlet” and “Shakespeare,” which makes it easier to
    capture its “dramatic work for the stage” meaning of “play.”
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因为BERT基于Transformer架构，它继承了Transformer的所有优点。其自注意力机制使其能够在输入上进行“随机访问”，并捕获输入标记之间的长期依赖关系。与传统的语言模型（例如我们在第5.5节中介绍的基于LSTM的语言模型）不同，后者只能沿着一个方向进行预测，Transformer可以在两个方向上考虑上下文。以“哈姆雷特是莎士比亚的一部戏剧”为例，对于“戏剧”这个词的上下文化嵌入可以包含来自“哈姆雷特”和“莎士比亚”的信息，这样就更容易捕捉到“戏剧舞台作品”的意思。
- en: 'If this concept is as simple as “BERT is just a Transformer encoder,” why does
    it deserve an entire section here? Because we haven’t answered two important practical
    questions yet: how to train and adapt the model. Neural network models, no matter
    how powerful, are useless without specific strategies for training and where to
    get the training data. Also, transfer learning is useless without specific strategies
    for adapting the pretrained model. We will discuss these questions in the following
    subsections.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个概念就像“BERT只是一个Transformer编码器”那么简单，为什么它在这里值得有一个完整的章节呢？因为我们还没有回答两个重要的实际问题：如何训练和调整模型。神经网络模型，无论多么强大，如果没有特定的训练策略和获取训练数据的途径，都是无用的。此外，预训练模型没有特定的调整策略也是无用的。我们将在以下小节中讨论这些问题。
- en: 9.2.2 Self-supervised learning
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.2 自监督学习
- en: The Transformer, which was originally proposed for machine translation, is trained
    using parallel text. Its encoder and decoder are optimized to minimize the loss
    function, which is the cross entropy defined by the difference between the decoder
    output and the expected, correct translation. However, the purpose of pretraining
    BERT is to derive high-quality contextualized embeddings, and BERT has only an
    encoder. How can we “train” BERT so that it is useful for downstream NLP tasks?
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer最初是为了机器翻译而提出的，它是使用平行文本进行训练的。它的编码器和解码器被优化以最小化损失函数，即解码器输出和预期正确翻译之间的差异所定义的交叉熵。然而，预训练BERT的目的是得到高质量的上下文嵌入，而BERT只有一个编码器。我们如何“训练”BERT以使其对下游自然语言处理任务有用呢？
- en: If you think of BERT just as another way of deriving embeddings, you can draw
    inspiration from how word embeddings are trained. Recall that in section 3.4,
    to train word embeddings, we make up a “fake” task where surrounding words are
    predicted with word embeddings. We are not interested in the prediction per se
    but rather the “by-product” of the training, which is the word embeddings derived
    as the parameters of the model. This type of training paradigm where the data
    itself provides training signals is called *self-supervised learning,* or simply
    *self-supervision,* in modern machine learning*.* Self-supervised learning is
    still one type of supervised learning from the model’s point of view—the model
    is trained in such a way that it minimizes the loss function defined by the training
    signal. It is where the training signal comes from that is different. In supervised
    learning, training signals usually come from human annotations. In self-supervised
    learning, training signals come from the data itself with no human intervention.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你把BERT只看作是另一种得到嵌入的方式，你可以从词嵌入是如何训练的中得到灵感。回想一下，在第3.4节中，为了训练词嵌入，我们构造了一个“假”任务，即用词嵌入预测周围的单词。我们对预测本身不感兴趣，而是对训练的“副产品”感兴趣，即作为模型参数的词嵌入。这种数据本身提供训练信号的训练范式称为*自监督学习*，或者简称为*自监督*，在现代机器学习中。从模型的角度来看，自监督学习仍然是监督学习的一种类型——模型被训练以使得它最小化由训练信号定义的损失函数。不同之处在于训练信号的来源。在监督学习中，训练信号通常来自人类注释。在自监督学习中，训练信号来自数据本身，没有人类干预。
- en: With increasingly larger datasets and more powerful models, self-supervised
    learning has become a popular way to pretrain NLP models in the past several years.
    But why does it work so well? Two factors contribute to this—one is that the type
    of self-supervision here is trivially simple to create (just extracting surrounding
    words for Word2vec), but it requires deep understanding of the language to solve
    it. For example, reusing the example from the language model we discussed in chapter
    5, to answer “My trip to the beach was ruined by bad ___,” not only does the system
    need to understand the sentence but it also needs to be equipped with some sort
    of “common sense” for what type of things could ruin a trip to a beach (e.g.,
    bad weather, heavy traffic). The knowledge required to predict the surrounding
    words ranges from simple collocation/association (e.g., The Statue of ____ in
    New ____), syntactic and grammatical (e.g., “My birthday is ___ May”), and semantic
    (the previous example). Second, there is virtually no limit on the amount of data
    used for self-supervision, because all you need is clean, plain text. You can
    download large datasets (e.g., Wikipedia dump) or crawl and filter web pages,
    which is a popular way to train many pretrained language models.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几年中，随着数据集越来越大和模型越来越强大，自监督学习已经成为预训练 NLP 模型的流行方式。但是为什么它能够如此成功呢？其中两个因素起到了作用——一个是这里的自监督类型在创建时非常简单（只需提取周围单词用于
    Word2vec），但是解决它需要对语言有深入的理解。例如，重新使用我们在第 5 章中讨论的语言模型的例子，要回答“我的海滩之行被糟糕的___毁了”，系统不仅需要理解句子，还需要具备某种“常识”，了解什么样的事情可能会毁了一次海滩之行（例如，糟糕的天气，交通拥堵）。预测周围单词所需的知识范围从简单的搭配/联想（例如，“纽约的____雕像”），到句法和语法（例如，“我的生日是___五月”），再到语义（前面的例子）。第二个因素是几乎没有限制用于自监督的数据量，因为你所需要的只是干净的纯文本。你可以下载大型数据集（例如，维基百科转储）或爬取和过滤网页，这是训练许多预训练语言模型的流行方式之一。
- en: 9.2.3 Pretraining BERT
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.3 BERT 预训练
- en: Now that we all understand how useful self-supervised learning can be for pretraining
    language models, let’s see how we can use it for pretraining BERT. As mentioned
    earlier, BERT is just a Transformer encoder that transforms the input into a series
    of embeddings that take context into account. For pretraining word embeddings,
    you could simply predict surrounding words based on the embeddings of the target
    word. For pretraining unidirectional language models, you could simply predict
    the next token based on the tokens that come before the target. But for bidirectional
    language models such as BERT, you cannot use these strategies, because the input
    for the prediction (contextualized embeddings) also depends on what comes before
    and after the input. This sounds like a chicken-and-egg problem.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们都明白了自监督学习对于预训练语言模型有多么有用，让我们看看我们如何将其用于预训练 BERT。如前所述，BERT 只是一个将输入转换为考虑上下文的一系列嵌入的
    Transformer 编码器。对于预训练词嵌入，你可以根据目标词的嵌入预测周围的单词。对于预训练单向语言模型，你可以根据目标之前的标记预测下一个标记。但是对于诸如
    BERT 这样的双向语言模型，你不能使用这些策略，因为用于预测的输入（上下文化的嵌入）还取决于输入之前和之后的内容。这听起来像是一个先有鸡还是先有蛋的问题。
- en: The inventors of BERT solved this with a brilliant idea called *masked language
    model* (MLM), where they drop (mask) words randomly in a given sentence and let
    the model predict what the dropped word is. Specifically, after replacing a small
    percentage of words in a sentence with a special placeholder, BERT uses the Transformer
    to encode the input and then uses a feed-forward layer and a softmax layers to
    derive a probability distribution over possible words that can fill in that blank.
    Because you already know the answer (because you dropped the words in the first
    place), you can use the regular cross entropy to train the model, as illustrated
    in figure 9.5.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 的发明者们通过一个称为*掩码语言模型*（MLM）的精妙思想来解决这个问题，在给定的句子中随机删除（掩盖）单词，并让模型预测被删除的单词是什么。具体来说，在句子中用一个特殊的占位符替换一小部分单词后，BERT
    使用 Transformer 对输入进行编码，然后使用前馈层和 softmax 层推导出可能填充该空白的单词的概率分布。因为你已经知道答案（因为你首先删除了这些单词），所以你可以使用常规的交叉熵来训练模型，如图
    9.5 所示。
- en: '![CH09_F05_Hagiwara](../Images/CH09_F05_Hagiwara.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F05_Hagiwara](../Images/CH09_F05_Hagiwara.png)'
- en: Figure 9.5 Pretraining BERT with a masked language model
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5 使用掩码语言模型对 BERT 进行预训练
- en: Masking and predicting words is not a completely new idea—it’s closely related
    to *cloze tests*, where the test-taker is asked to replace the removed words in
    a sentence. This test form is often used to assess how well students can understand
    the language. As we saw earlier, completing missing words in a natural language
    text requires deep understanding of the language, ranging from simple associations
    to semantic relationships. As a result, by telling the model to solve this fill-in-the-blank
    type of task over a huge amount of textual data, the neural network model is trained
    so that it can produce contextualized embeddings that incorporate deep linguistic
    knowledge.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 掩码和预测单词并不是一个完全新的想法——它与*填空测试*密切相关，测试者被要求在句子中替换被移除的单词。这种测试形式经常用于评估学生对语言的理解程度。正如我们之前所见，填写自然语言文本中的缺失单词需要对语言的深入理解，从简单的关联到语义关系。因此，通过告诉模型解决这种填空类型的任务，涵盖了大量文本数据，神经网络模型经过训练，使其能够产生融合了深层语言知识的上下文嵌入。
- en: You may be wondering what this input [MASK] is and what you actually need to
    do if you want to implement pretraining BERT yourself. In training neural networks,
    people often use special tokens such as [MASK] that we mentioned here. These special
    tokens are just like other (naturally occurring) tokens such as the words “dog”
    and “cat,” except they don’t occur in text naturally (you can’t find any [MASK]
    in natural language corpora, no matter how hard you look) and the designers of
    the neural networks define what they mean. The model will learn to give representations
    to these tokens so that it can solve the task at hand. Other special tokens include
    BOS (beginning of sentence), EOS (end of sentence), and UNK (unknown word), which
    we already encountered in earlier chapters.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想自己实现 BERT 的预训练，你可能会想知道这个输入[MASK]是什么，以及你实际上需要做什么。在训练神经网络时，人们经常使用特殊的标记，比如我们在这里提到的[MASK]。这些特殊的标记就像其他（自然出现的）标记一样，比如“狗”和“猫”的单词，只是它们在文本中不会自然出现（无论你多么努力，都找不到任何[MASK]在自然语言语料库中），神经网络的设计者定义了它们的含义。模型将学会为这些标记提供表示，以便它可以解决手头的任务。其他特殊标记包括
    BOS（句子的开始）、EOS（句子的结束）和 UNK（未知单词），我们在之前的章节中已经遇到过。
- en: Finally, BERT is pretrained not just with the masked language model but also
    with another type of task called *next-sentence prediction* (NSP), where two sentences
    are given to BERT and the model is asked to predict whether the second sentence
    is the “real” next sentence of the first. This is another type of self-supervised
    learning (“fake” task) for which the training data can be created in an unlimited
    manner without much human intervention, because you can extract two consecutive
    sentences (or just stitch together two sentences at random) from any corpus and
    make the training data for this task. The rationale behind this task is that by
    training with this objective, the model will learn how to infer the relationship
    of two sentences. However, the effectiveness of this task has been actively debated
    (e.g., RoBERTa dropped this task whereas ALBERT replaced it with another task
    called *sentence-order prediction*), and we will not go into the details of this
    task here.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，BERT 不仅使用掩码语言模型进行预训练，还使用了另一种类型的任务，称为*下一句预测*（NSP），其中向 BERT 提供了两个句子，并要求模型预测第二个句子是否是第一个句子的“真正”下一个句子。这是另一种类型的自监督学习（“伪造”任务），其训练数据可以在很少的人工干预下无限制地创建，因为你可以从任何语料库中提取两个连续的句子（或仅随机拼接两个句子）并为此任务创建训练数据。这个任务背后的原理是通过训练这个目标，模型将学会如何推断两个句子之间的关系。然而，这个任务的有效性一直在积极地讨论中（例如，RoBERTa
    放弃了这个任务，而 ALBERT 将其替换为另一个称为*句子顺序预测*的任务），我们将不在这里详细讨论这个任务。
- en: All this pretraining sounds somewhat complex, but good news is that you rarely
    need to implement this step yourself. Similar to word embeddings, developers and
    researchers of these language models pretrain their models on a huge amount of
    natural language text (usually 10 GB-plus or even 100 GB-plus of uncompressed
    text) with many GPUs and make the pretrained models publicly available so that
    anyone can use them.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些预训练听起来有些复杂，但好消息是你很少需要自己实现这一步。类似于词嵌入，这些语言模型的开发人员和研究人员在大量自然语言文本上预训练他们的模型（通常是
    10 GB 或更多，甚至是 100 GB 或更多的未压缩文本），并使用许多 GPU，并且将预训练模型公开可用，以便任何人都可以使用它们。
- en: 9.2.4 Adapting BERT
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.4 调整 BERT
- en: 'At the second (and final) stage of transfer learning, a pretrained model is
    adapted to the target task so that the latter can leverage signals learned by
    the former. There are two main ways to adapt BERT to individual downstream tasks:
    *fine-tuning* and *feature extraction*. In fine-tuning, the neural network architecture
    is slightly modified so that it can produce the type of predictions for the task
    in question, and the entire network is continuously trained on the training data
    for the task so that the loss function is minimized. This is exactly the way you
    train a neural network for NLP tasks, such as sentiment analysis, with one important
    difference—BERT “inherits” the model weights learned through pretraining, instead
    of being initialized randomly and trained from scratch. In this way, the downstream
    task can leverage the powerful representations learned by BERT through pretraining
    on a large amount of data.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在迁移学习的第二（也是最后）阶段，预训练模型被调整以适应目标任务，使后者可以利用前者学到的信号。有两种主要方式可以使BERT适应个别下游任务：*微调*
    和 *特征提取*。在微调中，神经网络架构稍微修改，以便为所讨论的任务产生类型的预测，并且整个网络在任务的训练数据上持续训练，以使损失函数最小化。这正是你训练NLP任务的神经网络的方式，例如情感分析，其中有一个重要的区别—BERT“继承”了通过预训练学到的模型权重，而不是从头开始随机初始化并进行训练。通过这种方式，下游任务可以利用BERT通过大量数据预训练学到的强大表示。
- en: The exact way the BERT architecture is modified varies, depending on the final
    task, but here I’m going to describe the simplest case where the task is to predict
    some sort of label for a given sentence. This is also called a *sentence-prediction
    task*, which includes sentiment analysis, which we covered in chapter 2\. For
    downstream tasks to be able to extract representations for a sentence, BERT prepends
    a special token [CLS] (for *classification*) to every sentence at the pretraining
    phase. You can extract the hidden states of BERT with this token and use them
    as the representation of the sentence. As with other classification tasks, a linear
    layer can compress this representation into a set of “scores” that correspond
    to how likely each label is the correct answer. You can then use softmax to derive
    a probability distribution. For example, if you are working on a sentiment analysis
    dataset with five labels (strongly negative to strongly positive), you’ll use
    a linear layer to reduce the dimensionality to 5\. This type of linear layer combined
    with softmax, which is plugged into a larger pretrained model such as BERT, is
    often called a *head*. In other words, we are attaching a *classification head*
    to BERT to solve a sentence-prediction task. The weights for the entire network
    (the head and BERT) are adjusted so that the loss function is minimized. This
    means that the BERT weights initialized with pretrained ones also are adjusted
    (fine-tuned) through backpropagation. See figure 9.6 for an illustration.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: BERT架构修改的确切方式因最终任务而异，但在这里我将描述最简单的情况，即任务是对给定句子预测某种标签。这也被称为 *句子预测任务*，其中包括我们在第2章中介绍的情感分析。为了使下游任务能够提取句子的表示，BERT在预训练阶段为每个句子添加一个特殊标记[CLS]（用于
    *分类*）。您可以使用此标记提取BERT的隐藏状态，并将其用作句子的表示。与其他分类任务一样，线性层可以将此表示压缩为一组“分数”，这些分数对应于每个标签是正确答案的可能性。然后，您可以使用softmax推导出一个概率分布。例如，如果您正在处理一个情感分析数据集，其中有五个标签（非常负面到非常正面），则您将使用线性层将维度降低到5。这种线性层与softmax结合起来，插入到诸如BERT之类的较大的预训练模型中，通常被称为
    *头部*。换句话说，我们正在将一个 *分类头* 附加到BERT上，以解决句子预测任务。整个网络的权重（头部和BERT）都会被调整，以使损失函数最小化。这意味着通过反向传播微调BERT权重初始化的权重也会被调整。见图9.6以示例说明。
- en: '![CH09_F06_Hagiwara](../Images/CH09_F06_Hagiwara.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F06_Hagiwara](../Images/CH09_F06_Hagiwara.png)'
- en: Figure 9.6 Pretraining and fine-tuning BERT with an attached classification
    head
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6 使用附加的分类头对BERT进行预训练和微调
- en: Another variation in fine-tuning BERT uses all the embeddings, averaged over
    the input tokens. In this method, called *mean over time* or *bag of embeddings*,
    all the embeddings produced by BERT are summed up and divided by the length of
    input, just like the bag-of-words model, to produce a single vector. This method
    is less popular than using the CLS special token but may work better depending
    on the task. Figure 9.7 illustrates this.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种微调 BERT 的变体使用了所有嵌入，这些嵌入是在输入令牌上进行平均的。在这种称为 *mean over time* 或 *bag of embeddings*
    的方法中，BERT 生成的所有嵌入被求和并除以输入的长度，就像词袋模型一样，以产生一个单一的向量。这种方法不如使用 CLS 特殊令牌那么受欢迎，但根据任务的不同可能效果更好。图
    9.7 阐明了这一点。
- en: '![CH09_F07_Hagiwara](../Images/CH09_F07_Hagiwara.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F07_Hagiwara](../Images/CH09_F07_Hagiwara.png)'
- en: Figure 9.7 Pretraining and fine-tuning BERT using mean over time and a classification
    head
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7 预训练和微调 BERT 使用时间平均和分类头
- en: Another way to adapt BERT for downstream NLP tasks is *feature extraction*.
    Here BERT is used to extract features, which are simply a sequence of contextualized
    embeddings produced by the final layer of BERT. You can simply feed these vectors
    to another machine learning model as features and make predictions, as shown in
    figure 9.8.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种用于下游 NLP 任务的 BERT 适应方式是 *feature extraction*。在这里，BERT 被用来提取特征，这些特征只是由 BERT
    的最终层产生的一系列上下文化嵌入。你可以将这些向量简单地作为特征馈送到另一个机器学习模型中，并进行预测，如图 9.8 所示。
- en: '![CH09_F08_Hagiwara](../Images/CH09_F08_Hagiwara.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F08_Hagiwara](../Images/CH09_F08_Hagiwara.png)'
- en: Figure 9.8 Pretraining and using BERT for feature extraction
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8 预训练和使用 BERT 进行特征提取
- en: 'Graphically, this approach looks similar to fine-tuning. After all, you are
    feeding the output from BERT to another ML model. However, there are two subtle
    but important differences: first, because you are no longer optimizing the neural
    network, the second ML model doesn’t have to be a neural network. Some machine
    learning tasks (e.g., unsupervised clustering) are not what neural networks are
    good at solving, and feature extraction offers a perfect solution in these situations.
    Also, you are free to use more “traditional” ML algorithms, such as SVMs (support
    vector machines), decision trees, and gradient-boosted methods (such as GBDT,
    or gradient-boosted decision trees), which may offer a better tradeoff in terms
    of computational cost and performance. Second, because BERT is used only as a
    feature extractor, there is no back-propagation and its internal parameters won’t
    be updated during the adaptation phase. In many cases, you get better accuracy
    in the downstream task if you fine-tune the BERT parameters, because by doing
    so, you are also teaching BERT to get better at the task at hand.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 从图形上看，这种方法与微调类似。毕竟，你正在将 BERT 的输出馈送到另一个 ML 模型中。然而，存在两个微妙但重要的区别：首先，因为你不再优化神经网络，第二个
    ML 模型不必是神经网络。一些机器学习任务（例如，无监督聚类）不是神经网络擅长解决的，特征提取在这些情况下提供了完美的解决方案。此外，你可以自由使用更“传统”的
    ML 算法，如 SVM（支持向量机）、决策树和梯度提升方法（如 GBDT 或梯度提升决策树），这些算法可能在计算成本和性能方面提供更好的折衷方案。其次，因为
    BERT 仅用作特征提取器，在适应阶段不会进行反向传播，其内部参数也不会更新。在许多情况下，如果微调 BERT 参数，你可以在下游任务中获得更高的准确性，因为这样做也会教导
    BERT 更好地解决手头的任务。
- en: Finally, note that these two are not the only ways to adapt BERT. Transfer learning
    is an actively researched topic, not just in NLP but also in many fields of artificial
    intelligence, and we have many other ways to use pretrained language models to
    make the best of them. If you are interested in learning more, I recommend checking
    out the tutorial given at NAACL 2019 (one of the top NLP conferences) titled “Transfer
    Learning in Natural Language Processing” ([http://mng.bz/o8qp](http://mng.bz/o8qp)).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请注意这两种方式不是适应 BERT 的唯一方式。迁移学习是一个正在积极研究的主题，不仅在自然语言处理领域，而且在人工智能的许多领域都是如此，我们有许多其他方法来使用预训练的语言模型以发挥其最佳作用。如果你对此感兴趣，我建议查看在
    NAACL 2019（顶级自然语言处理会议之一）上给出的教程，标题为“自然语言处理中的迁移学习”（[http://mng.bz/o8qp](http://mng.bz/o8qp)）。
- en: '9.3 Case study 1: Sentiment analysis with BERT'
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3 案例研究 1：使用 BERT 进行情感分析
- en: 'In this section, we will build a sentiment analyzer (again), but this time
    with BERT. Instead of AllenNLP, we will use the Transformers library developed
    by Hugging Face, which we used for making predictions with language models in
    the previous chapter. All the code here is accessible on a Google Colab notebook
    ([http://www.realworldnlpbook.com/ch9.html#sst](http://www.realworldnlpbook.com/ch9.html#sst)).
    The code snippets you see in this section all assume that you import related modules,
    classes, and methods as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将再次构建情感分析器，但这次我们将使用 BERT，而不是 AllenNLP，我们将使用由 Hugging Face 开发的 Transformers
    库，在上一章中使用该库进行语言模型预测。这里的所有代码都可以在 Google Colab 笔记本上访问（[http://www.realworldnlpbook.com/ch9.html#sst](http://www.realworldnlpbook.com/ch9.html#sst)）。你在本节中看到的代码片段都假定你按照以下方式导入了相关的模块、类和方法：
- en: '[PRE0]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In the Transformers library, you specify the pretrained models by their names.
    We’ll use the cased BERT-base model (''bert-base-cased'') throughout this section,
    so let’s define a constant first as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Transformers 库中，你可以通过他们的名称指定预训练模型。在本节中，我们将一直使用大写的 BERT-base 模型 ('bert-base-cased')，因此让我们首先定义一个常量，如下所示：
- en: '[PRE1]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The Transformers library also supports other pretrained BERT models, which you
    can see in their documentation ([https://huggingface.co/transformers/pretrained_
    models.html](https://huggingface.co/transformers/pretrained_models.html)). If
    you want to use other models, you can simply replace this constant with the name
    of the model you want to use, and the rest of the code works as-is in many cases
    (but not always).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers 库还支持其他预训练的 BERT 模型，你可以在他们的文档（[https://huggingface.co/transformers/pretrained_models.html](https://huggingface.co/transformers/pretrained_models.html)）中看到。如果你想使用其他模型，你可以简单地将这个变量替换为你想要使用的模型名称，代码的其余部分在许多情况下都可以原封不动地工作（但并非总是如此）。
- en: 9.3.1 Tokenizing input
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.1 将输入划分为单词
- en: The first step we took for building an NLP model is to build a dataset reader.
    Although AllenNLP (or more precisely speaking, the allennlp-modules package) is
    shipped with a dataset reader for the Stanford Sentiment Treebank, the dataset
    reader’s output is compatible only with AllenNLP. In this section, we are going
    to write a simple method that reads the dataset and returns a sequence of batched
    input instances.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建 NLP 模型的第一步是构建一个数据集读取器。虽然 AllenNLP（或更确切地说，allennlp-modules 包）附带了一个用于 Stanford
    情感树库的数据集读取器，但是该数据集读取器的输出仅与 AllenNLP 兼容。在本节中，我们将编写一个简单的方法来读取数据集并返回一系列批量输入实例。
- en: 'Tokenization is one of the most important steps in processing natural language
    input. As we saw in the previous chapter, tokenizers in the Transformers library
    can be initialized with the AutoTokenizer.from_pretrained() class method as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理自然语言输入时，分词是最重要的步骤之一。正如我们在上一章中看到的那样，Transformers 库中的分词器可以通过 AutoTokenizer.from_pretrained()
    类方法进行初始化，如下所示：
- en: '[PRE2]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Because different pretrained models use different tokenizers, it is important
    to initialize the one that matches the pretrained model you are going to use by
    supplying the same model name.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 因为不同的预训练模型使用不同的分词器，所以重要的是要通过提供相同的模型名称来初始化与你将要使用的预训练模型匹配的分词器。
- en: 'You can use the tokenizer to convert between a string and a list of token IDs
    back and forth, as shown next:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用分词器在字符串和令牌 ID 的列表之间进行转换，如下所示：
- en: '[PRE3]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Notice that BERT’s tokenizer added two special tokens—[CLS] and [SEP]—to your
    sentence. As discussed earlier, CLS is a special token used to extract the embedding
    for the entire input, whereas SEP is used to separate two sentences if your task
    involves making predictions on a pair of sentences. Because we are making predictions
    for single sentences here, there’s no need to pay much attention to this token.
    We’ll discuss sentence-pair classification tasks later in section 9.5.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 BERT 的分词器在你的句子中添加了两个特殊的标记——[CLS] 和 [SEP]。正如之前讨论的那样，CLS 是一个特殊的标记，用于提取整个输入的嵌入，而
    SEP 用于分隔两个句子，如果你的任务涉及对一对句子进行预测。因为我们在这里对单个句子进行预测，所以不需要过多关注这个标记。我们将在第 9.5 节讨论句子对分类任务。
- en: 'Deep neural networks rarely operate on single instances. They usually are trained
    on and make predictions for batches of instances for stability and performance
    reasons. The tokenizer also supports converting the given input in batches by
    invoking the __call__ method (i.e., just use the object as a method) as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络很少处理单个实例。它们通常通过训练并为实例的批次进行预测以保持稳定性和性能。分词器还支持通过调用 __call__ 方法（即，只需将对象用作方法）将给定输入转换为批次，如下所示：
- en: '[PRE4]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'When you run this, each string in the input list is tokenized and then resulting
    tensors are *padded* with 0s to have the same lengths. Padding here means adding
    0s at the end of each sequence so that individual instances have the same length
    and can be bundled as a single tensor, which is needed for more efficient computation
    (we’ll cover padding in more detail in chapter 10). The method call contains several
    other parameters that control the maximum length (max_length=10, meaning to pad
    everything to the length of 10), whether to pad to the maximum length, whether
    to truncate sequences that are too long, and the type of the returned tensors
    (return_tensors=''pt'', meaning it returns PyTorch tensors). The result of this
    tokenizer() call is a dictionary that contains the following three keys and three
    different types of packed tensors:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码时，输入列表中的每个字符串都将被标记化，然后生成的张量将用0进行*填充*，以使它们具有相同的长度。这里的填充意味着在每个序列的末尾添加0，以便单个实例具有相同的长度并可以捆绑为单个张量，这对于更有效的计算是必需的（我们将在第10章中更详细地讨论填充）。方法调用包含几个其他参数，用于控制最大长度（max_length=10，表示将所有内容填充到长度为10），是否填充到最大长度，是否截断过长的序列以及返回张量的类型（return_tensors='pt'，表示它返回PyTorch张量）。此tokenizer()调用的结果是一个包含以下三个键和三种不同类型的打包张量的字典：
- en: '[PRE5]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The input_ids tensor is a packed version of token IDs converted from the texts.
    Notice that each row is a vectorized token ID padded with 0s so that its length
    is always 10\. The token_type_ids tensor specifies which sentence each token comes
    from. As with the SEP special token earlier, this is relevant only if you are
    working with sentence pairs, which is why the tensor is simply filled with just
    0s. The attention_mask tensor specifies which tokens the Transformer should attend
    to. Because there are no tokens at the padded elements (0s in input_ids), the
    corresponding elements in attention_mask are all 0s, and attention to these tokens
    is simply ignored. Masking is a common technique often used in neural networks
    to ignore irrelevant elements in batched tensors like the ones shown here. Chapter
    10 covers masking in more detail.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: input_ids张量是从文本转换而来的标记ID的打包版本。请注意，每行都是一个矢量化的标记ID，用0进行了填充，以便其长度始终为10。token_type_ids张量指定每个标记来自哪个句子。与之前的SEP特殊标记一样，只有在处理句对时才相关，因此张量只是简单地填满了0。attention_mask张量指定Transformer应该关注哪些标记。由于在输入_ids中存在填充元素（填充为0），因此attention_mask中的相应元素都为0，并且对这些标记的关注将被简单地忽略。掩码是神经网络中经常使用的一种常见技术，通常用于忽略类似于这里所示的批量张量中的不相关元素。第10章将更详细地介绍掩码。
- en: 'As you see here, the Transformers library’s tokenizers do more than just tokenizing—they
    take a list of strings and create batched tensors for you, including the auxiliary
    tensors (token_type_ids and attention_mask). You just need to create lists of
    strings from your dataset and pass them to tokenizer()to create batches to pass
    on to the model. This logic for reading datasets is rather boring and a bit lengthy,
    so I packaged it in a method named read_dataset, which is not shown here. If you
    are interested, you can check the Google Colab notebook mentioned earlier. Using
    this method, you can read a dataset and convert it to a list of batches as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在这里看到的，Transformers库的标记器不仅仅是标记化 - 它们为您创建了一个字符串列表，并为您创建了批量张量，包括辅助张量（token_type_ids和attention_mask）。您只需从数据集创建字符串列表，并将它们传递给tokenizer()以创建传递给模型的批次。这种读取数据集的逻辑相当乏味且有点冗长，因此我将其打包在一个名为read_dataset的方法中，这里没有显示。如果您感兴趣，可以检查之前提到的Google
    Colab笔记本。使用此方法，您可以读取数据集并将其转换为批次列表，如下所示：
- en: '[PRE6]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 9.3.2 Building the model
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.2 构建模型
- en: In the next step, we’ll build the model to classify texts into their sentiment
    labels. The model we build here is nothing but a thin wrapper around BERT. All
    it does is pass the input through BERT, take out its embedding at CLS, pass it
    through a linear layer to convert to a set of scores (logits), and compute the
    loss.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们将构建模型，将文本分类到它们的情感标签中。我们在这里构建的模型只是BERT的一个薄包装器。它所做的就是将输入通过BERT传递，取出其在CLS处的嵌入，将其传递到线性层以转换为一组分数（logits），并计算损失。
- en: Note that we are building a PyTorch Module, not an AllenNLP Model, so make sure
    to inherit from nn.Module, although the structure of these two types of models
    are usually very similar (because AllenNLP’s Models inherit from PyTorch Modules).
    You need to implement __init__(), where you define and initialize submodules of
    the model, and forward(), where the main computation (“forward pass”) happens.
    The entire code snippet is shown next.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们正在构建一个PyTorch模块，而不是AllenNLP模型，因此请确保从nn.Module继承，尽管这两种类型的模型的结构通常非常相似（因为AllenNLP的模型从PyTorch模块继承）。您需要实现__init__()，在其中定义和初始化模型的子模块，以及forward()，其中进行主要计算（“前向传递”）。下面显示了整个代码片段。
- en: Listing 9.1 Sentiment analysis model with BERT
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.1 使用BERT的情感分析模型
- en: '[PRE7]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Initializes BERT
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 初始化BERT
- en: ❷ Defines a linear layer
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义一个线性层
- en: ❸ Applies BERT
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 应用BERT
- en: ❹ Applies the linear layer
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 应用线性层
- en: ❺ Computes the loss
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 计算损失
- en: The module first defines the BERT model (via the AutoModel.from_pretrained()
    class method), a linear layer (nn.Linear), and the loss function (nn.CrossEntropyLoss)
    in __init__(). Note that the module has no way of knowing the number of labels
    it needs to classify into, so we are passing it as a parameter (num_labels).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 该模块首先在__init__()中定义BERT模型（通过AutoModel.from_pretrained()类方法）、一个线性层（nn.Linear）和损失函数（nn.CrossEntropyLoss）。请注意，模块无法知道它需要分类到的标签数量，因此我们将其作为参数传递（num_labels）。
- en: In the forward() method, it first calls the BERT model. You can simply pass
    the three types of tensors (input_ids, attention_mask, and token_type_ids) to
    the model. The model returns a data structure that contains last_hidden_state
    and pooler_output among other things, where last_hidden_state is a sequence of
    hidden states of the last layer, whereas pooler_output is a pooled output, which
    is basically the embedding at CLS transformed with a linear layer. Because we
    are interested only in the pooled output that represents the entire input, we’ll
    pass the latter to the linear layer. Finally, the method computes the loss (if
    the label is supplied) and returns it, along with the logits, which are used for
    making predictions and measuring the accuracy.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在forward()方法中，它首先调用BERT模型。您可以简单地将三种类型的张量（input_ids、attention_mask和token_type_ids）传递给模型。模型返回一个包含last_hidden_state和pooler_output等内容的数据结构，其中last_hidden_state是最后一层的隐藏状态序列，而pooler_output是一个汇总输出，基本上是经过线性层转换的CLS的嵌入。因为我们只关心代表整个输入的池化输出，所以我们将后者传递给线性层。最后，该方法计算损失（如果提供了标签）并返回它，以及logits，用于进行预测和衡量准确性。
- en: 'Pay attention to the way we designed the method signature—it takes the three
    tensors we inspected earlier with their exact names. This lets us simply destruct
    a batch and pass it to the forward method, as shown here:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们设计方法签名的方式——它接受我们之前检查的三个张量，使用它们的确切名称。这样我们可以简单地解构一个批次并将其传递给forward方法，如下所示：
- en: '[PRE8]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Notice that the return value of the forward pass is a tuple of the loss and
    the logits. Now you are ready to train your model!
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，forward传递的返回值是损失和logits的元组。现在您已经准备好训练您的模型了！
- en: 9.3.3 Training the model
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.3 训练模型
- en: In the third and the final step of this case study, we will train and validate
    the model. Although AllenNLP took care of the training process in the previous
    chapters, in this section we’ll write our own training loop from scratch so we
    can better understand what it takes to train a model yourself. Note that you can
    also choose to use the library’s own Trainer class ([https://huggingface.co/transformers/main_classes/trainer.html](https://huggingface.co/transformers/main_classes/trainer.html)),
    which works similarly to AllenNLP’s Trainer, to run the training loop by specifying
    its parameters.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例研究的第三和最后一步中，我们将训练和验证模型。尽管在前几章中，AllenNLP已经处理了训练过程，但在本节中，我们将从头开始编写自己的训练循环，以便更好地理解自己训练模型所需的工作量。请注意，您也可以选择使用库自己的Trainer类([https://huggingface.co/transformers/main_classes/trainer.html](https://huggingface.co/transformers/main_classes/trainer.html))，该类的工作方式类似于AllenNLP的Trainer，通过指定其参数来运行训练循环。
- en: We covered the basics of training loops in section 2.5, but to recap, in modern
    machine learning, every training loop looks somewhat similar. If you write it
    in pseudocode, it would look like the one shown as follows.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第2.5节中介绍了训练循环的基础知识，但是为了回顾一下，现代机器学习中，每个训练循环看起来都有些相似。如果您以伪代码的形式编写它，它将类似于下面显示的内容。
- en: Listing 9.2 Pseudocode for the neural network training loop
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.2 神经网络训练循环的伪代码
- en: '[PRE9]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This training loop is almost identical to listing 2.2, except it operates on
    batches instead of single instances. The dataset yields a series of batches, which
    are then passed to the forward method of the model. The method returns the loss,
    which is then used to optimize the model. It is also common for the model to return
    the predictions so that the caller can use the result to compute some metrics,
    such as accuracy.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这个训练循环几乎与清单 2.2 相同，只是它操作的是批次而不是单个实例。数据集产生一系列批次，然后传递给模型的前向方法。该方法返回损失，然后用于优化模型。通常模型还会返回预测结果，以便调用者可以使用结果计算一些指标，如准确率。
- en: Before we move on to writing our own training loop, we need to note two things—it
    is customary to alternate between training and validation during each epoch. In
    the training phase, the model is optimized (the “magic constants” are changed)
    based on the loss function and the optimizer. The training data is used during
    this phase. In the validation phase, the model’s parameters are fixed, and its
    accuracy of prediction is measured against validation data. Although the loss
    is not used for optimization during validation, it is common to compute it to
    monitor how the loss changes during the course of the training, as we did in section
    6.3.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续编写自己的训练循环之前，我们需要注意两件事——在每个 epoch 中交替进行训练和验证是习惯的。在训练阶段，模型被优化（“魔法常数”被改变）基于损失函数和优化器。在这个阶段使用训练数据。在验证阶段，模型的参数是固定的，并且它的预测准确率是根据验证数据进行测量的。虽然在验证期间不使用损失进行优化，但通常会计算它以监视损失在训练过程中的变化，就像我们在第
    6.3 节中所做的那样。
- en: 'Another thing to note is that when training Transformer-based models such as
    BERT, we usually use *warm-up*, a process where the learning rate (how much to
    change the magic constants) is gradually increased for the first few thousand
    steps. A step here is just another name for one execution of backpropagation,
    which corresponds to the inner loop of listing 9.2\. This is useful for stabilizing
    training. We are not going into the mathematical details of warm-up and controlling
    the learning rate here—we just note that a learning rate scheduler is usually
    used for controlling the learning rate over the course of the training. With the
    Transformers library, you can define an optimizer (AdamW) and a learning controller
    as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要注意的是，当训练诸如 BERT 之类的 Transformer 模型时，我们通常使用 *热身*，即在前几千个步骤中逐渐增加学习率（改变“魔法常数”）。这里的步骤只是反向传播的另一个名称，对应于清单
    9.2 中的内部循环。这对于稳定训练是有用的。我们不会在这里讨论热身和控制学习率的数学细节——我们只是指出通常会使用学习率调度器来控制整个训练过程中的学习率。使用
    Transformers 库，你可以定义一个优化器（AdamW）和一个学习率控制器如下：
- en: '[PRE10]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The controller we are using here (get_cosine_schedule_with_warmup) increases
    the learning rate from zero to the maximum during the first 100 steps, then gradually
    decreases it afterward (based on the cosine function, which is where it got its
    name). If you plot how the learning rate changes over time, it’ll look like the
    graph in figure 9.9.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用的控制器（get_cosine_schedule_with_warmup）将学习率从零增加到最大值，在前100个步骤内，然后逐渐降低（基于余弦函数，这就是它得名的原因）。如果你绘制学习率随时间变化的图表，它会像图
    9.9 中的图表一样。
- en: '![CH09_F09_Hagiwara](../Images/CH09_F09_Hagiwara.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F09_Hagiwara](../Images/CH09_F09_Hagiwara.png)'
- en: Figure 9.9 With a cosine learning rate schedule with warm-up, the learning rate
    ramps up first, then declines following a cosine function.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.9 使用余弦学习率调度和热身，学习率首先上升，然后按照余弦函数下降。
- en: Now we are ready to train our BERT-based sentiment analyzer. The next listing
    shows our training loop.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备训练我们基于 BERT 的情感分析器。接下来的清单展示了我们的训练循环。
- en: Listing 9.3 Training loop for the BERT-based sentiment analyzer
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 9.3 基于 BERT 的情感分析器的训练循环
- en: '[PRE11]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Turns on the training mode
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 打开训练模式
- en: ❷ Moves the batch to GPU (if available)
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将批次移到 GPU（如果可用）
- en: ❸ Remember to reset the gradients (in PyTorch gradients accumulate).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 记得重置梯度（在 PyTorch 中梯度会累积）。
- en: ❹ Forward pass
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 前向传播
- en: ❺ Backpropagation
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 反向传播
- en: ❻ Computes the accuracy by counting the number of correct instances
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 通过计算正确实例的数量来计算准确率。
- en: When you train a model using PyTorch (and, consequently, AllenNLP and Transformers,
    two libraries that are built on top of it), remember to call model.train() to
    turn on the “training mode” of the model. This is important because some layers
    such as BatchNorm and dropout behave differently between training and evaluation
    (we’ll cover dropout in chapter 10). On the other hand, when you validate or test
    your model, be sure to call model.eval().
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用PyTorch训练模型时（因此，也是使用AllenNLP和Transformers两个构建在其上的库），请记得调用model.train()以打开模型的“训练模式”。这很重要，因为一些层（如BatchNorm和dropout）在训练和评估之间的行为不同（我们将在第10章中涵盖dropout）。另一方面，在验证或测试模型时，请务必调用model.eval()。
- en: 'The code in listing 9.3 does not show the validation phase, but the code for
    validation would look almost the same as that for training. When you validate/test
    your model, pay attention to the following:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.3中的代码没有显示验证阶段，但验证的代码几乎与训练的代码相同。在验证/测试模型时，请注意以下事项：
- en: As mentioned previously, make sure to call model.eval() before validating/testing
    your model.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如前所述，请确保在验证/测试模型之前调用model.eval()。
- en: Optimization calls (loss.backward(), optimizer.step(), and scheduler.step())
    are not necessary because you are not updating the model.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化调用（loss.backward()，optimizer.step()和scheduler.step()）是不必要的，因为您没有更新模型。
- en: Losses are still recorded and reported for monitoring. Be sure to wrap your
    forward pass call with with torch.no_grad()—this will disable gradient computation
    and save memory.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失仍然被记录和报告以进行监视。确保将您的前向传递调用包装在torch.no_grad()中——这将禁用梯度计算并节省内存。
- en: Accuracy is computed in exactly the same way (this is the point of validation!).
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精度的计算方式完全相同（这是验证的重点！）。
- en: 'When I ran this, I got the following output to stdout (with intermediate epochs
    omitted):'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 当我运行这个时，我得到了以下输出到标准输出（省略了中间时期）：
- en: '[PRE12]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The dev accuracy peaked around 0.483 at epoch 8 and didn’t improve after that.
    Compared to the result we got from LSTM (dev accuracy ~0.35, in chapter 2) and
    CNN (dev accuracy ~0.40, in chapter 7), this is the best result we have achieved
    on this dataset. We’ve done very little hyperparameter tuning, so it’s too early
    to conclude that BERT is the best model of the three we compared, but we at least
    know that it is a strong baseline to start from!
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 开发精度在第8个时期达到了约0.483的峰值，此后没有改善。与我们从LSTM（开发精度约为0.35，在第2章中）和CNN（开发精度约为0.40，在第7章中）得到的结果相比，这是我们在此数据集上取得的最佳结果。我们做了很少的超参数调整，所以现在就得出BERT是我们比较的三个模型中最好的模型的结论还为时过早，但至少我们知道它是一个强大的基准模型！
- en: 9.4 Other pretrained language models
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.4 其他预训练语言模型
- en: BERT is neither the first nor the last of popular pretrained language models
    (PLMs) commonly used in the NLP community nowadays. In this section, we’ll learn
    several other popular PLMs and how they are different from BERT. Most of these
    models are already implemented and publicly available from the Transformers library,
    so you can integrate them with your NLP application by changing just a couple
    of lines of your code.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: BERT既不是目前NLP社区中常用的预训练语言模型（PLMs）中的第一个，也不是最后一个。在本节中，我们将学习其他几个流行的PLMs以及它们与BERT的区别。这些模型中的大多数已经在Transformers库中实现并公开可用，因此您只需更改代码中的几行即可将它们与您的NLP应用集成。
- en: 9.4.1 ELMo
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.1 ELMo
- en: ELMo (Embeddings from Language Models), proposed[⁴](#pgfId-1111949) in early
    2018, is one of the earliest PLMs for deriving contextualized embeddings using
    unlabeled texts. Its core idea is simple—train an LSTM-based language model (similar
    to the one we trained back in chapter 5) and use its hidden states as additional
    “features” for downstream NLP tasks. Because the language model is trained to
    predict the next token given the previous context, the hidden states can encode
    the information needed to “understand the language.” ELMo does the same with another,
    backward LM and combines the embeddings from both directions so that it can also
    encode the information in both directions. See figure 9.10 for an illustration.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ELMo（来自语言模型的嵌入），于2018年初提出[⁴](#pgfId-1111949)，是最早用于从未标记文本中获取上下文嵌入的预训练语言模型之一。其核心思想很简单——训练一个基于LSTM的语言模型（类似于我们在第5章中训练的模型），并使用其隐藏状态作为下游自然语言处理任务的额外“特征”。因为语言模型被训练为在给定前文的情况下预测下一个标记，所以隐藏状态可以编码“理解语言”所需的信息。ELMo还使用另一个反向LM执行相同的操作，并结合来自两个方向的嵌入，以便它还可以编码双向信息。请参见图9.10进行说明。
- en: '![CH09_F10_Hagiwara](../Images/CH09_F10_Hagiwara.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F10_Hagiwara](../Images/CH09_F10_Hagiwara.png)'
- en: Figure 9.10 ELMo computes contextualized embeddings by combining forward and
    backward LSTMs.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.10 ELMo通过组合前向和后向LSTM计算上下文嵌入。
- en: After pretraining LMs in both directions, downstream NLP tasks can simply use
    the ELMo embeddings as features. Note that ELMo uses multilayer LSTM, so the features
    are the sum of hidden states taken from different layers, weighted in a task-specific
    way. The inventors of ELMo showed that adding these features improves the performance
    of a wide range of NLP tasks, including sentiment analysis, named entity recognition,
    and question answering. Although ELMo is not implemented in Hugging Face’s Transformers
    library, you can use it with AllenNLP fairly easily.[⁵](#pgfId-1111959)
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在两个方向上对LM进行预训练之后，下游NLP任务可以简单地使用ELMo嵌入作为特征。请注意，ELMo使用多层LSTM，因此特征是从不同层中取出的隐藏状态的总和，以任务特定的方式加权。ELMo的发明者们表明，添加这些特征可以提高各种NLP任务的性能，包括情感分析、命名实体识别和问答。虽然ELMo没有在Hugging
    Face的Transformers库中实现，但你可以在AllenNLP中相当容易地使用它。
- en: ELMo is a historically important PLM, although it is not often used in research
    or production anymore today—it predates BERT (and the advent of the Transformer)
    and there are other PLMs (including BERT) that outperform ELMo and are widely
    available today.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ELMo是一个历史上重要的PLM，尽管它今天在研究或生产中不再经常使用——它早于BERT（和Transformer的出现），而且有其他PLM（包括BERT）在今天广泛可用并且性能优于ELMo。
- en: 9.4.2 XLNet
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.2 XLNet
- en: 'XLNet, proposed in 2019, is an important successor of BERT and often referenced
    as one of the most powerful PLMs as of today. XLNet addresses two main issues
    of how BERT is trained: train-test skew and the independence of masks. The first
    issue has to do with how BERT is pretrained using the masked language model (MLM)
    objective. During training time, BERT is trained so that it can accurately predict
    masked tokens, whereas during prediction, it just sees the input sentence, which
    does not contain any masks. This means that there’s a discrepancy of information
    to which BERT is exposed to between training and testing, and that creates the
    train-test skew problem.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 2019年提出的XLNet是BERT的重要后继者，通常被引用为当今最强大的PLM之一。XLNet解决了BERT训练中的两个主要问题：训练-测试偏差和掩码的独立性。第一个问题与BERT如何使用掩码语言模型（MLM）目标进行预训练有关。在训练时，BERT被训练以便能够准确预测掩码标记，而在预测时，它只看到输入句子，其中不包含任何掩码。这意味着BERT在训练和测试之间暴露给的信息存在差异，从而产生了训练-测试偏差问题。
- en: 'The second issue has to do with how BERT makes predictions for masked tokens.
    If there is more than one [MASK] token in the input, BERT makes predictions for
    them in parallel. There doesn’t seem to be anything wrong with this approach at
    first glance—for example, if the input was “The Statue of [MASK] in New [MASK],”
    the model wouldn’t have difficulties answering this as “Liberty” and “York.” If
    the input was “The Statue of [MASK] in Washington, [MASK],” most of you (and probably
    a language model) would predict “Lincoln” and “DC.” However, what if the input
    was the following:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个问题与BERT如何对掩码标记进行预测有关。如果输入中有多个[MASK]标记，BERT会同时对它们进行预测。乍一看，这种方法似乎没有任何问题——例如，如果输入是“The
    Statue of [MASK] in New [MASK]”，模型不会有困难地回答“Liberty”和“York”。如果输入是“The Statue of
    [MASK] in Washington, [MASK]”，大多数人（也可能是语言模型）会预测“Lincoln”和“DC”。但是，如果输入是以下内容：
- en: The Statue of [MASK] in [MASK] [MASK]
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[MASK]雕像位于[MASK][MASK]中'
- en: Then there is no information to bias your prediction one way or the other. BERT
    won’t learn the fact that “The Statue of Liberty in Washington, DC” or “The Statue
    of Lincoln in New York” don’t make much sense during the training from this example,
    because these predictions are all made in parallel. This is a good example showing
    that you cannot simply make independent predictions on tokens and combine them
    to create a sentence that makes sense.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 然后没有信息偏向于你的预测。BERT不会从这个例子的训练中学习到“华盛顿特区的自由女神像”或“纽约的林肯像”这样的事实，因为这些预测都是并行进行的。这是一个很好的例子，表明你不能简单地对标记进行独立的预测，然后将它们组合起来创建一个有意义的句子。
- en: NOTE This issue is related to the multimodality of natural language, which means
    there are multiple modes in the joint probability distribution, and combinations
    of best decisions made independently do not necessarily lead to globally best
    decisions. Multimodality is a big challenge in natural language generation.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 这个问题与自然语言的多模态性有关，这意味着联合概率分布中存在多种模式，并且独立做出的最佳决策的组合并不一定导致全局最佳决策。多模态性是自然语言生成中的一个重大挑战。
- en: To address this issue, instead of making predictions in parallel, you can make
    predictions sequentially. In fact, this is exactly what typical language models
    do—generate tokens from left to right, one by one. However, here we have a sentence
    interspersed with masked tokens, and predictions depend not only on the tokens
    on the left (e.g., “The Statue of” in the previous example) but also on the right
    (“in”). XLNet solves this by generating missing tokens in a random order, as shown
    in figure 9.11\. For example, you can choose to generate “New” first, which gives
    a strong clue for the next words, “York” and “Liberty,” and so on. Note that prediction
    is still made based on all the tokens generated previously. If the model chose
    to generate “Washington” first, then the model would proceed to generate “DC”
    and “Lincoln” and would never mix up these two.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，你可以将预测顺序改为顺序预测，而不是并行预测。事实上，这正是典型语言模型所做的——逐个从左到右生成标记。然而，在这里，我们有一个插入了屏蔽标记的句子，并且预测不仅依赖于左边的标记（例如，前面示例中的“雕像的”），还依赖于右边的标记（“in”）。XLNet通过以随机顺序生成缺失的标记来解决这个问题，如图9.11所示。例如，您可以选择首先生成“New”，这为下一个单词“York”和“Liberty”提供了强有力的线索，依此类推。请注意，预测仍然基于先前生成的所有标记。如果模型选择首先生成“Washington”，那么模型将继续生成“DC”和“Lincoln”，而不会混淆这两个标记。
- en: '![CH09_F11_Hagiwara](../Images/CH09_F11_Hagiwara.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F11_Hagiwara](../Images/CH09_F11_Hagiwara.png)'
- en: Figure 9.11 XLNet generates tokens in an arbitrary order.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.11 XLNet以任意顺序生成标记。
- en: XLNet is already implemented in the Transformers library, and you can use the
    model with only a few lines of code change.[⁶](#pgfId-1111990)
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: XLNet已经在Transformers库中实现，您只需更改几行代码即可使用该模型。
- en: 9.4.3 RoBERTa
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.3 RoBERTa
- en: RoBERTa (from “robustly optimized BERT”)[⁷](#pgfId-1111996) is another important
    PLM that is commonly used in research and industry. RoBERTa revisits and modifies
    many training decisions of BERT, which makes it match or even exceed the performance
    of post-BERT PLMs, including XLNet, which we covered earlier. My personal impression
    is that RoBERTa is the second most referenced PLM after BERT as of this writing
    (mid-2020), and it shows robust performance in many downstream NLP tasks in English.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: RoBERTa（来自“robustly optimized BERT”）是另一个在研究和工业中常用的重要PLM。RoBERTa重新审视并修改了BERT的许多训练决策，使其达到甚至超过后BERT
    PLMs的性能，包括我们之前介绍的XLNet。截至本文撰写时（2020年中期），我个人的印象是，RoBERTa在BERT之后是被引用最多的第二个PLM，并且在许多英文下游NLP任务中表现出稳健的性能。
- en: RoBERTa makes several improvements over BERT, but the most important (and the
    most straightforward) is the amount of its training data. The developers of RoBERTa
    collected five English corpora of varying sizes and domains, which total over
    160 GB of text (versus 16 GB used for training BERT). Simply by using a lot more
    data for training, RoBERTa overperforms some of the other powerful PLMs, including
    XLNet, in downstream tasks after fine-tuning. The second modification has to do
    with the next-sentence prediction (NSP) objective we touched on in section 9.2.3,
    where BERT is pretrained to classify whether the second sentence is the “true”
    sentence that follows the first one in a corpus. The developers of RoBERTa found
    that, by removing NSP (and training with the MLM objective only), the performance
    of downstream tasks stays about the same or slightly improves. In addition to
    these, they also revisited the batch size and the way masking is done for MLM.
    Combined, the new pretrained language model achieved the state-of-the-art results
    on downstream tasks such as question answering and reading comprehension.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: RoBERTa在BERT的基础上进行了几项改进，但最重要（也是最直接）的是其训练数据量。RoBERTa的开发者收集了五个不同大小和领域的英文语料库，总计超过160
    GB的文本（而训练BERT仅使用了16 GB）。仅仅通过使用更多的数据进行训练，RoBERTa在微调后的下游任务中超越了一些其他强大的PLMs，包括XLNet。第二个修改涉及我们在第9.2.3节中提到的下一句预测（NSP）目标，在该目标中，BERT被预先训练以分类第二个句子是否是跟随语料库中第一个句子的“真实”句子。RoBERTa的开发者发现，通过移除NSP（仅使用MLM目标进行训练），下游任务的性能保持大致相同或略有提高。除此之外，他们还重新审视了批量大小以及MLM的屏蔽方式。综合起来，这个新的预训练语言模型在诸如问答和阅读理解等下游任务中取得了最先进的结果。
- en: Because RoBERTa uses the identical architecture to BERT and both are implemented
    in Transformers, switching to RoBERTa is extremely easy if your application already
    uses BERT.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 因为RoBERTa使用与BERT相同的架构，并且两者都在Transformers中实现，所以如果您的应用程序已经使用BERT，那么切换到RoBERTa将非常容易。
- en: NOTE Similar to BERT versus RoBERTa, the cross-lingual language model XLM (covered
    in section 8.4.4) has its “robustly optimized” sibling called XLM-R (short for
    XML-RoBERTa).[⁸](#pgfId-1112008) XLM-R pretrains on 100 languages and shows competitive
    performance on many cross-lingual NLP tasks.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 注意与BERT与RoBERTa类似，跨语言语言模型XLM（在第8.4.4节中介绍）有其“优化鲁棒性”的同类称为XLM-R（缩写为XML-RoBERTa）。[⁸](#pgfId-1112008)
    XLM-R对100种语言进行了预训练，并在许多跨语言NLP任务中表现出竞争力。
- en: 9.4.4 DistilBERT
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.4 DistilBERT
- en: Although pretrained models such as BERT and RoBERTa are powerful, they are computationally
    expensive, not just for pretraining but also for tuning and making predictions.
    For example, BERT-base (the regular-sized BERT) and BERT-large (the larger counterpart)
    have 110 million and 340 million parameters, respectively, and virtually every
    input has to go through this huge network to get predictions. If you were to fine-tune
    and make predictions with a BERT-based model (such as the one we built in section
    9.3), you’d most certainly need a GPU, which is not always available, depending
    on your computational environment. For example, if you’d like to run some real-time
    text analytics on a mobile phone, BERT wouldn’t be a great choice (and it might
    not even fit in the memory).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管诸如BERT和RoBERTa等预训练模型功能强大，但它们在计算上是昂贵的，不仅用于预训练，而且用于调整和进行预测。例如，BERT-base（常规大小的BERT）和BERT-large（较大的对应物）分别具有1.1亿和3.4亿个参数，几乎每个输入都必须通过这个巨大的网络进行预测。如果您要对基于BERT的模型（例如我们在第9.3节中构建的模型）进行微调和预测，那么您几乎肯定需要一个GPU，这并不总是可用的，这取决于您的计算环境。例如，如果您想在手机上运行一些实时文本分析，BERT将不是一个很好的选择（它甚至可能无法适应内存）。
- en: To reduce the computational requirement of modern large neural networks, *knowledge
    distillation* (or simply *distillation*) is often used. This is a machine learning
    technique where, given a large pretrained model (called the *teacher model*),
    a smaller model (called the *student model*) is trained to mimic the behavior
    of the larger model. See figure 9.12 for more details. The student model is trained
    with the masked language model (MLM) loss (same as BERT), as well as the cross-entropy
    loss between the teacher and the student. This pushes the student model to produce
    the probability distribution over predicted tokens that are as similar to the
    teacher as possible.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 为了降低现代大型神经网络的计算需求，通常使用*知识蒸馏*（或简称*蒸馏*）。这是一种机器学习技术，其中给定一个大型预训练模型（称为*教师模型*），会训练一个较小的模型（称为*学生模型*）来模仿较大模型的行为。有关更多详细信息，请参见图9.12。学生模型使用掩码语言模型（MLM）损失（与BERT相同），以及教师和学生之间的交叉熵损失。这将推动学生模型产生与教师尽可能相似的预测标记的概率分布。
- en: '![CH09_F12_Hagiwara](../Images/CH09_F12_Hagiwara.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F12_Hagiwara](../Images/CH09_F12_Hagiwara.png)'
- en: Figure 9.12 Knowledge distillation combines cross entropy and the masked LM
    objectives.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.12 知识蒸馏结合了交叉熵和掩码LM目标。
- en: Researchers at Hugging Face developed a distilled version of BERT called *DistilBERT*,[⁹](#pgfId-1112026)
    which is 40% smaller and 60% faster while retraining 97% of task performance compared
    to BERT. You can use DistilBERT by simply replacing the model name you pass to
    AutoModel.from_pretrained() for BERT (e.g., bert-base-cased) with distilled versions
    (e.g., distilbert-base-cased), while keeping the rest of your code the same.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face的研究人员开发了BERT的精简版本称为*DistilBERT*，[⁹](#pgfId-1112026)它的大小缩小了40%，速度提高了60%，同时与BERT相比重新训练了97%的任务性能。您只需将传递给AutoModel.from_pretrained()的模型名称（例如，bert-base-cased）替换为精炼版本（例如，distilbert-base-cased），同时保持其余代码不变即可使用DistilBERT。
- en: 9.4.5 ALBERT
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.5 ALBERT
- en: Another pretrained language model that addresses the computational complexity
    problem of BERT is ALBERT,[^(10)](#pgfId-1112036) short for “A Lite BERT.” Instead
    of resorting to knowledge distillation, ALBERT makes a few changes to its model
    and the training procedure.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个解决了BERT计算复杂性问题的预训练语言模型是ALBERT，[^(10)](#pgfId-1112036)简称“轻量BERT”。与采用知识蒸馏不同，ALBERT对其模型和训练过程进行了一些修改。
- en: One design change ALBERT makes to its model is how it handles word embeddings.
    In most deep NLP models, word embeddings are represented by and stored in a big
    lookup table that contains one word embedding vector per word. This way of managing
    embeddings is usually fine for smaller models such as RNNs and CNNs. However,
    for Transformer-based models such as BERT, the dimensionality (i.e., the length)
    of input needs to match that of the hidden states, which is usually as big as
    768 dimensions. This means that the model needs to maintain a big lookup table
    of size V times 768, where V is the number of unique vocabulary items. Because
    in many NLP models V is also large (e.g., 30,000), the resulting lookup table
    becomes huge and takes up a lot of memory and computation.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ALBERT 对其模型进行的一个设计变化是它如何处理词嵌入。在大多数深度 NLP 模型中，词嵌入由一个大的查找表表示和存储，该表包含每个词的一个词嵌入向量。这种管理嵌入的方式通常适用于较小的模型，如
    RNN 和 CNN。然而，对于基于 Transformer 的模型，如 BERT，输入的维度（即长度）需要与隐藏状态的维度匹配，通常为 768 维。这意味着模型需要维护一个大小为
    V 乘以 768 的大查找表，其中 V 是唯一词汇项的数量。因为在许多 NLP 模型中 V 也很大（例如，30,000），所以产生的查找表变得巨大，并且占用了大量的内存和计算。
- en: ALBERT addresses this issue by decomposing word embedding lookup into two stages,
    as shown in figure 9.13\. The first stage is similar to how word embeddings are
    retrieved from a mapping table, except that the output dimensionality of word
    embedding vectors is smaller (say, 128 dimensions). In the next stage, these shorter
    vectors are expanded using a linear layer so that they match the desired input
    dimensionality of the model (say, 768). This is similar to how we expanded word
    embeddings with the Skip-gram model (section 3.4). Thanks to this decomposition,
    ALBERT needs to store only two smaller lookup tables (V × 128, plus 128 × 768)
    instead of one big look-up table (V × 768).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ALBERT 通过将词嵌入查找分解为两个阶段来解决这个问题，如图 9.13 所示。第一阶段类似于从映射表中检索词嵌入，只是词嵌入向量的输出维度较小（比如，128
    维）。在下一个阶段，使用线性层扩展这些较短的向量，使它们与模型的所需输入维度相匹配（比如，768）。这类似于我们如何使用 Skip-gram 模型扩展词嵌入（第
    3.4 节）。由于这种分解，ALBERT 只需要存储两个较小的查找表（V × 128，加上 128 × 768），而不是一个大的查找表（V × 768）。
- en: '![CH09_F13_Hagiwara](../Images/CH09_F13_Hagiwara.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F13_Hagiwara](../Images/CH09_F13_Hagiwara.png)'
- en: Figure 9.13 ALBERT (right) decomposes word embeddings into two smaller projections.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.13 ALBERT（右）将词嵌入分解为两个较小的投影。
- en: Another design change that ALBERT implements is parameter sharing between Transformer
    layers. Transformer models use a series of self-attention layers to transform
    the input vector. The way these layers transform the input is usually different
    from layer to layer—the first layer may transform the input one way (e.g., capture
    basic phrases), and the second one may do so another way (e.g., capture some syntactic
    information). However, this means that the model needs to retain all the necessary
    parameters (projections for keys, queries, and values) per each layer, which is
    expensive and takes up a lot of memory. Instead, ALBERT’s layers all share the
    same set of parameters, meaning that the model applies the same transformation
    repeatedly to the input. These parameters are adjusted in such a way that the
    series of transformations are effective for predicting the objective, even though
    they are identical.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ALBERT 实施的另一个设计变化是 Transformer 层之间的参数共享。Transformer 模型使用一系列自注意力层来转换输入向量。这些层将输入转换的方式通常各不相同——第一层可能以一种方式转换输入（例如，捕获基本短语），而第二层可能以另一种方式进行转换（例如，捕获一些句法信息）。然而，这意味着模型需要针对每一层保留所有必要的参数（用于键、查询和值的投影），这是昂贵的，并且占用了大量内存。相反，ALBERT
    的所有层都共享相同的参数集，这意味着模型重复应用相同的转换到输入上。这些参数被调整为这样一种方式，即尽管它们相同，一系列转换对于预测目标是有效的。
- en: Finally, ALBERT uses a training objective called *sentence-order prediction*
    (SOP) for pretraining, instead of the next-sentence prediction (NSP) adopted by
    BERT. As mentioned earlier, the developers of RoBERTa and some others found out
    that the NSP objective is basically useless and decided to eliminate it. ALBERT
    replaces NSP with sentence-order prediction (SOP), a task where the model is asked
    to predict the ordering of two consecutive segments of text. For example:[^(11)](#pgfId-1112056)
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，ALBERT 使用一种称为 *句子顺序预测*（SOP）的训练目标进行预训练，而不是 BERT 采用的下一个句子预测（NSP）。正如前面提到的，RoBERTa
    的开发人员和其他一些人发现 NSP 目标基本无用，并决定将其排除。ALBERT 用句子顺序预测 (SOP) 取代了 NSP，在这个任务中，模型被要求预测两个连续文本段落的顺序。例如：[^(11)](#pgfId-1112056)
- en: (A) She and her boyfriend decided to go for a long walk. (B) After walking for
    over a mile, something happened.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (A) 她和她的男朋友决定去散步。 (B) 走了一英里后，发生了一些事情。
- en: (C) However, one of the teachers around the area helped me get up. (D) At first,
    no one was willing to help me up.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (C) 然而，周边区域的一位老师帮助了我站起来。(D) 起初，没有人愿意帮我站起来。
- en: In the first example, you can tell that A happens before B. In the second, the
    order is flipped, and D should come before C. This is an easy feat for humans,
    but a difficult task for machines—an NLP model needs to learn to ignore superficial
    topical signals (e.g., “go for a long walk,” “walking for over a mile,” “helped
    me get up,” and “help me up”) and focus on discourse-level coherence. Training
    with this objective makes the model more robust and effective for deeper natural
    language understanding tasks.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个示例中，你可以知道 A 发生在 B 之前。在第二个示例中，顺序被颠倒，D 应该在 C 之前。这对人类来说很容易，但对机器来说是一个困难的任务——自然语言处理模型需要学会忽略表面主题信号（例如，“去散步”，“步行超过一英里”，“帮我站起来”），并专注于话语级连贯性。使用这种目标进行训练使得模型更加强大且可用于更深入的自然语言理解任务。
- en: As a result, ALBERT was able to scale up its training and outperform BERT-large
    with fewer parameters. As with DistilBERT, the model architecture of ALBERT is
    almost identical to that of BERT, and you can use it by simply supplying the model
    name when you call AutoModel.from_pretrained() (e.g., albert-base-v1).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，ALBERT 能够通过更少的参数扩大其训练并超越 BERT-large。与 DistilBERT 一样，ALBERT 的模型架构与 BERT 几乎完全相同，您只需在调用
    AutoModel.from_pretrained() 时提供模型名称即可使用它 （例如，albert-base-v1）。
- en: '9.5 Case study 2: Natural language inference with BERT'
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.5 案例研究 2：BERT 自然语言推理
- en: In this final section of this chapter, we will build an NLP model for natural
    language inference, a task where the system predicts logical relationship between
    sentences. We’ll use AllenNLP for building the model while demonstrating how to
    integrate BERT (or any other Transformer-based pretrained models) into your pipeline.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后一部分，我们将构建自然语言推理的 NLP 模型，这是一个预测句子之间逻辑关系的任务。我们将使用 AllenNLP 构建模型，同时演示如何将
    BERT（或任何其他基于 Transformer 的预训练模型）集成到你的管道中。
- en: 9.5.1 What is natural language inference?
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.5.1 什么是自然语言推理？
- en: '*Natural* *language inference* (or NLI, for short) is the task of determining
    the logical relationship between a pair of sentences. Specifically, given one
    sentence (called *premise*) and another sentence (called *hypothesis*), you need
    to determine whether the hypothesis is logically inferred from the premise. This
    is easier to see in the following examples.[^(12)](#pgfId-1112076)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '*自然语言推理*（简称 NLI）是确定一对句子之间逻辑关系的任务。具体而言，给定一个句子（称为*前提*）和另一个句子（称为*假设*），你需要确定假设是否从前提中逻辑推演出来。在以下示例中，这更容易理解。[^(12)](#pgfId-1112076)'
- en: '| Premise | Hypothesis | Label |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 前提 | 假设 | 标签 |'
- en: '| A man inspects the uniform of a figure in some East Asian country. | The
    man is sleeping. | contradiction |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 一名男子查看一个身穿东亚某国制服的人物。| 男子正在睡觉。| 矛盾 |'
- en: '| An older and younger man smiling. | Two men are smiling and laughing at the
    cats playing on the floor. | neutral |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 一名年长和一名年轻男子微笑着。| 两名男子对在地板上玩耍的猫笑着。| 中性 |'
- en: '| A soccer game with multiple males playing. | Some men are playing a sport.
    | entailment |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 进行多人踢足球的比赛。| 一些男人正在运动。| 蕴涵 |'
- en: In the first example, the hypothesis (“The man is sleeping”) clearly contradicts
    the premise (“A man inspects . . .”) because someone cannot be inspecting something
    while asleep. In the second example, you cannot tell if the hypothesis contradicts
    or is entailed by the premise (especially the “laughing at the cats” part), which
    makes the relationship “neutral.” In the third example, you can logically infer
    the hypothesis from the premise—in other words, the hypothesis is entailed by
    the premise.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个例子中，假设（“这个人正在睡觉”）显然与前提（“一个人正在检查…”）相矛盾，因为一个人不能在睡觉时检查某事。在第二个例子中，你无法确定假设是否与前提矛盾或被前提蕴含（特别是“笑猫”的部分），这使得关系是“中性”的。在第三个例子中，你可以从前提中逻辑推断出假设——换句话说，假设被前提蕴含。
- en: As you can guess, NLI can be tricky even for humans. The task requires not only
    lexical knowledge (e.g., plural of “man” is “men,” soccer is one type of sport)
    but also some “common sense” (e.g., you cannot inspect while sleeping). NLI is
    one of the most typical natural language understanding (NLU) tasks. How can you
    build an NLP model to solve this task?
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你猜到的那样，即使对于人类来说，NLI 也可能是棘手的。这项任务不仅需要词汇知识（例如，“人”的复数是“人们”，足球是一种运动），还需要一些“常识”（例如，你不能在睡觉时检查）。NLI
    是最典型的自然语言理解（NLU）任务之一。你如何构建一个 NLP 模型来解决这个任务？
- en: Fortunately, NLI is a well-studied field in NLP. The most popular dataset for
    NLI, the Standard Natural Language Inference (SNLI) corpus ([https://nlp.stanford.edu/projects/snli/](https://nlp.stanford.edu/projects/snli/)),
    has been used in numerous NLP studies as a benchmark. In what follows, we’ll build
    a neural NLI model with AllenNLP and learn how to use BERT for this particular
    task.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，NLI 是 NLP 中一个经过深入研究的领域。NLI 最流行的数据集是标准自然语言推理（SNLI）语料库（[https://nlp.stanford.edu/projects/snli/](https://nlp.stanford.edu/projects/snli/)），已被大量用作
    NLP 研究的基准。接下来，我们将使用 AllenNLP 构建一个神经 NLI 模型，并学习如何为这个特定任务使用 BERT。
- en: 'Before moving on, make sure that you have AllenNLP (we use version 2.5.0) and
    the AllenNLP model’s modules installed. You can install them by running the following
    code:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，请确保你已经安装了 AllenNLP（我们使用的是版本 2.5.0）和 AllenNLP 模型的模块。你可以通过运行以下代码来安装它们：
- en: '[PRE13]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This also installs the Transformers library as a dependency.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这也将 Transformers 库安装为一个依赖项。
- en: 9.5.2 Using BERT for sentence-pair classification
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.5.2 使用 BERT 进行句对分类
- en: 'Before we start building the model, notice that every input to the NLI task
    consists of two pieces: a premise and a hypothesis. Most of the NLP tasks we covered
    in this book had just one part—usually a single sentence—as the input to the model.
    How can we build a model that makes predictions for instances that are pairs of
    sentences?'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始构建模型之前，请注意，NLI 任务的每个输入都由两部分组成：前提和假设。本书涵盖的大多数 NLP 任务仅有一个部分——通常是模型的输入的一个部分——通常是单个句子。我们如何构建一个可以对句子对进行预测的模型？
- en: We have multiple ways to deal with multipart input for NLP models. We can encode
    each sentence with an encoder and apply some mathematical operations (e.g., concatenation,
    subtractions) to the result to derive an embedding for the pair (which, by the
    way, is the basic idea of Siamese networks[^(13)](#pgfId-1112121)). Researchers
    have also come up with more complex neural network models with attention (such
    as BiDAF[^(14)](#pgfId-1112124)).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有多种方法来处理 NLP 模型的多部分输入。我们可以使用编码器对每个句子进行编码，并对结果应用一些数学操作（例如，串联、减法），以得到一对句子的嵌入（顺便说一句，这是孪生网络的基本思想[^(13)](#pgfId-1112121)）。研究人员还提出了更复杂的具有注意力的神经网络模型，例如
    BiDAF[^(14)](#pgfId-1112124)。
- en: However, there’s inherently nothing preventing BERT from accepting more than
    one sentence. Because the Transformer accepts a sequence of any tokens, you can
    simply concatenate the two sentences and feed them to the model. If you are worried
    about the model mixing up the two sentences, you can separate them with a special
    token, [SEP]. You can also add different values to each sentence as an extra signal
    to the model. BERT uses these two techniques to solve sentence-pair classification
    tasks such as NLI with little modification to the model.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，从本质上讲，没有什么阻止 BERT 接受多个句子。因为 Transformer 接受任何令牌序列，你可以简单地将两个句子串联起来并将它们输入模型。如果你担心模型混淆了两个句子，你可以用一个特殊的令牌，[SEP]，将它们分开。你还可以为每个句子添加不同的值作为模型的额外信号。BERT
    使用这两种技术对模型进行了少量修改，以解决句对分类任务，如 NLI。
- en: The rest of the pipeline proceeds in a similar way to other classification tasks.
    A special token [CLS] is appended to every sentence pair, from which the final
    embedding of the input is extracted. Finally, you can use a classification head
    to convert the embedding into a set of values (called *logits*) corresponding
    to the classes. This is illustrated in figure 9.14.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 流水线的其余部分与其他分类任务类似。特殊令牌[CLS]被附加到每个句子对，从中提取输入的最终嵌入。最后，您可以使用分类头将嵌入转换为一组与类相对应的值（称为*logits*）。这在图9.14中有所说明。
- en: '![CH09_F14_Hagiwara](../Images/CH09_F14_Hagiwara.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F14_Hagiwara](../Images/CH09_F14_Hagiwara.png)'
- en: Figure 9.14 Feeding and classifying a pair of sentences with BERT
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.14 使用BERT对一对句子进行馈送和分类
- en: 'In practice, concatenating and inserting special tokens are both taken care
    of by SnliReader, an AllenNLP dataset reader specifically built for dealing with
    the SNLI dataset. You can initialize the dataset and observe how it turns the
    data into AllenNLP instances with the following snippet:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，连接和插入特殊令牌都是由SnliReader处理的，这是专门用于处理SNLI数据集的AllenNLP数据集读取器。您可以初始化数据集并观察它如何将数据转换为AllenNLP实例，代码如下：
- en: '[PRE14]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The dataset reader takes a JSONL (JSON line) file from the Stanford NLI corpus
    and turns it into a series of AllenNLP instances. We specify the URL of a dataset
    file that I put online (S3). Note that you need to specify add_special_tokens=False
    when initializing the tokenizer. This sounds a little bit strange—aren’t the special
    tokens the very things we need to add here? This is necessary because the dataset
    reader (SnliReader), not the tokenizer, will take care of the special tokens.
    If you were to use the Transformer library only (without AllenNLP), you wouldn’t
    need this option.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集读取器从斯坦福NLI语料库中获取一个JSONL（JSON行）文件，并将其转换为一系列AllenNLP实例。我们指定了一个我在线上（S3）放置的数据集文件的URL。请注意，在初始化分词器时，您需要指定add_special_tokens=False。这听起来有点奇怪——难道我们不是应该在这里添加特殊令牌吗？这是必需的，因为数据集读取器（SnliReader）而不是分词器会处理特殊令牌。如果您仅使用Transformer库（而不是AllenNLP），则不需要此选项。
- en: 'The previous snippet produces the following dump of generated instances:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码片段生成了以下生成的实例的转储：
- en: '[PRE15]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Notice that every sentence is tokenized, and the sentences are concatenated
    and separated by [SEP] special tokens. Each instance also has a label field containing
    the gold label.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，每个句子都经过了标记化，并且句子被连接并由[SEP]特殊令牌分隔。每个实例还有一个包含金标签的标签字段。
- en: 'NOTE You may have noticed some weird characters in the tokenized results, such
    as ##bracing and ##i. These are the results of byte-pair encoding (BPE), a tokenization
    algorithm for splitting words into what’s called *subword units*. We’ll cover
    BPE in detail in chapter 10.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：您可能已经注意到令牌化结果中出现了一些奇怪的字符，例如##bracing和##i。这些是字节对编码（BPE）的结果，这是一种将单词分割为所谓的*子词单元*的标记化算法。我们将在第10章中详细介绍BPE。
- en: 9.5.3 Using Transformers with AllenNLP
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.5.3 使用AllenNLP与Transformers
- en: Now we are ready to build our model with AllenNLP. The good news is you don’t
    need to write any Python code to build an NLI model thanks to AllenNLP’s built-in
    modules—all you need to do is write a Jsonnet config file (as we did in chapter
    4). AllenNLP also integrates Hugging Face’s Transformer library seamlessly, so
    you usually need to make little change, even if you want to integrate Transformer-based
    models such as BERT into your existing models.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备使用AllenNLP构建我们的模型。好消息是，由于AllenNLP的内置模块，您不需要编写任何Python代码来构建NLI模型——您只需要编写一个Jsonnet配置文件（就像我们在第4章中所做的那样）。AllenNLP还无缝集成了Hugging
    Face的Transformer库，因此即使您想要将基于Transformer的模型（如BERT）集成到现有模型中，通常也只需要进行很少的更改。
- en: 'You need to make changes to the following four components when integrating
    BERT into your model and pipeline:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 当将BERT集成到您的模型和流水线中时，您需要对以下四个组件进行更改：
- en: '*Tokenizer*—As you did in section 9.3 earlier, you need to use a tokenizer
    that matches the pretrained model you are using.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Tokenizer*—就像您在之前的9.3节中所做的那样，您需要使用与您正在使用的预训练模型相匹配的分词器。'
- en: '*Token indexer*—Token indexers turn tokens into integer indices. Because pretrained
    models come with their own predefined vocabularies, it is important that you use
    a matching token indexer.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Token indexer*—Token indexer将令牌转换为整数索引。由于预训练模型带有其自己预定义的词汇表，因此很重要您使用匹配的令牌索引器。'
- en: '*Token embedder*—Token embedders turn tokens into embeddings. This is where
    the main computation of BERT happens.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Token embedder*—Token embedder将令牌转换为嵌入。这是BERT的主要计算发生的地方。'
- en: '*Seq2Vec encoder*—The raw output from BERT is a sequence of embeddings. You
    need a Seq2Vec encoder to turn it into a single embedding vector.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Seq2Vec编码器*—BERT的原始输出是一系列嵌入。您需要一个Seq2Vec编码器将其转换为单个嵌入向量。'
- en: Don’t worry if this sounds intimidating—in most cases, all you need to do is
    remember to initialize the right modules with the name of the model you want.
    I’ll walk you through these steps next.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这听起来令人生畏，不要担心—在大多数情况下，你只需要记住使用所需模型的名称来初始化正确的模块。接下来我会引导你完成这些步骤。
- en: 'First, let’s define the dataset we use for reading and converting the SNLI
    dataset. We already did this with Python code earlier, but here we will write
    the corresponding initialization in Jsonnet. First, let’s define the model name
    we’ll use throughout the pipeline using the following code. One of the cool features
    of Jsonnet over vanilla JSON is you can define and use variables:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们定义我们用于读取和转换SNLI数据集的数据集。我们之前已经用Python代码做过这个了，但在这里我们将在Jsonnet中编写相应的初始化。首先，让我们使用以下代码定义我们将在整个流水线中使用的模型名称。Jsonnet相对于普通JSON的一个很酷的功能是你可以定义和使用变量：
- en: '[PRE16]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The first section of the config file where the dataset is initialized looks
    like the following:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 配置文件中初始化数据集的第一部分看起来像以下内容：
- en: '[PRE17]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'At the top level, this is initializing a dataset reader specified by the type
    snli, which is the SnliReader we experimented with previously. The dataset reader
    takes two parameters—tokenizer and token_indexers. For the tokenizer, we initialize
    a PretrainedTransformerTokenizer (type: pretrained_transformer) with a model name.
    Again, this is the tokenizer we initialized and used earlier in the Python code.
    Notice how the Python code and the Jsonnet config file correspond to each other
    nicely. Most of AllenNLP modules are designed in such a way that there’s nice
    correspondence between these two, as shown in the following table.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶层，这是初始化一个由类型snli指定的数据集读取器，它是我们之前尝试过的SnliReader。数据集读取器需要两个参数—tokenizer和token_indexers。对于tokenizer，我们使用一个PretrainedTransformerTokenizer（类型：pretrained_transformer）并提供一个模型名称。同样，这是我们之前在Python代码中初始化和使用的分词器。请注意Python代码和Jsonnet配置文件之间的良好对应关系。大多数AllenNLP模块都设计得非常好，使得这两者之间有着很好的对应关系，如下表所示。
- en: '| Python code | Jsonnet config |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| Python代码 | Jsonnet配置 |'
- en: '| tokenizer = PretrainedTransformerTokenizer(model_name=BERT_MODEL,add_special_tokens=False)
    | "tokenizer": {"type": "pretrained_transformer","model_name": bert_model,"add_special_tokens":
    false} |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| tokenizer = PretrainedTransformerTokenizer(model_name=BERT_MODEL,add_special_tokens=False)
    | "tokenizer": {"type": "pretrained_transformer","model_name": bert_model,"add_special_tokens":
    false} |'
- en: 'The section for initializing a token indexer may look a bit confusing. It is
    initializing a PretrainedTransformerIndexer (type: pretrained_transformer) with
    a model name. The indexer will store the indexed result to a section named bert
    (the key corresponding to the token indexer). Fortunately, this code is a boilerplate
    that changes little from model to model, and chances are you can simply copy and
    paste this section when you work on a new Transformer-based model.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化令牌索引器部分可能看起来有点混乱。它正在使用模型名称初始化一个PretrainedTransformerIndexer（类型：pretrained_transformer）。索引器将把索引结果存储到名为bert的部分（对应于令牌索引器的键）。幸运的是，这段代码是一个样板，从一个模型到另一个模型几乎没有变化，很可能当你在一个新的基于Transformer的模型上工作时，你可以简单地复制并粘贴这一部分。
- en: 'As for the training/validation data, we can use the ones in this book’s S3
    repository, shown here:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 至于训练/验证数据，我们可以使用本书的S3存储库中的数据，如下所示：
- en: '[PRE18]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now we are ready to move on to defining our model:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备开始定义我们的模型：
- en: '[PRE19]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'At the top level, this section is defining a BasicClassifier model (type: basic_
    classifier). It is a generic text classification model that embeds the input,
    encodes it with a Seq2Vec encoder, and classifies it with a classification head
    (with a softmax layer). You can “plug in” embedders and encoders of your choice
    as the subcomponents of the model. For example, you can embed the tokens via word
    embeddings and encode the sequence with an RNN (this is what we did in chapter
    4). Alternatively, you can encode the sequence with a CNN, as we did in chapter
    7\. This is where the design of AllenNLP excels—the generic model specifies only
    *what* (e.g., a TextFieldEmbedder and a Seq2VecEncoder) but not exactly *how*
    (e.g., word embeddings, RNNs, BERT). You can use any submodules for embedding/encoding
    input, as long as those submodules conform to the specified interfaces (i.e.,
    they are subclasses of the required classes).'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶层，此部分定义了一个BasicClassifier模型（类型：basic_classifier）。它是一个通用的文本分类模型，它嵌入输入，使用Seq2Vec编码器对其进行编码，并使用分类头进行分类（带有softmax层）。您可以将您选择的嵌入器和编码器作为模型的子组件“插入”。例如，您可以通过单词嵌入嵌入标记，并使用RNN对序列进行编码（这是我们在第4章中所做的）。或者，您可以使用CNN对序列进行编码，就像我们在第7章中所做的那样。这就是AllenNLP设计的优点所在——通用模型仅指定了*什么*（例如，一个TextFieldEmbedder和一个Seq2VecEncoder），但不是确切的*如何*（例如，单词嵌入、RNN、BERT）。您可以使用任何嵌入/编码输入的子模块，只要这些子模块符合指定的接口（即，它们是所需类的子类）。
- en: 'In this case study, we will use BERT to embed the input sequence first. This
    is achieved by a special token embedder, PretrainedTransformerEmbedder (type:
    pretrained_transformer), which takes the result of a Transformer tokenizer, puts
    it through a pretrained BERT model, and produces the embedded input. You need
    to pass this embedder as the value for the bert key (the one you specified for
    token_indexers earlier) of the token_embedders parameter.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例研究中，我们将首先使用BERT对输入序列进行嵌入。这是通过一个特殊的标记嵌入器，PretrainedTransformerEmbedder（类型：pretrained_transformer）实现的，它接受Transformer分词器的结果，经过预训练的BERT模型，并产生嵌入的输入。您需要将此嵌入器作为token_embedders参数的bert键的值传递（您之前为token_indexers指定的那个）。
- en: 'The raw output from BERT, however, is a sequence of embeddings. Because we
    are interested in classifying the given pair of sentences, we need to extract
    the embeddings for the entire sequence, which can be done by taking out the embeddings
    corresponding to the CLS special token. AllenNLP implements a type of Seq2VecEncoder
    called BertPooler (type: bert_pooler) that does exactly this.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，从BERT中得到的原始输出是一系列嵌入。因为我们感兴趣的是对给定的句子对进行分类，我们需要提取整个序列的嵌入，这可以通过提取与CLS特殊标记对应的嵌入来完成。AllenNLP实现了一种称为BertPooler（类型：bert_pooler）的Seq2VecEncoder类型，它正是这样做的。
- en: After embedding and encoding the input, the basic classifier model takes care
    of the rest—the embeddings go through a linear layer that converts them into a
    set of logits, and the entire network is trained with a cross-entropy loss, just
    like other classification models. The entire config file is shown here.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在嵌入和编码输入之后，基本分类器模型处理剩下的事情——嵌入经过一个线性层，将它们转换为一组logits，并且整个网络使用交叉熵损失进行训练，就像其他分类模型一样。整个配置文件如下所示。
- en: Listing 9.4 Config file for training an NLI model with BERT
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.4 使用BERT训练NLI模型的配置文件
- en: '[PRE20]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'It’s OK if you are not familiar with what’s going on in the data_loader and
    trainer sections. We’ll discuss these topics (batching, padding, optimizing, hyperparameter
    tuning) in chapter 10\. After saving this config file under examples/nli/snli_
    transformers.jsonnnet, you can start the training process by running the following
    code:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不熟悉数据加载器和训练器部分正在发生的事情也没关系。我们将在第10章讨论这些主题（批处理、填充、优化、超参数调整）。在将此配置文件保存在examples/nli/snli_transformers.jsonnet后，您可以通过运行以下代码开始训练过程：
- en: '[PRE21]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This will run for a while (even on a fast GPU such as Nvidia V100) and produce
    a large amount of log messages on stdout. The following is a snippet of log messages
    I got after four epochs:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这将运行一段时间（即使在诸如Nvidia V100这样的快速GPU上也是如此），并在stdout上产生大量的日志消息。以下是我在四个时期后得到的日志消息的片段：
- en: '[PRE22]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Pay attention to the validation accuracy (0.908). This looks very good considering
    that this is a three-class classification and the random baseline would be just
    0.3\. In comparison, when I replaced BERT with an LSTM-based RNN, the best validation
    accuracy I got was around ~0.68\. We need to run experiments more carefully to
    make a fair comparison between different models, but this result seems to suggest
    that BERT is a powerful model for solving natural language understanding problems.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Transfer learning is a machine learning concept where a model learned for one
    task is applied to another by transferring knowledge between them. It is an underlying
    concept for many modern, powerful, pretrained models.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BERT is a Transformer encoder pretrained with masked language modeling and next-sentence
    prediction objectives to produce contextualized embeddings, a series of word embeddings
    that take context into account.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ELMo, XLNet, RoBERTa, DistilBERT, and ALBERT are other popular pretrained models
    commonly used in modern deep NLP.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can build BERT-based NLP applications by using Hugging Face’s Transformers
    library directly, or by using AllenNLP, which integrates the Transformers library
    seamlessly.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ^(1.)Ramponi and Plank, “Neural Unsupervised Domain Adaptation in NLP—A Survey,”
    (2020). [https://arxiv.org/abs/2006.00632](https://arxiv.org/abs/2006.00632).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '^(2.)Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova, “BERT:
    Pre-Training of Deep Bidirectional Transformers for Language Understanding,” (2018).
    [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '^(3.)Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher, “Learned
    in Translation: Contextualized Word Vectors,” in NIPS 2017.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: ^(4.)Peters et al., “Deep Contextualized Word Representations,” (2018). [https://arxiv.org/abs/1802.05365](https://arxiv.org/abs/1802.05365).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '^(5.)See here for the detailed documentation on how to use ELMo with AllenNLP:
    [https://allennlp.org/elmo](https://allennlp.org/elmo).'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: ^(6.) See [https://huggingface.co/transformers/model_doc/xlnet.html](https://huggingface.co/transformers/model_doc/xlnet.html)
    for the documentation.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '^(7.)Liu et al., “RoBERTa: A Robustly Optimized BERT Pretraining Approach,”
    (2019). [https://arxiv.org/abs/1907.11692](https://arxiv.org/abs/1907.11692).'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: ^(8.)Conneau et al., “Unsupervised Cross-lingual Representation Learning at
    Scale,” (2019). [https://arxiv.org/abs/1911.02116](https://arxiv.org/abs/1911.02116).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '^(9.)Sanh et al., “DistilBERT, a distilled version of BERT: smaller, faster,
    cheaper and lighter,” (2019). [https://arxiv.org/abs/1910.01108](https://arxiv.org/abs/1910.01108).'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '^(10.)Lan et al., “ALBERT: A Lite BERT for Self-Supervised Learning of Language
    Representations,” (2020). [https://arxiv.org/abs/1909.11942](https://arxiv.org/abs/1909.11942).'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '^(11.)These examples are taken from ROCStories: [https://cs.rochester.edu/nlp/rocstories/](https://cs.rochester.edu/nlp/rocstories/).'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: ^(12.)These examples are taken from [http://nlpprogress.com/english/natural_language_inference.html](http://nlpprogress.com/english/natural_language_inference.html).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ^(12.)这些示例来自于[http://nlpprogress.com/english/natural_language_inference.html](http://nlpprogress.com/english/natural_language_inference.html).
- en: '^(13.)Reimers and Gurevych, “Sentence-BERT: Sentence Embeddings Using Siamese
    BERT-Networks,” (2019). [https://arxiv.org/abs/1908.10084](https://arxiv.org/abs/1908.10084).'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ^(13.)Reimers和Gurevych，“使用Siamese BERT-Network的句子嵌入：Sentence-BERT”，(2019)。[https://arxiv.org/abs/1908.10084](https://arxiv.org/abs/1908.10084).
- en: ^(14.)Seo et al., “Bidirectional Attention Flow for Machine Comprehension,”
    (2018). [https://arxiv.org/abs/1611 .01603](https://arxiv.org/abs/1611.01603).
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ^(14.)Seo等人，“面向机器理解的双向注意力流”，(2018)。[https://arxiv.org/abs/1611.01603](https://arxiv.org/abs/1611.01603).
