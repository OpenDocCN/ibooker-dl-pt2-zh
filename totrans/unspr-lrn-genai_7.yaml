- en: 7 Unsupervised learning for text data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: “Everybody smiles in the same language – George Carlin”
  prefs: []
  type: TYPE_NORMAL
- en: Our world has so many languages. These languages are the most common medium
    of communication to express our thoughts and emotions to each other. This ability
    to express our thoughts in words is unique to humans. These words are a source
    of information to us. These words can be written into text. In this chapter, we
    are going to explore the analysis we can do on text data. Text data falls under
    unstructured data and carries a lot of useful information and hence is a useful
    source of insights for the business. We use natural language processing or NLP
    to analyse the text data.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, to analyse text data, we have to make the data analysis ready.
    Or in very simple terms, since our algorithms and processors can only understand
    numbers, we have to represent the text data in numbers or *vectors*. We are exploring
    all such steps in this chapter. Text data holds the key to quite a few important
    use cases like sentiment analysis, document categorization, language translation
    etc. to name a few. We will cover the use cases using a case study and develop
    Python solution on the same.
  prefs: []
  type: TYPE_NORMAL
- en: The chapter starts with defining text data, sources of text data and various
    use cases of text data. We will then move to the steps and processes to clean
    and handle the text data. We will cover the concepts of NLP, mathematical foundation
    and methods to represent text data into vectors. We will create Python codes for
    the use cases. And at the end, we are sharing case study on text data. In this
    book, we are providing that level of mathematical support without going in too
    much depth – an optimal mix of practical world and mathematical concepts.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this seventh chapter of the book, we are going to cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Text data and various use cases of Text data analysis
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Challenges we face with text data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pre-processing of text data and data cleaning
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Methods to represent text data in vectors
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sentiment analysis using Python – a case study
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Text clustering using Python
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Welcome to the seventh chapter and all the very best!
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Technical toolkit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will continue to use the same version of Python and Jupyter notebook as we
    have used so far. The codes and datasets used in this chapter have been checked-in
    at this location.
  prefs: []
  type: TYPE_NORMAL
- en: You would need to install a few Python libraries in this chapter which are –
    XXXX. Along with this we will need numpy and pandas. Using libraries, we can implement
    the algorithms very quickly. Otherwise, coding these algorithms is quite a time-consuming
    and painstaking task.
  prefs: []
  type: TYPE_NORMAL
- en: Here we are dealing with text data, perhaps you will find it very interesting
    and useful.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started with Chapter 7 of the book!
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Text data is everywhere
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Recall in the very first chapter of the book, we explored structured and unstructured
    datasets. Unstructured data can be text, audio, image or a video. The examples
    of unstructured data and their respective sources are given in (Figure 7-1) below,
    where we explain the primary types of unstructured data: text, images, audio and
    video along with their examples. The focus for this chapter is on text data.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7-1 Unstructured data can be text, images, audio, video. We are dealing
    with text data in this chapter
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![A screenshot of a cell phone Description automatically generated](images/07_img_0001.png)'
  prefs: []
  type: TYPE_IMG
- en: Language is indeed a gift to the mankind. We indulge in speaking, messaging,
    writing, listening to convey our thoughts. And this is done using text data which
    is generated using blogs and social media posts, tweets, comments, stories, reviews,
    chats and comments to name a few. Text data is generally much more direct, and
    emotionally expressive. It is imperative that business unlocks the potential of
    text data and derives insights from it. We can understand our customers better,
    explore the business processes and gauge the quality of services offered. We generate
    text data is the form of news, Facebook comments, tweets, Instagram posts, customer
    reviews, feedback, blogs, articles, literature, stories etc. This data represents
    a wide range of emotions and expressions.
  prefs: []
  type: TYPE_NORMAL
- en: Have you even reviewed a product or a service on Amazon? You award stars to
    a product; at the same time, you can also input free text. Go to Amazon and look
    at some of the reviews. You might find some reviews have good amount of text as
    the feedback. This text is useful for the product/service providers to enhance
    their offerings. Also, you might have participated in a few surveys which ask
    you to share your feedback. Moreover, with the advent of Alexa, Siri, Cortona
    etc. the voice command is acting as an interface between humans and machines –
    which is again a rich source of data. Even the customer calls we make to a call
    centre can be source of text data. These calls can be recorded and using speech-to-text
    conversion, we can generate huge amount of text data. Massive dataset, right!
  prefs: []
  type: TYPE_NORMAL
- en: We are discussing some of the important use cases for text data in the following
    section.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 Use cases of text data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Text data is indeed super useful. It is really a rich source of insights for
    the business. There are some of the use cases which are of much interest, which
    we are listing below. The list is not exhaustive. At the same time, not all the
    use cases given implement unsupervised learning. Some of the use cases require
    supervised learning too. Nevertheless, for your knowledge we are sharing both
    types of use cases – based on supervised learning and unsupervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: '**Sentiment analysis:** You might have participated in surveys or given your
    feedback of products/surveys. These survey generate tons of text data for us.
    That text data can be analyzed and we can determine whether the sentiment in the
    review is positive or negative. In simple words, sentiment analysis is gauging
    what is the positiveness or negativity in the text data. And hence, what is the
    sentiment about a product or service in the minds of the customers. We can use
    both supervised and unsupervised learning for sentiment analysis.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**News categorization or document categorization:** Look at the Google News
    webpage, you will find that each news item has been categorized to sports, politics,
    science, business or any other category. Incoming news will be classified based
    on the content of the news which is the actual text. Similarly, imagine we have
    some documents with us and we might want to segregate them based on their categories
    of based on the domain of study. For example, medical, economics, history, arts,
    physics etc. Such a use case will save a lot of time and man power indeed.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Language Translation:** Translation of text from one language to another
    is a very interesting use case. Using natural language processing we can translate
    between languages. Language translation is very tricky as different languages
    have different grammatical rules. Generally, deep learning based solutions are
    the best fit for language translation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Spam filtering:** email spam filter can be composed using NLP and supervised
    machine learning. A supervised learning algorithm can analyze incoming mail parameters
    and can give a prediction if that email belongs to a spam folder or not. The prediction
    can be based on the various parameters like sender email-id, subject line, body
    of the mail, attachments, time of mail etc. Generally, supervised learning algorithms
    are used here.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Part of speech tagging** or POS tagging is one of the popular use cases.
    It means that we are able to distinguish the noun, adjectives, verbs, adverbs
    etc. in a sentence. **Named-entity recognition** or NER is also one of the famous
    applications of NLP. It involves identifying a person, place, organization, time,
    number in a sentence. For example, John lives in London and works for Google.
    NER can generate understanding like [John][Person] lives in [London][Location]
    and works for [Google][organization].'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sentence generation, captioning the images, speech-to-text or text-to-speech
    tasks, handwriting recognition are a few other significant and popular use cases.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The use cases listed above are not exhaustive. There are tons of other use cases
    which can be implemented using NLP. NLP is a very popular research field too.
    We are sharing some significant papers at the end of the chapter for your perusal.
  prefs: []
  type: TYPE_NORMAL
- en: Whilst, text data is of much importance, at the same time is quite a difficult
    dataset to analyze. To be noted is, our computers and processors understand only
    numbers. So, the text still needs to be represented as numbers so that we can
    perform mathematical and statistical calculations on them. But before diving into
    the preparation of text data, we will cover some of the challenges we face while
    working on text dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 7.4 Challenges with text data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Text is perhaps the most difficult data to work with. There are a large number
    of permutations to express the same thought. For example, if I may ask, “Hey buddy,
    what is your age?” and “Hey buddy, may I know how old are you?” mean the same,
    right! The answer to both the questions is same, and it is quite easy for humans
    to decipher. But can be an equally daunting task for a machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most common challenges we face are:'
  prefs: []
  type: TYPE_NORMAL
- en: Text data can be complex to handle. There can be a lot of junk characters like
    $^%*& present in the text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With the advent of modern communications, we have started to use short forms
    of words like “u” can be used for “you”, “brb” for “be right back” and so on.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Language is changing, unbounding and ever evolving. It changes every day and
    new words are added to the language.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you do a simple Google search, you will find that quite a few words are added
    to the dictionary each year.
  prefs: []
  type: TYPE_NORMAL
- en: The world has close to 6500 languages, and each and every one carries their
    uniqueness. Each and every one complete our world. For example, Arabic, Chinese,
    English, French, German, Hindi , Italian, Japanese, Spanish etc. Each language
    follows its own rules and grammar which are unique in usage and pattern. Even
    the writing can be different - some are written left to right; some might be right
    to left or may be vertically! The same emotion, might take lesser or a greater
    number of words in different languages.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The meaning of a word is dependent on the context. A word can be an adjective
    and can be a noun too depending on the context. Look at these examples below:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: “This book is a must read” and “Please book a room for me”.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: “Tommy” can be a name but when used as “Tommy Hilfiger” its usage is completely
    changed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: “Apple” is a fruit while “Apple” is a company producing Macintosh, iPhones etc.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: “April” is a month and can be a name too.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Look at one more example - “Mark travelled from the UK to France and is working
    with John over there. He misses his friends”. The humans can easily understand
    that “he” in the second sentence is Mark and not John, which might not be that
    simple for a machine.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There can be many synonyms for the same word, like “good” can be replaced by
    positive, wonderful, superb, exceptional in different scenarios. Or, words like
    “studying”, “studying”, “studies”, “studies” are related to the same root word
    “study”.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: And the size of text data can be daunting too. Managing a text dataset, storing
    it, cleaning it and refreshing it is a herculean task in itself.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Like any other machine learning project, text analytics follow the principles
    of machine learning albeit the precise process is slightly different. Recall in
    chapter 1, we examined the process of a machine learning project as shown in Figure
    7-2\. You are advised to refresh the process from Chapter 1.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7-2 Overall steps in data science project are same for text data too.
    The pre-processing of text data is very different from the structured dataset.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![A screenshot of a cell phone Description automatically generated](images/07_img_0002.png)'
  prefs: []
  type: TYPE_IMG
- en: Defining the business problem, data collection and monitoring etc. remain the
    same. The major difference is in processing of the text, which involves data cleaning,
    creation of features, representation of text data etc. We are covering it now.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/tgt.png)'
  prefs:
  - PREF_H5
  type: TYPE_IMG
- en: POP QUIZ – answer these question to check your understanding.. Answers at the
    end of the book
  prefs: []
  type: TYPE_NORMAL
- en: (1)      Note the three most impactful use cases for the text data.
  prefs: []
  type: TYPE_NORMAL
- en: (2)      Why is working on text data so tedious?
  prefs: []
  type: TYPE_NORMAL
- en: 7.5 Preprocessing of the text data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Text data, like any other data source can be messy and noisy. We clean some
    of it in the data discovery phase and a lot of it in the pre-processing phase.
    At the same time, we have to extract the features from our dataset. This cleaning
    process is sometimes and can be implemented on most of the text datasets. Some
    text datasets might require a customized approach. We will start with cleaning
    the raw text data.
  prefs: []
  type: TYPE_NORMAL
- en: 7.5.1 Data cleaning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There is no second thought about the importance of data quality. The cleaner
    the text data is, the better the analysis will be. At the same time, the pre-processing
    is not a straight-forward task. It is complex and time-consuming task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Text data is to be cleaned as it contains a lot of junk characters, irrelevant
    words, noise and punctuations, URLs etc. The primary ways of cleaning the text
    data are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stop words removal:** out of all the words that are used in any language,
    there are some words which are most common. Stop words are the most common words
    in a vocabulary which carry less importance than the key words. For example, “is”,
    “an”, “the”, “a”, “be”, “has”, “had”, “it” etc. Once we remove the stop words
    from the text, the dimensions of the data are reduced and hence complexity of
    the solution is reduced.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At the same time, it is imperative that we understand the context very well
    while removing the stop words. For example, if we ask a question “is it raining?”.
    Then the answer “it is” is a complete answer in itself.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To remove the stop words, we can define a customized list of stop words and
    remove them. Else there are standard libraries are to remove the stop words.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: When we are working with solutions where contextual information is important,
    we do not remove stop words.
  prefs: []
  type: TYPE_NORMAL
- en: '**Frequency based removal of words:** Sometimes, you might wish to remove the
    words which are most common in your text or very unique. The process is to get
    the frequency of the words in the text and then set a threshold of frequency.
    We can remove the most common ones. Or maybe you might wish to remove the ones
    which have occurred only once/twice in the entire dataset. Based on the requirement,
    you will decide.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Library based cleaning** is done when we wish to clean the data using pre-defined
    and customized library. We can create a repository of words which we do not want
    in our text and can iteratively remove from the text data. This approach allows
    us flexibility to implement the cleaning of our own choice.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Junk or unwanted characters:** A text data particularly tweets, comments
    etc. might contain a lot of URLs, hashtags, numbers, punctuations, social media
    mentions, special characters etc. We might need to clean them from the text. At
    the same time, we have to be careful as some words which are not important for
    one domain might be quite required for a different domain. If data has been scraped
    from websites or HTML/XML sources, we need to get rid of all the HTML entities,
    punctuations, non-alphabets and so on.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Always keep business context in mind while cleaning the text data.
  prefs: []
  type: TYPE_NORMAL
- en: As we know that a lot of new type of expressions have entered the language.
    For example, lol, hahahaha, brb, rofl etc. These expressions have to be converted
    to their original meanings. Even emojis like :-), ;-) etc. have to be converted
    to their original meanings.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data encoding:** There are a number of data encodings available like ISO/IEC,
    UTF-8 etc. Generally, UTF-8 is the most popular one. But it is not a hard and
    fast rule to always use UTF-8 only.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Lexicon normalization:** Depending on the context and usage, the same word
    might get represented in different manners. During lexicon normalization we clean
    such ambiguities. The basic idea is to reduce the word to its root form. Hence,
    words which are derived from each other can be mapped to the central word provided
    they have the same core meaning.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Look at Figure 7-2 wherein we have shown that the same word “eat”, has been
    used in various forms. The root word is “eat” but these different forms are so
    many different representations for eat.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7-3 Ate, eaten, eats, eating all have the same root word – eat. Stemming
    and lemmatization can be used to get the root word.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/07_img_0003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We wish to map all of these words like eating, eaten etc. to their central
    word “eat”, as they have the same core meaning. There are two primary methods
    to work on this:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stemming**: *Stemming* is a basic rule-based approach of mapping a word to
    its core word. It removes “es”, “ing”, “ly”, “ed” etc. from the end of the word.
    For example, studies will become studi and studying will become study. As visible
    being a rule-based approach, the output spellings might not always be accurate.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Lemmatization**: is an organized approach which reduces words to their dictionary
    form. *Lemma* of a word is its dictionary or canonical form. For example, eats,
    eating, eaten etc. all have the same root word eat. Lemmatization provides better
    results than stemming but it takes more time then stemming.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These are only some of the methods to clean the text data. These techniques
    will help to a large extent. But still business acumen is required to further
    make sense to the dataset. We will clean the text data using these approaches
    by developing Python solution.
  prefs: []
  type: TYPE_NORMAL
- en: Once the data is cleaned, we have to start representation of data so that it
    can be processed by machine learning algorithms – which is our next topic.
  prefs: []
  type: TYPE_NORMAL
- en: 7.5.2 Extracting features from the text dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Text data, like any other data source can be messy and noisy. We explored the
    concepts and techniques to clean it in the last section. Now we have cleaned the
    data and it is ready to be used. The next step is to represent this data in a
    format which can be understood by our algorithms. As we know that our algorithms
    can only understand numbers. Text data in its purest form cannot be understood
    by algorithms. So, everything needs to be converted to numbers.
  prefs: []
  type: TYPE_NORMAL
- en: A very simple technique can be to simply perform *one-hot encoding* on our words
    and represent them in a matrix.
  prefs: []
  type: TYPE_NORMAL
- en: We have covered one hot encoding in the previous chapters of the book
  prefs: []
  type: TYPE_NORMAL
- en: If we describe the steps, the words can be first converted to lowercase and
    then sorted in an alphabetical order. And then a numeric label can be assigned
    to them. And finally, words are converted to binary vectors. Let us understand
    using an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the text is “It is raining heavily”. We will use the steps below:'
  prefs: []
  type: TYPE_NORMAL
- en: Lowercase the words so the output will be “it is raining heavily”
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will now arrange them in alphabetical order. The result is – heavily, is,
    it, raining.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can now assign place values to each word as heavily:0, is:1, it:2, raining:3.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we can transform them to binary vectors as shown below
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Though this approach is quite intuitive and simple to comprehend, it is pragmatically
    not possible due to the massive size of the corpus and the vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: Corpus refers to a collection of texts. It is Latin for body. It can be a body
    of written words or spoken words, which will be used to perform a linguistic analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, handling massive data size with so many dimensions will be computationally
    very expensive. The resulting matrix thus created will be very sparse too. Hence,
    we look at other means and ways to represent our text data.
  prefs: []
  type: TYPE_NORMAL
- en: There are better alternatives available to one-hot encoding. These techniques
    focus on the frequency of the word or the context in which the word is being used.
    This scientific method of text representation is much more accurate, robust and
    explanatory. It generates better results too. There are multiple such techniques
    like tf-idf, bag-of-words approach etc. We are discussing a few of these techniques
    in the next sections. But we will examine an important concept of tokenization
    first!
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Tokenization is simply breaking a text or a set of text into individual tokens.
    It is the building block of NLP. Look at the example in Figure 7-3, where we have
    created individual tokens for each of the word of the sentence. Tokenization is
    an important step as it allows us to assign unique identifiers or tokens to each
    of the words. Once we have allocated each word a specific token, the analysis
    become less complex.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7-3 Tokenization can be used to break a sentence into different tokens
    of words.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/07_img_0004.png)'
  prefs: []
  type: TYPE_IMG
- en: Tokens are usually used on individual words, but it is not always necessary.
    We are allowed to tokenize a word or the sub-words or characters in a word. In
    the case of sub-words, the same sentence can have sub-word tokens as rain-ing.
  prefs: []
  type: TYPE_NORMAL
- en: If we wish to perform tokenization at a character level, it can be r-a-i-n-i-n-g.
    In fact, in the one-hot encoding approach discussed in the last section as a first
    step, tokenization was done on the words.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization is the building blocks for Natural Language Processing solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have obtained the tokens, then the tokens can be used to prepare a vocabulary.
    Vocabulary is the set of unique tokens in the corpus.
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple libraries for tokenization. *Regexp* tokenization uses the
    given patterns arguments to match the tokens or separators between the tokens.
    *Whitespace* tokenization uses by treating any sequence of whitespace characters
    as a separator. Then we have *blankline* which use sequence of blank lines as
    a separator. And *wordpunct* tokenizes by matching sequence of alphabetic characters
    and sequence of non-alphabetic and non-whitespace characters. We will perform
    tokenization when we create Python solutions for our text data.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will explore more methods to represent text data. The first such method
    is Bag of Words.
  prefs: []
  type: TYPE_NORMAL
- en: Bag of words approach
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As the name suggests, all the words in the corpus are used. In bag of words
    approach, or BOW, the text data is tokenized for each word in the corpus and then
    the respective frequency of each token is calculated. During this process, we
    disregard the grammar, or the order or the context of the word. We simply focus
    on the simplicity. Hence, we will represent each text (sentence or a document)
    as a *bag of its own words*.
  prefs: []
  type: TYPE_NORMAL
- en: In the BOW approach for the entire document, we define the vocabulary of the
    corpus as all of the unique words present in the corpus. Please note we use all
    the unique words in the corpus. If we want, we can also set a threshold i.e.,
    the upper and lower limit for the frequency of the words to be selected. Once
    we have got the unique words, then each of the sentence can be represented by
    a vector of the same dimension as of the base vocabulary vector. This vector representation
    contains the frequency of each word of the sentence in the vocabulary. It might
    sound complicated to understand, but it is actually a straightforward approach.
  prefs: []
  type: TYPE_NORMAL
- en: Let us understand this approach with an example. Let’s say that we have two
    sentences – It is raining heavily and We should eat fruits.
  prefs: []
  type: TYPE_NORMAL
- en: To represent these two sentences, we will calculate the frequency of each of
    the word in these sentences as shown in Figure 7-4.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7-4 The frequency of each word has been calculated. In this example,
    we have two sentences.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/07_img_0005.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, if we assume that only these two words represent the entire vocabulary,
    we can represent the first sentence as shown in Figure 7-5\. Note that the table
    contains all the words, but the words which are not present in the sentence have
    received value as 0.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7-5 The first sentence if represented for all the words in the vocabulary,
    we are assuming that in the vocabulary only two sentences are present
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/07_img_0006.png)'
  prefs: []
  type: TYPE_IMG
- en: In this example, we examined how BOW approach has been used to represent a sentence
    as a vector. But BOW approach has not considered the order of the words or the
    context. It focuses only on the frequency of the word. Hence, it is a very fast
    approach to represent the data and computationally less expensive as compared
    to its peers. Since it is frequency based, it is commonly used for document classifications.
  prefs: []
  type: TYPE_NORMAL
- en: But, due to its pure frequency-based calculation and representation, the solution
    accuracy can take a hit. In language, the context of the word plays a significant
    role. As we have seen earlier, apple is both a fruit as well as a well-known brand
    and organization. And that is why we have other advanced methods which consider
    more parameters than frequency alone. One of such methods is tf-idf or term frequency-inverse
    document frequency, which we are studying next.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/tgt.png)'
  prefs:
  - PREF_H5
  type: TYPE_IMG
- en: POP QUIZ – answer these question to check your understanding.. Answers at the
    end of the book
  prefs: []
  type: TYPE_NORMAL
- en: (1)      Explain tokenization in simple language as if you are explaining to
    a person who does not know NLP.
  prefs: []
  type: TYPE_NORMAL
- en: (2)      Bag of words approach the context of the words and not frequency alone.
    True or False.
  prefs: []
  type: TYPE_NORMAL
- en: (3)      Lemmatization is less rigorous approach then stemming. True or False.
  prefs: []
  type: TYPE_NORMAL
- en: tf-idf (Term frequency and inverse document frequency)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We studied Bag of words approach in the last section. In the BOW approach, we
    gave importance to the frequency of a word only. The idea is that the words which
    have higher frequency might not offer meaningful information as compared to words
    which are rare but carry more importance. For example, if we have a collection
    of medical documents, and we wish to compare two words “disease” and “diabetes”.
    Since the corpus consists of medical documents, the word disease is bound to be
    more frequent whilst the word “diabetes” will be less frequent but more important
    to identify the documents which deal with diabetes. The approach tf-idf allow
    us to resolve this issue and extract information on the more important words.
  prefs: []
  type: TYPE_NORMAL
- en: 'In term-frequency and inverse-document-frequency (tf-idf), we consider the
    relative importance of the word. TF-idf means term frequency and idf means inverse
    document frequency. We can define tf-idf as:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Term frequency (t** is the count of a term in the entire document. For example,
    the count of the word “a” in the document “D”.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Inverse document frequency (id** is the log of the ratio of total documents
    (N) in the entire corpus and number of documents(df) which contain the word “a”.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So, the tf-idf formula will give us the relative importance of a word in the
    entire corpus. The mathematical formula is the multiplication of tf and idf and
    is given by
  prefs: []
  type: TYPE_NORMAL
- en: w[i,j] = tf[i,j] * log (N/df[i]) (Equation 7-1)
  prefs: []
  type: TYPE_NORMAL
- en: 'where N: total number of documents in the corpus'
  prefs: []
  type: TYPE_NORMAL
- en: tf[i,j] is the frequency of the word in the document
  prefs: []
  type: TYPE_NORMAL
- en: df[i] is the number of documents in the corpus which contain that word.
  prefs: []
  type: TYPE_NORMAL
- en: The concept might sound complex to comprehend. Let’s understand this with an
    example.
  prefs: []
  type: TYPE_NORMAL
- en: Consider we have a collection of 1 million sports journals. These sports journals
    contain have many articles of various lengths. We also assume that all the articles
    are in English language only. So, let’s say, in these documents, we want to calculate
    tf-idf value for the word “ground” and “backhand”.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s assume that there is a document of 100 words having “ground” word five
    times and backhand only twice. So tf for ground is 5/100 = 0.05 and for backhand
    is 2/100 = 0.02.
  prefs: []
  type: TYPE_NORMAL
- en: We understand that the word “ground” is quite a common word in sports, while
    the word “backhand” will have lesser number of usage. Now, we assume that “ground”
    appears in 100,000 documents out of 1 million documents while “backhand” appears
    only in 10\. So, idf for “ground” is log (1,000,000/100,000) = log (10) = 1\.
    For “backhand” it will be log (1,000,000/10) = log (100,000) = 5.
  prefs: []
  type: TYPE_NORMAL
- en: To get the final values for “ground” we will multiply tf and idf = 0.05 x 1
    = 0.05.
  prefs: []
  type: TYPE_NORMAL
- en: To get the final values for “backhand” we will multiply tf and idf = 0.02 x
    5 = 0.1.
  prefs: []
  type: TYPE_NORMAL
- en: We can observe that the relative importance of “backhand” is more than the relative
    importance of the word “ground”. This is the advantage of tf-idf over frequency-based
    BOW approach. But tf-idf takes more time to compute as compared to BOW since all
    the tf and idf have to be calculated. Nevertheless, tf-idf offer a better and
    more mature solution as compared to BOW approach.
  prefs: []
  type: TYPE_NORMAL
- en: We will now cover language models in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Language models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: So far, we have studied the bag of words approach and tf-idf. Now we are focusing
    on language models.
  prefs: []
  type: TYPE_NORMAL
- en: Language models assign probabilities to the sequence of words. N-grams are the
    simplest in language models. We know that to analyze the text data they have to
    be converted to feature vectors. N-gram models create the feature vectors so that
    text can be represented in a format which can be analyzed further.
  prefs: []
  type: TYPE_NORMAL
- en: n-gram is a probabilistic language model. In a n-gram model we calculate the
    probability of the N^(th) word given the sequence of (N-1) words. If we go more
    specific, an n-gram model will predict the next word x[i] based on the words x[i-(n-1)],
    x[i-(n-2)]…x[i-1]. If we wish to use the probability terms, we can represent as
    conditional probability of x[i] given the previous words which can be represented
    as P(x[i] | x[i-(n-1)], x[i-(n-2)]…x[i-1]). The probability is calculated by using
    the relative frequency of the sequence occurring in the text corpus.
  prefs: []
  type: TYPE_NORMAL
- en: If the items are words, n-grams may be referred to as *shingles*.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s study this using an example.
  prefs: []
  type: TYPE_NORMAL
- en: Consider we have a sentence It is raining heavily. We have shown the respective
    representations using different values of n. You should note that how the sequence
    of words and their respective combinations is getting changed for different values
    of n. If we wish to use n=1 or a single word to make a prediction, the representation
    will be as shown in Figure 7-6\. Note that each word is used separately here.
    They are referred to as *unigrams*.
  prefs: []
  type: TYPE_NORMAL
- en: If we wish to use n=2, the number of words now used will become two. They are
    referred to as *bigrams* and the process will continue.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7-6 Unigrams, bigrams, trigrams can be used to represent the same sentence.
    The concept can be extended to n-grams too.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/07_img_0007.png)'
  prefs: []
  type: TYPE_IMG
- en: Hence, if we are having a unigram, it is a sequence of one word, for two words
    it is bi-gram, for three words it is tri-gram and so on. So, a tri-gram model
    will approximate the probability of a word given all the previous words by using
    the conditional probability of only the preceding two words. Whereas a bi-gram
    will do the same by considering only the preceding word. This is a very strong
    assumption indeed that the probability of a word will depend only on the preceding
    word and is referred to as *Markov* assumption. Generally, N > 1 is considered
    to be much more informative than unigrams. But obviously the computation time
    will increase too.
  prefs: []
  type: TYPE_NORMAL
- en: n-gram approach is very sensitive to the choice of n. It also depends significantly
    on the training corpus which have been used, which makes the probabilities heavily
    dependent on the training corpus. So, if an unknown word is encountered, it will
    be difficult for the model to work on that new word.
  prefs: []
  type: TYPE_NORMAL
- en: We will create a Python example now. We will show a few examples of text cleaning
    using Python.
  prefs: []
  type: TYPE_NORMAL
- en: Text cleaning using Python
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We will clean the text data using Python now. There are a few libraries which
    you may need to install. We will show a number of small code snippets. You are
    advised to use them as per the requirement. We are also pasting the respective
    screenshot of the code snippets and their results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Code 1: Remove the blank spaces in the text. We import the library `re`, it
    is called `Regex` expression. The text is It is raining outside with a lot of
    blank spaces in between.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](images/07_img_0008.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Code 2: Now we will remove the punctuations in the text data.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](images/07_img_0009.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Code 3: This is one more method to remove the punctuation.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](images/07_img_0010.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Code 4: We will now remove the punctutions as well as convert the text to lowercase.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](images/07_img_0011.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Code 5: We will now use a standard `nltk` library. Tokenization will be done
    here using NLTK library. The output is also pasted below.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](images/07_img_0012.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that in the output of the code, we have all the words including the punctuation
    marks as different tokens. If you wish to exclude the punctuations, you can clean
    the punctuations using the code snippets shared earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Code 6: Next comes the stop words. We will remove the stopwords using nltk
    library. Post that, we are tokenizing the words.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](images/07_img_0013.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Code 7: We will now perform stemming on a text example. We use NLTK library
    for it. The words are first tokenized and then we apply stemming on them.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](images/07_img_0014.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Code 8: We will now perform lemmatization on a text example. We use NLTK library
    for it. The words are first tokenized and then we apply lemmatization on them.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](images/07_img_0015.png)'
  prefs: []
  type: TYPE_IMG
- en: Observe and compare the difference between the two outputs of stemming and lemmatization.
    For studies and studying, stemming generated the output at studi while lemmatization
    generated correct output as study.
  prefs: []
  type: TYPE_NORMAL
- en: We covered bag-of-words, tf-idf and N-gram approaches so far. But in all of
    these techniques, the relationship between words has been neglected which is used
    in *word embeddings* – our next topic.
  prefs: []
  type: TYPE_NORMAL
- en: Word Embeddings
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '"A word is characterized by the company it keeps” – John Rupert Firth.'
  prefs: []
  type: TYPE_NORMAL
- en: So far, we studied a number of approaches, but all the techniques ignore the
    contextual relationship between words. Let’s study by an example.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine we have 100,000 words in our vocabulary – starting from aa to zoom.
    Now, if we perform one-hot encoding we studied in the last section, all of these
    words can be represented in a vector form. Each word will have a unique vector.
    For example, if the position of the word king in 21000, the vector will have shape
    like the vector below which has 1 at the 21000 position and rest of the values
    as 0.
  prefs: []
  type: TYPE_NORMAL
- en: '[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0…………………1, 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few glaring issues with this approach:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of dimensions is very high to compute and complex.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The data is very sparse in nature.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If n new words have to be entered, the vocabulary increases by n and hence each
    vector dimensionality increases by n.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This approach ignores the relationship between words. We know that ruler, king,
    monarch is sometimes used interchangeably. In the one-hot-encoding approach, any
    such relationships are ignored.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If we wish to perform language translation, or generate a chat bot, we need
    to pass such knowledge to the machine learning solution. Word embeddings provide
    a solution to the problem. They convert the high-dimensional word features into
    lower dimensions while maintaining the contextual relationship. Word-embeddings
    allow us to create much more generalized models. We can understand the meaning
    by looking at an example.
  prefs: []
  type: TYPE_NORMAL
- en: In the example shown below in Figure 7-7, the relation of “man” to “woman” is
    similar to “king” to “queen”, “eat” to “eating” is similar as “study” to “studying”
    or “UK” to “London” is similar to “Japan” to “Tokyo”.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7-7 Word embeddings can be used to represent the relationships between
    words. For example, there is a relation from men to women which is similar to
    king to queen.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/07_img_0016.png)'
  prefs: []
  type: TYPE_IMG
- en: In simple terms, using word embeddings we can represent the words similarly
    which have similar meaning. Word embeddings can be thought as a class of techniques
    where we represent each of the individual words in a predefined vector space.
    Each of the word in the corpus is mapped to one vector. The distributed representation
    is understood based on the word’s usage. Hence, words which can be used similarly
    have similar representations. This allows the solution to capture the underlying
    meaning of the words and their relationships. Hence, the meaning of the word plays
    a significant role. This representation hence is more intelligent as compared
    to bag of words approach where each word is treated differently, irrespective
    of their usage. Also, the number of dimensions is lesser as compared to one-hot
    encoding. Each word is represented by 10 or 100s of dimensions which is significantly
    less than one-hot encoding approach where more than 1000s of dimensions are used
    for representation.
  prefs: []
  type: TYPE_NORMAL
- en: We will cover two most popular techniques Word2Vec and GloVe in the next section.
    The section provides an understanding of Word2Vec and GloVe. The mathematical
    foundation for Word2Vec and GloVe are beyond the scope of this book. We are providing
    un understanding on the working mechanism of the solutions and then developing
    Python code using Word2Vec and GloVe. There are a few terms which we have not
    discussed in the book so far, so the next section on Word2Vec and GloVe might
    be quite tedious to understand. If you are interested only in the application
    of the solutions, you can skip the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Word2Vec and GloVe
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Word2Vec was first published in 2013\. It was developed by Tomas Mikolov, et
    al. at Google. We are sharing the link to the paper at the end of the chapter.
    You are advised to study the paper thoroughly.
  prefs: []
  type: TYPE_NORMAL
- en: Word2Vec is a group of models used to produce word embeddings. The input is
    a large corpus of text. The output is a vector space, with a very large number
    of dimensions. In this output, each of the word in the corpus is assigned a unique
    and corresponding vector. The most important point is that the words which have
    similar or common context in the corpus, are located similar in the vector space
    produced.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Word2Vec, the researchers introduced two different learning models – Continuous
    Bag of Words and Continuous Skip-gram model, which we are covering briefly:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuous bag of words or CBOW: in CBOW, the model makes a prediction of the
    current word from a window of surrounding context words. So, the CBOW learns Recall
    that in bag of words approach, the order of the words does not play any part.
    Similarly, in CBOW, the order of the words is insignificant.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Continuous skip-gram model: it uses the current word to predict the surrounding
    window of context words. While doing so, it allocates more weight to the neighboring
    words as compared to the distant words.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: GloVe or Global vectors for Word Representation is an unsupervised learning
    algorithm for generating vector representation for words. It was developed by
    Pennington, et al. at Stanford and launched in 2014\. It is a combination of two
    techniques – matrix factorization techniques and local context-based learning
    used in Word2Vec. GloVe can be used to find relationships like zip codes and cities,
    synonyms etc. It generated a single set of vectors for the words having the same
    morphological structure.
  prefs: []
  type: TYPE_NORMAL
- en: Both the models (Word2Vec and GloVe) learn and understand vector representation
    of their words from the co-occurrence information. Co-occurrence means how frequently
    the words are appearing together in a large corpus. The prime difference is that
    word2vec is a prediction-based model, while GloVe is a frequency-based model.
    Word2Vec predicts the context given a word while GloVe learns the context by creating
    a co-occurrence matrix on how frequent a word appears in a given context.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/tgt.png)'
  prefs:
  - PREF_H5
  type: TYPE_IMG
- en: POP QUIZ – answer these question to check your understanding.. Answers at the
    end of the book
  prefs: []
  type: TYPE_NORMAL
- en: (1)      BOW is more rigorous than tf-idf approach. True or False.
  prefs: []
  type: TYPE_NORMAL
- en: (2)      Differentiate between Word2Vec and GloVe.
  prefs: []
  type: TYPE_NORMAL
- en: We will now move to the Case Study and Python implementation in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis case study with Python implementation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We have so far discussed a lot of concepts on NLP and Text data. In this section,
    we are first going to explore a business case and then develop Python solution
    on the same. We are working on Sentiment analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Product reviews are a rich source of information – both to the customers and
    the organizations. Whenever we wish to buy any new product or services, we tend
    to look at the reviews by fellow customers. You might have reviewed products and
    services yourself. These reviews are available at Amazon, blogs, surveys etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s consider a case. A water utilities provider receives complaints from
    its customers, reviews about the supply and comments about the overall experience.
    The streams can be – product quality, pricing, onboarding experience, ease of
    registration, payment process, supply reviews, power reviews etc. We want to determine
    the general context of the review – whether it is positive, negative or neutral.
    The reviews have the number of stars allocated, actual text reviews, pros and
    cons about the product/service, attributes etc. But at the same time, there are
    a few business problems like:'
  prefs: []
  type: TYPE_NORMAL
- en: Many times, it is observed that the numbers of stars received ay a product/service
    is very high, while the actual reviews are quite negative.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The organizations and the product owners, need to know which features are appreciated
    by the customers and which features are disliked by the customers. The team can
    then work on improving the features which are disliked by the customers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There is also a need to gauge and keep an eye on the competition! The organizations
    need to know, the attributes of the popular products of their competitors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The product owners can better plan for the upcoming features they wish to release
    in the future.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'So, the business teams will be able to answer these two most important questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What is our customer’s satisfaction levels for the products and services?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the major pain points and dissatisfactions of the customers, what drives
    the customers engagement, which services are complex and time-consuming, and which
    are the most liked services/products?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This business use case will drive the following business benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: The products and services which are most satisfactory and are most liked ones
    should be continued.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The ones which are not liked and receiving a negative score have to be improved
    and challenges have to be mitigated.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The respective teams like finance, operations, complaints, CRM etc. can be notified
    and they can work individually to improve the customer experience.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The precise reasons of liking or disliking the services will be useful for the
    respective teams to work in the correct direction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Overall, it will provide a benchmark to measure the Net Promoter Score (NPS)
    score for the customer base. The business can strive to enhance the overall customer
    experience.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We might like to represent these findings by means of a dashboard. This dashboard
    will be refreshed at a regular cycle like monthly or quarterly refresh.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To solve this business problem, the teams can collect relevant data from websites,
    surveys, Amazon, blogs etc. And then an analysis can be done on that dataset.
    It is relatively easy to analyze the structured data. In this example we are going
    to work on text data.
  prefs: []
  type: TYPE_NORMAL
- en: The Python Jupyter notebook is checkedin at the Github location. You are advised
    to use the Jupyter notebook from the GitHub location as it contains more steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: We import all the libraries here.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 2: We will define the tags here. These tags are used to get the attributes
    of the product from the reviews.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 3: We are now making everything ready to extract the data. We are creating
    a dataframe to store the customer reviews. Then we are iterating through all the
    reviews and then extracting the information.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 4: Let’s iterate through the reviews and then fill the details.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 5: Let’s have a look at the output we generated.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 6: we will now save the output to a path. You can give your own path.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 7: Load the data and analyze it.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 8: We will now look at the basic information about the dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 9: We will now look at the distribution of the stars given in the reviews.
    This will allow us to understand the reviews given by the customers.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 10: Make the text as lowercase, and then we remove the stop words and
    the words which have highest frequency.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 11: tokenize the data now.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 12: we are performing lemmatization now.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 13: Now we are appending all the reviews to the string.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 14: the sentiment analysis is done here. From TextBlob we are taking the
    sentiment method. It generates polarity and subjectivity for a sentiment.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 15: save the sentiment to a csv file.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 15: we will now allocate a meaning or a tag to the sentiment. We are classifying
    each of the score from extremely satisfied to extremely dissatisfied.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 16: We will now look at the sentiment scores and plot them too. Finally,
    we will merge them with the main dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: In this case study, you not only scraped the reviews from the website, you also
    analyzed the dataset. If we compare the sentiments, we can see that the stars
    given to a product, do not represent a true picture.
  prefs: []
  type: TYPE_NORMAL
- en: In (), we are comparing the actual stars and the output from sentiment analysis.
    We can observe that 73% have given 5 stars and 7% have given 4 stars, while in
    the sentiment analysis most of the reviews have been classified as neutral. This
    is the real power of sentiment analysis!
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7-8 Compare the original distribution of number of stars on the left
    side, observe the real results from the sentiment analysis
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/07_img_0017.png)'
  prefs: []
  type: TYPE_IMG
- en: Sentiment analysis is quite an important use case. It is very useful for the
    business and the product teams.
  prefs: []
  type: TYPE_NORMAL
- en: We will now move to the second case studt on document classification using Python.
  prefs: []
  type: TYPE_NORMAL
- en: Text clustering using Python
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Consider this. You have got a bunch of text datasets or documents. But they
    all are mixed up. We do not know the text belongs to which class. In this case,
    we will assume that we have two types of text datasets with us – one which has
    all the data related to football and second class is travel. We will develop a
    model which can segregate these two classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Import all the libraries'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 2: We are now creating a dummy dataset. This text data has a few sentences
    we have written ourselves. There are two categories -'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 3: We will use tfidf to vectorize the data.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 4: let’s do the clustering now.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 5: Lets represent centroids and print the outputs.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '![](images/07_img_0019.png)'
  prefs: []
  type: TYPE_IMG
- en: You can extend this example to other datasets too. Use your own dataset and
    replicate the code in the example above.
  prefs: []
  type: TYPE_NORMAL
- en: There is no more Python Jupyter notebook which uses Word2Vec and GloVe. We have
    checked-in the code at the GitHub location of the book. You are advised to use
    it. It is really an important source to represent text data.
  prefs: []
  type: TYPE_NORMAL
- en: With this, we are coming to the end of this exciting chapter. Let’s move to
    the summary now.
  prefs: []
  type: TYPE_NORMAL
- en: 7.6 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Text data is one of the most useful datasets. A lot of intelligence is hidden
    in the texts. Logs, blogs, reviews, posts, tweets, complaints, comments, articles
    and so on – the sources of text data are many. Organizations have started to invest
    in setting up the infrastructure for accessing text data and storing it. Analyzing
    text data requires better processing powers and better machines. It requires special
    skill sets and deeper understanding of the concepts. NLP is an evolving field
    and a lot of research is underway. At the same time, we cannot ignore the importance
    of sound business acumen and knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Data analysis and machine learning are not easy. We have to understand a lot
    of concepts around data cleaning, exploration, representation and modelling. But,
    analyzing unstructured data might be even more complex than structured datasets.
    We worked on images dataset in the last chapter. In the current chapter, we worked
    on text data.
  prefs: []
  type: TYPE_NORMAL
- en: Text data is one of the most difficult to analyze. There are so many permutations
    and combinations for text data. Cleaning the text data is not easy and is quite
    a complex task. In this chapter we discussed few of those important techniques
    to clean the text data. We also covered few important methods to represent text
    data in vector forms. You are advised to do practice of each of these methods
    and compare the performances by applying each of the techniques.
  prefs: []
  type: TYPE_NORMAL
- en: With this, we come to an end to the chapter seven of the book. This also marks
    an end to part two of the book. In the next part of the book, the complexity increases.
    We will be studying even deeper concepts of unsupervised learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: You can now move to the practice questions.
  prefs: []
  type: TYPE_NORMAL
- en: Practical next steps and suggested readings
  prefs: []
  type: TYPE_NORMAL
- en: Get the datasets from the link below. You will find a lot of text datasets here.
    You are advised to implement clustering and dimensionality reduction solutions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://blog.cambridgespark.com/50-free-machine-learning-datasets-natural-language-processing-d88fb9c5c8da](blog.cambridgespark.com.html)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This is the second source of text datasets, where you will find a lot of useful
    datasets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.kaggle.com/datasets?search=text](www.kaggle.com.html)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You are advised to go through the research paper – Efficient Estimation of Word
    Representations in Vector Space by Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey
    Dean
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://arxiv.org/pdf/1301.3781.pdf](pdf.html)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You are advised to go through the research paper – GloVe: Global Vectors for
    Word Representation by Jeffrey Pennington, Richard Socher, Christopher D. Manning'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://nlp.stanford.edu/pubs/glove.pdf](pubs.html)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: There are a few papers which are really quoted widely
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Avrim Blum and Tom Mitchell: Combining Labeled and Unlabeled Data with Co-Training,
    1998'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Kevin Knight: Bayesian Inference with Tears, 2009.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Thomas Hofmann: Probabilistic Latent Semantic Indexing, SIGIR 1999.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Donald Hindle and Mats Rooth. Structural Ambiguity and Lexical Relations, Computational
    Linguistics, 1993.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Collins and Singer: Unsupervised Models for Named Entity Classification, EMNLP
    1999.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You are advised to go through the research paper- Using TF-IDF to Determine
    Word Relevance in Document Queries by Juan Ramos
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.121.1424&rep=rep1&type=pdf](viewdoc.html)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You are advised to go through the research paper – An Improved Text Sentiment
    Classification Model Using TF-IDF and Next Word Negation by Bijoyan das and Sarit
    Chakraborty
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://arxiv.org/pdf/1806.06407.pdf](pdf.html)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
