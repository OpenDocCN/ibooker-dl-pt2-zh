- en: 3 Exploring and preparing the data set
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with AWS Athena for interactive querying
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing between manually specified and discovered data schemas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Approaching data quality with VACUUM normative principles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing DC taxi data quality through interactive querying
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing data quality processing in PySpark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the previous chapter, you imported the DC taxi data set into AWS and stored
    it in your project’s S3 object storage bucket. You created, configured, and ran
    an AWS Glue data catalog crawler that analyzed the data set and discovered its
    data schema. You also learned about the column-based data storage formats (e.g.,
    Apache Parquet) and their advantages over row-based formats for analytical workloads.
    At the conclusion of the chapter, you used a PySpark job running on AWS Glue to
    convert the original, row-based, comma-separated values (CSV) format of the DC
    taxi data set to Parquet and stored it in your S3 bucket.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn about Athena, another serverless feature of
    AWS that is going to prove valuable for the analysis of the DC taxi rides data
    set using Standard Query Language (SQL). You will use Athena to start with exploratory
    data analysis (EDA) and identify some of the data quality issues that exist in
    the data set. Next, you will learn about VACUUM, an acronym for a set of normative
    principles about data cleaning and data quality for effective machine learning.
    Following the VACUUM principles, you will explore the data quality issues that
    exist in the DC taxi data set and learn about using Athena to repeatably and reliably
    sample subsets of the entire DC taxi data set for analysis. Finally, you will
    implement a PySpark job to create a clean, analysis-ready version of the data
    set.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, you will learn about and practice the basics of data quality for
    tabular data sets,[¹](#pgfId-1011829) an important aspect of an effective machine
    learning project. While working on data quality, you will learn about the principles
    behind data quality for machine learning and how to apply them using SQL and PySpark
    on your machine learning platform.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Getting started with interactive querying
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section starts by providing you with an overview of the use cases for data
    queries as opposed to the data-processing jobs used to transform CSV to Parquet
    in chapter 2\. Then, as you are introduced to Athena, an interactive query service
    from AWS, you will learn about the advantages and disadvantages of using a *schema-on-read
    approach* to structured data querying, prepare to experiment with a sample taxi
    data set, and apply alternative schemas to that data set. By the conclusion of
    the section, the objective is to get you ready to use a browser-based interface
    to AWS Athena and to explore data quality issues in the DC taxi fare data set.
    As you are working on the implementation in this section, you are going to start
    picking up the skills needed for exploratory data analysis of the DC taxi fare
    data set and start practicing the kinds of queries that will become useful when
    working on improving its data quality.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1 Choosing the right use case for interactive querying
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section clarifies the distinction between I/O-intensive and compute-intensive
    workloads as well as how to choose from technologies like AWS Glue, AWS Athena,
    Google Cloud DataProc, or Google Cloud BigQuery for these two categories of workloads.
  prefs: []
  type: TYPE_NORMAL
- en: To develop an intuition about when to use interactive query services, it is
    valuable to first introduce a distinction between high throughput versus low latency
    in data processing. Recall that both row-oriented formats (like CSV) and column-oriented
    formats (like Parquet) can be used to store a structured data set organized into
    tables of rows and columns. This book uses the term *record* to describe a single
    row of data from a structured data set. Describing data sets in terms of records
    as opposed to rows helps to avoid confusion about whether the data is stored in
    a row- or a column-oriented format. In other words, a record is independent of
    the underlying data storage format.
  prefs: []
  type: TYPE_NORMAL
- en: 'In chapter 2, you used a PySpark job hosted on AWS Glue to execute a high-throughput
    workload to migrate data records from CSV to Parquet. A distinguishing characteristic
    of high-throughput workloads is a *one-to-many* (sometimes *one-to-any*) relationship
    between input and output records: a single record used as an input to the workload
    can produce zero, one, or many output records. For example, a simple SQL statement
    that begins with SELECT * returns an output record for every input record in the
    data storage, a SELECT paired with a WHERE clause can filter a portion of records,
    while a more complex SQL statement involving a SELF JOIN can square the total
    number of records returned from a table. In practice, the one-to-many relationship
    means that the number of the output records is of the same order of magnitude
    and is not substantially different from the number of the input records. Such
    workloads can also be described as input/output intensive because the underlying
    IT infrastructure executing the workload spends more time reading from and writing
    to storage compared to the amount of time spent computing.'
  prefs: []
  type: TYPE_NORMAL
- en: As you observed when starting a PySpark job in chapter 2, the CSV-to-Parquet
    re-encoding workload took on the order of minutes for completion. The high latency
    of the workload (here, latency describes the duration from the start to the finish
    of the Glue job) was caused by writing a record of Parquet output for every record
    of CSV input. The high throughput of the workload describes the total quantity
    of records processed in terms of the sum of the quantities of the input and output
    records. Since time spent processing the input and output records is a sizable
    majority of the total duration of this category of the workloads, they are also
    described as input/output (I/O) intensive.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to AWS Glue, which is designed for high-throughput workloads, interactive
    query services like Athena from AWS and BigQuery from Google are designed for
    low-latency, many-to-one (or many-to-few) workloads where many input records (think
    majority of all the records in a table) are aggregated to a few (or often to just
    one) output record. Examples of many-to-one workloads include SQL statements that
    use functions like COUNT, SUM, or AVG and other aggregation functions used with
    SQL GROUP BY clauses. Many-to-few workloads are common when identifying sets of
    unique values for a column using SQL operations like SELECT DISTINCT. The many-to-one
    and many-to-few workloads can also be described as compute intensive, because
    the underlying IT infrastructure spends more time performing compute (e.g., computing
    an arithmetic mean) than input/output operations (e.g., reading or writing data).
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 Introducing AWS Athena
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section provides a high-level overview of the AWS Athena interactive query
    service and describes how Athena applies a schema-on-read approach for data processing
    and analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Athena is a serverless query service from AWS, designed primarily for interactive
    analysis of structured and semi-structured data using ANSI SQL and SQL extensions.
    Interactive analysis means that Athena is designed to execute compute-intensive
    SQL workloads with low latency and return results within a matter of seconds.
    This also means that while it is possible to use Athena to extract, transform,
    and load (ETL) data, you should plan on using PySpark instead of Athena for your
    ETL code to support high-throughput rather than low-latency data processing. If
    you have ever used an interactive query interface for a relational database, such
    as MySQL or PostgreSQL, you know that Athena provides similar functionality. Although
    Athena is targeted at interactive analysis by end users through a browser-based
    interface, there is also support for API-based access. As a query service, Athena
    differs from traditional relational databases and data warehouses in the following
    important ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Athena relies on AWS services for data storage and does not store source data
    or metadata for queries. For example, Athena can query data sets from S3, as well
    as from MySQL, DynamoDB, or other data sources that provide Athena data source
    connectors ([http://mng.bz/p9vP](http://mng.bz/p9vP)). When data is produced as
    a result of a query, Athena stores the data to a pre-configured S3 bucket.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Athena software is based on an open source PrestoDB distributed query engine
    ([https://prestodb.io](https://prestodb.io)) developed in part by Facebook engineers.
    The implementation was demonstrated to scale to Facebook’s internal workloads
    involving queries over hundreds of petabytes of data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Athena does not use the schema-on-write approach of traditional, relational,
    data warehouses. This means that Athena can interpret the same data according
    to mutually exclusive schema definitions; for example, Athena can query a column
    of identical data values as a string or as a number. This approach is often described
    as *schema-on-read*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In chapter 2, you learned about using crawlers to discover data schemas from
    data as well as how to create databases and tables in the data catalog based on
    the discovered schemas. Athena requires that a table be defined in the data catalog
    before the service can query the data described by the table. As illustrated by
    the dashed lines on figure 3.1, Athena can be used to define schemas for data
    stored in a data storage service such as S3\. Alternatively, Athena can query
    tables defined based on the metadata discovered by a crawler.
  prefs: []
  type: TYPE_NORMAL
- en: '![03-01](Images/03-01.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 The Athena query service can both define a schema and use one defined
    by a Glue crawler to analyze the data using alternative schema definitions for
    the same data set. Alternative and mutually exclusive schemas can help you apply
    the right schema for your specific use case.
  prefs: []
  type: TYPE_NORMAL
- en: Relying on Athena to define schemas for a table in the data catalog has both
    advantages and disadvantages. Since many real-world data sets stored in data warehouses
    and used for machine learning are wide (consisting of many columns), defining
    a table schema using Athena translates to the effort required to explicitly specify
    SQL data types for each column in the schema. Although it may seem that the effort
    is limited in scope, keep in mind that the schema definition needs to be maintained
    and updated whenever there is a change in the underlying data set. However, if
    you need to be able to query the same data using data schemas containing mutually
    exclusive data types, then using Athena to define schemas is the right option.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, if you use the crawler-based schema definition approach, you don’t
    need to explicitly specify the data types as they are automatically discovered
    by the crawler. The crawler can also be scheduled to run periodically, updating
    the schema definition based on changes in the data. The downside of using the
    crawler is relevant when you need to query data using an alternative data schema
    with differences from the automatically discovered one. In the crawler-based approach,
    this translates to either using Athena to define the alternative schema or implementing
    a PySpark job that applies the alternative schema to the data set. Recall that
    the PySpark job that you implemented at the conclusion of chapter 2 re-encoded
    STRING data types (for example, for fare amount) to DOUBLE.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.3 Preparing a sample data set
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, you will start working with a tiny CSV data set to better understand
    the advantages and disadvantages of relying on Athena to define the schema for
    it. In the data set, the rows contain values representing a taxi trip fare amount
    as well as the latitude and longitude coordinates of the trip pickup and drop-off
    locations.
  prefs: []
  type: TYPE_NORMAL
- en: To begin querying this tiny data set using Athena, you need to first upload
    the corresponding CSV file, consisting of just five rows of data to a folder of
    your S3 bucket.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a CSV file named trips_sample.csv on your local filesystem and preview
    it by executing the following bash commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Assuming the bash commands executed successfully, the output of cat should have
    produced an output resembling table 3.1.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3.1 The type interpretation of data values in this data set[²](#pgfId-1012363)
    depends on your choice of a schema.
  prefs: []
  type: TYPE_NORMAL
- en: '| Fare amount | Origin | Destination |'
  prefs: []
  type: TYPE_TB
- en: '| Latitude | Longitude | Latitude | Longitude |'
  prefs: []
  type: TYPE_TB
- en: '| 8.11 | 38.900769 | −77.033644 | 38.912239 | −77.036514 |'
  prefs: []
  type: TYPE_TB
- en: '| 5.95 | 38.912609 | −77.030788 | 38.906445 | −77.023978 |'
  prefs: []
  type: TYPE_TB
- en: '| 7.57 | 38.900773 | −77.03655 | 38.896131 | −77.024975 |'
  prefs: []
  type: TYPE_TB
- en: '| 11.61 | 38.892101 | −77.044208 | 38.905969 | −77.0656439 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.87 | 38.899615 | −76.980387 | 38.900638 | −76.97023 |'
  prefs: []
  type: TYPE_TB
- en: 'Next, copy the contents of the file to the samples folder in your S3 object
    storage bucket and confirm that it copied successfully by running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If you correctly uploaded the sample file, the output of the aws s3 ls command
    should report that it is 378 bytes in size.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.4 Interactive querying using Athena from a browser
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section introduces the browser-based graphical user interface (GUI) for
    AWS Athena. Although it is possible to use the Athena GUI to perform the queries
    used throughout this chapter, data analysis automation and reproducibility is
    more straightforward to demonstrate with a command line interface (CLI)—based
    access to the Athena API rather than with the browser. So, while this section
    covers how to use the browser-based interface, later sections focus on scripting
    CLI-based queries.
  prefs: []
  type: TYPE_NORMAL
- en: To access the Athena interface from your browser, navigate to the Athena service
    using the AWS Services dropdown menu in the AWS Console top menu. You should be
    able to click through to a screen that resembles the one shown in figure 3.2.
  prefs: []
  type: TYPE_NORMAL
- en: '![03-02](Images/03-02.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 Screen capture of the Athena’s browser-based interface illustrating
    the key components you need to know for interactive querying
  prefs: []
  type: TYPE_NORMAL
- en: Note that on the Athena interface screen you need to make sure you are accessing
    Athena from the region that matches the value of your $AWS_DEFAULT_REGION environment
    variable, the one where you have uploaded your CSV file. As with other AWS services,
    you can change the region using the dropdown menu in the upper right-hand corner
    of the AWS console top menu.
  prefs: []
  type: TYPE_NORMAL
- en: The selection highlighted as 1 in figure 3.2 specifies the data catalog database
    you created in chapter 2\. Make sure you have dc_taxi_db selected as the database.
    Once the database is selected, confirm that in the selection highlighted as 2
    you can see the tables you created using the crawler in the dc_taxi_db database.
    The tables should be named dc_taxi_csv and dc_taxi_parquet.
  prefs: []
  type: TYPE_NORMAL
- en: 'SQL queries for Athena are specified using a tabbed SQL editor highlighted
    as 3 in figure 3.2\. If this is your first time using Athena, before running a
    query you will need to specify a query result location for the service. By default,
    the output of every query executed by Athena is saved to the query result location
    in S3\. Execute the following bash shell command and copy the output to your clipboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Notice from the output of the shell command that Athena will store the query
    locations results to the athena folder in your bucket.
  prefs: []
  type: TYPE_NORMAL
- en: Before you run your first query, you should configure the S3 query result location
    by first clicking on the “set a query result location” hyperlink shown in the
    upper part of the screenshot in figure 3.2, then pasting the value you just copied
    to the clipboard into the result location text field in the dialog, and finally
    clicking Save.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.5 Interactive querying using a sample data set
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section explains how to apply schema-on-read in Athena using the few records
    from the trips_sample.csv file. In later sections, you are going to be able to
    apply the same technique to larger data sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the upcoming Athena examples rely on using a scripted, CLI-based access
    to Athena API, start by configuring Athena to use the athena folder in your S3
    bucket as the location to store results of Athena queries. This means that you
    should execute the following from your shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Once the dc_taxi_athena_workgroup is created, you can start using Athena via
    the CLI.
  prefs: []
  type: TYPE_NORMAL
- en: Since Athena is integrated with the Glue data catalog, the database and the
    schema definitions from a data catalog table can be applied at data read time
    (i.e., when querying data) as opposed to data write time. However, to illustrate
    the schema-on-read capability of Athena, instead of using a crawler to discover
    a table schema for the five sample trips, you will first pre-populate the data
    catalog using tables with manually defined schemas. The first table you will create
    treats all of the data values in trips_sample.csv as STRING data type, as shown
    in listing 3.1\. Later, you will create a second table that treats the same values
    as DOUBLE.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.1 Defining a schema for the five DC trips data set using STRING types
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: To define the schema for the dc_taxi_db.dc_taxi_csv_sample_strings table using
    the SQL statement from listing 3.1, execute the following sequence of commands
    from your bash shell.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.2 Shell-based query of AWS Athena using AWS CLI
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Save the string-based schema definition to the SQL shell variable.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Start the Athena query based on the contents of the SQL variable.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Repeatedly check and report on whether the Athena query is finished.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, based on your experience with using SQL to query relational databases,
    you might be tempted to query the dc_taxi_csv_sample_strings table using a SQL
    statement that starts with SELECT *. However, when working with columnar data
    stores, it is better to avoid SELECT * whenever possible. As you learned in chapter
    2, columnar stores maintain individual columns of data across multiple files as
    well as across different parts of files. By specifying just the names of the columns
    that you need for your query, you direct a column-aware query engine like Athena
    to process just the parts of the data you need, reducing the overall quantity
    of data processed. With Athena, as well as with serverless query services from
    other public cloud vendors,[³](#pgfId-1044495) lower amounts of data processed
    translate to lower costs. Since Athena is serverless, you are billed by AWS based
    on the amount of data processed by your Athena queries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, the script in listing 3.2 is quite verbose. To keep query examples in
    this chapter concise, proceed by downloading a utils.sh script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Once downloaded, the script should take up 4,776 bytes on your filesystem. This
    script is loaded in the upcoming examples using the source utils.sh command and
    is invoked by passing a SQL query for Athena to the athena_query_to_table function.
  prefs: []
  type: TYPE_NORMAL
- en: When Athena is querying data using the schema from the dc_taxi_csv_sample_ strings
    table you just created, the data is processed by interpreting the latitude and
    longitude coordinates as a STRING data type. Treating the coordinate values as
    strings can be useful when passing a pair of the coordinates to a web page script
    in order to display a pin on an interactive map in a browser. Notice that the
    following query does not involve any data type casting since the data is read
    by Athena from the source CSV data as a STRING. Hence, it is possible to use the
    ANSI SQL || (double vertical bar) operation directly on the data values to perform
    the concatenation operation.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.3 Using STRING data type for coordinates to simplify browser-based
    use cases
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in an output resembling the following, where each row contains
    string data types, concatenating the latitude and longitude values:'
  prefs: []
  type: TYPE_NORMAL
- en: '| origin | destination |'
  prefs: []
  type: TYPE_TB
- en: '| 38.900769, –77.033644 | 38.912239, –77.036514 |'
  prefs: []
  type: TYPE_TB
- en: '| 38.912609, –77.030788 | 38.906445, –77.023978 |'
  prefs: []
  type: TYPE_TB
- en: '| 38.900773, –77.03655 | 38.896131,–77.024975 |'
  prefs: []
  type: TYPE_TB
- en: '| 38.892101000000004, –77.044208 | 38.905969, –77.06564399999999 |'
  prefs: []
  type: TYPE_TB
- en: '| 38.899615000000004, –76.980387 | 38.900638, –76.97023 |'
  prefs: []
  type: TYPE_TB
- en: 'Alternatively, Athena can use a different schema, treating the same coordinate
    values as a floating point data type to compute the differences between the largest
    and smallest fare amounts in the data set. Execute the following Athena operation
    from your shell to create the dc_taxi_csv_sample_double table where every value
    in the trips_sample.csv file is interpreted as an SQL DOUBLE:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'After the dc_taxi_csv_sample_double table becomes available for a query, you
    can try processing the values in the source data file as doubles, for example,
    by trying to find the difference between the largest and the smallest amounts
    for the taxi fare in the five-row data set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The athena_query_to_pandas function in the listing saves the output of the Athena
    query to a temporary /tmp/awscli.json file on your filesystem. First, define the
    Python utility function as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.4 Reporting Athena results as a pandas DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you can conveniently preview the contents of the tmp/awscli.json file
    as a pandas DataFrame, so that calling awscli_to_df() outputs the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| _col0 |'
  prefs: []
  type: TYPE_TB
- en: '| 6.74 |'
  prefs: []
  type: TYPE_TB
- en: The output shows that there was a $6.74 difference between the maximum and the
    minimum values for the taxi fare in the data set. Also, since the last query did
    not use the AS keyword to assign a name for the sole column in the result, Athena
    used an automatically generated column name of _col0.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.6 Querying the DC taxi data set
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section gets you started with using AWS Athena to query the DC taxi data
    set so that in the upcoming sections you are prepared to analyze DC taxi data
    quality.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you recall from chapter 2, the Parquet-formatted version of the DC taxi
    data was stored as the dc_taxi_parquet table in the dc_taxi_db database. Let’s
    attempt a query of 10 rows of this table using the Athena CLI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Don’t forget to use the awscli_to_df() function to output the result using pandas.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the parallel and distributed nature of data processing performed by Athena,
    the order of rows in the dc_taxi_parquet table will be different for every execution
    of the last query. Hence, the 10 rows you will see as the output of the query
    will be different from mine. However, even with just 10 rows of the results you
    should be able to find rows with missing values. The missing values will appear
    as empty cells or None values in one or more columns.
  prefs: []
  type: TYPE_NORMAL
- en: For example, you may find that your output has missing values for the origin
    but not for the destination coordinates. In some cases, all but the fare amount
    and the trip origin date/time values will be missing in the results. The imported
    DC taxi trip data set has data quality issues.
  prefs: []
  type: TYPE_NORMAL
- en: While the transformation of the DC taxi data to the Parquet format in chapter
    2 helped you optimize query and analytics performance for working with the data,
    you have not yet performed any quality checks against the data set. In short,
    you don’t know if the data available to you can be trusted. What does it mean
    to address these quality issues? What problems should or should not be fixed?
    How much effort should you put into cleaning the data? When does the cleaned-up
    data set achieve sufficiently good quality for data analysis and machine learning?
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Getting started with data quality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This part of the chapter is written differently from other sections. Whereas
    most of the chapter focuses on the technical knowledge and detailed instruction
    in a form of specific steps for working with serverless machine learning technologies,
    this part is normative rather than prescriptive. In other words, you should first
    understand what data quality for machine learning should be like, and then learn
    the steps involved in applying data quality to your machine learning data set.
    My goal for this part is to teach you the data quality criteria you should use
    across any machine learning project, regardless of the data set, so this section
    deals primarily with concepts rather than code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s face it: data cleanup is an important but not the most exciting topic
    in machine learning, so to make the data quality principles more concrete, easier
    to remember, and hopefully more interesting, this section relies heavily on real-world
    case studies and data cleanup examples you can apply to your next machine learning
    project. If you prefer to proceed directly to the practical steps of cleaning
    up the DC taxi data, feel free to jump ahead to section 3.3.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 From “garbage in, garbage out” to data quality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This subsection illustrates a rationale for addressing data quality issues and
    describes the data quality questions that are answered in the later parts of this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: “Garbage in, garbage out” is a well-known cliché in the information technology
    industry. In the context of this book, it means that if the input into your machine
    learning system is garbage, then the machine learning algorithms will train on
    garbage, and the outputs of machine learning will be garbage as well. The cliché
    points to the importance of data quality for a machine learning project, but it
    does not give evidence that garbage in, garbage out is critical or relevant to
    real-world data analysis and machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: In 2010, as the global economy was still recovering from the financial meltdown
    that took place a few years prior, two Harvard economists, Carmen M. Reinhart
    and Kenneth S. Rogoff, published a research paper deconstructing policies that
    could help countries get their economies growing again. In the paper, the economists
    argued that countries that incur debt of over 90% of their gross domestic product
    (GDP) face economic declines. In part based on the analysis from the economists,
    some European Union (EU) countries adopted harsh austerity measures, slashing
    salaries and eliminating thousands of jobs. As it turned out, the data used for
    the analysis was wrong.
  prefs: []
  type: TYPE_NORMAL
- en: The politicians who based their policies on the Reinhart-Rogoff results fell
    victim to the classic garbage in, garbage out problem. The Reinhart-Rogoff fiasco
    is just one instance of many where an analysis of poor quality data led to billions
    of dollars in negative consequences. Even prior to the digital transformation
    accelerated by the COVID-19 pandemic, the highly respected *Harvard Business Review*
    magazine published a notable claim that the total cost of bad data to the US economy
    should be measured in thousands of billions of US dollars.[⁴](#pgfId-1045716)
  prefs: []
  type: TYPE_NORMAL
- en: The issue of data quality is important, but as a machine learning practitioner,
    it may not be immediately obvious that you are working with a poor quality data
    set. How do you know if your data is garbage or if it is of sufficient quality
    to perform machine learning?
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Before starting with data quality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This subsection helps you understand the questions that should be answered about
    any structured (tabular) data set before addressing the data quality issues in
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before you can begin to work on data quality, you need more than just a structured
    data set. You need to know answers to the kinds of questions you have already
    answered about the DC taxi data:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Can the data set be queried as one or more tables of rows and columns?* In
    other words, are you querying data stored using a structured data set format?
    Recall that in chapter 2 you looked at definitions of row- (e.g., CSV) and column-oriented
    (e.g., Apache Parquet) storage formats for structured data. Since VACUUM is a
    set of data quality principles for structured data sets, it does not apply to
    unstructured formats used for natural language text, images, audio, and video.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*What questions do you need to answer based on which columns?* The DC taxi
    data set—based machine learning example in this book is built around the question
    “What is the value for the fare amount column, given that you know the start time
    of a DC taxi trip, as well as the latitude and longitude coordinates for the pickup
    and the drop-off locations of the trip?” Knowing the questions you wish to ask
    about the data also helps you understand the *essential data* in your data set—in
    other words, the data in scope for training of your machine learning system to
    answer the questions. In addition to the essential data, your data set may also
    contain *reference data* that is useful for ensuring the quality (specifically
    the accuracy) of your essential data but does not need to be cleaned up with the
    same degree of rigor. For example, the values in the mileage column of the DC
    taxi data set are not essential to answering the questions but are useful as a
    reference to compare against the values of the fareamount column and to ensure
    that the fare amount values have the right degree of data quality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*What is the schema for the essential data?* Before you can begin cleaning
    up the data set, you need to create a data schema in the catalog with changes
    that ensure the data values are specified using appropriate data types and constraints.
    The data schema specifies the data type for every column of a data set. Yet, while
    the data type specifications are necessary for the schema to help ensure data
    quality, they are not sufficient. For every data type, you should also be able
    to specify whether it is *nullable*. Here, data type nullability is equivalent
    to the DDL (data definition language) nullability and indicates whether a value
    is allowed to be missing. You should also specify any constraints that further
    limit the range of possible values: with string types, these can include regular
    expressions, while with integer types, these can be specified using interval ranges.
    The section on valid data illustrates the constraints using practical examples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the previous chapter, you used a crawler and a data catalog to discover and
    store the discovered data schema for the DC taxi data set. The schema in the catalog
    is similar to a DDL schema (part of the SQL standard) that describes data types
    such as integers, floats, timestamps, and more. Keep in mind that the discovered
    schema may or may not be the right schema to use.
  prefs: []
  type: TYPE_NORMAL
- en: So what does it mean to have the right schema? More precisely, what does it
    mean for a schema to consist of data types that are appropriate for the data set’s
    values? Just as with DDL schemas, the choice of the appropriate data types is
    a tradeoff. On one hand, the schema should use data types that are sufficiently
    general to preserve the data values without loss of information. On the other
    hand, the data types should support (without type casting) expected operations
    on the data values while making efficient use of storage space. For example, the
    latitude and longitude coordinates from the DC taxi data set should be specified
    in the schema as floating point values (DOUBLE data type) instead of Unicode strings
    so that the coordinate values can be used for distance calculations.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.3 Normative principles for data quality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section is about the principles behind the *valid, accurate, consistent,
    uniform,* and *unified model* (VACUUM) for structured data quality, along with
    cautionary tales to serve as case studies. The principles are normative, meaning
    that they define what quality data ought to be like instead of prescribing the
    specific steps or code for the implementation of data quality processing. The
    value of the principles is in a comprehensive and rigorous definition behind the
    claim that the data that complies with VACUUM is sufficiently “clean” and ready
    for machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Think of the VACUUM principles as a checklist of guidelines, criteria, or metrics
    of data quality that you should explore as part of your machine learning project.
    Keep in mind that doctors and pilots (as well as many other professionals) use
    checklists, but having a checklist will not make you a pilot or a doctor. If you
    are planning to develop professional expertise in data quality, you will need
    to develop your data-cleaning skills. Once you have the right experience in place,
    a checklist can help jog your memory and ensure that you do not miss important
    data quality aspects.
  prefs: []
  type: TYPE_NORMAL
- en: Valid
  prefs: []
  type: TYPE_NORMAL
- en: On January 31, 2020, the United Kingdom left the EU. So, should an EU data warehouse
    store the string value United Kingdom as a valid value in a column with names
    of EU member countries?
  prefs: []
  type: TYPE_NORMAL
- en: 'You could argue that beginning February 1, 2020, United Kingdom should stop
    being a valid data value in any column mentioning EU member states. However, this
    approach is counterproductive: excluding United Kingdom from a set of valid values
    means that any historical data associated with the column (in other words, any
    records dated prior to February 1, 2020) are associated with a value that is not
    valid. If a value in a data set was valid at any point of the data set’s existence,
    it should remain valid.'
  prefs: []
  type: TYPE_NORMAL
- en: Note This definition does not specify whether a combination of multiple valid
    values across multiple columns is valid; this issue will be addressed in the upcoming
    section on accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'More precisely, a data value in a column is *valid* if it:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Matches the column data type specified by the schema*. For a data value to
    be valid, it must match the data type specified by the schema. SQL-based data
    type definitions in a schema may include the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: INTEGER (for example, in a column storing an elevator floor number)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: DOUBLE (for example, a percentage of users who click a Subscribe button on a
    website)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: TIMESTAMP (for example, the time an order was placed on a website)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: BOOLEAN (for example, whether a taxi trip ended at an airport)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: STRING (for example, the text of comments left in a comment box on a survey)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Matches one or more of the following constraints* :'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Nullability* —This constraint applies to any data type and specifies whether
    a value in a data column is allowed to have a NULL value. For example, a TIMESTAMP
    data value storing the date of birth in a driver’s license database must be non-nullable
    (i.e., should not be permitted to have a NULL value), while a user’s Twitter handle
    on a customer profile web page can be specified as nullable to handle the cases
    when the handle is unknown or not specified. Nullable data types can also include
    INTEGERs (e.g., a rating of a taxi ride by a passenger on a scale of 1—5, with
    a NULL value representing no rating), and other data types.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Enumeration* —This constraint applies to any data type and specifies a validation
    set, a dictionary, or an enumeration of valid values for a data type. With STRING
    values, the enumerations may include names of US states, or major airports in
    the New York City area, such as LGA, JFK, EWR. The enumeration constraint for
    a schema may specify an INTEGER data type for a country code column in a data
    set of phone numbers using an enumeration of valid country phone codes. Recall
    from the example at the beginning of this section that the enumeration must include
    all values that have ever been valid for the column. So, in any data set older
    than February 1, 2020, in a data column that stores EU country names, United Kingdom
    is a valid value, regardless of the fact that the UK left the EU on January 31,
    2020.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Range* —This constraint is data type specific and can be one of the following
    types:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Interval constraint* is used for numeric or date/time data types. As an example
    of valid integer data values, consider a data set with an activity log for a single
    elevator in a skyscraper. One of the data columns in the data set stores the floor
    number where the elevator stopped. Since not all floors in this hypothetical skyscraper
    are accessible by the elevator and the numbering systems skips the 13th floor
    due to superstition, the constraints on possible values include intervals from
    —3 to —1 for the parking garage, and 1 to 12 and 14 to 42\. The typical notation
    for this interval is [[—3, —1] or (0, 12] or [14,42]], where the square brackets
    indicate that the value is included in the interval while the parentheses indicate
    that the interval does not include the value neighboring the parenthesis. The
    or keyword in this case represents the set union operation (in other words, a
    logical or).'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A similar approach is used when working with DOUBLE and other floating point
    data types. For example, a probability value can be specified with an interval
    range constraint from 0.0 to 1.0, [0.0, 1.0]
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Intervals are also common with TIMESTAMP data types for a date/time range where
    they are used to describe periods such as workdays, weekends, or holidays (e.g.,
    dates: [2020-01-01 00:00:00, 2021-01-01 00:00:00]).'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Regular expression constraint* is used in the cases of the STRING data type
    to specify the space of the valid values. For example, in a database that stores
    Twitter handles of social media influencers, a regular expression can specify
    that any value that matches /^@[a-zA-Z0-9_]{1,15}$/ is valid. Keep in mind that
    regular expressions also apply to many data columns that appear numeric; for instance,
    IP addresses consist primarily of numbers but are commonly stored as a string.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Rule* —This constraint applies to any data type and specifies computable conditions
    to decide whether a value is valid. For example, if you have ever used a “Save
    my payment for later” feature on a website to permit a PCI-DSS-compliant[⁵](#pgfId-1046619)
    vendor to store your credit card number, you should know that a rule constraint
    for a credit card number is based on Luhn algorithm,[⁶](#pgfId-1066715) which
    computes parity check bits that ensure a credit card number is valid.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At this point, you have seen the criteria specifying and examples illustrating
    what it means for a single value in a data set to be valid or invalid. However,
    it is straightforward to come up with an example of a record consisting entirely
    of valid values but with an obvious data quality problem. Here’s a made-up record
    from a hypothetical data set that lists locations with a continent and country
    information:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Continent | Country | Lat | Lon |'
  prefs: []
  type: TYPE_TB
- en: '| South America | United States | 38.91 | –77.03 |'
  prefs: []
  type: TYPE_TB
- en: All the values, including South America, United States, as well as the latitude/longitude
    coordinates, have valid values for the respective columns. Recall that the valid
    principle from VACUUM focuses on data quality problems and validation checks within
    a single value. To address the data quality issue in this example, you need to
    learn about the accuracy principle.
  prefs: []
  type: TYPE_NORMAL
- en: Accurate
  prefs: []
  type: TYPE_NORMAL
- en: 'When you learned about valid data, you looked at an example of a data set of
    records about member states of the EU. As part of the example, you saw that United
    Kingdom is a valid value for the EU country column. Suppose you are working with
    a data record that has two columns: the first with a date/time of the membership
    and the second with the name of the country:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Date of Record | Member State |'
  prefs: []
  type: TYPE_TB
- en: '| 2020-01-31 | United Kingdom |'
  prefs: []
  type: TYPE_TB
- en: '| 2020-02-01 | United Kingdom |'
  prefs: []
  type: TYPE_TB
- en: While all of the values in the example are valid, it is impossible to assert
    that the second row is garbage without using an external (to the data record)
    reference data source. The reference should be able to process the values from
    the entire record and indicate whether (or to what extent) the record is inaccurate.
  prefs: []
  type: TYPE_NORMAL
- en: More precisely, a data record is *accurate* if all the data values that are
    part of the record are valid and the combination of the values in the record are
    reconciled to a reference data source. By the way of an example, consider a database
    of college alumni, with the date of alumni college enrollment and the date of
    their college graduation. Checking the database for accuracy requires references
    to external sources of truth, such as an admissions database and a transcript
    database. In financial records, inaccuracy can be due to a mismatch between a
    credit card number and its PIN code. Sometimes accuracy problems arise when joining
    multiple tables incorrectly, for example, in a data record stating that the movie
    *Titanic* was produced by Guy Ritchie in 1920.
  prefs: []
  type: TYPE_NORMAL
- en: The accuracy assurance for values such as domain names are a particularly difficult
    category because the reference data source, domain registration, and DNS databases
    change over time. For example, if you try to create an email mailing list and
    check the domain name part of an email using a regular expression, the data in
    the list may be valid but not accurate in the sense that some of the emails do
    not map to valid domain names. You may attempt to send an email to the address
    from the mailing list to confirm that the domain name and the email resolve to
    an accurate address. Even prior to sending the email, you may attempt to perform
    a DNS lookup to verify the accuracy of the domain name.
  prefs: []
  type: TYPE_NORMAL
- en: In the UK leaving the EU example, improving the quality of the data in the data
    set means that the reference data source must exist with the master record of
    the timestamps for the start and the end dates of an EU state membership. However,
    for many organizations the challenge with reference data sources isn’t that there
    are too few of them, but rather that there are too many. The next section on consistency
    will illustrate this problem with more examples.
  prefs: []
  type: TYPE_NORMAL
- en: Consistent
  prefs: []
  type: TYPE_NORMAL
- en: In January of 2005, an owner of a small house in Valparaiso, a town of about
    30,000 residents in Porter County, Indiana, received a a notice that the annual
    tax assessment value of his house was set at $400 million. The notice, which also
    included a demand for a tax payment of $8 million, came as a surprise to the owner
    of the modest house since just the year prior the tax payment amounted to $1,500\.
    Although the issue with data accuracy was soon resolved, the story did not end
    there.
  prefs: []
  type: TYPE_NORMAL
- en: The data systems of Valparaiso did not follow the data quality consistency principle,
    so the original data accuracy problem propagated into the budgeting system for
    the town. The small town’s budget was drawn with an assumption of an $8 million
    tax payment, so the town had to claw back $3.1 million from schools, libraries,
    and other budget-funded units. That year, Porter County ended up with many unhappy
    students and parents as the schools had to cover a $200,000 budget shortfall.[⁷](#pgfId-1050388)
  prefs: []
  type: TYPE_NORMAL
- en: 'Consistency issues arise when different and conflicting validation and accuracy
    implementations are used across different data silos: databases, data stores,
    or data systems. While each individual silo can be valid and accurate according
    to a silo-specific set of definitions, achieving consistency means ensuring a
    common set of standards for valid and accurate data before integrating the data
    from systems across silos that span different technology and organizational boundaries.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, was the UK a member of the EU at 11:30 p.m. on January 31, 2020?
    If you are not mindful of data quality, the answer may depend on your data set.
    In a UK data set, you can expect a valid and accurate record showing that the
    UK was not an EU member country at 11:30 p.m. on January 31, 2020\. Yet, in an
    EU data set, an identical combination of the date, time, and country name values
    is an accurate record for an EU country member. As you have probably guessed,
    the inconsistency is due to different assumptions about storing the date and time
    values across different data sets. The UK data set in this example uses the Greenwich
    Mean time zone, while the EU data set uses Central European time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Even when joining tables within a single data set, or a single data silo, it
    is important to ensure consistency of validation and accuracy rules. Typical issues
    arise when using phone numbers and emails as unique identifiers for a user: since
    phone numbers and emails may change owners, joining tables based on this information
    might lead to problems. Another example might include different approaches for
    storing other identifiers such as phone numbers. Some might uniquely identify
    with a country code, and others may not. This might be as simple as using different
    primary keys with the same individual across different systems, maybe creating
    a new primary key to join together or it might be more subtle. For example, some
    systems might use a 5+4 ZIP code; other systems might use a five-digit ZIP code
    per individual.'
  prefs: []
  type: TYPE_NORMAL
- en: Uniform
  prefs: []
  type: TYPE_NORMAL
- en: 'The Mars Climate Orbiter, a $125 million robotic space probe, was launched
    by NASA to Mars in 1998\. Less than 12 months later, during an orbit change maneuver,
    it skidded off the atmosphere of Mars and was gone. The cause was simple: the
    designers of the orbiter integrated two independently developed systems where
    one was using US customary (imperial) units of measurement and another was based
    on the SI (metric) units. Thus, non-uniform tables were UNION-ed together (concatenating
    records) and data with non-uniform records appeared in the data set. Since the
    data measurement values used by the orbiter were not uniform across multiple data
    records, NASA wasted $125 million of its budget.'
  prefs: []
  type: TYPE_NORMAL
- en: The distinction between consistency and uniformity is subtle but important.
    As illustrated by the Mars orbiter example, ensuring consistency of validation
    and accuracy rules across data silos is insufficient to address data quality.
    The *uniform* principle states that for every column in a data set, all records
    should use data that was recorded using the same (uniform) measurement system.
    Instead of continuing with the NASA example, consider a more down-to-earth scenario
    of a data set created to analyze customer satisfaction with different video streaming
    services.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose some streaming services are prompting users to rate their satisfaction
    with content on a scale from 0—4 stars after each viewing. Other services may
    use a value of 0 to indicate no response on a scale of 1—4 stars. Although the
    rules around valid values for both are identical, to ensure data quality, it is
    insufficient to specify that customer satisfaction should be a DOUBLE value with
    valid interval of [0, 4] and then apply this interval consistently across video-streaming
    service data silos. For instance, if the average satisfaction scores for each
    service are recorded daily and joined to prepare an aggregated average score,
    the result is not uniform across rows in the data set. Specifically, the service
    that uses the value of 0 to indicate no response will be penalized in the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Uniformity issues often arise over the lifetime of a data set. Consider a grocery
    store chain that enforces a store aisle coding system where all stores have aisles
    numbered 1—8, with each aisle corresponding to a produce category, such as 1 for
    dairy, 2 for meats, 3 for frozen foods, and so on. As soon as a single store violates
    the aisle coding system, for instance by coding dairy as 2 instead of 1, uniformity
    is violated across the entire grocery chain.
  prefs: []
  type: TYPE_NORMAL
- en: Unified
  prefs: []
  type: TYPE_NORMAL
- en: 'In a 1912 book, influential logician and philosopher Bertrand Russell used
    a story to illustrate the problem with inductive reasoning, a fundamental principle
    behind machine learning. Paraphrasing Russell, here’s the fable in a nutshell:'
  prefs: []
  type: TYPE_NORMAL
- en: On December 1, a turkey was born in the United States. It was no ordinary turkey.
    Some say it was the most intelligent turkey that ever lived. The genius turkey
    soon figured out the patterns of the night sky and the role of the sun in casting
    shadows, and realized that it was fed at 7:00 a.m. every day. Reasoning that food
    was critical to well-being, it proceeded to ask itself whether it would be worthwhile
    to plot an escape, risking hunger and death, or if it would be safer to remain
    a well-fed captive. Having re-invented statistics, the rational genius turkey
    gathered data, developing increasing confidence that it will be fed every day
    at 7:00 a.m. regardless of the position of the sun, the moon, the stars, temperature,
    precipitation, and other factors. Sadly, the morning of the Thanksgiving Day the
    food did not come, and the head of the genius turkey landed on a chopping block.
  prefs: []
  type: TYPE_NORMAL
- en: 'The story (which is meant to be more facetious rather than tragic) is here
    to help you remember that the machine learning models that you create, no matter
    how complex, are little more than digital versions of Russell’s turkey. Their
    success is based solely on their capacity to take advantage of the data available
    to them. In contrast, as a machine learning practitioner, you can make your machine
    learning project more successful through curiosity and causal and deductive reasoning:
    by discovering facts and data sets that are novel and relevant to the project’s
    use case, unifying the discovered information with the data set on hand, and expanding
    the scope of the relevant training data available to train models. You can also
    help minimize the risks to the machine learning project’s success by discovering
    and addressing potential sources of non-obvious systemic bias in the training
    data set, unifying and bringing into alignment the cultural values of the project’s
    operating environment and the contents of the data set.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While you can rely on machine learning models to perform effective inductive
    reasoning, it is your responsibility to enforce the *unified* principle, meaning
    that your data set:'
  prefs: []
  type: TYPE_NORMAL
- en: Is a single place for the data relevant to your project’s machine learning use
    case(s)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aligns the criteria used by your use case to achieve unbiased data-driven decision
    making, with the content of the data being used for machine learning model training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depends on a common data quality process for the data used for machine learning
    model training and for the data used with a trained machine learning model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The unified principle is a part of VACUUM to remind you that data quality is
    a journey and not a destination.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Applying VACUUM to the DC taxi data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you know about the VACUUM data quality principles, it is time to apply
    the principles to the DC taxi data set. In this section, you are going to start
    with a single table of data and focus on how to implement the data quality queries
    that ensure that the data set is valid, accurate, and uniform.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1 Enforcing the schema to ensure valid values
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section introduces the SQL statements you can execute against the DC taxi
    data set to check for invalid values and eliminate them from further analysis.
  prefs: []
  type: TYPE_NORMAL
- en: The schema shown in table 3.2 matches the version you have first encountered
    in chapter 2\. The schema specifies the required data types for the taxi fare
    estimation service interface using SQL types. In the upcoming chapters of the
    book, when you will start training machine learning models from the DC taxi data,
    NULL values in the training data set can create problems. (Consider asking a machine
    learning model to estimate taxi fare for a NULL pickup location!) So, the schema
    is designed to ensure that none of the data values are permitted to be NULL.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3.2 Schema and example values for a taxi fare estimation service interface
  prefs: []
  type: TYPE_NORMAL
- en: '| Input |'
  prefs: []
  type: TYPE_TB
- en: '| Name | Data Type | Example Value |'
  prefs: []
  type: TYPE_TB
- en: '| Pickup location latitude | FLOAT | 38.907243 |'
  prefs: []
  type: TYPE_TB
- en: '| Pickup location longitude | FLOAT | –77.042754 |'
  prefs: []
  type: TYPE_TB
- en: '| Dropoff location latitude | FLOAT | 38.90451 |'
  prefs: []
  type: TYPE_TB
- en: '| Dropoff location longitude | FLOAT | –77.048813 |'
  prefs: []
  type: TYPE_TB
- en: '| Expected start time of the trip | STRING [⁸](#pgfId-1071136) | 01/12/2015
    12:42 |'
  prefs: []
  type: TYPE_TB
- en: '| Output |'
  prefs: []
  type: TYPE_TB
- en: '| Name | Data Type | Example Value |'
  prefs: []
  type: TYPE_TB
- en: '| Estimated fare (dollars) | FLOAT | 6.12 |'
  prefs: []
  type: TYPE_TB
- en: 'Let’s kick off the effort of cleaning up the data set for machine learning
    by finding out the number of the timestamps with NULL values in the origindatetime_tr
    column using the following query from your shell, assuming you executed the Python
    awscli_to_df() function from listing 3.4 to output the query result using pandas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| total | null_origindate_time_total |'
  prefs: []
  type: TYPE_TB
- en: '| 67435888 | 14262196 |'
  prefs: []
  type: TYPE_TB
- en: For brevity the upcoming code snippets will no longer remind you to run source
    utils.sh ; athena_query_to_pandas or awscli_to_df().
  prefs: []
  type: TYPE_NORMAL
- en: Recall that the SQL COUNT(*) function[⁹](#pgfId-1051295) returns the count of
    both NULL and non-NULL values. However, since the WHERE clause of the SQL query
    restricts the output to the rows where origindatetime_tr is NULL, the output of
    the SQL query reports that 14,262,196 rows are NULL out of the total 67,435,888
    rows in the entire data set.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond ensuring that the origindatetime_tr values are non-NULL, it is also critical
    to confirm the values comply with the regular expression definition for valid
    timestamp values. In practice this means that it should be possible to parse the
    non-NULL values of the origindatetime_tr column into relevant elements of a timestamp,
    including year, month, day, hour, and minute.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, you do not have to implement the regular expression parsing rules
    to process the date/times. The following SQL query takes the difference between
    the total number of the rows in the data set and the number of the origindatetime_tr
    values that are not NULL and can be correctly parsed using the SQL DATE_PARSE
    function which uses the %m/%d/%Y %H:%i format in the DC taxi data set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| origindatetime_not_parsed |'
  prefs: []
  type: TYPE_TB
- en: '| 14262196 |'
  prefs: []
  type: TYPE_TB
- en: Since the difference returned by the statement is also equal to 14,262,196,
    this means that all but the NULL values of the timestamp can be parsed. Also,
    notice that the SQL statement uses a SQL subquery to compute the total number
    of rows in the data set, including both NULL and non-NULL values because the subquery
    does not include a WHERE clause. The WHERE clause at the ending of the outer SQL
    query applies only to the calculation of the COUNT of the values that can be correctly
    parsed by the DATE_PARSE function.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s continue to apply the validation rules to the origin and destination locations.
    Since in the use case the latitude and longitude coordinates for the origin and
    destination locations are non-nullable, let’s explore the impact of the validation
    rules on the coordinate values as shown next.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.5 How often parts of the pickup location coordinate are missing
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| percentage_null | either_null | both_null |'
  prefs: []
  type: TYPE_TB
- en: '| 14.04 | 9469667 | 9469667 |'
  prefs: []
  type: TYPE_TB
- en: According to the results of the query, in the data set, origin_block_latitude
    and origin_block_latitude are missing in pairs (i.e., both are NULL values if
    either is NULL) in 9,469,667 rows, or roughly 14.04% of the data set.
  prefs: []
  type: TYPE_NORMAL
- en: 'A similar analysis of the destination coordinates uses the following SQL statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This results in
  prefs: []
  type: TYPE_NORMAL
- en: '| percentage_null | either_null | both_null |'
  prefs: []
  type: TYPE_TB
- en: '| 19.39 | 13074278 | 13074278 |'
  prefs: []
  type: TYPE_TB
- en: which shows that 13,074,278 rows have destination coordinates with NULL values,
    which is roughly 19.39% of the entire data set.
  prefs: []
  type: TYPE_NORMAL
- en: 'The fractions of the NULL values for the origin and destination coordinates
    are clearly significant. In the potential worst case of missing values, you could
    find that 42.4% (i.e., 24.59% + 17.81%) of the rows had either the origin or destination
    coordinates missing. However, in the data set, a large portion of the missing
    values overlap, meaning that if either origin or destination is NULL, the other
    coordinate is NULL as well. You can find the count and the fraction of the missing
    coordinates using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This results in
  prefs: []
  type: TYPE_NORMAL
- en: '| total | percent |'
  prefs: []
  type: TYPE_TB
- en: '| 16578716 | 24.58 |'
  prefs: []
  type: TYPE_TB
- en: which shows that 24.58%, or 16,578,716 rows, in the data set do not have a useful
    pair of origin and destination coordinates. Since the pickup and the drop-off
    locations are a required part of the taxi fare estimation service specification,
    let’s focus the data quality efforts on the remaining 75.42% of the rows with
    usable pickup and drop-off coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2 Cleaning up invalid fare amounts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section walks you though the SQL statements to analyze the fare_amount
    column and enforce validation rules for the column values.
  prefs: []
  type: TYPE_NORMAL
- en: The PySpark job that populated the dc_taxi_parquet table performed some validation
    processing on the original data set. If you query Athena for the schema of the
    table, notice that the values needed for the project exist as both string and
    double types. Having both types means that in the cases when a value cannot be
    converted to the desired DOUBLE type (e.g., when a value cannot be parsed as a
    double), the original value is preserved and available for troubleshooting of
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: According to the schema specifications described in chapter 2, every taxi trip
    record must have non-NULL values in the fare amount, trip start timestamp, and
    origin and destination latitude/longitude coordinates. Let’s start by investigating
    the instances where the fareamount_double column contains NULL values, which are
    not allowed according to the schema. Since the fareamount_string column is a source
    of information for fare amount values that failed the parsing from STRING to DOUBLE,
    you can learn more about the problem values using the following SQL statement.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.6 Values of the fareamount_string column that failed to parse as doubles
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| fareamount_string | rows | percent |'
  prefs: []
  type: TYPE_TB
- en: '| NULL | 334964 | 0.5 |'
  prefs: []
  type: TYPE_TB
- en: The SQL statement in listing 3.6 filters the set of the values of the fareamount_string
    to focus only on the cases where PySpark failed to parse the fare amount, or more
    precisely on the rows where fareamount_double (the column containing the outputs
    for the parsing algorithm) has a NULL value while the fareamount_string (the column
    containing the inputs to the parsing algorithm) is not a NULL value.
  prefs: []
  type: TYPE_NORMAL
- en: 'According to the output of the query, there are 334,964 such entries where
    the parse failed. All correspond to the case where fareamount_string is equal
    to a string value of ''NULL''. This is good news because only about 0.5% of the
    data set is impacted by this problem, and there is no additional work to be done:
    the ''NULL'' values cannot be converted to DOUBLE. Had the output of listing 3.6
    found cases where some of the DOUBLE values were not parsed because they contained
    extra characters, such as in strings ''$7.86'', it would have been necessary to
    implement additional code to correctly parse such values to DOUBLE.'
  prefs: []
  type: TYPE_NORMAL
- en: To continue the search for invalid fareamount values, it is worthwhile to explore
    some of the summary statistics of the fareamount_double column. The following
    SQL query moves the summary statistics computation into a separate subquery using
    two WITH clauses. Note that the data-specific query is packaged as a subquery
    named src and with the stats subquery referencing the result from src.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.7 Reusable pattern for de-coupling the statistics query from the data
    query
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| min | q1 | q2 | q3 | max |'
  prefs: []
  type: TYPE_TB
- en: '| –2064.71 | 7.03 | 9.73 | 13.78 | 745557.28 |'
  prefs: []
  type: TYPE_TB
- en: 'Based on the minimum value in the data set reported by the output of the query
    in listing 3.7, it should be clear that the data set is impacted by a category
    of values that are not valid: the taxi fares should not be negative or less than
    $3.25\. Recall from the review of the DC taxi business rules in chapter 2 that
    the minimum charge for a taxi ride in DC is $3.25\. Let’s find the percentage
    of the data set impacted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| percent |'
  prefs: []
  type: TYPE_TB
- en: '| 0.49 |'
  prefs: []
  type: TYPE_TB
- en: The output indicates that only 0.49% of the rows are impacted by the fare amount
    values that are negative or below the minimum threshold, so they can be readily
    ignored by the analysis. From the validation standpoint this means that implementation
    of the validation rules should be modified to use the values greater than or equal
    to 3.25\.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.3 Improving the accuracy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, let’s take a closer look at the accuracy of the NULL values
    by comparing them against a reference data source of trip mileage values. As you
    learned in the previous section, the NULL values for taxi fare add up to just
    0.5% of the DC taxi data set. Using reference data in the mileage_double column
    can help you better understand the cases when the mileage of the trip translates
    into NULL fare amounts.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.8 Summary statistics for the mileage_double values
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| fareamount_string | min | q1 | q2 | q3 | max |'
  prefs: []
  type: TYPE_TB
- en: '| NULL | 0.0 | 0.0 | 1.49 | 4.79 | 2591.82 |'
  prefs: []
  type: TYPE_TB
- en: The SQL statement in listing 3.8 reports the summary statistics (including minimum,
    maximum, and quartile values) for the mileage column only for the cases where
    the fareamount_string failed to parse, or more specifically where it is equal
    to 'NULL'. The output of the query indicates that more than a quarter of the cases
    (the lower quartile, the range from the minimum value up to 25th percentile) correspond
    to trips of 0 miles. At least a quarter of the mileage values (between the middle
    and upper quartiles, the range that includes 50th and 75th percentiles) appear
    to be in a reasonable mileage range for DC taxis.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you may consider several data augmentation experiments to try
    to recover missing fareamount_double data values by computing an estimate of the
    fare from the mileage column. The experiments could replace the missing fare amount
    values using an estimate. For example, you could replace the NULL fare amounts
    in the cases where the mileage is in the middle quartile with the arithmetic mean
    (average) fareamount for the known fare amounts in the same range. More sophisticated
    estimators, including machine learning models, are also possible.
  prefs: []
  type: TYPE_NORMAL
- en: However, since the output in listing 3.8 indicates that it would help address
    roughly 0.12% (= 0.25 * 0.49%) of the data set, these experiments are unlikely
    to have a significant impact on the overall performance of the fare estimation
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Based on the output of the query in listing 3.7, the maximum value for the fare
    amount appears to be a garbage data point. Yet from the standpoint of the data
    schema it is valid, as 745,557.28 is less than the maximum value of the SQL DOUBLE
    data type.
  prefs: []
  type: TYPE_NORMAL
- en: Addressing the issue with the upper bound for fare amount values requires an
    application of an accuracy rule. Recall that validation checks should be performed
    without a reference to an external data source.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of the DC taxi data set, the maximum fare amount is not explicitly
    specified as a business rule. However, using some commonsense reasoning and reference
    data outside of the DC taxi data set, you can come up with some sensible upper
    bounds on the maximum fare amount:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Estimate 1*. The maximum fare amount depends on the miles driven per work
    shift of a taxi driver. A quick internet search tells us that a DC taxi driver
    is required to take at least 8 hours of a break from driving within every 24-hour
    period. So, hypothetically, a driver may drive a maximum of 16 hours. According
    to websites of DC, Maryland, and Virginia, the maximum speed limit across those
    areas are capped at 70 mph. Even in the absurd case where the driver is driving
    16 hours at the maximum speed limit, the maximum distance travelled during that
    time is 1,120 miles.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clearly a 1,120-mile taxi ride with an estimated fare of **$2,422.45** (1,120
    miles * $2.16/mile + $3.25 base fare) is a hypothetical upper boundary that will
    not translate to an accurate DC taxi fare amount. However, instead of throwing
    out this estimate, the right thing to do is to take it under advisement and refine
    it by aggregating it with more estimates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Estimate 2*. Instead of focusing on the distance traveled, you can also estimate
    the maximum fare amount based on time. Consider that according to the DC taxi
    fare rules, a taxi can be hired at $35 per hour. Since the maximum amount of time
    a cabbie is permitted to work is 16 hours, you can calculate another, distance-independent,
    estimate for the upper bound of the fare amount at $560 = 16 hour * $35/hour.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Estimate 3*. An upper bound on the taxi fare can also be based on the distance
    of a trip between the two furthest corners of the locations in the data set. The
    DC taxi data set boundary described in chapter 2 is roughly a square with downtown
    DC in the middle. You can find the locations of the lower-left and the upper-right
    points on the square using the following query:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_UL
  type: TYPE_PRE
- en: 'This results in the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| lower_left_latitude | lower_left_longitude | upper_right_latitude | upper_right_longitude
    |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| 38.81138 | –77.113633 | 38.994909 | –76.910012 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: Plugging the latitude and longitude coordinates reported by the query into OpenStreetMap
    ([http://mng.bz/zEOZ](http://mng.bz/zEOZ)) yields 21.13 miles, or an estimate
    of **$48.89** (21.13 * $2.16/mile + $3.25).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Estimate 4*. For yet another estimation technique, recall that according to
    the central limit theorem (CLT) of statistics, the sum (and consequently the average)
    of arithmetic means of random samples[^(10)](#pgfId-1055007) of fare amount values
    is distributed according to the Gaussian (bell-shaped) distribution. You can generate
    a thousand random samples of arithmetic means of taxi mileages from the data (so
    you can later compute their mean) using an SQL statement.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Listing 3.9 Unzipped files of the DC taxi trips data set from 2015 to 2019
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Note the complex logic in the GROUP BY portion of the statement. The objectid
    column in the data set contains a unique identifier for every row of data represented
    with sequentially ordered integer values. You could use a GROUP BY MOD(CAST(objectid
    AS INTEGER), 1000) clause instead of the version in listing 3.9\. However, if
    the objectid values are based on the original order of the taxi trips in the data
    set, each resulting sample would be made of mileage values that are exactly 1,000
    rows apart from each other in the data set. Such an ordered, interval-structured
    sampling may introduce unintended bias in the calculations. For example, if there
    are roughly 1,000 taxi rides in DC per hour, and trains from DC to New York City
    leave the train station at the top of every hour, then some samples will contain
    primarily taxi rides terminating at the train station. Other regularly spaced
    samples may consist of too many end-of-day taxi rides.
  prefs: []
  type: TYPE_NORMAL
- en: 'Random sampling (based on pseudorandom values used in computing) can address
    the bias problem of sampling over regular intervals. However, using a pseudorandom
    number generator to group values as in the following GROUP BY clause has several
    disadvantages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'First, it is impossible to exactly reproduce the results of the sampling since
    the random number generator does not guarantee deterministic behavior: there is
    no way to specify a random number seed that will guarantee a sequence of identical
    pseudorandom values across multiple executions of the SQL statement.'
  prefs: []
  type: TYPE_NORMAL
- en: Second, even if you attempted to pre-compute pseudo-random identifiers for every
    row in the data set and save the rows along with the identifiers to a separate
    table for future re-use, the table would soon become out of date. For example,
    if the DC taxi data set expanded to include 2020 taxi rides, a subsequent Glue
    crawler indexing of the data would invalidate the source data table and force
    re-creation of new pseudo-random identifiers.
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast, the approach used in listing 3.9 and shown here has the advantages
    of the pseudorandom shuffling of the data set, eliminating unintended bias, and
    produces identical results across queries regardless of additions to the data
    set, as long as each row of data can be uniquely identified:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: In the SQL statement, the application of the functions to objectid play the
    role of the unique identifier. The combination of the xxhash64 hashing function
    and the from_big_endian_64 produces what is effectively a pseudorandom but deterministic
    value from objectid.
  prefs: []
  type: TYPE_NORMAL
- en: As a visual confirmation that the averages of the fare amount samples generated
    in listing 3.9 approximate the Gaussian distribution, the following histogram
    in figure 3.3 is a plot based on the listing with an arbitrary choice of the pseudorandom
    number seed value.
  prefs: []
  type: TYPE_NORMAL
- en: '![03-03](Images/03-03.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 Random sampling in listing 3.9 relies on the CLT for estimation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that the original intention behind using 1,000 random samples of mean
    values in the average_mileage column was to compute the mean of the samples. Since
    in a normal distribution roughly 99.99% of the values are within four standard
    deviations away from the mean, the following SQL statement yields another statistical
    estimate for the upper bound on the taxi ride mileage and consequently another
    estimate for the upper bound on the fare amount:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This execution yields approximately 12.138 miles, or roughly **$29.47** (12.01
    * $2.16/mile + $3.25) as yet another upper-bound fare estimate. Of course, the
    advantage of the statistical approach explained in this section is that it can
    be used directly with the fareamount_double column, as in the following SQL statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: This yields an upper bound of $15.96.
  prefs: []
  type: TYPE_NORMAL
- en: While you can continue exploring alternative ideas for estimates, this is a
    good point to stop and evaluate the average upper bound for the fare amount so
    far.
  prefs: []
  type: TYPE_NORMAL
- en: Using a simple averaging implementation in Python
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: shows that the estimated upper bound for taxi fare is $179.75
  prefs: []
  type: TYPE_NORMAL
- en: 'While it is certainly possible to continue working on ideas for a better upper
    bound, let’s estimate how much data is left after using the upper bound of $179.75:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| percent |'
  prefs: []
  type: TYPE_TB
- en: '| 0.48841 |'
  prefs: []
  type: TYPE_TB
- en: Note that only about 0.49% percent of the data was excluded based on the bounds.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, rerunning the summary statistics on the fareamount_double column using
    the new bounds produces significantly more sensible summary statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| min | q1 | q2 | q3 | max | mean | std |'
  prefs: []
  type: TYPE_TB
- en: '| 3.25 | 7.03 | 9.73 | 13.78 | 179.83 | 11.84 | 8.65 |'
  prefs: []
  type: TYPE_TB
- en: Now that the accuracy checks for the fareamount column are done, you should
    be ready to repeat the accuracy exercise with the pickup and drop-off coordinates.
    While it is possible to determine whether the latitude and longitude coordinates
    are valid based on their value alone, you need a reference data source to decide
    whether a value is accurate. The OpenStreetMap service used to generate the DC
    taxi map in chapter 2 can also be used to confirm the accuracy of the origin and
    destination coordinates in the data set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the SQL statement and OpenStreetMap ([http://mng.bz/01ez](http://mng.bz/01ez))
    to check the minimum and maximum coordinates for the origin latitude and longitude
    columns confirms that resulting pairs (38.81138, —77.113633) and (38.994217, —76.910012)
    are within DC boundaries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This outputs the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| olat_min | olon_min | olat_max | olon_max | dlat_min | dlon_min | dlat_max
    | dlon_max |'
  prefs: []
  type: TYPE_TB
- en: '| 38.81138 | –77.113633 | 38.994909 | –76.910012 | 38.81138 | –77.113633 |
    38.994217 | –76.910012 |'
  prefs: []
  type: TYPE_TB
- en: 3.4 Implementing VACUUM in a PySpark job
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, you will use what you learned about data quality in the DC
    taxi data set and apply your findings to implement a PySpark job. The purpose
    of the job is to perform high-throughput data cleanup of the dc_taxi_parquet table
    populated in chapter 2 using a distributed cluster of Apache Spark servers available
    from AWS Glue. The job should be implemented as a single Python file named dctaxi_parquet_vacuum.py;
    however, in this section, the file is split into separate code snippets, which
    are explained one by one in the upcoming paragraphs. The cleaned-up copy of the
    data set will be saved by the job to the parquet/vacuum subfolder in your S3 bucket.
  prefs: []
  type: TYPE_NORMAL
- en: The initial part of the code snippet for the PySpark job is in listing 3.10\.
    Note that the lines of code up until ❶ are identical to the code from the PySpark
    job in chapter 2\. This should not come as a surprise because this part of the
    code involves the import of prerequisite libraries and assignment of commonly
    used variables in PySpark jobs. The line of code annotated with ❶ is the first
    that’s distinct from the chapter 2 PySpark job. Note that the line is reading
    the Parquet-formatted data set you created at the end of chapter 2 and have been
    querying using Athena in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.10 PySpark DataFrame reading code in dctaxi_parquet_vacuum.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Read the source Parquet data set into a Spark DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: To select the subset of the data for cleanup, the Spark DataFramecreateOrReplaceTempView
    method is invoked from the line with ❶ in listing 3.11\. The method creates a
    temporary view named dc_taxi_parquet as part of the SparkSession, which is accessible
    via the spark variable. The view enables Spark to query the DataFrame created
    at ❶ using the SQL query that starts on the line with ❷ referencing the dc_taxi_
    parquet view ❸.
  prefs: []
  type: TYPE_NORMAL
- en: The content of the WHERE clause that begins at ❹ should not come as a surprise.
    The checks for NULL values and the range bounds for the fareamount_double column
    are exactly the condition defined in section 3.3.
  prefs: []
  type: TYPE_NORMAL
- en: The call to the method replace at ❺ replaces any instances of newline characters
    in the multiline string with an empty character. The replace method is needed
    to ensure that the multiline string used to specify the SQL query in the PySpark
    job is compatible with the SQL query parser used by Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.11 PySpark data cleanup implementation saved to dc_taxi_vacuum.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Alias the source data set in df as dc_taxi_parquet for Spark SQL API.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Create a DataFrame query_df populated based on the SQL query in this snippet.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Query dc_taxi_parquet to output the clean values for further analysis.
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Filter records according to the VACUUM analysis in section 3.3.
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Eliminate newline characters in the Python multiline string for Spark SQL
    API compatibility.
  prefs: []
  type: TYPE_NORMAL
- en: Since the original STRING-formatted column origindatetime_tr in the data set
    needs to be formatted as a numeric value for machine learning, the PySpark DataFrame
    API code in listing 3.12 first converts the column to a SQL TIMESTAMP ❶, eliminating
    any NULL values that may have been produced due to failed conversion from STRING
    to TIMESTAMP. The derived column is then further broken up into numeric, INTEGER
    columns, including year, month, day of the week (dow), and hour of the taxi trip.
    The last step following the conversion removes the temporary origindatetime_ts
    column, drops any records with missing data, and eliminates duplicate records.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.12 PySpark data cleanup implementation saved to dc_taxi_vacuum.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Parse the trip origindatetime_tr timestamp using the dd/MM/yyyy HH:mm pattern.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Construct numeric columns based on year, month, day of the week, and hour
    of trip.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Eliminate any records with missing or duplicated data.
  prefs: []
  type: TYPE_NORMAL
- en: The concluding part of the PySpark job, shown in listing 3.13, persists the
    resulting PySpark DataFrame as a Parquet-formatted data set in the AWS S3 location
    specified by the BUCKET_DST_PATH parameter. Note that the listing declares a save_stats_metadata
    function, which computes the summary statistics (using the PySpark describe function)
    of the cleaned-up data set, and saves the statistics as a single CSV file located
    in a AWS S3 subfolder named .meta/stats under the S3 location from the BUCKET_DST_
    PATH parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.13 PySpark data cleanup implementation saved to dc_taxi_vacuum.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Persist the cleaned-up data set to BUCKET_DST_PATH in Parquet format.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Save the metadata about the cleaned-up data set as a separate CSV file.
  prefs: []
  type: TYPE_NORMAL
- en: For convenience, the entire PySpark job described is shown. Prior to executing
    this job, make sure you save the contents of the code listing to a file named
    dc_taxi_ vacuum.py.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.14 PySpark data clean-up code saved to dc_taxi_vacuum.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The utils.sh script file first introduced in section 3.3 includes bash functions
    to simplify execution of PySpark jobs in AWS Glue from your bash shell. Notice
    that in listing 3.15 the PySpark job from listing 3.14 is referenced by the file
    name dctaxi_ parquet_vacuum.py and is used to start the AWS Glue job named dc-taxi-parquet-vacuum-job.
    The job uses the Parquet-formatted DC taxi data set that you analyzed earlier
    in this chapter to populate the parquet/vacuum subfolder of your AWS S3 bucket
    with a cleaned-up version of the data. The VACUUM-ed version is also persisted
    in the Parquet format.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.15 Using bash to launch the PySpark job in dctaxi_parquet_vacuum.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Assuming that the PySpark job from listing 3.15 completes successfully, you
    should observe an output resembling the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An interactive query service like AWS Athena can help with exploratory data
    analysis of structured data sets ranging from gigabytes to terabytes in size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The schema-on-read approach enables interactive query services to apply multiple,
    different data schemas to the same data set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VACUUM principles can help your machine learning project develop mature data
    quality practices compared to the garbage in, garbage out approach.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An interactive query service, such as the PrestoDB-based AWS Athena and distributed
    data-processing platform Apache Spark-based AWS Glue, can be used to implement
    VACUUM principles for data sets stored in a public cloud.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ^(1.)While a 2016 survey ([http://mng.bz/Mvr2](http://mng.bz/Mvr2)) claimed
    that 60% of the data scientists’ time is spent addressing data quality issues,
    a more recent and larger survey brought the estimate down to 15% ([http://mng.bz/
    g1KR](http://mng.bz/g1KR)). For more insights about these oft-cited statistics,
    check out [http://mng.bz/ePzJ](http://mng.bz/ePzJ).
  prefs: []
  type: TYPE_NORMAL
- en: ^(2.) The CSV file for the sample data set of five DC taxi rides is available
    from [http://mng.bz/OQrP](http://mng.bz/OQrP).
  prefs: []
  type: TYPE_NORMAL
- en: '^(3.)Examples include Google BigQuery: [https://cloud.google.com/bigquery](https://cloud.google.com/bigquery).'
  prefs: []
  type: TYPE_NORMAL
- en: '^(4.)In 2016, an influential *Harvard Business Review* article cited another
    study, “Bad Data Costs the U.S. $3 Trillion Per Year”: [http://mng.bz/Yw47](http://mng.bz/Yw47).'
  prefs: []
  type: TYPE_NORMAL
- en: '^(5.)The Payment Card Industry Data Security Standard specifies, among other
    things, the requirements for storing cardholder data: [https://www.pcicomplianceguide.org/faq/#1](https://www.pcicomplianceguide.org/faq/#1).'
  prefs: []
  type: TYPE_NORMAL
- en: '^(6.)The Luhn algorithm is named after IBM scientist Hans Peter Luhn: [https://spectrum.ieee.org/hans-peter-luhn-and-the-birth-of-the-hashing-algorithm](https://spectrum.ieee.org/hans-peter-luhn-and-the-birth-of-the-hashing-algorithm).'
  prefs: []
  type: TYPE_NORMAL
- en: '^(7.)*Chesterton Tribune*, a daily newspaper in Indiana, published an article
    about the Valparaiso debacle: [http:// mng.bz/GOAN](http://mng.bz/GOAN).'
  prefs: []
  type: TYPE_NORMAL
- en: ^(8.) The timestamp is stored as a string using the month/day/year hour:minute
    format.
  prefs: []
  type: TYPE_NORMAL
- en: '^(9.)The warning about SELECT * that you read about in this chapter does not
    apply to COUNT(*) since the two are fundamentally different operations in SQL:
    the former returns the values from all columns for every row, while the latter
    returns just the row count.'
  prefs: []
  type: TYPE_NORMAL
- en: ^(10.)The sample size is expected to consist of at least a few tens of records,
    and the records should be independent and sampled with replacement.
  prefs: []
  type: TYPE_NORMAL
