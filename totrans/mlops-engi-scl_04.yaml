- en: 3 Exploring and preparing the data set
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 探索和准备数据集
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容包括
- en: Getting started with AWS Athena for interactive querying
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用AWS Athena进行互动查询入门
- en: Choosing between manually specified and discovered data schemas
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在手动指定数据模式和发现数据模式之间进行选择
- en: Approaching data quality with VACUUM normative principles
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用VACUUM规范原则处理数据质量
- en: Analyzing DC taxi data quality through interactive querying
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过互动查询分析DC出租车数据质量
- en: Implementing data quality processing in PySpark
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在PySpark中实现数据质量处理
- en: In the previous chapter, you imported the DC taxi data set into AWS and stored
    it in your project’s S3 object storage bucket. You created, configured, and ran
    an AWS Glue data catalog crawler that analyzed the data set and discovered its
    data schema. You also learned about the column-based data storage formats (e.g.,
    Apache Parquet) and their advantages over row-based formats for analytical workloads.
    At the conclusion of the chapter, you used a PySpark job running on AWS Glue to
    convert the original, row-based, comma-separated values (CSV) format of the DC
    taxi data set to Parquet and stored it in your S3 bucket.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，将DC出租车数据集导入AWS，并将其存储在项目的S3对象存储桶中。您创建、配置并运行了一个AWS Glue数据目录爬虫，分析了数据集并发现了其数据模式。您还学习了基于列的数据存储格式（例如Apache
    Parquet）及其在分析工作负载中相对于基于行的格式的优势。在章节的结尾，您使用在AWS Glue上运行的PySpark作业将DC出租车数据集的原始基于行的逗号分隔值（CSV）格式转换为Parquet，并将其存储在S3存储桶中。
- en: In this chapter, you will learn about Athena, another serverless feature of
    AWS that is going to prove valuable for the analysis of the DC taxi rides data
    set using Standard Query Language (SQL). You will use Athena to start with exploratory
    data analysis (EDA) and identify some of the data quality issues that exist in
    the data set. Next, you will learn about VACUUM, an acronym for a set of normative
    principles about data cleaning and data quality for effective machine learning.
    Following the VACUUM principles, you will explore the data quality issues that
    exist in the DC taxi data set and learn about using Athena to repeatably and reliably
    sample subsets of the entire DC taxi data set for analysis. Finally, you will
    implement a PySpark job to create a clean, analysis-ready version of the data
    set.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习关于Athena的内容，这是AWS的另一个无服务器功能，将使用标准查询语言（SQL）对DC出租车出行数据集进行分析将证明其价值。您将使用Athena开始探索性数据分析（EDA），并识别数据集中存在的一些数据质量问题。接下来，您将了解VACUUM，这是一个关于数据清理和数据质量的一组规范原则的缩写词，用于有效的机器学习。遵循VACUUM原则，您将探索DC出租车数据集中存在的数据质量问题，并学习使用Athena来重复和可靠地对整个DC出租车数据集的子集进行抽样分析。最后，您将实现一个PySpark作业，创建一个干净且可以进行分析的数据集版本。
- en: In addition, you will learn about and practice the basics of data quality for
    tabular data sets,[¹](#pgfId-1011829) an important aspect of an effective machine
    learning project. While working on data quality, you will learn about the principles
    behind data quality for machine learning and how to apply them using SQL and PySpark
    on your machine learning platform.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您将学习有关表格数据集数据质量的基础知识，并在有效的机器学习项目中进行实践，这是一个重要的方面。在处理数据质量时，您将了解机器学习数据质量背后的原则，以及如何在机器学习平台上使用SQL和PySpark应用它们。
- en: 3.1 Getting started with interactive querying
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 进行互动查询的入门
- en: This section starts by providing you with an overview of the use cases for data
    queries as opposed to the data-processing jobs used to transform CSV to Parquet
    in chapter 2\. Then, as you are introduced to Athena, an interactive query service
    from AWS, you will learn about the advantages and disadvantages of using a *schema-on-read
    approach* to structured data querying, prepare to experiment with a sample taxi
    data set, and apply alternative schemas to that data set. By the conclusion of
    the section, the objective is to get you ready to use a browser-based interface
    to AWS Athena and to explore data quality issues in the DC taxi fare data set.
    As you are working on the implementation in this section, you are going to start
    picking up the skills needed for exploratory data analysis of the DC taxi fare
    data set and start practicing the kinds of queries that will become useful when
    working on improving its data quality.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本节首先概述了数据查询的用例，与第2章中用于将CSV转换为Parquet的数据处理作业相对应。然后，当您介绍AWS的交互式查询服务Athena时，您将了解使用*模式读取方法*进行结构化数据查询的优缺点，并准备尝试使用示例出租车数据集，并将替代方案应用于该数据集。到本节结束时，您将准备好使用AWS
    Athena的基于浏览器的界面，并探索DC出租车车费数据集中的数据质量问题。在本节实现中，您将开始掌握对DC出租车车费数据集的探索性数据分析所需的技能，并开始练习在改进数据质量时有用的查询类型。
- en: 3.1.1 Choosing the right use case for interactive querying
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.1 选择交互式查询的正确用例
- en: This section clarifies the distinction between I/O-intensive and compute-intensive
    workloads as well as how to choose from technologies like AWS Glue, AWS Athena,
    Google Cloud DataProc, or Google Cloud BigQuery for these two categories of workloads.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本节澄清了I/O密集型和计算密集型工作负载之间的区别，以及如何从AWS Glue、AWS Athena、Google Cloud DataProc或Google
    Cloud BigQuery等技术中选择这两类工作负载。
- en: To develop an intuition about when to use interactive query services, it is
    valuable to first introduce a distinction between high throughput versus low latency
    in data processing. Recall that both row-oriented formats (like CSV) and column-oriented
    formats (like Parquet) can be used to store a structured data set organized into
    tables of rows and columns. This book uses the term *record* to describe a single
    row of data from a structured data set. Describing data sets in terms of records
    as opposed to rows helps to avoid confusion about whether the data is stored in
    a row- or a column-oriented format. In other words, a record is independent of
    the underlying data storage format.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 要对何时使用交互式查询服务有直觉，首先要介绍数据处理中高吞吐量与低延迟的区别是很有价值的。请记住，既可以使用面向行的格式（如CSV），也可以使用面向列的格式（如Parquet）来存储结构化数据集，这些数据集被组织成行和列的表。本书使用术语*记录*来描述来自结构化数据集的单个数据行。将数据集描述为记录而不是行有助于避免混淆，即数据是存储在行向或列向格式中。换句话说，记录独立于底层数据存储格式。
- en: 'In chapter 2, you used a PySpark job hosted on AWS Glue to execute a high-throughput
    workload to migrate data records from CSV to Parquet. A distinguishing characteristic
    of high-throughput workloads is a *one-to-many* (sometimes *one-to-any*) relationship
    between input and output records: a single record used as an input to the workload
    can produce zero, one, or many output records. For example, a simple SQL statement
    that begins with SELECT * returns an output record for every input record in the
    data storage, a SELECT paired with a WHERE clause can filter a portion of records,
    while a more complex SQL statement involving a SELF JOIN can square the total
    number of records returned from a table. In practice, the one-to-many relationship
    means that the number of the output records is of the same order of magnitude
    and is not substantially different from the number of the input records. Such
    workloads can also be described as input/output intensive because the underlying
    IT infrastructure executing the workload spends more time reading from and writing
    to storage compared to the amount of time spent computing.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二章中，您使用托管在 AWS Glue 上的 PySpark 作业执行了一个高吞吐量的工作负载，以将数据记录从 CSV 迁移到 Parquet。高吞吐量工作负载的一个特点是输入和输出记录之间的
    *一对多*（有时是 *一对任意*）关系：用作工作负载输入的单个记录可能产生零个、一个或多个输出记录。例如，一个以 SELECT * 开头的简单 SQL 语句会为数据存储中的每个输入记录返回一个输出记录，带有
    WHERE 子句的 SELECT 可以过滤一部分记录，而涉及 SELF JOIN 的更复杂的 SQL 语句可以将从表中返回的记录总数平方。在实践中，一对多关系意味着输出记录的数量与输入记录的数量具有相同的数量级，并且与输入记录的数量没有实质性的不同。这样的工作负载也可以描述为输入/输出密集型，因为执行工作负载的底层
    IT 基础设施花费的时间用于读取和写入存储，而不是计算。
- en: As you observed when starting a PySpark job in chapter 2, the CSV-to-Parquet
    re-encoding workload took on the order of minutes for completion. The high latency
    of the workload (here, latency describes the duration from the start to the finish
    of the Glue job) was caused by writing a record of Parquet output for every record
    of CSV input. The high throughput of the workload describes the total quantity
    of records processed in terms of the sum of the quantities of the input and output
    records. Since time spent processing the input and output records is a sizable
    majority of the total duration of this category of the workloads, they are also
    described as input/output (I/O) intensive.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二章开始执行 PySpark 作业时，您可能已经注意到 CSV 到 Parquet 的重新编码工作量需要大约几分钟才能完成。工作量的高延迟（这里的延迟描述了
    Glue 作业从开始到结束的持续时间）是由于为每个 CSV 输入记录写入 Parquet 输出记录引起的。工作量的高吞吐量描述了以输入和输出记录的数量之和为总量来处理的记录的总数量。由于处理输入和输出记录所花费的时间占此类工作负载总持续时间的相当大比例，因此它们也被描述为输入/输出（I/O）密集型。
- en: In contrast to AWS Glue, which is designed for high-throughput workloads, interactive
    query services like Athena from AWS and BigQuery from Google are designed for
    low-latency, many-to-one (or many-to-few) workloads where many input records (think
    majority of all the records in a table) are aggregated to a few (or often to just
    one) output record. Examples of many-to-one workloads include SQL statements that
    use functions like COUNT, SUM, or AVG and other aggregation functions used with
    SQL GROUP BY clauses. Many-to-few workloads are common when identifying sets of
    unique values for a column using SQL operations like SELECT DISTINCT. The many-to-one
    and many-to-few workloads can also be described as compute intensive, because
    the underlying IT infrastructure spends more time performing compute (e.g., computing
    an arithmetic mean) than input/output operations (e.g., reading or writing data).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 与专为高吞吐量工作负载设计的 AWS Glue 相比，AWS 和 Google 的交互式查询服务（如 AWS Athena 和 BigQuery）旨在处理低延迟的多对一（或多对少）工作负载，其中许多输入记录（考虑表中的所有记录的大多数）被聚合到少数（或通常仅一个）输出记录中。多对一工作负载的示例包括使用
    COUNT、SUM 或 AVG 等函数以及与 SQL GROUP BY 子句一起使用的其他聚合函数的 SQL 语句。通过使用 SQL 操作识别列的唯一值集合，多对少工作负载在
    SELECT DISTINCT 时很常见。多对一和多对少工作负载也可以描述为计算密集型，因为底层 IT 基础设施花费更多时间执行计算（例如，计算算术平均值）而不是输入/输出操作（例如，读取或写入数据）。
- en: 3.1.2 Introducing AWS Athena
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.2 介绍 AWS Athena
- en: This section provides a high-level overview of the AWS Athena interactive query
    service and describes how Athena applies a schema-on-read approach for data processing
    and analysis.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本节概述了 AWS Athena 交互式查询服务，并描述了 Athena 如何应用基于读取的模式来进行数据处理和分析。
- en: 'Athena is a serverless query service from AWS, designed primarily for interactive
    analysis of structured and semi-structured data using ANSI SQL and SQL extensions.
    Interactive analysis means that Athena is designed to execute compute-intensive
    SQL workloads with low latency and return results within a matter of seconds.
    This also means that while it is possible to use Athena to extract, transform,
    and load (ETL) data, you should plan on using PySpark instead of Athena for your
    ETL code to support high-throughput rather than low-latency data processing. If
    you have ever used an interactive query interface for a relational database, such
    as MySQL or PostgreSQL, you know that Athena provides similar functionality. Although
    Athena is targeted at interactive analysis by end users through a browser-based
    interface, there is also support for API-based access. As a query service, Athena
    differs from traditional relational databases and data warehouses in the following
    important ways:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Athena 是 AWS 的无服务器查询服务，主要用于使用 ANSI SQL 和 SQL 扩展对结构化和半结构化数据进行交互式分析。交互式分析意味着 Athena
    被设计用于执行计算密集型 SQL 工作负载，并在几秒内返回结果。这也意味着，虽然可以使用 Athena 提取、转换和加载（ETL）数据，但你应该计划使用 PySpark
    而不是 Athena 编写 ETL 代码，以支持高吞吐量而不是低延迟的数据处理。如果你曾经使用过关系数据库（如 MySQL 或 PostgreSQL）的交互式查询界面，你就知道
    Athena 提供了类似的功能。尽管 Athena 面向通过基于浏览器的界面进行交互式分析的最终用户，但也支持基于 API 的访问。作为查询服务，Athena
    在以下重要方面与传统的关系数据库和数据仓库有所不同：
- en: Athena relies on AWS services for data storage and does not store source data
    or metadata for queries. For example, Athena can query data sets from S3, as well
    as from MySQL, DynamoDB, or other data sources that provide Athena data source
    connectors ([http://mng.bz/p9vP](http://mng.bz/p9vP)). When data is produced as
    a result of a query, Athena stores the data to a pre-configured S3 bucket.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Athena 依赖 AWS 服务进行数据存储，并不存储查询的源数据或元数据。例如，Athena 可以查询来自 S3 的数据集，以及来自 MySQL、DynamoDB
    或其他提供 Athena 数据源连接器的数据源。当数据作为查询结果生成时，Athena 将数据存储到预配置的 S3 存储桶中。
- en: Athena software is based on an open source PrestoDB distributed query engine
    ([https://prestodb.io](https://prestodb.io)) developed in part by Facebook engineers.
    The implementation was demonstrated to scale to Facebook’s internal workloads
    involving queries over hundreds of petabytes of data.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Athena 软件基于 Facebook 工程师部分开发的开源 PrestoDB 分布式查询引擎。该实现已经被证明可以扩展到 Facebook 内部的工作负载，涉及对数百
    PB 数据的查询。
- en: Athena does not use the schema-on-write approach of traditional, relational,
    data warehouses. This means that Athena can interpret the same data according
    to mutually exclusive schema definitions; for example, Athena can query a column
    of identical data values as a string or as a number. This approach is often described
    as *schema-on-read*.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Athena 不使用传统关系型数据仓库的写入时模式。这意味着 Athena 可以根据互斥的模式定义解释相同的数据；例如，Athena 可以将相同数据值的列查询为字符串或数字。这种方法通常被描述为*读取时模式*。
- en: In chapter 2, you learned about using crawlers to discover data schemas from
    data as well as how to create databases and tables in the data catalog based on
    the discovered schemas. Athena requires that a table be defined in the data catalog
    before the service can query the data described by the table. As illustrated by
    the dashed lines on figure 3.1, Athena can be used to define schemas for data
    stored in a data storage service such as S3\. Alternatively, Athena can query
    tables defined based on the metadata discovered by a crawler.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 2 章中，你学习了如何使用网络爬虫从数据中发现数据模式，并学习了如何根据发现的模式在数据目录中创建数据库和表。Athena 要求在服务查询由表描述的数据之前，必须在数据目录中定义表。如图
    3.1 上的虚线所示，Athena 可用于为存储在数据存储服务中的数据定义模式，例如 S3。或者，Athena 可以查询根据爬虫发现的元数据定义的表。
- en: '![03-01](Images/03-01.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![03-01](Images/03-01.png)'
- en: Figure 3.1 The Athena query service can both define a schema and use one defined
    by a Glue crawler to analyze the data using alternative schema definitions for
    the same data set. Alternative and mutually exclusive schemas can help you apply
    the right schema for your specific use case.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1 Athena 查询服务既可以定义模式，也可以使用由 Glue 爬虫定义的模式来分析数据，使用相同数据集的替代模式定义。替代且互斥的模式可以帮助您为特定用例应用正确的模式。
- en: Relying on Athena to define schemas for a table in the data catalog has both
    advantages and disadvantages. Since many real-world data sets stored in data warehouses
    and used for machine learning are wide (consisting of many columns), defining
    a table schema using Athena translates to the effort required to explicitly specify
    SQL data types for each column in the schema. Although it may seem that the effort
    is limited in scope, keep in mind that the schema definition needs to be maintained
    and updated whenever there is a change in the underlying data set. However, if
    you need to be able to query the same data using data schemas containing mutually
    exclusive data types, then using Athena to define schemas is the right option.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖 Athena 为数据目录中的表定义模式既有优势也有劣势。由于许多存储在数据仓库中并用于机器学习的真实世界数据集是宽的（包含许多列），因此使用 Athena
    定义表模式意味着需要为模式中的每一列显式指定 SQL 数据类型所需的工作量。虽然看起来工作量是有限的，但请记住，模式定义需要在基础数据集发生变化时进行维护和更新。然而，如果你需要能够使用包含互斥数据类型的数据模式查询相同的数据，那么使用
    Athena 定义模式就是正确的选择。
- en: In contrast, if you use the crawler-based schema definition approach, you don’t
    need to explicitly specify the data types as they are automatically discovered
    by the crawler. The crawler can also be scheduled to run periodically, updating
    the schema definition based on changes in the data. The downside of using the
    crawler is relevant when you need to query data using an alternative data schema
    with differences from the automatically discovered one. In the crawler-based approach,
    this translates to either using Athena to define the alternative schema or implementing
    a PySpark job that applies the alternative schema to the data set. Recall that
    the PySpark job that you implemented at the conclusion of chapter 2 re-encoded
    STRING data types (for example, for fare amount) to DOUBLE.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，如果你使用基于爬虫的模式定义方法，你不需要显式指定数据类型，因为它们会被爬虫自动发现。爬虫还可以定期运行，根据数据的变化更新模式定义。使用爬虫的缺点在于，当你需要使用与自动发现的模式不同的替代数据模式来查询数据时，它就显得不那么相关了。在基于爬虫的方法中，这意味着要么使用
    Athena 定义替代模式，要么实施一个 PySpark 作业，将替代模式应用于数据集。请记住，在第 2 章结束时你实施的 PySpark 作业重新编码了
    STRING 数据类型（例如，对于车费金额）为 DOUBLE。
- en: 3.1.3 Preparing a sample data set
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备示例数据集
- en: In this section, you will start working with a tiny CSV data set to better understand
    the advantages and disadvantages of relying on Athena to define the schema for
    it. In the data set, the rows contain values representing a taxi trip fare amount
    as well as the latitude and longitude coordinates of the trip pickup and drop-off
    locations.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将开始使用一个小型 CSV 数据集，以更好地了解依赖 Athena 为其定义模式的优缺点。在数据集中，行包含表示出租车行程费用以及行程上车和下车位置的纬度和经度坐标的值。
- en: To begin querying this tiny data set using Athena, you need to first upload
    the corresponding CSV file, consisting of just five rows of data to a folder of
    your S3 bucket.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用 Athena 查询此小型数据集，您需要首先将相应的 CSV 文件上传到您 S3 存储桶的一个文件夹中，该文件夹只包含五行数据。
- en: 'Create a CSV file named trips_sample.csv on your local filesystem and preview
    it by executing the following bash commands:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在您本地文件系统上创建一个名为 trips_sample.csv 的 CSV 文件，并通过执行以下 bash 命令预览它：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Assuming the bash commands executed successfully, the output of cat should have
    produced an output resembling table 3.1.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 bash 命令成功执行，则 cat 的输出应该产生类似于表 3.1 的输出。
- en: Table 3.1 The type interpretation of data values in this data set[²](#pgfId-1012363)
    depends on your choice of a schema.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3.1 本数据集中数据值的类型解释[²](#pgfId-1012363)取决于你选择的模式。
- en: '| Fare amount | Origin | Destination |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 车费金额 | 起点 | 终点 |'
- en: '| Latitude | Longitude | Latitude | Longitude |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 纬度 | 经度 | 纬度 | 经度 |'
- en: '| 8.11 | 38.900769 | −77.033644 | 38.912239 | −77.036514 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 8.11 | 38.900769 | −77.033644 | 38.912239 | −77.036514 |'
- en: '| 5.95 | 38.912609 | −77.030788 | 38.906445 | −77.023978 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 5.95 | 38.912609 | −77.030788 | 38.906445 | −77.023978 |'
- en: '| 7.57 | 38.900773 | −77.03655 | 38.896131 | −77.024975 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 7.57 | 38.900773 | −77.03655 | 38.896131 | −77.024975 |'
- en: '| 11.61 | 38.892101 | −77.044208 | 38.905969 | −77.0656439 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 11.61 | 38.892101 | −77.044208 | 38.905969 | −77.0656439 |'
- en: '| 4.87 | 38.899615 | −76.980387 | 38.900638 | −76.97023 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 4.87 | 38.899615 | −76.980387 | 38.900638 | −76.97023 |'
- en: 'Next, copy the contents of the file to the samples folder in your S3 object
    storage bucket and confirm that it copied successfully by running the following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，将文件内容复制到您 S3 对象存储桶中的 samples 文件夹中，并通过运行以下命令确认它已成功复制：
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If you correctly uploaded the sample file, the output of the aws s3 ls command
    should report that it is 378 bytes in size.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正确上传了示例文件，则`aws s3 ls`命令的输出应该报告其大小为378字节。
- en: 3.1.4 Interactive querying using Athena from a browser
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.4 使用浏览器从 Athena 进行交互式查询
- en: This section introduces the browser-based graphical user interface (GUI) for
    AWS Athena. Although it is possible to use the Athena GUI to perform the queries
    used throughout this chapter, data analysis automation and reproducibility is
    more straightforward to demonstrate with a command line interface (CLI)—based
    access to the Athena API rather than with the browser. So, while this section
    covers how to use the browser-based interface, later sections focus on scripting
    CLI-based queries.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了 AWS Athena 的基于浏览器的图形用户界面（GUI）。虽然可以使用 Athena GUI 执行本章中使用的查询，但使用基于命令行界面（CLI）的
    Athena API 访问可以更直观地演示数据分析自动化和可重现性，而不是使用浏览器。因此，虽然本节涵盖了如何使用基于浏览器的界面，但后续章节将专注于脚本化基于
    CLI 的查询。
- en: To access the Athena interface from your browser, navigate to the Athena service
    using the AWS Services dropdown menu in the AWS Console top menu. You should be
    able to click through to a screen that resembles the one shown in figure 3.2.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 要从浏览器访问 Athena 界面，请使用 AWS 控制台顶部菜单中的 AWS 服务下拉菜单导航到 Athena 服务。您应该能够点击到类似于图3.2所示的屏幕。
- en: '![03-02](Images/03-02.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![03-02](Images/03-02.png)'
- en: Figure 3.2 Screen capture of the Athena’s browser-based interface illustrating
    the key components you need to know for interactive querying
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2 显示了 Athena 基于浏览器的界面的屏幕截图，说明了您需要了解的交互式查询的关键组件。
- en: Note that on the Athena interface screen you need to make sure you are accessing
    Athena from the region that matches the value of your $AWS_DEFAULT_REGION environment
    variable, the one where you have uploaded your CSV file. As with other AWS services,
    you can change the region using the dropdown menu in the upper right-hand corner
    of the AWS console top menu.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在 Athena 界面屏幕上，您需要确保您正在访问的 Athena 与您的 $AWS_DEFAULT_REGION 环境变量的值匹配的地区，也就是您上传
    CSV 文件的地区。与其他 AWS 服务一样，您可以使用 AWS 控制台顶部菜单中右上角的下拉菜单更改地区。
- en: The selection highlighted as 1 in figure 3.2 specifies the data catalog database
    you created in chapter 2\. Make sure you have dc_taxi_db selected as the database.
    Once the database is selected, confirm that in the selection highlighted as 2
    you can see the tables you created using the crawler in the dc_taxi_db database.
    The tables should be named dc_taxi_csv and dc_taxi_parquet.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2 中高亮显示的选择项 1 指定了您在第2章中创建的数据目录数据库。确保您已选择 dc_taxi_db 作为数据库。选择数据库后，请确认在高亮显示的选择项
    2 中，您可以看到您在 dc_taxi_db 数据库中使用爬虫创建的表。表应命名为 dc_taxi_csv 和 dc_taxi_parquet。
- en: 'SQL queries for Athena are specified using a tabbed SQL editor highlighted
    as 3 in figure 3.2\. If this is your first time using Athena, before running a
    query you will need to specify a query result location for the service. By default,
    the output of every query executed by Athena is saved to the query result location
    in S3\. Execute the following bash shell command and copy the output to your clipboard:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Athena 的 SQL 查询使用图3.2 中高亮显示的分栏 SQL 编辑器指定。如果这是您第一次使用 Athena，请在运行查询之前为服务指定查询结果位置。默认情况下，Athena
    执行的每个查询的输出都保存到 S3 中的查询结果位置。执行以下 bash shell 命令并将输出复制到剪贴板：
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Notice from the output of the shell command that Athena will store the query
    locations results to the athena folder in your bucket.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 从 shell 命令的输出中注意到，Athena 将查询位置结果存储到您的存储桶中的 athena 文件夹中。
- en: Before you run your first query, you should configure the S3 query result location
    by first clicking on the “set a query result location” hyperlink shown in the
    upper part of the screenshot in figure 3.2, then pasting the value you just copied
    to the clipboard into the result location text field in the dialog, and finally
    clicking Save.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在您运行第一个查询之前，您应该配置 S3 查询结果位置，首先点击图3.2中屏幕截图上部显示的“设置查询结果位置”超链接，然后将您刚刚复制到剪贴板的值粘贴到对话框中的结果位置文本字段中，最后点击保存。
- en: 3.1.5 Interactive querying using a sample data set
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.5 使用示例数据集进行交互式查询
- en: This section explains how to apply schema-on-read in Athena using the few records
    from the trips_sample.csv file. In later sections, you are going to be able to
    apply the same technique to larger data sets.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将解释如何使用 trips_sample.csv 文件中的少量记录在 Athena 中应用基于读取的模式。在后续章节中，您将能够将相同的技术应用于更大的数据集。
- en: 'Since the upcoming Athena examples rely on using a scripted, CLI-based access
    to Athena API, start by configuring Athena to use the athena folder in your S3
    bucket as the location to store results of Athena queries. This means that you
    should execute the following from your shell:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 由于接下来的 Athena 示例依赖于使用脚本化的基于 CLI 的 Athena API 访问，请从配置 Athena 开始，将 Athena 文件夹配置为您
    S3 存储桶中用于存储 Athena 查询结果的位置。 这意味着您应该从您的 shell 执行以下操作：
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Once the dc_taxi_athena_workgroup is created, you can start using Athena via
    the CLI.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 创建完 `dc_taxi_athena_workgroup` 后，您可以通过 CLI 开始使用 Athena。
- en: Since Athena is integrated with the Glue data catalog, the database and the
    schema definitions from a data catalog table can be applied at data read time
    (i.e., when querying data) as opposed to data write time. However, to illustrate
    the schema-on-read capability of Athena, instead of using a crawler to discover
    a table schema for the five sample trips, you will first pre-populate the data
    catalog using tables with manually defined schemas. The first table you will create
    treats all of the data values in trips_sample.csv as STRING data type, as shown
    in listing 3.1\. Later, you will create a second table that treats the same values
    as DOUBLE.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Athena 与 Glue 数据目录集成，因此可以在数据读取时（即查询数据时）应用数据目录表的数据库和模式定义，而不是在数据写入时。 然而，为了说明
    Athena 的模式读取功能，而不是使用爬虫来发现五个样本行程的表模式，您将首先使用手动定义的模式填充数据目录。 您将创建的第一个表将所有的数据值都视为 STRING
    数据类型，如列表 3.1 中所示。 后来，您将创建一个将相同值视为 DOUBLE 的第二个表。
- en: Listing 3.1 Defining a schema for the five DC trips data set using STRING types
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.1 使用 STRING 类型为五个 DC 行程数据集定义模式
- en: '[PRE4]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: To define the schema for the dc_taxi_db.dc_taxi_csv_sample_strings table using
    the SQL statement from listing 3.1, execute the following sequence of commands
    from your bash shell.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 使用列表 3.1 中的 SQL 语句定义 `dc_taxi_db.dc_taxi_csv_sample_strings` 表的模式，请从您的 bash
    shell 执行以下命令序列。
- en: Listing 3.2 Shell-based query of AWS Athena using AWS CLI
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.2 使用 AWS CLI 对 AWS Athena 进行基于 Shell 的查询
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Save the string-based schema definition to the SQL shell variable.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将基于字符串的模式定义保存到 SQL shell 变量中。
- en: ❷ Start the Athena query based on the contents of the SQL variable.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 根据 SQL 变量的内容启动 Athena 查询。
- en: ❸ Repeatedly check and report on whether the Athena query is finished.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 反复检查并报告 Athena 查询是否完成。
- en: At this point, based on your experience with using SQL to query relational databases,
    you might be tempted to query the dc_taxi_csv_sample_strings table using a SQL
    statement that starts with SELECT *. However, when working with columnar data
    stores, it is better to avoid SELECT * whenever possible. As you learned in chapter
    2, columnar stores maintain individual columns of data across multiple files as
    well as across different parts of files. By specifying just the names of the columns
    that you need for your query, you direct a column-aware query engine like Athena
    to process just the parts of the data you need, reducing the overall quantity
    of data processed. With Athena, as well as with serverless query services from
    other public cloud vendors,[³](#pgfId-1044495) lower amounts of data processed
    translate to lower costs. Since Athena is serverless, you are billed by AWS based
    on the amount of data processed by your Athena queries.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，根据您使用 SQL 查询关系数据库的经验，您可能会尝试使用以 `SELECT *` 开头的 SQL 语句来查询 `dc_taxi_csv_sample_strings`
    表。 然而，在处理列式数据存储时，尽可能避免使用 `SELECT *` 是更好的选择。 正如您在第 2 章中学到的，列式存储在多个文件以及文件的不同部分中维护数据的各个列。
    通过仅指定您查询所需的列的名称，您可以将像 Athena 这样的列感知查询引擎指向仅处理您需要的数据部分，从而减少处理的数据总量。 对于 Athena 以及其他公共云供应商的无服务器查询服务，处理的数据量越少，成本就越低。
    由于 Athena 是无服务器的，因此您按照 Athena 查询处理的数据量来计费。
- en: 'Also, the script in listing 3.2 is quite verbose. To keep query examples in
    this chapter concise, proceed by downloading a utils.sh script:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，列表 3.2 中的脚本相当冗长。 为了保持本章中查询示例的简洁性，请继续下载 utils.sh 脚本：
- en: '[PRE6]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Once downloaded, the script should take up 4,776 bytes on your filesystem. This
    script is loaded in the upcoming examples using the source utils.sh command and
    is invoked by passing a SQL query for Athena to the athena_query_to_table function.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 下载完成后，脚本将占用文件系统的 4,776 字节。 这个脚本在接下来的示例中使用 `source utils.sh` 命令加载，并通过向 `athena_query_to_table`
    函数传递 Athena 的 SQL 查询来调用。
- en: When Athena is querying data using the schema from the dc_taxi_csv_sample_ strings
    table you just created, the data is processed by interpreting the latitude and
    longitude coordinates as a STRING data type. Treating the coordinate values as
    strings can be useful when passing a pair of the coordinates to a web page script
    in order to display a pin on an interactive map in a browser. Notice that the
    following query does not involve any data type casting since the data is read
    by Athena from the source CSV data as a STRING. Hence, it is possible to use the
    ANSI SQL || (double vertical bar) operation directly on the data values to perform
    the concatenation operation.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当 Athena 使用您刚刚创建的 dc_taxi_csv_sample_ strings 表的方案查询数据时，数据被处理为将纬度和经度坐标解释为字符串数据类型。将坐标值视为字符串类型可在将坐标对传递给网页脚本以在浏览器中显示交互式映射上的锥标时，非常有用。请注意，以下查询不涉及任何数据类型转换，因为数据是由
    Athena 从源 CSV 数据作为 STRING 读取的。因此，可以直接在数据值上使用 ANSI SQL ||（双竖杠）操作来执行连接操作。
- en: Listing 3.3 Using STRING data type for coordinates to simplify browser-based
    use cases
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 列出 3.3 使用 STRING 数据类型为坐标简化基于浏览器的用例
- en: '[PRE7]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This results in an output resembling the following, where each row contains
    string data types, concatenating the latitude and longitude values:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致一个类似于以下内容的输出，其中每行都包含字符串数据类型，将纬度和经度值连接在一起：
- en: '| origin | destination |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| origin | destination |'
- en: '| 38.900769, –77.033644 | 38.912239, –77.036514 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 38.900769，–77.033644 | 38.912239，–77.036514 |'
- en: '| 38.912609, –77.030788 | 38.906445, –77.023978 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 38.912609，–77.030788 | 38.906445，–77.023978 |'
- en: '| 38.900773, –77.03655 | 38.896131,–77.024975 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 38.900773，–77.03655 | 38.896131，–77.024975 |'
- en: '| 38.892101000000004, –77.044208 | 38.905969, –77.06564399999999 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 38.892101000000004，–77.044208 | 38.905969，–77.06564399999999 |'
- en: '| 38.899615000000004, –76.980387 | 38.900638, –76.97023 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 38.899615000000004，–76.980387 | 38.900638，–76.97023 |'
- en: 'Alternatively, Athena can use a different schema, treating the same coordinate
    values as a floating point data type to compute the differences between the largest
    and smallest fare amounts in the data set. Execute the following Athena operation
    from your shell to create the dc_taxi_csv_sample_double table where every value
    in the trips_sample.csv file is interpreted as an SQL DOUBLE:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，Athena 可以使用不同的架构，把相同的坐标值作为浮点数据类型来处理，以计算数据集中最大和最小车费之间的差异。从 shell 中执行下面的 Athena
    操作，以创建 dc_taxi_csv_sample_double 表，其中 trips_sample.csv 文件中的每个值都被解释为 SQL DOUBLE：
- en: '[PRE8]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'After the dc_taxi_csv_sample_double table becomes available for a query, you
    can try processing the values in the source data file as doubles, for example,
    by trying to find the difference between the largest and the smallest amounts
    for the taxi fare in the five-row data set:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: dc_taxi_csv_sample_double 表可以成为查询的数据源之后，您可以尝试处理源数据文件中的值作为双精度浮点数，例如，通过尝试查找五行数据集中的最大和最小车费之间的差异：
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The athena_query_to_pandas function in the listing saves the output of the Athena
    query to a temporary /tmp/awscli.json file on your filesystem. First, define the
    Python utility function as shown in the following listing.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 列出中的 athena_query_to_pandas 函数将 Athena 查询的输出保存到文件系统上的临时/tmp/awscli.json 文件中。首先，按照下面的列表所示定义
    Python 实用程序函数。
- en: Listing 3.4 Reporting Athena results as a pandas DataFrame
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 列出 3.4 报告 Athena 结果为 pandas DataFrame
- en: '[PRE10]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then, you can conveniently preview the contents of the tmp/awscli.json file
    as a pandas DataFrame, so that calling awscli_to_df() outputs the following:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以方便地将 tmp/awscli.json 文件的内容预览为 pandas DataFrame，以便调用 awscli_to_df() 输出以下结果：
- en: '| _col0 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| _col0 |'
- en: '| 6.74 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 6.74 |'
- en: The output shows that there was a $6.74 difference between the maximum and the
    minimum values for the taxi fare in the data set. Also, since the last query did
    not use the AS keyword to assign a name for the sole column in the result, Athena
    used an automatically generated column name of _col0.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示，数据集中的出租车费用的最大值和最小值之间存在 $6.74 的差异。此外，由于最后一个查询未使用 AS 关键字为结果中唯一列分配名称，因此 Athena
    使用了自动生成的列名称 _col0。
- en: 3.1.6 Querying the DC taxi data set
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.6 查询 DC 出租车数据集
- en: This section gets you started with using AWS Athena to query the DC taxi data
    set so that in the upcoming sections you are prepared to analyze DC taxi data
    quality.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍如何使用 AWS Athena 查询 DC 出租车数据集，以便在即将到来的部分中，您可以准备分析 DC 出租车数据的质量。
- en: 'As you recall from chapter 2, the Parquet-formatted version of the DC taxi
    data was stored as the dc_taxi_parquet table in the dc_taxi_db database. Let’s
    attempt a query of 10 rows of this table using the Athena CLI:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在第2章中回忆起的那样，DC出租车数据的Parquet格式版本被存储为dc_taxi_db数据库中的dc_taxi_parquet表。让我们尝试使用Athena
    CLI查询这个表的10行数据：
- en: '[PRE11]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Don’t forget to use the awscli_to_df() function to output the result using pandas.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 不要忘记使用awscli_to_df()函数使用pandas输出结果。
- en: Due to the parallel and distributed nature of data processing performed by Athena,
    the order of rows in the dc_taxi_parquet table will be different for every execution
    of the last query. Hence, the 10 rows you will see as the output of the query
    will be different from mine. However, even with just 10 rows of the results you
    should be able to find rows with missing values. The missing values will appear
    as empty cells or None values in one or more columns.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Athena执行的数据处理是并行和分布式的，所以dc_taxi_parquet表中的行顺序在每次执行最后一个查询时都会不同。因此，你将看到的查询结果中的10行与我的不同。然而，即使只有10行的结果，你也应该能够找到包含缺失值的行。缺失值将出现在一个或多个列中的空单元格或None值中。
- en: For example, you may find that your output has missing values for the origin
    but not for the destination coordinates. In some cases, all but the fare amount
    and the trip origin date/time values will be missing in the results. The imported
    DC taxi trip data set has data quality issues.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你可能会发现你的输出缺少起点的数值，但目的地坐标没有缺失。在某些情况下，结果中除了车费和行程起点的日期/时间值之外，其他值都会缺失。导入的DC出租车行程数据集存在数据质量问题。
- en: While the transformation of the DC taxi data to the Parquet format in chapter
    2 helped you optimize query and analytics performance for working with the data,
    you have not yet performed any quality checks against the data set. In short,
    you don’t know if the data available to you can be trusted. What does it mean
    to address these quality issues? What problems should or should not be fixed?
    How much effort should you put into cleaning the data? When does the cleaned-up
    data set achieve sufficiently good quality for data analysis and machine learning?
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在第2章将DC出租车数据转换为Parquet格式有助于优化查询和分析性能，但你尚未对数据集进行任何质量检查。简而言之，你不知道可用的数据是否可信。解决这些质量问题意味着什么？应该或不应该修复哪些问题？你应该花多少精力清理数据？清理后的数据集在数据分析和机器学习方面何时达到足够好的质量？
- en: 3.2 Getting started with data quality
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 开始进行数据质量测试
- en: This part of the chapter is written differently from other sections. Whereas
    most of the chapter focuses on the technical knowledge and detailed instruction
    in a form of specific steps for working with serverless machine learning technologies,
    this part is normative rather than prescriptive. In other words, you should first
    understand what data quality for machine learning should be like, and then learn
    the steps involved in applying data quality to your machine learning data set.
    My goal for this part is to teach you the data quality criteria you should use
    across any machine learning project, regardless of the data set, so this section
    deals primarily with concepts rather than code.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的这一部分与其他部分的写作方式不同。虽然大部分章节关注于技术知识和详细指导，以特定步骤的形式与无服务器机器学习技术一起使用，但本部分是规范化的而不是指导性的。换句话说，你首先应该了解机器学习中的数据质量应该是怎样的，然后学习将数据质量应用于机器学习数据集的步骤。我希望通过这一部分教会你应该在任何机器学习项目中使用的数据质量标准，无论数据集如何，因此本节主要涉及概念而不是代码。
- en: 'Let’s face it: data cleanup is an important but not the most exciting topic
    in machine learning, so to make the data quality principles more concrete, easier
    to remember, and hopefully more interesting, this section relies heavily on real-world
    case studies and data cleanup examples you can apply to your next machine learning
    project. If you prefer to proceed directly to the practical steps of cleaning
    up the DC taxi data, feel free to jump ahead to section 3.3.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 可以说，数据清理是机器学习中重要但并非最令人兴奋的话题，为了将数据质量原则更加具体、容易记住，以及希望更有趣，这一部分主要依赖于实际案例和数据清理示例，你可以应用到下一个机器学习项目中。如果你愿意直接进入清理DC出租车数据的实际步骤，可以直接跳到3.3节。
- en: 3.2.1 From “garbage in, garbage out” to data quality
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.1 从“垃圾进垃圾出”到数据质量
- en: This subsection illustrates a rationale for addressing data quality issues and
    describes the data quality questions that are answered in the later parts of this
    chapter.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 本小节说明了解决数据质量问题的理由，并描述了本章后面部分回答的数据质量问题。
- en: “Garbage in, garbage out” is a well-known cliché in the information technology
    industry. In the context of this book, it means that if the input into your machine
    learning system is garbage, then the machine learning algorithms will train on
    garbage, and the outputs of machine learning will be garbage as well. The cliché
    points to the importance of data quality for a machine learning project, but it
    does not give evidence that garbage in, garbage out is critical or relevant to
    real-world data analysis and machine learning.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: “垃圾进，垃圾出”是信息技术行业中众所周知的陈词滥调。在本书的背景下，它意味着如果输入到您的机器学习系统中的是垃圾，那么机器学习算法将会在垃圾上进行训练，机器学习的输出也将是垃圾。这个陈词强调了对机器学习项目的数据质量的重要性，但它并没有证明垃圾进，垃圾出对于现实世界的数据分析和机器学习是至关重要或相关的。
- en: In 2010, as the global economy was still recovering from the financial meltdown
    that took place a few years prior, two Harvard economists, Carmen M. Reinhart
    and Kenneth S. Rogoff, published a research paper deconstructing policies that
    could help countries get their economies growing again. In the paper, the economists
    argued that countries that incur debt of over 90% of their gross domestic product
    (GDP) face economic declines. In part based on the analysis from the economists,
    some European Union (EU) countries adopted harsh austerity measures, slashing
    salaries and eliminating thousands of jobs. As it turned out, the data used for
    the analysis was wrong.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 2010 年，当全球经济仍在从几年前的金融危机中恢复时，两位哈佛经济学家卡门·M·莱因哈特和肯尼斯·S·罗戈夫发表了一篇研究论文，解构了可以帮助国家重新实现经济增长的政策。在这篇论文中，经济学家们认为，债务超过其国内生产总值（GDP）90%
    的国家将面临经济衰退。部分基于这些经济学家的分析，一些欧盟（EU）国家采取了严厉的紧缩政策，削减工资并裁减了数千个工作岗位。结果证明，用于分析的数据是错误的。
- en: The politicians who based their policies on the Reinhart-Rogoff results fell
    victim to the classic garbage in, garbage out problem. The Reinhart-Rogoff fiasco
    is just one instance of many where an analysis of poor quality data led to billions
    of dollars in negative consequences. Even prior to the digital transformation
    accelerated by the COVID-19 pandemic, the highly respected *Harvard Business Review*
    magazine published a notable claim that the total cost of bad data to the US economy
    should be measured in thousands of billions of US dollars.[⁴](#pgfId-1045716)
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 政客们基于莱因哈特-罗戈夫（Reinhart-Rogoff）的结果制定政策，成为经典的垃圾进垃圾出问题的受害者。莱因哈特-罗戈夫惨败只是许多情况之一，其中低质量数据的分析导致数十亿美元的负面后果。即使在
    COVID-19 疫情加速数字转型之前，备受尊敬的*哈佛商业评论*杂志也发表了一个引人注目的说法，即美国经济因糟糕数据而产生的总成本应该以数万亿美元来衡量。[⁴](#pgfId-1045716)
- en: The issue of data quality is important, but as a machine learning practitioner,
    it may not be immediately obvious that you are working with a poor quality data
    set. How do you know if your data is garbage or if it is of sufficient quality
    to perform machine learning?
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 数据质量问题很重要，但作为一名机器学习从业者，你可能不会立即意识到自己正在使用低质量的数据集。你如何知道你的数据是垃圾还是足够质量以进行机器学习？
- en: 3.2.2 Before starting with data quality
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.2 在开始处理数据质量之前
- en: This subsection helps you understand the questions that should be answered about
    any structured (tabular) data set before addressing the data quality issues in
    it.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 本小节帮助您了解在解决其中任何一个结构化（表格）数据集的数据质量问题之前应该回答的问题。
- en: 'Before you can begin to work on data quality, you need more than just a structured
    data set. You need to know answers to the kinds of questions you have already
    answered about the DC taxi data:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始处理数据质量之前，你需要的不仅仅是一个结构化数据集。你需要知道关于 DC 出租车数据的那些问题的答案：
- en: '*Can the data set be queried as one or more tables of rows and columns?* In
    other words, are you querying data stored using a structured data set format?
    Recall that in chapter 2 you looked at definitions of row- (e.g., CSV) and column-oriented
    (e.g., Apache Parquet) storage formats for structured data. Since VACUUM is a
    set of data quality principles for structured data sets, it does not apply to
    unstructured formats used for natural language text, images, audio, and video.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据集是否可以查询为一个或多个行列表？* 换句话说，你是否正在查询使用结构化数据集格式存储的数据？回想一下，在第 2 章中，你了解了用于结构化数据的行向（例如
    CSV）和列向（例如 Apache Parquet）存储格式的定义。由于 VACUUM 是用于结构化数据集的一套数据质量原则，因此它不适用于用于自然语言文本、图像、音频和视频的非结构化格式。'
- en: '*What questions do you need to answer based on which columns?* The DC taxi
    data set—based machine learning example in this book is built around the question
    “What is the value for the fare amount column, given that you know the start time
    of a DC taxi trip, as well as the latitude and longitude coordinates for the pickup
    and the drop-off locations of the trip?” Knowing the questions you wish to ask
    about the data also helps you understand the *essential data* in your data set—in
    other words, the data in scope for training of your machine learning system to
    answer the questions. In addition to the essential data, your data set may also
    contain *reference data* that is useful for ensuring the quality (specifically
    the accuracy) of your essential data but does not need to be cleaned up with the
    same degree of rigor. For example, the values in the mileage column of the DC
    taxi data set are not essential to answering the questions but are useful as a
    reference to compare against the values of the fareamount column and to ensure
    that the fare amount values have the right degree of data quality.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*你需要基于哪些列回答哪些问题？* 本书中基于 DC 出租车数据集的机器学习示例围绕着一个问题构建：“在你知道 DC 出租车行程的开始时间以及行程的上车和下车地点的纬度和经度坐标时，车费金额列的值是多少？”
    知道你希望对数据提出的问题也有助于你了解数据集中的*基本数据*，换句话说，是用于训练机器学习系统以回答问题的数据范围。除了基本数据外，你的数据集还可能包含*参考数据*，这些数据对于确保你的基本数据的质量（特别是准确性）是有用的，但不需要以相同严格的程度进行清理。例如，DC
    出租车数据集中里程表列中的值并不是回答问题所必需的，但作为参考来与车费金额列的值进行比较，并确保车费金额值具有正确的数据质量程度是有用的。'
- en: '*What is the schema for the essential data?* Before you can begin cleaning
    up the data set, you need to create a data schema in the catalog with changes
    that ensure the data values are specified using appropriate data types and constraints.
    The data schema specifies the data type for every column of a data set. Yet, while
    the data type specifications are necessary for the schema to help ensure data
    quality, they are not sufficient. For every data type, you should also be able
    to specify whether it is *nullable*. Here, data type nullability is equivalent
    to the DDL (data definition language) nullability and indicates whether a value
    is allowed to be missing. You should also specify any constraints that further
    limit the range of possible values: with string types, these can include regular
    expressions, while with integer types, these can be specified using interval ranges.
    The section on valid data illustrates the constraints using practical examples.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基本数据的模式是什么？* 在你开始清理数据集之前，你需要在目录中创建一个数据模式，其中包含确保数据值使用适当的数据类型和约束进行指定的更改。数据模式指定了数据集的每一列的数据类型。然而，虽然数据类型规范对于模式来帮助确保数据质量是必要的，但它们并不足够。对于每种数据类型，你还应该能够指定它是否是*可为空*的。在这里，数据类型的可为空性等同于
    DDL（数据定义语言）的可为空性，并指示值是否允许缺失。你还应该指定任何进一步限制可能值范围的约束：对于字符串类型，这些可以包括正则表达式，而对于整数类型，这些可以使用区间范围来指定。关于有效数据的部分使用实际示例说明了约束。'
- en: In the previous chapter, you used a crawler and a data catalog to discover and
    store the discovered data schema for the DC taxi data set. The schema in the catalog
    is similar to a DDL schema (part of the SQL standard) that describes data types
    such as integers, floats, timestamps, and more. Keep in mind that the discovered
    schema may or may not be the right schema to use.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你使用了一个爬虫和一个数据目录来发现并存储 DC 出租车数据集的发现数据模式。目录中的模式类似于描述数据类型（如整数、浮点数、时间戳等）的
    DDL 模式（SQL 标准的一部分）。请记住，发现的模式可能是正确的模式，也可能不是正确的模式。
- en: So what does it mean to have the right schema? More precisely, what does it
    mean for a schema to consist of data types that are appropriate for the data set’s
    values? Just as with DDL schemas, the choice of the appropriate data types is
    a tradeoff. On one hand, the schema should use data types that are sufficiently
    general to preserve the data values without loss of information. On the other
    hand, the data types should support (without type casting) expected operations
    on the data values while making efficient use of storage space. For example, the
    latitude and longitude coordinates from the DC taxi data set should be specified
    in the schema as floating point values (DOUBLE data type) instead of Unicode strings
    so that the coordinate values can be used for distance calculations.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 那么什么是正确的模式呢？更准确地说，模式由适用于数据集的值的数据类型组成，这意味着什么？就像DDL模式一样，选择适当的数据类型是一种权衡考虑。一方面，模式应该使用足够通用的数据类型，以便保留数据值而不丢失信息。另一方面，数据类型应该支持数据值的预期操作（无需类型转换），同时高效利用存储空间。例如，DC出租车数据集中的纬度和经度坐标应该在模式中指定为浮点数值（DOUBLE数据类型），而不是Unicode字符串，以便坐标值可以用于距离计算。
- en: 3.2.3 Normative principles for data quality
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.3 数据质量的规范性原则
- en: This section is about the principles behind the *valid, accurate, consistent,
    uniform,* and *unified model* (VACUUM) for structured data quality, along with
    cautionary tales to serve as case studies. The principles are normative, meaning
    that they define what quality data ought to be like instead of prescribing the
    specific steps or code for the implementation of data quality processing. The
    value of the principles is in a comprehensive and rigorous definition behind the
    claim that the data that complies with VACUUM is sufficiently “clean” and ready
    for machine learning.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了结构化数据质量的**有效、准确、一致、统一和完整模型（VACUUM）**背后的原则，以及作为案例研究的教训。这些原则是规范性的，意味着它们定义了数据质量应该是什么样子，而不是规定数据质量处理的具体步骤或代码实现。这些原则的价值在于通过充分且严谨地定义，为符合VACUUM标准的数据提供足够“干净”且能够用于机器学习的准备。
- en: Think of the VACUUM principles as a checklist of guidelines, criteria, or metrics
    of data quality that you should explore as part of your machine learning project.
    Keep in mind that doctors and pilots (as well as many other professionals) use
    checklists, but having a checklist will not make you a pilot or a doctor. If you
    are planning to develop professional expertise in data quality, you will need
    to develop your data-cleaning skills. Once you have the right experience in place,
    a checklist can help jog your memory and ensure that you do not miss important
    data quality aspects.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 将VACUUM原则视为数据质量的一份指南、标准或度量的清单，作为机器学习项目的一部分进行探索。要记住，医生和飞行员（以及许多其他专业人士）使用清单，但拥有清单并不会使您成为飞行员或医生。如果您计划在数据质量方面开发专业技能，您需要培养数据清理的技能。一旦您具备了正确的经验，清单可以帮助您复习并确保不会错过重要的数据质量方面。
- en: Valid
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 有效
- en: On January 31, 2020, the United Kingdom left the EU. So, should an EU data warehouse
    store the string value United Kingdom as a valid value in a column with names
    of EU member countries?
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 2020年1月31日，英国脱离了欧盟。那么，一个欧盟数据仓库是否应该将字符串值“United Kingdom”作为列名中的有效值存储起来？
- en: 'You could argue that beginning February 1, 2020, United Kingdom should stop
    being a valid data value in any column mentioning EU member states. However, this
    approach is counterproductive: excluding United Kingdom from a set of valid values
    means that any historical data associated with the column (in other words, any
    records dated prior to February 1, 2020) are associated with a value that is not
    valid. If a value in a data set was valid at any point of the data set’s existence,
    it should remain valid.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以争辩说，从2020年2月1日开始，“United Kingdom”不应再是提到欧盟成员国的任何列中的有效数据值。然而，这种方法是适得其反的：排除“United
    Kingdom”作为有效值集合的一部分意味着与该列相关的任何历史数据（换句话说，任何日期早于2020年2月1日的记录）都与无效的值相关联。如果数据集中的某个值在其存在的任何时间点都是有效的，那么它应该保持有效。
- en: Note This definition does not specify whether a combination of multiple valid
    values across multiple columns is valid; this issue will be addressed in the upcoming
    section on accuracy.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 该定义没有指明是否多个列的多个有效值的组合是有效的；这个问题将在准确性部分的即将到来的章节中解决。
- en: 'More precisely, a data value in a column is *valid* if it:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 更准确地说，列中的数据值如果满足以下条件，则为**有效**：
- en: '*Matches the column data type specified by the schema*. For a data value to
    be valid, it must match the data type specified by the schema. SQL-based data
    type definitions in a schema may include the following:'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*与模式指定的列数据类型匹配*。对于数据值而言，要有效必须与模式指定的数据类型匹配。模式中基于 SQL 的数据类型定义可能包括以下内容：'
- en: INTEGER (for example, in a column storing an elevator floor number)
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: INTEGER（例如，存储电梯楼层编号的列）
- en: DOUBLE (for example, a percentage of users who click a Subscribe button on a
    website)
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: DOUBLE（例如，点击网站上的订阅按钮的用户百分比）
- en: TIMESTAMP (for example, the time an order was placed on a website)
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: TIMESTAMP（例如，网站上下订单的时间）
- en: BOOLEAN (for example, whether a taxi trip ended at an airport)
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: BOOLEAN（例如，出租车行程是否在机场结束）
- en: STRING (for example, the text of comments left in a comment box on a survey)
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: STRING（例如，在调查的评论框中留下的评论文本）
- en: '*Matches one or more of the following constraints* :'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*匹配一个或多个以下约束*：'
- en: '*Nullability* —This constraint applies to any data type and specifies whether
    a value in a data column is allowed to have a NULL value. For example, a TIMESTAMP
    data value storing the date of birth in a driver’s license database must be non-nullable
    (i.e., should not be permitted to have a NULL value), while a user’s Twitter handle
    on a customer profile web page can be specified as nullable to handle the cases
    when the handle is unknown or not specified. Nullable data types can also include
    INTEGERs (e.g., a rating of a taxi ride by a passenger on a scale of 1—5, with
    a NULL value representing no rating), and other data types.'
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可空性* —此约束适用于任何数据类型，并指定数据列中的值是否允许具有 NULL 值。例如，在驾驶执照数据库中存储出生日期的 TIMESTAMP 数据值必须是非可空的（即，不应允许具有
    NULL 值），而客户配置文件网页上的用户 Twitter 用户名可以指定为可空，以处理用户名未知或未指定的情况。可空数据类型还可以包括 INTEGER（例如，乘客对出租车行程的评分，评分范围为
    1—5，NULL 值表示没有评分）和其他数据类型。'
- en: '*Enumeration* —This constraint applies to any data type and specifies a validation
    set, a dictionary, or an enumeration of valid values for a data type. With STRING
    values, the enumerations may include names of US states, or major airports in
    the New York City area, such as LGA, JFK, EWR. The enumeration constraint for
    a schema may specify an INTEGER data type for a country code column in a data
    set of phone numbers using an enumeration of valid country phone codes. Recall
    from the example at the beginning of this section that the enumeration must include
    all values that have ever been valid for the column. So, in any data set older
    than February 1, 2020, in a data column that stores EU country names, United Kingdom
    is a valid value, regardless of the fact that the UK left the EU on January 31,
    2020.'
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*枚举* —此约束适用于任何数据类型，并指定了数据类型的验证集、字典或有效值的枚举。对于 STRING 值，枚举可能包括美国州名或纽约市区域的主要机场名称，如
    LGA、JFK、EWR。模式的枚举约束可以为电话号码数据集中的国家代码列指定 INTEGER 数据类型，并使用有效国家电话代码的枚举。请从本节开头的示例中回忆，枚举必须包括对该列曾经有效的所有值。因此，在
    2020 年 2 月 1 日之前的任何数据集中，存储 EU 国家名称的数据列中，英国是一个有效值，而不管英国于 2020 年 1 月 31 日离开欧盟的事实如何。'
- en: '*Range* —This constraint is data type specific and can be one of the following
    types:'
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*范围* —此约束是数据类型特定的，可以是以下类型之一：'
- en: '*Interval constraint* is used for numeric or date/time data types. As an example
    of valid integer data values, consider a data set with an activity log for a single
    elevator in a skyscraper. One of the data columns in the data set stores the floor
    number where the elevator stopped. Since not all floors in this hypothetical skyscraper
    are accessible by the elevator and the numbering systems skips the 13th floor
    due to superstition, the constraints on possible values include intervals from
    —3 to —1 for the parking garage, and 1 to 12 and 14 to 42\. The typical notation
    for this interval is [[—3, —1] or (0, 12] or [14,42]], where the square brackets
    indicate that the value is included in the interval while the parentheses indicate
    that the interval does not include the value neighboring the parenthesis. The
    or keyword in this case represents the set union operation (in other words, a
    logical or).'
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*间隔约束* 用于数字或日期/时间数据类型。作为有效整数数据值的示例，考虑一个用于摩天大楼中单个电梯的活动日志的数据集。数据集中的一个数据列存储电梯停靠的楼层数。由于这个假想摩天大楼中并非所有楼层都可由电梯到达，并且由于迷信原因编号系统跳过了第
    13 层，因此可能值的约束包括从—3 到—1 的间隔表示停车场，以及从 1 到 12 和 14 到 42。这个间隔的典型表示法是 [[—3, —1] 或 (0,
    12] 或 [14,42]]，其中方括号表示值包含在间隔中，而圆括号表示间隔不包括与括号相邻的值。在这种情况下，“或”关键字表示集合并操作（换句话说，逻辑或）。'
- en: A similar approach is used when working with DOUBLE and other floating point
    data types. For example, a probability value can be specified with an interval
    range constraint from 0.0 to 1.0, [0.0, 1.0]
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在使用 DOUBLE 和其他浮点数据类型时采用类似的方法。例如，可以使用区间范围约束指定概率值为 0.0 到 1.0，[0.0, 1.0]。
- en: 'Intervals are also common with TIMESTAMP data types for a date/time range where
    they are used to describe periods such as workdays, weekends, or holidays (e.g.,
    dates: [2020-01-01 00:00:00, 2021-01-01 00:00:00]).'
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间戳数据类型常见的间隔用于描述日期/时间范围，例如工作日、周末或假日（例如，日期：[2020-01-01 00:00:00, 2021-01-01 00:00:00]）。
- en: '*Regular expression constraint* is used in the cases of the STRING data type
    to specify the space of the valid values. For example, in a database that stores
    Twitter handles of social media influencers, a regular expression can specify
    that any value that matches /^@[a-zA-Z0-9_]{1,15}$/ is valid. Keep in mind that
    regular expressions also apply to many data columns that appear numeric; for instance,
    IP addresses consist primarily of numbers but are commonly stored as a string.'
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*正则表达式约束* 用于字符串数据类型的情况下，用于指定有效值的范围。例如，在存储社交媒体影响者 Twitter 账号的数据库中，正则表达式可以指定任何匹配
    /^@[a-zA-Z0-9_]{1,15}$/ 的值是有效的。请注意，正则表达式也适用于许多看起来是数值的数据列；例如，IP 地址主要由数字组成，但通常存储为字符串。'
- en: '*Rule* —This constraint applies to any data type and specifies computable conditions
    to decide whether a value is valid. For example, if you have ever used a “Save
    my payment for later” feature on a website to permit a PCI-DSS-compliant[⁵](#pgfId-1046619)
    vendor to store your credit card number, you should know that a rule constraint
    for a credit card number is based on Luhn algorithm,[⁶](#pgfId-1066715) which
    computes parity check bits that ensure a credit card number is valid.'
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*规则* ——此约束适用于任何数据类型，并指定计算条件以确定值是否有效。例如，如果你曾经在网站上使用“保存我的付款以供以后使用”的功能，以允许符合PCI-DSS标准的[⁵](#pgfId-1046619)供应商存储你的信用卡号，你应该知道信用卡号的规则约束是基于
    Luhn 算法的[⁶](#pgfId-1066715)，该算法计算出奇偶校验位以确保信用卡号有效。'
- en: 'At this point, you have seen the criteria specifying and examples illustrating
    what it means for a single value in a data set to be valid or invalid. However,
    it is straightforward to come up with an example of a record consisting entirely
    of valid values but with an obvious data quality problem. Here’s a made-up record
    from a hypothetical data set that lists locations with a continent and country
    information:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经看到了指定条件和举例说明数据集中单个值有效或无效的含义。然而，很容易举出一个由完全有效值组成但存在明显数据质量问题的记录示例。以下是来自假设数据集的一个虚构记录，其中列出了带有大陆和国家信息的位置：
- en: '| Continent | Country | Lat | Lon |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 大陆 | 国家 | 纬度 | 经度 |'
- en: '| South America | United States | 38.91 | –77.03 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 南美洲 | 美国 | 38.91 | –77.03 |'
- en: All the values, including South America, United States, as well as the latitude/longitude
    coordinates, have valid values for the respective columns. Recall that the valid
    principle from VACUUM focuses on data quality problems and validation checks within
    a single value. To address the data quality issue in this example, you need to
    learn about the accuracy principle.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 所有值，包括南美洲、美国以及纬度/经度坐标，都对应着各自列的有效值。回想一下来自 VACUUM 的有效原则侧重于数据质量问题和单个值内的验证检查。要解决此示例中的数据质量问题，您需要了解准确性原则。
- en: Accurate
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 准确
- en: 'When you learned about valid data, you looked at an example of a data set of
    records about member states of the EU. As part of the example, you saw that United
    Kingdom is a valid value for the EU country column. Suppose you are working with
    a data record that has two columns: the first with a date/time of the membership
    and the second with the name of the country:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 当您了解有效数据时，您看到了有关欧盟成员国的记录数据集的示例。作为示例的一部分，您看到英国是欧盟国家列的有效值。假设您正在处理一个包含两列的数据记录：第一列是入会日期/时间，第二列是国家名称：
- en: '| Date of Record | Member State |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 记录日期 | 成员国 |'
- en: '| 2020-01-31 | United Kingdom |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 2020-01-31 | 英国 |'
- en: '| 2020-02-01 | United Kingdom |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 2020-02-01 | 英国 |'
- en: While all of the values in the example are valid, it is impossible to assert
    that the second row is garbage without using an external (to the data record)
    reference data source. The reference should be able to process the values from
    the entire record and indicate whether (or to what extent) the record is inaccurate.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然示例中的所有值都是有效的，但是如果不使用外部（数据记录之外的）参考数据源，就不可能断言第二行是垃圾。参考数据应该能够处理整个记录中的值，并指示记录是否（或在何种程度上）不准确。
- en: More precisely, a data record is *accurate* if all the data values that are
    part of the record are valid and the combination of the values in the record are
    reconciled to a reference data source. By the way of an example, consider a database
    of college alumni, with the date of alumni college enrollment and the date of
    their college graduation. Checking the database for accuracy requires references
    to external sources of truth, such as an admissions database and a transcript
    database. In financial records, inaccuracy can be due to a mismatch between a
    credit card number and its PIN code. Sometimes accuracy problems arise when joining
    multiple tables incorrectly, for example, in a data record stating that the movie
    *Titanic* was produced by Guy Ritchie in 1920.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 更准确地说，如果记录中的所有数据值都是有效的，并且记录中的值的组合与参考数据源一致，那么数据记录就是*准确的*。举个例子，考虑一个大学校友数据库，其中包括校友入学日期和毕业日期。检查数据库的准确性需要参考外部的真实数据源，例如招生数据库和成绩单数据库。在财务记录中，准确性问题可能是由信用卡号和
    PIN 码之间的不匹配引起的。有时准确性问题是由于错误地连接多个表，例如，一条数据记录声称电影 *泰坦尼克号* 是由盖·里奇在 1920 年制作的。
- en: The accuracy assurance for values such as domain names are a particularly difficult
    category because the reference data source, domain registration, and DNS databases
    change over time. For example, if you try to create an email mailing list and
    check the domain name part of an email using a regular expression, the data in
    the list may be valid but not accurate in the sense that some of the emails do
    not map to valid domain names. You may attempt to send an email to the address
    from the mailing list to confirm that the domain name and the email resolve to
    an accurate address. Even prior to sending the email, you may attempt to perform
    a DNS lookup to verify the accuracy of the domain name.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 对于诸如域名等值的准确性保证是一项特别困难的任务，因为参考数据源、域名注册和 DNS 数据库随着时间的推移而发生变化。例如，如果您尝试创建一个电子邮件邮寄列表，并使用正则表达式检查电子邮件的域名部分，那么列表中的数据可能是有效的，但从某种意义上来说并不准确，因为其中一些电子邮件没有映射到有效的域名。您可以尝试向邮寄列表中的地址发送电子邮件，以确认域名和电子邮件是否解析到准确的地址。甚至在发送电子邮件之前，您可能会尝试执行
    DNS 查询来验证域名的准确性。
- en: In the UK leaving the EU example, improving the quality of the data in the data
    set means that the reference data source must exist with the master record of
    the timestamps for the start and the end dates of an EU state membership. However,
    for many organizations the challenge with reference data sources isn’t that there
    are too few of them, but rather that there are too many. The next section on consistency
    will illustrate this problem with more examples.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在英国退出欧盟的例子中，改善数据集中数据的质量意味着参考数据源必须存在于欧盟成员国开始和结束日期的时间戳的主记录中。然而，对于许多组织来说，参考数据源的挑战并不是它们太少，而是它们太多。下一节关于一致性将用更多的例子说明这个问题。
- en: Consistent
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 一致性
- en: In January of 2005, an owner of a small house in Valparaiso, a town of about
    30,000 residents in Porter County, Indiana, received a a notice that the annual
    tax assessment value of his house was set at $400 million. The notice, which also
    included a demand for a tax payment of $8 million, came as a surprise to the owner
    of the modest house since just the year prior the tax payment amounted to $1,500\.
    Although the issue with data accuracy was soon resolved, the story did not end
    there.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 2005年1月，印第安纳州波特郡约有3万名居民的小镇瓦尔帕莱索的一位小房屋业主收到通知，他房屋的年度税收评估价值被设定为4亿美元。这份通知还包括要求交纳800万美元税款的要求，对这座普通房屋的所有者来说，这是一个惊喜，因为就在前一年，税款金额仅为1500美元。尽管数据准确性的问题很快得到解决，但故事并没有就此结束。
- en: The data systems of Valparaiso did not follow the data quality consistency principle,
    so the original data accuracy problem propagated into the budgeting system for
    the town. The small town’s budget was drawn with an assumption of an $8 million
    tax payment, so the town had to claw back $3.1 million from schools, libraries,
    and other budget-funded units. That year, Porter County ended up with many unhappy
    students and parents as the schools had to cover a $200,000 budget shortfall.[⁷](#pgfId-1050388)
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 瓦尔帕莱索的数据系统没有遵循数据质量一致性原则，因此原始的数据准确性问题传播到了该镇的预算系统中。这个小镇的预算假设了800万美元的税款，因此该镇不得不从学校、图书馆和其他预算资助单位中收回310万美元。那一年，波特郡有很多不满的学生和家长，因为学校不得不填补20万美元的预算缺口。
- en: 'Consistency issues arise when different and conflicting validation and accuracy
    implementations are used across different data silos: databases, data stores,
    or data systems. While each individual silo can be valid and accurate according
    to a silo-specific set of definitions, achieving consistency means ensuring a
    common set of standards for valid and accurate data before integrating the data
    from systems across silos that span different technology and organizational boundaries.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 一致性问题在不同的数据孤岛（数据库、数据存储或数据系统）中使用不同和冲突的验证和准确性实现时会出现：虽然每个单独的孤岛可以根据孤岛特定的定义集合有效和准确，但实现一致性意味着在将跨越不同技术和组织边界的系统中的数据集成之前，确保有效和准确的数据的一个共同标准。
- en: For example, was the UK a member of the EU at 11:30 p.m. on January 31, 2020?
    If you are not mindful of data quality, the answer may depend on your data set.
    In a UK data set, you can expect a valid and accurate record showing that the
    UK was not an EU member country at 11:30 p.m. on January 31, 2020\. Yet, in an
    EU data set, an identical combination of the date, time, and country name values
    is an accurate record for an EU country member. As you have probably guessed,
    the inconsistency is due to different assumptions about storing the date and time
    values across different data sets. The UK data set in this example uses the Greenwich
    Mean time zone, while the EU data set uses Central European time.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，英国在2020年1月31日晚上11:30是否是欧盟成员国？如果你对数据质量不够谨慎，答案可能取决于你的数据集。在英国的数据集中，你可以期待一条有效和准确的记录，显示英国在2020年1月31日晚上11:30不是欧盟成员国。然而，在欧盟的数据集中，相同的日期、时间和国家名称值的组合是一个欧盟成员国的准确记录。正如你可能已经猜到的那样，不一致是由于在不同的数据集中存储日期和时间值的假设不同。这个例子中的英国数据集使用格林尼治平均时区，而欧盟数据集使用中欧时间。
- en: 'Even when joining tables within a single data set, or a single data silo, it
    is important to ensure consistency of validation and accuracy rules. Typical issues
    arise when using phone numbers and emails as unique identifiers for a user: since
    phone numbers and emails may change owners, joining tables based on this information
    might lead to problems. Another example might include different approaches for
    storing other identifiers such as phone numbers. Some might uniquely identify
    with a country code, and others may not. This might be as simple as using different
    primary keys with the same individual across different systems, maybe creating
    a new primary key to join together or it might be more subtle. For example, some
    systems might use a 5+4 ZIP code; other systems might use a five-digit ZIP code
    per individual.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在单个数据集或数据隔离中连接表时，确保验证和准确性规则的一致性也很重要。典型的问题出现在使用电话号码和电子邮件作为用户的唯一标识符时：由于电话号码和电子邮件可能更换所有者，基于这些信息连接表可能会导致问题。另一个例子可能包括存储其他标识符的不同方法，比如电话号码。有些可能用国家代码唯一标识，而其他可能不是。这可能非常简单，比如在不同系统中使用不同的主键来标识同一个人，可能创建一个新的主键来连接，或者可能更微妙。例如，有些系统可能使用5+4的邮政编码，其他系统可能为每个个体使用一个五位数的邮政编码。
- en: Uniform
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 统一
- en: 'The Mars Climate Orbiter, a $125 million robotic space probe, was launched
    by NASA to Mars in 1998\. Less than 12 months later, during an orbit change maneuver,
    it skidded off the atmosphere of Mars and was gone. The cause was simple: the
    designers of the orbiter integrated two independently developed systems where
    one was using US customary (imperial) units of measurement and another was based
    on the SI (metric) units. Thus, non-uniform tables were UNION-ed together (concatenating
    records) and data with non-uniform records appeared in the data set. Since the
    data measurement values used by the orbiter were not uniform across multiple data
    records, NASA wasted $125 million of its budget.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 火星气候轨道飞行器是一项耗资1.25亿美元的火星无人空间探测器，由NASA于1998年发射到火星。不到12个月后，在进行轨道变化机动时，它滑离了火星的大气层，从此销声匿迹。原因很简单：轨道飞行器的设计者集成了两个独立开发的系统，一个使用美国习惯（英制）度量单位，另一个基于国际单位制（公制）单位。因此，非统一的表被连结在一起（连接记录），数据集中出现非统一的记录。由于轨道飞行器使用的数据测量值在多个数据记录中不统一，NASA浪费了1.25亿美元的预算。
- en: The distinction between consistency and uniformity is subtle but important.
    As illustrated by the Mars orbiter example, ensuring consistency of validation
    and accuracy rules across data silos is insufficient to address data quality.
    The *uniform* principle states that for every column in a data set, all records
    should use data that was recorded using the same (uniform) measurement system.
    Instead of continuing with the NASA example, consider a more down-to-earth scenario
    of a data set created to analyze customer satisfaction with different video streaming
    services.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 一致性和统一性的区别微妙但重要。正如火星轨道飞行器的例子所示，确保跨数据隔离的验证和准确性规则的一致性是不足以解决数据质量问题的。*统一*原则规定数据集中的每一列，所有记录都应该使用相同（统一）的测量系统记录的数据。不再使用NASA的例子，考虑一个更贴近生活的场景，即创建用来分析用户对不同视频流媒体服务的满意度的数据集。
- en: Suppose some streaming services are prompting users to rate their satisfaction
    with content on a scale from 0—4 stars after each viewing. Other services may
    use a value of 0 to indicate no response on a scale of 1—4 stars. Although the
    rules around valid values for both are identical, to ensure data quality, it is
    insufficient to specify that customer satisfaction should be a DOUBLE value with
    valid interval of [0, 4] and then apply this interval consistently across video-streaming
    service data silos. For instance, if the average satisfaction scores for each
    service are recorded daily and joined to prepare an aggregated average score,
    the result is not uniform across rows in the data set. Specifically, the service
    that uses the value of 0 to indicate no response will be penalized in the analysis.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 假设某些流媒体服务在每次观看后，提示用户对内容满意度进行0—4星的评分。其他服务可能使用0来表示1—4星的范围内没有回应。尽管两者的有效值规则相同，为了确保数据质量，仅仅指定客户满意度应该是一个[0,
    4]的DOUBLE值，并一致应用于视频流媒体服务的数据隔离中是不够的。例如，如果每个服务的平均满意度分数按照每日记录并连接以准备聚合平均分数，则结果在数据集中的行之间不是统一的。特别是，使用0值表示没有回应的服务将在分析中受到处罚。
- en: Uniformity issues often arise over the lifetime of a data set. Consider a grocery
    store chain that enforces a store aisle coding system where all stores have aisles
    numbered 1—8, with each aisle corresponding to a produce category, such as 1 for
    dairy, 2 for meats, 3 for frozen foods, and so on. As soon as a single store violates
    the aisle coding system, for instance by coding dairy as 2 instead of 1, uniformity
    is violated across the entire grocery chain.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的统一问题往往在数据集的生命周期中出现。考虑一个强制执行商店货架编码系统的杂货连锁店，在这个系统中，所有的商店都有标号为1-8的货架，每个货架对应一个产品类别，比如1代表奶制品，2代表肉类，3代表冷冻食品，以此类推。一旦有一个商店违反了货架编码系统，比如把奶制品编码为2而不是1，整个杂货连锁店的统一性就被破坏了。
- en: Unified
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 统一
- en: 'In a 1912 book, influential logician and philosopher Bertrand Russell used
    a story to illustrate the problem with inductive reasoning, a fundamental principle
    behind machine learning. Paraphrasing Russell, here’s the fable in a nutshell:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在1912年的一本书中，有影响力的逻辑学家和哲学家贝特兰·罗素用一个故事来阐述归纳推理的问题，这是机器学习背后的一个基本原则。对罗素的话进行改写，以下是这个寓言的概括：
- en: On December 1, a turkey was born in the United States. It was no ordinary turkey.
    Some say it was the most intelligent turkey that ever lived. The genius turkey
    soon figured out the patterns of the night sky and the role of the sun in casting
    shadows, and realized that it was fed at 7:00 a.m. every day. Reasoning that food
    was critical to well-being, it proceeded to ask itself whether it would be worthwhile
    to plot an escape, risking hunger and death, or if it would be safer to remain
    a well-fed captive. Having re-invented statistics, the rational genius turkey
    gathered data, developing increasing confidence that it will be fed every day
    at 7:00 a.m. regardless of the position of the sun, the moon, the stars, temperature,
    precipitation, and other factors. Sadly, the morning of the Thanksgiving Day the
    food did not come, and the head of the genius turkey landed on a chopping block.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在12月1日，一个火鸡在美国出生了。它不是普通的火鸡。有人说它是有史以来最聪明的火鸡。这只天才火鸡很快就摸清了夜空的规律和太阳在投射阴影中的作用，并意识到每天早上7点它都会被喂食。它推理出食物对健康至关重要，因此开始思考是否值得策划一次逃亡，冒着饥饿和死亡的风险，还是继续作为一个受到良好喂养的囚徒。天才火鸡重新发明了统计学，收集数据，并逐渐增加信心，无论太阳、月亮、星星、温度、降水和其他因素的位置如何，它每天早上7点都会被喂食。可悲的是，在感恩节的早晨，食物没有来，天才火鸡的头落在了砧板上。
- en: 'The story (which is meant to be more facetious rather than tragic) is here
    to help you remember that the machine learning models that you create, no matter
    how complex, are little more than digital versions of Russell’s turkey. Their
    success is based solely on their capacity to take advantage of the data available
    to them. In contrast, as a machine learning practitioner, you can make your machine
    learning project more successful through curiosity and causal and deductive reasoning:
    by discovering facts and data sets that are novel and relevant to the project’s
    use case, unifying the discovered information with the data set on hand, and expanding
    the scope of the relevant training data available to train models. You can also
    help minimize the risks to the machine learning project’s success by discovering
    and addressing potential sources of non-obvious systemic bias in the training
    data set, unifying and bringing into alignment the cultural values of the project’s
    operating environment and the contents of the data set.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这个故事（主要是生动而不是悲惨）是为了帮助你记住，无论你创建多么复杂的机器学习模型，它们只不过是罗素火鸡的数字化版本。它们的成功完全取决于它们利用的可用数据的能力。相比之下，作为一个机器学习从业者，你可以通过好奇心、因果推理和演绎推理让你的机器学习项目更加成功：通过发现对项目使用案例来说是新颖且相关的事实和数据集，将发现的信息与手头的数据集合并，并扩大用于训练模型的相关训练数据的范围。你还可以通过发现和解决训练数据集中潜在的非明显系统性偏差的可能来源，将项目运行环境的文化价值观与数据集的内容统一和调整，以最大限度地减少机器学习项目成功的风险。
- en: 'While you can rely on machine learning models to perform effective inductive
    reasoning, it is your responsibility to enforce the *unified* principle, meaning
    that your data set:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然你可以依靠机器学习模型进行有效的归纳推理，但是你有责任来执行**统一**原则，也就是说，你的数据集：
- en: Is a single place for the data relevant to your project’s machine learning use
    case(s)
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否有一个单一的位置存放与你的项目的机器学习用例相关的数据
- en: Aligns the criteria used by your use case to achieve unbiased data-driven decision
    making, with the content of the data being used for machine learning model training
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将您的用例使用的标准与用于机器学习模型训练的数据内容对齐，以实现无偏的数据驱动决策制定。
- en: Depends on a common data quality process for the data used for machine learning
    model training and for the data used with a trained machine learning model
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 取决于用于机器学习模型训练的数据和用于已经训练的机器学习模型一起使用的数据的共同数据质量过程。
- en: The unified principle is a part of VACUUM to remind you that data quality is
    a journey and not a destination.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 统一的原则是VACUUM的一部分，提醒您数据质量是一项旅程，而不是目的地。
- en: 3.3 Applying VACUUM to the DC taxi data
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3将VACUUM应用于DC出租车数据
- en: Now that you know about the VACUUM data quality principles, it is time to apply
    the principles to the DC taxi data set. In this section, you are going to start
    with a single table of data and focus on how to implement the data quality queries
    that ensure that the data set is valid, accurate, and uniform.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了VACUUM数据质量原则，是时候将这些原则应用于DC出租车数据集了。在本节中，您将从单个数据表开始，并专注于如何实现数据质量查询，以确保数据集是有效，准确和统一的。
- en: 3.3.1 Enforcing the schema to ensure valid values
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.1强制执行模式以确保有效值
- en: This section introduces the SQL statements you can execute against the DC taxi
    data set to check for invalid values and eliminate them from further analysis.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了您可以对DC出租车数据集执行的SQL语句，以检查无效值并将其排除在进一步分析之外。
- en: The schema shown in table 3.2 matches the version you have first encountered
    in chapter 2\. The schema specifies the required data types for the taxi fare
    estimation service interface using SQL types. In the upcoming chapters of the
    book, when you will start training machine learning models from the DC taxi data,
    NULL values in the training data set can create problems. (Consider asking a machine
    learning model to estimate taxi fare for a NULL pickup location!) So, the schema
    is designed to ensure that none of the data values are permitted to be NULL.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.2中显示的模式与您在第2章中首次遇到的版本相匹配。模式使用SQL类型指定出租车费用估计服务接口所需的数据类型。在本书的后续章节中，当您开始从DC出租车数据中训练机器学习模型时，训练数据集中的NULL值可能会造成问题（考虑要求机器学习模型为NULL取车位置估算出租车费用！）。因此，该模式旨在确保不允许任何数据值为空。
- en: Table 3.2 Schema and example values for a taxi fare estimation service interface
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.2出租车费用估计服务界面的模式和示例值
- en: '| Input |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 输入 |'
- en: '| Name | Data Type | Example Value |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 数据类型 | 示例值 |'
- en: '| Pickup location latitude | FLOAT | 38.907243 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 取车位置纬度 | 浮点数 | 38.907243 |'
- en: '| Pickup location longitude | FLOAT | –77.042754 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 取车位置经度 | 浮点数 | –77.042754 |'
- en: '| Dropoff location latitude | FLOAT | 38.90451 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 下车位置纬度 | 浮点数 | 38.90451 |'
- en: '| Dropoff location longitude | FLOAT | –77.048813 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 下车位置经度 | 浮点数 | –77.048813 |'
- en: '| Expected start time of the trip | STRING [⁸](#pgfId-1071136) | 01/12/2015
    12:42 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 旅行的预期开始时间 | 字符串 [⁸](#pgfId-1071136) | 2015年1月12日12:42 |'
- en: '| Output |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 输出 |'
- en: '| Name | Data Type | Example Value |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 数据类型 | 示例值 |'
- en: '| Estimated fare (dollars) | FLOAT | 6.12 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 预估费用（美元） | 浮点数 | 6.12 |'
- en: 'Let’s kick off the effort of cleaning up the data set for machine learning
    by finding out the number of the timestamps with NULL values in the origindatetime_tr
    column using the following query from your shell, assuming you executed the Python
    awscli_to_df() function from listing 3.4 to output the query result using pandas:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用以下查询语句从您的shell中找出origindatetime_tr列中NULL值的时间戳数，假设您执行了清单3.4中的Python awscli_to_df()函数，并使用pandas输出查询结果：
- en: '[PRE12]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This results in the following:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下结果：
- en: '| total | null_origindate_time_total |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 总数 | null_origindate_time_total |'
- en: '| 67435888 | 14262196 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 67435888 | 14262196 |'
- en: For brevity the upcoming code snippets will no longer remind you to run source
    utils.sh ; athena_query_to_pandas or awscli_to_df().
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简洁起见，即将到来的代码片段将不再提醒您运行source utils.sh;athena_query_to_pandas或awscli_to_df()。
- en: Recall that the SQL COUNT(*) function[⁹](#pgfId-1051295) returns the count of
    both NULL and non-NULL values. However, since the WHERE clause of the SQL query
    restricts the output to the rows where origindatetime_tr is NULL, the output of
    the SQL query reports that 14,262,196 rows are NULL out of the total 67,435,888
    rows in the entire data set.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，SQL COUNT(*)函数[⁹](#pgfId-1051295)返回NULL和非NULL值的计数。但是，由于SQL查询的WHERE子句将输出限制为origindatetime_tr为NULL的行，因此SQL查询的输出报告了在整个数据集的67435888行中，有14262196行为空。
- en: Beyond ensuring that the origindatetime_tr values are non-NULL, it is also critical
    to confirm the values comply with the regular expression definition for valid
    timestamp values. In practice this means that it should be possible to parse the
    non-NULL values of the origindatetime_tr column into relevant elements of a timestamp,
    including year, month, day, hour, and minute.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 除了确保 origindatetime_tr 值为非 NULL 外，还必须确认值符合有效时间戳值的正则表达式定义。在实践中，这意味着应该可以解析 origindatetime_tr
    列的非 NULL 值为时间戳的相关元素，包括年、月、日、小时和分钟。
- en: 'Fortunately, you do not have to implement the regular expression parsing rules
    to process the date/times. The following SQL query takes the difference between
    the total number of the rows in the data set and the number of the origindatetime_tr
    values that are not NULL and can be correctly parsed using the SQL DATE_PARSE
    function which uses the %m/%d/%Y %H:%i format in the DC taxi data set:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，您不必实现正则表达式解析规则来处理日期/时间。以下 SQL 查询计算数据集中行数与非 NULL 的 origindatetime_tr 值之间的差异，并且可以使用
    SQL DATE_PARSE 函数正确解析，该函数使用 DC 出租车数据集中的 %m/%d/%Y %H:%i 格式：
- en: '[PRE13]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This results in the following:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下结果：
- en: '| origindatetime_not_parsed |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| origindatetime_not_parsed |'
- en: '| 14262196 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 14262196 |'
- en: Since the difference returned by the statement is also equal to 14,262,196,
    this means that all but the NULL values of the timestamp can be parsed. Also,
    notice that the SQL statement uses a SQL subquery to compute the total number
    of rows in the data set, including both NULL and non-NULL values because the subquery
    does not include a WHERE clause. The WHERE clause at the ending of the outer SQL
    query applies only to the calculation of the COUNT of the values that can be correctly
    parsed by the DATE_PARSE function.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 由于语句返回的差值也等于 14,262,196，这意味着时间戳的所有非 NULL 值都可以解析。此外，请注意，SQL 语句使用 SQL 子查询来计算数据集中的总行数，包括
    NULL 和非 NULL 值，因为子查询不包括 WHERE 子句。外部 SQL 查询的结尾处的 WHERE 子句仅适用于计算 COUNT 函数，该函数计算
    DATE_PARSE 函数可以正确解析的值的数量。
- en: Let’s continue to apply the validation rules to the origin and destination locations.
    Since in the use case the latitude and longitude coordinates for the origin and
    destination locations are non-nullable, let’s explore the impact of the validation
    rules on the coordinate values as shown next.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续将验证规则应用于起始点和目的地位置。由于在使用情况中，起始点和目的地位置的纬度和经度坐标是非空的，请看下面展示的验证规则对坐标值的影响。
- en: Listing 3.5 How often parts of the pickup location coordinate are missing
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.5 位置坐标的缺失频率
- en: '[PRE14]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This results in the following:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下结果：
- en: '| percentage_null | either_null | both_null |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| percentage_null | either_null | both_null |'
- en: '| 14.04 | 9469667 | 9469667 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 14.04 | 9469667 | 9469667 |'
- en: According to the results of the query, in the data set, origin_block_latitude
    and origin_block_latitude are missing in pairs (i.e., both are NULL values if
    either is NULL) in 9,469,667 rows, or roughly 14.04% of the data set.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 根据查询结果，在数据集中，原始块纬度和原始块经度成对缺失（即，如果其中一个为 NULL，则另一个也为 NULL）的行数为 9,469,667，大约占数据集的
    14.04%。
- en: 'A similar analysis of the destination coordinates uses the following SQL statement:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 对目的地坐标的类似分析使用以下 SQL 语句：
- en: '[PRE15]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This results in
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致
- en: '| percentage_null | either_null | both_null |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| percentage_null | either_null | both_null |'
- en: '| 19.39 | 13074278 | 13074278 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 19.39 | 13074278 | 13074278 |'
- en: which shows that 13,074,278 rows have destination coordinates with NULL values,
    which is roughly 19.39% of the entire data set.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明有 13,074,278 行的目的地坐标具有 NULL 值，大约占整个数据集的 19.39%。
- en: 'The fractions of the NULL values for the origin and destination coordinates
    are clearly significant. In the potential worst case of missing values, you could
    find that 42.4% (i.e., 24.59% + 17.81%) of the rows had either the origin or destination
    coordinates missing. However, in the data set, a large portion of the missing
    values overlap, meaning that if either origin or destination is NULL, the other
    coordinate is NULL as well. You can find the count and the fraction of the missing
    coordinates using the following:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 起始点和目的地坐标的 NULL 值的比例显然非常显著。在缺少值的潜在最坏情况下，您可能会发现 42.4%（即 24.59% + 17.81%）的行的起点或目的地坐标缺失。然而，在数据集中，大部分缺失值是重叠的，这意味着如果起点或目的地任一坐标为
    NULL，则另一个坐标也为 NULL。您可以使用以下方法找到缺失坐标的计数和比例：
- en: '[PRE16]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This results in
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致
- en: '| total | percent |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| total | percent |'
- en: '| 16578716 | 24.58 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 16578716 | 24.58 |'
- en: which shows that 24.58%, or 16,578,716 rows, in the data set do not have a useful
    pair of origin and destination coordinates. Since the pickup and the drop-off
    locations are a required part of the taxi fare estimation service specification,
    let’s focus the data quality efforts on the remaining 75.42% of the rows with
    usable pickup and drop-off coordinates.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示出数据集中24.58%，或者16,578,716行，没有有效的起点和终点坐标。由于乘车和下车位置是出租车费用估算服务规范的必需部分，让我们将数据质量工作集中在剩下的75.42%具有可用乘车和下车坐标的行上。
- en: 3.3.2 Cleaning up invalid fare amounts
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.2清理无效的费用金额
- en: This section walks you though the SQL statements to analyze the fare_amount
    column and enforce validation rules for the column values.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将通过SQL语句对fare_amount列进行分析，并对列值强制执行验证规则。
- en: The PySpark job that populated the dc_taxi_parquet table performed some validation
    processing on the original data set. If you query Athena for the schema of the
    table, notice that the values needed for the project exist as both string and
    double types. Having both types means that in the cases when a value cannot be
    converted to the desired DOUBLE type (e.g., when a value cannot be parsed as a
    double), the original value is preserved and available for troubleshooting of
    the data.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 填充dc_taxi_parquet表的PySpark作业对原始数据集执行了一些验证处理。如果您查询Athena以获取表的模式，请注意项目所需的值同时存在字符串和双精度类型。同时存在两种类型意味着，在某些情况下，值无法转换为所需的DOUBLE类型（例如，无法解析值为双精度数值），原始值将被保留并可用于数据故障排除。
- en: According to the schema specifications described in chapter 2, every taxi trip
    record must have non-NULL values in the fare amount, trip start timestamp, and
    origin and destination latitude/longitude coordinates. Let’s start by investigating
    the instances where the fareamount_double column contains NULL values, which are
    not allowed according to the schema. Since the fareamount_string column is a source
    of information for fare amount values that failed the parsing from STRING to DOUBLE,
    you can learn more about the problem values using the following SQL statement.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 根据第2章中描述的模式规范，每个出租车行程记录必须在车费金额、行程开始时间戳和起点和终点纬度/经度坐标中具有非NULL值。让我们从调查fareamount_double列包含NULL值的实例开始，这是根据模式不允许的。由于fareamount_string列是从STRING到DOUBLE解析失败的车费金额值的信息来源，您可以使用以下SQL语句了解更多有关问题值的信息。
- en: Listing 3.6 Values of the fareamount_string column that failed to parse as doubles
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.6fareamount_string列的解析失败的值
- en: '[PRE17]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This results in the following:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 得到以下结果：
- en: '| fareamount_string | rows | percent |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| fareamount_string | 行数 | 百分比 |'
- en: '| NULL | 334964 | 0.5 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| NULL | 334,964 | 0.5 |'
- en: The SQL statement in listing 3.6 filters the set of the values of the fareamount_string
    to focus only on the cases where PySpark failed to parse the fare amount, or more
    precisely on the rows where fareamount_double (the column containing the outputs
    for the parsing algorithm) has a NULL value while the fareamount_string (the column
    containing the inputs to the parsing algorithm) is not a NULL value.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.6中的SQL语句过滤了fareamount_string值集合的一组值，仅关注PySpark无法解析车费金额的情况，或者更准确地说，fareamount_double（包含解析算法输出的列）的值为NULL而fareamount_string（包含解析算法输入的列）的值不为NULL的行。
- en: 'According to the output of the query, there are 334,964 such entries where
    the parse failed. All correspond to the case where fareamount_string is equal
    to a string value of ''NULL''. This is good news because only about 0.5% of the
    data set is impacted by this problem, and there is no additional work to be done:
    the ''NULL'' values cannot be converted to DOUBLE. Had the output of listing 3.6
    found cases where some of the DOUBLE values were not parsed because they contained
    extra characters, such as in strings ''$7.86'', it would have been necessary to
    implement additional code to correctly parse such values to DOUBLE.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 根据查询的输出，有334,964个条目的解析失败。所有这些对应的情况是fareamount_string等于'NULL'字符串值的情况。这是一个好消息，因为只有大约0.5%的数据集受到这个问题的影响，而且没有额外的工作要做：'NULL'值不能转换为DOUBLE。如果列表3.6的输出发现了一些DOUBLE值没有解析成功，因为它们包含额外的字符，比如'＄7.86'，那么就需要实现额外的代码来正确解析这样的值为DOUBLE。
- en: To continue the search for invalid fareamount values, it is worthwhile to explore
    some of the summary statistics of the fareamount_double column. The following
    SQL query moves the summary statistics computation into a separate subquery using
    two WITH clauses. Note that the data-specific query is packaged as a subquery
    named src and with the stats subquery referencing the result from src.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 为了继续搜索无效的fareamount值，值得探索fareamount_double列的一些摘要统计信息。以下SQL查询将摘要统计计算移到一个单独的子查询中，使用两个WITH子句。请注意，数据特定的查询被打包为一个名为src的子查询，并且stats子查询引用来自src的结果。
- en: Listing 3.7 Reusable pattern for de-coupling the statistics query from the data
    query
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.7 解耦统计查询与数据查询的可重用模式
- en: '[PRE18]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This results in the following:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是结果：
- en: '| min | q1 | q2 | q3 | max |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 分钟 | 四分位数1 | 四分位数2 | 四分位数3 | 最大值 |'
- en: '| –2064.71 | 7.03 | 9.73 | 13.78 | 745557.28 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| –2064.71 | 7.03 | 9.73 | 13.78 | 745557.28 |'
- en: 'Based on the minimum value in the data set reported by the output of the query
    in listing 3.7, it should be clear that the data set is impacted by a category
    of values that are not valid: the taxi fares should not be negative or less than
    $3.25\. Recall from the review of the DC taxi business rules in chapter 2 that
    the minimum charge for a taxi ride in DC is $3.25\. Let’s find the percentage
    of the data set impacted:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 根据列表3.7中查询输出报告的数据集中的最小值，应清楚地看出，数据集受到了一类无效值的影响：出租车费用不应为负值或低于3.25美元。回想一下第2章中对DC出租车业务规则的审查，DC的出租车乘车费用的最低收费为3.25美元。让我们找出数据集受影响的百分比：
- en: '[PRE19]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This results in the following:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是结果：
- en: '| percent |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 百分比 |'
- en: '| 0.49 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 0.49 |'
- en: The output indicates that only 0.49% of the rows are impacted by the fare amount
    values that are negative or below the minimum threshold, so they can be readily
    ignored by the analysis. From the validation standpoint this means that implementation
    of the validation rules should be modified to use the values greater than or equal
    to 3.25\.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 输出表明，只有0.49%的行受到了负值或低于最小阈值的车费值的影响，因此它们可以被分析时轻松忽略。从验证的角度来看，这意味着验证规则的实施应该修改为使用大于或等于3.25的值。
- en: 3.3.3 Improving the accuracy
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.3 提高准确性
- en: In this section, let’s take a closer look at the accuracy of the NULL values
    by comparing them against a reference data source of trip mileage values. As you
    learned in the previous section, the NULL values for taxi fare add up to just
    0.5% of the DC taxi data set. Using reference data in the mileage_double column
    can help you better understand the cases when the mileage of the trip translates
    into NULL fare amounts.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，让我们通过将它们与行程里程值的参考数据源进行比较，来更仔细地查看NULL值的准确性。正如您在上一节中学到的，DC出租车数据集中的NULL值仅占0.5%。在mileage_double列中使用参考数据可以帮助您更好地了解行程里程导致NULL车费金额的情况。
- en: Listing 3.8 Summary statistics for the mileage_double values
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.8 里程_double值的摘要统计
- en: '[PRE20]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This results in the following:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是结果：
- en: '| fareamount_string | min | q1 | q2 | q3 | max |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| fareamount_string | 最小值 | 四分位数1 | 四分位数2 | 四分位数3 | 最大值 |'
- en: '| NULL | 0.0 | 0.0 | 1.49 | 4.79 | 2591.82 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| NULL | 0.0 | 0.0 | 1.49 | 4.79 | 2591.82 |'
- en: The SQL statement in listing 3.8 reports the summary statistics (including minimum,
    maximum, and quartile values) for the mileage column only for the cases where
    the fareamount_string failed to parse, or more specifically where it is equal
    to 'NULL'. The output of the query indicates that more than a quarter of the cases
    (the lower quartile, the range from the minimum value up to 25th percentile) correspond
    to trips of 0 miles. At least a quarter of the mileage values (between the middle
    and upper quartiles, the range that includes 50th and 75th percentiles) appear
    to be in a reasonable mileage range for DC taxis.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.8中的SQL语句仅报告了里程列的摘要统计信息（包括最小值、最大值和四分位值），仅适用于fareamount_string解析失败的情况，或者更具体地说，它等于'NULL'的情况。查询的输出表明，超过四分之一的情况（下四分位数，从最小值到第25百分位数的范围）对应于0英里的行程。至少四分之一的里程值（位于中间和上四分位数之间，包括50和75百分位数的范围）似乎在DC出租车的合理里程范围内。
- en: At this point, you may consider several data augmentation experiments to try
    to recover missing fareamount_double data values by computing an estimate of the
    fare from the mileage column. The experiments could replace the missing fare amount
    values using an estimate. For example, you could replace the NULL fare amounts
    in the cases where the mileage is in the middle quartile with the arithmetic mean
    (average) fareamount for the known fare amounts in the same range. More sophisticated
    estimators, including machine learning models, are also possible.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，你可以考虑进行几项数据增强实验，试图通过从里程列计算车费的估算值来恢复丢失的fareamount_double数据值。这些实验可以使用估算值替换丢失的车费金额。例如，你可以将里程处于中四分位数范围内的缺失车费金额值替换为相同范围内已知车费金额的算术平均值（平均值）。也可以使用更复杂的估算器，包括机器学习模型。
- en: However, since the output in listing 3.8 indicates that it would help address
    roughly 0.12% (= 0.25 * 0.49%) of the data set, these experiments are unlikely
    to have a significant impact on the overall performance of the fare estimation
    model.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于列表3.8中的输出表明，它将帮助解决数据集约0.12%（= 0.25 * 0.49%）的问题，因此这些实验不太可能对车费估算模型的整体性能产生显著影响。
- en: Based on the output of the query in listing 3.7, the maximum value for the fare
    amount appears to be a garbage data point. Yet from the standpoint of the data
    schema it is valid, as 745,557.28 is less than the maximum value of the SQL DOUBLE
    data type.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 根据列表3.7中查询的输出，车费金额的最大值似乎是一个垃圾数据点。然而，从数据模式的角度来看，它是有效的，因为745,557.28小于SQL DOUBLE数据类型的最大值。
- en: Addressing the issue with the upper bound for fare amount values requires an
    application of an accuracy rule. Recall that validation checks should be performed
    without a reference to an external data source.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 解决车费金额值上限问题需要应用准确性规则。请回忆，验证检查应该在没有外部数据源的参考情况下进行。
- en: 'In the case of the DC taxi data set, the maximum fare amount is not explicitly
    specified as a business rule. However, using some commonsense reasoning and reference
    data outside of the DC taxi data set, you can come up with some sensible upper
    bounds on the maximum fare amount:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在华盛顿特区出租车数据集的情况下，最大车费金额未明确规定为一项业务规则。然而，通过一些常识推理和华盛顿特区出租车数据集之外的参考数据，你可以得出一些合理的最大车费金额上限：
- en: '*Estimate 1*. The maximum fare amount depends on the miles driven per work
    shift of a taxi driver. A quick internet search tells us that a DC taxi driver
    is required to take at least 8 hours of a break from driving within every 24-hour
    period. So, hypothetically, a driver may drive a maximum of 16 hours. According
    to websites of DC, Maryland, and Virginia, the maximum speed limit across those
    areas are capped at 70 mph. Even in the absurd case where the driver is driving
    16 hours at the maximum speed limit, the maximum distance travelled during that
    time is 1,120 miles.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*估算 1*。最大车费金额取决于出租车司机每个工作班次行驶的里程。一个快速的互联网搜索告诉我们，一名华盛顿特区出租车司机每24小时至少需要休息8小时。因此，假设性地，司机可能连续驾驶最长16小时。根据华盛顿特区、马里兰州和弗吉尼亚州的网站，这些地区的最高速度限制为70英里/小时。即使在司机以最高限速连续驾驶16小时的荒谬情况下，这段时间内的最大行驶里程也仅为1,120英里。'
- en: Clearly a 1,120-mile taxi ride with an estimated fare of **$2,422.45** (1,120
    miles * $2.16/mile + $3.25 base fare) is a hypothetical upper boundary that will
    not translate to an accurate DC taxi fare amount. However, instead of throwing
    out this estimate, the right thing to do is to take it under advisement and refine
    it by aggregating it with more estimates.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显然，一趟里程为1,120英里的出租车行程，估计车费为**$2,422.45**（1,120英里 * $2.16/英里 + $3.25基本车费），是一个不可能实现的上限，不会转化为准确的华盛顿特区出租车车费金额。然而，与其将此估算结果丢弃，正确的做法是加以考虑，并通过与更多估算结果的汇总来完善它。
- en: '*Estimate 2*. Instead of focusing on the distance traveled, you can also estimate
    the maximum fare amount based on time. Consider that according to the DC taxi
    fare rules, a taxi can be hired at $35 per hour. Since the maximum amount of time
    a cabbie is permitted to work is 16 hours, you can calculate another, distance-independent,
    estimate for the upper bound of the fare amount at $560 = 16 hour * $35/hour.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*估算 2*。与其专注于行驶距离，你也可以根据时间来估算最大车费金额。考虑到华盛顿特区出租车车费规定，一辆出租车每小时可以收取$35。由于出租车司机被允许工作的最长时间为16小时，你可以计算出另一个与距离无关的、车费金额的上限估算值为$560
    = 16小时 * $35/小时。'
- en: '*Estimate 3*. An upper bound on the taxi fare can also be based on the distance
    of a trip between the two furthest corners of the locations in the data set. The
    DC taxi data set boundary described in chapter 2 is roughly a square with downtown
    DC in the middle. You can find the locations of the lower-left and the upper-right
    points on the square using the following query:'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*估算 3*。出租车费用的上限也可以基于数据集中两个最远角落之间的行程距离。第 2 章所述的 DC 出租车数据集边界大致是个以市中心为中心的正方形。使用以下查询可以找出正方形上左下角和右上角点的位置:'
- en: '[PRE21]'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This results in the following:'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '得出以下结果:'
- en: '| lower_left_latitude | lower_left_longitude | upper_right_latitude | upper_right_longitude
    |'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| lower_left_latitude | lower_left_longitude | upper_right_latitude | upper_right_longitude
    |'
- en: '| 38.81138 | –77.113633 | 38.994909 | –76.910012 |'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 38.81138 | –77.113633 | 38.994909 | –76.910012 |'
- en: Plugging the latitude and longitude coordinates reported by the query into OpenStreetMap
    ([http://mng.bz/zEOZ](http://mng.bz/zEOZ)) yields 21.13 miles, or an estimate
    of **$48.89** (21.13 * $2.16/mile + $3.25).
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将查询报告的纬度和经度坐标插入 OpenStreetMap ([http://mng.bz/zEOZ](http://mng.bz/zEOZ)) 中，可得到
    21.13 英里总行程，或者 **$48.89** (21.13 X $2.16/英里 + $3.25) 的估算费用。
- en: '*Estimate 4*. For yet another estimation technique, recall that according to
    the central limit theorem (CLT) of statistics, the sum (and consequently the average)
    of arithmetic means of random samples[^(10)](#pgfId-1055007) of fare amount values
    is distributed according to the Gaussian (bell-shaped) distribution. You can generate
    a thousand random samples of arithmetic means of taxi mileages from the data (so
    you can later compute their mean) using an SQL statement.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*估算 4*。对于另一种估算技术，根据统计学中的中心极限定理(CL​T)，随机抽样[^(10)](#pgfId-1055007)所得到的车费值的算术平均数的总和(因此也是平均数)符合高斯（钟形）分布。根据
    SQL 语句，你可以从数据中生成一千个的出租车里程的算术平均数样本(以后可以计算它们的平均数)。'
- en: Listing 3.9 Unzipped files of the DC taxi trips data set from 2015 to 2019
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.9 2015 年至 2019 年 DC 出租车行程数据集的解压文件。
- en: '[PRE22]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Note the complex logic in the GROUP BY portion of the statement. The objectid
    column in the data set contains a unique identifier for every row of data represented
    with sequentially ordered integer values. You could use a GROUP BY MOD(CAST(objectid
    AS INTEGER), 1000) clause instead of the version in listing 3.9\. However, if
    the objectid values are based on the original order of the taxi trips in the data
    set, each resulting sample would be made of mileage values that are exactly 1,000
    rows apart from each other in the data set. Such an ordered, interval-structured
    sampling may introduce unintended bias in the calculations. For example, if there
    are roughly 1,000 taxi rides in DC per hour, and trains from DC to New York City
    leave the train station at the top of every hour, then some samples will contain
    primarily taxi rides terminating at the train station. Other regularly spaced
    samples may consist of too many end-of-day taxi rides.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 GROUP BY 版本语句的 GROUP BY 部分中的复杂逻辑。数据集中的 objectid 列包含每个数据行的唯一标识符，用顺序排列的整数值表示。你可以使用
    GROUP BY MOD(CAST(objectid AS INTEGER), 1000) 子句来替代列表 3.9 版本。然而，如果 objectid 值是基于数据集出租车行程的原始顺序排序，则每个结果样本都包含数据集中相距
    1,000 行的里程值。这种有序的间隔结构抽样可能会在计算中引入意外偏差。例如，如果华盛顿特区每小时大约有1,000辆出租车，而开往纽约市的火车车站每小时在整点留下的出租车可能会占据一些样本。其他定期间隔样本可能包含太多日终出租车行程。
- en: 'Random sampling (based on pseudorandom values used in computing) can address
    the bias problem of sampling over regular intervals. However, using a pseudorandom
    number generator to group values as in the following GROUP BY clause has several
    disadvantages:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '基于偏置问题而进行正常间隔抽样的随机抽样（基于计算中使用的伪随机值）可以解决。然而，在用如下 GROUP BY 子句将值分组时，使用伪随机数生成器有几个不足之处:'
- en: '[PRE23]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'First, it is impossible to exactly reproduce the results of the sampling since
    the random number generator does not guarantee deterministic behavior: there is
    no way to specify a random number seed that will guarantee a sequence of identical
    pseudorandom values across multiple executions of the SQL statement.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，由于随机数生成器不能保证确定性行为，所以无法准确地复制抽样结果。不能指定一个随机数种子，以在 SQL 语句的多个执行之间保证相同的伪随机值序列。
- en: Second, even if you attempted to pre-compute pseudo-random identifiers for every
    row in the data set and save the rows along with the identifiers to a separate
    table for future re-use, the table would soon become out of date. For example,
    if the DC taxi data set expanded to include 2020 taxi rides, a subsequent Glue
    crawler indexing of the data would invalidate the source data table and force
    re-creation of new pseudo-random identifiers.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast, the approach used in listing 3.9 and shown here has the advantages
    of the pseudorandom shuffling of the data set, eliminating unintended bias, and
    produces identical results across queries regardless of additions to the data
    set, as long as each row of data can be uniquely identified:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In the SQL statement, the application of the functions to objectid play the
    role of the unique identifier. The combination of the xxhash64 hashing function
    and the from_big_endian_64 produces what is effectively a pseudorandom but deterministic
    value from objectid.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: As a visual confirmation that the averages of the fare amount samples generated
    in listing 3.9 approximate the Gaussian distribution, the following histogram
    in figure 3.3 is a plot based on the listing with an arbitrary choice of the pseudorandom
    number seed value.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '![03-03](Images/03-03.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 Random sampling in listing 3.9 relies on the CLT for estimation.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that the original intention behind using 1,000 random samples of mean
    values in the average_mileage column was to compute the mean of the samples. Since
    in a normal distribution roughly 99.99% of the values are within four standard
    deviations away from the mean, the following SQL statement yields another statistical
    estimate for the upper bound on the taxi ride mileage and consequently another
    estimate for the upper bound on the fare amount:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This execution yields approximately 12.138 miles, or roughly **$29.47** (12.01
    * $2.16/mile + $3.25) as yet another upper-bound fare estimate. Of course, the
    advantage of the statistical approach explained in this section is that it can
    be used directly with the fareamount_double column, as in the following SQL statement:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This yields an upper bound of $15.96.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: While you can continue exploring alternative ideas for estimates, this is a
    good point to stop and evaluate the average upper bound for the fare amount so
    far.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: Using a simple averaging implementation in Python
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: shows that the estimated upper bound for taxi fare is $179.75
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: 'While it is certainly possible to continue working on ideas for a better upper
    bound, let’s estimate how much data is left after using the upper bound of $179.75:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This results in the following:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '| percent |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
- en: '| 0.48841 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
- en: Note that only about 0.49% percent of the data was excluded based on the bounds.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: 'However, rerunning the summary statistics on the fareamount_double column using
    the new bounds produces significantly more sensible summary statistics:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This results in the following:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '| min | q1 | q2 | q3 | max | mean | std |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
- en: '| 3.25 | 7.03 | 9.73 | 13.78 | 179.83 | 11.84 | 8.65 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 3.25 | 7.03 | 9.73 | 13.78 | 179.83 | 11.84 | 8.65 |'
- en: Now that the accuracy checks for the fareamount column are done, you should
    be ready to repeat the accuracy exercise with the pickup and drop-off coordinates.
    While it is possible to determine whether the latitude and longitude coordinates
    are valid based on their value alone, you need a reference data source to decide
    whether a value is accurate. The OpenStreetMap service used to generate the DC
    taxi map in chapter 2 can also be used to confirm the accuracy of the origin and
    destination coordinates in the data set.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，已经完成了对 fareamount 列的准确性检查，您应该准备好使用接送坐标重复进行准确性练习了。虽然可能根据值本身确定纬度和经度坐标是否有效，但是您需要一个参考数据源来决定一个值是否准确。用于在第二章生成
    DC 出租车地图的 OpenStreetMap 服务也可以用于确认数据集中起始点和目的地坐标的准确性。
- en: 'Using the SQL statement and OpenStreetMap ([http://mng.bz/01ez](http://mng.bz/01ez))
    to check the minimum and maximum coordinates for the origin latitude and longitude
    columns confirms that resulting pairs (38.81138, —77.113633) and (38.994217, —76.910012)
    are within DC boundaries:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 SQL 语句和 OpenStreetMap ([http://mng.bz/01ez](http://mng.bz/01ez)) 来检查原始纬度和经度列的最小和最大坐标，确认结果对(38.81138,
    —77.113633)和(38.994217, —76.910012)在 DC 边界内：
- en: '[PRE30]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This outputs the following:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下内容：
- en: '| olat_min | olon_min | olat_max | olon_max | dlat_min | dlon_min | dlat_max
    | dlon_max |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| olat_min | olon_min | olat_max | olon_max | dlat_min | dlon_min | dlat_max
    | dlon_max |'
- en: '| 38.81138 | –77.113633 | 38.994909 | –76.910012 | 38.81138 | –77.113633 |
    38.994217 | –76.910012 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 38.81138 | –77.113633 | 38.994909 | –76.910012 | 38.81138 | –77.113633 |
    38.994217 | –76.910012 |'
- en: 3.4 Implementing VACUUM in a PySpark job
  id: totrans-311
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 在 PySpark 作业中实现 VACUUM
- en: In this section, you will use what you learned about data quality in the DC
    taxi data set and apply your findings to implement a PySpark job. The purpose
    of the job is to perform high-throughput data cleanup of the dc_taxi_parquet table
    populated in chapter 2 using a distributed cluster of Apache Spark servers available
    from AWS Glue. The job should be implemented as a single Python file named dctaxi_parquet_vacuum.py;
    however, in this section, the file is split into separate code snippets, which
    are explained one by one in the upcoming paragraphs. The cleaned-up copy of the
    data set will be saved by the job to the parquet/vacuum subfolder in your S3 bucket.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，您将运用在 DC 出租车数据集中学到的数据质量知识，并将您的发现应用于实现一个 PySpark 作业。该作业的目的是使用 AWS Glue
    提供的分布式 Apache Spark 服务器群集执行 dc_taxi_parquet 表的高吞吐量数据清理，该表在第二章中填充。该作业应该实现为一个名为
    dctaxi_parquet_vacuum.py 的单个 Python 文件；然而，在这一节中，该文件被拆分成了几个单独的代码片段，这些片段将在接下来的段落中逐一解释。数据集的清理副本将由该作业保存到您
    S3 存储桶中的 parquet/vacuum 子文件夹中。
- en: The initial part of the code snippet for the PySpark job is in listing 3.10\.
    Note that the lines of code up until ❶ are identical to the code from the PySpark
    job in chapter 2\. This should not come as a surprise because this part of the
    code involves the import of prerequisite libraries and assignment of commonly
    used variables in PySpark jobs. The line of code annotated with ❶ is the first
    that’s distinct from the chapter 2 PySpark job. Note that the line is reading
    the Parquet-formatted data set you created at the end of chapter 2 and have been
    querying using Athena in this chapter.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 该 PySpark 作业的代码片段的初始部分在列表 3.10 中。请注意，直到❶处的代码行与第二章中的 PySpark 作业中的代码是相同的。这应该不会让人感到惊讶，因为代码的这部分涉及到
    PySpark 作业中的先决条件库的导入和常用变量的分配。带有❶注释的代码行是与第二章 PySpark 作业不同的第一个代码行。请注意，该行正在读取您在第二章末尾创建的
    Parquet 格式数据集，并在本章中一直在使用 Athena 进行查询。
- en: Listing 3.10 PySpark DataFrame reading code in dctaxi_parquet_vacuum.py
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在 dctaxi_parquet_vacuum.py 中的列表 3.10 中的 PySpark DataFrame 读取代码
- en: '[PRE31]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: ❶ Read the source Parquet data set into a Spark DataFrame.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将源 Parquet 数据集读入 Spark DataFrame。
- en: To select the subset of the data for cleanup, the Spark DataFramecreateOrReplaceTempView
    method is invoked from the line with ❶ in listing 3.11\. The method creates a
    temporary view named dc_taxi_parquet as part of the SparkSession, which is accessible
    via the spark variable. The view enables Spark to query the DataFrame created
    at ❶ using the SQL query that starts on the line with ❷ referencing the dc_taxi_
    parquet view ❸.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 为了选择要清理的数据子集，从列表 3.11 中带有❶的行开始调用 Spark DataFramecreateOrReplaceTempView 方法。该方法创建一个名为
    dc_taxi_parquet 的临时视图，作为 SparkSession 的一部分，可以通过 spark 变量访问。该视图使 Spark 能够查询在❶处创建的
    DataFrame，使用从❷行开始的 SQL 查询，引用 dc_taxi_parquet 视图❸。
- en: The content of the WHERE clause that begins at ❹ should not come as a surprise.
    The checks for NULL values and the range bounds for the fareamount_double column
    are exactly the condition defined in section 3.3.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 从 ❹ 开始的 WHERE 子句的内容不应该令人惊讶。对于 NULL 值的检查和 fareamount_double 列的范围边界检查恰好是在第 3.3
    节中定义的条件。
- en: The call to the method replace at ❺ replaces any instances of newline characters
    in the multiline string with an empty character. The replace method is needed
    to ensure that the multiline string used to specify the SQL query in the PySpark
    job is compatible with the SQL query parser used by Spark.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ❺ 处调用 replace 方法，将多行字符串中的任何换行符实例替换为空字符。需要使用 replace 方法确保用于指定 PySpark 作业中 SQL
    查询的多行字符串与 Spark 使用的 SQL 查询解析器兼容。
- en: Listing 3.11 PySpark data cleanup implementation saved to dc_taxi_vacuum.py
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.11 PySpark 数据清理实现保存到 dc_taxi_vacuum.py
- en: '[PRE32]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: ❶ Alias the source data set in df as dc_taxi_parquet for Spark SQL API.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将源数据集在 df 中别名为 dc_taxi_parquet，以供 Spark SQL API 使用。
- en: ❷ Create a DataFrame query_df populated based on the SQL query in this snippet.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建一个基于此片段中 SQL 查询填充的 DataFrame 查询_df。
- en: ❸ Query dc_taxi_parquet to output the clean values for further analysis.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 查询 dc_taxi_parquet 以输出干净的值以进行进一步分析。
- en: ❹ Filter records according to the VACUUM analysis in section 3.3.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 根据第 3.3 节中的 VACUUM 分析过滤记录。
- en: ❺ Eliminate newline characters in the Python multiline string for Spark SQL
    API compatibility.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 消除 Python 多行字符串中的换行符，以确保与 Spark SQL API 的兼容性。
- en: Since the original STRING-formatted column origindatetime_tr in the data set
    needs to be formatted as a numeric value for machine learning, the PySpark DataFrame
    API code in listing 3.12 first converts the column to a SQL TIMESTAMP ❶, eliminating
    any NULL values that may have been produced due to failed conversion from STRING
    to TIMESTAMP. The derived column is then further broken up into numeric, INTEGER
    columns, including year, month, day of the week (dow), and hour of the taxi trip.
    The last step following the conversion removes the temporary origindatetime_ts
    column, drops any records with missing data, and eliminates duplicate records.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据集中原始的 STRING 格式列 origindatetime_tr 需要格式化为机器学习的数值，列表 3.12 中的 PySpark DataFrame
    API 代码首先将该列转换为 SQL TIMESTAMP ❶，消除由于从 STRING 转换为 TIMESTAMP 失败而产生的任何 NULL 值。然后，衍生的列进一步分解为数字、INTEGER
    列，包括出租车行程的年、月、星期几（dow）和小时。转换后的最后一步移除了临时的 origindatetime_ts 列，删除了任何缺失数据的记录，并消除了重复记录。
- en: Listing 3.12 PySpark data cleanup implementation saved to dc_taxi_vacuum.py
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.12 PySpark 数据清理实现保存到 dc_taxi_vacuum.py
- en: '[PRE33]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: ❶ Parse the trip origindatetime_tr timestamp using the dd/MM/yyyy HH:mm pattern.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用 dd/MM/yyyy HH:mm 模式解析行程 origindatetime_tr 时间戳。
- en: ❷ Construct numeric columns based on year, month, day of the week, and hour
    of trip.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 根据行程的年、月、星期几和小时构建数字列。
- en: ❸ Eliminate any records with missing or duplicated data.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 消除任何具有缺失或重复数据的记录。
- en: The concluding part of the PySpark job, shown in listing 3.13, persists the
    resulting PySpark DataFrame as a Parquet-formatted data set in the AWS S3 location
    specified by the BUCKET_DST_PATH parameter. Note that the listing declares a save_stats_metadata
    function, which computes the summary statistics (using the PySpark describe function)
    of the cleaned-up data set, and saves the statistics as a single CSV file located
    in a AWS S3 subfolder named .meta/stats under the S3 location from the BUCKET_DST_
    PATH parameter.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 作业的结束部分，如列表 3.13 所示，将结果的 PySpark DataFrame 持久化为 Parquet 格式的数据集，保存在由
    BUCKET_DST_PATH 参数指定的 AWS S3 位置。请注意，该列表声明了一个 save_stats_metadata 函数，该函数使用 PySpark
    describe 函数计算清理后数据集的摘要统计信息，并将统计信息保存为位于 AWS S3 位置下名为 .meta/stats 的 S3 子文件夹中的单个
    CSV 文件。
- en: Listing 3.13 PySpark data cleanup implementation saved to dc_taxi_vacuum.py
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.13 PySpark 数据清理实现保存到 dc_taxi_vacuum.py
- en: '[PRE34]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: ❶ Persist the cleaned-up data set to BUCKET_DST_PATH in Parquet format.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将清理后的数据集持久化到 Parquet 格式的 BUCKET_DST_PATH。
- en: ❷ Save the metadata about the cleaned-up data set as a separate CSV file.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将关于清理后数据集的元数据保存为单独的 CSV 文件。
- en: For convenience, the entire PySpark job described is shown. Prior to executing
    this job, make sure you save the contents of the code listing to a file named
    dc_taxi_ vacuum.py.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 为方便起见，显示了整个 PySpark 作业的描述。在执行此作业之前，请确保将代码列表的内容保存到名为 dc_taxi_vacuum.py 的文件中。
- en: Listing 3.14 PySpark data clean-up code saved to dc_taxi_vacuum.py
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.14 PySpark 数据清理代码保存到 dc_taxi_vacuum.py
- en: '[PRE35]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The utils.sh script file first introduced in section 3.3 includes bash functions
    to simplify execution of PySpark jobs in AWS Glue from your bash shell. Notice
    that in listing 3.15 the PySpark job from listing 3.14 is referenced by the file
    name dctaxi_ parquet_vacuum.py and is used to start the AWS Glue job named dc-taxi-parquet-vacuum-job.
    The job uses the Parquet-formatted DC taxi data set that you analyzed earlier
    in this chapter to populate the parquet/vacuum subfolder of your AWS S3 bucket
    with a cleaned-up version of the data. The VACUUM-ed version is also persisted
    in the Parquet format.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.15 Using bash to launch the PySpark job in dctaxi_parquet_vacuum.py
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Assuming that the PySpark job from listing 3.15 completes successfully, you
    should observe an output resembling the following:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Summary
  id: totrans-346
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An interactive query service like AWS Athena can help with exploratory data
    analysis of structured data sets ranging from gigabytes to terabytes in size.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The schema-on-read approach enables interactive query services to apply multiple,
    different data schemas to the same data set.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VACUUM principles can help your machine learning project develop mature data
    quality practices compared to the garbage in, garbage out approach.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An interactive query service, such as the PrestoDB-based AWS Athena and distributed
    data-processing platform Apache Spark-based AWS Glue, can be used to implement
    VACUUM principles for data sets stored in a public cloud.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ^(1.)While a 2016 survey ([http://mng.bz/Mvr2](http://mng.bz/Mvr2)) claimed
    that 60% of the data scientists’ time is spent addressing data quality issues,
    a more recent and larger survey brought the estimate down to 15% ([http://mng.bz/
    g1KR](http://mng.bz/g1KR)). For more insights about these oft-cited statistics,
    check out [http://mng.bz/ePzJ](http://mng.bz/ePzJ).
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: ^(2.) The CSV file for the sample data set of five DC taxi rides is available
    from [http://mng.bz/OQrP](http://mng.bz/OQrP).
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: '^(3.)Examples include Google BigQuery: [https://cloud.google.com/bigquery](https://cloud.google.com/bigquery).'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '^(4.)In 2016, an influential *Harvard Business Review* article cited another
    study, “Bad Data Costs the U.S. $3 Trillion Per Year”: [http://mng.bz/Yw47](http://mng.bz/Yw47).'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: '^(5.)The Payment Card Industry Data Security Standard specifies, among other
    things, the requirements for storing cardholder data: [https://www.pcicomplianceguide.org/faq/#1](https://www.pcicomplianceguide.org/faq/#1).'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: '^(6.)The Luhn algorithm is named after IBM scientist Hans Peter Luhn: [https://spectrum.ieee.org/hans-peter-luhn-and-the-birth-of-the-hashing-algorithm](https://spectrum.ieee.org/hans-peter-luhn-and-the-birth-of-the-hashing-algorithm).'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: '^(7.)*Chesterton Tribune*, a daily newspaper in Indiana, published an article
    about the Valparaiso debacle: [http:// mng.bz/GOAN](http://mng.bz/GOAN).'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: ^(8.) The timestamp is stored as a string using the month/day/year hour:minute
    format.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: '^(9.)The warning about SELECT * that you read about in this chapter does not
    apply to COUNT(*) since the two are fundamentally different operations in SQL:
    the former returns the values from all columns for every row, while the latter
    returns just the row count.'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: ^(9.)关于在本章中阅读到的关于SELECT *的警告不适用于COUNT(*)，因为在SQL中，这两者是基本不同的操作：前者返回每一行的所有列的值，而后者仅返回行数。
- en: ^(10.)The sample size is expected to consist of at least a few tens of records,
    and the records should be independent and sampled with replacement.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: ^(10.)样本量预计至少包含几十个记录，并且记录应该是独立的，并带有替换抽样。
