- en: 8 Metadata and artifact store
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 元数据和工件存储
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容包括：
- en: Understanding and managing metadata in the deep learning context
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在深度学习背景下理解和管理元数据。
- en: Designing a metadata and artifact store to manage metadata
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计一个元数据和工件存储来管理元数据
- en: 'Introducing two open source metadata management tools: ML Metadata and MLflow'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍两个开源的元数据管理工具：ML Metadata 和 MLflow。
- en: To produce a high-quality model that fits business requirements, data scientists
    need to experiment with all kinds of datasets, data processing techniques, and
    training algorithms. To build and ship the best model, they spend a significant
    amount of time conducting these experiments.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成符合业务需求的高质量模型，数据科学家需要尝试各种数据集、数据处理技术和训练算法。为了构建和发布最佳模型，他们花费了大量时间进行这些实验。
- en: A variety of *artifacts* (datasets and model files) and *metadata* are produced
    from model training experiments. The metadata may include model algorithms, hyperparameters,
    training metrics, and model versions, which are very helpful in analyzing model
    performance. To be useful, this data must be persistent and retrievable.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练实验产生了各种*工件*（数据集和模型文件）和*元数据*。元数据可能包括模型算法、超参数、训练指标和模型版本等，这些对分析模型性能非常有帮助。为了有用，这些数据必须是持久的和可检索的。
- en: When data scientists need to investigate a model performance problem or compare
    different training experiments, is there anything we, as engineers, can do to
    facilitate these efforts? For example, can we make model reproducing and experiment
    comparison easier?
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据科学家需要调查模型性能问题或比较不同的训练实验时，作为工程师，我们是否有什么可以做的来促进这些努力呢？例如，我们是否可以使模型的再现和实验比较更容易？
- en: The answer is yes. As engineers, we can build a system that retains the experimental
    metadata and artifacts that data scientists need to reproduce and compare models.
    And if we design this storage and retrieval system well, with proper metadata
    management, data scientists can easily select the best model from a series of
    experiments or figure out the root cause of a model degradation quickly without
    a deep understanding of the metadata system.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是肯定的。作为工程师，我们可以构建一个系统，保留数据科学家需要再现和比较模型的实验元数据和工件。如果我们设计这个存储和检索系统得当，并进行适当的元数据管理，数据科学家可以轻松地从一系列实验中选择最佳模型，或者快速找出模型退化的根本原因，而不需要深入了解元数据系统。
- en: 'In previous chapters, we’ve learned about designing services to produce and
    serve models. Here, we turn our attention to the metadata and artifacts management
    system that facilitates two more key operations: troubleshooting and comparing
    experiments.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们已经学习了设计服务来生成和提供模型。在这里，我们将注意力转向元数据和工件管理系统，这个系统有助于两个更重要的操作：故障排除和比较实验。
- en: 'We will start this chapter with an introduction to *artifacts* and *metadata*
    and the meaning of these concepts in the context of deep learning. Then we will
    show you how to design metadata management systems using examples and emphasizing
    design principles. Finally, we will discuss two open source metadata management
    systems: MLMD (ML Metadata) and MLflow. By reading this chapter, you will gain
    a clear vision of how to manage metadata and artifacts to facilitate experiment
    comparison and model troubleshooting.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从介绍*工件*和*元数据*开始这一章，并讨论这些概念在深度学习背景下的含义。然后我们将通过示例和强调设计原则来向您展示如何设计元数据管理系统。最后，我们将讨论两个开源的元数据管理系统：MLMD（ML
    Metadata）和 MLflow。通过阅读本章，您将清楚地了解如何管理元数据和工件，以便促进实验比较和模型故障排除。
- en: 8.1 Introducing artifacts
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 介绍工件
- en: People often assume that an artifact in deep learning is the model file produced
    by the model training process. This is partially true. Artifacts are actually
    the files and objects that form both the inputs and outputs of the components
    in the model training process. This is a crucial distinction, and it is important
    to keep this broader definition in mind if you want to engineer a system that
    supports model reproducibility.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 人们经常假设在深度学习中，工件是模型训练过程产生的模型文件。这在某种程度上是正确的。工件实际上是组成模型训练过程中组件的输入和输出的文件和对象。这是一个关键的区别，如果你想设计一个支持模型再现性的系统，记住这个更广泛的定义是很重要的。
- en: Under this definition, artifacts can include datasets, models, code, or any
    other number of objects used in a deep learning project. For example, the raw
    input training data, the labeled dataset produced from a labeling tool, and the
    results data of a data processing pipeline are all considered artifacts.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个定义下，工件可以包括用于深度学习项目的数据集、模型、代码或任何其他数量的对象。例如，原始输入训练数据、通过标记工具生成的带标签数据集以及数据处理流水线的结果数据都被认为是工件。
- en: In addition, artifacts must be preserved with metadata that describes their
    facts and lineage to allow performance comparisons, reproducibility, and troubleshooting.
    In practice, artifacts are stored as raw files on a file server or cloud storage
    service, such as Amazon Simple Storage Service or Azure Blob Storage. And we associate
    artifacts with their metadata in a *metadata store* on a separate storage service.
    See figure 8.1 for a diagram of what this arrangement typically looks like.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了进行性能比较、可重现性和故障排除，必须将工件与描述其事实和血统的元数据一起保存。在实践中，工件以原始文件的形式存储在文件服务器或云存储服务（例如Amazon
    Simple Storage Service或Azure Blob Storage）上。我们将工件与其元数据关联在一个独立的存储服务上的*元数据存储*中。请参见图
    8.1，了解此安排通常的外观。
- en: Figure 8.1 displays the common practice of managing artifacts. The artifact
    files are saved to a file storage system, and their file URLs are saved with other
    related metadata (such as model training execution ID and model ID) in the metadata
    store. This setup allows us—or data scientists—to search for a model in the metadata
    store and easily find all the input and output artifacts of the corresponding
    model training process.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 显示了管理工件的常见做法。工件文件保存在文件存储系统中，并将其文件 URL 与其他相关元数据（例如模型训练执行 ID 和模型 ID）一起保存在元数据存储中。这样的设置允许我们
    - 或数据科学家 - 在元数据存储中搜索模型，并轻松找到相应模型训练过程的所有输入和输出工件。
- en: '![](../Images/08-01.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08-01.png)'
- en: Figure 8.1 Artifacts are associated with their metadata in the metadata store.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 工件与其元数据相关联存储在元数据存储中。
- en: 8.2 Metadata in a deep learning context
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 深度学习上下文中的元数据
- en: In general terms, metadata is structured reference data that provides information
    about other data or objects, such as the nutrition facts label on packaged food.
    In machine learning (ML) and deep learning, however, metadata is more specific
    to models; it’s the data that describes model training executions (runs), workflows,
    models, datasets, and other artifacts.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 从一般意义上讲，元数据是结构化的参考数据，提供有关其他数据或对象的信息，例如包装食品上的营养事实标签。然而，在机器学习（ML）和深度学习中，元数据更具体地指的是模型的数据；它是描述模型训练执行（运行）、工作流、模型、数据集和其他工件的数据。
- en: For any distributed system, we track service metadata in the form of logs and
    metrics at a service level. For example, we might track metrics such as CPU rate,
    number of active users, and number of failed web requests. We use these metrics
    for system/ service monitoring, troubleshooting, and observation purposes.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何分布式系统，我们通过日志和度量标准以服务级别跟踪服务元数据。例如，我们可能跟踪 CPU 使用率、活跃用户数量和失败的 Web 请求数量等度量标准。我们使用这些度量标准进行系统/服务监控、故障排除和观察。
- en: In deep learning systems, beyond the service-level metric, we collect metadata
    for model troubleshooting, comparison, and reproduction purposes. You can think
    of deep learning metadata as a special subset of logs and metrics that we use
    to monitor and track every deep learning activity in the system. These activities
    include data parsing, model training, and model serving.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习系统中，除了服务级别指标之外，我们还收集用于模型故障排除、比较和重现的元数据。您可以将深度学习元数据视为系统中用于监视和跟踪每个深度学习活动的一种特殊子集的日志和度量标准。这些活动包括数据解析、模型训练和模型服务。
- en: 8.2.1 Common metadata categories
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.1 常见的元数据类别
- en: 'Although we’ve just defined metadata, the term is actually somewhat arbitrary;
    there is no set guideline on which data should be considered metadata. For engineers
    of deep learning systems, we recommend defining metadata in the following four
    categories: model training run, general artifact, model file, and orchestration
    workflow. To give you a concrete feel for these categories, let’s look at each
    category and some examples of metadata that go into each one.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们刚刚定义了元数据，但这个术语在某种程度上实际上是相当任意的；没有关于哪些数据应被视为元数据的固定准则。对于深度学习系统的工程师，我们建议将元数据定义分为以下四个类别：模型训练运行、通用工件、模型文件和编排工作流。为了让您对这些类别有一个具体的感觉，让我们来看看每个类别以及每个类别中包含的一些示例元数据。
- en: Metadata for a model training run
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练运行的元数据
- en: To reproduce models, analyze model performance, and facilitate model troubleshooting,
    we need to track all the input and output data and artifacts of a model training
    run. This includes
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '*Dataset ID and version*—The unique identity of the dataset used in model training.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hyperparameters*—The hyperparameters used in the training, such as learning
    rate and number of epochs.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hardware resources*—CPU, GPU, TPU, memory, and disk size allocated in the
    training and the actual consumption of these resources.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Training code version*—The unique identity of the training code snapshot used
    in model training.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Training code configuration*—The configuration for recreating the training
    code execution environment, such as conda.yml, Dockerfile, and requirement.txt.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Training metrics*—The metrics show how model training progresses, for example,
    the loss value at each training epoch.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model evaluation metrics*—The metrics show the model performance, such as
    F-score and root-mean-squared error (RMSE).'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metadata for general artifacts
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 'Artifacts can be any arbitrary files, such as datasets, models, and prediction
    results. To be able to find the artifact in artifact storage, we want to track
    below metadata for artifacts:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '*File location*—The path to the place where the artifact is stored, for example,
    Amazon S3 file path or internal file system path'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*File version*—The unique identity to distinguish different file updates'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Description*—The additional information to describe what’s inside the artifact
    file'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Audit history*—The information about who created the artifact version, when
    the artifact was created, and how it was created'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metadata for model file
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: 'Models are one kind of artifact, but because models are the main product of
    every deep learning system, we recommend tracking model metadata separately from
    other artifacts. When we define model metadata, it’s best to consider two perspectives:
    model training and model serving.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: For model training, to have model lineage, we want to keep a mapping between
    a model and the model training run that produced it. Model lineage is important
    for model comparison and reproduction. For example, when comparing two models,
    by having the link of model training run and model, data scientists can easily
    determine all the details of how the model is produced, including the input datasets,
    training parameters, and training metrics. The model training metrics are very
    useful for understanding model performance.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: For model serving, we want to track the model execution data for future model
    performance analysis. These execution data, such as model response latency and
    prediction miss rate, are very useful for detecting model performance degradation.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are a few recommended model metadata categories besides the aforementioned
    general artifact metadata:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '*Resource consumption*—Memory, GPU, CPU, and TPU consumption for model serving'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model training run*—The model training run ID, which is used to find the code,
    dataset, hyperparameters, and environments that create the model'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型训练运行*——模型训练运行ID，用于查找创建模型的代码、数据集、超参数和环境'
- en: '*Experiment*—Tracking the model experiment activities in production, for example,
    customer traffic distribution for different model versions'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*实验*——跟踪在生产环境中的模型实验活动，例如不同模型版本的用户流量分布'
- en: '*Production*—Model usage in production, such as query per second and model
    prediction statistics'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*生产*——模型在生产环境中的使用，例如每秒查询和模型预测统计'
- en: '*Model performance*—Tracking model evaluation metrics for drift detection,
    such as concept drift and performance drift'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型性能*——跟踪模型评估指标以检测漂移，如概念漂移和性能漂移'
- en: Note Models will, unavoidably, start performing worse once they are shipped
    to production. We call this behavior *model degradation*. As the statistical distribution
    of the target group changes over time, model prediction becomes less accurate.
    New popular slogans, for example, can affect the accuracy of voice recognition.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 注：不可避免地，一旦模型上线，它们将开始表现得更差。我们称此行为为*模型退化*。随着目标组的统计分布随时间变化，模型预测变得不太准确。例如，新的流行口号可能会影响语音识别的准确性。
- en: Metadata for pipeline
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 用于流水线的元数据
- en: 'A pipeline or workflow is needed when we want to automate a multiple-step model
    training task. For example, we can use workflow management tools like Airflow,
    Kubeflow, or Metaflow to automate a model training process that contains multiple
    functional steps: data collection, feature extraction, dataset augmentation, training,
    and model deployment. We will discuss workflow in detail in the next chapter.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要自动化多步模型训练任务时，需要使用流水线或工作流程。例如，我们可以使用工作流管理工具，如Airflow、Kubeflow或Metaflow，自动化包含多个功能步骤的模型训练过程：数据收集、特征提取、数据集增强、训练和模型部署。我们将在下一章中详细讨论工作流程。
- en: For pipeline metadata, we usually track the pipeline execution history and the
    pipeline input and output. This data can provide audit information for future
    troubleshooting.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对于流水线元数据，我们通常会跟踪流水线执行历史和流水线输入输出。这些数据可以为将来的故障排除提供审计信息。
- en: Note Deep learning projects vary a lot. The model training code is dramatically
    different for voice recognition, natural language processing, and image generation.
    There are many factors specific to the project, such as the size/type of the dataset,
    type of the ML model, and input artifacts. Besides the sample metadata mentioned
    previously, we recommend you define and collect the metadata based on your project.
    When you are looking for data that helps model reproducing and troubleshooting,
    the metadata list will come to you naturally.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 注：深度学习项目差异很大。语音识别、自然语言处理和图像生成的模型训练代码截然不同。项目特定的因素有很多，例如数据集的大小/类型、ML模型的类型和输入资产。除了先前提到的样本元数据外，我们建议您基于项目定义和收集元数据。当您寻找有助于模型再现和故障排除的数据时，元数据列表将自然而然地出现在您面前。
- en: 8.2.2 Why manage metadata?
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.2 为什么要管理元数据？
- en: Because metadata is normally instrumented or recorded in the form of logs or
    metrics, you may wonder why we need to manage deep learning metadata separately.
    Can we simply fetch the deep learning metadata from log files? Log management
    systems like Splunk ([https://www.splunk.com/](https://www.splunk.com/)) and Sumo
    Logic ([https://www.sumologic.com/](https://www.sumologic.com/)) come in very
    handy because they allow developers to search and analyze logs and events produced
    by distributed systems.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 由于元数据通常是通过日志或指标的形式进行仪表化或记录的，因此您可能会想知道为什么我们需要单独管理深度学习元数据。我们能否简单地从日志文件中提取深度学习元数据？日志管理系统，如Splunk（[https://www.splunk.com/](https://www.splunk.com/)）和Sumo
    Logic（[https://www.sumologic.com/](https://www.sumologic.com/)），非常方便，因为它们允许开发人员搜索和分析分布式系统生成的日志和事件。
- en: To better explain the necessity of having a dedicated component for managing
    metadata in a deep learning system, we will convey a story. Julia (data engineer),
    Ravi (data scientist), and Jianguo (system developer) work together on a deep
    learning system to develop intent classification models for a chatbot application.
    Ravi develops intent classification algorithms, Julia works on data collection
    and parsing, and Jianguo develops and maintains the deep learning system.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地解释在深度学习系统中需要专门的元数据管理组件的必要性，我们将讲述一个故事。Julia（数据工程师）、Ravi（数据科学家）和Jianguo（系统开发人员）一起开发一个深度学习系统，为聊天机器人应用程序开发意图分类模型。Ravi开发意图分类算法，Julia负责数据收集和解析，而Jianguo则开发和维护深度学习系统。
- en: During the project development and test phases, Julia and Ravi work together
    to build an experimental training pipeline to produce intent models. After the
    models are built, Ravi passes them to Jianguo to deploy the experiment models
    to the prediction service and test them with real customer requests.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在项目开发和测试阶段，Julia 和 Ravi 共同合作构建了一个实验性的训练流水线来生成意图模型。模型构建完成后，Ravi 将其传递给 Jianguo，以部署实验模型到预测服务并使用真实客户请求进行测试。
- en: When Ravi feels good about the experimentation, he promotes the training algorithm
    from the experimental pipeline to an automated production training pipeline. This
    pipeline runs in the production environment and produces intent models with customer
    data as input. The pipeline also deploys the models to the prediction service
    automatically. Figure 8.2 illustrates the whole story setting.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当 Ravi 对实验感到满意时，他会将训练算法从实验性流水线推广到自动化生产训练流水线。该流水线在生产环境中运行，并以客户数据作为输入生成意图模型。该流水线还会自动将模型部署到预测服务。图
    8.2 描绘了整个故事背景。
- en: '![](../Images/08-02.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08-02.png)'
- en: Figure 8.2 Model development without metadata management
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2 没有元数据管理的模型开发
- en: A few weeks later, after Ravi released the latest intent classification algorithm,
    one chatbot customer—BestFood Inc.—reported a model performance degradation problem
    to Ravi. In the investigation request, BestFood mentioned that their bot’s intent
    classification accuracy dropped 10% after using a new dataset.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 几周后，当 Ravi 发布了最新的意图分类算法后，一个聊天机器人客户——BestFood 公司向 Ravi 报告了模型性能下降的问题。在调查请求中，BestFood
    提到他们的机器人在使用新数据集后的意图分类准确率下降了 10%。
- en: To troubleshoot the reported model performance degradation problem, Ravi needs
    to verify lots of information. He first needs to check which model version is
    currently being used by BestFood in the prediction service and then check the
    model lineage of the current model, such as the dataset version and code version
    used in the training pipeline. After that, Ravi may also need to reproduce the
    model for local debugging. He needs to compare the current model and the previous
    model to test the data distribution effect (current new dataset vs. previous dataset).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了排除报告的模型性能下降问题，Ravi 需要验证大量信息。他首先需要检查 BestFood 当前在预测服务中使用的模型版本，然后检查当前模型的模型衍生情况，例如在训练流水线中使用的数据集版本和代码版本。之后，Ravi
    还可能需要重新生成模型进行本地调试。他需要比较当前模型和之前的模型，以测试数据分布的影响（当前新数据集 vs. 之前的数据集）。
- en: Ravi is a natural language process (NLP) expert, but he has very little knowledge
    about the deep learning system on which his training code runs. To continue his
    investigation, he has to ask Jianguo and Julia to obtain the relevant model, dataset,
    and code information. Because everyone has only a fragment of knowledge about
    the model training application and underlying deep learning system/infrastructure,
    for each model performance troubleshooting, Ravi, Julia, and Jianguo have to work
    together to grasp the full context, which is time-consuming and inefficient.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Ravi 是一个自然语言处理（NLP）专家，但他对于运行其训练代码的深度学习系统了解甚少。为了继续他的调查，他不得不向 Jianguo 和 Julia
    请求获取相关的模型、数据集和代码信息。因为每个人对模型训练应用和底层深度学习系统/基础设施的知识只是一知半解，对于每一个模型性能故障排除，Ravi、Julia
    和 Jianguo 不得不共同努力来掌握完整的背景，这是耗时且低效的。
- en: This story, of course, is oversimplified. In practice, deep learning project
    development is made of data, algorithms, system/runtime development, and hardware
    management. The whole project is owned by different teams, and there is seldom
    one person who knows everything. Relying on cross-team collaboration to troubleshoot
    model-related problems is unrealistic in a corporate setting.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这个故事过于简化了。在实践中，深度学习项目开发包含了数据、算法、系统/运行时开发和硬件管理。整个项目由不同的团队负责，并且很少有一个人了解所有内容。在企业环境中依赖跨团队合作来排除与模型相关的问题是不现实的。
- en: The key factor missing in figure 8.2 is an effective method for searching and
    connecting deep learning metadata in a centralized place so that Julia, Ravi,
    and Jianguo can obtain the model metadata easily. In figure 8.3, we add the missing
    piece—a metadata and artifact store (the gray box in the middle)—to improve the
    debuggability.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 8.2 中缺少的关键因素是在一个集中的地方搜索和连接深度学习元数据的有效方法，以便 Julia、Ravi 和 Jianguo 能够轻松获取模型元数据。在图
    8.3 中，我们增加了缺失的部分——一个元数据和制品存储（中间的灰色方框）来提高调试能力。
- en: '![](../Images/08-03.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08-03.png)'
- en: Figure 8.3 Model troubleshooting with metadata management
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 使用元数据管理进行模型故障排除
- en: If you compare figure 8.3 to figure 8.2, you’ll see a new component (the metadata
    and artifact store) is introduced in the middle of figure 8.3\. All the deep learning
    metadata we described in section 8.2.1, regardless of whether they are from the
    experiment pipeline or production pipeline, are collected and stored in this metadata
    store.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您将图8.3与图8.2进行比较，您会看到一个新组件（元数据和工件存储）被引入到图8.3的中间。我们在第8.2.1节中描述的所有深度学习元数据，无论它们来自实验流水线还是生产流水线，都会被收集并存储在此元数据存储中。
- en: The metadata store provides a holistic view of metadata for every data science
    activity in the deep learning system. Metadata of the model, pipeline/training
    run, and artifacts are not only saved but also correlated inside this store, so
    people can obtain related information easily. For example, because model files
    and model training runs are linked in the store, people can easily determine the
    model lineage of a given model.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据存储为深度学习系统中的每个数据科学活动提供了元数据的全面视图。模型、流水线/训练运行和工件的元数据不仅被保存，而且在此存储内部相关联，因此人们可以轻松获取相关信息。例如，由于模型文件和模型训练运行在存储中被链接，人们可以轻松确定给定模型的模型谱系。
- en: Now, Ravi, the data scientist, can use the metadata store UI to list all the
    models and training runs in the system. Then he can dive deep into the metadata
    store to find the input parameters, datasets, and training metric used in the
    past training run, which are super helpful in evaluating the model. More importantly,
    Ravi can retrieve the metadata quickly and completely on his own, without knowing
    the underlying infrastructure of model training and serving.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，数据科学家拉维可以使用元数据存储的用户界面列出系统中的所有模型和训练运行。然后，他可以深入研究元数据存储，找到过去训练运行中使用的输入参数、数据集和训练指标，这对评估模型非常有帮助。更重要的是，拉维可以自行快速完整地检索元数据，而无需了解模型训练和服务的基础架构。
- en: 8.3 Designing a metadata and artifacts store
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 设计一个元数据和工件存储
- en: In this section, we will first discuss the design principles for building a
    metadata and artifact store and then introduce a general design proposal that
    follows those principles. Even if you prefer to use open source technology to
    manage metadata, the discussion in this section will still benefit you; understanding
    the design requirements and solutions will help you choose the right tool for
    your needs.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将首先讨论构建元数据和工件存储的设计原则，然后介绍一个遵循这些原则的通用设计方案。即使您更喜欢使用开源技术来管理元数据，本节中的讨论仍将使您受益；了解设计需求和解决方案将帮助您选择适合您需求的正确工具。
- en: Note To keep things short, we use *metadata and artifact store* and *metadata
    store* interchangeably in this chapter. When we mention *metadata store*, it includes
    the artifacts management as well.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 注意为了简洁起见，我们在本章中将*元数据和工件存储*和*元数据存储*互换使用。当我们提到*元数据存储*时，它包括工件管理。
- en: 8.3.1 Design principles
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.1 设计原则
- en: A metadata and artifact store is designed to facilitate model performance troubleshooting
    and experiment comparison. It stores all kinds of metadata and aggregates it around
    models and training runs, so data scientists can obtain the correlated model lineage
    and model training metadata quickly for an arbitrary model. A good metadata store
    should address the following four principles.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据和工件存储被设计为便于模型性能故障排除和实验比较。它存储各种元数据并将其聚合到模型和训练运行周围，因此数据科学家可以快速获取任意模型的相关模型谱系和模型训练元数据。一个好的元数据存储应该遵循以下四个原则。
- en: 'Principle 1: Showing model lineage and versioning'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 原则1：显示模型谱系和版本控制
- en: 'When receiving a model name, a metadata store should be able to determine the
    versions of that model and the lineage for each model version, such as which training
    run produced the model and what the input parameters and dataset are. Model version
    and lineage are essential to model troubleshooting. When a customer reports a
    problem on a model, such as model performance degradation, the first questions
    we ask are: When is the model produced? Has the training dataset changed? Which
    version of training code is used, and where can we find the training metric? We
    can find all the answers in the model lineage data.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 当收到模型名称时，元数据存储应该能够确定该模型的版本和每个模型版本的血统，例如，哪个训练运行生成了该模型，输入参数和数据集是什么。模型版本和血统对于模型故障排除至关重要。当客户报告模型的问题，例如模型性能下降时，我们首先问的问题是：模型是何时生成的？训练数据集是否发生了变化？使用了哪个版本的训练代码，以及我们从哪里可以找到训练指标？我们可以在模型血统数据中找到所有答案。
- en: 'Principle 2: Enabling model reproducibility'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 原则二：确保模型可复现性
- en: A metadata store should track all the metadata required to reproduce a model,
    such as the training pipeline/run configuration, input dataset files, and algorithm
    code version. Being able to reproduce a model is crucial for model experiment
    evaluation and troubleshooting. We need a place to capture the configuration,
    input parameters, and artifacts to kick off a model training run to reproduce
    the same model. The metadata store is the ideal place to retain such information.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据存储应跟踪重现模型所需的所有元数据，例如训练管道/运行配置、输入数据集文件和算法代码版本。能够重现模型对于模型实验评估和故障排除至关重要。我们需要一个地方来捕获配置、输入参数和工件，以启动模型训练运行以重现相同的模型。元数据存储是保留此类信息的理想地点。
- en: 'Principle 3: Easy access to packaged models'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 原则三：方便获取打包的模型
- en: A metadata store should let data scientists access model files easily, without
    having to understand the complex backend system. The store should have both manual
    and programmatic methods, as data scientists need to be able to run both manual
    and automated model performance testing.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据存储应该让数据科学家轻松访问模型文件，而不必理解复杂的后端系统。存储库应该具有手动和程序化两种方法，因为数据科学家需要能够运行手动和自动化的模型性能测试。
- en: For example, by using a metadata store, data scientists can quickly identify
    the model file that is currently used in production service and download it for
    debugging. Data scientists can also write code to pull arbitrary versions of models
    from the metadata store to automate the model comparison between the new and old
    model versions.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，通过使用元数据存储，数据科学家可以快速识别当前在生产服务中使用的模型文件，并下载它进行调试。数据科学家还可以编写代码从元数据存储中提取任意版本的模型，以自动比较新旧模型版本。
- en: 'Principle 4: Visualizing model training tracking and comparing'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 原则四：可视化模型训练跟踪和比较
- en: Good visualization can greatly improve the efficiency of the model troubleshooting
    process. Data scientists rely on a huge range of metrics to compare and analyze
    model experiments, and the metadata store needs to be equipped with visualization
    tools that can handle all (or any type of) metadata queries.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 良好的可视化可以极大地提高模型故障排除过程的效率。数据科学家依赖于大量的指标来比较和分析模型实验，元数据存储需要配备能够处理所有（或任何类型的）元数据查询的可视化工具。
- en: For example, it needs to be able to show the differences and trending behavior
    on the model evaluation metrics for a set of model training runs. It also needs
    to be able to show model performance trends on the latest 10 released models.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，它需要能够显示一组模型训练运行的模型评估指标的差异和趋势行为。它还需要能够显示最新发布的 10 个模型的模型性能趋势。
- en: 8.3.2 A general metadata and artifact store design proposal
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.2 一个通用的元数据和工件存储设计建议
- en: To address the design principles in section 8.3.1, a deep learning metadata
    store should be a metric storage system, and it needs to store all kinds of metadata
    and the relationships between them. This metadata should be aggregated around
    model and training/experiment executions, so we can find all model-correlated
    metadata quickly during troubleshooting and performance analysis. Therefore, the
    data schema of the internal metadata storage is the key to the metadata store
    design.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决第 8.3.1 节中的设计原则，深度学习元数据存储应该是一个度量存储系统，并且需要存储所有类型的元数据及其之间的关系。此类元数据应围绕模型和训练/实验执行聚合，以便在故障排除和性能分析过程中快速找到所有与模型相关的元数据。因此，内部元数据存储的数据架构是元数据存储设计的关键。
- en: Although a metadata store is a data storage system, data scaling is usually
    not a concern because the data volume of metadata for a deep learning system is
    not high. Because the metadata size depends on the number of model training executions
    and models and we don’t expect to have more than 1,000 model training runs each
    day, a single database instance should be good enough for a metadata store system.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管元数据存储是一个数据存储系统，但数据扩展通常不是一个问题，因为深度学习系统的元数据量并不高。由于元数据大小取决于模型训练执行和模型的数量，而我们不希望每天进行超过
    1,000 次模型训练运行，一个单独的数据库实例应该足够用于元数据存储系统。
- en: For user convenience, a metadata store should offer a web data ingestion interface
    and logging SDK, so deep learning metadata can be instrumented in a similar way
    as application logs and metrics. Based on the design principles and this analysis
    of the system requirements, we have come up with a sample metadata store design
    for your reference. Figure 8.4 shows the overview of this component.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了用户方便，元数据存储应该提供一个 Web 数据摄入接口和日志记录 SDK，这样深度学习元数据就可以以类似应用程序日志和度量的方式被记录。基于设计原则和对系统需求的分析，我们提出了一个示例元数据存储设计供参考。图
    8.4 显示了此组件的概述。
- en: '![](../Images/08-04.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08-04.png)'
- en: Figure 8.4 A general metadata and artifact store system design
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4 通用元数据和工件存储系统设计
- en: 'In figure 8.4, the sample metadata store system is composed of four components:
    a client SDK, a web server, backend storage, and a web UI. Each component and
    step in a deep learning workflow uses the client SDK to send metadata to the metadata
    store server. The metadata store exposes a RESTful interface to metadata ingestion
    and querying. The web UI visualizes the metadata store server’s RESTful interface.
    Besides the basic metadata and artifact organizing and searching, it can also
    visualize the model performance metrics and model differences for various model
    training runs.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 8.4 中，示例元数据存储系统由四个组件组成：一个客户端 SDK、一个 Web 服务器、后端存储和一个 Web UI。深度学习工作流程中的每个组件和步骤都使用客户端
    SDK 将元数据发送到元数据存储服务器。元数据存储提供一个 RESTful 接口来进行元数据摄入和查询。Web UI 可视化元数据存储服务器的 RESTful
    接口。除了基本的元数据和工件组织和搜索外，它还可以可视化各种模型训练运行的模型性能指标和模型差异。
- en: The metadata store server is at the center of this design. It has three layers—a
    RESTful web interface, a data aggregator, and storage. The data aggregator component
    knows how the metadata is organized and interlinked, so it knows where to add
    new metadata and how to serve different kinds of metadata-searching queries. In
    terms of storage, we recommend building an abstract metadata and artifact storage
    layer. This abstract layer works as an adapter that encapsulates the actual metadata
    and filestoring logic. So the metadata store can run on top of different types
    of storage backends, such as cloud object storage, local files, and a local or
    remote SQL server.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据存储服务器位于此设计的中心位置。它有三层——一个 RESTful Web 接口、一个数据聚合器和存储。数据聚合器组件知道元数据如何组织和相互关联，因此它知道在哪里添加新元数据以及如何为不同类型的元数据搜索查询提供服务。在存储方面，我们建议构建一个抽象的元数据和工件存储层。这个抽象层作为一个适配器，封装了实际的元数据和文件存储逻辑。因此，元数据存储可以在不同类型的存储后端上运行，例如云对象存储、本地文件和本地或远程
    SQL 服务器。
- en: The metadata storage schema
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据存储数据模式
- en: Now let’s look at the data schema of the metadata storage. Whether we save the
    metadata in an SQL or noSQL database or plain files, we need to define a data
    schema for how the metadata is structured and serialized. Figure 8.4 shows the
    entity relationship diagram of our metadata storage.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下元数据存储的数据模式。无论我们是将元数据保存在 SQL 数据库、noSQL 数据库还是纯文件中，我们都需要定义一个数据模式来描述元数据的结构和序列化方式。图
    8.4 展示了我们元数据存储的实体关系图。
- en: In figure 8.5, the model training run (`Training_Runs` object) is at the central
    stage of the entity relationship map. This is because model performance troubleshooting
    always starts from the process (training runs or workflow) that produced the model,
    so we want to have a dedicated data object to track the training executions that
    produce model files.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 8.5 中，模型训练运行（`Training_Runs` 对象）处于实体关系图的中心位置。这是因为模型性能故障排除总是从生成模型的过程（训练运行或工作流程）开始的，所以我们希望有一个专用的数据对象来跟踪生成模型文件的训练执行。
- en: '![](../Images/08-05.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08-05.png)'
- en: Figure 8.5 An entity relationship diagram of the data schema of the metadata
    storage
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.5 元数据存储数据模式的实体关系图
- en: The detailed metadata for model training is saved in the `Metrics` and `Parameters`
    objects. The `Parameters` object stores the input parameters of the training run,
    such as dataset ID, dataset version, and training hyperparameters. The `Metrics`
    object stores the training metrics produced during training, such as model F2
    score.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练的详细元数据保存在`Metrics`和`Parameters`对象中。`Parameters`对象存储训练运行的输入参数，例如数据集ID、数据集版本和训练超参数。`Metrics`对象存储训练期间产生的训练指标，例如模型F2分数。
- en: The `Experiments` objects are used to organize and group model training runs.
    One experiment can have multiple training runs. For example, we could define our
    intent classification model development project as one training experiment and
    then associate all the intent classification model training execution with this
    experiment. Then, on the UI, we can group training executions by different experiments.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`Experiments`对象用于组织和分组模型训练运行。一个实验可以有多个训练运行。例如，我们可以将意图分类模型开发项目定义为一个训练实验，然后将所有意图分类模型训练执行与该实验关联。然后，在UI上，我们可以按不同的实验组织训练执行。'
- en: The `Models` objects store the metadata of model files, such as model version,
    type, and stage. A model can have multiple stages, such as test, preproduction,
    and production, and all of these can be preserved as well.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`Models`对象存储模型文件的元数据，例如模型版本、类型和阶段。一个模型可以有多个阶段，例如测试、预生产和生产，所有这些都可以保留。'
- en: Notice also that each of the entities in figure 8.5 is linked (noted by lines
    in the diagram) to the specific training run that produces them, so they will
    all share a common `training_run_id`. By leveraging this data link, you can start
    with any training run object and find its output model, training input data, and
    model training metrics.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意的是，图8.5中的每个实体都链接（由图中的线表示）到产生它们的特定训练运行，因此它们都将共享一个公共的`training_run_id`。通过利用这个数据链接，你可以从任何训练运行对象开始，找到其输出模型、训练输入数据和模型训练指标。
- en: Earlier we said we may call it the metadata store for short, but it also stores
    artifacts. So where is the artifact in this design? We store the artifact URL
    in the `Training_Runs` object as the training run’s output. If we query the model
    or training execution, we will get the artifacts’ URL.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 早些时候，我们说可以将其简称为元数据存储，但也存储工件。那么这个设计中的工件在哪里呢？我们将工件的URL作为训练运行的输出存储在`Training_Runs`对象中。如果查询模型或训练执行，则会得到工件的URL。
- en: Model focus vs. pipeline focus
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 模型焦点与流水线焦点
- en: 'There are two schools of thought on designing metadata systems: model focus
    and pipeline focus. The model focus method correlates metadata around model files,
    whereas the pipeline focus method aggregates metadata around the pipeline/training
    run, like what we proposed in figure 8.4.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 设计元数据系统有两种方法：模型焦点和流水线焦点。模型焦点方法将元数据与模型文件相关联，而流水线焦点方法聚合绕流水线/训练运行的元数据，就像我们在图8.4中提议的那样。
- en: We think model focus and pipeline focus are equally useful to the end users
    (data scientists), and they are not mutually exclusive. We can support both of
    them.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为模型焦点和流水线焦点对最终用户（数据科学家）同样有用，它们并不是互斥的。我们可以支持两种焦点。
- en: You can implement the metadata store’s storage layer by using the pipeline focus
    method, similar to our sample in figure 8.5, and then build search functionality
    on the web UI to support both pipeline search and model search.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用流水线焦点方法实现元数据存储的存储层，类似于图8.5中的示例，然后在Web UI上构建搜索功能，以支持流水线搜索和模型搜索。
- en: 8.4 Open source solutions
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.4 开源解决方案
- en: In this section, we will discuss two widely used metadata management tools,
    MLMD and MLflow. Both systems are open source and free to use. We will first give
    an overview of each of the tools and then provide a comparison to determine which
    one to use and when.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论两个广泛使用的元数据管理工具：MLMD和MLflow。两个系统都是开源的，可以免费使用。我们首先对每个工具进行概述，然后提供比较，确定哪个工具在什么时候使用。
- en: 8.4.1 ML Metadata
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.1 机器学习元数据
- en: MLMD ([https://github.com/google/ml-metadata](https://github.com/google/ml-metadata))
    is a lightweight library for recording and retrieving metadata associated with
    ML developer and data scientist workflows. MLMD is an integral part of TensorFlow
    Extended (TFX; [https://www.tensorflow.org/tfx](https://www.tensorflow.org/tfx))
    but is designed so that it can be used independently. For example, Kubeflow ([https://www.kubeflow.org/](https://www.kubeflow.org/))
    uses MLMD to manage metadata produced by its pipeline and notebook service. For
    more details, see the Kubeflow metadata documentation ([http://mng.bz/Blo1](http://mng.bz/Blo1)).
    You can consider MLMD a logging library and use it to instrument metadata in each
    step of your ML pipeline, so you can understand and analyze all the interconnecting
    parts of your workflow/ pipeline.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: System overview
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'The metadata instrumentation with the MLMD library can be set up with two different
    backends: an SQL or gRPC server. See figure 8.6 for the concept.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/08-06.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.6 Two different setups for instrumenting metadata with MLMD: (A) directly
    report metadata to the backend database and (B) report metadata to the gRPC server
    DB, database.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: In figure 8.6, we see that each step of an ML pipeline/workflow uses MLMD library
    (MLMD client API) to instrument metadata. On the backend, MLMD will save the metadata
    in a relational database, such as mySQL or PostgreSQL.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: You can choose to let each MLMD library talk directly to an SQL server (figure
    8.5, A) or use the server setup code in the MLMD library to set up a gRPC server
    and let the client libraries talk to the server (figure 8.5, B). Approach A is
    simple; you don’t need to host a dedicated logging server, but approach B is recommended
    because it avoids exposing the backend database.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 'You can check out the following two docs for detailed metadata storage configuration:
    “Metadata Storage Backends and Store Connection Configuration” ([http://mng.bz/dJMo](http://mng.bz/dJMo))
    and “Use MLMD with a Remote gRPC Server” ([http://mng.bz/rd8J](http://mng.bz/rd8J)).'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Logging API
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: The metadata store in MLMD uses the following data structures to record metadata
    in the storage backend. An *execution* represents a component or a step in a workflow;
    an *artifact* describes an input or output object in an execution; and an *event*
    is a record of relationships between artifacts and executions. A *context* is
    a logic group that is used to correlate artifacts and executions together in the
    same workflow.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'With this concept in mind, let’s look at some sample metadata instrumentation
    code:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ A dataset is recorded as an artifact.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Saves the metadata to storage
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: ❸ A model training run is recorded as an execution.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: ❹ A model is recorded as an artifact.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Defines the logic group for the metadata of model training
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Saves the relationship between metadata
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Check out the MLMD “Get Started” doc ([http://mng.bz/VpWy](http://mng.bz/VpWy))
    for the detailed code sample and local setup instructions.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Searching metadata
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 'MLMD doesn’t provide a UI to show the metadata it stores. So, for querying
    metadata, we need to use its client API. See the following code example:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Queries all registered artifacts
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Queries artifact by ID
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Queries artifact by uri
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Queries artifact by using a filter
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: The MLMD “Get Started” doc ([http://mng.bz/VpWy](http://mng.bz/VpWy)) provides
    lots of query examples for fetching metadata of artifacts, executions, and context.
    If you are interested, please take a look.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'The best way to learn the data model of MLMD is to look at its database schema.
    You can first create an SQLite database and configure the MLMD metadata store
    to use that database and then run the MLMD sample code. At the end, all the entities
    and tables are created in the local SQLite database. By looking at the table schema
    and content, you will gain a deep understanding of how the metadata is organized
    in MLMD, so you can build a nice UI on top of it yourself. The following sample
    code shows how to configure the MLMD metadata store to use a local SQLite database:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Local files path of the SQLite database
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Allows read, write, and create
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.2 MLflow
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: MLflow ([https://mlflow.org/docs/latest/tracking.html](https://mlflow.org/docs/latest/tracking.html))
    is an open source MLOps platform. It is designed to manage the ML lifecycle, including
    experimentation, reproducibility, deployment, and central model registry.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'Compared to MLMD, MLflow is a whole system, not a library. It’s composed of
    four main components:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '*MLflow Tracking (a metadata tracking server*) —For recording and querying
    metadata'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*MLflow Projects*—For packaging code in a reusable and reproducible way'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*MLflow Models*—For packaging ML models that can be used for different model
    serving tools'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*MLflow Model Registry*—For managing the full life cycle of MLflow models with
    a UI, such as model lineage, model versioning, annotation, and production promotion'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we focus on the tracking server, since that’s most relevant
    to metadata management.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: System overview
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: MLflow provides six different setup approaches. For example, the metadata of
    MLflow runs (training pipeline) can be recorded to local files, an SQL database,
    a remote server, or a remote server with proxied storage backend access. For details
    about these six different setup methods, you can check out the MLflow doc “How
    Runs and Artifacts Are Recorded” ([https://mlflow.org/docs/latest/tracking.html#id27](https://mlflow.org/docs/latest/tracking.html#id27)).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we focus on the most commonly used setup approach: remote
    server with proxied artifact storage access. See figure 8.7 for the system overview
    diagram.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/08-07.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 Setting up MLflow tracker server for metadata ingestion and query
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: From figure 8.7, we see that each step/action of a deep learning pipeline (workflow)
    uses the MLflow client to instrument metadata and artifact to the MLflow tracking
    server. The tracking server saves metadata, such as metrics, parameters, and tags,
    in a specified SQL database; artifacts, such as models, images, and documents,
    are saved in a configured object storage, such as Amazon S3.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'MLflow offers two ways to upload artifacts: (1) direct upload from the client
    and (2) proxy upload through the tracking server. In figure 8.6, we illustrated
    the latter method: utilizing the tracking server as a proxy server for any operations
    involving artifacts. The advantage is the end users can have a direct path to
    access the backend remote object store without providing access credentials.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Another nice thing about MLflow is that it offers a nice UI; data scientists
    can check and search metadata thru the website hosted in the tracking server.
    The UI allows users not only to view metadata from a pipeline execution perspective
    but also to search and operate models directly.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Logging API
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'Sending metadata to the MLflow tracking server is straightforward. We can start
    by creating an active run as a context manager and then call the log function
    to log artifacts or a single key-value parameter, metric, and tag. See the sample
    code as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Defines the MLflow server URL
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Logs metadata in Python context manager made by the MLflow ActiveRun object
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Automatic logging
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: If you are tired of specifying lots of metadata yourself, MLflow supports automatic
    logging. By calling `mlflow.autolog()` or a library-specific autolog function
    before your training code, such as `mlflow.tensorflow.autolog()`, `mlflow.keras.autolog()`,
    or `mlflow.pytorch.autolog()`, MLflow will log metadata, even artifacts, automatically
    without the need for an explicit log statement. If you want to learn more about
    MLflow logging, check out the MLflow logging functions doc ([http://mng.bz/xd1d](http://mng.bz/xd1d)).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Searching metadata
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'The tracking UI hosted by the MLflow tracking server lets you visualize, search,
    and compare runs, as well as download run artifacts or metadata for analysis in
    other tools. The UI contains the following key features: experiment-based run
    listing and comparison, searching for runs by parameter or metric value, visualizing
    run metrics, and downloading run results. Besides UI, you can also achieve all
    the operations provided in the tracking UI programmatically, as in the following
    examples:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Initializes client
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Prints out the execution stages of a run
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: The programmatic metadata access is not only helpful when using your analysis
    tool (for example, pandas) to query and compare the model performance of different
    training runs, but also for integrating models with your model serving system
    since it allows you to fetch models from the MLflow model registry programmatically.
    For full `MLflowClient` API usage, you can check out the MLflow Tracking API doc
    ([http://mng.bz/GRzO](http://mng.bz/GRzO)).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.3 MLflow vs. MLMD
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: From the descriptions in the previous sections, we see that MLMD is more of
    a lightweight library while MLflow is an MLOps platform. Both tools can run independently,
    offer metadata ingestion and search functionality, and track metadata on the basis
    of model training runs. But MLflow offers much more.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: In addition to MLMD’s functionality, MLflow supports automatic metadata logging
    and a well-designed UI to visualize experiment metadata (including experiment
    comparison), model registry, artifact management, code reproducibility, model
    package, and more.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: If you need to introduce a complete, new metadata and artifact store to your
    system, MFflow is your first choice. It’s supported by an active open source community,
    and it covers most of the user requirements of ML metadata management. As a bonus,
    MLflow has a great support on MLOps, such as MLflow project management and model
    deployment.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: If you already have an artifact registry and a metric visualization website
    and you want to integrate metadata functionality into your existing system, then
    MLMD is a good choice. MLMD is lightweight, easy to use, and simple to learn.
    For example, the Kubeflow ([https://www.kubeflow.org/](https://www.kubeflow.org/))
    deep learning platform integrates MLMD as the metadata tracking tool into its
    components, such as Kubeflow pipeline ([https://www.kubeflow.org/docs/components/pipelines/](https://www.kubeflow.org/docs/components/pipelines/)).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Machine learning metadata can be categorized into four buckets: model training
    runs, general artifacts, model artifacts, and pipelines.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A metadata and artifact store is designed to support model performance comparison,
    troubleshooting, and reproducing.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A good metadata management system can help to show model lineage, enable model
    reproducibility, and facilitate model comparison.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLMD is a lightweight metadata management tool, which originated from the TensorFlow
    pipeline, but it can be used independently. For example, Kubeflow uses MLMD to
    manage ML metadata in its pipeline component.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLMD is good for integrating metadata management into an existing system.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLflow is an MLOps platform; it’s designed to manage the ML lifecycle, including
    experimentation, reproducibility, deployment, and a central model registry.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLflow is applicable if you want to introduce a completely independent metadata
    and artifact management system.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
