- en: 8 Metadata and artifact store
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Understanding and managing metadata in the deep learning context
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing a metadata and artifact store to manage metadata
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Introducing two open source metadata management tools: ML Metadata and MLflow'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To produce a high-quality model that fits business requirements, data scientists
    need to experiment with all kinds of datasets, data processing techniques, and
    training algorithms. To build and ship the best model, they spend a significant
    amount of time conducting these experiments.
  prefs: []
  type: TYPE_NORMAL
- en: A variety of *artifacts* (datasets and model files) and *metadata* are produced
    from model training experiments. The metadata may include model algorithms, hyperparameters,
    training metrics, and model versions, which are very helpful in analyzing model
    performance. To be useful, this data must be persistent and retrievable.
  prefs: []
  type: TYPE_NORMAL
- en: When data scientists need to investigate a model performance problem or compare
    different training experiments, is there anything we, as engineers, can do to
    facilitate these efforts? For example, can we make model reproducing and experiment
    comparison easier?
  prefs: []
  type: TYPE_NORMAL
- en: The answer is yes. As engineers, we can build a system that retains the experimental
    metadata and artifacts that data scientists need to reproduce and compare models.
    And if we design this storage and retrieval system well, with proper metadata
    management, data scientists can easily select the best model from a series of
    experiments or figure out the root cause of a model degradation quickly without
    a deep understanding of the metadata system.
  prefs: []
  type: TYPE_NORMAL
- en: 'In previous chapters, we’ve learned about designing services to produce and
    serve models. Here, we turn our attention to the metadata and artifacts management
    system that facilitates two more key operations: troubleshooting and comparing
    experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start this chapter with an introduction to *artifacts* and *metadata*
    and the meaning of these concepts in the context of deep learning. Then we will
    show you how to design metadata management systems using examples and emphasizing
    design principles. Finally, we will discuss two open source metadata management
    systems: MLMD (ML Metadata) and MLflow. By reading this chapter, you will gain
    a clear vision of how to manage metadata and artifacts to facilitate experiment
    comparison and model troubleshooting.'
  prefs: []
  type: TYPE_NORMAL
- en: 8.1 Introducing artifacts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: People often assume that an artifact in deep learning is the model file produced
    by the model training process. This is partially true. Artifacts are actually
    the files and objects that form both the inputs and outputs of the components
    in the model training process. This is a crucial distinction, and it is important
    to keep this broader definition in mind if you want to engineer a system that
    supports model reproducibility.
  prefs: []
  type: TYPE_NORMAL
- en: Under this definition, artifacts can include datasets, models, code, or any
    other number of objects used in a deep learning project. For example, the raw
    input training data, the labeled dataset produced from a labeling tool, and the
    results data of a data processing pipeline are all considered artifacts.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, artifacts must be preserved with metadata that describes their
    facts and lineage to allow performance comparisons, reproducibility, and troubleshooting.
    In practice, artifacts are stored as raw files on a file server or cloud storage
    service, such as Amazon Simple Storage Service or Azure Blob Storage. And we associate
    artifacts with their metadata in a *metadata store* on a separate storage service.
    See figure 8.1 for a diagram of what this arrangement typically looks like.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.1 displays the common practice of managing artifacts. The artifact
    files are saved to a file storage system, and their file URLs are saved with other
    related metadata (such as model training execution ID and model ID) in the metadata
    store. This setup allows us—or data scientists—to search for a model in the metadata
    store and easily find all the input and output artifacts of the corresponding
    model training process.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/08-01.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 Artifacts are associated with their metadata in the metadata store.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 Metadata in a deep learning context
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In general terms, metadata is structured reference data that provides information
    about other data or objects, such as the nutrition facts label on packaged food.
    In machine learning (ML) and deep learning, however, metadata is more specific
    to models; it’s the data that describes model training executions (runs), workflows,
    models, datasets, and other artifacts.
  prefs: []
  type: TYPE_NORMAL
- en: For any distributed system, we track service metadata in the form of logs and
    metrics at a service level. For example, we might track metrics such as CPU rate,
    number of active users, and number of failed web requests. We use these metrics
    for system/ service monitoring, troubleshooting, and observation purposes.
  prefs: []
  type: TYPE_NORMAL
- en: In deep learning systems, beyond the service-level metric, we collect metadata
    for model troubleshooting, comparison, and reproduction purposes. You can think
    of deep learning metadata as a special subset of logs and metrics that we use
    to monitor and track every deep learning activity in the system. These activities
    include data parsing, model training, and model serving.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.1 Common metadata categories
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Although we’ve just defined metadata, the term is actually somewhat arbitrary;
    there is no set guideline on which data should be considered metadata. For engineers
    of deep learning systems, we recommend defining metadata in the following four
    categories: model training run, general artifact, model file, and orchestration
    workflow. To give you a concrete feel for these categories, let’s look at each
    category and some examples of metadata that go into each one.'
  prefs: []
  type: TYPE_NORMAL
- en: Metadata for a model training run
  prefs: []
  type: TYPE_NORMAL
- en: To reproduce models, analyze model performance, and facilitate model troubleshooting,
    we need to track all the input and output data and artifacts of a model training
    run. This includes
  prefs: []
  type: TYPE_NORMAL
- en: '*Dataset ID and version*—The unique identity of the dataset used in model training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hyperparameters*—The hyperparameters used in the training, such as learning
    rate and number of epochs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hardware resources*—CPU, GPU, TPU, memory, and disk size allocated in the
    training and the actual consumption of these resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Training code version*—The unique identity of the training code snapshot used
    in model training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Training code configuration*—The configuration for recreating the training
    code execution environment, such as conda.yml, Dockerfile, and requirement.txt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Training metrics*—The metrics show how model training progresses, for example,
    the loss value at each training epoch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model evaluation metrics*—The metrics show the model performance, such as
    F-score and root-mean-squared error (RMSE).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metadata for general artifacts
  prefs: []
  type: TYPE_NORMAL
- en: 'Artifacts can be any arbitrary files, such as datasets, models, and prediction
    results. To be able to find the artifact in artifact storage, we want to track
    below metadata for artifacts:'
  prefs: []
  type: TYPE_NORMAL
- en: '*File location*—The path to the place where the artifact is stored, for example,
    Amazon S3 file path or internal file system path'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*File version*—The unique identity to distinguish different file updates'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Description*—The additional information to describe what’s inside the artifact
    file'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Audit history*—The information about who created the artifact version, when
    the artifact was created, and how it was created'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metadata for model file
  prefs: []
  type: TYPE_NORMAL
- en: 'Models are one kind of artifact, but because models are the main product of
    every deep learning system, we recommend tracking model metadata separately from
    other artifacts. When we define model metadata, it’s best to consider two perspectives:
    model training and model serving.'
  prefs: []
  type: TYPE_NORMAL
- en: For model training, to have model lineage, we want to keep a mapping between
    a model and the model training run that produced it. Model lineage is important
    for model comparison and reproduction. For example, when comparing two models,
    by having the link of model training run and model, data scientists can easily
    determine all the details of how the model is produced, including the input datasets,
    training parameters, and training metrics. The model training metrics are very
    useful for understanding model performance.
  prefs: []
  type: TYPE_NORMAL
- en: For model serving, we want to track the model execution data for future model
    performance analysis. These execution data, such as model response latency and
    prediction miss rate, are very useful for detecting model performance degradation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are a few recommended model metadata categories besides the aforementioned
    general artifact metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Resource consumption*—Memory, GPU, CPU, and TPU consumption for model serving'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model training run*—The model training run ID, which is used to find the code,
    dataset, hyperparameters, and environments that create the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Experiment*—Tracking the model experiment activities in production, for example,
    customer traffic distribution for different model versions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Production*—Model usage in production, such as query per second and model
    prediction statistics'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model performance*—Tracking model evaluation metrics for drift detection,
    such as concept drift and performance drift'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note Models will, unavoidably, start performing worse once they are shipped
    to production. We call this behavior *model degradation*. As the statistical distribution
    of the target group changes over time, model prediction becomes less accurate.
    New popular slogans, for example, can affect the accuracy of voice recognition.
  prefs: []
  type: TYPE_NORMAL
- en: Metadata for pipeline
  prefs: []
  type: TYPE_NORMAL
- en: 'A pipeline or workflow is needed when we want to automate a multiple-step model
    training task. For example, we can use workflow management tools like Airflow,
    Kubeflow, or Metaflow to automate a model training process that contains multiple
    functional steps: data collection, feature extraction, dataset augmentation, training,
    and model deployment. We will discuss workflow in detail in the next chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: For pipeline metadata, we usually track the pipeline execution history and the
    pipeline input and output. This data can provide audit information for future
    troubleshooting.
  prefs: []
  type: TYPE_NORMAL
- en: Note Deep learning projects vary a lot. The model training code is dramatically
    different for voice recognition, natural language processing, and image generation.
    There are many factors specific to the project, such as the size/type of the dataset,
    type of the ML model, and input artifacts. Besides the sample metadata mentioned
    previously, we recommend you define and collect the metadata based on your project.
    When you are looking for data that helps model reproducing and troubleshooting,
    the metadata list will come to you naturally.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.2 Why manage metadata?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Because metadata is normally instrumented or recorded in the form of logs or
    metrics, you may wonder why we need to manage deep learning metadata separately.
    Can we simply fetch the deep learning metadata from log files? Log management
    systems like Splunk ([https://www.splunk.com/](https://www.splunk.com/)) and Sumo
    Logic ([https://www.sumologic.com/](https://www.sumologic.com/)) come in very
    handy because they allow developers to search and analyze logs and events produced
    by distributed systems.
  prefs: []
  type: TYPE_NORMAL
- en: To better explain the necessity of having a dedicated component for managing
    metadata in a deep learning system, we will convey a story. Julia (data engineer),
    Ravi (data scientist), and Jianguo (system developer) work together on a deep
    learning system to develop intent classification models for a chatbot application.
    Ravi develops intent classification algorithms, Julia works on data collection
    and parsing, and Jianguo develops and maintains the deep learning system.
  prefs: []
  type: TYPE_NORMAL
- en: During the project development and test phases, Julia and Ravi work together
    to build an experimental training pipeline to produce intent models. After the
    models are built, Ravi passes them to Jianguo to deploy the experiment models
    to the prediction service and test them with real customer requests.
  prefs: []
  type: TYPE_NORMAL
- en: When Ravi feels good about the experimentation, he promotes the training algorithm
    from the experimental pipeline to an automated production training pipeline. This
    pipeline runs in the production environment and produces intent models with customer
    data as input. The pipeline also deploys the models to the prediction service
    automatically. Figure 8.2 illustrates the whole story setting.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/08-02.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 Model development without metadata management
  prefs: []
  type: TYPE_NORMAL
- en: A few weeks later, after Ravi released the latest intent classification algorithm,
    one chatbot customer—BestFood Inc.—reported a model performance degradation problem
    to Ravi. In the investigation request, BestFood mentioned that their bot’s intent
    classification accuracy dropped 10% after using a new dataset.
  prefs: []
  type: TYPE_NORMAL
- en: To troubleshoot the reported model performance degradation problem, Ravi needs
    to verify lots of information. He first needs to check which model version is
    currently being used by BestFood in the prediction service and then check the
    model lineage of the current model, such as the dataset version and code version
    used in the training pipeline. After that, Ravi may also need to reproduce the
    model for local debugging. He needs to compare the current model and the previous
    model to test the data distribution effect (current new dataset vs. previous dataset).
  prefs: []
  type: TYPE_NORMAL
- en: Ravi is a natural language process (NLP) expert, but he has very little knowledge
    about the deep learning system on which his training code runs. To continue his
    investigation, he has to ask Jianguo and Julia to obtain the relevant model, dataset,
    and code information. Because everyone has only a fragment of knowledge about
    the model training application and underlying deep learning system/infrastructure,
    for each model performance troubleshooting, Ravi, Julia, and Jianguo have to work
    together to grasp the full context, which is time-consuming and inefficient.
  prefs: []
  type: TYPE_NORMAL
- en: This story, of course, is oversimplified. In practice, deep learning project
    development is made of data, algorithms, system/runtime development, and hardware
    management. The whole project is owned by different teams, and there is seldom
    one person who knows everything. Relying on cross-team collaboration to troubleshoot
    model-related problems is unrealistic in a corporate setting.
  prefs: []
  type: TYPE_NORMAL
- en: The key factor missing in figure 8.2 is an effective method for searching and
    connecting deep learning metadata in a centralized place so that Julia, Ravi,
    and Jianguo can obtain the model metadata easily. In figure 8.3, we add the missing
    piece—a metadata and artifact store (the gray box in the middle)—to improve the
    debuggability.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/08-03.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 Model troubleshooting with metadata management
  prefs: []
  type: TYPE_NORMAL
- en: If you compare figure 8.3 to figure 8.2, you’ll see a new component (the metadata
    and artifact store) is introduced in the middle of figure 8.3\. All the deep learning
    metadata we described in section 8.2.1, regardless of whether they are from the
    experiment pipeline or production pipeline, are collected and stored in this metadata
    store.
  prefs: []
  type: TYPE_NORMAL
- en: The metadata store provides a holistic view of metadata for every data science
    activity in the deep learning system. Metadata of the model, pipeline/training
    run, and artifacts are not only saved but also correlated inside this store, so
    people can obtain related information easily. For example, because model files
    and model training runs are linked in the store, people can easily determine the
    model lineage of a given model.
  prefs: []
  type: TYPE_NORMAL
- en: Now, Ravi, the data scientist, can use the metadata store UI to list all the
    models and training runs in the system. Then he can dive deep into the metadata
    store to find the input parameters, datasets, and training metric used in the
    past training run, which are super helpful in evaluating the model. More importantly,
    Ravi can retrieve the metadata quickly and completely on his own, without knowing
    the underlying infrastructure of model training and serving.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3 Designing a metadata and artifacts store
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will first discuss the design principles for building a
    metadata and artifact store and then introduce a general design proposal that
    follows those principles. Even if you prefer to use open source technology to
    manage metadata, the discussion in this section will still benefit you; understanding
    the design requirements and solutions will help you choose the right tool for
    your needs.
  prefs: []
  type: TYPE_NORMAL
- en: Note To keep things short, we use *metadata and artifact store* and *metadata
    store* interchangeably in this chapter. When we mention *metadata store*, it includes
    the artifacts management as well.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.1 Design principles
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A metadata and artifact store is designed to facilitate model performance troubleshooting
    and experiment comparison. It stores all kinds of metadata and aggregates it around
    models and training runs, so data scientists can obtain the correlated model lineage
    and model training metadata quickly for an arbitrary model. A good metadata store
    should address the following four principles.
  prefs: []
  type: TYPE_NORMAL
- en: 'Principle 1: Showing model lineage and versioning'
  prefs: []
  type: TYPE_NORMAL
- en: 'When receiving a model name, a metadata store should be able to determine the
    versions of that model and the lineage for each model version, such as which training
    run produced the model and what the input parameters and dataset are. Model version
    and lineage are essential to model troubleshooting. When a customer reports a
    problem on a model, such as model performance degradation, the first questions
    we ask are: When is the model produced? Has the training dataset changed? Which
    version of training code is used, and where can we find the training metric? We
    can find all the answers in the model lineage data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Principle 2: Enabling model reproducibility'
  prefs: []
  type: TYPE_NORMAL
- en: A metadata store should track all the metadata required to reproduce a model,
    such as the training pipeline/run configuration, input dataset files, and algorithm
    code version. Being able to reproduce a model is crucial for model experiment
    evaluation and troubleshooting. We need a place to capture the configuration,
    input parameters, and artifacts to kick off a model training run to reproduce
    the same model. The metadata store is the ideal place to retain such information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Principle 3: Easy access to packaged models'
  prefs: []
  type: TYPE_NORMAL
- en: A metadata store should let data scientists access model files easily, without
    having to understand the complex backend system. The store should have both manual
    and programmatic methods, as data scientists need to be able to run both manual
    and automated model performance testing.
  prefs: []
  type: TYPE_NORMAL
- en: For example, by using a metadata store, data scientists can quickly identify
    the model file that is currently used in production service and download it for
    debugging. Data scientists can also write code to pull arbitrary versions of models
    from the metadata store to automate the model comparison between the new and old
    model versions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Principle 4: Visualizing model training tracking and comparing'
  prefs: []
  type: TYPE_NORMAL
- en: Good visualization can greatly improve the efficiency of the model troubleshooting
    process. Data scientists rely on a huge range of metrics to compare and analyze
    model experiments, and the metadata store needs to be equipped with visualization
    tools that can handle all (or any type of) metadata queries.
  prefs: []
  type: TYPE_NORMAL
- en: For example, it needs to be able to show the differences and trending behavior
    on the model evaluation metrics for a set of model training runs. It also needs
    to be able to show model performance trends on the latest 10 released models.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.2 A general metadata and artifact store design proposal
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To address the design principles in section 8.3.1, a deep learning metadata
    store should be a metric storage system, and it needs to store all kinds of metadata
    and the relationships between them. This metadata should be aggregated around
    model and training/experiment executions, so we can find all model-correlated
    metadata quickly during troubleshooting and performance analysis. Therefore, the
    data schema of the internal metadata storage is the key to the metadata store
    design.
  prefs: []
  type: TYPE_NORMAL
- en: Although a metadata store is a data storage system, data scaling is usually
    not a concern because the data volume of metadata for a deep learning system is
    not high. Because the metadata size depends on the number of model training executions
    and models and we don’t expect to have more than 1,000 model training runs each
    day, a single database instance should be good enough for a metadata store system.
  prefs: []
  type: TYPE_NORMAL
- en: For user convenience, a metadata store should offer a web data ingestion interface
    and logging SDK, so deep learning metadata can be instrumented in a similar way
    as application logs and metrics. Based on the design principles and this analysis
    of the system requirements, we have come up with a sample metadata store design
    for your reference. Figure 8.4 shows the overview of this component.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/08-04.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 A general metadata and artifact store system design
  prefs: []
  type: TYPE_NORMAL
- en: 'In figure 8.4, the sample metadata store system is composed of four components:
    a client SDK, a web server, backend storage, and a web UI. Each component and
    step in a deep learning workflow uses the client SDK to send metadata to the metadata
    store server. The metadata store exposes a RESTful interface to metadata ingestion
    and querying. The web UI visualizes the metadata store server’s RESTful interface.
    Besides the basic metadata and artifact organizing and searching, it can also
    visualize the model performance metrics and model differences for various model
    training runs.'
  prefs: []
  type: TYPE_NORMAL
- en: The metadata store server is at the center of this design. It has three layers—a
    RESTful web interface, a data aggregator, and storage. The data aggregator component
    knows how the metadata is organized and interlinked, so it knows where to add
    new metadata and how to serve different kinds of metadata-searching queries. In
    terms of storage, we recommend building an abstract metadata and artifact storage
    layer. This abstract layer works as an adapter that encapsulates the actual metadata
    and filestoring logic. So the metadata store can run on top of different types
    of storage backends, such as cloud object storage, local files, and a local or
    remote SQL server.
  prefs: []
  type: TYPE_NORMAL
- en: The metadata storage schema
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s look at the data schema of the metadata storage. Whether we save the
    metadata in an SQL or noSQL database or plain files, we need to define a data
    schema for how the metadata is structured and serialized. Figure 8.4 shows the
    entity relationship diagram of our metadata storage.
  prefs: []
  type: TYPE_NORMAL
- en: In figure 8.5, the model training run (`Training_Runs` object) is at the central
    stage of the entity relationship map. This is because model performance troubleshooting
    always starts from the process (training runs or workflow) that produced the model,
    so we want to have a dedicated data object to track the training executions that
    produce model files.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/08-05.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 An entity relationship diagram of the data schema of the metadata
    storage
  prefs: []
  type: TYPE_NORMAL
- en: The detailed metadata for model training is saved in the `Metrics` and `Parameters`
    objects. The `Parameters` object stores the input parameters of the training run,
    such as dataset ID, dataset version, and training hyperparameters. The `Metrics`
    object stores the training metrics produced during training, such as model F2
    score.
  prefs: []
  type: TYPE_NORMAL
- en: The `Experiments` objects are used to organize and group model training runs.
    One experiment can have multiple training runs. For example, we could define our
    intent classification model development project as one training experiment and
    then associate all the intent classification model training execution with this
    experiment. Then, on the UI, we can group training executions by different experiments.
  prefs: []
  type: TYPE_NORMAL
- en: The `Models` objects store the metadata of model files, such as model version,
    type, and stage. A model can have multiple stages, such as test, preproduction,
    and production, and all of these can be preserved as well.
  prefs: []
  type: TYPE_NORMAL
- en: Notice also that each of the entities in figure 8.5 is linked (noted by lines
    in the diagram) to the specific training run that produces them, so they will
    all share a common `training_run_id`. By leveraging this data link, you can start
    with any training run object and find its output model, training input data, and
    model training metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Earlier we said we may call it the metadata store for short, but it also stores
    artifacts. So where is the artifact in this design? We store the artifact URL
    in the `Training_Runs` object as the training run’s output. If we query the model
    or training execution, we will get the artifacts’ URL.
  prefs: []
  type: TYPE_NORMAL
- en: Model focus vs. pipeline focus
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two schools of thought on designing metadata systems: model focus
    and pipeline focus. The model focus method correlates metadata around model files,
    whereas the pipeline focus method aggregates metadata around the pipeline/training
    run, like what we proposed in figure 8.4.'
  prefs: []
  type: TYPE_NORMAL
- en: We think model focus and pipeline focus are equally useful to the end users
    (data scientists), and they are not mutually exclusive. We can support both of
    them.
  prefs: []
  type: TYPE_NORMAL
- en: You can implement the metadata store’s storage layer by using the pipeline focus
    method, similar to our sample in figure 8.5, and then build search functionality
    on the web UI to support both pipeline search and model search.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4 Open source solutions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will discuss two widely used metadata management tools,
    MLMD and MLflow. Both systems are open source and free to use. We will first give
    an overview of each of the tools and then provide a comparison to determine which
    one to use and when.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.1 ML Metadata
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: MLMD ([https://github.com/google/ml-metadata](https://github.com/google/ml-metadata))
    is a lightweight library for recording and retrieving metadata associated with
    ML developer and data scientist workflows. MLMD is an integral part of TensorFlow
    Extended (TFX; [https://www.tensorflow.org/tfx](https://www.tensorflow.org/tfx))
    but is designed so that it can be used independently. For example, Kubeflow ([https://www.kubeflow.org/](https://www.kubeflow.org/))
    uses MLMD to manage metadata produced by its pipeline and notebook service. For
    more details, see the Kubeflow metadata documentation ([http://mng.bz/Blo1](http://mng.bz/Blo1)).
    You can consider MLMD a logging library and use it to instrument metadata in each
    step of your ML pipeline, so you can understand and analyze all the interconnecting
    parts of your workflow/ pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: System overview
  prefs: []
  type: TYPE_NORMAL
- en: 'The metadata instrumentation with the MLMD library can be set up with two different
    backends: an SQL or gRPC server. See figure 8.6 for the concept.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/08-06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.6 Two different setups for instrumenting metadata with MLMD: (A) directly
    report metadata to the backend database and (B) report metadata to the gRPC server
    DB, database.'
  prefs: []
  type: TYPE_NORMAL
- en: In figure 8.6, we see that each step of an ML pipeline/workflow uses MLMD library
    (MLMD client API) to instrument metadata. On the backend, MLMD will save the metadata
    in a relational database, such as mySQL or PostgreSQL.
  prefs: []
  type: TYPE_NORMAL
- en: You can choose to let each MLMD library talk directly to an SQL server (figure
    8.5, A) or use the server setup code in the MLMD library to set up a gRPC server
    and let the client libraries talk to the server (figure 8.5, B). Approach A is
    simple; you don’t need to host a dedicated logging server, but approach B is recommended
    because it avoids exposing the backend database.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can check out the following two docs for detailed metadata storage configuration:
    “Metadata Storage Backends and Store Connection Configuration” ([http://mng.bz/dJMo](http://mng.bz/dJMo))
    and “Use MLMD with a Remote gRPC Server” ([http://mng.bz/rd8J](http://mng.bz/rd8J)).'
  prefs: []
  type: TYPE_NORMAL
- en: Logging API
  prefs: []
  type: TYPE_NORMAL
- en: The metadata store in MLMD uses the following data structures to record metadata
    in the storage backend. An *execution* represents a component or a step in a workflow;
    an *artifact* describes an input or output object in an execution; and an *event*
    is a record of relationships between artifacts and executions. A *context* is
    a logic group that is used to correlate artifacts and executions together in the
    same workflow.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this concept in mind, let’s look at some sample metadata instrumentation
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: ❶ A dataset is recorded as an artifact.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Saves the metadata to storage
  prefs: []
  type: TYPE_NORMAL
- en: ❸ A model training run is recorded as an execution.
  prefs: []
  type: TYPE_NORMAL
- en: ❹ A model is recorded as an artifact.
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Defines the logic group for the metadata of model training
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Saves the relationship between metadata
  prefs: []
  type: TYPE_NORMAL
- en: Check out the MLMD “Get Started” doc ([http://mng.bz/VpWy](http://mng.bz/VpWy))
    for the detailed code sample and local setup instructions.
  prefs: []
  type: TYPE_NORMAL
- en: Searching metadata
  prefs: []
  type: TYPE_NORMAL
- en: 'MLMD doesn’t provide a UI to show the metadata it stores. So, for querying
    metadata, we need to use its client API. See the following code example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Queries all registered artifacts
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Queries artifact by ID
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Queries artifact by uri
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Queries artifact by using a filter
  prefs: []
  type: TYPE_NORMAL
- en: The MLMD “Get Started” doc ([http://mng.bz/VpWy](http://mng.bz/VpWy)) provides
    lots of query examples for fetching metadata of artifacts, executions, and context.
    If you are interested, please take a look.
  prefs: []
  type: TYPE_NORMAL
- en: 'The best way to learn the data model of MLMD is to look at its database schema.
    You can first create an SQLite database and configure the MLMD metadata store
    to use that database and then run the MLMD sample code. At the end, all the entities
    and tables are created in the local SQLite database. By looking at the table schema
    and content, you will gain a deep understanding of how the metadata is organized
    in MLMD, so you can build a nice UI on top of it yourself. The following sample
    code shows how to configure the MLMD metadata store to use a local SQLite database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Local files path of the SQLite database
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Allows read, write, and create
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.2 MLflow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: MLflow ([https://mlflow.org/docs/latest/tracking.html](https://mlflow.org/docs/latest/tracking.html))
    is an open source MLOps platform. It is designed to manage the ML lifecycle, including
    experimentation, reproducibility, deployment, and central model registry.
  prefs: []
  type: TYPE_NORMAL
- en: 'Compared to MLMD, MLflow is a whole system, not a library. It’s composed of
    four main components:'
  prefs: []
  type: TYPE_NORMAL
- en: '*MLflow Tracking (a metadata tracking server*) —For recording and querying
    metadata'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*MLflow Projects*—For packaging code in a reusable and reproducible way'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*MLflow Models*—For packaging ML models that can be used for different model
    serving tools'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*MLflow Model Registry*—For managing the full life cycle of MLflow models with
    a UI, such as model lineage, model versioning, annotation, and production promotion'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we focus on the tracking server, since that’s most relevant
    to metadata management.
  prefs: []
  type: TYPE_NORMAL
- en: System overview
  prefs: []
  type: TYPE_NORMAL
- en: MLflow provides six different setup approaches. For example, the metadata of
    MLflow runs (training pipeline) can be recorded to local files, an SQL database,
    a remote server, or a remote server with proxied storage backend access. For details
    about these six different setup methods, you can check out the MLflow doc “How
    Runs and Artifacts Are Recorded” ([https://mlflow.org/docs/latest/tracking.html#id27](https://mlflow.org/docs/latest/tracking.html#id27)).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we focus on the most commonly used setup approach: remote
    server with proxied artifact storage access. See figure 8.7 for the system overview
    diagram.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/08-07.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 Setting up MLflow tracker server for metadata ingestion and query
  prefs: []
  type: TYPE_NORMAL
- en: From figure 8.7, we see that each step/action of a deep learning pipeline (workflow)
    uses the MLflow client to instrument metadata and artifact to the MLflow tracking
    server. The tracking server saves metadata, such as metrics, parameters, and tags,
    in a specified SQL database; artifacts, such as models, images, and documents,
    are saved in a configured object storage, such as Amazon S3.
  prefs: []
  type: TYPE_NORMAL
- en: 'MLflow offers two ways to upload artifacts: (1) direct upload from the client
    and (2) proxy upload through the tracking server. In figure 8.6, we illustrated
    the latter method: utilizing the tracking server as a proxy server for any operations
    involving artifacts. The advantage is the end users can have a direct path to
    access the backend remote object store without providing access credentials.'
  prefs: []
  type: TYPE_NORMAL
- en: Another nice thing about MLflow is that it offers a nice UI; data scientists
    can check and search metadata thru the website hosted in the tracking server.
    The UI allows users not only to view metadata from a pipeline execution perspective
    but also to search and operate models directly.
  prefs: []
  type: TYPE_NORMAL
- en: Logging API
  prefs: []
  type: TYPE_NORMAL
- en: 'Sending metadata to the MLflow tracking server is straightforward. We can start
    by creating an active run as a context manager and then call the log function
    to log artifacts or a single key-value parameter, metric, and tag. See the sample
    code as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Defines the MLflow server URL
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Logs metadata in Python context manager made by the MLflow ActiveRun object
  prefs: []
  type: TYPE_NORMAL
- en: Automatic logging
  prefs: []
  type: TYPE_NORMAL
- en: If you are tired of specifying lots of metadata yourself, MLflow supports automatic
    logging. By calling `mlflow.autolog()` or a library-specific autolog function
    before your training code, such as `mlflow.tensorflow.autolog()`, `mlflow.keras.autolog()`,
    or `mlflow.pytorch.autolog()`, MLflow will log metadata, even artifacts, automatically
    without the need for an explicit log statement. If you want to learn more about
    MLflow logging, check out the MLflow logging functions doc ([http://mng.bz/xd1d](http://mng.bz/xd1d)).
  prefs: []
  type: TYPE_NORMAL
- en: Searching metadata
  prefs: []
  type: TYPE_NORMAL
- en: 'The tracking UI hosted by the MLflow tracking server lets you visualize, search,
    and compare runs, as well as download run artifacts or metadata for analysis in
    other tools. The UI contains the following key features: experiment-based run
    listing and comparison, searching for runs by parameter or metric value, visualizing
    run metrics, and downloading run results. Besides UI, you can also achieve all
    the operations provided in the tracking UI programmatically, as in the following
    examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Initializes client
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Prints out the execution stages of a run
  prefs: []
  type: TYPE_NORMAL
- en: The programmatic metadata access is not only helpful when using your analysis
    tool (for example, pandas) to query and compare the model performance of different
    training runs, but also for integrating models with your model serving system
    since it allows you to fetch models from the MLflow model registry programmatically.
    For full `MLflowClient` API usage, you can check out the MLflow Tracking API doc
    ([http://mng.bz/GRzO](http://mng.bz/GRzO)).
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.3 MLflow vs. MLMD
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: From the descriptions in the previous sections, we see that MLMD is more of
    a lightweight library while MLflow is an MLOps platform. Both tools can run independently,
    offer metadata ingestion and search functionality, and track metadata on the basis
    of model training runs. But MLflow offers much more.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to MLMD’s functionality, MLflow supports automatic metadata logging
    and a well-designed UI to visualize experiment metadata (including experiment
    comparison), model registry, artifact management, code reproducibility, model
    package, and more.
  prefs: []
  type: TYPE_NORMAL
- en: If you need to introduce a complete, new metadata and artifact store to your
    system, MFflow is your first choice. It’s supported by an active open source community,
    and it covers most of the user requirements of ML metadata management. As a bonus,
    MLflow has a great support on MLOps, such as MLflow project management and model
    deployment.
  prefs: []
  type: TYPE_NORMAL
- en: If you already have an artifact registry and a metric visualization website
    and you want to integrate metadata functionality into your existing system, then
    MLMD is a good choice. MLMD is lightweight, easy to use, and simple to learn.
    For example, the Kubeflow ([https://www.kubeflow.org/](https://www.kubeflow.org/))
    deep learning platform integrates MLMD as the metadata tracking tool into its
    components, such as Kubeflow pipeline ([https://www.kubeflow.org/docs/components/pipelines/](https://www.kubeflow.org/docs/components/pipelines/)).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Machine learning metadata can be categorized into four buckets: model training
    runs, general artifacts, model artifacts, and pipelines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A metadata and artifact store is designed to support model performance comparison,
    troubleshooting, and reproducing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A good metadata management system can help to show model lineage, enable model
    reproducibility, and facilitate model comparison.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLMD is a lightweight metadata management tool, which originated from the TensorFlow
    pipeline, but it can be used independently. For example, Kubeflow uses MLMD to
    manage ML metadata in its pipeline component.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLMD is good for integrating metadata management into an existing system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLflow is an MLOps platform; it’s designed to manage the ML lifecycle, including
    experimentation, reproducibility, deployment, and a central model registry.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLflow is applicable if you want to introduce a completely independent metadata
    and artifact management system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
