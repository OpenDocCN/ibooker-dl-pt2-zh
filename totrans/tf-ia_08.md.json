["```py\nimport os\nimport requests\nimport zipfile\nif not os.path.exists(os.path.join('data','tiny-imagenet-200.zip')):\n    url = \"http:/ /cs231n.stanford.edu/tiny-imagenet-200.zip\"\n    r = requests.get(url)\n\n    if not os.path.exists('data'):\n        os.mkdir('data')\n\n    with open(os.path.join('data','tiny-imagenet-200.zip'), 'wb') as f:\n        f.write(r.content)\n\n    with zipfile.ZipFile(\n        os.path.join('data','tiny-imagenet-200.zip'), 'r'\n    ) as zip_ref:\n        zip_ref.extractall('data')\nelse:\n    print(\"The file already exists.\")\n```", "```py\nimport pandas as pd                                                      ❶\nimport os                                                                ❶\n\ndata_dir = os.path.join('data', 'tiny-imagenet-200')                     ❷\nwnids_path = os.path.join(data_dir, 'wnids.txt')                         ❷\nwords_path = os.path.join(data_dir, 'words.txt')                         ❷\n\ndef get_tiny_imagenet_classes(wnids_path, words_path):                   ❸\n    wnids = pd.read_csv(wnids_path, header=None, squeeze=True)           ❹\n    words = pd.read_csv(words_path, sep='\\t', index_col=0, header=None)  ❹\n    words_200 = words.loc[wnids].rename({1:'class'}, axis=1)             ❺\n    words_200.index.name = 'wnid'                                        ❻\n    return words_200.reset_index()                                       ❼\n\nlabels = get_tiny_imagenet_classes(wnids_path, words_path)               ❽\nlabels.head(n=25)                                                        ❾\n```", "```py\ndef get_image_count(data_dir):    \n    # Get the count of JPEG files in a given folder (data_dir)\n    return len(\n        [f for f in os.listdir(data_dir) if f.lower().endswith('jpeg')]\n    )\n\n   # Apply the function above to all the subdirectories in the train folder \nlabels[\"n_train\"] = labels[\"wnid\"].apply(\n    lambda x: get_image_count(os.path.join(data_dir, 'train', x, 'images'))\n)\n# Get the top 10 entries in the labels dataframe\nlabels.head(n=10)\n```", "```py\nlabels[\"n_train\"].describe()\n```", "```py\ncount    200.0\nmean     500.0\nstd        0.0\nmin      500.0\n25%      500.0\n50%      500.0\n75%      500.0\nmax      500.0\nName: n_train, dtype: float64\n```", "```py\nimport os                                                                 ❶\nfrom PIL import Image                                                     ❶\nimport pandas as pd                                                       ❶\n\nimage_sizes = []                                                          ❷\nfor wnid in labels[\"wnid\"].iloc[:25]:                                     ❸\n    img_dir = os.path.join(\n        'data', 'tiny-imagenet-200', 'train', wnid, 'images'\n    )                                                                     ❹\n    for f in os.listdir(img_dir):                                         ❺\n        if f.endswith('JPEG'):                                            ❺\n            image_sizes.append(Image.open(os.path.join(img_dir, f)).size) ❻\n\nimg_df = pd.DataFrame.from_records(image_sizes)                           ❼\nimg_df.columns = [\"width\", \"height\"]                                      ❽\nimg_df.describe()                                                         ❾\n```", "```py\nImage.open(os.path.join(img_dir, f)).size \n```", "```py\nimage_sizes = [(image_1.width, image_1.height), (image_2.width, image_2.height), ..., (image_n.width, image_n.height)]\n```", "```py\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport os\n\nrandom_seed = 4321\nbatch_size = 128\nimage_gen = ImageDataGenerator(samplewise_center=True, validation_split=0.1)\n```", "```py\ndef simple_generator():\n    for i in range(0, 100):\n        yield (i, i*2)\n```", "```py\niterator = simple_generator()\n```", "```py\ntarget_size = (56,56)\n\ntrain_gen = image_gen.flow_from_directory(\n    directory=os.path.join('data','tiny-imagenet-200', 'train'), \n    target_size=target_size, classes=None,\n    class_mode='categorical', batch_size=batch_size, \n    shuffle=True, seed=random_seed, subset='training'\n)\nvalid_gen = image_gen.flow_from_directory (\n    directory=os.path.join('data','tiny-imagenet-200', 'train'), \n    target_size=target_size, classes=None,\n    class_mode='categorical', batch_size=batch_size, \n    shuffle=True, seed=random_seed, subset='validation'\n)\n```", "```py\nfrom functools import partial\ntarget_size = (56,56)\n\npartial_flow_func = partial(\n        image_gen.flow_from_directory, \n        directory=os.path.join('data','tiny-imagenet-200', 'train'), \n        target_size=target_size, classes=None,\n        class_mode='categorical', batch_size=batch_size, \n        shuffle=True, seed=random_seed)\n\ntrain_gen = partial_flow_func(subset='training')\nvalid_gen = partial_flow_func(subset='validation')\n```", "```py\ndef data_gen_aux(gen):\n    for x,y in gen:\n        yield x,(y,y,y)\n\ntrain_gen_aux = data_gen_aux(train_gen)\nvalid_gen_aux = data_gen_aux(valid_gen)\n```", "```py\ndef get_test_labels_df(test_labels_path):\n    test_df = pd.read_csv(test_labels_path, sep='\\t', index_col=None, header=None)\n    test_df = test_df.iloc[:,[0,1]].rename({0:\"filename\", 1:\"class\"}, axis=1)\n    return test_df\n\ntest_df = get_test_labels_df(os.path.join('data','tiny-imagenet-200',  'val', 'val_annotations.txt'))\n```", "```py\n    test_gen = image_gen.flow_from_dataframe(\n        dataframe=test_df, directory=os.path.join('data','tiny-imagenet-\n➥ 200',  'val', 'images'), target_size=target_size, \n➥ class_mode='categorical', batch_size=batch_size, shuffle=False\n    )\n```", "```py\ndef data_gen_corrupt(gen):\n    for x,y in gen:\n        yield x,(y,y,y)\n```", "```py\ndef stem(inp):\n    conv1 = Conv2D(\n        64, (7,7), strides=(1,1), activation='relu', padding='same'\n    )(inp)                                                                ❶\nmaxpool2 = MaxPool2D((3,3), strides=(2,2), padding='same')(conv1)         ❷\nlrn3 = Lambda(\n    lambda x: tf.nn.local_response_normalization(x)\n)(maxpool2)                                                               ❸\n\nconv4 = Conv2D(\n    64, (1,1), strides=(1,1), padding='same'\n)(lrn3)                                                                   ❹\nconv5 = Conv2D(\n    192, (3,3), strides=(1,1), activation='relu', padding='same'\n)(conv4)                                                                  ❹\n    lrn6 = Lambda(lambda x: tf.nn.local_response_normalization(x))(conv5) ❺\n\n    maxpool7 = MaxPool2D((3,3), strides=(1,1), padding='same')(lrn6)      ❻\n\n    return maxpool7                                                       ❼\n```", "```py\nx = tf.keras.layers.Input(shape=(10,))\nmax_out = tf.keras.layers.Lambda(lambda x: tf.reduce_max(x, axis=1))(x)\n```", "```py\ndef inception(inp, n_filters):\n\n    # 1x1 layer\n    out1 = Conv2D(\n        n_filters[0][0], (1,1), strides=(1,1), activation='relu', \n➥ padding='same'\n    )(inp)\n\n    # 1x1 followed by 3x3\n    out2_1 = Conv2D(\n        n_filters[1][0], (1,1), strides=(1,1), activation='relu', \n➥ padding='same')\n(inp)\n    out2_2 = Conv2D(\n        n_filters[1][1], (3,3), strides=(1,1), activation='relu', \n➥ padding='same'\n)(out2_1)\n\n# 1x1 followed by 5x5\nout3_1 = Conv2D(\n    n_filters[2][0], (1,1), strides=(1,1), activation='relu', \n➥ padding='same'\n)(inp)\nout3_2 = Conv2D(\n    n_filters[2][1], (5,5), strides=(1,1), activation='relu', \n➥ padding='same'\n)(out3_1)\n\n# 3x3 (pool) followed by 1x1\nout4_1 = MaxPool2D(\n    (3,3), strides=(1,1), padding='same'\n)(inp)\nout4_2 = Conv2D(\n    n_filters[3][0], (1,1), strides=(1,1), activation='relu', \n➥ padding='same'\n)(out4_1)\n\nout = Concatenate(axis=-1)([out1, out2_2, out3_2, out4_2])\nreturn out\n```", "```py\n[(1x1 filters), (1x1 filters, 3x3 filters), (1x1 filters, 5x5 filters), (1x1 filters)]\n```", "```py\ndef aux_out(inp,name=None):    \n    avgpool1 = AvgPool2D((5,5), strides=(3,3), padding='valid')(inp)       ❶\n    conv1 = Conv2D(128, (1,1), activation='relu', padding='same')(avgpool1)❷\n    flat = Flatten()(conv1)                                                ❸\n    dense1 = Dense(1024, activation='relu')(flat)                          ❹\n    aux_out = Dense(200, activation='softmax', name=name)(dense1)          ❺\n    return aux_out\n```", "```py\ndef inception_v1():\n\n    K.clear_session()\n\n    inp = Input(shape=(56,56,3))                                          ❶\n    stem_out = stem(inp)                                                  ❷\n    inc_3a = inception(stem_out, [(64,),(96,128),(16,32),(32,)])          ❸\n    inc_3b = inception(inc_3a, [(128,),(128,192),(32,96),(64,)])          ❸\n\n    maxpool = MaxPool2D((3,3), strides=(2,2), padding='same')(inc_3b)\n\n    inc_4a = inception(maxpool, [(192,),(96,208),(16,48),(64,)])          ❸\n    inc_4b = inception(inc_4a, [(160,),(112,224),(24,64),(64,)])          ❸\n\n    aux_out1 = aux_out(inc_4a, name='aux1')                               ❹\n\n    inc_4c = inception(inc_4b, [(128,),(128,256),(24,64),(64,)])\n    inc_4d = inception(inc_4c, [(112,),(144,288),(32,64),(64,)])\n    inc_4e = inception(inc_4d, [(256,),(160,320),(32,128),(128,)])\n\n    maxpool = MaxPool2D((3,3), strides=(2,2), padding='same')(inc_4e)\n\n    aux_out2 = aux_out(inc_4d, name='aux2')                               ❹\n\n    inc_5a = inception(maxpool, [(256,),(160,320),(32,128),(128,)])\n    inc_5b = inception(inc_5a, [(384,),(192,384),(48,128),(128,)])\n    avgpool1 = AvgPool2D((7,7), strides=(1,1), padding='valid')(inc_5b)   ❺\n\n    flat_out = Flatten()(avgpool1)                                        ❻\n    out_main = Dense(200, activation='softmax', name='final')(flat_out)   ❼\n\n    model = Model(inputs=inp, outputs=[out_main, aux_out1, aux_out2])   \n    model.compile(loss='categorical_crossentropy', \n                       optimizer='adam', metrics=['accuracy'])            ❽\n    return model\n```", "```py\nmodel = Model(inputs=inp, outputs=[out_main, aux_out1, aux_out2])\n```", "```py\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n```", "```py\nmodel = inception_v1()\n```", "```py\n# 1x1 layer\nout1 = Conv2D(64, (1,1), strides=(1,1), activation='relu', padding='same')(inp)\n# 1x1 followed by 3x3\nout2_1 = Conv2D(\n    96, (1,1), strides=(1,1), activation='relu', padding='same'\n)(inp)\nout2_2 = Conv2D(\n    128, (3,3), strides=(1,1), activation='relu', padding='same'\n)(out2_1)\n\n# 1x1 followed by 5x5\n# Here 5x5 is represented by two 3x3 convolution layers\nout3_1 = Conv2D(\n    16, (1,1), strides=(1,1), activation='relu', padding='same'\n)(inp)\nout3_2 = Conv2D(\n    32, (3,3), strides=(1,1), activation='relu', padding='same'\n)(out3_1)\nout3_3 = Conv2D(\n    32, (3,3), strides=(1,1), activation='relu', padding='same'\n)(out3_2)\n\n# 3x3 (pool) followed by 1x1\nout4_1 = MaxPool2D((3,3), strides=(1,1), padding='same')(inp)\nout4_2 = Conv2D(\n    32, (1,1), strides=(1,1), activation='relu', padding='same'\n)(out4_1)\n\nout = Concatenate(axis=-1)([out1, out2_2, out3_3, out4_2])\n```", "```py\ndef get_steps_per_epoch(n_data, batch_size):\n    if n_data%batch_size==0:\n        return int(n_data/batch_size)\n    else:\n        return int(n_data*1.0/batch_size)+1\n```", "```py\nfrom tensorflow.keras.callbacks import CSVLogger\nimport time\nimport os\n\nif not os.path.exists('eval'):\n    os.mkdir('eval')                                               ❶\n\ncsv_logger = CSVLogger(os.path.join('eval','1_eval_base.log'))     ❷\n\nhistory = model.fit(\n    x=train_gen_aux,                                               ❸\n    validation_data=valid_gen_aux,                                 ❸\n    steps_per_epoch=get_steps_per_epoch(0.9*500*200,batch_size),   ❸\n    validation_steps=get_steps_per_epoch(0.1*500*200,batch_size),  ❸\n    epochs=50, \n    callbacks=[csv_logger]                                         ❸\n)                                                                  ❸\n\nif not os.path.exists('models'):\n    os.mkdir(\"models\")\nmodel.save(os.path.join('models', 'inception_v1_base.h5'))         ❹\n```", "```py\nTrain for 704 steps, validate for 79 steps\nEpoch 1/50\n704/704 [==============================] - 196s 279ms/step - loss: 14.6223 \n➥ - final_loss: 4.9449 - aux1_loss: 4.8074 - aux2_loss: 4.8700 - \n➥ final_accuracy: 0.0252 - aux1_accuracy: 0.0411 - aux2_accuracy: 0.0347 \n➥ - val_loss: 13.3207 - val_final_loss: 4.5473 - val_aux1_loss: 4.3426 - \n➥ val_aux2_loss: 4.4308 - val_final_accuracy: 0.0595 - val_aux1_accuracy: \n➥ 0.0860 - val_aux2_accuracy: 0.0765\n...\nEpoch 50/50\n704/704 [==============================] - 196s 279ms/step - loss: 0.6361 - \n➥ final_loss: 0.2271 - aux1_loss: 0.1816 - aux2_loss: 0.2274 - \n➥ final_accuracy: 0.9296 - aux1_accuracy: 0.9411 - aux2_accuracy: 0.9264 \n➥ - val_loss: 27.6959 - val_final_loss: 7.9506 - val_aux1_loss: 10.4079 - \n➥ val_aux2_loss: 9.3375 - val_final_accuracy: 0.2703 - val_aux1_accuracy: \n➥ 0.2318 - val_aux2_accuracy: 0.2361\n```", "```py\nmodel = load_model(os.path.join('models','inception_v1_base.h5'))\ntest_res = model.evaluate(test_gen_aux, steps=get_steps_per_epoch(200*50, \n➥ batch_size))\ntest_res_dict = dict(zip(model.metrics_names, test_res))\n```", "```py\n196/196 [==============================] - 17s 88ms/step - loss: 27.7303 - \n➥ final_loss: 7.9470 - aux1_loss: 10.3892 - aux2_loss: 9.3941 - \n➥ final_accuracy: 0.2700 - aux1_accuracy: 0.2307 - aux2_accuracy: 0.2367\n```", "```py\ndef get_img_minimum(path):\n    img = np.array(Image.open(path))\n    return np.min(img)\n\ndf[“minimum”] = df[“filepath”].apply(lambda x: get_img_minimum(x))\n```", "```py\ndef data_gen_corrupt(gen):\n    for x,y in gen:\n        if np.random.normal()>0:\n            y = 0\n        yield x,(y,y,y)\n\n```", "```py\ndef poolception(x):\n    out1 = MaxPool2D(pool_size=(3,3), strides=(2,2), padding=’same’)(x)\n    out2 = MaxPool2D(pool_size=(5,5), strides=(2,2), padding=’same’)(out1)\n    out3 = AvgPool2D(pool_size=(3,3), strides=(2,2), padding=’same’)(out2)\n    out = Concatenate(axis=-1)([out1, out2, out3])\n    return out\n```", "```py\nmodel.fit(x=x, y=y, batch_size=250, epochs=10)\n```"]