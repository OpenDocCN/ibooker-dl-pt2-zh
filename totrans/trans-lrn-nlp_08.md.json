["```py\nfrom Simon.DataGenerator import DataGenerator      ❶\n\ndata_cols = 5                                      ❷\ndata_count = 10                                    ❸\n\ntry_reuse_data = False                             ❹\nsimulated_data, header = DataGenerator.gen_test_data((data_count, data_cols), try_reuse_data)\nprint(\"SIMULATED DATA\")                            ❺\nprint(simulated_data)\nprint(\"SIMULATED DATA HEADER:\")\nprint(header)\n```", "```py\nSIMULATED DATA:\n[['byoung@hotmail.com' 'Jesse' 'True' 'PPC' 'Lauraview']\n ['cindygilbert@gmail.com' 'Jason' 'True' 'Intel' 'West Brandonburgh']\n ['wilsonalexis@yahoo.com' 'Matthew' 'True' 'U; Intel'\n  'South Christopherside']\n ['cbrown@yahoo.com' 'Andrew' 'False' 'U; PPC' 'Loganside']\n ['christopher90@gmail.com' 'Devon' 'True' 'PPC' 'East Charlesview']\n ['deanna75@gmail.com' 'Eric' 'False' 'U; PPC' 'West Janethaven']\n ['james80@hotmail.com' 'Ryan' 'True' 'U; Intel' 'Loriborough']\n ['cookjennifer@yahoo.com' 'Richard' 'True' 'U; Intel' 'Robertsonchester']\n ['jonestyler@gmail.com' 'John' 'True' 'PPC' 'New Kevinfort']\n ['johnsonmichael@gmail.com' 'Justin' 'True' 'U; Intel' 'Victormouth']]\nSIMULATED DATA HEADER:\n[list(['email', 'text']) list(['text']) list(['boolean', 'text'])\n list(['text']) list(['text'])]\n```", "```py\nmodel = Classifier.generate_model(max_len, max_cells, category_count)   ❶\nClassifier.load_weights(checkpoint, None, model, checkpoint_dir)        ❷\nmodel.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])                                              ❸\n```", "```py\nmodel.summary() \n```", "```py\n______________________________________________________________________________________\nLayer (type)                    Output Shape         Param #   Connected to           \n======================================================================================\ninput_1 (InputLayer)            (None, 500, 20)      0                                \n______________________________________________________________________________________\ntime_distributed_1 (TimeDistrib (None, 500, 512)     3202416   input_1[0][0]          \n______________________________________________________________________________________\nlstm_3 (LSTM)                   (None, 128)          328192    time_distributed_1[0][0]\n______________________________________________________________________________________\nlstm_4 (LSTM)                   (None, 128)          328192    time_distributed_1[0][0]\n______________________________________________________________________________________\nconcatenate_2 (Concatenate)     (None, 256)          0         lstm_3[0][0]           \n                                                               lstm_4[0][0]           \n______________________________________________________________________________________\ndropout_5 (Dropout)             (None, 256)          0         concatenate_2[0][0]    \n______________________________________________________________________________________\ndense_1 (Dense)                 (None, 128)          32896     dropout_5[0][0]        \n______________________________________________________________________________________\ndropout_6 (Dropout)             (None, 128)          0         dense_1[0][0]          \n______________________________________________________________________________________\ndense_2 (Dense)                 (None, 9)            1161      dropout_6[0][0]        \n```", "```py\np_threshold = 0.5                                       ❶\ny = model.predict(X_baseball)                           ❷\nresult = encoder.reverse_label_encode(y,p_threshold)    ❸\nprint(\"Recall that the column headers were:\")           ❹\nprint(list(raw_baseball_data))\nprint(\"The predicted classes and probabilities are respectively:\")\nprint(result)\n```", "```py\nRecall that the column headers were:\n['Player', 'Number_seasons', 'Games_played', 'At_bats', 'Runs', 'Hits', 'Doubles', 'Triples', 'Home_runs', 'RBIs', 'Walks', 'Strikeouts', 'Batting_average', 'On_base_pct', 'Slugging_pct', 'Fielding_ave', 'Position', 'Hall_of_Fame']\nThe predicted classes and probabilities are respectively:\n([('text',), ('int',), ('int',), ('int',), ('int',), ('int',), ('int',), ('int',), ('int',), ('int',), ('int',), ('int',), ('float',), ('float',), ('float',), ('float',), ('text',), ('int',)], [[0.9970826506614685], [0.9877430200576782], [0.9899477362632751], [0.9903284907341003], [0.9894667267799377], [0.9854978322982788], [0.9892633557319641], [0.9895514845848083], [0.989467203617096], [0.9895854592323303], [0.9896339178085327], [0.9897230863571167], [0.9998295307159424], [0.9998230338096619], [0.9998272061347961], [0.9998039603233337], [0.9975670576095581], [0.9894945025444031]])\n```", "```py\n         Player Number_seasons Games_played At_bats  Runs  Hits Doubles  \\\n0    HANK_AARON             23         3298   12364  2174  3771     624   \n1   JERRY_ADAIR             13         1165    4019   378  1022     163   \n2  SPARKY_ADAMS             13         1424    5557   844  1588     249   \n3   BOBBY_ADAMS             14         1281    4019   591  1082     188   \n4    JOE_ADCOCK             17         1959    6606   823  1832     295   \n\n  Triples Home_runs  RBIs Walks Strikeouts Batting_average On_base_pct  \\\n0      98       755  2297  1402       1383           0.305       0.377   \n1      19        57   366   208        499           0.254       0.294   \n2      48         9   394   453        223           0.286       0.343   \n3      49        37   303   414        447           0.269        0.34   \n4      35       336  1122   594       1059           0.277       0.339   \n\n  Slugging_pct Fielding_ave     Position Hall_of_Fame  \n0        0.555         0.98     Outfield            1  \n1        0.347        0.985  Second_base            0  \n2        0.353        0.974  Second_base            0  \n3        0.368        0.955   Third_base            0  \n4        0.485        0.994   First_base            0  \n```", "```py\nX = encoder.encodeDataFrame(raw_library_data)          ❶\ny = model.predict(X)                                   ❷\nresult = encoder.reverse_label_encode(y,p_threshold)   ❸\nprint(\"Recall that the column headers were:\")\nprint(list(raw_library_data))\nprint(\"The predicted class/probability:\")\nprint(result)\n```", "```py\nRecall that the column headers were:\n['PCT_ELEC_IN_TOT_VOLS', 'TOT_AV_VOLS']\nThe predicted class/probability:\n([('text',), ('int',)], [[0.7253058552742004], [0.7712462544441223]])\n```", "```py\n     PCT_ELEC_IN_TOT_VOLS TOT_AV_VOLS\n0                  90.42%          57\n1                  74.83%       2,778\n2                  85.55%       1,590\n3                   9.22%      83,906\n4                  66.63%       4,261\n...                   ...         ...\n1202                0.00%      35,215\n1203                0.00%     109,499\n1204                0.00%         209\n1205                0.00%      18,748\n1206                0.00%        2403\n\n[1207 rows x 2 columns]\n```", "```py\nprint(raw_library_data.shape)\n```", "```py\n                                                                           ❶\npercent_value_list = raw_library_data['PCT_ELEC_IN_TOT_VOLS'].values.tolist()\nint_value_list = raw_library_data['TOT_AV_VOLS'].values.tolist()\n\n                                                                           ❷\noriginal_length = raw_data.shape[0]                                        ❸\nchunk_size = 20 # length of each newly generated column\nheader_list = list(range(2*original_length/ /chunk_size))                   ❹\nnew_raw_data = pd.DataFrame(columns = header_list)                         ❺\nfor i in range(original_length/ /chunk_size):                               ❻\n    new_raw_data[i] = percent_value_list[i:i+chunk_size]                   ❼\n    new_raw_data[original_length/ /chunk_size+i] = int_value_list[i:i+chunk_size]                                        ❽\n\nheader = [(\"percent\",),]*(original_length/ /chunk_size)                     ❾\nheader.extend([(\"int\",),]*(original_length/ /chunk_size))\n```", "```py\nimport numpy as np\n\nold_weights = model.layers[8].get_weights()                          ❶\nold_category_index = encoder.categories.index('text')                ❷\nencoder.categories.append(\"percent\")                                 ❸\nencoder.categories.sort()                                            ❹\nnew_category_index = encoder.categories.index('percent')             ❺\n\nnew_weights = np.copy(old_weights)                                   ❻\nnew_weights[0] = np.insert(new_weights[0], new_category_index, old_weights[0][:,old_category_index], axis=1)                   ❼\nnew_weights[1] = np.insert(new_weights[1], new_category_index, 0)    ❽\n```", "```py\nmodel = Classifier.generate_transfer_model(max_len, max_cells, category_count, category_count+1, checkpoint, checkpoint_dir)\n```", "```py\nfor layer in model.layers:                                                      ❶\n    layer.trainable = False\nmodel.layers[-1].trainable = True                                               ❷\n\nmodel.layers[8].set_weights(new_weights)                                        ❸\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])❹\n```", "```py\nimport time\n\nX = encoder.encodeDataFrame(new_raw_data)                                          ❶\ny = encoder.label_encode(header)                                                   ❷\ndata = Classifier.setup_test_sets(X, y)                                            ❸\n\nbatch_size = 4\nnb_epoch = 10\nstart = time.time()\nhistory = Classifier.train_model(batch_size, checkpoint_dir, model, nb_epoch, data)❹\nend = time.time()\nprint(\"Time for training is %f sec\"%(end-start)) \n```", "```py\ny = model.predict(data.X_test)                                        ❶\nresult = encoder.reverse_label_encode(y,p_threshold)                  ❷\n\nprint(\"The predicted classes and probabilities are respectively:\")    ❸\nprint(result) \nprint(\"True labels/probabilities, for comparision:\") print(encoder.reverse_label_encode(data.y_test,p_threshold))\n```", "```py\nThe predicted classes and probabilities are respectively:\n([('percent',), ('percent',), ('int',), ('int',), ('percent',), ('int',), ('percent',), ('int',), ('int',), ('percent',), ('percent',), ('int',)], [[0.7889140248298645], [0.7893422842025757], [0.7004106640815735], [0.7190601229667664], [0.7961368560791016], [0.9885498881340027], [0.8160757422447205], [0.8141483068466187], [0.5697212815284729], [0.8359809517860413], [0.8188782930374146], [0.5185337066650391]])\nTrue labels/probabilities, for comparision:\n([('percent',), ('percent',), ('int',), ('int',), ('percent',), ('int',), ('percent',), ('int',), ('int',), ('percent',), ('percent',), ('int',)], [[1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1]])\n```", "```py\ndef build_model(): \n  input_text = layers.Input(shape=(1,), dtype=\"string\")\n  embedding = ElmoEmbeddingLayer()(input_text)\n  dense = layers.Dense(256, activation='relu')(embedding)    ❶\n  pred = layers.Dense(1, activation='sigmoid')(dense)        ❷\n\n  model = Model(inputs=[input_text], outputs=pred)\n\n  model.compile(loss='binary_crossentropy', optimizer='adam',\n                                 metrics=['accuracy'])       ❸\n  model.summary()                                            ❹\n\n  return model\n\n# Build and fit\nmodel = build_model()\nmodel.fit(train_x,                                           ❺\n          train_y,\n          validation_data=(test_x, test_y),\n          epochs=10,\n          batch_size=4)\n```", "```py\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         (None, 1)                 0         \n_________________________________________________________________\nelmo_embedding_layer_1 (Elmo (None, 1024)              4         \n_________________________________________________________________\ndense_1 (Dense)              (None, 256)               262400    \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 257       \n=================================================================\nTotal params: 262,661\nTrainable params: 262,661\nNon-trainable params: 0\n```"]