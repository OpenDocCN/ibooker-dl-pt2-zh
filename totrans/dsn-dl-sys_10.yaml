- en: 10 Path to production
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10 生产路径 '
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '本章涵盖 '
- en: Preliminary work and tasks before productionizing deep learning models
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '在生产化深度学习模型之前的初步工作和任务 '
- en: Productionizing deep learning models with a deep learning system
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '使用深度学习系统生产化深度学习模型 '
- en: Model deployment strategies for experimentation in production
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '用于生产中实验的模型部署策略 '
- en: For the concluding chapter of the book, we think it makes sense to return to
    a high-level view and connect all the dots from previous chapters. We have now
    discussed in detail each service in a deep learning system. In this chapter, we
    will talk about how the services work together to support the deep learning *product
    development cycle* we introduced in chapter 1\. That cycle, if you remember, brings
    the efforts of research and data science all the way through productionization
    to the end products that customers use.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '对于书的结尾章节，我们认为回到高层次视角并连接前几章的所有内容是有意义的。我们现在已经详细讨论了深度学习系统中的每个服务。在本章中，我们将讨论这些服务如何共同支持我们在第
    1 章介绍的深度学习*产品开发周期*。如果你还记得的话，该周期将研究和数据科学的努力一直延伸到生产化，最终产品是客户使用的产品。 '
- en: 'As a reminder, figure 10.1, borrowed from chapter 1, shows the product development
    cycle. Our focus in this chapter will be on three phases that occur toward the
    end of the process: deep learning research, prototyping, and productionization.
    This focus means we’ll ignore the cycles of experimentation, testing, training,
    and exploration and look at how to take a final product from the research phase
    to the end product, making it ready to be released to the public.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '作为提醒，图 10.1 取自第 1 章，展示了产品开发周期。本章我们的重点将放在该过程末尾发生的三个阶段上：深度学习研究、原型制作和生产化。这意味着我们将忽略实验、测试、训练和探索的循环，并关注如何将最终产品从研究阶段转化为最终产品，使其准备好发布到公众。 '
- en: '![](../Images/10-01.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10-01.png)'
- en: Figure 10.1 This deep learning development cycle is a typical scenario for bringing
    deep learning from research to the finished product. Boxes (3), (4), and (5) are
    the focus of this chapter.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10.1 这个深度学习开发周期是将深度学习从研究转化为成品的典型情景。框 (3)、(4) 和 (5) 是本章的重点。 '
- en: Definition Productionization is the process of producing a product that is worthwhile
    and ready to be consumed by its users. Production worthiness is commonly defined
    as being able to serve customer requests, withstand a certain level of request
    load, and gracefully handle adverse situations such as malformed input and request
    overload.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 生产化是生产出一个值得用户消费的产品的过程。生产值得性通常被定义为能够服务于客户请求、承受一定水平的请求负载，并优雅地处理诸如格式不正确的输入和请求过载等不利情况。
- en: As we’ve said, this chapter focuses on the path of the production cycle from
    research, through prototyping, to productionization. Let’s lift those three phases
    out of the typical development cycle shown in figure 10.1, so we can see them
    in greater detail. We’ve placed those phases in the next diagram, figure 10.2,
    and zoomed in on them to reveal the steps within each phase, as well as the ways
    the three phases connect to each other. Don’t let the complexity of this diagram
    alarm you! We will walk you through each phase, and each step, in this chapter.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '正如我们所说，本章重点讨论从研究、原型制作到生产化的生产周期路径。让我们将这三个阶段从图 10.1 所示的典型开发周期中拿出来，以便更详细地查看它们。我们将这些阶段放在下一个图表中，图
    10.2，并放大它们，以显示每个阶段内的步骤，以及三个阶段之间的连接方式。不要让这个图表的复杂性吓到你！在本章中，我们将带领你走过每个阶段和每个步骤。 '
- en: '![](../Images/10-02.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10-02.png)'
- en: Figure 10.2 Three major stages in a sample path to production. Research and
    prototyping go through many iterations before productionization.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10.2 生产样品路径中的三个主要阶段。在生产化之前，研究和原型制作会经历许多迭代。 '
- en: Let’s briefly review this diagram, as it will provide a preview of the chapter.
    The first two phases in figure 10.2 are research and prototyping. Both of these
    efforts require rapid iteration and turnaround from model training and experimentation.
    The primary interaction point in these phases (steps 1–8) is a notebooking environment.
    Using notebooks, researchers and data scientists invoke the dataset management
    service for tracking training datasets (during steps 2 and 6) and may use the
    training service and hyperparameter optimization library/service for model training
    and experimentation (during steps 4 and 8). We go over these phases in section
    10.1, ending at the point where training data shape and code become fairly stable
    and are ready to be productionized. In other words, the team has come up with
    the more-or-less final version, and it is ready to go through the final steps
    to release it to the public.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要回顾一下这张图表，因为它将提供本章的预览。图 10.2 中的前两个阶段是研究和原型制作。这两项工作都需要从模型训练和实验中进行快速迭代和周转。这些阶段（步骤
    1–8）中的主要交互点是笔记本环境。使用笔记本，研究人员和数据科学家会调用数据集管理服务来跟踪训练数据集（在步骤 2 和 6 中），并可能使用训练服务和超参数优化库/服务来进行模型训练和实验（在步骤
    4 和 8 中）。我们在 10.1 节中详细介绍了这些阶段，直到训练数据形状和代码变得相当稳定并且可以进行产品化。换句话说，团队已经提出了更多或更少的最终版本，并且准备通过最后的步骤将其发布给公众。
- en: In section 10.2, we will pick up from the previous section and walk through
    the productionization of models, up to the point where models are served to production
    inference request traffic.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在 10.2 节中，我们将从上一节开始，介绍模型的产品化，直到模型被提供给生产推断请求流量的点。
- en: Definition *Inference requests* are inputs generated by a user or an application
    against a trained model to produce inferences. Take visual recognition as an example.
    An inference request can be a picture of a cat. Using a trained visual recognition
    model, or an inference, a label in the form of the word *cat* can be generated.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 *推断请求* 是用户或应用程序针对经过训练的模型生成推断的输入。以视觉识别为例。推断请求可以是一张猫的图片。使用经过训练的视觉识别模型，或者推断，可以生成一个形式为
    *猫* 的标签。
- en: This section corresponds to the third and final phase in figure 10.2\. In productionization,
    pretty much every service in our system will come into play. The dataset management
    service manages training data; the workflow management service launches and tracks
    training workflows; the training service executes and manages model training jobs;
    the metadata and artifacts store contains and tracks code artifacts, trained models,
    and their metadata; and the model service serves trained models to inference request
    traffic.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这个部分对应于图 10.2 中的第三个也是最后一个阶段。在产品化中，我们系统中的几乎每个服务都会发挥作用。数据集管理服务管理训练数据；工作流管理服务启动和跟踪训练工作流；训练服务执行和管理模型训练作业；元数据和工件存储包含和跟踪代码工件、训练模型及其元数据；模型服务将经过训练的模型提供给推断请求流量。
- en: From productionization, we move to deployment. In section 10.3, we look at a
    number of model deployment strategies that support updating models to new versions
    in production. These strategies also support experimentation in production. The
    main focus here will be on the model service because this is where all inference
    requests are served.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 从产品化转移到部署。在 10.3 节中，我们将研究一些支持在生产中更新模型到新版本的模型部署策略。这些策略还支持在生产中进行实验。这里的主要重点将放在模型服务上，因为这是所有推断请求都被服务的地方。
- en: By walking through the full journey to production, we hope that you will be
    able to see how the first principles that we discussed in previous chapters affect
    the work of different parties that use the system to deliver deep learning features.
    The understanding that you gain from this chapter should help you adapt your own
    design to different situations. We will use the development of an image recognition
    product as an example to illustrate all the steps in action.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 通过全程跟踪到产品的完整过程，我们希望您能够看到我们在前几章中讨论的第一原则如何影响使用该系统提供深度学习功能的不同方。您从本章中获得的理解应该有助于您将自己的设计适应不同的情况。我们将使用图像识别产品的开发作为示例，以说明所有操作步骤。
- en: 10.1 Preparing for productionization
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.1 准备产品化
- en: In this section, we will look at the journey of a deep learning model from before
    its birth to when it is ready to be productionized. In figure 10.3, we highlight
    the phases of deep learning research and prototyping from the larger deep learning
    development cycle (shown in figure 10.1). We will start from the deep learning
    research step, where model training algorithms are born. Not every organization
    performs deep learning research—some use out-of-the-box training algorithms—so
    feel free to skip this step if it does not apply to your situation.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将研究深度学习模型从诞生前到准备投入生产的过程。在图 10.3 中，我们突出显示了深度学习研究和原型设计的阶段，这些阶段来自更大的深度学习开发循环（如图
    10.1所示）。我们将从深度学习研究阶段开始，在这个阶段，模型训练算法诞生。并不是每个组织都进行深度学习研究，有些使用现成的训练算法，如果这种情况不适用于您，请随意跳过这一步。
- en: '![](../Images/10-03.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10-03.png)'
- en: Figure 10.3 Excerpt of the path to production in the research and prototyping
    phases
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3 深度学习研究和原型设计阶段通往生产的路径摘录
- en: After deep learning research, we proceed to prototyping. At this stage, we assume
    algorithms are ready to use for training models. A rapid and iterative process
    of data exploration and experimental model training forms the central part of
    this step. The goal of this step is to find the appropriate training data shape
    and develop a stable codebase for model training.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习研究之后，我们继续进行原型设计。在此阶段，我们假定算法已经准备好用于训练模型。数据探索和实验模型训练的快速迭代过程构成了这一步骤的核心。这一步的目标是找到适当的训练数据形状，并开发一个稳定的模型训练代码库。
- en: 10.1.1 Research
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.1 研究
- en: New deep learning algorithms are invented, and existing ones are improved through
    research. Because peer-reviewed research requires reproducible results, model
    training data needs to be publicly accessible. Many public datasets, such as ImageNet
    for example, are available for research teams to use.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 通过研究发明了新的深度学习算法，并通过研究改进了现有算法。因为同行评审研究需要可重复的结果，所以模型训练数据需要公开获取。许多公共数据集，例如 ImageNet，都可以供研究团队使用。
- en: 'The notebooking environment, such as JupyterLab, is a popular choice among
    researchers for prototyping model training due to its interactivity and flexibility.
    Let’s go through some sample steps that a researcher may take during model training
    prototyping:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本环境，如 JupyterLab，是研究人员原型设计模型培训最流行的选择，因为它的交互性和灵活性。让我们看一下研究人员在模型训练原型设计期间可能采取的一些示例步骤：
- en: Alice, a deep learning researcher, is working on improving a visual recognition
    algorithm. After working on her theories, she is ready to start prototyping.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 深度学习研究员Alice正在致力于改进视觉识别算法。经过理论探讨后，她准备开始原型设计。
- en: Alice creates a new notebook in JupyterLab.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Alice 在 JupyterLab 中创建了一个新的笔记本。
- en: Alice wants to use the ImageNet dataset for training and benchmarking her algorithm.
    She may
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Alice 想要使用 ImageNet 数据集来训练和基准测试她的算法。她可能
- en: Write code to download the dataset to her notebook and store it in the dataset
    management service (chapter 2) for reuse.
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写代码将数据集下载到她的笔记本中，并在数据集管理服务中存储它以供重复使用（第 2 章）。
- en: Find that the dataset is already stored in the dataset management service and
    write code to use it as is.
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 发现数据集已经存储在数据集管理服务中，并编写代码来直接使用它。
- en: Alice starts coding up improvements on an existing visual recognition algorithm
    until it can produce experimental models locally within the notebook.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Alice 开始对现有的视觉识别算法进行改进，直到它能够在笔记本中本地生成实验模型。
- en: Alice tries to change some hyperparameters, train and test a few experimental
    models, and compare the metrics that they generate.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Alice 尝试更改一些超参数，训练和测试一些实验模型，并比较它们生成的指标。
- en: Alice may further use hyperparameter optimization techniques (chapter 5) to
    run more experiments automatically to confirm that she has indeed made improvements
    to the existing algorithm.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Alice 可以进一步使用超参数优化技术（第 5 章）自动运行更多实验，以确认她确实对现有算法进行了改进。
- en: Alice publishes her results and packages her training code improvement as a
    library for others to use.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Alice 发布了她的研究结果，并将她的训练代码改进打包成一个库供他人使用。
- en: By using a versioned dataset for training, Alice is sure that the input training
    data of all her experimental model training runs are the same. She also uses a
    source-control management system, such as Git, to keep track of her code so that
    all experimental models can be traced back to a version of her code.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用版本化数据集进行训练，爱丽丝确保她所有实验模型训练运行的输入训练数据是相同的。她还使用源控制管理系统，如Git，以跟踪她的代码，以便所有实验模型都可以追溯到她的代码版本。
- en: Notice that model training at this stage usually takes place locally at the
    computing node where the notebooking environment is hosted, so it is a good idea
    to allocate sufficient resources to these nodes. If training data is stored over
    the network, make sure the read speed does not become a bottleneck for model training.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这个阶段，模型训练通常在笔记环境托管的计算节点上进行，因此很有必要为这些节点分配足够的资源。如果训练数据存储在网络上，请确保读取速度不会成为模型训练的瓶颈。
- en: 10.1.2 Prototyping
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.2 原型制作
- en: Prototyping is where research is bridged with real-world use cases. It is a
    practice of looking for the right combination of training data, algorithm, hyperparameter,
    and inference support to provide the right deep learning feature that meets product
    requirements.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 原型制作是将研究与实际用例联系起来的实践。它是寻找合适的训练数据、算法、超参数和推断支持的正确深度学习特征的实践，以满足产品需求。
- en: 'At this stage, it is still very common to find notebooking environments to
    be the top choice for data scientists and engineers due to the rapid iterative
    nature of prototyping. A fast turnaround is expected. Let’s walk through one possible
    scenario of prototyping:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，很常见发现笔记环境仍然是数据科学家和工程师的首选，原因是原型制作的快速迭代性质。期望快速交付。让我们走一遍原型制作的一个可能的场景：
- en: The model development team receives product requirements to improve the motion
    detection of a security camera product.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型开发团队收到产品需求，要改善安全摄像头产品的运动检测。
- en: Based on the requirements, the team finds that Alice’s new vision recognition
    training algorithm may help improve motion detection.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于需求，团队发现爱丽丝的新视觉识别训练算法可能有助于改善运动检测。
- en: 'The team creates a new notebook and begins exploring data that is relevant
    for model training, given the set of algorithms that they picked:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 团队创建一个新的笔记本，并开始探索与他们选择的算法集相关的用于模型训练的数据：
- en: The team may be able to use existing, collected data for model training if they
    happen to fit the problem that is being solved.
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果团队有已收集的数据与正在解决的问题相匹配，他们可能能够使用现有数据进行模型训练。
- en: In some cases, the team may need to collect new data for training.
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在某些情况下，团队可能需要收集新数据进行训练。
- en: In most cases, transfer learning is applied at this stage, and the team picks
    one or more existing models as source models.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在大多数情况下，这个阶段应用迁移学习，并且团队会选择一个或多个现有模型作为源模型。
- en: The team develops modeling code with algorithms and trains experimental models
    with collected data and source models.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 团队开发带有算法的建模代码，并使用已收集的数据和源模型训练实验模型。
- en: Experimental models are evaluated to see whether they yield satisfactory results.
    Steps 3 to 6 are repeated until the training data shape and code become stable.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实验模型经过评估，以查看是否产生令人满意的结果。步骤3到6会重复进行，直到训练数据形状和代码稳定。
- en: We call steps 3 to 6 the exploratory loop. This loop corresponds to the iterate
    circles in the blow-up section of prototyping in figure 10.3\. When prototyping
    begins, the loop is iterated rapidly. The focus at this stage is to narrow down
    the training data shape and code.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称步骤3到6为探索循环。这个循环对应于图10.3中原型制作的放大部分的迭代圈。当原型制作开始时，这个循环会迅速迭代。此阶段的重点是缩小训练数据形状和代码。
- en: Once training data shape and code become stable, they will be ready for further
    tuning and optimization. The goal in this phase is to converge to a state where
    model training and inference code are ready to be packaged and deployed to production.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练数据形状和代码稳定，它们将准备好进行进一步的调整和优化。这个阶段的目标是收敛到一个状态，使得模型训练和推断代码可以被打包并部署到生产环境。
- en: 10.1.3 Key takeaways
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.3 关键收获
- en: 'We have walked through both the research and prototyping phases of our reference
    deep learning development cycle in figure 10.1\. Even though they serve different
    purposes, we see quite a bit of overlap in how they work with the deep learning
    system:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经走过图10.1中我们参考的深度学习开发周期中的研究和原型制作阶段。尽管它们有不同的目的，但我们看到它们在如何处理深度学习系统方面有相当大的重叠：
- en: The notebooking environment is a common choice for both research and preproduction
    prototyping due to its high degree of interactivity and verbosity.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 笔记本环境是研究和预生产原型设计的常见选择，因为它具有高度的交互性和冗长性。
- en: Access to training data should be as wide and flexible as possible (to the limit
    of legality and compliance), which helps accelerate the data exploration process.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对训练数据的访问应尽可能宽泛和灵活（在合法性和合规性的限制范围内），这有助于加速数据探索过程。
- en: Sufficient computing resources should be allocated for model training so that
    turnaround time is short.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应为模型训练分配足够的计算资源，以确保周转时间短。
- en: At a minimum, use a dataset management service and a source-control management
    system to keep track of the provenance of experimental models. In addition, use
    a metadata store to contain metrics and associate them with a training dataset
    and code for complete lineage tracking.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 至少使用数据集管理服务和源代码控制管理系统来跟踪实验模型的来源。此外，使用元数据存储来包含指标，并将其与训练数据集和代码关联起来，以进行完整的渊源追踪。
- en: 10.2 Model productionization
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.2 模型生产化
- en: 'Before deep learning models can be integrated into an end product, they need
    to go through the process of productionization. There are certainly many interpretations
    of this term, but fundamentally:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习模型能够集成到最终产品之前，它们需要经历生产化过程。对于这个术语肯定有很多解释，但基本上是：
- en: Models need to serve production inference requests either from end products
    or end users.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型需要为生产推断请求提供服务，无论是来自最终产品还是最终用户。
- en: Model serving should meet a predefined service level agreement, such as responding
    within 50 milliseconds or being available 99.999% of the time.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型服务应满足预定义的服务水平协议，例如在 50 毫秒内响应或可用时间达到 99.999%。
- en: Production problems related to models should be easy to troubleshoot.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与模型相关的生产问题应易于故障排除。
- en: In this section, we will look at how deep learning models transition from living
    in a rather dynamic environment, such as a notebook, to a production environment
    where they are hit by various harsh conditions. Figure 10.4 shows the productionization
    phase relative to the rest of the development cycle. Let’s review the steps in
    this phase.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看看深度学习模型如何从一个相当动态的环境，比如笔记本，过渡到一个生产环境，在那里它们会受到各种严苛条件的影响。图 10.4 显示了生产化阶段相对于开发周期的其余部分。让我们回顾一下这个阶段的步骤。
- en: '![](../Images/10-04.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10-04.png)'
- en: Figure 10.4 Excerpt of the path to production in the productionization phase
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.4 生产化阶段的生产路径摘录
- en: 10.2.1 Code componentization
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.1 代码组件化
- en: As shown in the previous section, it is common during prototyping for training
    data preparation, model training, and inference code to exist in a single notebook.
    To productionize them into a deep learning system, we need to split them apart
    as separate components. One approach to splitting the components, or *code componentization*,
    is shown in figure 10.5.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一节所示，在原型设计期间，常常将训练数据准备、模型训练和推断代码存在于单个笔记本中。为了将它们生产化为一个深度学习系统，我们需要将它们拆分为单独的组件。拆分组件的一种方法，即*代码组件化*，如图
    10.5 所示。
- en: '![](../Images/10-05.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10-05.png)'
- en: Figure 10.5 Componentizing code from a single notebook into multiple pieces
    that can be packaged separately. The first split happens where a trained model
    is the output. An optional second split happens where training data is the output.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.5 将代码从单个笔记本组件化为可以单独打包的多个部分。第一个分割发生在训练好的模型是输出的地方。可选的第二次分割发生在训练数据是输出的地方。
- en: 'Let’s put the process in the figure into action. The first place to draw a
    line for separation in the code is where the model is the output. This should
    result in two pieces of code as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将图中的流程付诸实施。在代码中划分的第一条分割线是模型是输出的地方。这应该导致两段代码如下所示：
- en: Model training code that outputs a model
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出模型的模型训练代码
- en: Model inference code that takes a model and an inference request as input to
    produce an inference
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型推断代码，以模型和推断请求作为输入，产生推断作为输出
- en: 'Optionally, the model training code can be split as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 可选择地，模型训练代码可以分为以下几部分：
- en: Training data transformation code, which takes raw data as input and output
    training data that can be consumed by model training code
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据转换代码，以原始数据作为输入，并输出可被模型训练代码使用的训练数据
- en: Model training code, which takes training data and trains a model as its output
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型训练代码，它以训练数据作为输入，并训练出一个模型作为输出
- en: If you have other model training code that will benefit from the same kind of
    prepared data, it is a good idea to perform this separation. Separation is also
    a good idea if your data preparation step needs to be executed on a different
    cadence from model training.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有其他受益于相同类型准备好的数据的模型训练代码，将这些代码分离开是个好主意。如果你的数据准备步骤需要以不同的节奏执行模型训练，分离也是一个好主意。
- en: 10.2.2 Code packaging
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.2 代码打包
- en: Once code components are separated cleanly, they are ready to be packaged for
    deployment. To be able to run them on a training service (chapter 3), model service
    (chapter 6), and workflow service (chapter 9), we first need to make sure they
    follow the conventions set by these services.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦代码组件被清晰地分离，它们就可以被打包部署。为了能够在训练服务（第3章）、模型服务（第6章）和工作流服务（第9章）上运行它们，我们首先需要确保它们遵循这些服务设置的约定。
- en: Model training code should be modified to fetch training data from the location
    indicated by an environment variable set by the training service. A similar convention
    should be followed in other components.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练代码应修改为从训练服务设置的环境变量指示的位置获取训练数据。其他组件应遵循类似的约定。
- en: 'Model inference code should follow the convention of the model serving strategy
    of your choice:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 模型推理代码应遵循您选择的模型服务策略的约定：
- en: If you use direct model embedding, work with the team that embeds your model
    to make sure your inference code works.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你使用直接的模型嵌入，需要与嵌入模型的团队合作，确保你的推理代码可以正常工作。
- en: If you plan to serve your model with model service, make sure your inference
    code provides an interface with which the model service can communicate.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你计划使用模型服务来提供模型，确保你的推理代码提供了一个接口，使得模型服务可以进行通信。
- en: If you use a model server, you may not need model inference code as long as
    the model server can serve the model properly.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你使用模型服务器，只要模型服务器能够正确地提供模型，你可能就不需要模型推理代码。
- en: We package these code components as Docker containers so that they can be launched,
    accessed, and tracked by their respective host services. An example of how this
    is done can be found in appendix A. If special data transformation is needed,
    we can integrate data transformation code into the data management service.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这些代码组件打包为Docker容器，以便它们可以被它们各自的主机服务启动、访问和跟踪。如何做到这一点的示例可以在附录A中找到。如果需要特殊的数据转换，我们可以将数据转换代码集成到数据管理服务中。
- en: 10.2.3 Code registration
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.3 代码注册
- en: Before training code and inference code can be used by the system, their packages
    must be registered and stored with the metadata and artifacts service. This provides
    the necessary link between the training code and the inference code. Let’s look
    at how they are related (figure 10.6).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练代码和推理代码可以被系统使用之前，它们的包必须被注册并存储到元数据和工件服务中。这提供了训练代码和推理代码之间的必要联系。让我们看看它们是如何相关的（图10.6）。
- en: '![](../Images/10-06.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10-06.png)'
- en: Figure 10.6 A simple training and inference execution flow in a production deep
    learning system
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.6 在生产深度学习系统中的简单训练和推理执行流程
- en: Once the training and inference codes are packaged as containers (the training
    container and inference container in the diagram), they can be registered with
    the metadata and artifacts store using a common handle, such as `visual_recognition`
    like in the example shown in figure 10.6\. This helps system services find and
    use correct code containers when they receive requests that provide the same handle
    name. We will continue walking through the figure in the next few sections.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练和推理代码被打包为容器（如图中的训练容器和推理容器），它们可以使用一个共同的句柄（例如`visual_recognition`，就像图10.6中所示的示例一样）注册到元数据和工件存储中。这有助于系统服务在接收到提供相同句柄名称的请求时找到并使用正确的代码容器。我们将在接下来的几个部分继续讲解这个图。
- en: 10.2.4 Training workflow setup
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.4 训练工作流程设置
- en: We recommend setting up a training workflow even if you do not regularly train
    your model. The main reason is to provide reproducibility of the same model training
    flow in production. This is helpful when someone other than you needs to train
    a model and can use the flow that you set up. In some cases, the production environment
    is isolated, and going through a workflow that is set up in the production environment
    may be the only way to produce a model there. In figure 10.7, we’ve enlarged the
    model training portion of the previous diagram so you can see the details.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议即使您不经常训练模型，也要设置训练工作流程。 主要原因是在生产中提供相同模型训练流程的可重复性。 当除您之外的其他人需要训练模型并且可以使用您设置的流程时，这非常有帮助。
    在某些情况下，生产环境是隔离的，并且通过在生产环境中设置的工作流程可能是在那里生成模型的唯一方法。 在图 10.7 中，我们已将先前图表的模型训练部分放大，以便您可以看到细节。
- en: '![](../Images/10-07.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10-07.png)'
- en: Figure 10.7 A typical production model training setup. The workflow service
    manages what, when, and how training flows are run. The training service runs
    model training jobs. The metadata and artifacts store provides training code,
    stores trained models, and associates them with metadata.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.7 典型的生产模型训练设置。 工作流服务管理训练流程的何时以及如何运行。 训练服务运行模型训练作业。 元数据和工件存储提供训练代码，存储训练过的模型，并将其与元数据关联起来。
- en: Referring to figure 10.7, once a training workflow for `visual_recognition`
    is set up, training can be triggered to the training service. The training service
    uses the handle to look up the training code container to execute from the metadata
    and artifacts store. Once a model is trained, it saves the model to the metadata
    and artifacts store with the handle name.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 参考图 10.7，一旦设置了 `visual_recognition` 的训练工作流程，就可以触发训练到训练服务。 训练服务使用句柄从元数据和工件存储中查找要执行的训练代码容器。
    一旦模型训练完成，它会使用句柄名称将模型保存到元数据和工件存储中。
- en: At this stage, it is also common to find hyperparameter optimization techniques
    being used to find optimal training hyperparameters during model training. If
    an HPO service is used, the workflow will talk to the HPO service instead of to
    the training service directly. If you need a reminder of how the HPO service works,
    refer to chapter 5.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，通常也会发现使用超参数优化技术来在模型训练期间找到最佳训练超参数。 如果使用了 HPO 服务，则工作流程将与 HPO 服务而不是直接与训练服务进行通信。
    如果您需要提醒 HPO 服务如何工作，请参阅第 5 章。
- en: 10.2.5 Model inferences
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.5 模型推理
- en: Once a model is trained and registered in the production environment, the next
    step is to make sure it can handle inference requests coming into our system at
    a certain rate and producing inferences within a certain latency. We can do so
    by sending inference requests to the model service. When the model service receives
    a request, it finds the handle name `visual_recognition` in the request and queries
    the metadata and artifacts store for the matching model inference container and
    model file. The model service can then use these artifacts together to produce
    an inference response. You can see this process in figure 10.8, which, again,
    is an enlarged version of the model serving portion of figure 10.6.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型在生产环境中训练并注册，下一步是确保它能够处理系统中进入的推理请求，并在一定的速率内产生推理。 我们可以通过将推理请求发送到模型服务来实现这一点。
    当模型服务收到请求时，它会在请求中找到句柄名称 `visual_recognition`，并查询元数据和工件存储以获取匹配的模型推理容器和模型文件。 然后，模型服务可以一起使用这些工件来生成推理响应。
    您可以在图 10.8 中看到这个过程，再次强调，这是图 10.6 模型服务部分的放大版本。
- en: '![](../Images/10-08.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10-08.png)'
- en: Figure 10.8 A typical production model serving setup. When inference requests
    arrive at the model service, the service looks for the inference container and
    model using the metadata and artifacts store to produce inference responses.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.8 典型的生产模型服务设置。 当推理请求到达模型服务时，服务使用元数据和工件存储查找推理容器和模型，以产生推理响应。
- en: If you use a model server, you may need a thin layer in front of it so that
    it knows where to obtain the model file. Some model server implementations support
    custom model manager implementation, which can also be used to make queries against
    the metadata and artifacts store to load the correct model.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用模型服务器，可能需要在其前面加上一层薄膜，以便它知道从哪里获取模型文件。 一些模型服务器实现支持自定义模型管理器实现，也可以用于针对元数据和工件存储进行查询以加载正确的模型。
- en: 10.2.6 Product integration
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.6 产品集成
- en: 'Once you get a proper inference response back from the model service, it is
    time to integrate a model service client into the product that will use these
    inferences. This is the final step in productionization, and we should make sure
    to check a few things before launching it to the end customer. Because we are
    improving the motion detection of our security camera product, we must integrate
    a model service client in the security camera video-processing backend that will
    request inferences from the newly improved model:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在从模型服务获得适当的推断响应后，就该将模型服务客户端集成到将使用这些推断的产品中。这是生产化的最后一步，我们在推出给最终客户之前需要确保检查几个问题。因为我们正在改进安全摄像头产品的运动检测，所以我们必须将一个模型服务客户端集成到安全摄像头视频处理后端中，以便从新改进的模型请求推断：
- en: Make sure the inference response is consumable by the product using it.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保推断响应可被使用它的产品消耗。
- en: Stress test inferencing by sending inference requests at a rate that approximates
    the production traffic.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过以接近生产流量的速率发送推断请求来进行压力测试推断。
- en: Test inferencing with malformed inference requests to make sure they do not
    break the model inference code or the model service.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用不规范的推断请求进行测试推断，以确保其不会破坏模型推断代码或模型服务。
- en: This is a very basic list of items to look for. Your organization may define
    more production readiness criteria that you need to fulfill before launching the
    integration. Besides system metrics that can tell us whether our model is serving
    inference requests properly, we should also set up business metrics that will
    tell us whether the model helps the business use case.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个非常基本的检查项目列表。您的组织可能会定义更多生产准备性标准，您需要在集成之前满足这些标准。除了可以告诉我们模型是否正常提供推断请求的系统指标外，我们还应该设置业务指标，这些指标将告诉我们模型是否有助于实现业务用例。
- en: 10.3 Model deployment strategies
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.3 模型部署策略
- en: 'In the previous section, we went through a sample path from prototyping to
    production. That process assumed that the model was deployed for the first time,
    without an existing version of the model to replace. Once a model is being used
    in production, unless there is a maintenance window allowance, we usually need
    to use a model deployment strategy to ensure production inference request traffic
    is not disrupted. In fact, these model deployment strategies can also double as
    performing experimentation in production by using business metrics that we set
    up in the previous section. We will look at three strategies: canary, blue-green,
    and multi-armed bandit.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们通过一个从原型设计到生产的示例路径。这个过程假设模型是首次部署，没有现有版本的模型需要替换。一旦模型在生产中使用，除非有维护窗口允许，否则通常需要使用模型部署策略来确保生产推断请求流量不会中断。事实上，这些模型部署策略也可以作为在生产中进行实验的方式，通过使用前一节中设置的业务指标。我们将看下三种策略：金丝雀、蓝绿和多臂赌博机。
- en: 10.3.1 Canary deployment
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.1 金丝雀部署
- en: A canary deployment, similar to A/B testing, means deploying a new model to
    serve a small portion of production inference requests while keeping the old one
    to serve the remaining majority of requests. An example is shown in figure 10.9\.
    This requires the model service to support segmenting and routing a small portion
    of inference request traffic to the new model.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 金丝雀部署（类似A/B测试）是指在保留旧模型为大多数请求提供服务的同时，将新模型部署到生产推断请求的一小部分上。示例如图10.9所示。这需要模型服务支持将一小部分推断请求流量分段和路由到新模型。
- en: '![](../Images/10-09.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10-09.png)'
- en: Figure 10.9 Canary deployment showing the redirection of a small portion of
    traffic to a new version of the model
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.9金丝雀部署示例，显示将一小部分流量重定向到新模型的版本
- en: With this strategy, any potential adverse effect from deploying a new model
    is contained within a small portion of end users. Rolling back is rather straightforward
    by routing all inference request traffic back to the old model.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种策略，可能由于部署新模型而产生的任何不利影响都局限在少部分最终用户之内。通过将所有推断请求流量路由回旧模型，回滚变得相当简单。
- en: A drawback to this approach is that you only get to know the performance of
    the model to a small portion of end users. Releasing the new model to serve all
    inference request traffic may have a different effect from what you observe serving
    only a small portion of the traffic.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的一个缺点是，您只能了解到模型对一小部分最终用户的性能。将新模型发布以服务于所有推断请求流量可能会产生与仅为流量的一小部分提供服务时所观察到的不同效果。
- en: 10.3.2 Blue-green deployment
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.2 蓝绿部署
- en: In our context, a blue-green deployment means deploying a new model, routing
    all inference request traffic to the new model, and keeping the old model online
    until we have confidence that the new model’s performance meets expectations.
    Implementation-wise it is the simplest among all three strategies because there
    is no traffic splitting at all. All the service needs to do is point to the new
    model internally to serve all inference requests. Blue-green deployment is depicted
    in figure 10.10.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的上下文中，蓝绿部署意味着部署一个新模型，将所有推理请求流量路由到新模型，并保持旧模型在线，直到我们确信新模型的性能符合预期。在实现上，它是三种策略中最简单的，因为根本没有流量分割。服务所需做的一切就是内部指向新模型以服务所有推理请求。图
    10.10 描绘了蓝绿部署。
- en: '![](../Images/10-10.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10-10.png)'
- en: Figure 10.10 Blue–green deployment showing the direction of all traffic to either
    the old (blue) or the new (green) model
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.10 蓝绿部署显示所有流量指向旧（蓝色）或新（绿色）模型的方向
- en: Not only is this strategy simple, but it also gives you a full picture of how
    the model performs when serving all end users. Rolling back is also straightforward.
    Simply point the model service back to the old model.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这种策略不仅简单，而且在为所有终端用户提供服务时可以全面了解模型的表现。回滚也很简单。只需将模型服务指向旧模型即可。
- en: The obvious downside to this approach is if something goes wrong with the new
    model, it affects all end users. This strategy may make sense when you are developing
    a new product feature based on a new model. As you iterate on training a better
    model over time, you may want to move away from this strategy, as end users would
    have built their expectations on having a stable experience.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的明显缺点是，如果新模型出现问题，将影响所有终端用户。当您基于新模型开发新产品功能时，这种策略可能是合理的。随着您随时间迭代训练更好的模型，您可能希望摆脱这种策略，因为终端用户会根据稳定的体验建立他们的期望。
- en: 10.3.3 Multi-armed bandit deployment
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.3 多臂赌博机部署
- en: Multi-armed bandit (MAB) is the most complex deployment strategy among the three.
    MAB refers to a technique that continuously monitors multiple models’ performance
    and redirects more and more inference request traffic to the winning model over
    time. This uses the most elaborate implementation of the model service because
    it requires the service to understand model performance, which can be complicated
    depending on how your model performance metrics are defined. MAB deployment is
    illustrated in figure 10.11.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 多臂赌博机（MAB）是三种策略中最复杂的部署策略。MAB 指的是一种技术，它持续监控多个模型的性能，并随着时间的推移将越来越多的推理请求流量重定向到胜利模型。这使用了模型服务最复杂的实现，因为它要求服务了解模型性能，这取决于您的模型性能指标如何定义。MAB
    部署在图 10.11 中说明。
- en: '![](../Images/10-11.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10-11.png)'
- en: Figure 10.11 Multi-armed bandit deployment showing traffic patterns on day 0
    and day 1\. Notice that model v2.0a is leading in terms of model performance on
    day 1 as it is receiving the most traffic.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.11 多臂赌博机部署显示了第 0 天和第 1 天的流量模式。注意，模型 v2.0a 在第 1 天的模型性能方面处于领先地位，因为它接收到了最多的流量。
- en: This strategy does come with an advantage, though, because it maximizes the
    benefits of the best-performing models within a set time frame, whereas with a
    canary deployment, you may only gain minimal benefits if the new model outperforms
    the old one. Notice that you should make sure the model service reports how the
    traffic split changes over time. This helps correlate with the models’ performance.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，这种策略确实带来了一个优势，因为它在一定的时间范围内最大化了表现最佳模型的利益，而使用金丝雀部署，如果新模型胜过旧模型，您可能只会获得最小的利益。注意，您应确保模型服务报告流量分割随时间的变化。这有助于与模型的性能相关联。
- en: Summary
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Deep learning research teams invent and improve deep learning algorithms that
    are used to train models.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习研究团队发明并改进用于训练模型的深度学习算法。
- en: Model development teams make use of existing algorithms and available data to
    train models that help solve a deep learning use case.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型开发团队利用现有的算法和可用数据来训练帮助解决深度学习用例的模型。
- en: Both research and prototyping require a high degree of interactivity with code
    development, data exploration, and visualization. Notebooking environments are
    popular choices for these teams.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 研究和原型制作都需要与代码开发、数据探索和可视化的高度互动。笔记本环境是这些团队的流行选择。
- en: Dataset management service can be used during research and prototyping to help
    track training data used to train experimental models.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集管理服务可以在研究和原型制作过程中使用，帮助跟踪用于训练实验模型的训练数据。
- en: Once training data and code are stable enough, the first step to productionization
    is to package model training code, model inference code, and any source models.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦训练数据和代码足够稳定，投入生产的第一步是打包模型训练代码、模型推理代码和任何源模型。
- en: These packages can be used by all services of the deep learning system to train,
    track, and serve models.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有深度学习系统的服务都可以使用这些软件包来训练、跟踪和提供模型。
- en: Once a model training workflow is up and running and a satisfactory inference
    response is obtained, integration with the end-user product can begin.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦模型训练工作流程开始运行并获得令人满意的推理响应，就可以开始与最终用户产品的集成。
- en: A model deployment strategy is required if serving inference requests cannot
    be interrupted.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果提供推理请求不能中断，则需要一个模型部署策略。
- en: Multiple model deployment strategies are available, and they can double as performing
    experimentation in production.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多种模型部署策略可供选择，它们可以兼作在生产中进行实验。
