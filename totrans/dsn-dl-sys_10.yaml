- en: 10 Path to production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Preliminary work and tasks before productionizing deep learning models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Productionizing deep learning models with a deep learning system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model deployment strategies for experimentation in production
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the concluding chapter of the book, we think it makes sense to return to
    a high-level view and connect all the dots from previous chapters. We have now
    discussed in detail each service in a deep learning system. In this chapter, we
    will talk about how the services work together to support the deep learning *product
    development cycle* we introduced in chapter 1\. That cycle, if you remember, brings
    the efforts of research and data science all the way through productionization
    to the end products that customers use.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a reminder, figure 10.1, borrowed from chapter 1, shows the product development
    cycle. Our focus in this chapter will be on three phases that occur toward the
    end of the process: deep learning research, prototyping, and productionization.
    This focus means we’ll ignore the cycles of experimentation, testing, training,
    and exploration and look at how to take a final product from the research phase
    to the end product, making it ready to be released to the public.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/10-01.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 This deep learning development cycle is a typical scenario for bringing
    deep learning from research to the finished product. Boxes (3), (4), and (5) are
    the focus of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Definition Productionization is the process of producing a product that is worthwhile
    and ready to be consumed by its users. Production worthiness is commonly defined
    as being able to serve customer requests, withstand a certain level of request
    load, and gracefully handle adverse situations such as malformed input and request
    overload.
  prefs: []
  type: TYPE_NORMAL
- en: As we’ve said, this chapter focuses on the path of the production cycle from
    research, through prototyping, to productionization. Let’s lift those three phases
    out of the typical development cycle shown in figure 10.1, so we can see them
    in greater detail. We’ve placed those phases in the next diagram, figure 10.2,
    and zoomed in on them to reveal the steps within each phase, as well as the ways
    the three phases connect to each other. Don’t let the complexity of this diagram
    alarm you! We will walk you through each phase, and each step, in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/10-02.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 Three major stages in a sample path to production. Research and
    prototyping go through many iterations before productionization.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s briefly review this diagram, as it will provide a preview of the chapter.
    The first two phases in figure 10.2 are research and prototyping. Both of these
    efforts require rapid iteration and turnaround from model training and experimentation.
    The primary interaction point in these phases (steps 1–8) is a notebooking environment.
    Using notebooks, researchers and data scientists invoke the dataset management
    service for tracking training datasets (during steps 2 and 6) and may use the
    training service and hyperparameter optimization library/service for model training
    and experimentation (during steps 4 and 8). We go over these phases in section
    10.1, ending at the point where training data shape and code become fairly stable
    and are ready to be productionized. In other words, the team has come up with
    the more-or-less final version, and it is ready to go through the final steps
    to release it to the public.
  prefs: []
  type: TYPE_NORMAL
- en: In section 10.2, we will pick up from the previous section and walk through
    the productionization of models, up to the point where models are served to production
    inference request traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Definition *Inference requests* are inputs generated by a user or an application
    against a trained model to produce inferences. Take visual recognition as an example.
    An inference request can be a picture of a cat. Using a trained visual recognition
    model, or an inference, a label in the form of the word *cat* can be generated.
  prefs: []
  type: TYPE_NORMAL
- en: This section corresponds to the third and final phase in figure 10.2\. In productionization,
    pretty much every service in our system will come into play. The dataset management
    service manages training data; the workflow management service launches and tracks
    training workflows; the training service executes and manages model training jobs;
    the metadata and artifacts store contains and tracks code artifacts, trained models,
    and their metadata; and the model service serves trained models to inference request
    traffic.
  prefs: []
  type: TYPE_NORMAL
- en: From productionization, we move to deployment. In section 10.3, we look at a
    number of model deployment strategies that support updating models to new versions
    in production. These strategies also support experimentation in production. The
    main focus here will be on the model service because this is where all inference
    requests are served.
  prefs: []
  type: TYPE_NORMAL
- en: By walking through the full journey to production, we hope that you will be
    able to see how the first principles that we discussed in previous chapters affect
    the work of different parties that use the system to deliver deep learning features.
    The understanding that you gain from this chapter should help you adapt your own
    design to different situations. We will use the development of an image recognition
    product as an example to illustrate all the steps in action.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1 Preparing for productionization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will look at the journey of a deep learning model from before
    its birth to when it is ready to be productionized. In figure 10.3, we highlight
    the phases of deep learning research and prototyping from the larger deep learning
    development cycle (shown in figure 10.1). We will start from the deep learning
    research step, where model training algorithms are born. Not every organization
    performs deep learning research—some use out-of-the-box training algorithms—so
    feel free to skip this step if it does not apply to your situation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/10-03.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 Excerpt of the path to production in the research and prototyping
    phases
  prefs: []
  type: TYPE_NORMAL
- en: After deep learning research, we proceed to prototyping. At this stage, we assume
    algorithms are ready to use for training models. A rapid and iterative process
    of data exploration and experimental model training forms the central part of
    this step. The goal of this step is to find the appropriate training data shape
    and develop a stable codebase for model training.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.1 Research
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: New deep learning algorithms are invented, and existing ones are improved through
    research. Because peer-reviewed research requires reproducible results, model
    training data needs to be publicly accessible. Many public datasets, such as ImageNet
    for example, are available for research teams to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'The notebooking environment, such as JupyterLab, is a popular choice among
    researchers for prototyping model training due to its interactivity and flexibility.
    Let’s go through some sample steps that a researcher may take during model training
    prototyping:'
  prefs: []
  type: TYPE_NORMAL
- en: Alice, a deep learning researcher, is working on improving a visual recognition
    algorithm. After working on her theories, she is ready to start prototyping.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Alice creates a new notebook in JupyterLab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Alice wants to use the ImageNet dataset for training and benchmarking her algorithm.
    She may
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write code to download the dataset to her notebook and store it in the dataset
    management service (chapter 2) for reuse.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Find that the dataset is already stored in the dataset management service and
    write code to use it as is.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Alice starts coding up improvements on an existing visual recognition algorithm
    until it can produce experimental models locally within the notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Alice tries to change some hyperparameters, train and test a few experimental
    models, and compare the metrics that they generate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Alice may further use hyperparameter optimization techniques (chapter 5) to
    run more experiments automatically to confirm that she has indeed made improvements
    to the existing algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Alice publishes her results and packages her training code improvement as a
    library for others to use.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By using a versioned dataset for training, Alice is sure that the input training
    data of all her experimental model training runs are the same. She also uses a
    source-control management system, such as Git, to keep track of her code so that
    all experimental models can be traced back to a version of her code.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that model training at this stage usually takes place locally at the
    computing node where the notebooking environment is hosted, so it is a good idea
    to allocate sufficient resources to these nodes. If training data is stored over
    the network, make sure the read speed does not become a bottleneck for model training.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.2 Prototyping
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Prototyping is where research is bridged with real-world use cases. It is a
    practice of looking for the right combination of training data, algorithm, hyperparameter,
    and inference support to provide the right deep learning feature that meets product
    requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this stage, it is still very common to find notebooking environments to
    be the top choice for data scientists and engineers due to the rapid iterative
    nature of prototyping. A fast turnaround is expected. Let’s walk through one possible
    scenario of prototyping:'
  prefs: []
  type: TYPE_NORMAL
- en: The model development team receives product requirements to improve the motion
    detection of a security camera product.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Based on the requirements, the team finds that Alice’s new vision recognition
    training algorithm may help improve motion detection.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The team creates a new notebook and begins exploring data that is relevant
    for model training, given the set of algorithms that they picked:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The team may be able to use existing, collected data for model training if they
    happen to fit the problem that is being solved.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: In some cases, the team may need to collect new data for training.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: In most cases, transfer learning is applied at this stage, and the team picks
    one or more existing models as source models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The team develops modeling code with algorithms and trains experimental models
    with collected data and source models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Experimental models are evaluated to see whether they yield satisfactory results.
    Steps 3 to 6 are repeated until the training data shape and code become stable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We call steps 3 to 6 the exploratory loop. This loop corresponds to the iterate
    circles in the blow-up section of prototyping in figure 10.3\. When prototyping
    begins, the loop is iterated rapidly. The focus at this stage is to narrow down
    the training data shape and code.
  prefs: []
  type: TYPE_NORMAL
- en: Once training data shape and code become stable, they will be ready for further
    tuning and optimization. The goal in this phase is to converge to a state where
    model training and inference code are ready to be packaged and deployed to production.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.3 Key takeaways
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have walked through both the research and prototyping phases of our reference
    deep learning development cycle in figure 10.1\. Even though they serve different
    purposes, we see quite a bit of overlap in how they work with the deep learning
    system:'
  prefs: []
  type: TYPE_NORMAL
- en: The notebooking environment is a common choice for both research and preproduction
    prototyping due to its high degree of interactivity and verbosity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to training data should be as wide and flexible as possible (to the limit
    of legality and compliance), which helps accelerate the data exploration process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sufficient computing resources should be allocated for model training so that
    turnaround time is short.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At a minimum, use a dataset management service and a source-control management
    system to keep track of the provenance of experimental models. In addition, use
    a metadata store to contain metrics and associate them with a training dataset
    and code for complete lineage tracking.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 10.2 Model productionization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before deep learning models can be integrated into an end product, they need
    to go through the process of productionization. There are certainly many interpretations
    of this term, but fundamentally:'
  prefs: []
  type: TYPE_NORMAL
- en: Models need to serve production inference requests either from end products
    or end users.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model serving should meet a predefined service level agreement, such as responding
    within 50 milliseconds or being available 99.999% of the time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Production problems related to models should be easy to troubleshoot.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we will look at how deep learning models transition from living
    in a rather dynamic environment, such as a notebook, to a production environment
    where they are hit by various harsh conditions. Figure 10.4 shows the productionization
    phase relative to the rest of the development cycle. Let’s review the steps in
    this phase.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/10-04.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 Excerpt of the path to production in the productionization phase
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.1 Code componentization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As shown in the previous section, it is common during prototyping for training
    data preparation, model training, and inference code to exist in a single notebook.
    To productionize them into a deep learning system, we need to split them apart
    as separate components. One approach to splitting the components, or *code componentization*,
    is shown in figure 10.5.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/10-05.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 Componentizing code from a single notebook into multiple pieces
    that can be packaged separately. The first split happens where a trained model
    is the output. An optional second split happens where training data is the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s put the process in the figure into action. The first place to draw a
    line for separation in the code is where the model is the output. This should
    result in two pieces of code as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Model training code that outputs a model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model inference code that takes a model and an inference request as input to
    produce an inference
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Optionally, the model training code can be split as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Training data transformation code, which takes raw data as input and output
    training data that can be consumed by model training code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model training code, which takes training data and trains a model as its output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you have other model training code that will benefit from the same kind of
    prepared data, it is a good idea to perform this separation. Separation is also
    a good idea if your data preparation step needs to be executed on a different
    cadence from model training.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.2 Code packaging
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once code components are separated cleanly, they are ready to be packaged for
    deployment. To be able to run them on a training service (chapter 3), model service
    (chapter 6), and workflow service (chapter 9), we first need to make sure they
    follow the conventions set by these services.
  prefs: []
  type: TYPE_NORMAL
- en: Model training code should be modified to fetch training data from the location
    indicated by an environment variable set by the training service. A similar convention
    should be followed in other components.
  prefs: []
  type: TYPE_NORMAL
- en: 'Model inference code should follow the convention of the model serving strategy
    of your choice:'
  prefs: []
  type: TYPE_NORMAL
- en: If you use direct model embedding, work with the team that embeds your model
    to make sure your inference code works.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you plan to serve your model with model service, make sure your inference
    code provides an interface with which the model service can communicate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you use a model server, you may not need model inference code as long as
    the model server can serve the model properly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We package these code components as Docker containers so that they can be launched,
    accessed, and tracked by their respective host services. An example of how this
    is done can be found in appendix A. If special data transformation is needed,
    we can integrate data transformation code into the data management service.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.3 Code registration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before training code and inference code can be used by the system, their packages
    must be registered and stored with the metadata and artifacts service. This provides
    the necessary link between the training code and the inference code. Let’s look
    at how they are related (figure 10.6).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/10-06.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6 A simple training and inference execution flow in a production deep
    learning system
  prefs: []
  type: TYPE_NORMAL
- en: Once the training and inference codes are packaged as containers (the training
    container and inference container in the diagram), they can be registered with
    the metadata and artifacts store using a common handle, such as `visual_recognition`
    like in the example shown in figure 10.6\. This helps system services find and
    use correct code containers when they receive requests that provide the same handle
    name. We will continue walking through the figure in the next few sections.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.4 Training workflow setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We recommend setting up a training workflow even if you do not regularly train
    your model. The main reason is to provide reproducibility of the same model training
    flow in production. This is helpful when someone other than you needs to train
    a model and can use the flow that you set up. In some cases, the production environment
    is isolated, and going through a workflow that is set up in the production environment
    may be the only way to produce a model there. In figure 10.7, we’ve enlarged the
    model training portion of the previous diagram so you can see the details.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/10-07.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.7 A typical production model training setup. The workflow service
    manages what, when, and how training flows are run. The training service runs
    model training jobs. The metadata and artifacts store provides training code,
    stores trained models, and associates them with metadata.
  prefs: []
  type: TYPE_NORMAL
- en: Referring to figure 10.7, once a training workflow for `visual_recognition`
    is set up, training can be triggered to the training service. The training service
    uses the handle to look up the training code container to execute from the metadata
    and artifacts store. Once a model is trained, it saves the model to the metadata
    and artifacts store with the handle name.
  prefs: []
  type: TYPE_NORMAL
- en: At this stage, it is also common to find hyperparameter optimization techniques
    being used to find optimal training hyperparameters during model training. If
    an HPO service is used, the workflow will talk to the HPO service instead of to
    the training service directly. If you need a reminder of how the HPO service works,
    refer to chapter 5.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.5 Model inferences
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once a model is trained and registered in the production environment, the next
    step is to make sure it can handle inference requests coming into our system at
    a certain rate and producing inferences within a certain latency. We can do so
    by sending inference requests to the model service. When the model service receives
    a request, it finds the handle name `visual_recognition` in the request and queries
    the metadata and artifacts store for the matching model inference container and
    model file. The model service can then use these artifacts together to produce
    an inference response. You can see this process in figure 10.8, which, again,
    is an enlarged version of the model serving portion of figure 10.6.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/10-08.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.8 A typical production model serving setup. When inference requests
    arrive at the model service, the service looks for the inference container and
    model using the metadata and artifacts store to produce inference responses.
  prefs: []
  type: TYPE_NORMAL
- en: If you use a model server, you may need a thin layer in front of it so that
    it knows where to obtain the model file. Some model server implementations support
    custom model manager implementation, which can also be used to make queries against
    the metadata and artifacts store to load the correct model.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.6 Product integration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once you get a proper inference response back from the model service, it is
    time to integrate a model service client into the product that will use these
    inferences. This is the final step in productionization, and we should make sure
    to check a few things before launching it to the end customer. Because we are
    improving the motion detection of our security camera product, we must integrate
    a model service client in the security camera video-processing backend that will
    request inferences from the newly improved model:'
  prefs: []
  type: TYPE_NORMAL
- en: Make sure the inference response is consumable by the product using it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stress test inferencing by sending inference requests at a rate that approximates
    the production traffic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test inferencing with malformed inference requests to make sure they do not
    break the model inference code or the model service.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is a very basic list of items to look for. Your organization may define
    more production readiness criteria that you need to fulfill before launching the
    integration. Besides system metrics that can tell us whether our model is serving
    inference requests properly, we should also set up business metrics that will
    tell us whether the model helps the business use case.
  prefs: []
  type: TYPE_NORMAL
- en: 10.3 Model deployment strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous section, we went through a sample path from prototyping to
    production. That process assumed that the model was deployed for the first time,
    without an existing version of the model to replace. Once a model is being used
    in production, unless there is a maintenance window allowance, we usually need
    to use a model deployment strategy to ensure production inference request traffic
    is not disrupted. In fact, these model deployment strategies can also double as
    performing experimentation in production by using business metrics that we set
    up in the previous section. We will look at three strategies: canary, blue-green,
    and multi-armed bandit.'
  prefs: []
  type: TYPE_NORMAL
- en: 10.3.1 Canary deployment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A canary deployment, similar to A/B testing, means deploying a new model to
    serve a small portion of production inference requests while keeping the old one
    to serve the remaining majority of requests. An example is shown in figure 10.9\.
    This requires the model service to support segmenting and routing a small portion
    of inference request traffic to the new model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/10-09.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.9 Canary deployment showing the redirection of a small portion of
    traffic to a new version of the model
  prefs: []
  type: TYPE_NORMAL
- en: With this strategy, any potential adverse effect from deploying a new model
    is contained within a small portion of end users. Rolling back is rather straightforward
    by routing all inference request traffic back to the old model.
  prefs: []
  type: TYPE_NORMAL
- en: A drawback to this approach is that you only get to know the performance of
    the model to a small portion of end users. Releasing the new model to serve all
    inference request traffic may have a different effect from what you observe serving
    only a small portion of the traffic.
  prefs: []
  type: TYPE_NORMAL
- en: 10.3.2 Blue-green deployment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In our context, a blue-green deployment means deploying a new model, routing
    all inference request traffic to the new model, and keeping the old model online
    until we have confidence that the new model’s performance meets expectations.
    Implementation-wise it is the simplest among all three strategies because there
    is no traffic splitting at all. All the service needs to do is point to the new
    model internally to serve all inference requests. Blue-green deployment is depicted
    in figure 10.10.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/10-10.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.10 Blue–green deployment showing the direction of all traffic to either
    the old (blue) or the new (green) model
  prefs: []
  type: TYPE_NORMAL
- en: Not only is this strategy simple, but it also gives you a full picture of how
    the model performs when serving all end users. Rolling back is also straightforward.
    Simply point the model service back to the old model.
  prefs: []
  type: TYPE_NORMAL
- en: The obvious downside to this approach is if something goes wrong with the new
    model, it affects all end users. This strategy may make sense when you are developing
    a new product feature based on a new model. As you iterate on training a better
    model over time, you may want to move away from this strategy, as end users would
    have built their expectations on having a stable experience.
  prefs: []
  type: TYPE_NORMAL
- en: 10.3.3 Multi-armed bandit deployment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Multi-armed bandit (MAB) is the most complex deployment strategy among the three.
    MAB refers to a technique that continuously monitors multiple models’ performance
    and redirects more and more inference request traffic to the winning model over
    time. This uses the most elaborate implementation of the model service because
    it requires the service to understand model performance, which can be complicated
    depending on how your model performance metrics are defined. MAB deployment is
    illustrated in figure 10.11.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/10-11.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.11 Multi-armed bandit deployment showing traffic patterns on day 0
    and day 1\. Notice that model v2.0a is leading in terms of model performance on
    day 1 as it is receiving the most traffic.
  prefs: []
  type: TYPE_NORMAL
- en: This strategy does come with an advantage, though, because it maximizes the
    benefits of the best-performing models within a set time frame, whereas with a
    canary deployment, you may only gain minimal benefits if the new model outperforms
    the old one. Notice that you should make sure the model service reports how the
    traffic split changes over time. This helps correlate with the models’ performance.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning research teams invent and improve deep learning algorithms that
    are used to train models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model development teams make use of existing algorithms and available data to
    train models that help solve a deep learning use case.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both research and prototyping require a high degree of interactivity with code
    development, data exploration, and visualization. Notebooking environments are
    popular choices for these teams.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset management service can be used during research and prototyping to help
    track training data used to train experimental models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once training data and code are stable enough, the first step to productionization
    is to package model training code, model inference code, and any source models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These packages can be used by all services of the deep learning system to train,
    track, and serve models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once a model training workflow is up and running and a satisfactory inference
    response is obtained, integration with the end-user product can begin.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A model deployment strategy is required if serving inference requests cannot
    be interrupted.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiple model deployment strategies are available, and they can double as performing
    experimentation in production.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
