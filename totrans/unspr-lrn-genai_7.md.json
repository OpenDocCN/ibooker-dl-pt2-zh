["```py\n[[0\\. 0\\. 1\\. 0.] #it\n[0\\. 1\\. 0\\. 0.] #is\n[0\\. 0\\. 0\\. 1.] #raining\n[1\\. 0\\. 0\\. 0.]] #heavily\n```", "```py\nimport re\ndoc = \"It is     raining       outside\"\nnew_doc = re.sub(\"\\s+\",\" \", doc)\nprint(new_doc)\n```", "```py\ntext_d = \"Hey!!! How are you doing? And how is your health! Bye, take care.\"\nre.sub(\"[^-9A-Za-z ]\", \"\" , text_d)\n```", "```py\nimport string\ntext_d = \"Hey!!! How are you doing? And how is your health! Bye, take care.\"\ncleaned_text = \"\".join([i for i in text_d if i not in string.punctuation])\ncleaned_text\n```", "```py\ntext_d = \"Hey!!! How are you doing? And how is your health! Bye, take care.\"\ncleaned_text = \"\".join([i.lower() for i in text_d if i not in string.punctuation])\ncleaned_text\n```", "```py\nimport nltk\ntext_d = \"Hey!!! How are you doing? And how is your health! Bye, take care.\"\nnltk.tokenize.word_tokenize(text_d)\n```", "```py\nstopwords = nltk.corpus.stopwords.words('english')\ntext_d = \"Hey!!! How are you doing? And how is your health! Bye, take care.\"\ntext_new = \"\".join([i for i in text_d if i not in string.punctuation])\nprint(text_new)\nwords = nltk.tokenize.word_tokenize(text_new)\nprint(words)\nwords_new = [i for i in words if i not in stopwords]\nprint(words_new)\n```", "```py\nimport nltk\nfrom nltk.stem import PorterStemmer\nstem = PorterStemmer()\ntext = \"eats eating studies study\"\ntokenization = nltk.word_tokenize(text)\nfor word in tokenization:\n    print(\"Stem for {} is {}\".format(word, stem.stem(w)))\n```", "```py\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\ntext = \"eats eating studies study\"\ntokenization = nltk.word_tokenize(text)\nfor word in tokenization:\n    print(\"Lemma for {} is {}\".format(word, wordnet_lemmatizer.lemmatize(w)))\n```", "```py\n#### Loading all the required libraries here\nfrom lxml import html  \nimport requests\nimport pandas as pd\nfrom nltk.corpus import stopwords\nfrom textblob import TextBlob\nimport matplotlib.pyplot as plt\nimport sys\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport sklearn\nimport scikitplot as skplt\nimport nltk\n#to ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\n```", "```py\nxpath_reviews = '//div[@data-hook=\"review\"]'\nreviews = parser.xpath(xpath_reviews)\nxpath_rating  = './/i[@data-hook=\"review-star-rating\"]//text()' \nxpath_title   = './/a[@data-hook=\"review-title\"]//text()'\nxpath_author  = './/a[@data-hook=\"review-author\"]//text()'\nxpath_date    = './/span[@data-hook=\"review-date\"]//text()'\nxpath_body    = './/span[@data-hook=\"review-body\"]//text()'\nxpath_helpful = './/span[@data-hook=\"helpful-vote-statement\"]//text()'\n```", "```py\n# Create a dataframe here. \n\nreviews_df = pd.DataFrame()\nfor review in reviews:\n    rating  = review.xpath(xpath_rating)\n    title   = review.xpath(xpath_title)\n    author  = review.xpath(xpath_author)\n    date    = review.xpath(xpath_date)\n    body    = review.xpath(xpath_body)\n    helpful = review.xpath(xpath_helpful)\n\n    review_dict = {'rating': rating,\n                   'title': title,\n                   'author': author,             \n                   'date': date,\n                   'body': body,\n                   'helpful': helpful}\n    reviews_df = reviews_df.append(review_dict, ignore_index=True)\nall_reviews = pd.DataFrame()\n```", "```py\n# Fill the values of the reviews here. . \n\nfor i in range(1,90):\n    amazon_url = 'https://www.amazon.co.uk/Hive-Heating-Thermostat-Professional-Installation/product-reviews/B011B3J6KY/ref=cm_cr_othr_d_show_all?ie=UTF8&reviewerType=all_reviews&pageNumber='+str(i)\n    headers = {'User-Agent': user_agent}\n    page = requests.get(amazon_url, headers = headers)\n    parser = html.fromstring(page.content)\n    xpath_reviews = '//div[@data-hook=\"review\"]'\n    reviews = parser.xpath(xpath_reviews)\n    reviews_df = pd.DataFrame()\n    xpath_rating  = './/i[@data-hook=\"review-star-rating\"]//text()' \n    xpath_title   = './/a[@data-hook=\"review-title\"]//text()'\n    xpath_author  = './/a[@data-hook=\"review-author\"]//text()'\n    xpath_date    = './/span[@data-hook=\"review-date\"]//text()'\n    xpath_body    = './/span[@data-hook=\"review-body\"]//text()'\n    xpath_helpful = './/span[@data-hook=\"helpful-vote-statement\"]//text()'\n    #print(i)\n    for review in reviews:\n        rating  = review.xpath(xpath_rating)\n        title   = review.xpath(xpath_title)\n        author  = review.xpath(xpath_author)\n        date    = review.xpath(xpath_date)\n        body    = review.xpath(xpath_body)\n        helpful = review.xpath(xpath_helpful)\n\n        review_dict = {'rating': rating,\n                       'title': title,\n                       'author': author,             \n                       'date': date,\n                       'body': body,\n                       'helpful': helpful}\n        reviews_df = reviews_df.append(review_dict, ignore_index=True)\n    #print(reviews_df)\n    all_reviews = all_reviews.append(reviews_df)\n\n```", "```py\nall_reviews.head()\n```", "```py\nout_folder = '/Users/vaibhavverdhan/Book/UnsupervisedLearningBookFinal/'\nall_reviews.to_csv(out_folder + 'Reviews.csv')\n```", "```py\n#Load the data now and analyse it\ndata_path = '/Users/vaibhavverdhan/Book/UnsupervisedLearningBookFinal/'\nreviewDataCSV = 'Reviews.csv'\nreviewData = (pd.read_csv(data_path+reviewDataCSV,index_col=0,))\n```", "```py\nreviewData.shape\nreviewData.rating.unique()\nreviewData.rating.value_counts()\n```", "```py\nlabels = '5 Stars', '1 Star', '4 Stars', '3 Stars', '2 Stars'\nsizes = [reviewData.rating.value_counts()[0], reviewData.rating.value_counts()[1],reviewData.rating.value_counts()[2],reviewData.rating.value_counts()[3],reviewData.rating.value_counts()[4]]\ncolors = ['green', 'yellowgreen', 'coral', 'lightblue', 'grey']\nexplode = (0, 0, 0, 0, 0)  # explode 1st slice\n\n# Plot\nplt.pie(sizes, explode=explode, labels=labels, colors=colors,\n        autopct='%1.1f%%', shadow=True, startangle=140)\n\nplt.axis('equal')\nplt.show()\n```", "```py\nreviewData.body = reviewData.body.str.lower()\nreviewData.body = reviewData.body.str.replace('[^\\w\\s]','')\nstop = stopwords.words('english')\nreviewData.body = reviewData.body.apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\nfreq = list(freq.index)\nreviewData.body = reviewData.body.apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\nfreq = pd.Series(' '.join(reviewData.body).split()).value_counts()[-10:]\nfreq = list(freq.index)\nreviewData.body = reviewData.body.apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n```", "```py\nfrom nltk.tokenize import word_tokenize\ntokens = word_tokenize(reviewData.iloc[1,1])\nprint(tokens)\n```", "```py\nfrom textblob import Word\nreviewData.body = reviewData.body.apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\nreviewData.body.head()\n```", "```py\nsentimentString = reviewData.iloc[1,1]\n# append to this string \nfor i in range(2,len(reviewData)):\n    sentimentString = sentimentString + reviewData.iloc[i,1]\n```", "```py\n# the functions generates polarity and subjectivity here, subsetting the polarity only here\nallReviewsSentiment = reviewData.body[:900].apply(lambda x: TextBlob(x).sentiment[0])\n# this contains boths subjectivity and polarity\nallReviewsSentimentComplete = reviewData.body[:900].apply(lambda x: TextBlob(x).sentiment)\nallReviewsSentimentComplete.head()\n```", "```py\nallReviewsSentiment.to_csv(out_folder + 'ReviewsSentiment.csv')\n```", "```py\nallReviewsSentimentDF = allReviewsSentiment.to_frame()\n# Create a list to store the data\ngrades = []\n\n# For each row in the column,\nfor row in allReviewsSentimentDF['body']:\n    # if more than a value,\n    if row >= 0.75:\n       grades.append('Extremely Satisfied')\n    elif (row >= 0.5) & (row < 0.75):\n        grades.append('Satisfied')\n    elif (row >= 0.2) & (row < 0.5):\n        grades.append('Nice')\n    elif (row >= -0.2) & (row < 0.2):\n        grades.append('Neutral')\n    elif (row > -0.5) & (row <= -0.2):\n        grades.append('Bad')\n    elif (row >= -0.75) & (row < -0.5):\n        grades.append('Dis-satisfied')\n    elif  row < -0.75:\n        grades.append('Extremely Dis-satisfied')\n    else:\n        # Append a failing grade\n        grades.append('No Sentiment')\n\n# Create a column from the list\nallReviewsSentimentDF['SentimentScore'] = grades\nallReviewsSentimentDF.head()\n```", "```py\nallReviewsSentimentDF.SentimentScore.value_counts()\nallReviewsSentimentDF['SentimentScore'].value_counts().plot(kind='bar')\n#### Merge the review data with Sentiment generated\n\n# add column Polarity Score\nreviewData['polarityScore'] = allReviewsSentimentDF['body']\n```", "```py\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nimport numpy as np\nimport pandas as pd\n```", "```py\ntext = [\"It is a good place to travel\",\n            \"Football is a nice game\", \"Lets go for holidays and travel to Egypt\",\n            \"It is a goal, a great game.\", \"Enjoy your journey and fortget the rest\", \"The teams are ready for the same\" ]\n```", "```py\ntfidf_vectorizer = TfidfVectorizer(stop_words='english')\nX = tfidf_vectorizer.fit_transform(text)\n```", "```py\nk = 2\nmodel = KMeans(n_clusters=k, init='k-means++', max_iter=10, n_init=2)\nmodel.fit(X)\n```", "```py\ncentroids = model.cluster_centers_.argsort()[:, ::-1]\nfeatures = vectorizer.get_feature_names()\n\nfor i in range(k):\n    print(\"Cluster %d:\" % i),\n    for ind in centroids[i, :10]:\n        print(\"%s\" % terms[ind])\n```"]