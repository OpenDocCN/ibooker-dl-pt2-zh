["```py\n>>> from torch import nn\n\n>>> class RNN(nn.Module):\n...\n...     def __init__(self,\n...             vocab_size, hidden_size, output_size):  # #1\n...         super().__init__()\n...         self.W_c2h = nn.Linear(\n...             vocab_size + hidden_size, hidden_size)  # #2\n...         self.W_c2y = nn.Linear(vocab_size + hidden_size, output_size)\n...         self.softmax = nn.LogSoftmax(dim=1)\n...\n...     def forward(self, x, hidden):  # #3\n...         combined = torch.cat((x, hidden), axis=1)  # #4\n...         hidden = self.W_c2h(combined)  # #5\n...         y = self.W_c2y(combined)  # #6\n...         y = self.softmax(y)\n...         return y, hidden  # #7\n```", "```py\n>>> import pandas as pd\n>>> from nlpia2.spacy_language_model import nlp\n>>>\n>>> tagged_tokens = list(nlp('Hello world. Goodbye now!'))\n>>> interesting_tags = 'text dep_ head lang_ lemma_ pos_ sentiment'\n>>> interesting_tags = (interesting_tags +  'shape_ tag_').split()\n>>> pd.DataFrame([\n...         [getattr(t, a) for a in interesting_tags]\n...         for t in tagged_tokens],\n...     columns=interesting_tags)\n      text    dep_     head lang_   lemma_   pos_  sentiment shape_ tag_\n0    Hello    intj    world    en    hello   INTJ        0.0  Xxxxx   UH\n1    world    ROOT    world    en    world   NOUN        0.0   xxxx   NN\n2        .   punct    world    en        .  PUNCT        0.0      .    .\n3  Goodbye    ROOT  Goodbye    en  goodbye   INTJ        0.0  Xxxxx   UH\n4      now  advmod  Goodbye    en      now    ADV        0.0    xxx   RB\n5        !   punct  Goodbye    en        !  PUNCT        0.0      !    .\n```", "```py\n>>> from nlpia2.string_normalizers import Asciifier\n>>> asciify = Asciifier()\n\n>>> asciify(\"O’Néàl\")\n\"O'Neal\"\n>>> asciify(\"Çetin\")\n'Cetin'\n```", "```py\n>>> repo = 'tangibleai/nlpia2'  # #1\n>>> filepath = 'src/nlpia2/data/surname-nationality.csv.gz'\n>>> url = f\"https://gitlab.com/{repo}/-/raw/main/{filepath}\"\n>>> df = pd.read_csv(url)  # #2\n>>> df[['surname', 'nationality']].sort_values('surname').head(9)\n        surname nationality\n16760   Aalbers       Dutch\n16829   Aalders       Dutch\n35706  Aalsburg       Dutch\n35707     Aalst       Dutch\n11070     Aalto     Finnish\n11052  Aaltonen     Finnish\n10853     Aarab    Moroccan\n35708     Aarle       Dutch\n11410    Aarnio     Finnish\n```", "```py\n>>> df['nationality'].nunique()\n37\n>>> sorted(df['nationality'].unique())\n['Algerian', 'Arabic', 'Brazilian', 'Chilean', 'Chinese', 'Czech', 'Dutch',\n 'English', 'Ethiopian', 'Finnish', 'French', 'German', 'Greek',\n 'Honduran', 'Indian', 'Irish', 'Italian', 'Japanese', 'Korean',\n 'Malaysian', 'Mexican', 'Moroccan', 'Nepalese', 'Nicaraguan', 'Nigerian',\n 'Palestinian', 'Papua New Guinean', 'Peruvian', 'Polish', 'Portuguese',\n 'Russian', 'Scottish', 'South African', 'Spanish', 'Ukrainian',\n 'Venezuelan', 'Vietnamese']\n```", "```py\n>>> fraction_unique = {}\n>>> for i, g in df.groupby('nationality'):\n>>>     fraction_unique[i] = g['surname'].nunique() / len(g)\n>>> pd.Series(fraction_unique).sort_values().head(7)\nPortuguese           0.860092\nDutch                0.966115\nBrazilian            0.988012\nEthiopian            0.993958\nMexican              0.995000\nNepalese             0.995108\nChilean              0.998000\n```", "```py\n>>> arabic = [x.strip() for x in open('.nlpia2-data/names/Arabic.txt')]\n>>> arabic = pd.Series(sorted(arabic))\n0       Abadi\n1       Abadi\n2       Abadi\n        ...\n1995    Zogby\n1996    Zogby\n1997    Zogby\nLength: 2000, dtype: object\n```", "```py\n>>> df.groupby('surname')\n>>> overlap = {}\n... for i, g in df.groupby('surname'):\n...     n = g['nationality'].nunique()\n...     if n > 1:\n...         overlap[i] = {'nunique': n,\n 'unique': list(g['nationality'].unique())}\n>>> overlap.sort_values('nunique', ascending=False)\n         nunique                                             unique\nMichel         6  [Spanish, French, German, English, Polish, Dutch]\nAbel           5        [Spanish, French, German, English, Russian]\nSimon          5            [Irish, French, German, English, Dutch]\nMartin         5       [French, German, English, Scottish, Russian]\nAdam           5          [Irish, French, German, English, Russian]\n...          ...                                                ...\nBest           2                                  [German, English]\nKatz           2                                  [German, Russian]\nKarl           2                                    [German, Dutch]\nKappel         2                                    [German, Dutch]\nZambrano       2                                 [Spanish, Italian]\n```", "```py\n>>> class RNN(nn.Module):\n\n>>> def __init__(self, n_hidden=128, categories, char2i):  # #1\n...     super().__init__()\n...     self.categories = categories\n...     self.n_categories = len(self.categories)  # #2\n...     print(f'RNN.categories: {self.categories}')\n...     print(f'RNN.n_categories: {self.n_categories}')\n\n...     self.char2i = dict(char2i)\n...     self.vocab_size = len(self.char2i)\n\n...     self.n_hidden = n_hidden\n\n...     self.W_c2h = nn.Linear(self.vocab_size + self.n_hidden,\n self.n_hidden)\n...     self.W_c2y = nn.Linear(self.vocab_size + self.n_hidden,\n self.n_categories)\n...     self.softmax = nn.LogSoftmax(dim=1)\n\n>>> def forward(self, x, hidden):  # #3\n...     combined = torch.cat((x, hidden), 1)\n...     hidden = self.W_c2h(combined)\n...     y = self.W_c2y(combined)\n...     y = self.softmax(y)\n...     return y, hidden  # #4\n```", "```py\n>>> def train_sample(model, category_tensor, char_seq_tens,\n...                 criterion=nn.NLLLoss(), lr=.005):\n    \"\"\" Train for one epoch (one example name nationality tensor pair) \"\"\"\n...    hidden = torch.zeros(1, model.n_hidden)  # #1\n...    model.zero_grad()  # #2\n...    for char_onehot_vector in char_seq_tens:\n...        category_predictions, hidden = model(  # #3\n...            x=char_onehot_vector, hidden=hidden)  # #4\n...    loss = criterion(category_predictions, category_tensor)\n...    loss.backward()\n\n...    for p in model.parameters():\n...        p.data.add_(p.grad.data, alpha=-lr)\n\n...    return model, category_predictions, loss.item()\n```", "```py\n>>> %run classify_name_nationality.py  # #1\n    surname  nationality\n0   Tesfaye    Ethiopian\n...\n[36241 rows x 7 columns]\n```", "```py\nHow many nationalities would you like to train on? [10]? 25\nmodel: RNN(\n    n_hidden=128,\n    n_categories=25,\n    categories=[Algerian..Nigerian],\n    vocab_size=58,\n    char2i['A']=6\n)\n\nHow many samples would you like to train on? [10000]? 1500\n\nWhat learning rate would you like to train with? [0.005]? 0.010\n\n  2%|▊        | 30/1500 [00:06<05:16,  4.64it/s]000030 2% 00:06 3.0791\n   Haddad => Arabic (1) ✓\n000030 2% 00:06 3.1712 Cai => Moroccan (21) ✗ should be Nepalese (22=22)\n```", "```py\n001470 98% 06:31 1.7358 Maouche => Algerian (0) ✓\n001470 98% 06:31 1.8221 Quevedo => Mexican (20) ✓\n...\n001470 98% 06:31 0.7960 Tong => Chinese (4) ✓\n001470 98% 06:31 1.2560 Nassiri => Moroccan (21) ✓\n  mean_train_loss: 2.1883266236980754\n  mean_train_acc: 0.5706666666666667\n  mean_val_acc: 0.2934249263984298\n100%|███████████| 1500/1500 [06:39<00:00,  3.75it/s]\n```", "```py\n>>> model.predict_category(\"Khalid\")\n'Algerian'\n>>> predictions = topk_predictions(model, 'Khalid', topk=4)\n>>> predictions\n        text  log_loss nationality\nrank\n0     Khalid     -1.17    Algerian\n1     Khalid     -1.35    Moroccan\n2     Khalid     -1.80   Malaysian\n3     Khalid     -2.40      Arabic\n```", "```py\n>>> predictions = topk_predictions(model, 'Khalid', topk=4)\n>>> predictions['likelihood'] = np.exp(predictions['log_loss'])\n>>> predictions\n        text  log_loss nationality  likelihood\nrank\n0     Khalid     -1.17    Algerian        0.31\n1     Khalid     -1.35    Moroccan        0.26\n2     Khalid     -1.80   Malaysian        0.17\n3     Khalid     -2.40      Arabic        0.09\n```", "```py\n>>> def predict_hidden(self, text=\"Khalid\"):\n...    text_tensor = self.encode_one_hot_seq(text)\n...    with torch.no_grad():  # #1\n...    hidden = self.hidden_init\n...        for i in range(text_tensor.shape[0]):  # #2\n...            y, hidden = self(text_tensor[i], hidden)  # #3\n...    return hidden\n```", "```py\n>>> def predict_proba(self, text=\"Khalid\"):\n...    text_tensor = self.encode_one_hot_seq(text)\n...    with torch.no_grad():\n...        hidden = self.hidden_init\n...        for i in range(text_tensor.shape[0]):\n...            y, hidden = self(text_tensor[i], hidden)\n...    return y  # #1\n```", "```py\n>>> def predict_category(self, text):\n...    tensor = self.encode_one_hot_seq(text)\n...    y = self.predict_proba(tensor)  # #1\n...    pred_i = y.topk(1)[1][0].item()  # #2\n...    return self.categories[pred_i]\n```", "```py\n>>> text = 'Khalid'\n>>> pred_categories = []\n>>> pred_hiddens = []\n\n>>> for i in range(1, len(text) + 1):\n...    pred_hiddens.append(model.predict_hidden(text[:i]))  # #1\n...    pred_categories.append(model.predict_category(text[:i]))\n\n>>> pd.Series(pred_categories, input_texts)\n# K English\n# Kh Chinese\n# Kha Chinese\n# Khal Chinese\n# Khali Algerian\n# Khalid Arabic\n```", "```py\n>>> hiddens = [h[0].tolist() for h in hiddens]\n>>> df_hidden = pd.DataFrame(hidden_lists, index=list(text))\n>>> df_hidden = df_hidden.T.round(2)  # #1\n\n>>> df_hidden\n    0     1     2     3     4     5   ...  122   123   124   125   126   127\nK  0.10 -0.06 -0.06  0.21  0.07  0.04 ... 0.16  0.12  0.03  0.06 -0.11  0.11\nh -0.03  0.03  0.02  0.38  0.29  0.27 ...-0.08  0.04  0.12  0.30 -0.11  0.37\na -0.06  0.14  0.15  0.60  0.02  0.16 ...-0.37  0.22  0.30  0.33  0.26  0.63\nl -0.04  0.18  0.14  0.24 -0.18  0.02 ... 0.27 -0.04  0.08 -0.02  0.46  0.00\ni -0.11  0.12 -0.00  0.23  0.03 -0.19 ...-0.04  0.29 -0.17  0.08  0.14  0.24\nd  0.01  0.01 -0.28 -0.32  0.10 -0.18 ... 0.09  0.14 -0.47 -0.02  0.26 -0.11\n[6 rows x 128 columns]\n```", "```py\n>>> position = pd.Series(range(len(text)), index=df_hidden.index)\n>>> pd.DataFrame(position).T\n# K h a l i d\n# 0 0 1 2 3 4 5\n\n>>> df_hidden_raw.corrwith(position).sort_values()\n# 11 -0.99\n# 84 -0.98\n# 21 -0.97\n# ...\n# 6 0.94\n# 70 0.96\n# 18 0.96\n```", "```py\n>>> lines = open('data/wikitext-2/train.txt').readlines()\n>>> for line in lines[:4]:\n...     print(line.rstrip()[:70])\n\n = Valkyria Chronicles III =\n =======\n\n Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 ,\n  lit\n```", "```py\n>>> from nlpia2.ch08.data import Corpus\n\n>>> corpus = Corpus('data/wikitext-2')\n>>> corpus.train\ntensor([ 4,  0,  1,  ..., 15,  4,  4])\n```", "```py\n>>> vocab = corpus.dictionary\n>>> [vocab.idx2word[i] for i in corpus.train[:7]]\n['<eos>', '=', 'Valkyria', 'Chronicles', 'III', '=', '<eos>']\n```", "```py\n>>> def batchify_slow(x, batch_size=8, num_batches=5):\n...    batches = []\n...    for i in range(int(len(x)/batch_size)):\n...        if i > num_batches:\n...            break\n...        batches.append(x[i*batch_size:i*batch_size + batch_size])\n...    return batches\n>>> batches = batchify_slow(corpus.train)\n```", "```py\n>>> batches\n[tensor([4, 0, 1, 2, 3, 0, 4, 4]),\n tensor([ 5,  6,  1,  7,  8,  9,  2, 10]),\n tensor([11,  8, 12, 13, 14, 15,  1, 16]),\n tensor([17, 18,  7, 19, 13, 20, 21, 22]),\n tensor([23,  1,  2,  3, 24, 25, 13, 26]),\n tensor([27, 28, 29, 30, 31, 32, 33, 34])]\n```", "```py\n>>> torch.stack(batches)\ntensor([[4, 0, 1, 2, 3, 0, 4, 4],\n        [ 5,  6,  1,  7,  8,  9,  2, 10],\n        [11,  8, 12, 13, 14, 15,  1, 16],\n        ...\n```", "```py\n>>> r = sigmoid(W_i2r.mm(x) + b_i2r +    W_h2r.mm(h) + b_h2r)  # #1\n>>> z = sigmoid(W_i2z.mm(x) + b_i2z +    W_h2z.mm(h) + b_h2z)  # #2\n\n>>> n =    tanh(W_i2n.mm(x) + b_i2n + r∗(W_h2n.mm(h) + b_h2n))  # #3\n```", "```py\nr = sigmoid(W_i2r.dot(x) + b_i2r + W_h2r.dot(h) + b_h2r)  # #1\nz = sigmoid(W_i2z.dot(x) + b_i2z + W_h2z.dot(h) + b_h2z)  # #2\n```", "```py\n>>> def count_parameters(model, learned=True):\n...     return sum(\n...         p.numel() for p in model.parameters()  # #1\n...         if not learned or p.requires_grad  # #2\n...     )\n```", "```py\n>>> import jsonlines  # #1\n\n>>> with jsonlines.open('experiments.jsonl') as fin:\n...     lines = list(fin)\n>>> df = pd.DataFrame(lines)\n>>> df.to_csv('experiments.csv')\n>>> cols = 'learned_parameters rnn_type epochs lr num_layers'\n>>> cols += ' dropout epoch_time test_loss'\n>>> cols = cols.split()\n>>> df[cols].round(2).sort_values('test_loss', ascending=False)\n```", "```py\n>>> df\n     parameters  rnn_type  epochs   lr  layers  drop  time (s)  loss\n3      13746478  RNN_TANH       1  0.5       5   0.0     55.46  6.90\n155    14550478       GRU       1  0.5       5   0.2     72.42  6.89\n147    14550478       GRU       1  0.5       5   0.0     58.94  6.89\n146    14068078       GRU       1  0.5       3   0.0     39.83  6.88\n1      13505278  RNN_TANH       1  0.5       2   0.0     32.11  6.84\n..          ...       ...     ...  ...     ...   ...       ...   ...\n133    13505278  RNN_RELU      32  2.0       2   0.2   1138.91  5.02\n134    13585678  RNN_RELU      32  2.0       3   0.2   1475.43  4.99\n198    14068078       GRU      32  2.0       3   0.0   1223.56  4.94\n196    13585678       GRU      32  2.0       1   0.0    754.08  4.91\n197    13826878       GRU      32  2.0       2   0.0    875.17  4.90\n```", "```py\nr = sigmoid(W_i2r.mm(x) + b_i2r +    W_h2r.mm(h) + b_h2r)\nz = sigmoid(W_i2z.mm(x) + b_i2z +    W_h2z.mm(h) + b_h2z)\nn =    tanh(W_i2n.mm(x) + b_i2n + r∗(W_h2n.mm(h) + b_h2n))\n\nf = sigmoid(W_i2f.mm(x) + b_i2f + W_h2f.mm(h) + b_h2f)  # #1\ni = sigmoid(W_i2i.mm(x) + b_i2i + W_h2i.mm(h) + b_h2i)  # #2\ng = tanh(W_i2g.mm(x) + b_i2g + W_h2y.mm(h) + b_h2g)  # #3\no = sigmoid(W_i2o.mm(x) + b_i2o + W_h2o.mm(h) + b_h2o)  # #4\nc = f*c + i*g  # #5\nh = o*tanh(c)\n```", "```py\nimport pandas as pd\nimport jsonlines\n\nwith jsonlines.open('experiments.jsonl') as fin:\n    lines = list(fin)\ndf = pd.DataFrame(lines)\ndf.to_csv('experiments.csv')\ncols = 'rnn_type epochs lr num_layers dropout epoch_time test_loss'\ncols = cols.split()\ndf[cols].round(2).sort_values('test_loss').head(10)\n```", "```py\nepochs   lr  num_layers  dropout  epoch_time  test_loss\n37      12  2.0           2      0.2       35.43       5.23\n28      12  2.0           1      0.0       22.66       5.23\n49      32  0.5           2      0.0       32.35       5.22\n57      32  0.5           2      0.2       35.50       5.22\n38      12  2.0           3      0.2       46.14       5.21\n50      32  0.5           3      0.0       37.36       5.20\n52      32  2.0           1      0.0       22.90       5.10\n55      32  2.0           5      0.0       56.23       5.09\n53      32  2.0           2      0.0       32.49       5.06\n54      32  2.0           3      0.0       38.78       5.04\n```", "```py\n>>> from nlpia2.ch08.rnn_word.data import Corpus\n>>> corpus = Corpus('data/wikitext-2')\n>>> passage = corpus.train.numpy()[-89:-35]\n```", "```py\n>>> '  '.join([vocab.idx2word[i] for i in passage])\nTheir ability at mimicry is so great that strangers have looked in vain\nfor the human they think they have just heard speak . <eos>\nCommon starlings are trapped for food in some Mediterranean countries .\nThe meat is tough and of low quality , so it is <unk> or made into <unk> .\n```", "```py\n>>> num_eos = sum([vocab.idx2word[i] == '<eos>' for i in\n corpus.train.numpy()])\n>>> num_eos\n36718\n>>> num_unk = sum([vocab.idx2word[i] == '<unk>' for i in\n corpus.train.numpy()])\n>>> num_unk\n54625\n>>> num_normal = sum([\n...     vocab.idx2word[i] not in ('<unk>', '<eos>')\n...     for i in corpus.train.numpy()])\n>>> num_normal\n1997285\n>>> num_unk / (num_normal + num_eos + num_unk)\n0.0261...\n```", "```py\n>>> import torch\n>>> from preprocessing import Corpus\n>>> from generate import generate_words\n>>> from model import RNNModel\n\n>>> corpus = Corpus('data/wikitext-2')\n>>> vocab = corpus.dictionary\n>>> with open('model.pt', 'rb') as f:\n...    orig_model = torch.load(f, map_location='cpu')  # #1\n\n>>> model = RNNModel('GRU', vocab=corpus.dictionary, num_layers=1)  # #2\n>>> model.load_state_dict(orig_model.state_dict())\n>>> words = generate_words(\n...    model=model, vocab=vocab, prompt='The', temperature=.1)  # #3\n```", "```py\n>>> print('  '.join(w for w in words))\n...\n= =  Valkyria Valkyria Valkyria Valkyria = = The kakapo is a common\n starling , and the of the of the ,\n...\n```"]