["```py\nfrom sklearn.ensemble import RandomForestClassifier          ❶\n\nclf = RandomForestClassifier(n_jobs=1, random_state=0)       ❷\n\nstart_time = time.time()                                     ❸\nclf.fit(train_x, train_y)\nend_time = time.time()\nprint(\"Training the Random Forest Classifier took %3d seconds\"%(end_time-start_time))\n\npredicted_labels = clf.predict(test_x)\n\nacc_score = accuracy_score(test_y, predicted_labels)\n\nprint(\"The RF testing accuracy score is::\")\nprint(acc_score)\n```", "```py\nfrom sklearn.ensemble import GradientBoostingClassifier        ❶\nfrom sklearn import metrics                                   ❷\nfrom sklearn.model_selection import cross_val_score\n\ndef modelfit(alg, train_x, train_y, predictors, test_x, performCV=True, cv_folds=5):\n    alg.fit(train_x, train_y)                                 ❸\n    predictions = alg.predict(train_x)                        ❹\n    predprob = alg.predict_proba(train_x)[:,1]\n    if performCV:                                             ❺\n        cv_score = cross_val_score(alg, train_x, train_y, cv=cv_folds, scoring='roc_auc')\n\n    print(\"\\nModel Report\")                                  ❻\n    print(\"Accuracy : %.4g\" % metrics.accuracy_score(train_y,predictions))\n    print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(train_y, predprob))\n    if performCV:\n        print(\"CV Score : Mean - %.7g | Std - %.7g | Min - %.7g | Max - %.7g\" % \n(np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score)))\n\n    return alg.predict(test_x),alg.predict_proba(test_x)      ❼\n```", "```py\ngbm0 = GradientBoostingClassifier(random_state=10)\nstart_time = time.time()\ntest_predictions, test_probs = modelfit(gbm0, train_x, train_y, predictors, test_x)\nend_time = time.time()\nprint(\"Training the Gradient Boosting Classifier took %3d seconds\"%(end_time-start_time))\n\npredicted_labels = test_predictions\nacc_score = accuracy_score(test_y, predicted_labels)\nprint(\"The Gradient Boosting testing accuracy score is::\")\nprint(acc_score)\n```", "```py\nModel Report\nAccuracy : 0.9814\nAUC Score (Train): 0.997601\nCV Score : Mean - 0.9854882 | Std - 0.006275645 | Min - 0.9770558 | Max - 0.9922158\nTraining the Gradient Boosting Classifier took 159 seconds\nThe Gradient Boosting testing accuracy score is::\n\n```", "```py\nModel Report\nAccuracy : 0.8943\nAUC Score (Train): 0.961556\nCV Score : Mean - 0.707521 | Std - 0.03483452 | Min - 0.6635249 | Max - 0.7681968\nTraining the Gradient Boosting Classifier took 596 seconds\nThe Gradient Boosting testing accuracy score is::\n0.665   \n```", "```py\ntest_probs_max = []                                                        ❶\nfor i in range(test_probs.shape[0]):\n    test_probs_max.append(test_probs[i,test_y[i]])\n\nfpr, tpr, thresholds = metrics.roc_curve(test_y, np.array(test_probs_max))  ❷\n\nimport matplotlib.pyplot as plt                                            ❸\nfig,ax = plt.subplots()\nplt.plot(fpr,tpr,label='ROC curve')\nplt.plot([0, 1], [0, 1], color='navy', linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic for Email Example')\nplt.legend(loc=\"lower right\")\nplt.show()\n```", "```py\nfrom sklearn.ensemble import GradientBoostingClassifier               ❶\nfrom sklearn import metrics                                          ❷\nfrom sklearn.model_selection import cross_val_score\n\ndef modelfit(alg, train_x, train_y, predictors, test_x, performCV=True, cv_folds=5):\n    alg.fit(train_x, train_y)                                        ❸\n    predictions = alg.predict(train_x)                               ❹\n    predprob = alg.predict_proba(train_x)[:,1]\n    if performCV:                                                     ❺\n        cv_score = cross_val_score(alg, train_x, train_y, cv=cv_folds, scoring='roc_auc')\n\n    print(\"\\nModel Report\")                                          ❻\n    print(\"Accuracy : %.4g\" % metrics.accuracy_score(train_y,predictions))\n    print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(train_y, predprob))\n    if performCV:\n        print(\"CV Score : Mean - %.7g | Std - %.7g | Min - %.7g | Max - %.7g\" % \n(np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score)))\n\n    feat_imp = pd.Series(alg.feature_importances_, predictors).sort_values(ascending=False) \n    feat_imp[:10].plot(kind='bar',title='Feature Importances')        ❼\n\n    return alg.predict(test_x),alg.predict_proba(test_x)              ❽\n```", "```py\nimport tensorflow as tf                                                 ❶\nimport tensorflow_hub as hub\nfrom keras import backend as K\nimport keras.layers as layers\nfrom keras.models import Model, load_model\nfrom keras.engine import Layer\nimport numpy as np\n\nsess = tf.Session()                                                     ❷\nK.set_session(sess)\n\nclass ElmoEmbeddingLayer(Layer):                                        ❸\n    def __init__(self, **kwargs):\n        self.dimensions = 1024\n        self.trainable=True\n        super(ElmoEmbeddingLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.elmo =hub.Module('https:/ /tfhub.dev/google/elmo/2', trainable=self.trainable,\n                               name=\"{}_module\".format(self.name))      ❹\n\n        self.trainable_weights += \n                         K.tf.trainable_variables(scope=\"^{}_module/.*\".format(self.name))  ❺\n        super(ElmoEmbeddingLayer, self).build(input_shape)\n\n    def call(self, x, mask=None):\n        result = self.elmo(K.squeeze(K.cast(x, tf.string), axis=1),\n                      as_dict=True,\n                      signature='default',\n                      )['default']\n        return result\n\n    def compute_output_shape(self, input_shape):                        ❻\n        return (input_shape[0], self.dimensions)\n```", "```py\ndef convert_data(raw_data,header):                              ❶\n    converted_data, labels = [], []\n    for i in range(raw_data.shape[0]):\n        out = ' '.join(raw_data[i])                             ❷\n        converted_data.append(out)\n        labels.append(header[i])\n    converted_data = np.array(converted_data, dtype=object)[:, np.newaxis]\n\n    return converted_data, np.array(labels)\n\nraw_data, header = unison_shuffle(raw_data, header)             ❸\n\nidx = int(0.7*data_train.shape[0])\ntrain_x, train_y = convert_data(raw_data[:idx],header[:idx])    ❹\ntest_x, test_y = convert_data(raw_data[idx:],header[idx:])      ❺\n```", "```py\ndef build_model(): \n  input_text = layers.Input(shape=(1,), dtype=\"string\")\n  embedding = ElmoEmbeddingLayer()(input_text)\n  dense = layers.Dense(256, activation='relu')(embedding)    ❶\n  pred = layers.Dense(1, activation='sigmoid')(dense)        ❷\n\n  model = Model(inputs=[input_text], outputs=pred)\n\n  model.compile(loss='binary_crossentropy', optimizer='adam',\n                                 metrics=['accuracy'])       ❸\n  model.summary()                                            ❹\n\n  return model\n\n# Build and fit\nmodel = build_model()\nmodel.fit(train_x,                                           ❺\n          train_y,\n          validation_data=(test_x, test_y),\n          epochs=5,\n          batch_size=32)\n```", "```py\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         (None, 1)                 0         \n_________________________________________________________________\nelmo_embedding_layer_2 (Elmo (None, 1024)              4         \n_________________________________________________________________\ndense_3 (Dense)              (None, 256)               262400    \n_________________________________________________________________\ndense_4 (Dense)              (None, 2)                 514       \n=================================================================\nTotal params: 262,918\nTrainable params: 262,918\nNon-trainable params: 0\n```", "```py\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom bert.tokenization import FullTokenizer\nfrom tensorflow.keras import backend as K\n\n# Initialize session\nsess = tf.Session()\n\nclass BertLayer(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        n_fine_tune_layers=10,                                             ❶\n        pooling=\"mean\",                                                    ❷\n        bert_path=\"https:/ /tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\",❸\n        **kwargs,\n    ):\n        self.n_fine_tune_layers = n_fine_tune_layers\n        self.trainable = True\n        self.output_size = 768                                             ❹\n        self.pooling = pooling\n        self.bert_path = bert_path\n\n        super(BertLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.bert = hub.Module(\n            self.bert_path, trainable=self.trainable, name=f\"{self.name}_module\"\n        )\n\n        trainable_vars = self.bert.variables                               ❺\n        if self.pooling == \"first\":\n            trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n            trainable_layers = [\"pooler/dense\"]\n\n        elif self.pooling == \"mean\":\n            trainable_vars = [\n                var\n                for var in trainable_vars\n                if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\n            ]\n            trainable_layers = []\n        else:\n            raise NameError(\"Undefined pooling type\")\n\n        for i in range(self.n_fine_tune_layers):                           ❻\n            trainable_layers.append(f\"encoder/layer_{str(11 - i)}\")\n\n        trainable_vars = [\n            var\n            for var in trainable_vars\n            if any([l in var.name for l in trainable_layers])\n        ]\n\n        for var in trainable_vars:                                         ❼\n            self._trainable_weights.append(var)\n\n        for var in self.bert.variables:\n            if var not in self._trainable_weights:\n                self._non_trainable_weights.append(var)\n\n        super(BertLayer, self).build(input_shape)\n\n    def call(self, inputs):\n        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n        input_ids, input_mask, segment_ids = inputs\n        bert_inputs = dict(\n            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids                                               ❽\n        )\n        if self.pooling == \"first\":\n            pooled = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n                \"pooled_output\"\n            ]\n        elif self.pooling == \"mean\":\n            result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n                \"sequence_output\"\n            ]\n\n            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)         ❾\n            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n            input_mask = tf.cast(input_mask, tf.float32)\n            pooled = masked_reduce_mean(result, input_mask)\n        else:\n            raise NameError(\"Undefined pooling type\")\n\n        return pooled\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], self.output_size)\n```", "```py\ndef build_model(max_seq_length):                                           ❶\n    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n    bert_inputs = [in_id, in_mask, in_segment]\n    bert_output = BertLayer(n_fine_tune_layers=0)(bert_inputs)             ❷\n    dense = tf.keras.layers.Dense(256, activation=\"relu\")(bert_output)\n    pred = tf.keras.layers.Dense(1, activation=\"sigmoid\")(dense)\n\n    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n    model.summary()\n\n    return model\n\ndef initialize_vars(sess):                                                 ❸\n    sess.run(tf.local_variables_initializer())\n    sess.run(tf.global_variables_initializer())\n    sess.run(tf.tables_initializer())\n    K.set_session(sess)\n\nbert_path = \"https:/ /tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\ntokenizer = create_tokenizer_from_hub_module(bert_path)                    ❹\n\ntrain_examples = convert_text_to_examples(train_x, train_y)                ❺\ntest_examples = convert_text_to_examples(test_x, test_y)\n\n# Convert to features\n(train_input_ids,train_input_masks,train_segment_ids,train_labels) =       ❻\n     convert_examples_to_features(tokenizer, train_examples,               ❻\n     max_seq_length=maxtokens)                                             ❻\n(test_input_ids,test_input_masks,test_segment_ids,test_labels) = \n     convert_examples_to_features(tokenizer, test_examples,  \n     max_seq_length=maxtokens)\n\nmodel = build_model(maxtokens)                                             ❼\n\ninitialize_vars(sess)                                                      ❽\n\nhistory = model.fit([train_input_ids, train_input_masks, train_segment_ids],❾\ntrain_labels,validation_data=([test_input_ids, test_input_masks, \ntest_segment_ids],test_labels), epochs=5, batch_size=32)\n```", "```py\nfrom sklearn.model_selection import GridSearchCV       ❶\nprint(\"Available hyper-parameters for systematic tuning available with RF:\") \nprint(clf.get_params())                                ❷\n```", "```py\n{'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, 'n_jobs': 1, 'oob_score': False, 'random_state': 0, 'verbose': 0, 'warm_start': False}\n```", "```py\nparam_grid = {\n    'min_samples_leaf': [1, 2, 3],\n    'min_samples_split': [2, 6, 10],\n    'n_estimators': [10, 100, 1000]\n\n```", "```py\ngrid_search = GridSearchCV(estimator = clf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)     ❶\n\ngrid_search.fit(train_x, train_y)                               ❷\n\nprint(\"Best parameters found:\")                                 ❸\nprint(grid_search.best_params_)\n\nprint(\"Estimated accuracy is:\")\nacc_score = accuracy_score(test_y, grid_search.best_estimator_.predict(test_x))\nprint(acc_score)\n```", "```py\nBest parameters found:\n{'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 1000}\nEstimated accuracy is:\n\n```"]