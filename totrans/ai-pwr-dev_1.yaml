- en: 1 Understanding Large Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Introducing Generative AI (Specifically Large Language Models)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: History of Generative AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the benefits of Generative AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determining when and when not to use Generative AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether you realized it or not, and whether you want to admit it or not, you
    have quietly received a promotion. In fact, every professional software engineer
    has. Almost overnight, we have gone from staff engineers to engineering managers.
    You now have the world’s smartest and most talented junior developer on your team.
    Guiding, mentoring, and performing code reviews should become part of your daily
    routine. You now have Generative AI as your new coding partner. This chapter will
    provide you with an overview a subset of Generative AIs called Large Language
    Models (LLM), specifically Chat GPT, GitHub Copilot, and AWS CodeWhisperer.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'This book will not be a traditional programming book. You will not be able
    to use it like you would a script. You are going to engage with a dialogue with
    Large Language Models and like any conversation the words and direction will change,
    depending on the model, as well as the context that came before. The output that
    you receive will very likely differ from what is printed in this book. This should
    not discourage you. Instead, you should explore. The journey is as rewarding as
    the destination. You might find yourself frustrated that they could not follow
    along. Have patience. If you are disciplined (and somewhat adventurous), you can
    get GPT cooperate with the general themes and aim of this book: learning how to
    use generative AI to make you a better programmer.'
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 An introduction to Large Language Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Generative Ais, specifically Large Language Models (LLMs) are radically transforming
    how we think about and develop software. Rather than designing and coding out
    modules, components, and tests, we will describe the software that we want these
    Ais to build, and they will generate the body of this work for us. This is a natural
    trend in the fields of software engineering: our compliers have gotten smarter
    (Rust’s compiler being a prime example, which eliminates an entire category of
    bugs), as has our tooling (IntelliSense in source code), and our programming languages
    have become more expressive and more productive. While this might make these Generative
    Ais seem more evolutionary, than a revolutionary; they are, in a sense, both.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This book will examine, compare, and contrast three such Large Language Models:
    GitHub’s Copilot, OpenAI’s ChatGPT, and Amazon’s CodeWhisperer. The latter will
    receive the least coverage as it is largely analogous with Copilot but is more
    useful and focused on development within the AWS eco-system.'
  prefs: []
  type: TYPE_NORMAL
- en: 'GitHub Copilot and ChatGPT both use the GPT-4 Codex model created by OpenAI
    behind the scenes. Microsoft licensed the software from OpenAI, using the source
    code within the public repositories within GitHub (which Microsoft owns). GitHub
    built a service that will take the context provided by your Integrated Development
    Environment (IDE), such as Visual Studio Code or IntelliJ, and send that context
    to the GitHub Copilot service. This Service will use the OpenAI Codex to generate
    up to ten possible solutions, given the context that you have provided via the
    comments and code in your file. Codex attempts to match this context against examples
    that it finds in the corpus of its training data. These code solutions will be
    returned to your Integrated Development Environment for you to select from. You
    review all of the code suggestions and accept the one that is closest to your
    intent. The supervision that you provide here is very important: it is not uncommon
    for the solutions to be out of date or inexact. Should you accept one the solutions,
    then your “accepted” solution is then sent back to the GitHub Copilot Service
    to enhance the model further and thus the suggestions.'
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft is betting big on this technology with Copilot. As is Google with
    Bard. And it is easy to see why.
  prefs: []
  type: TYPE_NORMAL
- en: This book teaches you how to manage Generative Ais such as GitHub Copilot. Trivial
    examples will give way to incredibly complex ones that will leave you stunned.
    This book assumes you have little knowledge of *using* Generative AIs. You have
    likely heard about Generative AIs for some time. You have probably thought the
    concept is exciting and worth looking into Generative AI at some point. Well,
    there is no time like the present. This book will take you through the basics,
    from setting it up in an Integrated Development Environment to using it to 10x
    your productivity, output, and hopefully enjoyment of coding.
  prefs: []
  type: TYPE_NORMAL
- en: Over the course of this book, you will see example after example of how one
    Generative AI is better suited for a given task. This will help you build intuition
    around when you would want to use one or the other, as well as when you might
    want to avoid them all. Let us take a brief survey of each of the Generative AI’s
    core strengths.
  prefs: []
  type: TYPE_NORMAL
- en: 'ChatGPT excels at generating responses that mimic human speech and written
    language. It is therefore very good at documentation and commenting in code. Because
    of its ability to process Nature Languages (NLP), it can also perform the reverse:
    summarize text and capture the sentiment. You can also use it to improve these
    areas: have it rewrite or rephrase copy.'
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT can generate code snippets, functions, applications, and whole chatbots.
    In addition, you can use it to autogenerate tests. We will do all of these things
    in subsequent chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'GitHub Copilot (and CodeWhisperer) confer the following benefits to developers:
    they assist with code completion, fixing errors, and refactoring. They reside
    within the developer’s Integrated Development Environment (IDE), which can help
    retain a developer’s focus on the task at hand. This will make developers more
    productive, in terms of output (lines of code per time period), but they can also
    automate repetitive tasks. Given that Copilot’s training data was collected by
    culling public repositories, the developer will have suggestions to increase the
    overall code quality.'
  prefs: []
  type: TYPE_NORMAL
- en: Copilot can also assist in better understanding of a foreign code base. It will
    provide suggestions as to how to navigate this code base, as it can help one better
    understand the relationships amongst the classes and code.
  prefs: []
  type: TYPE_NORMAL
- en: As you use these tools, you will notice that your velocity changes dramatically
    as you better understand the capabilities and limitations of your new programming
    partner. Your programming partner will also get better at working with you since
    it can “remember” your programming style and approach. Working with generative
    Ais will allow you to tackle much more complex problems. You will write better,
    cleaner code, with fewer bugs. All while moving faster than you thought possible.
    Sound like a dream, or worse, fluff? It isn’t.
  prefs: []
  type: TYPE_NORMAL
- en: One might ask themselves Isn’t this just a better version of IntelliSense? You
    might ask yourself this question after looking over the first, few examples; however,
    by the end of the next chapter, having used Generative AI, you will be able to
    appreciate the differences.
  prefs: []
  type: TYPE_NORMAL
- en: In figure 1.1, you will see Microsoft Visual Studio Code provide an IntelliSense
    auto-completion suggestion to start the Flask application. Note that this is inline,
    and the suggestion comes as I edit the code.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.1 IntelliSense auto-completing the Flask run method.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application Description automatically generated](images/01_img_0001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.2 shows that GitHub Copilot has made the same suggestion based on
    the method name and signature. That is, it wrote the code before I started writing
    the implementation. The excitement behind generative Ais is tied to this power:
    its predictive nature. As the prompts get more explicit, the suggestions get more
    exact. We will explore this further in later chapters.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.2 GitHub Copilot solution as to how to run the Flask application.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application Description automatically generated](images/01_img_0002.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.2 presents a trivial example and does not make a compelling case for
    why one would want to use Generative AI. However, in this same Flask application,
    what if you need to create an end point that can handle the input from a POST
    method but forget the syntax? Would we need to open the official documentation
    and try to find how to do it? No, we could just ask GitHub Copilot.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.3 GitHub Copilot generating a POST method handler
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application, chat or text message Description
    automatically generated](images/01_img_0003.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see that Copilot offered several similar suggestions on how to complete
    this code. Declaring the method would have gotten the first suggestion auto-completed
    inline in our IDE. No need to stop and use the mouse. This approach keeps you
    in the code and in the flow state for longer without unnecessary distraction.
    Now, if only Copilot could fetch us a coffee…
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 History of Generative AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is worth taking a quick detour to understand a little bit about the genesis
    of the technologies that we study throughout the next few chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Generative AIs are a sub-set of Artificial Intelligence. Artificial Intelligence
    has been around and actively researched for more than sixty years. The Logic Theorist
    is considered the first application of artificial intelligence, predating the
    term “artificial intelligence”. The Logic Theorist was the brainchild of Herbert
    Simon and Allen Newell, with some contributions by Cliff Shaw. Simon and Newell
    were attempting to teach a computer to think.
  prefs: []
  type: TYPE_NORMAL
- en: While this attempt did not result in a genuinely thinking machine, the Logic
    Theorist was able to produce better, more detailed mathematical proofs than contemporary
    mathematicians Alfred North Whitehead and Bertrand Russell. The peculation and
    theorizing about what Logic Theorist would do to the field of mathematics mirrors
    what we are seeing in the news surrounding ChatGPT today.
  prefs: []
  type: TYPE_NORMAL
- en: The term “artificial intelligence” would not exist until the RAND Corporation
    hosted the Dartmouth Summer Research Project on Artificial Intelligence in 1956\.
    Then, prominent researcher John McCarthy (the original author of Lisp) and computer
    scientist coined the term “artificial intelligence,” unveiling it at this conference.
  prefs: []
  type: TYPE_NORMAL
- en: Research into AI continued for decades, incrementally improving with public
    interest waxing and waning. Garry Kasparov was defeated by IBM’s Deep Blue beat
    at chess in 1997\. Ken Jennings lost to IBM’s Watson at Jeopardy! in 2011\. A
    few years later, in 2014, generative adversarial networks (GANs) were invented.
    The advent of GANs kicked off renewed interest in AI, as it was able to create
    realistic images and so-called deep fakes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In 2017, a new type of deep learning model was invented: the transformer. This
    laid the groundwork for the Large Language Model (LLM): LLMs such as GPT-3\. Whereas
    GANs could generate images from noise maps, LLMs could create text and software.
    Around this time, the company OpenAI – the stewards of ChatGPT – was founded.'
  prefs: []
  type: TYPE_NORMAL
- en: But why was OpenAI founded? What was their goal? Let’s ask ChatGPT.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Q: what is the history of openai?'
  prefs: []
  type: TYPE_NORMAL
- en: 'A: OpenAI is a research company that was founded in December 2015 by Elon Musk,
    Sam Altman, Greg Brockman, Ilya Sutskever, Wojciech Zaremba and John Schulman.
    The company''s mission is to develop and promote friendly AI in a way that benefits
    humanity as a whole. OpenAI initially focused on developing and promoting AI in
    the gaming and robotics industries but has since expanded its focus to include
    a wide range of industries. In 2019, OpenAI released GPT-2, a powerful language
    processing AI model, and in 2020 GPT-3, which has been widely adopted in natural
    language processing tasks and has become one of the biggest AI models to date.
    In 2023, we saw the release of GPT-4, the most advanced model to date. It has
    the ability to connect to the internet amongst other vast improvements. OpenAI
    has also been involved in numerous research projects in the fields of AI safety,
    robotics, and machine learning, and has established partnerships with a number
    of organizations and companies to advance AI research and development.'
  prefs: []
  type: TYPE_NORMAL
- en: A key detail buried in this text is that there are companies that are actively
    working on making AI systems easier to use. Given that this trend is unlikely
    to end (and will likely only intensify in the future) does it not make sense to
    get in early? You will stay ahead of the curve. This will make your skills as
    an AI assisted developer all the more valuable.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 Grokking Generative AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generative AIs are a subset of artificial intelligence. They are trained on
    a large dataset to learn the patterns and structures of the data. Once trained,
    they will use this data to generate new data similar to the dataset that they
    were trained on in terms of this new data’s structure. This is the generative
    part in the name Generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three prominent and highly publicized types of Generative AIs: Generative
    Adversarial Networks (GANs), Variational Autoencoders (VAEs), and transformer-based
    language models. ChatGPT and OpenAI’s Codex are examples of the latter. We will
    briefly walk through how each of these types of Generative AIs function.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.4 There are three main types of Generative AI: Generative Models,
    Generative Adversarial Networks, and transformer based.'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![Diagram Description automatically generated](images/01_img_0004.png)'
  prefs: []
  type: TYPE_IMG
- en: All three of these Generative AIs employ a neural network to create output;
    be that text or code generation or images. A neural network is patterned after
    how humans’ minds work, as neurons pass signals to one another. You can visualize
    this as a directed graph in which data that exceeds certain thresholds are passed
    to the next node in the graph.
  prefs: []
  type: TYPE_NORMAL
- en: Data is encoding the input layer, which is called the outer layer. The output
    layer is connected to hidden layers. Behind the hidden layers are other, numerous
    hidden layers through which the data must traverse. All of the nodes in the neural
    network are connected via calculated numerical values, representing the strength
    of the connection between the neurons using back-propagation (represented as lines
    in Figure 1.5), which have thresholds that must be exceeded in order for the data
    activate the next layer. If the data makes it to the output layer, then the data
    is returned from the network. There is no guarantee that data will be returned,
    however. The data might be filtered out.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.5 Visualizing a neural network. A very tiny one. Outer nodes are exposed
    so that they can accept input. As Hidden nodes are traversed these inputs are
    either discarded or forwarded to the next node. If input makes to the Output nodes,
    then it is returned.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![Diagram Description automatically generated](images/01_img_0005.png)'
  prefs: []
  type: TYPE_IMG
- en: A Generative AI uses very large data sets to train these models. In the case
    of GitHub’s Copilot, this large data set was the contents of the publicly accessible
    repositories within GitHub itself. If you have ever contributed to an open-source
    project, then you might have code from which Copilot has trained.
  prefs: []
  type: TYPE_NORMAL
- en: 'While many Generative AIs use a neural network, how they use it determines
    the AI’s type. A Generative Adversarial Network (GAN) use two neural networks:
    one called generator and one called the discriminator. The generator network generates
    fake data based on the training data set. The discriminator tries to identify
    fake data. These networks are adversarial in nature as the generator attempts
    to create data that is indistinguishable from the real data and the discriminator
    attempts to discern if the data is real or fake.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Variational Autoencoders (VAEs) use two networks are well: one for encoding
    and one for decoding. In one sense the encoding network simplifies the input by
    reducing the data into a lower-dimensional representation. The decoding network
    then maps this lower-dimensional representation back to the original data space.
    The whole point of this is to be able to generate new data through sampling.'
  prefs: []
  type: TYPE_NORMAL
- en: The final type is *transformer-based models*. The transformer model is a type
    of *feedforward* neural network that uses *self-attention mechanisms* to process
    sequential data, such as natural language text. During training, the weights of
    the network are adjusted to minimize a loss function, such as cross-entropy.
  prefs: []
  type: TYPE_NORMAL
- en: In a feedforward neural network, the input flows in one direction, from input
    layer to output layer, with no feedback connections between the layers. Additionally,
    no information or error signal flows back from the output to the input layer.
    Therefore, the neural network's output is determined solely by the input data
    and the weights assigned to the connections between the layers.
  prefs: []
  type: TYPE_NORMAL
- en: Self-attention mechanisms allow the network to selectively attend to different
    parts of the input sequence based on their relevance to the current output. In
    a transformer, the input sequence is first embedded into a vector space using
    an embedding layer. The embedded input sequence is then fed into an encoder network,
    which consists of multiple layers of feedforward neural networks. Each encoder
    layer applies self-attention mechanisms to capture the relationships between the
    different parts of the input sequence.
  prefs: []
  type: TYPE_NORMAL
- en: The self-attention mechanism calculates an attention score for each part of
    the input sequence based on its relationship to the other parts of the sequence.
    These attention scores are then used to weight the contributions of each part
    of the sequence to the final output of the encoder network. This allows the encoder
    network to selectively focus on the most important parts of the input sequence,
    while ignoring irrelevant parts.
  prefs: []
  type: TYPE_NORMAL
- en: The output of the encoder network is then fed into a decoder network, which
    also consists of multiple layers of feedforward neural networks. The decoder uses
    self-attention mechanisms to generate an output sequence based on the input sequence,
    one token at a time.
  prefs: []
  type: TYPE_NORMAL
- en: An analogy for the relationship between encoders and decoders in a transformer
    network is that of a compiler and linker. Just as a compiler breaks down high-level
    code into low-level instructions and a linker combines those instructions into
    a final executable program, the encoder network breaks down the input sequence
    into meaningful units and the decoder network combines those units into a final
    output sequence. The use of self-attention mechanisms in transformers is similar
    to the way a compiler and linker optimize code for better performance.
  prefs: []
  type: TYPE_NORMAL
- en: As previously stated, many Generative AIs use neural networks, but not all of
    them do. Some are rules-based, generating output by applying the rules to the
    inputs. Still others are evolutionary in nature, iterating on results, and selecting
    based on the goodness of fit.
  prefs: []
  type: TYPE_NORMAL
- en: We can now walk through a descriptive example of how you would interact with
    Copilot. As you begin to type in your favorite IDE (VS Code in this example),
    the Copilot plugin will send your comments or code (sometime all it takes is a
    function’s name!) into the Copilot service.
  prefs: []
  type: TYPE_NORMAL
- en: This service turns your lines of code or comments into natural language prompts,
    which are then run through OpenAi’s Codex model. The model will generate suggestions
    based on the training data set. GitHub refers to this as code synthesis. They
    claim that this training data set contains billions of lines of code from dozens
    of programming languages.
  prefs: []
  type: TYPE_NORMAL
- en: Once the top ten solutions are returned from the Codex model, the Copilot service
    will return these suggestions back to your editor. You select the suggestion that
    most accurately captures your intent or need. Or you reject all of the suggestions.
    Your selection is then returned to the Copilot service to better train the model.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.6 Your code is sampled and encoded by the Copilot plug-in. It is then
    set to the OpenAI Codex model, where suggestions are generated. These suggestions
    are then returned to your VS Code session.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/01_img_0006.png)'
  prefs: []
  type: TYPE_IMG
- en: GitHub is constantly improving their Copilot service. A recent release (as of
    December 2022), they boast that the acceptance rate is 46% across all programming
    languages and with Java specifically, it is 61% on average[^([1])](v-5.html).
  prefs: []
  type: TYPE_NORMAL
- en: 1.4 When to use and when to avoid Generative AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Excitement around these technologies is growing. Since its public release in
    late November of 2022, there are hundreds (possibly thousands) of articles about
    the various dimensions of ChatGPT. Will it ruin education? Is education required
    anymore? Are software engineers necessary anymore?
  prefs: []
  type: TYPE_NORMAL
- en: It is easy to give into the pessimism. There are lots of unknowns and the full
    impact of this technology has yet to be revealed. However, you should form your
    own opinion as you work through this book. It is my hope that you will see the
    positives of generative AI and use them for good. You will use it to grow as a
    programmer. As you use them, you will become a better programmer.
  prefs: []
  type: TYPE_NORMAL
- en: One of the best ways to grow as a developer is to read good code. OpenAI has
    curated some of the best code on the planet; it is all at your fingertips. You
    also now have some of the worst code available at your fingertips as well. You
    can learn from good examples as well as bad. Being able to discern the difference
    is the key to growing.
  prefs: []
  type: TYPE_NORMAL
- en: So, when should you use generative Ais? Every opportunity that you can! (We
    will discuss some exceptions to their usage shortly.) It is fascinating to engage
    with generative AIs. You will learn how to use them better, find shortcuts, discover
    new features, and have stary-eyed child-like delight every moment of it.
  prefs: []
  type: TYPE_NORMAL
- en: While employing generative Ais in your daily job would appear to make a lot
    of sense (because it does), it should be noted that it is not pertinent to use
    in all cases.
  prefs: []
  type: TYPE_NORMAL
- en: If you were given a take home coding exam, should you use a generative AI to
    complete this exam? Unless it was explicitly stated that you can, then you should
    avoid it. It could be construed as cheating if the tester did not anticipate you
    using it.
  prefs: []
  type: TYPE_NORMAL
- en: Should they anticipate that you would use you? Yes, they probably should at
    this point. Further, one might argue that, given that the purpose of the exam
    is to assess the coding abilities of potential candidates, potential employers
    should try to construct real world conditions in order to best assess one’s ability.
    This should include all tools and resources that one would have available to them
    doing the course of their day. These tools would include generative Ais.
  prefs: []
  type: TYPE_NORMAL
- en: 'You should be especially careful of using them in an academic setting. Proper
    attribution is complex, at a minimum, and the line between inspiration and plagiarism
    with Generative AI is a very fine one. If your usage is determined to be plagiarism,
    the consequences will be dire and permanent: expulsion and you might be barred
    from future enrollment at other institutions. Proceed with extreme caution.'
  prefs: []
  type: TYPE_NORMAL
- en: In general, use your better judgement. If there is a chance that you might run
    afoul of any copyright laws or administrative policy, then do not use it unless
    you are granted specific permission to do so. It would not hurt to talk to your
    corporate IT or InfoSec team at work, just to make certain that you comply with
    corporate policies related to corporate systems and computers.
  prefs: []
  type: TYPE_NORMAL
- en: 'One final note: Generative AIs are tools and like any tool, you need to have
    some idea of what you are doing. You should have some idea about what the correct
    answer should be. You should use them in domains, in which you have some idea
    what is going on. In cases like these, you will find yourself exploring the landscape,
    deepening your understanding of the domain as well as learning faster.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have explored an abridged history of Generative AI, seen some use
    cases for Generative AI, and applied some important guardrails, we will go hands-on
    in the next chapter, examining how to start the same project using these three
    Generative tools.
  prefs: []
  type: TYPE_NORMAL
- en: 1.5 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generative AIs are both evolutionary and revolutionary. Evolutionary in the
    sense that they are a just another iterating on the tools that we as developers
    use every day. Revolutionary in that they will transform how we do our jobs. In
    fact, they will change our jobs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The future of development will be the management of Generative AI. Even the
    mythical 10x developer will not have the productivity of a developer with an AI
    partner; an AI powered developer will produce higher quality code at a substantially
    faster rate, at lower cost than one who is not. We will spend more of our time
    training our AI partner to do what we want, how we want, then we will write code
    without the AI.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'While there are several Generative AI out in the wild, we will explore three
    of the most popular:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ChatGPT – Has been making headlines since November 2022.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: GitHub Copilot – The most popular Generative AI that uses in an Integrated Development
    Environment. Financed and promoted by Microsoft.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Web Services CodeWhisperer – A product similar to Copilot but backed
    by Amazon.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Some of the world’s biggest companies are making a significant investment in
    Generative AIs (Microsoft, Amazon, Alphabet) and making them easier to use (Open
    AI).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative AIs make use of extremely sophisticated neural networks, resembling
    ours, to filter and map input to new previously unseen output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You should check with your professor or teacher prior to making use of generative
    AIs for school work.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '[^([1])](v-5.html) Shuyin Zhao. “GitHub Copilot now has a better AI model and
    new capabilities.” *GitHub Blog.* [https://github.blog/2023-02-14-github-copilot-now-has-a-better-ai-model-and-new-capabilities/](2023-02-14-github-copilot-now-has-a-better-ai-model-and-new-capabilities.html).
    Last accessed: Feb 14, 2023.'
  prefs: []
  type: TYPE_NORMAL
