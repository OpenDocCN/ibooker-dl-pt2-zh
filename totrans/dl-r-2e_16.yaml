- en: 13 Best practices for the real world
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 13 实践的最佳实践
- en: '*This chapter covers*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章包括*'
- en: Hyperparameter tuning
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数调优
- en: Model ensembling
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型集成
- en: Mixed-precision training
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混合精度训练
- en: Training Keras models on multiple GPUs or on a TPU
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在多个 GPU 或 TPU 上训练 Keras 模型
- en: You’ve come far since the beginning of this book. You can now train image classification
    models, image segmentation models, models for classification or regression on
    vector data, time-series forecasting models, text classification models, sequence-to-sequence
    models, and even generative models for text and images. You’ve got all the bases
    covered.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 从这本书开始以来，你已经走了很远。你现在可以训练图像分类模型、图像分割模型、用于向量数据的分类或回归模型、时间序列预测模型、文本分类模型、序列到序列模型，甚至是文本和图像的生成模型。你已经覆盖了所有的基础。
- en: However, your models so far have all been trained at a small scale—on small
    datasets, with a single GPU—and they generally haven’t reached the best achievable
    performance on each dataset we looked at. This book is, after all, an introductory
    book. If you are to go out in the real world and achieve state-of-the-art results
    on brand-new problems, there’s still a bit of a chasm that you’ll need to cross.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，到目前为止，你的模型都是在小规模上训练的——在小数据集上，用单个 GPU——它们通常还没有达到我们所看到的每个数据集的最佳性能。毕竟，这本书是一本入门书。如果你要走进现实世界，在全新的问题上取得最先进的结果，你还需要跨越一定的鸿沟。
- en: 'This penultimate chapter is about bridging that gap and giving you the best
    practices you’ll need as you go from machine learning student to fully fledged
    machine learning engineer. We’ll review essential techniques for systematically
    improving model performance: hyperparameter tuning and model ensembling. Then
    we’ll look at how you can speed up and scale up model training, with multi-GPU
    and TPU training, mixed precision, and leveraging remote computing resources in
    the cloud.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '这个倒数第二章是关于弥合这一鸿沟，并为你提供你在从机器学习学生到完全成熟的机器学习工程师所需的最佳实践。我们将回顾系统地改进模型性能的基本技术：超参数调优和模型集成。然后我们将看看如何加速和扩展模型训练，使用多
    GPU 和 TPU 训练，混合精度，以及利用云中的远程计算资源。  '
- en: We’ll also use this chapter to show how you can access Python packages directly,
    even when there is no R wrapper conveniently available. This will be an essential
    skill as you continue in your deep learning journey. You don’t need to know Python
    to use Python packages from R, but if you find yourself ever reading Python documentation
    and asking questions like, “What are all the underscores?” head over to the appendix,
    *Python primer for R users*, which will get you up to speed as quickly as possible.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将利用这一章节来展示如何直接访问 Python 包，即使没有方便的 R 包装器可用。这将是你在深度学习旅程中的一个基本技能。你不需要知道 Python
    来使用 R 中的 Python 包，但是如果你发现自己曾经阅读过 Python 文档，并问过“所有的下划线是什么？”可以直接去附录，*R 用户的 Python
    入门指南*，它会尽快让你掌握速度。
- en: 13.1 Getting the most out of your models
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.1 充分利用你的模型
- en: Blindly trying out different architecture configurations works well enough if
    you just need something that works okay. In this section, we’ll go beyond “works
    okay” to “works great and wins machine learning competitions” via a set of must-know
    techniques for building state-of-the-art deep learning models.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 盲目尝试不同的架构配置，如果你只需要一个可以正常工作的东西，那么这样做就足够了。在本节中，我们将超越“正常工作”到“出色工作并赢得机器学习竞赛”的一系列必须知道的技术，用于构建最先进的深度学习模型。
- en: 13.1.1 Hyperparameter optimization
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.1.1 超参数优化
- en: 'When building a deep learning model, you have to make many seemingly arbitrary
    decisions: How many layers should you stack? How many units or filters should
    go in each layer? Should you use relu as activation, or a different function?
    Should you use layer_batch_normalization() after a given layer? How much dropout
    should you use? and so on. These architecture-level parameters are called *hyperparameters*
    to distinguish them from the *parameters* of a model, which are trained via backpropagation.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建深度学习模型时，你必须做出许多看似任意的决定：应该堆叠多少层？每层应该放多少个单元或滤波器？应该使用 relu 作为激活函数，还是其他函数？在给定的层之后应该使用
    layer_batch_normalization() 吗？应该使用多少的 dropout？等等。这些架构级参数被称为*超参数*，以区别于模型的*参数*，后者通过反向传播进行训练。
- en: In practice, experienced machine learning engineers and researchers build intuition
    over time as to what works and what doesn’t when it comes to these choices— they
    develop hyperparameter-tuning skills. But no formal rules exist. If you want to
    get to the very limit of what can be achieved on a given task, you can’t be content
    with such arbitrary choices. Your initial decisions are almost always suboptimal,
    even if you have very good intuition. You can refine your choices by tweaking
    them by hand and retraining the model repeatedly—that’s what machine learning
    engineers and researchers spend most of their time doing. But it shouldn’t be
    your job as a human to fiddle with hyperparameters all day—that is better left
    to a machine.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，经验丰富的机器学习工程师和研究人员随着时间的推移逐渐形成了有关这些选择的有效性的直觉，他们发展了超参数调整技能。但是，不存在正式的规则。如果您想达到在给定任务上可以实现的极限，您不能满足于这些随意的选择。即使您有很好的直觉，您的初始决策几乎总是次优的。您可以通过手动调整并重复训练模型来改进您的选择-这就是机器学习工程师和研究人员花费大量时间的内容。但是作为人类，您不应该整天瞎折腾超参数，最好交给机器处理。
- en: 'Thus, you need to explore the space of possible decisions automatically, systematically,
    in a principled way. You need to search the architecture space and find the best-performing
    architectures empirically. That’s what the field of automatic hyper-parameter
    optimization is about: it’s an entire field of research—and an important one.
    The process of optimizing hyperparameters typically looks like this:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您需要以系统化、有原则的方式自动探索可能的决策空间。您需要搜索体系结构空间，并经验性地找到最佳的性能体系结构。这就是自动超参数优化领域：这是一个独立的研究领域，也是重要的一个。优化超参数的过程通常是这样的：
- en: '**1** Choose a set of hyperparameters (automatically).'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**1** 自动选择一组超参数。'
- en: '**2** Build the corresponding model.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**2** 构建相应的模型。'
- en: '**3** Fit it to your training data, and measure performance on the validation
    data.'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**3** 将其应用于训练数据，并在验证数据上测量性能。'
- en: '**4** Choose the next set of hyperparameters to try (automatically).'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**4** 自动选择下一组要尝试的超参数。'
- en: '**5** Repeat.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**5** 重复。'
- en: '**6** Eventually, measure performance on your test data.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**6** 最终，在测试数据上测量性能。'
- en: 'The key to this process is the algorithm that analyzes the relationship between
    validation performance and various hyperparameter values to choose the next set
    of hyper-parameters to evaluate. Many different techniques are possible: Bayesian
    optimization, genetic algorithms, simple random search, and so on.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程的关键是算法，它分析验证性能和各种超参数值之间的关系，选择下一组要评估的超参数。有许多不同的技术可用：贝叶斯优化、遗传算法、简单的随机搜索等等。
- en: 'Training the weights of a model is relatively easy: you compute a loss function
    on a mini-batch of data and then use backpropagation to move the weights in the
    right direction. Updating hyperparameters, on the other hand, presents unique
    challenges. Consider these points:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型的权重相对容易：您在一小批数据上计算损失函数，然后使用反向传播将权重向正确的方向移动。另一方面，更新超参数则具有独特的挑战。考虑以下几点：
- en: The hyperparameter space is typically made up of discrete decisions and, thus,
    isn’t continuous or differentiable. Hence, you typically can’t do gradient descent
    in hyperparameter space. Instead, you must rely on gradient-free optimization
    techniques, which naturally are far less efficient than gradient descent.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数空间通常由离散决策组成，因此不是连续或可微分的。因此，您通常无法在超参数空间中执行梯度下降。相反，您必须依赖于无梯度优化技术，这自然比梯度下降效率低得多。
- en: 'Computing the feedback signal of this optimization process (does this set of
    hyperparameters lead to a high-performing model on this task?) can be extremely
    expensive: it requires creating and training a new model from scratch on your
    dataset.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个优化过程的反馈信号（这组超参数是否导致在这个任务上的高性能模型？）可能非常昂贵：需要从头开始创建和训练一个新模型。
- en: 'The feedback signal may be noisy: if a training run performs 0.2% better, is
    that because of a better model configuration, or because you got lucky with the
    initial weight values'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反馈信号可能带有噪声：如果一个训练运行表现更好，是因为更好的模型配置，还是因为您得到了初值较好的权重值？
- en: 'Thankfully, there’s a tool that makes hyperparameter tuning simpler: KerasTuner.
    Let’s check it out.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有一个工具可以使超参数调整更简单：KerasTuner。我们来看看吧。
- en: USING KERASTUNER
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用KerasTuner
- en: 'Let’s start by installing the KerasTuner Python package:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先安装KerasTuner Python包：
- en: reticulate::py_install("keras-tuner", pip = TRUE)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: reticulate::py_install("keras-tuner", pip = TRUE)
- en: KerasTuner lets you replace hardcoded hyperparameter values, such as units =
    32, with a range of possible choices, such as Int(name = “units”, min_value =
    16, max_ value = 64, step = 16). This set of choices in a given model is called
    the *search space* of the hyperparameter tuning process. To specify a search space,
    define a model-building function (see the next listing). It takes an hp argument,
    from which you can sample hyperparameter ranges, and it returns a compiled Keras
    model.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: KerasTuner 可以让你用一系列可能的选择替换硬编码的超参数值，比如 units = 32，比如 Int(name = “units”, min_value
    = 16, max_ value = 64, step = 16)。在给定模型中，这组选择被称为超参数调整过程的 *搜索空间*。要指定搜索空间，定义一个模型构建函数（见下面的列表）。它接受一个
    hp 参数，从中你可以取样超参数范围，并返回一个编译过的 Keras 模型。
- en: '**Listing 13.1 A KerasTuner model-building function**'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**列表 13.1 一个 KerasTuner 模型构建函数**'
- en: build_model <- function(hp, num_classes = 10) {
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: build_model <- function(hp, num_classes = 10) {
- en: units <- hp$Int(name = "units",➊
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: units <- hp$Int(name = "units",➊
- en: min_value = 16L, max_value = 64L, step = 16L)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: min_value = 16L, max_value = 64L, step = 16L)
- en: model <- keras_model_sequential() %>%
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model_sequential() %>%
- en: layer_dense(units, activation = "relu") %>%
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(units, activation = "relu") %>%
- en: layer_dense(num_classes, activation = "softmax")
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(num_classes, activation = "softmax")
- en: optimizer <- hp$Choice(name = "optimizer",➋
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: optimizer <- hp$Choice(name = "optimizer",➋
- en: values = c("rmsprop", "adam"))
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: values = c("rmsprop", "adam"))
- en: model %>% compile(optimizer = optimizer,
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% compile(optimizer = optimizer,
- en: loss = "sparse_categorical_crossentropy",
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: loss = "sparse_categorical_crossentropy",
- en: metrics = c("accuracy"))
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = c("accuracy"))
- en: model➌
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: model➌
- en: '}'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ **Sample hyperparameter values from the hp object. After sampling, these values
    (such as the units and optimizer variables here) are just regular R constants.**
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **从 hp 对象中取样超参数值。取样后，这些值（比如这里的 units 和 optimizer 变量）就是普通的 R 常量了。**
- en: '➋ **Different kinds of hyperparameters are available: Int, Float, Boolean,
    Choice.**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **有不同类型的超参数可用：Int、Float、Boolean、Choice。**
- en: ➌ **The function returns a compiled model.**
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **函数返回一个编译过的模型。**
- en: If you want to adopt a more modular and configurable approach to model-building,
    you can also subclass the HyperModel class and define a build method, as follows.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想采用更模块化和可配置的方法来构建模型，你也可以子类化 HyperModel 类并定义一个 build 方法，如下所示。
- en: '**Listing 13.2 A KerasTuner HyperModel**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**列表 13.2 一个 KerasTuner HyperModel**'
- en: kt <- reticulate::import("kerastuner")
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: kt <- reticulate::import("kerastuner")
- en: SimpleMLP(kt$HyperModel) %py_class% {
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: SimpleMLP(kt$HyperModel) %py_class% {
- en: '`__init__` <- function(self, num_classes) {➊'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`__init__` <- function(self, num_classes) {➊'
- en: self$num_classes <- num_classes
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: self$num_classes <- num_classes
- en: '}'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: build <- function(self, hp) {➋
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: build <- function(self, hp) {➋
- en: build_model(hp, self$num_classes)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: build_model(hp, self$num_classes)
- en: '}'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: hypermodel <- SimpleMLP(num_classes = 10)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: hypermodel <- SimpleMLP(num_classes = 10)
- en: ➊ **With the object-oriented approach, we can configure model constants like
    num_classes to be constructor arguments.**
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **通过面向对象的方法，我们可以将模型常量配置为构造函数参数，如 num_classes。**
- en: ➋ **The build() method is identical to our prior build_model() standalone function,
    except now it is invoked by a method of a subclassed kt$HyperModel.**
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **build() 方法与我们先前的 build_model() 独立函数相同，只是现在它由子类 kt$HyperModel 的一个方法调用。**
- en: '**Custom Python classes with %py_class%**'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用 %py_class% 自定义 Python 类**'
- en: '%py_class% can be used to define custom Python classes in R. It mirrors the
    Python syntax for defining Python classes and allows for an almost mechanical
    translation of Python to R. It is especially useful when using Python APIs that
    are designed around subclassing, like kt$HyperModel. The equivalent definition
    of SimpleMLP in Python, (like you might encounter in the Python documentation
    for KerasTuner) would look like this:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '%py_class% 可用于在 R 中定义自定义 Python 类。它反映了定义 Python 类的 Python 语法，并允许将 Python 几乎机械地转换为
    R。当使用围绕子类化设计的 Python API 时，比如 kt$HyperModel 时，它尤其有用。在 Python 文档中，KerasTuner 的等效
    SimpleMLP 定义（你可能在 Python 文档中找到）将如下所示：'
- en: import kerastuner as kt
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: import kerastuner as kt
- en: 'class SimpleMLP(kt.HyperModel):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 'class SimpleMLP(kt.HyperModel):'
- en: 'def __init__(self, num_classes):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 'def __init__(self, num_classes):'
- en: self.num_classes = num_classes
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: self.num_classes = num_classes
- en: 'def build(self, hp):'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 'def build(self, hp):'
- en: return build_model(hp, self.num_classes)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: return build_model(hp, self.num_classes)
- en: hypermodel = SimpleMLP(num_classes=10)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: hypermodel = SimpleMLP(num_classes=10)
- en: See ?’%py_class%’ in R for more info and examples.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 R 中的 ?’%py_class%’ 以获取更多信息和示例。
- en: The next step is to define a “tuner.” Schematically, you can think of a tuner
    as a for loop that will repeatedly
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是定义一个“调谐器”。概括地说，你可以将调谐器想象为一个会重复执行的 for 循环，该循环将
- en: Pick a set of hyperparameter values
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一组超参数值
- en: Call the model-building function with these values to create a model
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用这些值调用模型构建函数以创建一个模型
- en: Train the model and record its metric
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练模型并记录其度量
- en: 'KerasTuner has several built-in tuners available—RandomSearch, BayesianOptimization,
    and Hyperband. Let’s try BayesianOptimization, a tuner that attempts to make smart
    predictions for which new hyperparameter values are likely to perform best given
    the outcomes of previous choices:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: KerasTuner有几个内置的调谐器可用——RandomSearch、BayesianOptimization和Hyperband。让我们尝试BayesianOptimization，这是一种试图根据先前选择的结果智能预测哪些新的超参数值可能表现最佳的调谐器：
- en: tuner <- kt$BayesianOptimization(➊
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: tuner <- kt$BayesianOptimization(➊
- en: build_model,
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: build_model,
- en: objective = "val_accuracy",➋
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: objective = "val_accuracy",➋
- en: max_trials = 100L,➌
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: max_trials = 100L,➌
- en: executions_per_trial = 2L,➍
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: executions_per_trial = 2L,➍
- en: directory = "mnist_kt_test",➎
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: directory = "mnist_kt_test",➎
- en: overwrite = TRUE )➏
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: overwrite = TRUE )➏
- en: ➊ **Specify the model-building function (or hypermodel instance).**
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **指定模型构建函数（或超模型实例）。**
- en: ➋ **Specify the metric that the tuner will seek to optimize. Always specify
    validation metrics, because the goal of the search process is to find models that
    generalize.**
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **指定调谐器将寻求优化的指标。始终指定验证指标，因为搜索过程的目标是找到具有泛化能力的模型。**
- en: ➌ **Maximum number of different model configurations ("trials") to try before
    ending the search.**
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **尝试结束搜索之前要尝试的不同模型配置（“试验”）的最大数量。**
- en: ➍ **To reduce metrics variance, you can train the same model multiple times
    and average the results. executions_per_trial is how many training rounds (executions)
    to run for each model configuration (trial).**
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **为减少指标方差，您可以多次训练相同的模型并平均结果。executions_per_trial是每个模型配置（试验）运行的训练轮数（执行次数）。**
- en: ➎ **Where to store search logs**
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **存储搜索日志的位置**
- en: ➏ **Whether to overwrite data in directory to start a new search. Set this to
    TRUE if you've modified the model-building function, or to FALSE to resume a previously
    started search with the same model-building function.**
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ➏ **是否覆盖目录中的数据以启动新的搜索。如果您修改了模型构建函数，则将其设置为TRUE；如果要恢复以前开始的具有相同模型构建函数的搜索，则设置为FALSE。**
- en: 'You can display an overview of the search space via search_space_summary():'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过search_space_summary()显示搜索空间的概述：
- en: tuner$search_space_summary()
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: tuner$search_space_summary()
- en: Search space summary
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索空间摘要
- en: 'Default search space size: 2'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 默认搜索空间大小：2
- en: units (Int)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: units (Int)
- en: '{"default": None,'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '{"default": None,'
- en: '"conditions": [],'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '"conditions": [],'
- en: '"min_value": 128,'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '"min_value": 128,'
- en: '"max_value": 1024,'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '"max_value": 1024,'
- en: '"step": 128,'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '"step": 128,'
- en: '"sampling": None}'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '"sampling": None}'
- en: optimizer (Choice)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: optimizer (Choice)
- en: '{"default": "rmsprop",'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '{"default": "rmsprop",'
- en: '"conditions": [],'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '"conditions": [],'
- en: '"values": ["rmsprop", "adam"],'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '"values": ["rmsprop", "adam"],'
- en: '"ordered": False}'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '"ordered": False}'
- en: '**Objective maximization and minimization**'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**目标最大化和最小化**'
- en: 'For built-in metrics (like accuracy, in our case), the *direction* of the metric
    (accuracy should be maximized, but a loss should be minimized) is inferred by
    KerasTuner. However, for a custom metric, you should specify it yourself, like
    this:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 对于内置指标（例如准确性，在我们的情况下），KerasTuner推断出指标的*方向*（准确性应最大化，但损失应最小化）。但是，对于自定义指标，您应该自己指定，就像这样：
- en: objective <- kt$Objective(
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: objective <- kt$Objective(
- en: name = "val_accuracy",➊
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: name = "val_accuracy",➊
- en: direction = "max" ➋
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: direction = "max" ➋
- en: )
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: tuner <- kt$BayesianOptimization(
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: tuner <- kt$BayesianOptimization(
- en: build_model,
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: build_model,
- en: objective = objective,
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: objective = objective,
- en: …
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: …
- en: )
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: ➊ **The metric's name, as found in epoch logs**
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **指标的名称，如在epoch日志中找到的**
- en: '➋ **The metric''s desired direction: "min" or "max"**'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **指标的期望方向："min"或"max"**
- en: 'Finally, let’s launch the search. Don’t forget to pass validation data, and
    make sure not to use your test set as validation data—otherwise, you’d quickly
    start overfitting to your test data, and you wouldn’t be able to trust your test
    metrics anymore:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们启动搜索。不要忘记传递验证数据，并确保不要使用测试集作为验证数据——否则，您将很快开始过度拟合您的测试数据，并且您将无法信任您的测试指标：
- en: c(c(x_train, y_train), c(x_test, y_test)) %<-% dataset_mnist()
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: c(c(x_train, y_train), c(x_test, y_test)) %<-% dataset_mnist()
- en: x_train %<>% { array_reshape(., c(-1, 28 * 28)) / 255 }
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: x_train %<>% { array_reshape(., c(-1, 28 * 28)) / 255 }
- en: x_test %<>% { array_reshape(., c(-1, 28 * 28)) / 255 }
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: x_test %<>% { array_reshape(., c(-1, 28 * 28)) / 255 }
- en: x_train_full <- x_train➊
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: x_train_full <- x_train➊
- en: y_train_full <- y_train➊
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: y_train_full <- y_train➊
- en: num_val_samples <- 10000
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: num_val_samples <- 10000
- en: c(x_train, x_val) %<-%
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: c(x_train, x_val) %<-%
- en: list(x_train[seq(num_val_samples), ],➋
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: list(x_train[seq(num_val_samples), ],➋
- en: x_train[-seq(num_val_samples), ])➋
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: x_train[-seq(num_val_samples), ])➋
- en: c(y_train, y_val) %<-%➋
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: c(y_train, y_val) %<-%➋
- en: list(y_train[seq(num_val_samples)],➋
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: list(y_train[seq(num_val_samples)],➋
- en: y_train[-seq(num_val_samples)])
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: y_train[-seq(num_val_samples)])
- en: callbacks <- c(
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: callbacks <- c(
- en: callback_early_stopping(monitor = "val_loss",
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: callback_early_stopping(monitor = "val_loss",
- en: patience = 5)
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: patience = 5)
- en: )
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: tuner$search(➌
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: tuner$search(➌
- en: x_train, y_train,
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: x_train, y_train,
- en: batch_size = 128L,➍
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: batch_size = 128L,➍
- en: epochs = 100L,➎
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 100L,➎
- en: validation_data = list(x_val, y_val),
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: validation_data = list(x_val, y_val),
- en: callbacks = callbacks,
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: callbacks = callbacks,
- en: verbose = 2L
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: verbose = 2L
- en: )
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: ➊ **Reserve these for later.**
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **这些要留着以备后用。**
- en: ➋ **Set aside a validation set.**
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **设置一个验证集。**
- en: ➌ **This takes the same arguments as fit() (it simply passes them down to fit()
    for each new model).**
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **这接受与 fit() 相同的参数（它只是将它们传递给每个新模型的 fit()）。**
- en: ➍ **Make sure to pass integers where Python functions expect them, not doubles.**
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **确保在 Python 函数期望整数的地方传递整数，而不是双精度数。**
- en: ➎ **Use a large number of epochs (you don't know in advance how many epochs
    each model will need), and use a callback_early_stopping() to stop training when
    you start overfitting.**
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **使用大量时期数（你事先不知道每个模型需要多少时期），并使用 callback_early_stopping() 在开始过拟合时停止训练。**
- en: The preceding example will run in just a few minutes, because we’re looking
    at only a few possible choices and we’re training on MNIST. However, with a typical
    search space and dataset, you’ll often find yourself letting the hyperparameter
    search run overnight or even over several days. If your search process crashes,
    you can always restart it—just specify overwrite = FALSE in the tuner so that
    it can resume from the trial logs stored on disk. Once the search is complete,
    you can query the best hyper-parameter configurations, which you can use to create
    high-performing models that you can then retrain.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 前述示例将在几分钟内运行完成，因为我们只考虑了几种可能的选择，并且我们是在 MNIST 上进行训练。然而，对于典型的搜索空间和数据集，你经常会发现自己让超参数搜索运行一整夜甚至几天。如果你的搜索过程崩溃了，你总是可以重新启动它——只需在调节器中指定
    `overwrite = FALSE`，这样它就可以从存储在磁盘上的试验日志中恢复。一旦搜索完成，你可以查询最佳的超参数配置，然后可以使用它们创建性能优越的模型，然后重新训练。
- en: Listing 13.3 Querying the best hyperparameter configurations
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 查询最佳超参数配置列表 13.3
- en: top_n <- 4L
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: top_n <- 4L
- en: best_hps <- tuner$get_best_hyperparameters(top_n)➊
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: best_hps <- tuner$get_best_hyperparameters(top_n)➊
- en: ➊ **Return a list of HyperParameter objects, which you can pass to the model-building
    function.**
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **返回 HyperParameter 对象的列表，你可以将它们传递给模型构建函数。**
- en: Usually, when retraining these models, you may want to include the validation
    data as part of the training data, because you won’t be making any further hyperparameter
    changes, and thus you will no longer be evaluating performance on the validation
    data. In our example, we’d train these final models on the totality of the original
    MNIST training data, without reserving a validation set.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当重新训练这些模型时，你可能希望将验证数据包括在训练数据中，因为你不会再进行任何超参数更改，因此不会再在验证数据上评估性能。在我们的示例中，我们将最终模型训练在原始
    MNIST 训练数据的全部范围内，而不保留验证集。
- en: 'Before we can train on the full training data, though, there’s one last parameter
    we need to settle: the optimal number of epochs to train for. Typically, you’ll
    want to train the new models for longer than you did during the search: using
    an aggressive patience value in the callback_early_stopping() saves time during
    the search, but it may lead to underfitting the models. Just use the validation
    set to find the best epoch:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能够在完整的训练数据上训练之前，还有一个最后需要解决的参数：训练的最佳时期数。通常情况下，你会希望比在搜索期间更长时间地训练新模型：在 callback_early_stopping()
    中使用激进的耐心值可以节省搜索时间，但可能会导致模型欠拟合。只需使用验证集找到最佳时期：
- en: get_best_epoch <- function(hp) {
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: get_best_epoch <- function(hp) {
- en: model <- build_model(hp)
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: model <- build_model(hp)
- en: callbacks <- c(
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: callbacks <- c(
- en: callback_early_stopping(monitor = "val_loss", mode = "min",
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: callback_early_stopping(monitor = "val_loss", mode = "min",
- en: patience = 10))➊
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: patience = 10))➊
- en: history <- model %>% fit(
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: history <- model %>% fit(
- en: x_train, y_train,
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: x_train, y_train,
- en: validation_data = list(x_val, y_val),
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: validation_data = list(x_val, y_val),
- en: epochs = 100,
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 100,
- en: batch_size = 128,
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: batch_size = 128,
- en: callbacks = callbacks
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: callbacks = callbacks
- en: )
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: best_epoch <- which.min(history$metrics$val_loss)
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: best_epoch <- which.min(history$metrics$val_loss)
- en: 'print(glue::glue("Best epoch: {best_epoch}"))'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: print(glue::glue("最佳时期：{best_epoch}"))
- en: invisible(best_epoch)
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: invisible(best_epoch)
- en: '}'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ **Note the very high patience value.**
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **注意非常高的耐心值。**
- en: 'Finally, train on the full dataset for just a bit longer than this epoch count,
    because you’re training on more data; 20% more in this case:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在这个时期计数稍微长一点的完整数据集上训练，因为你在训练更多的数据；在这种情况下多了 20%：
- en: get_best_trained_model <- function(hp) {
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: get_best_trained_model <- function(hp) {
- en: best_epoch <- get_best_epoch(hp)
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: best_epoch <- get_best_epoch(hp)
- en: model <- build_model(hp)
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: model <- build_model(hp)
- en: model %>% fit(
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% fit(
- en: x_train_full,
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: x_train_full，
- en: y_train_full,
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: y_train_full，
- en: batch_size = 128,
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: batch_size = 128，
- en: epochs = round(best_epoch * 1.2)
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = round(best_epoch * 1.2)
- en: )
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: model
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: model
- en: '}'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: best_models <- best_hps %>%
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: best_models <- best_hps %>%
- en: lapply(get_best_trained_model)
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: lapply(get_best_trained_model)
- en: 'Note that if you’re not worried about slightly underperforming, there’s a shortcut
    you can take: just use the tuner to reload the top-performing models with the
    best weights saved during the hyperparameter search, without retraining new models
    from scratch:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果您不担心轻微的性能下降，您可以采取捷径：只需使用调谐器重新加载在超参数搜索期间保存的最佳权重的表现最佳模型，而无需从头开始重新训练新模型：
- en: best_models <- tuner$get_best_models(top_n)
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: best_models <- tuner$get_best_models(top_n)
- en: One important issue to think about when doing automatic hyperparameter optimization
    at scale is validation set overfitting. Because you’re updating hyperparameters
    based on a signal that is computed using your validation data, you’re effectively
    training them on the validation data, and thus they will quickly overfit to the
    validation data. Always keep this in mind.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行规模化的自动超参数优化时，一个重要问题需要考虑，那就是验证集过拟合。因为您正在根据使用验证数据计算的信号更新超参数，所以您实际上是在验证数据上训练它们，因此它们会迅速过拟合验证数据。请务必牢记这一点。
- en: THE ART OF CRAFTING THE RIGHT SEARCH SPACE
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 精心设计正确的搜索空间的艺术
- en: 'Overall, hyperparameter optimization is a powerful technique that is an absolute
    requirement for getting to state-of-the-art models on any task or to win machine
    learning competitions. Think about it: once upon a time, people handcrafted the
    features that went into shallow machine learning models. That was very much suboptimal.
    Now, deep learning automates the task of hierarchical feature engineering—features
    are learned using a feedback signal, not hand-tuned, and that’s the way it should
    be. In the same way, you shouldn’t handcraft your model architectures; you should
    optimize them in a principled way.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，超参数优化是一种强大的技术，是在任何任务中获得最先进模型或赢得机器学习竞赛的绝对要求。想想看：很久以前，人们手工制作了进入浅层机器学习模型的特征。那是非常次优的。现在，深度学习自动化了分层特征工程的任务——特征是使用反馈信号学习的，而不是手工调整的，这才是正确的方式。同样，您不应手工制作您的模型架构；您应以有原则的方式对其进行优化。
- en: 'However, doing hyperparameter tuning is not a replacement for being familiar
    with model architecture best practices. Search spaces grow combinatorially with
    the number of choices, so it would be far too expensive to turn everything into
    a hyper-parameter and let the tuner sort it out. You need to be smart about designing
    the right search space. Hyperparameter tuning is automation, not magic: you use
    it to automate experiments that you would otherwise have run by hand, but you
    still need to handpick experiment configurations that have the potential to yield
    good metrics.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，进行超参数调整并不能替代熟悉模型架构最佳实践。随着选择数量的增加，搜索空间呈组合增长，因此将所有内容转化为超参数并让调谐器进行排序将是非常昂贵的。您需要聪明地设计正确的搜索空间。超参数调整是自动化的，而不是魔术：您使用它来自动化您本来需要手动运行的实验，但您仍然需要手动选择具有潜力产生良好指标的实验配置。
- en: The good news is that by leveraging hyperparameter tuning, the configuration
    decisions you have to make graduate from microdecisions (what number of units
    do I pick for this layer?) to higher-level architecture decisions (should I use
    residual connections throughout this model?). And although microdecisions are
    specific to a certain model and a certain dataset, higher-level decisions generalize
    better across different tasks and datasets. For instance, pretty much every image
    classification problem can be solved via the same sort of search-space template.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，通过利用超参数调整，您需要做出的配置决策从微观决策（我应该为这个层选择多少个单元？）升级到更高级的架构决策（我应该在整个模型中使用残差连接吗？）。尽管微观决策是特定于某个模型和某个数据集的，但更高级的决策在不同的任务和数据集中更容易推广。例如，几乎每个图像分类问题都可以通过相同类型的搜索空间模板解决。
- en: Following this logic, KerasTuner attempts to provide *premade search spaces*
    that are relevant to broad categories of problems, such as image classification.
    Just add data, run the search, and get a pretty good model. You can try the hypermodels
    kt$applications$HyperXception and kt$applications$HyperResNet, which are effectively
    tunable versions of Keras Applications models.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 按照这个逻辑，KerasTuner尝试提供与广泛问题类别相关的*预制搜索空间*，比如图像分类。只需添加数据，运行搜索，就可以得到一个相当不错的模型。您可以尝试超模型kt$applications$HyperXception和kt$applications$HyperResNet，它们是Keras应用模型的可调版本。
- en: 'THE FUTURE OF HYPERPARAMETER TUNING: AUTOMATED MACHINE LEARNING'
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 超参数调整的未来：自动化机器学习
- en: Currently, most of your job as a deep learning engineer consists of munging
    data with R scripts and then tuning the architecture and hyperparameters of a
    deep network at length to get a working model, or even to get a state-of-the-art
    model, if you are that ambitious. Needless to say, that isn’t an optimal setup.
    But automation can help, and it won’t stop merely at hyperparameter tuning.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，作为深度学习工程师，你的大部分工作都是用 R 脚本处理数据，然后长时间调整深度网络的架构和超参数，以获得一个可用的模型，甚至是一个最先进的模型，如果你有这个雄心的话。不用说，这不是一个最佳的设置。但自动化可以帮助，而且不仅仅在超参数调整方面停止。
- en: Searching over a set of possible learning rates or possible layer sizes is just
    the first step. We can also be far more ambitious and attempt to generate the
    *model architecture* itself from scratch, with as few constraints as possible,
    such as via reinforcement learning or genetic algorithms. In the future, entire
    end-to-end machine learning pipelines will be automatically generated, rather
    than handcrafted by engineer-artisans. This is called automated machine learning,
    or *AutoML*. You can already leverage libraries like AutoKeras [(https://github.com/keras-team/autokeras)](https://www.github.com/keras-team/autokeras)
    to solve basic machine learning problems with very little involvement on your
    part.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索可能的学习速率或可能的层大小只是第一步。我们也可以更加雄心勃勃，尝试从头开始生成*模型架构*，尽可能少地受到约束，例如通过强化学习或遗传算法。未来，整个端到端的机器学习流水线将自动生成，而不是由工程师手工制作。这被称为自动化机器学习，或者*AutoML*。你已经可以利用像
    AutoKeras [(https://github.com/keras-team/autokeras)](https://www.github.com/keras-team/autokeras)
    这样的库来解决基本的机器学习问题，而几乎不需要你的参与。
- en: Today, AutoML is still in its early days, and it doesn’t scale to large problems.
    But when AutoML becomes mature enough for widespread adoption, the jobs of machine
    learning engineers won’t disappear—rather, engineers will move up the value-creation
    chain. They will begin to put much more effort into data curation, crafting complex
    loss functions that truly reflect business goals, as well as understanding how
    their models impact the digital ecosystems in which they’re deployed (such as
    the users who consume the model’s predictions and generate the model’s training
    data). These are problems that only the largest companies can afford to consider
    at present.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，AutoML 还处于起步阶段，并且不适用于大规模问题。但当 AutoML 足够成熟，可以广泛采用时，机器学习工程师的工作不会消失——相反，工程师将向价值创造链上移动。他们将开始更多地投入数据筛选、打造真正反映业务目标的复杂损失函数，以及理解他们的模型如何影响部署它们的数字生态系统（例如使用模型预测的用户以及生成模型训练数据的用户）。这些是目前只有最大的公司才能考虑的问题。
- en: Always look at the big picture, focus on understanding the fundamentals, and
    keep in mind that the highly specialized tedium will eventually be automated away.
    See it as a gift—greater productivity for your workflows—and not as a threat to
    your own relevance. It shouldn’t be your job to tune knobs endlessly.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 时刻着眼于整体，专注于理解基础知识，并记住高度专业化的枯燥工作最终将被自动化。将其视为一份礼物——为你的工作流程带来更大的生产力——而不是对你自身影响的威胁。调整旋钮无休止地不应该是你的工作。
- en: 13.1.2 Model ensembling
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.1.2 模型集成
- en: Another powerful technique for obtaining the best possible results on a task
    is *model ensembling*. Ensembling consists of pooling the predictions of a set
    of different models to produce better predictions. If you look at machine learning
    competitions, in particular, on Kaggle, you’ll see that the winners use very large
    ensembles of models that inevitably beat any single model, no matter how good.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 获得任务上最佳结果的另一种强大技术是*模型集成*。集成包括汇总一组不同模型的预测结果以产生更好的预测。如果你看看机器学习竞赛，特别是在 Kaggle 上，你会发现获胜者使用非常庞大的模型集合，无论这些模型有多么好，都必然会击败任何单一模型。
- en: 'Ensembling relies on the assumption that different well-performing models trained
    independently are likely to be good for *different reasons*: each model looks
    at slightly different aspects of the data to make its predictions, getting part
    of the “truth” but not all of it. You may be familiar with the ancient parable
    of the blind men and the elephant: a group of blind men come across an elephant
    for the first time and try to understand what the elephant is by touching it.
    Each man touches a different part of the elephant’s body—just one part, such as
    the trunk or a leg. Then the men describe to each other what an elephant is: “It’s
    like a snake,” “Like a pillar or a tree,” and so on. The blind men are essentially
    machine learning models trying to understand the manifold of the training data,
    each from their own perspective, using their own assumptions (provided by the
    unique architecture of the model and the unique random weight initialization).
    Each of them gets part of the truth of the data, but not the whole truth. By pooling
    their perspectives, you can get a far more accurate description of the data. The
    elephant is a combination of parts: not any single blind man gets it quite right,
    but, interviewed together, they can tell a fairly accurate story.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 集成依赖于这样的假设：训练独立的不同表现良好的模型很可能是出于*不同的原因*而优秀的：每个模型都从稍微不同的角度观察数据的各个方面，以进行预测，从而获取“真相”的一部分，但并非全部。你可能熟悉盲人摸象的古老寓言：一群盲人第一次遇到大象，并试图通过触摸来了解大象是什么。每个人触摸大象的不同部分，比如象鼻或一条腿。然后，这些人互相描述大象是什么：“它像一条蛇”，“像一根柱子或一棵树”，等等。这些盲人本质上就是机器学习模型，试图从自己的角度，使用自己的假设（由模型的独特架构和独特的随机权重初始化提供）来理解训练数据的多样性。他们各自都得到了数据的部分真相，但并非全部真相。通过汇集他们的视角，你可以获得更准确的数据描述。大象是由部分组成的：没有一个盲人完全搞对了，但如果他们一起接受采访，他们可以讲出相当准确的故事。
- en: 'Let’s use classification as an example. The easiest way to pool the predictions
    of a set of classifiers (to *ensemble the classifiers*) is to average their predictions
    at inference time:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以分类为例。汇集一组分类器的预测（*集成分类器*）的最简单方法是在推断时对其预测求平均：
- en: preds_a <- model_a %>% predict(x_val))➊
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: preds_a <- model_a %>% 预测(x_val))➊
- en: preds_b <- model_b %>% predict(x_val)➊
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: preds_b <- model_b %>% 预测(x_val)➊
- en: preds_c <- model_c %>% predict(x_val)➊
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: preds_c <- model_c %>% 预测(x_val)➊
- en: preds_d <- model_d %>% predict(x_val)➊
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: preds_d <- model_d %>% 预测(x_val)➊
- en: final_preds <-
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: final_preds <-
- en: 0.25 * (preds_a + preds_b + preds_c + preds_d)➋
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 0.25 * (preds_a + preds_b + preds_c + preds_d)➋
- en: ➊ **Use four different models to compute initial predictions.**
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **使用四种不同的模型计算初始预测。**
- en: ➋ **This new prediction array should be more accurate than any of the initial
    ones.**
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **这个新的预测数组应该比任何初始预测更准确。**
- en: However, this will work only if the classifiers are more or less equally good.
    If one of them is significantly worse than the others, the final predictions may
    not be as good as the best classifier of the group.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，这只有在分类器大致相当优秀的情况下才会奏效。如果其中一个分类器明显比其他分类器差，那么最终预测可能不如该组中的最佳分类器。
- en: 'A smarter way to ensemble classifiers is to do a weighted average, where the
    weights are learned on the validation data—typically, the better classifiers are
    given a higher weight, and the worse classifiers are given a lower weight. To
    search for a good set of ensembling weights, you can use random search or a simple
    optimization algorithm, such as the Nelder–Mead algorithm:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 集成分类器的更智能的方式是进行加权平均，其中权重是在验证数据上学习得到的——通常，更好的分类器分配更高的权重，而更差的分类器分配更低的权重。要搜索一组良好的集成权重，可以使用随机搜索或简单的优化算法，比如
    Nelder–Mead 算法：
- en: preds_a <- model_a %>% predict(x_val)
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: preds_a <- model_a %>% 预测(x_val)
- en: preds_b <- model_b %>% predict(x_val)
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: preds_b <- model_b %>% 预测(x_val)
- en: preds_c <- model_c %>% predict(x_val)
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: preds_c <- model_c %>% 预测(x_val)
- en: preds_d <- model_d %>% predict(x_val)
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: preds_d <- model_d %>% 预测(x_val)
- en: final_preds <
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: final_preds <
- en: (0.5 * preds_a) + (0.25 * preds_b) +
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: (0.5 * preds_a) + (0.25 * preds_b) +
- en: (0.1 * preds_c) + (0.15 * preds_d)➊
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: (0.1 * preds_c) + (0.15 * preds_d)➊
- en: ➊ **These weights (0.5, 0.25, 0.1, 0.15) are assumed to be learned empirically.**
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **这些权重（0.5、0.25、0.1、0.15）被假设是基于经验学习得到的。**
- en: 'Many possible variants exist: you can do an average of an exponential of the
    predictions, for instance. In general, a simple weighted average with weights
    optimized on the validation data provides a very strong baseline.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 存在许多可能的变体：例如，你可以对预测进行指数平均。一般来说，通过在验证数据上优化权重的简单加权平均法可以提供一个非常强大的基线。
- en: The key to making ensembling work is the *diversity* of the set of classifiers.
    Diversity is strength. If all the blind men only touched the elephant’s trunk,
    they would agree that elephants are like snakes, and they would forever stay ignorant
    of the truth of the elephant. Diversity is what makes ensembling work. In machine
    learning terms, if all of your models are biased in the same way, your ensemble
    will retain this same bias. If your models are *biased in different ways*, the
    biases will cancel each other out, and the ensemble will be more robust and more
    accurate.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 使集成工作的关键是分类器集合的*多样性*。多样性是力量。如果所有的盲人只触摸大象的鼻子，他们会认为大象和蛇一样，他们将永远无法真正了解大象的真相。多样性是使集成方法工作的原因。在机器学习的术语中，如果你的所有模型在同一方面有偏见，那么你的集成将保留这种偏见。如果你的模型在*不同方面有偏见*，这些偏见将互相抵消，集成将更强大、更准确。
- en: For this reason, you should ensemble models that are *as good as possible* while
    being *as different as possible*. This typically means using very different architectures
    or even different brands of machine learning approaches. One thing that is largely
    not worth doing is ensembling the same network trained several times independently,
    from different random initializations. If the only difference between your models
    is their random initialization and the order in which they were exposed to the
    training data, then your ensemble will be low diversity and will provide only
    a tiny improvement over any single model.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 出于这个原因，你应该使用*尽可能好且尽可能不同的*模型进行集成。通常这意味着使用非常不同的架构甚至不同品牌的机器学习方法。但通常不值得将多个独立训练的相同网络进行集成，只是其随机初始化和接触训练数据的顺序不同而已。如果你的模型之间唯一的区别是它们的随机初始化和接触训练数据的顺序，那么你的集成将缺乏多样性，并且只会比单个模型提供微小的改进。
- en: 'One thing I have found to work well in practice—but that doesn’t generalize
    to every problem domain—is using an ensemble of tree-based methods (such as random
    forests or gradient-boosted trees) and deep neural networks. In 2014, Andrey Kolev
    and I took fourth place in the Higgs boson decay detection challenge on Kaggle
    [(http://www.kaggle.com/c/higgs-boson)](http://www.kaggle.com/c/higgs-boson) using
    an ensemble of various tree models and deep neural networks. Remarkably, one of
    the models in the ensemble originated from a different method than the others
    (it was a regularized greedy forest), and it had a significantly worse score than
    the others. Unsurprisingly, it was assigned a small weight in the ensemble. But
    to our surprise, it turned out to improve the overall ensemble by a large factor,
    because it was so different from every other model: it provided information that
    the other models didn’t have access to. That’s precisely the point of ensembling.
    It’s not so much about how good your best model is; it’s about the diversity of
    your set of candidate models.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中我发现，有一种方法非常有效，但并不适用于所有问题领域，那就是使用一组基于树的方法（如随机森林或梯度提升树）和深度神经网络的集成方法。在2014年，我和安德烈·科列夫在Kaggle的希格斯玻色子衰变检测挑战赛中获得了第四名，使用了各种树模型和深度神经网络的集成方法。值得注意的是，集成中的一个模型来源于不同的方法（它是一个经过正则化的贪婪森林），与其他模型相比得分显著较低。毫不奇怪，它在集成中被分配了较小的权重。但令我们惊讶的是，它实际上显著提高了整体集成的性能，因为它与其他模型非常不同：它提供了其他模型没有访问权的信息。这正是集成的目的所在。关键不在于你最好的模型有多好，而在于候选模型集合的多样性。
- en: 13.2 Scaling-up model training
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.2 模型训练的扩展
- en: 'Recall the “loop of progress” concept we introduced in chapter 7: the quality
    of your ideas is a function of how many refinement cycles they’ve been through
    (see [figure 13.1](#fig13-1)). And the speed at which you can iterate on an idea
    is a function of how fast you can set up an experiment, how fast you can run that
    experiment, and, finally, how well you can analyze the resulting data.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾我们在第7章介绍的“进步循环”概念：你的思想质量取决于它们经历了多少次优化迭代（见[图13.1](#fig13-1)）。而你能够迭代一个想法的速度取决于你能够多快地建立一个实验，运行这个实验的速度，以及你能够分析所得数据的好坏。
- en: '![Image](../images/f0464-01.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0464-01.jpg)'
- en: '**Figure 13.1 The loop of progress**'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '**图13.1 进步循环**'
- en: As you develop your expertise with the Keras API, how fast you can code up your
    deep learning experiments will cease to be the bottleneck of this progress cycle.
    The next bottleneck will become the speed at which you can train your models.
    Fast training infrastructure means that you can get your results back in 10–15
    minutes, and hence, you can go through dozens of iterations every day. Faster
    training directly improves the *quality* of your deep learning solutions.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 随着您对 Keras API 的专业知识的发展，您能够编写深度学习实验的速度将不再是进展周期的瓶颈。下一个瓶颈将变为您能够训练模型的速度。快速的训练基础设施意味着您可以在
    10 到 15 分钟内收到结果，因此，您每天可以进行数十次迭代。更快的训练直接提高了您深度学习解决方案的*质量*。
- en: 'In this section, you’ll learn about three ways you can train your models faster:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将学习三种可以加速模型训练的方法：
- en: Mixed-precision training, which you can use even with a single GPU
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混合精度训练，即使只使用单个 GPU 也可以使用
- en: Training on multiple GPUs
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在多个 GPU 上训练
- en: Training on TPU
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 TPU 上训练
- en: Let’s go.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: 13.2.1 Speeding up training on GPU with mixed precision
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用混合精度加速 GPU 上的训练的方法
- en: What if I told you there’s a simple technique you can use to speed up the training
    of almost any model by up to 3×, basically for free? It seems too good to but
    true, and yet, such a trick does exist—it’s *mixed-precision training*. To understand
    how it works, we first need to take a look at the notion of “precision” in computer
    science.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我告诉您有一种简单的技术可以免费将几乎任何模型的训练速度加快至多 3 倍，您会怎么想？听起来太好了，但事实却是如此，这样的技巧确实存在 —— 它是*混合精度训练*。要理解它的工作原理，我们首先需要看一下计算机科学中“精度”的概念。
- en: UNDERSTANDING FLOATING-POINT PRECISION
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**浮点数精度理解**'
- en: 'Precision is to numbers what resolution is to images. Because computers can
    process only ones and zeros, any number seen by a computer has to be encoded as
    a binary string. For instance, you may be familiar with uint8 integers, which
    are integers encoded on eight bits: 00000000 represents 0 in uint8, and 11111111
    represents 255\. To represent integers beyond 255, you’d need to add more bits—eight
    isn’t enough. Most integers are stored on 32 bits, with which you can represent
    signed integers ranging from –2147483648 to 2147483647.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 精度对于数字来说就像对于图像的分辨率一样重要。因为计算机只能处理 1 和 0，计算机看到的任何数字都必须被编码为二进制字符串。例如，您可能熟悉 uint8
    整数，这些整数是编码在八位上的整数：00000000 表示 uint8 中的 0，11111111 表示 255。要表示超过 255 的整数，您需要添加更多位数
    —— 八位不够用。大多数整数存储在 32 位上，您可以使用它来表示范围从 -2147483648 到 2147483647 的有符号整数。
- en: 'Floating-point numbers are the same. In mathematics, real numbers form a continuous
    axis: there’s an infinite number of points in between any two numbers. You can
    always zoom in on the axis of reals. In computer science, this isn’t true: there’s
    a finite number of intermediate points between 3 and 4, for instance. How many?
    Well, it depends on the *precision* you’re working with—the number of bits you’re
    using to store a number. You can zoom up to only a certain resolution. There are
    three levels of precision you’d typically use:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 浮点数也是一样的。在数学中，实数形成一个连续的轴：在任意两个数字之间有无穷多个点。您可以始终在实轴上放大。在计算机科学中，这不是真的：在3和4之间有一个有限数量的中间点。有多少个？好吧，这取决于您正在使用的*精度*——用于存储数字的位数。您只能放大到某个分辨率。通常会使用三个精度级别：
- en: Half precision, or float16, where numbers are stored on 16 bits
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 半精度，或者 float16，其中数字存储在16位上
- en: Single precision, or float32, where numbers are stored on 32 bits
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单精度，或者 float32，其中数字存储在32位上
- en: Double precision, or float64, where numbers are stored on 64 bit
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双精度，或者 float64，其中数字存储在 64 位上
- en: The way to think about the resolution of floating-point numbers is in terms
    of the smallest distance between two arbitrary numbers that you’ll be able to
    safely process. In single precision, that’s around 1e-7\. In double precision,
    that’s around 1e-16\. And in half precision, it’s only 1e-3.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 关于浮点数精度的思考方式是以两个任意数字之间的最小距离为安全处理的基准。在单精度中，约为1e-7。在双精度中，约为1e-16。而在半精度中，只有1e-3。
- en: 'Almost every model you’ve seen in this book so far used single-precision numbers:
    it stored its state as float32 weight variables and ran its computations on float32
    inputs. That’s enough precision to run the forward and backward pass of a model
    without losing any information—particularly when it comes to small gradient updates
    (recall that the typical learning rate is 1e-3, and it’s pretty common to see
    weight updates on the order of 1e-6).'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本书中你看到的几乎每个模型都使用了单精度数字：它将其状态存储为 float32 权重变量，并在 float32 输入上运行其计算。这足以在不丢失任何信息的情况下运行模型的前向和反向传播——特别是当涉及到小梯度更新时（回想一下，典型的学习速率为
    1e-3，看到梯度更新量在 1e-6 的数量级是非常常见的）。
- en: You could also use float64, though that would be wasteful—operations like matrix
    multiplication or addition are much more expensive in double precision, so you’d
    be doing twice as much work for no clear benefits. But you could not do the same
    with float16 weights and computation; the gradient descent process wouldn’t run
    smoothly, because you couldn’t represent small gradient updates of around 1e-5
    or 1e-6.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用 float64，尽管这样做会很浪费——在双精度中，矩阵乘法或加法等操作要显得更昂贵，因此你将做两倍的工作而没有明显的好处。但是你不能使用
    float16 权重和计算做同样的事情；梯度下降过程不会顺利运行，因为你不能表示约为 1e-5 或 1e-6 的小梯度更新。
- en: 'You can, however, use a hybrid approach: that’s what mixed precision is about.
    The idea is to leverage 16-bit computations in places where precision isn’t an
    issue and to work with 32-bit values in other places to maintain numerical stability.
    Modern GPUs and TPUs feature specialized hardware that can run 16-bit operations
    much faster and use less memory than equivalent 32-bits operations. By using these
    lower-precision operations whenever possible, you can speed up training on those
    devices by a significant factor. Meanwhile, by maintaining the precision-sensitive
    parts of the model in single precision, you can get these benefits without meaningfully
    impacting model quality.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，你可以使用混合方法：这就是混合精度的含义。其思想是在不需要精度的地方利用 16 位计算，并在其他地方使用 32 位值以保持数值稳定性。现代 GPU
    和 TPU 具有专门的硬件，可以比等价的 32 位操作更快地运行 16 位操作，并且使用的内存更少。通过尽可能使用这些低精度操作，你可以显著加速这些设备上的训练。同时，通过在单精度中保持对模型的精度敏感部分，你可以在不实质影响模型质量的情况下获得这些好处。
- en: '**A note on floating-point encoding**'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '**关于浮点数编码的说明**'
- en: 'A counterintuitive fact about floating-point numbers is that representable
    numbers are not uniformly distributed. Larger numbers have lower precision: there
    are the same number of representable values between 2^N and 2^(N + 1) as there
    are between 1 and 2, for any *N*. That’s because floating-point numbers are encoded
    in three parts—the sign, the significant value (called the “mantissa”), and the
    exponent, in the form'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 浮点数的一个反直觉事实是可表示的数字并不均匀分布。较大的数字精度较低：在任意*N*的情况下，2^N 和 2^(N + 1) 之间的可表示值与1和2之间的可表示值相同。这是因为浮点数分为三个部分编码——符号、有效值（称为“尾数”）和指数，形式如下
- en: <sign> * (2 ^ (<exponent> - 127)) * 1.<mantissa>
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: <sign> * (2 ^ (<exponent> - 127)) * 1.<mantissa>
- en: For example, the following figure shows how you would encode the closest float32
    value approximating Pi.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以下图显示了如何编码近似 Pi 的最接近的 float32 值。
- en: '![Image](../images/f0466-01.jpg)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0466-01.jpg)'
- en: value = +1 * (2 ^ (128 - 127)) * 1.570796370562866
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: value = +1 * (2 ^ (128 - 127)) * 1.570796370562866
- en: value = 3.1415927410125732
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: value = 3.1415927410125732
- en: The number Pi encoded in single precision via a sign bit, an integer exponent,
    and an integer mantissa
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 通过符号位、整数指数和整数尾数编码的 Pi 数值
- en: For this reason, the numerical error incurred when converting a number to its
    floating-point representation can vary wildly depending on the exact value considered,
    and the error tends to get larger for numbers with a large absolute value.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，将数字转换为其浮点表示时产生的数值误差可能会因考虑的确切值而大相径庭，并且对于绝对值较大的数字，误差往往会变得更大。
- en: 'And those benefits are considerable: on modern NVIDIA GPUs, mixed precision
    can speed up training by up to 3×. It’s also beneficial when training on a TPU
    (a subject we’ll get to in a bit), where it can speed up training by up to 60%.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 而这些好处是相当可观的：在现代 NVIDIA GPU 上，混合精度可以将训练加速最多 3 倍。当在 TPU 上进行训练时（稍后我们会讨论此问题），也会有所益处，它可以将训练加速最多
    60%。
- en: '**Beware of dtype defaults**'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意 dtype 的默认值**'
- en: 'Single precision is the default floating-point type throughout Keras and TensorFlow:
    the tensor or variable you create will be in float32 unless you specify otherwise.
    For R arrays, however, the default is float64!'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 单精度是 Keras 和 TensorFlow 中的默认浮点类型：您创建的张量或变量将为 float32，除非您另行指定。但是，对于 R 数组，默认值是
    float64！
- en: 'Converting an R array to a TensorFlow tensor will result in a float64 tensor,
    which may not be what you want:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 将 R 数组转换为 TensorFlow 张量将导致一个 float64 张量，这可能不是您想要的：
- en: r_array <- base::array(0, dim = c(2, 2))
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: r_array <- base::array(0, dim = c(2, 2))
- en: tf_tensor <- tensorflow::as_tensor(r_array)
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: tf_tensor <- tensorflow::as_tensor(r_array)
- en: tf_tensor$dtype
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: tf_tensor$dtype
- en: tf.float64
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: tf.float64
- en: 'Remember to be explicit about data types when converting R arrays:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在转换 R 数组时，请明确指定数据类型：
- en: r_array <- base::array(0, dim = c(2, 2))
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: r_array <- base::array(0, dim = c(2, 2))
- en: tf_tensor <- tensorflow::as_tensor(r_array, dtype = "float32")➊
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: tf_tensor <- tensorflow::as_tensor(r_array, dtype = "float32")➊
- en: tf_tensor$dtype
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: tf_tensor$dtype
- en: tf.float32
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: tf.float32
- en: ➊ **Specify the dtype explicitly.**
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **显式指定 dtype。**
- en: Note that when you call the Keras fit() method with R arrays, it will automatically
    cast them to k_floatx()—float32 by default.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当您使用 R 数组调用 Keras 的 fit() 方法时，它将自动将它们转换为 k_floatx()——默认情况下为 float32。
- en: MIXED-PRECISION TRAINING IN PRACTICE
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在实践中的混合精度训练
- en: 'When training on a GPU, you can turn on mixed precision like this:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GPU 上进行训练时，可以像这样启用混合精度：
- en: keras::keras$mixed_precision$set_global_policy("mixed_float16")➊
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: keras::keras$mixed_precision$set_global_policy("mixed_float16")➊
- en: ➊ **keras::keras is the Python module imported by reticulate.**
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **keras::keras 是由 reticulate 导入的 Python 模块。**
- en: Typically, most of the forward pass of the model will be done in float16 (with
    the exception of numerically unstable operations like softmax), whereas the weights
    of the model will be stored and updated in float32.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，模型的大部分前向传递将在 float16 中完成（除了 softmax 等数值不稳定的操作），而模型的权重将以 float32 存储和更新。
- en: Keras layers have a variable_dtype and a compute_dtype property. By default,
    both of these are set to float32. When you turn on mixed precision, the compute_
    dtype of most layers switches to float16, and those layers will cast their inputs
    to float16 and will perform their computations in float16 (using half-precision
    copies of the weights). However, because their variable_dtype is still float32,
    their weights will be able to receive accurate float32 updates from the optimizer,
    as opposed to half-precision updates.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 层具有 variable_dtype 和 compute_dtype 属性。默认情况下，这两个属性都设置为 float32。当您启用混合精度时，大多数层的
    compute_ dtype 将切换为 float16，并且这些层将会将它们的输入转换为 float16，并在 float16 中执行计算（使用权重的半精度副本）。然而，由于它们的
    variable_dtype 仍然是 float32，所以它们的权重将能够从优化器接收准确的 float32 更新，而不是半精度更新。
- en: Note that some operations may be numerically unstable in float16 (in particular,
    softmax and cross-entropy). If you need to opt out of mixed precision for a specific
    layer, just pass the argument dtype = “float32” to the constructor of this layer.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，某些操作在 float16 下可能不稳定（特别是 softmax 和交叉熵）。如果您需要针对特定层退出混合精度，只需将参数 dtype = "float32"
    传递给该层的构造函数即可。
- en: 13.2.2 Multi-GPU training
  id: totrans-280
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2.2 多 GPU 训练
- en: Although GPUs are getting more powerful every year, deep learning models are
    getting increasingly larger, requiring ever more computational resources. Training
    on a single GPU puts a hard bound on how fast you can move. The solution? You
    could simply add more GPUs and start doing *multi-GPU distributed training*.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管每年 GPU 的性能都在增强，但深度学习模型变得越来越大，需要越来越多的计算资源。在单个 GPU 上进行训练会限制您的运行速度。解决方法？您可以简单地添加更多的
    GPU 并开始进行*多 GPU 分布式训练*。
- en: 'There are two ways to distribute computation across multiple devices: *data
    parallelism* and *model parallelism*.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种方式可以将计算分布到多个设备上：*数据并行* 和 *模型并行*。
- en: With data parallelism, a single model is replicated on multiple devices or multiple
    machines. Each of the model replicas processes different batches of data, and
    then they merge their results.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 使用数据并行时，单个模型在多个设备或多台机器上复制。每个模型副本处理不同的数据批次，然后它们合并结果。
- en: With model parallelism, different parts of a single model run on different devices,
    processing a single batch of data together at the same time. This works best with
    models that have a naturally parallel architecture, such as models that feature
    multiple branches.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 使用模型并行时，单个模型的不同部分在不同的设备上运行，同时处理单个数据批次。这对具有自然并行架构的模型效果最佳，例如具有多个分支的模型。
- en: 'In practice, model parallelism is used only for models that are too large to
    fit on any single device: it isn’t used as a way to speed up training of regular
    models but as a way to train larger models. We won’t cover model parallelism in
    these pages; instead, we’ll focus on what you’ll be using most of the time: data
    parallelism. Let’s take a look at how it works.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，模型并行仅用于那些太大而无法放在任何单个设备上的模型：它不是用于加速常规模型训练的方法，而是用于训练更大模型的方法。我们不会在这些页面上涵盖模型并行性；相反，我们将专注于你大部分时间都会使用的数据并行性。让我们看看它是如何工作的。
- en: GETTING YOUR HANDS ON TWO OR MORE GPUS
  id: totrans-286
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 获取两个或更多 GPU
- en: 'First, you need to get access to several GPUs. You will need to do one of two
    things:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要获取几个 GPU 的访问权限。你需要做以下两件事之一：
- en: Acquire two to four GPUs, mount them on a single machine (it will require a
    beefy power supply), and install CUDA drivers, cuDNN, and so on. For most people,
    this isn’t the best option.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获得两到四个 GPU，将它们安装在一台机器上（这将需要一个强大的电源供应器），并安装 CUDA 驱动程序、cuDNN 等。对于大多数人来说，这不是最佳选择。
- en: Rent a multi-GPU virtual machine (VM) on Google Cloud, Azure, or AWS You’ll
    be able to use VM images with preinstalled drivers and software, and you’ll have
    very little setup overhead. This is likely the best option for anyone who isn’t
    training models 24/7.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Google Cloud、Azure 或 AWS 上租用一个多 GPU 虚拟机（VM）。你可以使用预安装了驱动程序和软件的 VM 映像，并且几乎没有设置开销。对于那些不是全天候训练模型的人来说，这可能是最佳选择。
- en: We won’t cover the details of how to spin up multi-GPU cloud VMs, because such
    instructions would be relatively short-lived, and this information is readily
    available online.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会涵盖如何启动多 GPU 云 VM 的详细信息，因为这样的说明相对来说是短暂的，并且这些信息在网上很容易找到。
- en: SINGLE-HOST, MULTIDEVICE SYNCHRONOUS TRAINING
  id: totrans-291
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 单主机、多设备同步训练
- en: 'Once you’re able to call library(tensorflow) on a machine with multiple GPUs,
    you’re seconds away from training a distributed model. It works like this:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你能在拥有多个 GPU 的计算机上调用 library(tensorflow)，你距离训练分布式模型只有几秒钟的时间。它的工作方式如下：
- en: library(tensorflow)
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: library(tensorflow)
- en: strategy <- tf$distribute$MirroredStrategy()➊
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: strategy <- tf$distribute$MirroredStrategy()➊
- en: cat("Number of devices:", strategy$num_replicas_in_sync, "\n")
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: cat("设备数量:", strategy$num_replicas_in_sync, "\n")
- en: with(strategy$scope(), { ➋
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: with(strategy$scope(), { ➋
- en: model <- get_compiled_model()➌
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: model <- get_compiled_model()➌
- en: '})'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '})'
- en: model %>% fit(➍
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% fit(➍
- en: train_dataset,
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: train_dataset,
- en: epochs = 100,
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 100,
- en: validation_data = val_dataset,
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: validation_data = val_dataset,
- en: callbacks = callbacks
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: callbacks = callbacks
- en: )
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: ➊ **Create a "distribution strategy" object. MirroredStrategy() should be your
    go-to solution.**
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **创建一个“分发策略”对象。MirroredStrategy() 应该是你的首选解决方案。**
- en: ➋ **Use it to open a "strategy scope."**
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **使用它来打开“策略范围”。**
- en: ➌ **Everything that creates variables should be under the strategy scope. In
    general, this is only model construction and compile().**
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **所有创建变量的操作都应该在策略范围内。一般来说，这只涉及模型构建和编译（compile()）。**
- en: ➍ **Train the model on all available devices.**
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **在所有可用设备上训练模型。**
- en: 'These few lines implement the most common training setup: *single-host, multidevice
    synchronous training*, also known in TensorFlow as the “mirrored distribution
    strategy.” “Single host” means that the different GPUs considered are all on a
    single machine (as opposed to a cluster of many machines, each with its own GPU,
    communicating over a network). “Synchronous training” means that the state of
    the per-GPU model replicas stays the same at all times—there are variants of distributed
    training where this isn’t the case.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 这几行代码实现了最常见的训练设置：*单主机、多设备同步训练*，在 TensorFlow 中也称为“镜像分布策略”。“单主机”意味着考虑的不同 GPU 都在一台机器上（而不是许多机器的集群，每台机器都有自己的
    GPU，在网络上进行通信）。“同步训练”意味着每个 GPU 模型副本的状态始终保持一致——在分布式训练的变体中，情况可能并非如此。
- en: 'When you open a MirroredStrategy() scope and build your model within it, the
    MirroredStrategy() object will create one model copy (replica) on each available
    GPU. For example, if you have two GPUs, then each step of training unfolds in
    the following way (see [figure 13.2](#fig13-2)):'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在 MirroredStrategy() 范围内打开并在其中构建模型时，MirroredStrategy() 对象将在每个可用 GPU 上创建一个模型副本（replica）。例如，如果你有两个
    GPU，那么每一步训练的过程如下（见 [图 13.2](#fig13-2)）：
- en: '**1** A batch of data (called the *global batch*) is drawn from the dataset.'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**1** 从数据集中抽取一批数据（称为*全局批次*）。'
- en: '**2** It gets split into two different sub-batches (called *local batches*).
    For instance, if the global batch has 256 samples, each of the two local batches
    will have 128 samples. Because you want local batches to be large enough to keep
    the GPU busy, the global batch size typically needs to be very large.'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**2** 它被分成两个不同的小批次（称为*本地批次*）。例如，如果全局批次有256个样本，那么每个本地批次将有128个样本。因为你希望本地批次足够大以保持GPU繁忙，所以全局批次大小通常需要非常大。'
- en: '**3** Each of the two replicas processes one local batch, independently, on
    its own device: they run a forward pass and then a backward pass. Each replica
    outputs a “weight delta” describing by how much to update each weight variable
    in the model, given the gradient of the previous weights with respect to the loss
    of the model on the local batch.'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**3** 每个副本独立地处理一个本地批次：它们在自己的设备上运行正向传递，然后向后传递。每个副本输出描述在本地批次的模型损失对先前权重的梯度基础上，如何更新模型中的每个权重变量的“权重增量”。'
- en: '**4** The weight deltas originating from local gradients are efficiently merged
    across the two replicas to obtain a global delta, which is applied to all replicas.
    Because this is done at the end of every step, the replicas always stay in sync:
    their weights are always equal.'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**4** 本地梯度产生的权重增量在两个副本之间高效合并，以获得全局增量，该增量应用于所有副本。因为这是在每个步骤结束时完成的，所以副本始终保持同步：它们的权重始终相等。'
- en: '![Image](../images/f0469-01.jpg)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0469-01.jpg)'
- en: '**Figure 13.2 One step of MirroredStrategy training: Each model replica computes
    local weight updates, which are then merged and used to update the state of all
    replicas.**'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 13.2 MirroredStrategy 训练的一步：每个模型副本计算本地权重更新，然后将这些更新合并并用于更新所有副本的状态。**'
- en: 'When doing distributed training, always provide your data as a TF Dataset object
    to guarantee best performance. (Passing your data as R arrays also works, because
    those are converted to TF Dataset objects by fit()). You should also make sure
    you leverage data prefetching: before passing the dataset to fit(), call dataset_prefetch(buffer_
    size). If you aren’t sure what buffer size to pick, try leaving the default value
    of tf$data$AUTOTUNE, which will pick a buffer size for you.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式训练时，始终将数据作为 TF 数据集对象提供，以保证最佳性能。（也可以将数据作为 R 数组传递，因为这些数组会被 fit() 转换为 TF 数据集对象。）你还应该确保充分利用数据预取：在将数据集传递给
    fit() 之前，调用 dataset_prefetch(buffer_size)。如果你不确定要选择什么缓冲区大小，可以尝试保留 tf$data$AUTOTUNE
    的默认值，它会为你选择缓冲区大小。
- en: Here’s a simple example.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个简单的示例。
- en: '**Listing 13.4 Building a model in a MirroredStrategy scope**'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 13.4 在 MirroredStrategy 范围内构建模型**'
- en: build_model <- function(input_size) {
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: build_model <- function(input_size) {
- en: resnet <- application_resnet50(weights = NULL,
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: resnet <- application_resnet50(weights = NULL,
- en: include_top = FALSE,
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: include_top = FALSE,
- en: pooling = "max")
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: pooling = "max")
- en: inputs <- layer_input(c(input_size, 3))
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: inputs <- layer_input(c(input_size, 3))
- en: outputs <- inputs %>%
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: outputs <- inputs %>%
- en: resnet_preprocess_input() %>%
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: resnet_preprocess_input() %>%
- en: resnet() %>%
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: resnet() %>%
- en: layer_dense(10, activation = "softmax")
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(10, activation = "softmax")
- en: model <- keras_model(inputs, outputs)
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model(inputs, outputs)
- en: model %>% compile(
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% compile(
- en: optimizer = "rmsprop",
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: optimizer = "rmsprop",
- en: loss = "sparse_categorical_crossentropy",
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: loss = "sparse_categorical_crossentropy",
- en: metrics = "accuracy"
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = "accuracy"
- en: )
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: model
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: model
- en: '}'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: strategy <- tf$distribute$MirroredStrategy()
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: strategy <- tf$distribute$MirroredStrategy()
- en: cat("Number of replicas:", strategy$num_replicas_in_sync, "\n")
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: cat("副本数量：", strategy$num_replicas_in_sync, "\n")
- en: 'Number of replicas: 2'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 副本数量：2
- en: with(strategy$scope(), {
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: with(strategy$scope(), {
- en: model <- build_model(input_size = c(32, 32))
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: model <- build_model(input_size = c(32, 32))
- en: '})'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '})'
- en: 'In this case, let’s train straight from R arrays in memory (which are efficiently
    converted to a TF Dataset by fit())—the CIFAR10 dataset:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，让我们直接从内存中的 R 数组（通过 fit() 效率高效地转换为 TF 数据集）来训练 CIFAR10 数据集：
- en: c(c(x_train, y_train), c(x_test, y_test)) %<-% dataset_cifar10()
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: c(c(x_train, y_train), c(x_test, y_test)) %<-% dataset_cifar10()
- en: model %>% fit(x_train, y_train, batch_size = 1024)➊
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% fit(x_train, y_train, batch_size = 1024)➊
- en: ➊ **Note that multi-GPU training requires large batch sizes to make sure the
    device stays well utilized.**
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **注意，多GPU训练需要大批量训练数据才能确保设备得到充分的利用。**
- en: 'In an ideal world, training on *N* GPUs would result in a speedup of factor
    *N*. In practice, however, distribution introduces some overhead, in particular,
    merging the weight deltas originating from different devices takes some time.
    The effective speedup you get is a function of the number of GPUs used:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 在理想的情况下，使用 *N* 个GPU进行训练将会导致速度提升 *N* 倍。然而，在实践中，分布式训练会引入一些开销，特别是，合并来自不同设备的权重增量需要一些时间。你获得的有效加速度取决于所使用的GPU数量：
- en: With two GPUs, the speedup stays close to 2×.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用两个GPU，加速度接近 2×。
- en: With four, the speedup is around 3.8×.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用四个GPU时，速度提升约为 3.8×。
- en: With eight, it’s around 7.3×
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用八个GPU时，速度提升约为 7.3×。
- en: This assumes that you’re using a large enough global batch size to keep each
    GPU used at full capacity. If your batch size is too small, the local batch size
    won’t be enough to keep your GPUs busy.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 这假设你使用了足够大的全局批次大小，以使每个GPU都保持满负荷运行。如果你的批次大小太小，本地批次大小将不足以使GPU保持忙碌状态。
- en: 13.2.3 TPU training
  id: totrans-352
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2.3 TPU 训练
- en: Beyond just GPUs, there is a trend in the deep learning world toward moving
    work-flows to increasingly specialized hardware designed specifically for deep
    learning workflows (such single-purpose chips are known as ASICs—application-specific
    integrated circuits). Various companies big and small are working on new chips,
    but today the most prominent effort along these lines is Google’s Tensor Processing
    Unit (TPU), which is available on Google Cloud and via Google Colab.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 除了GPU之外，在深度学习世界中存在一种趋势，即将工作流程转移到专门设计用于深度学习工作流程的越来越专业的硬件上（这些专用芯片被称为ASICs——特定应用集成电路）。各种大大小小的公司都在研发新的芯片，但如今这方面最突出的努力是Google的Tensor
    Processing Unit（TPU），它可在Google Cloud和Google Colab上使用。
- en: 'Training on a TPU does involve jumping through some hoops, but it can be worth
    the extra work: TPUs are really, really fast. Training on a TPU V2 will typically
    be 15× faster than training an NVIDIA P100 GPU.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 在TPU上进行训练确实涉及一些复杂的步骤，但额外的工作可能是值得的：TPU的速度非常快。在TPU V2上进行训练通常比训练NVIDIA P100 GPU快15倍。
- en: 'Here are some tips when using TPUs: when you’re using the GPU runtime in the
    cloud, your models have direct access to the GPU without you needing to do anything
    special. This isn’t true for the TPU runtime; there’s an extra step you need to
    take before you can start building a model: you need to connect to the TPU cluster.
    It works like this:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 使用TPU时有一些技巧：当你在云中使用GPU运行时，你的模型可以直接访问GPU，无需进行任何特殊操作。但对于TPU运行时，情况并非如此；在构建模型之前，你需要执行额外的步骤：连接到TPU集群。操作如下：
- en: tpu <- tf$distribute$cluster_resolver$TPUClusterResolver$connect()
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: tpu <- tf$distribute$cluster_resolver$TPUClusterResolver$connect()
- en: cat("Device:", tpu$master(), "\n")
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: cat("Device:", tpu$master(), "\n")
- en: strategy <- tf$distribute$TPUStrategy(tpu)➊
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 策略 <- tf$distribute$TPUStrategy(tpu)➊
- en: with(strategy$scope(), { … })
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: with(strategy$scope(), { … })
- en: ➊ **Use TPUStrategy() just like tf$distribute$MirroredStrategy().**
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **像使用tf$distribute$MirroredStrategy()一样使用TPUStrategy()。**
- en: You don’t have to worry too much about what this does—it’s just a little incantation
    that connects your runtime to the device. Open sesame.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 你不必太担心这是做什么的——这只是一个将你的运行时连接到设备的小咒语。打开吧。
- en: Much like in the case of multi-GPU training, using the TPU requires you to open
    a distribution strategy scope—in this case, a TPUStrategy() scope. TPUStrategy()
    fol-lows the same distribution template as MirroredStrategy()—the model is replicated
    once per TPU core, and the replicas are kept in sync.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 与多GPU训练类似，使用TPU也需要你打开一个分布策略作用域——在这种情况下是TPUStrategy()作用域。TPUStrategy()遵循与MirroredStrategy()相同的分布模板——模型会被复制一次，每个TPU核心一个，而且这些副本会保持同步。
- en: 'Note there’s something else a bit curious about the TPU runtime: it’s a two-VM
    setup, meaning that the VM that hosts your notebook runtime isn’t the same VM
    that the TPU lives in. Because of this, you won’t be able to train from files
    stored on the local disk (that is, on the disk linked to the VM that hosts the
    instance). The TPU runtime can’t read from there. You have two options for data
    loading:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，TPU运行时还有一点有趣的地方：它是一个双VM设置，这意味着托管笔记本运行时的VM与TPU所在的VM不同。因此，你将无法从存储在本地磁盘上的文件进行训练（即，从托管实例的VM链接的磁盘）。TPU运行时无法从那里读取。关于数据加载，你有两个选择：
- en: Train from data that lives in the memory of the VM (not on disk). If your data
    is in an R array, this is what you’re already doing.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从存储在VM内存中的数据进行训练（而不是在磁盘上）。如果你的数据是在R数组中，那么你已经在做这个了。
- en: Store the data in a Google Cloud Storage (GCS) bucket, and create a dataset
    that reads the data directly from the bucket, without downloading locally. The
    TPU runtime can read data from GCS. This is your only option for datasets that
    are too large to live entirely in memory
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据存储在Google Cloud Storage（GCS）存储桶中，并创建一个直接从存储桶读取数据的数据集，而不必从本地下载。TPU运行时可以从GCS读取数据。这是数据集太大无法完全存放在内存中的唯一选择。
- en: You’ll also notice that the first epoch takes a while to start. That’s because
    your model is getting compiled to something that the TPU can execute. Once that
    step is done, the training itself is blazing fast.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到第一个时期需要一段时间才能开始。那是因为你的模型正在被编译成TPU可以执行的东西。一旦这一步完成，训练本身就会飞快进行。
- en: '**Beware of I/O bottlenecks**'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意I/O瓶颈**'
- en: Because TPUs can process batches of data extremely quickly, the speed at which
    you can read data from GCS can easily become a bottleneck.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 因为TPU可以非常快地处理数据批次，你读取数据从GCS中的速度很容易成为瓶颈。
- en: If your dataset is small enough, you should keep it in the memory of the VM.
    You can do so by calling dataset_cache() on your dataset. That way, the data will
    be read from GCS only once.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你的数据集足够小，你应该将其保留在VM的内存中。你可以通过在数据集上调用dataset_cache()来实现。这样，数据将只从GCS读取一次。
- en: If your dataset is too large to fit in memory, make sure to store it as TFRecord
    files—an efficient binary storage format that can be loaded very quickly. On [https://keras.rstudio.com,](https://keras.rstudio.com)
    you’ll find example code demonstrating how to format your data as TFRecord files.
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你的数据集太大无法放入内存，确保将其存储为TFRecord文件 - 一种可以非常快地加载的高效二进制存储格式。在[https://keras.rstudio.com](https://keras.rstudio.com)上，你会找到示例代码，演示如何将数据格式化为TFRecord文件。
- en: LEVERAGING STEP FUSING TO IMPROVE TPU UTILIZATION
  id: totrans-371
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 利用步骤融合来提高TPU利用率
- en: Because a TPU has a lot of compute power available, you need to train with very
    large batches to keep the TPU cores busy. For small models, the batch size required
    can get extraordinarily large—upward of 10,000 samples per batch. When working
    with enormous batches, you should make sure to increase your optimizer learning
    rate accordingly; you’re going to be making fewer updates to your weights, but
    each update will be more accurate (because the gradients are computed using more
    data points), so you should move the weights by a greater magnitude with each
    update.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 因为TPU有大量的计算能力可用，你需要用非常大的批次来训练，以保持TPU核心的繁忙。对于小型模型，所需的批量大小可能会变得非常大 - 高达每批10,000个样本。当使用巨大的批次时，你应该确保相应地增加你的优化器学习率；你会对权重进行较少的更新，但每次更新将更准确（因为梯度是使用更多数据点计算的），所以你应该以更大的幅度移动权重。
- en: 'You can leverage a simple trick, however, to keep reasonably sized batches
    while maintaining full TPU utilization: *step fusing*. The idea is to run multiple
    steps of training during each TPU execution step. Basically, do more work in between
    two round trips from the VM memory to the TPU. To do this, simply specify the
    steps_per_execution argument in compile()—for instance, steps_per_execution =
    8 to run eight steps of training during each TPU execution. For small models that
    are underutilizing the TPU (or GPU), this can result in a dramatic speed-up.'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，你可以利用一个简单的技巧，保持合理大小的批次，同时保持TPU的充分利用：*步骤融合*。这个想法是在每个TPU执行步骤中运行多个训练步骤。基本上，在VM内存到TPU之间进行更多的工作。要做到这一点，只需在compile()中指定steps_per_execution参数
    - 例如，steps_per_execution = 8，以在每个TPU执行过程中运行八个训练步骤。对于未充分利用TPU（或GPU）的小型模型，这可能会导致显著加速。
- en: Summary
  id: totrans-374
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: You can leverage hyperparameter tuning and KerasTuner to automate the tedium
    out of finding the best model configuration. But be mindful of validation-set
    overfitting!
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以利用超参数调优和KerasTuner来自动化找到最佳模型配置中的繁琐工作。但要注意验证集过拟合！
- en: An ensemble of diverse models can often significantly improve the quality of
    your predictions.
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个多样化模型的组合往往可以显著提高预测的质量。
- en: You can speed up model training on GPU by turning on mixed precision— you’ll
    generally get a nice speed boost at virtually no cost.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以通过开启混合精度来加快GPU上的模型训练 - 通常会获得良好的速度提升，几乎没有额外成本。
- en: To further scale your workflows, you can use the tf$distribute$Mirrored-Strategy()
    API to train models on multiple GPUs.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要进一步扩展你的工作流程，可以使用tf$distribute$Mirrored-Strategy() API来在多个GPU上训练模型。
- en: You can even train on Google’s TPUs by using the TPUStrategy() API. If your
    model is small, make sure to leverage step fusing (via the compile(…, steps_ per_execution
    = N) argument) to fully utilize the TPU cores.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你甚至可以通过使用 TPUStrategy() API 在 Google 的 TPU 上进行训练。如果你的模型很小，请确保利用步骤融合（通过 compile(…,
    steps_per_execution = N) 参数）充分利用 TPU 核心。
