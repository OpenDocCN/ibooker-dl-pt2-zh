["```py\nimport pandas as pd\nfrom datasets import load_dataset\ntomatoes = load_dataset(\"rotten_tomatoes\")\n\n# Pandas for easier control\ntrain_df = pd.DataFrame(tomatoes[\"train\"])\neval_df = pd.DataFrame(tomatoes[\"test\"])\n```", "```py\nfrom simpletransformers.classification import ClassificationModel, ClassificationArgs\n\n# Train only the classifier layers\nmodel_args = ClassificationArgs()\nmodel_args.train_custom_parameters_only = True\nmodel_args.custom_parameter_groups = [\n    {\n        \"params\": [\"classifier.weight\"],\n        \"lr\": 1e-3,\n    },\n    {\n        \"params\": [\"classifier.bias\"],\n        \"lr\": 1e-3,\n        \"weight_decay\": 0.0,\n    },\n]\n\n# Initializing pre-trained BERT model\nmodel = ClassificationModel(\"bert\", \"bert-base-cased\", args=model_args)\n```", "```py\nimport numpy as np\nfrom sklearn.metrics import f1_score\n\n# Train the model\nmodel.train_model(train_df)\n\n# Predict unseen instances\nresult, model_outputs, wrong_predictions = model.eval_model(eval_df, f1=f1_score)\ny_pred = np.argmax(model_outputs, axis=1)\n```", "```py\n>>> from sklearn.metrics import classification_report\n>>> print(classification_report(eval_df.label, y_pred))\n              precision    recall  f1-score   support\n\n           0       0.84      0.86      0.85       533\n           1       0.86      0.83      0.84       533\n\n    accuracy                           0.85      1066\n   macro avg       0.85      0.85      0.85      1066\nweighted avg       0.85      0.85      0.85      1066\n```", "```py\nfrom sentence_transformers import SentenceTransformer, util\nmodel = SentenceTransformer('all-mpnet-base-v2')\ntrain_embeddings = model.encode(train_df.text)\neval_embeddings = model.encode(eval_df.text)\n```", "```py\nfrom sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(random_state=42).fit(train_embeddings, train_df.label)\n```", "```py\n>>> from sklearn.metrics import classification_report\n>>> y_pred = clf.predict(eval_embeddings)\n>>> print(classification_report(eval_df.label, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.84      0.86      0.85       151\n           1       0.86      0.83      0.84       149\n\n    accuracy                           0.85       300\n   macro avg       0.85      0.85      0.85       300\nweighted avg       0.85      0.85      0.85       300\n```", "```py\nfrom sentence_transformers import SentenceTransformer, util\n\n# Create embeddings for the input documents\nmodel = SentenceTransformer('all-mpnet-base-v2')\neval_embeddings = model.encode(eval_df.text)\n```", "```py\n# Create embeddings for our labels\nlabel_embeddings = model.encode([\"A negative review\", \"A positive review\"])\n```", "```py\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Find the best matching label for each document\nsim_matrix = cosine_similarity(eval_embeddings, label_embeddings)\ny_pred = np.argmax(sim_matrix, axis=1)\n```", "```py\n>>> print(classification_report(eval_df.label, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.83      0.77      0.80       151\n           1       0.79      0.84      0.81       149\n\n    accuracy                           0.81       300\n   macro avg       0.81      0.81      0.81       300\nweighted avg       0.81      0.81      0.81       300\n```", "```py\n>>> # Create embeddings for our labels\n>>> label_embeddings = model.encode([\"A very negative movie review\", \"A very positive movie review\"])\n>>> \n>>> # Find the best matching label for each document\n>>> sim_matrix = cosine_similarity(eval_embeddings, label_embeddings)\n>>> y_pred = np.argmax(sim_matrix, axis=1)\n>>> \n>>> # Report results\n>>> print(classification_report(eval_df.label, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.90      0.74      0.81       151\n           1       0.78      0.91      0.84       149\n\n    accuracy                           0.83       300\n   macro avg       0.84      0.83      0.83       300\nweighted avg       0.84      0.83      0.83       300\n```", "```py\nfrom transformers import pipeline\n\n# Pre-trained MNLI model\npipe = pipeline(model=\"facebook/bart-large-mnli\")\n\n# Candidate labels\ncandidate_labels_dict = {\"negative movie review\": 0, \"positive movie review\": 1}\ncandidate_labels = [\"negative movie review\", \"positive movie review\"]\n\n# Create predictions\npredictions = pipe(eval_df.text.values.tolist(), candidate_labels=candidate_labels)\n```", "```py\n>>> from sklearn.metrics import classification_report\n>>> y_pred = [candidate_labels_dict[prediction[\"labels\"][0]] for prediction in predictions]\n>>> print(classification_report(eval_df.label, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.77      0.89      0.83       151\n           1       0.87      0.74      0.80       149\n\n    accuracy                           0.81       300\n   macro avg       0.82      0.81      0.81       300\nweighted avg       0.82      0.81      0.81       300\n```", "```py\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\n\n@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\ndef gpt_prediction(prompt, document, model=\"gpt-3.5-turbo-0301\"):\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\":   prompt.replace(\"[DOCUMENT]\", document)}\n  ]\n  response = openai.ChatCompletion.create(model=model, messages=messages, temperature=0)\n  return response[\"choices\"][0][\"message\"][\"content\"]\n```", "```py\nimport openai\nopenai.api_key = \"sk-...\"\n```", "```py\n# Define a zero-shot prompt as a base\nzeroshot_prompt = \"\"\"Predict whether the following document is a positive or negative movie review:\n\n[DOCUMENT]\n\nIf it is positive say 1 and if it is negative say 0\\. Do not give any other answers.\n\"\"\"\n```", "```py\n# Define a zero-shot prompt as a base\nzeroshot_prompt = \"\"\"Predict whether the following document is a positive or negative movie review:\n\n[DOCUMENT]\n\nIf it is positive say 1 and if it is negative say 0\\. Do not give any other answers.\n\"\"\"\n\n# Predict the target using GPT\ndocument = \"unpretentious , charming , quirky , original\"\ngpt_prediction(zeroshot_prompt, document)\n```", "```py\n> from sklearn.metrics import classification_report\n> from tqdm import tqdm\n>\n> y_pred = [int(gpt_prediction(zeroshot_prompt, doc)) for doc in tqdm(eval_df.text)]\n> print(classification_report(eval_df.label, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.86      0.96      0.91       151\n           1       0.95      0.86      0.91       149\n\n    accuracy                           0.91       300\n   macro avg       0.91      0.91      0.91       300\nweighted avg       0.91      0.91      0.91       300\n```", "```py\n# Define a few-shot prompt as a base\nfewshot_prompt = \"\"\"Predict whether the following document is a positive or negative moview review:\n\n[DOCUMENT]\n\nExamples of negative reviews are:\n- a film really has to be exceptional to justify a three hour running time , and this isn't .\n- the film , like jimmy's routines , could use a few good laughs .\n\nExamples of positive reviews are:\n- very predictable but still entertaining\n- a solid examination of the male midlife crisis .\n\nIf it is positive say 1 and if it is negative say 0\\. Do not give any other answers.\n\"\"\"\n```", "```py\n# Predict the target using GPT\ndocument = \"unpretentious , charming , quirky , original\"\ngpt_prediction(fewshot_prompt, document)\n```", "```py\n>>> predictions = [gpt_prediction(fewshot_prompt, doc) for doc in tqdm(eval_df.text)]\n\n              precision    recall  f1-score   support\n\n           0       0.88      0.97      0.92       151\n           1       0.96      0.87      0.92       149\n\n    accuracy                           0.92       300\n   macro avg       0.92      0.92      0.92       300\nweighted avg       0.92      0.92      0.92       300\n```", "```py\nimport os\nos.environ['OPENAI_API_KEY'] = \"sk-...\"\n```", "```py\nimport spacy\n\nnlp = spacy.blank(\"en\")\n\n# Create a Named Entity Recognition Task and define labels\ntask = {\"task\": {\n            \"@llm_tasks\": \"spacy.NER.v1\",\n            \"labels\": \"DATE,AGE,LOCATION, DISEASE, SYMPTOM\"}}\n\n# Choose which backend to use\nbackend = {\"backend\": {\n            \"@llm_backends\": \"spacy.REST.v1\",\n            \"api\": \"OpenAI\",\n            \"config\": {\"model\": \"gpt-3.5-turbo\"}}}\n\n# Combine configurations and create SpaCy pipeline\nconfig = task | backend\nnlp.add_pipe(\"llm\", config=config)\n```", "```py\n> doc = nlp(\"On February 11, 2020, a 73-year-old woman came to the hospital \\n and was diagnosed with COVID-19 and has a cough.\")\n> print([(ent.text, ent.label_) for ent in doc.ents])\n\n[('February 11', 'DATE'), ('2020', 'DATE'), ('73-year-old', 'AGE'), ('hospital', 'LOCATION'), ('COVID-19', ' DISEASE'), ('cough', ' SYMPTOM')]\n```", "```py\nfrom spacy import displacy\nfrom IPython.core.display import display, HTML\n\n# Display entities\nhtml = displacy.render(doc, style=\"ent\")\ndisplay(HTML(html))\n```"]