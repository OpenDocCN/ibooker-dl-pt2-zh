- en: Chapter 1\. Categorizing Text
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章\. 文本分类
- en: One of the most common tasks in natural language processing, and machine learning
    in general, is classification. The goal of the task is to train a model to assign
    a label or class to some input text. Categorizing text is used across the world
    for a wide range of applications, from sentiment analysis and intent detection
    to extracting entities and detecting language.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理和机器学习中最常见的任务之一是分类。该任务的目标是训练一个模型，为一些输入文本分配标签或类别。文本分类在全球范围内用于广泛的应用，从情感分析和意图检测到实体提取和语言检测。
- en: The impact of Large Language Models on categorization cannot be understated.
    The addition of these models has quickly settled as the default for these kinds
    of tasks.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型对分类的影响不容小觑。这些模型的加入迅速成为这类任务的默认选择。
- en: In this chapter, we will discuss a variety of ways to use Large Language Modeling
    for categorizing text. Due to the broad field of text categorization, a variety
    of techniques, as well as use cases, will be discussed. This chapter also serves
    as a nice introduction to LLMs as most of them can be used for classification.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论使用大型语言模型进行文本分类的多种方法。由于文本分类的广泛领域，将讨论多种技术和应用案例。本章还可以很好地引入LLM，因为大多数模型都可以用于分类。
- en: We will focus on leveraging pre-trained LLMs, models that already have been
    trained on large amounts of data and that can be used for categorizing text. Fine-tuning
    these models for categorizing text and domain adaptation will be discussed in
    more detail in Chapter 10.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重点利用预训练的LLM，这些模型已经在大量数据上进行训练，可以用于文本分类。对这些模型进行文本分类和领域适应的微调将在第10章中详细讨论。
- en: Let’s start by looking at the most basic application and technique, fully-supervised
    text classification.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从最基本的应用和技术开始，完全监督的文本分类。
- en: Supervised Text Classification
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督文本分类
- en: Classification comes in many flavors, such as few-shot and zero-shot classification
    which we will discuss later in this chapter, but the most frequently used method
    is a fully supervised classification. This means that during training, every input
    has a target category from which the model can learn.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 分类有多种形式，例如稍微学习（few-shot）和零学习（zero-shot）分类，我们将在本章后面讨论，但最常用的方法是完全监督的分类。这意味着在训练过程中，每个输入都有一个目标类别，模型可以从中学习。
- en: For supervised classification using textual data as our input, there is a common
    procedure that is typically followed. As illustrated in [Figure 1-1](#fig_1_an_example_of_supervised_classification_can_we_pr),
    we first convert our textual input to numerical representations using a feature
    extraction model. Traditionally, such a model would represent text as a bag of
    words, simply counting the number of times a word appears in a document. In this
    book, however, we will be focusing on LLMs as our feature extraction model.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用文本数据作为输入的监督分类，通常遵循一个常见的程序。如[图1-1](#fig_1_an_example_of_supervised_classification_can_we_pr)所示，我们首先使用特征提取模型将文本输入转换为数值表示。传统上，这样的模型将文本表示为词袋，简单地计算一个单词在文档中出现的次数。然而，在本书中，我们将重点关注LLM作为我们的特征提取模型。
- en: '![An example of supervised classification. Can we predict whether a movie review
    is either positive or negative ](assets/categorizing_text_559117_01.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![监督分类的示例。我们能否预测电影评论是正面还是负面](assets/categorizing_text_559117_01.png)'
- en: Figure 1-1\. An example of supervised classification. Can we predict whether
    a movie review is either positive or negative?
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-1\. 监督分类的示例。我们能否预测电影评论是正面还是负面？
- en: Then, we train a classifier on the numerical representations, such as embeddings
    (remember from Chapter X?), to classify the textual data. The classifier can be
    a number of things, such as a neural network or logistic regression. It can even
    be the classifier used in many Kaggle competitions, namely XGBoost!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在数值表示上训练分类器，例如嵌入（还记得第X章吗？），以对文本数据进行分类。分类器可以是多种形式，例如神经网络或逻辑回归。它甚至可以是许多Kaggle竞赛中使用的分类器，即XGBoost！
- en: In this pipeline, we always need to train the classifier but we can choose to
    fine-tune either the entire LLM, certain parts of it, or keep it as is. If we
    choose not to fine-tune it all, we refer to this procedure as *freezing its layers*.
    This means that the layers cannot be updated during the training process. However,
    it may be beneficial to *unfreeze* at least some of its layers such that the Large
    Language Models can be *fine-tuned* for the specific classification task. This
    process is illustrated in [Figure 1-2](#fig_2_a_common_procedure_for_supervised_text_classificat).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个流程中，我们总是需要训练分类器，但可以选择微调整个LLM、其中某些部分或保持不变。如果我们选择不进行微调，我们称这一过程为*冻结其层*。这意味着在训练过程中这些层无法更新。然而，至少*解冻*一些层可能是有益的，这样大型语言模型就可以针对特定的分类任务进行*微调*。该过程在[图
    1-2](#fig_2_a_common_procedure_for_supervised_text_classificat)中有所说明。
- en: '![A common procedure for supervised text classification. We convert our textual
    input data to numerical representations through feature extraction. Then  a classifier
    is trained to predict labels. ](assets/categorizing_text_559117_02.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![监督文本分类的常见过程。我们通过特征提取将文本输入数据转换为数值表示。然后，训练分类器以预测标签。](assets/categorizing_text_559117_02.png)'
- en: Figure 1-2\. A common procedure for supervised text classification. We convert
    our textual input data to numerical representations through feature extraction.
    Then, a classifier is trained to predict labels.
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-2\. 监督文本分类的常见过程。我们通过特征提取将文本输入数据转换为数值表示。然后，训练分类器以预测标签。
- en: Model Selection
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型选择
- en: We can use an LLM to represent the text to be fed into our classifier. The choice
    of this model, however, may not be as straightforward as you might think. Models
    differ in the language they can handle, their architecture, size, inference speed,
    architecture, accuracy for certain tasks, and many more differences exist.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用LLM来表示要输入到分类器中的文本。然而，这个模型的选择可能并不像您想象的那样简单。模型在可处理的语言、架构、大小、推理速度、架构、特定任务的准确性等方面存在差异，且还有许多其他差异。
- en: BERT is a great underlying architecture for representing tasks that can be fine-tuned
    for a number of tasks, including classification. Although there are generative
    models that we can use, like the well-known Generated Pretrained Transformers
    (GPT) such as ChatGPT, BERT models often excel at being fine-tuned for specific
    tasks. In contrast, GPT-like models typically excel at a broad and wide variety
    of tasks. In a sense, it is specialization versus generalization.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: BERT是一个优秀的基础架构，可以针对多种任务进行微调，包括分类。尽管我们可以使用生成模型，比如知名的生成预训练变换器（GPT），例如ChatGPT，但BERT模型通常在特定任务的微调上表现优越。相比之下，GPT类模型通常在广泛的任务上表现突出。从某种意义上说，这是专业化与泛化的对比。
- en: Now that we know to choose a BERT-like model for our supervised classification
    task, which are we going to use? BERT has a number of variations, including BERT,
    RoBERTa, DistilBERT, ALBERT, DeBERTa, and each architecture has been pre-trained
    in numerous forms, from training in certain domains to training for multi-lingual
    data. You can find an overview of some well-known Large Language Models in [Figure 1-3](#fig_3_a_timeline_of_common_large_language_model_releases).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道要为监督分类任务选择一个类似BERT的模型，我们将使用哪个呢？BERT有多种变体，包括BERT、RoBERTa、DistilBERT、ALBERT、DeBERTa，每种架构都以不同的形式进行了预训练，从特定领域的训练到多语言数据的训练。您可以在[图
    1-3](#fig_3_a_timeline_of_common_large_language_model_releases)中找到一些著名大型语言模型的概述。
- en: 'Selecting the right model for the job can be a form of art in itself. Trying
    thousands of pre-trained models that can be found on HuggingFace’s Hub is not
    feasible so we need to be efficient with the models that we choose. Having said
    that, there are a number of models that are a great starting point and give you
    an idea of the base performance of these kinds of models. Consider them solid
    baselines:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为工作选择合适的模型本身可以是一种艺术。尝试数千个可以在HuggingFace Hub上找到的预训练模型是不可行的，因此我们需要高效地选择模型。话虽如此，仍然有一些模型是很好的起点，并能让您了解这些模型的基础性能。将它们视为稳固的基线：
- en: '[BERT-base-uncased](https://huggingface.co/bert-base-uncased)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[BERT-base-uncased](https://huggingface.co/bert-base-uncased)'
- en: '[Roberta-base](https://huggingface.co/roberta-base)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Roberta-base](https://huggingface.co/roberta-base)'
- en: '[Distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased)'
- en: '[Deberta-base](https://huggingface.co/microsoft/deberta-base)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Deberta-base](https://huggingface.co/microsoft/deberta-base)'
- en: '[BERT-tiny](https://huggingface.co/prajjwal1/bert-tiny)'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[BERT-tiny](https://huggingface.co/prajjwal1/bert-tiny)'
- en: '[Albert-base-v2](https://huggingface.co/albert-base-v2)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Albert-base-v2](https://huggingface.co/albert-base-v2)'
- en: '**![A timeline of common Large Language Model releases. ](assets/categorizing_text_559117_03.png)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**![常见大型语言模型发布的时间线。](assets/categorizing_text_559117_03.png)'
- en: Figure 1-3\. A timeline of common Large Language Model releases.**  **In this
    section, we will be using “bert-base-cased” for some of our examples. Feel free
    to replace “bert-base-cased” with any of the models above. Play around with different
    models to get a feeling for the trade-off in performance/training speed.**  **##
    Data
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-3\. 常见大型语言模型发布的时间线。** **在本节中，我们将使用“bert-base-cased”进行一些示例。可以随意将“bert-base-cased”替换为上述任意模型。尝试不同的模型，以感受性能与训练速度之间的权衡。**  **##
    数据
- en: Throughout this chapter, we will be demonstrating many techniques for categorizing
    text. The dataset that we will be using to train and evaluate the models is the
    [“rotten_tomatoes”](https://huggingface.co/datasets/rotten_tomatoes); pang2005seeing)
    dataset. It contains roughly 5000 positive and 5000 negative movie reviews from
    [Rotten Tomatoes](https://www.rottentomatoes.com/).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将演示许多文本分类技术。我们将用来训练和评估模型的数据集是[“rotten_tomatoes”](https://huggingface.co/datasets/rotten_tomatoes)；pang2005seeing)数据集。它包含约5000条正面和5000条负面电影评论，来自[Rotten
    Tomatoes](https://www.rottentomatoes.com/)。
- en: 'We load the data and convert it to a `pandas dataframe` for easier control:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们加载数据并将其转换为`pandas dataframe`以便于控制：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Tip
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Although this book focuses on LLMs, it is highly advised to compare these examples
    against classic, but strong baselines such as representing text with TF-IDF and
    training a LogisticRegression classifier on top of that.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本书专注于LLMs，但强烈建议将这些示例与经典且强大的基准进行比较，例如使用TF-IDF表示文本并在其上训练LogisticRegression分类器。
- en: Classification Head
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类头
- en: Using the Rotten Tomatoes dataset, we can start with the most straightforward
    example of a predictive task, namely binary classification. This is often applied
    in sentiment analysis, detecting whether a certain document is positive or negative.
    This can be customer reviews with a label indicating whether that review is positive
    or negative (binary). In our case, we are going to predict whether a movie review
    is negative (0) or positive (1).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Rotten Tomatoes数据集，我们可以从最简单的预测任务开始，即二分类。这通常应用于情感分析，检测某个文档是正面还是负面。这可以是带有指示该评论是正面还是负面的标签（0或1）的客户评论。在我们的案例中，我们将预测一条电影评论是负面（0）还是正面（1）。
- en: 'Training a classifier with transformer-based models generally follows a two-step
    approach:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 使用基于变换器的模型训练分类器通常遵循两步法：
- en: First, as we show in [Figure 1-4](#fig_4_first_we_start_by_using_a_generic_pre_trained_llm),
    we take an existing transformer model and use it to convert our textual data to
    numerical representations.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，如[图1-4](#fig_4_first_we_start_by_using_a_generic_pre_trained_llm)所示，我们采用现有的变换器模型，将文本数据转换为数值表示。
- en: '![First  we start by using a generic pre trained LLM  e.g.  BERT  to convert
    our textual data into more numerical representations. During training  we will  freeze  the
    model such that its weights will not be updated. This speeds up training significantly
    but is generally less accurate.](assets/categorizing_text_559117_04.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![首先，我们使用通用的预训练LLM（例如BERT）将文本数据转换为更数值化的表示。在训练过程中，我们将冻结模型，使其权重不会被更新。这显著加快了训练速度，但通常精度较低。](assets/categorizing_text_559117_04.png)'
- en: Figure 1-4\. First, we start by using a generic pre-trained LLM (e.g., BERT)
    to convert our textual data into more numerical representations. During training,
    we will “freeze” the model such that its weights will not be updated. This speeds
    up training significantly but is generally less accurate.
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-4\. 首先，我们使用通用的预训练LLM（例如BERT）将我们的文本数据转换为更数值化的表示。在训练过程中，我们将“冻结”模型，以便其权重不会被更新。这显著加快了训练速度，但通常精度较低。
- en: Second, as shown in [Figure 1-5](#fig_5_after_fine_tuning_our_llm_we_train_a_classifier_o),
    we put a classification head on top of the pre-trained model. This classification
    head is generally a single linear layer that we can fine-tune.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，如[图1-5](#fig_5_after_fine_tuning_our_llm_we_train_a_classifier_o)所示，我们在预训练模型的顶部添加一个分类头。这个分类头通常是一个单一的线性层，我们可以对其进行微调。
- en: '![After fine tuning our LLM  we train a classifier on the numerical representations
    and labels. Typically  a Feed Forward Neural Network is chosen as the classifier.
    ](assets/categorizing_text_559117_05.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![在微调我们的LLM后，我们在数值表示和标签上训练分类器。通常，选择前馈神经网络作为分类器。](assets/categorizing_text_559117_05.png)'
- en: Figure 1-5\. After fine-tuning our LLM, we train a classifier on the numerical
    representations and labels. Typically, a Feed Forward Neural Network is chosen
    as the classifier.
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-5\. 在微调我们的LLM后，我们在数值表示和标签上训练分类器。通常，选择前馈神经网络作为分类器。
- en: These two steps each describe the same model since the classification head is
    added directly to the BERT model. As illustrated in [Figure 1-6](#fig_6_we_adopt_the_bert_model_such_that_its_output_embed),
    our classifier is nothing more than a pre-trained LLM with a linear layer attached
    to it. It is feature extraction and classification in one.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个步骤描述的是同一模型，因为分类头直接添加到BERT模型中。如[图1-6](#fig_6_we_adopt_the_bert_model_such_that_its_output_embed)所示，我们的分类器只不过是一个附加了线性层的预训练LLM。它实现了特征提取和分类的结合。
- en: '![We adopt the BERT model such that its output embeddings are fed into a classification
    head. This head generally consists of a linear layer but might include dropout
    beforehand.](assets/categorizing_text_559117_06.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![我们采用BERT模型，其输出嵌入被输入到分类头中。该头通常由一个线性层组成，但可能会提前包含dropout。](assets/categorizing_text_559117_06.png)'
- en: Figure 1-6\. We adopt the BERT model such that its output embeddings are fed
    into a classification head. This head generally consists of a linear layer but
    might include dropout beforehand.
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-6\. 我们采用BERT模型，其输出嵌入被输入到分类头中。该头通常由一个线性层组成，但可能会提前包含dropout。
- en: Note
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In Chapter 10, we will use the same pipeline shown in Figures 2-4 and 2-5 but
    will instead fine-tune the Large Language Model. There, we will go more in-depth
    into how fine-tuning works and why it improves upon the pipeline as shown here.
    For now, it is essential to know that fine-tuning this model together with the
    classification head improves the accuracy during the classification task. The
    reason for this is that it allows the Large Language Model to better represent
    the text for classification purposes. It is fine-tuned toward the domain-specific
    texts.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在第10章中，我们将使用图2-4和2-5中显示的相同管道，但将微调大型语言模型。在那里，我们将更深入地探讨微调的工作原理以及为什么它能改善这里展示的管道。目前，重要的是要知道，微调这个模型和分类头一起提高了分类任务的准确性。这是因为它使大型语言模型能够更好地表示文本以进行分类，针对特定领域文本进行微调。
- en: Example
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例
- en: 'To train our model, we are going to be using the [simpletransformers package](https://github.com/ThilinaRajapakse/simpletransformers).
    It abstracts most of the technical difficulty away so that we can focus on the
    classification task at hand. We start by initializing our model:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练我们的模型，我们将使用[简单变换器包](https://github.com/ThilinaRajapakse/simpletransformers)。它抽象了大部分技术难点，让我们可以专注于当前的分类任务。我们从初始化模型开始：
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We have chosen the popular “bert-base-cased” but as mentioned before, there
    are many other models that we could have chosen instead. Feel free to play around
    with models to see how it influences performance.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了流行的“bert-base-cased”，但如前所述，我们还有许多其他模型可以选择。可以随意尝试不同模型，以查看其对性能的影响。
- en: 'Next, we can train the model on our training dataset and predict the labels
    of our evaluation dataset:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以在训练数据集上训练模型，并预测评估数据集的标签：
- en: '[PRE2]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now that we have trained our model, all that is left is evaluation:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练了模型，剩下的就是评估：
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Using a pre-trained BERT model for classification gives us an F-1 score of 0.85\.
    We can use this score as a baseline throughout the examples in this section.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预训练的BERT模型进行分类使我们的F-1得分达到0.85。我们可以将这个得分作为本节示例中的基准。
- en: Tip
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The `simpletransformers` package has a number of easy-to-use features for different
    tasks. For example, you could also use it to create a custom Named Entity Recognition
    model with only a few lines of code.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`simpletransformers`包提供了许多易于使用的功能来处理不同任务。例如，你也可以用它创建一个自定义的命名实体识别模型，只需几行代码。'
- en: Pre-Trained Embeddings
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预训练嵌入
- en: Unlike the example shown before, we can approach supervised classification in
    a more classical form. Instead of freezing layers before training and using a
    feed-forward neural network on top of it, we can completely separate feature extraction
    and classification training.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前展示的示例不同，我们可以以更经典的形式进行监督分类。我们可以完全将特征提取与分类训练分开，而不是在训练前冻结层并在其上使用前馈神经网络。
- en: 'This two-step approach completely separates feature extraction from classification:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这种两步法完全将特征提取与分类分开：
- en: First, as we can see in [Figure 1-7](#fig_7_first_we_use_an_llm_that_was_trained_specifically),
    we perform our feature extraction with an LLM, SBERT ([https://www.sbert.net/](https://www.sbert.net/)),
    which is trained specifically to create embeddings.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，正如我们在[图1-7](#fig_7_first_we_use_an_llm_that_was_trained_specifically)中看到的，我们使用一个专门训练以创建嵌入的LLM，SBERT（[https://www.sbert.net/](https://www.sbert.net/)）。
- en: '![First  we use an LLM that was trained specifically to generate accurate numerical
    representations. These tend to be better representative vectors than we receive
    from a general Transformer based model like BERT. ](assets/categorizing_text_559117_07.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![首先，我们使用一个专门训练以生成准确数值表示的LLM。这些通常比我们从像BERT这样的一般Transformer模型中获得的更具代表性。](assets/categorizing_text_559117_07.png)'
- en: Figure 1-7\. First, we use an LLM that was trained specifically to generate
    accurate numerical representations. These tend to be better representative vectors
    than we receive from a general Transformer-based model like BERT.
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-7\. 首先，我们使用一个专门训练用于生成准确数值表示的LLM。这些通常比我们从像BERT这样的一般Transformer模型中获得的更具代表性。
- en: Second, as shown in [Figure 1-8](#fig_8_using_the_embeddings_as_our_features_we_train_a_l),
    we use the embeddings as input for a logistic regression model. We are completely
    separating the feature extraction model from the classification model.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，如[图1-8](#fig_8_using_the_embeddings_as_our_features_we_train_a_l)所示，我们使用嵌入作为逻辑回归模型的输入。我们完全将特征提取模型与分类模型分开。
- en: '![Using the embeddings as our features  we train a logistic regression model
    on our training data. ](assets/categorizing_text_559117_08.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![使用嵌入作为特征，我们在训练数据上训练一个逻辑回归模型。](assets/categorizing_text_559117_08.png)'
- en: Figure 1-8\. Using the embeddings as our features, we train a logistic regression
    model on our training data.
  id: totrans-66
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-8\. 使用嵌入作为特征，我们在训练数据上训练一个逻辑回归模型。
- en: In contrast to our previous example, these two steps each describe a different
    model. SBERT for generating features, namely embeddings, and a Logistic Regression
    as the classifier. As illustrated in Figure 2-9, our classifier is nothing more
    than a pre-trained LLM with a linear layer attached to it.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前的示例相比，这两个步骤分别描述了不同的模型。SBERT用于生成特征，即嵌入，而逻辑回归则作为分类器。如图2-9所示，我们的分类器仅仅是一个附加了线性层的预训练LLM。
- en: '**![The classifier is a separate model that leverages the embeddings from SBERT
    to learn from. ](assets/categorizing_text_559117_09.png)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**![分类器是一个单独的模型，它利用来自SBERT的嵌入进行学习。](assets/categorizing_text_559117_09.png)**'
- en: Figure 1-9\. The classifier is a separate model that leverages the embeddings
    from SBERT to learn from.**  **### Example
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-9\. 分类器是一个单独的模型，它利用来自SBERT的嵌入进行学习。**  **### 示例
- en: 'Using sentence-transformer, we can create our features before training our
    classification model:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 使用句子转换器，我们可以在训练分类模型之前创建我们的特征：
- en: '[PRE4]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We created the embeddings for our training (train_df) and evaluation (eval_df)
    data. Each instance in the resulting embeddings is represented by 768 values.
    We consider these values the features on which we can train our model.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为训练（train_df）和评估（eval_df）数据创建了嵌入。生成的每个嵌入实例由768个值表示。我们将这些值视为可以用于训练模型的特征。
- en: 'Selecting the model can be straightforward. Instead of using a feed-forward
    neural network, we can go back to the basics and use a Logistic Regression instead:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 选择模型可以很简单。我们可以回归基础，使用逻辑回归，而不是使用前馈神经网络：
- en: '[PRE5]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In practice, you can use any classifier on top of our generated embeddings,
    like Decision Trees or Neural Networks.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，你可以在我们生成的嵌入上使用任何分类器，例如决策树或神经网络。
- en: 'Next, let’s evaluate our model:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们评估我们的模型：
- en: '[PRE6]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Without needing to fine-tune our LLM, we managed to achieve an F1-score of 0.85\.
    This is especially impressive since it is a much smaller model compared to our
    previous example.****  ****# Zero-shot Classification
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在不需要微调我们的LLM的情况下，我们成功地达到了0.85的F1得分。这一点尤其令人印象深刻，因为它相比于我们的前一个示例，模型要小得多。****  ****#
    零-shot分类
- en: We started this chapter with examples where all of our training data has labels.
    In practice, however, this might not always be the case. Getting labeled data
    is a resource-intensive task that can require significant human labor. Instead,
    we can use zero-shot classification models. This method is a nice example of transfer
    learning where a model trained for one task is used for a task different than
    what it was originally trained for. An overview of zero-shot classification is
    given in Figure 2-11\. Note that this pipeline also demonstrates the capabilities
    of performing multi-label classification if the probabilities of multiple labels
    exceed a given threshold.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 本章开始时的例子中，我们所有的训练数据都有标签。然而，在实践中，这可能并不总是如此。获取标记数据是一项资源密集型任务，可能需要大量人力。相反，我们可以使用零样本分类模型。这种方法是迁移学习的一个良好例子，训练用于一项任务的模型被用于与其最初训练的任务不同的任务。零样本分类的概述在图
    2-11 中给出。请注意，这个流程还展示了如果多个标签的概率超过给定阈值，则执行多标签分类的能力。
- en: '![ Figure 2 11\. In zero shot classification  the LLM is not trained on any
    of the candidate labels. It learned from different labels and generalized that
    information to the candidate labels. ](assets/categorizing_text_559117_10.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图 2-11\. 在零样本分类中，LLM并未在任何候选标签上进行训练。它从不同的标签中学习，并将这些信息推广到候选标签上。](assets/categorizing_text_559117_10.png)'
- en: Figure 1-10\. Figure 2-11\. In zero-shot classification, the LLM is not trained
    on any of the candidate labels. It learned from different labels and generalized
    that information to the candidate labels.
  id: totrans-81
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-10\. 图 2-11\. 在零样本分类中，LLM并未在任何候选标签上进行训练。它从不同的标签中学习，并将这些信息推广到候选标签上。
- en: Often, zero-shot classification tasks are used with pre-trained LLMs that use
    natural language to describe what we want our model to do. It is often referred
    to as an emergent feature of LLMs as the models increase in size (wei2022emergent).
    As we will see later in this chapter on classification with generative models,
    GPT-like models can often do these kinds of tasks quite well.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，零样本分类任务与使用自然语言描述我们希望模型执行的操作的预训练LLM一起使用。随着模型规模的增加，这通常被称为LLM的涌现特性(wei2022emergent)。正如我们将在本章后面关于生成模型分类时看到的，类似GPT的模型通常能够很好地完成这些任务。
- en: Pre-Trained Embeddings
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预训练嵌入
- en: As we have seen in our supervised classification examples, embeddings are a
    great and often accurate way of representing textual data. When dealing with no
    labeled documents, we have to be a bit creative in how we are going to be using
    pre-trained embeddings. A classifier cannot be trained since we have no labeled
    data to work with.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在监督分类示例中所看到的，嵌入是一种出色且常常准确地表示文本数据的方法。在处理没有标记的文档时，我们需要在如何使用预训练嵌入方面稍微富有创造性。由于没有可用的标记数据，分类器无法进行训练。
- en: Fortunately, there is a trick that we can use. We can describe our labels based
    on what they should represent. For example, a negative label for movie reviews
    can be described as “This is a negative movie review”. By describing and embedding
    the labels and documents, we have data that we can work with. This process, as
    illustrated in [Figure 1-11](#fig_11_to_embed_the_labels_we_first_need_to_give_them_a),
    allows us to generate our own target labels without the need to actually have
    any labeled data.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们可以使用一个技巧。我们可以根据标签应表示的内容来描述它们。例如，电影评论的负面标签可以描述为“这是一条负面的电影评论”。通过描述和嵌入标签和文档，我们有了可以使用的数据。这个过程如[图
    1-11](#fig_11_to_embed_the_labels_we_first_need_to_give_them_a)所示，使我们能够生成自己的目标标签，而无需实际拥有任何标记数据。
- en: '![To embed the labels  we first need to give them a description. For example  the
    description of a negative label could be  A negative movie review . This description
    can then be embedded through sentence transformers. In the end  both labels as
    well as all the documents are embedded. ](assets/categorizing_text_559117_11.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![要嵌入标签，我们首先需要给它们一个描述。例如，负面标签的描述可以是“负面的电影评论”。这个描述可以通过句子变换器嵌入。最后，标签和所有文档都会被嵌入。](assets/categorizing_text_559117_11.png)'
- en: Figure 1-11\. To embed the labels, we first need to give them a description.
    For example, the description of a negative label could be “A negative movie review”.
    This description can then be embedded through sentence-transformers. In the end,
    both labels as well as all the documents are embedded.
  id: totrans-87
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-11\. 要嵌入标签，我们首先需要给它们一个描述。例如，负面标签的描述可以是“负面的电影评论”。这个描述可以通过句子变换器嵌入。最后，标签和所有文档都会被嵌入。
- en: To assign labels to documents, we can apply cosine similarity to the document
    label pairs. Cosine similarity, which will often be used throughout this book,
    is a similarity measure that checks how similar two vectors are to each other.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 要为文档分配标签，我们可以对文档标签对应用余弦相似度。余弦相似度是检查两个向量彼此相似程度的相似性度量，整个书中会经常使用。
- en: It is the cosine of the angle between vectors which is calculated through the
    dot product of the embeddings and divided by the product of their lengths. It
    definitely sounds more complicated than it is and, hopefully, the illustration
    in [Figure 1-12](#fig_12_the_cosine_similarity_is_the_angle_between_two_vec) should
    provide additional intuition.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这是向量之间角度的余弦，通过嵌入的点积计算，并除以它们长度的乘积。听起来确实比实际复杂，希望[图1-12](#fig_12_the_cosine_similarity_is_the_angle_between_two_vec)中的插图能提供额外的直觉。
- en: '![The cosine similarity is the angle between two vectors or embeddings. In
    this example  we calculate the similarity between a document and the two possible
    labels  positive and negative. ](assets/categorizing_text_559117_12.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![余弦相似度是两个向量或嵌入之间的角度。在这个例子中，我们计算文档与两个可能标签（正面和负面）之间的相似度。](assets/categorizing_text_559117_12.png)'
- en: Figure 1-12\. The cosine similarity is the angle between two vectors or embeddings.
    In this example, we calculate the similarity between a document and the two possible
    labels, positive and negative.
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-12\. 余弦相似度是两个向量或嵌入之间的角度。在这个例子中，我们计算文档与两个可能标签（正面和负面）之间的相似度。
- en: For each document, its embedding is compared to that of each label. The label
    with the highest similarity to the document is chosen. [Figure 1-13](#fig_13_after_embedding_the_label_descriptions_and_the_doc)
    gives a nice example of how a document is assigned a label.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个文档，它的嵌入与每个标签的嵌入进行比较。选择与文档相似度最高的标签。[图1-13](#fig_13_after_embedding_the_label_descriptions_and_the_doc)很好地展示了文档如何被分配标签。
- en: '![After embedding the label descriptions and the documents  we can use cosine
    similarity for each label document pair. For each document  the label with the
    highest similarity to the document is chosen. ](assets/categorizing_text_559117_13.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![在嵌入标签描述和文档之后，我们可以对每个标签文档对使用余弦相似度。对于每个文档，选择与该文档相似度最高的标签。](assets/categorizing_text_559117_13.png)'
- en: Figure 1-13\. After embedding the label descriptions and the documents, we can
    use cosine similarity for each label document pair. For each document, the label
    with the highest similarity to the document is chosen.
  id: totrans-94
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-13\. 在嵌入标签描述和文档后，我们可以对每个标签文档对使用余弦相似度。对于每个文档，选择与该文档相似度最高的标签。
- en: Example
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例
- en: We start by generating the embeddings for our evaluation dataset. These embeddings
    are generated with sentence-transformers as they are quite accurate and are computationally
    quite fast.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先为评估数据集生成嵌入。这些嵌入是使用句子转换器生成的，因为它们相当准确且计算速度较快。
- en: '[PRE7]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Next, embeddings of the labels need to be generated. The labels, however, do
    not have a textual representation that we can leverage so we will instead have
    to name the labels ourselves.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，需要生成标签的嵌入。然而，这些标签没有可以利用的文本表示，因此我们需要自己命名这些标签。
- en: 'Since we are dealing with positive and negative movie reviews, let’s name the
    labels “A positive review” and “A negative review”. This allows us to embed those
    labels:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们要处理正面和负面电影评论，我们将标签命名为“正面评论”和“负面评论”。这使我们能够嵌入这些标签：
- en: '[PRE8]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now that we have embeddings for our reviews and the labels, we can apply cosine
    similarity between them to see which label fits best with which review. Doing
    so requires only a few lines of code:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了评论和标签的嵌入，我们可以在它们之间应用余弦相似度，以查看哪个标签最适合哪个评论。这样只需要几行代码：
- en: '[PRE9]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'And that is it! We only needed to come up with names for our labels to perform
    our classification tasks. Let’s see how well this method works:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！我们只需为我们的标签想出名称，就可以执行分类任务。让我们看看这种方法效果如何：
- en: '[PRE10]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: An F-1 score of 0.81 is quite impressive considering we did not use any labeled
    data at all! This just shows how versatile and useful embeddings are especially
    if you are a bit creative with how they are used.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们根本没有使用任何标记数据，0.81的F-1分数相当令人印象深刻！这显示了嵌入的多功能性和实用性，尤其是当你在使用方式上稍微有点创意时。
- en: Let’s put that creativity to the test. We decided upon “A negative/positive
    review” as the names of our labels but that can be improved. Instead, we can make
    them a bit more concrete and specific towards our data by using “A very negative/positive
    movie review” instead. This way, the embedding will capture that it is a movie
    review and will focus a bit more on the extremes of the two labels.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来测试一下这个创造力。我们决定将“负面/积极评论”作为我们的标签名称，但可以进一步改进。相反，我们可以通过使用“非常负面/积极的电影评论”使其更具体，更贴合我们的数据。这样，嵌入将捕捉到这是一个电影评论，并将更加关注两个标签的极端情况。
- en: 'We use the code we used before to see whether this actually works:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用之前的代码来查看这是否真的有效：
- en: '[PRE11]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: By only changing the phrasing of the labels, we increased our F-1 score quite
    a bit!
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 仅通过改变标签的措辞，我们大大提高了我们的 F-1 分数！
- en: Tip
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: In the example, we applied zero-shot classification by naming the labels and
    embedding them. When we have a few labeled examples, embedding them and adding
    them to the pipeline could help increase the performance. For example, we could
    average the embeddings of the labeled examples together with the label embeddings.
    We could even do a voting procedure by creating different types of representations
    (label embeddings, document embeddings, averaged embeddings, etc.) and see which
    label is most often found. This would make our zero-shot classification example
    a few-shot approach.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们通过命名标签并嵌入它们来应用零样本分类。当我们有少量标记的示例时，嵌入它们并将其添加到管道中可以帮助提高性能。例如，我们可以将标记示例的嵌入与标签嵌入进行平均。我们甚至可以通过创建不同类型的表示（标签嵌入、文档嵌入、平均嵌入等）进行投票程序，看看哪个标签最常被找到。这将使我们的零样本分类示例成为少样本方法。
- en: Natural Language Inference
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自然语言推理
- en: Zero-shot classification can also be done using natural language inference (NLI),
    which refers to the task of investigating whether, for a given premise, a hypothesis
    is true (entailment) or false (contradiction). [Figure 1-14](#fig_14_an_example_of_natural_language_inference_nli_th)
    shows a nice example how they relate to one another.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 零样本分类也可以使用自然语言推理（NLI）进行，这指的是调查给定前提时，假设是否为真（蕴含）或为假（矛盾）的任务。[图 1-14](#fig_14_an_example_of_natural_language_inference_nli_th)展示了它们之间的良好示例。
- en: '![An example of natural language inference  NLI . The hypothesis is contradicted
    by the premise and is not relevant to one another. ](assets/categorizing_text_559117_14.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![自然语言推理 NLI 的示例。假设与前提相矛盾，彼此之间没有关联。](assets/categorizing_text_559117_14.png)'
- en: Figure 1-14\. An example of natural language inference (NLI). The hypothesis
    is contradicted by the premise and is not relevant to one another.
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-14\. 自然语言推理（NLI）的示例。假设与前提相矛盾，彼此之间没有关联。
- en: 'NLI can be used for zero-shot classification by being a bit creative with how
    the premise/hypothesis pair is used, as demonstrated in [Figure 1-15](#fig_15_an_example_of_zero_shot_classification_with_natura).
    We use the input document, the review that we want to extract sentiment from and
    use that as our premise (yin2019benchmarking). Then, we create a hypothesis asking
    whether the premise is about our target label. In our movie reviews example, the
    hypothesis could be: “This example is a positive movie review”. When the model
    finds it to be an entailment, we can label the review as positive and negative
    when it is a contradiction. Using NLI for zero-shot classification is illustrated
    with an example in [Figure 1-15](#fig_15_an_example_of_zero_shot_classification_with_natura).'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: NLI 可以通过稍微创造性地使用前提/假设对进行零样本分类，如在[图 1-15](#fig_15_an_example_of_zero_shot_classification_with_natura)中所示。我们使用输入文档，即我们想要提取情感的评论，并将其作为我们的前提（yin2019benchmarking）。然后，我们创建一个假设，询问前提是否与我们的目标标签有关。在我们的电影评论示例中，假设可以是：“这个例子是一个积极的电影评论”。当模型发现这是一个蕴含关系时，我们可以将评论标记为正面，而当其为矛盾时则标记为负面。使用
    NLI 进行零样本分类的示例在[图 1-15](#fig_15_an_example_of_zero_shot_classification_with_natura)中进行了说明。
- en: '![An example of zero shot classification with natural language inference  NLI
    . The hypothesis is supported by the premise and the model will return that the
    review is indeed a positive movie review. ](assets/categorizing_text_559117_15.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![自然语言推理 NLI 的零样本分类示例。假设得到了前提的支持，模型会返回该评论确实是积极的电影评论。](assets/categorizing_text_559117_15.png)'
- en: Figure 1-15\. An example of zero-shot classification with natural language inference
    (NLI). The hypothesis is supported by the premise and the model will return that
    the review is indeed a positive movie review.
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-15\. 自然语言推理（NLI）中的零样本分类示例。假设得到了前提的支持，模型将返回该评论确实是积极的电影评论。
- en: Example
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例
- en: With transformers, loading and running a pre-trained NLI model is straightforward.
    Let’s select “`facebook``/bart-large-mnli`” as our pre-trained model. The model
    was trained on more than 400k premise/hypothesis pairs and should serve well for
    our use case.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 使用变压器，加载和运行预训练的 NLI 模型非常简单。我们选择“`facebook``/bart-large-mnli`”作为我们的预训练模型。该模型在超过
    40 万个前提/假设对上进行了训练，应该非常适合我们的用例。
- en: Note
  id: totrans-121
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'Over the course of the last few years, Hugging Face has strived to become the
    Github of Machine Learning by hosting pretty much everything related to Machine
    Learning. As a result, there is a large amount of pre-trained models available
    on their hub. For zero-shot classification tasks, you can follow this link: [https://huggingface.co/models?pipeline_tag=zero-shot-classification](https://huggingface.co/models?pipeline_tag=zero-shot-classification&sort=downloads).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几年中，Hugging Face 努力成为机器学习的 Github，托管与机器学习相关的几乎所有内容。因此，他们的中心提供了大量预训练模型。对于零样本分类任务，您可以查看此链接：[https://huggingface.co/models?pipeline_tag=zero-shot-classification](https://huggingface.co/models?pipeline_tag=zero-shot-classification&sort=downloads)。
- en: 'We load in our transformers pipeline and run it on our evaluation dataset:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们加载变压器管道，并在评估数据集上运行它：
- en: '[PRE12]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Since this is a zero-shot classification task, no training is necessary for
    us to get the predictions that we are interested in. The predictions variable
    contains not only the prediction but also a score indicating the probability of
    a candidate label (hypothesis) to entail the input document (premise).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个零样本分类任务，因此我们无需进行训练即可获得我们感兴趣的预测。预测变量不仅包含预测结果，还包含一个分数，指示候选标签（假设）蕴含输入文档（前提）的概率。
- en: '[PRE13]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Without any fine-tuning whatsoever, it received an F1-score of 0.81\. We might
    be able to increase this value depending on how we phrase the candidate labels.
    For example, see what happens if the candidate labels were simply “negative” and
    “positive” instead.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 完全没有进行微调，它的 F1 分数达到了 0.81\. 根据我们措辞候选标签的方式，可能能够提高这个值。例如，如果候选标签简单为“消极”和“积极”，会发生什么情况？
- en: Tip
  id: totrans-128
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Another great pre-trained model for zero-shot classification is sentence-transformers’
    cross-encoder, namely '`cross-encoder/``nli``-deberta-base`‘. Since training a
    sentence-transformers model focuses on pairs of sentences, it naturally lends
    itself to zero-shot classification tasks that leverage premise/hypothesis pairs.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个优秀的零样本分类预训练模型是 sentence-transformers 的交叉编码器，即 '`cross-encoder/``nli``-deberta-base`'。由于训练
    sentence-transformers 模型侧重于句子对，因此它自然而然地适用于利用前提/假设对的零样本分类任务。
- en: Classification with Generative Models
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用生成模型进行分类
- en: Classification with generative large language models, such as OpenAI’s GPT models,
    works a bit differently from what we have done thus far. Instead of fine-tuning
    a model to our data, we use the model and try to guide it toward the type of answers
    that we are looking for.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 使用生成性大型语言模型（如 OpenAI 的 GPT 模型）进行分类，与我们之前所做的稍有不同。我们不是对模型进行微调以适应我们的数据，而是使用模型并尝试引导它朝向我们所寻找的答案类型。
- en: This guiding process is done mainly through the prompts that you give such as
    a model. Optimizing the prompts such that the model understands what kind of answer
    you are looking for is called **prompt engineering**. This section will demonstrate
    how we can leverage generative models to perform a wide variety of classification
    tasks.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这个引导过程主要通过您提供的提示来完成，例如模型。优化提示以使模型理解您所寻找的答案类型被称为 **提示工程**。本节将演示如何利用生成模型执行各种分类任务。
- en: This is especially true for extremely large language models, such as GPT-3\.
    An excellent paper and read on this subject, “Language Models are Few-Shot Learners”,
    describes that these models are competitive on downstream tasks whilst needing
    less task-specific data (brown2020language).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 对于极大型语言模型，如 GPT-3，这一点尤其真实。一篇优秀的论文和相关阅读，“语言模型是少样本学习者”，描述了这些模型在下游任务上具有竞争力，同时需要更少的特定任务数据
    (brown2020language)。
- en: In-Context Learning
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 上下文学习
- en: What makes generative models so interesting is their ability to follow the prompts
    they are given. A generative model can even do something entirely new by merely
    being shown a few examples of this new task. This process is also called in-context
    learning and refers to the process of having the model learn or do something new
    without actually fine-tuning it.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型如此有趣的原因在于它们能够遵循给定的提示。生成模型甚至可以通过仅仅展示几个新任务的示例而做出完全新的事情。这一过程也称为上下文学习，指的是在不实际微调模型的情况下，让模型学习或做一些新的事情。
- en: For example, if we ask a generative model to write a haiku (a traditional Japanese
    poetic form), it might not be able to if it has not seen a haiku before. However,
    if the prompt contains a few examples of what a haiku is, then the model “learns”
    from that and is able to create haikus.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们要求生成模型写一首俳句（传统的日本诗歌形式），如果它之前没有见过俳句，可能无法做到。然而，如果提示中包含几条俳句的示例，那么模型就会“学习”并能够创作俳句。
- en: We purposely put “learning” in quotation marks since the model is not actually
    learning but following examples. After successfully having generated the haikus,
    we would still need to continuously provide it with examples as the internal model
    was not updated. These examples of in-context learning are shown in [Figure 1-16](#fig_16_zero_shot_and_few_shot_classification_through_prom)
    and demonstrate the creativity needed to create successful and performant prompts.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们故意将“学习”放在引号中，因为模型实际上并没有学习，而是遵循示例。在成功生成俳句后，我们仍需不断提供示例，因为内部模型并未更新。上下文学习的这些示例显示在[图1-16](#fig_16_zero_shot_and_few_shot_classification_through_prom)中，展示了创作成功且高效提示所需的创造力。
- en: '![Zero shot and few shot classification through prompt engineering with generative
    models. ](assets/categorizing_text_559117_16.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![通过提示工程与生成模型进行零-shot和少-shot分类。](assets/categorizing_text_559117_16.png)'
- en: Figure 1-16\. Zero-shot and few-shot classification through prompt engineering
    with generative models.
  id: totrans-139
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-16\. 通过提示工程与生成模型进行零-shot和少-shot分类。
- en: In-context learning is especially helpful in few-shot classification tasks where
    we have a small number of examples that the generative model can follow.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文学习在少量示例的少-shot分类任务中尤其有用，生成模型可以跟随这些少量示例。
- en: Not needing to fine-tune the internal model is a major advantage of in-context
    learning. These generative models are often quite large in size and are difficult
    to run on consumer hardware let alone fine-tune them. Optimizing your prompts
    to guide the generative model is relatively low-effort and often does not need
    somebody well-versed in generative AI.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要对内部模型进行微调是上下文学习的一个主要优势。这些生成模型通常体积庞大，难以在消费者硬件上运行，更不用说微调它们了。优化你的提示以引导生成模型相对容易，通常不需要精通生成AI的人。
- en: Example
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例
- en: Before we go into the examples of in-context learning, we first create a function
    that allows us to perform prediction with OpenAI’s GPT models.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进入上下文学习的示例之前，首先创建一个允许我们使用OpenAI的GPT模型进行预测的函数。
- en: '[PRE14]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This function allows us to pass a specific `prompt` and `document` for which
    we want to create a prediction. The `tenacity` module that you also see here allows
    us to deal with rate limit errors, which happen when you call the API too often.
    OpenAI, and other external APIs, often want to limit the rate at which you call
    their API so as not to overload their servers.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数允许我们传递一个特定的`prompt`和`document`，用于我们想要创建预测的内容。你在这里看到的`tenacity`模块帮助我们处理速率限制错误，这种错误发生在你调用API过于频繁时。OpenAI和其他外部API通常希望限制调用其API的速率，以免过载其服务器。
- en: This `tenacity` module is essentially a “retrying module” that allows us to
    retry API calls in specific ways. Here, we implemented something called **exponential
    backoff** to our `gpt_prediction` function. Exponential backoff performs a short
    sleep when we hit a rate limit error and then retries the unsuccessful request.
    Every time the request is unsuccessful, the sleep length is increased until the
    request is successful or we hit a maximum number of retries.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`tenacity`模块本质上是一个“重试模块”，允许我们以特定方式重试API调用。在这里，我们在`gpt_prediction`函数中实现了一种叫做**指数退避**的机制。当我们遇到速率限制错误时，指数退避会进行短暂休眠，然后重试未成功的请求。每当请求未成功时，休眠时间会增加，直到请求成功或达到最大重试次数。
- en: One easy way to avoid rate limit errors is to automatically retry requests with
    a random exponential backoff. Retrying with exponential backoff means performing
    a short sleep when a rate limit error is hit, then retrying the unsuccessful request.
    If the request is still unsuccessful, the sleep length is increased and the process
    is repeated. This continues until the request is successful or until a maximum
    number of retries is reached.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 避免速率限制错误的一种简单方法是自动重试请求并使用随机指数退避。当遇到速率限制错误时，重试时会进行短暂的休眠，然后重试失败的请求。如果请求仍然失败，休眠时间将增加，并重复此过程。直到请求成功或达到最大重试次数为止。
- en: 'Lastly, we need to sign in to OpenAI’s API with an API-key that you can get
    from your account:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要使用从你的账户获取的 API 密钥登录 OpenAI 的 API：
- en: '[PRE15]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Warning
  id: totrans-150
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: When using external APIs, always keep track of your usage. External APIs, such
    as OpenAI or Cohere, can quickly become costly if you request too often from their
    APIs.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用外部 API 时，始终跟踪你的使用情况。如果你频繁请求外部 API，如 OpenAI 或 Cohere，费用会迅速增加。
- en: Zero-shot Classification
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 零样本分类
- en: Zero-shot classification with generative models is essentially what we typically
    do when interacting with these types of models, simply ask them if they can do
    something. In our examples, we ask the model whether a specific document is a
    positive or negative movie review.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 使用生成模型进行零样本分类本质上是我们与这些模型交互时通常所做的，简单地询问它们是否能执行某项任务。在我们的示例中，我们询问模型特定文档是否为积极或消极的电影评论。
- en: 'To do so, we create a base template for our zero-shot classification prompt
    and ask the model if it can predict whether a review is positive or negative:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们创建一个零样本分类提示的基础模板，并询问模型是否能预测评论是积极还是消极：
- en: '[PRE16]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: You might have noticed that we explicitly say to not give any other answers.
    These generative models tend to have a mind of their own and return large explanations
    as to why something is or isn’t negative. Since we are evaluating its results,
    we want either a 0 or a 1 to be returned.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能注意到我们明确要求不要提供其他答案。这些生成模型往往有自己的想法，会返回大量关于某事为何是或不是消极的解释。由于我们在评估其结果，我们希望返回的是
    0 或 1。
- en: 'Next, let’s see if it can correctly predict that the review “unpretentious,
    charming, quickie, original” is positive:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看它是否能正确预测评论“谦逊、迷人、快速、原创”是积极的：
- en: '[PRE17]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The output indeed shows that the review was labeled by OpenAI’s model as positive!
    Using this prompt template, we can insert any document at the “[DOCUMENT]” tag.
    These models have token limits which means that we might not be able to insert
    an entire book into the prompt. Fortunately, reviews tend not to be the sizes
    of books but are often quite short.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 输出确实显示该评论被 OpenAI 的模型标记为积极！使用此提示模板，我们可以在“[DOCUMENT]”标签中插入任何文档。这些模型有令牌限制，这意味着我们可能无法将整本书插入提示中。幸运的是，评论通常不会像书那样长，而是相对较短。
- en: 'Next, we can run this for all reviews in the evaluation dataset and look at
    its performance. Do note though that this requires 300 requests to OpenAI’s API:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以对评估数据集中的所有评论进行此操作，并观察其性能。不过请注意，这需要向 OpenAI 的 API 发送 300 个请求：
- en: '[PRE18]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: An F-1 score of 0.91! That is the highest we have seen thus far and is quite
    impressive considering we did not fine-tune the model at all.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: F-1 分数为 0.91！这是我们迄今为止看到的最高分数，考虑到我们完全没有对模型进行微调，这实在令人印象深刻。
- en: Note
  id: totrans-163
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Although this zero-shot classification with GPT has shown high performance,
    it should be noted that fine-tuning generally outperforms in-context learning
    as presented in this section. This is especially true if domain-specific data
    is involved which the model during pre-training is unlikely to have seen. A model’s
    adaptability to task-specific nuances might be limited when its parameters are
    not updated for the task at hand. Preferably, we would want to fine-tune this
    GPT model on this data to improve its performance even further!
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这种基于 GPT 的零样本分类表现出色，但需要注意的是，微调通常优于本节中所述的上下文学习。特别是在涉及特定领域数据时，模型在预训练期间不太可能见过这些数据。当模型的参数未针对当前任务进行更新时，其对任务特定细微差别的适应性可能有限。理想情况下，我们希望在这些数据上对
    GPT 模型进行微调，以进一步提升其性能！
- en: Few-shot Classification
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 少样本分类
- en: In-context learning works especially well when we perform few-shot classification.
    Compared to zero-shot classification, we simply add a few examples of movie reviews
    as a way to guide the generative model. By doing so, it has a better understanding
    of the task that we want to accomplish.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在上下文学习中，少样本分类效果尤其好。与零样本分类相比，我们只需添加一些电影评论示例，以引导生成模型。这样，它对我们想要完成的任务有了更好的理解。
- en: 'We start by updating our prompt template to include a few hand-picked examples:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先更新我们的提示模板，以包含几个精心挑选的示例：
- en: '[PRE19]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We picked two examples per class as a quick way to guide the model toward assigning
    sentiment to movie reviews.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为每个类别选择了两个示例，以快速引导模型为电影评论分配情感。
- en: Note
  id: totrans-170
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Since we added a few examples to the prompt, the generative model consumes more
    tokens and as a result could increase the costs of requesting the API. However,
    that is relatively little compared to fine-tuning and updating the entire model.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在提示中添加了一些示例，生成模型消耗了更多的标记，因此可能会增加请求 API 的成本。然而，相较于微调和更新整个模型，这相对较少。
- en: 'Prediction is the same as before but replacing the zero-shot prompt with the
    few-shot prompt:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 预测与之前相同，但将零样本提示替换为少样本提示：
- en: '[PRE20]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Unsurprisingly, it correctly assigned sentiment to the review. The more difficult
    or complex the task is, the bigger the effect of providing examples, especially
    if they are high-quality.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 不出所料，它正确地为评论分配了情感。任务越困难或复杂，提供示例的效果就越显著，尤其是当示例质量较高时。
- en: 'As before, let’s run the improved prompt against the entire evaluation dataset:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 和以前一样，让我们对整个评估数据集运行改进的提示：
- en: '[PRE21]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The F1-score is now 0.92 which is a very slight increase compared to what we
    had before. This is not unexpected since its score was already quite high and
    the task at hand was not particularly complex.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的 F1 分数为 0.92，与之前相比略有提高。这并不意外，因为之前的分数已经相当高，而手头的任务也不是特别复杂。
- en: Note
  id: totrans-178
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: We can extend the examples of in-context learning to multi-label classification
    by engineering the prompt. For example, we can ask the model to choose one or
    multiple labels and return them separated by commas.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过设计提示将上下文学习的示例扩展到多标签分类。例如，我们可以要求模型选择一个或多个标签，并将它们用逗号分隔返回。
- en: Named Entity Recognition
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 命名实体识别
- en: In the previous examples, we have tried to classify entire texts, such as reviews.
    There are many cases though where we are more interested in specific information
    inside those texts. We may want to extract certain medications from textual electronic
    health records or find out which organizations are mentioned in news posts.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的示例中，我们尝试对整个文本（如评论）进行分类。然而，有许多情况下，我们更关注这些文本中的具体信息。我们可能希望从文本电子健康记录中提取某些药物，或找出新闻帖子中提到的组织。
- en: These tasks are typically referred to as token classification or Named Entity
    Recognition (NER) which involves detecting these entities in text. As illustrated
    in [Figure 1-17](#fig_17_an_example_of_named_entity_recognition_that_detect),
    instead of classifying an entire text, we are now going to classify certain tokens
    or token sets.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这些任务通常被称为标记分类或命名实体识别（NER），涉及在文本中检测这些实体。如[图 1-17](#fig_17_an_example_of_named_entity_recognition_that_detect)所示，我们现在将对某些标记或标记集进行分类，而不是对整个文本进行分类。
- en: '![An example of named entity recognition that detects the entities  place  and  time
    . ](assets/categorizing_text_559117_17.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![一个识别“地点”和“时间”实体的命名实体识别示例。](assets/categorizing_text_559117_17.png)'
- en: Figure 1-17\. An example of named entity recognition that detects the entities
    “place” and “time”.
  id: totrans-184
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-17\. 一个识别“地点”和“时间”实体的命名实体识别示例。
- en: When we think about token classification, one major framework comes into mind,
    namely SpaCy ([https://spacy.io/](https://spacy.io/)). It is an incredible package
    for performing many industrial-strength NLP applications and has been the go-to
    framework for NER tasks. So, let’s use it!
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想到标记分类时，一个主要框架浮现在脑海中，即 SpaCy ([https://spacy.io/](https://spacy.io/))。它是执行许多工业级自然语言处理应用的绝佳工具，并且一直是命名实体识别（NER）任务的首选框架。所以，让我们来使用它吧！
- en: Example
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例
- en: 'To use OpenAI’s models with SpaCy, we will first need to save the API key as
    an environment variable. This makes it easier for SpaCy to access it without the
    need to save it locally:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 SpaCy 中使用 OpenAI 的模型，我们首先需要将 API 密钥保存为环境变量。这使得 SpaCy 更容易访问，而无需在本地保存：
- en: '[PRE22]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, we need to configure our SpaCy pipeline. A “task” and a “backend” will
    need to be defined. The “task” is what we want the SpaCy pipeline to do, which
    is Named Entity Recognition. The “backend” is the underlying LLM that is used
    to perform the “task” which is OpenAI’s GPT-3.5-turbo model. In the task, we can
    create any labels that we would like to extract from our text. Let’s assume that
    we have information about patients and we would like to extract some personal
    information but also the disease and symptoms they developed. We create the entities
    date, age, location, disease, and symptom:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要配置我们的SpaCy管道。需要定义一个“任务”和一个“后端”。“任务”是我们希望SpaCy管道执行的内容，即命名实体识别。“后端”是用于执行该“任务”的基础LLM，即OpenAI的GPT-3.5-turbo模型。在任务中，我们可以创建任何希望从文本中提取的标签。假设我们有关于患者的信息，我们希望提取一些个人信息，以及他们所患的疾病和症状。我们创建实体：日期、年龄、地点、疾病和症状：
- en: '[PRE23]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Next, we only need two lines of code to automatically extract the entities
    that we are interested in:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们只需要两行代码即可自动提取我们感兴趣的实体：
- en: '[PRE24]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'It seems to correctly extract the entities but it is difficult to immediately
    see if everything worked out correctly. Fortunately, SpaCy has a display function
    that allows us to visualize the entities found in the document ([Figure 1-18](#fig_18_the_output_of_spacy_using_openai_s_gpt_3_5_model)):'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 它似乎能够正确提取实体，但很难立即看到一切是否顺利进行。幸运的是，SpaCy有一个显示功能，可以让我们可视化文档中找到的实体（[图 1-18](#fig_18_the_output_of_spacy_using_openai_s_gpt_3_5_model)）：
- en: '[PRE25]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![The output of SpaCy using OpenAI s GPT 3.5 model. Without any training  it
    correctly identifies our custom entities. ](assets/categorizing_text_559117_18.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![使用OpenAI的GPT-3.5模型的SpaCy输出。没有任何训练，它正确识别了我们的自定义实体。](assets/categorizing_text_559117_18.png)'
- en: Figure 1-18\. The output of SpaCy using OpenAI’s GPT-3.5 model. Without any
    training, it correctly identifies our custom entities.
  id: totrans-196
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-18\. 使用OpenAI的GPT-3.5模型的SpaCy输出。没有任何训练，它正确识别了我们的自定义实体。
- en: That is much better! Figure 2-X shows that we can clearly see that the model
    has correctly identified our custom entities. Without any fine-tuning or training
    of the model, we can easily detect entities that we are interested in.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这要好得多！图2-X显示我们可以清晰地看到模型正确识别了我们的自定义实体。在没有任何微调或模型训练的情况下，我们可以轻松检测到我们感兴趣的实体。
- en: Tip
  id: totrans-198
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Training a NER model from scratch with SpaCy is not possible with only a few
    lines of code but it is also by no means difficult! Their [documentation and tutorials](https://spacy.io/usage/training)
    are, in our opinions, state-of-the-art and do an excellent job of explaining how
    to create a custom model.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 从头开始使用SpaCy训练一个NER模型并不能仅仅通过几行代码实现，但这也绝对不是困难的事情！他们的[文档和教程](https://spacy.io/usage/training)在我们看来是最先进的，并且在解释如何创建自定义模型方面做得非常出色。
- en: Summary
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we saw many different techniques for performing a wide variety
    of classification tasks. From fine-tuning your entire model to no tuning at all!
    Classifying textual data is not as straightforward as it may seem on the surface
    and there is an incredible amount of creative techniques for doing so.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看到了多种不同的技术来执行各种分类任务。从微调整个模型到完全不调优！对文本数据的分类并不像表面上看起来那么简单，而且有大量创造性的技术可以实现这一目标。
- en: In the next chapter, we will continue with classification but focus instead
    on unsupervised classification. What can we do if we have textual data without
    any labels? What information can we extract? We will focus on clustering our data
    as well as naming the clusters with topic modeling techniques.****
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将继续讨论分类，但将重点放在无监督分类上。如果我们有没有任何标签的文本数据，我们该怎么办？我们可以提取什么信息？我们将重点关注对数据进行聚类，以及使用主题建模技术为聚类命名。
