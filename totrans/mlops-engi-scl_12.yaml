- en: 9 Feature selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Understanding principles for feature selection and feature engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying feature selection principles to case studies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharpening feature selection skills based on case study analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus far, you have been using the original (raw) data values from the DC taxi
    data set as the features for your machine learning models. A *feature* is a value
    or a collection of values used as an input to a machine learning model during
    both the training and inference phases of machine learning (see appendix A). *Feature
    engineering*, the process of selecting, designing, and implementing synthetic
    (made-up) features using raw data values, can significantly improve the machine
    learning performance of your models. Some examples of feature engineering are
    simple, formulaic transformations of the original data values, for instance rescaling
    arbitrary numeric values to a range from —1 to 1\. *Feature selection* (also known
    as *feature design*), the initial phase of feature engineering, is the more creative
    part of the effort and involves specification of features that capture human knowledge
    or intuition about the data set, such as choosing a feature that measures the
    distance between pickup and drop-off locations for each ride in the taxi trips
    data set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Indiscriminately adding a large number of features to your project data set
    can be a costly mistake (see the “curse of dimensionality” problem). Feature “over-engineering”
    may result in overfitting as well as in an overall decline of your machine learning
    model performance. Which begs the question: what guiding principles can apply
    to select the right features so that you can avoid the dreaded feature over-engineering?
    This chapter uses case studies to introduce you to these principles and illustrates
    how you can apply them in practice.'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter covers three case studies that span the financial, advertising,
    and mobile gaming industries. For each case study, you are given a description
    of a machine learning project for the industry as well as a high-level specification
    of the machine learning model you are expected to train for the project. Then,
    you are given descriptions of candidate features for the project. Finally, a discussion
    section for each case study describes how to apply the five guiding principles
    to help you decide whether you should select the candidate feature.
  prefs: []
  type: TYPE_NORMAL
- en: 9.1 Guiding principles for feature selection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section introduces the five guiding principles to help you choose the
    right features for your machine learning project. While I have not seen these
    principles codified as an industry standard, they condense over a decade of my
    experience in selecting features for data science, machine learning, and deep
    learning projects. The remainder of this section explains these guiding principles
    in more detail so that in section 9.2, you can apply them to the case studies
    and to the concrete examples of candidate features. According to these principles,
    a candidate feature should be:'
  prefs: []
  type: TYPE_NORMAL
- en: Related to the label
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recorded before inference time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supported by abundant examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Expressed as a number with a meaningful scale
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on expert insights about the project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 9.1.1 Related to the label
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section teaches you what to consider when evaluating a relationship between
    a feature (or a potential feature) and a label so that you can select and prioritize
    features for your feature engineering efforts. Before you decide to include a
    potential feature in a machine learning model training experiment, make sure you
    can articulate a rationale (a justification) explaining why the feature is related
    to the label value. Articulating the rationale can help you convince yourself
    (and in an ideal case, an impartial observer) that the feature is in fact relevant
    to the problem you are attempting to solve using machine learning. Of course,
    some rationales are better than others: “why not?” does not make for a strong
    rationale for a feature. In general, weak rationales translate to an excessive
    quantity of potential features with varying strengths of relatedness to the label.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that having a rationale for a feature is also important because the relationship
    between the candidate feature and the label can change depending on your problem.
    In practice, changing the question you are answering using a data set can change
    whether a relationship exists between the candidate feature and the label. For
    example, when estimating the taxi fare for a trip across DC, a feature value of
    a distance between the pickup and drop-off locations is related to the taxi fare
    estimate. However, if instead of estimating the taxi fare, you decide to use the
    distance feature to estimate the number of taxi pickups at a given location in
    DC at a given time, then the feature loses the relatedness to the label. While
    it may seem obvious that a feature selected for a different project may become
    useless when changing the label, in practice, training data sets and feature stores
    are re-used across machine learning projects, leading to inadvertent re-use of
    irrelevant and potentially harmful (as illustrated in the rest of this chapter)
    features.
  prefs: []
  type: TYPE_NORMAL
- en: The meaning of the word *related* when used in this section translates to a
    variety of possibilities; for instance, *statistical correlation* is one kind
    of relatedness. As you know, statistical correlation means that causation may
    or may not play a role in the relationship between a feature and a label.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a well-known example: according to the standard Pearson measure of
    statistical correlation, reading ability in children is correlated with their
    shoe size. In other words, according to classical statistics, if you wish to estimate
    a child’s shoe size, you can use the score from the child’s latest reading test.
    Of course, there is no causation in the relationship between the shoe size and
    the reading ability variables; the correlation exists due to the underlying (confounding)
    variable of the child’s age.'
  prefs: []
  type: TYPE_NORMAL
- en: The shoe size estimate example is illustrated in figure 9.1\. Notice that the
    causal relationship between the child’s age and shoe size, as well as the child’s
    age and the reading score, also translate to statistical correlation between these
    pairs of variables. So, while a statistical correlation between a candidate feature
    and the label can serve as a baseline rationale for the feature, a stronger justification
    can be based on a causal relationship between the feature and the label.
  prefs: []
  type: TYPE_NORMAL
- en: '![09-01](Images/09-01.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 A feature can be based on a child’s reading test score or age variables,
    but the age-based feature is more strongly related to the shoe size due to a counterfactual
    causal relationship between the age and the shoe size.
  prefs: []
  type: TYPE_NORMAL
- en: 'Is it possible to tell correlation apart from causation? It depends on your
    definition of causality. Most contemporary machine learning and data science practitioners
    in the industry and in academia use a kind of causality known as *counterfactual
    causality*.[¹](#pgfId-1012029) You can decide whether a counterfactual causal
    relationship exists between the candidate cause and effect variables by answering
    a hypothetical question: all other things being equal, if an omnipotent actor
    were to intervene to change only the cause, would the effect inevitably change?
    Notice that this kind of causality does not exist for the relationship between
    the reading test scores and the shoe sizes: had someone intervened and changed
    the average test scores for kids of a certain age, their shoe size would not change.
    In contrast, substituting a cohort of younger kids with a cohort of older kids
    translates to both higher average reading test scores (assuming the tests are
    not age-adjusted) and larger average shoe sizes.'
  prefs: []
  type: TYPE_NORMAL
- en: When articulating the relatedness between a candidate feature and a label, it
    is useful to identify and prioritize features that have counterfactual causal
    relationships to the label over the features that demonstrate a correlation or
    an unclear relationship to the label.
  prefs: []
  type: TYPE_NORMAL
- en: 9.1.2 Recorded before inference time
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section teaches you how to navigate around a common pitfall in feature
    selection: using features that are known at training time but are unavailable
    or difficult to obtain once a trained model is deployed to production.'
  prefs: []
  type: TYPE_NORMAL
- en: The feature value you are considering must be available at inference time, once
    your machine learning model is processing data beyond the training and test data
    sets used to create the model. This is an important but often overlooked aspect
    of feature selection, particularly since training data sets are retrospective
    and usually do not contain information about the order in which data values become
    available. For example, a training data set describing the history of a company’s
    sales deals often does not capture the fact that the customer’s name and contact
    information are available to the company prior to the dollar amount of a closed
    sales deal.
  prefs: []
  type: TYPE_NORMAL
- en: What makes this issue subtle is that retroactively looking at data from past
    events, as recorded in the training data set, the sequencing of the events may
    not be immediately obvious, and you may inadvertently use future information at
    inference time. For example, suppose you are attempting to estimate the weight
    of a newborn baby and one of the candidate features is the duration of pregnancy.
  prefs: []
  type: TYPE_NORMAL
- en: Note Pre-mature newborns have lower-than-average weights while babies born after
    42 weeks of pregnancy are heavier on average, so the duration of pregnancy in
    terms of the number of the weeks seems to be a useful feature to select.
  prefs: []
  type: TYPE_NORMAL
- en: You should not be surprised to find the duration of pregnancy (in terms of weeks)
    in the training data set since it is a historical measure that makes sense to
    record at the time of the birth. However, if you are creating a model to estimate
    the expected weight of the newborn during a routine doctor’s visit halfway through
    the pregnancy, the number of the weeks of pregnancy at birth is unknown.
  prefs: []
  type: TYPE_NORMAL
- en: The example with the estimation of the newborn weight is just one instance of
    an issue known as *data leakage*. Other examples of data leakage include duplication
    of observations across training, validation, and test data sets. In general, data
    leakage occurs when the information about the label value is inadvertently included
    or “leaked” into the training data set. Symptoms of data leakage include inflated
    machine learning model performance on the training data set and even complete
    failures of the model once it is deployed to production. For instance, this issue
    can arise when working with features from the well-known Titanic data set,[²](#pgfId-1013110)
    as illustrated in figure 9.2.
  prefs: []
  type: TYPE_NORMAL
- en: '![09-02](Images/09-02.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 In the subset of features from the Titanic data set, age and gender
    are known before the survival outcome for the passengers. The values for the boat
    and body features are recorded for a passenger depending on whether the passenger
    survived or perished. Hence, boat and body features “leak” data about the survival
    label.
  prefs: []
  type: TYPE_NORMAL
- en: It is counterintuitive to reason about prediction of past events; however, the
    Titanic data set invites you to do exactly that. You can train a machine learning
    model to predict a binary survival outcome (whether a person survived or perished)
    for passengers on the Titanic during the fateful crossing of the Atlantic in April
    of 1912\. While the entire data set contains 13 features and a label, figure 9.2
    shows just the 4 features that are relevant to this example along with the label
    (survival). The survival label in the data set uses a value of 1 to indicate that
    a passenger survived while the value of 0 represents that a passenger perished.
    The age feature is numeric with the age of the passengers at the time when they
    boarded the ship, while the gender feature is limited to male and female values
    represented using 1 and 0, respectively. The boat and body features are both categorical
    and encoded as strings. The boat feature stores the identifier of the boat (for
    example "11") on which a passenger was rescued. The morbid body feature stores
    values like "328" that specify the identifiers assigned to passengers who perished.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid the counterintuitive implication of using the word *predict*, it is
    helpful to think about inferring the survival outcome for each passenger when
    considering candidate features for training the machine learning model. From the
    standpoint of time sequencing, it is reasonable to infer survival based on the
    features age and gender because both existed for each passenger prior to their
    survival outcome. This is shown in figure 9.2 using arrows pointing from age and
    gender features to the survived label. Hence, a machine learning model can be
    trained to use age and gender features since both should be available at inference
    time.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, once a passenger on the Titanic survived or perished, that passenger
    survived on a rescue boat or ended up with a body tag. This is shown using the
    arrows pointing from the survival label to body and boat features in figure 9.2\.
    Both body and boat feature values should not be used to train a machine learning
    model to infer the survival outcome for passengers on the Titanic. Of course,
    we don’t expect more events like this to occur, and hence such a model will never
    be used to infer survival outcomes for any passengers other than those who traveled
    on the Titanic. The goal of this example is to introduce you to the importance
    of understanding the features in your data set and the time sequencing relationship
    across the features.
  prefs: []
  type: TYPE_NORMAL
- en: In other cases, the feature value may not be available at inference time for
    legal or ethical reasons. For example, for a European Union (EU)-based company
    it is perfectly legal to record the birth date of job applicants. If you are using
    this type of human resources information in the EU to estimate the likelihood
    that an applicant accepts the job offer, the age of the applicant is a reasonable
    feature to select. However, as soon as you attempt to apply the same feature in
    the United States, access to an applicant’s birth date might be against the law
    because, in the US, it is illegal to discriminate on the basis of age when making
    a hiring decision. Moreover, features that leak information about the birth date,
    such as the year of high school or college graduation, can also cause legal concerns
    for the business. So, if you are attempting to adapt a machine learning system
    built in the EU to the United States, you should re-evaluate whether the features
    in the training data set are permitted by your organization. As you can imagine,
    human resources data sets can be full of information (e.g., race or health records)
    but are illegal or unethical as features for training a machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: 9.1.3 Supported by abundant examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section explores what can go wrong when selecting features that have too
    many missing values or features that have too few distinct values.
  prefs: []
  type: TYPE_NORMAL
- en: It is embarrassingly simple to introduce a feature to a training data set by
    simply adding a column of NaN or NULL values. While such a feature is obviously
    useless, this example serves as a reminder that more does not mean better when
    it comes to increasing the number of features in a training data set. The other
    extreme involves adding features that in combination serve as a unique identifier
    for a label. As a worst-case example for feature engineering, consider a training
    data set for a classification problem where the categorical label takes on 2^N
    distinct values. Since N binary features can encode 2^N possibilities, each of
    the label values might be uniquely identified by the binary values of the N feature
    columns.
  prefs: []
  type: TYPE_NORMAL
- en: Of course such extreme scenarios of feature over-engineering rarely, if ever,
    occur in practice, but they make convenient reference points for comparison again
    your own feature engineering efforts. For example, if you are selecting a feature,
    do you expect to have a significant fraction (greater than 5%) of the feature
    values in the training data set to have values other than NaN? If you are selecting
    multiple features, would the feature values taken together result in a unique
    identifier for the label value in the training data set but not in the test data
    set?
  prefs: []
  type: TYPE_NORMAL
- en: To answer these questions, you need to ensure that as you are considering candidate
    features you are also maintaining statistics on the missing values of the features
    along with counts of the cross product of the sets of feature values for each
    of the label values.
  prefs: []
  type: TYPE_NORMAL
- en: 9.1.4 Expressed as a number with a meaningful scale
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section teaches a convenient rule of thumb you can use to check for whether
    a feature can be correctly expressed as a number for using with machine learning
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: As you know from chapter 1, this book focuses on supervised machine learning
    from structured data sets. For the purposes of feature engineering, this means
    that if the raw data you are planning to use for training machine learning models
    contains unstructured content such as video, images, audio, or natural language
    text, then before you can start with the feature engineering steps described in
    this book, you must convert the corresponding unstructured data values to a numeric
    representation. Specific techniques for these conversions, such as word embeddings
    for natural language text or image classifications for photographic data, are
    outside the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Having a project data set that consists of numeric values is a prerequisite
    to feature engineering since supervised machine learning models amount to sequences
    of arithmetic operations on numeric (either continuous or categorical per the
    the definition in appendix A) feature values to estimate label values. However,
    even if your raw data values are numeric (i.e., expressed using a number) that
    does not mean that the corresponding features that you use to train a machine
    learning model have meaningful numeric scales and magnitudes. If you are not convinced,
    consider that letters A and B are represented as numbers 65 and 66, respectively,
    by the ASCII encoding standard. The sum of the encoding, 131 (65 + 66) corresponds
    to the character â (letter a with a circumflex accent), which is not a meaningful
    result.
  prefs: []
  type: TYPE_NORMAL
- en: If you are comfortable applying the formal definitions from figure A.9 in appendix
    A to this example, you should recognize that ASCII encodes a categorical variable
    that describes the finite dictionary of the ASCII characters. As a general rule
    of thumb, you can check whether a numeric value can be treated as a continuous
    variable by performing basic arithmetic operations on the values to confirm whether
    you can obtain meaningful results.
  prefs: []
  type: TYPE_NORMAL
- en: 9.1.5 Based on expert insights about the project
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section describes the most important guiding principle for feature design,
    selection, and engineering—the one that can have the most positive impact on the
    performance of your machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: When working on machine learning projects with subject matter experts (SMEs)
    who have deep domain expertise in their respective industries, I find that the
    most productive way to encourage the SMEs to propose useful features is to ask
    them, “What information would you share with your team to help them better estimate
    the label value?” This style of conversation motivates the SMEs to take their
    mind away from the complexity of machine learning system design and instead think
    about the problem in natural, human-centric terms. Next, I ask, “How can this
    information be gleaned from the existing data in the project data set?”
  prefs: []
  type: TYPE_NORMAL
- en: Taken together, answers to these questions provide a pathway to generate candidate
    features from the project data set. For example, suppose you are working on the
    problem of classifying photos of cars taken in cities around the world to estimate
    whether a photo contains or does not contain a taxi from the respective city.
    While it is possible to set out on a complex engineering exercise, extracting
    text of medallion number, license plates, or other unique taxi identifiers to
    join this information with a municipal data set of taxis, there is an easier path
    forward. Human SMEs who know the city can immediately spot a taxi by color, such
    as yellow in New York City or black in London. This rule of thumb is something
    an SME can easily share with someone who never traveled to the city and help them
    identify a taxi from a sea of cars in a street.
  prefs: []
  type: TYPE_NORMAL
- en: Successful feature selection is more than just algorithmic data processing;
    it is a creative process based on applying human common sense about the world
    and insights about the problem domain to the machine learning project. Keep in
    mind that the case study approach in this section can introduce you to the basics
    of ideation and design for feature selection. However, it is not a replacement
    for the real-world experience of putting a machine learning system in production.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2 Feature selection case studies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section introduces applications of supervised machine learning across three
    different case studies spanning the financial, advertising, and mobile gaming
    industries. In each case study, you will learn about an industry-specific machine
    learning project so that you can learn how to apply feature selection principles
    to other projects.
  prefs: []
  type: TYPE_NORMAL
- en: '*Financial: Credit card fraud classification*. You are working with a financial
    industry company that issues credit cards to its customers and monitors customer
    credit card transactions for signs of fraud. The purpose of your supervised machine
    learning classification model is to estimate whether a given transaction is fraudulent
    or non-fraudulent. To keep the feature engineering exercise simple, assume that
    you are working with a balanced data set (which isn’t true in production) of fraudulent
    versus non-fraudulent examples, so the accuracy of your classifier is a meaningful
    machine learning model performance metric.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Advertising: Online banner advertisement clicks estimation*. You are working
    with an advertising industry company that manages online ad banners for its customers
    and charges them whenever a viewer clicks on the banner. Since the company has
    a tool that designs the banner ads for the customers (for an example of a banner
    ad designer, look at [https://github.com/osipov/banner-designer](https://github.com/osipov/banner-designer)),
    it has a data set of the banner ad designs and the corresponding number of clicks
    that the ad received during the campaign. The purpose of your supervised machine
    learning regression model is to estimate the total number of clicks an ad should
    receive based on the banner design. The features for the model should be selected
    based on the ad content and design settings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Mobile gaming: Churn prediction*. You are working with a rapidly growing mobile
    gaming startup to help them improve customer satisfaction with upgrades to their
    top-selling Shoot’em Up Clash Legends game. The purpose of your supervised machine
    learning regression model is to estimate the total number of customers (game players)
    expected to uninstall the game from their mobile device (i.e., churn) in the next
    week.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 9.3 Feature selection using guiding principles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, for each case study from section 9.2, there are several proposed
    features along with a discussion about whether they should be selected for the
    application. Notice that for every case study, using domain knowledge and some
    common sense helps with the decision about whether to choose the feature.
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.1 Related to the label
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section teaches the concepts needed to assess candidate features based
    on how strongly they are related to the label so that you can prioritize and select
    more effective features.
  prefs: []
  type: TYPE_NORMAL
- en: '**Case study: Credit card fraud classification**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** The number of times the credit card in the transaction was used
    to purchase from the same vendor'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** A fraudulent credit card transaction is likely to involve a
    purchase that was not authorized. For example, a fraudster may use the stolen
    credit card to purchase from a fake online store that belongs to the fraudster
    or from a physical vendor that operates without video security or keeps poor records.
    Conversely, if the card was used for many purchases from a vendor in the transaction
    without fraud reports, then the association is less likely. For this feature,
    the association should be clear, and the feature should be selected for the model.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** The number of milliseconds that the credit card was inserted into
    the credit card reader during the transaction'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** This is an example of a technical piece of information that
    is often available to a machine learning practitioner but does not translate to
    a valuable feature for the model. Notice that during physical transactions with
    credit cards, sometimes the card is taken out of the reader quickly; other times
    it remains in the reader longer. As long as the transaction completes successfully,
    there is no association between this feature and whether a given transaction is
    fraudulent. Even though you might argue that a fraudster is likely to have stolen
    a batch of cards and cycles though them quickly when making fraudulent transactions,
    recall that the purpose of the model does not classify a batch of transactions
    and instead must classify any single arbitrary transaction. Most transactions
    are non-fraudulent and involve a variety of credit card readers and many users
    who may leave the cards in the reader for arbitrary periods of time. There is
    no clear association in this case to select this feature.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** Vendor’s business category in the transaction'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** It is well known that in the United States it is common for
    a stolen credit card to be used for a small purchase at a gas station so that
    the fraudster can confirm that the card is working as expected. Although other
    associations may not be known at the time of feature selection, it is possible
    that the application of machine learning can uncover associations between specific
    business categories of the vendors and fraudulent transactions.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** Expiration date of the credit card'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** If this candidate feature simply captures the expiration month
    of the card, and the cards have approximately a uniform chance of having expiration
    months, then there is no reason to believe that an association exists between
    the feature and fraud. Also, capturing just the expiration month and year does
    not indicate whether fraud occurred with an expired card: when a card is expired
    a transaction cannot happen, so it does not need to be classified as fraudulent
    or non-fraudulent. In other words, there is no reason to believe that fraudsters
    somehow target cards expiring in December versus January. However, in the United
    States, there is a potential for fraudulent transactions with credit cards stolen
    out of mailboxes. This means that you can select a more sophisticated feature
    that captures the difference between the date of the transaction and the expiration
    date of the card so that you can detect when fraudulent transactions happen close
    to the date of the card issuance, for example two to five years before the card
    expiry.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Case study: Online banner advertisement clicks estimation**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** The price of the item in the banner ad'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** This is one of the most obvious features that can be selected
    from the ad. The price of the item is likely to be the single most important factor
    in whether an ad viewer has interest in the ad and clicks on the banner. Given
    the strength of the association, this should be a priority feature for your regression
    model.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** The category of the font for the text in the banner ad'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** In this case, the feature is based on the named category of
    the font used in the advertisement, such as Times New Roman or Comic Sans. To
    describe the association of this candidate feature with the label, keep in mind
    that design elements such as fonts elicit an emotional response from viewers.
    Since some fonts can be more or less engaging, a categorical feature capturing
    the type of the font used in the ad can model the nuanced association between
    the engagement generated by the font and the total number of clicks on the ad.
    Of course, this feature should be used in conjunction with the other design elements
    of the ad, including content. For instance, choosing Comic Sans to advertise clown
    apparel is likely to generate more clicks than using that font for a wealth management
    ad.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** The number of items left in stock as shown on the banner ad'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** You must have shopped on one of the online retail websites
    and seen a message like “only three left” next to the item you wanted to buy.
    So, would it be appropriate to select a feature indicating the number of items
    shown as left in the inventory on the banner ad? Since you must have seen this
    message on real banner ads, you are probably tempted to pick this feature, but
    what is the association between this feature and the label? The association in
    this case is related to a greater sense of urgency the ad viewer might have after
    seeing the limited stock of the advertised item. The sense of urgency may translate
    to a higher click rate, which is what you are attempting to capture.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** The number of advertised items as reported by the inventory system'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion**: Although you might try to make the case that low inventory
    levels for an item are a proxy for popularity, indicating an in-demand item, you
    should think about this feature in terms of many ad campaigns across many different
    banner ads. For example, toy cars are stocked at different quantities than actual
    cars and in different quantities than sheets of paper. Further, the ad viewer
    has no knowledge of the actual number of items stocked for item advertised, so
    there is no association between the decision to click on the ad and the actual
    number of items in stock.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Case study: Churn prediction**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** ZIP code of the customer'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** The geographical location of the players can be associated
    with their potential to churn in the next quarter for a variety of reasons. Some
    reasons are technical: perhaps the networking infrastructure used by the gaming
    server infrastructure results in higher latencies (and hence a poor experience)
    for ZIP codes in the southeast of the United States compared to the customers
    in California’s Bay Area. Other reasons could originate from demographics: some
    ZIP codes have older or younger populations compared to the “sweet spot” for the
    game.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** USD amount spent on the service per month'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** Since it is common for mobile games to include both a recurring
    monthly subscription price as well as various options, such as the ability to
    spend money to purchase power-ups, player character decorations, and other options
    to stand out from other players, the amount of money a player spends on the game
    is a proxy for their level of engagement and a measure of how invested they are.
    So, there is a possibility for a complex relationship between the amount of money
    spent and the likelihood of churning in the next quarter. For example, spending
    no money on the game can mean that the player lost interest and is more likely
    to uninstall the game. On the other hand, if the player is overly engaged, spending
    an excessive amount of money (e.g., in the top 0.1% percentile), they are also
    likely to burn out from the game or blame it for consuming too much of their time,
    also leading to uninstalling the game from their device.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** Number of days until the next tax filing date for the gaming company'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** Companies in the United States are required to file quarterly
    and annual tax returns with the Internal Revenue Service, the United States’ tax
    authority. Surprisingly, many companies noticed a correlation between the tax
    filing dates and the changes in the customer churn. Although an internal analyst
    in the company may notice and report this correlation, you should be skeptical
    about this association: do the players even know these dates when making a decision
    to uninstall the date? More likely if the correlation exists, it is about the
    end of the month, when many individuals review their monthly spending and make
    a decision to cut down on nonessential items. Since the IRS tax filing dates coincide
    with the time many individuals re-evaluate their spending, this can show up as
    a spurious correlation between the IRS schedule and the customer churn.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** Number of weeks subscribed to the game'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** Many uninstalls of a game happen shortly after the player installs
    it for the first time and decides against keeping it.'
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.2 Recorded before inference time
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section explains the concept of data leakage, how it may subtly disrupt
    the performance of your machine learning models, and illustrates how effective
    feature selection can help you avoid problems from this.
  prefs: []
  type: TYPE_NORMAL
- en: '**Case study: Credit card fraud classification**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** The card was used at the store before'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** This information should be available at inference time as physical
    card transactions made with the physical card readers are classified differently,
    with the appropriate information recorded for each transaction.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** The store item sold in the transaction is a newly stocked item'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** If you have made credit card purchases, you know that, at the
    transaction level, the details of the purchase are not available to the credit
    card-issuing company. For example, if you are making a purchase at a grocery store
    and are buying a new flavor of CocaCola, the credit card company can’t distinguish
    this information from other purchases at the store; all the items are simply aggregated
    into a single charge, so this would not make a good feature for your machine learning
    model.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** Category of the item sold in the transaction'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** This may or not be available at inference time. For example,
    if the transaction is for a gas (petrol) purchase, then the category is obvious.
    In other cases, when the purchase is at a general retailer such as Target or Walmart,
    where there are thousands of different item categories, the information it not
    available at inference time.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** The card was used at a physical (versus online) location'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** This information should be available at inference time as physical
    card transactions made with the physical card readers are classified differently,
    with the appropriate information recorded for each transaction.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Case study: Online banner advertisement clicks estimation**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** Total number of items purchased using the discount code'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** Although this information should be available in the data warehouse
    of the company running the campaign after the campaign has concluded, this information
    cannot be available at inference time.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** Number of items purchased using the discount code over the past
    30 days'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** If the company running the ad maintains the transaction data
    about the number of times the discount code was used, then it is possible to maintain
    the *sliding window* of the 30 days’ worth of data and calculate this value for
    any given transaction. Further, by predicting the number of clicks on the banner
    ad with the discount code, it is possible to use this daily value to make a better
    estimate of the total banner ad clicks.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** Number of customers who viewed the banner ad about the item'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** For an online banner ad, information should be available from
    a data warehouse or a data analytics source.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** The number of items advertised in stock at the manufacturer'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** This candidate feature is deliberately chosen to spark a thought-provoking
    discussion about the potential features and the ad engagement. Recall that training
    data sets are historical, based on the data from the past advertising campaigns
    and the actual numbers of banner ad clicks. While you may have historical inventory
    data about the items used in the advertising campaigns at the time of the ad impression
    (when the ad was viewed), should this data be used for a feature in your machine
    learning model? Although you may make the case that low inventory levels indicate
    a popular or in-demand item, you should think about the feature in terms of many
    ad campaigns across many different banner ads. There is no reasonable association
    that you can establish for this feature. Further, obtaining the value for this
    feature at runtime (when performing the inference or producing the estimate),
    can be a technical challenge that outweighs the value from experimenting with
    this association.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Case study: Churn prediction**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** Total minutes spent playing the game'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** Note that using the word *total* in the description of this
    feature can lead to confusion. For someone who uninstalled the game, the total
    refers to the number of minutes spent playing the game over the entire period
    of having it installed. In contrast, the total for someone who did not uninstall
    the game describes the number of minutes up to the point that their data was recorded
    in the training data set. Once the possibility of the dual interpretation is clear,
    it should also be clear that this feature should not be used, since at inference
    time it is impossible to tell the total number of minutes spent with the game
    for a player who never unsubscribed or uninstalled the game.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** Total number of minutes spent playing the game in the past 28
    days'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** With a minor change to restrict the measurement of the number
    of minutes spent with the game to the past 28 days, it is possible to use the
    key idea from the previous feature. Regardless of whether a player churns in the
    next month, it is possible to measure their previous 28 days’ worth of game activity
    and use that as a feature for both training and inference.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** Uninstall reason provided by the customer'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** When the player uninstalls the game, they can specify the feedback
    with a reason as to why they decided to uninstall. Clearly this information is
    available only for the players who have uninstalled the game and only after they
    have uninstalled it. Hence, it is not available at inference time and should not
    be used for training the machine learning model.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** Customer satisfaction score in three months prior to churn'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** If customer satisfaction is gathered randomly across players,
    including those who uninstalled the game as well as those who kept on playing,
    this is a useful feature to include for inference.'
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.3 Supported by abundant examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section provides examples of candidate features that may or may not have
    enough values to train a machine learning model to guide you on the use of this
    principle in real-world examples.
  prefs: []
  type: TYPE_NORMAL
- en: '**Case Study: Credit card fraud classification**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** Distance between the cardholder address and the merchant address'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** Having information about the distance between the address of
    the cardholder and the location the transaction took place can be useful when
    classifying fraud. However, to use this feature, a practitioner needs to evaluate
    whether it is feasible to have examples for this feature in practice. Unless the
    customer has been proactive at geocoding the distances at the time of the transaction,
    you should not expect to have enough examples for this feature.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** Whether the ZIP codes of the cardholder address and the merchant
    address are the same'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** Notice that unlike the feature that attempts to use geocoding
    to estimate the distances between the cardholder and the merchant locations, a
    feature that checks whether the ZIP codes of the merchant and the cardholder match
    can be supported by the customer’s historical transactions data. Since every financial
    transaction should have this feature there should be an abundant number of examples
    for this feature.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** The card was used at the merchant before'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** The financial company maintains a history of transactions for
    a given card, so it is possible to check the previous transactions for whether
    the card was used at a given merchant. An absence of transactions with a merchant
    indicates that the card was not used at the merchant, so for every transaction
    in the data set it is possible to assign either a true or a false value to this
    feature.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** Category of item being purchased'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** Although for some merchants such as gas stations, it is possible
    to uniquely identify the category of the item or items purchased in a transaction,
    many companies maintain proprietary item inventory codes. In general, there is
    no guarantee that a transaction includes information about the items purchased.
    You should not expect to have a sufficient number of examples of the categories
    of the item purchased for a general purpose fraud versus non-fraud classifier.
    However, you may be able to create a more specialized classifier for specific
    subcategories of merchants such as gas stations.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Case study: Online banner advertisement clicks estimation**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** The day of the year the advertising campaign started'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** As long as the company managing the banner ad campaigns maintains
    the start and end for the campaign, this date should be available for every training
    example.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** The person viewing the banner ad already purchased the item in
    the ad'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** Most of the time, the number of purchases of an item shown
    in a banner ad is less than 0.1% compared to the number of the banner ad views.
    You should not expect to have a significant number of examples where you know
    whether the ad viewer purchased the item.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** The discount percentage offered in the banner ad'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** Most discount percentages offered in ads are based on a small
    collection of well-known values, such as 10%, 20%, 25%, 50%, 75%, or 90% off.
    You should not expect to see discounts of 27.54% off shown on the banner ad. As
    long as the number of the distinct discount amounts offered across marketing campaigns
    is a small fraction of the training data set, you should have a sufficient number
    of examples for this feature.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Case study: Churn prediction**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** Latitude and longitude of the player’s billing address location'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** Since both latitude and longitude of the billing address location
    are unique per the customer billing location, using these values to estimate whether
    the player is going to uninstall the game is a mistake. Unless used with caution,
    the specific coordinate values may produce a model that overfits the training
    data set. The raw latitude and longitude values of the billing address location
    should not be used to predict customer churn.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** The number of times the player previously uninstalled the game'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** If a player uninstalled the game in the past, it likely that
    they will again. Notice that an absence of a record of the game uninstall can
    mean that the player never uninstalled it, as long as the gaming platform accurately
    reports uninstall events to the gaming company. Hence, it should be possible to
    select this feature with a value for every training example.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** The USD amount the player intends to spend next month on gaming'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** Although knowing how much a given player has in their budget
    to pay for a mobile game in the next month can be exceptionally useful to estimate
    whether they will churn, it is also highly unlikely that the company will have
    this information for any or any meaningful fraction of the total players.'
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.4 Numeric with meaningful magnitude
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section compares alternative feature representations to help you chose
    between continuous and categorical features.
  prefs: []
  type: TYPE_NORMAL
- en: '**Case study: Credit card fraud classification**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** ZIP code of vendor in the transaction'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** Although the ZIP code in the transaction is an integer number,
    it should be clear that it is not a continuous value to be used as is in the machine
    learning model. If you are not convinced, you can apply the rule of thumb about
    the arithmetic values: adding 10001, a ZIP code for New York City, to 20002, a
    ZIP code for Washington DC, yields a ZIP code of 30003, which is a ZIP code for
    Norcross, Georgia—clearly a meaningless result. The ZIP code value should be encoded
    and treated as a categorical variable.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** The number of times the credit card in the transaction was used
    to purchase from the same vendor'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** Although it is possible to treat this count as a continuous
    variable and obtain meaningful results, notice that knowing the actual number
    of the times the card was used at a vendor is not particularly meaningful.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** Expiration date of the credit card'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** Month and year of the credit card expiration are not useful
    as continuous variables. You can re-encode this information to the number of days
    until expiration, but the raw numeric values for this information are not meaningful.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Case study: Online banner advertisement clicks estimation**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** Percent value of the discount, such as 10%, 25%, or 50% off'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** On the surface this appears as a simple numeric feature value.
    However, what about negative values? Is it possible to get a negative 100% discount?
    This should be classified as a categorical feature for relisting feature values.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** Size of the banner ad'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** On the surface, amounts like 400px by 400px or 100px by 400px
    seem like traditional numeric features. However, as the model attempts to both
    extrapolate and interpolate from these values, you may find yourself working with
    unexpected results.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** Font used by the banner ad'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** Adding or multiplying font values produces meaningless results.
    This is not a continuous but a categorical feature.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** Color used for the banner ad font'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** Although it is possible to represent color as a combination
    of numbers, for example using red-green-blue values, in this value the feature
    is about predicting the clicks on the ad, so a categorical representation is more
    appropriate, since the color is uniform across the ad and falls into the human-readable
    categories such as blue, black, or green.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** Item category identifier discounted by the coupon'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** Number categories like 1 for dairy, 3 for canned goods, and
    so on are not meaningful as continuous values and should be re-encoded as categorical
    variables.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Case study: Churn prediction**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** Customer average minutes played versus average minutes played
    per user across the user base'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** This feature should be encoded as a continuous value. It has
    a meaningful zero value (i.e., when the average of the customer coincides with
    the average for the entire user base) and has a meaningful range from negative
    to positive values.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** Number of weeks subscribed to the game'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** This feature should be encoded as a continuous value since
    it can be subdivided into more fine-grained parts.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** Mobile operating system used by the customer (e.g., iOS, Android,
    or other)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** Since there is a finite set of operating systems supported
    by the mobile gaming application, this feature should be encoded as a categorical
    variable.'
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.5 Bring expert insight to the problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section guides you through examples of expert insights about the domains
    of finance, advertising, and gaming to help you hone your skills on these features
    for more effective machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: As described in this chapter, although it is technically feasible to select
    a variety of features, not every potential feature contributes to the success
    of a machine learning system, and some can result in more harm than benefit to
    a system. The successful features are those where expert knowledge as well as
    common sense augments the raw data set and simplifies the task of the machine
    learning algorithm by making the relationship between the feature and the label
    values more direct.
  prefs: []
  type: TYPE_NORMAL
- en: '**Case study: Credit card fraud classification**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** A transaction with a suspect vendor in the past month'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** According to experts, a significant number of fraudulent transactions
    are due to a compromised vendor where an employee steals credit card information
    and later uses the stolen information to commit fraud. Human experts often use
    graph-based analysis, as shown in figure 9.3, to identify vendors suspected of
    facilitating fraud. Figure 9.3 shows that three credit cards that went on to report
    a fraudulent transaction were used in the same suspect vendor, shown on the left
    in bold.'
  prefs: []
  type: TYPE_NORMAL
- en: '![09-03](Images/09-03.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 Legitimate credit card transactions with a suspect vendor (on the
    left in bold) led to transactions that were reported as fraudulent (on the right).
  prefs: []
  type: TYPE_NORMAL
- en: Based on the expert insight, you can use a numeric feature for each vendor of
    the total number of credit cards that reported fraud over the past month.
  prefs: []
  type: TYPE_NORMAL
- en: '**Case study: Online banner advertisement clicks estimation**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** A score measuring the relevance of the ad topic to the top trending
    topics on Twitter'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** As you probably expect, the number of clicks on an online banner
    ad depends not just on the content of the ad itself but also on its relevance
    to the marketplace. For example, an ad about flashlights is going to generate
    more clicks on the eve of a hurricane’s landfall. Similarly, ads about private
    tennis lessons generate more clicks during the weekend of the Wimbledon finals.
    Based on this insight, you can design a numeric feature that captures the similarity
    between the topic of the words in the online banner ad and the trending topics
    on Twitter during the ad campaign.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Case study: Churn prediction**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature:** A number of connections on social networks who play the game.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion:** Video game designers will tell you that peer pressure is one
    of the strongest predictors of whether a player continues to play the game. That’s
    why so many mobile games try to connect to your Facebook and other social media
    accounts: if the game developers know that your “friends” play the game, they
    will know that you are likely to play it too.'
  prefs: []
  type: TYPE_NORMAL
- en: 9.4 Selecting features for the DC taxi data set
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, you will learn about expert insights that concern the DC taxi
    fare data set and how to apply these insights to select a set of candidate features
    to use for your machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall from chapter 4 that the set of features for the DC taxi data set is
    quite sparse: the only raw data available at inference time includes the date
    and timestamp of the start of the trip along with the latitude and longitude coordinates
    for the pickup and drop-off locations of the taxi ride. What insights can you
    bring to the taxi fare estimation problem in order to select the right features?'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of the DC taxi data set, a key expert insight concerns the GPS
    used to specify the location coordinates for the trip. The GPS system is precise
    down to 1 meter (just over 3 feet) from the actual location. However, from the
    standpoint of taxi fare estimation, GPS precision is excessive: as described in
    chapter 2, the business rules for the taxi rides use the granularity of 1/8th
    of a mile for pricing. This corresponds to a roughly 200-meter (or about 660 feet)
    precision for a location. The pricing precision is about two orders of magnitude
    more coarse than what’s estimated by the GPS coordinates. Hence, features for
    this data set could include a more coarse representation of the taxi pickup and
    drop-off coordinates.'
  prefs: []
  type: TYPE_NORMAL
- en: The boundaries of the locations for the taxi rides in the DC taxi data set are
    specified using minimum and maximum values for the latitude and longitude coordinates.
    How can these minimum and maximum values be used for features? The illustration
    in figure 9.4 explains the concept behind the coarse-gained representation of
    the pickup and drop-off coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: '![09-04](Images/09-04.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 Original minimum and maximum latitude and longitude coordinates (left)
    can be used to engineer numeric features based on binning of the coordinates (right).
  prefs: []
  type: TYPE_NORMAL
- en: For clarity, the actual DC taxi data set’s minimum and maximum values for the
    latitude and longitude coordinates are replaced with a more convenient set of
    numbers. Although these made-up coordinate numbers are in close proximity to Washington,
    DC, they were picked to make the explanation easier. Hence, both the left and
    the right side in figure 9.4 assume that the coordinates for all the trips in
    the DC taxi data set range from 38.30 to 38.90 along the latitude (north-south)
    and from –76.48 to –76.12 along the longitude (east-west) coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to using the raw latitude and longitude values to train the machine
    learning model, features for a coarse-grained representation of the pickup and
    drop-off coordinates can use the concept of *binning* (also known as *discretization*
    or *quantanization*) the GPS coordinate values, as illustrated on the right side
    of figure 9.4, which assumes that both the latitude and the longitude coordinates
    were “binned” into three bins each, corresponding to three equally sized intervals:'
  prefs: []
  type: TYPE_NORMAL
- en: (38.30, 38.50), (38.50, 38.70), and (38.70, 38.90) for the latitude
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (–76.48, –76.36) (–76.36, –76.24), and (–76.24, –76.12) for the longitude
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The diagram on the right side of figure 9.4 indexes each of the three bins using
    integers 0, 1, and 2\. As the result, each of the nine coarse-grained locations
    can be represented using a pair of integers. For example, a taxi ride from a pickup
    location (38.31, -76.47) to a drop-off location (38.61, -76.14) can be represented
    using the locations (0, 0) and (2, 1), respectively. Keep in mind that the selection
    of nine locations is arbitrary. Since it is unclear how coarse the location boundaries
    should be, ultimately the selection of the coarseness for the coordinates should
    be measured by how well the corresponding features help the model to predict the
    taxi fare.
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming chapters, you will learn how to apply feature engineering techniques
    to implement the coarse-grained pickup and drop-off location features described
    in this section. Note that while the features based on the expert insight about
    the coarse-grained location representation do not guarantee improved taxi fare
    estimation, the features can be used during the iterative process of machine learning
    model development and can be evaluated in terms of their impact on the machine
    learning model performance metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Effective feature engineering can make the difference between a mediocre and
    a successful machine learning system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although feature selection is more of an art than a science, a machine learning
    practitioner can learn the skills needed to identify the right features for a
    machine learning system by practicing guiding principles to real-world examples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Case studies in this chapter help machine learning practitioners learn how to
    consistently apply the feature selection principles to case studies from diverse
    industries, including financial, advertising, and mobile gaming.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Successful feature engineering complements the raw training data with carefully
    selected and designed features that incorporate commonsense knowledge and expert
    insights about the machine learning problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '^(1.)Counterfactual causality is closely related to the potential outcomes
    definition of causality. Judea Pearl et al.’s book, *Causal Inference in Statistics:
    A Primer* (Wiley, 2016), is a great resource with a more formal treatment of causality.'
  prefs: []
  type: TYPE_NORMAL
- en: ^(2.)The Titanic data set, along with documentation, is available from [https://www.openml.org/d/40945](https://www.openml.org/d/40945).
  prefs: []
  type: TYPE_NORMAL
