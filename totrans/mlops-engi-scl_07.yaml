- en: '5 Introducing PyTorch: Tensor basics'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 PyTorch介绍：张量基础
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Introducing PyTorch and PyTorch tensors
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍PyTorch和PyTorch张量
- en: Using PyTorch tensor creation methods
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PyTorch张量创建方法
- en: Understanding tensor operations and broadcasting
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解张量操作和广播
- en: Exploring PyTorch tensor performance on CPUs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索在CPU上的PyTorch张量性能
- en: 'In the previous chapter, you started with a cleaned-up version of the DC taxi
    data set and applied a data-driven sampling procedure in order to identify the
    right fraction of the data set to allocate to a held-out, test data subset. You
    also analyzed the results of the sampling experiments and then launched a PySpark
    job to generate three separate subsets of data: training, validation, and test.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你从DC出租车数据集的清理版本开始，并应用了数据驱动的抽样过程，以确定要分配给一个保留的测试数据子集的数据集的正确部分。你还分析了抽样实验的结果，然后启动了一个PySpark作业来生成三个不同的数据子集：训练、验证和测试。
- en: This chapter takes you on a temporary detour from the DC taxi data set to prepare
    you to write scalable machine learning code using PyTorch. Don’t worry; chapter
    7 returns to the DC taxi data set to benchmark a baseline PyTorch machine learning
    model. In this chapter, you will focus on learning about PyTorch, one of the top
    frameworks for deep learning and many other types of machine learning algorithms.
    I have used TensorFlow 2.0, Keras, and PyTorch for machine learning projects that
    required distributed training on a machine learning platform and found PyTorch
    to be the best one. PyTorch scales from mission-critical, production machine learning
    use cases at Tesla[¹](#pgfId-1178834) to state-of-the-art research at OpenAI.[²](#pgfId-1179232)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这一章将暂时偏离DC出租车数据集，为你准备好使用PyTorch编写可扩展的机器学习代码。别担心；第7章会回到DC出租车数据集，以基准测试基线PyTorch机器学习模型。在本章中，你将专注于学习PyTorch，这是深度学习和许多其他类型的机器学习算法的顶级框架之一。我曾在需要在机器学习平台上进行分布式训练的机器学习项目中使用过TensorFlow
    2.0、Keras和PyTorch，并发现PyTorch是最好的选择。PyTorch可从特斯拉的关键生产机器学习用例[¹](#pgfId-1178834)扩展到OpenAI的最新研究[²](#pgfId-1179232)。
- en: 'Since you will need a practical understanding of core PyTorch concepts before
    you can start applying them to machine learning with the DC taxi data set, this
    chapter focuses on equipping you with an in-depth knowledge of the core PyTorch
    data structure: a tensor. Most software engineers and machine learning practitioners
    have not used tensors as a part of their mathematics, programming, or data structures
    curricula, so you should not be surprised if this is new.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你需要在开始将它们应用于DC出租车数据集的机器学习之前对核心PyTorch概念有实际的理解，所以本章重点是为你提供对核心PyTorch数据结构：张量的深入知识。大多数软件工程师和机器学习实践者在数学、编程或数据结构课程中都没有使用张量，所以如果这是新的，你不应感到惊讶。
- en: In section 5.1, I introduce a comprehensive definition of a PyTorch tensor.
    For now, keep in mind that if you have ever implemented a matrix in a programming
    language using an array of arrays (i.e., an array containing other arrays), you
    are well on your way to understanding tensors. As a working definition, you can
    consider a *tensor* to be a generic data structure that can store and operate
    on variables, arrays, matrices, and their combinations. In this book, the most
    complex tensors you encounter are effectively arrays of matrices, or arrays of
    arrays of arrays, if you prefer a more recursive description.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在第5.1节中，我介绍了PyTorch张量的全面定义。暂时记住，如果你曾经在编程语言中使用数组的数组（即，包含其他数组的数组）实现过矩阵，那么你已经在理解张量的路上走得很远了。作为一个工作定义，你可以将*张量*视为一种通用数据结构，可以存储和操作变量、数组、矩阵及其组合。在本书中，你遇到的最复杂的张量实际上是矩阵的数组，或者如果你更喜欢更递归的描述，是数组的数组的数组。
- en: 5.1 Getting started with tensors
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 开始使用张量
- en: This section defines a tensor in the context of machine learning use cases,
    explains the attributes of tensors, including tensor dimension and shape, and
    finally introduces you to the basics of creating tensors using PyTorch as opposed
    to using native Python data types. By the conclusion of this section, you should
    be prepared to study the advantages of PyTorch tensors over native Python data
    types for machine learning use cases.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本节在机器学习用例的背景下定义了张量，解释了张量的属性，包括张量的维度和形状，并最终向你介绍了使用PyTorch创建张量的基础知识，而不是使用本地Python数据类型。通过本节的结论，你应该准备好研究PyTorch张量相对于本地Python数据类型在机器学习用例中的优势了。
- en: The term *tensor* has subtly different definitions depending on whether it is
    used in mathematics, physics, or computer science. While learning about a geometric
    interpretation of tensors from mathematics or a stress mechanics interpretation
    from physics can enrich your understanding of the abstract aspects of a tensor,
    this book uses a narrower definition that is more relevant for a practitioner
    applying tensors to machine learning. Throughout this book the term describes
    a data structure (i.e., a data container) for basic data types such as integers,
    floats, and Booleans.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*张量*一词在数学、物理或计算机科学中使用时有微妙不同的定义。虽然从数学中了解张量的几何解释或从物理学中了解张量的应力力学解释可以丰富您对张量抽象方面的理解，但本书使用一个更狭窄的定义，更符合将张量应用于机器学习的从业者。本书中，该术语描述了一种数据结构（即数据容器），用于基本数据类型，如整数、浮点数和布尔值。'
- en: Since tensors are closely related to arrays, it is worthwhile to spend a moment
    reviewing the key attributes of an array or a Python list. An *array* is just
    an ordered collection of data values. In most programming languages, an array
    index can take on a value from a finite set of integers, based on a range of values
    from zero to one less than the number of elements in the array.[³](#pgfId-1012456)
    For example, in Python, this range is
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 由于张量与数组密切相关，因此值得花一点时间回顾数组或Python列表的关键属性。*数组*只是数据值的有序集合。在大多数编程语言中，数组索引可以取值于一组有限的整数，基于数组中元素数量减一的范围。[³](#pgfId-1012456)例如，在Python中，这个范围是
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: where a_list is the name of an instance of a Python list. Hence, different arrays
    have different valid index values. In contrast, all arrays consisting of basic
    data types, regardless of length, have a tensor dimension equal to one.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 其中a_list是Python列表实例的名称。因此，不同的数组具有不同的有效索引值。相反，所有由基本数据类型组成的数组，无论长度如何，其张量维度都等于一。
- en: 'A *dimension* is defined here as the total number of indices (not the values
    of indices) needed to access a value in a data structure. This definition is convenient
    because it helps describe different data structures using a single number. For
    example, a matrix has a dimension of two because it takes two indices, a row index
    and a column index, to pinpoint a data value in a matrix. For example, using Python,
    a naive matrix implementation can use a list of lists:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*维度*在这里定义为访问数据结构中值所需的索引总数（而不是索引的值）。这个定义很方便，因为它帮助使用一个数字描述不同的数据结构。例如，矩阵的维度为二，因为需要两个索引，行索引和列索引，才能在矩阵中定位数据值。例如，使用Python，一个简单的矩阵实现可以使用列表的列表：'
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: where mtx evaluates to
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 其中mtx的求值结果为
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: and the value of mtx[1][2] is printed out as 5. Since the matrix has a dimension
    of two, two index values—1 for the row index and 2 for the column index—had to
    be specified to access the value of 5 in the matrix.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 和mtx[1][2]的值打印为5。由于矩阵的维度为二，因此必须指定两个索引值——行索引为1，列索引为2——才能访问矩阵中5的值。
- en: The dimension also specifies a measure of array nesting that is needed to implement
    a data structure. For example, mtx, with a dimension of 2, requires an array of
    arrays, while an array of matrices (dimension 3), requires an array of arrays
    of arrays. If you consider a data structure with a dimension of 0, in other words
    one that requires zero indices to access a value, you’ll soon realize that this
    data structure is just a regular variable. For a visualization of tensors with
    dimensions (also known as *tensor* *rank*) 0, 1, 2, and 3, look at figure 5.1.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 维度还指定了实现数据结构所需的数组嵌套的度量。例如，维度为2的mtx需要一个数组的数组，而维度为3的矩阵数组需要一个数组的数组的数组。如果考虑一个维度为0的数据结构，换句话说，需要零个索引来访问值的数据结构，很快就会意识到这个数据结构只是一个常规变量。对于维度（也称为*张量秩*）为0、1、2和3的张量的可视化，请参阅图5.1。
- en: '![05-01](Images/05-01.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![05-01](Images/05-01.png)'
- en: Figure 5.1 Tensor rank (dimension) equals the number of indices needed to access
    a data value in a tensor. Unlike tensors of lower rank, tensors of rank 3 and
    higher do not have commonly accepted names in machine learning.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 张量秩（维度）等于访问张量中数据值所需的索引数量。与较低秩的张量不同，机器学习中没有公认的命名。
- en: A tensor is a data structure capable of storing arrays of an arbitrary number
    of dimensions, or more concisely, a tensor is an n-dimensional array. Based on
    this definition, a flat Python list, or any flattened array, is a one-dimensional
    tensor, sometimes also described as a rank 1 tensor. A Python variable is a zero-dimensional
    tensor, commonly described as a scalar, or less often as a rank 0 tensor. A two-dimensional
    tensor is usually called a matrix. For higher dimensional examples, consider a
    matrix used to represent grayscale images in terms of pixel values, where a value
    of 0 is black, 255 is white, and the numbers in between are gray colors of increasing
    brightness. Then, a three-dimensional tensor is a convenient data structure for
    an ordered collection of grayscale images, so the first of the three indices specifies
    an image and the remaining two specify the row and column location of a pixel
    in the image. A three-dimensional tensor also works well for a color image (but
    not a collection of color images), such that the first index specifies red, green,
    blue, or opacity (alpha) channels for the color, while the remaining indices specify
    a pixel location in the corresponding image. Continuing with this example, a four-dimensional
    tensor can be used for a sequential collection of color images.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 张量是一种能够存储任意数量维度数组的数据结构，或者更简洁地说，张量是一个 n 维数组。根据此定义，一个平面 Python 列表或任何扁平化的数组都是一维张量，有时也被描述为秩
    1 张量。Python 变量是零维张量，通常被描述为标量，或者更少见地被描述为秩 0 张量。一个二维张量通常被称为矩阵。对于更高维的例子，请考虑用于表示灰度图像的矩阵，其中像素值为
    0 为黑色，255 为白色，中间的数字为逐渐增加亮度的灰色颜色。然后，三维张量是一种有序灰度图像集合的便捷数据结构，因此三个指数中的第一个指定图像，其余两个指定图像中像素的行和列位置。三维张量也适用于彩色图像（但不适用于彩色图像集合），因此第一个指数指定颜色为红色、绿色、蓝色或不透明度（alpha）通道，而其余指数指定相应图像中的像素位置。继续这个例子，四维张量可以用于顺序的彩色图像集合。
- en: With this foundational knowledge in place, you are ready to create your first
    tensor in PyTorch.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些基础知识，您就可以准备在 PyTorch 中创建您的第一个张量了。
- en: Listing 5.1 A rank 0 tensor implemented using PyTorch
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.1 使用 PyTorch 实现的秩 0 张量
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Import PyTorch library and alias it as pt.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入 PyTorch 库并将其别名为 pt。
- en: ❷ Create a rank 0 tensor (scalar) with a value of 42.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建一个值为 42 的秩 0 张量（标量）。
- en: Once this code is executed, it outputs
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦执行了这段代码，它会输出
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: After you have imported the PyTorch library and aliased it as pt ❶, the next
    line of the code ❷ simply creates a scalar (rank 0) tensor and assigns it to a
    variable named alpha. When executed on 64-bit Python runtimes, where the value
    of 42 from listing 5.1 is represented as a 64-bit integer, the alpha tensor is
    instantiated using the PyTorch torch.LongTensor class.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在导入 PyTorch 库并将其别名为 pt ❶ 之后，代码的下一行 ❷ 简单地创建一个标量（秩 0 张量）并将其赋值给一个名为 alpha 的变量。在
    64 位 Python 运行时执行时，从列表 5.1 中的值 42 被表示为 64 位整数，alpha 张量将使用 PyTorch 的 torch.LongTensor
    类进行实例化。
- en: 'For any PyTorch tensor, you can use the type method to discover the specific
    tensor class used to instantiate the tensor:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何 PyTorch 张量，您可以使用 type 方法来发现用于实例化张量的特定张量类：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This outputs
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这会输出
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The torch.LongTensor, as well as torch.FloatTensor and other tensor classes
    for various basic Python data types, are subclasses of the torch.Tensor class.[⁴](#pgfId-1013029)
    The subclasses of torch.Tensor include support for different processor architectures
    (devices); for example, torch.LongTensor is a class with a CPU-specific tensor
    implementation, while torch.cuda.LongTensor is a GPU-specific tensor implementation.
    PyTorch support for GPUs is described in more detail in chapter 7.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: torch.LongTensor，以及其他用于各种基本 Python 数据类型的张量类，都是 torch.Tensor 类的子类。[⁴](#pgfId-1013029)
    torch.Tensor 的子类包括对不同处理器架构（设备）的支持；例如，torch.LongTensor 是具有 CPU 特定张量实现的类，而 torch.cuda.LongTensor
    是具有 GPU 特定张量实现的类。有关 PyTorch 对 GPU 的支持，将在第 7 章中详细描述。
- en: In your machine learning code, you should primarily rely on the tensor dtype
    attribute instead of the type method because dtype returns the tensor’s type in
    a device-independent fashion, ensuring that your code can be easily ported across
    devices. The dtype for alpha,
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的机器学习代码中，您应主要依赖于张量的 dtype 属性，而不是 type 方法，因为 dtype 以与设备无关的方式返回张量的类型，确保您的代码可以轻松地在不同设备之间移植。对于
    alpha 的 dtype，
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: outputs a device-independent description of the data type[⁵](#pgfId-1015715)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 输出数据类型的与设备无关的描述[⁵](#pgfId-1015715)
- en: '[PRE8]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: To access the value stored by the tensor, you can use the item method
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问张量存储的值，您可以使用 item 方法
- en: '[PRE9]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: which in this case displays 42.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下显示 42。
- en: To confirm that alpha tensor is a scalar, you can access the tensor’s shape
    attribute,
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 要确认 alpha 张量是一个标量，您可以访问张量的 shape 属性，
- en: '[PRE10]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: which prints out torch.Size([]).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 打印出 torch.Size([])。
- en: The PyTorch library uses the torch.Size class to specify the details of the
    size (also known as the shape) of a tensor. Here, the size consists of an empty,
    zero-length list since the alpha scalar is rank 0\. In general, the length of
    the torch.Size list is equal to the dimension of the tensor. For example,
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 库使用 torch.Size 类来指定张量的大小（也称为形状）的详细信息。在这里，大小由一个空的、长度为零的列表组成，因为 alpha
    标量的秩为 0。一般来说，torch.Size 列表的长度等于张量的维度。例如，
- en: '[PRE11]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: outputs 0. The shape of a tensor specifies the number of elements stored in
    a tensor along the tensor’s dimensions. For example, a one-dimensional PyTorch
    tensor created from a Python list of the first five Fibonacci numbers,
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 输出 0。张量的形状指定了沿张量维度存储的元素数量。例如，从 Python 列表创建的一个一维 PyTorch 张量的前五个斐波那契数，
- en: '[PRE12]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: produces torch.Size([5]) which confirms that there are five elements in the
    first and only dimension of the arr tensor.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 产生了 torch.Size([5])，这证实了 arr 张量的第一个和唯一维度中有五个元素。
- en: If you create a PyTorch matrix (rank 2 tensor) from a Python list of lists,
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您从 Python 列表的列表中创建一个 PyTorch 矩阵（秩为 2 的张量），
- en: '[PRE13]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: then mtx.shape returns the size of torch.Size([2, 5]) since there are two rows
    and five columns in the mtx matrix.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 然后 mtx.shape 返回 torch.Size([2, 5]) 的大小，因为 mtx 矩阵有两行和五列。
- en: 'Standard Python indexing and the item method continue to work as expected:
    to retrieve the value in the upper left-hand corner of the mtx tensor, you use
    mtx[0][0].item(), which returns 2.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 标准的 Python 索引和 item 方法继续按预期工作：要检索 mtx 张量左上角的值，您使用 mtx[0][0].item()，它返回 2。
- en: 'When working with tensors of rank 2 or higher in PyTorch you need to know about
    an important default limitation: the number of elements in the trailing dimensions;
    in other words, second and higher dimensions must be consistent, such as if you
    attempt to create a matrix with four elements in the second (column) dimension
    while the other columns have five elements.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyTorch 中处理秩为 2 或更高的张量时，您需要了解一个重要的默认限制：尾部维度中的元素数量；换句话说，第二个及更高的维度必须保持一致，例如，如果您尝试创建一个第二个（列）维度有四个元素的矩阵，而其他列有五个元素。
- en: Listing 5.2 PyTorch tensors with support variable
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 用支持变量的 PyTorch 张量的 5.2 列表
- en: '[PRE14]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'PyTorch reports an error:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 报告了一个错误：
- en: '[PRE15]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Since PyTorch uses zero-based indexing for the dimensions, the second dimension
    has an index of 1 as reported by the ValueError. Although the default PyTorch
    tensor implementation does not support “ragged” tensors, the NestedTensor package
    aims to provide support for this category of tensors.[⁶](#pgfId-1016285)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 PyTorch 使用零为基础的索引来表示维度，因此第二个维度的索引为 1，如 ValueError 所报告的那样。尽管默认的 PyTorch 张量实现不支持“不规则”张量，但
    NestedTensor 包旨在为这类张量提供支持。[⁶](#pgfId-1016285)
- en: 5.2 Getting started with PyTorch tensor creation operations
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 开始使用 PyTorch 张量创建操作
- en: Previously, you saw that you can create a PyTorch scalar tensor from a value
    (e.g., a Python integer) and an array tensor from a collection of values (e.g.,
    from a Python list); however, there are other factory methods that can help you
    create tensors. In this section, you will practice creating PyTorch tensors using
    factory methods from the PyTorch APIs. These methods are useful when creating
    tensors for mathematical operations common in machine learning code, and in general,
    when a tensor is based on non-data set values.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，您已经看到您可以从一个值（例如，一个 Python 整数）创建一个 PyTorch 标量张量，并从一组值（例如，从一个 Python 列表）创建一个数组张量；但是，还有其他工厂方法可以帮助您创建张量。在本节中，您将练习使用
    PyTorch API 中的工厂方法创建 PyTorch 张量。当创建用于机器学习代码中常见的数学操作的张量时，以及当张量基于非数据集值时，这些方法非常有用。
- en: When instantiating a tensor using the factory methods, the shape of the desired
    tensor is explicitly specified unless the desired shape can be inferred by PyTorch
    from the method’s arguments (as explained later in this section). For example,
    to create a two-row and three-column matrix of zeros using the zeros factory method,
    use
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用工厂方法实例化张量时，除非 PyTorch 可以从方法的参数中推断出所需张量的形状（如本节稍后解释的那样），否则将显式指定所需张量的形状。例如，要使用
    zeros 工厂方法创建一个两行三列的零矩阵，请使用
- en: '[PRE16]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: which produces
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 产生
- en: '[PRE17]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Given an instance of a tensor, you can confirm that the tensor has the desired
    shape by using the tensor’s shape attribute,
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 给定张量的一个实例，您可以通过使用张量的shape属性来确认张量具有所需的形状，
- en: '[PRE18]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'which returns an instance of torch.Size for the shape, in this case, matching
    what you have passed to the zeros method:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个torch.Size实例，表示形状，本例中与您传递给zeros方法的内容匹配：
- en: '[PRE19]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: PyTorch tensor factory methods let you specify the tensor shape by passing one
    or more integers to a method. For example, to create an array of 10 ones, you
    can use the ones method,
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch张量工厂方法允许您通过将一个或多个整数传递给方法来指定张量形状。例如，要创建一个包含10个1的数组，您可以使用ones方法，
- en: '[PRE20]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: which returns an array of length 10,
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 返回长度为10的数组，
- en: '[PRE21]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'while pt.ones(2, 10) returns a matrix of 2 rows and 10 columns:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 而pt.ones(2, 10)返回一个2行10列的矩阵：
- en: '[PRE22]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: When working with factory methods you can specify the data type for the values
    in a tensor. While methods like ones return tensors of float values by default,
    you can override the default data type using the dtype attribute. For example,
    to create an array of integer ones, you can invoke
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用工厂方法时，您可以为张量中的值指定数据类型。虽然ones等方法默认返回浮点数张量，但您可以使用dtype属性覆盖默认数据类型。例如，要创建一个整数1的数组，您可以调用
- en: '[PRE23]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: which returns
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[PRE24]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Other PyTorch-supported data types include 16- and 32-bit integers, 16-, 32-,
    and 64-bit floats, bytes (unsigned 8-bit integers), and Booleans.[⁷](#pgfId-1186358)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 其他PyTorch支持的数据类型包括16位和32位整数、16位、32位和64位浮点数、字节（无符号8位整数）和布尔值。[⁷](#pgfId-1186358)
- en: 5.3 Creating PyTorch tensors of pseudorandom and interval values
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 创建PyTorch伪随机和间隔值张量
- en: This section introduces you to the PyTorch APIs used to create tensors populated
    with data values sampled from commonly used probability distributions, including
    standard normal, normal (Gaussian), and uniform distribution. The section also
    describes how to create tensors consisting of interval (regularly spaced) values.
    Learning the APIs described in this section is going to help you generate synthetic
    data sets valuable for testing and troubleshooting machine learning algorithms.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 本节向您介绍了用于创建填充有从常用概率分布中抽样的数据值的张量的PyTorch API，包括标准正态、正态（高斯）和均匀分布。本节还描述了如何创建由间隔（等间距）值组成的张量。学习本节中描述的API将帮助您生成用于测试和故障排除机器学习算法的合成数据集。
- en: 'Deep learning and many machine learning algorithms depend on the capability
    of generating pseudorandom numbers. Before using PyTorch random sampling factory
    methods, you want to invoke the manual_seed method to set the seed value used
    to sample the pseudorandom numbers. If you invoke the manual_seed using the same
    seed value as used in this book, you will be able to reproduce the results described
    in this section. Otherwise, your results look different. The following code snippets
    assume that you are using a seed value of 42 set:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习和许多机器学习算法依赖于生成伪随机数的能力。在使用PyTorch随机抽样工厂方法之前，您需要调用manual_seed方法来设置用于抽样伪随机数的种子值。如果您使用与本书中使用的相同的种子值调用manual_seed，您将能够重现本节中描述的结果。否则，您的结果看起来会不同。以下代码片段假定您使用的种子值为42：
- en: '[PRE25]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: After the seed is set, if you are using PyTorch v1.9.0, you should expect to
    obtain pseudorandom numbers identical to the ones in the following examples. The
    randn method samples from the standard normal distribution, so you can expect
    the values to have a mean of 0 and a standard deviation of 1\. To create a 3 ×
    3 tensor of the sampled values, invoke
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 设置种子后，如果您使用的是PyTorch v1.9.0，您应该期望获得与以下示例中相同的伪随机数。randn方法从标准正态分布中抽样，因此您可以期望这些值的均值为0，标准差为1。要创建一个3×3的张量以抽样值，调用
- en: '[PRE26]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: which outputs
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 输出
- en: '[PRE27]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'To sample values from a normal distribution having the mean and standard deviation
    different from 1 and 0, respectively, you can use the normal method, for example,
    specifying the mean of 100, standard deviation of 10, and a rank 2 tensor of 3
    rows and 3 columns:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 要从均值和标准差不同于1和0的正态分布中抽样值，您可以使用normal方法，例如，指定均值为100，标准差为10，以及3行3列的秩2张量：
- en: '[PRE28]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: resulting in
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 导致
- en: '[PRE29]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'For tensors of pseudorandom values sampled from a uniform distribution, you
    can use the randint method, for example, to sample uniformly from 0 (inclusive)
    to 10 (exclusive), and return a 3 × 3 matrix:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 对于从均匀分布中抽样的伪随机值的张量，您可以使用randint方法，例如，从0（包括）到10（不包括）均匀抽样，并返回一个3×3矩阵：
- en: '[PRE30]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: which produces
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 产生
- en: '[PRE31]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The randint and normal methods are the ones most frequently used in this book.
    PyTorch provides a comprehensive library of pseudorandom tensor generators,[⁸](#pgfId-1021730)
    but covering them all is beyond the scope of this book.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: randint 和 normal 方法是本书中最常用的方法。PyTorch 提供了一个全面的伪随机张量生成器库，[⁸](#pgfId-1021730)但本书不涵盖所有内容。
- en: As explained in more detail in section 5.5, there is significant memory overhead
    involved in creating a Python list of integer values. Instead, you can use the
    arange method to create PyTorch tensors with interval (regularly spaced) values
    in a specified range. PyTorch arange behaves similarly to the range operator in
    Python, so
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如 5.5 节更详细地解释的那样，在创建 Python 整数值列表时会涉及显著的内存开销。相反，您可以使用 arange 方法在指定范围内创建具有间隔（等间距）值的
    PyTorch 张量。PyTorch arange 的行为类似于 Python 中的 range 运算符，因此
- en: '[PRE32]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: returns
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[PRE33]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: As you would expect from your experience using Python, arange in PyTorch can
    be called with additional parameters for the range start, end, and step (sometimes
    called a stride), so to create a tensor of odd numbers from 1 to 11, you can use
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在使用 Python 时所期望的那样，在 PyTorch 中调用 arange 可以带有附加参数用于范围的起始、结束和步长（有时称为步进），因此要创建一个从
    1 到 11 的奇数张量，可以使用
- en: '[PRE34]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: which outputs
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 输出
- en: '[PRE35]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Just as with the Python range, the resulting sequence excludes the end sequence
    parameter value (the second argument), while the step is specified as the third
    argument to the method.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 Python range 一样，生成的序列不包括结束序列参数值（第二个参数），而步长被指定为该方法的第三个参数。
- en: Instead of having to calculate the value of the step size for the arange method,
    it can be more convenient to use the linspace method and specify the number of
    elements that should exist in the resulting tensor. For example, to create a tensor
    of 5 elements with values in the range that starts with 0 and ends with and includes
    the value of 10, you can use the linspace method,
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是必须计算 arange 方法的步长值，使用 linspace 方法并指定结果张量中应存在的元素数量可能更加方便。例如，要创建一个包含值在从 0 开始到
    10 结束并包括值 10 的 5 个元素的张量，可以使用 linspace 方法，
- en: '[PRE36]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: which results in
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 导致
- en: '[PRE37]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: As a part of the implementation, the linspace method calculates the appropriate
    step size to use such that all elements in the resulting tensor are within an
    equal distance of each other. Also, by default, linspace creates a tensor of floating
    point values.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 作为实现的一部分，linspace 方法计算适当的步长大小，以便生成的张量中的所有元素之间距离相等。此外，默认情况下，linspace 创建浮点值的张量。
- en: Now that you are familiar with functions that create tensors, you are prepared
    to move on to performing common tensor operations such as addition, multiplication,
    exponentiation, and others.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经熟悉了创建张量的函数，可以继续执行常见张量操作，例如加法、乘法、指数运算等。
- en: 5.4 PyTorch tensor operations and broadcasting
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4 PyTorch 张量操作和广播
- en: This section introduces you to the PyTorch features for performing common mathematical
    operations on PyTorch tensors and clarifies the rules for applying operations
    to tensors of various shapes. Upon completion of this section, you should be prepared
    to use PyTorch tensors as part of mathematical expressions that are used in machine
    learning code.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 本节向您介绍了 PyTorch 张量执行常见数学运算的功能，并澄清了对不同形状的张量应用操作的规则。完成本节后，您将能够在机器学习代码中将 PyTorch
    张量作为数学表达式的一部分使用。
- en: Since PyTorch overloads standard Python mathematical operators, including +,
    —, *, /, and **, it is easy to get started with tensor operations. For example,
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 PyTorch 重载了标准 Python 数学运算符，包括 +、-、*、/ 和 **，因此使用张量操作非常容易。例如，
- en: '[PRE38]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: is equivalent to invoking pt.arange(10).add(1), and both output
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 等同于调用 pt.arange(10).add(1)，两者都输出
- en: '[PRE39]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: When adding a PyTorch tensor and a compatible Python basic data type (float,
    integer, or Boolean), PyTorch atomically converts the latter to a PyTorch scalar
    tensor (this is known as *type coercion*). Hence, the operations
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 当添加 PyTorch 张量和兼容的基本 Python 数据类型（浮点数、整数或布尔值）时，PyTorch 会自动将后者转换为 PyTorch 标量张量（这称为*类型强制转换*）。因此，这些操作
- en: '[PRE40]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: and
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '[PRE41]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: are equivalent.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 是等价的。
- en: The default implementations of the PyTorch APIs perform immutable operations
    on tensors. So, when developing PyTorch machine learning code, it is critical
    that you remember that the addition operation as well as the other standard Python
    math operations overloaded by PyTorch return a new tensor instance. You can easily
    confirm that by running
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch API的默认实现对张量执行不可变操作。因此，在开发PyTorch机器学习代码时，您必须记住加法操作以及其他由PyTorch重载的标准Python数学运算符都会返回一个新的张量实例。您可以轻松通过运行以下命令进行确认
- en: '[PRE42]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: which outputs a tuple of PyObject identifiers with two distinct values, one
    for each tensor. PyTorch also provides a collection of in-place operators that
    mutate (modify) a tensor. This means that PyTorch replaces the tensor’s values
    directly in the memory of the tensor’s device, without allocating a new PyObject
    instance for a tensor. For example, using the add_ method
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch还提供了一组可就地（in-place）操作的操作符，这些操作符可更改张量的值。这意味着PyTorch将直接在张量设备的内存中替换张量的值，而不是为张量分配新的PyObject实例。例如，使用add\_方法
- en: '[PRE43]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: returns a tuple with two identical object identifiers.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个带有两个相同对象标识符的元组。
- en: Note In PyTorch API design, all operations that mutate (change) a tensor in
    place use a _ postfix, for example, mul_ for in-place multiplication, pow_ for
    in-place exponentiation, abs_ for in-place absolute value function, and so on.[⁹](#pgfId-1188847)
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 在PyTorch API设计中，所有的就地（in place）更改张量的操作都使用_后缀，例如mul_表示就地乘法，pow_表示就地幂运算，abs_表示就地绝对值函数等等。[⁹](#pgfId-1188847)
- en: When working on machine learning code you will surely find yourself having to
    perform common mathematical operations on nonscalar tensors. For example, if you
    are given two tensors a and b, what does it mean for PyTorch to find the value
    of a + b?
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理机器学习代码时，您肯定会发现自己不得不在非标量张量上执行常见的数学运算。例如，如果给定两个张量a和b，则PyTorch找到a + b的值是什么意思？
- en: Listing 5.3 Tensors added element-wise as they have identical shapes
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 列出5.3 张量按元素加和，因为它们具有相同的形状
- en: '[PRE44]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: As you should expect, since a has the value of
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所期望的那样，因为a的值是
- en: '[PRE45]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: and b has the value of
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: b的值是
- en: '[PRE46]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: their sum is equal to
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的和等于
- en: '[PRE47]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: so here a + b is equivalent to incrementing each element of the tensor a by
    1. This operation can be described as an element-wise addition of the tensors
    a and b since for each element of the tensor a PyTorch finds a value with a corresponding
    element index in the tensor b and adds them together to produce the output.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，a + b相当于将张量a的每个元素递增1。这个操作可以被描述为张量a和b的逐元素相加，因为对于张量a的每个元素，PyTorch找到仅一个对应的张量b的元素索引值，并将它们相加产生输出。
- en: What if you try to add
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如果尝试添加
- en: '[PRE48]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'where the logic of element-wise addition does not make immediate sense: which
    elements of tensor a should be increased by one? Should the first, second, or
    both rows of tensor a be incremented by one?'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在tensor a的逻辑中，元素按位相加没有立即意义，这意味着应将张量a的哪些元素按1递增？应将张量a的第一行，第二行还是两行都增加1？
- en: 'To understand how addition works in this example and other situations where
    the shapes of the tensors in an operation are different, you need to become familiar
    with *broadcasting*,[^(10)](#pgfId-1189687) which PyTorch performs behind the
    scenes to produce the following result for a + b:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '要理解此示例中加法的原理以及在操作中张量的形状不同时的其他情况，您需要熟悉*broadcasting*（[^(10)](#pgfId-1189687)），PyTorch在幕后执行它来生成a
    + b 的下面的结果:'
- en: '[PRE49]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: PyTorch attempts tensor broadcasting whenever the shapes of the tensors used
    in an operation are not identical. When performing an operation on two tensors
    with different sizes, some dimensions might be re-used, or “broadcast,” to complete
    the operation. If you are given two tensors, a and b, you can check whether an
    operation involving both can be performed using broadcasting by invoking can_broadcast.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 当操作中使用的张量的形状不相同时，PyTorch尝试执行张量广播。在两个大小不同的张量上执行操作时，一些维度可能会被重用或广播以完成操作。如果给定两个张量a和b，并且要在两个张量之间执行广播运算，可以通过调用can_broadcast来检查能否执行此操作。
- en: Listing 5.4 Broadcasting possible when can_broadcast returns true
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 列出5.4，当can_broadcast返回true时，可以broadcast
- en: '[PRE50]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'This broadcasting rule depends on the trailing dimensions of the tensors, or
    the dimensions of the tensors aligned in the reverse order. Even the simple example
    of adding a scalar to a tensor involves broadcasting: when a = pt.ones(5) and
    b = pt.tensor(42), the shapes are torch.Size([5]) and torch.Size([]), respectively.
    Hence, the scalar must be broadcast five times to tensor a as shown in figure
    5.2.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这个广播规则取决于张量的尾部维度，或者以相反顺序对齐的张量的维度。即使是将标量添加到张量的简单示例也涉及广播：当 a = pt.ones(5) 且 b
    = pt.tensor(42) 时，它们的形状分别为 torch.Size([5]) 和 torch.Size([])。因此，标量必须像图 5.2 中所示一样广播五次到张量
    a。
- en: '![05-02](Images/05-02.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![05-02](Images/05-02.png)'
- en: Figure 5.2 Broadcasting a scalar b to a rank 1 tensor a
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 将标量 b 广播到秩为 1 的张量 a
- en: Broadcasting does not require copying or duplicating tensor data in memory;
    instead, the contents of the tensor that result from broadcasting are created
    by directly computing from the values of the tensors used in the operation. Effective
    use and understanding of broadcasting can help you reduce the amount of memory
    needed for your tensors and increase the performance of your tensor operations.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 广播不需要在内存中复制或复制张量数据；相反，从操作中使用的张量的值直接计算产生广播结果的张量的内容。有效地使用和理解广播可以帮助您减少张量所需的内存量，并提高张量操作的性能。
- en: To illustrate broadcasting with a more complex example where a = pt.ones([2,
    5]) and b = pt.ones(5), notice that the broadcasting re-uses the values from the
    tensor b (right side of figure 5.3) such that the indices along the trailing dimension
    are aligned in the resulting a + b tensor, while the leading dimension is preserved
    from tensor a.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 为了用更复杂的示例说明广播，其中 a = pt.ones([2, 5]) 且 b = pt.ones(5)，请注意广播重复使用张量 b 的值（图 5.3
    的右侧），以便在生成 a + b 张量时对齐结果的尾部维度，同时保留来自张量 a 的前导维度。
- en: '![05-03](Images/05-03.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![05-03](Images/05-03.png)'
- en: Figure 5.3 Broadcasting a rank 1 tensor b to a rank 2 tensor a
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 将秩为 1 的张量 b 广播到秩为 2 的张量 a
- en: 'Based on the broadcasting examples you have seen so far, you might come away
    with an incorrect impression that broadcasting happens only in one direction:
    from one of the tensors in an operation to another. This is false. Notice that
    according to the rule from listing 5.4, both tensors involved in an operation
    can broadcast data to each other. For instance, in figure 5.4, tensor a is broadcast
    to tensor b three times (based on the first dimension), and then contents of tensor
    b are broadcast in the opposite direction (along the second dimension) to produce
    the resulting tensor a+b of the size torch.Size([3, 2, 5]).'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您目前所见的广播示例，您可能会错误地认为广播只发生在一个方向上：从操作中的一个张量到另一个张量。这是错误的。请注意，根据列表 5.4 中的规则，参与操作的两个张量都可以相互广播数据。例如，在图
    5.4 中，张量 a 被广播到张量 b 三次（基于第一维），然后张量 b 的内容沿着相反的方向（沿第二维）广播，以产生尺寸为 torch.Size([3,
    2, 5]) 的结果张量 a+b。
- en: '![05-04](Images/05-04.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![05-04](Images/05-04.png)'
- en: Figure 5.4 Broadcasting is bidirectional with the first dimension of b broadcast
    to a and the second dimension of a broadcast to b.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4 广播是双向的，其中 b 的第一维广播到 a，a 的第二维广播到 b。
- en: 5.5 PyTorch tensors vs. native Python lists
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.5 PyTorch 张量 vs. 原生 Python 列表
- en: In this section, you will dive into the details of how native Python data structures
    compare to PyTorch tensors with respect to memory use and into why PyTorch tensors
    can help you make more efficient use of memory for machine learning use cases.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将深入了解原生 Python 数据结构与 PyTorch 张量在内存使用方面的比较，并了解为什么 PyTorch 张量可以帮助您更有效地利用内存来处理机器学习用例。
- en: Most modern-day laptops use central processing units (CPUs) that run at frequencies
    from 2 to 3 Ghz. To keep the calculations simple, let’s ignore some of the advanced
    instruction for execution pipelining features of modern processors and interpret
    a 2-Ghz processor frequency to mean that it takes a processor roughly half a nanosecond
    (ns) to execute a single instruction, such as an instruction to add two numbers
    and store the result. While a processor can execute an instruction in less than
    1 ns, the processor has to wait over 100 times as long, anywhere from 50 to 100
    ns, to fetch a piece of data from the main computer memory (dynamic random access
    memory). Of course, some data a processor uses resides in a cache, which can be
    accessed within single-digit nanoseconds, but low-latency caches are limited in
    size, typically measured in single-digit MiB.[^(11)](#pgfId-1207914)
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现代笔记本电脑使用的中央处理单元（CPU）的运行频率为 2 到 3 GHz。为了保持计算简单，让我们忽略一些现代处理器执行流水线功能的高级指令，并将
    2 GHz 处理器频率解释为处理器大约需要半个纳秒（ns）来执行一条单个指令，例如执行加法运算并存储结果的指令。虽然处理器可以在少于 1 ns 的时间内执行一条指令，但处理器必须等待超过
    100 倍的时间，从 50 到 100 ns 不等，才能从主计算机内存（动态随机访问存储器）中获取一段数据。当然，处理器使用的一些数据存储在缓存中，可以在单个数字纳秒内访问，但低延迟缓存的大小有限，通常以单个数字
    MiB 为单位进行度量。[^11]
- en: Suppose you are writing a computer program where you need to perform some computations
    with a tensor of data, such as processing an array of 4,096 integers and incrementing
    each by 1\. To achieve high performance for such a program, lower-level programming
    languages such as C or C++ can allocate the input data array within a single chunk
    in computer memory. For example, in the C programming language, an array of 4,096
    integer values, each 64 bits in width, can be stored as a sequence of 64-bit values
    within some contiguous memory range, such as from address 0x8000f to address 0x9000f.[^(12)](#pgfId-1044906)
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在编写一个计算机程序，需要对数据张量执行一些计算，比如处理一个包含 4,096 个整数的数组，并将每个整数加 1。为了使这样的程序获得高性能，可以使用较低级别的编程语言，如
    C 或 C++，在计算机内存中为输入数据数组分配一个单一的块。例如，在 C 编程语言中，一个包含 4,096 个整数值的数组，每个整数值为 64 位，可以存储为某个连续内存范围内的
    64 位值序列，例如从地址 0x8000f 到地址 0x9000f。[^12]
- en: 'Assuming all of the 4,096 integer values are in a contiguous memory range,
    the values can be transferred from the main memory to the processor cache as a
    single chunk, helping to reduce the total latency of the addition calculation
    for the values. As shown on the left side of figure 5.5, a C integer occupies
    just enough memory needed to store the integer’s value so that an array of C integers
    can be stored as a sequence of addressable memory locations. Note that the number
    4,096 is chosen deliberately: since 4,096 * 8 (bytes per 64-bit integer) = 32,768
    bytes is a common L1 cache size for x86 processors in the year 2020\. This means
    that the roughly 100-ns latency penalty is incurred every time the cache needs
    to be flushed and refilled with another 4,096 integers that need to be fetched
    from the computer memory.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 假设所有 4,096 个整数值都在连续的内存范围内，那么这些值可以作为一个单一块从主存储器传输到处理器缓存中，有助于减少对值的加法计算的总延迟。如图 5.5
    的左侧所示，C 整数仅占用足够的内存以存储整数值，以便可以将一系列 C 整数存储为可寻址的内存位置序列。请注意，数字 4,096 是有意选择的：因为 4,096
    * 8（每个 64 位整数的字节数）= 32,768 字节是 2020 年 x86 处理器的常见 L1 缓存大小。这意味着每次需要刷新缓存并用另外 4,096
    个整数重新填充缓存时，都会产生大约 100 ns 的延迟惩罚，这些整数需要从计算机内存中获取。
- en: '![05-05](Images/05-05.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![05-05](Images/05-05.png)'
- en: Figure 5.5 Values of C integers (left) are stored directly as addressable memory
    locations. Python integers (right) are stored as address references to a generic
    PyObject_HEAD structure, which specifies the data type (integer) and the data
    value.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5 C 整数的值（左侧）直接存储为可寻址的内存位置。Python 整数（右侧）存储为对通用 PyObject_HEAD 结构的地址引用，该结构指定了数据类型（整数）和数据值。
- en: This high-performance approach does not work with native Python integers or
    lists. In Python, all data types are stored in the computer memory as Python objects
    (PyObjects). This means that for any data value, Python allocates memory to store
    the value along with a metadata descriptor known as PyObject_HEAD (right side
    of figure 5.5), which keeps track of the data type (e.g., whether the data bits
    describe an integer or a floating point number) and supporting metadata, including
    a reference counter to keep track of whether the data value is in use.[^(13)](#pgfId-1044986)
    For floating point and other primitive data values, the overhead of the PyObject
    metadata can more than triple the amount of memory required to store a data value.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这种高性能方法不适用于本地 Python 整数或列表。在 Python 中，所有数据类型都以 Python 对象（PyObjects）的形式存储在计算机内存中。这意味着对于任何数据值，Python
    分配内存来存储值以及称为 PyObject_HEAD 的元数据描述符（图 5.5 右侧），该描述符跟踪数据类型（例如，数据位描述整数还是浮点数）和支持元数据，包括一个引用计数器，用于跟踪数据值是否正在使用。对于浮点数和其他原始数据值，PyObject
    元数据的开销可能会使存储数据值所需的内存量增加两倍以上。
- en: To make matters worse from a performance standpoint, Python lists (e.g., a list
    PyObject as illustrated on the left side of figure 5.6) store the values by reference
    to their PyObject memory addresses (right side of figure 5.6) and rarely store
    all the values within a continuous chunk in memory. Since each PyObject stored
    by a Python list may be scattered across many dispersed locations in computer
    memory, in the worst case scenario, there is a potential for 100-ns latency penalty
    per value in a Python list, due to the need to flush out and refill the cache
    for each of the values.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 从性能角度来看，情况更糟糕的是，Python 列表（例如，如图 5.6 左侧所示的 list PyObject）通过引用存储值到它们的 PyObject
    内存地址（图 5.6 右侧）并且很少将所有值存储在连续的内存块中。由于 Python 列表存储的每个 PyObject 可能分散在计算机内存中的许多位置，所以在最坏的情况下，每个
    Python 列表中的值可能会有 100 纳秒的潜在延迟惩罚，因为需要为每个值刷新并重新填充缓存。
- en: '![05-06](Images/05-06.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![05-06](Images/05-06.png)'
- en: Figure 5.6 Integers in a Python list (PyListObject) are accessed by reference
    (memory address) as items in the PyListObjects, requiring an additional memory
    access to find the PyObject for each integer. Depending on memory fragmentation,
    individual-integer PyObjects can be scattered across memory, leading to frequent
    cache misses.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6 中的整数在 Python 列表（PyListObject）中通过引用（内存地址）作为 PyListObjects 中的项目进行访问，需要额外的内存访问以查找每个整数的
    PyObject。根据内存碎片化程度，单个整数的 PyObjects 可以分散在内存中，导致频繁的缓存未命中。
- en: PyTorch tensors (as well as other libraries such as NumPy) implement high-performance
    data structures using low-level, C-based code to overcome the inefficiencies of
    the higher-level Python-native data structures. PyTorch, specifically, uses a
    C-based ATen tensor library[^(14)](#pgfId-1208747) to ensure that PyTorch tensors
    store the underlying data using cache-friendly, contiguous memory chunks (called
    *blobs* in ATen) and provides bindings from Python to C++ to support access to
    the data via PyTorch Python APIs.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 张量（以及其他库，如 NumPy）使用基于低级别 C 代码实现高性能数据结构，以克服较高级别的 Python 本地数据结构的低效率。具体而言，PyTorch
    使用基于 C 的 ATen 张量库[^(14)](#pgfId-1208747)，确保 PyTorch 张量使用友好的缓存、连续的内存块（在 ATen 中称为
    *blobs*）存储底层数据，并提供了从 Python 到 C++ 的绑定，以支持通过 PyTorch Python API 访问数据。
- en: To illustrate the difference in performance, look at the following code snippet
    that uses the Python timeit library to measure the performance of processing lists
    of integer values ranging from 2 to roughly 268 million (2^(28)) in length and
    incrementing each value in a list by 1,
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明性能差异，请看下面的代码片段，使用 Python 的 timeit 库来测量处理长度从 2 到大约 268 百万（2^(28)）的整数值列表的性能，并将列表中的每个值递增
    1，
- en: '[PRE51]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'and using a similar approach to measure the time it takes to increment the
    values in a tensor array by 1:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 使用类似的方法来测量递增张量数组中值所需的时间：
- en: '[PRE52]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'As shown in figure 5.7, you can compare the performance of PyTorch tensors
    over native Python lists by plotting the sizes on the x-axis versus ratio on the
    y-axis, where the ratio is defined as:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 5.7 所示，可以通过将大小作为 x 轴、比率作为 y 轴的图形比较 PyTorch 张量与本地 Python 列表的性能，其中比率定义为：
- en: '[PRE53]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '![05-07](Images/05-07.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![05-07](Images/05-07.png)'
- en: Figure 5.7 The ratio of Python to PyTorch performance shows consistently faster
    PyTorch performance for the increment operation benchmark, starting with lists
    of 10,000 elements and higher.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7 Python 到 PyTorch 性能比的比例显示了增量操作基准测试的一致更快的 PyTorch 性能，从具有 10,000 个元素及更高数量的列表开始。
- en: Summary
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: PyTorch is a deep learning—oriented framework with support for high-performance
    tensor-based machine learning algorithms.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch 是一个面向深度学习的框架，支持基于高性能张量的机器学习算法。
- en: PyTorch tensors generalize scalars, arrays (lists), matrices, and higher-dimensional
    arrays into a single high-performance data structure.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch 张量将标量、数组（列表）、矩阵和更高维数组泛化为单个高性能数据结构。
- en: Operations that use multiple tensors, including tensor addition and multiplication,
    depend on broadcasting to align tensor shapes.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用多个张量的操作，包括张量加法和乘法，依赖于广播以对齐张量形状。
- en: C/C++-based tensors in PyTorch are more memory efficient and enable higher compute
    performance compared to Python-native data structures.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch 中基于 C/C++ 的张量比 Python 本地数据结构更节省内存，并且能够实现更高的计算性能。
- en: '^(1.)Tesla machine learning use cases for PyTorch are presented by Andrej Karpathy,
    director of AI at Tesla: [https://www.youtube.com/watch?v=oBklltKXtDE](https://www.youtube.com/watch?v=oBklltKXtDE).'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 由特斯拉 AI 主管 Andrej Karpathy 提出的 PyTorch 机器学习用例：[https://www.youtube.com/watch?v=oBklltKXtDE](https://www.youtube.com/watch?v=oBklltKXtDE)。
- en: '^(2.)OpenAI is well known for creating state-of-the-art natural language processing
    GPT models, standardized on PyTorch: [https://openai.com/blog/openai-pytorch/](https://openai.com/blog/openai-pytorch/).'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 因创建基于 PyTorch 的最先进自然语言处理 GPT 模型而闻名：[https://openai.com/blog/openai-pytorch/](https://openai.com/blog/openai-pytorch/)。
- en: ^(3.)Of course in Python it is possible to use slicing notation, but that is
    not relevant to this explanation.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在 Python 中可以使用切片符号，但这与本说明无关。
- en: '^(4.)Refer to the torch.Tensor documentation for the complete list of the subclasses
    in PyTorch: [https:// pytorch.org/docs/stable/tensors.html#torch-tensor](https://pytorch.org/docs/stable/tensors.html#torch-tensor).'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 PyTorch 中所有子类的完整列表，请参阅 torch.Tensor 文档：[https:// pytorch.org/docs/stable/tensors.html#torch-tensor](https://pytorch.org/docs/stable/tensors.html#torch-tensor)。
- en: ^(5.)The comprehensive listing of the PyTorch supported dtype values is available
    at [http://mng.bz/YwyB](http://mng.bz/YwyB).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 支持的 dtype 值的全面列表可在[http://mng.bz/YwyB](http://mng.bz/YwyB)上找到。
- en: '^(6.)The NestedTensor class is available as a PyTorch package here: [https://github.com/pytorch/nestedtensor](https://github.com/pytorch/nestedtensor).'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: NestedTensor 类作为 PyTorch 包在这里提供：[https://github.com/pytorch/nestedtensor](https://github.com/pytorch/nestedtensor)。
- en: ^(7.)Detailed information about the PyTorch tensor data types is available from
    [https://pytorch.org/docs/stable/ tensors.html](https://pytorch.org/docs/stable/tensors.html).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 PyTorch 张量数据类型的详细信息，请参阅[https://pytorch.org/docs/stable/ tensors.html](https://pytorch.org/docs/stable/tensors.html)。
- en: ^(8.)For detailed documentation of the PyTorch random sampling factory methods,
    visit [http://mng.bz/GOqv](http://mng.bz/GOqv).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 PyTorch 随机抽样工厂方法的详细文档，请访问[http://mng.bz/GOqv](http://mng.bz/GOqv)。
- en: ^(9.)For a detailed reference on in-place tensor operations, visit [https://pytorch.org/docs/stable/tensors.html](https://pytorch.org/docs/stable/tensors.html)
    and search for “in-place.”
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 关于原地张量操作的详细参考，请访问[https://pytorch.org/docs/stable/tensors.html](https://pytorch.org/docs/stable/tensors.html)
    并搜索“in-place”。
- en: ^(10.)Broadcasting is a popular technique used by various computing libraries
    and packages such as NumPy, Octave, and others. For more information about PyTorch
    broadcasting, visit [https://pytorch.org/docs/stable/ notes/broadcasting.html](https://pytorch.org/docs/stable/notes/broadcasting.html).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 广播是各种计算库和软件包（如 NumPy、Octave 等）中常用的技术。有关 PyTorch 广播的更多信息，请访问[https://pytorch.org/docs/stable/
    notes/broadcasting.html](https://pytorch.org/docs/stable/notes/broadcasting.html)。
- en: ^(11.)For a comprehensive breakdown of latency numbers that every computer programmer
    should know, visit [https://gist.github.com/jboner/2841832](https://gist.github.com/jboner/2841832).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 了解每位计算机程序员都应该知道的延迟数字的全面解析，请访问[https://gist.github.com/jboner/2841832](https://gist.github.com/jboner/2841832)。
- en: ^(12.)Of course, actual memory addresses for a program in a modern computer
    are unlikely to have values like 0x8000f or 0x9000f; these values are used here
    for illustration purposes.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，现代计算机程序的实际内存地址不太可能具有像 0x8000f 或 0x9000f 的值；这些值仅用于说明目的。
- en: ^(13.)Python uses this reference counter to decide whether the data value is
    no longer in use so that the memory used for the data can be safely de-allocated
    and released for use by other data.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ^(13.)Python 使用此引用计数器来确定数据值是否不再使用，以便可以安全地释放用于数据的内存，并释放给其他数据使用。
- en: ^(14.)For ATen documentation, visit [https://pytorch.org/cppdocs/#aten](https://pytorch.org/cppdocs/#aten).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ^(14.)有关 ATen 文档，请访问[https://pytorch.org/cppdocs/#aten](https://pytorch.org/cppdocs/#aten)。
