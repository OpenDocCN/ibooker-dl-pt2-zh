["```py\ndef visualize_gp_belief(model, likelihood):\n    with torch.no_grad():                                ❶\n        predictive_distribution = likelihood(model(xs))  ❶\n        predictive_mean = predictive_distribution.mean   ❶\n        predictive_upper, predictive_lower =             ❶\n        ➥ predictive_distribution .confidence_region()  ❶\n\n    plt.figure(figsize=(8, 6))\n\n    plt.plot(xs, ys, label=\"objective\", c=\"r\")\n    plt.scatter(train_x, train_y, marker=\"x\", c=\"k\", label=\"observations\")\n\n    plt.plot(xs, predictive_mean, label=\"mean\")          ❷\n    plt.fill_between(\n        xs.flatten(), predictive_upper, predictive_lower, alpha=0.3,\n        label=\"95% CI\"\n    )                                                    ❸\n\n    plt.legend(fontsize=15);\n```", "```py\nclass ConstantMeanGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n       super().__init__(train_x, train_y, likelihood)\n       self.mean_module = gpytorch.means.ConstantMean()   ❶\n       self.covar_module = gpytorch.kernels.RBFKernel()\n\n    def forward(self, x):\n       mean_x = self.mean_module(x)\n       covar_x = self.covar_module(x)\n       return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n```", "```py\nlengthscale = 1\nnoise = 1e-4\n\nlikelihood = gpytorch.likelihoods.GaussianLikelihood()     ❶\nmodel = ConstantMeanGPModel(train_x, train_y, likelihood)  ❶\n\nmodel.covar_module.lengthscale = lengthscale               ❷\nmodel.likelihood.noise = noise                             ❷\n\nmodel.eval()\nlikelihood.eval()\n\nvisualize_gp_belief(model, likelihood)\n```", "```py\n# declare the GP\nlengthscale = 1\nnoise = 1e-4\n\nlikelihood = gpytorch.likelihoods.GaussianLikelihood()\nmodel = ConstantMeanGPModel(train_x, train_y, likelihood)\n\n# fix the hyperparameters\nmodel.covar_module.lengthscale = lengthscale\nmodel.likelihood.noise = noise\n```", "```py\nmll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n```", "```py\noptimizer = torch.optim.Adam([model.mean_module.constant], lr=0.01)\n```", "```py\nmodel.train()                         ❶\nlikelihood.train()                    ❶\n\nlosses = []\nconstants = []\nfor i in tqdm(range(500)):\n    optimizer.zero_grad()\n\n    output = model(train_x)\n    loss = -mll(output, train_y)      ❷\n\n    loss.backward()                   ❸\n\n    losses.append(loss.item())\n    constants.append(model.mean_module.constant.item())\n\n    optimizer.step()                  ❸\n\nmodel.eval()                          ❹\nlikelihood.eval()                     ❹\n```", "```py\nclass LinearMeanGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super().__init__(train_x, train_y, likelihood)\n        self.mean_module = gpytorch.means.LinearMean(1)    ❶\n        self.covar_module = gpytorch.kernels.RBFKernel()\n```", "```py\nclass QuadraticMean(gpytorch.means.Mean):\n    def __init__(self, batch_shape=torch.Size(), bias=True):\n        ...\n\n    def forward(self, x):\n        ...\n```", "```py\nclass QuadraticMean(gpytorch.means.Mean):\n    def __init__(self, batch_shape=torch.Size(), bias=True):\n        super().__init__()\n        self.register_parameter(\n            name=\"second\",\n            parameter=torch.nn.Parameter(torch.randn(*batch_shape, 1, 1))\n        )                                                                 ❶\n\n        self.register_parameter(\n            name=\"first\",\n            parameter=torch.nn.Parameter(torch.randn(*batch_shape, 1, 1))\n        )                                                                 ❷\n\n        if bias:\n            self.register_parameter(\n                name=\"bias\",\n                parameter=torch.nn.Parameter(torch.randn(*batch_shape, 1))\n            )                                                             ❸\n        else:\n            self.bias = None\n```", "```py\nclass QuadraticMean(gpytorch.means.Mean):\n    def __init__(self, train_x, train_y, likelihood):\n        ...                                               ❶\n\n    def forward(self, x):\n        res = x.pow(2).matmul(self.second).squeeze(-1) \\\n            + x.matmul(self.first).squeeze(-1)            ❷\n        if self.bias is not None:\n            res = res + self.bias\n        return res\n```", "```py\nclass QuadraticMeanGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super().__init__(train_x, train_y, likelihood)\n        self.mean_module = QuadraticMean()\n        self.covar_module = gpytorch.kernels.RBFKernel()\n\n    def forward(self, x):\n        ...                 ❶\n```", "```py\nclass ScaleGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super().__init__(train_x, train_y, likelihood)\n        self.mean_module = gpytorch.means.ZeroMean()\n        self.covar_module =\n        ➥ gpytorch.kernels.ScaleKernel(\n          gpytorch.kernels.RBFKernel())     ❶\n\n    def forward(self, x):\n        ...                                 ❷\n```", "```py\noptimizer = torch.optim.Adam(model.covar_module.parameters(), lr=0.01)\n```", "```py\nclass MaternGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood, nu):\n        super().__init__(train_x, train_y, likelihood)\n        self.mean_module = gpytorch.means.ZeroMean()\n        self.covar_module = gpytorch.kernels.MaternKernel(nu)\n\n    def forward(self, x):\n        ...                  ❶\n```", "```py\nlikelihood = gpytorch.likelihoods.GaussianLikelihood()\nmodel = MaternGPModel(train_x, train_y, likelihood, 0.5)\n\n...     ❶\n\nvisualize_gp_belief(model, likelihood)\n```", "```py\ndef ackley(x):\n    # a modification of https:/ /www.sfu.ca/~ssurjano/ackley.xhtml\n    return -20 * torch.exp(\n        -0.2 * torch.sqrt((x[:, 0] ** 2 + x[:, 1] ** 2) / 2)\n    )\n    ➥ - torch.exp(torch.cos(2 * pi * x[:, 0] / 3)\n    ➥ + torch.cos(2 * pi * x[:, 1]))\n```", "```py\nclass ARDGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super().__init__(train_x, train_y, likelihood)\n        self.mean_module = gpytorch.means.ZeroMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(\n            gpytorch.kernels.RBFKernel(ard_num_dims=2)\n        )\n\n    def forward(self, x):\n        ...                 ❶\n```", "```py\ntorch.manual_seed(0)\ntrain_x = torch.rand(size=(100, 2)) * 6 - 3\ntrain_y = ackley(train_x)\n```", "```py\n>>> model.covar_module.base_kernel.lengthscale\ntensor([[0.7175, 0.4117]])\n```"]