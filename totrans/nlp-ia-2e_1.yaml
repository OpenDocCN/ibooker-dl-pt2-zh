- en: 1 Machines that read and write (NLP overview)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 读写的机器（NLP概述）
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章内容包括
- en: The power of human language
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人类语言的力量
- en: How natural language processing (NLP) is changing society
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）如何改变社会
- en: The kinds of NLP tasks that machines can now do well
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器现在可以很好地完成的NLP任务的种类
- en: Why unleashing the NLP genie is profitable …​ and dangerous
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 释放NLP精灵的利润...和危险
- en: How to start building a simple chatbot
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何开始构建一个简单的聊天机器人
- en: How NLP technology is programming itself and making itself smarter
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）技术是如何自我编程并使自身变得更加智能的
- en: Words are powerful. They can change minds. And they can change the world. Natural
    language processing puts the power of words into algorithms. Those algorithms
    are changing your world right before your eyes. You are about to see how the majority
    of the words and ideas that enter your mind are filtered and generated by NLP
    and how you can take back some of that control over your world.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 话语是有力量的。它们可以改变思想。它们也可以改变世界。自然语言处理将话语的力量置于算法之中。这些算法正在改变你的世界，就在你眼前。你将看到大多数进入你头脑的词语和想法是如何经过NLP过滤和生成的，以及你如何重新获得对你世界的部分控制。
- en: Imagine what you would do with a machine that could understand and act on every
    word it reads on the Internet? Imagine the information and knowledge you’d be
    able to harvest and profit from. NLP promises to create the second information
    revolution by turning vast amounts of unstructured data into actionable knowledge
    and understanding.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，如果有一台机器能够理解并对其在互联网上阅读的每个词语采取行动，你会做什么？想象一下你将能够收获和从中获利的信息和知识。NLP承诺通过将大量非结构化数据转化为可操作的知识和理解来创建第二次信息革命。
- en: Early on, Big Tech discovered the power of NLP to glean knowledge from natural
    language text. They use that power to affect our behavior and our minds in order
    to improve their bottom line.^([[1](#_footnotedef_1 "View footnote.")]) Governments
    too are waking up to the impact NLP has on culture, society and humanity. Fortunately,
    a few courageous liberal democracies are attempting to free your mind by steering
    businesses towards sustainable and ethical uses for NLP.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 早期，大型科技公司发现了从自然语言文本中获取知识的NLP的力量。他们利用这种力量来影响我们的行为和思想，以改善他们的底线。政府也开始意识到NLP对文化、社会和人类的影响。幸运的是，一些勇敢的自由民主国家正试图通过引导企业走向可持续和道德的NLP用途来解放你的思想。
- en: On the other end of the spectrum, authoritarian governments are using NLP to
    coopt our prosocial instincts to make us easier to track and control. The Chinese
    government uses NLP to prevent you from even talking about Tibet or Hong Kong
    in the video games you play.^([[2](#_footnotedef_2 "View footnote.")]) The authors
    of this book needed to dig through the Internet Archive to replace disappearing
    article links with permalinks.^([[3](#_footnotedef_3 "View footnote.")]) Governments
    and businesses that censor public media are corrupting the datasets used by even
    the most careful NLP engineers who only use high quality online encyclopedias
    for training.^([[4](#_footnotedef_4 "View footnote.")]) And surprisingly, even
    in the US, there are corporations, politicians, and government agencies which
    use NLP to influence the public discourse about pandemics, climate change, and
    many other of the *21 Lessons for the 21st Century*.^([[5](#_footnotedef_5 "View
    footnote.")]) NLP is even being used to influence what you think about AI and
    NLP itself. Of course, not all corporations and politicians have your best interests
    at heart.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一端，威权政府正在利用NLP来利用我们的亲社会本能，使我们更容易被追踪和控制。中国政府利用NLP阻止你甚至在玩的视频游戏中谈论西藏或香港。这本书的作者们需要翻阅互联网档案来用永久链接替换消失的文章链接。审查公共媒体的政府和企业正在腐蚀甚至是最仔细的NLP工程师所使用的数据集，这些工程师仅使用高质量的在线百科全书进行培训。令人惊讶的是，即使在美国，也有一些公司、政治家和政府机构利用NLP影响关于大流行病、气候变化以及许多其他“21世纪的21课”的公共话语。NLP甚至被用来影响你对AI和NLP本身的看法。当然，并非所有的公司和政治家都把你的最好利益放在心上。
- en: In this chapter, you will begin to build your NLP understanding and skill so
    you can take control of the information and ideas that affect what you believe
    and think. You first need to see all the ways NLP is used in the modern world.
    This chapter will open your eyes to these NLP applications happening behind the
    scenes in your everyday life. Hopefully this will help you write a few lines of
    Python code to help you track, classify, and influence the packets of thought
    bouncing around on the Internet and into your brain. Your understanding of natural
    language processing will give you greater influence and control over the words
    and ideas in your world. And it will give you and your business the ability to
    escape Big Tech’s stranglehold on information, so you can succeed.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，你将开始建立你对自然语言处理（NLP）的理解和技能，以便能够控制影响你所相信和思考的信息和思想。你首先需要看到现代世界中 NLP 被应用的各种方式。这一章将让你看到这些
    NLP 应用在你日常生活中背后发生的情况。希望这将帮助你写几行 Python 代码，以帮助你追踪、分类和影响在互联网上和你的大脑中弹来弹去的思想包。你对自然语言处理的理解将让你在你的世界中拥有更大的影响力和控制力。它将让你和你的业务有能力摆脱大科技公司对信息的控制，从而取得成功。
- en: 1.1 Programming language vs. natural language
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 编程语言与自然语言
- en: Programming languages are very similar to natural languages like English. Both
    kinds of languages are used to communicate instructions from one information processing
    system to another. Both languages can communicate thoughts from human to human,
    human to machine, or even machine to machine. Both languages define the concept
    of *tokens*, the smallest packet of meaningful text. No matter whether your text
    is natural language or a programming language, the first thing that a machine
    does is to split the text into tokens. For natural language, tokens are usually
    words or combinations of words that go together (compound words).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 编程语言与自然语言如英语非常相似。两种语言都用于将指令从一个信息处理系统传递到另一个信息处理系统。两种语言都可以将思想从人类传达到人类、从人类传达到机器，甚至是从机器传达到机器。两种语言都定义了“标记”的概念，即最小的有意义文本包。无论你的文本是自然语言还是编程语言，机器首先要做的事情是将文本分割成标记。对于自然语言，标记通常是单词或一起出现的单词组合（复合词）。
- en: And both natural and programming languages use *grammars*. A grammar is a set
    of rules that tell you how to combine words in a sequence to create an expression
    or statement that others will understand. And the words "expression" and "statement"
    mean similar things whether you are in a computer science class or an English
    grammar class. And you may have heard of *regular expressions* in computer science.
    They give you a way to create grammar rules for processing text. In this book,
    you will use regular expressions to match patterns in all kinds of text, including
    natural language and computer programs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言和编程语言都使用*语法*。语法是一组规则，告诉你如何将单词组合成序列，以创建他人可以理解的表达式或语句。无论你是在计算机科学课上还是在英语语法课上，单词“表达式”和“语句”都意味着类似的东西。你可能已经听说过计算机科学中的*正则表达式*。它们为你提供了一种创建处理文本的语法规则的方式。在本书中，你将使用正则表达式来匹配各种文本，包括自然语言和计算机程序。
- en: Despite these similarities between programming and natural language, you need
    new skills and new tools to process natural language with a machine. Programming
    languages are artificially designed languages we use to tell a computer what to
    do. Computer programming languages are used to explicitly define a sequence of
    mathematical operations on bits of information, ones and zeros. And programming
    languages only need to be *processed* by machines rather than *understood*. A
    machine needs to do *what* the programmer asks it to do. It does not need to understand
    *why* the program is the way it is. And it doesn’t need abstractions or mental
    models of the computer program to understand anything outside of the world of
    ones and zeroes that it is processing. And almost all computers use the Von Neumann
    architecture developed in 1945.^([[6](#_footnotedef_6 "View footnote.")]) Modern
    CPUs (Central Processing Units) implement the *Von Neumann architecture* as a
    register machine, a version of the *universal Turing machine* idea of 1936.^([[7](#_footnotedef_7
    "View footnote.")])
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管编程语言和自然语言之间存在这些相似之处，但你需要新的技能和新的工具来使用机器处理自然语言。编程语言是我们用来告诉计算机要做什么的人工设计语言。计算机编程语言用于明确定义一系列对信息位进行数学操作的操作，即1和0。而编程语言只需要被机器*处理*而不是*理解*。机器需要做*程序员要求它做的事情*。它不需要理解*为什么*程序是这样的。它不需要计算机程序的抽象或心理模型来理解在处理的1和0之外的任何事情。几乎所有计算机都使用1945年开发的冯·诺依曼体系结构。现代
    CPU（中央处理器）将*冯·诺依曼体系结构*实现为一个寄存器机器，这是1936年*通用图灵机*思想的一个版本。
- en: Natural languages, however, evolved naturally, *organically*. Natural languages
    communicate ideas, understanding, and knowledge between living organisms that
    have brains rather than CPUs. These natural languages must be "runnable" or *understandable*
    on a wide variety of wetware (brains). In some cases, natural language even enables
    communication across animal species. Koko (gorilla), Woshoe (chimpanzee), Alex
    (parrot) and other famous animals have demonstrated command of some English words.^([[8](#_footnotedef_8
    "View footnote.")]). Reportedly, Alex the parrot discovered the meaning of the
    word "none" on its own. Alex’s dying words to its grieving owner were "Be good,
    I love you" ([https://www.wired.com/2007/09/super-smart-par](09.html)). And Alex’s
    words inspired Ted Chiang’s masterful short story "The Great Silence." That is
    profound cross-species communication, no matter whether the words came from intelligence
    and sentience or not.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，自然语言是自然地、*有机地*演变的。自然语言在具有大脑而不是 CPU 的生物之间传达思想、理解和知识。这些自然语言必须能够在各种各样的湿件（大脑）上“运行”或*理解*。在某些情况下，自然语言甚至能够实现跨物种的交流。Koko（大猩猩）、Woshoe（黑猩猩）、Alex（鹦鹉）和其他一些有名的动物已经证明了它们掌握了一些英语单词。据报道，Alex
    鹦鹉自己发现了单词“none”的含义。Alex 对它悲伤的主人说的最后一句话是“做个好孩子，我爱你”。Alex 的这句话激发了 Ted Chiang 的杰作短篇小说《The
    Great Silence》。这是深刻的跨物种交流，不管这些词语来自于智慧和感知还是其他什么。
- en: Given how differently natural languages and programming languages evolved, it
    is no surprise they’re used for different things. We do not use programming languages
    to tell each other about our day or to give directions to the grocery store. Similarly,
    natural languages did not evolve to be readily compiled into thought packets that
    can be manipulated by machines to derive conclusions. But that’s exactly what
    you are going to learn how to do with this book. With NLP you can program machines
    to process natural language text to derive conclusions, infer new facts, create
    meaningful abstractions, and even respond meaningfully in a conversation.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管自然语言和编程语言的演变方式有所不同，它们被用于不同的事物并不令人意外。我们不会用编程语言来互相讲述我们的一天，或者给出去杂货店的指示。同样，自然语言并没有演变成为可以被机器处理以推导结论的思维包。但这正是你将要通过这本书学会的。通过自然语言处理（NLP），你可以编写机器处理自然语言文本以推导结论、推断新事实、创建有意义的抽象，甚至在对话中进行有意义的回应。
- en: Even though there are no compilers for natural language there are *parsers*
    and *parser generators*, such as PEGN ^([[9](#_footnotedef_9 "View footnote.")])
    and SpaCy’s Matcher class. And SpaCy allows you to define word patterns or grammars
    with a syntax similar to regular expressions. But there is no single algorithm
    or Python package that takes natural language text and turns it into machine instructions
    for automatic computation or execution. Stephen Wolfram has essentially spent
    his life trying to build a general-purpose intelligent "computational" machine
    that can interact with us in plain English. Even he has resorted to assembling
    a system out of many different NLP and AI algorithms that must be constantly expanded
    and evolved to handle new kinds of natural language instructions.^([[10](#_footnotedef_10
    "View footnote.")]) And towards the end of this book you will learn about our
    open source chatbot framework `qary.ai` that allows you to plug in any Python
    algorithm you can find or dream up.^([[11](#_footnotedef_11 "View footnote.")])
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然自然语言没有编译器，但有*解析器*和*解析器生成器*，比如 PEGN ^([[9](#_footnotedef_9 "查看脚注.")]) 和 SpaCy
    的 Matcher 类。而 SpaCy 允许您使用类似正则表达式的语法定义单词模式或语法。但是，并没有单一的算法或Python包可以将自然语言文本转换为自动计算或执行的机器指令。斯蒂芬·沃尔夫勒姆（Stephen
    Wolfram）基本上花费了他的一生来构建一个通用的智能“计算”机器，可以用普通英语与我们交互。甚至他也不得不将一个系统组装成许多不同的NLP和AI算法，必须不断扩展和发展以处理新种类的自然语言指令。^([[10](#_footnotedef_10
    "查看脚注.")]) 而且，在本书的最后，您将了解到我们的开源聊天机器人框架`qary.ai`，它允许您插入任何您能找到或构想出的Python算法。^([[11](#_footnotedef_11
    "查看脚注.")])
- en: With this book, you can build on the shoulders of giants. If you understand
    all the concepts in this book, you too will be able to combine these approaches
    to create remarkably intelligent conversational chatbots. You will even be able
    to build bots that understand and generate more meaningful and truthful text than
    ChatGPT or whatever comes next in this world of rent-seeking AI apps.^([[12](#_footnotedef_12
    "View footnote.")]) You have a big advantage over BigTech, you actually care about
    your users.^([[13](#_footnotedef_13 "View footnote.")])
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这本书，您可以站在巨人的肩膀上。如果您理解了本书中的所有概念，您也将能够结合这些方法创建出非常智能的对话式聊天机器人。您甚至可以构建出比ChatGPT或这个世界上接下来出现的任何租赁AI应用更有意义和更真实的文本理解和生成机器人。^([[12](#_footnotedef_12
    "查看脚注.")]) 您比BigTech有很大的优势，您真的关心您的用户。^([[13](#_footnotedef_13 "查看脚注.")])
- en: Natural language processing
  id: totrans-21
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 自然语言处理
- en: Natural language processing is an evolving practice in computer science and
    artificial intelligence (AI) concerned with processing natural languages such
    as English or Mandarin. This processing generally involves translating natural
    language into data (numbers) that a computer can use to learn about the world.
    This understanding of the world is sometimes used to generate natural language
    text that reflects that understanding.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理是计算机科学和人工智能（AI）中不断发展的一项实践，涉及处理诸如英语或普通话之类的自然语言。这个处理通常涉及将自然语言转换为计算机可以用来了解世界的数据（数字）。这种对世界的理解有时用于生成反映该理解的自然语言文本。
- en: This chapter shows you how your software can *process* natural language to produce
    useful output. You might even think of your program as a natural language interpreter,
    similar to how the Python interpreter processes source code. When the computer
    program you develop processes natural language, it will be able to act on those
    statements or even reply to them.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本章向您展示了您的软件如何*处理*自然语言以生成有用的输出。您甚至可以将您的程序视为自然语言解释器，类似于Python解释器处理源代码的方式。当您开发的计算机程序处理自然语言时，它将能够对这些陈述采取行动，甚至回复它们。
- en: Unlike a programming language where each keyword has an unambiguous interpretation,
    natural languages are much more fuzzy. This fuzziness of natural language leaves
    open to you the interpretation of each word. So, you get to choose how the bot
    responds to each situation. Later you will explore advanced techniques in which
    the machine can learn from examples, without you knowing anything about the content
    of those examples.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 与编程语言不同，每个关键字都有一个明确的解释，自然语言要模糊得多。这种自然语言的模糊性使您可以解释每个单词。因此，您可以选择机器如何响应每种情况。稍后，您将探索高级技术，其中机器可以从示例中学习，而您对这些示例的内容一无所知。
- en: Pipeline
  id: totrans-25
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 流水线
- en: A natural language processing system is called a "pipeline" because it natural
    language must be processed in several stages. Natural language text flows in one
    end and text or data flows out of the other end, depending on what sections of
    "pipe" (Python code) you include in your pipeline. It’s like a conga line of Python
    snakes passing the data along from one to the next.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理系统被称为“管道”，因为自然语言必须经过几个阶段的处理。自然语言文本从一端流入，根据你在管道中包含的“管”（Python代码），文本或数据从另一端流出。就像一队Python蛇依次将数据从一端传递到另一端。
- en: You will soon have the power to write software that does interesting, human-like
    things with text. This book will teach you how to teach machines to carry on a
    conversation. It may seem a bit like magic, as new technology often does, at first.
    But you will pull back the curtain and explore the technology behind these magic
    shows. You will soon discover all the props and tools you need to do the magic
    tricks yourself.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 您很快就会拥有编写可以进行有趣的类人操作的软件的能力。本书将教你如何教机器进行对话。一开始，新技术通常会给人一种魔力的感觉。但你会拉开帷幕，探索这些魔术背后的技术。您很快将发现您需要执行这些魔术的所有道具和工具。
- en: 1.1.1 Natural Language Understanding (NLU)
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1.1 自然语言理解（NLU）
- en: A really important part of NLP is the automatic processing of text to extract
    a numerical representation of the *meaning* of that text. This is the *natural
    language understanding* (NLU) part of NLP. The numerical representation of the
    meaning of natural language usually takes the form of a vector called an embedding.
    Machines can use embeddings to do all sorts of useful things. Embeddings are used
    by search engines to understand what your search query means and then find you
    web pages that contain information about that topic. And the embedding vectors
    for emails in your inbox are used by your email service to classify those emails
    as Important or not.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: NLP的一个非常重要的部分是自动处理文本，以提取文本*含义*的数字表示。这是NLP的*自然语言理解*（NLU）部分。自然语言的含义的数字表示通常采用被称为嵌入的向量形式。机器可以使用嵌入来做各种有用的事情。搜索引擎使用嵌入来理解你的搜索查询的含义，然后找到包含有关该主题信息的网页。在你的收件箱中，电子邮件的嵌入向量被你的电子邮件服务用来对这些电子邮件进行分类，判断是否重要。
- en: Figure 1.1 Natural Language Understanding (NLU)
  id: totrans-30
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.1 自然语言理解（NLU）
- en: '![text NLU vector graphviz](images/text-NLU-vector-graphviz.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![文本NLU向量图形式](images/text-NLU-vector-graphviz.png)'
- en: 'Machines can accomplish many common NLU tasks with high accuracy:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 机器可以以高准确度完成许多常见的NLU任务：
- en: semantic search
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语义搜索
- en: text alignment (for translation or plagiarism detection)
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本对齐（用于翻译或抄袭检测）
- en: paraphrase recognition
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 释义识别
- en: intent classification
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 意图分类
- en: authorship attribution
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作者归属
- en: 'And recent advances in deep learning have made it possible to solve many NLU
    tasks that were impossible only ten years ago:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，深度学习的最新进展使得解决许多只有十年前不可能的NLU任务成为可能：
- en: analogy problem solving
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类比问题解决
- en: reading comprehension
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阅读理解
- en: extractive summarization
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 摘要提取
- en: medical diagnosis based on symptom descriptions
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于症状描述的医学诊断
- en: 'However, there remain many NLU tasks where humans significantly outperform
    machines. Some problems require the machine to have common-sense knowledge, learn
    the logical relationships between those common-sense facts, and use all of this
    on the context surrounding a particular piece of text. This makes these problems
    much more difficult for machines:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，仍然存在许多NLU任务，人类在其中显著优于机器。有些问题需要机器具有常识知识，学习常识事实之间的逻辑关系，并将所有这些用于围绕特定文本片段的上下文。这使得这些问题对机器来说更加困难：
- en: euphemism & pun recognition
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 委婉语与双关意识识别
- en: humor & sarcasm recognition
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 幽默与挖苦识别
- en: hate-speech & troll detection
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仇恨言论与喷子检测
- en: logical entailment and fallacy recognition
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑蕴涵和谬误识别
- en: database schema discovery
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据库架构发现
- en: knowledge extraction
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 知识提取
- en: You’ll learn the current state-of-the-art approaches to NLU and what is possible
    for these difficult problems. And your *behind-the-scenes* understanding of NLU
    will help you increase the effectiveness of your NLU pipelines for your particular
    applications, even on these hard problems.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 你将学习NLU的当前最先进的方法，以及这些困难问题的可能性。你对NLU的*幕后*理解将帮助您增加您特定应用程序的NLU管道的有效性，即使在这些困难问题上。
- en: 1.1.2 Natural Language Generation (NLG)
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1.2 自然语言生成（NLG）
- en: You may not be aware that machines can also compose text that sounds human-like.
    Machines can create human-readable text based on a numerical representation of
    the meaning and sentiment you would like to convey. This is the *natural language
    generation* (NLG) side of NLP.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能没有意识到机器也能够创作听起来像人类的文本。机器可以根据您希望传达的含义和情感的数值表示创建可读性强的文本。这是自然语言生成（NLG）在自然语言处理中的一部分。
- en: Figure 1.2 Natural Language Generation (NLG)
  id: totrans-53
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 1.2 自然语言生成（NLG）
- en: '![vector NLG text graphviz](images/vector-NLG-text-graphviz.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![矢量 NLG 文本图形](images/vector-NLG-text-graphviz.png)'
- en: You will soon master many common NLG tasks that build on your NLU skills. The
    following tasks mainly rely on your ability to *encode* natural language into
    meaningful embedding vectors with NLU.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 你很快就会掌握许多常见的建立在自然语言理解技能基础上的自然语言生成任务。以下任务主要依赖于您使用 NLU 将自然语言编码成有意义的嵌入向量的能力。
- en: synonym substitution
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同义词替换
- en: frequently-asked question answering (information retrieval)
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常见问题解答（信息检索）
- en: extractive generation of question answers (reading comprehension tests)
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提取式生成问题答案（阅读理解测试）
- en: spelling and grammar correction
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拼写和语法纠正
- en: casual conversation
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随意对话
- en: Once you understand how to accomplish these foundational tasks that help you
    hone your NLU skill, more advanced NLG tasks will be within your reach.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你了解如何完成这些帮助您磨练自然语言理解技能的基础任务，更高级的自然语言生成任务就会在您的掌握之中。
- en: abstractive summarization and simplification
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 抽象总结和简化
- en: machine translation with neural networks
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用神经网络进行机器翻译
- en: sentence paraphrasing
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子改写
- en: therapeutic conversational AI
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 治疗性对话 AI
- en: factual question generation
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成事实性问题
- en: discussion facilitation and moderation
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论引导和管理
- en: argumentative essay writing
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 论述性文章写作
- en: Once you understand how to summarize, paraphrase and translate text that gives
    you the ability to "translate" a text message into an appropriate response. You
    can even suggest new text for your user to include in their own writing. And you
    will discover approaches that help you summarize and generate longer and longer,
    and more complicated text.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你了解了如何总结、改写和翻译文本，你就能够将一条文本消息“翻译”成一个合适的回应。你甚至可以建议用户在他们自己的写作中包含新的文本。并且你会发现一些方法帮助你总结和生成越来越长、越来越复杂的文本。
- en: build a bot that can participate in debate on social media
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个能够参与社交媒体辩论的机器人
- en: compose poetry and song lyrics that don’t sound robotic
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作诗和写歌词，听起来不像机器人
- en: compose jokes and sarcastic comments
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创作笑话和讽刺性评论
- en: generate text that fools (hacks) other people’s NLU pipelines into doing what
    you want
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成可以欺骗（黑客）他人 NLU 流水线以执行您想要的操作的文本
- en: measure the robustness of NLP pipelines
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测量自然语言处理流水线的健壮性
- en: automatically summarize long technical documents
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动总结长篇技术文档
- en: compose programming language expressions from natural language descriptions
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从自然语言描述中构建编程语言表达式
- en: This last development in NLG is particularly powerful. Machines can now write
    correct code that comes close to matching your intent based only on a natural
    language description. Machines aren’t programming themselves yet, but they may
    soon, according to the latest (September 2023) consensus on Metaculus. The community
    predicts that by September, 2026, we will have "AIs program programs that can
    program AIs."^([[14](#_footnotedef_14 "View footnote.")])
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: NLG 中的最新发展尤其强大。现在，机器可以仅基于自然语言描述写出接近您意图的正确代码。机器还没有自己编程，但根据 Metaculus 最新（2023
    年 9 月）的共识，很快可能会有这种情况。社区预测到 2026 年 9 月，我们将会有“能够编程 AI 的 AI 程序”。^([[14](#_footnotedef_14
    "查看脚注。")])
- en: The combination of NLU and NLG will give you the tools to create machines that
    interact with humans in surprising ways.^([[15](#_footnotedef_15 "View footnote.")])
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言理解和自然语言生成的结合将为您提供以出人意料的方式与人类交互的工具。^([[15](#_footnotedef_15 "查看脚注。")])
- en: 1.1.3 Plumbing it all together for positive-impact AI
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1.3 将所有内容连接起来以实现积极影响的 AI
- en: Once you understand how NLG and NLU work, you will be able to assemble them
    into your own NLP pipelines, like a plumber. Businesses are already using pipelines
    like these to extract value from their users.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你了解了 NLG 和 NLU 的工作原理，你就能将它们组装成你自己的自然语言处理流水线，就像一个管道工。企业已经在使用这样的流水线从他们的用户中提取价值。
- en: You too can use these pipelines to further *your* own objectives in life, business,
    and social impact. This technology explosion is a rocket that you can ride and
    maybe steer a little bit. You can use it in your life to handle your inbox and
    journals while protecting your privacy and maximizing your mental well-being.
    Or you can advance your career by showing your peers how machines that understand
    and generate words can improve the efficiency and quality of almost any information-age
    task. And as an engineer who thinks about the impact of your work on society,
    you can help nonprofits build NLU and NLG pipelines that lift up the needy. As
    an entrepreneur, you can help create a regenerative prosocial business that spawns
    whole new industries and communities that thrive together.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以利用这些管道进一步实现你个人的、商业的和社交的目标。这股技术爆发是一架火箭，你可以骑上它，甚至可以在其中略为操纵。你可以利用它来管理收件箱和日记，保护你的隐私和最大化你的精神健康。或者你可以通过向同行展示理解和生成文字的机器可以提高几乎任何信息时代任务的效率和质量来发展你的职业生涯。作为一个思考你的工作对社会影响的工程师，你可以帮助非营利组织建立起提高弱势人群自然语言理解和生成能力的管道。作为一位企业家，你可以帮助创建一个再生的互利业务，孕育出全新的产业和共同繁荣的社区。
- en: Understanding how NLP works will open your eyes and empower you. You will soon
    see all the ways machines are being used to mine your words for profit, often
    at your expense. And you will see how machines are training you to become more
    easily manipulated. This will help you insulate yourself, and perhaps even fight
    back. You will soon learn how to survive in a world overrun with algorithms that
    manipulate you. You will harness the power of NLP to protect your well-being and
    contribute to the health of society as a whole.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 理解自然语言处理的工作原理将打开你的眼界和增强你的能力。你很快会看到机器被用于从你的言辞中挖掘利润，而你往往处于失利之中。你会看到机器如何训练你变得更容易受到操纵。这将帮助你隔离自己并可能进行反击。你很快会学会如何在充斥着操纵你的算法的世界中生存下来。你将利用自然语言处理的力量来保护你的健康，为整个社会做出贡献。
- en: Machines that can understand and generate natural language harness the power
    of words. Because machines can now understand and generate text that seems human,
    they can act on your behalf in the world. You’ll be able to create bots that will
    automatically follow your wishes and accomplish the goals you program them to
    achieve. But, beware Aladdin’s Three Wishes trap. Your bots may create a tsunami
    of blowback for your business or your personal life. Be careful about the goals
    you give your bots.^([[16](#_footnotedef_16 "View footnote.")]) This is called
    the "AI control problem" or the challenge of "AI safety."^([[17](#_footnotedef_17
    "View footnote.")]) Like the age-old three-wishes problem, you may find yourself
    trying to undo all the damage caused by your earlier wishes and bots.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 能够理解和生成自然语言的机器利用了文字的力量。因为机器现在能够理解和生成看起来与人类相似的文本，它们可以代表你行动。你可以创建机器人，自动按照你设定的目标完成任务。但是，要小心阿拉丁的三个愿望陷阱。你的机器人可能给你的商业或个人生活带来一连串的问题。因此，请谨慎确保你设置的目标吻合你的初衷^([[16](#_footnotedef_16
    "View footnote.")])。这被称为“AI 控制问题”或“AI 安全”挑战^([[17](#_footnotedef_17 "View footnote.")])。就像古老的三个愿望问题一样，你可能发现自己试图撤销先前的愿望和机器人所造成的所有损害。
- en: The control problem and AI safety are not the only challenges you will face
    on your quest for positive-impact NLP. The danger of superintelligent AI that
    can manipulate us into giving it ever greater power and control may be decades
    away, but the danger of dumb AI that deceives and manipulates us has been around
    for years. The search and recommendation engine NLP that determines which posts
    you are allowed to see is not doing what you want, it is doing what investors
    want, stealing your attention, time and money.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 控制问题和 AI 安全不是在追求积极影响的自然语言处理过程中你将面临的唯一挑战。超级智能 AI 可能会把我们操纵成越来越强大的力量和控制，但这种威胁可能是几十年之后的事情。然而，已经有多年了，即愚蠢的人工智能会欺骗和操纵我们。搜索和推荐引擎
    NLP 决定哪些帖子显示在你的屏幕上，它不是在满足你的需求，而是在满足投资者的需求，窃取你的注意力、时间和金钱。
- en: For example, if you use the search feature of meetup.com to try to find when
    the next San Diego Python User Group meetup is happening, you will find that they
    give you everything except what you are looking for. It doesn’t matter if you
    have previously signed up for and attended these meetups for years, no matter
    how much information you give them their NLP will always choose money-making links
    for them over useful links for you. Try searching for "DefCon 31 Cory Doctorow"
    on YouTube. Instead of his famous rant against platform rent-seeking, you will
    only see ads and videos that the platform’s owners think will keep you enthralled
    in ads and prevent you from waking up from this trance. Researchers call this
    the "AI ethics" challenge, and the more direct ones call it what it is, the AI
    enshittification problem.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你使用 meetup.com 的搜索功能来寻找下一次圣地亚哥 Python 用户组的聚会时间，你会发现他们提供的一切都不是你在寻找的东西。无论你之前是否注册并参加了多年的聚会，无论你给出多少信息，他们的自然语言处理始终会选择为他们带来利润的链接，而不是对你有用的链接。在
    YouTube 上搜索"DefCon 31 Cory Doctorow"试试看。你将看到的不是他那个著名的反对平台寻租的演讲，而只会看到广告和平台所有者认为能让你着迷于广告并防止你从这个幻觉中醒来的视频。研究人员称之为"AI伦理"挑战，而更直接的人称之为AI的恶化问题。
- en: 1.2 The magic
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 这魔术
- en: What is so magical about a machine that can read and write in a natural language?
    Machines have been processing languages since computers were invented. But those
    were computer languages, such as Ada, Bash, and C, designed for computers to be
    able to understand. Programming languages avoid ambiguity so that computers can
    always do exactly what you *tell* them to do, even if that is not always what
    you *want* them to do.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一台能够读写自然语言的机器有什么神奇之处？自从计算机问世以来，机器一直在处理语言。但那些是计算机语言，例如Ada、Bash和C，它们是为计算机理解而设计的。编程语言避免了歧义，以便计算机始终能够按照你的指令做确切的事情，即使那并不一定是你想要它们做的事情。
- en: Computer languages can only be interpreted (or compiled) in one correct way.
    With NLP you can talk to machines in your own language rather than having to learn
    computerese. When software can process languages not designed for machines to
    understand, it is magic — something we thought only humans could do.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机语言只能以一种正确的方式进行解释（或编译）。使用自然语言处理，你可以用自己的语言与机器对话，而无需学习计算机术语。当软件能够处理不是为机器设计的语言时，它是一种魔术——我们过去认为只有人类才能做到的。
- en: Moreover, machines can access a massive amount of natural language text, such
    as Wikipedia, to learn about the world and human thought. Google’s index of natural
    language documents is well over 100 million gigabytes,^([[18](#_footnotedef_18
    "View footnote.")]) and that is just the index. And that index is incomplete.
    The size of the actual natural language content currently online probably exceeds
    100 billion gigabytes.^([[19](#_footnotedef_19 "View footnote.")]) This massive
    amount of natural language text makes NLP a useful tool.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，机器可以访问大量的自然语言文本，例如维基百科，从而了解世界和人类思维。谷歌的自然语言文档索引超过了100百万千兆字节，[[18](#_footnotedef_18
    "查看脚注。")])而这仅仅是索引而已。而且这个索引是不完整的。当前在线的实际自然语言内容的大小可能超过1000百亿千兆字节。[[19](#_footnotedef_19
    "查看脚注。")])这大量的自然语言文本使得自然语言处理成为一个有用的工具。
- en: Note
  id: totrans-90
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Today, Wikipedia lists approximately 700 programming languages. Ethnologue_
    ^([[20](#_footnotedef_20 "View footnote.")]) identifies more than 7,000 natural
    languages. And that doesn’t include many other natural language sequences that
    can be processed using the techniques you’ll learn in this book. The sounds, gestures,
    and body language of animals, as well as the DNA and RNA sequences within their
    cells, can all be processed with NLP.^([[21](#_footnotedef_21 "View footnote.")])^([[22](#_footnotedef_22
    "View footnote.")])
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，维基百科列出了大约700种编程语言。Ethnologue_ ^([[20](#_footnotedef_20 "查看脚注。")])标识出了7000多种自然语言。而这还不包括许多其他可以通过本书学到的技术进行处理的自然语言序列。动物的声音、手势和身体语言，以及它们细胞中的DNA和RNA序列，都可以通过自然语言处理进行处理。^([[21](#_footnotedef_21
    "查看脚注。")])^([[22](#_footnotedef_22 "查看脚注。")])
- en: Machines with the capability to process something natural is not natural. It
    is kind of like building a building that can do something useful with architectural
    designs. When software can process languages not designed for machines to understand,
    it seems magical — something we thought was a uniquely human capability.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 具备处理自然事物能力的机器并不是自然的。这有点像建造一座可以利用建筑设计完成某些有用事物的建筑物。当软件能够处理不是为机器设计的语言时，它看起来是一种魔术——我们过去认为这是人类独有的能力。
- en: For now, you only need to think about one natural language —  English. You’ll
    ease into more difficult languages like Mandarin Chinese later in the book. But
    you can use the techniques you learn in this book to build software that can process
    any language, even a language you do not understand or has yet to be deciphered
    by archaeologists and linguists. We are going to show you how to write software
    to process and generate that language using only one programming language, Python.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，您只需要考虑一种自然语言-英语。您以后可以逐渐学习诸如中文之类的更难的语言。但是，您可以使用本书中学到的技术来构建能够处理任何语言的软件，即使是您不了解的语言，或是尚未被考古学家和语言学家破解的语言。我们将向您展示如何使用一种编程语言-
    Python 来编写处理和生成该语言的软件。
- en: Python was designed from the ground up to be a readable language. It also exposes
    a lot of its own language processing "guts." Both of these characteristics make
    it a natural choice for learning natural language processing. It is a great language
    for building maintainable production pipelines for NLP algorithms in an enterprise
    environment, with many contributors to a single codebase. We even use Python in
    lieu of the "universal language" of mathematics and mathematical symbols, wherever
    possible. After all, Python is an unambiguous way to express mathematical algorithms,
    ^([[23](#_footnotedef_23 "View footnote.")]) and it is designed to be as readable
    as possible by programmers like you.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Python从头至尾都是为了编写可读性更高的语言而设计的。它还暴露了许多自己的语言处理“内脏”。这两个特点使其成为学习自然语言处理的自然选择。在企业环境中，Python是构建可维护的NLP算法生产线的绝佳语言，一个代码库会有许多贡献者。我们甚至使用Python代替了“通用语言”的数学和数学符号，无论何时都是如此。毕竟，Python是一种明确表达数学算法的方式，^([[23](#_footnotedef_23
    "查看脚注")])并且它被设计成尽可能易读，适合像你一样的程序员。
- en: 1.2.1 Language and thought
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.1 语言与思维
- en: Linguists and philosophers such as Sapir and Whorf postulated that our vocabulary
    affects the thoughts we think. For example, Australian Aborigines have words to
    describe the position of objects on their body according to the cardinal points
    of the compass. They don’t talk about the boomerang in their right hand, they
    talk about the boomerang on the north side of their body. This makes them adept
    at communicating and orienteering during hunting expeditions. Their brains are
    constantly updating their understanding of their orientation in the world.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 语言学家和哲学家（如莫·沃尔夫和爱德华·萨皮尔）认为，词汇表影响我们的思维方式。例如，澳大利亚土著人有词汇来描述物体在其身体上的位置，基于罗盘的基本方位。他们不用右手中的回力镖，而用身体北侧的回力镖进行交流。这使他们在狩猎远征中善于沟通和定向。他们的大脑不断更新他们对世界定位的理解。
- en: 'Stephen Pinker flips that notion around and sees language as a window into
    our brains and how we think: "Language is a collective human creation, reflecting
    human nature, how we conceptualize reality, how we relate to one another."^([[24](#_footnotedef_24
    "View footnote.")]) Whether you think of words as affecting your thoughts or as
    helping you see and understand your thoughts, either way, they are packets of
    thought. You will soon learn the power of NLP to manipulate those packets of thought
    and amp up your understanding of words, …​ and maybe thought itself. It’s no wonder
    many businesses refer to NLP and chatbots as AI - Artificial Intelligence.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 史蒂芬·平克把这个观念扭转过来，将语言视为我们的大脑和思维方式的窗口：“语言是集体人类创造的，反映了人类的本性，我们对现实的概念化方式，以及我们如何与他人相关。”^([[24](#_footnotedef_24
    "查看脚注")])无论您认为词语对思维的影响，还是认为它们有助于您看到和理解自己的思维，无论哪种方式，它们都是思维的载体。您很快将了解到NLP的强大之处，可以操控这些思维的载体，加深对词语...
    的理解，也许还有对思维本身的理解。难怪许多企业将NLP和聊天机器人称为AI - 人工智能。
- en: What about math? We think with precise mathematical symbols and programming
    languages as well as with fuzzier natural language words and symbols. And we can
    use fuzzy words to express logical thoughts like mathematics concepts, theorems,
    and proofs. But words aren’t the only way we think. Jordan Elenberg, a geometer
    at Harvard, writes in his new book *Shape* about how he first "discovered" the
    commutative property of algebra while staring at a stereo speaker with a grid
    of dots, 6x8\. He’d memorized the multiplication table, the symbols for numbers.
    And he knew that you could reverse the order of symbols on either side of a multiplication
    symbol. But he didn’t really *know* it until he realized that he could visualize
    the 48 dots as 6 columns of 8 dots, or 8 rows of 6 dots. And it was the same dots!
    So it had to be the same number. It hit him at a deeper level, even deeper than
    the symbol manipulation rules that he learned in algebra class.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: So you use words to communicate thoughts with others and with yourself. When
    ephemeral thoughts can be gathered up into words or symbols, they become compressed
    packets of thought that are easier to remember and to work with in your brain.
    You may not realize it, but as you are composing sentences you are actually rethinking
    and manipulating and repackaging these thoughts. What you want to say, and the
    idea you want to share is crafted while you are speaking or writing. This act
    of manipulating packets of thought in your mind is called "symbol manipulation"
    by AI researchers and neuroscientists. In fact, in the age of GOFAI (Good Old-Fashioned
    AI) researchers assumed that AI would need to learn to manipulate natural language
    symbols and logical statements the same way it compiles programming languages.
    In this book, you’re going to learn how to teach a machine to do symbol manipulation
    on natural language in Chapter 11.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: But that’s not the most impressive power of NLP. Think back to a time when you
    had a difficult email to send to someone close. Perhaps you needed to apologize
    to a boss or teacher, or maybe your partner or a close friend. Before you started
    typing, you probably started thinking about the words you would use, the reasons
    or excuses for why you did what you did. And then you imagined how your boss or
    teacher would perceive those words. You probably reviewed in your mind what you
    would say many many times before you finally started typing. You manipulated packets
    of thought as words in your mind. And when you did start typing, you probably
    wrote and rewrote twice as many words as you actually sent. You chose your words
    carefully, discarding some words or ideas and focusing on others.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: The act of revision and editing is a thinking process. It helps you gather your
    thoughts and revise them. And in the end, whatever comes out of your mind is not
    at all like the first thoughts that came to you. The act of writing improves how
    you think, and it will improve how machines think as they get better and better
    at reading and writing.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: So reading and writing is thinking. And words are packets of thought that you
    can store and manipulate to improve those thoughts. We use words to put thoughts
    into clumps or compartments that we can play with in our minds. We break complicated
    thoughts into several sentences. And we reorder those thoughts so they make more
    sense to our reader or even our future self. Every sentence in this 2nd edition
    of the book has been edited several times - sometimes with the help of generous
    readers of the LiveBook. ^([[25](#_footnotedef_25 "View footnote.")]) I’ve deleted,
    rewritten and reordered these paragraphs several times just now, with the help
    of suggestions and ideas from friends and readers like you.^([[26](#_footnotedef_26
    "View footnote.")])
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: But words and writing aren’t the *only* way to think logically and deeply. Drawing,
    diagramming, and even dancing and acting out are all expressions of thought. And
    we visually imagine these drawings in our minds — sketching ideas and concepts
    and thoughts in our head. And sometimes you just physically move things around
    or act things out in the real world. But the act of composing words into sentences
    and sentences into paragraphs is something that we do almost constantly.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Reading and writing is also a special kind of thought. It seems to compress
    our thoughts and make them easier to remember and manage within our heads. Once
    we know the perfect word for a concept, we can file it away in our minds. We don’t
    have to keep refreshing it to understand it. We know that once we think of the
    word again, the concept will come flooding back and we can use it again.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: This is all thinking or what is sometimes called *cognition*. So by teaching
    machines to understand and compose text, you are in some small way, teaching them
    to think. This is why people think of NLP as artificial intelligence (AI). And
    conversational AI is one of the most widely recognized and useful forms of AI.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.2 Machines that converse
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Though you spend a lot of time working with words as packets of thought internally
    within your head, the real fun is when you use those words to interact with others.
    The act of conversation brings two (or more!) people into your thinking. This
    can create a powerful positive feedback loop that reinforces good ideas and weeds
    out weak ones.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Words are critical to this process. They are our shared thought vocabulary.
    When you want to trigger a thought in another person’s brain, all you need to
    do is to say the right words so that they understand some of the thoughts in your
    mind. For example, when you are feeling great pain, frustration or shock, you
    can use a curse word. And you can almost be guaranteed to cause that shock and
    discomfort to be conveyed to your listener or reader. That is the sole purpose
    of curse words — to shock (and awe?) your listener.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-109
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: There is *another_NLP* that takes this idea to the extreme. Neuro-linguistic
    programming (the *other_NLP*) is a pseudoscientific psychotherapy approach that
    claims to change your behavior through the use of words. Because there is money
    to be made in claiming to help people achieve their life goals, this pseudoscience
    has taken on a cult status for the practitioners who teach it (preach it?).^([[27](#_footnotedef_27
    "View footnote.")])
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: As with astrology, fortune telling, hypnotherapy, conspiracy theories, religions
    and cults, there is usually a small hint of truth somewhere within it. Words do
    indeed affect our thoughts. And thoughts do affect our behavior.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Though we cannot "program" another human with our words, we can use them to
    communicate extremely complex ideas. When you engage in conversation you are acting
    as a neuron in the collective consciousness, the hive mind. Unfortunately, when
    profit motives and unfettered competition is the rule of the day, the hornet nest
    of social media is the result.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Natural language cannot be directly translated into a precise set of mathematical
    operations. But natural language does contain information and instructions that
    can be extracted. Those pieces of information and instruction can be stored, indexed,
    searched, or immediately acted upon. One of those actions could be to generate
    a sequence of words in response to a statement. This is the function of the "dialog
    engine" or chatbot that you will build.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: This book focuses entirely on English text documents and messages, not spoken
    statements. Chapter 7 does give you a brief foray into processing audio files,
    Morse code. But most of NLPiA is focused on the words that have been put to paper…​
    or at least put to transistors in a computer. There are whole books on speech
    recognition and speech-to-text (STT) systems and text-to-speech (TTS) systems.
    There are ready-made open-source projects for STT and TTS. If your application
    is a mobile application, modern smartphone SDKs provide you with speech recognition
    and speech generation APIs. If you want your virtual assistant to live in the
    cloud, there are Python packages to accomplish SST and TTS on any Linux server
    with access to your audio stream.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: In this book you will focus on what happens between the *ears* of the machine.
    This can help you build a smarter voice assistant when you add your *brains* to
    open source projects such as Home Assistant,^([[28](#_footnotedef_28 "View footnote.")])
    Mycroft AI,^([[29](#_footnotedef_29 "View footnote.")]) or OVAL Genie,^([[30](#_footnotedef_30
    "View footnote.")]). And you’ll understand all the helpful NLP that the big boys
    could be giving you within their voice assistants …​ assuming commercial voice
    assistants wanted to help you with more than just lightening your wallet.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Speech recognition systems
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: If you want to build a customized speech recognition or generation system, that
    undertaking is a whole book in itself; we leave that as an "exercise for the reader."
    It requires a lot of high-quality labeled data, voice recordings annotated with
    their phonetic spellings, and natural language transcriptions aligned with the
    audio files. Some of the algorithms you learn in this book might help, but most
    of the algorithms are quite different.^([[31](#_footnotedef_31 "View footnote.")])
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.3 The math
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Processing natural language to extract useful information can be difficult.
    It requires tedious statistical bookkeeping, but that is what machines are for.
    Like many other technical problems, solving it is a lot easier once you know the
    answer. Machines still cannot perform most practical NLP tasks, such as conversation
    and reading comprehension, as accurately and reliably as humans. So you might
    be able to tweak the algorithms you learn in this book to do some NLP tasks a
    bit better.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: The techniques you will learn, however, are powerful enough to create machines
    that can surpass humans in both accuracy and speed for some surprisingly subtle
    tasks. For example, you might not have guessed that recognizing sarcasm in an
    isolated Twitter message can be done more accurately by a machine than by a human.
    Well-trained human judges could not match the performance (68% accuracy) of a
    simple sarcasm detection NLP algorithm.^([[32](#_footnotedef_32 "View footnote.")])
    Simple BOW (bag-of-words) models achieve 63% accuracy and state of the art transformer
    models achieve 81% accuracy. ^([[33](#_footnotedef_33 "View footnote.")]) Do not
    worry, humans are still better at recognizing humor and sarcasm within an ongoing
    dialog because we are able to maintain information about the context of a statement.
    However, machines are getting better and better at maintaining context. This book
    helps you incorporate context (metadata) into your NLP pipeline if you want to
    try your hand at advancing the state of the art.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Once you extract structured numerical data, or vectors, from natural language,
    you can take advantage of all the tools of mathematics and machine learning. We
    use the same linear algebra tricks as the projection of 3D objects onto a 2D computer
    screen, something that computers and drafters were doing long before natural language
    processing came into its own. These breakthrough ideas opened up a world of "semantic"
    analysis, allowing computers to interpret and store the "meaning" of statements
    rather than just word or character counts. Semantic analysis, along with statistics,
    can help resolve the ambiguity of natural language — the fact that words or phrases
    often have multiple meanings or interpretations.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您从自然语言中提取出结构化的数值数据或向量，您就可以利用数学和机器学习的所有工具。我们使用与将三维对象投影到二维计算机屏幕相同的线性代数技巧，这是计算机和制图员在自然语言处理成为自己的一部分之前就在做的事情。这些突破性的思想开辟了一个“语义”分析的世界，使计算机能够解释和存储陈述的“意义”，而不仅仅是单词或字符的计数。语义分析以及统计学可以帮助解决自然语言的歧义性，即单词或短语经常具有多种含义或解释。
- en: So extracting information is not at all like building a programming language
    compiler (fortunately for you). The most promising techniques bypass the rigid
    rules of regular grammars (patterns) or formal languages. You can rely on statistical
    relationships between words instead of a deep system of logical rules.^([[34](#_footnotedef_34
    "View footnote.")]) Imagine if you had to define English grammar and spelling
    rules in a nested tree of if…​then statements. Could you ever write enough rules
    to deal with every possible way that words, letters, and punctuation can be combined
    to make a statement? Would you even begin to capture the semantics, the meaning
    of English statements? Even if it were useful for some kinds of statements, imagine
    how limited and brittle this software would be. Unanticipated spelling or punctuation
    would break or befuddle your algorithm.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，提取信息与构建编程语言编译器完全不同（对你来说幸运）。最有前途的技术绕过了常规语法（模式）或形式语言的严格规则。您可以依赖单词之间的统计关系，而不是深层的逻辑规则系统。想象一下，如果您必须在
    if…​then 语句的嵌套树中定义英语语法和拼写规则。您能写出足够处理单词、字母和标点组合成陈述的每种可能方式的规则吗？您是否会开始捕捉英语陈述的语义，意思？即使对于某些类型的陈述而言，这可能是有用的，想象一下这种软件会多么有限和脆弱。未预料到的拼写或标点将打破或使您的算法困惑。
- en: Natural languages have an additional "decoding" challenge that is even harder
    to solve. Speakers and writers of natural languages assume that a human is the
    one doing the processing (listening or reading), not a machine. So when I say
    "good morning," I assume that you have some knowledge about what makes up a morning,
    including that the morning comes before noon, afternoon, and evening, but it also
    comes after midnight. You need to know that morning can represent times of day
    as well as a general period of time. The interpreter is assumed to know that "good
    morning" is a common greeting, and that it does not contain much information at
    all about the morning. Rather, it reflects the state of mind of the speaker and
    her readiness to speak with others.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言有一个更难解决的额外的“解码”挑战。自然语言的说话者和写作者假设是一个人在进行处理（听或读），而不是一台机器。因此，当我说“早上好”时，我假设你对组成早上的东西有一些了解，包括早上在中午、下午和晚上之前，但它也在午夜之后。您需要知道早晨既可以代表一天中的时间，也可以代表一段时间。解释器被认为知道“早上好”是一个常见的问候语，而且它几乎不包含关于早上的任何信息。相反，它反映了说话者的心态以及她与他人交谈的准备情况。
- en: This theory of mind about the human processor of language turns out to be a
    powerful assumption. It allows us to say a lot with few words if we assume that
    the "processor" has access to a lifetime of common sense knowledge about the world.
    This degree of compression is still out of reach for machines. There is no clear
    "theory of mind" you can point to in an NLP pipeline. However, we show you techniques
    in later chapters to help machines build ontologies, or knowledge bases, of common
    sense knowledge to help interpret statements that rely on this knowledge.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这种关于语言人类处理器的心理理论被证明是一个强大的假设。如果我们假设“处理器”可以访问关于世界的常识知识的一生，那么我们可以用很少的词语说很多话。这种压缩程度对于机器来说仍然是难以实现的。在
    NLP 流水线中没有明确的“心理理论”你可以指出。然而，我们将在后面的章节中向您展示一些技术，以帮助机器构建常识知识的本体论或知识库，以帮助解释依赖于这些知识的陈述。
- en: 1.3 Applications
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.3 应用
- en: Natural language processing is everywhere. It is so ubiquitous that you’d have
    a hard time getting through the day without interacting with several NLP algorithms
    every hour. Some of the examples here may surprise you.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.3 Graph of NLP applications
  id: totrans-127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![nlp applications](images/nlp-applications.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
- en: At the core of this network diagram are the NLU and NLG **sides** of NLP. Branching
    out from the NLU hub node are foundational applications like sentiment analysis
    and search. These eventually connect with foundational NLG tools such as spelling
    correctors and automatic code generators to create conversational AI and even
    pair programming assistants.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: A search engine can provide more meaningful results if it indexes web pages
    or document archives in a way that takes into account the meaning of natural language
    text. Autocomplete uses NLP to complete your thought and is common among search
    engines and mobile phone keyboards. Many word processors, browser plugins, and
    text editors have spelling correctors, grammar checkers, concordance composers,
    and most recently, style coaches. Some dialog engines (chatbots) use natural language
    search to find a response to their conversation partner’s message.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: NLP pipelines that generate text can be used not only to compose short replies
    in chatbots and virtual assistants but also to assemble much longer passages of
    text. The Associated Press uses NLP "robot journalists" to write entire financial
    news articles and sporting event reports.^([[35](#_footnotedef_35 "View footnote.")])
    Bots can compose weather forecasts that sound a lot like what your hometown weather
    person might say, perhaps because human meteorologists use word processors with
    NLP features to draft scripts.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: More and more businesses are using NLP to automate their business processes.
    This can improve team productivity and job satisfaction, as well as the quality
    of the product. For example, chatbots can automate the responses to many customer
    service requests.^([[36](#_footnotedef_36 "View footnote.")]) NLP spam filters
    in early email programs helped email overtake telephone and fax communication
    channels in the '90s. And some teams use NLP to automate and personalize e-mails
    between teammates or communicate with job applicants.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: NLP pipelines, like all algorithms, make mistakes and are almost always biased
    in many ways. So if you use NLP to automate communication with humans, be careful.
    At Tangible AI we use NLP for the critical job of helping us find developers to
    join our team, so we were extremely cautious. We used NLP to help us filter out
    job applications only when the candidate was nonresponsive or did not appear to
    understand several questions on the application. We had rigorous quality control
    on the NLP pipeline with periodic random sampling of the model predictions. We
    used simple models and sample-efficient NLP models ^([[37](#_footnotedef_37 "View
    footnote.")]) to focus human attention on those predictions where the machine
    learning was least confident — see the `predict_proba` method on SciKit Learn
    classifiers. As a result NLP for HR (human relations) actually cost us more time
    and attention and did not save us money. But it did help us cast a broader net
    when looking for candidates. We had hundreds of applications from around the globe
    for a junior developer role, including applicants located in Ukraine, Africa,
    Asia and South America. NLP helped us quickly evaluate English and technical skill
    before proceeding with interviews and paid take-home assignments.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: The spam filters have retained their edge in the cat-and-mouse game between
    spam filters and spam generators for email but may be losing in other environments
    like social networks. An estimated 20% of the tweets about the 2016 US presidential
    election were composed by chatbots.^([[38](#_footnotedef_38 "View footnote.")])
    These bots amplify their owners' and developers' viewpoints with the resources
    and motivation to influence popular opinion. And these "puppet masters" tend to
    be foreign governments or large corporations.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: NLP systems can generate more than just short social network posts. NLP can
    be used to compose lengthy movie and product reviews on online shop websites and
    elsewhere. Many reviews are the creation of autonomous NLP pipelines that have
    never set foot in a movie theater or purchased the product they are reviewing.
    And it’s not just movies, a large portion of all product reviews that bubble to
    the top in search engines and online retailers are fake. You can use NLP to help
    search engines and prosocial social media communities (Mastodon) ^([[39](#_footnotedef_39
    "View footnote.")]) ^([[40](#_footnotedef_40 "View footnote.")]) detect and remove
    misleading or fake posts and reviews.^([[41](#_footnotedef_41 "View footnote.")])
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: There are chatbots on Slack, IRC, and even customer service websites — places
    where chatbots have to deal with ambiguous commands or questions. And chatbots
    paired with voice recognition and generation systems can even handle lengthy conversations
    with an indefinite goal or "objective function" such as making a reservation at
    a local restaurant.^([[42](#_footnotedef_42 "View footnote.")]) NLP systems can
    answer phones for companies that want something better than a phone tree, but
    they do not want to pay humans to help their customers.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-137
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Consider the ethical implications whenever you, or your boss, decide to deceive
    your users. With its **Duplex** demonstration at Google IO, engineers and managers
    overlooked concerns about the ethics of teaching chatbots to deceive humans. In
    most "entertainment" social networks, bots are not required to reveal themselves.
    We unknowingly interact with these bots on Facebook, Reddit, Twitter and even
    dating apps. Now that bots and deep fakes can so convincingly deceive us, the
    AI control problem ^([[43](#_footnotedef_43 "View footnote.")]). Yuval Harari’s
    cautionary forecast of "Homo Deus"^([[44](#_footnotedef_44 "View footnote.")])
    may come sooner than we think.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: NLP systems exist that can act as email "receptionists" for businesses or executive
    assistants for managers. These assistants schedule meetings and record summary
    details in an electronic Rolodex, or CRM (customer relationship management system),
    interacting with others by email on their boss’s behalf. Companies are putting
    their brand and face in the hands of NLP systems, allowing bots to execute marketing
    and messaging campaigns. And some inexperienced daredevil NLP textbook authors
    are letting bots author several sentences in their book. More on that later.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: The most surprising and powerful application of NLP is in psychology. If you
    think that a chatbot could never replace your therapist, you may be surprised
    by recent advancements.^([[45](#_footnotedef_45 "View footnote.")]) Commercial
    virtual companions such as Xiaoice in China and Replika.AI in the US helped hundreds
    of millions of lonely people survive the emotional impact of social isolation
    during Covid-19 lockdowns in 2020 and 2021.^([[46](#_footnotedef_46 "View footnote.")])
    Fortunately, you don’t have to rely on engineers at large corporations to look
    out for your best interests. Many psychotherapy and cognitive assistant technology
    is completely free and open source.^([[47](#_footnotedef_47 "View footnote.")])
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.1 Processing programming languages with NLP
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Modern deep-learning NLP pipelines have proven so powerful and versatile that
    they can now accurately understand and generate programming languages. Rule-based
    compilers and generators for NLP were helpful for simple tasks like autocomplete
    and providing snippet suggestions. And users can often use information retrieval
    systems, or search engines, to find snippets of code to complete their software
    development project.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: And these tools just got a whole lot smarter. Previous code generation tools
    were **extractive**. Extractive text generation algorithms find the most relevant
    text in your history and just regurgitate it, verbatim as a suggestion to you.
    So if the term "prosocial artificial intelligence" appears a lot in the text an
    algorithm was trained on, an auto-complete will recommend the word "artificial
    intelligence" to follow prosocial rather than just "intelligence". You can see
    how this might start to influence what you type and how you think.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'And transformers have advanced NLP even further recently with massive deep
    learning networks that are more **abstractive**, generating new text you haven’t
    seen or typed before. For example, the 175 billion parameter version of GPT-3
    was trained on all of GitHub to create a model called Codex. Codex is part of
    the Copilot plugin for VSCode. It suggests entire function and class definitions
    and all you have to supply is a short comment and the first line of the function
    definition. Here is the example for the typescript prompt shown on the copilot
    home page: ^([[48](#_footnotedef_48 "View footnote.")])'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the demo animation, Copilot then generated the rest of the typescript required
    for a working function that estimated the sentiment of a body of text. Think about
    that for a second. Microsoft’s algorithm is writing code for you to analyze the
    sentiment of natural language text, such as the text you might be writing up in
    your emails or personal essay. And the examples shown on the Copilot home page
    all lean towards Microsoft products and services. This means you will end up with
    an NLP pipeline that has **Microsoft’s** perspective on what is positive and what
    is not. It values what **Microsoft** told it to value. Just as Google Search influenced
    the kind of code you wrote indirectly, now Microsoft algorithms are directly writing
    code for you.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Since you’re reading this book, you are probably planning to build some pretty
    cool NLP pipelines. You may even build a pipeline that helps you write blog posts
    and chatbots and core NLP algorithms. This can create a positive feedback loop
    that shifts the kinds of NLP pipelines and models that are built and deployed
    by engineers like you. So pay attention to the **meta** tools that you use to
    help you code and think. These have huge leverage on the direction of your code,
    and the direction of your life.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 1.4 Language through a computer’s "eyes"
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When you type "Good Morning Rosa", a computer sees only "01000111 01101111
    01101111 …​". How can you program a chatbot to respond to this binary stream intelligently?
    Could a nested tree of conditionals (`if`…​ `else`…​" statements) check each one
    of those bits and act on them individually? This would be equivalent to writing
    a special kind of program called a finite state machine (FSM). An FSM that outputs
    a sequence of new symbols as it runs, like the Python `str.translate` function,
    is called a finite state transducer (FST). You’ve probably already built a FSM
    without even knowing it. Have you ever written a regular expression? That’s the
    kind of FSM we use in the next section to show you one possible approach to NLP:
    the pattern-based approach.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: What if you decided to search a memory bank (database) for the exact same string
    of bits, characters, or words, and use one of the responses that other humans
    and authors have used for that statement in the past? But imagine if there was
    a typo or variation in the statement. Our bot would be sent off the rails. And
    bits aren’t continuous or forgiving — they either match or they do not. There
    is no obvious way to find a similarity between two streams of bits that takes
    into account what they signify. The bits for "good" will be just as similar to
    "bad!" as they are to "okay".
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: But let’s see how this approach would work before we show you a better way.
    Let’s build a small regular expression to recognize greetings like "Good morning
    Rosa" and respond appropriately — our first tiny chatbot!
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 1.4.1 The language of locks
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Surprisingly the humble combination lock is actually a simple language processing
    machine. So, if you are mechanically inclined, this section may be illuminating.
    But if you do not need mechanical analogies to help you understand algorithms
    and how regular expressions work, then you can skip this section.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: 'After finishing this section, you will never think of your combination bicycle
    lock the same way again. A combination lock certainly can’t read and understand
    the textbooks stored inside a school locker, but it can understand the language
    of locks. It can understand when you try to "tell" it a "password": a combination.
    A padlock combination is any sequence of symbols that matches the "grammar" (pattern)
    of lock language. Even more importantly, the padlock can tell if a lock "statement"
    matches a particularly meaningful statement, the one for which there is only one
    correct "response," to release the catch holding the U-shaped hasp so you can
    get into your locker.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: This lock language (regular expressions) is a particularly simple one. But it’s
    not so simple that we can’t use it in a chatbot. We can use it to recognize a
    key phrase or command to unlock a particular action or behavior.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: For example, we’d like our chatbot to recognize greetings such as "Hello Rosa,"
    and respond to them appropriately. This kind of language, like the language of
    locks, is a formal language because it has strict rules about how an acceptable
    statement must be composed and interpreted. If you’ve ever written a math equation
    or coded a programming language expression, you’ve written a formal language statement.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Formal languages are a subset of natural languages. Many natural language statements
    can be matched or generated using a formal language grammar, such as regular expressions
    or regular grammars. That’s the reason for this diversion into the mechanical,
    "click, whirr"^([[49](#_footnotedef_49 "View footnote.")]) language of locks.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 1.4.2 Regular expressions
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Regular expressions use a special class of formal language grammars called a
    regular grammar. Regular grammars have predictable, provable behavior, and yet
    are flexible enough to power some of the most sophisticated dialog engines and
    chatbots on the market. Amazon Alexa and Google Now are mostly pattern-based engines
    that rely on regular grammars. Deep, complex regular grammar rules can often be
    expressed in a single line of code called a regular expression. There are successful
    chatbot frameworks in Python, like `Will`, ^([[50](#_footnotedef_50 "View footnote.")])
    and `qary` ^([[51](#_footnotedef_51 "View footnote.")]) that rely exclusively
    on this kind of language processing to produce some effective chatbots.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-160
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Regular expressions implemented in Python and in Posix (Unix) applications such
    as `grep` are not true regular grammars. They have language and logic features
    such as look-ahead and look-back that make leaps of logic and recursion that aren’t
    allowed in a regular grammar. As a result, regular expressions aren’t provably
    halting; they can sometimes "crash" or run forever. ^([[52](#_footnotedef_52 "View
    footnote.")])
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: You may be saying to yourself, "I’ve heard of regular expressions. I use `grep`.
    But that’s only for search!" And you are right. **R**egular **E**xpressions are
    indeed used mostly for search, for sequence matching. But anything that can find
    matches within text is also great for carrying out a dialog. Some chatbots, use
    "search" to find sequences of characters within a user statement that they know
    how to respond to. These recognized sequences then trigger a scripted response
    appropriate to that particular regular expression match. And that same regular
    expression can also be used to extract a useful piece of information from a statement.
    A chatbot can add that bit of information to its knowledge base about the user
    or about the world the user is describing.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: A machine that processes this kind of language can be thought of as a formal
    mathematical object called a finite state machine or deterministic finite automaton
    (DFA). FSMs come up again and again in this book. So, you will eventually get
    a good feel for what they’re used for without digging into FSM theory and math.
    For those who can’t resist trying to understand a bit more about these computer
    science tools, figure 1.1 shows where FSMs fit into the nested world of automata
    (bots). And the side note that follows explains a bit more formal detail about
    formal languages.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.4 Kinds of automata
  id: totrans-164
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![kinds of automata](images/kinds-of-automata.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
- en: Formal mathematical explanation of formal languages
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Kyle Gorman describes programming languages this way:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Most (if not all) programming languages are drawn from the class of context-free
    languages.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Context free languages are parsed with context-free grammars, which provide
    efficient parsing.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The regular languages are also efficiently parsable and used extensively in
    computing for string matching.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: String matching applications rarely require the expressiveness of context-free.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are a number of formal language classes, a few of which are shown here
    (in decreasing complexity):^([[53](#_footnotedef_53 "View footnote.")])
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recursively enumerable
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Context-sensitive
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Context-free
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Regular
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Natural languages are:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Not regular ^([[54](#_footnotedef_54 "View footnote.")])
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Not context-free ^([[55](#_footnotedef_55 "View footnote.")])
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can’t be defined by any formal grammar ^([[56](#_footnotedef_56 "View footnote.")])
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1.5 A simple chatbot
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let us build a quick and dirty chatbot. It will not be very capable, and it
    will require a lot of thinking about the English language. You will also have
    to hardcode regular expressions to match the ways people may try to say something.
    But do not worry if you think you couldn’t have come up with this Python code
    yourself. You will not have to try to think of all the different ways people can
    say something, like we did in this example. You will not even have to write regular
    expressions (regexes) to build an awesome chatbot. We show you how to build a
    chatbot of your own in later chapters without hardcoding anything. A modern chatbot
    can learn from reading (processing) a bunch of English text. And we show you how
    to do that in later chapters.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: This pattern-matching chatbot is an example of a tightly controlled chatbot.
    Pattern-matching chatbots were common before modern machine learning chatbot techniques
    were developed. And a variation of the pattern-matching approach we show you here
    is used in chatbots like Amazon Alexa and other virtual assistants.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: For now let’s build an FSM, a regular expression, that can speak lock language
    (regular language). We could program it to understand lock language statements,
    such as "01-02-03." Even better, we’d like it to understand greetings, things
    like "open sesame" or "hello Rosa."
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: An important feature of a prosocial chatbot is to be able to respond to a greeting.
    In high school, teachers often chastised me for being impolite when I’d ignore
    greetings like this while rushing to class. We surely do not want that for our
    benevolent chatbot.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: For communication between two machines, you would define a handshake with something
    like an `ACK` (acknowledgement) signal to confirm receipt of each message. But
    our machines are going to be interacting with humans who say things like "Good
    morning, Rosa". We do not want it sending out a bunch of chirps, beeps, or `ACK`
    messages, like it’s syncing up a modem or HTTP connection at the start of a conversation
    or web browsing session.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Human greetings and handshakes are a little more informal and flexible. So recognizing
    the greeting *intent* won’t be as simple as building a machine handshake. You
    will want a few different approaches in your toolbox.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-188
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: An intent is a category of possible intentions the user has for the NLP system
    or chatbot. Words "hello" and "hi" might be collected together under the *greeting*
    intent, for when the user wants to start a conversation. Another intent might
    be to carry out some task or command, such as a "translate" command or the query
    "How do I say 'Hello' in Ukrainian?". You’ll learn about intent recognition throughout
    the book and put it to use in a chatbot in chapter 12.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 1.6 Keyword-based greeting recognizer
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Your first chatbot will be straight out of the 80’s. Imagine you want a chatbot
    to help you select a game to play, like chess…​ or a Thermonuclear War. But first,
    your chatbot must find out if you are Professor Falken or General Beringer, or
    some other user that knows what they are doing.^([[57](#_footnotedef_57 "View
    footnote.")]) It will only be able to recognize a few greetings. But this approach
    can be extended to help you implement simple keyword-based intent recognizers
    on projects similar to those mentioned earlier in this chapter.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Listing 1.1 Keyword detection using `str.split`
  id: totrans-192
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This simple NLP pipeline (program) has only two intent categories: "greeting"
    and "unknown" (`else`). And it uses a very simple algorithm called keyword detection.
    Chatbots that recognize the user’s intent like this have capabilities similar
    to modern command line applications or phone trees from the 90’s.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Rule-based chatbots can be much more fun and flexible than this simple program.
    Developers have so much fun building and interacting with chatbots that they build
    chatbots to make even deploying and monitoring servers a lot of fun. *Chatops*,
    or DevOps with chatbots, has become popular on most software development teams.
    You can build a chatbot like this to recognize more intents by adding `elif` statements
    before the `else`. Or you can go beyond keyword-based NLP and start thinking about
    ways to improve it using regular expressions.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 1.6.1 Pattern-based intent recognition
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A keyword-based chatbot would recognize "Hi", "Hello", and "Greetings", but
    it wouldn’t recognize "Hiiii" or "Hiiiiiiiiiiii" - the more excited renditions
    of "Hi". Perhaps you could hardcode the first 200 versions of "Hi", such as `["Hi",
    "Hii", "Hiii", …​]`. Or you could programmatically create such a list of keywords.
    Or you could save yourself a lot of trouble and make your bot deal with literally
    infinite variations of "Hi" using *regular expressions*. Regular expression *patterns*
    can match text much more robustly than any hard-coded rules or lists of keywords.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Regular expressions recognize patterns for any sequence of characters or symbols.^([[58](#_footnotedef_58
    "View footnote.")]) With keyword-based NLP, you and your users need to spell keywords
    and commands in exactly the same way for the machine to respond correctly. So
    your keyword greeting recognizer would miss greetings like "Hey" or even "hi"
    because those strings aren’t in your list of greeting words. And what if your
    "user" used a greeting that starts or ends with punctuation, such as "'sup" or
    "Hi,". You could do *case folding* with the `str.split()` method on both your
    greetings and the user statement. And you could add more greetings to your list
    of greeting words. You could even add misspellings and typos to ensure they aren’t
    missed. But that is a lot of manual "hard-coding" of data into your NLP pipeline.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: You will soon learn how to use machine learning for more data-driven and automated
    NLP pipelines. And when you graduate to the much more complex and accurate *deep
    learning* models of chapter 7 and beyond, you will find that there is still much
    "brittleness" in modern NLP pipelines. Robin Jia’s thesis explains how to measure
    and improve NLP robustness in his thesis ([https://proai.org/robinjia-thesis](proai.org.html))]
    But for now, you need to understand the basics. When your user wants to specify
    actions with precise patterns of characters similar to programming language commands,
    that’s where regular expressions shine.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In regular expressions, you can specify a character class with square brackets.
    And you can use a dash (`-`) to indicate a range of characters without having
    to type them all out individually. So the regular expression `"[a-z]"` will match
    any single lowercase letter, "a" through "z". The star ("\*") after a character
    class means that the regular expression will match any number of consecutive characters
    if they are all within that character class.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Let’s make our regular expression a lot more detailed to try to match more greetings.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Tip
  id: totrans-204
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The "r" before the quote symbol (`r'`) indicates that the quoted string literal
    is a *raw* string. The "r" does not mean **regular** expression. A Python raw
    string just makes it easier to use the backslashes used to escape special symbols
    within a regular expression. Telling Python that a string is "raw" means that
    Python will skip processing the backslashes and pass them on to the regular expression
    parser (`re` package). Otherwise, you would have to escape each and every backslash
    in your regular expression with a double backslash (`'\\'`). So the whitespace
    matching symbol `'\s'` would become `'\\s'`, and special characters like literal
    curly braces would become `'\\{'` and `'\\}'`.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: There is a lot of logic packed into that first line of code, the regular expression.
    It gets the job done for a surprising range of greetings. But it missed that "Manning"
    typo, which is one of the reasons NLP is hard. In machine learning and medical
    diagnostic testing, that’s called a *false negative* classification error. Unfortunately,
    it will also match some statements that humans would be unlikely to ever say — a
    *false positive*, which is also a bad thing. Having both false positive and false
    negative errors means that our regular expression is both too liberal (inclusive)
    and too strict (exclusive). These mistakes could make our bot sound a bit dull
    and mechanical. We’d have to do a lot more work to refine the phrases it matches
    for the bot to behave in a more intelligent human-like way.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: And this tedious work would be highly unlikely to ever succeed at capturing
    all the slang and misspellings people use. Fortunately, composing regular expressions
    by hand isn’t the only way to train a chatbot. Stay tuned for more on that later
    (the entire rest of the book). So we only use them when we need precise control
    over a chatbot’s behavior, such as when issuing commands to a voice assistant
    on your mobile phone.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: But let’s go ahead and finish up our one-trick chatbot by adding an output generator.
    It needs to say something. We use Python’s string formatter to create a "template"
    for our chatbot response.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: So if you run this little script and chat to our bot with a phrase like "Hello
    Rosa", it will respond by asking about your day. If you use a slightly rude name
    to address the chatbot, she will be less responsive, but not inflammatory, to
    encourage politeness.^([[59](#_footnotedef_59 "View footnote.")]) If you name
    someone else who might be monitoring the conversation on a party line or forum,
    the bot will keep quiet and allow you and whomever you are addressing to chat.
    Obviously, there is no one else out there watching our `input()` line, but if
    this were a function within a larger chatbot, you want to deal with these sorts
    of things.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Because of the limitations of computational resources, early NLP researchers
    had to use their human brain’s computational power to design and hand-tune complex
    logical rules to extract information from a natural language string. This is called
    a pattern-based approach to NLP. The patterns do not have to be merely character
    sequence patterns, like our regular expression. NLP also often involves patterns
    of word sequences, or parts of speech, or other "higher level" patterns. The core
    NLP building blocks like stemmers and tokenizers as well as sophisticated end-to-end
    NLP dialog engines (chatbots) like ELIZA were built this way, from regular expressions
    and pattern matching. The art of pattern-matching approaches to NLP is coming
    up with elegant patterns that capture just what you want, without too many lines
    of regular expression code.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Theory of a computational mind
  id: totrans-212
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This classical NLP pattern-matching approach is based on the computational theory
    of mind (CTM). CTM theorizes that thinking is a deterministic computational process
    that acts in a single logical thread or sequence.^([[60](#_footnotedef_60 "View
    footnote.")]) Advancements in neuroscience and NLP led to the development of a
    "connectionist" theory of mind around the turn of the century. This newer theory
    inspired the artificial neural networks of deep learning used that process natural
    language sequences in many different ways simultaneously, in parallel.^([[61](#_footnotedef_61
    "View footnote.")]) ^([[62](#_footnotedef_62 "View footnote.")])
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: In Chapter 2 you will learn more about pattern-based approaches to tokenizing — splitting
    text into tokens or words with algorithms such as the "Treebank tokenizer." You
    will also learn how to use pattern matching to stem (shorten and consolidate)
    tokens with something called a Porter stemmer. But in later chapters we take advantage
    of the exponentially greater computational resources, as well as our larger datasets,
    to shortcut this laborious hand programming and refining.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: If you are new to regular expressions and want to learn more, you can check
    out Appendix B or the online documentation for Python regular expressions. But
    you do not have to understand them just yet. We’ll continue to provide you with
    sample regular expressions as we use them for the building blocks of our NLP pipeline.
    So, do not worry if they look like gibberish. Human brains are pretty good at
    generalizing from a set of examples, and I’m sure it will become clear by the
    end of this book. And it turns out machines can learn this way as well…​
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 1.6.2 Another way
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Imagine a giant database containing sessions of dialog between humans. You might
    have statements paired with responses from thousands or even millions of conversations.
    One way to build a chatbot would be to search such a database for the exact same
    string of characters the user just "said" to your chatbot. And then you could
    use one of the responses to that statement that other humans have said in the
    past. That would result in a statistical or data-driven approach to chatbot design.
    And that could take the place of all that tedious pattern-matching algorithm design.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Think about how a single typo or variation in the statement would trip up a
    pattern-matching bot or even a data-driven bot with millions of statements (utterances)
    in its database. Bit and character sequences are discrete and very precise. They
    either match or they do not. And people are creative. It may not seem like it
    sometimes, but very often people say something with new patterns of characters
    never seen before. So you’d like your bot to be able to measure the difference
    in *meaning* between character sequences. In later chapters, you’ll get better
    and better at extracting meaning from text!
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'When we use character sequence matches to measure distance between natural
    language phrases, we’ll often get it wrong. Phrases with similar meanings, like
    "good" and "okay", can often have different character sequences and large distances
    when we count up character-by-character matches to measure distance. And sometimes
    two words look almost the same but mean completely different things: "bad" and
    "bag." You can count the number of characters that change from one word to another
    with algorithms such as Jaccard and Levenshtein algorithms. But these distance
    or "change" counts fail to capture the essence of the relationship between two
    dissimilar strings of characters such as "good" and "okay.".= And they fail to
    account for how small spelling differences might not really be typos but rather
    completely different words, such as "bad" and "bag".'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Distance metrics designed for numerical sequences and vectors are useful for
    a few NLP applications, like spelling correctors and recognizing proper nouns.
    So we use these distance metrics when they make sense. But for NLP applications
    where we are more interested in the meaning of the natural language than its spelling,
    there are better approaches. We use vector representations of natural language
    words and text and some distance metrics for those vectors for those NLP applications.
    We show you each approach, one by one, as we talk about these different applications
    and the kinds of vectors they are used with.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: We do not stay in this confusing binary world of logic for long, but let’s imagine
    we’re famous World War II-era code-breaker Mavis Batey at Bletchley Park and we
    have just been handed that binary, Morse code message intercepted from communication
    between two German military officers. It could hold the key to winning the war.
    Where would we start? Well, the first layer of deciding would be to do something
    statistical with that stream of bits to see if we can find patterns. We can first
    use the Morse code table (or ASCII table, in our case) to assign letters to each
    group of bits. Then, if the characters are gibberish to us, as they are to a computer
    or a cryptographer in WWII, we could start counting them up, looking up the short
    sequences in a dictionary of all the words we have seen before and putting a mark
    next to the entry every time it occurs. We might also make a mark in some other
    log book to indicate which message the word occurred in, creating an encyclopedic
    index to all the documents we have read before. This collection of documents is
    called a *corpus*, and the words or sequences we have listed in our index are
    called a *lexicon*.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: If we’re lucky, and we’re not at war, and the messages we’re looking at aren’t
    strongly encrypted, we’ll see patterns in those German word counts that mirror
    counts of English words used to communicate similar kinds of messages. Unlike
    a cryptographer trying to decipher German Morse code intercepts, we know that
    the symbols have consistent meaning and aren’t changed with every key click to
    try to confuse us. This tedious counting of characters and words is just the sort
    of thing a computer can do without thinking. And surprisingly, it’s nearly enough
    to make the machine appear to understand our language. It can even do math on
    these statistical vectors that coincides with our human understanding of those
    phrases and words. When we show you how to teach a machine our language using
    Word2Vec in later chapters, it may seem magical, but it’s not. It’s just math,
    computation.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: But let’s think for a moment about what information has been lost in our effort
    to count all the words in the messages we receive. We assign the words to bins
    and store them away as bit vectors like a coin or token sorter (see Figure 1.2)
    directing different kinds of tokens to one side or the other in a cascade of decisions
    that piles them in bins at the bottom. Our sorting machine must take into account
    hundreds of thousands if not millions of possible token "denominations," one for
    each possible word that a speaker or author might use. Each phrase or sentence
    or document we feed into our token sorting machine will come out the bottom, where
    we have a "vector" with a count of the tokens in each slot. Most of our counts
    are zero, even for large documents with verbose vocabulary. But we have not lost
    any words yet. What have we lost? Could you, as a human understand a document
    that we presented you in this way, as a count of each possible word in your language,
    without any sequence or order associated with those words? I doubt it. But if
    it was a short sentence or tweet, you’d probably be able to rearrange them into
    their intended order and meaning most of the time.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.5 Canadian coin sorter
  id: totrans-224
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![canadian coin sorter](images/canadian-coin-sorter.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
- en: Here’s how our token sorter fits into an NLP pipeline right after a tokenizer
    (see Chapter 2). We have included a stopword filter as well as a "rare" word filter
    in our mechanical token sorter sketch. Strings flow in from the top, and bag-of-word
    vectors are created from the height profile of the token "stacks" at the bottom.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.6 Token sorting tray
  id: totrans-227
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![sketch token sorter](images/sketch-token-sorter.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
- en: It turns out that machines can handle this bag of words quite well and glean
    most of the information content of even moderately long documents this way. Each
    document, after token sorting and counting, can be represented as a vector, a
    sequence of integers for each word or token in that document. You see a crude
    example in Figure 1.3, and then Chapter 2 shows some more useful data structures
    for bag-of-word vectors.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: This is our first vector space model of a language. Those bins and the numbers
    they contain for each word are represented as long vectors containing a lot of
    zeros and a few ones or twos scattered around wherever the word for that bin occurred.
    All the different ways that words could be combined to create these vectors is
    called a *vector space*. And relationships between vectors in this space are what
    make up our model, which is attempting to predict combinations of these words
    occurring within a collection of various sequences of words (typically sentences
    or documents). In Python, we can represent these sparse (mostly empty) vectors
    (lists of numbers) as dictionaries. And a Python `Counter` is a special kind of
    dictionary that bins objects (including strings) and counts them just like we
    want.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You can probably imagine some ways to clean those tokens up. We do just that
    in the next chapter. But you might also think to yourself that these sparse, high-dimensional
    vectors (many bins, one for each possible word) aren’t very useful for language
    processing. They are, however, good enough for some industry-changing tools like
    spam filters, which we discuss in Chapter 3.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: And we can imagine feeding into this machine, one at a time, all the documents,
    statements, sentences, and even single words we could find. We’d count up the
    tokens in each slot at the bottom after each of these statements was processed,
    and we’d call that a vector representation of that statement. All the possible
    vectors a machine might create this way is called a *vector space*. And this model
    of documents and statements and words is called a *vector space model*. It allows
    us to use linear algebra to manipulate these vectors and compute things like distances
    and statistics about natural language statements, which helps us solve a much
    wider range of problems with less human programming and less brittleness in the
    NLP pipeline. One statistical question that is asked of bag-of-words vector sequences
    is, "What is the combination of words most likely to follow a particular bag of
    words?" Or, even better, if a user enters a sequence of words, "What is the closest
    bag of words in our database to a bag-of-words vector provided by the user?" This
    is a search query. The input words are the words you might type into a search
    box, and the closest bag-of-words vector corresponds to the document or web page
    you were looking for. The ability to efficiently answer these two questions would
    be sufficient to build a machine learning chatbot that could get better and better
    as we gave it more and more data.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: But wait a minute, perhaps these vectors aren’t like any you’ve ever worked
    with before. They’re extremely high-dimensional. It’s possible to have millions
    of dimensions for a 3-gram vocabulary computed from a large corpus. In Chapter
    3, we discuss the curse of dimensionality and some other properties that make
    high-dimensional vectors difficult to work with.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 1.7 A brief overflight of hyperspace
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Chapter 3 you will learn how to consolidate words into a smaller number of
    vector dimensions to deal with the *curse of dimensionality*. You may even be
    able to turn the curse into a blessing by using all those dimensions to identify
    the subtle things that you want your NLU pipeline to understand. You project vectors
    onto each other to determine the distance between each pair. This gives you a
    reasonable estimate of the similarity in their *meaning* rather than merely their
    statistical word usage. When you compute a vector distance this way it is called
    a *cosine distance metric*. You will first use cosine distance in Chapter 3, and
    then uncover its true power when you are able to reduce the thousands of dimensions
    of topic vectors down to just a few in Chapter 4\. You can even project ("embed"
    is the more precise term) these vectors onto a 2D plane to have a "look" at them
    in plots and diagrams. This is one of the best ways to find patterns and clusters
    in high dimensional data. You can then teach a computer to recognize and act on
    these patterns in ways that reflect the underlying meaning of the words that produced
    those vectors.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine all the possible tweets or messages or sentences that humans might
    write. Even though we do repeat ourselves a lot, that’s still a lot of possibilities.
    And when those tokens are each treated as separate, distinct dimensions, there
    is no concept that "Good morning, Hobs" has some shared meaning with "Guten Morgen,
    Hannes." We need to create some reduced dimension vector space model of messages
    so we can label them with a set of continuous (float) values. We could rate messages
    and words for qualities like subject matter and sentiment. We could ask questions
    like:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: How likely is this message to be a question?
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How much is it about a person?
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How much is it about me?
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How angry or happy does it sound?
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is it something I need to respond to?
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Think of all the ratings we could give statements. We could put these ratings
    in order and "compute" them for each statement to compile a "vector" for each
    statement. The list of ratings or dimensions we could give a set of statements
    should be much smaller than the number of possible statements, and statements
    that mean the same thing should have similar values for all our questions.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: These rating vectors become something that a machine can be programmed to react
    to. We can simplify and generalize vectors further by clumping (clustering) statements
    together, making them close on some dimensions and not on others.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: But how can a computer assign values to each of these vector dimensions? Well,
    if we simplified our vector dimension questions to things like, "Does it contain
    the word 'good'? Does it contain the word 'morning'?" And so on. You can see that
    we might be able to come up with a million or so questions resulting in numerical
    value assignments that a computer could make to a phrase. This is the first practical
    vector space model, called a bit vector language model, or the sum of "one-hot
    encoded" vectors. You can see why computers are just now getting powerful enough
    to make sense of natural language. The millions of million-dimensional vectors
    that humans might generate simply "Does not compute!" on a supercomputer of the
    80s, but is no problem on a commodity laptop in the 21st century. More than just
    raw hardware power and capacity made NLP practical; incremental, constant-RAM,
    linear algebra algorithms were the final piece of the puzzle that allowed machines
    to crack the code of natural language.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: 'There is an even simpler, but much larger representation that can be used in
    a chatbot. What if our vector dimensions completely described the exact sequence
    of characters? The vector for each character would contain the answer to binary
    (yes/no) questions about every letter and punctuation mark in your alphabet:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '"Is the first letter an ''A''?"'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '"Is the first letter a ''B''?"'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: …​
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '"Is the first letter a ''z''?"'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: And the next vector would answer the same boring questions about the next letter
    in the sequence.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '"Is the second letter an A?"'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '"Is the second letter a B?"'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: …​
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Despite all the "no" answers or zeroes in this vector sequence, it does have
    one advantage over all other possible representations of text - it retains every
    tiny detail, every bit of information contained in the original text, including
    the order of the characters and words. This is like the paper representation of
    a song for a player piano that only plays a single note at a time. The "notes"
    for this natural language mechanical player piano are the 26 uppercase and lowercase
    letters plus any punctuation that the piano must know how to "play." The paper
    roll wouldn’t have to be much wider than for a real player piano and the number
    of notes in some long piano songs doesn’t exceed the number of characters in a
    small document.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: But this one-hot character sequence encoding representation is mainly useful
    for recording and then replaying an exact piece rather than composing something
    new or extracting the essence of a piece. We can’t easily compare the piano paper
    roll for one song to that of another. And this representation is longer than the
    original ASCII-encoded representation of the document. The number of possible
    document representations just exploded to retain information about each sequence
    of characters. We retained the order of characters and words but expanded the
    dimensionality of our NLP problem.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: These representations of documents do not cluster together well in this character-based
    vector world. The Russian mathematician Vladimir Levenshtein came up with a brilliant
    approach for quickly finding similarities between vectors (strings of characters)
    in this world. Levenshtein’s algorithm made it possible to create some surprisingly
    fun and useful chatbots, with only this simplistic, mechanical view of language.
    But the real magic happened when we figured out how to compress/embed these higher
    dimensional spaces into a lower dimensional space of fuzzy meaning or topic vectors.
    We peek behind the magician’s curtain in Chapter 4, when we talk about latent
    semantic indexing and latent Dirichlet allocation, two techniques for creating
    much more dense and meaningful vector representations of statements and documents.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: 1.8 Word order and grammar
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The order of words matters. Those rules that govern word order in a sequence
    of words (like a sentence) are called the grammar of a language. That’s something
    that our bag of words or word vector discarded in the earlier examples. Fortunately,
    in most short phrases and even many complete sentences, this word vector approximation
    works OK. If you just want to encode the general sense and sentiment of a short
    sentence, word order is not terribly important. Take a look at all these orderings
    of our "Good morning Rosa" example.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now if you tried to interpret each of those strings in isolation (without looking
    at the others), you’d probably conclude that they all probably had similar intent
    or meaning. You might even notice the capitalization of the word "Good" and place
    the word at the front of the phrase in your mind. But you might also think that
    "Good Rosa" was some sort of proper noun, like the name of a restaurant or flower
    shop. Nonetheless, a smart chatbot or clever woman of the 1940s in Bletchley Park
    would likely respond to any of these six permutations with the same innocuous
    greeting, "Good morning my dear General."
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try that (in our heads) on a much longer, more complex phrase, a logical
    statement where the order of the words matters a lot:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The number of permutations exploded from `factorial(3) == 6` in our simple greeting
    to `factorial(12) == 479001600` in our longer statement! And it’s clear that the
    logic contained in the order of the words is important to any machine that would
    like to reply with the correct response. Even though common greetings are not
    usually garbled by bag-of-words processing, more complex statements can lose most
    of their meaning when thrown into a bag. A bag of words is not the best way to
    begin processing a database query, like the natural language query in the preceding
    example.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Whether a statement is written in a formal programming language like SQL, or
    in an informal natural language like English, word order and grammar are important
    when a statement intends to convey logical relationships between things. That’s
    why computer languages depend on rigid grammar and syntax rule parsers. Fortunately,
    recent advances in natural language syntax tree parsers have made possible the
    extraction of syntactical and logical relationships from natural language with
    remarkable accuracy (greater than 90%).^([[63](#_footnotedef_63 "View footnote.")])
    In later chapters, we show you how to use packages like `SyntaxNet` (Parsey McParseface)
    and `SpaCy` to identify these relationships.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: And just as in the Bletchley Park example greeting, even if a statement doesn’t
    rely on word order for logical interpretation, sometimes paying attention to that
    word order can reveal subtle hints of meaning that might facilitate deeper responses.
    These deeper layers of natural language processing are discussed in the next section.
    And Chapter 2 shows you a trick for incorporating some of the information conveyed
    by word order into our word-vector representation. It also shows you how to refine
    the crude tokenizer used in the previous examples (`str.split()`) to more accurately
    bin words into more appropriate slots within the word vector, so that strings
    like "good" and "Good" are assigned the same bin, and separate bins can be allocated
    for tokens like "rosa" and "Rosa" but not "Rosa!".
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: 1.9 A chatbot natural language pipeline
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The NLP pipeline required to build a dialog engine, or chatbot, is similar to
    the pipeline required to build a question answering system described in *Taming
    Text* (Manning, 2013).^([[64](#_footnotedef_64 "View footnote.")]) However, some
    of the algorithms listed within the five subsystem blocks may be new to you. We
    help you implement these in Python to accomplish various NLP tasks essential for
    most applications, including chatbots.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.7 Chatbot recirculating (recurrent) pipeline
  id: totrans-269
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![chatbot pipeline](images/chatbot-pipeline.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
- en: A chatbot requires four kinds of processing as well as a database to maintain
    a memory of past statements and responses. Each of the four processing stages
    can contain one or more processing algorithms working in parallel or in series
    (see figure 1.4).
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '*Parse* — Extract features, structured numerical data, from natural language
    text.'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Analyze* — Generate and combine features by scoring text for sentiment, grammaticality,
    semantics.'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Generate* — Compose possible responses using templates, search, or language
    models.'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Execute* — Plan statements based on conversation history and objectives, and
    select the next response.'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each of these four stages can be implemented using one or more of the algorithms
    listed within the corresponding boxes in the block diagram. We show you how to
    use Python to accomplish near-state-of-the-art performance for each of these processing
    steps. And we show you several alternative approaches to implementing these five
    subsystems.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: Most chatbots will contain elements of all five of these subsystems (the four
    processing stages as well as the database). But many applications require only
    simple algorithms for many of these steps. Some chatbots are better at answering
    factual questions, and others are better at generating lengthy, complex, convincingly
    human responses. Each of these capabilities requires different approaches; we
    show you techniques for both.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: In addition, deep learning and data-driven programming (machine learning, or
    probabilistic language modeling) have rapidly diversified the possible applications
    for NLP and chatbots. This data-driven approach allows ever greater sophistication
    for an NLP pipeline by providing it with greater and greater amounts of data in
    the domain you want to apply it to. And when a new machine learning approach is
    discovered that makes even better use of this data, with more efficient model
    generalization or regularization, then large jumps in capability are possible.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: The NLP pipeline for a chatbot shown in Figure 1.4 contains all the building
    blocks for most of the NLP applications that we described at the start of this
    chapter. As in *Taming Text*, we break out our pipeline into four main subsystems
    or stages. In addition, we have explicitly called out a database to record data
    required for each of these stages and persist their configuration and training
    sets over time. This can enable batch or online retraining of each of the stages
    as the chatbot interacts with the world. We have also shown a "feedback loop"
    on our generated text responses so that our responses can be processed using the
    same algorithms used to process the user statements. The response "scores" or
    features can then be combined in an objective function to evaluate and select
    the best possible response, depending on the chatbot’s plan or goals for the dialog.
    This book is focused on configuring this NLP pipeline for a chatbot, but you may
    also be able to see the analogy to the NLP problem of text retrieval or "search,"
    perhaps the most common NLP application. And our chatbot pipeline is certainly
    appropriate for the question-answering application that was the focus of *Taming
    Text*.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: The application of this pipeline to financial forecasting or business analytics
    may not be so obvious. But imagine the features generated by the analysis portion
    of your pipeline. These features of your analysis or feature generation can be
    optimized for your particular finance or business prediction. That way they can
    help you incorporate natural language data into a machine learning pipeline for
    forecasting. Despite focusing on building a chatbot, this book gives you the tools
    you need for a broad range of NLP applications, from search to financial forecasting.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: One processing element in Figure 1.4 that is not typically employed in search,
    forecasting, or question-answering systems is natural language *generation*. For
    chatbots, this is their central feature. Nonetheless, the text generation step
    is often incorporated into a search engine NLP application and can give such an
    engine a large competitive advantage. The ability to consolidate or summarize
    search results is a winning feature for many popular search engines (DuckDuckGo,
    Bing, and Google). And you can imagine how valuable it is for a financial forecasting
    engine to be able to generate statements, tweets, or entire articles based on
    the business-actionable events it detects in natural language streams from social
    media networks and news feeds.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: The next section shows how the layers of such a system can be combined to create
    greater sophistication and capability at each stage of the NLP pipeline.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: 1.10 Processing in depth
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The stages of a natural language processing pipeline can be thought of as layers,
    like the layers in a feed-forward neural network. Deep learning is all about creating
    more complex models and behavior by adding additional processing layers to the
    conventional two-layer machine learning model architecture of feature extraction
    followed by modeling. In Chapter 5 we explain how neural networks help spread
    the learning across layers by backpropagating model errors from the output layers
    back to the input layers. But here we talk about the top layers and what can be
    done by training each layer independently of the other layers.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.8 Example layers for an NLP pipeline
  id: totrans-285
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![nlp layers](images/nlp-layers.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
- en: The top four layers in Figure 1.8 correspond to the first two stages in the
    chatbot pipeline (feature extraction and feature analysis) in the previous section.
    For example, part-of-speech tagging (POS tagging), is one way to generate features
    within the Analyze stage of our chatbot pipeline. POS tags are generated automatically
    by the default `SpaCY` pipeline, which includes all the top four layers in this
    diagram. POS tagging is typically accomplished with a finite state transducer
    like the methods in the `nltk.tag` package.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: The bottom two layers (Entity Relationships and a Knowledge Base) are used to
    populate a database containing information (knowledge) about a particular domain.
    And the information extracted from a particular statement or document using all
    six of these layers can then be used in combination with that database to make
    inferences. Inferences are logical extrapolations from a set of conditions detected
    in the environment, like the logic contained in the statement of a chatbot user.
    This kind of "inference engine" in the deeper layers of this diagram is considered
    the domain of artificial intelligence, where machines can make inferences about
    their world and use those inferences to make logical decisions. However, chatbots
    can make reasonable decisions without this knowledge database, using only the
    algorithms of the upper few layers. And these decisions can combine to produce
    surprisingly human-like behaviors.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: Over the next few chapters, we dive down through the top few layers of NLP.
    The top three layers are all that is required to perform meaningful sentiment
    analysis and semantic search and to build human-mimicking chatbots. In fact, it’s
    possible to build a useful and interesting chatbot using only a single layer of
    processing, using the text (character sequences) directly as the features for
    a language model. A chatbot that only does string matching and search is capable
    of participating in a reasonably convincing conversation if given enough example
    statements and responses.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: For example, the open source project `ChatterBot` simplifies this pipeline by
    merely computing the string "edit distance" (Levenshtein distance) between an
    input statement and the statements recorded in its database. If its database of
    statement-response pairs contains a matching statement, the corresponding reply
    (from a previously "learned" human or machine dialog) can be reused as the reply
    to the latest user statement. For this pipeline, all that is required is step
    3 (Generate) of our chatbot pipeline. And within this stage, only a brute-force
    search algorithm is required to find the best response. With this simple technique
    (no tokenization or feature generation required), `ChatterBot` can maintain a
    convincing conversion as the dialog engine for Salvius, a mechanical robot built
    from salvaged parts by Gunther Cox.^([[65](#_footnotedef_65 "View footnote.")])
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '`Will` is an open source Python chatbot framework by Steven Skoczen with a
    completely different approach.^([[66](#_footnotedef_66 "View footnote.")]) `Will`
    can only be trained to respond to statements by programming it with regular expressions.
    This is the labor-intensive and data-light approach to NLP. This grammar-based
    approach is especially effective for question-answering systems and task-execution
    assistant bots, like Lex, Siri, and Google Now. These kinds of systems overcome
    the "brittleness" of regular expressions by employing "fuzzy regular expressions."footnote:[The
    Python `regex` package is backward compatible with `re` and adds fuzziness among
    other features. The `regex` will replace the `re` package in future Python versions
    ([https://pypi.python.org/pypi/regex](pypi.html)).'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Similarly `TRE agrep`, or "approximate grep," ([https://github.com/laurikari/tre](laurikari.html))
    is an alternative to the UNIX command-line application `grep.`] and other techniques
    for finding approximate grammar matches. Fuzzy regular expressions find the closest
    grammar matches among a list of possible grammar rules (regular expressions) instead
    of exact matches by ignoring some maximum number of insertion, deletion, and substitution
    errors. However, expanding the breadth and complexity of behaviors for pattern-matching
    chatbots requires a lot of difficult human development work. Even the most advanced
    grammar-based chatbots, built and maintained by some of the largest corporations
    on the planet (Google, Amazon, Apple, Microsoft), remain in the middle of the
    pack for depth and breadth of chatbot IQ.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: A lot of powerful things can be done with shallow NLP. And little, if any, human
    supervision (labeling or curating of text) is required. Often a machine can be
    left to learn perpetually from its environment (the stream of words it can pull
    from Twitter or some other source).^([[67](#_footnotedef_67 "View footnote.")])
    We show you how to do this in Chapter 6.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: 1.11 Natural language IQ
  id: totrans-294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Like human brainpower, the power of an NLP pipeline cannot be easily gauged
    with a single IQ score without considering multiple "smarts" dimensions. A common
    way to measure the capability of a robotic system is along the dimensions of behavior
    complexity and the degree of human supervision required. But for a natural language
    processing pipeline, the goal is to build systems that fully automate the processing
    of natural language, eliminating all human supervision (once the model is trained
    and deployed). So a better pair of IQ dimensions should capture the breadth and
    depth of the complexity of the natural language pipeline.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: A consumer product chatbot or virtual assistant like Alexa or Allo is usually
    designed to have extremely broad knowledge and capabilities. However, the logic
    used to respond to requests tends to be shallow, often consisting of a set of
    trigger phrases that all produce the same response with a single if-then decision
    branch. Alexa (and the underlying Lex engine) behave like a single layer, flat
    tree of (if, elif, elif, …​) statements.^([[68](#_footnotedef_68 "View footnote.")])
    Google Dialogflow (which was developed independently of Google’s Allo and Google
    Assistant) has similar capabilities to Amazon Lex, Contact Flow, and Lambda, but
    without the drag-and-drop user interface for designing your dialog tree.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the Google Translate pipeline (or any similar machine translation
    system) relies on a deep tree of feature extractors, decision trees, and knowledge
    graphs connecting bits of knowledge about the world. Sometimes these feature extractors,
    decision trees, and knowledge graphs are explicitly programmed into the system,
    as in Figure 1.5\. Another approach rapidly overtaking this "hand-coded" pipeline
    is the deep learning data-driven approach. Feature extractors for deep neural
    networks are learned rather than hard-coded, but they often require much more
    training data to achieve the same performance as intentionally designed algorithms.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: You will use both approaches (neural networks and hand-coded algorithms) as
    you incrementally build an NLP pipeline for a chatbot capable of conversing within
    a focused knowledge domain. This will give you the skills you need to accomplish
    the natural language processing tasks within your industry or business domain.
    Along the way you will probably get ideas about how to expand the breadth of things
    this NLP pipeline can do. Figure 1.6 puts the chatbot in its place among the natural
    language processing systems that are already out there. Imagine the chatbots you
    have interacted with. Where do you think they might fit in a plot like this? Have
    you attempted to gauge their intelligence by probing them with difficult questions
    or something like an IQ test? Try asking a chatbot something ambiguous that requires
    common sense logic and the ability to ask clarifying questions, such as "What’s
    larger, the sun or a nickel?"^([[69](#_footnotedef_69 "View footnote.")]) you
    will get a chance to do exactly that in later chapters, to help you decide how
    your chatbot stacks up against some of the others in this diagram.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1\. 9\. 2D IQ of some natural language processing systems
  id: totrans-299
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![nlp iq](images/nlp-iq.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
- en: 'As you progress through this book, you will be building the elements of a chatbot.
    Chatbots require all the tools of NLP to work well:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Feature extraction (usually to produce a vector space model)
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Information extraction to be able to answer factual questions
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semantic search to learn from previously recorded natural language text or dialog
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Natural language generation to compose new, meaningful statements
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning gives us a way to trick machines into behaving as if we had
    spent a lifetime programming them with hundreds of complex regular expressions
    or algorithms. We can teach a machine to respond to patterns similar to the patterns
    defined in regular expressions by merely providing it examples of user statements
    and the responses we want the chatbot to mimic. And the "models" of language,
    the FSMs, produced by machine learning, are much better. They are less picky about
    mispelings and typoz.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: And machine learning NLP pipelines are easier to "program." We do not have to
    anticipate every possible use of symbols in our language. We just have to feed
    the training pipeline with examples of the phrases that match and with example
    phrases that do not match. As long as we label the example phrases during training
    so that the chatbot knows which is which, it will learn to discriminate between
    them. And there are even machine learning approaches that require little if any
    "labeled" data.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: We have given you some exciting reasons to learn about natural language processing.
    You want to help save the world, do you not? And we have attempted to pique your
    interest with some practical NLP applications that are revolutionizing the way
    we communicate, learn, do business, and even think. It will not be long before
    you are able to build a system that approaches human-like conversational behavior.
    And you should be able to see in upcoming chapters how to train a chatbot or NLP
    pipeline with any domain knowledge that interests you — from finance and sports
    to psychology and literature. If you can find a corpus of writing about it, then
    you can train a machine to understand it.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: This book is about using machine learning to build smart text-reading machines
    without you having to anticipate all the ways people can say things. Each chapter
    incrementally improves on the basic NLP pipeline for the chatbot introduced in
    this chapter. As you learn the tools of natural language processing, you will
    be building an NLP pipeline that can not only carry on a conversation but help
    you accomplish your goals in business and in life.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: 1.12 Test yourself
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Chapter 1 review questions**'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some review questions for you to test your understanding:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: Why is NLP considered to be a core enabling feature for AGI (human-like AI)?
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do advanced NLP models tend to show significant discriminatory biases?
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How is it possible to create a prosocial chatbot using training data from sources
    that include antisocial examples?
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are 4 different approaches or architectures for building a chatbot?
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How is NLP used within a search engine?
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a regular expression to recognize your name and all the variations on
    its spelling (including nicknames) that you’ve seen.
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a regular expression to try to recognize a sentence boundary (usually
    a period ("."), question mark "?", or exclamation mark "!")
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tip
  id: totrans-320
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Active learning, quizzing yourself with questions such as these, is a fast way
    to gain deep understanding of any new topic. It turns out, this same approach
    is effective for machine learning and model evaluation as well.footnote:[Suggested
    answers are provided within the Python packages `nlpia` ([https://gitlab.com/tangibleai/nlpia](tangibleai.html))
    and `qary` ([https://gitlab.com/tangibleai/qary](tangibleai.html)) where they
    are used to evaluate advanced NLP models for reading comprehension and question
    answering. Pooja Sethi will share active learning NLP insights on Substack ([https://activelearning.substack.com](.html))
    and github ([https://poojasethi.github.io](.html)) by the time this book goes
    to print. ProAI.org, the team of contributing authors for this book is doing the
    same on substack ([https://proai.substack.com](.html)) and their home page ([https://proai.org](.html)).
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: 1.13 Summary
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Good NLP may help save the world.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The meaning and intent of words can be deciphered by machines.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A smart NLP pipeline will be able to deal with ambiguity.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can teach machines common sense knowledge without spending a lifetime training
    them.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chatbots can be thought of as semantic search engines.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regular expressions are useful for more than just search.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[1]](#_footnoteref_1) In 2013 The Guardian and other news organizations revealed
    Facebook’s experiments to maniuplate users'' emotions using NLP ( [https://www.theguardian.com/technology/2014/jun/29/facebook-users-emotions-news-feeds](29.html)).
    Search engine giants and their algorithms perform these same kinds of experiments
    each time you enter text into the search box ( [https://www.computerservicesolutions.in/all-google-search-algorithm-updates/](all-google-search-algorithm-updates.html)).'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '[[2]](#_footnoteref_2) "Genshin Impact won’t let players write ''Tibet'', ''Hong
    Kong'', ''Taiwan'' because of Chinese censorship" by Paolo Sirio ( [https://archive.is/MxNZI](archive.is.html))'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: '[[3]](#_footnoteref_3) "How to circumvent Facebook’s content censorship using
    archive" by Emil O. W. Kirkegaard ( [https://archive.is/CA71O](archive.is.html))'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: '[[4]](#_footnoteref_4) "Censorship of Online Encyclopedias Implications for
    NLP Models" ( [https://www.researchgate.net/publication/348757384_Censorship_of_Online_Encyclopedias_Implications_for_NLP_Models](publication.html))'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '[[5]](#_footnoteref_5) Yuval Noah Harari’s seminal book *21 Lessons for the
    21st Century* - Wikipedia article ( [https://en.wikipedia.org/wiki/21_Lessons_for_the_21st_Century](wiki.html))'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '[[6]](#_footnoteref_6) Von Neumann Architecture on Wikipedia ( [https://en.wikipedia.org/wiki/Von_Neumann_architecture](wiki.html))'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: '[[7]](#_footnoteref_7) "The secrets of computer power revealed" by Daniel Dennett
    ( [https://sites.tufts.edu/rodrego/](rodrego.html))'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '[[8]](#_footnoteref_8) Animal Language" on Wikipedia ( [https://en.wikipedia.org/wiki/Animal_language](wiki.html))'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: '[[9]](#_footnoteref_9) Parsing Expression Grammar Notation home page ( [https://pegn.dev/](pegn.dev.html))'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '[[10]](#_footnoteref_10) ( [https://writings.stephenwolfram.com/2023/02/a-50-year-quest-my-personal-journey-with-the-second-law-of-thermodynamics/](a-50-year-quest-my-personal-journey-with-the-second-law-of-thermodynamics.html))'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: '[[11]](#_footnoteref_11) The ConvoHub project at ( [https://qary.ai](.html))
    and on GitLab ( [https://gitlab.com/tangibleai/community/convohub](community.html))'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '[[12]](#_footnoteref_12) Wikipedia article on Enshittification and rent-seeking
    ( [https://en.wikipedia.org/wiki/Enshittification](wiki.html))'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: '[[13]](#_footnoteref_13) At DefCon 31, Cory Doctorow explained how interoperable
    APIs will win out over walled gardens and rent-seeking on YouTube( [https://www.youtube.com/watch?v=rimtaSgGz_4](www.youtube.com.html))'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '[[14]](#_footnoteref_14) "When will AIs program programs that can program AIs"
    on Metaculus ( [https://www.metaculus.com/questions/406/when-will-ais-program-programs-that-can-program-ais/](when-will-ais-program-programs-that-can-program-ais.html))'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: '[[15]](#_footnoteref_15) You may have heard of Microsoft’s and OpenAI’s Copilot
    project. GPT-J can do almost as well, and it’s completely open source and open
    data. ( [https://huggingface.co/models?sort=likes&search=gpt-j](huggingface.co.html))'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: '[[16]](#_footnoteref_16) *Human Compatible: Artificial Intelligence and the
    Problem of Control* by Stuart Russell'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: '[[17]](#_footnoteref_17) AI safety article on Wikipedia ( [https://en.wikipedia.org/wiki/AI_safety](wiki.html))'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: '[[18]](#_footnoteref_18) See the web page titled, "How Google’s Site Crawlers
    Index Your Site - Google Search" ( [https://proai.org/google-search](proai.org.html)).'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '[[19]](#_footnoteref_19) You can estimate the amount of actual natural language
    text out there to be at least a thousand times the size of Google’s index.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: '[[20]](#_footnoteref_20) [http://ethnologue.com](.html) maintains statistics
    about natural languages. ISO 639-3 lists 7,486 three-letter language codes ( [http://proai.org/language-codes](proai.org.html)).'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: '[[21]](#_footnoteref_21) *The Great Silence* by Ted Chiang ( [https://proai.org/great-silence](proai.org.html))
    describes an imagined dialog with an endangered species of parrot that concludes
    with the parrot saying to humanity, "Be Good. I love you."'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: '[[22]](#_footnoteref_22) Dolphin Communication Project ( [https://proai.org/dolphin-communication](proai.org.html))'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: '[[23]](#_footnoteref_23) Mathematical notation is ambiguous. See the "Mathematical
    notation" section of the Wikipedia article "Ambguity" ( [https://en.wikipedia.org/wiki/Ambiguity#Mathematical_notation](wiki.html)).'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: '[[24]](#_footnoteref_24) Thank you to "Tudor" on MEAP for setting me straight
    about this. ( [https://www.ted.com/talks/steven_pinker_what_our_language_habits_reveal/transcript](steven_pinker_what_our_language_habits_reveal.html))'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: '[[25]](#_footnoteref_25) Thank you "Tudor" for improving this section and my
    thinking about linguistic relativism'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '[[26]](#_footnoteref_26) Thank you Leo Hepis!'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: '[[27]](#_footnoteref_27) From the Wikipedia article on Neuro-linguistic-programming
    ( [https://en.wikipedia.org/wiki/Neuro-linguistic_programming](wiki.html))'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: '[[28]](#_footnoteref_28) Open source Home Assistant pipeline on GitHub ( [https://github.com/home-assistant/](home-assistant.html))'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: '[[29]](#_footnoteref_29) You can install MyCroft AI on any RaspberryPi with
    a speaker and a microphone ( [https://mycroft.ai/](mycroft.ai.html))'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: '[[30]](#_footnoteref_30) Stanford’s Open Virtual Assistant Lab within their
    Human-centered AI Institute ( [https://hai.stanford.edu/news/open-source-challenger-popular-virtual-assistants](news.html))'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: '[[31]](#_footnoteref_31) Some open source voice assistants you could contribute
    to ( [https://gitlab.com/tangibleai/team/-/tree/main/exercises/1-voice/](1-voice.html)).'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: '[[32]](#_footnoteref_32) "Identifying Sarcasm in Twitter: A Closer Look" by
    Roberto González-Ibáñez ( [https://aclanthology.org/P11-2102.pdf](aclanthology.org.html))'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: '[[33]](#_footnoteref_33) Interpretable Multi-Head Self-Attention Architecture
    for Sarcasm Detection in Social Media by Ramya Akula et al., 2021 ( [https://www.mdpi.com/1099-4300/23/4/394/pdf](394.html))'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: '[[34]](#_footnoteref_34) Some grammar rules can be implemented in a computer
    science abstraction called a finite state machine. Regular grammars can be implemented
    in regular expressions. There are two Python packages for running regular expression
    finite state machines, `re` which is built in, and `regex` which must be installed,
    but may soon replace `re`. Finite state machines are just trees of if…​then…​else
    statements for each token (character/word/n-gram) or action that a machine needs
    to react to or generate.'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: '[[35]](#_footnoteref_35) "AP’s ''robot journalists'' are writing their own
    stories now", The Verge, Jan 29, 2015, [http://www.theverge.com/2015/1/29/7939067/ap-journalism-automation-robots-financial-reporting](7939067.html)'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: '[[36]](#_footnoteref_36) Many chatbot frameworks, such as qary ( [http://gitlab.com/tangibleai.com/qary](tangibleai.com.html))
    allow importing of legacy FAQ lists to automatically compose a rule-based dialog
    engine for your chatbot.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: '[[37]](#_footnoteref_37) "Are Sample-Efficient NLP Models More Robust?" by
    Nelson F. Liu, Ananya Kumar, Percy Liang, Robin Jia ( [https://arxiv.org/abs/2210.06456](abs.html))'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: '[[38]](#_footnoteref_38) New York Times, Oct 18, 2016, [https://www.nytimes.com/2016/11/18/technology/automated-pro-trump-bots-overwhelmed-pro-clinton-messages-researchers-say.html](technology.html)
    and MIT Technology Review, Nov 2016, [https://www.technologyreview.com/s/602817/how-the-bot-y-politic-influenced-this-election/](how-the-bot-y-politic-influenced-this-election.html)'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: '[[39]](#_footnoteref_39) "A beginners guide to Mastodon" by Amanda Silberling
    at Tech Crunch ( [https://techcrunch.com/2022/11/08/what-is-mastodon/](what-is-mastodon.html))'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '[[40]](#_footnoteref_40) Amanda Silberling, Senior Tech editor on Mastodon
    ( [https://journa.host/@amanda](journa.host.html) )'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: '[[41]](#_footnoteref_41) 2021, E.Madhorubagan et al "Intelligent Interface
    for Fake Product Review Monitoring and Removal" ( [https://ijirt.org/master/publishedpaper/IJIRT151055_PAPER.pdf](publishedpaper.html))'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: '[[42]](#_footnoteref_42) Google Blog May 2018 about their *Duplex* system [https://ai.googleblog.com/2018/05/advances-in-semantic-textual-similarity.html](05.html)'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '[[43]](#_footnoteref_43) Wikipedia is probably your most objective reference
    on the "AI control problem" ( [https://en.wikipedia.org/wiki/AI_control_problem](wiki.html)).'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: '[[44]](#_footnoteref_44) WSJ Blog, March 10, 2017 [https://blogs.wsj.com/cio/2017/03/10/homo-deus-author-yuval-noah-harari-says-authority-shifting-from-people-to-ai/](homo-deus-author-yuval-noah-harari-says-authority-shifting-from-people-to-ai.html)'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: '[[45]](#_footnoteref_45) John Michael Innes and Ben W. Morrison at the University
    of South Australia "Machines can do most of a psychologist’s job", 2021, ( [https://theconversation.com/machines-can-do-most-of-a-psychologists-job-the-industry-must-prepare-for-disruption-154064](theconversation.com.html))'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: '[[46]](#_footnoteref_46) "Humans Bonding with Virtual Companions" by C.S. Voll
    ( [https://archive.ph/6nSx2](archive.ph.html))'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: '[[47]](#_footnoteref_47) Tangible AI builds open source cognitive assistants
    that help users take control of their emotions such as Syndee and `qary` ( [https://gitlab.com/tangibleai/qary](tangibleai.html))
    Some of Replika.AI’s core technologies are open source ( [https://github.com/lukalabs](github.com.html))'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: '[[48]](#_footnoteref_48) Taken from animation on copilot.github.com that was
    unchanged from 2022 to March 2023 ( [https://copilot.github.com/](copilot.github.com.html))'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: '[[49]](#_footnoteref_49) One of Cialdini’s six psychology principles in his
    popular book *Influence* [http://changingminds.org/techniques/general/cialdini/click-whirr.htm](cialdini.html)'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: '[[50]](#_footnoteref_50) Steven Skoczen’s Will chatbot framework ( [https://github.com/skoczen/will](skoczen.html))'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: '[[51]](#_footnoteref_51) Tangible AI’s chatbot framework called `qary` ( [https://docs.qary.ai](.html))
    with examples deployed for WeSpeakNYC ( [https://wespeaknyc.cityofnewyork.us/](wespeaknyc.cityofnewyork.us.html))
    and others'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: '[[52]](#_footnoteref_52) Stack Exchange Went Down for 30 minutes on July 20,
    2016 when a regex "crashed" ( [http://stackstatus.net/post/147710624694/outage-postmortem-july-20-2016](147710624694.html))'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: '[[53]](#_footnoteref_53) See the web page titled "Chomsky hierarchy - Wikipedia"
    ( [https://en.wikipedia.org/wiki/Chomsky_hierarchy](wiki.html)).'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: '[[54]](#_footnoteref_54) "English is not a regular language" ( [http://cs.haifa.ac.il/~shuly/teaching/08/nlp/complexity.pdf#page=20](nlp.html))
    by Shuly Wintner'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: '[[55]](#_footnoteref_55) "Is English context-free?" ( [http://cs.haifa.ac.il/~shuly/teaching/08/nlp/complexity.pdf#page=24](nlp.html))
    by Shuly Wintner'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: '[[56]](#_footnoteref_56) See the web page titled "1.11\. Formal and Natural
    Languages — How to Think like a Computer Scientist: Interactive Edition" ( [https://runestone.academy/ns/books/published/fopp/GeneralIntro/FormalandNaturalLanguages.html](GeneralIntro.html)).'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: '[[57]](#_footnoteref_57) The code here simplifies the behavior of the chatbot
    called "Joshua" in the "War Games" movie. See Wikiquote ( [https://en.wikiquote.org/wiki/WarGames](wiki.html))
    for more chatbot script ideas.'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: '[[58]](#_footnoteref_58) SpaCy ''Matcher'' ( [https://spacy.io/api/matcher](api.html))
    is a regular expression interpreter for patterns of words, parts of speech, and
    other symbol sequences.'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: '[[59]](#_footnoteref_59) The idea for this defusing response originated with
    Viktor Frankl’s *Man’s Search for Meaning*, his Logotherapy ( [https://en.wikipedia.org/wiki/Logotherapy](wiki.html))
    approach to psychology and the many popular novels where a child protagonist like
    Owen Meany has the wisdom to respond to an insult with a response like this.'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '[[60]](#_footnoteref_60) Stanford Encyclopedia of Philosophy, Computational
    Theory of Mind, [https://plato.stanford.edu/entries/computational-mind/](computational-mind.html)'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: '[[61]](#_footnoteref_61) Stanford Encyclopedia of Philosophy, Connectionism,
    [https://plato.stanford.edu/entries/connectionism/](connectionism.html)'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: '[[62]](#_footnoteref_62) "Toward a Connectionist Model of Recursion in Human
    Linguistic Performance" Christiansen and Chater, 1999, Southern Illinois University
    ( [https://archive.is/0Lx3o](archive.is.html))'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: '[[63]](#_footnoteref_63) A comparison of the syntax parsing accuracy of SpaCy
    (93%), SyntaxNet (94%), Stanford’s CoreNLP (90%), and others is available at [https://spacy.io/docs/api/](api.html)'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: '[[64]](#_footnoteref_64) Ingersol, Morton, and Farris, [http://www.manning.com/books/taming-text/?a_aid=totalgood](taming-text.html)'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: '[[65]](#_footnoteref_65) ChatterBot by Gunther Cox and others at [https://github.com/gunthercox/ChatterBot](gunthercox.html)'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: '[[66]](#_footnoteref_66) See the GitHub page for "Will," a chatbot for HipChat,
    by Steven Skoczen and the HipChat community ( [https://github.com/skoczen/will](skoczen.html)).
    In 2018 it was updated to integrate with Slack'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: '[[67]](#_footnoteref_67) Simple neural networks are often used for unsupervised
    feature extraction from character and word sequences.'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: '[[68]](#_footnoteref_68) More complicated logic and behaviors are now possible
    when you incorporate Lambdas into an AWS Contact Flow dialog tree. See "Creating
    Call Center Bot with AWS Connect" ( [https://greenice.net/creating-call-center-bot-aws-connect-amazon-lex-can-speak-understand](greenice.net.html)).'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: '[[69]](#_footnoteref_69) "CommonSense QA: A Question Answering Challenge Targeting
    Commonsense Knowledge" by Alon Talmor et al ( [https://aclanthology.org/N19-1421.pdf](aclanthology.org.html)).
    You can help collect more questions and datasets like this in the nlpia2 GitLab
    repository ( [https://gitlab.com/tangibleai/nlpia2/-/blob/main/src/nlpia2/data/iq_test.csv](data.html)
    ).'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
