["```py\nfrom datasets import load_dataset\n\n# Load MNLI dataset from GLUE\n# 0 = entailment, 1 = neutral, 2 = contradiction\ndataset = load_dataset(\"glue\", \"mnli\", split=\"train\")\n```", "```py\n>>> dataset[2]\n{'premise': 'One of our number will carry out your instructions minutely.',\n 'hypothesis': 'A member of my team will execute your orders with immense precision.',\n 'label': 0,\n 'idx': 2}\n```", "```py\nfrom tqdm.auto import tqdm\nfrom torch.utils.data import DataLoader\nfrom sentence_transformers import InputExample\n\ntrain_examples = [InputExample(texts=[row[\"premise\"], row[\"hypothesis\"]],  \n                               label=row[\"label\"]) for row in tqdm(dataset)]\ntrain_dataloader = DataLoader(train_examples, shuffle=True, batch_size=32)\n```", "```py\nfrom sentence_transformers import SentenceTransformer, models\n\n# Define a model that will embed individual words\nword_embedding_model = models.Transformer('bert-base-uncased', max_seq_length=256)\n\n# Define a model that will pool each individual words\n# NOTE: This automatically uses average pooling but other strategies exist\n# such as taking the maximum or mode of word embeddings across all dimensions\npooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n\n# Create\nmodel = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n```", "```py\nfrom sentence_transformers import losses\n\n# Define the loss function. In soft-max loss, we will also need to explicitly set the number of labels.\ntrain_loss = losses.SoftmaxLoss(model=model, sentence_embedding_dimension=model.get_sentence_embedding_dimension(), num_labels=3)\n```", "```py\n# Train our model for a single epoch\nmodel.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100, show_progress_bar=True)\n```", "```py\nimport datasets\nsts = datasets.load_dataset('glue', 'stsb', split='validation')\n\n# Make sure every value is between 0 and 1\nsts = sts.map(lambda x: {'label': x['label'] / 5.0})\n\n# Process the data to be used from sentence_transformers\nsamples = [InputExample(texts=[sample['sentence1'], sample['sentence2']],\n                        label=sample['label']) for sample in sts]\n```", "```py\nfrom sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n\n# Create an embedding similarity evaluator for stsb\nevaluator = EmbeddingSimilarityEvaluator.from_input_examples(samples)\n```", "```py\n>>> # Evaluate the original model\n>>> orig_model = SentenceTransformer('bert-base-uncased')\n>>> print(\"Baseline: \", evaluator(orig_model))\n>>> \n>>> # Evaluate our trained model\n>>> print(\"Trained model: \", evaluator(model))\n\n\"Baseline: 0.6146254081453191\"\n\"Trained model: 0.7376971430125273\"\n```", "```py\nfrom mteb import MTEB\n\n# Choose evaluation task\nevaluation = MTEB(tasks=[\"Banking77Classification\"])\n\n# Calculate results\nresults = evaluation.run(model)\n```", "```py\n>>> results\n\n{'Banking77Classification': {'mteb_version': '1.0.2',\n  'dataset_revision': '0fd18e25b25c072e09e0d92ab615fda904d66300',\n  'mteb_dataset_name': 'Banking77Classification',\n  'test': {'accuracy': 0.7825324675324674,\n   'f1': 0.782082703333302,\n   'accuracy_stderr': 0.010099229383338676,\n   'f1_stderr': 0.010381981136492737,\n   'main_score': 0.7825324675324674,\n   'evaluation_time': 23.44}}}\n```", "```py\n# Prepare data\nsts = datasets.load_dataset('glue', 'stsb', split='train')\nsts = sts.map(lambda x: {'label': x['label'] / 5.0})\ntrain_examples = [InputExample(texts=[sample['sentence1'], sample['sentence2']],\n                               label=sample['label']) for sample in sts]\ntrain_dataloader = DataLoader(train_examples, shuffle=True, batch_size=32)\n\n# Define model\nword_embedding_model = models.Transformer('bert-base-uncased', max_seq_length=256)\npooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\ncosine_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n\n# Loss function\ntrain_loss = losses.CosineSimilarityLoss(model=cosine_model)\n\n# Fit model\ncosine_model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100, show_progress_bar=True)\n```", "```py\n>>> # Evaluate trained model with MNR loss\n>>> print(\"Trained model + Cosine Similarity Loss: \", evaluator(mnr_model))\n\n\"Trained model + Cosine Similarity Loss: 0.848027994316\"\n```", "```py\n# Prepare data and only keep positive pairs\ndataset = load_dataset(\"glue\", \"mnli\", split=\"train\")\ndataset = dataset.filter(lambda x: True if x['label'] == 0 else False)\ntrain_examples = [InputExample(texts=[row[\"premise\"], row[\"hypothesis\"]], label=row[\"label\"]) for row in tqdm(dataset)]\ntrain_dataloader = DataLoader(train_examples, shuffle=True, batch_size=32)\n\n# Define model\nword_embedding_model = models.Transformer('bert-base-uncased', max_seq_length=256)\npooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\nmnr_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n\n# Loss function\ntrain_loss = losses.MultipleNegativesRankingLoss(model=mnr_model)\n\n# Fit model\nmnr_model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100, show_progress_bar=True)\n```", "```py\n>>> # Evaluate trained model with MNR loss\n>>> print(\"Trained model + MNR Loss: \", evaluator(mnr_model))\n\n\"Trained model + MNR Loss: 0.8183052945831789\"\n```", "```py\nfrom sentence_transformers import SentenceTransformer\n\n# Prepare data and only keep positive pairs\ndataset = load_dataset(\"glue\", \"mnli\", split=\"train\")\ndataset = dataset.filter(lambda x: True if x['label'] == 0 else False)\ntrain_examples = [InputExample(texts=[row[\"premise\"], row[\"hypothesis\"]], label=row[\"label\"]) for row in tqdm(dataset)]\ntrain_dataloader = DataLoader(train_examples, shuffle=True, batch_size=32)\n\n# Load a pre-trained model\nmodel = SentenceTransformer('all-mpnet-base-v2')\n\n# Loss function\ntrain_loss = losses.MultipleNegativesRankingLoss(model=model)\n\n# Fine-tune our model for a single epoch\nmodel.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100, show_progress_bar=True)\n```", "```py\n# Make sure every value is between 0 and 1\nsts = datasets.load_dataset('glue', 'stsb', split='train')\nsts = sts.map(lambda x: {'label': x['label'] / 5.0})\n\n# Process the data to be used in sentence_transformers\ngold_examples = [InputExample(texts=[sample['sentence1'], sample['sentence2']], label=sample['label']) for sample in sts]\ngold_dataloader = DataLoader(gold_examples, shuffle=True, batch_size=32)\n\n# Fully labeled gold dataset\ngold = pd.DataFrame({'sentence1': sts['sentence1'], 'sentence2': sts['sentence2'], 'label': sts['label']})\n```", "```py\nfrom sentence_transformers.cross_encoder import CrossEncoder\n\n# Train a cross-encoder on the gold dataset\ncross_encoder = CrossEncoder('bert-base-uncased', num_labels=1)\ncross_encoder.fit(train_dataloader=gold_dataloader, epochs=1, warmup_steps=300)\n```", "```py\n# Prepare unlabeled to-be silver dataset\nsilver = pd.DataFrame(columns=[\"sentence1\", \"sentence2\"])\nfor sentence in gold.sentence1:\n      sampled = gold[gold['sentence1'] != sentence].sample(10, random_state=42)\n      sampled.sentence1 = sentence\n      silver = pd.concat([silver, sampled], ignore_index=True, axis=0)\nsilver = silver.drop_duplicates()\n```", "```py\n# Predict labels for the unlabeled silver data\npairs = list(zip(silver['sentence1'], silver['sentence2']))\nsilver['label'] = cross_encoder.predict(pairs)\n```", "```py\n# Combine gold + silver\ndata = pd.concat([gold, silver], ignore_index=True, axis=0)\ndata = data.drop_duplicates(subset=['sentence1', 'sentence2'], keep=\"first\")\n\n# initialize dataloader\nexamples = [InputExample(texts=[sample['sentence1'], sample['sentence2']], label=sample['label']) for _, sample in data.iterrows()]\ndataloader = DataLoader(examples, shuffle=True, batch_size=32)\n\n# Initialize bi-encoder\nword_embedding_model = models.Transformer('bert-base-uncased', max_seq_length=256)\npooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\nmodel = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n\n# Loss function\nloss = losses.CosineSimilarityLoss(model=model)\n\n# Fine-tune our model for a single epoch\nmodel.fit(train_objectives=[(dataloader, loss)], epochs=1, warmup_steps=200, show_progress_bar=True)\n```", "```py\n# Create your embedding model\nword_embedding_model = models.Transformer('bert-base-uncased')\npooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), 'cls')\nmodel = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n```", "```py\nfrom sentence_transformers.datasets import DenoisingAutoEncoderDataset\n\n# Extract training data, we ignore the labels that we have\ntrain_data = datasets.load_dataset('glue', 'stsb', split='validation')[\"sentence1\"]\ntrain_data = train_data[\"sentence1\"] + train_data[\"sentence2\"]\n\n# Add noise to our input data\ntrain_dataset = DenoisingAutoEncoderDataset(list(set(train_data)))\ntrain_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n\n# Use the denoising auto-encoder loss\ntrain_loss = losses.DenoisingAutoEncoderLoss(model, 'bert-base-uncased', tie_encoder_decoder=True)\n```", "```py\n# Train our embedding model\nmodel.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, show_progress_bar=True)\n\n# Evaluate\nevaluator(model)\n```", "```py\nimport json\n\n# Convert training data to the right format for GPL\ntrain_data = datasets.load_dataset('glue', 'stsb', split='validation')[\"sentence1\"]\nwith open('corpus.jsonl', 'w') as jsonl:\n    for index, sentence in enumerate(train_data):\n        line = {'_id': str(index), 'title': \"\", 'text': sentence, 'metadata': \"\"}\n        jsonl.write(json.dumps(line)+'\\n')\n```", "```py\nimport gpl\n\n# Train an embedding model with GPL\ngpl.train(\n    path_to_generated_data=\".\",\n    batch_size_gpl=8,\n    batch_size_generation=8,\n    gpl_steps=10000,\n    queries_per_passage=1,\n    output_dir=\"output\",\n    do_evaluation=False,\n    use_amp=True\n\n    # The model that we will fine-tune as our embedding model\n    base_ckpt=\"distilbert-base-uncased\",  \n\n    # The model used to generate queries\n    generator=\"BeIR/query-gen-msmarco-t5-base-v1\",\n\n    # The model used for mining negatives\n    retrievers=[\"msmarco-distilbert-base-v3\", \"msmarco-MiniLM-L-6-v3\"],\n\n    # The model  used for rating query/passage pairs\n    cross_encoder=\"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n)\n```", "```py\n>>> # Load our new model and evaluate it\n>>> model = SentenceTransformer('output')\n>>> evaluator(model)\n\n0.8246360833250379\n```"]