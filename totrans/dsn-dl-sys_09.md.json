["```py\n# define workflow DAG\nwith DAG( \n  description='Vena’s sample training workflow',\n  schedule_interval=timedelta(months=1),\n  start_date=datetime(2022, 1, 1),\n  ) as dag:                                       ❶\n\n # define execution logic for each step\n  data_parse_step = BashOperator( .. .. ..)\n  data_augment_step = BashOperator( .. .. ..)     ❷\n  dataset_building_step = BashOperator( .. .. ..)\n  training_step = BashOperator( .. .. ..)\n\n # Declares step dependencies\n  data_parse_step >> data_augment_step            ❸\n  >> dataset_building_step >> training_step       ❸\n```", "```py\n# declare the workflow DAG. \nwith DAG(dag_id=\"data_process_dag\",\n        schedule_interval=\"@daily\",\n        default_args=default_args,\n        template_searchpath=[f\"{os.environ['AIRFLOW_HOME']}\"],\n        catchup=False) as dag:\n\n # define tasks of the workflow, each code section below is a task \n\n   is_new_data_available = FileSensor(       ❶\n       task_id=\"is_new_data_available\",\n       fs_conn_id=\"data_path\",\n       filepath=\"data.csv\",\n       .. .. ..\n   )\n\n # define data transformation task\n   transform_data = PythonOperator( \n       task_id=\"transform_data\",\n       python_callable=transform_data        ❷\n   )\n\n # define table creation task\n   create_table = PostgresOperator(          ❸\n       task_id=\"create_table\",\n       sql='''CREATE TABLE IF NOT EXISTS invoices (\n               .. .. ..\n               );''',\n       postgres_conn_id='postgres',\n       database='customer_data'\n   )\n\n   save_into_db = PythonOperator(\n       task_id='save_into_db',\n       python_callable=store_in_db\n   )\n\n   notify_data_science_team = SlackWebhookOperator(\n       task_id='notify_data_science_team',\n       http_conn_id='slack_conn',\n       webhook_token=slack_token,\n       message=\"Data Science Notification \\n\"\n       .. .. ..\n   )\n\n# Step two, declare task dependencies in the workflow\n  is_new_data_available >> transform_data\n  transform_data >> create_table >> save_into_db\n  save_into_db >> notify_data_science_team\n  save_into_db >> create_report\n\n# The actual data transformation logic, which is referenced\n# in the “transform_data” task.\ndef transform_data(*args, **kwargs):\n   .. .. ..\n```", "```py\nairflow dags list                             ❶\n\nairflow tasks list data_process_dag           ❷\n\nairflow tasks list data_process_dag --tree    ❸\n```", "```py\napiVersion: argoproj.io/v1alpha1\nkind: Workflow                                         ❶\nmetadata:\n generateName: data-processing-  \nspec:\n entrypoint: argo-steps-workflow-example \n templates:\n   - name: argo-steps-workflow-example\n     Steps:                                            ❷\n       - - name: check-new-data\n           template: data-checker                      ❸\n       - - name: transform-data\n           template: data-converter\n           arguments:\n             artifacts:\n               - name: data-paths                      ❹\n                 from: \"{{steps.check-new-data.outputs. \n                          artifacts.new-data-paths}}\"  ❺\n       - - name: save-into-db\n           template: postgres-operator\n       - - name: notify-data-science-team\n           template: slack-messenger\n\n   - name: data-checker                                ❻\n     container:\n       image: docker/data-checker:latest\n       command: [scan, /datastore/ds/]\n     outputs:\n       artifacts:\n         - name: new-data-paths                        ❼\n           path: /tmp/data_paths.txt\n\n   - name: data-converter\n     inputs:\n       artifacts:\n         - name: data_paths                            ❽\n           path: /tmp/raw_data/data_paths.txt\n     container:\n       image: docker/data-checker:latest\n       command: [data_converter, /tmp/raw_data/data_paths.txt]\n\n   - name: save-into-db\n     .. .. ..\n   - name: notify-data-science-team\n     .. .. ..\n```", "```py\n# list all the workflows\nargo list -n argo\n\n# get details of a workflow run\nargo get -n argo {workflow_name}\n```", "```py\n# list all argo customer resource definitions \nkubectl get crd -n argo\n\n# list all workflows\nkubectl get workflows -n argo\n\n# check specific workflow\nkubectl describe workflow/{workflow_name} -n argo\n```", "```py\n# define workflow DAG in a python class\nclass DataProcessWorkflow(FlowSpec):\n\n  # define \"data source\" as an input parameter for the workflow \n  data_source = Parameter(\n     \"datasource_path\", help=\"the data storage location for data process\"\n     , required=True\n  )\n\n  @step\n  def start(self):\n    # The parameter “self.data_source” are available in all steps.\n    self.newdata_path = dataUtil.fetch_new_data(self.data_source)\n\n    self.next(self.transform_data)\n\n  @step\n  def transform_data(self):\n    self.new_data = dataUtil.convert(self.newdata_path)\n\n    # fan out to two parallel branches after data transfer.\n    self.next(self.save_to_db, self.notify_data_science_team)\n\n  @step\n  def save_to_db(self):\n    dataUtil.store_data(self.new_data)\n    self.next(self.join)\n\n  @step\n  def notify_data_science_team(self):\n    slackUtil.send_notification(messageUtil.build_message(self.new_data))\n\n    self.next(self.join)\n\n  # join the two parallel branches steps: \n  # notify_data_science_team and save_to_db \n  @step\n  def join(self, inputs):\n\n    self.next(self.end)\n\n  @step\n  def end(self, inputs):\n    # end the flow.\n    pass\n\nif __name__ == \"__main__\":\n  DataProcessWorkflow()\n```", "```py\n# display workflow DAG\npython data_process_workflow.py show\n\n# run the workflow locally\npython data_process_workflow.py run\n```", "```py\n# push the workflow from local to AWS step functions\npython data_process_workflow.py --with retry step-functions create\n\n# push the workflow from local to Argo workflows\npython data_process_workflow.py --with retry argo-workflows create\n```"]