- en: Part 2\. Making decisions with Bayesian optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The GP is only one part of the equation. To fully realize the BayesOpt technique,
    we need the second part of the equation: decision-making policies that dictate
    how function evaluations should be carried out to optimize the objective function
    as quickly as possible. This part enumerates the most popular BayesOpt policies,
    including their motivations, mathematical intuitions, and implementations. While
    different policies are motivated by different objectives, they all aim to balance
    the tradeoff between exploration and exploitation—a core challenge in BayesOpt,
    specifically, and decision-making under uncertainty problems, more generally.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Chapter 4 kicks things off by introducing the idea of an acquisition score
    as a way to quantify the value of making a function evaluation. The chapter also
    describes the heuristic of seeking to improve from the best point we have seen
    so far, which leads to two popular BayesOpt policies: Probability of Improvement
    and Expected Improvement.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Chapter 5 connects BayesOpt with a closely related problem: multi-armed bandits.
    We explore the popular Upper Confidence Bound policy, which uses the optimism
    under uncertainty heuristic, and the Thompson sampling policy, which uses the
    probabilistic nature of the GP to aid decision-making.'
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 6 introduces us to information theory, a subfield of mathematics that
    has many applications in decision-making problems. Using a central concept of
    information theory called entropy, we design a BayesOpt policy that seeks to gain
    the most information about our search target.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this part, we learn about the different ways to address the exploration–exploitation
    tradeoff and build a diverse tool set of optimization methods. While we have learned
    to use GPyTorch to implement GPs in the previous part, the premiere BayesOpt library,
    BoTorch, is our focus in this part. We learn how to declare BayesOpt policies,
    use them to facilitate an optimization loop, and compare their performance across
    a wide range of tasks. By the end of this part, you will obtain hands-on knowledge
    regarding implementing and running BayesOpt policies.
  prefs: []
  type: TYPE_NORMAL
