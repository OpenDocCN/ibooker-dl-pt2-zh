- en: Chapter 8\. Underfitting, overfitting, and the universal workflow of machine
    learning
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第八章。欠拟合、过拟合和机器学习的通用工作流程
- en: '*This chapter covers*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章内容*'
- en: Why it is important to visualize the model-training process and what the important
    things are to look for
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么可视化模型训练过程很重要，以及要注意的重要事项
- en: How to visualize and understand underfitting and overfitting
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何可视化和理解欠拟合和过拟合
- en: 'The primary way of dealing with overfitting: regularization, and how to visualize
    its effect'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理过拟合的主要方式：正则化，以及如何可视化其效果
- en: What the universal workflow of machine learning is, what steps it includes,
    and why it is an important recipe that guides all supervised machine-learning
    tasks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习的通用工作流程是什么，包括哪些步骤，以及为什么它是指导所有监督式机器学习任务的重要配方
- en: In the previous chapter, you learned how to use tfjs-vis to visualize data before
    you start designing and training machine-learning models for it. This chapter
    will start where that one left off and describe how tfjs-vis can be used to visualize
    the structure and metrics of models during their training. The most important
    goal in doing so is to spot the all-important phenomena of *underfitting* and
    *overfitting*. Once we can spot them, we’ll delve into how to remedy them and
    how to verify that our remedying approaches are working using visualization.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，您学习了如何使用 tfjs-vis 在开始设计和训练机器学习模型之前可视化数据。本章将从那一章结束的地方开始，并描述 tfjs-vis 如何用于在模型训练过程中可视化模型的结构和指标。这样做的最重要目标是发现
    *欠拟合* 和 *过拟合* 这两个至关重要的现象。一旦我们能够发现它们，我们将深入研究如何解决它们以及如何使用可视化验证我们的解决方法是否有效。
- en: 8.1\. Formulation of the temperature-prediction problem
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1。温度预测问题的制定
- en: To demonstrate underfitting and overfitting, we need a concrete machine-learning
    problem. The problem we’ll use is predicting temperature based on the Jena-weather
    dataset you’ve just seen in the previous chapter. [Section 7.1](kindle_split_019.html#ch07lev1sec1)
    showed the power of visualizing data in the browser and the benefits of doing
    so using the Jena-weather dataset. Hopefully, you’ve formed an intuition of the
    dataset through playing with the visualization UI in the previous section. We
    are now ready to start applying some machine learning to the dataset. But first,
    we need to define the problem.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示欠拟合和过拟合，我们需要一个具体的机器学习问题。我们将使用的问题是根据您在上一章中刚刚看到的 Jena-weather 数据集来预测温度。[第
    7.1 节](kindle_split_019.html#ch07lev1sec1)展示了在浏览器中可视化数据的威力以及使用 Jena-weather 数据集进行此操作的好处。希望您通过在前一节中玩弄可视化
    UI 来形成对数据集的直觉。我们现在准备好开始对数据集应用一些机器学习了。但首先，我们需要定义问题。
- en: The prediction task can be thought of as a toy weather-forecast problem. What
    we are trying to predict is the temperature 24 hours after a certain moment in
    time. We try to make this prediction using the 14 types of weather measurements
    taken in the 10-day period leading up to that moment.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 预测任务可以被看作是一个玩具天气预报问题。我们试图预测的是在某一时刻之后 24 小时的温度。我们试图使用在此前 10 天内进行的 14 种天气测量来进行此预测。
- en: Although the problem definition is straightforward, the way we generate the
    training data from the CSV file requires some careful explanation because it is
    different from the data-generation procedures in the problems seen in this book
    so far. In those problems, every row in the raw data file corresponded to a training
    example. That was how the iris-flower, Boston-housing, and phishing-detection
    examples worked (see [chapters 2](kindle_split_013.html#ch02) and [3](kindle_split_014.html#ch03)).
    However, in this problem, each example is formed by sampling and combining multiple
    rows from the CSV file. This is because a temperature prediction is made not just
    by looking at one moment in time, but instead by looking at the data over a time
    span. See [figure 8.1](#ch08fig01) for a schematic illustration of the example-generation
    process.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然问题定义很简单，但我们从 CSV 文件生成训练数据的方式需要进行一些仔细的解释，因为它与此前在本书中看到的问题的数据生成过程有所不同。在那些问题中，原始数据文件中的每一行都对应一个训练样例。这就是鸢尾花、波士顿房价和钓鱼检测示例的工作方式（见[第
    2 章](kindle_split_013.html#ch02)和[第 3 章](kindle_split_014.html#ch03)）。然而，在这个问题中，每个示例是通过从
    CSV 文件中对多行进行采样和组合而形成的。这是因为温度预测不仅仅是通过查看某一时刻的数据来进行的，而是通过查看一段时间内的数据来进行的。请参见[图 8.1](#ch08fig01)以了解示例生成过程的示意图。
- en: 'Figure 8.1\. Schematic diagram showing how a single training example is generated
    from the tabular data. To generate the feature tensor of the example, the CSV
    file is sampled every `step` rows (for example, `step = 6`) up to `timeSteps`
    such rows (for example, `timeSteps = 240`). This forms a tensor of shape `[timeSteps,
    numFeatures]`, where `numFeatures` (default: 14) is the number of feature columns
    in the CSV file. To generate the target, sample the temperature (`T`) value at
    the row delay (for example, 144) step after the last row that went into the feature
    tensor. Other examples can be generated by starting from a different row in the
    CSV file, but they follow the same rule. This forms the temperature-prediction
    problem: given the 14 weather measurements for a certain period of time (such
    as 10 days) until now, predict the temperature a certain delay (such as 24 hours)
    from now. The code that does what’s shown in this diagram is in the `getNextBatchFunction()`
    function in jena-weather/data.js.'
  id: totrans-11
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.1\. 示意图显示了如何从表格数据中生成单个训练样本。为了生成示例的特征张量，从 CSV 文件中每隔`step`行采样一次（例如，`step =
    6`），直到采样到`timeSteps`行为止（例如，`timeSteps = 240`）。这形成了一个形状为`[timeSteps, numFeatures]`的张量，其中`numFeatures`（默认为14）是
    CSV 文件中特征列的数量。为了生成目标，从进入特征张量的最后一行后延迟（例如，144）步采样温度（`T`）值。可以通过从 CSV 文件的不同行开始来生成其他示例，但它们遵循相同的规则。这构成了温度预测问题：给定某一段时间（例如，10天）内的
    14 个天气测量值，预测从现在开始的一定延迟（例如，24小时）内的温度。在`jena-weather/data.js`中的`getNextBatchFunction()`函数中实现了此图中所示的代码。
- en: '![](08fig01_alt.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](08fig01_alt.jpg)'
- en: To generate the features of a training example, we sample a set of rows over
    a time span of 10 days. Instead of using all the data rows from the 10 days, we
    sample every sixth row. Why? For two reasons. First, sampling all the rows would
    give us six times as much data and lead to a bigger model size and longer training
    time. Second, the data at a time scale of 1 hour has a lot of redundancy (the
    air pressure from 6 hours ago is usually close to that from 6 hours and 10 minutes
    ago). By throwing away five-sixths of the data, we get a more lightweight and
    performant model without sacrificing much predictive power. The sampled rows are
    combined into a 2D feature tensor of shape `[timeSteps,` `numFeatures]` for our
    training example (see [figure 8.1](#ch08fig01)). By default, `timeSteps` has a
    value of 240, which corresponds to the 240 sampling times evenly distributed across
    the 10-day period. `numFeatures` is 14, which corresponds to the 14 weather-instrument
    readings available in the CSV dataset.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成训练示例的特征，我们在 10 天的时间跨度内对一组行进行采样。我们不使用这 10 天内的所有数据行，而是每隔六行进行一次采样。为什么？有两个原因。首先，对所有行进行采样会给我们带来六倍的数据，并导致更大的模型大小和更长的训练时间。其次，以
    1 小时为时间尺度的数据存在很多冗余性（6 小时前的气压通常接近于 6 小时零 10 分钟前的气压）。通过丢弃五分之一的数据，我们可以获得一个更轻量级和性能更好的模型，而不会牺牲太多的预测能力。采样的行被合并成了一个
    2D 特征张量，形状为`[timeSteps, numFeatures]`，用于我们的训练示例（参见[图 8.1](#ch08fig01)）。默认情况下，`timeSteps`的值为
    240，对应于在 10 天期间均匀分布的 240 个采样时间。`numFeatures`为 14，对应于 CSV 数据集中可用的 14 个气象仪读数。
- en: 'Getting the target for the training example is easier: we just move forward
    a certain time delay from the last row that goes into the feature tensor and extract
    the value from the temperature column. [Figure 8.1](#ch08fig01) shows how only
    a single training example is generated. To generate multiple training examples,
    we simply start from different rows of the CSV file.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 获取训练示例的目标更容易：我们只需从进入特征张量的最后一行向前移动一定的时间延迟，并从温度列中提取值。[图 8.1](#ch08fig01)显示了仅生成单个训练示例的方式。要生成多个训练示例，我们只需从
    CSV 文件的不同行开始。
- en: 'You may have noticed something peculiar about the feature tensor for our temperature-prediction
    problem (see [figure 8.1](#ch08fig01)): in all the previous problems, the feature
    tensor of a single example was 1D, which led to a 2D tensor when multiple examples
    were batched. However, in this problem, the feature tensor of a single example
    is already 2D, which means that we’ll get a 3D tensor (of shape `[batchSize, timeSteps,
    numFeatures]`) when we combine multiple examples into a batch. This is an astute
    observation! The 2D feature-tensor shape originates from the fact that the features
    come from a *sequence* of events. In particular, they are the weather measurements
    taken at 240 points in time. This distinguishes this problem from all the other
    problems you’ve seen so far, in which the input features for a given example do
    not span multiple moments in time, be it the flower size measurements in the iris-flower
    problem or the 28 × 28 pixel values of an MNIST image.^([[1](#ch08fn1)])'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到我们温度预测问题的特征张量（参见[图8.1](#ch08fig01)）有些奇怪：在所有以前的问题中，单个示例的特征张量是1D的，当多个示例被批处理时，会得到2D张量。然而，在这个问题中，单个示例的特征张量已经是2D的，这意味着当我们将多个示例组合成批处理时，我们将获得一个3D张量（形状为`[batchSize,
    timeSteps, numFeatures]`）。这是一个敏锐的观察！2D特征张量形状源于特征来自一系列事件的事实。特别是，它们是在240个时间点上采集的天气测量值。这将此问题与到目前为止您所看到的所有其他问题区分开来，其中给定示例的输入特征不涵盖多个时间点，无论是鸢尾花问题中的花大小测量还是MNIST图像中的28×28像素值。[^1]
- en: ¹
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The speech-command recognition problem in [chapter 4](kindle_split_015.html#ch04)
    did, in fact, involve a sequence of events: namely, the successive frames of audio
    spectra that formed the spectrogram. However, our methodology treated the entire
    spectrogram as an image, thereby ignoring the temporal dimension of the problem
    by treating it as a spatial dimension.'
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在[第4章](kindle_split_015.html#ch04)的语音命令识别问题实际上涉及到一系列事件：即形成频谱图的连续音频帧。然而，我们的方法论将整个频谱图视为图像，从而通过将其视为空间维度来忽略了问题的时间维度。
- en: 'This is the first time you encounter sequential input data in this book. In
    the next chapter, we will dive deeper into how to build specialized and more powerful
    models (RNNs) for sequential data in TensorFlow.js. But here, we will approach
    the problem using two types of models we already know: linear regressors and MLPs.
    This forms a buildup to our study of RNNs and gives us a baseline that can be
    compared with the more advanced models.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这是本书中你第一次遇到顺序输入数据。在下一章中，我们将深入探讨如何在 TensorFlow.js 中构建专业化和更强大的模型（RNNs）来处理顺序数据。但在这里，我们将使用我们已经了解的两种模型来解决问题：线性回归器和MLPs。这为我们学习RNNs铺平了道路，并为我们提供了可以与更高级模型进行比较的基线。
- en: The actual code that performs the data-generation process illustrated in [figure
    8.1](#ch08fig01) is in jena-weather/data.js, under the function `getNextBatchFunction()`.
    This is an interesting function because instead of returning a concrete value,
    it returns an object with a function called `next()`. The `next()` function returns
    actual data values when it’s called. The object with the `next()` function is
    referred to as an *iterator*. Why do we use this indirection instead of writing
    an iterator directly? First, this conforms to the generator/iterator specification
    of JavaScript.^([[2](#ch08fn2)]) We will soon pass it to the `tf.data .generator()`
    API in order to create a dataset object for model training. The API requires this
    function signature. Second, our iterator needs to be configurable; a function
    that returns the iterator is a good way to enable the configuration.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在 jena-weather/data.js 中实现了[图8.1](#ch08fig01)所示数据生成过程的实际代码，在函数`getNextBatchFunction()`下。这是一个有趣的函数，因为它不是返回一个具体的值，而是返回一个包含名为`next()`的函数的对象。当调用`next()`函数时，它会返回实际的数据值。具有`next()`函数的对象称为*迭代器*。为什么我们使用这种间接方式而不是直接编写迭代器呢？首先，这符合JavaScript的生成器/迭代器规范。[^2]我们将很快将其传递给`tf.data.generator()`API，以便为模型训练创建数据集对象。API需要此函数签名。其次，我们的迭代器需要可配置；返回迭代器的函数是启用配置的一种好方法。
- en: ²
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ²
- en: ''
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “Iterators and Generators,” MDN web docs, [http://mng.bz/RPWK](http://mng.bz/RPWK).
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请参阅“迭代器和生成器”，MDN web 文档，[http://mng.bz/RPWK](http://mng.bz/RPWK)。
- en: 'You can see the possible configuration options from the signature of `getNextBatchFunction()`:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从`getNextBatchFunction()`的签名中看到可能的配置选项：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: There are quite a few configurable parameters. For example, you can use the
    `lookBack` argument to specify how long a period to look back when making a temperature
    prediction. You can also use the `delay` argument to specify how far in the future
    the temperature prediction will be made for. The arguments `minIndex` and `maxIndex`
    allow you to specify the range of rows to draw data from, and so forth.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 有相当多的可配置参数。例如，您可以使用 `lookBack` 参数来指定在进行温度预测时要向后查看多长时间段。您还可以使用 `delay` 参数来指定温度预测将来要做出的时间。`minIndex`
    和 `maxIndex` 参数允许您指定要从中提取数据的行范围等。
- en: We convert the `getNextBatchFunction()` function into a `tf.data.Dataset` object
    by passing it to the `tf.data.generator()` function. As we described in [chapter
    6](kindle_split_018.html#ch06), a `tf.data.Dataset` object, when used in conjunction
    with the `fitDataset()` method of a `tf.Model` object, enables us to train the
    model even if the data is too large to fit into WebGL memory (or any applicable
    backing memory type) as a whole. The `Dataset` object will create a batch of training
    data on the GPU only when it is about to go into the training. This is exactly
    what we do for the temperature-prediction problem here. In fact, we wouldn’t be
    able to train the model using the model’s ordinary `fit()` method due to the large
    number and size of the examples. The `fitDataset()` call can be found in jena-weather/models.js
    and looks like the following listing.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过将 `getNextBatchFunction()` 函数传递给 `tf.data.generator()` 函数，将其转换为 `tf.data.Dataset`
    对象。正如我们在[第 6 章](kindle_split_018.html#ch06)中所描述的，当与 `tf.Model` 对象的 `fitDataset()`
    方法一起使用时，`tf.data.Dataset` 对象能够使我们即使数据过大而无法一次性装入 WebGL 内存（或任何适用的后备内存类型）也能训练模型。`Dataset`
    对象将仅当即将进入训练时才在 GPU 上创建批量训练数据。这正是我们在这里为温度预测问题所做的。实际上，由于示例的数量和大小过大，我们无法使用普通的 `fit()`
    方法来训练模型。`fitDataset()` 调用可以在 jena-weather/models.js 中找到，看起来像以下列表。
- en: Listing 8.1\. Visualizing the `fitDataset`-based model training with tfjs-vis
  id: totrans-28
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.1。使用 tfjs-vis 对基于 `fitDataset` 的模型进行可视化训练
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '***1*** The first Dataset object will generate the training data.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 第一个 Dataset 对象将生成训练数据。'
- en: '***2*** The second Dataset object will generate the validation data.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 第二个 Dataset 对象将生成验证数据。'
- en: '***3*** The validationData config for fitDataset() accepts either a Dataset
    object or a set of tensors. Here, the first option is used.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 用于 `fitDataset()` 的 validationData 配置可以接受 Dataset 对象或一组张量。这里使用了第一个选项。'
- en: 'The first two fields of the configuration object for `fitDataset()` specify
    how many epochs to train the model for and how many batches to draw for every
    epoch. As you learned in [chapter 6](kindle_split_018.html#ch06), they are the
    standard configuration fields for a `fitDataset()` call. However, the third field
    (`callbacks: customCallback`) is something new. It is how we visualize the training
    process. Our `customCallback` takes different values depending on whether the
    model training occurs in the browser or, as we’ll see in the next chapter, in
    Node.js.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '`fitDataset()`的配置对象的前两个字段指定了模型训练的时期数量和每个时期抽取的批次数量。正如您在[第 6 章](kindle_split_018.html#ch06)中学到的那样，它们是
    `fitDataset()` 调用的标准配置字段。然而，第三个字段 (`callbacks: customCallback`) 是新内容。这是我们可视化训练过程的方式。我们的
    `customCallback` 根据模型训练是在浏览器中进行还是（正如我们将在下一章中看到的）在 Node.js 中进行，而取不同的值。'
- en: 'In the browser, the function `tfvis.show.fitCallbacks()` provides the value
    of `customCallback`. The function helps us visualize the model training in the
    web page with just one line of JavaScript code. It not only saves us all the work
    of accessing and keeping track of batch-by-batch and epoch-by-epoch loss and metric
    values, but it also removes the need to manually create and maintain the HTML
    elements in which the plots will be rendered:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在浏览器中，`tfvis.show.fitCallbacks()` 函数提供 `customCallback` 的值。该函数帮助我们通过只需一行 JavaScript
    代码在网页中可视化模型训练。它不仅省去了我们访问并跟踪逐批次和逐时期的损失和指标值的所有工作，而且也消除了手动创建和维护将呈现图表的 HTML 元素的需要：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The first argument to `fitCallbacks()` specifies a rendering area created with
    the `tfvis.visor().surface()` method. It is called a *visor surface* in the terminology
    of tfjs-vis. A visor is a container that helps you conveniently organize all the
    visualization related to your in-browser machine-learning tasks. Structurally,
    a visor is organized on two levels of hierarchy. At the higher level, there can
    be one or more tabs that the user can navigate using clicks. At the lower level,
    every tab contains one or more *surfaces*. The `tfvis.visor().surface()` method,
    with its `tab` and `name` configuration fields, lets you create a surface in a
    designated visor tab with a designated name. A visor surface is not limited to
    rendering loss and metric curves. In fact, all the basic charts we showed with
    the CodePen example in [section 7.1](kindle_split_019.html#ch07lev1sec1) can be
    rendered on visor surfaces. We leave this as an exercise for you at the end of
    this chapter.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`fitCallbacks()`的第一个参数指定了一个由`tfvis.visor().surface()`方法创建的渲染区域，这在tfjs-vis的术语中被称为*visor
    surface*。Visor是一个容器，可以帮助你方便地组织所有与浏览器机器学习任务相关的可视化内容。在结构上，Visor有两个层次的层次结构。在较高的层次上，用户可以使用点击来导航一个或多个选项卡。在较低的级别上，每个选项卡都包含一个或多个*surfaces*。`tfvis.visor().surface()`方法通过其`tab`和`name`配置字段，允许你在指定的Visor选项卡上以指定的名称创建一个表面。Visor
    surface不仅限于渲染损失和度量曲线。实际上，我们在[第7.1节](kindle_split_019.html#ch07lev1sec1)的CodePen示例中展示的所有基本图表都可以渲染在visor
    surfaces上。我们将在本章末尾留下这个问题作为练习。'
- en: The second argument for `fitCallbacks()` specifies what losses and metrics will
    be rendered in the visor surface. In this case, we plot the loss from the training
    and validation datasets. The third argument contains a field that controls the
    frequency at which the plots are updated. By using both `onBatchEnd` and `onEpochEnd`,
    we will get updates at the end of every batch and every epoch. In the next section,
    we will examine the loss curves created by `fitCallbacks()` and use them to spot
    underfitting and overfitting.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`fitCallbacks()`的第二个参数指定了在visor surface上渲染的损失和度量。在这种情况下，我们绘制了训练和验证数据集的损失。第三个参数包含一个字段，控制绘图更新的频率。通过同时使用`onBatchEnd`和`onEpochEnd`，我们将在每个批次和每个epoch结束时获得更新。在下一节中，我们将检查`fitCallbacks()`创建的损失曲线，并使用它们来发现欠拟合和过拟合。'
- en: 8.2\. Underfitting, overfitting, and countermeasures
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2\. 欠拟合、过拟合和对策
- en: During the training of a machine-learning model, we want to monitor how well
    our model is capturing the patterns in the training data. A model that doesn’t
    capture the patterns very well is said to be *underfit*; a model that captures
    the patterns *too* well, to the extent that what it learns generalizes poorly
    to new data, is said to be *overfit*. An overfit model can be brought back on
    track through countermeasures such as regularization. In this section, we’ll show
    how visualization can help us spot these model behaviors and the effects of the
    countermeasures.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练机器学习模型期间，我们希望监控我们的模型在训练数据中捕捉到的模式。一个无法很好地捕捉模式的模型被称为*欠拟合*；一个捕捉模式过于完美，以至于它学到的内容在新数据上泛化能力较差的模型被称为*过拟合*。可以通过正则化等对策来使过拟合的模型恢复正常。在本节中，我们将展示可视化如何帮助我们发现这些模型行为以及对策的影响。
- en: 8.2.1\. Underfitting
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.1\. 欠拟合
- en: 'To solve the temperature-prediction problem, let’s first try the simplest possible
    machine-learning model: a linear regressor. The code in [listing 8.2](#ch08ex02)
    (from jena-weather/index.js) creates such a model. It uses a dense layer with
    a single unit and the default linear activation to generate the prediction. However,
    compared with the linear regressor we built for the download-time prediction problem
    in [chapter 2](kindle_split_013.html#ch02), this model has an extra flatten layer.
    This is because the shape of the feature tensor in this problem is 2D, which must
    be flattened into 1D to meet the requirement of the dense layer used for linear
    regression. This flattening process is illustrated in [figure 8.2](#ch08fig02).
    It is important to note is that this flattening operation discards the information
    about the sequential (temporal) ordering in the data.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 要解决温度预测问题，让我们首先尝试最简单的机器学习模型：线性回归器。[清单 8.2](#ch08ex02)（来自 jena-weather/index.js）中的代码创建了这样一个模型。它使用一个具有单个单位和默认线性激活的密集层来生成预测。然而，与我们在[第2章](kindle_split_013.html#ch02)中为下载时间预测问题构建的线性回归器相比，此模型多了一个展平层。这是因为这个问题中特征张量的形状是2D的，必须被展平为1D，以满足用于线性回归的密集层的要求。这个展平过程在[图
    8.2](#ch08fig02)中有所说明。重要的是要注意，这个展平操作丢弃了关于数据顺序（时间顺序）的信息。
- en: Figure 8.2\. Flattening the 2D feature tensor of shape `[timeSteps, numFeatures]`
    into a 1D tensor of shape `[timeSteps × numFeatures]`, as done by both the linear
    regressor in [listing 8.2](#ch08ex02) and the MLP model in [listing 8.3](#ch08ex03)
  id: totrans-42
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.2\. 将形状为`[timeSteps, numFeatures]`的2D特征张量展平为形状为`[timeSteps × numFeatures]`的1D张量，正如[清单
    8.2](#ch08ex02)中的线性回归器和[清单 8.3](#ch08ex03)中的MLP模型所做的那样
- en: '![](08fig02_alt.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](08fig02_alt.jpg)'
- en: Listing 8.2\. Creating a linear-regression model for the temperature-prediction
    problem
  id: totrans-44
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 8.2\. 为温度预测问题创建一个线性回归模型
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '***1*** Flattens the [batchSize, timeSteps, numFeatures] input shape to [batchSize,
    timeSteps * numFeatures] in order to apply the dense layer'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 将[batchSize, timeSteps, numFeatures]输入形状压平为[batchSize, timeSteps *
    numFeatures]，以应用密集层'
- en: '***2*** A single-unit dense layer with the default (linear) activation is a
    linear regressor.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 带有默认（线性）激活的单单元密集层是一个线性回归器。'
- en: Once the model is constructed, we compile it for training with
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型构建完成，我们就为训练编译它
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here, we use the loss function `meanAbsoluteError` because our problem is predicting
    a continuous value (the normalized temperature). Unlike in some of the previous
    problems, no separate metric is defined, because the MAE loss function itself
    serves as the human-interpretable metric. However, beware that since we are predicting
    the *normalized* temperature, the MAE loss has to be multiplied with the standard
    deviation of the temperature column (8.476 degrees Celsius) to be converted into
    a prediction error in absolute terms. For example, if we get an MAE of 0.5, it
    translates to 8.476 * 0.5 = 4.238 degrees Celsius of prediction error.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们使用损失函数`meanAbsoluteError`，因为我们的问题是预测一个连续值（标准化温度）。与之前的一些问题不同，没有定义单独的度量标准，因为MAE损失函数本身就是人可解释的度量标准。但是，请注意，由于我们正在预测*标准化*温度，MAE损失必须乘以温度列的标准差（8.476摄氏度），以将其转换为绝对误差的预测。例如，如果我们得到的MAE为0.5，那么它就相当于8.476
    * 0.5 = 4.238摄氏度的预测误差。
- en: 'In the demo UI, choose Linear Regression in the Model Type drop-down menu and
    click Train Model to kick off the training of the linear regressor. Right after
    the training starts, you’ll see a tabular summary of the model in a “card” that
    pops up on the right-hand side of the page (see the screenshot in [figure 8.3](#ch08fig03)).
    This model-summary table is somewhat similar to the text output of a `model.summary()`
    call but is rendered graphically in HTML. The code that creates the table is as
    follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在演示界面中，选择模型类型下拉菜单中的线性回归，并单击“训练模型”以开始训练线性回归器。训练开始后，您将立即在页面右侧弹出的“卡片”中看到模型的表格摘要（请参阅[图
    8.3](#ch08fig03)中的屏幕截图）。这个模型摘要表在某种程度上类似于`model.summary()`调用的文本输出，但在HTML中以图形方式呈现。创建表的代码如下：
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Figure 8.3\. The tfjs-vis visor visualizing the training of a linear-regression
    model. Top: a summary table for the model. Bottom: the loss curves over 20 epochs
    of training. This chart is created with `tfvis.show .fitCallbacks()` (see jena-weather/index.js).'
  id: totrans-53
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.3\. tfjs-vis 可视化线性回归模型的训练。上图：模型的摘要表。下图：20次训练时的损失曲线。此图是使用`tfvis.show .fitCallbacks()`创建的（请参阅
    jena-weather/index.js）。
- en: '![](08fig03_alt.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](08fig03_alt.jpg)'
- en: With the surface created, we draw a model-summary table in it by passing the
    surface to `tfvis.show.modelSummary()`, as in the second line of the previous
    code snippet.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 创建了表面后，我们通过将表面传递给`tfvis.show.modelSummary()`来在其中绘制一个模型摘要表，就像前一个代码片段的第二行那样。
- en: Under the Model Summary part of the linear-regression tab is a plot that displays
    the loss curves from the model training ([figure 8.3](#ch08fig03)). It is created
    by the `fitCallbacks()` call that we described in the last section. From the plot,
    we can see how well the linear regressor does on the temperature-prediction problem.
    Both the training and validation losses end up oscillating around 0.9, which corresponds
    to 8.476 * 0.9 = 7.6 degrees Celsius in absolute terms (recall that 8.476 is the
    standard deviation of the temperature column in the CSV file). This means that
    after training, our linear regressor makes a prediction error of 7.6 degrees Celsius
    (or 13.7 degrees Fahrenheit) on average. These predictions are pretty bad. No
    one would want to trust the weather forecast based on this model! This is an example
    of *underfitting*.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性回归选项卡的模型摘要部分下，有一个显示模型训练的损失曲线的图表（[图 8.3](#ch08fig03)）。它是由我们在上一节中描述的 `fitCallbacks()`
    调用创建的。从图中，我们可以看到线性回归器在温度预测问题上的表现如何。训练损失和验证损失最终都在 0.9 左右波动，这对应于绝对值为 8.476 * 0.9
    = 7.6 摄氏度（请记住，8.476 是 CSV 文件中温度列的标准偏差）。这意味着在训练后，我们的线性回归器平均预测误差为 7.6 摄氏度（或 13.7
    华氏度）。这些预测相当糟糕。没有人会想要依靠这个模型进行天气预报！这是一个*欠拟合*的例子。
- en: Underfitting is usually a result of using an insufficient representational capacity
    (power) to model the feature-target relationship. In this example, our linear
    regressor is structurally too simple and hence is underpowered to capture the
    relation between the weather data of the previous 10 days and the temperature
    of the next day. To overcome underfitting, we usually increase the power of the
    model by making it bigger. Typical approaches include adding more layers (with
    nonlinear activations) to the model and increasing the size of the layers (such
    as the number of units in a dense layer). So, let’s add a hidden layer to the
    linear regressor and see how much improvement we can get from the resultant MLP.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 欠拟合通常是由于使用不足的表示能力（功率）来建模特征-目标关系而导致的。在这个例子中，我们的线性回归器结构太简单，因此无法捕获前 10 天的天气数据与第二天温度之间的关系。为了克服欠拟合，我们通常通过使模型更强大来增加模型的功率。典型的方法包括向模型添加更多的层（具有非线性激活）和增加层的大小（例如，在密集层中的单位数）。所以，让我们向线性回归器添加一个隐藏层，看看我们能从结果
    MLP 中获得多少改进。
- en: 8.2.2\. Overfitting
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.2\. 过拟合
- en: The function that creates MLP models is in [listing 8.3](#ch08ex03) (from jena-weather/index.js).
    The MLP it creates includes two dense layers, one as the hidden layer and one
    as the output layer, in addition to a flatten layer that serves the same purpose
    as in the linear-regression model. You can see that the function has two more
    arguments compared to `buildLinearRegressionModel()` in [listing 8.2](#ch08ex02).
    In particular, the `kernelRegularizer` and `dropoutRate` parameters are the ways
    in which we’ll combat overfitting later. For now, let’s see what prediction accuracy
    an MLP that doesn’t use `kernelRegularizer` or `dropoutRate` is capable of achieving.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 MLP 模型的函数位于[列表 8.3](#ch08ex03)（来自 jena-weather/index.js）。它创建的 MLP 包括两个密集层，一个作为隐藏层，一个作为输出层，另外还有一个扁平层，其作用与线性回归模型中的相同。您可以看到，与
    [列表 8.2](#ch08ex02) 中的 `buildLinearRegressionModel()` 相比，该函数有两个额外的参数。特别是，`kernelRegularizer`
    和 `dropoutRate` 参数是我们稍后将用来对抗过拟合的方法。现在，让我们看看一个不使用 `kernelRegularizer` 或 `dropoutRate`
    的 MLP 能够达到什么样的预测准确度。
- en: Listing 8.3\. Creating an MLP for the temperature-prediction problem
  id: totrans-60
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.3\. 为温度预测问题创建 MLP
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '***1*** If specified by the caller, add regularization to the kernel of the
    hidden dense layer.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 如果由调用者指定，则向隐藏的密集层的内核添加正则化。'
- en: '***2*** If specified by the caller, add a dropout layer between the hidden
    dense layer and the output dense layer.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 如果由调用者指定，则在隐藏的密集层和输出密集层之间添加一个 dropout 层。'
- en: 'Panel A of [figure 8.4](#ch08fig04) shows the loss curves from the MLP. Compared
    with the loss curves of the linear regressor, we can see a few important differences:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 8.4](#ch08fig04)的面板 A 显示了 MLP 的损失曲线。与线性回归器的损失曲线相比，我们可以看到一些重要的区别：'
- en: The training and validation loss curves show a divergent pattern. This is different
    from the pattern in [figure 8.3](#ch08fig03), where two loss curves show largely
    consistent trends.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练和验证损失曲线呈现出发散的模式。这与 [图 8.3](#ch08fig03) 中的模式不同，其中两个损失曲线呈现出基本一致的趋势。
- en: The training loss converges toward a much lower error than before. After 20
    epochs of training, the training loss has a value of about 0.2, which corresponds
    to an error of 8.476 * 0.2 = 1.7 degrees Celsius—much better than the result from
    linear regression.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练损失收敛到比之前低得多的错误。经过 20 个周期的训练，训练损失约为 0.2，对应于误差为 8.476 * 0.2 = 1.7 摄氏度——比线性回归的结果要好得多。
- en: However, the validation loss decreases briefly in the first two epochs and then
    starts to go back up slowly. At the end of epoch 20, it has a significantly higher
    value than the training loss (0.35, or about 3 degrees Celsius).
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然而，验证损失在前两个周期内短暂下降，然后开始缓慢上升。到第 20 个周期结束时，它的值明显高于训练损失（0.35，约为 3 摄氏度）。
- en: 'Figure 8.4\. The loss curves from applying two different MLP models on the
    temperature-prediction problem. Panel A: from an MLP model without any regularization.
    Panel B: from an MLP model of the same layer size and count as the model in panel
    A, but with L2 regularization of the dense layers’ kernels. Notice that the y-axis
    ranges differ slightly between the two panels.'
  id: totrans-68
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.4\. 两种不同 MLP 模型在温度预测问题上的损失曲线。面板 A：没有任何正则化的 MLP 模型。面板 B：与面板 A 中模型相同层大小和数量的
    MLP 模型，但是具有密集层核的 L2 正则化。请注意，两个面板之间的 y 轴范围略有不同。
- en: '![](08fig04_alt.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](08fig04_alt.jpg)'
- en: 'The more than four-fold decrease in training loss relative to the previous
    result is due to the fact that our MLP has a higher power than the linear-regression
    model thanks to one more layer and several times more trainable weight parameters.
    However, the increased model power has a side effect: it causes the model to fit
    the training data significantly better than the validation data (data the model
    doesn’t get to see during training). This is an example of *overfitting*. It is
    a case in which a model “pays too much attention” to the irrelevant details in
    the training data, to the extent that the model’s predictions start to generalize
    poorly to unseen data.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 相对于之前的结果，训练损失的四倍以上的减少是由于我们的 MLP 比线性回归模型具有更高的能力，这得益于一个更多的层和几倍于线性回归模型的可训练权重参数。然而，增加的模型能力带来了一个副作用：它导致模型在训练数据上拟合得比验证数据（模型在训练过程中没有看到的数据）显着好。这是*过拟合*的一个例子。这是一种情况，其中模型对训练数据中的不相关细节“过于关注”，以至于模型的预测开始对未见数据的泛化能力变差。
- en: 8.2.3\. Reducing overfitting with weight regularization and visualizing it working
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.3\. 使用权重正则化减少过拟合并可视化其工作
- en: 'In [chapter 4](kindle_split_015.html#ch04), we reduced overfitting in a convnet
    by adding dropout layers to the model. Here, let’s look at another frequently
    used overfitting-reduction approach: adding regularization to weights. In the
    Jena-weather demo UI, if you select the model type MLP with L2 Regularization,
    the underlying code will create an MLP by calling `buildMLPModel()` ([listing
    8.3](#ch08ex03)) in the following manner:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 4 章](kindle_split_015.html#ch04) 中，我们通过向模型添加 dropout 层来减少卷积神经网络的过拟合。在这里，让我们看另一种经常使用的减少过拟合的方法：向权重添加正则化。在
    Jena-weather 演示 UI 中，如果选择具有 L2 正则化的 MLP 模型，底层代码将通过以下方式调用 `buildMLPModel()` 来创建
    MLP（[列表 8.3](#ch08ex03)）：
- en: '[PRE7]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The second argument—the return value of `tf.regularizers.l2()`—is an *L2 regularizer*.
    By plugging the previous code into the `buildMLPModel()` function in [listing
    8.3](#ch08ex03), you can see that the L2 regularizer goes into the `kernelRegularizer`
    of the hidden dense layer’s configuration. This attaches the L2 regularizer to
    the kernel of the dense layer. When a weight (such as the kernel of a dense layer)
    has an attached regularizer, we say that the weight is *regularized*. Similarly,
    when some or all of a model’s weights are regularized, we say the model is regularized.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个参数——`tf.regularizers.l2()` 的返回值——是一个 *L2 正则化器*。通过将上述代码插入 [列表 8.3](#ch08ex03)
    中的 `buildMLPModel()` 函数中，您可以看到 L2 正则化器进入隐藏的密集层配置的 `kernelRegularizer`。这将 L2 正则化器附加到密集层的内核上。当一个权重（例如密集层的内核）有一个附加的正则化器时，我们称该权重是*正则化的*。同样，当模型的一些或全部权重被正则化时，我们称该模型为正则化的。
- en: 'What does the regularizer do to the dense-layer kernel and the MLP that it
    belongs to? It adds an extra term to the loss function. Consider how the loss
    of the unregularized MLP is calculated: it’s defined simply as the MAE between
    the targets and the model’s predictions. In pseudo-code, it can be expressed as'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化器对于稠密层的`kernel`和它所属的MLP有什么作用呢？它会在损失函数中添加一个额外的项。来看看未经正则化的MLP的损失如何计算：它简单地定义为目标和模型预测之间的MAE。伪代码如下：
- en: '[PRE8]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: With a regularized weight, the loss of the model includes an extra term. In
    pseudo-code,
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在加入正则化后，模型的损失函数会包含一个额外的项。伪代码如下：
- en: '[PRE9]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Here, `l2Rate * l2(kernel)` is the extra L2-regularization term of the loss
    function. Unlike the MAE, this term does *not* depend on the model’s predictions.
    Instead, it depends only on the kernel (a weight of the layer) being regularized.
    Given the value of the kernel, it outputs a number associated only with the kernel’s
    values. You can think of the number as a measure of how undesirable the current
    value of the kernel is.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`l2Rate * l2(kernel)`是损失函数中额外的L2正则化项。与MAE不同，这个项不依赖于模型的预测结果，而是仅与被正则化的`kernel`（一层的权重）有关。给定`kernel`的值，它输出一个只与`kernel`的值相关的数值。可以将这个数值看作是当前`kernel`值的不理想程度的度量。
- en: 'Now let’s look at the detailed definition of the L2-regularization function:
    `l2(kernel)`. It calculates the summed squares of all the weight values. For example,
    pretend our kernel has a small shape of `[2, 2]` for the sake of simplicity, and
    suppose its values are `[[0.1, 0.2], [-0.3, -0.4]]`; then,'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下L2正则化函数`l2(kernel)`的详细定义：它计算所有权重值的平方和。举个例子，假设为了简单起见，我们的`kernel`的形状很小，为`[2,
    2]`，其值为`[[0.1, 0.2], [-0.3, -0.4]]`，那么，
- en: '[PRE10]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Therefore, `l2(kernel)` always returns a positive number that penalizes large
    weight values in `kernel`. With the term included in the total loss, it encourages
    all elements of `kernel` to be smaller in absolute value, everything else being
    equal.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，`l2(kernel)`始终返回一个正数，对`kernel`中的大权重值进行惩罚。在总的损失函数中加入这个项，会鼓励`kernel`的所有元素在绝对值上都变得更小，其他条件保持不变。
- en: 'Now the total loss includes two different terms: the target-prediction mismatch
    and a term related to `kernel`’s magnitudes. As a result, the training process
    will try to not only minimize the target-prediction mismatch but also reduce the
    sum of the squares of the kernel’s elements. Oftentimes, the two goals will conflict
    with each other. For example, a reduction in the magnitude of the kernel’s elements
    may reduce the second term but increase the first one (the MSE loss). How does
    the total loss balance the relative importance of the two conflicting terms? That’s
    where the `l2Rate` multiplier comes into play. It quantifies the importance of
    the L2 term relative to the target-prediction-error term. The larger the value
    of `l2Rate`, the more the training process will tend to reduce the L2-regularization
    term at the cost of increased target-prediction error. This term, which defaults
    to `1e-3`, is a hyperparameter whose value can be tuned through hyperparameter
    optimization.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在总的损失函数包含两个不同的项：目标预测不匹配项和与`kernel`大小有关的项。因此，训练过程不仅会尽量减少目标预测不匹配项，还会减少`kernel`元素平方和。通常情况下，这两个目标会相互冲突。例如，减小`kernel`元素大小可能会减小第二个项，但会增加第一个项（均方误差损失）。总的损失函数是如何平衡这两个相互冲突的项的相对重要性的？这就是`l2Rate`乘子发挥作用的地方。它量化了L2项相对于目标预测误差项的重要性。`l2Rate`的值越大，训练过程就越倾向于减少L2正则化项，但会增加目标预测误差。这个参数的默认值是`1e-3`，可以通过超参数优化进行调整。
- en: So how does the L2 regularizer help us? Panel B of [figure 8.4](#ch08fig04)
    shows the loss curves from the regularized MLP. By comparing it with the curves
    from the unregularized MLP (panel A of the same figure), you can see that the
    regularized model yields less divergent training and validation loss curves. This
    means that the model is no longer “paying undue attention” to idiosyncratic patterns
    in the training dataset. Instead, the pattern it learns from the training set
    generalizes well to unseen examples in the validation set. In our regularized
    MLP, only the first dense layer incorporated a regularizer, while the second dense
    layer didn’t. But that turned out to be sufficient to overcome the overfitting
    in this case. In the next section, we will look deeper at why smaller kernel values
    lead to less overfitting.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: L2 正则化如何帮助我们？ 图8.4B 展示了经过正则化的 MLPs 的损失曲线。通过与未经正则化的 MLPs 的曲线（图8.4A）进行比较，您可以看到使用正则化的模型产生了较少的训练和验证损失曲线。这意味着模型不再“过度关注”训练数据集中的偶发模式，而是从训练集中学到的模式可以很好地推广到验证集中看不见的例子。在我们的经过正则化的
    MLPs 中，只有第一个密集层加入了正则化，而第二个密集层没有。但这足以克服这种过拟合情况。在下一节中，我们将更深入地探讨为什么较小的卷积核值会降低过拟合。
- en: Visualizing the effect of regularization on weight values
  id: totrans-85
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 可视化正则化对权值的影响
- en: 'Since the L2 regularizer works by encouraging the kernel of the hidden dense
    layer to have smaller values, we ought to be able to see that the post-training
    kernel values are smaller in the regularized MLP than in the unregularized one.
    How can we do that in TensorFlow.js? The `tfvis.show.layer()` function from tfjs-vis
    makes it possible to visualize a TensorFlow.js model’s weights with one line of
    code. [Listing 8.4](#ch08ex04) is a code excerpt that shows how this is done.
    The code is executed when the training of an MLP model ends. The `tfvis.show.layer()`
    call takes two arguments: the visor surface on which the rendering will happen
    and the layer being rendered.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 L2 正则化通过鼓励隐藏的 dense 层中的卷积核具有更小的值来起作用，因此我们应该看到经过训练后的 MLPs 中，使用正则化的模型的卷积核的值更小。在
    TensorFlow.js 中，我们可以使用 tfjs-vis 库的 `tfvis.show.layer()` 函数实现这一点。代码清单 8.4 展示了如何使用该函数可视化
    TensorFlow.js 模型的权重。该代码在 MLP 模型训练结束时执行。`tfvis.show.layer()` 函数接受两个参数：可视化器上的渲染和要渲染的层。
- en: Listing 8.4\. Visualizing the weight distribution of layers (from jena-weather/index.js)
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 代码清单 8.4。展示层权值分布的可视化代码（来自 jena-weather/index.js）
- en: '[PRE11]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The visualization made by this code is shown in [figure 8.5](#ch08fig05). Panels
    A and B show the results from the unregularized and regularized MLPs, respectively.
    In each panel, `tfvis.show.layer()` displays a table of the layer’s weights, with
    details about the names of the weights, their shape and parameter count, min/max
    of the parameter values, and counts of zero and NaN parameter values (the last
    of which can be useful for diagnosing problematic training runs). The layer visualization
    also contains Show Values Distribution buttons for each of the layer’s weights,
    which, when clicked, will create a histogram of the values in the weight.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 代码生成的可视化结果见[图8.5](#ch08fig05)。A 和 B 两图分别展示了使用未经正则化和经过正则化的 MLPs 的结果。每个图中，`tfvis.show.layer()`
    函数展示了该层的权值表格，其中包括权值的名称、形状和参数数量、参数值的最小/最大值，以及零值和 NaN 值的数量（最后一个参数可以用于诊断训练过程中出现的问题）。此外，该层的可视化界面还包含了每个权值的值分布展示按钮。当点击此按钮时，它将创建权值的值的直方图。
- en: Figure 8.5\. Distribution of the values in the kernel with (panel A) and without
    (panel B) L2 regularization. The visualization is created with `tfvis.show.layer()`.
    Note that the x-axes of the two histograms have different scales.
  id: totrans-90
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.5。正则化和未正则化情况下卷积核的值的分布。A 和 B 两幅图分别展示了经过/未经过 L2 正则化的 MLPs 的结果。该可视化结果基于 `tfvis.show.layer()`
    函数生成。请注意两个直方图的 x 轴比例不同。
- en: '![](08fig05_alt.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](08fig05_alt.jpg)'
- en: 'Comparing the plots for the two flavors of MLP, you can see a clear difference:
    the values of the kernel are distributed over a considerably narrower range with
    the L2 regularization than without. This is reflected in both the min/max values
    (the first row) and in the value histogram. This is regularization at work!'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 通过比较两个 MLPs 的绘图，可以看到明显差异：使用 L2 正则化的情况下，卷积核的值分布范围要比未经正则化的情况窄得多。这也反映在最小值和最大值（第一行）以及值的直方图中。这就是正则化的作用！
- en: But why do smaller kernel values result in reduced overfitting and improved
    generalization? An intuitive way to understand this is that L2 regularization
    enforces the principle of Occam’s razor. Generally speaking, a larger magnitude
    in a weight parameter tends to cause the model to fit to fine-grained details
    in the training features that it sees, and a smaller magnitude tends to let the
    model ignore such details. In the extreme case, a kernel value of zero means the
    model doesn’t attend to its corresponding input feature at all. The L2 regularization
    encourages the model to be more “economical” by avoiding large-magnitude weight
    values, and to retain those only when it is worth the cost (when the reduction
    in the target-prediction mismatch term outweighs the regularizer loss).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么较小的核值会导致减少过拟合和改善泛化呢？理解这一点的直观方法是 L2 正则化强制执行奥卡姆剃刀原则。一般来说，权重参数中的较大幅度倾向于导致模型拟合到它看到的训练特征中的细微细节，而较小幅度则倾向于让模型忽略这些细节。在极端情况下，核值为零意味着模型根本不关注其对应的输入特征。L2
    正则化鼓励模型通过避免大幅度的权重值来更“经济地”运行，并且仅在值得成本的情况下保留这些值（当减少目标预测不匹配项的损失超过正则化损失时）。
- en: L2 regularization is but one of the weapons against overfitting in the machine-learning
    practitioner’s arsenal. In [chapter 4](kindle_split_015.html#ch04), we demonstrated
    the power of dropout layers. Dropout is a powerful countermeasure to overfitting
    in general. It helps us reduce overfitting in this temperature-prediction problem
    as well. You can see that yourself by choosing the model type MLP with Dropout
    in the demo UI. The quality of training you get from the dropout-enabled MLP is
    comparable to the one you get from the L2-regularized MLP. We discussed how and
    why dropout works in [section 4.3.2](kindle_split_015.html#ch04lev2sec9) when
    we applied it to an MNIST convnet, so we won’t repeat it here. However, [table
    8.1](#ch08table01) provides a quick overview of the most widely used countermeasures
    to overfitting. It includes an intuitive description of how each of them works
    and the corresponding API in TensorFlow.js. The question as to which countermeasure
    to use for a particular problem is usually answered through 1) following well-established
    models that solve similar problems and 2) treating the countermeasure as a hyperparameter
    and searching for it through hyperparameter optimization ([section 3.1.2](kindle_split_014.html#ch03lev2sec2)).
    In addition, each overfitting-reducing method itself contains tunable parameters
    that can also be determined through hyperparameter optimization (see the last
    column of [table 8.1](#ch08table01).)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: L2 正则化只是机器学习从业者工具库中针对过拟合的其中一种武器。在[第 4 章](kindle_split_015.html#ch04)中，我们展示了辍学层的强大威力。辍学是一种对抗过拟合的强大措施。它同样帮助我们减少了这个温度预测问题中的过拟合。你可以通过在演示
    UI 中选择带有辍学的 MLP 模型类型来自己看到这一点。辍学启用的 MLP 所获得的训练质量与 L2 正则化的 MLP 相媲美。当我们将其应用于 MNIST
    卷积网络时，我们在[第 4.3.2 节](kindle_split_015.html#ch04lev2sec9)讨论了辍学是如何以及为什么起作用的，因此我们在这里不再赘述。然而，[表
    8.1](#ch08table01)提供了对抗过拟合最常用的快速概述。它包括了每种方法如何工作的直观描述以及 TensorFlow.js 中对应的 API。对于特定问题使用哪种对抗过拟合的方法的问题通常通过以下两种方式回答：1）遵循解决类似问题的成熟模型；2）将对抗过拟合方法视为一个超参数，并通过超参数优化来搜索它（[第
    3.1.2 节](kindle_split_014.html#ch03lev2sec2)）。此外，每种减少过拟合的方法本身都包含可调参数，这些参数也可以通过超参数优化确定（参见[表
    8.1](#ch08table01)的最后一列）。
- en: Table 8.1\. An overview of commonly used methods for reducing overfitting in
    TensorFlow.js
  id: totrans-95
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 8.1\. TensorFlow.js 中常用的减少过拟合方法概览
- en: '| Name of method | How the method works | Corresponding API in TensorFlow.js
    | Main free parameter(s) |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 方法名称 | 方法如何工作 | TensorFlow.js 中的对应 API | 主要自由参数 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| L2 regularizer | Assigns a positive loss (penalty) to the weight by calculating
    the summed squares of parameter values of the weight. It encourages the weight
    to have smaller parameter values. | tf.regularizers.l2() See the “[Reducing overfitting
    with weight regularization](#ch08lev2sec3)” section, for example. | L2-regularization
    rate |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| L2 正则化器 | 通过计算权重的参数值的平方和来对权重分配正的损失（惩罚）。它鼓励权重具有较小的参数值。 | tf.regularizers.l2()
    例如，见“[使用权重正则化减少过拟合](#ch08lev2sec3)”部分。 | L2-正则化率 |'
- en: '| L1 regularizer | Like L2 regularizers, encourages the weight parameters to
    be smaller. However, the loss it assigns to a weight is based on the summed *absolute
    values* of the parameters, instead of summed squares. This definition of regularization
    loss causes more weight parameters to become zero (that is, “sparser weights”).
    | tf.regularizers.l1() | L1-regularization rate |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| L1 正则化器 | 类似于 L2 正则化器，鼓励权重参数更小。但是，它对权重的损失基于参数的*绝对值之和*，而不是平方和。这种正则化损失的定义导致更多的权重参数变为零（即“更稀疏的权重”）。
    | tf.regularizers.l1() | L1 正则化率 |'
- en: '| Combined L1-L2 regularizer | A weighted sum of L1 and L2 regularization losses.
    | tf.regularizers.l1l2() | L1-regularization rate L2-regularization rate |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 组合 L1-L2 正则化器 | L1 和 L2 正则化损失的加权和。 | tf.regularizers.l1l2() | L1 正则化率 L2
    正则化率 |'
- en: '| Dropout | Randomly sets a fraction of the inputs to zero during training
    (but not during inference) in order to break spurious correlations (or “conspiracy”
    in Geoff Hinton’s words) among weight parameters that emerge during training.
    | tf.layers.dropout() See [section 4.3.2](kindle_split_015.html#ch04lev2sec9),
    for example. | Dropout rate |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 丢弃 | 在训练过程中随机将一部分输入设为零（但在推断过程中不设为零），以打破在训练过程中出现的权重参数之间的虚假相关性（或“阴谋”）。 | tf.layers.dropout()
    例如，请参阅 [4.3.2 节](kindle_split_015.html#ch04lev2sec9)。 | 丢弃率 |'
- en: '| Batch normalization | Learns the mean and standard deviation of its input
    values during training and uses the learned statistics to normalize the inputs
    to zero mean and unit standard deviation as its output. | tf.layers.batchNormalization()
    | Various (see [https://js.tensorflow.org/api/latest/#layers.batchNormalization](https://js.tensorflow.org/api/latest/#layers.batchNormalization))
    |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 批量归一化 | 在训练过程中学习其输入值的均值和标准差，并使用所学统计数据将输入归一化为零均值和单位标准差。 | tf.layers.batchNormalization()
    | 各种（参见 [https://js.tensorflow.org/api/latest/#layers.batchNormalization](https://js.tensorflow.org/api/latest/#layers.batchNormalization)）
    |'
- en: '| Early stopping of training based on validation-set loss | Stops model training
    as soon as the epoch-end loss value on the validation set stops decreasing. |
    tf.callbacks.earlyStopping() | minDelta: The threshold below which changes will
    be ignored patience: How many consecutive epochs of no improvement are tolerated
    at most |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 基于验证集损失的早期停止训练 | 当验证集上的每个周期结束时损失值不再减少时，停止模型训练。 | tf.callbacks.earlyStopping()
    | minDelta：忽略更改的阈值 patience：最多容忍连续几个周期的无改善 |'
- en: To wrap up this section on visualizing underfitting and overfitting, we provide
    a schematic diagram as a quick rule of thumb for spotting those states ([figure
    8.6](#ch08fig06)). As panel A shows, underfitting is when the model achieves a
    suboptimal (high) loss value, regardless of whether it’s on the training or validation
    set. In panel B, we see a typical pattern of overfitting, where the training loss
    looks fairly satisfactory (low), but the validation loss is worse (higher) in
    comparison. The validation loss can plateau and even start to edge up, even when
    the training-set loss continues to go down. Panel C is the state we want to be
    in—namely, a state where the loss value doesn’t diverge too much between the training
    and validation sets so that the final validation loss is low. Be aware that the
    phrase “sufficiently low” can be relative, especially for problems that no existing
    models can solve perfectly. New models may come out in the future and lower the
    achievable loss relative to what we have in panel C. At that point, the pattern
    in panel C would become a case of underfitting, and we would need to adopt the
    new model type in order to fix it, possibly by going through the cycle of overfitting
    and regularization again.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节关于可视化欠拟合和过拟合的总结中，我们提供了一个简略图表，以快速判断这些状态（[图 8.6](#ch08fig06)）。如面板 A 所示，欠拟合是指模型达到次优（高）损失值的状态，无论是在训练集还是验证集上。在面板
    B 中，我们看到了典型的过拟合模式，其中训练损失看起来相当令人满意（低），但是验证损失较差（更高）。即使训练集损失继续下降，验证损失也可能趋于平稳甚至上升。面板
    C 是我们想要达到的状态，即损失值在训练集和验证集之间没有太大差异，以便最终验证损失较低。请注意，术语“足够低”可以是相对的，特别是对于现有模型无法完美解决的问题。未来可能会推出新模型，并降低相对于面板
    C 的可达损失。在那时，面板 C 中的模式将变为欠拟合的情况，我们将需要采用新的模型类型来解决它，可能需要再次经历过拟合和正则化的周期。
- en: Figure 8.6\. A schematic diagram showing the loss curves from simplified cases
    of underfitting (panel A), overfitting (panel B), and just-right fitting (panel
    C) in model training.
  id: totrans-105
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.6\. 示意图显示了模型训练中欠拟合（面板 A）、过拟合（面板 B）和适度拟合（面板 C）的损失曲线。
- en: '![](08fig06_alt.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](08fig06_alt.jpg)'
- en: Finally, note that visualization of training is not limited to the losses. Other
    metrics are often visualized to aid in monitoring the training process as well.
    Examples of this are sprinkled throughout the book. For example, in [chapter 3](kindle_split_014.html#ch03),
    we plotted the ROC curves when training a binary classifier for phishing websites.
    We also rendered the confusion matrix when training the iris-flower classifier.
    In [chapter 9](kindle_split_021.html#ch09), we’ll show an example of displaying
    machine-generated text during the training of a text generator. That example won’t
    involve a GUI but will nonetheless provide useful and intuitive real-time information
    about the state of the model’s training. Specifically, by looking at the text
    generated by the model, you can get an intuitive sense of how good the text generated
    by the model currently is.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请注意，对训练的可视化不仅限于损失。其他指标通常也被可视化以帮助监视训练过程。本书中随处可见此类示例。例如，在[第 3 章](kindle_split_014.html#ch03)中，我们在训练二元分类器以识别网络钓鱼网站时绘制了
    ROC 曲线。我们还在训练 iris 花分类器时绘制了混淆矩阵。在[第 9 章](kindle_split_021.html#ch09)中，我们将展示一个在训练文本生成器时显示机器生成文本的示例。该示例不涉及
    GUI，但仍会提供关于模型训练状态的有用和直观的实时信息。具体来说，通过查看模型生成的文本，你可以直观地了解当前模型生成的文本质量如何。
- en: 8.3\. The universal workflow of machine learning
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3\. 机器学习的通用工作流程
- en: 'Up to this point, you have seen all the important steps in designing and training
    a machine-learning model, including acquiring, formatting, visualizing, and ingesting
    data; choosing the appropriate model topology and loss function for the dataset;
    and training the model. You’ve also seen some of the most important failure modes
    that may appear during the training process: underfitting and overfitting. So,
    this is a good place for us to look back at what we’ve learned so far and reflect
    on what’s common among the machine-learning model processes for different datasets.
    The resulting abstraction is what we refer to as *the universal workflow of machine
    learning*. We’ll list the workflow step-by-step and expand on the key considerations
    in each step:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经了解了设计和训练机器学习模型的所有重要步骤，包括获取、格式化、可视化和摄取数据；为数据集选择适当的模型拓扑和损失函数；以及训练模型。你还看到了在训练过程中可能出现的一些最重要的失败模式：欠拟合和过拟合。因此，现在是我们回顾一下迄今为止学到的东西，并思考不同数据集的机器学习模型过程中的共同之处的好时机。结果抽象化就是我们所说的*机器学习的通用工作流程*。我们将逐步列出工作流程，并扩展每个步骤中的关键考虑因素：
- en: '*Determine if machine learning is the right approach*. First, consider if machine
    learning is the right approach to your problem, and proceed to the next steps
    only if the answer is yes. In some cases, a non-machine-learning approach will
    work equally well or perhaps even better, at a lower cost. For example, given
    enough model-tuning efforts, you can train a neural network to “predict” the sum
    of two integers by taking the integers as text input data (for example, the addition-rnn
    example in the tfjs-examples repository). But this is far from the most efficient
    or reliable solution to this problem: the good old addition operation on the CPU
    suffices in this case.'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*确定机器学习是否是正确的方法*。首先，考虑一下机器学习是否是解决你的问题的正确方法，只有当答案是肯定的时候才继续下一步。在某些情况下，非机器学习方法同样有效，甚至可能成本更低。例如，通过足够的模型调整工作，你可以训练一个神经网络来“预测”两个整数的和，将整数作为文本输入数据（例如，在
    tfjs-examples 仓库中的 addition-rnn 示例）。但这远非是这个问题的最有效或最可靠的解决方案：在这种情况下，CPU 上的传统加法运算就足够了。'
- en: '*Define the machine-learning problem and what you are trying to predict using
    the data*. In this step, you need to answer two questions:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*定义机器学习问题及你尝试使用数据预测什么*。在这一步中，你需要回答两个问题：'
- en: '*What sort of data is available?* In supervised learning, you can only learn
    to predict something if you have labeled training data available. For example,
    the weather-prediction model we saw earlier in this chapter is possible only because
    the Jena-weather dataset is available. Data availability is usually a limiting
    factor in this stage. If the available data is insufficient, you may need to collect
    more data or hire people to manually label an unlabeled dataset.'
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*有哪些数据可用？* 在监督学习中，只有当有标记的训练数据可用时，你才能学习预测某些东西。例如，我们在本章前面看到的天气预测模型之所以可能，仅仅是因为有了
    Jena-weather 数据集。数据的可用性通常是这一阶段的限制因素。如果可用数据不足，你可能需要收集更多数据或者雇人手动标记未标记的数据集。'
- en: '*What type of problem are you facing?* Is it binary classification, multiclass
    classification, regression, or something else? Identifying the problem type will
    guide your choice of model architecture, loss function, and so forth.You can’t
    move on to the next step until you know what the inputs and outputs are and what
    data you’ll use. Be aware of the hypotheses you’ve made implicitly at this stage:'
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*你面临的是什么类型的问题？* 是二元分类、多类分类、回归还是其他？识别问题类型将指导你选择模型架构、损失函数等。在你知道输入和输出以及将使用的数据之前，你不能进入下一步。在这个阶段，要注意你隐含假设的假设：'
- en: You hypothesize that the outputs can be predicted given the inputs (the input
    alone contains enough information for a model to predict the output for all possible
    examples in this problem).
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你假设在给定输入的情况下可以预测输出（仅凭输入就包含了足够的信息，使模型能够预测该问题中所有可能的示例的输出）。
- en: 'You hypothesize that the data available is sufficient for a model to learn
    this input-output relationship.Until you have a working model, these are just
    hypotheses waiting to be validated or invalidated. Not all problems are solvable:
    just because you’ve assembled a large labeled dataset that maps from X to Y doesn’t
    mean that X contains enough information for the value of Y. For instance, if you’re
    trying to predict the future price of a stock based on the history of the stock’s
    price, you’ll likely fail because the price history doesn’t contain enough predictive
    information about the future price. One class of unsolvable problems you should
    be aware of is *nonstationary* problems, in which the input-output relation changes
    with time. Suppose you’re trying to build a recommendation engine for clothes
    (given a user’s clothes purchase history), and you’re training your model on only
    one year’s data. The big issue here is that people’s tastes for clothes change
    with time. A model that works accurately on the validation data from last year
    isn’t guaranteed to work equally accurately this year. Keep in mind that machine
    learning can only be used to learn patterns that are present in the training data.
    In this case, getting up-to-date data and continuously training new models will
    be a viable solution.'
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你假设可用的数据足以让模型学习这种输入输出关系。在你有一个可用的模型之前，这些只是等待验证或无效化的假设。并非所有问题都是可解的：仅仅因为你组装了一个大型标记数据集，从
    X 到 Y 的映射并不意味着 X 包含足够的信息来推断 Y 的值。例如，如果你试图根据股票的历史价格来预测股票的未来价格，你可能会失败，因为价格历史并不包含足够的有关未来价格的预测信息。你应该意识到一个不可解问题类别是
    *非平稳* 问题，即输入输出关系随时间变化。假设你正在尝试构建一个服装推荐引擎（根据用户的服装购买历史），并且你正在使用一年的数据来训练你的模型。这里的主要问题是人们对服装的品味随时间而改变。在去年验证数据上准确工作的模型不一定今年同样准确。请记住，机器学习只能用于学习训练数据中存在的模式。在这种情况下，获取最新的数据并持续训练新模型将是一个可行的解决方案。
- en: '*Identify a way to reliably measure the success of a trained model on your
    goal*. For simple tasks, this may be just prediction accuracy, precision and recall,
    or the ROC curve and the AUC value (see [chapter 3](kindle_split_014.html#ch03)).
    But in many cases, it will require more sophisticated domain-specific metrics,
    such as customer retention rate and sales, which are better aligned with higher-level
    goals, such as the success of the business.'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*确定一种可靠地衡量训练模型在目标上成功的方法*。对于简单的任务，这可能仅仅是预测准确性、精确率和召回率，或者 ROC 曲线和 AUC 值（参见[第
    3 章](kindle_split_014.html#ch03)）。但在许多情况下，它将需要更复杂的领域特定指标，如客户保留率和销售额，这些指标与更高级别的目标（如业务的成功）更加一致。'
- en: '*Prepare the evaluation process*. Design the validation process that you’ll
    use to evaluate your models. In particular, you should split your data into three
    homogeneous yet nonoverlapping sets: a training set, a validation set, and a test
    set. The validation- and test-set labels ought not to leak into the training data.
    For instance, with temporal prediction, the validation and test data should come
    from time intervals *after* the training data. Your data preprocessing code should
    be covered by tests to guard against bugs.'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*准备评估过程*。设计您将用于评估模型的验证过程。特别是，您应将数据分为三组同质但不重叠的集合：训练集、验证集和测试集。验证集和测试集的标签不应泄漏到训练数据中。例如，对于时间预测，验证和测试数据应来自训练数据之后的时间间隔。您的数据预处理代码应该由测试覆盖以防止错误。'
- en: '*Vectorize the data*. Turn the data into tensors, also known as *n*-dimensional
    arrays, the lingua franca of machine-learning models in frameworks such as TensorFlow.js
    and TensorFlow. Note the following guidelines for data vectorization:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*将数据向量化*。将数据转换为张量，也称为*n*维数组，这是机器学习模型在诸如 TensorFlow.js 和 TensorFlow 等框架中的通用语言。注意以下有关数据向量化的准则：'
- en: 'The numeric values taken by the tensors should usually be scaled to small and
    centered values: for example, within the `[-1, 1]` or `[0, 1]` interval.'
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张量取值通常应缩放为小而居中的值：例如，在`[-1, 1]`或`[0, 1]`区间内。
- en: If different features (such as temperature and wind speed) take values in different
    ranges (heterogeneous data), then the data ought to be normalized, usually z-normalized
    to zero mean and unit standard deviation for each feature.Once your tensors of
    input data and target (output) data are ready, you can begin to develop models.
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果不同特征（例如温度和风速）具有不同范围的值（异构数据），那么数据应该被归一化，通常是针对每个特征进行零均值和单位标准差的z归一化。一旦您的输入数据张量和目标（输出）数据准备好了，您就可以开始开发模型。
- en: '*Develop a model that beats a commonsense baseline*. Develop a model that beats
    a non-machine-learning baseline (such as predicting the population average for
    a regression problem or predicting the last data point in a time-series prediction
    problem), thereby demonstrating that machine learning can truly add value to your
    solution. This may not always be the case (see step 1). Assuming things are going
    well, you need to make three key choices to build your first baseline-beating,
    machine-learning model:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*开发一个能击败常识基准线的模型*。开发一个能击败非机器学习基准线的模型（例如对于回归问题，预测人口平均值，或者对于时间序列预测问题，预测最后一个数据点），从而证明机器学习确实可以为您的解决方案增加价值。这可能并不总是事实（参见步骤
    1）。假设事情进展顺利，您需要做出三个关键选择来构建您的第一个击败基准线的机器学习模型：'
- en: '*Last-layer activation*—This establishes useful constraints for the model’s
    output. This activation should suit the type of problem you are solving. For example,
    the phishing-website classifier in [chapter 3](kindle_split_014.html#ch03) used
    the sigmoid activation for its last (output) layer due to the binary-classification
    nature of the problem, and the temperature-prediction models in this chapter used
    the linear activation for the layer owing to the regression nature of the problem.'
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*最后一层激活*——这为模型的输出建立了有用的约束条件。该激活应适合您正在解决的问题类型。例如，本书的第 3 章中的网络钓鱼网站分类器使用了 Sigmoid
    激活作为其最后（输出）层，因为该问题具有二分类的性质；而本章的温度预测模型使用了线性激活作为层的激活，因为该问题是回归问题。'
- en: '*Loss function*—In a way similar to last-layer activation, the loss function
    should match the problem you’re solving. For instance, use `binaryCrossentropy`
    for binary-classification problems, `categoricalCrossentropy` for multiclass-classification
    problems, and `meanSquaredError` for regression problems.'
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*损失函数*——与最后一层激活类似，损失函数应与您正在解决的问题相匹配。例如，对于二分类问题使用`binaryCrossentropy`，对于多类分类问题使用`categoricalCrossentropy`，对于回归问题使用`meanSquaredError`。'
- en: '*Optimizer configuration*—The optimizer is what drives the updates to the neural
    network’s weights. What type of optimizer should be used? What should its learning
    rate be? These are generally questions answered by hyperparameter tuning. But
    in most cases, you can safely start with the `rmsprop` optimizer and its default
    learning rate.'
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*优化器配置*——优化器是推动神经网络权重更新的驱动器。应该使用什么类型的优化器？其学习率应该是多少？这些通常是由超参数调整回答的问题。但在大多数情况下，您可以安全地从`rmsprop`优化器及其默认学习率开始。'
- en: '*Develop a model with sufficient capacity and to overfit the training data*.
    Gradually scale up your model architecture by manually changing hyperparameters.
    You want to reach at a model that overfits the training set. Remember that the
    universal and central tension in supervised machine learning is between *optimization*
    (fitting the data seen during training) and *generalization* (being able to make
    accurate predictions for unseen data). The ideal model is one that stands right
    at the border between underfitting and overfitting: that is, between under-capacity
    and over-capacity. To figure out where this border is, you must first cross it.
    In order to cross it, you must develop a model that overfits. This is usually
    fairly easy. You may'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*开发一个具有足够容量且过拟合训练数据的模型*。通过手动更改超参数逐渐扩展您的模型架构。您希望达到一个过拟合训练集的模型。请记住，监督机器学习中的通用和核心紧张关系在于*优化*（适合训练期间看到的数据）和*泛化*（能够为未看到的数据进行准确预测）。理想的模型是一个恰好位于欠拟合和过拟合之间的模型：即，在容量不足和容量过大之间。要弄清楚这个边界在哪里，您必须首先越过它。为了越过它，您必须开发一个过拟合的模型。这通常相当容易。你可能'
- en: Add more layers
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加更多层
- en: Make each layer bigger
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使每一层更大
- en: Train the model for more epochsAlways use visualization to monitor the training
    and validation losses, as well as any additional metrics that you care about (such
    as AUC) on both the training and validation sets. When you see the model’s accuracy
    on the validation set begin to degrade ([figure 8.6](#ch08fig06), panel B), you’ve
    achieved overfitting.
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为模型训练更多的epochs。始终使用可视化来监视训练和验证损失，以及您关心的任何其他指标（例如AUC）在训练和验证集上。当您看到验证集上模型的准确性开始下降（[图8.6](#ch08fig06)，面板B）时，您已经达到了过拟合。
- en: '*Add regularization to your model and tune the hyperparameters*. The next step
    is to add regularization to your model and further tune its hyperparameters (usually
    in an automated way) to get as close as possible to the ideal model that neither
    underfits nor overfits. This step will take the most time, even though it can
    be automated. You’ll repeatedly modify your model, train it, evaluate it on the
    validation set (not the test set at this point), modify it again, and repeat until
    the model is as good as it can get. These are the things you should try in terms
    of regularization:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*为模型添加正则化并调整超参数*。下一步是为模型添加正则化，并进一步调整其超参数（通常以自动方式），以尽可能接近既不欠拟合也不过拟合的理想模型。这一步将花费最多的时间，即使它可以自动化。您将反复修改模型，训练它，在验证集上评估它（此时不是测试集），再次修改它，然后重复，直到模型尽可能好。在正则化方面应尝试以下事项：'
- en: Add dropout layers with different dropout rates.
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加具有不同dropout率的dropout层。
- en: Try L1 and/or L2 regularization.
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试L1和/或L2正则化。
- en: 'Try different architectures: add or remove a small number of layers.'
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试不同的架构：增加或减少少量层。
- en: Change other hyperparameters (for example, the number of units of a dense layer).Beware
    of validation-set overfitting when tuning hyperparameters. Because the hyperparameters
    are determined based on the performance on the validation set, their values will
    be overspecialized for the validation set and therefore may not generalize well
    to other data. It is the purpose of the test set to obtain an unbiased estimate
    of the model’s accuracy after hyperparameter tuning. So, you shouldn’t use the
    test set when tuning the hyperparameters.
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更改其他超参数（例如，密集层的单位数）。在调整超参数时要注意验证集的过拟合。因为超参数是根据验证集的性能确定的，它们的值将对验证集过于专门化，因此可能不会很好地推广到其他数据。测试集的目的是在超参数调整后获得模型准确性的无偏估计。因此，在调整超参数时不应使用测试集。
- en: This is the universal workflow of machine learning! In [chapter 12](kindle_split_025.html#ch12),
    we’ll add two more practically oriented steps to it (an evaluation step and a
    deployment step). But for now, this is a recipe for how to go from a vaguely defined
    machine-learning idea to a model that’s trained and ready to make some useful
    predictions.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这是机器学习的通用工作流程！在[第12章](kindle_split_025.html#ch12)中，我们将为其添加两个更具实践性的步骤（评估步骤和部署步骤）。但是现在，这是一个从模糊定义的机器学习想法到训练完毕并准备好进行一些有用预测的模型的配方。
- en: With this foundational knowledge, we’ll start exploring more advanced types
    of neural networks in the upcoming part of the book. We’ll start from models designed
    for sequential data in [chapter 9](kindle_split_021.html#ch09).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些基础知识，我们将开始在本书的后续部分探索更高级的神经网络类型。我们将从[第9章](kindle_split_021.html#ch09)中设计用于序列数据的模型开始。
- en: Exercises
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习
- en: In the temperature-prediction problem, we found that the linear regressor significantly
    underfit the data and produced poor prediction results on both the training and
    validation sets. Would adding L2 regularization to the linear regressor help improve
    the accuracy of such an underfitting model? It should be easy to try it out yourself
    by modifying the `buildLinearRegressionModel()` function in the file jena-weather/models.js.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在温度预测问题中，我们发现线性回归器明显欠拟合了数据，并在训练集和验证集上产生了较差的预测结果。将 L2 正则化添加到线性回归器是否有助于提高这种欠拟合模型的准确性？你可以通过修改文件
    jena-weather/models.js 中的`buildLinearRegressionModel()`函数自行尝试。
- en: When predicting the temperature of the next day in the Jena-weather example,
    we used a look-back period of 10 days to produce the input features. A natural
    question is, what if we use a longer look-back period? Is including more data
    going to help us get more accurate predictions? You can find this out by modifying
    the `const lookBack` in jena-weather/index.js and running the training in the
    browser (for example, by using the MLP with L2 regularization). Of course, a longer
    look-back period will increase the size of the input features and lead to longer
    training time. So, the flip side of the question is, can we use a shorter look-back
    period without sacrificing the prediction accuracy significantly? Try this out
    as well.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Jena-weather 示例中预测第二天的温度时，我们使用了 10 天的回溯期来生成输入特征。一个自然的问题是，如果我们使用更长的回溯期会怎样？包含更多数据是否会帮助我们获得更准确的预测？你可以通过修改
    jena-weather/index.js 中的`const lookBack`并在浏览器中运行训练（例如，使用具有 L2 正则化的 MLP）来找出答案。当然，更长的回溯期会增加输入特征的大小并导致更长的训练时间。因此，问题的另一面是，我们是否可以使用更短的回溯期而不明显牺牲预测准确性？也试试这个。
- en: Summary
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: tfjs-vis can aid the visualization of a machine-learning model’s training process
    in the browser. Specifically, we showed how tfjs-vis can be used to
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: tfjs-vis可以在浏览器中辅助可视化机器学习模型的训练过程。具体来说，我们展示了tfjs-vis如何用于
- en: Visualize the topology of TensorFlow.js models.
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化 TensorFlow.js 模型的拓扑结构。
- en: Plot loss and metrics curves during training.
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绘制训练过程中的损失和指标曲线。
- en: Summarize weight distributions after training. We showed concrete examples of
    these visualization workflows.
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练后总结权重分布。我们展示了这些可视化工作流程的具体示例。
- en: Underfitting and overfitting are fundamental behaviors of machine-learning models
    and should be monitored and understood in every machine-learning problem. They
    can both be seen by comparing the loss curves from the training and validation
    sets during training. The built-in `tfvis.show.fitCallbacks()` method helps you
    visualize these curves in the browser with ease.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欠拟合和过拟合是机器学习模型的基本行为，应该在每一个机器学习问题中进行监控和理解。它们都可以通过比较训练和验证集的损失曲线来观察。内置的`tfvis.show.fitCallbacks()`方法可以帮助你轻松在浏览器中可视化这些曲线。
- en: The universal workflow of machine learning is a list of common steps and best
    practices of different types of supervised learning tasks. It goes from deciding
    the nature of the problem and the requirements on the data to finding a model
    that sits nicely on the border between underfitting and overfitting.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习的通用工作流程是不同类型的监督学习任务的一系列常见步骤和最佳实践。它从确定问题的性质和对数据的需求开始，到找到一个恰到好处的模型，位于欠拟合和过拟合之间的边界上。
