- en: Chapter 8\. Underfitting, overfitting, and the universal workflow of machine
    learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*This chapter covers*'
  prefs: []
  type: TYPE_NORMAL
- en: Why it is important to visualize the model-training process and what the important
    things are to look for
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to visualize and understand underfitting and overfitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The primary way of dealing with overfitting: regularization, and how to visualize
    its effect'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What the universal workflow of machine learning is, what steps it includes,
    and why it is an important recipe that guides all supervised machine-learning
    tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the previous chapter, you learned how to use tfjs-vis to visualize data before
    you start designing and training machine-learning models for it. This chapter
    will start where that one left off and describe how tfjs-vis can be used to visualize
    the structure and metrics of models during their training. The most important
    goal in doing so is to spot the all-important phenomena of *underfitting* and
    *overfitting*. Once we can spot them, we’ll delve into how to remedy them and
    how to verify that our remedying approaches are working using visualization.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1\. Formulation of the temperature-prediction problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To demonstrate underfitting and overfitting, we need a concrete machine-learning
    problem. The problem we’ll use is predicting temperature based on the Jena-weather
    dataset you’ve just seen in the previous chapter. [Section 7.1](kindle_split_019.html#ch07lev1sec1)
    showed the power of visualizing data in the browser and the benefits of doing
    so using the Jena-weather dataset. Hopefully, you’ve formed an intuition of the
    dataset through playing with the visualization UI in the previous section. We
    are now ready to start applying some machine learning to the dataset. But first,
    we need to define the problem.
  prefs: []
  type: TYPE_NORMAL
- en: The prediction task can be thought of as a toy weather-forecast problem. What
    we are trying to predict is the temperature 24 hours after a certain moment in
    time. We try to make this prediction using the 14 types of weather measurements
    taken in the 10-day period leading up to that moment.
  prefs: []
  type: TYPE_NORMAL
- en: Although the problem definition is straightforward, the way we generate the
    training data from the CSV file requires some careful explanation because it is
    different from the data-generation procedures in the problems seen in this book
    so far. In those problems, every row in the raw data file corresponded to a training
    example. That was how the iris-flower, Boston-housing, and phishing-detection
    examples worked (see [chapters 2](kindle_split_013.html#ch02) and [3](kindle_split_014.html#ch03)).
    However, in this problem, each example is formed by sampling and combining multiple
    rows from the CSV file. This is because a temperature prediction is made not just
    by looking at one moment in time, but instead by looking at the data over a time
    span. See [figure 8.1](#ch08fig01) for a schematic illustration of the example-generation
    process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.1\. Schematic diagram showing how a single training example is generated
    from the tabular data. To generate the feature tensor of the example, the CSV
    file is sampled every `step` rows (for example, `step = 6`) up to `timeSteps`
    such rows (for example, `timeSteps = 240`). This forms a tensor of shape `[timeSteps,
    numFeatures]`, where `numFeatures` (default: 14) is the number of feature columns
    in the CSV file. To generate the target, sample the temperature (`T`) value at
    the row delay (for example, 144) step after the last row that went into the feature
    tensor. Other examples can be generated by starting from a different row in the
    CSV file, but they follow the same rule. This forms the temperature-prediction
    problem: given the 14 weather measurements for a certain period of time (such
    as 10 days) until now, predict the temperature a certain delay (such as 24 hours)
    from now. The code that does what’s shown in this diagram is in the `getNextBatchFunction()`
    function in jena-weather/data.js.'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](08fig01_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: To generate the features of a training example, we sample a set of rows over
    a time span of 10 days. Instead of using all the data rows from the 10 days, we
    sample every sixth row. Why? For two reasons. First, sampling all the rows would
    give us six times as much data and lead to a bigger model size and longer training
    time. Second, the data at a time scale of 1 hour has a lot of redundancy (the
    air pressure from 6 hours ago is usually close to that from 6 hours and 10 minutes
    ago). By throwing away five-sixths of the data, we get a more lightweight and
    performant model without sacrificing much predictive power. The sampled rows are
    combined into a 2D feature tensor of shape `[timeSteps,` `numFeatures]` for our
    training example (see [figure 8.1](#ch08fig01)). By default, `timeSteps` has a
    value of 240, which corresponds to the 240 sampling times evenly distributed across
    the 10-day period. `numFeatures` is 14, which corresponds to the 14 weather-instrument
    readings available in the CSV dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Getting the target for the training example is easier: we just move forward
    a certain time delay from the last row that goes into the feature tensor and extract
    the value from the temperature column. [Figure 8.1](#ch08fig01) shows how only
    a single training example is generated. To generate multiple training examples,
    we simply start from different rows of the CSV file.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You may have noticed something peculiar about the feature tensor for our temperature-prediction
    problem (see [figure 8.1](#ch08fig01)): in all the previous problems, the feature
    tensor of a single example was 1D, which led to a 2D tensor when multiple examples
    were batched. However, in this problem, the feature tensor of a single example
    is already 2D, which means that we’ll get a 3D tensor (of shape `[batchSize, timeSteps,
    numFeatures]`) when we combine multiple examples into a batch. This is an astute
    observation! The 2D feature-tensor shape originates from the fact that the features
    come from a *sequence* of events. In particular, they are the weather measurements
    taken at 240 points in time. This distinguishes this problem from all the other
    problems you’ve seen so far, in which the input features for a given example do
    not span multiple moments in time, be it the flower size measurements in the iris-flower
    problem or the 28 × 28 pixel values of an MNIST image.^([[1](#ch08fn1)])'
  prefs: []
  type: TYPE_NORMAL
- en: ¹
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The speech-command recognition problem in [chapter 4](kindle_split_015.html#ch04)
    did, in fact, involve a sequence of events: namely, the successive frames of audio
    spectra that formed the spectrogram. However, our methodology treated the entire
    spectrogram as an image, thereby ignoring the temporal dimension of the problem
    by treating it as a spatial dimension.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'This is the first time you encounter sequential input data in this book. In
    the next chapter, we will dive deeper into how to build specialized and more powerful
    models (RNNs) for sequential data in TensorFlow.js. But here, we will approach
    the problem using two types of models we already know: linear regressors and MLPs.
    This forms a buildup to our study of RNNs and gives us a baseline that can be
    compared with the more advanced models.'
  prefs: []
  type: TYPE_NORMAL
- en: The actual code that performs the data-generation process illustrated in [figure
    8.1](#ch08fig01) is in jena-weather/data.js, under the function `getNextBatchFunction()`.
    This is an interesting function because instead of returning a concrete value,
    it returns an object with a function called `next()`. The `next()` function returns
    actual data values when it’s called. The object with the `next()` function is
    referred to as an *iterator*. Why do we use this indirection instead of writing
    an iterator directly? First, this conforms to the generator/iterator specification
    of JavaScript.^([[2](#ch08fn2)]) We will soon pass it to the `tf.data .generator()`
    API in order to create a dataset object for model training. The API requires this
    function signature. Second, our iterator needs to be configurable; a function
    that returns the iterator is a good way to enable the configuration.
  prefs: []
  type: TYPE_NORMAL
- en: ²
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “Iterators and Generators,” MDN web docs, [http://mng.bz/RPWK](http://mng.bz/RPWK).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'You can see the possible configuration options from the signature of `getNextBatchFunction()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: There are quite a few configurable parameters. For example, you can use the
    `lookBack` argument to specify how long a period to look back when making a temperature
    prediction. You can also use the `delay` argument to specify how far in the future
    the temperature prediction will be made for. The arguments `minIndex` and `maxIndex`
    allow you to specify the range of rows to draw data from, and so forth.
  prefs: []
  type: TYPE_NORMAL
- en: We convert the `getNextBatchFunction()` function into a `tf.data.Dataset` object
    by passing it to the `tf.data.generator()` function. As we described in [chapter
    6](kindle_split_018.html#ch06), a `tf.data.Dataset` object, when used in conjunction
    with the `fitDataset()` method of a `tf.Model` object, enables us to train the
    model even if the data is too large to fit into WebGL memory (or any applicable
    backing memory type) as a whole. The `Dataset` object will create a batch of training
    data on the GPU only when it is about to go into the training. This is exactly
    what we do for the temperature-prediction problem here. In fact, we wouldn’t be
    able to train the model using the model’s ordinary `fit()` method due to the large
    number and size of the examples. The `fitDataset()` call can be found in jena-weather/models.js
    and looks like the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.1\. Visualizing the `fitDataset`-based model training with tfjs-vis
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '***1*** The first Dataset object will generate the training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***2*** The second Dataset object will generate the validation data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***3*** The validationData config for fitDataset() accepts either a Dataset
    object or a set of tensors. Here, the first option is used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first two fields of the configuration object for `fitDataset()` specify
    how many epochs to train the model for and how many batches to draw for every
    epoch. As you learned in [chapter 6](kindle_split_018.html#ch06), they are the
    standard configuration fields for a `fitDataset()` call. However, the third field
    (`callbacks: customCallback`) is something new. It is how we visualize the training
    process. Our `customCallback` takes different values depending on whether the
    model training occurs in the browser or, as we’ll see in the next chapter, in
    Node.js.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the browser, the function `tfvis.show.fitCallbacks()` provides the value
    of `customCallback`. The function helps us visualize the model training in the
    web page with just one line of JavaScript code. It not only saves us all the work
    of accessing and keeping track of batch-by-batch and epoch-by-epoch loss and metric
    values, but it also removes the need to manually create and maintain the HTML
    elements in which the plots will be rendered:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The first argument to `fitCallbacks()` specifies a rendering area created with
    the `tfvis.visor().surface()` method. It is called a *visor surface* in the terminology
    of tfjs-vis. A visor is a container that helps you conveniently organize all the
    visualization related to your in-browser machine-learning tasks. Structurally,
    a visor is organized on two levels of hierarchy. At the higher level, there can
    be one or more tabs that the user can navigate using clicks. At the lower level,
    every tab contains one or more *surfaces*. The `tfvis.visor().surface()` method,
    with its `tab` and `name` configuration fields, lets you create a surface in a
    designated visor tab with a designated name. A visor surface is not limited to
    rendering loss and metric curves. In fact, all the basic charts we showed with
    the CodePen example in [section 7.1](kindle_split_019.html#ch07lev1sec1) can be
    rendered on visor surfaces. We leave this as an exercise for you at the end of
    this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The second argument for `fitCallbacks()` specifies what losses and metrics will
    be rendered in the visor surface. In this case, we plot the loss from the training
    and validation datasets. The third argument contains a field that controls the
    frequency at which the plots are updated. By using both `onBatchEnd` and `onEpochEnd`,
    we will get updates at the end of every batch and every epoch. In the next section,
    we will examine the loss curves created by `fitCallbacks()` and use them to spot
    underfitting and overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2\. Underfitting, overfitting, and countermeasures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: During the training of a machine-learning model, we want to monitor how well
    our model is capturing the patterns in the training data. A model that doesn’t
    capture the patterns very well is said to be *underfit*; a model that captures
    the patterns *too* well, to the extent that what it learns generalizes poorly
    to new data, is said to be *overfit*. An overfit model can be brought back on
    track through countermeasures such as regularization. In this section, we’ll show
    how visualization can help us spot these model behaviors and the effects of the
    countermeasures.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.1\. Underfitting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To solve the temperature-prediction problem, let’s first try the simplest possible
    machine-learning model: a linear regressor. The code in [listing 8.2](#ch08ex02)
    (from jena-weather/index.js) creates such a model. It uses a dense layer with
    a single unit and the default linear activation to generate the prediction. However,
    compared with the linear regressor we built for the download-time prediction problem
    in [chapter 2](kindle_split_013.html#ch02), this model has an extra flatten layer.
    This is because the shape of the feature tensor in this problem is 2D, which must
    be flattened into 1D to meet the requirement of the dense layer used for linear
    regression. This flattening process is illustrated in [figure 8.2](#ch08fig02).
    It is important to note is that this flattening operation discards the information
    about the sequential (temporal) ordering in the data.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.2\. Flattening the 2D feature tensor of shape `[timeSteps, numFeatures]`
    into a 1D tensor of shape `[timeSteps × numFeatures]`, as done by both the linear
    regressor in [listing 8.2](#ch08ex02) and the MLP model in [listing 8.3](#ch08ex03)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](08fig02_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Listing 8.2\. Creating a linear-regression model for the temperature-prediction
    problem
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '***1*** Flattens the [batchSize, timeSteps, numFeatures] input shape to [batchSize,
    timeSteps * numFeatures] in order to apply the dense layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***2*** A single-unit dense layer with the default (linear) activation is a
    linear regressor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the model is constructed, we compile it for training with
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Here, we use the loss function `meanAbsoluteError` because our problem is predicting
    a continuous value (the normalized temperature). Unlike in some of the previous
    problems, no separate metric is defined, because the MAE loss function itself
    serves as the human-interpretable metric. However, beware that since we are predicting
    the *normalized* temperature, the MAE loss has to be multiplied with the standard
    deviation of the temperature column (8.476 degrees Celsius) to be converted into
    a prediction error in absolute terms. For example, if we get an MAE of 0.5, it
    translates to 8.476 * 0.5 = 4.238 degrees Celsius of prediction error.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the demo UI, choose Linear Regression in the Model Type drop-down menu and
    click Train Model to kick off the training of the linear regressor. Right after
    the training starts, you’ll see a tabular summary of the model in a “card” that
    pops up on the right-hand side of the page (see the screenshot in [figure 8.3](#ch08fig03)).
    This model-summary table is somewhat similar to the text output of a `model.summary()`
    call but is rendered graphically in HTML. The code that creates the table is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Figure 8.3\. The tfjs-vis visor visualizing the training of a linear-regression
    model. Top: a summary table for the model. Bottom: the loss curves over 20 epochs
    of training. This chart is created with `tfvis.show .fitCallbacks()` (see jena-weather/index.js).'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](08fig03_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: With the surface created, we draw a model-summary table in it by passing the
    surface to `tfvis.show.modelSummary()`, as in the second line of the previous
    code snippet.
  prefs: []
  type: TYPE_NORMAL
- en: Under the Model Summary part of the linear-regression tab is a plot that displays
    the loss curves from the model training ([figure 8.3](#ch08fig03)). It is created
    by the `fitCallbacks()` call that we described in the last section. From the plot,
    we can see how well the linear regressor does on the temperature-prediction problem.
    Both the training and validation losses end up oscillating around 0.9, which corresponds
    to 8.476 * 0.9 = 7.6 degrees Celsius in absolute terms (recall that 8.476 is the
    standard deviation of the temperature column in the CSV file). This means that
    after training, our linear regressor makes a prediction error of 7.6 degrees Celsius
    (or 13.7 degrees Fahrenheit) on average. These predictions are pretty bad. No
    one would want to trust the weather forecast based on this model! This is an example
    of *underfitting*.
  prefs: []
  type: TYPE_NORMAL
- en: Underfitting is usually a result of using an insufficient representational capacity
    (power) to model the feature-target relationship. In this example, our linear
    regressor is structurally too simple and hence is underpowered to capture the
    relation between the weather data of the previous 10 days and the temperature
    of the next day. To overcome underfitting, we usually increase the power of the
    model by making it bigger. Typical approaches include adding more layers (with
    nonlinear activations) to the model and increasing the size of the layers (such
    as the number of units in a dense layer). So, let’s add a hidden layer to the
    linear regressor and see how much improvement we can get from the resultant MLP.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.2\. Overfitting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The function that creates MLP models is in [listing 8.3](#ch08ex03) (from jena-weather/index.js).
    The MLP it creates includes two dense layers, one as the hidden layer and one
    as the output layer, in addition to a flatten layer that serves the same purpose
    as in the linear-regression model. You can see that the function has two more
    arguments compared to `buildLinearRegressionModel()` in [listing 8.2](#ch08ex02).
    In particular, the `kernelRegularizer` and `dropoutRate` parameters are the ways
    in which we’ll combat overfitting later. For now, let’s see what prediction accuracy
    an MLP that doesn’t use `kernelRegularizer` or `dropoutRate` is capable of achieving.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.3\. Creating an MLP for the temperature-prediction problem
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '***1*** If specified by the caller, add regularization to the kernel of the
    hidden dense layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***2*** If specified by the caller, add a dropout layer between the hidden
    dense layer and the output dense layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Panel A of [figure 8.4](#ch08fig04) shows the loss curves from the MLP. Compared
    with the loss curves of the linear regressor, we can see a few important differences:'
  prefs: []
  type: TYPE_NORMAL
- en: The training and validation loss curves show a divergent pattern. This is different
    from the pattern in [figure 8.3](#ch08fig03), where two loss curves show largely
    consistent trends.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The training loss converges toward a much lower error than before. After 20
    epochs of training, the training loss has a value of about 0.2, which corresponds
    to an error of 8.476 * 0.2 = 1.7 degrees Celsius—much better than the result from
    linear regression.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, the validation loss decreases briefly in the first two epochs and then
    starts to go back up slowly. At the end of epoch 20, it has a significantly higher
    value than the training loss (0.35, or about 3 degrees Celsius).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Figure 8.4\. The loss curves from applying two different MLP models on the
    temperature-prediction problem. Panel A: from an MLP model without any regularization.
    Panel B: from an MLP model of the same layer size and count as the model in panel
    A, but with L2 regularization of the dense layers’ kernels. Notice that the y-axis
    ranges differ slightly between the two panels.'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](08fig04_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The more than four-fold decrease in training loss relative to the previous
    result is due to the fact that our MLP has a higher power than the linear-regression
    model thanks to one more layer and several times more trainable weight parameters.
    However, the increased model power has a side effect: it causes the model to fit
    the training data significantly better than the validation data (data the model
    doesn’t get to see during training). This is an example of *overfitting*. It is
    a case in which a model “pays too much attention” to the irrelevant details in
    the training data, to the extent that the model’s predictions start to generalize
    poorly to unseen data.'
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.3\. Reducing overfitting with weight regularization and visualizing it working
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In [chapter 4](kindle_split_015.html#ch04), we reduced overfitting in a convnet
    by adding dropout layers to the model. Here, let’s look at another frequently
    used overfitting-reduction approach: adding regularization to weights. In the
    Jena-weather demo UI, if you select the model type MLP with L2 Regularization,
    the underlying code will create an MLP by calling `buildMLPModel()` ([listing
    8.3](#ch08ex03)) in the following manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The second argument—the return value of `tf.regularizers.l2()`—is an *L2 regularizer*.
    By plugging the previous code into the `buildMLPModel()` function in [listing
    8.3](#ch08ex03), you can see that the L2 regularizer goes into the `kernelRegularizer`
    of the hidden dense layer’s configuration. This attaches the L2 regularizer to
    the kernel of the dense layer. When a weight (such as the kernel of a dense layer)
    has an attached regularizer, we say that the weight is *regularized*. Similarly,
    when some or all of a model’s weights are regularized, we say the model is regularized.
  prefs: []
  type: TYPE_NORMAL
- en: 'What does the regularizer do to the dense-layer kernel and the MLP that it
    belongs to? It adds an extra term to the loss function. Consider how the loss
    of the unregularized MLP is calculated: it’s defined simply as the MAE between
    the targets and the model’s predictions. In pseudo-code, it can be expressed as'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: With a regularized weight, the loss of the model includes an extra term. In
    pseudo-code,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Here, `l2Rate * l2(kernel)` is the extra L2-regularization term of the loss
    function. Unlike the MAE, this term does *not* depend on the model’s predictions.
    Instead, it depends only on the kernel (a weight of the layer) being regularized.
    Given the value of the kernel, it outputs a number associated only with the kernel’s
    values. You can think of the number as a measure of how undesirable the current
    value of the kernel is.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s look at the detailed definition of the L2-regularization function:
    `l2(kernel)`. It calculates the summed squares of all the weight values. For example,
    pretend our kernel has a small shape of `[2, 2]` for the sake of simplicity, and
    suppose its values are `[[0.1, 0.2], [-0.3, -0.4]]`; then,'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Therefore, `l2(kernel)` always returns a positive number that penalizes large
    weight values in `kernel`. With the term included in the total loss, it encourages
    all elements of `kernel` to be smaller in absolute value, everything else being
    equal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now the total loss includes two different terms: the target-prediction mismatch
    and a term related to `kernel`’s magnitudes. As a result, the training process
    will try to not only minimize the target-prediction mismatch but also reduce the
    sum of the squares of the kernel’s elements. Oftentimes, the two goals will conflict
    with each other. For example, a reduction in the magnitude of the kernel’s elements
    may reduce the second term but increase the first one (the MSE loss). How does
    the total loss balance the relative importance of the two conflicting terms? That’s
    where the `l2Rate` multiplier comes into play. It quantifies the importance of
    the L2 term relative to the target-prediction-error term. The larger the value
    of `l2Rate`, the more the training process will tend to reduce the L2-regularization
    term at the cost of increased target-prediction error. This term, which defaults
    to `1e-3`, is a hyperparameter whose value can be tuned through hyperparameter
    optimization.'
  prefs: []
  type: TYPE_NORMAL
- en: So how does the L2 regularizer help us? Panel B of [figure 8.4](#ch08fig04)
    shows the loss curves from the regularized MLP. By comparing it with the curves
    from the unregularized MLP (panel A of the same figure), you can see that the
    regularized model yields less divergent training and validation loss curves. This
    means that the model is no longer “paying undue attention” to idiosyncratic patterns
    in the training dataset. Instead, the pattern it learns from the training set
    generalizes well to unseen examples in the validation set. In our regularized
    MLP, only the first dense layer incorporated a regularizer, while the second dense
    layer didn’t. But that turned out to be sufficient to overcome the overfitting
    in this case. In the next section, we will look deeper at why smaller kernel values
    lead to less overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the effect of regularization on weight values
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Since the L2 regularizer works by encouraging the kernel of the hidden dense
    layer to have smaller values, we ought to be able to see that the post-training
    kernel values are smaller in the regularized MLP than in the unregularized one.
    How can we do that in TensorFlow.js? The `tfvis.show.layer()` function from tfjs-vis
    makes it possible to visualize a TensorFlow.js model’s weights with one line of
    code. [Listing 8.4](#ch08ex04) is a code excerpt that shows how this is done.
    The code is executed when the training of an MLP model ends. The `tfvis.show.layer()`
    call takes two arguments: the visor surface on which the rendering will happen
    and the layer being rendered.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.4\. Visualizing the weight distribution of layers (from jena-weather/index.js)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The visualization made by this code is shown in [figure 8.5](#ch08fig05). Panels
    A and B show the results from the unregularized and regularized MLPs, respectively.
    In each panel, `tfvis.show.layer()` displays a table of the layer’s weights, with
    details about the names of the weights, their shape and parameter count, min/max
    of the parameter values, and counts of zero and NaN parameter values (the last
    of which can be useful for diagnosing problematic training runs). The layer visualization
    also contains Show Values Distribution buttons for each of the layer’s weights,
    which, when clicked, will create a histogram of the values in the weight.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.5\. Distribution of the values in the kernel with (panel A) and without
    (panel B) L2 regularization. The visualization is created with `tfvis.show.layer()`.
    Note that the x-axes of the two histograms have different scales.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](08fig05_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Comparing the plots for the two flavors of MLP, you can see a clear difference:
    the values of the kernel are distributed over a considerably narrower range with
    the L2 regularization than without. This is reflected in both the min/max values
    (the first row) and in the value histogram. This is regularization at work!'
  prefs: []
  type: TYPE_NORMAL
- en: But why do smaller kernel values result in reduced overfitting and improved
    generalization? An intuitive way to understand this is that L2 regularization
    enforces the principle of Occam’s razor. Generally speaking, a larger magnitude
    in a weight parameter tends to cause the model to fit to fine-grained details
    in the training features that it sees, and a smaller magnitude tends to let the
    model ignore such details. In the extreme case, a kernel value of zero means the
    model doesn’t attend to its corresponding input feature at all. The L2 regularization
    encourages the model to be more “economical” by avoiding large-magnitude weight
    values, and to retain those only when it is worth the cost (when the reduction
    in the target-prediction mismatch term outweighs the regularizer loss).
  prefs: []
  type: TYPE_NORMAL
- en: L2 regularization is but one of the weapons against overfitting in the machine-learning
    practitioner’s arsenal. In [chapter 4](kindle_split_015.html#ch04), we demonstrated
    the power of dropout layers. Dropout is a powerful countermeasure to overfitting
    in general. It helps us reduce overfitting in this temperature-prediction problem
    as well. You can see that yourself by choosing the model type MLP with Dropout
    in the demo UI. The quality of training you get from the dropout-enabled MLP is
    comparable to the one you get from the L2-regularized MLP. We discussed how and
    why dropout works in [section 4.3.2](kindle_split_015.html#ch04lev2sec9) when
    we applied it to an MNIST convnet, so we won’t repeat it here. However, [table
    8.1](#ch08table01) provides a quick overview of the most widely used countermeasures
    to overfitting. It includes an intuitive description of how each of them works
    and the corresponding API in TensorFlow.js. The question as to which countermeasure
    to use for a particular problem is usually answered through 1) following well-established
    models that solve similar problems and 2) treating the countermeasure as a hyperparameter
    and searching for it through hyperparameter optimization ([section 3.1.2](kindle_split_014.html#ch03lev2sec2)).
    In addition, each overfitting-reducing method itself contains tunable parameters
    that can also be determined through hyperparameter optimization (see the last
    column of [table 8.1](#ch08table01).)
  prefs: []
  type: TYPE_NORMAL
- en: Table 8.1\. An overview of commonly used methods for reducing overfitting in
    TensorFlow.js
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Name of method | How the method works | Corresponding API in TensorFlow.js
    | Main free parameter(s) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| L2 regularizer | Assigns a positive loss (penalty) to the weight by calculating
    the summed squares of parameter values of the weight. It encourages the weight
    to have smaller parameter values. | tf.regularizers.l2() See the “[Reducing overfitting
    with weight regularization](#ch08lev2sec3)” section, for example. | L2-regularization
    rate |'
  prefs: []
  type: TYPE_TB
- en: '| L1 regularizer | Like L2 regularizers, encourages the weight parameters to
    be smaller. However, the loss it assigns to a weight is based on the summed *absolute
    values* of the parameters, instead of summed squares. This definition of regularization
    loss causes more weight parameters to become zero (that is, “sparser weights”).
    | tf.regularizers.l1() | L1-regularization rate |'
  prefs: []
  type: TYPE_TB
- en: '| Combined L1-L2 regularizer | A weighted sum of L1 and L2 regularization losses.
    | tf.regularizers.l1l2() | L1-regularization rate L2-regularization rate |'
  prefs: []
  type: TYPE_TB
- en: '| Dropout | Randomly sets a fraction of the inputs to zero during training
    (but not during inference) in order to break spurious correlations (or “conspiracy”
    in Geoff Hinton’s words) among weight parameters that emerge during training.
    | tf.layers.dropout() See [section 4.3.2](kindle_split_015.html#ch04lev2sec9),
    for example. | Dropout rate |'
  prefs: []
  type: TYPE_TB
- en: '| Batch normalization | Learns the mean and standard deviation of its input
    values during training and uses the learned statistics to normalize the inputs
    to zero mean and unit standard deviation as its output. | tf.layers.batchNormalization()
    | Various (see [https://js.tensorflow.org/api/latest/#layers.batchNormalization](https://js.tensorflow.org/api/latest/#layers.batchNormalization))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Early stopping of training based on validation-set loss | Stops model training
    as soon as the epoch-end loss value on the validation set stops decreasing. |
    tf.callbacks.earlyStopping() | minDelta: The threshold below which changes will
    be ignored patience: How many consecutive epochs of no improvement are tolerated
    at most |'
  prefs: []
  type: TYPE_TB
- en: To wrap up this section on visualizing underfitting and overfitting, we provide
    a schematic diagram as a quick rule of thumb for spotting those states ([figure
    8.6](#ch08fig06)). As panel A shows, underfitting is when the model achieves a
    suboptimal (high) loss value, regardless of whether it’s on the training or validation
    set. In panel B, we see a typical pattern of overfitting, where the training loss
    looks fairly satisfactory (low), but the validation loss is worse (higher) in
    comparison. The validation loss can plateau and even start to edge up, even when
    the training-set loss continues to go down. Panel C is the state we want to be
    in—namely, a state where the loss value doesn’t diverge too much between the training
    and validation sets so that the final validation loss is low. Be aware that the
    phrase “sufficiently low” can be relative, especially for problems that no existing
    models can solve perfectly. New models may come out in the future and lower the
    achievable loss relative to what we have in panel C. At that point, the pattern
    in panel C would become a case of underfitting, and we would need to adopt the
    new model type in order to fix it, possibly by going through the cycle of overfitting
    and regularization again.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.6\. A schematic diagram showing the loss curves from simplified cases
    of underfitting (panel A), overfitting (panel B), and just-right fitting (panel
    C) in model training.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](08fig06_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Finally, note that visualization of training is not limited to the losses. Other
    metrics are often visualized to aid in monitoring the training process as well.
    Examples of this are sprinkled throughout the book. For example, in [chapter 3](kindle_split_014.html#ch03),
    we plotted the ROC curves when training a binary classifier for phishing websites.
    We also rendered the confusion matrix when training the iris-flower classifier.
    In [chapter 9](kindle_split_021.html#ch09), we’ll show an example of displaying
    machine-generated text during the training of a text generator. That example won’t
    involve a GUI but will nonetheless provide useful and intuitive real-time information
    about the state of the model’s training. Specifically, by looking at the text
    generated by the model, you can get an intuitive sense of how good the text generated
    by the model currently is.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3\. The universal workflow of machine learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Up to this point, you have seen all the important steps in designing and training
    a machine-learning model, including acquiring, formatting, visualizing, and ingesting
    data; choosing the appropriate model topology and loss function for the dataset;
    and training the model. You’ve also seen some of the most important failure modes
    that may appear during the training process: underfitting and overfitting. So,
    this is a good place for us to look back at what we’ve learned so far and reflect
    on what’s common among the machine-learning model processes for different datasets.
    The resulting abstraction is what we refer to as *the universal workflow of machine
    learning*. We’ll list the workflow step-by-step and expand on the key considerations
    in each step:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Determine if machine learning is the right approach*. First, consider if machine
    learning is the right approach to your problem, and proceed to the next steps
    only if the answer is yes. In some cases, a non-machine-learning approach will
    work equally well or perhaps even better, at a lower cost. For example, given
    enough model-tuning efforts, you can train a neural network to “predict” the sum
    of two integers by taking the integers as text input data (for example, the addition-rnn
    example in the tfjs-examples repository). But this is far from the most efficient
    or reliable solution to this problem: the good old addition operation on the CPU
    suffices in this case.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Define the machine-learning problem and what you are trying to predict using
    the data*. In this step, you need to answer two questions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*What sort of data is available?* In supervised learning, you can only learn
    to predict something if you have labeled training data available. For example,
    the weather-prediction model we saw earlier in this chapter is possible only because
    the Jena-weather dataset is available. Data availability is usually a limiting
    factor in this stage. If the available data is insufficient, you may need to collect
    more data or hire people to manually label an unlabeled dataset.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*What type of problem are you facing?* Is it binary classification, multiclass
    classification, regression, or something else? Identifying the problem type will
    guide your choice of model architecture, loss function, and so forth.You can’t
    move on to the next step until you know what the inputs and outputs are and what
    data you’ll use. Be aware of the hypotheses you’ve made implicitly at this stage:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You hypothesize that the outputs can be predicted given the inputs (the input
    alone contains enough information for a model to predict the output for all possible
    examples in this problem).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You hypothesize that the data available is sufficient for a model to learn
    this input-output relationship.Until you have a working model, these are just
    hypotheses waiting to be validated or invalidated. Not all problems are solvable:
    just because you’ve assembled a large labeled dataset that maps from X to Y doesn’t
    mean that X contains enough information for the value of Y. For instance, if you’re
    trying to predict the future price of a stock based on the history of the stock’s
    price, you’ll likely fail because the price history doesn’t contain enough predictive
    information about the future price. One class of unsolvable problems you should
    be aware of is *nonstationary* problems, in which the input-output relation changes
    with time. Suppose you’re trying to build a recommendation engine for clothes
    (given a user’s clothes purchase history), and you’re training your model on only
    one year’s data. The big issue here is that people’s tastes for clothes change
    with time. A model that works accurately on the validation data from last year
    isn’t guaranteed to work equally accurately this year. Keep in mind that machine
    learning can only be used to learn patterns that are present in the training data.
    In this case, getting up-to-date data and continuously training new models will
    be a viable solution.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Identify a way to reliably measure the success of a trained model on your
    goal*. For simple tasks, this may be just prediction accuracy, precision and recall,
    or the ROC curve and the AUC value (see [chapter 3](kindle_split_014.html#ch03)).
    But in many cases, it will require more sophisticated domain-specific metrics,
    such as customer retention rate and sales, which are better aligned with higher-level
    goals, such as the success of the business.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Prepare the evaluation process*. Design the validation process that you’ll
    use to evaluate your models. In particular, you should split your data into three
    homogeneous yet nonoverlapping sets: a training set, a validation set, and a test
    set. The validation- and test-set labels ought not to leak into the training data.
    For instance, with temporal prediction, the validation and test data should come
    from time intervals *after* the training data. Your data preprocessing code should
    be covered by tests to guard against bugs.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Vectorize the data*. Turn the data into tensors, also known as *n*-dimensional
    arrays, the lingua franca of machine-learning models in frameworks such as TensorFlow.js
    and TensorFlow. Note the following guidelines for data vectorization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The numeric values taken by the tensors should usually be scaled to small and
    centered values: for example, within the `[-1, 1]` or `[0, 1]` interval.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If different features (such as temperature and wind speed) take values in different
    ranges (heterogeneous data), then the data ought to be normalized, usually z-normalized
    to zero mean and unit standard deviation for each feature.Once your tensors of
    input data and target (output) data are ready, you can begin to develop models.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Develop a model that beats a commonsense baseline*. Develop a model that beats
    a non-machine-learning baseline (such as predicting the population average for
    a regression problem or predicting the last data point in a time-series prediction
    problem), thereby demonstrating that machine learning can truly add value to your
    solution. This may not always be the case (see step 1). Assuming things are going
    well, you need to make three key choices to build your first baseline-beating,
    machine-learning model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Last-layer activation*—This establishes useful constraints for the model’s
    output. This activation should suit the type of problem you are solving. For example,
    the phishing-website classifier in [chapter 3](kindle_split_014.html#ch03) used
    the sigmoid activation for its last (output) layer due to the binary-classification
    nature of the problem, and the temperature-prediction models in this chapter used
    the linear activation for the layer owing to the regression nature of the problem.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Loss function*—In a way similar to last-layer activation, the loss function
    should match the problem you’re solving. For instance, use `binaryCrossentropy`
    for binary-classification problems, `categoricalCrossentropy` for multiclass-classification
    problems, and `meanSquaredError` for regression problems.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Optimizer configuration*—The optimizer is what drives the updates to the neural
    network’s weights. What type of optimizer should be used? What should its learning
    rate be? These are generally questions answered by hyperparameter tuning. But
    in most cases, you can safely start with the `rmsprop` optimizer and its default
    learning rate.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Develop a model with sufficient capacity and to overfit the training data*.
    Gradually scale up your model architecture by manually changing hyperparameters.
    You want to reach at a model that overfits the training set. Remember that the
    universal and central tension in supervised machine learning is between *optimization*
    (fitting the data seen during training) and *generalization* (being able to make
    accurate predictions for unseen data). The ideal model is one that stands right
    at the border between underfitting and overfitting: that is, between under-capacity
    and over-capacity. To figure out where this border is, you must first cross it.
    In order to cross it, you must develop a model that overfits. This is usually
    fairly easy. You may'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add more layers
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Make each layer bigger
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Train the model for more epochsAlways use visualization to monitor the training
    and validation losses, as well as any additional metrics that you care about (such
    as AUC) on both the training and validation sets. When you see the model’s accuracy
    on the validation set begin to degrade ([figure 8.6](#ch08fig06), panel B), you’ve
    achieved overfitting.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Add regularization to your model and tune the hyperparameters*. The next step
    is to add regularization to your model and further tune its hyperparameters (usually
    in an automated way) to get as close as possible to the ideal model that neither
    underfits nor overfits. This step will take the most time, even though it can
    be automated. You’ll repeatedly modify your model, train it, evaluate it on the
    validation set (not the test set at this point), modify it again, and repeat until
    the model is as good as it can get. These are the things you should try in terms
    of regularization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add dropout layers with different dropout rates.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Try L1 and/or L2 regularization.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Try different architectures: add or remove a small number of layers.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Change other hyperparameters (for example, the number of units of a dense layer).Beware
    of validation-set overfitting when tuning hyperparameters. Because the hyperparameters
    are determined based on the performance on the validation set, their values will
    be overspecialized for the validation set and therefore may not generalize well
    to other data. It is the purpose of the test set to obtain an unbiased estimate
    of the model’s accuracy after hyperparameter tuning. So, you shouldn’t use the
    test set when tuning the hyperparameters.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the universal workflow of machine learning! In [chapter 12](kindle_split_025.html#ch12),
    we’ll add two more practically oriented steps to it (an evaluation step and a
    deployment step). But for now, this is a recipe for how to go from a vaguely defined
    machine-learning idea to a model that’s trained and ready to make some useful
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: With this foundational knowledge, we’ll start exploring more advanced types
    of neural networks in the upcoming part of the book. We’ll start from models designed
    for sequential data in [chapter 9](kindle_split_021.html#ch09).
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the temperature-prediction problem, we found that the linear regressor significantly
    underfit the data and produced poor prediction results on both the training and
    validation sets. Would adding L2 regularization to the linear regressor help improve
    the accuracy of such an underfitting model? It should be easy to try it out yourself
    by modifying the `buildLinearRegressionModel()` function in the file jena-weather/models.js.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When predicting the temperature of the next day in the Jena-weather example,
    we used a look-back period of 10 days to produce the input features. A natural
    question is, what if we use a longer look-back period? Is including more data
    going to help us get more accurate predictions? You can find this out by modifying
    the `const lookBack` in jena-weather/index.js and running the training in the
    browser (for example, by using the MLP with L2 regularization). Of course, a longer
    look-back period will increase the size of the input features and lead to longer
    training time. So, the flip side of the question is, can we use a shorter look-back
    period without sacrificing the prediction accuracy significantly? Try this out
    as well.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: tfjs-vis can aid the visualization of a machine-learning model’s training process
    in the browser. Specifically, we showed how tfjs-vis can be used to
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualize the topology of TensorFlow.js models.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Plot loss and metrics curves during training.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Summarize weight distributions after training. We showed concrete examples of
    these visualization workflows.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Underfitting and overfitting are fundamental behaviors of machine-learning models
    and should be monitored and understood in every machine-learning problem. They
    can both be seen by comparing the loss curves from the training and validation
    sets during training. The built-in `tfvis.show.fitCallbacks()` method helps you
    visualize these curves in the browser with ease.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The universal workflow of machine learning is a list of common steps and best
    practices of different types of supervised learning tasks. It goes from deciding
    the nature of the problem and the requirements on the data to finding a model
    that sits nicely on the border between underfitting and overfitting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
