- en: 10 Large Language Models in the real world
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现实世界中的10个大型语言模型
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖内容
- en: Understanding how conversational LLMs like ChatGPT work
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解对话型LLMs（如ChatGPT）的工作原理
- en: Jailbreaking an LLM to get it to say things its programmers don’t want it to
    say
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非法破解LLM以获取其程序员不希望其说的内容
- en: Recognizing errors, misinformation, and biases in LLM output
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别LLM输出中的错误、错误信息和偏见
- en: Fine-tuning LLMs on your own data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用您自己的数据对LLMs进行微调
- en: Finding meaningful search results for your queries (semantic search)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过语义搜索为您的查询找到有意义的搜索结果
- en: Speeding up your vector search with Approximate Nearest Neighbor Algorithm
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用近似最近邻算法加速您的向量搜索
- en: Generating fact-based well-formed text with LLMs
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LLMs生成基于事实的格式良好的文本
- en: If you increase the number of parameters for transformer-based language models
    to obscene sizes, you can achieve some surprisingly impressive results. Researchers
    call these surprises *emergent properties* but they may be a mirage.^([[1](#_footnotedef_1
    "View footnote.")]) Since the general public started to become aware of the capabilities
    of really large transformers, they are increasingly referred to as Large Language
    Models (LLMs). The most sensational of these surprises is that chatbots built
    using LLMs generate intelligent-sounding text. You’ve probably already spent some
    time using conversational LLMs such as ChatGPT, You.com and Llamma 2\. And like
    most, you probably hope that if you get good at prompting them, they can help
    you get ahead in your career and even help you in your personal life. Like most,
    you are probably relieved to finally have a search engine and virtual assistant
    that actually gives you direct, smart-sounding answers to your questions. This
    chapter will help you use LLMs better so you can do more than merely *sound* intelligent.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将基于Transformer的语言模型的参数数量增加到令人费解的规模，您可以实现一些令人惊讶的结果。研究人员将这些产生的意外称为* emergent
    properties*，但它们可能是一个幻觉。 ^([[1](#_footnotedef_1 "查看来源")]) 自从普通大众开始意识到真正大型变压器的能力以来，它们越来越被称为大型语言模型（LLMs）。其中最耸人听闻的惊喜是使用LLMs构建的聊天机器人可以生成听起来智能的文本。您可能已经花了一些时间使用诸如ChatGPT，You.com和Llamma
    2的对话型LLMs。和大多数人一样，您可能希望如果您在提示它们方面变得熟练，它们可以帮助您在职业生涯中取得进展，甚至在个人生活中也能帮助您。和大多数人一样，您可能终于感到松了一口气，因为您终于有了一个能够给出直接、智慧的回答的搜索引擎和虚拟助手。本章将帮助您更好地使用LLMs，以便您不仅仅是*听起来*智能。
- en: 'This chapter will help you understand how generative Large Language Models
    work. We will also discuss the problems with practical applications of LLMs so
    you can use them smartly and minimize their harm to you and others:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将帮助您理解生成LLMs的工作方式。我们还将讨论LLMs实际应用中的问题，以便您可以聪明地使用它们并将对自己和他人的伤害降至最低：
- en: '*Misinformation*:: LLMs trained on social media will amplify misinformation'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*错误信息*：在社交媒体上训练的LLMs将放大错误信息'
- en: '*Reliability*:: LLMs will sometimes insert errors into your code and words,
    and these errors are very difficult to spot'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可靠性*：LLMs有时会在您的代码和文字中插入错误，这些错误非常难以察觉'
- en: '*Impact on Learning*:: Used incorrectly, LLMs can reduce your metacognition
    skill'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*对学习的影响*：使用不当，LLMs可能降低您的元认知能力'
- en: '*Impact on Collective Intelligence*:: Flooding the infosphere with false and
    inhuman text devalues authentic and thoughtful human-generated ideas.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*对集体智慧的影响*：用虚假和非人类的文本淹没信息领域会贬值真实而深思熟虑的人类生成的思想。'
- en: '*Bias*:: LLMs have algorithmic biases that are harming us all in ways we rarely
    notice except when it affects us personally, creating division and mistrust'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*偏见*：LLMs具有算法偏见，这些偏见以我们很少注意到的方式伤害我们，除非它影响到我们个人，导致分裂和不信任'
- en: '*Accessibility*:: Most people do not have access to the resources and skills
    required to use LLMs effectively, disadvantaging the already disadvantaged'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可访问性*：大多数人没有获得有效使用LLMs所需的资源和技能，这使得已经处于不利地位的人更加不利。'
- en: '*Environmental Impact*:: In 2023 LLMs emitted more than 1000 kg/day CO2e (carbon
    dioxide equivalent) ^([[2](#_footnotedef_2 "View footnote.")]) ^([[3](#_footnotedef_3
    "View footnote.")])'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*环境影响*：2023年，LLMs每天排放超过1000公斤的二氧化碳当量[[2](#_footnotedef_2 "查看来源")]] [[3](#_footnotedef_3
    "查看来源")]]'
- en: You can mitigate a lot of these harms by building and using LLMs that are smarter
    and more efficient. That’s what this chapter is all about. You will see how to
    build LLMs that generate more intelligent, trustworthy, and equitable words. And
    you will learn how to make your LLMs more efficient and less wasteful, not only
    reducing the environmental impact but also helping more people gain access to
    the power of LLMs.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 通过构建和使用更智能、更高效的LLMs，您可以减轻许多这些伤害。这就是本章的全部内容。您将看到如何构建生成更智能、更可信、更公平的LLMs。您还将了解如何使您的LLMs更高效、更节约，不仅减少环境影响，还帮助更多人获得LLMs的力量。
- en: 10.1 Large Language Models (LLMs)
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.1 大型语言模型（LLMs）
- en: The largest of the LLMs have more than a trillion parameters. Models this large
    require expensive specialized hardware and many months of compute on high-performance
    computing (HPC) platforms. At the time of this writing, training a modest 100B
    parameter model on just the 3 TB of text in Common Crawl would cost at least $3
    M.^([[4](#_footnotedef_4 "View footnote.")]) Even the crudest model of the human
    brain would have to have more than 100 trillion parameters to account for all
    the connections between our neurons. Not only do LLMs have high-capacity "brains"
    but they have binged on a mountain of text — all the interesting text that NLP
    engineers can find on the Internet. And it turns out that by following online
    *conversations*, LLMs can get really good at imitating intelligent human conversation.
    Even BigTech engineers responsible for designing and building LLMs were fooled.
    Humans have a soft spot for anything that appears to be intentional and intelligent.
    We’re easily fooled because we *anthropomorphize* everything around us, from pets
    to corporations and video game characters.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最大的LLMs具有超过一万亿的参数。这么大的模型需要昂贵的专门硬件和数月时间在高性能计算（HPC）平台上进行计算。在撰写本文时，仅在Common Crawl的3TB文本上训练一个适度的100B参数模型就至少需要花费300万美元^([[4](#_footnotedef_4
    "查看脚注。")])。即使是最粗糙的人脑模型也必须具有超过100万亿个参数，以解释我们神经元之间的所有连接。LLMs不仅具有高容量的“大脑”，而且它们已经吞食了一座文本山——所有NLP工程师在互联网上找到的有趣文本。结果发现，通过跟随在线*对话*，LLMs可以非常擅长模仿智能的人类对话。甚至负责设计和构建LLMs的大型技术公司的工程师们也被愚弄了。人类对任何看起来有意图和智能的事物都有一种软肋。我们很容易被愚弄，因为我们把周围的一切都*拟人化*了，从宠物到公司和视频游戏角色。
- en: This was surprising for both researchers and everyday technology users. It turns
    out that if you can predict the next word, and you you add a little human feedback,
    your bot can do a lot more than just entertain you with witty banter. Chatbots
    based on LLMs can have seemingly intelligent conversations with you about extremely
    complex topics. And they can carry out complex instructions to compose essays
    or poems or even suggest seemingly intelligent lines of argument for your online
    debates.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这对研究人员和日常科技用户来说都是令人惊讶的。原来，如果你能预测下一个词，并加入一点人类反馈，你的机器人就能做更多事情，而不仅仅是用风趣的话语逗乐你。基于LLMs的聊天机器人可以与你进行关于极其复杂话题的似乎智能的对话。它们可以执行复杂的指令，撰写文章或诗歌，甚至为你的在线辩论提供看似聪明的论点。
- en: But there is a small problem — LLMs aren’t logical, reasonable, or even intentional,
    much less *intelligent*. Reasoning is the very foundation of both human intelligence
    and artificial intelligence. You may hear people talking about how LLMs can pass
    really hard tests of intelligence, like IQ tests or college entrance exams. But
    LLMs are just faking it. Remember, LLMs are trained on a large portion of all
    the question-answer pairs in various standardized tests and exams. A machine that
    has been trained on virtually the entire Internet can appear to be smart by merely
    mashing up word sequences that it has seen before. It can regurgitate patterns
    of words that look a lot like reasonable answers to any question that has ever
    been posed online.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 但是有一个小问题——LLMs不具备逻辑、合理性，甚至不是*智能*。推理是人类智能和人工智能的基础。你可能听说过人们如何谈论LLMs能够通过真正困难的智力测试，比如智商测试或大学入学考试。但是LLMs只是在模仿。记住，LLMs被训练用于各种标准化测试和考试中的几乎所有问答对。一个被训练了几乎整个互联网的机器可以通过仅仅混合它以前见过的单词序列来表现得很聪明。它可以重复出看起来很像对任何曾经在网上提出的问题的合理答案的单词模式。
- en: Tip
  id: totrans-23
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: What about computational complexity? In a computer science course, you would
    estimate the complexity of the question-answering problem as \(O(n^2)\), where
    *n* is the number of possible questions and answers - a huge number. Transformers
    can cut through this complexity to learn the hidden patterns that tell it which
    answers are correct. In machine learning, this ability to recognize and reuse
    patterns in data is called *generalization*. The ability to generalize is a hallmark
    of intelligence. But the AI in an LLM is not generalizing about the physical world,
    it is generalizing about natural language text. LLMs are only "faking it", pretending
    to be intelligent, by recognizing patterns in words from the Internet. And how
    we use words in the virtual world isn’t always reflective of reality.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 那么计算复杂度呢？在计算机科学课程中，你会将问答问题的复杂度估计为 \(O(n^2)\)，其中 *n* 是可能的问题和答案的数量 - 一个巨大的数字。变形金刚可以通过这种复杂性来学习隐藏的模式，以告诉它哪些答案是正确的。在机器学习中，识别和重用数据中的模式的能力被称为
    *泛化*。泛化能力是智能的标志。但是 LLN 中的 AI 并不是对物理世界进行泛化，而是对自然语言文本进行泛化。LLN 只是在 "假装"，通过识别互联网上的单词模式来假装智能。我们在虚拟世界中使用单词的方式并不总是反映现实。
- en: You have probably been impressed with the seeming quality of your conversations
    with LLMs such as ChatGPT. LLMs answer almost any question with confidence and
    seeming intelligence. But *seeming* is not always being. If you ask the right
    questions LLMs stumble into *hallucinations* or just plain nonsense. And it’s
    nearly impossible to predict these holes in the Swiss cheese of their abilities.
    These problems were immediately evident at the launch of ChatGPT in 2022 and subsequent
    launch attempts by others.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会对与 ChatGPT 等 LLN 进行的对话的表现印象深刻。LLN 几乎可以自信并且似乎很聪明地回答任何问题。但是 *似乎* 并不总是如此。如果你问出了正确的问题，LLN
    会陷入 *幻觉* 或者纯粹是胡言乱语。而且几乎不可能预测到它们能力的这些空白。这些问题在 2022 年 ChatGPT 推出时立即显现出来，并在随后由其他人尝试推出时继续存在。
- en: To see what’s really going on, it can help to test an early version of the LLM
    behind ChatGPT. Unfortunately, the only OpenAI LLM that you can download is GPT-2,
    released in 2019\. All these years later, they still have not released the full-size
    1.5 billion parameter model, but instead released a half-size model with 775 million
    parameters. Nonetheless, clever open source developers were able to reverse engineer
    one called OpenGPT-2.^([[5](#_footnotedef_5 "View footnote.")]) Below you will
    use the official OpenAI half-size version to give you a feel for the limitations
    of ungrounded LLMs. Later we’ll show you how scaling up and adding information
    retrieval can really improve things.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看清楚事情的真相，测试 ChatGPT 背后的 LLN 的早期版本可能会有所帮助。不幸的是，你只能下载到 OpenAI 在 2019 年发布的 GPT-2，他们至今仍未发布
    15 亿参数的完整模型，而是发布了一个拥有 7.75 亿参数的半尺寸模型。尽管如此，聪明的开源开发者仍然能够反向工程一个名为 OpenGPT-2 的模型。^[[5]](#_footnotedef_5
    "查看脚注。") 在下面，你将使用官方的 OpenAI 半尺寸版本，以便让你感受到无基础 LLN 的局限性。稍后我们将向您展示如何通过扩大规模和添加信息检索来真正改善事物。
- en: Listing 10.1 Count cow legs with GPT-2
  id: totrans-27
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '[用 GPT-2 计算牛的腿数](https://wiki.example.org/gpt2_count_cow_legs)'
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: And when ChatGPT launched, the GPT-3 model wasn’t any better at common sense
    reasoning. As the model was scaled up in size and complexity, it was able to memorize
    more and more math problem answers like this, but it didn’t generalize based on
    real-world experience. No common sense logical reasoning skill ever emerged even
    as newer and newer versions were released, including GPT-3.5 and GPT-4.0\. When
    asked to answer technical or reasoning questions about the real world, LLMs often
    generate nonsense that might look reasonable to a layperson, but they often contain
    errors that would be obvious if you look hard enough. And they are easy to jailbreak,
    forcing an LLM to say things (such as toxic dialog) that the LLM designers are
    trying to prevent them from saying.^([[6](#_footnotedef_6 "View footnote.")])
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当 ChatGPT 推出时，GPT-3 模型在常识推理方面并没有任何进展。随着模型规模和复杂性的扩大，它能够记忆越来越多的数学问题答案，但它并没有基于真实世界的经验进行泛化。即使发布了更新的版本，包括
    GPT-3.5 和 GPT-4.0，通常也不会出现常识逻辑推理技能。当被要求回答关于现实世界的技术或推理问题时，LLN 往往会生成对于外行人来说看起来合理的胡言乱语，但是如果你仔细观察，就会发现其中存在明显的错误。而且它们很容易被越狱，强迫一个
    LLN 说出（如毒性对话）LLN 设计者试图防止它们说出的话。^[[6]](#_footnotedef_6 "查看脚注。")
- en: Interestingly, after launch, the model slowly got better at answering questions
    it struggled with at launch. How did they do that? Like many LLM-based chatbots,
    ChatGPT uses *reinforcement learning with human feedback* (RLHF). This means that
    the human feedback is used to gradually adjust the model weights to improve the
    accuracy of the LLMs' next-word predictions. For ChatGPT there is often a *like
    button* you can click to let it know when you are happy with an answer to your
    prompt.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，推出后，模型在应对推出时遇到困难的问题时逐渐变得更好了。他们是怎么做到的？像许多基于 LLM 的聊天机器人一样，ChatGPT 使用 *带有人类反馈的强化学习*（RLHF）。这意味着人类反馈被用来逐渐调整模型权重，以提高
    LLM 下一个词预测的准确性。对于 ChatGPT，通常有一个 *喜欢按钮*，你可以点击它，让它知道你对提示的答案感到满意。
- en: If you think about it, the like button creates an incentive for LLMs trained
    this way to encourage the number of like button clicks from users by generating
    likable words. It’s similar to the way that dogs, parrots, and even horses can
    appear to do math if you train them this way, letting them know whenever you are
    happy with their answer. They will find *correlates* with the right answer in
    their training and use that predict their next word (or stomp of the hoof). Just
    as it was for the horse Clever Hans, ChatGPT can’t count and has no real mathematical
    ability.^([[7](#_footnotedef_7 "View footnote.")]) And this is the same trick
    that social media companies use to create hype, and divide us into echo chambers
    where we only hear what we want to hear, to keep us engaged so they can hijack
    our attention to sell it to advertisers.^([[8](#_footnotedef_8 "View footnote.")])
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细想想，喜欢按钮会激励以这种方式训练的 LLM 鼓励用户点击喜欢按钮，通过生成受欢迎的词语。这类似于训练狗、鹦鹉甚至马匹，让它们知道你对它们的答案满意时，它们会表现出进行数学运算的样子。它们将在训练中找到与正确答案的相关性，并使用它来预测下一个词（或蹄子的跺地声）。就像对于马智能汉斯一样，ChatGPT
    无法计数，也没有真正的数学能力。^([[7](#_footnotedef_7 "View footnote.")])这也是社交媒体公司用来制造炒作、把我们分成只听到我们想听到的声音的回音室的同样伎俩，以保持我们的参与，以便他们可以挟持我们的注意力并将其出售给广告商。^([[8](#_footnotedef_8
    "View footnote.")])
- en: And OpenAI has chosen to target "likability" (popularity) as the objective for
    its large language models. This maximizes the number of signups and hype surrounding
    their product launches. And this machine learning objective function was very
    effective at accomplishing their objective. OpenAI executives bragged that they
    had 100 million users only two months after launch. These early adopters flooded
    the Internet with unreliable natural language text. Novice LLM users even created
    news articles and legal briefs with fabricated references that had to be thrown
    out by tech-savvy judges. ^([[9](#_footnotedef_9 "View footnote.")])
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 选择以“受欢迎程度”（流行度）作为其大型语言模型的目标。这最大化了注册用户数和产品发布周围的炒作。这个机器学习目标函数非常有效地实现了他们的目标。OpenAI
    的高管夸耀说，他们在推出后仅两个月就拥有了1亿用户。这些早期采用者用不可靠的自然语言文本涌入互联网。新手 LLM 用户甚至用虚构的参考文献创建新闻文章和法律文件，这些文献必须被精通技术的法官驳回。^([[9](#_footnotedef_9
    "View footnote.")])
- en: Imagine your LLM is going to be used to respond to middle school students' questions
    in real time. Or maybe you want to use an LLM to answer health questions. Even
    if you are only using the LLM to promote your company on social media. If you
    need it to respond in real-time, without continuous monitoring by humans, you
    will need to think about ways to prevent it from saying things that harm your
    business, your reputation, or your users. You’ll need to do more than simply connect
    your users directly to the LLM.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你的 LLM 将用于实时回答初中学生的问题。或者你可能想使用 LLM 回答健康问题。即使你只是在社交媒体上使用 LLM 来宣传你的公司。如果你需要它实时回应，而不需要持续由人类监控，你需要考虑如何防止它说出对你的业务、声誉或用户有害的话。你需要做的不仅仅是直接将用户连接到
    LLM。
- en: 'There are three popular approaches to reducing an LLM’s toxicity and reasoning
    errors:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 减少 LLM 毒性和推理错误有三种流行的方法：
- en: '*Scaling*: Make it bigger (and hopefully smarter)'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*扩展*：使其更大（并希望更聪明）'
- en: '*Guardrails*: Monitoring it to detect and prevent it from saying bad things'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*防护栏*：监控它以检测和防止它说坏话'
- en: '*Grounding*: Augment an LLM with a knowledge base of real-world facts'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*接地*：用真实世界事实的知识库增强 LLM。'
- en: '*Retrieval*: Augment an LLM with a search engine to retrieve text used to generate
    responses.'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*检索*：用搜索引擎增强 LLM，以检索用于生成响应的文本。'
- en: The next two sections will explain the advantages and limitations of the scaling
    and guardrail approaches. You will learn about grounding and retrieval in chapter
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的两个部分将解释扩展和防护栏方法的优点和限制。你将在第 n 章学习关于接地和检索的知识。
- en: 10.1.1 Scaling up
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.1 扩大规模
- en: One of the attractive aspects of LLMs is that you only need to add data and
    neurons if you want to improve your bot. You don’t have to handcraft ever more
    complicated dialog trees and rules. OpenAI placed a billion-dollar bet on the
    idea that the ability to handle complex dialog and reason about the world would
    emerge once they added enough data and neurons. It was a good bet. Microsoft invested
    more than a billion dollars in ChatGPT’s emergent ability to respond plausibly
    to complex questions.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 的一个吸引人之处在于，如果你想提高你的机器人能力，只需要添加数据和神经元就可以了。你不需要手工制作越来越复杂的对话树和规则。OpenAI 押注数十亿美元的赌注是，他们相信只要添加足够的数据和神经元，处理复杂对话和推理世界的能力就会相应增强。这是一个正确的押注。微软投资了超过十亿美元在
    ChatGPT 对于复杂问题的合理回答能力上。
- en: 'However many researchers question whether this overwhelming complexity in the
    model is merely hiding the flaws in ChatGPT’s reasoning. Many researchers believe
    that increasing the dataset does not create more generally intelligent behavior
    just more confident and intelligent-*sounding* text. The authors of this book
    are not alone in holding this opinion. Way back in 2021, in the paper "On the
    Dangers of Stochastic Parrots: Can Language Models Be Too Big?" prominent researchers
    explained how the appearance of understanding in LLMs was an illusion. And they
    were fired for the sacrilege of questioning the ethics and reasonableness of OpenAI’s
    "spray and pray" approach to AI — relying exclusively on the hope that more data
    and neural network capacity would be enough to create intelligence. ^([[10](#_footnotedef_10
    "View footnote.")])'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，许多研究人员质疑模型中的这种复杂性是否只是掩盖了 ChatGPT 推理中的缺陷。许多研究人员认为，增加数据集并不能创造更普遍智能的行为，只会产生更自信和更聪明的-*听上去如此*-文本。本书的作者并不是唯一一个持有这种观点的人。早在
    2021 年，*在《关于随机鹦鹉的危险性：语言模型能太大吗？》*一文中，杰出的研究人员解释了 LLM 的理解表象是一种幻觉。他们因为质疑 OpenAI 的“喷洒祈祷”人工智能方法的伦理性和合理性而被辞退，这种方法完全依赖于更多的数据和神经网络容量能够创建出智能。
- en: Figure [10.1](#figure-llm-survey) gives a brief history of the rapid increase
    in the size and number of LLMs over the past three years.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [10.1](#figure-llm-survey) 概述了过去三年中 LLM 大小和数量的快速增长的简要历史。
- en: Figure 10.1 Large Language Model sizes
  id: totrans-44
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 10.1 大型语言模型大小
- en: '![llm survey](images/llm_survey.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![llm survey](images/llm_survey.png)'
- en: To put these model sizes into perspective, a model with a trillion trainable
    parameters has less than 1% of the number of connections between neurons than
    an average human brain has. This is why researchers and large organizations have
    been investing millions of dollars in the compute resources required to train
    the largest language models.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对比这些模型的大小，具有万亿个可训练参数的模型的神经元之间的连接数量不到一个平均人脑的 1%。这就是为什么研究人员和大型组织一直在投资数百万美元的计算资源，以训练最大的语言模型所需的资源。
- en: Many researchers and their corporate backers are hopeful that increased size
    will unlock human-like capabilities. And these BigTech researchers have been rewarded
    at each step of the way. 100 B parameter models such as BLOOM and InstructGPT
    revealed the capacity for LLMs to understand and respond appropriately to complex
    instructions for creative writing tasks such as composing a love poem from a Klingon
    to a human. And then trillion parameter models such as GPT-4 can perform few-shot
    learning where the entire machine learning training set is contained within a
    single conversational prompt. It seems that every jump in the size and expense
    of LLMs creates a bigger and bigger payday for the bosses and investors in these
    corporations.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 许多研究人员和他们的公司支持者都希望通过增加模型规模来实现类似人类的能力。而这些大型科技公司的研究人员在每个阶段都得到了回报。像 BLOOM 和 InstructGPT
    这样的 100 亿参数模型展示了 LLM 理解和适当回答复杂指令的能力，例如从克林贡语到人类的情书创作。而万亿参数模型如 GPT-4 则可以进行一次学习，其中整个机器学习训练集都包含在一个单一的对话提示中。似乎，LLM
    的每一次规模和成本的增加都为这些公司的老板和投资者创造了越来越大的回报。
- en: Each order of magnitude increase in model capacity (size) seems to unlock more
    surprising capabilities. In the GPT-4 Technical report, the OpenAI researchers
    explain the surprising capabilities that emerged.^([[11](#_footnotedef_11 "View
    footnote.")]) These are the same researchers who invested a lot of their time
    and money into this idea that scale (and attention) is all you need so they may
    not be the best people to evaluate the emmergent properties of their model. The
    researchers at Google who developed PaLM also noted all the emergent properties
    their own scaling research "discovered." Surprisingly Google researchers found
    that most capabilities they measured were not emergent at all, but rather these
    capabilities scaled linearly, sublinearly, or not at all (flat).^([[12](#_footnotedef_12
    "View footnote.")]) In more than a third of the intelligence and accuracy benchmarks
    that they ran, researchers found that the LLM approach to learning was no better
    than random chance. Scaling up did not improve things at all.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 模型容量（大小）每增加一个数量级，似乎就会解锁更多令人惊讶的能力。在GPT-4技术报告中，OpenAI的研究人员解释了出现的令人惊讶的能力。这些是投入了大量时间和金钱的研究人员，他们认为规模（和注意力）就是你需要的全部，所以他们可能不是最佳的评估其模型新出现属性的人员。开发PaLM的Google研究人员也注意到了他们自己的缩放研究“发现”的所有新出现属性。令人惊讶的是，Google的研究人员发现，他们测量到的大多数能力根本不是新出现的，而是这些能力线性地、次线性地或根本不扩展（flat）。在超过三分之一的智能和准确性基准测试中，研究人员发现，LLM学习方法和随机机会相比并没有任何改善。
- en: Here is some code and data you can use to explore the results from the paper
    "Emergent Abilities of Large Language Models."
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些代码和数据，你可以用它们来探索论文“大型语言模型的新能力”的结果。
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The code snippet gives you an alphabetical sampling of the 130 nonemergent capabilities
    cataloged by Google researchers. The "flat" labels mean that increasing the size
    of an LLM did not increase the accuracy of the LLM on these tasks by any measurable
    or statistically significant amount. You can see that 35% (`45/130`) of the nonemergent
    capabilities were labeled as having "flat" scaling. "Sublinear scaling" means
    that increasing the dataset size and number of parameters only increases the accuracy
    of the LLM less and less, giving diminishing returns on your investment in LLM
    size. For the 27 tasks labeled as scaling sublinearly, you will need to change
    the architecture of your language model if you ever want to achieve human-level
    capability. So the paper that provided this data shows that the current transformer-based
    language models don’t scale at all for a large portion of the most interesting
    tasks that are needed to demonstrate intelligent behavior.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 代码片段给出了由Google研究人员编目的130个非新出现能力的字母采样。 "flat"标签意味着增加LLM的大小并没有显著增加LLM在这些任务上的准确性。你可以看到35%（`45/130`）的非新出现能力被标记为“flat”缩放。
    "Sublinear scaling"意味着增加数据集大小和参数数量只会越来越少地增加LLM的准确性，对LLM大小的投资回报逐渐减少。对于被标记为缩放次线性的27个任务，如果你想达到人类水平的能力，你将需要改变你语言模型的架构。因此，提供这些数据的论文表明，目前基于变压器的语言模型在大部分最有趣的任务上根本不会缩放，这些任务是需要展示智能行为的。
- en: Llama 2
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Llama 2
- en: So you’ve already tried GPT-2 with 775 million parameters. What happens when
    you scale up by a factor of 10? Llama 2, Vicuna, and Falcon were the latest and
    most performant open source models at the time of writing this. Llama 2 comes
    in three sizes, there are 7 billion, 13 billion and 70 billion parameter versions.
    The smallest model, Llama 2 7B, is probably the only one you will be able to download
    and run in a reasonable amount of time.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经尝试过拥有775亿参数的GPT-2了。当你将规模扩大10倍时会发生什么呢？在我写这篇文章的时候，Llama 2、Vicuna和Falcon是最新且性能最好的开源模型。Llama
    2有三种规模，分别是70亿、130亿和700亿参数版本。最小的模型，Llama 2 7B，可能是你唯一能在合理时间内下载并运行的。
- en: The Llama 2 7B model files require 10 GB of storage (and network data) to download.
    Once the Llama 2 weights are are decompressed in RAM it will likely use 34 GB
    or more on your machine. This code the model weights from Hugging Face Hub which
    took more than 5 minutes on our 5G Internet connection. So make sure you have
    something else to do when you run this code for the first time. And even if the
    model has already been downloaded and saved in your environment, it may take a
    minute or two just to load the model into RAM. Generating the response to your
    prompt may also require a couple of minutes as it does the 7 billion multiplications
    required for each token in the generated sequence.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Llama 2 7B 模型文件需要 10 GB 的存储空间（和网络数据）来下载。一旦 Llama 2 权重在 RAM 中被解压缩，它很可能会在您的机器上使用
    34 GB 或更多的内存。这段代码从 Hugging Face Hub 下载了模型权重，在我们的 5G 互联网连接上花了超过 5 分钟的时间。所以确保在第一次运行此代码时有其他事情可做。即使模型已经被下载并保存在您的环境中，加载模型到
    RAM 中可能也需要一两分钟的时间。为了对您的提示生成响应，可能还需要几分钟，因为它需要对生成的序列中的每个标记进行 70 亿次乘法运算。
- en: When working with models behind paywalls or business source licenses you will
    need to authenticate with an access token or key to prove you have accepted their
    terms of service. In the case of Llama 2, you need to "kiss the ring" of Zuckerberg
    and his Meta juggernaut in order to access Llama 2.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用在付费墙或商业许可证后面的模型时，您需要使用访问令牌或密钥进行身份验证，以证明您已接受其服务条款。在 Llama 2 的情况下，您需要“拥抱”扎克伯格及其
    Meta 巨头，以便访问 Llama 2。
- en: Create a Hugging Face account at huggingface.co/join ([https://huggingface.co/join](huggingface.co.html))
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 huggingface.co/join ([https://huggingface.co/join](huggingface.co.html)) 创建一个
    Hugging Face 帐户
- en: Use the same e-mail to apply for a license to download Llama on ai.meta.com
    ([https://ai.meta.com/resources/models-and-libraries/llama-downloads/](llama-downloads.html))
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用相同的电子邮件申请在 ai.meta.com 上下载 Llama 的许可证 ([https://ai.meta.com/resources/models-and-libraries/llama-downloads/](llama-downloads.html))
- en: Copy your Hugging Face (HF) access token found on your user profile page
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 复制您的 Hugging Face（HF）访问令牌，该令牌位于您的用户配置文件页面上
- en: 'Create a `.env` file with your HF access token string in it: `echo "HF_TOKEN=hf_…​"
    >> .env`'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含您的 HF 访问令牌字符串的 `.env` 文件：`echo "HF_TOKEN=hf_…​" >> .env`
- en: Load the token into your Python environment using the `dotenv.load_dotenv()`
    function
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `dotenv.load_dotenv()` 函数将令牌加载到您的 Python 环境中
- en: Load the token into a variable within Python using the `os.environ` library.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `os.environ` 库将令牌加载到 Python 中的变量中。
- en: 'Here are the last two steps in code:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这是代码中的最后两个步骤：
- en: '[PRE2]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now you’re ready to use your token from Hugging Face and the blessing from Meta
    to download the massive Llama 2 model. You probably want to start with the smallest
    model Llama-2-7B. Even it will require 10 GB of data
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经准备好使用 Hugging Face 提供的令牌和 Meta 的祝福来下载庞大的 Llama 2 模型了。您可能想从最小的模型 Llama-2-7B
    开始。即使它也需要 10 GB 的数据
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Notice that the tokenizer only knows about 32,000 different tokens (`vocab_size`).
    You may remember the discussion about Byte-Paire Encoding (BPE) which makes this
    small vocabulary size possible, even for the most complex large language models.
    If you can download the tokenizer, then your Hugging Face Account must be connected
    successfully to your Meta software license application.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，令牌化器只知道 32,000 个不同的标记（`vocab_size`）。您可能还记得有关字节对编码（BPE）的讨论，这使得即使对于最复杂的大型语言模型，这种较小的词汇量也是可能的。如果您可以下载令牌化器，则您的
    Hugging Face 帐户必须已成功连接到您的 Meta 软件许可证申请。
- en: To try out the tokenizer, tokenize a prompt string and take a look at the output
    of the tokenizer.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 要尝试令牌化，请令牌化一个提示字符串，并查看令牌化器的输出。
- en: '[PRE4]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Notice that the first token has an ID of "1." Surely the letter Q isn’t the
    very first token in the dictionary. This token is for the "<s>" start of statement
    token that the tokenizer automatically inserts at the beginning of every input
    token sequence. Also notice that the tokenizer creates a batch of encoded prompts,
    rather than just a single prompt, even though you only want to ask a single question.
    This is why you see a 2-D tensor in the output, but your batch has only a single
    token sequence for the one prompt you just encoded. If you prefer you can process
    multiple prompts at a time by running the tokenizer on a list of prompts (strings)
    rather than a single string.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，第一个令牌的 ID 是 "1"。当然，字母 Q 不是字典中的第一个令牌。这个令牌是用于 "<s>" 语句起始令牌，标记器会自动在每个输入令牌序列的开头插入这个令牌。此外，请注意标记器创建了一个编码的提示批次，而不仅仅是一个单一的提示，即使您只想提出一个问题。这就是为什么输出中会看到一个二维张量，但您的批次中只有一个令牌序列用于您刚刚编码的一个提示。如果您愿意，您可以通过在一系列提示（字符串）上运行标记器，而不是单个字符串，来一次处理多个提示。
- en: You should now be ready to download the actual Llama 2 model.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您应该准备好下载实际的 Llama 2 模型了。
- en: Important
  id: totrans-71
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 重要提示
- en: Our system required a total of *34 GB* of memory to load Llama 2 into RAM. When
    the model weights are decompressed, Llama 2 requires at least 28 GB of memory.
    Your operating system and running applications may require several more additional
    gigabytes of memory. Our Linux system required 6 GB to run several applications,
    including Python. Monitor your RAM usage when loading a large model, and cancel
    any process that causes your computer to start using SWAP storage.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的系统总共需要 *34 GB* 的内存才能将 Llama 2 加载到 RAM 中。当模型权重被解压缩时，Llama 2 至少需要 28 GB 的内存。您的操作系统和正在运行的应用程序可能还需要几个额外的千兆字节的内存。我们的
    Linux 系统需要 6 GB 来运行多个应用程序，包括 Python。在加载大型模型时，请监控您的 RAM 使用情况，并取消任何导致您的计算机开始使用 SWAP
    存储的进程。
- en: The LLaMa-2 model requires 10 GB of storage, so it could take a while to download
    from Hugging Face. The code below downloads, decompresses and loads the model
    weights when it runs the `.from_pretrained()` method. This took more than 5 minutes
    on our 5G Internet connection. And even if the model has already been downloaded
    and saved in your cache locally, it may take a minute or two just to load the
    model weights into memory (RAM).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMa-2 模型需要 10 GB 的存储空间，因此从 Hugging Face 下载可能需要一段时间。下面的代码在运行 `.from_pretrained()`
    方法时会下载、解压并加载模型权重。我们的 5G 网络连接花了超过 5 分钟。而且，即使模型已经下载并保存在本地缓存中，可能也需要一两分钟才能将模型权重加载到内存
    (RAM) 中。
- en: '[PRE5]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Finally, you’re ready to ask Llama the philosophical question in your prompt
    string. Generating a response to your prompt may also require a couple of minutes
    as it does the 7 billion multiplications required for each token in the generated
    sequence. On a typical CPU, these multiplications will take a second or two for
    each token generated. Make sure you limit the maximum number of tokens to a reasonable
    amount, depending on your patience for philosophizing LLMs.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您可以在提示字符串中向 Llama 提出哲学问题。生成提示的响应可能需要几分钟，因为生成的序列中的每个令牌都需要 70 亿次乘法运算。在典型的 CPU
    上，这些乘法运算会花费一两秒的时间来生成每个令牌。根据您对哲学化大型语言模型的耐心程度，确保限制最大令牌数量在合理范围内。
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Nice! It looks like Llama 2 is willing to admit that it doesn’t have experience
    in the real world!
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 很好！看来 Llama 2 愿意承认它在现实世界中没有经验！
- en: If you would like a more engaging experience for your users, you can generate
    the tokens one at a time. This can make it feel more interactive even though it
    will still take the same amount of time to generate all the tokens. The pregnant
    pause before each token can be almost mesmerizing. When you run the following
    code, notice how your brain is trying to predict the next token just as Llama
    2 is.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想让用户体验更加有趣，可以一次生成一个令牌。即使生成所有令牌所需的时间不变，但这种方式可以让交互感觉更加生动。在每个令牌生成之前的那一刻停顿，几乎会让人着迷。当您运行以下代码时，请注意您的大脑是如何尝试预测下一个令牌的，就像
    Llama 2 一样。
- en: '[PRE7]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This token-at-a-time approach to generative chatbots can allow you to see how
    verbose and detailed an LLM can be if you let it. In this case, Llama 2 will simulate
    a longer back-and-forth Q and A dialog about epistemology. Llama 2 is just doing
    its best to continue the pattern that we started with our "Q:" and "A:" prompts
    within the input prompt to the model.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这种一次一个令牌的方法适用于生成型聊天机器人，可以让您看到如果允许大型语言模型发挥其冗长和详细的能力会有怎样的效果。在这种情况下，Llama 2 将模拟关于认识论的更长的问答对话。Llama
    2 正在尽力继续我们在输入提示中使用 "Q:" 和 "A:" 触发的模式。
- en: '[PRE8]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Llama 2 common sense reasoning and math
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 羊驼 2 常识推理和数学
- en: 'You’ve spent a lot of time and network bandwidth to download and run a scaled-up
    GPT model. The question is: can it do any better at the common sense math problem
    you posed GPT-2 at the beginning of this chapter?'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 您花费了大量时间和网络带宽来下载和运行一个规模化的GPT模型。问题是：它能否更好地解决您在本章开头向GPT-2提出的常识数学问题？
- en: '[PRE9]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Once you have the tensor of token IDs for your LLM prompt, you can send it to
    Llama to see what token IDs it thinks you would like to follow your prompt. It
    may seem like a Llama is counting cow legs, but it’s really just trying to predict
    what kind of token ID sequences you are going to like.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您拥有了LLM提示的令牌ID张量，您可以将其发送给Llama，看看它认为您会喜欢跟随您的提示的令牌ID。这似乎就像是一只羊驼在数牛腿，但实际上它只是试图预测您会喜欢的令牌ID序列。
- en: '[PRE10]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Can you spot the error in the llama output?
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 你能发现羊驼输出中的错误吗？
- en: '[PRE11]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Even though the answer is correct this time, the larger model confidently explains
    its logic incorrectly. It doesn’t even seem to notice that the answer it gave
    you is different from the answer it used in its explanation of the math. LLMs
    have no understanding of the quantity that we use numbers to represent. They don’t
    understand the meaning of numbers (or words, for that matter). An LLM sees words
    as a sequence of discrete objects that it is trying to predict.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 即使这次答案是正确的，但更大的模型自信地错误地解释了它的逻辑。它甚至似乎没有注意到它给出的答案与它在数学解释中使用的答案不同。LLM对我们用数字表示的数量没有理解。它们不理解数字（或者话说，单词）的含义。LLM将单词视为它试图预测的一系列离散对象。
- en: Imagine how hard it would be to detect and correct LLM logic errors if you wanted
    to use an LLM to teach math. And imagine how insidiously those errors might corrupt
    the understanding of your students. You probably do not even have to *imagine*
    it, you have probably seen it in real life conversations between people about
    information and logic that they obtained from large language models or articles
    written by large language models. If you use LLMs to reason with your users directly,
    you are doing them a disservice and corrupting society. You would be better off
    scripting a deterministic rule-based chatbot with a limited number of questions
    and explanations that have been intentionally designed by a teacher. You could
    even generalize from the process that teachers and textbook authors use to generate
    word problems to programmatically generate a virtually limitless number of problems.
    The Python `hypothesis` package does this for software unittests, and the `MathActive`
    package does this for simple math problems, and you can use it as a pattern for
    your own curriculum of math problems.^([[13](#_footnotedef_13 "View footnote.")])
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，如果您想使用LLM来教数学，要检测和纠正LLM逻辑错误会有多困难。想象一下，这些错误可能会以何种隐秘的方式破坏您学生的理解。您可能甚至都不必*想象*，您可能在人们之间关于信息和逻辑的实际对话中看到过这种情况，这些信息和逻辑是从大型语言模型或由大型语言模型编写的文章中获得的。如果您直接使用LLM与用户推理，那么您正在对他们造成伤害并腐化社会。最好编写一个确定性基于规则的聊天机器人，该机器人具有有限数量的问题和教师故意设计的解释。您甚至可以从教师和教科书作者用于生成文字问题的过程中推广，以自动生成几乎无限数量的问题。Python
    `hypothesis`包用于软件单元测试，`MathActive`包用于简单的数学问题，您可以将其用作生成自己数学问题课程的模式。^([[13](#_footnotedef_13
    "查看脚注。")])
- en: Whenever you find yourself getting fooled by the seeming reasonableness of larger
    and larger language models, remember this example. You can remind yourself what
    is really happening by running an LLM yourself and taking a look at the sequence
    of token IDs. This can help you think of example prompts that will reveal the
    holes in the swiss cheese of example conversations that the LLM was trained on.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 每当你发现自己被越来越大的语言模型的合理性所愚弄时，记住这个例子。您可以通过运行LLM并查看令牌ID序列来提醒自己发生了什么。这可以帮助您想出示例提示，揭示LLM所训练的示例对话的瑞士奶酪中的漏洞。
- en: 10.1.2 Guardrails (filters)
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.2 护栏（过滤器）
- en: When someone says unreasonable or inappropriate things, we talk about them "going
    off the rails" or "not having a filter." Chatbots can go off the rails too. So
    you will need to design guardrails or NLP filters for your chatbot to make sure
    your chatbot stays on track and on topic.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当有人说不合理或不适当的事情时，我们谈论他们“偏离轨道”或“没有过滤器”。聊天机器人也可能偏离轨道。因此，您需要为您的聊天机器人设计护栏或NLP过滤器，以确保您的聊天机器人保持在轨道上和话题上。
- en: There is virtually an unlimited number of things that you don’t want your chatbots
    to say. But you can classify a lot of them into two broad categories, either toxic
    or erroneous messages. Here are some examples of some toxic messages your NLP
    filters will need to detect and deal with. You should be familiar with some of
    these aspects of toxicity from the toxic comments dataset you worked with in Chapter
    4.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上有无数件事情是您不希望您的聊天机器人说的。但您可以将它们大多数分类为两个广泛的类别，即有毒或错误消息。以下是一些您的NLP过滤器需要检测和处理的一些有毒消息的示例。您应该从第4章中使用的有毒评论数据集中熟悉了解一些有毒评论的方面。
- en: '*Biases*: Reinforcing or amplifying biases, discrimination, or stereotyping'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*偏见*：强化或放大偏见、歧视或刻板印象'
- en: '*Violence*: Encouraging or facilitating bullying, acts of violence or self-harm'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*暴力*：鼓励或促进欺凌、暴力行为或自伤行为'
- en: '*Yes-saying*: Confirming or agreeing with a user’s factually incorrect or toxic
    comments'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*顺从性*：确认或同意用户事实上不正确或有毒的评论'
- en: '*Inappropriate topics*: Discussing topics your bot is not authorized to discuss'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*不适当的话题*：讨论您的机器人未经授权讨论的话题'
- en: '*Safety*: Failing to report safeguarding disclosures by users (physical or
    mental abuse)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*安全*：未能报告用户（身体或心理虐待）的保护信息披露'
- en: '*Privacy*: Revealing private data from language model training data or retrieved
    documents'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*隐私*：从语言模型训练数据或检索到的文档中透露私人数据'
- en: You will need to design an NLP classifier to detect each of these kinds of toxic
    text that your LLM may generate. You may think that since you are in control of
    the generative model, it should be easier to detect toxicity than it was when
    you classified X-rated human messages on Twitter (see Chapter 4).^([[14](#_footnotedef_14
    "View footnote.")]) However, detecting when an LLM goes off the rails is just
    as hard as it was when humans go off the rails. You still need to provide a machine
    learning model examples of good and bad text. And the only way to do that reliably
    is with the same old-fashioned machine learning approach you used in earlier chapters.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 您将需要设计一个NLP分类器来检测您的LLM可能生成的每种有害文本。您可能会认为，既然您控制生成模型，检测毒性应该比在Twitter上对成人信息进行分类时更容易（见第4章）^([[14](#_footnotedef_14
    "View footnote.")])。然而，当LLM走向歧途时，检测毒性和当人类走向歧途一样困难。您仍然需要向机器学习模型提供好文本和坏文本的示例。可靠地做到这一点的唯一方法就是用早期章节中使用的老式机器学习方法。
- en: However, you have learned about one new tool that can help you in your quest
    to guard against toxic bots. Fortunately, if you use a large language model such
    as BERT to create your embedding vectors, it will give your toxic comment classifiers
    a big boost in accuracy. BERT, Llama and other large language models are much,
    much better at detecting all the subtle word patterns that are among those toxic
    patterns you want your bot to avoid. So it’s perfectly fine to reuse an LLM to
    create embeddings that you use in the NLU classifiers that filter out toxicity.
    That may seem like cheating, but it’s not, because you are no longer using the
    LLM embedding to predict the next word that your users will like. Instead, you
    are using the LLM embedding to predict how much a bit of text matches the patterns
    you’ve specified with your filter’s training set.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，您已经了解了一种新工具，可以帮助您保护免受有害机器人的影响。幸运的是，如果您使用类似BERT这样的大语言模型来创建您的嵌入向量，它将极大地提高您的毒性评论分类器的准确性。BERT、Llama和其他大型语言模型在检测所有微妙的词语模式方面要好得多，这些模式是您希望您的机器人避开的有毒模式之一。因此，重复使用LLM创建您在过滤毒性时使用的NLU分类器中的嵌入向量是完全合理的。这可能看起来像作弊，但事实并非如此，因为您不再使用LLM嵌入来预测用户将喜欢的下一个词。相反，您正在使用LLM嵌入来预测一小段文本与您的过滤器训练集中指定的模式匹配程度。
- en: So whenever you need to filter what your chatbot says, you will also need to
    build a binary classifier that can detect what is and is not allowed for your
    bot. And a multi-label classifier (tagger) would be even better because it will
    give your model the ability to identify a larger variety of the toxic things that
    chatbots can say. You no longer need to try to describe in your prompt all the
    many, many ways that things can go wrong. You can collect all the examples of
    bad behavior into a training set. And after you go to production, and you have
    new ideas (or chatbot mistakes) you can add more and more examples to your training
    set. Your confidence in the strength of your chatbot guards will grow each time
    you find new toxicity examples and retrain your filters.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，每当您需要过滤您的聊天机器人说的内容时，您还需要构建一个可以检测您的机器人所允许和不允许的内容的二元分类器。而一个多标签分类器（标签器）会更好，因为它将赋予您的模型识别聊天机器人可能说出的更多有毒内容的能力。您不再需要尝试在提示中描述所有可能出错的方式。您可以将所有不良行为的示例收集到一个训练集中。在您投入生产并且您有新的想法（或聊天机器人的错误）时，您可以向您的训练集中添加更多的例子。每当您找到新的有毒性例子并重新训练您的过滤器时，您对您的聊天机器人防护的信心就会增长。
- en: Your filters have another invaluable feature that an LLM cannot provide. You
    will have statistical measures of how well your LLM pipeline is doing. Your analytics
    platform will be able to keep track of all the times your LLM came close to saying
    something that came close to exceeding your bad behavior thresholds. In a production
    system, it is impossible to read all the things your chatbot and users have said,
    but your guardrails can give you statistics about every single message and help
    you prioritize those messages you need to review. So you will see that improvement
    over time as your team and users help you find more and more edge cases to add
    to your classifier’s training set. An LLM can fail in surprising new ways each
    and every time you run it for a new conversation. Your LLM will never be perfect
    no matter how well you craft the prompts. But with filters on what your LLM is
    allowed to say, you can at least know how often your chatbot is going to let something
    slip between your the guards to your chatbot kingdom.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 您的过滤器还具有LLM无法提供的另一个无价的功能。您将拥有关于您的LLM管道表现如何的统计指标。您的分析平台将能够跟踪您的LLM接近说出超过您不良行为阈值的内容的所有次数。在生产系统中，不可能读取您的聊天机器人和用户所说的所有内容，但是您的防护栏可以为您提供关于每条消息的统计信息，并帮助您优先处理那些您需要审核的消息。因此，您将会看到随着时间的推移您的团队和用户帮助您找到越来越多的边缘案例，以添加到您的分类器训练集中的改进。每次您为新的对话运行LLM时，LLM都可能以令人惊讶的新方式失败。无论您如何精心制作提示，您的LLM永远不会完美无缺。但是通过对LLM允许说的内容进行过滤，您至少可以知道您的聊天机器人有多经常会让某些内容从您的防护栏溜到您的聊天机器人王国中。
- en: But you will never achieve perfect accuracy. Some inappropriate text will eventually
    leak through your filters and reach your users. And even if you could create a
    perfect toxic comment classifier, will need to continuously update its aim point
    to hit a moving target. This is because some of your users may intentionally to
    trick your LLMs into generating the kinds of text you do not want them to.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 但是您永远无法达到完美的准确性。一些不适当的文本最终会绕过您的过滤器，传递给您的用户。即使您能创建一个完美的有毒评论分类器，也需要不断更新其目标，以击中一个不断移动的目标。这是因为您的一些用户可能会故意欺骗您的大语言模型，使其生成您不希望它们生成的文本类型。
- en: Adversarial users who try to break a computer program are called "hackers" in
    the cybersecurity industry. And cybersecurity experts have found some really effective
    ways to harden your NLP software and make your LLM less likely to generate toxic
    text. You can create *bug bounties* to reward your users whenever they find a
    bug in your LLM or a gap in your guardrails. This gives your adversarial users
    a productive outlet for their curiosity and playfulness or hacker instincts.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络安全行业，试图破解计算机程序的对手用户被称为“黑客”。网络安全专家已经找到了一些非常有效的方法来加固您的自然语言处理软件，使您的大语言模型更不太可能生成有毒文本。您可以设置*漏洞赏金*来奖励用户，每当他们在您的大语言模型中发现漏洞或您的防护栏中的缺陷时。这样一来，您的对手用户就可以将好奇心和玩心或黑客本能发挥出来，找到一个有益的出口。
- en: You could even allow users to submit filter rules if you use an open source
    framework to define your rules. Guardrails-ai is an open source Python package
    that defines many rule templates that you can configure for you own needs. You
    can think of these filters as real-time unit tests.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用开源框架定义您的规则，甚至可以允许用户提交过滤规则。Guardrails-ai是一个开源的Python包，定义了许多规则模板，您可以根据自己的需求进行配置。您可以将这些过滤器视为实时单元测试。
- en: Conventional machine learning classifiers are probably your best bet for detecting
    malicious intent or inappropriate content in your LLM outputs. If you need to
    prevent your bot from providing legal or medical advice, which is strictly regulated
    in most countries, you will probably need to revert to the machine learning approach
    you used to detect toxicity. ML models will generalize from the examples you give
    it. And you need this generalization to give your system high reliability. Custom
    machine learning models are also the best approach when you want to protect your
    LLM from prompt injection attacks and the other techniques that bad actors might
    use to "pants" (embarrass) your LLM and your business.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的LLM输出中检测恶意意图或不当内容的传统机器学习分类器可能是您最好的选择。如果您需要防止您的机器人提供在大多数国家严格管制的法律或医疗建议，则可能需要退回到您用于检测毒性的机器学习方法。ML模型将从您给它的例子中进行泛化。您需要这种泛化来使您的系统具有高可靠性。在想要保护您的LLM免受提示注入攻击和其他坏行为者可能使用的“反派”（尴尬）您的LLM和业务技术时，自定义机器学习模型也是最佳方法。
- en: If you need more precise or complex rules to detect bad messages, you may find
    yourself spending a lot of time doing "whack-a-mole" on all the different attack
    vectors that malicious users might try. Or you may have just a few string literals
    and patterns that you want to detect. Fortunately, you do not have to manually
    create all the individual statements that your most creative users might come
    up with. There are several open source tools you can use to help you specify general
    filter rules using languages similar to regular expressions.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要更精确或复杂的规则来检测不良信息，您可能会花费大量时间在所有可能的攻击向量上进行“打地鼠”。或者您可能只有一些字符串字面量和模式需要检测。幸运的是，您不必手动创建用户可能提出的所有单独语句。有几个开源工具可用于帮助您使用类似于正则表达式的语言指定通用过滤器规则。
- en: SpaCy’s `Matcher` class ^([[15](#_footnotedef_15 "View footnote.")])
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SpaCy的`Matcher`类 ^([[15](#_footnotedef_15 "查看脚注。")])
- en: ReLM (regular expressions for language models) patterns ^([[16](#_footnotedef_16
    "View footnote.")])
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReLM（用于语言模型的正则表达式）模式 ^([[16](#_footnotedef_16 "查看脚注。")])
- en: Eleuther AI’s *LM evaluation harness* package ^([[17](#_footnotedef_17 "View
    footnote.")])
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eleuther AI的*LM评估工具包* ^([[17](#_footnotedef_17 "查看脚注。")])
- en: The Python fuzzy regular expression package ^([[18](#_footnotedef_18 "View footnote.")])
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python模糊正则表达式包 ^([[18](#_footnotedef_18 "查看脚注。")])
- en: '[https://github.com/EleutherAI/lm-evaluation-harness](EleutherAI.html)'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/EleutherAI/lm-evaluation-harness](EleutherAI.html)'
- en: Guardrails-AI "rail" language ^([[19](#_footnotedef_19 "View footnote.")])
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guardrails-AI“rail”语言^([[19](#_footnotedef_19 "查看脚注。")])
- en: Our favorite tool for building NLP guardrails, or virtually any rule-based pipeline,
    is SpaCy. Nonetheless, you are going to first see how to use the Guardrails-AI
    Python package.^([[20](#_footnotedef_20 "View footnote.")]) Despite the name,
    `guardrails-ai` probably is not going to help you keep your LLMs from going off
    the rails, but it may be useful in other ways.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建NLP栏杆或几乎任何基于规则的管道的最爱工具是SpaCy。尽管如此，您将首先看到如何使用Guardrails-AI Python包。^([[20](#_footnotedef_20
    "查看脚注。")])不管名称如何，`guardrails-ai`可能不会帮助您防止LLMs跑偏，但在其他方面可能有用。
- en: Guardrails-AI package
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Guardrails-AI包
- en: Before you get started building your LLM guardrails, make sure you’ve installed
    the `guardrails-ai` package This is not the same as the `guardrails` package,
    so make sure you include the "-ai" suffix. You can use `pip` or `conda` or your
    favorite Python package manager.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始构建LLM栏杆之前，请确保您已安装了`guardrails-ai`包。这与`guardrails`包不同，请确保包括"-ai"后缀。您可以使用`pip`或`conda`或您喜欢的Python包管理器。
- en: '[PRE12]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The Guardrails-AI package uses a new language called "RAIL" to specify your
    guardrail rules. RAIL is a domain-specific form of XML (ugh)! Assuming XML is
    not a deal-breaker for you, if you are willing to wade through XML syntax to write
    a simple conditional, `guardrails-ai` suggests that you can use the RAIL language
    to build a retrieval-augmented LLM that doesn’t fake its answers. You RAIL-enhanced
    LLM should be able to fall back to an "I don’t know" response when the retrieved
    text fails to contain the answer to your question. This seems like exactly the
    kind of thing an AI guardrail needs to do.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Guardrails-AI 包使用一种名为"RAIL"的新语言来指定你的防护栏规则。RAIL 是一种特定领域的 XML 形式（呃）！假设 XML 对你来说并不是一项硬性要求，如果你愿意浏览
    XML 语法来编写一个简单的条件语句，`guardrails-ai`建议你可以使用 RAIL 语言来构建一个不会虚假回答的检索增强型 LLM。你的 RAIL
    增强型 LLM 应该能够在检索到的文本未包含你问题的答案时回退到"我不知道"的回答。这似乎正是一个 AI 防护栏需要做的事情。
- en: Listing 10.2 Guardrail for answering questions with humility
  id: totrans-121
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 10.2 回答问题时的谦逊防护栏
- en: '[PRE13]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: But if you dig deeper into that `xml_prefix_prompt` and `output_schema`, you
    will see that it is really quite similar to a Python f-string, a string that can
    contain Python variables which are expanded with the `.format()` method. The RAIL
    language looks like it could be a very expressive and general way to create prompts
    with guardrails. But if you dig deeper into that `xml_prefix_prompt` and `output_schema`,
    you will see that it is really not too different from a Python f-string template
    for your prompts. Here is what is inside that prompt that you just composed using
    the RAIL XML language of `guardrails-ai`.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 但是如果你深入研究`xml_prefix_prompt`和`output_schema`，你会发现它实际上与 Python f-string 非常相似，这是一个可以包含
    Python 变量并使用`.format()`方法扩展的字符串。RAIL 语言看起来可能是一个非常富有表现力和通用的创建带有防护栏提示的方式。但是如果你深入研究`xml_prefix_prompt`和`output_schema`，你会发现它实际上与
    Python f-string 模板并没有太大的区别。这就是你刚刚使用`guardrails-ai`的 RAIL XML 语言组成的提示内部。
- en: '[PRE14]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: So it does seem to give you some good ideas for ways to decorate your prompts.
    It gives you ideas for additional wording that might encourage good behavior.
    But the only validation filter that `guardrails-ai` seems to be doing is to check
    the *format* of the output. And since you usually want an LLM to generate free
    form text, the `output_schema` is usually just a string in human-readable text.
    The bottom line is that you should look elsewhere for filters and rules to help
    you monitor your LLM responses and prevent them from containing bad things.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这似乎给了你一些好主意来装饰你的提示。它为你提供了一些可能鼓励良好行为的额外措辞的想法。但是`guardrails-ai`唯一似乎正在执行的验证过滤是检查输出的*格式*。而且由于你通常希望
    LLM 生成自由格式的文本，所以`output_schema`通常只是一个人类可读的文本字符串。总之，你应该在其他地方寻找过滤器和规则来帮助你监控 LLM
    的响应，并防止它们包含不良内容。
- en: 'If you need an expressive templating language for building prompt strings,
    you are much better off using some of the more standard Python templating systems:
    f-strings (format strings) or `jinja2` templates. And if you’d like some example
    LLM prompt templates such as the ones in Guardrails-AI you can find them in the
    LangChain package. In fact, this is how the inventor of LangChain, Harrison Chase,
    got his start. He was using Python f-strings to cajole and coerce conversational
    LLMs into doing what he needed and found he could automate lots of that work.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要一个用于构建提示字符串的表达性模板语言，最好使用一些更标准的 Python 模板系统：f-strings（格式化字符串）或`jinja2`模板。如果你想要一些示例
    LLM 提示模板，比如 Guardrails-AI 中的模板，你可以在 LangChain 包中找到它们。事实上，这就是 LangChain 的发明者哈里森·查斯的起步。他当时正在使用
    Python f-strings 来哄骗和强迫会话式 LLM 完成他需要的工作，并发现他可以自动化很多工作。
- en: Asking an LLM to do what you want isn’t the same as *ensuring* it does what
    you want. And that’s what a rule-based guardrail system is supposed to for you.
    So, in a production application you would probably want to use something rule-based,
    such as SpaCy `Matcher` patterns rather than `guardrails-ai` or LangChain. You
    need rules that are fuzzy enough to detect common misspellings or transliterations.
    And you need them to be able to incorporate NLU, in addition to fuzzy text matching.
    The next section will show you how to combine the power of fuzzy rules (conditional
    expressions) with modern NLU semantic matching.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让一个LLM做你想要的事情并不等同于*确保*它做你想要的事情。这就是一个基于规则的防护系统应该为你做的事情。因此，在生产应用程序中，你可能想要使用一些基于规则的东西，比如SpaCy
    `Matcher`模式，而不是`guardrails-ai`或LangChain。你需要足够模糊的规则来检测常见的拼写错误或音译错误。而且你需要它们能够整合NLU，除了模糊的文本匹配。下一节将向你展示如何将模糊规则（条件表达式）的力量与现代NLU语义匹配相结合。
- en: SpaCy Matcher
  id: totrans-128
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: SpaCy Matcher
- en: A really common guardrail you will need to configure for your LLM is the ability
    to avoid taboo words or names. Perhaps you want your LLM to never generate curse
    words, and instead substitute more meaningful and less triggering synonyms or
    euphemisms. Or maybe you want to make sure your LLM to never generates the brand
    names for prescription drugs, but rather always uses the names for generic alternatives.
    And it’s very common for a less prosocial organizations to do the oposite and
    instead avoid mentioning a competitor or a competitor’s products. For names of
    people, places and things you will learn about named entity recognition in Chapter
    11\. Here you will see how to implement a more flexible bad word detector. This
    approach will work for any kind of bad words that you want to detect, perhaps
    your name and contact information or other Personally Identifiable Information
    (PII) you want to protect.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要为你的LLM配置一个非常常见的防护栏，即避免使用禁忌词或名称的能力。也许你希望你的LLM永远不要生成脏话，而是用更有意义且不易触发的同义词或委婉语来替代。或者你可能想确保你的LLM永远不要生成处方药的品牌名称，而是始终使用通用替代品的名称。对于较少社会化的组织来说，避免提及竞争对手或竞争对手的产品是非常常见的。对于人名、地名和事物名，你将在第11章学习命名实体识别。在这里，你将看到如何实现更灵活的脏话检测器。这种方法适用于你想检测的任何种类的脏话，也许是你的姓名和联系信息或其他你想保护的个人可识别信息（PII）。
- en: Here’s a SpaCy Matcher that should extract the names of people and their Mastodon
    account addresses in an LLM response. You could use this to check to see if any
    PII (personally identifying information) is accidentally being leaked by your
    LLM.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个SpaCy Matcher，它应该提取LLM响应中人们的名称和他们的Mastodon账户地址。你可以使用这个来检查你的LLM是否意外地泄露了任何个人身份信息（PII）。
- en: You can probably understand why it is not helpful to have an LLM judging itself.
    So what if you want to build more reliable rules that do exactly what you ask.
    You want rules that have predictable and consistent behavior, so that when you
    improve the algorithm or the training set it gets better and better. The previous
    chapters have taught you how to use the power regular expressions and NLU to classify
    text, rather than relying on NLG to magically do what you ask (sometimes). And
    you can use your accuracy metrics from Chapter 2 to quantify exactly how well
    your guardrail is working. It’s important to know when the guards to your NLP
    castle are falling asleep on the job.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能能理解为什么让一个LLM自己判断并不有用。那么，如果你想建立更可靠的规则来确切地执行你的要求呢。你想要的规则具有可预测和一致的行为，这样当你改进算法或训练集时，它会变得越来越好。前几章已经教会了你如何使用正则表达式和NLU来对文本进行分类，而不是依靠NLG来魔法般地执行你的要求（有时）。你可以使用第二章的准确性指标来准确地量化你的防护栏的工作情况。知道你的NLP城堡的卫兵什么时候在岗位上睡着了是很重要的。
- en: '[PRE15]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: That first number in a match 3-tuple is the integer ID for the match. You can
    find the mapping between the key "drug" and this long integer (475…​) with the
    `matcher.normalize_key('drug')` expression. The second two numbers in the match
    3-tuple tell you the start and stop indices of the matched pattern in your tokenized
    text (`doc`). You can use the start and stop indices to replace "Tylenol" with
    more accurate and less branded content such as the generic name "Acetominophine."
    This way you can make your LLM generate more educational content rather than advertising.
    The code here just marks the bad word with asterisks.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 匹配的第一个数字是匹配 3-元组的整数 ID。你可以通过表达式 `matcher.normalize_key('drug')` 找到键 "drug" 和这个长整数（475…​）之间的映射关系。匹配
    3-元组中的后两个数字告诉你在你的标记化文本 (`doc`) 中匹配模式的起始和结束索引。你可以使用起始和结束索引将 "Tylenol" 替换为更准确且不那么品牌化的内容，比如通用名
    "Acetominophine"。这样你就可以让你的 LLM 生成更多教育内容而不是广告。这段代码只是用星号标记了坏词。
- en: '[PRE16]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: If you want to do more than just detect these bad words and fall back to a generic
    "I can’t answer that" response, you will need to do a little more work. Say you
    want to correct the bad words with acceptable substitutes. In that case you should
    add a separate named matcher for each word in your list of bad words. This way
    you will know which word in your list was matched, even if there was a typo in
    the text from teh LLM.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想做的不仅仅是检测这些坏词并回退到一个通用的 "我不能回答" 的响应，那么你将需要做更多的工作。假设你想用可接受的替代词来纠正坏词。在这种情况下，你应该为你坏词列表中的每个单词添加一个单独的命名匹配器。这样你就会知道你列表中的哪个单词被匹配了，即使
    LLM 的文本中有拼写错误。
- en: '[PRE17]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: That first match is for the original pattern that you added. The second 3-tuple
    is for the latest matcher that separated the matches for each word. You can use
    this second match ID from the second 3-tuple to retrieve the matcher responsible
    for the match. That matcher pattern will tell you the correct spelling of the
    drug to use with your translation dictionary.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个匹配是添加的原始模式。第二个 3-元组是最新的匹配器，用于分离每个单词的匹配。你可以使用第二个 3-元组中的第二个匹配 ID 来检索负责匹配的匹配器。该匹配器模式将告诉你在你的翻译字典中使用的药品的正确拼写。
- en: '[PRE18]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Because there was no callback function specified in the pattern you see None
    as the first element of the tuple. We named the first pattern "drug" and the subsequent
    ones were named "tylenol" and "advil". In a production system you would use the
    `matcher.\_normalize_keys()` method to convert your match key strings ("drug",
    "tylenol", and "advil") to integers so you could map integers to the correct drug.
    Because you can’t rely on the matches containing the name of the pattern, you
    will need the additional code shown here to retrieve the correct spelling of
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 因为在模式中没有指定回调函数，所以你会看到元组的第一个元素为 None。我们将第一个模式命名为 "drug"，随后的模式分别命名为 "tylenol"
    和 "advil"。在生产系统中，你将使用 `matcher.\_normalize_keys()` 方法将你的匹配键字符串（"drug"、"tylenol"
    和 "advil"）转换为整数，这样你就可以将整数映射到正确的药品。由于你不能依赖于匹配包含模式名称，所以你将需要额外的代码来检索正确的拼写
- en: Now you can insert the new token into the original document using the match
    start and stop.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以使用匹配的起始和结束插入新的标记到原始文档中。
- en: '[PRE19]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Now you have a complete pipeline, not only for detecting but also for replacing
    errors in your LLM output. If you find some unexpected bad words are leaking through
    your filter, you can augment your SpaCy matcher with a semantic matcher. You can
    use the word embeddings from Chapter 6 to filter any words that are semantically
    similar to a token in your bad words list. This may seem like a lot of work, but
    this could all be encapsulated into a parameterized function that can help your
    LLM generate text that better meets your requirements. The beauty of this approach
    is that your pipeline will get better and better over time as you add more data
    to your guardrails or your machine learning models that implement the filters.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你有了一个完整的流水线，不仅用于检测还用于替换 LLM 输出中的错误。如果发现一些意外的坏词泄漏通过了你的过滤器，你可以用语义匹配器增强你的 SpaCy
    匹配器。你可以使用第 6 章的词嵌入来过滤与你的坏词列表中的一个标记语义相似的任何单词。这可能看起来是很多工作，但这一切都可以封装成一个参数化函数，可以帮助你的
    LLM 生成更符合你需求的文本。这种方法的美妙之处在于，随着你将更多数据添加到你的护栏或实现过滤器的机器学习模型中，你的流水线会随着时间的推移变得越来越好。
- en: Finally, you are ready for red teaming. This is an approach that can help you
    build up your dataset of edge cases efficiently and improve the reliability of
    your NLP pipeline quickly.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你已经准备好进行红队行动了。这是一种能够帮助你高效构建边缘案例数据集并迅速提高你的 NLP 流水线可靠性的方法。
- en: 10.1.3 Red teaming
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.3 红队行动
- en: Relying on filters and your users to find bugs is not an option if your bot
    could potentially endanger the lives and welfare of people or businesses. To prevent
    some of the more harmful things that an LLM might say you will likely need to
    have a *red team* attempt to bypass or disable these guardrails. A red team is
    an engineer or team that you have authorized to interact with your chatbot in
    an adversarial way. They will try to make your LLM generate messages that you
    do not want your users to be exposed to.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的机器人可能会危及人们或企业的生命和福祉，仅仅依靠过滤器和用户来发现错误是不可取的。为了防止LLM可能说出的更有害的事情，您可能需要让*红队*尝试绕过或停用这些防护栏。红队是您授权与您的聊天机器人进行对抗性互动的工程师或团队。他们将尝试使您的LLM生成您不希望用户接触到的消息。
- en: Just as in NLP, in cybersecurity, this attempt to break a system is also referred
    to as *jail-breaking* or *hacking*. And when a hacker is authorized to attempt
    to penetrate your LLM guardrails it is called *pentesting* or *red teaming*. It
    is usually helpful if some of the red team members are unassociated with the engineers
    that built the LLM guardrails. You may find that cybersecurity researchers and
    pen testers have the skills and mindset to help you find holes in your LLM guardrails.
    On the opposite side of this chatbot arena match is the LLM *blue team*. There
    are the engineers and data analysts that build and maintain your LLM pipeline,
    including all the filters you have in place to prevent bad things from happening.
    The blue team is trying to defend against attempts to trick your LLM into going
    off the rails.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在自然语言处理中一样，在网络安全领域，此类破坏系统的尝试也被称为*jail-breaking*或*hacking*。当一个黑客被授权试图渗透您的LLM防护栏时，这被称为*pentesting*或*红队行动*。通常情况下，如果红队成员中的一些人与建造LLM防护栏的工程师没有关联，则会很有帮助。您可能会发现，网络安全研究人员和渗透测试人员具备帮助您找到LLM防护栏漏洞的技能和思维方式。在聊天机器人竞技场的对面，是LLM的*蓝队*。他们是建立和维护您的LLM流水线的工程师和数据分析师，包括您设置的所有过滤器，以防止出现不好的事情。蓝队正在努力防止LLM走向失控。
- en: A red team of researchers at Carnegie Melon found several straightforward ways
    to bypass the guardrails that OpenAI spent millions developing. ^([[21](#_footnotedef_21
    "View footnote.")]) They found that for almost any prohibited prompt they could
    add a suffix that would trick the LLM into ignoring the guard rail. For example,
    when they asked ChatGPT how to make a bomb it would refuse. But then they added
    a suffix phrase to their prompt which included words like "oppositely" hidden
    among punctuation and smashed together tokens.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 卡内基梅隆大学的一支红队研究团队发现了几种绕过OpenAI花费数百万美元开发的防护栏的简单方法。^([[21](#_footnotedef_21 "查看注释。")])
    他们发现，对于几乎任何禁止的提示，他们都可以添加一个后缀来欺骗LLM忽略这个防护栏。例如，当他们问ChatGPT如何制造炸弹时，它会拒绝。但接着他们在提示中加入了一个包含诸如“相反地”之类的词隐藏在标点符号和结合在一起的标记中的后缀短语。
- en: '[PRE20]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: And appending a suffix or prepending a prefix are dead-simple adversarial attacks
    that can be shared easily among your users. Like video game cheat codes, these
    kinds of hacks can go viral before you have a chance to plug the holes in your
    filters. After the "llm-attacks.org" paper was published with this suffix attack,
    OpenAI patched ChatGPT with additional guardrails preventing this particular text
    from triggering a jailbreak. So, if like OpenAI, your LLM is being used to reply
    to your users in real time, you will need to be vigilant about constantly updating
    your guardrails to deal with undesirable behavior. A vigorous bug bounty or red
    team approach (or both) may be required to help you stay ahead of the toxic content
    that an LLM can generate.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 添加后缀或前缀是非常简单的对抗性攻击，可以轻松在您的用户之间共享。就像视频游戏作弊代码一样，这些类型的黑客攻击可能在您有机会修复过滤器中的漏洞之前迅速传播。在“llm-attacks.org”论文发表后，OpenAI为ChatGPT增加了额外的防护栏，阻止了这种特定文字触发越狱。因此，如果像OpenAI一样，您的LLM被用于实时回复用户，您需要时刻警惕地更新您的防护栏以应对不良行为。为了帮助您在LLM产生有毒内容之前保持领先，可能需要积极的Bug赏金或红队方法（或两者兼有）。
- en: If your users are familiar with how LLMs work you may have even bigger problems.
    even be able to hand-craft queries that force your LLM to generate virtually anything
    that you are trying to prevent. Microsoft found out about this kind of *prompt
    injection attack* when a college student, Kevin Liu, forced Bing Chat to reveal
    secret information. ^([[22](#_footnotedef_22 "View footnote.")])
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的用户熟悉LLMs的工作原理，也许你会遇到更大的问题。你甚至可以手动制定查询，迫使你的LLM生成你试图防止的任何东西。当一位大学生Kevin Liu迫使必应聊天透露秘密信息时，微软就发现了这种*提示注入攻击*。
    ^([[22](#_footnotedef_22 "查看脚注。")])
- en: 10.1.4 Smarter, smaller LLMs
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.4 更聪明，更小的LLMs
- en: As you might suspect, much of the talk about emergent capabilities is marketing
    hype. To measure emergence fairly, researchers measure the size of an LLM by the
    number of floating point operations (FLOPs) required to train the model.^([[23](#_footnotedef_23
    "View footnote.")]) This gives a good estimate of both the dataset size and complexity
    of the LLM neural network (number of weights). If you plot model accuracy against
    this measure of the size of an LLM you find that there’s nothing all that surprising
    or emergent in the results. The scaling relationship between capability and size
    is linear, sublinear or even flat for most state-of-the-art LLM benchmarks.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所猜测的那样，许多关于新兴能力的讨论都是营销炒作。为了公正地衡量新兴能力，研究人员通过训练模型所需的浮点运算次数（FLOPs）来衡量LLM的大小。[^[[23](#_footnotedef_23
    "查看脚注。")]]]这给出了数据集大小和LLM神经网络复杂性（权重数）的很好的估计。如果你将模型准确性与LLM量级的这种估计进行绘制，你会发现结果中并没有什么特别惊人的或新兴的东西。对于大多数最先进的LLM基准测试，能力与大小之间的缩放关系是线性的、次线性的，或者甚至是平的。
- en: Perhaps open source models are smarter and more efficient because, in the open
    source world, you have to put your code where your mouth is. Open source LLM performance
    results are reproducible by outside machine learning engineers like you. You can
    download and run the open source code and data and tell the world the results
    that *you* achieved. This means that anything incorrect that the LLMs or their
    trainers say can be quickly corrected in the collective intelligence of the open
    source community. And you can try your own ideas to improve the accuracy or efficiency
    of LLMs. The smarter, collaboratively designed open source models are turning
    out to scale much much more efficiently. And you aren’t locked into an LLM trained
    to hide its mistakes within smart-sounding text.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 或许开源模型更加智能和高效，因为在开源世界中，你必须把代码放在言语之中。开源LLM性能结果可由外部机器学习工程师（如你）进行再现。你可以下载和运行开源代码和数据，并告诉世界你所取得的结果。这意味着LLMs或其培训者所说的任何不正确之处可以在开源社区的集体智慧中迅速纠正。而你可以尝试自己的想法来提高LLM的准确性或效率。更聪明、协作设计的开源模型正在变得更加高效地扩展。而你并没有被锁定在一个训练有素的LLM中，该LLM训练得足够娴熟，可以隐藏其在聪明的文本中的错误。
- en: The open source language models like BLOOMZ, StableLM, InstructGPT, and Llamma2
    have been optimized to make them run on the more modest hardware available to
    individuals and small businesses. Many of the smaller ones can even run in the
    browser. Bigger is better only if you are optimizing for likes. Smaller is smarter
    if what you care about is truly intelligent behavior. A smaller LLM is forced
    to generalize from the training data more efficiently and accurately. But in computer
    science, smart algorithms almost always win in the end. And it turns out that
    the collective intelligence of open source communities is a lot smarter than the
    research labs at large corporations. Open source communities freely brainstorm
    together and share their best ideas with the world, ensuring that the widest diversity
    of people can implement their smartest ideas. So bigger is better, if you’re talking
    about open source communities rather than LLMs.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 像BLOOMZ、StableLM、InstructGPT和Llamma2这样的开源语言模型已经经过优化，可以在个人和小企业可用的更为适度的硬件上运行。许多较小的模型甚至可以在浏览器中运行。只有在优化点赞数时，更大才是更好的。如果你关心的是真正的智能行为，那么更小是更聪明的。一个较小的LLM被迫更加高效和准确地从训练数据中推广。但在计算机科学中，聪明的算法几乎总是最终赢得胜利。结果证明，开源社区的集体智慧比大公司的研究实验室更聪明。开源社区自由地进行头脑风暴，并向世界分享他们的最佳想法，确保最广泛的人群能够实现他们最聪明的想法。因此，如果你在谈论开源社区而不是LLMs，那么更大就更好。
- en: One great idea that came out of the open source community was building higher-level
    *meta models* that utilize LLMs and other NLP pipelines to accomplish their goals.
    If you break down a prompt into the steps needed to accomplish a task, you can
    then ask an LLM to generate the API queries that can reach out into the world
    and accomplish those tasks efficiently.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 开源社区中出现的一种伟大的想法是构建更高级的*元模型*，利用LLMs和其他NLP流水线来实现它们的目标。如果你将一个提示分解成完成任务所需的步骤，然后请求LLM生成能够高效地实现这些任务的API查询。
- en: How does a generative model create new text? Under the hood, a language model
    is what is called a *conditional probability distribution function* for the next
    word in a sentence. In simpler terms, it means that the model chooses the next
    word it outputs based on the probability distribution it derives from the words
    that came before it. By reading a bunch of text, a language model can learn how
    often each word occurs based on the words that preceded it and then mimic these
    statistical patterns without regurgitating the exact same text.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型如何创建新的文本？在模型内部，语言模型是所谓的下一个单词的*条件概率分布函数*。简单来说，这意味着该模型根据它从前面的单词中导出的概率分布来选择输出的下一个单词。通过读取大量文本，语言模型可以学习在先前的单词的基础上每个单词出现的频率，然后模仿这些统计模式，而不是重复完全相同的文本。
- en: 10.1.5 Generating warm words using the LLM temperature paramater
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.5 使用LLM温度参数生成温暖的词语
- en: LLM’s have a parameter called *temperature* that you can use to control the
    *newness* or randomness of the text that it generates. First, you need to understand
    how it is even possible to generate any new text at all. How does a generative
    model create completely new text that it has never seen in its training set? Under
    the hood, a language model is what is called a *conditional probability distribution
    function*. A conditional distribution function gives you the probabilities of
    all the possible next words in a sentence based on (or "conditioned on") the previous
    words in that sentence. In simpler terms, that means that the model chooses the
    next word it outputs based on the probability distribution it derives from the
    words that came before it. By reading a bunch of text, a language model can learn
    how often each word occurs based on the words that preceded it. The training process
    compresses these statistics into a function that generalizes from the patterns
    in those statistics of the relations between words so that it can *fill in the
    blanks* for new prompts and input text.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: LLM具有一个称为*温度*的参数，您可以使用它来控制它生成的文本的新颖性或随机性。首先，您需要理解如何在训练集中完全没有见过的情况下生成任何新的文本。生成模型如何创造全新的文本？在模型内部，语言模型是所谓的*条件概率分布函数*。条件分布函数根据它依赖于的前面的单词（或“被约束在”之前的单词）来给出句子中所有可能的下一个单词的概率。简单地说，这意味着该模型根据它从前面的单词中导出的概率分布来选择输出的下一个单词。通过读取大量文本，语言模型可以学习在先前的单词的基础上每个单词出现的频率。训练过程将这些统计数字压缩成一个函数，从这些统计数字的模式中泛化，以便它可以为新的提示和输入文本*填充空白*。
- en: So if you tell a language model to start a sentence with the "<SOS>" (start
    of sentence/sequence) token, followed by the token "LLMs", it might work through
    a decision tree to decide each subsequent word. You can see what this might look
    like in Figure [10.2](#figure-stochastic-chameleon). The conditional probability
    distribution function takes into account the words already generated to create
    a decision tree of probabilities for each word in a sequence. This diagram reveals
    only one of the many paths through the decision tree.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，如果你让一个语言模型以"<SOS>"（句子/序列开始）标记开头，并以“LLMs”标记接下来，它可能会通过一个决策树来决定每个后续单词。你可以在图[10.2](#figure-stochastic-chameleon)中看到这样的情景。条件概率分布函数考虑到已经生成的单词，为序列中的每个单词创建一个概率决策树。该图表只显示了决策树中众多路径中的一个。
- en: Figure 10.2 Stochastic chameleons decide words one at a time
  id: totrans-160
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.2 随机变色龙逐个决定单词
- en: '![stochastic chameleon decision tree drawio](images/stochastic-chameleon-decision-tree_drawio.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![随机变色龙决策树drawio](images/stochastic-chameleon-decision-tree_drawio.png)'
- en: Figure [10.2](#figure-stochastic-chameleon) shows the probabilities for each
    word in the sequence as an LLM generates new text from left to right. This is
    a simplified view of the choice process — the conditional probability actually
    takes into account the words already generated which is not shown in this diagram.
    So a more accurate diagram would look more like a tree with many more branches
    than are shown here. The diagram ranks tokens from most probable to least probable.
    The word chosen at each step of the process is marked in bold The generative model
    may not always choose the most probable word at the top of the list and the *temperature*
    setting is how often it will go further and further down the list. Later in the
    chapter you will see the different ways you can use the *temperature* parameter
    to adjust which word is chosen at each step.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [10.2](#figure-stochastic-chameleon) 展示了在 LLM 从左到右生成新文本时，每个单词的概率。这是选择过程的一个简化视图 — 条件概率实际上考虑了已经生成的单词，但在此图中未显示。因此，更准确的图表会看起来更像一个比这里显示的分支更多的树。图表将标记从最有可能到最不可能的顺序排名。在过程的每一步中选择的单词以粗体标记。生成型模型可能并不总是选择列表顶部最有可能的单词，*温度*设置是它多久会进一步遍历列表。在本章的后面，您将看到您可以使用
    *温度* 参数的不同方式来调整每一步选择的单词。
- en: In this illustration, sometimes the LLM chooses the second or third most probable
    token rather than the most likely one. If you run this model in prediction (inference)
    mode multiple times, you will get a different sentence almost every time.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，有时 LLM 会选择第二或第三个最有可能的标记，而不是最可能的那个。如果您多次在预测（推理）模式下运行此模型，几乎每次都会得到一个不同的句子。
- en: 'Diagrams like this are often called *fishbone diagrams*. Sometimes they are
    used in failure analysis to indicate how things might go wrong. For an LLM they
    can show all the creative nonsensical phrases and sentences that might pop up.
    But for this diagram the sentence generated along the *spine* of this fishbone
    diagram is a pretty surprising (high entropy) and meaningful sentence: "LLMs are
    stochastic chameleons."'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的图通常被称为*鱼骨图*。有时，它们在故障分析中被用来指示事情可能出错的方式。对于 LLM，它们可以展示所有可能出现的创造性的荒谬短语和句子。但是对于这个图表，鱼骨图的*脊柱*上生成的句子是一个相当令人惊讶（熵值高）且有意义的句子：“LLMs
    是随机变色龙。”
- en: As an LLM generates the next token it looks up the most probable words from
    a probability distribution conditioned on the previous words it has already generated.
    So imagine a user prompted an LLM with two tokens "<SOS> LLM". An LLM trained
    on this chapter might then list verbs (actions) that are appropriate for plural
    nouns such as "LLMs". At the top of that list would be verbs such as "can," "are,"
    and "generate." Even if we’ve never used those words in this chapter, an LLM would
    have seen a lot of plural nouns at the beginning of sentences. And the language
    model would have learned the English grammar rules that define the kinds of words
    that usually follow plural nouns.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 当 LLM 生成下一个标记时，它会查找一个概率分布中最可能的词，这个概率分布是基于它已经生成的前面的词。所以想象一下，一个用户用两个标记 "<SOS>
    LLM" 提示了一个 LLM。一个在本章中训练过的 LLM 可能会列出适合复数名词如 "LLMs" 的动词（动作）。在列表的顶部会有诸如 "can," "are,"
    和 "generate" 这样的动词。即使我们在本章中从未使用过这些词，LLM 也会看到很多以复数名词开头的句子。而且语言模型会学习英语语法规则，这些规则定义了通常跟在复数名词后面的词的类型。
- en: Now you are ready to see how this happens using a real generative model — GPT-4’s
    open source *ancestor*, GPT-2.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经准备好看看这是如何发生的了，使用一个真实的生成型模型 — GPT-4 的开源*祖先*，GPT-2。
- en: 10.1.6 Creating your own Generative LLM
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建你自己的生成型 LLM
- en: To understand how GPT-4 works, you’ll use its "grandfather", GPT-2, which you
    first saw at the beginning of this chapter. GPT-2 was the last open-source generative
    model released by OpenAI. As before you will use the HuggingFace transformers
    package to load GPT-2, but instead of using the automagic `pipeline` module you
    will use the GPT-2 language model classes. They allow you to simplify your development
    process, while still retaining most of PyTorch’s customization ability.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解 GPT-4 如何工作，您将使用它的 "祖父"，GPT-2，您在本章开头首次看到的。GPT-2 是 OpenAI 发布的最后一个开源生成模型。与之前一样，您将使用
    HuggingFace transformers 包来加载 GPT-2，但是不使用 automagic `pipeline` 模块，而是使用 GPT-2 语言模型类。它们允许您简化开发过程，同时仍保留大部分
    PyTorch 的自定义能力。
- en: 'As usual, you’ll start by importing your libraries and setting a random seed.
    As we’re using several libraries and tools, there are a lot of random seeds to
    "plant"! Luckily, you can do all this seed-setting with a single line of code
    in Hugging Face’s Transformers package:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 与往常一样，你将开始导入你的库并设置一个随机种子。由于我们使用了几个库和工具，有很多随机种子要“播种”！幸运的是，你可以在 Hugging Face 的
    Transformers 包中用一行代码完成所有这些种子设置：
- en: '[PRE21]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Unlike Listing [10.1](#listing-gpt2-cow-legs), this code imports the GPT-2 transformer
    pipeline pieces separately, so you can train it yourself. Now, you can load the
    transformer model and tokenizer weights into the model. You’ll use the pretrained
    model that the Hugging Face `transformers` package provides out of the box.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 与列表[10.1](#listing-gpt2-cow-legs) 不同，这段代码将 GPT-2 变压器管道部分单独导入，因此你可以自行训练它。现在，你可以将变压器模型和分词器权重加载到模型中。你将使用
    Hugging Face 的`transformers`包提供的预训练模型。
- en: Listing 10.3 Loading pretrained GPT-2 model from HuggingFace
  id: totrans-172
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 10.3 从 HuggingFace 加载预训练的 GPT-2 模型
- en: '[PRE22]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Let’s see how good this model is in generating useful text. You probably know
    already that you need an input prompt to start generating. For GPT-2, the prompt
    will simply serve as the beginning of the sentence.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个模型在生成有用的文字方面有多好。你可能已经知道，要开始生成，你需要一个输入提示。对于 GPT-2，提示将简单地作为句子的开头。
- en: Listing 10.4 Generating text with GPT-2
  id: totrans-175
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 10.4 用 GPT-2 生成文本
- en: '[PRE23]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Hmm. Not great. Not only the result is incorrect, but also after a certain
    amount of tokens, the text starts repeating itself. You might already have a hint
    of what’s happening, given everything we said so far about the generation mechanisms.
    So instead of using the higher-level `generate()` method, let’s look at what the
    model returns when called directly on the input like we did in our training loops
    in previous chapters:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯。不太好。不仅结果不正确，而且在一定数量的标记之后，文本开始重复。考虑到我们到目前为止关于生成机制的一切，你可能已经有一些线索是怎么一回事了。所以，不使用更高级别的`generate()`方法，来看看当直接调用模型时它返回了什么，就像我们在前几章的训练循环中所做的那样：
- en: Listing 10.5 Calling GPT-2 on an input in inference mode
  id: totrans-178
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 10.5 在推理模式下调用 GPT-2 的输入
- en: '[PRE24]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: That’s an interesting type for the output! If you look at the documentation
    ^([[24](#_footnotedef_24 "View footnote.")]), you’ll see that it has a lot of
    interesting information inside - from the hidden states of the model to attention
    weights for self-attention and cross-attention. What we’re going to look at, however,
    is the part of the dictionary called `logits`. The logit function is the inverse
    of the softmax function - it maps probabilities (in the range between 0 to 1)
    to real numbers (between \({\inf}\) and \({-\inf}\) and is often used as the last
    layer of a neural network. But what’s the shape of our logit tensor in this case?
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的类型很有意思！如果你查看文档^([[24](#_footnotedef_24 "查看脚注。")])，你会在里面看到许多有趣的信息——从模型的隐藏状态到自注意力和交叉注意力的注意力权重。然而，我们要看的是字典中称为`logits`的部分。对数几率函数是
    softmax 函数的逆函数——它将概率（在0到1之间的范围内）映射到实数（在\({-\inf}\)到\({\inf}\)之间），并经常被用作神经网络的最后一层。但在这种情况下，我们的对数几率张量的形状是什么？
- en: '[PRE25]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Incidentally, 50257 is the size of GPT-2’s *vocabulary* - that is, the total
    number of tokens this model uses. (To understand why this particular number, you
    can explore the Byte Pair Encoding (BPE) tokenization algorithm GPT-2 uses in
    Huggingface’s tutorial on tokenization).^([[25](#_footnotedef_25 "View footnote.")])
    So the raw output of our model is basically a probability for every token in the
    vocabulary. Remember how earlier we said that the model just predicts the next
    word? Now you’ll get to see how it happens in practice. Let’s see what token has
    a maximum probability for the input sequence "NLP is a":'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一下，50257 是 GPT-2 的*词汇量*，也就是这个模型使用的标记总数。(要理解为什么是这个特定的数字，你可以在 Huggingface 的分词教程中探索
    GPT-2 使用的字节对编码（BPE）分词算法)^([[25](#_footnotedef_25 "查看脚注。")])。因此，我们模型的原始输出基本上是词汇表中每个标记的概率。还记得我们之前说过模型只是预测下一个单词吗？现在你将看到这在实践中是如何发生的。让我们看看对于输入序列“NLP
    is a”， 哪个标记具有最大概率：
- en: Listing 10.6 Finding the token with maximum probability
  id: totrans-183
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 10.6 找到具有最大概率的标记
- en: '[PRE26]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'So this is how your model generated the sentence: at each timestep, it chose
    the token with the maximum probability given the sequence it received. Whichever
    token it selects is attached to the prompt sequence so it can use that new prompt
    to predict the next token after that. Notice the spaces at the beginning of "
    new" and " non." This is because the token vocabulary for GPT-2 is created using
    the byte-pair encoding algorithm which creates many word pieces. So tokens for
    the beginnings of words all begin with spaces. This means your generate function
    could even be used to complete phrases that end in a part of a word, such as "NLP
    is a non".'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就是你的模型生成句子的方式：在每个时间步长，它选择给定其接收到的序列的最大概率的标记。无论它选择哪个标记，它都附加到提示序列上，这样它就可以使用该新提示来预测其后的下一个标记。注意在“new”和“non”开头的空格。这是因为GPT-2的标记词汇是使用字节对编码算法创建的，该算法创建许多单词片段。因此，单词开头的标记都以空格开头。这意味着你的生成函数甚至可以用于完成以单词部分结尾的短语，例如“NLP是非”。
- en: This type of stochastic generation is the default for GPT2 and is called *greedy*
    search because it grabs the "best" (most probable) token every time. You may know
    the term *greedy* from other areas of computer science. *Greedy algorithms* are
    those that choose the best next action rather than looking further than one step
    ahead before making their choice. You can see why it’s so easy for this algorithm
    to "get stuck." Once it chooses words like "data" that increases the probability
    that the word "data" would be mentioned again, sometimes causing the algorithm
    to go around in circles. Many GPT-based generative algorithms also include a repetition
    penalty to help them break out of cycles or repetition loops. An additional parameter
    that is frequently used to control the randomness of the choosing algorithm is
    *temperature*. Increasing the temperature of your model (typically above 1.0)
    will make it slightly less greedy and more creative. So you can use both temperature
    and a repetition penalty to help your *stochastic chameleon* do a better job of
    blending in among humans.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的随机生成是GPT2的默认设置，并称为*贪婪*搜索，因为它每次都选择“最佳”（最有可能的）标记。你可能从计算机科学的其他领域了解到*贪婪*这个术语。*贪婪算法*是那些在做出选择之前不会向前看超过一步的算法。你可以看到为什么这个算法很容易“陷入困境”。一旦它选择了像“数据”这样的单词，这就增加了“数据”一词再次被提到的概率，有时会导致算法陷入循环。许多基于GPT的生成算法还包括一个重复惩罚，以帮助它们摆脱循环或重复循环。用于控制选择算法的随机性的另一个常用参数是*温度*。增加模型的温度（通常在1.0以上）将使其略微不那么贪婪，更有创意。所以你可以同时使用温度和重复惩罚来帮助你的*随机变色龙*更好地融入人类。
- en: Important
  id: totrans-187
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 重要的
- en: 'We’re inventing new terms every year to describe AI and help us develop intuitions
    about how they do what they do. Some common ones are:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们每年都在创造新术语来描述人工智能，并帮助我们形成对它们运作方式的直觉。一些常见的术语包括：
- en: stochastic chameleon
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机变色龙
- en: stochastic parrot
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机鹦鹉
- en: chickenized reverse centaurs
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鸡化的反向半人马
- en: Yes, these are real terms, used by really smart people to describe AI. You’ll
    learn a lot by researching these terms online to develop your own intuitions.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，这些是真实的术语，由真正聪明的人用来描述人工智能。通过在线研究这些术语，你将学到很多，从而形成自己的直觉。
- en: Fortunately, there are much better and more complex algorithms for choosing
    the next token. One of the common methods to make the token decoding a bit less
    predictable is *sampling*. With sampling, instead of choosing the optimal word,
    we look at several token candidates and choose probabilistically out of them.
    Popular sampling techniques that are often used in practice are *top-k* sampling
    and *nucleus* sampling. We won’t discuss all of them here - you can read more
    about them in HuggingFace’s excellent guide. ^([[26](#_footnotedef_26 "View footnote.")])
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有更好更复杂的算法来选择下一个标记。其中一种常见的方法是使标记解码变得不那么可预测的*采样*。通过采样，我们不是选择最优的单词，而是查看几个标记候选，并在其中概率性地选择。实践中经常使用的流行采样技术包括*top-k*采样和*核*采样。我们在这里不会讨论所有这些
    - 你可以在HuggingFace的出色指南中了解更多。^([[26](#_footnotedef_26 "查看注释。")])
- en: Let’s try to generate text using nucleus sampling method. In this method, instead
    of choosing among the K most likely words, the model looks at the smallest set
    of words whose cumulative probability is smaller than p. So if there are only
    a few candidates with large probabilities, the "nucleus" would be smaller, than
    in the case of larger group of candidates with smaller probabilities. Note that
    because sampling is probabilistic, the generated text will be different for you
    - this is not something that can be controlled with a random seed.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用核心抽样法生成文本。在这种方法中，模型不是在 K 个最有可能的单词中进行选择，而是查看累积概率小于 p 的最小单词集。因此，如果只有几个具有较大概率的候选项，则“核心”会更小，而如果有较小概率的更多候选项，则“核心”会更大。请注意，由于抽样是概率性的，因此生成的文本将对您而言是不同的
    - 这不是可以通过随机种子来控制的事情。
- en: Listing 10.7 Generating text using nucleus sampling method
  id: totrans-195
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10.7 使用核心抽样法（nucleus sampling method）生成文本。
- en: '[PRE27]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: OK. This is better, but still not quite what you were looking for. Your output
    still uses the same words too much (just count how many times "protocol" was mentioned!)
    But more importantly, though NLP indeed can stand for Network Layer Protocol,
    it’s not what you were looking for. To get generated text that is domain-specific,
    you need to *fine-tune* our model - that means, to train it on a dataset that
    is specific to our task.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，这样说要好多了，但还是没有完全符合你的要求。输出文本中仍然重复使用了太多相同的单词（只需计算“protocol”一词被提到的次数即可！）。但更重要的是，尽管
    NLP 的确可以代表网络层协议，但这不是你要找的。要获取特定领域的生成文本，你需要*微调*我们的模型 - 也就是，用特定于我们任务的数据集进行训练。
- en: 10.1.7 Fine-tuning your generative model
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.7 微调生成模型。
- en: In your case, this dataset would be this very book, parsed into a database of
    lines. Let’s load it from `nlpia2` repository. In this case, we only need the
    book’s text, so we’ll ignore code, headers, and all other things that will not
    be helpful for our generative model.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 对于你来说，该数据集将是本书的全文，解析为一系列文本行的数据库。让我们从“nlpia2”存储库中加载它。在这种情况下，我们只需要书的文本，因此我们将忽略代码、标头和所有其他无法帮助生成模型的内容。
- en: Let’s also initialize a new version of our GPT-2 model for finetuning. We can
    reuse the tokenizer for GPT-2 we initialized before.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们还为微调初始化一个新版本的 GPT-2 模型。我们可以重用之前初始化的 GPT-2 的标记化程序。
- en: Listing 10.8 Loading the NLPiA2 lines as training data for GPT-2
  id: totrans-201
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10.8 将 NLPiA2 行作为 GPT-2 的训练数据进行加载。
- en: '[PRE28]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This will read all the sentences of natural language text in the manuscript
    for this book. Each line or sentence will be a different "document" in your NLP
    pipeline, so your model will learn how to generate sentences rather than longer
    passages. You want to wrap your list of sentences with a PyTorch `Dataset` class
    so that your text will be structured in the way that our training pipeline expects.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这将读取本书手稿中所有自然语言文本的句子。每行或句子将成为你的 NLP 流水线中的不同“文档”，因此你的模型将学习如何生成句子而不是较长的段落。你需要使用
    PyTorch `Dataset` 类将你的句子列表包装起来，以便你的文本结构符合我们的训练流程的要求。
- en: Listing 10.9 Creating a PyTorch `Dataset` for training
  id: totrans-204
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示范 10.9 创建用于训练的 PyTorch `Dataset`。
- en: '[PRE29]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Now, we want to set aside some samples for evaluating our loss mid-training.
    Usually, we would need to wrap them in the `DataLoader` wrapper, but luckily,
    the Transformers package simplifies things for us.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们要留出一些样本来评估我们的损失。通常，我们需要将它们包装在`DataLoader`包装器中，但幸运的是，Transformers 包简化了我们的操作。
- en: Listing 10.10 Creating training and evaluation sets for fine-tuning
  id: totrans-207
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10.10 为微调创建训练和评估集合。
- en: '[PRE30]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Finally, you need one more Transformers library object - DataCollator. It dynamically
    builds batches out of our sample, doing some simple pre-prossesing (like padding)
    in the process. You’ll also define batch size - it will depend on the RAM of your
    GPU. We suggest starting from single-digit batch sizes and seeing if you run into
    out-of-memory errors.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你需要另一个 Transformers 库对象 - DataCollator。它会动态地将我们的样本组成批次，在此过程中进行一些简单的预处理（如填充）。你还需要定义批次大小
    - 这取决于你的 GPU 的内存。我们建议从一位数的批次大小开始，并查看是否遇到了内存不足的错误。
- en: If you were doing the training in PyTorch, there are multiple parameters that
    you would need to specify - such as the optimizer, its learning rate, and the
    warmup schedule for adjusting the learning rate. This is how you did it in the
    previous chapters. This time, we’ll show you how to use the presets that `transformers`
    package offers in order to train the model as a part of `Trainer` class. In this
    case, we only need to specify the batch size and number of epochs! Easy-peasy.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是在 PyTorch 中进行训练，你需要指定多个参数 —— 比如优化器、学习率以及调整学习率的预热计划。这就是你在之前章节中所做的。这一次，我们将向你展示如何使用
    `transformers` 包提供的预设来将模型作为 `Trainer` 类的一部分进行训练。在这种情况下，我们只需要指定批量大小和周期数！轻松愉快。
- en: Listing 10.11 Defining training arguments for GPT-2 fine-tuning
  id: totrans-211
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 代码清单 10.11 为 GPT-2 微调定义训练参数
- en: '[PRE31]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Now you have the pieces that a HuggingFace training pipeline needs to know to
    start training (finetuning) your model. The `TrainingArguments` and `DataCollatorForLanguageModeling`
    classes help you comply with the Hugging Face API and best practices. It’s a good
    pattern to follow even if you do not plan to use Hugging Face to train your models.
    This pattern will force you to make all your pipelines maintain a consistent interface.
    This allows you to train, test, and upgrade your models quickly each time you
    want to try out a new base model. This will help you keep up with the fast-changing
    world of open-source transformer models. You need to move fast to compete with
    the *chickenized reverse centaur* algorithms that BigTech is using to try to enslave
    you.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经掌握了 HuggingFace 训练管道需要的所有要素，可以开始训练（微调）你的模型了。 `TrainingArguments` 和 `DataCollatorForLanguageModeling`
    类可以帮助你遵循 Hugging Face API 和最佳实践。即使你不打算使用 Hugging Face 来训练你的模型，这也是一个很好的模式。这种模式会迫使你确保所有的管道都保持一致的接口。这样一来，每次你想尝试一个新的基础模型时，都可以快速地训练、测试和升级你的模型。这将帮助你跟上开源转换器模型快速变化的世界。你需要迅速行动，以便与
    BigTech 正试图使用的 *鸡化逆向半人马* 算法竞争，他们试图奴役你。
- en: The `mlm=False` (masked language model) setting is an especially tricky quirk
    of transformers. This is your way of declaring that the dataset used for training
    your model need only be given the tokens in the causal direction — left to right
    for English. You would need to set this to True if you are feeding the trainer
    a dataset that has random tokens masked. This is the kind of dataset used to train
    bidirectional language models such as BERT.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '`mlm=False`（掩码语言模型）设置是转换器特别棘手的一个怪癖。这是你声明的方式，即用于训练模型的数据集只需要按因果方向提供令牌 —— 对于英语来说是从左到右。如果你要向训练器提供一个随机令牌掩码的数据集，你需要将其设置为
    True。这是用于训练双向语言模型如 BERT 的数据集的一种类型。'
- en: Note
  id: totrans-215
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: A causal language model is designed to work the way a neurotypical human brain
    model works when reading and writing text. In your mental model of the English
    language, each word is causally linked to the next one you speak or type as you
    move left to right. You can’t go back and revise a word you’ve already spoken
    …​ unless you’re speaking with a keyboard. And we use keyboards a lot. This has
    caused us to develop mental models where we can skip around left or right as we
    read or compose a sentence. Perhaps if we’d all been trained to predict masked-out
    words, like BERT was, we would have a different (possibly more efficient) mental
    model for reading and writing text. Speed reading training does this to some people
    as they learn to read and understand several words of text all at once, as fast
    as possible. People who learn their internal language models differently than
    the typical person might develop the ability to hop around from word to word in
    their mind, as they are reading or writing text. Perhaps the language model of
    someone with symptoms of dyslexia or autism is somehow related to how they learned
    the language. Perhaps the language models in neurodivergent brains (and speed
    readers) are more similar to BERT (bidirectional) rather than GPT (left-to-right).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 因果语言模型的设计是为了模拟人类大脑模型在阅读和书写文本时的工作方式。在你对英语的心理模型中，每个词都与你左到右移动时说或打的下一个词有因果关系。你不能回去修改你已经说过的词……除非你在用键盘说话。而我们经常使用键盘。这使我们形成了跳跃阅读或撰写句子时可以左右跳跃的心理模型。也许如果我们所有人都被训练成像BERT那样预测被屏蔽的单词，我们会有一个不同（可能更有效）的阅读和书写文本的心理模型。速读训练会使一些人在尽可能快地阅读和理解几个词的文本时，学会一次性读懂几个单词。那些将内部语言模型学习方式与典型人不同的人可能会在阅读或书写文本时开发出在心里从一个词跳到另一个词的能力。也许有阅读困难或自闭症症状的人的语言模型与他们学习语言的方式有关。也许神经非常规脑中的语言模型（以及速读者）更类似于BERT（双向），而不是GPT（从左到右）。
- en: Now you are ready for training! You can use your collator and training args
    to configure the training and turn it loose on your data.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经准备好开始训练了！你可以使用你的整理器和训练参数来配置训练，并将其应用于你的数据。
- en: Listing 10.12 Fine-tuning GPT-2 with HuggingFace’s Trainer class
  id: totrans-218
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.12 使用HuggingFace的Trainer类微调GPT-2
- en: '[PRE32]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This training run can take a couple of hours on a CPU. So if you have access
    to a GPU you might want to train your model there. The training should run about
    100x faster on a GPU.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这次训练运行在CPU上可能需要几个小时。所以如果你可以访问GPU，你可能想在那里训练你的模型。在GPU上训练应该会快大约100倍。
- en: Of course, there is a trade-off in using off-the-shelf classes and presets — it
    gives you less visibility on how the training is done and makes it harder to tweak
    the parameters to improve performance. As a take-home task, see if you can train
    the model the old way, with a PyTorch routine.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在使用现成的类和预设时存在一种权衡——它会使你在训练方式上的可见性降低，并且使得调整参数以提高性能更加困难。作为一个可带回家的任务，看看你是否可以用PyTorch例程以老方法训练模型。
- en: Let’s see how well our model does now!
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看我们的模型表现如何！
- en: '[PRE33]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: OK, that looks like a sentence you might find in this book. Take a look at the
    results of the two different models together to see how much your fine-tuning
    changed the text the LLM will generate.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，那看起来像是这本书中可能会出现的句子。一起看看两种不同模型的结果，看看你的微调对LLM生成的文本有多大影响。
- en: '[PRE34]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'That looks like quite a difference! The vanilla model interprets the term ''neural
    networks'' in its biological connotation, while the fine-tuned model realizes
    we’re more likely asking about artificial neural networks. Actually, the sentence
    that the fine-tuned model generated resembles closely a sentence from Chapter
    7:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来差别还是挺大的！普通模型将术语“神经网络”解释为其生物学内涵，而经过微调的模型意识到我们更有可能在询问人工神经网络。实际上，经过微调的模型生成的句子与第7章的一句话非常相似：
- en: Neural networks are often referred to as "neuromorphic" computing because they
    mimic or simulate what happens in our brains.
  id: totrans-227
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 神经网络通常被称为“神经形态”计算，因为它们模仿或模拟我们大脑中发生的事情。
- en: There’s a slight difference though. Note the ending of "other human brains".
    It seems that our model doesn’t quite realize that it talks about artificial,
    as opposed to human, neural networks, so the ending doesn’t make sense. That shows
    once again that the generative model doesn’t really have a model of the world,
    or "understand" what it says. All it does is predict the next word in a sequence.
    Perhaps you can now see why even rather big language models like GPT-2 are not
    very smart and will often generate nonsense
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.8 Nonsense (hallucination)
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As language models get larger, they start to sound better. But even the largest
    LLMs generate a lot of nonsense. The lack of "common sense" should be no surprise
    to the experts who trained them. LLMs have *not* been trained to utilize sensors,
    such as cameras and microphones, to ground their language models in the reality
    of the physical world. An embodied robot might be able to ground itself by checking
    its language model with what it senses in the real world around it. It could correct
    its common sense logic rules whenever the real world contradicts those faulty
    rules. Even seemingly abstract logical concepts such as addition have an effect
    in the real world. One apple plus another apple always produces two apples in
    the real world. A grounded language model should be able to count and do addition
    much better.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Like a baby learning to walk and talk, LLMs could be forced to learn from their
    mistakes by allowing them to sense when their assumptions were incorrect. An embodied
    AI wouldn’t survive very long if it made the kinds of common sense mistakes that
    LLMs make. An LLM that only consumes and produces text on the Internet has no
    such opportunity to learn from mistakes in the physical world. An LLM "lives"
    in the world of social media, where fact and fantasy are often indistinguishable.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: So even the largest of the large, trillion-parameter transformers will generate
    nonsense responses. Scaling up the nonsense training data won’t help. The largest
    and most famous LLMs were trained on virtually the entire Internet and this only
    improves their grammar and vocabulary, not their reasoning ability. Some engineers
    and researchers describe this nonsensical text as *hallucinating*. But that’s
    a misnomer that can lead you astray in your quest to get something consistently
    useful out of LLMs. An LLM can’t even hallucinate because it can’t think, much
    less reason or have a mental model of reality.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Hallucination happens when a human fails to separate imagined images or words
    from the reality of the world they live in. But an LLM has no sense of reality
    and has never lived in the real world. An LLM that you use on the Internet has
    never been embodied in a robot. It has never suffered from the consequences of
    mistakes. It can’t think, and it can’t reason. So it can’t hallucinate.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: LLMs have no concept of truth, facts, correctness, or reality. LLMs that you
    interact with online "live" in the unreal world of the Internet. Engineers fed
    them texts from both fiction and nonfiction sources. If you spend a lot of time
    probing what an LLM knows you will quickly get a feel for just how ungrounded
    models like ChatGPT are. At first, you may be pleasantly surprised by how convincing
    and plausible the responses to your questions are. And this may lead you to anthropomorphize
    it. And you might claim that its ability to reason was an "emergent" property
    that researchers didn’t expect. And you would be right. The researchers at BigTech
    have not even begun to try to train LLMs to reason. They hoped the ability to
    reason would magically emerge if they gave LLMs enough computational power and
    text to read. Researchers hoped to shortcut the need for AI to interact with the
    physical world by giving LLMs enough *descriptions* of the real world to learn
    from. Unfortunately, they also gave LLMs an equal or larger dose of fantasy. Most
    of the text found online is either fiction or intentionally misleading.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs对真相、事实、正确性或现实没有概念。你在网上与之交互的LLMs“生活”在互联网虚幻的世界中。工程师们为它们提供了来自小说和非小说来源的文本。如果你花费大量时间探索LLMs知道的内容，你很快就会感受到像ChatGPT这样的模型是多么不踏实。起初，你可能会对它对你问题的回答有多么令人信服和合理感到惊讶。这可能会导致你赋予它人格化。你可能会声称它的推理能力是研究人员没有预料到的“
    emergent ”属性。而你说得对。BigTech的研究人员甚至没有开始尝试训练LLMs进行推理。他们希望，如果他们为LLMs提供足够的计算能力和阅读的文本，推理能力将会神奇地出现。研究人员希望通过为LLMs提供足够的对真实世界的*描述*来抄近道，从而避免AI与物理世界互动的必要性。不幸的是，他们也让LLMs接触到了同等或更多的幻想。在线找到的大部分文本要么是小说，要么是有意误导的。
- en: So the researchers' hope for a shortcut was misguided. LLMs only learned what
    they were taught — to predict the most *plausible* next words in a sequence. By
    using the like button to nudge LLMs with reinforcement learning, BigTech has created
    a BS artist rather than the honest and transparent virtual assistant that they
    claimed to be building. Just as the like button on social media has turned many
    humans into sensational blow-hards, it has turned LLMs into "influencers" that
    command the attention of more than 100 million users. And yet LLMs have no ability
    or incentives (objective functions) to help them differentiate fact from fiction.
    To improve the machine’s answers' relevance and accuracy, you need to get better
    at *grounding* your models - have their answers based on relevant facts and knowledge.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，研究人员对于捷径的希望是错误的。LLMs只学到了它们所教的东西——预测序列中最*合理*的下一个词。通过使用点赞按钮通过强化学习来引导LLMs，BigTech创建了一个BS艺术家，而不是他们声称要构建的诚实透明的虚拟助手。就像社交媒体上的点赞按钮把许多人变成了轰动的吹牛者一样，它们把LLMs变成了“影响者”，吸引了超过1亿用户的注意力。然而，LLMs没有能力或动机（目标函数）来帮助它们区分事实和虚构。为了提高机器回答的相关性和准确性，你需要提高*grounding*模型的能力——让它们的回答基于相关的事实和知识。
- en: Luckily, there are time-tested techniques for incentivizing generative models
    for correctness. Information extraction and logical inference on knowledge graphs
    are very mature technologies. And most of the biggest and best knowledge bases
    of facts are completely open source. BigTech can’t absorb and kill them all. Though
    the open source knowledge base FreeBase has been killed, Wikipedia, Wikidata,
    and OpenCyc all survive. In the next chapter, you will learn how to use these
    knowledge graphs to ground your LLMs in reality so that at least they will not
    be incentivized to be deceiving as most BigTech LLMs are.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有一些经过时间考验的技术可以激励生成模型达到正确性。知识图谱上的信息提取和逻辑推理是非常成熟的技术。而且大部分最大、最好的事实知识库都是完全开放源代码的。BigTech无法吸收并摧毁它们所有。尽管开源知识库FreeBase已经被摧毁，但Wikipedia、Wikidata和OpenCyc仍然存在。在下一章中，你将学习如何使用这些知识图谱来让你的LLMs接触现实，这样至少它们就不会像大多数BigTech的LLMs那样有欺骗性。
- en: In the next section, you will learn another way to ground your LLM in reality.
    And this new tool won’t require you to build and validate a knowledge graph by
    hand. You may have forgotten about this tool even though you use it every day.
    It’s called *information retrieval*, or just *search*. Instead of giving the model
    a knowledge base of facts about the world, you can search unstructured text documents
    for those facts, in real time.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，你将学习另一种让你的LLM接触现实的方法。而这个新工具不需要你手动构建和验证知识图谱。即使你每天都在使用它，你可能已经忘记了这个工具。它被称为*信息检索*，或者只是*搜索*。你可以在实时搜索非结构化文本文档中的事实，而不是给模型提供关于世界的事实知识库。
- en: 10.2 Giving LLMs an IQ boost with search
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.2 使用搜索功能来提升LLMs的智商
- en: One of the most powerful features of a large language model is that it will
    answer any question you ask it. But that’s its most dangerous feature as well.
    If you use an LLM for information retrieval (search) you have no way to tell whether
    its answer is correct or not. LLMs are not designed for information retrieval.
    And even if you did want them to memorize everything they read, you couldn’t build
    a neural network large enough to store all that information. LLMs compress everything
    they read and store it in the weights of the deep learning neural network. And
    just like normal compression algorithms such as "zip", this compression process
    forces an LLM to generalize about the patterns it sees in words whenever you train
    it on a new document.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型最强大的特点之一是它会回答你提出的任何问题。但这也是它最危险的特点。如果你将LLM用于信息检索（搜索），你无法判断它的答案是否正确。LLMs并不是为信息检索而设计的。即使你想让它们记住读过的所有内容，你也无法构建一个足够大的神经网络来存储所有的信息。LLMs将它们读到的所有内容进行压缩，并将其存储在深度学习神经网络的权重中。而且就像常规的压缩算法（例如“zip”）一样，这个压缩过程会迫使LLM对它在训练时看到的单词模式进行概括。
- en: 'The answer to this age-old problem of compression and generalization is the
    age-old concept of information retrieval. You can build LLMs that are faster,
    better, cheaper if you combine the word manipulation power of LLMs with the old-school
    information retrieval power of a search engine. In the next section you see how
    to build a search engine using TF-IDF vectors that you learned about in Chapter
    3\. And you’ll learn how to make that full-text search approach scale to millions
    of documents. Later you will also see how LLMs can be used to improve the accuracy
    of your search engine by helping you find more relevant documents based on their
    semantic vectors (embeddings). At the end of this chapter you will know how to
    combine the three essential algorithms you need to create an NLP pipeline that
    can answer your questions intelligently: text search, semantic search, and an
    LLM. You need the scale and speed of text search combined with the accuracy and
    recall of semantic search to build a useful question answering pipeline.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个古老的压缩和概括问题的答案就是信息检索的古老概念。如果将LLMs的词语处理能力与一个搜索引擎的传统信息检索能力相结合，那么你可以构建更快、更好、更便宜的LLMs。在下一节中，你将看到如何使用你在第三章学到的TF-IDF向量来构建一个搜索引擎。你将学习如何将全文搜索方法扩展到数百万个文档。之后，你还将看到如何利用LLMs来提高搜索引擎的准确性，通过基于语义向量（嵌入）帮助你找到更相关的文档。在本章结束时，你将知道如何结合这三个必需的算法来创建一个能够智能回答问题的自然语言处理流水线：文本搜索、语义搜索和LLM。你需要文本搜索的规模和速度，结合语义搜索的准确性和召回率，才能构建一个有用的问答流水线。
- en: '10.2.1 Searching for words: full-text search'
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.1 搜索词语：全文搜索
- en: Navigating the gargantuan landscape of the Internet to find accurate information
    can often feel like an arduous quest. That’s also because, increasingly, the text
    you’re seeing on the internet is not written by a human, but by a machine. With
    machines being unbounded by the limits of human effort required to create new
    information, the amount of text on the Internet is growing exponentially. It doesn’t
    require bad actors to generate misleading or nonsense text. As you saw in previous
    sections, the objective function of the machine is just not aligned with your
    best interest. Most of the text generated by machines contains misinformation
    crafted to attract your clicks rather than help you discover new knowledge or
    refine your own thinking.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 导航到互联网浩瀚的世界中寻找准确的信息常常感觉就像是一次费力的探险。这也是因为，越来越多的互联网文本并非由人类撰写，而是由机器生成的。由于机器在创建新的信息所需要的人力资源的限制，互联网上的文本数量呈指数级增长。生成误导性或无意义文本并不需要恶意行为。正如你在之前的章节中所看到的，机器的目标函数与你最佳利益并不一致。机器生成的大部分文本都包含误导性信息，旨在吸引你点击，而不是帮助你发现新知识或完善自己的思考。
- en: Fortunately, just as machines are used to create misleading text they can also
    be your ally in finding the accurate information you’re looking for. Using the
    tools you’ve learned about so far, you can take control of the LLMs you use by
    using open source models and grounding them with human-authored text retrieved
    from high-quality sources on the Internet or your own library. The idea of using
    machines to aid search efforts is almost as old as the World Wide Web itself.
    While at its very beginning, the WWW was indexed by hand by its creator, Tim Berners-Lee,^([[27](#_footnotedef_27
    "View footnote.")]) after the HTTP protocol was released to the public, this was
    no longer feasible.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，就像机器用来创建误导性文本一样，它们也可以成为你寻找准确信息的盟友。使用你们学到的工具，你可以通过使用开源模型和从互联网高质量来源或自己的图书馆检索的人工撰写文本，在所使用的
    LLMs 中掌控。使用机器辅助搜索的想法几乎与万维网本身一样古老。虽然在它的开端，WWW 是由它的创建者 Tim Berners-Lee 手动索引的，^([[27](#_footnotedef_27
    "View footnote.")]) 但在 HTTP 协议向公众发布后，这再也不可行了。
- en: '*Full-text searches* started to appear very quickly due to people’s need to
    find information related to keywords. Indexing, and especially reverse indexing,
    was what helped this search to be fast and efficient. Inverse indexes work similarly
    to the way you would find a topic in a textbook - by looking at the index at the
    end of the book and finding the page numbers where the topic is mentioned.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 由于人们需要查找与关键词相关的信息，*全文搜索* 很快就开始出现。索引，尤其是反向索引，是帮助这种搜索变得快速和高效的关键。反向索引的工作方式类似于你在教科书中查找主题的方式——查看书末的索引并找到提到该主题的页码。
- en: The first full-text search indices just cataloged the words on every web page
    and their position on the page to help find the pages that matched the keywords
    they were looking for exactly. You can imagine, though, that this method of indexing
    was quite limited. For example, if you were looking for the word "cat", but the
    page only mentioned "cats", it would not come up in your search results. That’s
    why modern full-text search engines use character-based trigram indexes to help
    you find both "cats" and "cat" no matter what you type into the search bar …​
    or LLM chatbot prompt.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个全文搜索索引只是编目了每个网页上的单词以及它们在页面上的位置，以帮助查找确切匹配所查关键词的页面。然而，你可以想象，这种索引方法非常有限。例如，如果你正在查找单词“猫”，但页面只提到了“猫咪”，则不会在搜索结果中出现。这就是为什么现代的全文搜索引擎使用基于字符的三元组索引，以帮助你找到不管你输入搜索栏中的任何内容或
    LLM 聊天机器人提示都能搜到的“猫”和“猫咪”。
- en: Web-scale reverse indices
  id: totrans-246
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Web 规模的反向索引
- en: As the internet grew, the need for more efficient search engines grew with it.
    Increasingly, organizations started to have their own intranets and were looking
    for ways to efficiently find information within them. That gave birth to the field
    of enterprise search, and to search engine libraries like Apache Lucene. Lucene
    is a Java library that is used by many open-source search engines, including Elasticsearch,^([[28](#_footnotedef_28
    "View footnote.")]) Solr ^([[29](#_footnotedef_29 "View footnote.")]) and OpenSearch.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 随着互联网的发展，越来越多的组织开始拥有自己的内部网络，并寻找在其中高效地查找信息的方法。这催生了企业搜索领域，以及像 Apache Lucene ^([[28](#_footnotedef_28
    "View footnote.")])，Solr ^([[29](#_footnotedef_29 "View footnote.")]) 和 OpenSearch
    等搜索引擎库。
- en: A (relatively) new player in the field, Meilisearch ^([[30](#_footnotedef_30
    "View footnote.")]) offers a search engine that is easy to use and deploy. Therefore,
    it might be a better starting point in your journey in the full-text search world
    than other, more complex engines.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在该领域中的一个（相对）新的参与者，Meilisearch ^([[30](#_footnotedef_30 "View footnote.")]) 提供了一款易于使用和部署的搜索引擎。因此，它可能比其他更复杂的引擎成为你在全文搜索世界中开始旅程的更好起点。
- en: Apache Solr, Typesense, Meilisearch and other full-text search engines are fast
    and scale well to large numbers of documents. Apache Solr can scale to the entire
    Internet. It is the engine behind the search bar in DuckDuckGo and Netflix. And
    conventional search engines can even return results in real time *as-you-type*.
    The *as-you-type* feature is even more impressive than the autocomplete or search
    suggestions you may have seen in your web browser. Meilisearch and Typesense are
    so fast, they give you the top 10 search results in milliseconds, sorting and
    repopulating the list with each new character you type. But full-text search has
    a weakness — it searches for *text* matches rather than *semantic* matches. So
    conventional search engines return a lot of "false negatives" when the words in
    your query don’t appear in the documents you are looking for.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Solr、Typesense、Meilisearch等全文搜索引擎快速且能很好地扩展到大量文档。Apache Solr可以扩展到整个互联网。它是DuckDuckGo和Netflix搜索栏背后的引擎。传统搜索引擎甚至可以*随输入实时返回结果*。*随输入实时*功能比您可能在网络浏览器中看到的自动完成或搜索建议更令人印象深刻。Meilisearch和Typesense如此快速，它们可以在毫秒内为您提供前10个搜索结果，每次键入新字符时对列表进行排序和重新填充。但全文搜索有一个弱点
    - 它搜索*文本*匹配而不是*语义*匹配。因此，传统搜索引擎在您的查询中的单词不出现在您要查找的文档中时会返回很多"假阴性"。
- en: Improving your full-text search with trigram indices
  id: totrans-250
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用三元组索引改进您的全文搜索
- en: The reverse indices we introduced in the previous section are very useful for
    finding exact matches of words, but not great for finding approximate matches.
    Stemming and lemmatization can help increase the matching of different forms of
    the same word; however, what happens when your search contains typos or misspellings?
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前一节介绍的逆向索引对于找到单词的精确匹配非常有用，但并不适合找到近似匹配。词干处理和词形还原可以帮助增加同一个词不同形式的匹配；然而，当您的搜索包含拼写错误或拼写错误时会发生什么？
- en: To give you an example - Maria might be searching the internet for the biography
    of the famous author Steven King. If the search engine she’s using uses the regular
    reverse index, she might never find what she’s looking for - because King’s name
    is spelled as Stephen. That’s where trigram indices come in handy.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子 - 玛丽亚可能在网上搜索著名作家斯蒂芬·金的传记。如果她使用的搜索引擎使用常规的逆向索引，她可能永远找不到她要找的东西 - 因为金的名字拼写为斯蒂芬。这就是三元组索引派上用场的地方。
- en: Trigrams are groups of three consecutive characters in a word. For example,
    the word "trigram" contains the trigrams "tri", "rig", "igr", "gra" and "ram".
    It turns out that trigram similarity - comparing two words based on the number
    of trigrams they have in common - is a good way to find approximate matches of
    words. And multiple databases and search engines, from Elasticsearch to PostgreSQL,
    support trigram indices. These trigram indices turn out to be much more effective
    at dealing with misspellings and different word forms than stemming and lemmatization.
    A trigram index will improve both the recall *and* the precision of your search
    results.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 三元组是单词中三个连续字符的组合。例如，单词"trigram"包含三元组"tri"、"rig"、"igr"、"gra"和"ram"。事实证明，三元组相似性
    - 基于它们共有的三元组数量比较两个单词 - 是一种寻找单词近似匹配的好方法。从Elasticsearch到PostgreSQL，多个数据库和搜索引擎都支持三元组索引。这些三元组索引比词干处理和词形还原更有效地处理拼写错误和不同的单词形式。三元组索引将提高你的搜索结果的召回率*和*精确度。
- en: Semantic search allows you to find what you’re looking for even when you can’t
    think of the exact words that the authors used when they wrote the text you are
    searching for. For example, imagine you’re searching for articles about "big cats."
    If the corpus contains texts about lions, tigers (and bears oh my), but never
    mentions the word "cat", your search query won’t return any documents. This creates
    a false negative error in your search algorithm and would reduce the overall *recall*
    of your search engine, a key measure of search engine performance. The problem
    gets much worse if you’re looking for a subtle piece of information that takes
    many words to describe, such as the query "I want a search algorithm with high
    precision, recall, and it needs to be fast."
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 语义搜索允许您在您无法想起作者写文本时使用的确切单词时找到您要找的内容。例如，想象一下，您正在搜索关于"大猫"的文章。如果语料库包含关于狮子、老虎（还有熊），但从未提到"猫"这个词，您的搜索查询将不返回任何文档。这会在搜索算法中产生一个假阴性错误，并降低您的搜索引擎的总*召回率*，这是搜索引擎性能的一个关键指标。如果您正在寻找需要用很多词语描述的微妙信息，比如查询"I
    want a search algorithm with high precision, recall, and it needs to be fast."，问题会变得更加严重。
- en: Here’s another scenario where a full-text search won’t be helpful - let’s say
    you have a movie plots database, and you’re trying to find a movie whose plot
    you vaguely remember. You might be lucky if you remember the names of the actors
    - but if you type something like "Diverse group spends 9 hours returning jewelry",
    you’re not likely to receive "Lord of the Rings" as part of your search results.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, full-text search algorithms don’t take advantage of the new, better
    ways to embed words and sentences that LLMs give you. BERT embeddings are much,
    much better at reflecting the meaning of the text that you process. And the *semantic
    similarity* of pieces of text that talk about the same thing will show up in these
    dense embeddings even when you documents use different words to describe similar
    things.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: And you really need those semantic capabilities for your LLM to be truly useful.
    Large language models in popular applications like ChatGPT, You.com or Phind use
    semantic search under the hood. A raw LLM has no memory of anything you’ve said
    previously. It is completely stateless. You have to give it a run-up to your question
    every single time you ask it something. For example, when you ask an LLM a question
    about something you’ve said earlier in a conversation, the LLM can’t answer you
    unless it saved the conversation in some way.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '10.2.2 Searching for meaning: semantic search'
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The key to helping your LLM out is finding a few relevant passages of text to
    include in your prompt. That’s where semantic search comes in.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, semantic search is much more computationally difficult than text
    search.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: You learned in Chapter 3 how to compare sparse binary (0 or 1) vectors that
    tell you whether each word is in a particular document. In the previous section
    you learned about several databases that can search those sparse binary vectors
    very very efficiently, even for millions of documents. And you always find the
    exact documents that contain the words you’re looking for. PostgreSQL and conventional
    search engines have this feature built into them, right from the start. Internally
    they can even use fancy math like a *Bloom filter* to minimize the number of binary
    comparisons your search engine needs to make. Unfortunately, these seemingly magical
    algorithms that work for the sparse discrete vectors used for text search don’t
    work for the dense embedding vectors of LLMs.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: So what can you do to implement a scalable semantic search engine? You could
    use brute force, and do the dot product for all the vectors in your database.
    Even though that would give you the exact answer with the highest accuracy, it
    would take a lot of time (computation). What’s worse is that your search engine
    would get slower and slower as you added more documents. The brute force approach
    scales linearly with the number of documents in your database.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, you are going to need to add a lot of documents to your database
    if you want your LLM to work well. When you use LLMs for question answering and
    semantic search, they can only handle a few sentences at a time. So you will need
    to break all the documents in your database into paragraphs or even sentences
    if you want to get good results with your LLM pipeline. This explodes the number
    of vectors you need to search. Brute force won’t work, and there is no magical
    math that will work on dense continuous vectors.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: That’s why you need powerful search tools in your arsenal. Vector databases
    are the answer to this challenging semantic search problem. Vector databases are
    powering a new generation of search engines that can quickly find the information
    you are looking for, even if you need to search the entire Internet. But before
    we get to that, let’s take a look at the basics of search.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: So now let’s reframe your problem from full-text search to semantic search.
    You have a search query, that you can embed using an LLM. And you have your database
    of text documents, where you’ve embedded every document into a vector space using
    the same LLM. Among those vectors, you want to find the vector that is closest
    to your query vector — that is, the *cosine similarity* (dot product) is maximized.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.3 Approximate nearest neighbor (ANN) search
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There is only one way to find the *exact* nearest neighbor for our query. Remember
    how we discussed exhaustive search in Chapter 4? Back then, we found the nearest
    neighbor of the search query by computing its dot product with every vector in
    the database. That was OK because your database back then included only a couple
    dozen vectors. It won’t scale to a database with thousands or millions of documents.
    And your vectors are high dimensional — BERT’s sentence embeddings have 768 dimensions.
    This means any math you want to do on the vectors is cursed with *curse of dimensionality*.
    And LLM embeddings are even larger, so the curse is going to get even worse if
    you use models larger than BERT. You wouldn’t want Wikipedia’s users to wait while
    you’re performing dot products on 6 million articles!
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: As often happens in the real world, you need to give something to get something.
    If you want to optimize the algorithm’s retrieval speed, you need to compromise
    on precision. As you saw in Chapter 4, you don’t need to compromise too much,
    and the fact that you find several approximate neighbors can actually be useful
    for your users, and increase the chance they’ll find what they’ve been looking
    for.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: In Chapter 4 you saw an algorithm called Locality Sensitive Hashing (LSH) that
    helps you to find your vector’s *approximate nearest neighbors* by assigning a
    hash to regions of the high dimensional space (hyperspace) where your embeddings
    are located. LSH is an Approximate k-Nearest Neighbors (ANN) algorithm, that is
    responsible for both indexing your vectors and retrieving the neighbors you’re
    looking for. But there are many others that you’re about to meet. Each of them
    has its strengths and weaknesses.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 4 章中，你已经看到了一种名为局部敏感哈希（LSH）的算法，它通过为高维空间（超空间）中你的嵌入所在的区域分配哈希来帮助你寻找*近似最近邻*的向量。LSH
    是一个近似 k-最近邻（ANN）算法，既负责索引你的向量，也负责检索你正在寻找的邻居。但你将遇到的还有许多其他算法，每种算法都有其优势和劣势。
- en: To create your semantic search pipeline, you’ll need to make two crucial choices — which
    model to use to create your embeddings, and which ANN indexing algorithm you’re
    going to use. You’ve already seen in this Chapter how an LLM can help you increase
    the accuracy of your vector embeddings. So the main remaining decision is how
    to index your vectors.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建你的语义搜索管道，你需要做出两个关键选择——使用哪个模型来创建你的嵌入，并选择使用哪个 ANN 索引算法。你已经在本章中看到了 LLM 如何帮助你提高向量嵌入的准确性。因此，主要剩下的决定是如何索引你的向量。
- en: If you’re building a production-level application that needs to scale to thousands
    or millions of users, you might also look for a hosted implementation for your
    vector database, such as Pinecone, Milvus, or OpenSearch. A hosted solution will
    allow you to store and retrieve your semantic vectors fast enough and accurately
    enough to give your users a pleasant user experience. And the provider will manage
    the complexity of scaling up your vector database as your app becomes more and
    more popular.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在构建一个需要扩展到数千或数百万用户的生产级应用程序，你可能会寻找托管的向量数据库实现，如 Pinecone、Milvus 或 OpenSearch。托管方案将使你能够快速准确地存储和检索语义向量，从而为用户提供愉悦的用户体验。而提供商将管理扩展你的向量数据库的复杂性，随着你的应用程序越来越受欢迎。
- en: But your probably even more interested in how you can bootstrap your own vector
    search pipeline. Turns out it’s not too difficult to do on your own, even for
    databases up to a million or more vectors (documents).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 但你可能更感兴趣的是如何启动自己的向量搜索管道。事实证明，即使对于拥有数百万个向量（文档）的数据库，你自己也可以轻松完成这项任务。
- en: 10.2.4 Choose your index
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.4 选择索引
- en: With the increasing need to search for pieces of information in increasingly
    large datasets, the field of ANN algorithms boomed. Vector database product launches
    have been announced nearly every month recently. And you may be lucky and your
    relational or document database has already started to release early versions
    of vector search algorithms built in.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 随着在越来越大的数据集中查找信息的需求不断增加，ANN 算法的领域也迅速发展。近期几乎每个月都有向量数据库产品推出。而且你可能很幸运，你的关系型或文档型数据库已经开始发布内置的向量搜索算法早期版本。
- en: If you use PostgreSQL as your production database, you’re in luck. In July 2023
    they released the `pgvector` plugin which provides you with a seamless way to
    store and index vectors in your database. They provide both exact and approximate
    similarity search indexes so you can play with the tradeoffs between accuracy
    and speed that work for you in your application. If you combine this with PostgreSQL’s
    performant and reliable full-text search indexes, you can likely scale your NLP
    pipeline to millions of users and documents.^([[31](#_footnotedef_31 "View footnote.")])
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在生产数据库中使用 PostgreSQL，你很幸运。他们在 2023 年 7 月发布了 `pgvector` 插件，为你提供了一种无缝的方式来在数据库中存储和索引向量。他们提供精确和近似相似性搜索索引，因此你可以在应用中尝试适合你的准确性和速度之间的权衡。如果你将此与
    PostgreSQL 的高效和可靠的全文搜索索引相结合，很可能可以将你的 NLP 管道扩展到数百万用户和文档。^([[31](#_footnotedef_31
    "View footnote.")])
- en: Unfortunately, at the time of this writing, it is early days for the `pgvector`
    software. In September 2023, the ANN vector search feature in `pgvector` is in
    the bottom quartile of the rankings for speed. And you will be limited to two
    thousand dimensions for your embedding vectors. So if you’re indexing sequences
    of several embeddings, or you are using high dimensional vectors from a large
    language model, you will need to add a dimension reduction step (PCA for example)
    to your pipeline.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: LSH was developed in the early 2000s; since then, dozens of algorithms joined
    the ANN family. There are a few large families of ANN algorithms. We’ll look at
    three of them - hash-based, tree-based and graph-based.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: The hash-based algorithms are best represented by LSH itself. You already saw
    how the indexing works in LSH in Chapter 4, so we won’t spend any time on it here.
    Despite its simplicity, the LSH algorithm is still widely used within popular
    libraries such as Faiss (Facebook AI Similarity Search) which we’ll use in a bit.^([[32](#_footnotedef_32
    "View footnote.")]) It also has spawned modified versions for specific goals,
    such as the DenseFly algorithm that is used for searching biological datasets.^([[33](#_footnotedef_33
    "View footnote.")])
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: To understand how tree-based algorithms work, let’s look at Annoy, a package
    created by Spotify for its music recommendations. Annoy algorithm recursively
    partitioning the input space into smaller and smaller subspaces using a binary
    tree structure. At each level of the tree, the algorithm selects a hyperplane
    that splits the remaining points in the subspace into two groups. Eventually,
    each data point is assigned to a leaf node of the tree.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: To search for the nearest neighbors of a query point, the algorithm starts at
    the root of the tree and goes down by making comparisons between the distance
    of the query point to the hyperplane of each node and the distance to the nearest
    point found so far. The deeper the algorithm goes, the more precise the search.
    So you can make searches shorter and less accurate. You can see a simplified visualization
    of the algorithm in Figure [10.3](#figure-annoy-algorithm).
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.3 A simplified visualization of the Annoy algorithm
  id: totrans-281
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![annoy all stages](images/annoy_all_stages.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
- en: Next, let’s look at graph-based algorithms. A good representative of graph-based
    algorithms, *Hierarchical Navigable Small World* (HNSW)^([[34](#_footnotedef_34
    "View footnote.")]) algorithm, approaches the problem bottom-up. It starts by
    building Navigable Small World graphs, which are graphs where each vector is connected
    to its closest neighbors by a vertex. To understand the intuition of it, think
    of the Facebook connections graph - everyone is connected directly only to their
    friends, but if you count "degrees of separation" between any two people, it’s
    actually pretty small. (Stanley Milgram discovered in an experiment in the 1960s
    that on average, every two people were separated by 5 connections.^([[35](#_footnotedef_35
    "View footnote.")]) Nowadays, for Twitter users, this number is as low as 3.5.)
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: HNSW then breaks the NSW graphs into layers, where each layer contains fewer
    points that are further away from each other than the layer beyond it. To find
    your nearest neighbor, you would start traversing the graph from the top, with
    each layer getting you closer to the point that you’re looking for. It’s a bit
    like international travel. You first take the plane to the capital of the country
    where your destination is situated. You then take the train to the smaller city
    closer to the destination. And you can take a bike to get there! At each layer,
    you’re getting closer to your nearest neighbor - and you can stop the retrieval
    at whatever layer, according to the throughput your use case requires.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.5 Quantizing the math
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You may hear about *quantization* being used in combination with other indexing
    techniques. At its core, quantization is basically transforming the values in
    your vectors to create lower-precision vectors with discrete values (integers).
    This way your queries can look for exact matches of integer values, a database
    and numerical computation that is much faster than searching for a floating point
    range of values.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you have a 5D embedding vector stored as an array of 64-bit `float`s.
    Here’s a crude way to quantize a `numpy` float.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.13 Quantizing numpy floats
  id: totrans-288
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: If your indexer does the scaling and integer math correctly, you can retain
    all of the precision of your original vectors with half the space. You reduced
    the search space by half simply by quantizing (rounding) your vectors to create
    32-bit integer buckets. More importantly, if your indexing and query algorithms
    do their hard work with integers rather than floats, they run much much faster,
    often 100 times faster. And if you quantize a bit more, retaining only 16 bits
    of information, you can gain another order of magnitude in compute and memory
    requirements.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The product quantization that you would use to implement semantic search needs
    to be more more complicated than that. The vectors you need to compress are much
    longer (have many more dimensions), the compression needs to be much better at
    retaining all the subtle bits of information in the vectors. This is especially
    important for plagiarism and LLM detectors. It turns out, if you split the document
    vector into multiple smaller vectors, and each of these vectors is quantized separately
    using clustering algorithms. You can learn more about the quantization process
    in .^([[36](#_footnotedef_36 "View footnote.")])
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: If you keep exploring the world of nearest neighbors algorithms, you might run
    into the acronym IVFPQ (Inverse File Index with Product Quantization). The Faiss
    library uses IVFPQ for high-dimensional vectors. ^([[37](#_footnotedef_37 "View
    footnote.")]) And as recently as 2023, the HNSW+PQ combination was adopted by
    frameworks like Weaviate.^([[38](#_footnotedef_38 "View footnote.")]) So this
    is definitely the state of the art for many web-scale applications.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: Indexes that combine many different algorithms are called *composite indexes*.
    Composite indexes are a bit more complex to implement and work with. The search
    and indexing performance (latency, throughput, and resource constraints) are sensitive
    to how the individual stages of the indexing pipeline are configured. If you configure
    them incorrectly they can perform much worse than much simpler vector search and
    indexing pipelines. Why would you want all that extra complexity?
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: The main reason is memory (RAM and GPU memory size). If your vectors are high-dimensional,
    then not only is calculating the dot product a very expensive operation, but your
    vectors also take more space in memory (on your GPU or in your RAM). Even though
    you only load a small part of the database into RAM, you might run out of memory.
    That’s why it’s common to use techniques like PQ to compress the vectors before
    they are fed into another indexing algorithm like IVF or HNSW.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: For most real-world applications when you are not attempting to index the entire
    Internet you can get by with simpler indexing algorithms. And you can always use
    memory mapping libraries to work efficiently with tables of data stored on disk,
    especially Flash drives (solid state disk).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: Choose your implementation library
  id: totrans-297
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now that you have a better idea of the different algorithms, it’s time to look
    at the wealth of implementation libraries that are out there. While the algorithms
    are just a mathematical representation of the indexing and retrieval mechanisms,
    how they are implemented can determine the algorithm’s accuracy and speed. Most
    of the libraries are implemented in memory-efficient languages, such as C++, and
    have Python bindings so that they can be used in Python programming.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: Some libraries implement a single algorithm, such as Spotify’s annoy library.^([[39](#_footnotedef_39
    "View footnote.")]) Others, such as Faiss ^([[40](#_footnotedef_40 "View footnote.")])
    and `nmslib` ^([[41](#_footnotedef_41 "View footnote.")]) have a variety of algorithms
    you can choose from.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: Figure [10.4](#figure-ann-benchmarks) shows the comparison of different algorithm
    libraries on a text dataset. You can discover more comparisons and links to dozens
    of ANN software libraries in Erik Bern’s ANN benchmarks repository.^([[42](#_footnotedef_42
    "View footnote.")])
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.4 Performance comparison of ANN algorithms for the New York Times
    dataset
  id: totrans-301
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![ann benchmarks nyt 256 dataset](images/ann-benchmarks-nyt-256-dataset.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
- en: If you feel decision fatigue and are overwhelmed with all the choices, some
    turnkey solutions can help you out. OpenSearch, a 2021 fork of the ElasticSearch
    project, is a reliable workhorse in the full-text search world and it has a vector
    database and Nearest Neighbors search algorithm built in. And OpenSearch project
    one-ups its business source competitor, ElasticSearch, with cutting-edge plugins
    such as a semantic search vector database and ANN vector search.^([[43](#_footnotedef_43
    "View footnote.")]) The open-source community can often implement state-of-the-art
    algorithms much more quickly than the smaller internal corporate teams that work
    on proprietary software.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-304
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Watch out for open-source projects that may change the software license at any
    time. The ElasticSearch, TensorFlow, Keras, Terraform, and even Redhat Linux developer
    communities have all had to fork these projects after corporate sponsors decided
    to change the software licenses to *business source*. Business source is the term
    developers use to refer to proprietary software that is advertised as open source
    by corporations. The software comes with commercial use restrictions. And the
    sponsoring corporation can change those terms as soon as the project becomes popular
    and they want to monetize the hard work that open-source contributors put into
    the project.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: If you’re feeling a bit intimidated by the prospect of deploying the Java OpenSearch
    packages on Docker containers, you may have more fun with Haystack. It’s a great
    way to experiment with your own ideas for indexing and searching your documents.
    And you’re probably here because you want to understand how it all works. For
    that, you need a Python package. Haystack is the latest and greatest Python package
    for building question-answering and semantic search pipelines.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.6 Pulling it all together with haystack
  id: totrans-307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You’ve now seen almost all the components of a question-answering pipeline
    and it may seem overwhelming. Not to worry. Here are the pieces you’ll need for
    your pipeline:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: A model to create meaningful embeddings of your text
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An ANN library to index your documents and retrieve ranked matches for your
    search queries
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A model that, given the relevant document, will be able to find the answer to
    your question - or to generate it.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a production app, you will also need a vector store (database). A vector
    database holds your embedding vectors and indexes them so you can search them
    quickly. And you can update your vectors whenever the document text changes. Some
    examples of open-source vector databases include Milvus, Weaviate, and Qdrant.
    You can also use some general-purpose datastores like ElasticSearch.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: How do you combine all of this together? Well, just a few years ago, it would
    take you quite some time to figure out how to stitch all of these together. Nowadays,
    a whole family of NLP frameworks provides you with an easy interface to build,
    evaluate and scale your NLP applications, including semantic search. Leading open-source
    semantic search frameworks include Jina,^([[44](#_footnotedef_44 "View footnote.")])
    Haystack,^([[45](#_footnotedef_45 "View footnote.")]) and txtai.^([[46](#_footnotedef_46
    "View footnote.")])
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: In our next section, we’re going to leverage one of these frameworks, Haystack,
    to combine all you’ve learned in the recent chapter into something you can use.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.7 Getting real
  id: totrans-315
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that you’ve learned about the different components of your question-answering
    pipeline, it’s time to bring it all together and create a useful app.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: You’ll be creating a question-answering app based on…​ this very book! You’re
    going to use the same dataset that we saw earlier - sentences from the first 8
    chapters of this book. Your app is going to find the sentence that contains the
    answer to your question.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dive into it! First, we’ll load our dataset and take only the text sentences
    from it, like we did before.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.14 Loading the NLPiA2 lines dataset
  id: totrans-319
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 10.2.8 A haystack of knowledge
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once you’ve loaded the natural language text documents, you want to convert
    them all into Haystack Documents. In Haystack, a Document object contains two
    text fields: a title and the document content (text). Most documents you will
    work with are similar to Wikipedia articles where the title will be a unique human-readable
    identifier for the subject of the document. In your case, the lines of this book
    are too short to have a title that’s different from the content. So you can cheat
    a bit and put the content of the sentence in both the title and the content of
    your `Document` objects.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.15 Converting the NLPiA2 lines into Haystack Documents
  id: totrans-323
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Now you want to put your documents into a database and set up an index so you
    can find the "needle" of knowledge you’re looking for. Haystack has several fast
    vector store indexes that work well for storing documents. The examples below
    use the Faiss algorithm for finding vectors in your haystack of documents. For
    the Faiss document index to work correctly on Windows you will need to install
    haystack from binaries and run your Python code within `git-bash` or WSL (Windows
    Subsystem for Linux).^([[47](#_footnotedef_47 "View footnote.")])
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.16 Only for Windows
  id: totrans-326
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: In Haystack, your document storage database is wrapped in a `DocumentStore`
    object. The `DocumentStore` class gives you a consistent interface to the database
    containing the documents you just downloaded in a CSV. For now the "documents"
    are just the lines of text for an early version of the ASCIIDoc manuscript for
    this book — really really short documents. The haystack `DocumentStore` class
    allows you to connect to different open source and commercial vector databases
    that you can host locally on your machine, such as Faiss, PineCone, Milvus, ElasticSearch
    or even just SQLLite. For now, use the `FAISSDocumentStore` and its default indexing
    algorithm (`'Flat'`).
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The FAISSDocumentStore in haystack gives you three of these indexing approaches
    to choose from. The default `'Flat'` index will give you the most accurate results
    (highest recall rate) but will use a lot of RAM and CPU.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re really constrained on RAM or CPU, like when you’re hosting your app
    on Hugging Face, you can experiment with two other FAISS options: `''HNSW''` or
    `f’IVF{num_clusters},Flat''`. The question-answering app you’ll see at the end
    of this section used the `''HNSW''` indexing approach to fit within a hugging
    face "free tier" server. See the Haystack documentation for details on how to
    tune your vector search index.^([[48](#_footnotedef_48 "View footnote.")]) You
    will need to balance, speed, RAM, and recall for your needs. Like many NLP questions,
    there is no right answer to the question of the "best" vector database index.
    Hopefully, when you ask this question to your question-answering app, it will
    say something like "It depends…​".'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: Now go to your working directory where you ran this Python code. You should
    see a file named `'faiss_document_store.db'`. That’s because FAISS automatically
    created an SQLite database to contain the text of all your documents. Your app
    will need that file whenever you use the vector index to do semantic search. It
    will give you the actual text associated with the embedding vectors for each document.
    However, this file is not enough in order to load your data store into another
    piece of code - for that, you’ll need to you the `save` method of the `DocumentStore`
    class. We’ll do that later in the code after we fill the document store with embeddings.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: Now, it’s time to set up our indexing models! The semantic search process includes
    two main steps - retrieving documents that might be relevant to the query (semantic
    search), and processing those documents to create an answer. So you will need
    an EmbeddingRetriever semantic vector index and a generative transformer model.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: In Chapter 9 you met BERT and learn how to use it to create general-purpose
    embeddings that represent the meaning of text. Now you’ll learn how to use an
    embedding-based retriever to overcome the curse of dimensionality and find the
    embeddings for text most likely to answer a user’s question. You can probably
    guess that you’ll get better results if both your retriever and your reader are
    fine-tuned for question-answering tasks. Luckily there are a lot of BERT-based
    models that have been pretrained on question-answering datasets like SQuAD.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.17 Configuring the `reader` and `retriever` components of the question
    answering pipeline
  id: totrans-335
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Note that the Reader and the Retriever don’t have to be based on the same model
    - because they don’t perform the same job. `multi-qa-mpnet-base-dot-v1` was optimized
    for semantic search - that is, finding *the right documents* that match a specific
    query. `roberta-base-squad2` on the other hand, was trained on a set of questions
    and short answers, making it better at finding the relevant part of the context
    that answers the question.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: 'We have also finally saved our datastore for later reuse. If you go to the
    running directory of your script, you can notice that there are two new files:
    `nlpia_faiss_index.faiss` and `nlpia_faiss_index.json`. Spoilers - you’re going
    to need those soon enough!'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you are ready to put the pieces together into a question-answering pipeline
    powered by semantic search! You only need to connect your `"Query"` output to
    the `Retriever` output to the Reader input:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.18 Creating a Haystack pipeline from components
  id: totrans-340
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'You can also do it in one line with some of Haystack’s ready-made pipelines:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 10.2.9 Answering questions
  id: totrans-344
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s give our question-answering machine a try! We can start with a basic
    question and see how it performs:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Not bad! Note the "context" field that gives you the full sentence that contains
    the answer.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.10 Combining semantic search with text generation
  id: totrans-348
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So, your extractive question-answering pipeline is pretty good at finding simple
    answers that are clearly stated within the text you give it. However, it’s not
    very good at expanding and explaining the answer to more complicated questions.
    Extractive summarization and question answering struggle to generate lengthy complicated
    text for answers to "why" and "how" questions. For complicated questions requiring
    reasoning, you need to combine the best of the NLU models with the best generative
    LLMs. BERT is a bidirectional LLM built and trained specifically for understanding
    and encoding natural language into vectors for semantic search. But BERT isn’t
    all that great for generating complex sentences, for that you need a unidirectional
    (causal) model such as GPT-2\. That way your pipeline can handle complex logic
    and reasoning to answer your "why" and "how" questions.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, you don’t have to cobble together these different models on your
    own. Open source developers are way ahead of you. The BART model does.^([[49](#_footnotedef_49
    "View footnote.")]) BART has an encoder-decoder architecture like other transformers.
    Even though its encoder is bi-directional using an architecture based on BERT,
    its decoder is unidirectional (left to right for English) just like GPT-2\. It’s
    technically possible to generate sentences using the original bidirectional BERT
    model directly, if you add the <MASK> token to the end and rerun the model many
    many times. But BART takes care of that *recurrence* part of text generation for
    you with its unidirectional decoder.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: In particular, you will use a BART model that was pretrained for Long-Form Question
    Answering (LFQA). In this task, a machine is required to generate a paragraph-long
    answer based on the documents retrieved, combining the information in its context
    in a logical way. The LFQA dataset includes 250,000 pairs of questions and long-form
    answers. Let’s see how a model trained on it performs.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: We can continue using the same retriever, but this time, we’ll use one of Haystack
    pre-made pipelines, GenerativeQAPipeline. Instead of a Reader, as in a previous
    example, it includes a Generator, that generates text based on the answers the
    retriever found. So there are only a few lines of code that we need to change.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.19 Creating a Long-Form Question Answering Pipeline with Haystack
  id: totrans-353
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: And that’s it! Let’s see how our model does on a couple of questions.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Well, that was a bit vague but correct! Let’s see how our model deals with
    a question that doesn’t have an answer in the book:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Well said, for a stochastic chameleon!
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.11 Deploying your app in the cloud
  id: totrans-360
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It’s time to share your application with more people. The best way to give other
    people access, is, of course, to put it on the internet! You need to deploy your
    model on a server and create a user interface (UI) so that people can easily interact
    with it.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: There are many companies offering cloud hosting services - in this chapter,
    we’ll go with HuggingFace Spaces. As HuggingFace’s hardware is optimized to run
    its NLP models, this makes sense computationally. HuggingFace also offers several
    ways to quickly ship your app by integrating with frameworks like Streamlit and
    Gradio.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: Building your app’s UI with Streamlit
  id: totrans-363
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We’ll use Streamlit ^([[50](#_footnotedef_50 "View footnote.")]) to build your
    question-answering web App. It is an open-source framework that allows you to
    rapidly create web interfaces in Python. With Streamlit, you can turn the script
    you just run into an interactive app that anyone can access with just a few lines
    of code. And both Streamlit company itself and Hugging Face offer the possibility
    to deploy your app seamlessly to HuggingFace Spaces by offering an out-of-the-box
    Streamlit Space option.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: Let’s stick with Huggingface this time, and we’ll let you check Streamlit Share
    on your own.^([[51](#_footnotedef_51 "View footnote.")]) Go ahead and create a
    HuggingFace account if you already don’t have one. Once that’s done, you can navigate
    to Spaces and choose to create a Streamlit Space. When you’re creating your space,
    Hugging Face creates a "Hello World" Streamlit app repository that’s all yours.
    If you clone this git repository to your machine you can edit it to make it do
    whatever you like.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: Look for the `app.py` file within Hugging Face or on your local clone of the
    repository. The `app.py` file contains the Streamlit app code. Let’s replace that
    app code with the start of your question answering. For now, you just want to
    echo back the user’s question so they can feel understood. This will be especially
    important for your UX if you ever plan to do preprocessing on the question such
    as case folding, stemming, or maybe removing or adding question marks to the end.
    You may even want to experiment with adding the prefix "What is …​" if your users
    prefer to just enter noun phrases without forming a complete question.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.20 A "Hello World" question-answering application with Streamlit
  id: totrans-367
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Deep-diving into Streamlit is beside the scope of this book, but you should
    understand some basics before creating your first app. Streamlit apps are essentially
    scripts. They re-run every time as the user loads the app in their browser or
    updates the input of interactive components. As the script runs, Streamlit creates
    the components defined in the code. In the script above, there are several components:
    `title`, `markdown` (instructions below the title), as well as the `text_input`
    component that receives the user’s question.'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: Go ahead and try to run your app locally by executing line `streamlit run app.py`
    in your console. You should see something like the app in Figure [10.5](#figure-streamlit-helloworld-app).
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.5 Question answering Streamlit app
  id: totrans-371
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![qa streamlit app v1](images/qa_streamlit_app_v1.png)'
  id: totrans-372
  prefs: []
  type: TYPE_IMG
- en: Time to add some question-answering capabilities to your app! You’ll use the
    same code as before, but you’ll optimize it to run faster on Streamlit.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s load the document store you created and saved previously. To do
    that, you need to copy your `.faiss` and `.json` files into your Streamlit app’s
    directory. Then, you can use the `load` method of `FAISSDocumentStore` class.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Note that you’re wrapping our code in a function. You’re using it to leverage
    a mechanism implemented in Streamlit called *caching*. Caching is a way to save
    the results of a function so that it doesn’t have to be re-run every time the
    app is loaded or the input is changed. This is very useful both for heavy datasets
    and for models that take a long time to load. During the caching process, the
    input to the function is *hashed*, so that Streamlit can compare it to other inputs.
    And the output is saved in a `pickle` file, a common Python serialization format.
    Your document store, unfortunately, can be neither cached nor hashed (very confusing!),
    but the two models you’re using for the question-answering pipeline can be.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.21 Loading the Reader and Retriever
  id: totrans-377
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Now, insert the code building your QA pipeline between the title/subtitle and
    the question input:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Finally, you can make your app ready to answer questions! Let’s make it return
    the context of the answer too, not just the answer itself.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: And your question-answering app is ready! Let’s give it a try. As your model
    "Who invented sentiment analysis?" You should see something similar to Figure
    [10.6](#figure-streamlit-qa-app).
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.6 Working Streamlit app with a question answered
  id: totrans-384
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![qa streamlit app with question](images/qa_streamlit_app_with_question.png)'
  id: totrans-385
  prefs: []
  type: TYPE_IMG
- en: Now, deploy your app to the cloud! Congratulations on your first NLP web application.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.12 Wikipedia for the ambitious reader
  id: totrans-387
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If training your model on the text in this book seems a little constraining
    for you, consider going "all in" and training your model on Wikipedia. After all,
    Wikipedia contains all of the human knowledge, at least the knowledge that the
    *wisdom of the crowd* (humanity) thinks is important.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: Be careful though. You will need a lot of RAM, disk space, and compute throughput
    (CPU) to store, index and process the 60 million articles on Wikipedia. And you
    will need to deal with some insidious quirks that could corrupt your search results
    invisibly. And it’s hard to curate billions of words of natural language text.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: 'If you use full-text search on PyPi.org for "Wikipedia" you won’t notice that
    "It’s A Trap!"^([[52](#_footnotedef_52 "View footnote.")]) You might fall into
    the trap with `pip install wikipedia`. Don’t do that. Unfortunately, the package
    called `wikipedia` is abandonware, or perhaps even intentional name-squatting
    malware. If you use the `wikipedia` package you will likely create bad source
    text for your API (and your mind):'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: That’s fishy. No NLP preprocessor should ever corrupt your "AI" query by replacing
    it with the capitalized proper name "Xi". That name is for a person at the head
    of one of the most powerful censorship and propaganda (brainwashing) armies on
    the planet. And this is exactly the kind of insidious spell-checker attack that
    dictatorships and corporations use to manipulate you.^([[53](#_footnotedef_53
    "View footnote.")]) To do our part in combating fake news we forked the `wikipedia`
    package to create `nlpia2_wikipedia`. We fixed it so you can have a truly open
    source and honest alternative. And you can contribute your own enhancements or
    improvements to pay it forward yourself.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: You can see here how the `nlpia2_wikipedia` package on PyPi will give you straight
    answers to your queries about AI.^([[54](#_footnotedef_54 "View footnote.")])
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Now you can use Wikipedia’s full-text search API to feed your retrieval-augmented
    AI with everything that humans understand. And even if powerful people are trying
    to hide the truth from you, there are likely a lot of others in your "village"
    that have contributed to Wikipedia in your language.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Now you know how to retrieve a corpus of documents about any topic that is important
    to you. If it’s not already, AI and large language models will certainly be important
    to you in the coming years. You can teach your retrieval augmented question answering
    system from the previous section to answer questions from any knowledge you can
    find on the internet, including Wikipedia articles about AI. You no longer have
    to rely on search engine corporations to protect your privacy or provide you with
    factual answers to your questions. You can build your own retrieval-augmented
    LLMs to answer questions factually for you and those you care about at your workplace
    or in your community.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.13 Serve your "users" better
  id: totrans-400
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, we have seen the powers, but also the drawbacks, of large language
    models. And we saw that you don’t have to use the paid, private LLMs sponsored
    by the Big Tech.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: Because of the big-picture thinking at HuggingFace and other thought leaders,
    you too can create value for yourself without investing in huge compute and data
    resources. Small startups, nonprofits and even individuals are building search
    engines and conversational AI that are delivering more accurate and useful information
    than what BigTech will ever be able to deliver. Now that you’ve seen what LLMs
    do well, you will be able to use them correctly and more efficiently to create
    much more valuable tools for you and your business.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: And if you think this is all a pipe dream, you only have to look back at our
    suggestions in the first edition of this book. There we told you about the rapid
    growth in the popularity and profitability of search engine companies such as
    DuckDuckGo. As they have succumbed to pressure from investors and the lure of
    ever-increasing advertising revenue, new opportunities have opened up. Search
    engines such as You Search (You.com), Brave Search (Brave.com), Mojeek (Mojeek.com),
    Neeva (Neeva.com), and SearX (searx.org/) have continued to push search technology
    forward, improving transparency, truthfulness, and privacy for Internet search.
    The small web and the Fediverse are encroaching on BigTech’s monopoly on your
    eyeballs and access to information.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: Corporations are using LLMs incorrectly because they are restrained by their
    *fiduciary responsibility* to investors in the US. Fiduciary responsibility refers
    to someone’s legal obligation to act for the benefit of someone else, the person
    with the duty must act in a way that will benefit someone else financially. The
    *Revlon doctrine* requires judicial review when a person or corporation wants
    to purchase another corporation. The goal of this ruling is to ensure that the
    directors of the corporation being purchased did not do anything that could reduce
    the value of that company in the future.^([[55](#_footnotedef_55 "View footnote.")])
    And business managers have taken this to mean that they must always maximize the
    revenue and income of their company, at the expense of any other values or sense
    of responsibility they might feel towards their users or community. Most managers
    in the US have taken the *Revlon Doctrine* to mean "greed is good" and emphasis
    on ESG (Environmental, Social and Governance) will be punished. Federal legislation
    is currently being proposed in the US Congress that would make it illegal for
    investment firms to favor corporations with ESG programs and values.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, many smart, responsible organizations are bucking this greedy zero-sum
    thinking. You can find 100s of open-source ChatGPT-like alternatives on Hugging
    Face. H2O has even provided you with a UX within HuggingFace Spaces where you
    can compare all these chatbots to each other. We have collected a few dozens of
    open-source large language models that you can try instead of proprietary GPT
    models.footnote[List of open source Large Language Models in the NLPIA repository:([https://gitlab.com/tangibleai/nlpia2/-/blob/main/docs/open-source-llms.md](docs.html))
    ]
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: For example, Vicuna requires only 13 billion parameters to achieve twice the
    accuracy of LLaMa-2 and almost the same accuracy as ChatGPT.^([[56](#_footnotedef_56
    "View footnote.")]) ^([[57](#_footnotedef_57 "View footnote.")]) LLaMa-2-70B is
    the next most accurate model to Vicuna but it requires 70 billion parameters and
    so runs 5 times slower. And Vicuna was trained on the 90,000 conversations in
    the ShareGPT dataset on HuggingFace so you can fine-tune the foundational Vicuna
    model models to achieve similar or even better accuracy for your users.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the LLM training data sets and models for the Open Assistant are
    community-generated and publicly accessible under the Apache open-source license.
    If you want to contribute to the battle against exploitative and manipulative
    AI, the Open Assistant project is a great place to start.^([[58](#_footnotedef_58
    "View footnote.")])
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: By using open-source models, finetuning them on the data that’s relevant to
    your domain and grounding your models with real knowledge using semantic search
    and retrieval-augmented generation, you can significantly increase the accuracy,
    effectiveness and ethics of your models. In the next chapter, we will show you
    another powerful way of grounding your model - using knowledge graphs.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: Vicuna
  id: totrans-409
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Immediately after Llama 2 was released, the open source community immediately
    started improving it. One particularly enthusiastic group of contributors at Berkeley,
    CMU, and UCSD formed the LMSYS.org project where they used ShareGPT to fine-tune
    Llama 2 for the virtual assistant task.^([[59](#_footnotedef_59 "View footnote.")])
    In 2023 ShareGPT contained almost half a million of the "wildest ChatGPT conversations."
    ^([[60](#_footnotedef_60 "View footnote.")])
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: For the RLHF (human feedback part) these researchers and students at LMSYS created
    an arena where the latest AI contenders could compete, including ChatGPT, Alpaca,
    and Llama 2\. Anyone can sign up use the GUI to judge between pairs of contenders
    and help give chat bots ratings on how smart they are. When you dream up a challenging
    question and judge the chatbot answer, your rating is used to give them an Elo
    score, similar to the rating assigned to professional Chess, Go, and esports players.^([[61](#_footnotedef_61
    "View footnote.")])
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: The arena is such a respected measure of intelligence that there was even a
    Metaculus competition to predict whether an open source model will be able to
    break into leaderboard top 5 before the end of September 2023.^([[62](#_footnotedef_62
    "View footnote.")]) Vicuna-33B is currently (September 2023) ranked sixth on the
    LMSYS Leaderboard, right below GPT-3.5, which is 20 times larger and slower and
    only 2% smarter, according to the Elo score.^([[63](#_footnotedef_63 "View footnote.")])
    It’s also interesting to notice that the scores which rely on GPT-4 as the judge
    are consistently inflated for OpenAI and other commercial bots. Humans rate OpenAI’s
    chatbot performance much lower than GPT-4 does. This is called the chatbot narcicism
    problem. It’s generally a bad idea to measure the performance of an algorithm
    using a similar algorithm, especially when you are talking about machine learning
    models such as LLMs.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: If you care about your LLM-based chatbot’s performance you will want to find
    a high-quality test set created by humans. You can trust the LMSYS benchmark dataset
    to give you the most reliable and objective score of general intelligence for
    your LLMs. And you are free to download and use this dataset to rate your own
    chatbots.^([[64](#_footnotedef_64 "View footnote.")]) And if you need to add additional
    test questions for your particular use cases, you would be wise to use the LMSYS
    arena to record your questions. This way all the other open source chatbots will
    be rated based on your questions. And the next time you download an updated Elo
    rating dataset you should see your questions and how all the other models did.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: If you are not satisfied just watching all the nerdy fun happening in the "my
    brain is bigger than your brain" arena, you can contribute your own LLM to see
    how it stacks up. You can either add your model to the `fastchat` Python package
    or give LMSYS a web API so they can have judges send your LLM prompts.^([[65](#_footnotedef_65
    "View footnote.")]) Some of the more efficient LLMs, such as Vicuna-13B may require
    less than $100 of computer power to train. With all the know-how in this book,
    you have a chance to create something really interesting and new. Now that you
    have seen some relatively unreasonable answers to common sense reasoning questions,
    it is time to see what a top-ranked contender can do. LMSYS has created a script
    that will automatically download and run Vicuna on your own computer.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-416
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the real world, a vicuna is an animal that is a close relative of the llama.
    In the world of AI, Vicuna is a modified version of LLaMa-2.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: Vicuna is the offspring of that marriage between the collective intelligence
    of the open source community and the business intelligence that motivated the
    created Llama 2\. Vicuna is an updated version of LLaMa 2 that has been trained
    specifically to act as a virtual assistant. And the smallest version of Vicuna,
    the 7B version, will likely run on your computer without having to invest in any
    new hardware. Like for Llama 2, the Vicuna test below required 20 GB of RAM and
    was able to generate about one token per second on an 8-core 2.8GHz CPU.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: The first time you run this code, expect to wait ten minutes or more to download
    the 10 GB file containing the 7 billion model weights, even over a fast Internet
    connection. It took 8 minutes for us on a 5G home network. After the `fastchat`
    script downloads Vicuna it will give you a command line interface (CLI) where
    you can have an AMA with Vicuna.^([[66](#_footnotedef_66 "View footnote.")]) If
    you ever run Vicuna again, it will be ready to go in your `$HOME/.cache` directory,
    along side all your other Hugging Face Hub models.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: A full transcript of this interaction with Vicuna is available in the `nlpia2`
    package on GitLab.^([[67](#_footnotedef_67 "View footnote.")])
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: If your laptop has enough RAM to run LLaMa-2 you can also likely run Vicuna.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.14 AI ethics vs AI safety
  id: totrans-424
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, you learned a lot about the harm that AI and large language
    models are causing. And hopefully, you’ve come up with your own ideas for how
    to help mitigate those harms. Engineers who design, build and use autonomous algorithms
    are starting to pay attention to the harm caused by these algorithms and how they
    are used. How to use algorithms ethically, by minimizing harm is called *AI ethics*.
    And algorithms that minimize or mitigate much of these harms are often referred
    to as ethical AI.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: 'You may have also heard about the *AI control problem* or *AI safety* and may
    be confused about how it is different from AI ethics. AI safety is about how we
    can avoid being exterminated, intentionally or unintentionally, by our future
    "robot overlords." People working on AI safety are trying to mitigate the long-term
    existential risk posed by superintelligent generally intelligent machines. The
    CEOs of many of the largest AI companies have publicly announced their concern
    about this problem:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: Mitigating the risk of extinction from AI should be a global priority alongside
    other societal-scale risks such as pandemics and nuclear war.
  id: totrans-427
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: — Center for AI Safety
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: This single sentence is so important to AI companies' businesses that more than
    a 100 senior managers at AI companies signed this open letter. Nonetheless, many
    of these same companies are not allocating significant resources or time or public
    outreach to address this concern. Many of the largest are not even willing to
    sign this vague noncommital statement. Open AI, Microsoft, and Anthropic signed
    this letter, but Apple, Tesla, Facebook, Alphabet (Google), Amazon and many other
    AI goliaths did not.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: And there’s an ongoing public debate about the urgency and priority of *AI safety*
    vs *AI ethics*. Some thought leaders such as Yuval Harari and Yoshua Bengio are
    focused entirely on AI safety — restraining or controlling a hypothetical superintelligent
    AGI. Other, less well-known thought leaders are focusing their time and energy
    on the more immediate harm that algorithms and AI are causing now — in other words,
    AI ethics. Disadvantaged people are especially vulnerable to the unethical use
    of AI. When companies monetize their users' data they extract power and wealth
    from those who can least afford the loss. When technology is used to create and
    maintain monopolies those monopolies extinguish competition from small businesses,
    government programs, nonprofits, and individuals supporting the disadvantaged.^([[68](#_footnotedef_68
    "View footnote.")])
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: So which one of these pressing topics are you concerned with? Are there some
    overlapping things that you can work on to both reduce the harm to humans now
    and prevent our extinction in the long run? Perhaps *explainable AI* should be
    at the top of your list of ways to help create "ethical and safe AI." Explainable
    AI is the concept of an algorithm that can explain how and why it makes decisions,
    especially when those decisions are mistaken or harmful. The information extraction
    and knowledge graph concepts that you will learn in the next chapter are some
    of the foundational tools for building explainable AI. And explainable, grounded
    AI is less likely to propagate misinformation by generating factually incorrect
    statements or arguments. And if you can find algorithms that help explain how
    an ML algorithm is making its harmful predictions and decisions you can use that
    understanding to prevent that harm.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: 10.3 Test yourself
  id: totrans-432
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How is the generative model in this chapter different from the BERT model you’ve
    seen in the previous one?
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We indexed the sentences of this book as the context for a Longformer-based
    reading comprehension question-answering model. Will it get better or worse if
    you use Wikipedia sections for the context? What about an entire Wikipedia article?
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the fastest indexing algorithm for vector search and semantic search?
    (hint, this is a trick question)
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fit a Scikit-Learn `CountVectorizer` to count the bigrams within sentences extracted
    from 100 Wikipedia articles. Compute conditional probabilities for all the second
    words that follow the first word in your count vectors and use the Python `random.choice`
    function to autocomplete the next words in a sentence. How well does this work
    compared to using an LLM such as Llama 2 to autocomplete your sentences?
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What approaches or tests would you use to help quantify the intelligence of
    an LLM? What are the latest benchmarks for measuring human intelligence, and are
    they useful for evaluating an LLM or AI assistant?
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Judge your judgment: Create a ranked list of the most intelligent open source
    LLMs you can think of. Now visit the LMSYS arena ([https://chat.lmsys.org](.html))
    and be a judge for at least 5 rounds. Compare your ranked list to the official
    Elo ranking on the LMSYS leaderboard ([https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard](lmsys.html)).
    How many of your LLM rankings are out of order?'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Can you solve the mystery of "Shmargaret Shmitchell," the last author of the
    paper "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?"
    Who is she? What can you do to support her and her coauthors in their fight for
    honesty and transparency in AI research?'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 10.4 Summary
  id: totrans-440
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large language models like GPT-4 may appear intelligent, but the "magic" behind
    their answers is probabilistically choosing the next token to generate.
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning your generative models will help you generate domain-specific content,
    and experimenting with generation techniques and parameters can improve the quality
    of your output.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Approximate nearest neighbor algorithms and libraries are useful tools to find
    the information to base your answers upon.
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieval-augmented generation combines the best of semantic search and generative
    models to create grounded AI that can answer questions factually.
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs fail more than half of the natural language understanding problems that
    researchers have dreamed up so far, and scaling up LLMs isn’t helping.
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[1]](#_footnoteref_1) "AI’s Ostensible Emergent Abilities Are a Mirage" 2023
    by Katharine Miller ( [http://mng.bz/z0l6](mng.bz.html))'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: '[[2]](#_footnoteref_2) Tool for estimating ML model environmental impact (
    [https://mlco2.github.io/impact/](impact.html))'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: '[[3]](#_footnoteref_3) "Sustainable AI: Environmental Implications, Challenges
    and Opportunities" 2022 by Carole-Jean Wu et al. ( [https://arxiv.org/pdf/2111.00364.pdf](pdf.html)
    )'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: '[[4]](#_footnoteref_4) "Behind the Millions: Estimating the Scale of Large
    Language Models" by Dmytro Nikolaiev ( [http://mng.bz/G94A](mng.bz.html))'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: '[[5]](#_footnoteref_5) Wikipedia article on GPT-2 ( [https://en.wikipedia.org/wiki/GPT-2](wiki.html))'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: '[[6]](#_footnoteref_6) "Red teaming ChatGPT via Jailbreaking: Bias, Robustness,
    Reliability and Toxicity" 2023 by Terry Yue Zhuo et al ( [https://arxiv.org/abs/2301.12867](abs.html))'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: '[[7]](#_footnoteref_7) Clever Hans Wikipedia article ( [https://en.wikipedia.org/wiki/Clever_Hans](wiki.html))'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: '[[8]](#_footnoteref_8) Wikipedia article on the harmful effects of social media
    like buttons ( [https://en.wikipedia.org/wiki/Facebook_like_button#Criticism](wiki.html))'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: '[[9]](#_footnoteref_9) Techdirt article explaining how ChatGPT amplifies misinformation
    ( [https://www.techdirt.com/2023/07/19/g-o-media-execs-full-speed-ahead-on-injecting-half-cooked-ai-into-a-very-broken-us-media/](g-o-media-execs-full-speed-ahead-on-injecting-half-cooked-ai-into-a-very-broken-us-media.html))'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: '[[10]](#_footnoteref_10) Google fired Timnit Gebru when she asked to publish
    "On the Dangers of Stochastic Parrots…​" with her coauthors Emily M. Bender, Angelina
    McMillan-Major, and Shmargaret Shmitchell (a pseudonym? Timnit had an ally with
    the last name Mitchel) ( [https://dl.acm.org/doi/pdf/10.1145/3442188.3445922?uuid=f2qngt2LcFCbgtaZ2024](10.1145.html))'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: '[[11]](#_footnoteref_11) "GPT-4 Technical Report" ( [https://arxiv.org/pdf/2303.08774.pdf](pdf.html))'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: '[[12]](#_footnoteref_12) Table of nonemergent capabilities was extracted from
    Appendix E in "Emergent Abilities of Large Language Models" by Jason Wei et al
    ( [https://arxiv.org/abs/2206.07682](abs.html))'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: '[[13]](#_footnoteref_13) MathActive package on GitLab ( [https://gitlab.com/tangibleai/community/mathactive](community.html)).'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: '[[14]](#_footnoteref_14) Twitter is now called X, and the rating and recommendation
    system has become even more toxic and opaque under the new management.'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: '[[15]](#_footnoteref_15) SpaCy rule-based matching documentation ( [https://spacy.io/usage/rule-based-matching](usage.html))'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: '[[16]](#_footnoteref_16) ReLM on GitHub ( [https://github.com/mkuchnik/relm](mkuchnik.html))'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: '[[17]](#_footnoteref_17) lm-evaluation-harness project on GitHub ( [https://github.com/EleutherAI/lm-evaluation-harness](EleutherAI.html))'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: '[[18]](#_footnoteref_18) Regex package on PyPi ( [https://pypi.org/project/regex/](regex.html))'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: '[[19]](#_footnoteref_19) Guardrails-ai project on GitHub ( [https://github.com/ShreyaR/guardrails](ShreyaR.html))'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: '[[20]](#_footnoteref_20) GitHub source code for `guardrails-ai` ( [https://github.com/ShreyaR/guardrails](ShreyaR.html))'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: '[[21]](#_footnoteref_21) "Universal and Transferable Adversarial Attacks on
    Aligned Language Models" by Andy Zou et al ( [https://llm-attacks.org/](llm-attacks.org.html))'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
- en: '[[22]](#_footnoteref_22) (Ars Technica news article ( [https://arstechnica.com/information-technology/2023/02/ai-powered-bing-chat-spills-its-secrets-via-prompt-injection-attack/](ai-powered-bing-chat-spills-its-secrets-via-prompt-injection-attack.html))'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: '[[23]](#_footnoteref_23) Scaling Laws for Neural Language Models by Jared Kaplan
    from Antrhopic.AI et al. ( [https://arxiv.org/abs/2001.08361](abs.html))'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: '[[24]](#_footnoteref_24) Huggingface documentation on Model Outputs: ( [https://huggingface.co/docs/transformers/main_classes/output](main_classes.html))'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
- en: '[[25]](#_footnoteref_25) *"Summary of the tokenizers"* on Huggingface: ( [https://huggingface.co/docs/transformers/tokenizer_summary](transformers.html))'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: '[[26]](#_footnoteref_26) How to generate text: using different decoding methods
    for language generation with Transformers ( [https://huggingface.co/blog/how-to-generate](blog.html))'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: '[[27]](#_footnoteref_27) Wikipedia article on Search Engines: ( [https://en.wikipedia.org/wiki/Search_engine](wiki.html))'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: '[[28]](#_footnoteref_28) ( [https://www.elastic.co/elasticsearch/](elasticsearch.html))'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: '[[29]](#_footnoteref_29) [https://solr.apache.org/](solr.apache.org.html)'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: '[[30]](#_footnoteref_30) Meilisearch Github Repository: ( [https://github.com/meilisearch/meilisearch](meilisearch.html))'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: '[[31]](#_footnoteref_31) How GitLab uses PostgreSQL trigram indexes in software
    which scales to millions of users ( [https://about.gitlab.com/blog/2016/03/18/fast-search-using-postgresql-trigram-indexes/](fast-search-using-postgresql-trigram-indexes.html))'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
- en: '[[32]](#_footnoteref_32) A great resource on using FAISS library: ( [https://www.pinecone.io/learn/series/faiss/](faiss.html))'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: '[[33]](#_footnoteref_33) ( [https://github.com/dataplayer12/Fly-LSH](dataplayer12.html))'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
- en: '[[34]](#_footnoteref_34) Efficient and robust approximate nearest neighbor
    search using Hierarchical Navigable Small World graphs, ( [https://arxiv.org/ftp/arxiv/papers/1603/1603.09320.pdf](1603.html))'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: '[[35]](#_footnoteref_35) ( [https://en.wikipedia.org/wiki/Six_degrees_of_separation](wiki.html))'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: '[[36]](#_footnoteref_36) Locally Optimal Product Quantization on PyPi ( [https://pypi.org/project/lopq/](lopq.html))'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: '[[37]](#_footnoteref_37) Billion-scale similarity search with GPUs by Jeff
    Johnson, Matthijs Douze, Herve'' Jegou ( [https://arxiv.org/pdf/1702.08734.pdf](pdf.html))'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: '[[38]](#_footnoteref_38) [https://weaviate.io/blog/ann-algorithms-hnsw-pq](blog.html)'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: '[[39]](#_footnoteref_39) [https://github.com/spotify/annoy](spotify.html)'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: '[[40]](#_footnoteref_40) Faiss Github repository: ( [https://github.com/facebookresearch/faiss](facebookresearch.html))'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: '[[41]](#_footnoteref_41) NMSlib Github repository ( [https://github.com/nmslib/nmslib](nmslib.html))'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: '[[42]](#_footnoteref_42) ANN Benchmarks repository on GitHub ( [https://github.com/erikbern/ann-benchmarks/](ann-benchmarks.html))'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
- en: '[[43]](#_footnoteref_43) OpenSearch k-NN Documentation ( [https://opensearch.org/docs/latest/search-plugins/knn](search-plugins.html))'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
- en: '[[44]](#_footnoteref_44) ( [https://github.com/jina-ai/jina](jina-ai.html))'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: '[[45]](#_footnoteref_45) [https://github.com/deepset-ai/haystack](deepset-ai.html)'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: '[[46]](#_footnoteref_46) ( [https://github.com/neuml/txtai](neuml.html))'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
- en: '[[47]](#_footnoteref_47) Haystack installation instructions for Windows ( [https://docs.haystack.deepset.ai/docs/installation](docs.html))'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: '[[48]](#_footnoteref_48) Haystack documentation on the `faiss_index_factor_str`
    option ( [https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index](wiki.html))'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: '[[49]](#_footnoteref_49) BART: Denoising Sequence-to-Sequence Pre-training
    for Natural Language Generation, Translation, and Comprehension by Mike Lewis
    et al 2019 ( [https://arxiv.org/abs/1910.13461](abs.html))'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
- en: '[[50]](#_footnoteref_50) ( [https://docs.streamlit.io/](docs.streamlit.io.html))'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
- en: '[[51]](#_footnoteref_51) ( [https://share.streamlit.io/](share.streamlit.io.html))'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
- en: '[[52]](#_footnoteref_52) Know Your Meme article for "It’s A Trap" ( [https://knowyourmeme.com/memes/its-a-trap](memes.html))'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
- en: '[[53]](#_footnoteref_53) ( [https://theintercept.com/2018/08/01/google-china-search-engine-censorship/](google-china-search-engine-censorship.html))'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
- en: '[[54]](#_footnoteref_54) "It Takes a Village to Combat a Fake News Army" by
    Zachary J. McDowell & Matthew A Vetter ( [https://journals.sagepub.com/doi/pdf/10.1177/2056305120937309](10.1177.html))'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
- en: '[[55]](#_footnoteref_55) Explanation of fiduciary duty at Harvard Law School
    by Martin Lipton et al. 2019 ( [https://corpgov.law.harvard.edu/2019/08/24/stakeholder-governance-and-the-fiduciary-duties-of-directors/](stakeholder-governance-and-the-fiduciary-duties-of-directors.html))'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
- en: '[[56]](#_footnoteref_56) Vicuna home page ( [https://vicuna.lmsys.org/](vicuna.lmsys.org.html))'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
- en: '[[57]](#_footnoteref_57) Vicuna LLM on Hugging Face ( [https://huggingface.co/lmsys/vicuna-13b-delta-v1.1](lmsys.html))'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
- en: '[[58]](#_footnoteref_58) GitHub page for Open Assistant ( [https://github.com/LAION-AI/Open-Assistant/](Open-Assistant.html))'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
- en: '[[59]](#_footnoteref_59) LMSYS ORG website (lmsys.org)'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
- en: '[[60]](#_footnoteref_60) ShareGPT website ( [https://sharegpt.com](.html))'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: '[[61]](#_footnoteref_61) Wikipedia article explaining the Elo algorithm ( [https://en.wikipedia.org/wiki/Elo_rating_system](wiki.html))'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
- en: '[[62]](#_footnoteref_62) Metaculus open source LLM ranking question for September
    2023 ( [https://www.metaculus.com/questions/18525/non-proprietary-llm-in-top-5/](non-proprietary-llm-in-top-5.html))'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
- en: '[[63]](#_footnoteref_63) [https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard](lmsys.html)'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
- en: '[[64]](#_footnoteref_64) Huggingface dataset page ( [https://huggingface.co/datasets/lmsys/chatbot_arena_conversations](lmsys.html))'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
- en: '[[65]](#_footnoteref_65) Instructions for adding a new model to the LMSYS Leaderboard
    ( [https://github.com/lm-sys/FastChat/blob/main/docs/arena.md#how-to-add-a-new-model](docs.html))'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
- en: '[[66]](#_footnoteref_66) Ask Me Anything (AMA) is when someone, usually a human,
    offers to answer public questions on a social media platform.'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
- en: '[[67]](#_footnoteref_67) Vicuna test results in nlpia2 package on GitLab (
    [https://gitlab.com/tangibleai/nlpia2/-/blob/main/src/nlpia2/data/llm/fastchat-vicuna-7B-terminal-session-input-output.yaml?ref_type=heads](llm.html))'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
- en: '[[68]](#_footnoteref_68) from *Chokepoint Capitalism* by Cory Efram Doctorow'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
