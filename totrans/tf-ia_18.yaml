- en: '15 TFX: MLOps and deploying models with TensorFlow'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 15 TFX：MLOps和使用TensorFlow部署模型
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖内容
- en: Writing an end-to-end data pipeline using TFX (TensorFlow-Extended)
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TFX（TensorFlow-Extended）编写端到端数据流水线
- en: Training a simple neural network through the TFX Trainer API
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过TFX Trainer API训练一个简单的神经网络
- en: Using Docker to containerize model serving (inference) and present it as a service
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Docker将模型服务（推理）容器化，并将其作为服务呈现
- en: Deploying the model on your local machine so it can be used through an API
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本地机器上部署模型，以便通过API使用
- en: 'In chapter 14, we looked at a very versatile tool that comes with TensorFlow:
    the TensorBoard. TensorBoard is a visualization tool that helps you understand
    data and models better. Among other things, it facilitates'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在第14章，我们研究了一个非常多功能的工具，它与TensorFlow捆绑在一起：TensorBoard。TensorBoard是一个可视化工具，可以帮助你更好地理解数据和模型。除其他外，它可以方便
- en: Monitoring and tracking model performance
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控和追踪模型性能
- en: Visualizing data inputs to models (e.g., images, audio)
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化模型的数据输入（例如图片、音频）
- en: Profiling models to understand their performance or memory bottlenecks
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对模型进行分析以了解其性能或内存瓶颈
- en: We learned how we can use the TensorBoard to visualize high-dimensional data
    like images and word vectors. We looked at how we can incorporate Keras callbacks
    to send information to the TensorBoard to visualize model performance (accuracy
    and loss) and custom metrics. We then analyzed the execution of the model using
    the CUDA profiling tool kit to understand execution patterns and memory bottlenecks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学习了如何使用TensorBoard来可视化像图片和词向量这样的高维数据。我们探讨了如何将Keras回调嵌入到TensorBoard中，以便可视化模型性能（准确率和损失）以及自定义指标。然后，我们使用CUDA性能分析工具来分析模型的执行，以理解执行模式和内存瓶颈。
- en: 'In this chapter, we will explore a new domain of machine learning that has
    gained an enormous amount of attention in the recent past: MLOps. MLOps is derived
    from the terms ML and DevOps (derived from development and operations). According
    to Amazon Web Services, “DevOps is the combination of cultural philosophies, practices,
    and tools that increases an organization’s ability to deliver applications and
    services at high velocity: evolving and improving products at a faster pace than
    organizations using traditional software development and infrastructure management
    processes.” There is another term that goes hand in hand with MLOps, which is
    productionization of models. It is somewhat difficult to discriminate between
    the two terms as they overlap and occasionally are used interchangeably, but I
    like to think of these two things as follows: MLOps defines a workflow that will
    automate most of the steps, from collecting data to delivering a model trained
    on that data, with very little human intervention. Productionization is deploying
    a trained model (on a private server or cloud), enabling customers to use the
    model for its designed purpose in a robust fashion. It can include tasks such
    as designing scalable APIs that can scale to serve thousands of requests per second.
    In other words, MLOps is the journey that gets you to the destination, which is
    the productionization of a model.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探索最近引起极大关注的机器学习新领域：MLOps。MLOps源自ML和DevOps（源自开发和运维）术语。根据亚马逊网络服务（AWS）的说法，“DevOps是文化哲学、实践和工具的组合，它增加了组织交付应用和服务的能力：以比使用传统软件开发和基础设施管理流程的组织更快的速度进化和改进产品。”还有一个与MLOps密切相关的术语，即模型的实际投入使用。很难区分这两个术语，因为它们有重叠之处，有时可以互换使用，但我倾向于这样理解这两个事物：MLOps定义了一个工作流，将自动化大部分步骤，从收集数据到交付在该数据上训练的模型，几乎不需要人工干预。实际投入使用是部署训练好的模型（在私有服务器或云上），使客户能够以稳健的方式使用模型进行设计目的。它可以包括任务，例如设计可扩展的API，可以扩展以处理每秒数千个请求。换句话说，MLOps是一段旅程，让你到达的目的地是模型的实际投入使用。
- en: Let’s discuss why it is important to have a (mostly) automated pipeline to develop
    machine learning models. To realize the value of it, you have to think in scale.
    For companies like Google, Facebook, and Amazon, machine learning is deeply rooted
    in the products they offer. This means hundreds if not thousands of models produce
    predictions every second. Moreover, with a few billion users, they can’t afford
    their models to go stale, which means continuously training/fine-tuning the existing
    models as new data is collected. MLOps can take care of this problem. MLOps can
    be used to ingest the collected data, train models, automatically evaluate models,
    and push them to the production environment if they pass a predefined validation
    check. A validation check is important to ensure models meet expected performance
    standards and to safeguard against rogue underperforming models (e.g., a rogue
    model can be generated due to large changes in new incoming training data, a new
    untested hyperparameter change that is pushed, etc.). Finally, the model is pushed
    to a production environment, which is accessed through a Web API to retrieve predictions
    for an input. Specifically, the API will provide certain endpoints (in the form
    of URLs) to the user that the user can visit (optionally with parameters needed
    to complete the request). Having said that, even for a smaller company that is
    relying on machine learning models, MLOps can greatly standardize and speed up
    the workflows of data scientists and machine learning engineers. This will greatly
    reduce the time data scientists and machine learning engineers spend creating
    such workflows from the ground up every time they work on a new project. Read
    more about MLOps at [http://mng.bz/Pnd9](http://mng.bz/Pnd9).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论为什么拥有（大部分）自动化的流水线来开发机器学习模型是重要的。要实现其价值，你必须考虑到规模问题。对于像谷歌、Facebook和亚马逊这样的公司，机器学习已经深深扎根于他们提供的产品中。这意味着数以百计甚至数千个模型每秒产生预测。此外，对于拥有数十亿用户的公司来说，他们不能容忍他们的模型变得过时，这意味着不断地训练/微调现有模型以适应新数据的收集。MLOps可以解决这个问题。MLOps可用于摄取收集的数据、训练模型、自动评估模型，并在它们通过预定义的验证检查后将其推送到生产环境中。验证检查是为了确保模型达到预期的性能标准，并防范对抗不良表现的模型（例如，由于新的入站训练数据发生大幅变化、推送了新的未经测试的超参数变更等，可能会生成不良的模型）。最后，模型被推送到生产环境，通过
    Web API 访问以获取输入的预测。具体而言，API 将为用户提供一些端点（以 URL 的形式），用户可以访问这些端点（可选地带上需要完成请求的参数）。话虽如此，即使对于依赖机器学习模型的较小公司来说，MLOps也可以极大地标准化和加速数据科学家和机器学习工程师的工作流程。这将大大减少数据科学家和机器学习工程师在每次开展新项目时从头开始创建这些工作流程所花费的时间。阅读有关
    MLOps 的更多信息，请访问[http://mng.bz/Pnd9](http://mng.bz/Pnd9)。
- en: How can we do MLOps in TensorFlow? Look no further than TFX (TensorFlow Extended).
    TFX is a library that gives you all the bells and whistles needed to implement
    a machine learning pipeline that will ingest data, transform data into features,
    train a model, and push the model to a designated production environment. This
    is done by defining a series of components that perform very specific tasks. In
    the coming sections, we will look at how to use TFX to achieve this.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何在 TensorFlow 中进行 MLOps？无需寻找其他，TFX（TensorFlow 扩展）就是答案。TFX 是一个库，提供了实现摄取数据、将数据转换为特征、训练模型和将模型推送到指定生产环境所需的所有功能。这是通过定义一系列执行非常具体任务的组件来完成的。在接下来的几节中，我们将看看如何使用
    TFX 来实现这一目标。
- en: 15.1 Writing a data pipeline with TFX
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 TFX 编写数据管道
- en: Imagine you are developing a system to predict the severity of a forest fire
    given the weather conditions. You have been given a data set from past observed
    forest fires and asked to make a model. To make sure you can provide the model
    as a service, you decide to create a workflow to ingest data and train a model
    using TFX. The first step in this is to create a data pipeline that can read the
    data (in CSV format) and convert it to features. As part of this pipeline, you
    will have a data reader (that generates examples from CSV), show summary statistics
    of the fields, learn the schema of the data, and convert it to a proper format
    the model understands.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你正在开发一个系统，根据天气条件来预测森林火灾的严重程度。你已经获得了过去观察到的森林火灾的数据集，并被要求创建一个模型。为了确保你能够将模型提供为服务，你决定创建一个工作流程来摄取数据并使用
    TFX 训练模型。这个过程的第一步是创建一个能够读取数据（以 CSV 格式）并将其转换为特征的数据管道。作为这个管道的一部分，你将拥有一个数据读取器（从 CSV
    生成示例），显示字段的摘要统计信息，了解数据的模式，并将其转换为模型理解的正确格式。
- en: Important information about the environment
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 关于环境的重要信息
- en: 'To run the code for this chapter, we highly recommend using a Linux environment
    (e.g., Ubuntu), and the instructions will be provided for that environment. TFX
    is not tested against a Windows environment ([http://mng.bz/J2Y0](http://mng.bz/J2Y0)).
    Another important thing to note is that we will be using a slightly older version
    of TFX (1.6.0). At the time of writing, the latest version is 1.9.0\. This is
    because a crucial component necessary to run TFX in interactive environments such
    as notebooks is broken in versions after 1.6.0\. Additionally, later on in the
    chapter we will use a technology called Docker. It can be quite difficult to get
    Docker to behave in the way we need on Windows due to the highly restricted access
    to resources. Additionally, for this chapter, we will define a new Anaconda environment.
    To do that follow the following instructions:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行本章的代码，强烈建议使用Linux环境（例如Ubuntu），并且将提供该环境的说明。TFX未针对Windows环境进行测试([http://mng.bz/J2Y0](http://mng.bz/J2Y0))。另一个重要的事项是我们将使用稍旧版本的TFX（1.6.0）。撰写时，最新版本为1.9.0。这是因为在1.6.0版本之后的版本中，运行TFX在诸如笔记本等交互式环境中所需的关键组件已损坏。此外，本章后面我们将使用一种名为Docker的技术。由于对资源的访问受到严格限制，使Docker按我们所需的方式运行在Windows上可能会相当困难。此外，对于本章，我们将定义一个新的Anaconda环境。要执行此操作，请按照以下说明操作：
- en: Open a terminal window and move cd into the Ch15-TFX-for-MLOps-in-TF2 directory
    in the code repository.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 打开一个终端窗口，并进入代码存储库中的Ch15-TFX-for-MLOps-in-TF2目录。
- en: If you have an already activated Anaconda virtual environment (e.g., manning.tf2),
    deactivate it by running conda deactivate manning.tf2.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您已经激活了Anaconda虚拟环境（例如manning.tf2），请通过运行conda deactivate manning.tf2来停用它。
- en: Run conda create -n manning.tf2.tfx python=3.6 to create a new virtual Anaconda
    environment.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行conda create -n manning.tf2.tfx python=3.6来创建一个新的虚拟Anaconda环境。
- en: Run conda activate manning.tf2.tfx to activate the new environment.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行conda activate manning.tf2.tfx以激活新环境。
- en: Run pip install --use-deprecated=legacy-resolver -r requirements.txt.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行pip install --use-deprecated=legacy-resolver -r requirements.txt。
- en: Run jupyter notebook.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行jupyter notebook。
- en: Open the tfx/15.1_MLOps_with_tensorflow.ipynb notebook.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 打开tfx/15.1_MLOps_with_tensorflow.ipynb笔记本。
- en: 'The first thing to do is download the data sets (listing 15.1). We will use
    a data set that has recorded historical forest fires in the Montesinho park in
    Portugal. The data set is freely available at [http://archive.ics.uci.edu/ml/datasets/Forest+Fires](http://archive.ics.uci.edu/ml/datasets/Forest+Fires).
    It is is a CSV file with the following features:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 第一件事是下载数据集（列表15.1）。我们将使用一个记录了葡萄牙蒙特西尼奥公园历史森林火灾的数据集。该数据集在[http://archive.ics.uci.edu/ml/datasets/Forest+Fires](http://archive.ics.uci.edu/ml/datasets/Forest+Fires)上免费提供。它是一个CSV文件，具有以下特征：
- en: '*X*—x-axis spatial coordinate within the Montesinho park map'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*X*—蒙特西尼奥公园地图中的x轴空间坐标'
- en: '*Y*—y-axis spatial coordinate within the Montesinho park map'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Y*—蒙特西尼奥公园地图中的y轴空间坐标'
- en: '*month*—Month of the year'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*month*—一年中的月份'
- en: '*day*—Day of the week'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*day*—一周中的日期'
- en: '*Fine Fuel Moisture Code* (FFMC)—Represents fuel moisture of forest litter
    fuels under the shade of a forest canopy'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Fine Fuel Moisture Code* (FFMC)—代表森林树冠阴影下的林地燃料湿度'
- en: '*DMC*—A numerical rating of the average moisture content of soil'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*DMC*—土壤平均含水量的数字评级'
- en: '*Drought Code* (DC)—Represents the depth of dryness in the soil'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Drought Code* (DC)—表示土壤干燥程度的深度'
- en: '*Initial Spread Index* (ISI)—An expected rate of fire spread'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Initial Spread Index* (ISI)—预期的火灾蔓延速率'
- en: '*temp*—Temperature in Celsius degrees'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*temp*—摄氏度温度'
- en: '*RH*—Relative humidity in %'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*RH*—相对湿度，单位%'
- en: '*wind*—Wind speed in km/h'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*wind*—风速，单位km/h'
- en: '*rain*—Outside rain in mm/m2'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*rain*—外部降雨量，单位mm/m2'
- en: '*area*—The burnt area of the forest (in hectares)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*area*—森林烧毁面积（单位公顷）'
- en: Selecting features for a machine learning model
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 选择机器学习模型的特征
- en: Feature selection for a machine learning model is not a trivial task. Often
    you have to understand features, inter-feature correlation, feature-target correlation,
    and so on before making a good judgment call on whether a feature should be used.
    Therefore, one should not use all the given features of a model blindly. In this
    case, however, as the focus is more on MLOps and less on data-science decisions,
    we will use all features. Using all of these features will later lend itself to
    explaining various options that are available when defining an MLOps pipeline.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 选择机器学习模型的特征不是一个微不足道的任务。通常，在使用特征之前，您必须了解特征，特征间的相关性，特征-目标相关性等等，然后就可以判断是否应使用特征。因此，不应该盲目地使用模型的所有给定特征。然而，在这种情况下，重点在于MLOps，而不是数据科学决策，我们将使用所有特征。使用所有这些特征将稍后有助于解释在定义MLOps管道时可用的各种选项。
- en: Our task will be to predict the burnt area, given all the other features. Note
    that predicting a continuous value such as the area warrants a regression model.
    Therefore, this is a regression problem, not a classification problem.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的任务是在给出所有其他特征的情况下预测烧毁面积。请注意，预测连续值（如面积）需要回归模型。因此，这是一个回归问题，而不是分类问题。
- en: Listing 15.1 Downloading the data set
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.1 下载数据集
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ If the data file is not downloaded, download the file.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 如果未下载数据文件，请下载该文件。
- en: ❷ This line downloads a file given by a URL.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 此行下载给定URL的文件。
- en: ❸ Create the necessary folders and write the downloaded data into it.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 创建必要的文件夹并将下载的数据写入其中。
- en: ❹ If the file containing the data set description is not downloaded, download
    it.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 如果未下载包含数据集描述的文件，请下载它。
- en: ❺ Create the necessary directories and write the data into them.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 创建必要的目录并将数据写入其中。
- en: 'Here, we download two files: forestfires.csv and forestfires.names. forestfires.csv
    contains the data in a comma-separated format, where the first line is the header
    followed by data in the rest of the file. forestfires.names contains more information
    about the data, in case you want to understand more about it. Next, we will separate
    a small test data set to do manual testing on later. Having a dedicated test set
    that is not seen by the model at any stage will tell us how well the model has
    generalized. This will be 5% of the original data set. The other 95% will be left
    for training and validation data:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们需要下载两个文件：forestfires.csv和forestfires.names。forestfires.csv以逗号分隔的格式包含数据，第一行是标题，其余部分是数据。forestfires.names包含更多关于数据的信息，以便您想更多地了解它。接下来，我们将分离出一个小的测试数据集以供后续手动测试。拥有一个专用的测试集，在任何阶段都没有被模型看到，将告诉我们模型的泛化情况如何。这将是原始数据集的5％。其余95％将用于训练和验证数据：
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We will now start with the TFX pipeline. The first step is to define a root
    directory for storing pipeline artifacts. What are pipeline artifacts, you might
    ask? When running the TFX pipeline, it stores interim results of various stages
    in a directory (under a certain subdirectory structure). One example of this is
    that when you read the data from the CSV file, the TFX pipeline will split the
    data into train and validation subsets, convert those examples to TFRecord objects
    (i.e., an object type used by TensorFlow internally for data), and store the data
    as compressed files:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将开始TFX管道。第一步是定义存储管道工件的根目录。您可能会问什么是管道工件？在运行TFX管道时，它会在目录中存储各个阶段的中间结果（在某个子目录结构下）。其中的一个例子是，当您从CSV文件中读取数据时，TFX管道会将数据拆分为训练和验证子集，将这些示例转换为TFRecord对象（即TensorFlow内部用于数据的对象类型），并将数据存储为压缩文件：
- en: '[PRE2]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'TFX uses Abseil for logging purposes. Abseil is an open-source collection of
    C++ libraries drawn from Google’s internal codebase. It provides facilities for
    logging, command-line argument parsing, and so forth. If you are interested, read
    more about the library at [https://abseil.io/docs/python/](https://abseil.io/docs/python/).
    We will set the logging level to INFO so that we will see logging statements at
    the INFO level or higher. Logging is an important functionality to have, as we
    can glean lots of insights, including what steps ran successfully and what errors
    were thrown:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: TFX使用Abseil进行日志记录。Abseil是从Google的内部代码库中提取的开源C ++库集合。它提供了日志记录，命令行参数解析等功能。如果您感兴趣，请在[https://abseil.io/docs/python/](https://abseil.io/docs/python/)阅读有关该库的更多信息。我们将设置日志记录级别为INFO，以便我们可以在INFO级别或更高级别看到日志记录语句。日志记录是具有重要功能的，因为我们可以获得很多见解，包括哪些步骤成功运行以及哪些错误被抛出：
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'After the initial housekeeping, we will define an InteractiveContext:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 完成初始设置后，我们将定义一个InteractiveContext：
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: TFX runs pipelines in a context. The context is used to run various steps you
    define in the pipeline. It also serves a very important purpose, which is to manage
    states between different steps as we are progressing through the pipeline. In
    order to manage transitions between states and make sure the pipeline operates
    as expected, it also maintains a metadata store (a small-scale database). The
    metadata store contains various information, such as an execution order, the final
    state of the components, and resulting errors. You can read about metadata in
    the following sidebar.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: TFX在一个上下文中运行流水线。上下文被用来运行你在流水线中定义的各个步骤。它还起着非常重要的作用，就是在我们在流水线中进行过程中管理不同步骤之间的状态。为了管理状态之间的转换并确保流水线按预期运行，它还维护了一个元数据存储（一个小规模的数据库）。元数据存储包含各种信息，如执行顺序、组件的最终状态和产生的错误。你可以在以下侧边栏中了解有关元数据的信息。
- en: What’s in the metadata?
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据中有什么？
- en: As soon as you create your InteractiveContext, you will see a database called
    metadata.sqlite in the pipeline root. This is an SQLite database ([https://www
    .sqlite.org/index.xhtml](https://www.sqlite.org/index.xhtml)), a lightweight,
    fast SQL database designed for small amounts of data and incoming requests. This
    database will log important information about inputs, outputs, and execution-related
    outputs (the component’s run identifier, errors). This information can be used
    to debug your TFX pipeline. Metadata can be thought of as data that is not a direct
    input or an output but is still necessary to execute components correctly with
    greater transparency. Metadata can be immensely helpful for debugging complex
    TFX pipelines with many components interconnected in many different ways. You
    can read more about this at [https://www.tensorflow.org/tfx/guide /mlmd](https://www.tensorflow.org/tfx/guide/mlmd).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建了InteractiveContext，你会在流水线根目录中看到一个名为metadata.sqlite的数据库。这是一个轻量级、快速的SQL数据库（[https://www.sqlite.org/index.xhtml](https://www.sqlite.org/index.xhtml)），专为处理少量数据和传入请求而设计的。该数据库将记录有关输入、输出和执行相关输出的重要信息（组件的运行标识符、错误）。这些信息可以用于调试你的TFX流水线。元数据可以被视为不是直接输入或输出，但仍然是正确执行组件所必需的数据，以提供更大的透明度。在具有许多组件以许多不同方式相互连接的复杂TFX流水线的调试中，元数据可能非常有帮助。你可以在[https://www.tensorflow.org/tfx/guide/mlmd](https://www.tensorflow.org/tfx/guide/mlmd)上了解更多信息。
- en: We’re off to defining the pipeline. The primary purpose of the pipeline in this
    section is to
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要开始定义流水线了。本节流水线的主要目的是
- en: Load the data from a CSV file and split to training and validation data
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从CSV文件加载数据并拆分为训练和验证数据
- en: Learn the schema of the data (e.g., various columns, data types, min/max bounds,
    etc.)
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解数据的模式（例如各个列、数据类型、最小/最大值等）
- en: Display summary statistics and graphs about the distribution of various features
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显示关于各种特征分布的摘要统计和图形
- en: Transform the raw columns to features, which may require special intermediate
    processing steps
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将原始列转换为特征，可能需要特殊的中间处理步骤
- en: These steps are a lead-up to model training and deployment. Each of these tasks
    will be a single component in the pipeline, and we will discuss these in more
    detail when the time comes.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤是模型训练和部署的前奏。每个任务都将是流水线中的一个单独组件，在合适的时候我们将详细讨论这些步骤。
- en: 15.1.1 Loading data from CSV files
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.1.1 从CSV文件加载数据
- en: 'The first step is to define a component to read examples from the CSV file
    and split the data to train and eval. For that, you can use the tfx.components.CsvExampleGen
    object. All we need to do is provide the directory containing the data to the
    input_base argument:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是定义一个组件来从CSV文件中读取示例并将数据拆分为训练和评估数据。为此，你可以使用tfx.components.CsvExampleGen对象。我们所需要做的就是将包含数据的目录提供给input_base参数：
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then we use the previously defined InteractiveContext to run the example generator:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用之前定义的InteractiveContext来运行示例生成器：
- en: '[PRE6]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Let’s look at what this step produces. To see the data, let’s go to the _pipeline_root
    directory (e.g., Ch15-TFX-for-MLOps-in-TF2/tfx/pipeline). It should have a directory/
    file structure similar to what’s shown in figure 15.1.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看这一步骤产生了什么。要查看数据，请前往_pipeline_root目录（例如，Ch15-TFX-for-MLOps-in-TF2/tfx/pipeline）。它应该具有类似于图15.1所示的目录/文件结构。
- en: '![15-01](../../OEBPS/Images/15-01.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![15-01](../../OEBPS/Images/15-01.png)'
- en: Figure 15.1 The directory/file structure after running the CsvExampleGen
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.1 运行CsvExampleGen之后的目录/文件结构
- en: 'You will see two GZip files (i.e., with a .gz extension) created within the
    pipeline. You will notice that there are two sub-directories in the CsvExampleGen
    folder: Split-train and Split-eval, which contain training and validation data,
    respectively. When you run the notebook cell containing the previous code, you
    will also see an output HTML table displaying the inputs and outputs of the TFX
    component (figure 15.2).'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到在管道中创建了两个 GZip 文件（即带有 .gz 扩展名）。您会注意到在 CsvExampleGen 文件夹中有两个子目录：Split-train
    和 Split-eval，分别包含训练和验证数据。当您运行包含前述代码的笔记本单元时，您还将看到一个输出 HTML 表格，显示 TFX 组件的输入和输出（图
    15.2）。
- en: '![15-02](../../OEBPS/Images/15-02.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![15-02](../../OEBPS/Images/15-02.png)'
- en: Figure 15.2 Output HTML table generated by running the CsvExampleGen component
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.2 运行 CsvExampleGen 组件生成的输出 HTML 表格
- en: There are a few things worth noting. To start, you will see the execution_id,
    which is the value produced by a counter that keeps track of the number of times
    you run TFX components. In other words, every time you run a TFX component (like
    CsvExampleGen), the counter goes up by 1\. If you go down further, you can see
    some important information about how the CsvExampleGen has split your data. If
    you look under component > CsvExampleGen > exec_properties > output_config, you
    will see something like
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些值得注意的事项。首先，您将看到 execution_id，这是一个计数器生成的值，该计数器跟踪您运行 TFX 组件的次数。换句话说，每次运行 TFX
    组件（如 CsvExampleGen）时，计数器都会增加 1。如果您继续向下看，您会看到一些关于 CsvExampleGen 如何分割数据的重要信息。如果您查看
    component > CsvExampleGen > exec_properties > output_config 下，您会看到类似于
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This says that the data set has been split into two sets: train and eval. The
    train set is roughly two-thirds of the original data, and the eval set is around
    one-third of the original data. This information is inferred by looking at the
    hash_buckets property. TFX uses hashing to split the data into train and eval.
    By default, it will define three hash buckets. Then TFX uses the values in each
    record to generate a hash for that record. The values in the record are passed
    to a hashing function to generate a hash. The generated hash is then used to assign
    that example to a bucket. For example, if the hash is 7, then TFX can easily find
    the bucket with 7%, 3 = 1, meaning it will be assigned to the second bucket (as
    buckets are zero indexed). You can access the elements in CsvExampleGen as follows.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这里说数据集已被分成两组：train 和 eval。训练集大约占原始数据的三分之二，而评估集大约占原始数据的三分之一。这些信息是通过查看 hash_buckets
    属性推断出来的。TFX 使用哈希将数据分成训练集和评估集。默认情况下，它将定义三个哈希桶。然后 TFX 使用每个记录中的值为该记录生成哈希。记录中的值传递给哈希函数以生成哈希。然后使用生成的哈希来将该示例分配到一个桶中。例如，如果哈希值为
    7，则 TFX 可以轻松找到具有 7％ 的桶，3 = 1，这意味着它将被分配到第二个桶（因为桶是从零开始索引的）。您可以按以下方式访问 CsvExampleGen
    中的元素。
- en: More on hashing
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 关于哈希的更多信息
- en: 'There are many hashing functions, such as MD5, SHA1, and so forth. You can
    read more about hashing functions at [https://blog.jscrambler.com/hashing-algorithms/](https://blog.jscrambler.com/hashing-algorithms/).
    In TensorFlow, there are two different functions that can be used to generate
    hashes: tf.strings.to_hash_bucket_fast ([http://mng.bz/woJq](http://mng.bz/woJq))
    and tf.strings.to_ hash_bucket_strong (). The strong hash function is slower but
    is more robust against malicious attacks that may manipulate inputs in order to
    control the generated hash value.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多哈希函数，例如 MD5、SHA1 等。您可以在 [https://blog.jscrambler.com/hashing-algorithms/](https://blog.jscrambler.com/hashing-algorithms/)
    上阅读有关哈希函数的更多信息。在 TensorFlow 中，有两种不同的函数可用于生成哈希：tf.strings.to_hash_bucket_fast ([http://mng.bz/woJq](http://mng.bz/woJq))
    和 tf.strings.to_ hash_bucket_strong ()。强哈希函数速度较慢，但更能抵御可能操纵输入以控制生成的哈希值的恶意攻击。
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This will print the following output:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印以下输出：
- en: '[PRE9]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Earlier we said that TFX stores the interim outputs as we progress through the
    pipeline. We saw that the CsvExampleGen component has stored the data as .gz files.
    It in fact stores the examples found in the CSV file as TFRecord objects. A TFRecord
    is used to store data as byte streams. As TFRecord is a common method for storing
    data when working with TensorFlow; these records can be retrieved easily as a
    tf.data.Dataset, and the data can be inspected. The next listing shows how this
    can be done.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 之前我们说过，随着我们在管道中的进展，TFX 会将中间输出存储起来。我们看到 CsvExampleGen 组件已将数据存储为 .gz 文件。事实上，它将
    CSV 文件中找到的示例存储为 TFRecord 对象。TFRecord 用于将数据存储为字节流。由于 TFRecord 是在使用 TensorFlow 时存储数据的常用方法；这些记录可以轻松地作为
    tf.data.Dataset 检索，并且可以检查数据。下一个清单显示了如何做到这一点。
- en: Listing 15.2 Printing the data stored by the CsvExampleGen
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.2 打印 CsvExampleGen 存储的数据
- en: '[PRE10]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Get the URL of the output artifact representing the training examples, which
    is a directory.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取代表训练示例的输出工件的 URL，该工件是一个目录。
- en: ❷ Get the list of files in this directory (all compressed TFRecord files).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 获取此目录中的文件列表（所有压缩的 TFRecord 文件）。
- en: ❸ Create a TFRecordDataset to read these files. The GZip (extension .gz) has
    a set of TFRecord objects.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 创建一个 TFRecordDataset 来读取这些文件。GZip（扩展名为 .gz）包含一组 TFRecord 对象。
- en: ❹ Iterate over the first two records (can be any number less than or equal to
    the size of the data set).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 迭代前两个记录（可以是小于或等于数据集大小的任何数字）。
- en: ❺ Get the byte stream from the TFRecord (containing one example).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 从 TFRecord（包含一个示例）获取字节流。
- en: ❻ Define a tf.train.Example object that knows how to parse the byte stream.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 定义一个知道如何解析字节流的 tf.train.Example 对象。
- en: ❼ Parse the byte stream to a proper readable example.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 将字节流解析为适当可读的示例。
- en: ❽ Print the data.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 打印数据。
- en: 'If you run this code, you will see the following:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行这段代码，你会看到以下内容：
- en: '[PRE11]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: tf.train.Example keeps the data as a collection of features, where each feature
    has a key (column descriptor) and a value. You will see all of the features for
    a given example. For example, the DC feature has a floating value of 605.799,
    feature RH has an int value of 43, feature area has a floating value of 2.0, and
    feature day has a bytes_list (used to store strings) value of "tue" (i.e., Tuesday).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: tf.train.Example 将数据保存为一组特征，每个特征都有一个键（列描述符）和一个值。你会看到给定示例的所有特征。例如，DC 特征具有浮点值
    605.799，RH 特征具有整数值 43，area 特征具有浮点值 2.0，而 day 特征具有 bytes_list（用于存储字符串）值为 "tue"（即星期二）。
- en: 'Before moving to the next section, let’s remind ourselves what our objective
    is: to develop a model that can predict the fire spread (in hectares) given all
    the other features in the data set. This problem is framed as a regression problem.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在移动到下一节之前，让我们再次提醒自己我们的目标是什么：开发一个模型，可以根据数据集中的所有其他特征来预测火灾蔓延（以公顷为单位）。这个问题被构建为一个回归问题。
- en: 15.1.2 Generating basic statistics from the data
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.1.2 从数据生成基本统计信息
- en: 'As the next step, we will understand the data better. This is known as exploratory
    data analysis (EDA). EDA is not typically well defined and very much depends on
    the problem you are solving and the data. And you have to factor in the limited
    time you usually have until the delivery of a project. In other words, you cannot
    test everything and have to prioritize what you want to test and what you want
    to assume. For the structured data we are tackling here, a great place to start
    is understanding type (numerical versus categorical) and the distribution of values
    of the various columns present. TFX provides you a component just for that. StatisticsGen
    will automatically generate those statistics for you. We will soon see in more
    detail what sort of insights this module provides:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 作为下一步，我们将更好地理解数据。这称为探索性数据分析（EDA）。EDA 通常不是很明确，并且非常依赖于您正在解决的问题和数据。您还必须考虑到通常在项目交付之前的有限时间。换句话说，您不能测试所有内容，必须优先考虑要测试的内容和要假设的内容。对于我们在这里处理的结构化数据，一个很好的起点是了解类型（数值与分类）以及各列值的分布。TFX
    为此提供了一个组件。StatisticsGen 将自动生成这些统计信息。我们很快将更详细地看到此模块提供了什么样的见解：
- en: '[PRE12]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This will produce an HTML table similar to what you saw after running CsvExampleGen
    (figure 15.3).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成一个 HTML 表格，类似于您在运行 CsvExampleGen 后看到的表格（见图 15.3）。
- en: '![15-03](../../OEBPS/Images/15-03.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![15-03](../../OEBPS/Images/15-03.png)'
- en: Figure 15.3 The output provided by the StatisticsGen component
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.3 StatisticsGen 组件提供的输出
- en: 'However, to retrieve the most valuable output of this step, you have to run
    the following line:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，要检索此步骤的最有价值的输出，您必须运行以下命令：
- en: '[PRE13]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This will create the following files in the pipeline root (figure 15.4).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在管道根目录中创建以下文件（见图 15.4）。
- en: '![15-04](../../OEBPS/Images/15-04.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![15-04](../../OEBPS/Images/15-04.png)'
- en: Figure 15.4 The directory/file structure after running StatisticsGen
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.4 运行 StatisticsGen 后的目录/文件结构
- en: Figure 15.5 shows the valuable collection of information about data provided
    by TFX. The output graph shown in figure 15.5 is a goldmine containing rich information
    about the data we’re dealing with. It provides you a basic yet holistic suite
    of graphs that provides lots of information about the columns present in the data.
    Let’s go from top to bottom. At the top, you have options to sort and filter the
    outputs shown in figure 15.5\. For example, you can change the order of the graphs,
    select graphs based on data types, or filter them by a regular expression.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.5 展示了TFX提供的有关数据的宝贵信息集合。图15.5中的输出图是一个包含丰富数据的金矿，提供了大量关于我们处理的数据的信息。它为你提供了基本但全面的图表套件，提供了有关数据中存在的列的许多信息。让我们从上到下来看。在顶部，你可以选择排序和过滤图15.5中显示的输出。例如，你可以改变图表的顺序，选择基于数据类型的图表，或者通过正则表达式进行筛选。
- en: '![15-05](../../OEBPS/Images/15-05.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![15-05](../../OEBPS/Images/15-05.png)'
- en: Figure 15.5 The summary statistics graphs generated for the data by the StatisticsGen
    component
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.5 由StatisticsGen组件生成的数据的摘要统计图
- en: By default, StatisticsGen will generate graphs for both train and eval data
    sets. Then each train and eval section will have several subsections; in this
    case, we have a section for numerical columns and categorical columns.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，StatisticsGen将为训练集和评估集生成图表。然后每个训练和评估部分将有几个子部分；在这种情况下，我们有数值列和分类列的部分。
- en: On the left, you can see some numerical statistics and assessments of a feature,
    whereas on the right side, you can see a visual representation of how a feature
    is distributed. For example, take the FFMC feature in the training set. We can
    see that it has 333 examples and 0% have missing values for that feature. It has
    a mean of ~90 and a standard deviation of 6.34\. In the graph, you can see that
    the distribution is quite skewed. Almost all values are concentrated around the
    80-90 range. You will see later how this might create problems for us and how
    we will solve them.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在左边，你可以看到一些数字统计和特征的评估，而在右边，你可以看到特征分布的视觉表示。例如，拿训练集中的FFMC特征来说。我们可以看到它有333个例子且0%的特征缺失值。它的平均值约为90，标准偏差为6.34。在图表中，你可以看到分布是相当倾斜的。几乎所有的值都集中在80-90范围内。你将看到稍后这可能会给我们制造问题以及我们将如何解决它们。
- en: In the categorical section, you can see the values of the day and month features.
    For example, the day feature has seven unique values and 0% missing. The most
    frequent value (i.e., mode) of the day feature appears 60 times. Note that the
    day is represented as a bar graph and the month is represented as a line graph
    because for features with unique values above a threshold, a line graph is used
    to make the graph clear and less cluttered.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类部分，你可以看到日和月特征的值。例如，日特征有七个唯一值，且0%缺失。日特征的最频繁值（即模式）出现了60次。请注意，日表示为条形图，月表示为线图，因为对于唯一值高于阈值的特征，使用线图可以使图表清晰且减少混乱。
- en: 15.1.3 Inferring the schema from data
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.1.3 从数据推断模式
- en: Thus far, we have loaded the data from a CSV file and explored the basic statistics
    of the data set. The next big step is to infer the schema of the data. TFX can
    automatically derive the schema of the data once the data is provided. If you
    have worked with databases, the schema derived is the same as a database schema.
    It can be thought of as a blueprint for the data, expressing the structure and
    important attributes of data. It can also be thought of as a set of rules that
    dictate what the data should look like. For example, if you have the schema, you
    can classify whether a given record is valid by referring to the schema.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经从CSV文件中加载了数据并探索了数据集的基本统计信息。下一个重要的步骤是推断数据的模式。一旦提供了数据，TFX可以自动推断数据的模式。如果你使用过数据库，推断出的模式与数据库模式相同。它可以被视为数据的蓝图，表达数据的结构和重要属性。它也可以被视为一组规则，规定数据应该看起来像什么。例如，如果你有了模式，你可以通过参考模式来分类给定的记录是否有效。
- en: 'Without further ado, let’s create a SchemaGen object. The SchemaGen requires
    the output of the previous step (i.e., output of the StatisticsGen) and a Boolean
    argument named infer_feature_shape:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 不做更多的话，让我们创建一个SchemaGen对象。SchemaGen需要前一步的输出（即StatisticsGen的输出）和一个名为infer_feature_shape的布尔参数。
- en: '[PRE14]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Here, we set the infer_feature_shape to False, as we will do some transformations
    to the features down the road. Therefore, we will have the flexibility to manipulate
    the feature shapes more freely. However, setting this argument (infer_feature_shape)
    means an important change for a downstream step (called the transform step). When
    infer_feature_shape is set to False, the tensors passed to the transform step
    are represented as tf.SparseTensor objects, not tf.Tensor objects. If set to True,
    it will need to be a tf.Tensor object with a known shape. Next, to see the output
    of the SchemaGen, you can do
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将infer_feature_shape设置为False，因为我们将在特征上进行一些转换。因此，我们将有更大的灵活性来自由操作特征形状。然而，设置这个参数（infer_feature_shape）意味着对下游步骤（称为transform步骤）的重要改变。当infer_feature_shape设置为False时，传递给transform步骤的张量被表示为tf.SparseTensor对象，而不是tf.Tensor对象。如果设置为True，则需要是一个具有已知形状的tf.Tensor对象。接下来，要查看SchemaGen的输出，可以执行以下操作
- en: '[PRE15]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: which will produce the output shown in table 15.1.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生表15.1所示的输出。
- en: Table 15.1 The schema output generated by TFX
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 表15.1 TFX生成的模式输出
- en: '| **Feature name** | **Type** | **Presence** | **Valency** | **Domain** |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| **特征名称** | **类型** | **存在** | **价值** | **域** |'
- en: '| **‘day’** | STRING | required | single | ‘day’ |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| **‘day’** | STRING | 必须的 | 单个的 | ‘day’ |'
- en: '| **‘month’** | STRING | required | single | ‘month’ |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| **‘month’** | STRING | 必须的 | 单个的 | ‘month’ |'
- en: '| **‘DC’** | FLOAT | required | single | - |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| **‘DC’** | FLOAT | 必须的 | 单个的 | - |'
- en: '| **‘DMC’** | FLOAT | required | single | - |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| **‘DMC’** | FLOAT | 必须的 | 单个的 | - |'
- en: '| **‘FFMC’** | FLOAT | required | single | - |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| **‘FFMC’** | FLOAT | 必须的 | 单个的 | - |'
- en: '| **‘ISI’** | FLOAT | required | single | - |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| **‘ISI’** | FLOAT | 必须的 | 单个的 | - |'
- en: '| **‘RH’** | INT | required | single | - |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| **‘RH’** | INT | 必须的 | 单个的 | - |'
- en: '| **‘X’** | INT | required | single | - |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| **‘X’** | INT | 必须的 | 单个的 | - |'
- en: '| **‘Y’** | INT | required | single | - |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| **‘Y’** | INT | 必须的 | 单个的 | - |'
- en: '| **‘area’** | FLOAT | required | single | - |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| **‘area’** | FLOAT | 必须的 | 单个的 | - |'
- en: '| **‘rain’** | FLOAT | required | single | - |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| **‘rain’** | FLOAT | 必须的 | 单个的 | - |'
- en: '| **‘temp’** | FLOAT | required | single | - |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| **‘temp’** | FLOAT | 必须的 | 单个的 | - |'
- en: '| **‘wind’** | FLOAT | required | single |  |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| **‘wind’** | FLOAT | 必须的 | 单个的 |  |'
- en: '| **Domain** | **Values** |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| **域** | **值** |'
- en: '| **‘day’** |  |  |  |  |  | ‘fri’ | ‘mon’ | ‘sat’ | ‘sun’ | ‘thu’ | ‘tue’
    | ‘wed’ |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| **‘day’** |  |  |  |  |  | ‘fri’ | ‘mon’ | ‘sat’ | ‘sun’ | ‘thu’ | ‘tue’
    | ‘wed’ |'
- en: '| **‘month’** | ‘apr’ | ‘aug’ | ‘dec’ | ‘feb’ | ‘jan’ | ‘jul’ | ‘jun’ | ‘mar’
    | ‘may’ | ‘oct’ | ‘sep’ | ‘nov’ |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| **‘month’** | ‘apr’ | ‘aug’ | ‘dec’ | ‘feb’ | ‘jan’ | ‘jul’ | ‘jun’ | ‘mar’
    | ‘may’ | ‘oct’ | ‘sep’ | ‘nov’ |'
- en: 'Domain defines the constraints of a given feature. We list some of the most
    popular domains defined in TFX:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 域定义了给定特征的约束。我们列出了TFX中定义的一些最受欢迎的域：
- en: '*Integer domain values* (e.g., defines minimum/maximum of an integer feature)'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*整数域值*（例如，定义整数特征的最小/最大值）'
- en: '*Float domain values* (e.g., defines minimum/maximum of a floating-value feature)'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*浮点域值*（例如，定义浮点值特征的最小/最大值）'
- en: '*String domain value* (e.g., defines allowed values/tokens for a string features)'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*字符串域值*（例如，为字符串特征定义允许的值/标记）'
- en: '*Boolean domain values* (e.g., can be used to define custom values for true/false
    states)'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*布尔域值*（例如，可以用于定义真/假状态的自定义值）'
- en: '*Struct domain values* (e.g., can be used to define recursive domains [a domain
    within a domain] or domains with multiple features)'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*结构域值*（例如，可以用于定义递归域[域内的域]或具有多个特征的域）'
- en: '*Natural language domain values* (e.g., defines a vocabulary [allowed collection
    of tokens] for a related language feature)'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自然语言域值*（例如，为相关语言特征定义一个词汇表[允许的标记集合]）'
- en: '*Image domain values* (e.g., can be used to restrict the maximum byte size
    of images)'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图像域值*（例如，可以用来限制图像的最大字节大小）'
- en: '*Time domain values* (e.g., can be used to define data/time features)'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*时间域值*（例如，可以用来定义数据/时间特征）'
- en: '*Time of day domain values* (e.g., can be used to define a time without a date)'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*时间值域*（例如，可以用来定义不带日期的时间）'
- en: The list of domains is available in a file called schema.proto. schema.proto
    is defined at [http://mng.bz/7yp9](http://mng.bz/7yp9). These files are defined
    using a library called Protobuf. Protobuf is a library designed for object serialization.
    You can read the following sidebar to learn more about the Protobuf library.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 域的列表可在名为schema.proto的文件中找到。schema.proto在[http://mng.bz/7yp9](http://mng.bz/7yp9)上定义。这些文件是使用一个叫做Protobuf的库定义的。Protobuf是一种用于对象序列化的库。您可以阅读下面的侧边栏了解有关Protobuf库的更多信息。
- en: Protobuf library
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Protobuf库
- en: Protobuf is an object serialization/deserialization library developed by Google.
    The object that needs to be serialized is defined as a Protobuf message. The template
    of a message is defined with a .proto file. Then, to deserialize, Protobuf provides
    functions such as ParseFromString(). To read more about the library, refer to
    [http://mng.bz/R45P](http://mng.bz/R45P).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: Protobuf 是由 Google 开发的对象序列化/反序列化库。需要序列化的对象被定义为 Protobuf 消息。消息的模板由 .proto 文件定义。然后，为了进行反序列化，Protobuf
    提供了诸如 ParseFromString() 等函数。要了解有关该库的更多信息，请参阅 [http://mng.bz/R45P](http://mng.bz/R45P)。
- en: Next, we will see how we can convert data to features.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看到如何将数据转换为特征。
- en: 15.1.4 Converting data to features
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.1.4 将数据转换为特征
- en: 'We have reached the final stage of our data-processing pipeline. The final
    step is to convert the columns we have extracted to features that are meaningful
    to our model. We are going to create three types of features:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经到达了数据处理管道的最终阶段。最后一步是将我们提取的列转换为对我们的模型有意义的特征。我们将创建三种类型的特征：
- en: '*Dense floating-point features*—Values are presented as floating-point numbers
    (e.g., temperature). This means the value is passed as it is (with an optional
    normalizing step; e.g., Z-score normalization) to create a feature.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*密集的浮点数特征*—值以浮点数（例如，温度）的形式呈现。这意味着该值会按原样传递（可以选择进行归一化处理；例如，Z 分数归一化）以创建一个特征。'
- en: '*Bucketized features*—Numerical values that are binned according to predefined
    binning intervals. This means the value will be converted to a bin index, depending
    on which bin the value falls into (e.g., we can bucketize relative humidity to
    three values: low [-inf, 33), medium [33, 66), and high [66, inf)).'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分桶特征*—根据预定义的分桶间隔对数值进行分桶。这意味着该值将根据其落入的分桶而转换为桶索引（例如，我们可以将相对湿度分成三个值：低[-inf，33)，中[33，66)，高[66，inf)）。'
- en: '*Categorical features* (integer-based or string-based)—Value is chosen from
    a predefined set of values (e.g., day or month). If the value is not already an
    integer index (e.g., day as a string), it will be converted to an integer index
    using a vocabulary that maps each word to an index (e.g., "mon" is mapped to 0,
    "tue" is mapped to 1, etc.).'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分类特征*（基于整数或字符串）—值是从预定义的值集中选择的（例如，日期或月份）。如果该值尚未是整数索引（例如，日期作为字符串），则将使用将每个单词映射到索引的词汇表将其转换为整数索引（例如，“mon”
    被映射为 0，“tue” 被映射为 1，等等）。'
- en: 'We will introduce one of these feature transformations to each of the fields
    in the data set:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将向数据集中的每个字段介绍其中一种特征转换：
- en: '*X* (spatial coordinate)—Presented as a floating-point value'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*X*（空间坐标）—以浮点数值表示'
- en: '*Y* (spatial coordinate)—Presented as a floating-point value'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Y*（空间坐标）—以浮点数值表示'
- en: '*wind* (wind speed)—Presented as a floating-point value'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*wind*（风速）—以浮点数值表示'
- en: '*rain* (outside rain)—Presented as a floating-point value'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*rain*（室外降雨）—以浮点数值表示'
- en: '*FFMC* (fuel moisture)—Presented as a floating-point value'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*FFMC*（燃料湿度）—以浮点数值表示'
- en: '*DMC* (average moisture content)—Presented as a floating-point value'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*DMC*（平均含水量）—以浮点数值表示'
- en: '*DC* (depth of dryness in the soil)—Presented as a floating-point value'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*DC*（土壤干燥深度）—以浮点数值表示'
- en: '*ISI* (expected rate of fire spread)—Presented as a floating-point value'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ISI*（预期火灾蔓延速率）—以浮点数值表示'
- en: '*temp* (temperature)—Presented as a floating-point value'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*temp*（温度）—以浮点数值表示'
- en: '*RH* (relative humidity)—Presented as a bucketized value'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*RH*（相对湿度）—作为分桶值表示'
- en: '*month*—Presented as a categorical feature'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*month*—作为分类特征表示'
- en: '*day*—Presented as a categorical feature'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*day*—作为分类特征表示'
- en: '*area* (the burned area)—The label feature kept as a numerical value'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*area*（烧毁面积）—作为数值保留的标签特征'
- en: We are first going to define some constants, which will help us to keep track
    of which feature is assigned to which category. Additionally, we will keep variable
    specific properties (e.g., maximum number of classes for categorical features;
    see the next listing).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先要定义一些常量，这些常量将帮助我们跟踪哪个特征分配给了哪个类别。此外，我们将保留特定属性（例如，分类特征的最大类数；请参阅下一个列表）。
- en: Listing 15.3 Defining feature-related constants for the feature transformation
    step
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.3 定义特征转换步骤中与特征相关的常量
- en: '[PRE16]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ This command will write the content of this cell to a file (read the sidebar
    for more information).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 此命令将将此单元格的内容写入文件（阅读侧边栏以获取更多信息）。
- en: ❷ Vocabulary-based (or string-based) categorical features.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 基于词汇（或字符串）的分类特征。
- en: ❸ Categorical features are assumed to each have a maximum value in the data
    set.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 数据集中假设每个分类特征都有一个最大值。
- en: ❹ Dense features (these will go to the model as they are, or normalized)
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 密集特征（这些将作为模型输入，或进行归一化处理）。
- en: ❺ Bucketized features
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 分桶特征。
- en: '❻ The bucket boundaries for bucketized features (e.g., the feature RH will
    be bucketed to three bins: [0, 33), [33, 66), [66, inf)).'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 分桶特征的分桶边界（例如，特征RH将被分桶为三个箱子：[0, 33)，[33, 66)，[66，inf))。
- en: ❼ Label features will be kept as numerical features as we are solving a regression
    problem.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 标签特征将保留为数值特征，因为我们正在解决回归问题。
- en: ❽ Define a function that will add a suffix to the feature name. This will help
    us to distinguish the generated features from original data columns.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 定义一个函数，将在特征名称后添加后缀。这将帮助我们区分生成的特征和原始数据列。
- en: The reason we are writing these notebook cells as Python scripts (or Python
    modules) is because TFX expects some parts of the code it needs to run as a Python
    module.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这些笔记本单元格写为Python脚本（或Python模块）的原因是因为TFX期望运行所需的一些代码部分作为Python模块。
- en: '%%writefile magic command'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '%%writefile 魔术命令'
- en: '%%writefile is a Jupyter magic command (similar to %%tensorboard). It will
    cause the Jupyter notebook to write the content in a cell to a new file (e.g.,
    a Python module/script). This is a great way to create isolated Python modules
    from notebook cells. Notebooks are great for experimenting, but for production-level
    code, Python scripts are better. For example, our TFX pipeline expects certain
    functions (e.g., how to preprocess raw columns to features) to be independent
    Python modules. We can conveniently use the %%writefile command to achieve that.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '%%writefile 是一个Jupyter魔术命令（类似于%%tensorboard）。它会导致Jupyter笔记本将单元格中的内容写入到新文件中（例如，Python模块/脚本）。这是从笔记本单元格创建独立Python模块的好方法。笔记本很适合进行实验，但对于生产级别的代码，Python脚本更好。例如，我们的TFX管道期望某些函数（例如，如何将原始列预处理为特征）是独立的Python模块。我们可以方便地使用%%writefile命令来实现这一点。'
- en: This command must be specified as the very top command in the cell you want
    to be written out to a file.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令必须指定为要写入文件的单元格中的第一个命令。
- en: Next, we will write another module called forest_fires_transform.py, which will
    have a preprocessing function (called preprocessing_fn) that defines how each
    data column should be treated in order to become a feature (see the next listing).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将编写另一个模块forest_fires_transform.py，其中将有一个预处理函数（称为preprocessing_fn），该函数定义了每个数据列应如何处理以成为特征（请参见下一个列表）。
- en: Listing 15.4 Defining a Python module to convert raw data to features
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.4 定义将原始数据转换为特征的Python模块。
- en: '[PRE17]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ The content in this code listing will be written to a separate Python module.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 此代码列表中的内容将被写入到单独的Python模块中。
- en: ❷ Imports the feature constants defined previously
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 导入先前定义的特征常量。
- en: ❸ Imports all the constants defined in the forest_fires_constants module
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 导入forest_fires_constants模块中定义的所有常量。
- en: ❹ This is a must-have callback function for the tf.transform library to convert
    raw columns to features.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 这是tf.transform库中必不可少的回调函数，用于将原始列转换为特征。
- en: ❺ Treats all the dense features
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 对所有密集特征进行处理。
- en: ❻ Perform Z-score-based scaling (or normalization) on dense features
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 对密集特征执行基于Z-score的缩放（或标准化）。
- en: ❼ Because infer_feature_shape is set to False in the SchemaGen step, we have
    sparse tensors as inputs. They need to be converted to dense tensors.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 因为在SchemaGen步骤中infer_feature_shape设置为False，我们的输入是稀疏张量。它们需要转换为密集张量。
- en: ❽ For the vocabulary-based features, build the vocabulary and convert each token
    to an integer ID.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 对于基于词汇的特征，构建词汇表并将每个标记转换为整数ID。
- en: ❾ For the to-be-bucketized features, using the bucket boundaries defined, bucketize
    the features.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 对待分桶的特征，使用定义的分桶边界，对特征进行分桶。
- en: ❿ The label feature is simply converted to dense without any other feature transformations.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 标签特征只是简单地转换为密集张量，没有其他特征转换。
- en: ⓫ A utility function for converting sparse tensors to dense tensors
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ⓫ 一个将稀疏张量转换为密集张量的实用函数。
- en: You can see that this file is written as forest_fires_transform.py. It defines
    a preprocessing_fn(), which takes an argument called inputs. inputs is a dictionary
    mapping from feature keys to columns of data found in the CSV, flowing from the
    example_gen output. Finally, it returns a dictionary with feature keys mapped
    to transformed features using the tensorflow_transform library. In the middle
    of the method, you can see the preprocessing function doing three important jobs.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到该文件被命名为forest_fires_transform.py。它定义了一个preprocessing_fn()函数，该函数接受一个名为inputs的参数。inputs是一个从特征键到在CSV文件中找到的数据列的映射字典，从example_gen输出流动。最后，它返回一个字典，其中特征键映射到使用tensorflow_transform库转换的特征。在方法的中间，您可以看到预处理函数执行三项重要工作。
- en: First, it reads all dense features (whose names are stored in _DENSE_FLOAT_FEATURE_KEYS)
    and normalizes the values using z-score. The z-score normalizes a column *x* as
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，它读取所有密集特征（其名称存储在_DENSE_FLOAT_FEATURE_KEYS中），并使用z分数对值进行归一化。z分数将某一列*x*归一化为
- en: '![15_05a](../../OEBPS/Images/15_05a.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![15_05a](../../OEBPS/Images/15_05a.png)'
- en: where μ(*x*) is mean value of the column and σ(*x*) is the standard deviation
    of the column. To normalize data, you can call the function scale_to_z_score()
    in the tensorflow_transform library. You can read the sidebar on tensorflow_transform
    to understand more about what the library offers. Then the function stores each
    feature in the outputs under a new key (via the _transformed_name function) derived
    from the original feature name (the new key is generated by appending _xf to the
    end of the original feature name).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，μ(*x*)是列的平均值，σ(*x*)是列的标准差。要对数据进行归一化，可以调用tensorflow_transform库中的scale_to_z_score()函数。您可以阅读有关tensorflow_transform的侧边栏，了解更多有关该库提供的内容。然后，该函数使用新的键（通过_transformed_name函数生成）将每个特征存储在输出中，该新键衍生自原始特征名称（新键通过在原始特征名称末尾添加_xf生成）。
- en: Next, it treats the vocabulary-based categorical features (where names are stored
    in _VOCAB_FEATURE_KEYS) by converting each string to an index using a dictionary.
    The dictionary maps each string to an index and is learned from the provided training
    data automatically. This is similar to how we used the Keras Tokenizer object
    to learn a dictionary that we used to convert words to word IDs. In the tensorflow_transform
    library you have the handy compute_and_apply_vocabulary() function. To the compute_and_apply_vocabulary()*function,
    we can pass* num_oov_buckets=1 *in order to assign any unseen strings to a special
    category (apart from the ones already assigned to known categories).*
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，它处理基于词汇的分类特征（其名称存储在_VOCAB_FEATURE_KEYS中），通过使用字典将每个字符串转换为索引。该字典将每个字符串映射到索引，并且可以自动从提供的训练数据中学习。这类似于我们如何使用Keras的Tokenizer对象学习字典，将单词转换为单词ID。在tensorflow_transform库中，您可以使用compute_and_apply_vocabulary()函数完成这一操作。对于compute_and_apply_vocabulary()函数，我们可以通过传递num_oov_buckets=1来将任何未见字符串分配给特殊类别（除了已分配给已知类别的类别）。
- en: Afterward, the function tackles the to-be-bucketized features. Bucketization
    is the process of applying a continuous value to a bucket, where a bucket is defined
    by a set of boundaries. Bucketizing features can be achieved effortlessly with
    the apply_buckets() function, which takes the feature (provided in the inputs
    dictionary) and bucket boundaries as the input arguments.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，函数处理待进行桶化的特征。Bucketization是将连续值应用于桶的过程，其中桶由一组边界定义。使用apply_buckets()函数可以轻松地对特征进行bucket化，该函数将特征（在输入字典中提供）和桶边界作为输入参数。
- en: Finally, we keep the column containing the label as it is. With that, we define
    the Transform component ([http://mng.bz/mOGr](http://mng.bz/mOGr)).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们保留包含标签的列不变。通过这样，我们定义了Transform组件（[http://mng.bz/mOGr](http://mng.bz/mOGr)）。
- en: 'tensorflow_transform: Converting raw data to features'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: tensorflow_transform：将原始数据转换为特征
- en: 'tensorflow_transform is a sub-library in TensorFlow primarily focused on feature
    transformations. It offers a variety of functions to compute things:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: tensorflow_transform是TensorFlow中的一个子库，主要关注特征转换。它提供了各种功能来计算各种东西：
- en: Bucketizing features (e.g., binning a range of values to a predefined set of
    bins)
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对特征进行桶化（例如，将一系列值分组到预定义的一组桶中）
- en: Bag-of-words features from a string column
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从字符串列中提取词袋特征
- en: Covariance matrices of a data set
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集的协方差矩阵
- en: Mean, standard deviation, min, max, count, and so forth of columns
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列的均值、标准差、最小值、最大值、计数等
- en: You can read more about the functions this library offers at [http://mng.bz/5QgB](http://mng.bz/5QgB).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[http://mng.bz/5QgB](http://mng.bz/5QgB)上阅读有关此库提供的功能的更多信息。
- en: '[PRE18]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The Transform component takes three inputs:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: Transform组件接受三个输入：
- en: Output examples of the CsvExampleGen component
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CsvExampleGen组件的输出示例
- en: Schema from the SchemaGen
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SchemaGen生成的架构
- en: The Python module that defines the preprocessing_fn() function for transforming
    data to features
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于将数据转换为特征的preprocessing_fn()函数的Python模块
- en: One thing we must do when it comes to multi-component pipelines, like a TFX
    pipeline, is check every interim output whenever we can. It’s a much better choice
    than leaving things to chance and praying things work out fine (which is normally
    never the case). So, let’s inspect the output by printing some of the data saved
    to the disk after running the Transform step (see the next listing). The code
    for printing the data will be similar to when we printed the data when using the
    CsvExampleGen component.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到多组件流水线，比如TFX流水线时，我们必须尽可能地检查每一个中间输出。这比交给偶然性并祈祷一切顺利要好得多（通常情况下都不是这样）。因此，让我们通过打印运行Transform步骤后保存到磁盘上的一些数据来检查输出（见下一列表）。打印数据的代码与使用CsvExampleGen组件时打印数据的代码类似。
- en: Listing 15.5 Inspecting the outputs produced by the TFX Transform step
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.5 检查TFX Transform步骤产生的输出
- en: '[PRE19]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ Get the list of files in this directory (all compressed TFRecord files).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取此目录中文件的列表（所有压缩的TFRecord文件）。
- en: ❷ Create a TFRecordDataset to read these files.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建一个TFRecordDataset来读取这些文件。
- en: ❸ Used to store the retrieved feature values (for later inspection)
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 用于存储检索到的特征值（以供以后检查）
- en: ❹ Dense (i.e., float) and integer (i.e., vocab-based and bucketized) features
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 稠密（即，浮点数）和整数（即，基于词汇和分桶）特征
- en: ❺ Get the first five examples in the data set.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 获取数据集中的前五个示例。
- en: ❻ Get a tf record and convert that to a readable tf.train.Example.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 获取一个TF记录并将其转换为可读的tf.train.Example。
- en: ❼ We will extract the values of the features from the tf.train.Example object
    for subsequent inspections.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 我们将从tf.train.Example对象中提取特征的值以供后续检查。
- en: ❽ Append the extracted values as a record (i.e., tuple of values) to example_records.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 将提取的值作为记录（即，值的元组）附加到example_records中。
- en: The code explained will print the data after feature transformation. Each example
    stores integer values in the attribute path, example.features.feature[<feature
    name>] .int64_list.value, whereas the floating values are stored at example.features.feature
    [<feature name>].float_list.value. This will print examples such as
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 解释的代码将打印特征转换后的数据。每个示例都将整数值存储在属性路径下，例如example.features.feature[<feature name>]
    .int64_list.value，而浮点值存储在example.features.feature [<feature name>].float_list.value中。这将打印例如
- en: '[PRE20]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Note that we are using the _transformed_name() function to obtain the transformed
    feature names. We can see that the floating-point values (DC_xf) are normalized
    using z-score normalization, vocabulary-based features (day_xf) are converted
    to an integer, and bucketized features (RH_xf) are presented as integers.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用_transformed_name()函数来获取转换后的特征名称。我们可以看到，浮点值（DC_xf）使用z分数标准化，基于词汇的特征（day_xf）转换为整数，并且分桶特征（RH_xf）被呈现为整数。
- en: 'Rule of thumb: Check your pipeline whenever possible'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 经验法则：尽可能检查您的管道
- en: When using components offered by third-party libraries like TFX, there is very
    low visibility into what is actually taking place under the hood. This is exacerbated
    by the fact that TFX is not a highly matured tool and is in the process of development.
    Therefore, we always try to incorporate pieces of code that probe into these components,
    which will help us to sanity-check inputs and outputs of these components.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用TFX等第三方库提供的组件时，对于底层实际发生的事情几乎没有透明度。TFX并不是一个高度成熟的工具，并且正在开发过程中，这使问题更加严重。因此，我们总是尝试并入一些代码片段来探查这些组件，这将帮助我们检查这些组件的输入和输出是否正常。
- en: In the next section, we will train a simple regression model as a part of the
    pipeline we’ve been creating.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将训练一个简单的回归模型，作为我们一直在创建的流水线的一部分。
- en: Exercise 1
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 练习1
- en: 'Let’s say that, instead of the previously defined feature transformations,
    you want to do the following:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想要做以下事情而不是先前定义的特征转换：
- en: '*DC*—Scale data to a range of [0, 1]'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*DC*—将数据缩放到[0, 1]的范围内'
- en: '*temp*—Bucketize with the boundaries (-inf, 20], (20, 30] and (30, inf)'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*temp*—利用边界值（-inf，20]，（20，30]和（30，inf）进行分桶处理'
- en: Once the features are transformed, add them to a dictionary named outputs, where
    each feature is keyed by the transformed feature name. Assume you can obtain the
    transformed feature name for temp by calling, _transformed_name(‘temp’). How would
    you use the tensorflow_transform library to achieve this? You can use the scale_to_0_1()
    and apply_buckets() functions to achieve this.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦特征被转换，将它们添加到名为 outputs 的字典中，其中每个特征都以转换后的特征名称作为键。假设你可以通过调用 _transformed_name('temp')
    来获取 temp 的转换后的特征名称。您如何使用 tensorflow_transform 库来实现此目标？您可以使用 scale_to_0_1() 和 apply_buckets()
    函数来实现这一点。
- en: '15.2 Training a simple regression neural network: TFX Trainer API'
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.2 训练一个简单的回归神经网络：TFX Trainer API
- en: You have defined a TFX data pipeline that can convert examples in a CSV file
    to model-ready features. Now you will train a model on this data. You will use
    TFX to define a model trainer, which will take a simple two-layer fully connected
    regression model and train that on the data flowing from the data pipeline. Finally,
    you will predict using the model on some sample evaluation data.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经定义了一个 TFX 数据管道，可以将 CSV 文件中的示例转换为模型准备的特征。现在，您将使用 TFX 定义一个模型训练器，该模型训练器将采用一个简单的两层全连接回归模型，并将其训练在从数据管道流出的数据上。最后，您将使用模型对一些样本评估数据进行预测。
- en: With a well-defined data pipeline defined using TFX, we’re at the cusp of training
    a model with the data flowing from that pipeline. Training a model with TFX can
    be slightly demanding at first sight due to the rigid structure of functions and
    data it expects. However, once you are familiar with the format you need to adhere
    to, it gets easier.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 TFX 定义了一个良好定义的数据管道后，我们就可以使用从该管道流出的数据来训练模型。通过 TFX 训练模型一开始可能会稍微费劲，因为它期望的函数和数据的严格结构。但是，一旦您熟悉了您需要遵循的格式，它就会变得更容易。
- en: We will go through this section in three stages. First, let’s examine how we
    can define a Keras model to suit the output features we have defined in the TFX
    Transform component. Ultimately, the model will receive the output of the Transform
    component. Next, we will look at how we can write a function that encapsulates
    model training. This function will use the model defined and, along with several
    user-defined arguments, train the model and save it to a desired path. The saved
    model cannot be just any model; it has to have what are known as *signatures*
    in TensorFlow. These signatures dictate what the inputs to the model and outputs
    of the model look like when it’s finally used via an API. The API is served via
    a server that exposes a network port for the client to communicate with the API.
    Figure 15.6 depicts how the API ties in with the model.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将分三个阶段进行本节的学习。首先，让我们看看如何定义一个适合 TFX Transform 组件中定义的输出特征的 Keras 模型。最终，模型将接收
    Transform 组件的输出。接下来，我们将研究如何编写一个封装了模型训练的函数。此函数将使用所定义的模型，并结合几个用户定义的参数，对模型进行训练并将其保存到所需的路径。保存的模型不能只是任意模型；在
    TensorFlow 中它们必须具有所谓的 *签名*。这些签名规定了模型在最终通过 API 使用时的输入和输出是什么样子的。API 通过一个服务器提供，该服务器公开一个网络端口供客户端与
    API 通信。图 15.6 描述了 API 如何与模型关联。
- en: '![15-06](../../OEBPS/Images/15-06.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![15-06](../../OEBPS/Images/15-06.png)'
- en: Figure 15.6 How the model interacts with the API, the TensorFlow server, and
    the client
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.6 模型如何与 API、TensorFlow 服务器和客户端交互
- en: Let’s understand what is taking place in figure 15.6\. First, an HTTP client
    sends a request to the server. The server (i.e., a TensorFlow serving server)
    that is listening for any incoming requests will read the request and direct that
    to the required model signature. Once the data is received by the model signature,
    it will perform necessary processing on the data, run it through the model, and
    produce the output (e.g., predictions). Once the predictions are available, they
    will be returned by the server to the client. We will discuss the API and the
    server side in detail in a separate section. In this section, our focus is on
    the model.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们理解图 15.6 中发生了什么。首先，一个 HTTP 客户端发送请求到服务器。正在监听任何传入请求的服务器（即 TensorFlow 服务服务器）将读取请求并将其指向所需的模型签名。一旦模型签名接收到数据，它将对数据进行必要的处理，将其传递给模型，并生成输出（例如预测）。一旦预测可用，服务器将其返回给客户端。我们将在单独的部分详细讨论
    API 和服务器端。在本节中，我们的重点是模型。
- en: What is a signature in TensorFlow serving?
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 服务中的签名是什么？
- en: In real life, the purpose of a signature is to uniquely identify a person. Similarly,
    TensorFlow uses signatures to uniquely determine how a model should behave when
    an input is passed to the model via a HTTP request. A signature has a key and
    a value. The key is a unique identifier that defines to which exact URL that signature
    will be activated. The value is defined as a TensorFlow function (i.e., a function
    decorated with @tf.function). This function will define how an input is handled
    and passed to the model to obtain the final desired result. You don’t need to
    worry about the details at this point. We have a separate section dedicated to
    learning about signatures.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实生活中，签名的目的是唯一标识一个人。同样，TensorFlow 使用签名来唯一确定当通过 HTTP 请求将输入传递给模型时模型应该如何行为。一个签名有一个键和一个值。键是一个唯一标识符，定义了要激活该签名的确切
    URL。值被定义为一个 TensorFlow 函数（即用 @tf.function 装饰的函数）。这个函数将定义如何处理输入并将其传递给模型以获得最终期望的结果。你现在不需要担心细节。我们有一个专门的部分来学习关于签名的内容。
- en: We will circle back to signatures in a separate subsection to understand them
    in more detail. Finally, we will visually inspect model predictions by loading
    the model and feeding some data into it.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在单独的子部分回顾签名以更详细地理解它们。最后，我们将通过加载模型并向其提供一些数据来直观地检查模型预测。
- en: 15.2.1 Defining a Keras model
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.2.1 定义 Keras 模型
- en: 'The cornerstone for training the model with TFX is defining a model. There
    are two ways to define models for TFX: using the Estimator API or using the Keras
    API. We are going to go with the Keras API, as the Estimator API is not recommended
    for TensorFlow 2 (see the following sidebar for more details).'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 TFX 训练模型的基石是定义一个模型。有两种方法可以为 TFX 定义模型：使用 Estimator API 或使用 Keras API。我们将使用
    Keras API，因为 Estimator API 不推荐用于 TensorFlow 2（有关详细信息，请参见下面的侧边栏）。
- en: Estimator API vs. Keras API
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: Estimator API vs. Keras API
- en: 'My view is that going forward, Keras is probably going to be the go-to API
    for building models, and the Estimator API could perhaps be deprecated. The TensorFlow
    website says the following:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我的观点是，未来，Keras 可能会成为构建模型的首选 API，而 Estimator API 可能会被弃用。TensorFlow 网站上说：
- en: Estimators are not recommended for new code. Estimators run v1.Session-style
    code which is more difficult to write correctly, and can behave unexpectedly,
    especially when combined with TF 2 code. Estimators do fall under our compatibility
    guarantees but will receive no fixes other than security vulnerabilities. See
    the migration guide for details.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 不建议使用 Estimators 编写新代码。Estimators 运行 v1.Session 风格的代码，这更难以编写正确，并且可能表现出乎意料，特别是当与
    TF 2 代码结合使用时。Estimators 落在我们的兼容性保证下，但除了安全漏洞之外将不会收到任何修复。详情请参阅迁移指南。
- en: 'Source: [https://www.tensorflow.org/tfx/tutorials/tfx/components](https://www.tensorflow.org/tfx/tutorials/tfx/components)'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[https://www.tensorflow.org/tfx/tutorials/tfx/components](https://www.tensorflow.org/tfx/tutorials/tfx/components)
- en: 'We are first going to create a function called _build_keras_model(), which
    will do two things. First, it will create tf.feature_column-type objects for all
    the features we have defined in our Transform step. tf.feature_column is a feature
    representation standard and is accepted by models defined in TensorFlow. It is
    a handy tool for defining data in a column-oriented fashion (i.e., each feature
    represented as a column). Columnar representation is very suitable for structured
    data, where each column typically is an independent predictor for the target variable.
    Let’s examine a few specific tf.feature_column types that are found in TensorFlow:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先要创建一个名为 _build_keras_model() 的函数，它将执行两项任务。首先，它将为我们在 Transform 步骤中定义的所有特征创建
    tf.feature_column 类型的对象。tf.feature_column 是一种特征表示标准，被 TensorFlow 中定义的模型所接受。它是一种用于以列为导向的方式定义数据的便利工具（即，每个特征都表示为一列）。列式表示非常适用于结构化数据，其中每列通常是目标变量的独立预测器。让我们来看一些在
    TensorFlow 中找到的具体 tf.feature_column 类型：
- en: tf.feature_column.numeric_column—Used to represent dense floating-point fields
    like temperature.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: tf.feature_column.numeric_column——用于表示像温度这样的稠密浮点字段。
- en: tf.feature_column.categorical_column_with_identity—Used to represent categorical
    fields or bucketized fields where the value is an integer index pointing to a
    category or a bucket, such as day or month. Because the value passed to the column
    itself is the category ID, the term “identity” is used.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: tf.feature_column.categorical_column_with_identity——用于表示分类字段或桶化字段，其中值是指向类别或桶的整数索引，例如日或月。因为传递给列本身的值是类别
    ID，所以使用了“identity”这个术语。
- en: tf.feature_column.indicator_column—Converts a tf.feature_column.categorical_column_with_identity
    to a one-hot encoded representation.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: tf.feature_column.indicator_column—将tf.feature_column.categorical_column_with_identity转换为独热编码表示。
- en: tf.feature_column.embedding_column—Can be used to generate an embedding from
    an integer-based column like tf.feature_column.categorical_column_with_identity*.
    It maintains an embedding layer internally and will return the corresponding embedding,
    given an integer ID.*
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: tf.feature_column.embedding_column—可以用于从基于整数的列（如tf.feature_column.categorical_column_with_identity）生成嵌入。它在内部维护一个嵌入层，并将给定整数ID返回相应的嵌入。
- en: To see the full list, refer to [http://mng.bz/6Xeo](http://mng.bz/6Xeo). Here,
    we will use the top three types of tf.feature_columns as inputs to our to-be defined
    model. The following listing outlines how tf.feature_columns are used as inputs.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看完整列表，请参考[http://mng.bz/6Xeo](http://mng.bz/6Xeo)。在这里，我们将使用tf.feature_columns的前三种类型作为我们待定义模型的输入。以下列表概述了如何使用tf.feature_columns作为输入。
- en: Listing 15.6 Building the Keras model using feature columns
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 第15.6节 构建使用特征列的Keras模型
- en: '[PRE21]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ Define the function signature. It returns a Keras model as the output.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义函数签名。它将一个Keras模型作为输出返回。
- en: ❷ Create tf.feature_column objects for dense features.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 为密集特征创建tf.feature_column对象。
- en: ❸ Create tf.feature_column objects for the bucketized features.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 为分桶特征创建tf.feature_column对象。
- en: ❹ Create tf.feature_column objects for the categorical features.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 为分类特征创建tf.feature_column对象。
- en: ❺ Define a deep regressor model using the function.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用该函数定义一个深度回归模型。
- en: ❻ Uses the columns defined above
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 使用上面定义的列
- en: '❼ It will have two intermediate layers: 128 nodes and 64 nodes.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 它将有两个中间层：128个节点和64个节点。
- en: Let’s look at the first set of feature columns stored in real_valued_columns.
    We take transformed names of the original keys of dense floating-point valued
    columns, and for each column, we create a tf.feature_column.numeric_column. You
    can see that we are passing
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下存储在real_valued_columns中的第一组特征列。我们取密集浮点值列的原始键的转换名称，并为每列创建一个tf.feature_column.numeric_column。您可以看到我们正在传递
- en: '*A key* (string)—Name of the feature'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*键*（字符串）—特征的名称'
- en: '*A shape* (a list/tuple)—Full shape will be derived as [batch size] + shape'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*形状*（一个列表/元组）—完整形状将派生为[批量大小] + 形状'
- en: 'For example, the column temp will have the key as temp_xf and shape as (1,),
    meaning that the full shape is [batch size, 1]. This shape of [batch size, 1]
    makes sense since each dense feature has a single value per record (meaning that
    we don’t need a feature dimensionality in the shape). Let’s go through a toy example
    to see a tf.feature_column.numeric_column in action:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，列temp的键将为temp_xf，形状为(1,)，意味着完整形状为[批量大小，1]。这个形状为[批量大小，1]是有意义的，因为每个密集特征每条记录只有一个值（这意味着我们在形状中不需要特征维度）。让我们通过一个玩具例子来看看tf.feature_column.numeric_column的运作：
- en: '[PRE22]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This will output
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出
- en: '[PRE23]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: When defining tf.feature_column.categorical_column_with_identity for the bucketized
    features, you need to pass
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在为分桶特征定义tf.feature_column.categorical_column_with_identity时，您需要传递
- en: A key (string)—Name of the feature
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 键（字符串）—特征的名称
- en: num_buckets (int)—Number of buckets in the bucketized feature
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: num_buckets（整数）—分桶特征中的桶数
- en: 'For instance, the RH feature that was bucketized will have the key RH_xf and
    num_buckets = 3, where the buckets are [[-inf, 33), [33, 66), [66, inf]]. Since
    we defined the bucket boundary for RH as (33, 66), num_buckets is defined as len(boundaries)
    +1 = 3\. Finally, each categorical feature is wrapped in a tf.feature_column.indicator_column
    to convert each feature to one-hot encoded representation. Again, we can do a
    quick experiment to see the effects of these feature columns as follows:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于被分桶的RH特征，其键为RH_xf，num_buckets = 3，其中桶为[[-inf，33），[33，66），[66，inf]]。由于我们将RH的分桶边界定义为(33,
    66)，num_buckets被定义为len(boundaries) +1 = 3。最后，每个分类特征都包装在tf.feature_column.indicator_column中，以将每个特征转换为独热编码表示。同样，我们可以进行一个快速实验来查看这些特征列的效果如何：
- en: '[PRE24]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This will produce
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生
- en: '[PRE25]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Finally, the vocabulary-based categorical features are treated similarly to
    the bucketized features. For each feature, we get the feature name and the maximum
    number of categories and define a tf.feature_column.categorical_column_with_identity
    column with
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，基于词汇的分类特征与分桶特征类似处理。对于每个特征，我们获取特征名称和最大类别数，并使用tf.feature_column.categorical_column_with_identity列定义一个列，其中
- en: key (string)—Name of the feature.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 键（字符串）—特征的名称。
- en: num_buckets (int)—Number of categories.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: num_buckets（整数）—类别数。
- en: default_value (int)—If a previously unseen category is encountered, it will
    be assigned this value.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: default_value（int）—如果遇到以前看不见的类别，将分配该值。
- en: Here, default_value is an important part. It will dictate what happens to any
    unseen categories that might appear in the testing data and that weren’t a part
    of the training data. The vocabulary-based categorical features in our problem
    were day and month, which can only have 7 and 12 distinct values. But there could
    be situations where the training set only has 11 months and the test set has 12
    months. To tackle this, we will assign any unseen category to the last category
    ID (i.e., num_buckets - 1) available to us.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，默认值是一个重要的部分。它将决定测试数据中出现的任何看不见的类别会发生什么，这些类别不是训练数据的一部分。我们问题中基于词汇的分类特征是天和月，分别只能有
    7 和 12 个不同的值。但可能会出现这样的情况，训练集只有 11 个月，测试集有 12 个月。为了解决这个问题，我们将任何看不见的类别分配给我们可用的最后一个类别
    ID（即，num_buckets - 1）。
- en: We now have a collection of well-defined data columns that are wrapped in tf.feature_column
    objects ready to be fed to a model. Finally, we see a function called _dnn_regressor()
    that will create a Keras model, which is shown in the next listing, and pass the
    columns we create and some other hyperparameters. Let’s now discuss the specifics
    of this function.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一组明确定义的数据列，这些数据列包装在 tf.feature_column 对象中，准备馈送给模型。最后，我们看到一个名为 _dnn_regressor()
    的函数，它将创建一个 Keras 模型，如下图所示，并将我们创建的列和一些其他超参数传递给它。现在让我们讨论一下这个函数的具体内容。
- en: Listing 15.7 Defining the regression neural network
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.7 定义回归神经网络
- en: '[PRE26]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: ❶ Define a function that takes a bunch of columns and a list of hidden dimensions
    as the input.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义一个函数，它以一堆列和一列隐藏维度的列表作为输入。
- en: '❷ Inputs to the model: an input dictionary where the key is the feature name
    and the value is a Keras Input layer'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '❷ 模型的输入: 输入字典，其中键是特征名称，值是 Keras 输入层'
- en: ❸ Update the dictionary by creating Input layers for vocabulary-based categorical
    features.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 通过为基于词汇的分类特征创建输入层更新字典。
- en: ❹ Update the dictionary by creating Input layers for bucketized features.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 通过为分桶特征创建输入层更新字典。
- en: ❺ As input layers are defined as a dictionary, we use the DenseFeatures layer
    to generate a single tensor output.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 由于输入层被定义为字典，我们使用 DenseFeatures 层生成单一的张量输出。
- en: ❻ We recursively compute the output by creating a sequence of Dense layers.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 我们通过创建一系列稠密层来递归计算输出。
- en: ❼ Create a final regression layer that has one output node and a linear activation.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 创建一个最终的回归层，它有一个输出节点和线性激活。
- en: ❽ Define the model using inputs and outputs.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 使用输入和输出定义模型。
- en: ❾ Compile the model. Note how it uses the mean squared error as the loss function.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 编译模型。请注意它使用均方误差作为损失函数。
- en: ❿ Print a summary of the model through the absl logger we defined at the beginning.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 通过我们在开始定义的 absl 记录器打印模型的摘要。
- en: We have defined the data in a columnar fashion, where each column is a TensorFlow
    feature column. Once the data defined in this way, we use a special layer called
    tf.keras.layers.DenseFeatures *to process this data. The* DenseFeatures *layer*
    accepts
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已按列的形式定义了数据，其中每列都是 TensorFlow 特征列。定义数据后，我们使用一个特殊层叫做 tf.keras.layers.DenseFeatures
    *来处理这些数据。* DenseFeatures *层* 接受
- en: A list of feature columns
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征列列表
- en: A dictionary of tf.keras.layers.Input layers, where each Input layer is keyed
    with a column name found in the list of feature columns
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 tf.keras.layers.Input 层的字典，其中每个输入层的键都在特征列列表中找到的列名
- en: With this data, the DenseFeatures layer can map each Input layer to the corresponding
    feature column and produce a single tensor output at the end (stored in the variable
    output) (figure 15.7).
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些数据，DenseFeatures 层可以将每个输入层映射到相应的特征列，并在最后产生一个单一的张量输出（存储在变量输出中）（图 15.7）。
- en: '![15-07](../../OEBPS/Images/15-07.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![15-07](../../OEBPS/Images/15-07.png)'
- en: Figure 15.7 Overview of the functionality of the DenseFeatures layer
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.7 DenseFeatures 层功能概述
- en: Then we recursively compute the output by flowing the data through several hidden
    layers. The sizes of these hidden layers (a list of integers) are passed in as
    an argument to the function. We will use tanh nonlinear activation for the hidden
    layers. The final hidden output goes to a single node regression layer that has
    a linear activation.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们通过将数据通过几个隐藏层流动来递归计算输出。这些隐藏层的大小（一个整数列表）作为参数传递给函数。我们将使用 tanh 非线性激活作为隐藏层。最终的隐藏输出进入具有线性激活的单节点回归层。
- en: Finally, we compile the model with the Adam optimizer and mean-squared loss
    as the loss function. It is important to note that we have to use a regression-compatible
    loss function for the model. The mean-squared error is a very common loss function
    chosen for regression problems.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用 Adam 优化器和均方损失作为损失函数对模型进行编译。重要的是要注意，我们必须为模型使用与回归兼容的损失函数。均方误差是用于回归问题的非常常见的损失函数。
- en: Type hinting in Python
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: Python 中的类型提示
- en: You will see some functions defined differently than we have done in the past.
    For example, functions are defined as
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到一些函数的定义方式与我们过去所做的方式不同。例如，函数定义为
- en: '[PRE27]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: or
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 或
- en: '[PRE28]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This is visual type-hinting and is available in Python. This means that the
    types are not enforced by the Python interpreter in any way; rather, they are
    a visual cue to make sure the developer uses the correct types of inputs and outputs.
    When defining arguments in a function, you can define the type of the data expected
    for that argument using the syntax def <function>(<argument>: <type>):. For example,
    in the function run_fn(), the first argument fn_args must be of type tfx.components
    .FnArgs.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '这是 Python 中的可视类型提示，并且在 Python 中是可用的。这意味着类型不会以任何方式由 Python 解释器强制执行；相反，它们是一种视觉提示，以确保开发人员使用正确的输入和输出类型。在函数中定义参数时，可以使用以下语法定义该参数期望的数据类型
    def <function>(<argument>: <type>):。例如，在函数 run_fn() 中，第一个参数 fn_args 必须是 tfx.components.FnArgs
    类型。'
- en: 'Then you can also define the output returned by the function as def <function>
    (<argument>: <type>) -> <return type>:. For example, the returned object by _build_keras_model()
    function must be a tf.keras.Model object.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '然后，您还可以将函数返回的输出定义为 def <function> (<argument>: <type>) -> <return type>:。例如，_build_keras_model()
    函数返回的对象必须是一个 tf.keras.Model 对象。'
- en: Some objects require complex data types that need to be created using multiple
    data types or custom data types (e.g., a list of strings). For this, you can use
    a built-in Python library called typing. typing allows you to define data types
    conveniently. For more information, refer to [https://docs.python.org/3/library/typing.xhtml](https://docs.python.org/3/library/typing.xhtml).
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 有些对象需要使用多种数据类型或自定义数据类型（例如，字符串列表）创建复杂数据类型。对于这一点，您可以使用一个名为 typing 的内置 Python 库。typing
    允许您方便地定义数据类型。有关更多信息，请参阅 [https://docs.python.org/3/library/typing.xhtml](https://docs.python.org/3/library/typing.xhtml)。
- en: 'In listing 15.8, we define a function that, given a set of training data filenames
    and evaluation data filenames, generates tf.data.Dataset objects for training
    and evaluation data. We define this special function as _input_fn(). _input_fn()
    takes in three things:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表 15.8 中，我们定义了一个函数，给定一组训练数据文件名和评估数据文件名，生成用于训练和评估数据的 tf.data.Dataset 对象。我们将这个特殊函数定义为
    _input_fn()。_input_fn() 接受三个参数：
- en: file_pattern—A set of file paths, where files contain data
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: file_pattern — 一组文件路径，其中文件包含数据
- en: data_accessor—A special object in TFX that creates a tf.data.Dataset by taking
    in a list of filenames and other configuration
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: data_accessor — TFX 中的特殊对象，通过接受文件名列表和其他配置来创建 tf.data.Dataset
- en: batch_size—An integer specifying the size of a batch of data
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: batch_size — 指定数据批次大小的整数
- en: Listing 15.8 A function to generate a tf.data.Dataset using the input files
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.8 用于使用输入文件生成 tf.data.Dataset 的函数
- en: '[PRE29]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: ❶ The typing library defines the type of inputs to the function.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ typing 库定义了函数输入的类型。
- en: ❷ List of paths or patterns of input tfrecord files. It is a list of objects
    of type Text (i.e., strings).
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 输入 tfrecord 文件的路径或模式的列表。它是 Text 类型对象（即字符串）的列表。
- en: ❸ DataAccessor for converting input to RecordBatch
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ DataAccessor 用于将输入转换为 RecordBatch
- en: ❹ A TFTransformOutput
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 一个 TFTransformOutput
- en: ❺ Represents the number of consecutive elements of the returned data set to
    combine in a single batch
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 表示要合并为单个批次的返回数据集的连续元素的数量
- en: You can see how we are using type hints for the arguments as well as the return
    object. The function returns a tf.data.Dataset obtained by calling the tf_dataset_factory()
    function with a list of file paths and data set options like batch size and label
    key. The label key is important for the data_accessor to determine input fields
    and the target. You can see that the data accessor takes in the schema from the
    Transform step as well. This helps the data_accessor to transform the raw examples
    to features and then separate the inputs and the label. With all the key functions
    explained, we now move on to see how all of these will be orchestrated in order
    to do the model training.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到我们如何使用类型提示来标记参数以及返回对象。该函数通过调用 tf_dataset_factory() 函数获取 tf.data.Dataset，该函数使用文件路径列表和数据集选项（如批量大小和标签键）进行调用。标签键对于
    data_accessor 来说非常重要，因为它能确定输入字段和目标。您可以看到 data_accessor 也需要从 Transform 步骤获取模式。这有助于
    data_accessor 将原始示例转换为特征，然后分离输入和标签。在解释了所有关键函数之后，我们现在继续看看所有这些将如何被编排以进行模型训练。
- en: 15.2.2 Defining the model training
  id: totrans-336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.2.2 定义模型训练
- en: The main task that’s standing between us and a train model is the actual training
    of the model. The TFX component responsible for training the model (known as Trainer)
    expects a special function named run_fn() that will tell how the model should
    be trained and eventually saved (listing 15.9). This function takes in a special
    type of object called FnArgs, a utility object in TensorFlow that can be used
    to declare model training-related user-defined arguments that need to be passed
    to a model training function.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要做的主要任务是模型的实际训练。负责模型训练的 TFX 组件（称为 Trainer）期望有一个名为 run_fn() 的特殊函数，该函数将告诉模型应该如何被训练和最终保存（见清单
    15.9）。这个函数接受一个特殊类型的对象 called FnArgs，这是 TensorFlow 中的一个实用对象，可以用来声明需要传递给模型训练函数的与模型训练相关的用户定义参数。
- en: Listing 15.9 Running the Keras model training with the data
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 15.9 运行 Keras 模型训练与数据。
- en: '[PRE30]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: ❶ Define a function called run_fn that takes a tfx.components.FnArgs object
    as the input.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义一个名为 run_fn 的函数，该函数以 tfx.components.FnArgs 对象作为输入。
- en: ❷ Log the values in the fn_args object.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 记录 fn_args 对象中的值。
- en: ❸ Load the tensorflow_transform graph.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 加载 tensorflow_transform 图。
- en: ❹ Convert the data in the CSV files to tf.data.Dataset objects using the function
    _input_fn (discussed soon).
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用函数 _input_fn（即将讨论）将 CSV 文件中的数据转换为 tf.data.Dataset 对象。
- en: ❺ Build the Keras model using the previously defined function.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用先前定义的函数构建 Keras 模型。
- en: ❻ Define a directory to store CSV logs produced by the Keras callback CSVLogger.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 定义一个目录来存储 Keras 回调 CSVLogger 生成的 CSV 日志。
- en: ❼ Define the CSVLogger callback.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 定义 CSVLogger 回调。
- en: ❽ Fit the model using the data sets created and the hyperparameters present
    in the fn_args object.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 使用创建的数据集和 fn_args 对象中存在的超参数来拟合模型。
- en: ❾ Define signatures for the model. Signatures tell the model what to do when
    data is sent via an API call when the model is deployed.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 为模型定义签名。签名告诉模型在模型部署时通过 API 调用时该做什么。
- en: ❿ Save the model to the disk.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 将模型保存到磁盘。
- en: Let’s first check the method signature of the run_fn(). run_fn() takes in a
    single argument of type FnArgs as the input. As mentioned earlier, FnArgs is a
    utility object that stores a collection of key-value pairs that are useful for
    model training. Most of the elements in this object are populated by the TFX component
    itself. However, you also have the flexibility to pass some of the values. We
    will define some of the most important attributes in this object. But we will
    learn more about the full list of attributes once we see the full output produced
    by the TFX Trainer component. Table 15.2 provides you a taste of what is stored
    in this object. Don’t worry if you don’t fully understand the purpose of these
    elements. It will be clearer as we go through the chapter. Once we run the Trainer
    component, it will display the values used for every one of these attributes,
    as we have included logging statements to log the fn_args object. This will help
    us to contextualize these properties with the example we’re running and understand
    them more clearly.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先检查run_fn()的方法签名。run_fn()接受一个FnArgs类型的单一参数作为输入。如前所述，FnArgs是一个实用对象，它存储了对模型训练有用的键值对集合。这个对象中的大部分元素是由TFX组件本身填充的。不过，你也有灵活性传递一些值。我们将定义这个对象中一些最重要的属性。但是一旦我们看到TFX
    Trainer组件生成的完整输出，我们将学习更多关于这个对象的属性列表。表15.2为你提供了这个对象中存储的内容的概览。如果你对这些元素的用途不是很理解，不要担心。随着我们的学习，它们会变得更清晰。一旦我们运行Trainer组件，它将显示用于每一个属性的值，因为我们在其中包含了记录语句来记录fn_args对象。这将帮助我们对当前运行的示例将这些属性进行上下文化，并更清晰地理解它们。
- en: Table 15.2 An overview of the attributes stored in the fn_args-type object
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 表15.2 fn_args类型对象中存储的属性概览
- en: '| **Attribute** | **Description** | **Example** |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| **属性** | **描述** | **示例** |'
- en: '| train_files | A list of train filenames | [''.../Transform/transformed_examples/16/Split-train/*''],
    |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| train_files | 训练文件名列表 | [''.../Transform/transformed_examples/16/Split-train/*''],
    |'
- en: '| eval_files | A list of evaluation/validation filenames | [''.../Transform/transformed_examples/16/Split-eval/*'']
    |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| eval_files | 评估/验证文件名列表 | [''.../Transform/transformed_examples/16/Split-eval/*'']
    |'
- en: '| train_steps | Number of training steps | 100 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| train_steps | 训练步数 | 100 |'
- en: '| eval_steps | Number of evaluation/validation steps | 100 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| eval_steps | 评估/验证步数 | 100 |'
- en: '| schema_path | Path to the schema generated by the TFX component SchemaGen
    | ''.../SchemaGen/schema/15/schema.pbtxt'' |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| schema_path | TFX组件SchemaGen生成的模式路径 | ''.../SchemaGen/schema/15/schema.pbtxt''
    |'
- en: '| transform_graph_path | Path to the transform graph generated by the TFX component
    Transform | ''.../SchemaGen/schema/15/schema.pbtxt'' |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| transform_graph_path | TFX组件Transform生成的转换图路径 | ''.../SchemaGen/schema/15/schema.pbtxt''
    |'
- en: '| serve_model_dir | Output directory where the serve-able model will be saved
    | ''.../Trainer/model/17/Format-Serving'' |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| serve_model_dir | 存储可提供服务的模型的输出目录 | ''.../Trainer/model/17/Format-Serving''
    |'
- en: '| model_run_dir | Output directory where the model is saved | ''.../Trainer/model_run/17''
    |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| model_run_dir | 存储模型的输出目录 | ''.../Trainer/model_run/17'' |'
- en: The first important task done by this function is generating tf.data.Dataset
    objects for training and evaluation data. We have defined a special function called
    _input_fn() that achieves this (listing 15.8).
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数完成的第一个重要任务是为训练和评估数据生成tf.data.Dataset对象。我们定义了一个特殊的函数叫做_input_fn()来实现这个功能（见15.8节）。
- en: Once the data sets are defined, we define the Keras model using the _build_keras_model()
    function we discussed earlier. Then we define a CSVLogger callback to log the
    performance metrics over epochs, as we did earlier. As a brief review, the tf.keras.callbacks.CSVLogger
    creates a CSV file with all the losses and metrics defined during model compilation,
    logged every epoch. We will use the fn_arg object’s model_run_dir attribute to
    create a path for the CSV file inside the model creation directory. This will
    make sure that if we run multiple training trials, each will have its own CSV
    file saved along with the model. After that, we call the model.fit() function
    as we have done countless times. The arguments we have used are straightforward,
    so we will not discuss them in detail and lengthen this discussion unnecessarily.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了数据集之后，我们使用之前讨论过的 _build_keras_model() 函数定义 Keras 模型。然后我们定义了一个 CSVLogger 回调函数来记录性能指标随时间的变化，就像我们之前做的那样。简要回顾一下，tf.keras.callbacks.CSVLogger
    会在模型编译期间创建一个 CSV 文件，记录每个周期的所有损失和指标。我们将使用 fn_arg 对象的 model_run_dir 属性来为 CSV 文件创建一个路径，该路径位于模型创建目录内。这样，如果我们运行多个训练试验，每个试验都将与模型一起保存其自己的
    CSV 文件。之后，我们像之前无数次那样调用 model.fit() 函数。我们使用的参数很简单，所以我们不会详细讨论它们，也不会不必要地延长这个讨论。
- en: '15.2.3 SignatureDefs: Defining how models are used outside TensorFlow'
  id: totrans-363
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.2.3 SignatureDefs：定义模型在 TensorFlow 外部的使用方式
- en: Once the model is trained, we have to store the model on disk so that it can
    be reused later. The objective of storing this model is to use this via a web-based
    API (i.e., a REST API) to query the model using inputs and get predictions out.
    This is typically how machine learning models are used to serve customers in an
    online environment. For models to understand web-based requests, we need to define
    things called SignatureDefs. A signature defines things like what an input or
    target to the model looks like (e.g., data type). You can see that we have defined
    a dictionary called signatures and passed it as an argument to model.save()(listing
    15.9).
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，我们必须将模型存储在磁盘上，以便以后可以重用。存储此模型的目的是通过基于 Web 的 API（即 REST API）来查询模型使用输入并获取预测结果。这通常是在在线环境中为客户提供服务的机器学习模型的使用方式。为了让模型理解基于
    Web 的请求，我们需要定义称为 SignatureDefs 的东西。签名定义了模型的输入或目标是什么样子的（例如，数据类型）。您可以看到我们定义了一个叫做
    signatures 的字典，并将其作为参数传递给 model.save()（清单 15.9）。
- en: The signatures dictionary should have key-value pairs, where key is a signature
    name and value is a function decorated with the @tf.function decorator. If you
    want a quick refresher on what this decorator does, read the following sidebar.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: signatures 字典应该有键值对，其中键是签名名称，值是使用 @tf.function 装饰器装饰的函数。如果您想快速回顾一下此装饰器的作用，请阅读下面的侧边栏。
- en: The @tf.function decorator
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '@tf.function 装饰器'
- en: The @tf.function decorator takes in a function that performs various TensorFlow
    operations with TensorFlow operands, and then traces all the steps and turns that
    into a data-flow graph. In most cases, TensorFlow requires a data-flow graph showing
    how inputs and outputs are connected between operations. Though in TensorFlow
    1.x you had to explicitly build this graph, TensorFlow 2.x onward doesn’t encumber
    the developer with this responsibility. Whenever a function is decorated with
    the @tf.function decorator, it builds the data-flow graph for us.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '@tf.function 装饰器接受一个执行各种 TensorFlow 操作的函数，该函数使用 TensorFlow 操作数，然后跟踪所有步骤并将其转换为数据流图。在大多数情况下，TensorFlow
    需要显示输入和输出如何在操作之间连接的数据流图。尽管在 TensorFlow 1.x 中，您必须显式构建此图，但 TensorFlow 2.x 以后不再让开发人员负责此责任。每当一个函数被
    @tf.function 装饰器装饰时，它会为我们构建数据流图。'
- en: 'It is also important to note that you cannot use arbitrary names as signature
    names. TensorFlow has a set of defined signature names, depending on your needs.
    These are defined in a special constant module in TensorFlow ([http://mng.bz/o2Kd](http://mng.bz/o2Kd)).
    There are four signatures to choose from:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，您不能将任意名称用作签名名称。TensorFlow 有一组根据您的需求定义的签名名称。这些在 TensorFlow 的特殊常量模块中定义（[http://mng.bz/o2Kd](http://mng.bz/o2Kd)）。有四种签名可供选择：
- en: 'PREDICT_METHOD_NAME (value: ''tensorflow/serving/predict'')—This signature
    is used to predict the target for incoming inputs. This does not expect the target
    to be present.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PREDICT_METHOD_NAME（值：'tensorflow/serving/predict'）—这个签名用于预测传入输入的目标。这不期望目标存在。
- en: 'REGRESS_METHOD_NAME (value: ''tensorflow/serving/regress'')—This signature
    can be used to regress from an example. It expects both an input and an output
    (i.e., target value) to be present in the HTTP request body.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: REGRESS_METHOD_NAME（值为 'tensorflow/serving/regress'）——此签名可用于从示例进行回归。它期望 HTTP
    请求体中同时存在输入和输出（即目标值）。
- en: 'CLASSIFY_METHOD_NAME (value: ''tensorflow/serving/classify'')—This is similar
    to REGRESS_METHOD_NAME, except for classification. This signature can be used
    to classify an example. It expects both an input and an output (i.e., target value)
    to be present in the HTTP.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CLASSIFY_METHOD_NAME（值为 'tensorflow/serving/classify'）——与 REGRESS_METHOD_NAME
    类似，但用于分类。此签名可用于分类示例。它期望 HTTP 请求中同时存在输入和输出（即目标值）。
- en: 'DEFAULT_SERVING_SIGNATURE_DEF_KEY (value: ''serving_default'')—This is the
    default signature name. A model should at least have the default serving signature
    in order to be used via an API. If none of the other signatures are defined, requests
    will go through this signature.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DEFAULT_SERVING_SIGNATURE_DEF_KEY（值为 'serving_default'）——这是默认签名名称。模型至少应该有默认的服务签名才能通过
    API 使用。如果没有定义其他签名，则请求将经过此签名。
- en: We will only define the default signature here. Signatures take a TensorFlow
    function (i.e., a function decorated with @tf.function) as a value. Therefore,
    we need to define a function (which we will call _get_serve_tf_examples_fn() )
    that will tell TensorFlow what to do with an input (see the next listing).
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只定义了默认签名。签名采用 TensorFlow 函数（即用 @tf.function 装饰的函数）作为值。因此，我们需要定义一个函数（我们将其称为
    _get_serve_tf_examples_fn() ），以告诉 TensorFlow 对输入做什么（请参见下一个清单）。
- en: Listing 15.10 Parsing examples sent through API requests and predicting from
    them
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 15.10 解析通过 API 请求发送的示例并从中进行预测。
- en: '[PRE31]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: ❶ Returns a function that parses a serialized tf.Example and applies feature
    transformations
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 返回一个函数，该函数解析序列化的 tf.Example 并应用特征转换。
- en: ❷ Get the feature transformations to be performed as a Keras layer.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 以 Keras 层的形式获取要执行的特征转换。
- en: ❸ The function decorated with @tf.function to be returned
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 被 @tf.function 装饰的函数将被返回。
- en: ❹ Get the raw column specifications.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 获取原始列规范。
- en: ❺ Remove the feature spec for the label as we do not want that during predictions.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 删除标签的特征规范，因为我们不需要在预测中使用它。
- en: ❻ Parse the serialized example using the feature specifications.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 使用特征规范解析序列化示例。
- en: ❼ Convert raw columns to features using the layer defined.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 使用定义的层将原始列转换为特征。
- en: ❽ Return the output of the model after feeding the transformed features.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 在提供转换后的特征之后返回模型的输出。
- en: ❾ Return the TensorFlow function.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 返回 TensorFlow 函数。
- en: 'The first important thing to note is that _get_serve_tf_examples_fn() returns
    a function (i.e., serve_tf_examples_fn), which is a TensorFlow function. The _get_serve_tf_examples_fn()
    accepts two inputs:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要注意的一件重要事情是，_get_serve_tf_examples_fn() 返回一个函数（即 serve_tf_examples_fn ），它是
    TensorFlow 函数。_get_serve_tf_examples_fn() 接受两个输入：
- en: Model—The Keras model we built during training time
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Model — 我们在训练时建立的 Keras 模型。
- en: tf_transform_output—The transformation graph to convert raw data to features
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: tf_transform_output——将原始数据转换为特征的转换图。
- en: This returned function should instruct TensorFlow on what to do with the data
    that came in through an API request once the model is deployed. The returned function
    takes serialized examples as inputs, parses them to be in the correct format as
    per the model input specifications, generates the output, and returns it. We will
    not dive too deeply into what the inputs and outputs are of this function, as
    we will not call it directly, but rather access TFX, which will access it when
    an API call is made.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 返回函数应指示 TensorFlow 在模型部署后通过 API 请求传入的数据要执行什么操作。返回的函数以序列化示例作为输入，将它们解析为符合模型输入规范的正确格式，生成并返回输出。我们不会深入解析此功能的输入和输出，因为我们不会直接调用它，而是访问
    TFX，在 API 调用时将访问它。
- en: 'In this process, the function first gets a raw feature specifications map,
    which is a dictionary of column names mapped to a Feature type. The Feature type
    describes the type of data that goes in a feature. For instance, for our data,
    the feature spec will look like this:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，函数首先获得原始特征规范映射，这是一个列名映射到 Feature 类型的字典。Feature 类型描述了放入特征中的数据类型。例如，对于我们的数据，特征规范将是这样的：
- en: '[PRE32]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'It can be observed that different data types are used (e.g., float, int, string)
    depending on the data found in that column. You can see a list of feature types
    at [https://www.tensorflow.org/api_docs/python/tf/io/](https://www.tensorflow.org/api_docs/python/tf/io/).
    Next, we remove the feature having the _LABEL_KEY as it should not be a part of
    the input. We then use the tf.io.parse_example() function to parse the serialized
    examples by passing the feature specification map. The results are passed to a
    TransformFeaturesLayer ([http://mng.bz/nNRa](http://mng.bz/nNRa)) that knows how
    to convert a set of parsed examples to a batch of inputs, where each input has
    multiple features. Finally, the transformed features are passed to the model,
    which returns the final output (i.e., predicted forest burnt area). Let’s revisit
    the signature definition from listing 15.9:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 可以观察到，根据该列中的数据使用了不同的数据类型（例如 float、int、string）。您可以在 [https://www.tensorflow.org/api_docs/python/tf/io/](https://www.tensorflow.org/api_docs/python/tf/io/)
    上看到一列特征类型的列表。接下来，我们删除了具有 _LABEL_KEY 的特征，因为它不应该是输入的一部分。然后我们使用 tf.io.parse_example()
    函数通过传递特征规范映射来解析序列化的示例。结果被传递给 TransformFeaturesLayer ([http://mng.bz/nNRa](http://mng.bz/nNRa))，它知道如何将一组解析后的示例转换为一批输入，其中每个输入具有多个特征。最后，转换后的特征被传递给模型，该模型返回最终输出（即，预测的森林烧毁面积）。让我们重新审视列表
    15.9 中的签名定义：
- en: '[PRE33]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'You can see that we are not simply passing the returning TensorFlow function
    of _get_serve_tf_examples_fn(). Instead, we call the get_concrete_function() on
    the return function (i.e., TensorFlow function). If you remember from our previous
    discussions, when you execute a function decorated with @tf.function, it does
    two things:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，我们并不只是简单地传递 _get_serve_tf_examples_fn() 的返回 TensorFlow 函数。相反，我们在返回函数（即
    TensorFlow 函数）上调用了 get_concrete_function()。如果您还记得我们之前的讨论，当您执行带有 @tf.function 装饰的函数时，它会执行两件事：
- en: Traces the function and creates the data-flow graph to perform the work of the
    function
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 追踪函数并创建数据流图以执行函数的工作
- en: Executes the graph to return outputs
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行图以返回输出
- en: get_concrete_function() does the first task only. In other words, it returns
    the traced function. You can read more about this at [http://mng.bz/v6K7](http://mng.bz/v6K7).
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: get_concrete_function() 只做第一个任务。换句话说，它返回了追踪的函数。您可以在 [http://mng.bz/v6K7](http://mng.bz/v6K7)
    上阅读更多相关内容。
- en: 15.2.4 Training the Keras model with TFX Trainer
  id: totrans-397
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 TFX Trainer 训练 Keras 模型 15.2.4
- en: 'We now have all the bells and whistles to train the model. To reiterate, we
    first defined a Keras model, defined a function to run the model training, and
    finally defined signatures that instruct the model how to behave when an HTTP
    request is sent via the API. Now we will train the model as a part of the TFX
    pipeline. To train the model, we are going to use the TFX Trainer component:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有了训练模型的所有必要条件。再次强调，我们首先定义了一个 Keras 模型，定义了一个运行模型训练的函数，最后定义了指令，告诉模型当通过 API
    发送 HTTP 请求时应该如何行事。现在我们将在 TFX 流水线的一部分中训练模型。为了训练模型，我们将使用 TFX Trainer 组件：
- en: '[PRE34]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The code leading up to the Trainer component simply computes the correct number
    of iterations required in an epoch. To calculate that, we first get the total
    size of the data (remember that we stored our data set in the DataFrame df). We
    then used two hash buckets for training and one for evaluation. Therefore, we
    would have roughly two-thirds training data and one-third evaluation data. Finally,
    if the value is not fully divisible, we add +1 to incorporate the remainder of
    the data.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Trainer 组件之前的代码只是计算了一个周期中所需的正确迭代次数。为了计算这个值，我们首先得到了数据的总大小（记住我们将数据集存储在 DataFrame
    df 中）。然后我们为训练使用了两个哈希桶，评估使用了一个哈希桶。因此，我们大约有三分之二的训练数据和三分之一的评估数据。最后，如果值不能完全被整除，我们就会加上
    +1 来包含数据的余数。
- en: 'Let’s investigate the instantiation of the Trainer component in more detail.
    There are several important arguments to pass to the constructor:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地研究 Trainer 组件的实例化。有几个重要的参数需要传递给构造函数：
- en: module_file—Path to the Python module containing the run_fn().
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: module_file——包含 run_fn() 的 Python 模块的路径。
- en: transformed_examples—Output of the TFX Transform step, particularly the transformed
    examples.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: transformed_examples——TFX Transform 步骤的输出，特别是转换后的示例。
- en: schema—Output of the TFX SchemaGen step.
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: schema——TFX SchemaGen 步骤的输出。
- en: train_args—A TrainArgs object specifying training-related arguments. (To see
    the proto message defined for this object, see [http://mng.bz/44aw](http://mng.bz/44aw).)
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: train_args——指定与训练相关的参数的 TrainArgs 对象。（要查看为该对象定义的 proto 消息，请参见 [http://mng.bz/44aw](http://mng.bz/44aw)。）
- en: eval_args—An EvalArgs object specifying evaluation-related arguments. (To see
    the proto message defined for this object, see [http://mng.bz/44aw](http://mng.bz/44aw).)
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: eval_args—一个指定评估相关参数的 EvalArgs 对象。（要查看为此对象定义的 proto 消息，请参见[http://mng.bz/44aw](http://mng.bz/44aw)。）
- en: 'This will output the following log. Due to the length of the log output, we
    have truncated certain parts of the log messages:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下日志。由于日志输出的长度，我们已经截断了某些日志消息的部分：
- en: '[PRE35]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: In the log message, we can see that the Trainer does a lot of heavy lifting.
    First, it creates a wheel package using the model training code defined in the
    forest_fires_trainer.py. wheel (extension .whl) is how Python would package a
    library. For instance, when you do pip install tensorflow, it will first download
    the wheel package with the latest version and install it locally. If you have
    a locally downloaded wheel package, you can use pip install <path to wheel>. You
    can find the resulting wheel package at the <path to pipeline root>/examples/forest_fires_pipeline/_wheels
    directory. Then it prints the model summary. It has an Input layer for every feature
    passed to the model. You can see that the DenseFeatures layer aggregates all these
    Input layers to produce a [None, 31]-sized tensor. As the final output, the model
    produces a [None, 1]-sized tensor. Then the model training takes place. You will
    see warnings such as
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 在日志消息中，我们可以看到 Trainer 做了大量的繁重工作。首先，它使用 forest_fires_trainer.py 中定义的模型训练代码创建一个
    wheel 包。wheel（扩展名为 .whl）是 Python 打包库的方式。例如，当你执行 pip install tensorflow 时，它会首先下载带有最新版本的
    wheel 包并在本地安装。如果你有一个本地下载的 wheel 包，你可以使用 pip install <wheel 的路径>。你可以在 <pipeline
    根目录路径>/examples/forest_fires_pipeline/_wheels 目录中找到生成的 wheel 包。然后它打印模型摘要。它为传递给模型的每个特征都有一个输入层。你可以看到
    DenseFeatures 层聚合了所有这些输入层，以生成一个 [None, 31] 大小的张量。作为最终输出，模型产生了一个 [None, 1] 大小的张量。然后进行模型训练。你会看到警告，比如
- en: '[PRE36]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: This warning comes up when TensorFlow function tracing happens too many times.
    It can be a sign of poorly written code (e.g., the model getting recreated many
    times within a loop) and is sometimes unavoidable. In our case, it’s the latter.
    The behavior of the Trainer module is causing this behavior, and there’s not much
    we can do about that. Finally, the component writes the model as well as some
    utilities to a folder in the pipeline root. Here’s what our pipeline root directory
    looks like so far (figure 15.8).
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 当 TensorFlow 函数跟踪发生太多次时，就会出现这个警告。这可能是代码编写不佳的迹象（例如，模型在循环内部被重建多次），有时是不可避免的。在我们的案例中，是后者。Trainer
    模块的行为导致了这种行为，我们对此无能为力。最后，组件将模型以及一些实用工具写入到管道根目录的一个文件夹中。到目前为止，我们的管道根目录看起来是这样的（图
    15.8）。
- en: '![15-08](../../OEBPS/Images/15-08.png)'
  id: totrans-412
  prefs: []
  type: TYPE_IMG
  zh: '![15-08](../../OEBPS/Images/15-08.png)'
- en: Figure 15.8 The complete directory/file structure after running the Trainer
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.8 运行 Trainer 后的完整目录/文件结构
- en: A major issue we can note in the Trainer’s output log is the training and validation
    losses. For this problem, they are quite large. We are using the mean-squared
    error that is computed as
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Trainer 的输出日志中，我们可以注意到一个主要问题是训练和验证损失。对于这个问题，它们相当大。我们使用的是计算得出的均方误差。
- en: '![15_08a](../../OEBPS/Images/15_08a.png)'
  id: totrans-415
  prefs: []
  type: TYPE_IMG
  zh: '![15_08a](../../OEBPS/Images/15_08a.png)'
- en: 'where N is the number of examples, *y*[i] is the *i*^(th) example, and *ŷ*[1]
    is the predicted value for *i*^(th) example. At the end of the training, we have
    a squared loss of around 481, meaning an error of around 22 hectares (i.e., 0.22
    km²) per example. This is not a small error. If you investigate this matter, you
    will realize this is largely caused by anomalies present in the data. Some anomalies
    are so large that they can skew the model heavily in the wrong direction. We will
    address this in an upcoming section in the chapter. You will be able to see the
    values in the FnArgs object passed to the run_fn():'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 N 是示例的数量，*y*[i] 是第 *i* 个示例，*ŷ*[i] 是第 *i* 个示例的预测值。在训练结束时，我们的平方损失约为 481，意味着每个示例约有
    22 公顷（即 0.22 平方公里）的误差。这不是一个小错误。如果你调查这个问题，你会意识到这主要是由数据中存在的异常引起的。有些异常是如此之大，以至于它们可能会使模型严重偏离正确方向。我们将在本章的一个即将到来的部分中解决这个问题。你将能够看到传递给
    run_fn() 的 FnArgs 对象中的值：
- en: '[PRE37]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The following sidebar discusses how we can evaluate the model at this point
    in our discussion.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 以下侧边栏讨论了我们在本讨论中的这一点上如何评估模型。
- en: Evaluating the saved model
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 评估保存的模型
- en: 'In the pipeline, our model will be served via an HTTP interface in the form
    of URLs. But rather than waiting to do that, let’s load the model manually and
    use it to predict data. Doing so will provide us with two advantages:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 在流水线中，我们的模型将以URL形式通过HTTP接口提供服务。但是与其等待不如手动加载模型并用它来预测数据。这样做将为我们提供两个优势：
- en: Verifying the model is working as intended
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证模型是否按预期工作
- en: Providing a deeper understanding of the format of the inputs and outputs of
    the model
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供对模型输入和输出格式的深入理解
- en: We will not go into details about this in the book to keep our discussion focused
    on the pipeline. However, the code has been provided in the tfx/15.1_MLOps_with_
    tensorflow.ipynb notebook so you can experiment with it.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会在本书中详细介绍这个问题，以保持我们讨论的重点在流水线上。但是，已在tfv/15.1_MLOps_with_tensorflow.ipynb笔记本中提供了代码，因此您可以进行实验。
- en: Next, we will discuss how we can detect anomalies present in the data and remove
    them to create a clean data set to train our model.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论如何检测数据中存在的异常并将其移除，以创建一个干净的数据集来训练我们的模型。
- en: Detecting and removing anomalies
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 检测和移除异常值
- en: Our model is currently showing a validation loss of around 568\. The loss used
    here is the mean-squared error. We have already seen that this means every prediction
    is 24 hectares (i.e., 0.24 km²) off. This is no negligible matter. There are lots
    of outliers in our data, which could be a key reason we’re seeing such large error
    margins. The following figure shows the statistics graph we created earlier.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型目前显示的验证损失约为568。这里使用的损失是均方误差。我们已经看到，这意味着每个预测偏差24公顷（即0.24平方公里）。这不是一个可以忽略的问题。我们的数据中有很多异常值，这可能是我们看到如此大的误差边界的一个关键原因。以下图显示了我们早期创建的统计图。
- en: '![15-08-unnumb](../../OEBPS/Images/15-08-unnumb.png)'
  id: totrans-427
  prefs: []
  type: TYPE_IMG
  zh: '![15-08-unnumb](../../OEBPS/Images/15-08-unnumb.png)'
- en: The summary statistics graphs generated for the data by the StatisticsGen component
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 由StatisticsGen组件为数据生成的摘要统计图
- en: You can see that some columns are heavily skewed. For example, the feature FFMC
    has the highest density, around 80-90, but has a range of 18.7-96.2.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到一些列严重偏斜。例如，特征FFMC具有最高的密度，约为80-90，但范围为18.7-96.2。
- en: 'To tackle this issue, we will use the tensorflow_data_validation (abbreviated
    as tfdv) library. It provides valuable functions like tfdv.validate_statistics(),
    which can be used to validate data against the data schema we generated earlier,
    as well as the tfdv.display_anomalies() function to list the anomalous samples.
    Furthermore, we can edit the schema in order to modify the criteria for outliers.
    For example, to change the maximum value allowed for the ISI feature, you can
    do the following:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们将使用tensorflow_data_validation（缩写为tfdv）库。它提供了有用的功能，如tfdv.validate_statistics()，可用于根据我们之前生成的数据模式验证数据，以及tfdv.display_anomalies()函数，以列出异常样本。此外，我们可以编辑模式以修改异常值的标准。例如，要更改允许的ISI特征的最大值，您可以执行以下操作：
- en: '[PRE38]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Finally, you can visualize original data versus cleaned data using the tfdv.visualize_statistics()
    function. Finally, you can use the ExampleValidator object ([http://mng.bz/XZxv](http://mng.bz/XZxv))
    from the TFX pipeline to make sure there are no anomalies in your data set.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您可以使用tfdv.visualize_statistics()函数可视化原始数据与清理后的数据。最后，您可以使用TFX流水线中的ExampleValidator对象（[http://mng.bz/XZxv](http://mng.bz/XZxv)）确保数据集中没有异常。
- en: Once you run this, you should get a smaller loss than previously. For example,
    in this experiment, a loss of ~150 on average was observed. This is a 75% reduction
    of the previous error. You can find the code for this in the tfx/15.1_MLOps_ with_tensorflow.ipynb
    notebook.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此操作后，您应该比以前得到更小的损失。例如，在这个实验中，平均观察到了约150的损失。这是之前错误的75%减少。您可以在tfv/15.1_MLOps_with_tensorflow.ipynb笔记本中找到此代码。
- en: Next, we’ll look at a technology called Docker that is used for deploying models
    in isolated and portable environments. We will see how we can deploy our model
    in what is known as a Docker container.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看一看一种名为Docker的技术，该技术用于在隔离且便携的环境中部署模型。我们将看看如何将我们的模型部署在所谓的Docker容器中。
- en: Exercise 2
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 练习2
- en: Instead of using one-hot encoding for day and month features and appending them
    to the categorical_columns variable, let’s imagine you want to use embeddings
    to represent these features. You can use the feature column tf.feature_column.embedding_column
    for this. Assume an embedding dimensionality of 32\. You have the feature names
    of day and month columns stored in _VOCAB_FEATURE_KEYS (contains ['day', 'month'])
    and their dimensionality in _MAX_CATEGORICAL_FEATURE_VALUES (contains [7, 12]).
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是使用one-hot编码来表示日和月的特征，并将它们追加到categorical_columns变量中，让我们假设你想使用嵌入来表示这些特征。您可以使用特征列tf.feature_column.embedding_column来完成这个任务。假设嵌入的维度是32。你有存储在_VOCAB_FEATURE_KEYS中的特征名称（包括['day',
    'month']）以及存储在_MAX_CATEGORICAL_FEATURE_VALUES中的维度（包括[7, 12]）。
- en: 15.3 Setting up Docker to serve a trained model
  id: totrans-437
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.3 设置Docker以提供经过训练的模型
- en: You have developed a data pipeline and a robust model that can be used to predict
    the severity of forest fires based on the weather data. Now you want to go a step
    further and offer this as a more accessible service by deploying the model on
    a machine and enabling access through a REST API. This process is also known as
    productionizing a machine learning model. To do that, you are first going to create
    an isolated environment dedicated to model serving. The technology you will use
    is Docker.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经开发了一个数据管道和一个强大的模型，可以根据天气数据预测森林火灾的严重程度。现在，您希望更进一步，通过在一台机器上部署模型并通过REST API提供更易访问的服务，这个过程也称为生产化机器学习模型。为了做到这一点，您首先要创建一个专门用于模型服务的隔离环境。您将使用的技术是Docker。
- en: 'CAUTION It is vitally important that you have Docker installed on your machine
    before proceeding further. To install Docker, follow the guide: [https://docs.docker.com/engine/install/ubuntu/](https://docs.docker.com/engine/install/ubuntu/).'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在继续之前，确保您的计算机上已经安装了Docker。要安装Docker，请按照此指南：[https://docs.docker.com/engine/install/ubuntu/](https://docs.docker.com/engine/install/ubuntu/)。
- en: In TFX, you can deploy your model as a container, where the container is provisioned
    by Docker. According to the official Docker website, a Docker container is
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 在TFX中，你可以将你的模型部署为一个容器，而这个容器是由Docker提供的。根据官方Docker网站的说法，Docker容器是
- en: a standard unit of software that packages up code and all its dependencies so
    the application runs quickly and reliably from one computing environment to another.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 软件的标准单元，它打包了代码和所有的依赖项，以便应用程序可以在一个计算环境中快速、可靠地运行，并在另一个计算环境中运行。
- en: 'Source: [https://www.docker.com/resources/what-container](https://www.docker.com/resources/what-container)'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 源：[https://www.docker.com/resources/what-container](https://www.docker.com/resources/what-container)
- en: Docker is a containerization technology that helps you run a software (or a
    microservice) isolated from the host. In Docker, you can create an image, which
    will instruct Docker with various specifications (e.g., OS, libraries, dependencies)
    that you need in the container for it to run the software correctly. Then a container
    is simply a run time instance of that image. This means you enjoy a higher portability
    as you can create a container on one computer and run it on another computer easily
    (as long as Docker is installed on two computers). Virtual machines (VMs) also
    try to achieve a similar goal. There are many resources out there comparing and
    contrasting Docker containers and VMs (e.g., [http://mng.bz/yvNB](http://mng.bz/yvNB)).
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: Docker是一种容器技术，它可以帮助您在主机上隔离运行软件（或微服务）。在Docker中，您可以创建一个镜像，该镜像将使用各种规格（例如操作系统、库、依赖项）指示Docker需要在容器中以正确运行软件。然后，容器就是该镜像的运行时实例。这意味着您可以在一个计算机上创建一个容器，并且可以轻松地在另一台计算机上运行它（只要两台计算机上都安装了Docker）。虚拟机（VMs）也试图实现类似的目标。有许多资源可以比较和对比Docker容器和虚拟机（例如，[http://mng.bz/yvNB](http://mng.bz/yvNB)）。
- en: 'As we have said, to run a Docker container, you first need a Docker image.
    Docker has a public image registry (known as Docker Hub) available at [https://hub.docker.com/](https://hub.docker.com/).
    The Docker image we are looking for is the TensorFlow serving image. This image
    has everything installed to serve a TensorFlow model, using the TensorFlow serving
    ([https://github.com/tensorflow/serving](https://github.com/tensorflow/serving)),
    a sub-library in TensorFlow that can create a REST API around a given model so
    that you can send HTTP requests to use the model. You can download this image
    simply by running the following command:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所说，要运行一个 Docker 容器，首先需要一个 Docker 镜像。Docker 有一个公共镜像注册表（称为 Docker Hub），位于 [https://hub.docker.com/](https://hub.docker.com/)。我们正在寻找的
    Docker 镜像是 TensorFlow serving 镜像。这个镜像已经安装了一切用于提供 TensorFlow 模型的服务，使用了 TensorFlow
    serving ([https://github.com/tensorflow/serving](https://github.com/tensorflow/serving))，这是
    TensorFlow 中的一个子库，可以围绕给定的模型创建一个 REST API，以便你可以发送 HTTP 请求来使用模型。你可以通过运行以下命令来下载这个镜像：
- en: '[PRE39]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Let’s break down the anatomy of this command. docker pull is the command for
    downloading an image. tensorflow/serving is the image name. Docker images are
    version controlled, meaning every Docker image has a version tag (it defaults
    to the latest if you don’t provide one). 2.6.3-gpu is the image’s version. This
    image is quite large because it supports GPU execution. If you don’t have a GPU,
    you can use docker pull tensorflow/serving:2.6.3, which is more lightweight. Once
    the command successfully executes, you can run
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们解析一下这条命令的结构。docker pull 是下载镜像的命令。tensorflow/serving 是镜像名称。Docker 镜像是有版本控制的，意味着每个
    Docker 镜像都有一个版本标签（如果你没有提供的话，默认为最新版本）。2.6.3-gpu 是镜像的版本。这个镜像相当大，因为它支持 GPU 执行。如果你没有
    GPU，你可以使用 docker pull tensorflow/serving:2.6.3，这个版本更轻量级。一旦命令成功执行，你就可以运行
- en: '[PRE40]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'to list all the images you have downloaded. With the image downloaded, you
    can use the docker run <options> <Image> command to stand up a container using
    a given image. The command docker run is a very flexible command and comes with
    lots of parameters that you can set and change. We are using several of those:'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 列出你下载的所有镜像。有了下载的镜像，你可以使用 docker run <options> <Image> 命令来使用给定的镜像启动一个容器。docker
    run 命令非常灵活，带有许多可以设置和更改的参数。我们使用了其中的几个：
- en: '[PRE41]'
  id: totrans-449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'It’s important to understand the arguments provided here. Typically, when defining
    arguments in a shell environment, a single-dash prefix is used for single character-based
    arguments (e.g., -p) and a double-dash prefix is used for more verbose arguments
    (e.g., --gpus):'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这里提供的参数是很重要的。通常，在 shell 环境中定义参数时，使用单划线前缀来表示单字符的参数（例如，-p），使用双划线前缀来表示更详细的参数（例如，--gpus）：
- en: --rm—Containers are temporary runtimes that can be removed after the service
    has run. --rm implies that the container will be removed after exiting it.
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: --rm—容器是临时运行时，可以在服务运行后移除。--rm 意味着容器将在退出后被移除。
- en: -it (short for -i and -t)—This means that you can go into the container and
    interactively run commands in a shell within the container.
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: -it（简写形式为 -i 和 -t）—这意味着你可以进入容器，并在容器内部交互式地运行命令。
- en: --gpus all—This tells the container to ensure that GPU devices (if they exist)
    are visible inside the container.
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: --gpus all—这告诉容器确保 GPU 设备（如果存在）在容器内可见。
- en: -p—This maps a network port in the container to the host. This is important
    if you want to expose some service (e.g., the API that will be up to serve the
    model) to the outside. For instance, TensorFlow serving runs on 8501 by default.
    Therefore, we are mapping the container’s 8501 port to the host’s 8501 port.
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: -p—这将容器中的网络端口映射到主机。如果你想将某些服务（例如，用于提供模型的 API）暴露给外部，这一点很重要。例如，TensorFlow serving
    默认运行在 8501 端口上。因此，我们将容器的 8501 端口映射到主机的 8501 端口。
- en: --user $(id -u):$(id -g)—This means the commands will be run as the same user
    you’re logged in as on the host. Each user is identified by a user ID and is assigned
    to one or more groups (identified by the group ID). You can pass the user and
    the group following the syntax --user <user ID>:<group ID>. For example, your
    current user ID is given by the command id -u, and the group is given by id -g.
    By default, containers run commands as root user (i.e., running via sudo), which
    can make your services more vulnerable to outside attacks. So, we use a less-privileged
    user to execute commands in the container.
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: --user $(id -u):$(id -g)—这意味着命令将以与您在主机上登录的用户相同的用户身份运行。每个用户由用户 ID 标识，并分配给一个或多个组（由组
    ID 标识）。您可以按照 --user <用户 ID>:<组 ID> 的语法传递用户和组。例如，您当前的用户 ID 可以通过命令 id -u 给出，而组则由
    id -g 给出。默认情况下，容器以 root 用户（即通过 sudo 运行）运行命令，这可能会使您的服务更容易受到外部攻击。因此，我们使用较低特权的用户在容器中执行命令。
- en: -v—This mounts a directory on the host to a location inside the container. By
    default, things you store within a container are not visible to the outside. This
    is because the container has its own storage space/volume. If you need to make
    the container see something on the host or vice versa, you need to mount a directory
    on the host to a path inside the container. This is known as *bind mounting*.
    For instance, here we expose our pushed model (which will be at ./tfx/forest-fires-pushed)
    to the path /models/forest_fires_model inside the container.
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: -v—这将一个目录挂载到容器内的位置。默认情况下，您在容器内存储的东西对外部是不可见的。这是因为容器有自己的存储空间/卷。如果您需要使容器看到主机上的某些内容，或者反之亦然，则需要将主机上的目录挂载到容器内的路径上。这被称为*绑定挂载*。例如，在这里，我们将我们推送的模型（将位于
    ./tfx/forest-fires-pushed）暴露到容器内部路径 /models/forest_fires_model。
- en: -e—This option can be used to pass special environment variables to the container.
    For example, the TensorFlow serving service expects a model name (which will be
    a part of the URL you need to hit in order to get results from the model).
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: -e—此选项可用于将特殊环境变量传递给容器。例如，TensorFlow 服务服务期望一个模型名称（它将成为从模型获取结果所需命中的 URL 的一部分）。
- en: This command is provided to you in the tfx/run_server.sh script in the Ch15-TFX-for-MLOps-in-TF2
    directory. Let’s run the run_server.sh script to see what we will get. To run
    the script
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令在 Ch15-TFX-for-MLOps-in-TF2 目录中的 tfx/run_server.sh 脚本中为您提供。让我们运行 run_server.sh
    脚本，看看我们将得到什么。要运行脚本
- en: Open a terminal.
  id: totrans-459
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个终端。
- en: Move cd into the Ch15-TFX-for-MLOps-in-TF2/tfx directory.
  id: totrans-460
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 cd 移动到 Ch15-TFX-for-MLOps-in-TF2/tfx 目录中。
- en: Run ./run_server.sh.
  id: totrans-461
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 ./run_server.sh。
- en: 'It will show an output similar to the following:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 它将显示类似于以下的输出：
- en: '[PRE42]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Of course, it will not work fully, as the directory we provided as model’s location
    is not populated. We still need to do a few things to have the final model in
    the right location.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这并不能完全奏效，因为我们提供的目录作为模型位置尚未被填充。我们仍然需要做一些事情，以便将最终模型放置在正确的位置。
- en: In the next section, we will complete the rest of our pipeline. We will see
    how we can automatically evaluate as new models are trained in the pipeline, deploy
    the model if the performance is good, and enable prediction from the model using
    a REST API (i.e., a web-based API).
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将完成我们流水线的其余部分。我们将看到如何在流水线中自动评估新训练的模型，如果性能良好，则部署模型，并使用 REST API（即基于 Web
    的 API）从模型进行预测。
- en: Exercise 3
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 3
- en: Say you want to download the TensorFlow Docker image (it has the name tensorflow/
    tensorflow) with version 2.5.0 and stand up a container that mounts the /tmp/inputs
    directory on your computer to /data volume within the container. Additionally,
    you would like to map the 5000 port in the container to 5000 on your computer.
    How would you do this using Docker commands? You can assume you’re running the
    commands as the root within the container.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您想要下载 TensorFlow Docker 映像（其名称为 tensorflow/tensorflow），版本为 2.5.0，并启动一个容器，将您计算机上的
    /tmp/inputs 目录挂载到容器内的 /data 卷中。此外，您希望将容器中的 5000 端口映射到计算机上的 5000 端口。您如何使用 Docker
    命令执行此操作？您可以假设您在容器内以 root 用户身份运行命令。
- en: 15.4 Deploying the model and serving it through an API
  id: totrans-468
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.4 部署模型并通过 API 进行服务
- en: You now have a data pipeline, a trained model, and a shell script that can run
    a Docker container with everything needed to run the model and the API to access
    the model. Now, using some services provided in TFX, you will deploy the model
    within a Docker container and make it available to be used through an API. In
    this process, you will run steps to validate the infrastructure (e.g., the container
    can be run and is healthy) and the model (i.e., when a new version of the model
    comes out, check if it is better than the last one), and finally, if everything
    is good, deploy the model on the infrastructure.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经有了一个数据管道、训练好的模型以及一个可以运行包含了运行模型和访问模型的 API 所需的一切的脚本。现在，使用 TFX 提供的一些服务，你将在
    Docker 容器中部署模型，并通过 API 进行访问。在这个过程中，你将运行一些步骤来验证基础结构（例如，容器是否可运行且健康）和模型（即在模型的新版本发布时，检查它是否比上一个版本更好），最后，如果一切都好，将模型部署到基础结构上。
- en: 'It has been a long journey. Let’s look back and see what we’ve accomplished
    so far. We have used the following TFX components:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个漫长的旅程。让我们回顾一下我们到目前为止取得的成就。我们已经使用了以下 TFX 组件：
- en: CsvExampleGen—Load data as TFRecord objects from CSV files.
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CsvExampleGen—从 CSV 文件中以 TFRecord 对象的形式加载数据。
- en: StatisticsGen—Basic statistics and visualizations about the distribution of
    various columns in the CSV data.
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: StatisticsGen—关于 CSV 数据中各列分布的基本统计数据和可视化。
- en: SchemaGen—Generate the schema/template of the data (e.g., data types, domains,
    minimum/maximum values allowed, etc.).
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SchemaGen—生成数据的模式/模板（例如数据类型、域、允许的最小/最大值等）。
- en: Transform—Transform the raw columns to features using the operations available
    in the tensorflow_transform library (e.g., one-hot encoding, bucketizing).
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transform—使用 tensorflow_transform 库中提供的操作（例如，独热编码、桶化）将原始列转换为特征。
- en: Trainer—Define a Keras model, train it using the transformed data, and save
    to the disk. This model has a signature called serving default, which instructs
    the model what to do for an incoming request.
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Trainer—定义一个 Keras 模型，使用转换后的数据进行训练，并保存到磁盘。此模型具有一个名为 serving default 的签名，指示模型对于传入的请求应该执行什么操作。
- en: ExampleValidator—This is used to validate that training and evaluation examples
    used adhere to the defined schema and can be used to detect anomalies.
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ExampleValidator—用于验证训练和评估示例是否符合定义的模式，并可用于检测异常。
- en: 15.4.1 Validating the infrastructure
  id: totrans-477
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.4.1 验证基础结构
- en: 'Using TFX, you can ensure almost everything works well when you have a fully
    automated pipeline. We will discuss one such step here: the infrastructure validation
    step. In this, tfx.components.InfraValidator will automatically'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 TFX，当你拥有一个完全自动化的管道时，几乎可以确保一切工作正常。我们将在这里讨论一个这样的步骤：基础结构验证步骤。在这个步骤中，tfx.components.InfraValidator
    将自动进行
- en: Create a container using a specific version of the TensorFlow serving image
    provided
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用提供的特定版本的 TensorFlow serving 镜像创建一个容器
- en: Load and run the model in it
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载并在其中运行模型
- en: Send several requests to make sure the model responds
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发送多个请求以确保模型能够响应
- en: Stand down the container
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关闭容器
- en: Let’s look at how we can use this component to validate the local Docker configuration
    we set up in the previous section (see the next listing).
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下如何使用这个组件来验证我们在前一节中设置的本地 Docker 配置（请参阅下一个清单）。
- en: Listing 15.11 Defining the InfraValidator
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: Listing 15.11 定义 InfraValidator
- en: '[PRE43]'
  id: totrans-485
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: ❶ InfraValidator needs the location of the model it’s going to validate.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ InfraValidator 需要验证的模型的位置。
- en: ❷ Source for the data that will be used to build API calls to the model
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 用于构建对模型的 API 调用的数据来源
- en: ❸ Holds a collection of model serving-related specifications
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 包含一组与对模型进行的具体调用相关的规范
- en: ❹ Defines the version/tag of the TensorFlow serving Docker Image to be used
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 定义要使用的 TensorFlow serving Docker Image 的版本/标签
- en: ❺ Says to the InfraValidator that we are going to use the local Docker service
    to test
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 告诉 InfraValidator 我们将使用本地的 Docker 服务进行测试
- en: ❻ Holds a collection of specifications related to the specific call made to
    the model
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 包含与对模型进行的特定调用相关的规范集合
- en: ❼ Defines which model signature to use
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 定义要使用的模型签名
- en: ❽ Defines how many requests to make to the model
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 定义了向模型发送的请求数量
- en: 'The InfraValidator, just like any other TFX component, expects several arguments
    to run accurately:'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: InfraValidator 和其他任何 TFX 组件一样，需要准确地提供多个参数才能运行。
- en: model—The Keras model returned by the Trainer component.
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: model—由 Trainer 组件返回的 Keras 模型。
- en: examples—Loaded raw examples given by the CSVExampleGen.
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: examples—由 CSVExampleGen 给出的原始示例。
- en: serving_spec—Expects a ServingSpec protobuf message. It will specify the version
    of the TensorFlow serving Docker image and whether to use local Docker installation
    (which is done here).
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: serving_spec—期望一个ServingSpec protobuf消息。它将指定TensorFlow serving Docker镜像的版本以及是否使用本地Docker安装（这里已完成）。
- en: request_spec—A RequestSpec protobuf message that will specify the signature
    that needs to be reached to verify the model is working.
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: request_spec—一个RequestSpec protobuf消息，它将指定需要达到的签名以验证模型是否正常工作。
- en: If this step completes error-free, you will see the files shown in figure 15.9
    in the pipeline root.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 如果此步骤无误完成，您将在管道根目录中看到图15.9中显示的文件。
- en: '![15-09](../../OEBPS/Images/15-09.png)'
  id: totrans-500
  prefs: []
  type: TYPE_IMG
  zh: '![15-09](../../OEBPS/Images/15-09.png)'
- en: Figure 15.9 The directory/file structure after running the InfraValidator
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.9 运行InfraValidator后的目录/文件结构
- en: You can see a file called INFRA_BLESSED appearing in the InfraValidator sub-directory.
    This brings us to the concept of *blessing*. TFX will bless certain elements in
    the pipeline when they run successfully. Once blessed, it will create a file with
    the suffix BLESSED. If the step fails, then a file with the suffix NOT_BLESSED
    will be created. Blessing helps to discriminate between things that ran fine and
    things that failed. For examples, once blessed, we can be sure that the infrastructure
    is working as expected. This means that things like
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到一个名为INFRA_BLESSED的文件出现在InfraValidator子目录中。这引出了“祝福”的概念。TFX将在成功运行流水线中祝福某些元素。一旦被祝福，它将创建一个带有后缀BLESSED的文件。如果该步骤失败，那么将创建一个带有后缀NOT_BLESSED的文件。祝福有助于区分运行正常和运行失败的事物。例如，一旦被祝福，我们可以确信基础设施按预期工作。这意味着像
- en: Standing up a container
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 架设一个容器
- en: Loading the model
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载模型
- en: Reaching a defined API endpoint
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 达到定义的API端点
- en: can be performed without issues.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 可以无问题地执行。
- en: 15.4.2 Resolving the correct model
  id: totrans-507
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.4.2 解析正确的模型
- en: 'Moving forward, we will define a resolver. The purpose of the resolver is to
    resolve a special artifact (like a model) that can evolve over time using a well-defined
    strategy (e.g., the model with the lowest validation error). Then the resolver
    informs subsequent components (e.g., the model Evaluator component we will be
    defining next) which artifact version to use. As you might have guessed, we will
    use the resolver to resolve the trained Keras model in the pipeline. So, if you
    run the pipeline multiple times, the resolver will make sure the latest and greatest
    model is used in the downstream components:'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义一个解析器。解析器的目的是使用明确定义的策略（例如，使用最低验证错误的模型）解决随时间演变的特殊工件（如模型）。然后，解析器通知后续组件（例如，我们接下来将定义的模型评估器组件）要使用哪个工件版本。正如您可能已经猜到的那样，我们将使用解析器来解析管道中的经过训练的Keras模型。因此，如果您多次运行管道，则解析器将确保在下游组件中使用最新和最优秀的模型：
- en: '[PRE44]'
  id: totrans-509
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'When defining the resolver to validate a model, we will define three things:'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义验证模型的解析器时，我们将定义三件事：
- en: 'strategy_class (a class from the tfx.dsl.components.common.resolver.ResolverStrategy
    namespace)—Defines the resolution strategy. There are two strategies supported
    currently: the latest blessed model (i.e., the model that has passed a set of
    defined evaluation checks) and the latest model.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: strategy_class（来自tfx.dsl.components.common.resolver.ResolverStrategy命名空间的类）—定义解析策略。当前支持两种策略：最新的祝福模型（即通过一组定义的评估检查的模型）和最新的模型。
- en: model (tfx.dsl.Channel)—Wraps a TFX artifact-type model in a tfx.dsl.Channel
    object. A tfx.dsl.Channel is an TFX-specific abstract concept that connects data
    consumers and data producers. For example, a channel is required to choose the
    correct model from a pool of models available in the pipeline.
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型（tfx.dsl.Channel）—将TFX工件类型的模型包装在一个tfx.dsl.Channel对象中。tfx.dsl.Channel是一个TFX特定的抽象概念，连接数据消费者和数据生产者。例如，在管道中选择正确的模型时就需要一个通道，以从可用模型池中选择。
- en: model_blessing (tfx.dsl.Channel)—Wraps a TFX artifact of type ModelBlessing
    in a tfx.dsl.Channel object.
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: model_blessing（tfx.dsl.Channel）—将类型为ModelBlessing的TFX工件包装在tfx.dsl.Channel对象中。
- en: You can look at various artifacts that you can wrap in a tfx.dsl.Channel object
    at [http://mng.bz/2nQX](http://mng.bz/2nQX).
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以查看各种工件，将其包装在一个tf.dsl.Channel对象中，网址为[http://mng.bz/2nQX](http://mng.bz/2nQX)。
- en: 15.4.3 Evaluating the model
  id: totrans-515
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.4.3 评估模型
- en: 'We will evaluate the model as the last step before pushing it to a designated
    production environment. Essentially, we will define several evaluation checks
    that the model needs to pass. When a model is passed, TFX will bless the model.
    Otherwise, TFX will leave the model unblessed. We will learn later how to check
    if the model was blessed. To define the evaluation checks, we are going to use
    the tensorflow_model_analysis library. The first step is to define an evaluation
    configuration that specifies the checks:'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在将模型推送到指定的生产环境之前的最后一步对模型进行评估。基本上，我们将定义几个模型需要通过的评估检查。当模型通过时，TFX 将对模型进行认可。否则，TFX
    将使模型保持未认可状态。我们将在后面学习如何检查模型是否被认可。为了定义评估检查，我们将使用 tensorflow_model_analysis 库。第一步是定义一个评估配置，其中指定了检查项：
- en: '[PRE45]'
  id: totrans-517
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: ❶ Define a model spec containing the label feature name.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义一个包含标签特征名称的模型规范。
- en: ❷ Define a list of metric specifications.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义一个指标规范列表。
- en: ❸ Get the number of examples evaluated on.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 获取评估的示例数。
- en: ❹ Define the mean-squared error as a metric.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将均方误差定义为一项指标。
- en: ❺ Define a threshold upper bound as a check.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将阈值上限定义为一个检查。
- en: ❻ Define Change in error (compared to previous models) as a check (i.e., the
    lower the error the better).
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将误差变化（与先前模型相比）定义为一个检查（即，误差越低越好）。
- en: ❼ Slicing specs define how data needs to be partitioned when evaluating.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 切片规范定义了在评估时数据需要如何分区。
- en: ❽ Evaluate on the whole data set without slicing (i.e., an empty slice).
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 在整个数据集上进行评估，不进行切片（即，空切片）。
- en: ❾ Evaluate on partitioned data, where data is partitioned based on the month
    field.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 在分区数据上进行评估，其中数据根据月份字段进行分区。
- en: 'The EvalConfig is quite a mouthful. Let’s go through it slowly. We have to
    define three things: model specifications (as a ModelSpec object), metric specifications
    (as a list of MetricsSpec objects), and slicing specifications (as a list of SlicingSpec
    objects). The ModelSpec object can be used to define the following:'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: EvalConfig 相当复杂。让我们慢慢来。我们必须定义三件事：模型规范（作为 ModelSpec 对象）、指标规范（作为 MetricsSpec 对象列表）和切片规范（作为
    SlicingSpec 对象列表）。ModelSpec 对象可用于定义以下内容：
- en: name—An alias model name that can be used to identify the model in this step.
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: name—可用于在此步骤中标识模型的别名模型名称。
- en: model_type—A string identifying the type of model. Allowed values include tf_keras,
    tf_estimator, tf_lite, and tf_js, tf_generic. For Keras models like ours, type
    is automatically derived.
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: model_type—标识模型类型的字符串。允许的值包括 tf_keras、tf_estimator、tf_lite 和 tf_js、tf_generic。对于像我们的
    Keras 模型，类型会自动推导。
- en: signature_name—The model signature to be used for inference. By default, serving_default
    is used.
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: signature_name—用于推断的模型签名。默认情况下使用 serving_default。
- en: label_key—The name of the label feature in the examples.
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: label_key—示例中标签特征的名称。
- en: label_keys—For multi-output models, a list of label keys is used.
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: label_keys—对于多输出模型，使用标签键列表。
- en: example_weight_key—An optional key (or feature name) to retrieve example weights
    if present.
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: example_weight_key—如果存在，则用于检索示例权重的可选键（或特征名称）。
- en: 'For more information about the ModelSpec object, refer to [http://mng.bz/M5wW](http://mng.bz/M5wW).
    In a MetricsSpec object, the following attributes can be set:'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 ModelSpec 对象的更多信息，请参阅[http://mng.bz/M5wW](http://mng.bz/M5wW)。在 MetricsSpec
    对象中，可以设置以下属性：
- en: metrics—A list of MetricConfig objects. Each MetricConfig object takes a class_name
    as an input. You can choose any class defined in tfma.metrics.Metric ([http://mng.bz/aJ97](http://mng.bz/aJ97))
    or tf.keras.metrics.Metric ([http://mng.bz/gwmV](http://mng.bz/gwmV)) namespaces.
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: metrics—MetricConfig 对象的列表。每个 MetricConfig 对象将类名作为输入。您可以选择在 tfma.metrics.Metric（[http://mng.bz/aJ97](http://mng.bz/aJ97)）或
    tf.keras.metrics.Metric（[http://mng.bz/gwmV](http://mng.bz/gwmV)）命名空间中定义的任何类。
- en: 'The SlicingSpec defines how the data needs to be partitioned during evaluation.
    For example, for time series problems, you will need to see how the model performs
    across different months or days. For that, SlicingSpec is a handy config. SlicingSpec
    has the following arguments:'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: SlicingSpec 定义了评估期间数据需要如何进行分区。例如，对于时间序列问题，您需要查看模型在不同月份或天数上的表现。为此，SlicingSpec
    是一个方便的配置。SlicingSpec 具有以下参数：
- en: feature_keys—Can be used to define a feature key on which you can partition
    the data. For example, for feature key month, it will create a partition of data
    for each month by selecting data having a specific month. If not passed, it will
    return the whole data set.
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: feature_keys—可用于定义一个特征键，以便您可以根据其对数据进行分区。例如，对于特征键月份，它将通过选择具有特定月份的数据来为每个月份创建一个数据分区。如果未传递，它将返回整个数据集。
- en: 'Note that TFX uses the evaluation split you defined at the very beginning of
    the pipeline (i.e., when implementing the CsvExampleGen component) if not provided.
    In other words, all the metrics are evaluated on the evaluation split of the data
    set. Next, it defines two criteria for the evaluation to pass:'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果没有提供，TFX将使用您在管道最开始（即实施CsvExampleGen组件时）定义的评估集合。换句话说，所有指标都在数据集的评估集合上进行评估。接下来，它定义了两个评估通过的条件：
- en: The mean squared error is smaller than 200.
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均方误差小于200。
- en: The mean squared loss has improved by 1e - 10.
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均方损失改善了1e - 10。
- en: A model will be blessed (i.e., marked as passed) if these two conditions are
    satisfied for a newly trained model. Remember that we have seen a loss of around
    150 in our better model, so let’s set the threshold to 200\. The metrics added
    here are in addition to those saved when using the model.compile() step. For example,
    since the mean-squared error is used as the loss, it will already be a part of
    the metrics (even without defining it in eval_config).
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 如果对于一个新训练的模型满足下列两个条件，那么该模型将被标记为“通过”（即通过了测试）。
- en: 'Finally, we define the Evaluator ([http://mng.bz/e7BQ](http://mng.bz/e7BQ))
    that will take in a model and run the evaluation checks defined in eval_config.
    You can define a TFX Evaluator as follows by passing in values for examples, model,
    baseline_model, and eval_ config arguments. baseline_model is resolved by the
    Resolver:'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们定义了评估器（[http://mng.bz/e7BQ](http://mng.bz/e7BQ)），它将接收一个模型并运行在eval_config中定义的评估检查。您可以通过为examples、model、baseline_model和eval_config参数传入值来定义一个TFX评估器。baseline_model是由Resolver解析的：
- en: '[PRE46]'
  id: totrans-543
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Unfortunately, running the Evaluator will not provide the results you need.
    It will, in fact, fail the evaluation. At the bottom of the log, you will see
    an output like this
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，运行评估器不会提供您所需的结果。事实上，它会导致评估失败。在日志的底部，您将看到如下输出：
- en: '[PRE47]'
  id: totrans-545
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'which says Blessing resulted False. It’s still a mystery why the model failed,
    given it showed only a loss of around 150 and we set the threshold to 200\. To
    understand what happened, we need to look at the results written to the disk.
    If you look inside the <pipeline root>/ examples\forest_fires_pipeline\Evaluator\<execution
    ID> directory, you will see files like validation, metrics, and so forth. Along
    with the tensorflow_model_analysis library, these can provide invaluable insights
    to understand what when wrong. The tensorflow_model_analysis library provides
    several convenient functions to load the results stored in these files:'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的输出表明，Blessing结果为False。鉴于它只显示了约为150的损失，而我们将阈值设为200，为什么模型失败仍然是一个谜。要了解发生了什么，我们需要查看写入磁盘的结果。如果您在<pipeline
    root>/ examples\forest_fires_pipeline\Evaluator\<execution ID>目录中查看，您会看到像validation、metrics等文件。使用tensorflow_model_analysis库，这些文件可以提供宝贵的见解，帮助我们理解出了什么问题。tensorflow_model_analysis库提供了几个方便的函数来加载存储在这些文件中的结果：
- en: '[PRE48]'
  id: totrans-547
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: This will print out
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 运行结果为：
- en: '[PRE49]'
  id: totrans-549
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'You can clearly see what happened. It says that the slice created for the month
    "sep" resulted in an error of 269, which is why our evaluation failed. If you
    want details about all of the slices used and their results, you can inspect the
    metrics file:'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以清楚地看到发生了什么。它指出，为月份"sep"创建的切片导致了269的错误，这就是为什么我们的评估失败了。如果您想要关于所有使用的切片及其结果的详细信息，您可以检查指标文件：
- en: '[PRE50]'
  id: totrans-551
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'This would output the following. You will only see a small snippet of the full
    output here to save space:'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 运行结果为以下内容。为了节省空间，这里只显示了完整输出的一小部分：
- en: '[PRE51]'
  id: totrans-553
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'This output sheds more light on what happened. Since we used the example count
    as one of the metrics, we can see the number of examples in each slice. For example,
    in month May, there’s only one example present in the evaluation split, which
    is most probably an outlier. To fix this, we will bump up the threshold to 300\.
    Once you do that, you need to rerun the Evaluator, and you will see from the Evaluator’s
    logs that our model passes the checks:'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出让我们对发生了什么有了更多了解。由于我们将示例计数视为指标之一，我们可以看到每个切片中的示例数。例如，在五月份，评估集合中只有一个示例，这很可能是一个异常值。为了解决这个问题，我们将阈值提高到300。一旦您这样做了，需要重新运行评估器，从评估器的日志中可以看到我们的模型已通过检查：
- en: '[PRE52]'
  id: totrans-555
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: The best way to address this is to identify why the month of "sep" is giving
    such a large value while other months are on par with or below the overall loss
    value. After identifying the issue, we should identify remediation steps to correct
    this (e.g., reconsidering outlier definitions). On that note, we will move on
    to the next part of our pipeline.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的最佳方法是确定为什么“sep” 月份给出的值如此之大，而其他月份与整体损失值持平或低于。确定问题后，我们应该确定纠正措施以更正此问题（例如，重新考虑异常值定义）。在此之后，我们将继续进行管道的下一部分。
- en: 15.4.4 Pushing the final model
  id: totrans-557
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.4.4 推送最终模型
- en: 'We have reached the last steps in our pipeline. We need to define a Pusher.
    The Pusher ([http://mng.bz/pOZz](http://mng.bz/pOZz)) is responsible for pushing
    a blessed model (i.e., a model that passes the evaluation checks) to a defined
    production environment. The production environment can simply be a local location
    in your file system:'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经达到管道中的最后步骤。我们需要定义一个推送器。 推送器（[http://mng.bz/pOZz](http://mng.bz/pOZz)）负责将经过评估检查的认可模型（即通过的模型）推送到定义好的生产环境。
    生产环境可以简单地是您文件系统中的本地位置：
- en: '[PRE53]'
  id: totrans-559
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The Pusher takes the following elements as arguments:'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 推送器接受以下元素作为参数：
- en: model—The Keras model returned by the Trainer component
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: model—由 Trainer 组件返回的 Keras 模型
- en: model_blessing—Evaluator component’s blessed state
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: model_blessing—Evaluator 组件的认可状态
- en: infra_blessing—InfraValidator’s blessed state
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: infra_blessing—InfraValidator 的认可状态
- en: push_destination—A destination to be pushed to as a PushDestination protobuf
    message
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: push_destination—作为 PushDestination protobuf 消息推送的目标
- en: If the step runs successfully, you will have a model saved in a directory called
    forestfires-model-pushed in our pipeline root.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这一步运行成功，您将在我们的管道根目录中的称为 forestfires-model-pushed 的目录中保存模型。
- en: 15.4.5 Predicting with the TensorFlow serving API
  id: totrans-566
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.4.5 使用 TensorFlow serving API 进行预测
- en: The very last step is to retrieve the model from the pushed destination and
    start a Docker container based on the TensorFlow serving image we downloaded.
    The Docker container will provide an API that we can ping with various requests.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是从推送目的地检索模型，并基于我们下载的 TensorFlow 服务镜像启动 Docker 容器。 Docker 容器将提供一个 API，我们可以通过各种请求进行
    ping 。
- en: Let’s look at how the API fits into the big picture in more detail (figure 15.10).
    The machine learning model sits behind an API. The API defines various HTTP endpoints
    you can ping (through Python or a package like curl). These endpoints will be
    in the form of a URL and can expect parameters in the URL or data embedded in
    the request body. The API is served via a server. The server exposes a network
    port in which clients can communicate with the server. The client can send requests
    to the server using the format <host name>:<port>/<end point>. We will discuss
    what the request actually looks like in more detail.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看一下如何将 API 融入整体架构中（图15.10）。 机器学习模型位于 API 的后面。 API 定义了各种 HTTP 端点，您可以通过
    Python 或类似 curl 的包来 ping 这些端点。 这些端点将以 URL 的形式提供，并且可以在 URL 中期望参数或将数据嵌入请求体中。 API
    通过服务器提供。 服务器公开了一个网络端口，客户端可以与服务器进行通信。 客户端可以使用格式<主机名>:<端口>/<端点>向服务器发送请求。 我们将更详细地讨论请求实际的样子。
- en: '![15-10](../../OEBPS/Images/15-10.png)'
  id: totrans-569
  prefs: []
  type: TYPE_IMG
  zh: '![15-10](../../OEBPS/Images/15-10.png)'
- en: Figure 15.10 How the model interacts with the API, the TensorFlow server, and
    the client
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.10 模型如何与 API、TensorFlow 服务器和客户端交互
- en: To start the container, simply
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动容器，只需
- en: Open a terminal
  id: totrans-572
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个终端
- en: Move the cd into the Ch15-TFX-for-MLOps-in-TF2/tfx directory
  id: totrans-573
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 cd 移入 Ch15-TFX-for-MLOps-in-TF2/tfx 目录
- en: Run ./run_server.sh
  id: totrans-574
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 ./run_server.sh
- en: 'Next, in the Jupyter notebook, we will send a HTTP POST request. There are
    two main types of HTTP requests: GET and POST. Refer the sidebar if you’re interested
    in the differences. An HTTP POST request is a request that not only contains a
    URL to reach and header information, but also contains a payload, which is necessary
    for the API to complete the request. For example, if we are hitting the API endpoint
    corresponding to the serving_default signature, we have to send an input to predict
    with.'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在 Jupyter 笔记本中，我们将发送一个 HTTP POST 请求。 HTTP 请求有两种主要类型：GET 和 POST。 如果您对差异感兴趣，请参考侧边栏。
    HTTP POST 请求是一个包含了可以请求的 URL 和头信息的请求，也包含了负载，这对 API 完成请求是必要的。 例如，如果我们正在击中与 serving_default
    签名对应的 API 端点，我们必须发送一个输入以进行预测。
- en: GET vs. POST requests
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: GET vs. POST 请求
- en: GET and POST are HTTP methods. HTTP is a protocol that defines how a client
    and a server should communicate. A client will send requests, and the server will
    listen for requests on a specific network port. The client and the server don’t
    necessarily need to be two separate machines. In our case, the client and the
    server are both on the same machine.
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: GET 和 POST 是 HTTP 方法。HTTP 是一个定义客户端和服务器应该如何通信的协议。客户端将发送请求，服务器将在特定的网络端口上监听请求。客户端和服务器不一定需要是两台单独的机器。在我们的情况下，客户端和服务器都在同一台机器上。
- en: 'Every time you visit a website by typing a URL, you are making a request to
    that specific website. A request has the following anatomy ([http://mng.bz/OowE](http://mng.bz/OowE)):'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: '每当您通过键入 URL 访问网站时，您都在向该特定网站发出请求。一个请求具有以下解剖结构（[http://mng.bz/OowE](http://mng.bz/OowE)）:'
- en: '*A method type*—GET or POST'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*方法类型* — GET 或 POST'
- en: '*A path*—The URL to reach the endpoint of the server you want to reach'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*一个路径* — 到达您想要到达的服务器端点的 URL'
- en: '*A body*—Any large payload that needs the client to complete the request (e.g.,
    the input for a machine learning prediction service)'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*一个主体* — 需要客户端完成请求的任何大型有效载荷（例如，用于机器学习预测服务的输入）'
- en: '*A header*—Additional information needed for the server (e.g., the type of
    data sent in the body)'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*一个头部* — 服务器需要的附加信息（例如，发送主体中的数据类型）'
- en: The main difference is that GET is used to request data, as opposed to a POST
    request, which is used to post or send data to the server (which can optionally
    return something). A GET request will not have a request body, whereas a POST
    request will have a request body. Another difference is that GET requests can
    be cached, whereas POST requests will not be cached, making them more secure for
    sensitive data. You can read more about this at [http://mng.bz/YGZA](http://mng.bz/YGZA).
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 主要区别在于 GET 用于请求数据，而 POST 请求用于将数据发送到服务器（可以选择返回某些内容）。GET 请求不会有请求主体，而 POST 请求会有请求主体。另一个区别是
    GET 请求可以被缓存，而 POST 请求不会被缓存，这使得它们对于敏感数据更安全。您可以在[http://mng.bz/YGZA](http://mng.bz/YGZA)中了解更多信息。
- en: 'We will define a request body, which contains the signature name we want to
    hit and the input we want to predict for. Next, we will use the requests library
    in Python to send a request to our TensorFlow model server (i.e., Docker container).
    In this request, we will define the URL to reach (automatically generated by the
    TensorFlow model server) and the payload to carry. If the request is successful,
    we should get a valid prediction as the output:'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义一个请求主体，其中包含我们要击中的签名名称以及我们要为其预测的输入。接下来，我们将使用 Python 中的 requests 库发送一个请求到我们的
    TensorFlow 模型服务器（即 Docker 容器）。在此请求中，我们将定义要到达的 URL（由 TensorFlow 模型服务器自动生成）和要携带的有效载荷。如果请求成功，我们应该会得到一个有效的预测作为输出：
- en: '[PRE54]'
  id: totrans-585
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The first thing we do is define a request with a specific request body. The
    requirements for the request body are defined at [https://www.tensorflow.org/tfx/serving/api_rest](https://www.tensorflow.org/tfx/serving/api_rest).
    It is a dictionary of key-value pairs that should have two keys: signature_name
    and instances. signature_name defines which signature to invoke in the model,
    and instances will contain the input data. Note that we’re not passing input data
    in its raw form. Rather, we use base64 encoding. It encodes a byte stream (i.e.,
    a binary input) to an ASCII text string. You can read more about this at [http://mng.bz/1o4g](http://mng.bz/1o4g).
    You can see that we are first converting our dictionary to a byte-stream (i.e.,
    with a b"<data>" format) and then using base64 encoding on that. If you remember
    from our previous discussion on writing the model serve function (which had the
    signature def serve_tf_examples_fn(serialized_tf_examples): ), it expects a serialized
    set of examples. Serialization is done by converting the data to a byte stream.'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先要做的是用特定的请求主体定义一个请求。对请求主体的要求在[https://www.tensorflow.org/tfx/serving/api_rest](https://www.tensorflow.org/tfx/serving/api_rest)中定义。它是一个键值对字典，应该有两个键：signature_name
    和 instances。signature_name 定义要在模型中调用哪个签名，而 instances 将包含输入数据。请注意，我们不是直接传递原始形式的输入数据。相反，我们使用
    base64 编码。它将字节流（即二进制输入）编码为 ASCII 文本字符串。您可以在[http://mng.bz/1o4g](http://mng.bz/1o4g)中了解更多信息。您可以看到我们首先将字典转换为字节流（即
    b"<data>" 格式），然后在其上使用 base64 编码。如果您还记得我们之前讨论的编写模型服务函数（其中包含 signature def serve_tf_examples_fn(serialized_tf_examples)）时，它期望一组序列化的示例。序列化是通过将数据转换为字节流来完成的。
- en: 'When the data is ready, we use the requests library to create a POST request
    for the API. First, we define a header to say that the content or payload we’re
    passing is JSON. Next, we send a POST request via requests.post() giving the URL,
    which is in [http://<server’s hostname>:<port>/v1/models/<model name>:predict](http://<server%E2%80%99s%20hostname%3E:<port%3E/v1/models/<model%20name%3E:predict)
    format, data (i.e., the JSON payload), and the header. This is not the only API
    endpoint available to us. There are other endpoints as well ([https://www.tensorflow.org/tfx/serving/api_rest](https://www.tensorflow.org/tfx/serving/api_rest)).
    There are four main endpoints that are available:'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据准备好后，我们使用 requests 库创建一个 POST 请求到 API。首先，我们定义一个头部，以表示我们传递的内容或载荷是 JSON 格式的。接下来，我们通过
    requests.post() 方法发送一个 POST 请求，传递 URL，格式为 [http://<server’s hostname>:<port>/v1/models/<model
    name>:predict](http://<server%E2%80%99s%20hostname%3E:<port%3E/v1/models/<model%20name%3E:predict)，数据（即
    JSON 载荷），和头部信息。这不是我们唯一可以使用的 API 端点。我们还有其他端点（[https://www.tensorflow.org/tfx/serving/api_rest](https://www.tensorflow.org/tfx/serving/api_rest)）。主要有四个可用的端点：
- en: '*http:/ /<server’s hostname>:<port>/v1/models/<model name>:predict*—Predicts
    the output value using the model and the data passed in the request. Does not
    require a target to be available for the provided input.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*http:/ /<server’s hostname>:<port>/v1/models/<model name>:predict* — 使用模型和请求中传递的数据预测输出值。不需要提供给定输入的目标值。'
- en: '*http:*/ /*<server’s hostname>:<port>/v1/models/<model name>:regress* —Used
    in regression problems. Used when both inputs and target are available (i.e.,
    an error can be calculated).'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*http:*/ /*<server’s hostname>:<port>/v1/models/<model name>:regress* — 用于回归问题。当输入和目标值都可用时使用（即可以计算误差）。'
- en: '*http:*/ /*<server’s hostname>:<port>/v1/models/<model name>:classify*—Used
    in classification problems. Used when both inputs and target are available (i.e.,
    an error can be calculated).'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*http:*/ /*<server’s hostname>:<port>/v1/models/<model name>:classify* — 用于分类问题。当输入和目标值都可用时使用（即可以计算误差）。'
- en: '*http:*/ /*<server’s hostname>:<port>/v1/models/<model name>/metadata*—Provides
    metadata about available endpoints/model signatures.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*http:*/ /*<server’s hostname>:<port>/v1/models/<model name>/metadata* — 提供有关可用端点/模型签名的元数据。'
- en: This will return some response. If the request was successful, it will have
    the response; otherwise, it will contain an HTTP error. You can see various HTTP
    status/error codes at [http://mng.bz/Pn2P](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status).
    In our case, we should get something like
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回一些响应。如果请求成功，将包含响应；否则，会包含 HTTP 错误。您可以在 [http://mng.bz/Pn2P](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status)
    上看到各种 HTTP 状态/错误代码。在我们的情况下，我们应该得到类似于
- en: '[PRE55]'
  id: totrans-593
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: This means our model has successfully processed the input and produced a valid
    prediction. We can see that the model has returned a prediction that is well within
    the possible range of values we saw during our data exploration. This concludes
    our discussion of TensorFlow Extended (TFX).
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们的模型已成功处理了输入并产生了有效的预测。我们可以看到，模型返回的预测值完全在我们在数据探索期间看到的可能值范围内。这结束了我们对 TensorFlow
    扩展（TFX）的讨论。
- en: Exercise 4
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 4
- en: How would you send multiple inputs in your HTTP request to the model? Assume
    you have the following two inputs that you want to predict for using the model.
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 如何将多个输入发送到模型的 HTTP 请求中？假设您有以下两个输入，您想要使用模型进行预测。
- en: '|  | **Example 1** | **Example 2** |'
  id: totrans-597
  prefs: []
  type: TYPE_TB
  zh: '|  | **Example 1** | **Example 2** |'
- en: '| X | 9 | 7 |'
  id: totrans-598
  prefs: []
  type: TYPE_TB
  zh: '| X | 9 | 7 |'
- en: '| Y | 6 | 4 |'
  id: totrans-599
  prefs: []
  type: TYPE_TB
  zh: '| Y | 6 | 4 |'
- en: '| month | aug | aug |'
  id: totrans-600
  prefs: []
  type: TYPE_TB
  zh: '| month | aug | aug |'
- en: '| day | fri | fri |'
  id: totrans-601
  prefs: []
  type: TYPE_TB
  zh: '| day | fri | fri |'
- en: '| FFMC | 91 | 91 |'
  id: totrans-602
  prefs: []
  type: TYPE_TB
  zh: '| FFMC | 91 | 91 |'
- en: '| DMC | 248 | 248 |'
  id: totrans-603
  prefs: []
  type: TYPE_TB
  zh: '| DMC | 248 | 248 |'
- en: '| DC | 553 | 553 |'
  id: totrans-604
  prefs: []
  type: TYPE_TB
  zh: '| DC | 553 | 553 |'
- en: '| ISI | 6 | 6 |'
  id: totrans-605
  prefs: []
  type: TYPE_TB
  zh: '| ISI | 6 | 6 |'
- en: '| temp | 20.5 | 20.5 |'
  id: totrans-606
  prefs: []
  type: TYPE_TB
  zh: '| temp | 20.5 | 20.5 |'
- en: '| RH | 58 | 20 |'
  id: totrans-607
  prefs: []
  type: TYPE_TB
  zh: '| RH | 58 | 20 |'
- en: '| wind | 3 | 0 |'
  id: totrans-608
  prefs: []
  type: TYPE_TB
  zh: '| wind | 3 | 0 |'
- en: '| rain | 0 | 0 |'
  id: totrans-609
  prefs: []
  type: TYPE_TB
  zh: '| rain | 0 | 0 |'
- en: To pass multiple values for that input in an HTTP request, you can append more
    examples to the instances list in the JSON data.
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 HTTP 请求中为该输入传递多个值，可以在 JSON 数据的实例列表中附加更多示例。
- en: Summary
  id: totrans-611
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: MLOps defines a workflow that will automate most of the steps, from collecting
    data to delivering a model trained on that data.
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLOps 定义了一个工作流程，将自动化大部分步骤，从收集数据到交付对该数据进行训练的模型。
- en: Productionization involves deploying a trained model with a robust API to access
    the model, enabling customers to use the model for its designed purpose. The API
    provides several HTTP endpoints, which are in the form of URLs, which clients
    can use to communicate with the server.
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生产部署涉及部署一个带有健壮 API 的训练模型，使客户能够使用模型进行其设计目的的操作。该 API 提供几个 HTTP 端点，格式为客户端可以使用与服务器通信的
    URL。
- en: In TFX, you define a MLOps pipeline as a series of TFX components.
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在TFX中，您将MLOps管道定义为一系列TFX组件。
- en: TFX has components to load data (CsvExampleGen), generate basic statistics and
    visualizations (StatisticsGen), infer the schema (SchemaGen), and convert raw
    columns to features (Transform).
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TFX有组件用于加载数据（CsvExampleGen）、生成基本统计信息和可视化（StatisticsGen）、推断模式（SchemaGen）以及将原始列转换为特征（Transform）。
- en: For a Keras model to be served via HTTP requests, signatures are required.
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要通过HTTP请求提供Keras模型，需要签名。
- en: Signatures define the data format of inputs and outputs as well as the steps
    that need to happen in order to produce the output via a TensorFlow function (e.g.,
    a function decorated with @tf.function).
  id: totrans-617
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 签名定义输入和输出的数据格式，以及通过TensorFlow函数（例如，用@tf.function装饰的函数）生成输出所需的步骤。
- en: Docker is a containerization technology that can be used to encapsulate a unit
    of software as a single container and can be ported easily between different environments
    (or computers).
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker是一种容器化技术，可以将一个软件单元封装为一个单一容器，并可以在不同环境（或计算机）之间轻松移植。
- en: Docker runs a unit of software in a container.
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker在容器中运行一个软件单元。
- en: TFX provides validation components for validating infrastructure and the model.
    TFX can stand up a container and make sure it’s running as expected, as well as
    make sure the model passes various evaluation criteria (e.g., loss being smaller
    than a threshold), ensuring a high-quality model.
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TFX为验证基础设施和模型提供了验证组件。TFX可以启动一个容器并确保它按预期运行，还可以确保模型通过各种评估标准（例如，损失小于阈值），从而确保高质量的模型。
- en: Once the model is pushed to a production environment, we start a Docker container
    (based on the TensorFlow serving image) that will mount the model into the container
    and serve it via an API. We can make HTTP requests (with the inputs embedded)
    to generate predictions.
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦模型被推送到生产环境，我们会启动一个Docker容器（基于TensorFlow服务镜像），将模型装入容器并通过API提供服务。我们可以发出HTTP请求（嵌入输入），以生成预测。
- en: Answers to exercises
  id: totrans-622
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习答案
- en: '**Exercise 1**'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习1**'
- en: '[PRE56]'
  id: totrans-624
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '**Exercise 2**'
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习2**'
- en: '[PRE57]'
  id: totrans-626
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '**Exercise 3**'
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习3**'
- en: '[PRE58]'
  id: totrans-628
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '**Exercise 4**'
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习4**'
- en: '[PRE59]'
  id: totrans-630
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
