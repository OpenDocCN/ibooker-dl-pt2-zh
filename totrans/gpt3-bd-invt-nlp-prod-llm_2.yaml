- en: Chapter 2\. Using the OpenAI API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even though GPT-3 is the most sophisticated and complex language model in the
    world, its capabilities are abstracted to a simple “text-in, text-out” interface
    to end users. This chapter will get you started with using that interface, Playground,
    and cover the technical nuances of the OpenAI API, because it is always the details
    that reveal the true gems.
  prefs: []
  type: TYPE_NORMAL
- en: To work through this chapter, you will need to sign up for an OpenAI account
    at [*https://beta.openai.com/signup*](https://beta.openai.com/signup). If you
    haven’t done that, please do so now.
  prefs: []
  type: TYPE_NORMAL
- en: Navigating the OpenAI Playground
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Your OpenAI developer account provides access to the API and infinite possibilities.
    We’ll start with the Playground, a web-based sandbox environment that allows you
    to experiment with the API, learn how its components work, and access developer
    documentation and the OpenAI community. We will then show you how to build robust
    prompts that generate favorable responses for your application. We’ll finish the
    chapter with examples of GPT-3 performing four NLP tasks: classification, named
    entity recognition (NER), summarization, and text generation.'
  prefs: []
  type: TYPE_NORMAL
- en: In an interview with Peter Welinder, vice president of product and partnerships
    at OpenAI, we asked for key advice on navigating the Playground for first-time
    users. He told us his advice depends on the persona of the user. If the user has
    a machine learning background, Peter encourages them to “start by forgetting the
    things that they already know, and just go to the Playground and try to get GPT-3
    to do what you [want] it to do by just asking it.” He suggests users “imagine
    GPT-3 as a friend or a colleague that you’re asking to do something. How would
    you describe the task that you want them to do? And then, see how GPT-3 responds.
    And if it doesn’t respond in the way that you want, iterate on your instructions.”
  prefs: []
  type: TYPE_NORMAL
- en: 'As YouTuber and NLP influencer [Bakz Awan](https://oreil.ly/sPTfo) puts it,
    “The non-technical people ask: Do I need a degree to use this? Do I need to know
    how to code to use it? Absolutely not. You can use the Playground. You don’t need
    to write a single line of code. You’ll get results instantly. Anybody can do this.”'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Before you start using the Playground, we recommend reading OpenAI’s [Quickstart
    tutorial](https://oreil.ly/Zivxx) guide and the [developer documentation](https://oreil.ly/btPCR).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the steps to get started with the Playground:'
  prefs: []
  type: TYPE_NORMAL
- en: Log in at [*https://openai.com*](https://openai.com) and navigate to the Playground
    from the main menu.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take a look at the Playground screen ([Figure 2-1](#the_playground_interfacecomma_screensho)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The big text box marked 1 is where you provide text input (prompts).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The box marked 2 on the right is the parameter-setting pane, which enables you
    to tweak the parameters.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The box marked 3 allows you to load a *preset*: an example prompt and Playground
    settings. Provide your own training prompt or load an existing preset.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](Images/gpt3_0201.png)'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
- en: Figure 2-1\. The Playground interface, screenshot taken on January 10, 2022
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Select the existing Q&A preset (marked 3). This will automatically load the
    training prompt along with the associated parameter settings. Click the Generate
    button (marked 4 in [Figure 2-1](#the_playground_interfacecomma_screensho)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The API processes your input and provides a response (called a *completion*)
    in the same text box. It also shows you the number of tokens utilized. *Tokens*
    are numerical representations of words used to determine the pricing of each API
    call; we’ll discuss them later in this chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At the bottom of the screen on the right you’ll see the token count and on the
    left you have a Generate button (see [Figure 2-2](#qampersanda_prompt_completion_along_wit)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](Images/gpt3_0202.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 2-2\. Q&A prompt completion along with token count
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Every time you click the Generate button, GPT-3 takes the prompt and completions
    within the text input field into account and treats them as part of your training
    prompt for the next completion.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here is the prompt you can see in [Figure 2-2](#qampersanda_prompt_completion_along_wit):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'And here is the completion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now that you understand the basic outline of the Playground, let’s get into
    the nitty gritty of prompt engineering and design.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt Engineering and Design
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The OpenAI API radically changed the way we interact with an AI model, stripping
    out layers and layers of complicated programming languages and frameworks. Andrej
    Karpathy, director of AI at Tesla, said jokingly as soon as GPT-3 was released
    that programming 3.0 is all about prompt design (the meme he tweeted is in [Figure 2-3](#meme_source_unknowncomma_tweeted_by_and)).
    There is a direct relation between the training prompt you provide and the quality
    of the completion you get. The structure and arrangement of your words heavily
    influence the output. Understanding prompt design is the key to unlocking GPT-3’s
    true potential.
  prefs: []
  type: TYPE_NORMAL
- en: The secret to writing good prompts is understanding what GPT-3 knows about the
    world. As Awan points out, “It has only seen text. That means you shouldn’t expect
    that it knows about the physical world, even though it obviously does. It could
    describe the Mona Lisa, [could] tell you [about] the significance, the importance,
    the history [of] it probably, but it’s never seen [the painting] because it’s
    only trained on text.”
  prefs: []
  type: TYPE_NORMAL
- en: Your job is to get the model to use the information it already has to generate
    useful results. In the game of charades, the performer gives the other players
    just enough information to figure out the secret word. Similarly, with GPT-3,
    we give the model just enough context (in the form of a training prompt) to figure
    out patterns and perform the given task.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gpt3_0203.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-3\. Meme source unknown, [tweeted by Andrej Karpathy](https://oreil.ly/Fs6hp)
    on June 18, 2020
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'While designing the training prompt, aim for a *zero-shot* response from the
    model: that is, see if you can get the kind of response you want without priming
    the model with external training examples. If not, move forward by showing it
    a few examples rather than an entire dataset. The standard flow for designing
    a training prompt is to try for zero-shot first, then few-shot, then go for corpus-based
    fine-tuning (described later in this chapter).'
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3 is the first step toward general purpose artificial intelligence and thus
    has its limitations. It doesn’t know everything and can’t reason on a human level,
    but it’s highly capable when you know how to talk to it. That’s where the art
    of prompt engineering comes in.
  prefs: []
  type: TYPE_NORMAL
- en: 'GPT-3 isn’t a truth-teller, but it is an exceptional story-teller. It takes
    in the text input and attempts to respond with the text it thinks best completes
    the input. If you give it a few lines from your favorite novel, it will try to
    continue in the same style. It works by navigating through the context, and without
    proper context, it can generate inconsistent responses. Let’s look at an example
    to understand how GPT-3 processes the input prompt and generates the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If you provide a prompt like this to GPT-3 without any context, you are essentially
    asking it to look for general answers from its universe of training data. The
    result will be generalized and inconsistent responses, as the model doesn’t know
    which part of training data to use for answering the question.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, providing the right context will exponentially improve the
    quality of responses. It simply limits the universe of training data that the
    model has to examine for answering a question, resulting in more specific and
    to-the-point responses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: You can think of GPT-3 processing the input in the same way as the human brain.
    When somebody asks us any question without proper context we tend to give random
    responses. This happens because without any proper direction or context, it’s
    difficult to get to the precise response. The same is true of GPT-3; its universe
    of training data is so big that it’s difficult to navigate to a correct response
    without any external context or direction.
  prefs: []
  type: TYPE_NORMAL
- en: 'LLMs like GPT-3 are capable of creative writing and answering factual questions
    given the right context. Here is our five-step formula for creating efficient
    and effective training prompts:'
  prefs: []
  type: TYPE_NORMAL
- en: Define the problem you are trying to solve and what kind of NLP task it is,
    such as classification, Q&A, text generation, or creative writing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ask yourself if there is a way to get a zero-shot solution. If you think that
    you need external examples to prime the model for your use case, think really
    hard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now think of how you might formulate the problem in a textual fashion given
    the “text-in, text-out” interface of GPT-3\. Think about all the possible scenarios
    to represent your problem in textual form. For example, say you want to build
    an ad copy assistant that can generate creative copy by looking at product name
    and description. To frame this goal in the “text-in, text-out” format, you can
    define the input as the product name and description and the output as the ad
    copy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If you do end up using external examples, use as few as possible and try to
    incorporate diversity, capturing all the representations to avoid overfitting
    the model or skewing the predictions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These steps will act as a standard framework whenever you create a training
    prompt from scratch. Before you can build an end-to-end solution for your data
    problems, you need to understand a few more things about how the API works. Let’s
    dig deeper by looking at its components.
  prefs: []
  type: TYPE_NORMAL
- en: How the OpenAI API Works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ll discuss all of these components in [Figure 2-4](#components_of_the_api)
    in more detail in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gpt3_0204.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-4\. Components of the API
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Table 2-1](#components_in_the_openai_api) shows an overview of the components
    in the OpenAI API.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-1\. Components in the OpenAI API
  prefs: []
  type: TYPE_NORMAL
- en: '| Component | Function |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Execution engine | Determines the language model used for execution |'
  prefs: []
  type: TYPE_TB
- en: '| Response length | Sets a limit on how much text the API includes in its completion
    |'
  prefs: []
  type: TYPE_TB
- en: '| Temperature and Top P | Temperature controls the randomness of the response,
    represented as a range from 0 to 1. Top P controls how many random results the
    model should consider for completion, as suggested by the temperature; it determines
    the scope of randomness. |'
  prefs: []
  type: TYPE_TB
- en: '| Frequency penalty and Presence penalty | Frequency penalty decreases the
    likelihood that the model will repeat the same line verbatim by “punishing” it.
    Presence penalty increases the likelihood that it will talk about new topics.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Best of | Lets you specify the number of completions (n) to generate on the
    server side and returns the best of “n” completions |'
  prefs: []
  type: TYPE_TB
- en: '| Stop sequence | Specifies a set of characters that signals the API to stop
    generating completions |'
  prefs: []
  type: TYPE_TB
- en: '| Inject start and restart text | Inject start text allows you to insert text
    at the beginning of the completion. Inject restart text allows you to insert text
    at the end of the completion. |'
  prefs: []
  type: TYPE_TB
- en: '| Show probabilities | Lets you debug the text prompt by showing the probability
    of tokens that the model can generate for a given input |'
  prefs: []
  type: TYPE_TB
- en: Execution Engine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The *execution engine* determines the language model used for execution. Choosing
    the right engine is the key to determining your model’s capabilities and in turn
    getting the right output. GPT-3 comes with four execution engines of varying sizes
    and capabilities: Davinci, Ada, Babbage, and Curie. Davinci is the most powerful
    and the Playground’s default.'
  prefs: []
  type: TYPE_NORMAL
- en: Response Length
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The *response length* sets a limit on how much text the API includes in its
    completion. Because OpenAI charges by the length of text generated per API call
    (as noted earlier, this is translated into tokens, or numeric representations
    of words), response length (also measured in tokens) is a crucial parameter for
    anyone on a budget. A higher response length will use more tokens and cost more.
    For example, if you do a classification task, it is not a good idea to set the
    response text dial to 100: the API could generate irrelevant text and use extra
    tokens that will incur charges on your account. The API supports a maximum of
    2048 tokens in the prompt and completion combined due to technical limitations.
    So, while using the API you need to be careful that the prompt and expected completion
    don’t exceed the maximum response length to avoid abrupt completions. If your
    use case involves large text prompts and completions, the workaround is to think
    of creative ways to solve problems within token limits, such as condensing your
    prompt, breaking the text into smaller pieces, and chaining together multiple
    requests.'
  prefs: []
  type: TYPE_NORMAL
- en: Temperature and Top P
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *temperature* dial controls the creativity of the response, represented
    as a range from 0 to 1\. A lower value of temperature means the API will predict
    the first thing that the model sees, resulting in the most correct, but perhaps
    boring, text, with small variation. On the other hand a higher value of temperature
    means the model evaluates possible responses that could fit into the context before
    predicting the result. The generated text will be more diverse, but there is a
    higher possibility of grammar mistakes and the generation of nonsense.
  prefs: []
  type: TYPE_NORMAL
- en: '*Top P* controls how many random results the model should consider for completion,
    as suggested by the temperature dial; it determines the *scope* of randomness.
    Top P’s range is from 0 to 1\. A value close to zero means the random responses
    will be limited to a certain fraction: for example, if the value is 0.1, then
    only 10% of the random responses will be considered for completion. This makes
    the engine *deterministic*, which means that it will always generate the same
    output for a given input text. If the value is set to 1, the API will consider
    all responses for completion, taking risks and coming up with creative responses.
    A lower value limits creativity; a higher value expands horizons.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Temperature and Top P have a significant effect on output. It can be confusing
    at times to get your head around when and how to use them to get the desired output.
    The two are correlated: changing the value of one will affect the other. So, by
    setting Top P to 1, you can allow the model to unleash its creativity by exploring
    the entire spectrum of responses and control the randomness by using the temperature
    dial.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We always advise changing either Top P or temperature and keeping the dial for
    the other set at 1.
  prefs: []
  type: TYPE_NORMAL
- en: Large language models rely on probabilistic approaches rather than conventional
    logic. They can generate a variety of responses for the same input, depending
    on how you set the model’s parameters. The model tries to find the best probabilistic
    match within the universe of data it has been trained on, instead of looking for
    a perfect solution every time.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we mentioned in [Chapter 1](ch01.xhtml#the_era_of_large_language_models),
    GPT-3’s universe of training data is huge, consisting of a variety of publicly
    available books, internet forums, and Wikipedia articles specially curated by
    OpenAI, allowing it to generate a wide variety of completions for a given prompt.
    That’s where temperature and Top P, sometimes called the “creativity dials,” come
    in: you can tune them to generate more natural or abstract responses with an element
    of playful creativity.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say you are going to use GPT-3 to generate names for your start-up. You
    can set the temperature dial to a higher level to get the most creative response.
    When we were spending days and nights trying to come up with the perfect name
    for our start-up, we dialed up the temperature. GPT-3 came to the rescue and helped
    us to arrive at a name we love: Kairos Data Labs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'On other occasions, your task might require little to no creativity: classification
    and question-answering tasks, for example. For these, keep the temperature lower.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at [Figure 2-5](#temperature_component) with a simple classification
    example that categorizes companies into general buckets or categories based on
    their names.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gpt3_0205.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-5\. Temperature component
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Our prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'And the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in [Figure 2-5](#temperature_component), we have again used temperature
    to control the degree of randomness. You can also do this by changing Top P while
    keeping the temperature dial set to 1.
  prefs: []
  type: TYPE_NORMAL
- en: Frequency and Presence Penalties
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Like the temperature and Top P dials, the frequency penalty and presence penalty
    dials consider text prompts (the previous completion plus the new input) instead
    of internal model parameters when deciding on output. Existing text thus influences
    the new completions. The *frequency penalty* decreases the likelihood that the
    model will repeat the same line verbatim by “punishing” it. The *presence penalty*
    increases the likelihood that it will talk about new topics.
  prefs: []
  type: TYPE_NORMAL
- en: These come in handy when you want to prevent the same completion text from being
    repeated across multiple completions. Although these dials are similar, there
    is one important distinction. The frequency penalty is applied if the suggested
    text output is repeated (for example, the model used the exact token in previous
    completions or during the same session) and the model chooses an old output over
    a new one. The presence penalty is applied if a token is present in a given text
    *at all*.
  prefs: []
  type: TYPE_NORMAL
- en: Best Of
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GPT-3 uses the *best of* feature to generate multiple completions on the server
    side, evaluate them behind the scenes, and then provide you with the best probabilistic
    result. Using the “best of” parameter, you can specify the number of completions
    (*n*) to generate on the server side. The model will return the best of *n* completions
    (the one with the lowest log probability per token).
  prefs: []
  type: TYPE_NORMAL
- en: 'This enables you to evaluate multiple prompt completions in a single API call
    rather than calling the API repeatedly to check the quality of different completions
    for the same input. However, using “best of” is expensive: it costs *n* times
    the tokens in the prompt. For example, if you set the “best of” value to 2, then
    you will be charged double the tokens present in the input prompt because on the
    backend the API will generate two completions and show you the best one.'
  prefs: []
  type: TYPE_NORMAL
- en: “Best of” can range from 1 to 20 depending on your use case. If your use case
    serves clients for whom the quality of output needs to be consistent, then you
    can set the “best of” value to a higher number. On the other hand, if your use
    case involves too many API invocations, then it makes sense to have a lower “best
    of” value to avoid unnecessary latency and costs. We advise keeping response length
    minimal while generating multiple prompts using the “best of” parameter to avoid
    additional charges.
  prefs: []
  type: TYPE_NORMAL
- en: Stop Sequence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A *stop sequence* is a set of characters that signal the API to stop generating
    completions. This helps avoid using unnecessary tokens, an essential cost-saving
    feature for regular users.
  prefs: []
  type: TYPE_NORMAL
- en: You can provide up to four sequences for the API to stop generating further
    tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the example language translation task in [Figure 2-6](#stop_sequence_component)
    to understand how stop sequence works. In this example, English phrases are being
    translated into French. We use the restart sequence “English:” as a stop sequence:
    whenever the API encounters that phrase, it will stop generating new tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gpt3_0206.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-6\. Stop sequence component
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Inject Start Text and Inject Restart Text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *inject start text* and *inject restart text* parameters allow you to insert
    text at the beginning or end of the completion, respectively. You can use them
    to keep a desired pattern going. Often, these settings work in tandem with the
    stop sequence, as in our example. The prompt has the pattern where an English
    sentence is provided with the prefix “English:” (the restart text) and the translated
    output is generated with the prefix “French:” (the start text). As a result, anyone
    can easily distinguish between the two and create a training prompt that both
    the model and the user can clearly comprehend.
  prefs: []
  type: TYPE_NORMAL
- en: Whenever we run the model for such kinds of prompts, it automatically injects
    the start text “French:” before the output and the restart text “English:” before
    the next input, so that this pattern can be sustained.
  prefs: []
  type: TYPE_NORMAL
- en: Show Probabilities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *show probabilities* parameter is at the bottom of the Playground settings
    pane. In conventional software engineering, developers use a *debugger* to troubleshoot
    (debug) a piece of code. You can use the show probabilities parameter to debug
    your text prompt. Whenever you select this parameter, you will see highlighted
    text. Hovering over it with the cursor will show a list of tokens that the model
    can generate for the particular input specified, with their respective probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use this parameter to examine your options. In addition, it can make
    it easier to see alternatives that might be more effective. The show probabilities
    parameter has three settings:'
  prefs: []
  type: TYPE_NORMAL
- en: Most Likely
  prefs: []
  type: TYPE_NORMAL
- en: Lists the tokens most likely to be considered for completion, in decreasing
    order of probability.
  prefs: []
  type: TYPE_NORMAL
- en: Least Likely
  prefs: []
  type: TYPE_NORMAL
- en: Lists the tokens least likely to be considered for completion, in decreasing
    order of probability.
  prefs: []
  type: TYPE_NORMAL
- en: Full Spectrum
  prefs: []
  type: TYPE_NORMAL
- en: Shows the entire universe of tokens that could be selected for completion.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at this parameter in the context of a simple prompt. We want to
    start the output sentence with a simple, well-known phrase: “Once upon a time.”
    We provide the API with the prompt “Once upon a” and then we check the Most Likely
    option in the show probabilities tab.'
  prefs: []
  type: TYPE_NORMAL
- en: As [Figure 2-7](#show_probabilities_component_showing_th) shows, it generates
    “time” as the response. Because we have set the “show probabilities” parameter
    to Most Likely, the API shows not only the response but a list of possible options
    along with their probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you’ve had an overview, let’s look at these components in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gpt3_0207.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-7\. Show probabilities component showing the most likely tokens
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Execution Engines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As noted in [Figure 2-7](#show_probabilities_component_showing_th), the OpenAI
    API offers four execution engines, differentiated by number of parameters and
    performance capabilities. Execution engines power the OpenAI API. They serve as
    “autoML” solutions, providing automated ML methods and processes to make machine
    learning available to nonexperts. They are easy to configure and adapt to a given
    dataset and task.
  prefs: []
  type: TYPE_NORMAL
- en: 'The four primary execution engines were named after famous scientists in alphabetical
    order: Ada (named after Ada Lovelace), Babbage (Charles Babbage), Curie (Madame
    Marie Curie), and Davinci (Leonardo da Vinci). Let’s take a deep dive into each
    of these execution engines to understand when to use which engine when working
    with GPT-3, beginning with Davinci.'
  prefs: []
  type: TYPE_NORMAL
- en: Davinci
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Davinci is the largest execution engine and the default when you first open
    the Playground. It can do anything the other engines can, often with fewer instructions
    and better outcomes. However, the trade-off is that it costs more to use per API
    call and is slower than other engines. You might want to use other engines to
    optimize costs and runtimes.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When testing new ideas and prompts, we recommend starting with Davinci because
    of its superior capabilities. Experimenting with Davinci is a great way to find
    out what the API is capable of doing. You can then slowly move down the ladder
    to optimize budgets and runtimes as you become comfortable with your problem statement.
    Once you have an idea of what you want to accomplish, you can either stay with
    Davinci (if cost and speed are not a concern) or you can move on to Curie or other
    less costly engines and try to optimize the output around its capabilities. You
    can use [OpenAI’s Comparison Tool](https://oreil.ly/EDggA) to generate an Excel
    spreadsheet that compares engines’ outputs, settings, and response times.
  prefs: []
  type: TYPE_NORMAL
- en: Davinci should be your first choice for tasks that require understanding the
    content, like summarizing meeting notes or generating creative ad copy. It’s great
    at solving logic problems and explaining the motives of fictional characters.
    It can even write a story. Davinci has also been able to solve some of the most
    challenging AI problems involving cause and effect.
  prefs: []
  type: TYPE_NORMAL
- en: Curie
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Curie aims to find an optimal balance between power and speed that is very important
    for performing high-frequency tasks like classification on a large scale or putting
    a model into production.
  prefs: []
  type: TYPE_NORMAL
- en: Curie is also quite good at performing Q&As and serving as a general purpose
    chatbot. For instance, if you are building a customer support chatbot, you might
    choose Curie to serve high-volume requests faster.
  prefs: []
  type: TYPE_NORMAL
- en: While Davinci is stronger at analyzing complicated texts, Curie can perform
    with low latency and lightning-fast speed. It is always sensible to figure out
    what your use case is and do a cost-benefit analysis before choosing Davinci over
    Curie in production.
  prefs: []
  type: TYPE_NORMAL
- en: Babbage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Babbage is faster than Curie but not capable of performing tasks that involve
    understanding complex intent. However, it is quite capable and is preferable when
    it comes to semantic search rankings and analyzing how well documents match up
    with search queries. It’s less expensive than Curie and Davinci and is a preferred
    choice for simple problems involving frequent API calls.
  prefs: []
  type: TYPE_NORMAL
- en: Ada
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ada is the fastest and least expensive of the available engines. It can perform
    simple tasks that do not require a nuanced understanding of context, like parsing
    text, correcting grammar, or simple classification. It is often possible to improve
    Ada’s performance by providing more context with the input. For use cases involving
    frequent API invocations, Ada can be the preferred model; with the right configuration
    of settings, it can achieve results similar to bigger models. The more you experiment
    with the API parameters, the more understanding you will gain on what settings
    work for your use case.
  prefs: []
  type: TYPE_NORMAL
- en: Instruct Series
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Based on the four primary models, OpenAI has launched a series of [InstructGPT](https://oreil.ly/fzVk4)
    models that are better at understanding instructions and following them, while
    being less toxic and more truthful than the original GPT-3\. They have been developed
    using techniques coming from OpenAI’s alignment research. These models are trained
    with humans in the loop and are now deployed as the default language models on
    the OpenAI API.
  prefs: []
  type: TYPE_NORMAL
- en: Figures [2-8](#output_generated_by_instructgpt_davinci) and [2-9](#output_generated_by_gpt_davinci_model)
    present two outputs generated by the InstructGPT and GPT series of Davinci engines
    for the same input.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gpt3_0208.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-8\. Output generated by InstructGPT Davinci model
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![](Images/gpt3_0209.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-9\. Output generated by GPT Davinci model
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Our input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'InstructGPT output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'GPT output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'To make the process of building prompts really efficient, OpenAI decided to
    publicly launch the InstructGPT versions of the four models: text-davinci-001,
    text-curie-001, text-babbage-001, and text-ada-001\. With clear instructions,
    these models can produce better results than their base counterparts and are now
    the [default models of the API](https://oreil.ly/LoiuE). This series is an important
    step in bridging the gap between how humans think and how models operate.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We recommend using this model series as your default for all text-related tasks.
    The base versions of GPT-3 models are available as Davinci, Curie, Babbage, and
    Ada and are meant to be used with the fine-tuning, search, classification, and
    answers endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: Endpoints
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Playground is a graphical web interface that calls the OpenAI API behind
    the scenes, but there are several other ways to call the API. To do this, you
    will need to get familiar with its *endpoints*: the remote APIs that communicate
    back and forth when they are called. In this section, you will get familiar with
    the functionality and usage of eight API endpoints.'
  prefs: []
  type: TYPE_NORMAL
- en: List Engines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The *list engines endpoint*, also known as the *metadata endpoint*, provides
    a list of available engines along with specific metadata associated with each
    engine, such as owner and availability. To access it, you can invoke the following
    URI with the HTTP GET method without passing any request parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Retrieve Engine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When you provide an engine name to the *retrieve engine endpoint*, it returns
    detailed metadata about that engine. To access it, invoke the following URI with
    the HTTP GET method without passing any request parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Completions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Completions* is GPT-3’s most famous and widely used endpoint. It simply takes
    in the text prompt as input and returns the completed response as output. It uses
    the HTTP POST method and requires an engine ID as part of the URI path. As part
    of the HTTP Body, the completions endpoint accepts several of the additional parameters
    discussed in the previous section. Its signature is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Semantic Search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *semantic search endpoint* enables you to provide a query in natural language
    to search a set of documents, which can be words, sentences, paragraphs, or even
    longer texts. It will score and rank the documents based on how semantically related
    they are to the input query. For example, if you provide the documents [“school”,
    “hospital”, “park”] and the query “the doctor”, you’ll get a different similarity
    score for each document.
  prefs: []
  type: TYPE_NORMAL
- en: The *similarity score* is a positive score that usually ranges from 0 to 300
    (but can sometimes go higher), where a score above 200 usually indicates that
    the document is semantically similar to the query. The higher the similarity score,
    the more semantically similar the document is to the query (in this example, “hospital”
    will be most similar to “the doctor”). You can provide up to two hundred documents
    as part of your request to the API.^([1](ch02.xhtml#ch01fn3))
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is the signature for the semantic search endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The *files endpoint* can be used across different endpoints like answers, classification,
    and semantic search. It is used to upload documents or files to the OpenAI storage,
    which is accessible throughout the API. The same endpoint can be used with different
    signatures to perform the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: List files
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns a list of the files that belong to the user’s organization or that
    are linked to a particular user account. It’s an HTTP GET call that doesn’t require
    any parameters to be passed with the request:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Upload files
  prefs: []
  type: TYPE_NORMAL
- en: 'Uploads files that contain documents to be used across various endpoints. It
    uploads the documents to the already allocated internal space by OpenAI for the
    user’s organization. It’s a HTTP POST call that requires the file path be added
    with the API request:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Retrieve file
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns information about a specific file by providing the file ID as the request
    parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Delete file
  prefs: []
  type: TYPE_NORMAL
- en: 'Deletes a specific file by providing the file ID as the request parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Classification (Beta)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *classification endpoint* lets you leverage a labeled set of examples without
    fine-tuning. It classifies the query using the provided examples, thereby avoiding
    fine-tuning, and in turn eliminating the need for hyperparameter tuning. You can
    use it for virtually any machine learning classification task.
  prefs: []
  type: TYPE_NORMAL
- en: 'This endpoint provides an easy-to-configure “autoML” solution that can easily
    be adapted to the changing label schema. You can provide up to two hundred labeled
    examples as part of the request, or a pre-uploaded file can be provided during
    the query. In addition to providing a URI path, this endpoint requires a model
    and query, along with examples. Its signature is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Answers (Beta)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GPT-3’s *question-answering endpoint* is still in beta as of this writing in
    late 2021\. When given a question, the QA endpoint generates answers based on
    information provided in a set of documents or training examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if you want to implement a QA endpoint on a set of PDFs, you just
    upload them using the files endpoint and provide the file ID with the request
    parameters. The answers endpoint will use those files as the context to answer
    any query. It also allows you to steer the model’s contextual tone and responses
    by providing a list of (question, answer) pairs in the form of training examples.
    It first searches the provided documents or examples to find the relevant context,
    and then combines it with relevant examples and questions to generate a response.
    Its signature is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another experimental endpoint of the API is *embeddings*. Embeddings are the
    core of any machine learning model and allow you to capture semantics from the
    text by converting it into high-dimensional vectors. Currently, developers tend
    to use open source models to create embeddings for their data that can be used
    for a variety of tasks like recommendation, topic modeling, semantic search, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenAI realized that GPT-3 holds a great potential to power embedding-driven
    use cases and come up with state-of-the-art results. Generating embeddings for
    the input data is very straightforward and wrapped in the form of an API call.
    To create an embedding vector representing the input text, you can use the following
    signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: To invoke the embeddings endpoint, you can choose the type of engine depending
    on your use case by referring to the [embeddings documentation](https://oreil.ly/A8hQN).
    Each engine has its specific dimensions of embedding, with Davinci being the biggest
    and Ada the smallest. All the embedding engines are derived from the four base
    models and classified based on the use cases to allow efficient and cost friendly
    usage.
  prefs: []
  type: TYPE_NORMAL
- en: Customizing GPT-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenAI’s research paper [*“*Process for Adapting Language Models to Society
    (PALMS) with Values-Targeted Datasets*”*](https://oreil.ly/HM7jQ) by Irene Solaiman
    and Christy Dennison (June 2021) led the company to launch a first-of-its-kind
    fine-tuning endpoint that allows you to get more out of GPT-3 than was previously
    possible by customizing the model for your particular use case. (We discuss more
    about PALMS in [Chapter 6](ch06.xhtml#challengescomma_controversiescomma_and).)
    Customizing GPT-3 improves performance of any natural language task GPT-3 is capable
    of performing for your specific use case.
  prefs: []
  type: TYPE_NORMAL
- en: Let us explain how that works first.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI pre-trained GPT-3 on a [specially prepared dataset](https://oreil.ly/IR1SM)
    in a semi-supervised fashion. When given a prompt with just a few examples, it
    can often intuit what task you are trying to perform and generate a plausible
    completion. This is called few-shot learning, as you learned in [Chapter 1](ch01.xhtml#the_era_of_large_language_models).
  prefs: []
  type: TYPE_NORMAL
- en: Users can now fine-tune GPT-3 on their own data, creating a custom version of
    the model tailored to their project. Customizing makes GPT-3 reliable for a variety
    of use cases and makes running the model cheaper, more efficient, and faster.
    *Fine-tuning* is about tweaking the whole model so that it performs every time
    in the way you wish it to perform. You can use an existing dataset of any shape
    and size, or incrementally add data based on user feedback.
  prefs: []
  type: TYPE_NORMAL
- en: The capability and knowledge of the model will be narrowed and focused on the
    contents and semantics of the dataset used for fine-tuning. This in turn will
    limit the range of creativity and topic selections, which will be good for downstream
    tasks like classifying internal documents, or for any use case involving internal
    jargon. It works by focusing the attention of GPT-3 on the fine-tuned data and
    limiting its knowledge base.
  prefs: []
  type: TYPE_NORMAL
- en: Once a model has been fine-tuned, you won’t need to provide examples in the
    prompt anymore. This saves costs, decreases response times, and increases the
    quality and reliability of the outputs. Customizing GPT-3 seems to yield better
    results than what can be achieved with prompt design, because during this process
    you can provide more examples.
  prefs: []
  type: TYPE_NORMAL
- en: With fewer than one hundred examples you can already start seeing the benefits
    of fine-tuning GPT-3, and performance continues to improve as you add more data.
    In the PALMS research paper, OpenAI showed how fine-tuning with fewer than one
    hundred examples can improve GPT-3’s performance on certain tasks. OpenAI also
    found that each doubling of the number of examples tends to improve quality linearly.
  prefs: []
  type: TYPE_NORMAL
- en: Apps Powered by Customized GPT-3 Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Customizing GPT-3 improves the reliability of output, offering more consistent
    results that you can count on for production use cases. Existing OpenAI API customers
    found that customizing GPT-3 could dramatically reduce the frequency of unreliable
    outputs, and there is a growing group of customers that can vouch for it with
    their performance numbers. Let’s look at four companies that have customized GPT-3.
  prefs: []
  type: TYPE_NORMAL
- en: Keeper Tax helps independent contractors and freelancers with their taxes. It
    uses various models to extract text and classify transactions, and then identifies
    easy-to-miss tax write-offs to help customers file their taxes directly from the
    app. By customizing GPT-3, Keeper Tax experienced an increase in accuracy from
    85% to 93%. And it continuously improves thanks to adding 500 new training examples
    to its model once a week, which is leading to about a 1% accuracy improvement
    per week.
  prefs: []
  type: TYPE_NORMAL
- en: Viable helps companies get insights from their customer feedback. By customizing
    GPT-3, Viable was able to transform massive amounts of unstructured data into
    readable natural language reports and increase the reliability of its reports.
    As a result, accuracy in summarizing customer feedback has improved from 66% to
    90%. For an in-depth insight into Viable’s journey, refer to our interview with
    Viable’s CEO in [Chapter 4](ch04.xhtml#gpt_three_as_a_launchpad_for_next_gener).
  prefs: []
  type: TYPE_NORMAL
- en: Sana Labs is a global leader in the development and application of AI to learning.
    The company’s platform powers personalized learning experiences for businesses
    by leveraging the latest ML breakthroughs to personalize content. By customizing
    GPT-3 with its own data, Sana’s question and content generation went from grammatically
    correct but general responses to highly accurate responses. This yielded a 60%
    improvement, enabling more personalized experiences for their users.
  prefs: []
  type: TYPE_NORMAL
- en: 'Elicit is an AI research assistant that helps directly answer research questions
    using findings from academic papers. The assistant finds the most relevant abstracts
    from a large corpus of research papers, then applies GPT-3 to generate the claim
    that the paper makes about the question. A custom version of GPT-3 outperformed
    prompt design and led to improvement in three areas: results were 24% easier to
    understand, 17% more accurate, and 33% better overall.'
  prefs: []
  type: TYPE_NORMAL
- en: How to Customize GPT-3 for Your Application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To get started customizing GPT-3, you’ll just run a single command in the OpenAI
    command line tool with a file you provide. Your custom version will start training
    and then be available immediately in the OpenAI API.
  prefs: []
  type: TYPE_NORMAL
- en: 'At a very high level, customizing GPT-3 for your application involves the following
    three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare new training data and upload it to the OpenAI server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tune the existing models with the new training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the fine-tuned model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prepare and upload training data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Training data is what the model takes in as input for fine-tuning. Your training
    data must be a JSONL document, where each line is a prompt-completion pair corresponding
    to a training example. For model fine-tuning you can provide an arbitrary number
    of examples. It is highly recommended that you create a values-targeted dataset
    (which we’ll define and discuss in [Chapter 6](ch06.xhtml#challengescomma_controversiescomma_and))
    to provide the model with high-quality data and wide representation. Fine-tuning
    improves performance with more examples, so the more examples you provide, the
    better the outcome.
  prefs: []
  type: TYPE_NORMAL
- en: 'Your JSONL document should look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Where *prompt text* should include the exact prompt text you want to complete,
    and *ideal generated text* should include an example of the desired completion
    text that you want GPT-3 to generate.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use OpenAI’s CLI data preparation tool to easily convert your data
    into this file format. The CLI data preparation tool accepts files in different
    formats; the only requirement is that they contain a prompt and a completion column/key.
    You can pass a CSV, TSV, XLSX, JSON, or JSONL file, and the tool will save the
    output into a JSONL file ready for fine-tuning. To do this, use the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Where *LOCAL_FILE* is the file you prepared for conversion.
  prefs: []
  type: TYPE_NORMAL
- en: Train a new fine-tuned model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once you prepare your training data as described above, you can move on to
    the fine-tuning job with the help of the OpenAI CLI. For that, you need the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Where *BASE_MODEL* is the name of the base model you’re starting from (Ada,
    Babbage, Curie, or Davinci). Running this command does several things:'
  prefs: []
  type: TYPE_NORMAL
- en: Uploads the file using the files endpoint (as discussed earlier in this chapter)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tunes the model using the request configuration from the command
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streams the event logs until the fine-tuning job is completed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Log streaming is helpful to understand what’s happening in real time and to
    respond to any incidents/failures as they happen. The streaming may take from
    minutes to hours depending on the number of jobs in the queue and the size of
    your dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Use the fine-tuned model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once the model is successfully fine-tuned, you can start using it! You can now
    specify this model as a parameter to the completion endpoint and make requests
    to it using the Playground.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: After the fine-tuning job completes, it may take several minutes for your model
    to become ready to handle requests. If completion requests to your model time
    out, it is likely because your model is still being loaded. If this happens, try
    again in a few minutes.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can start making requests by passing the model name as the model parameter
    of a completion request using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Where *FINE_TUNED_MODEL* is the name of your model and *YOUR_PROMPT* is the
    prompt you want to complete in this request.
  prefs: []
  type: TYPE_NORMAL
- en: You can continue to use all the completion endpoint parameters that were discussed
    in this chapter, like temperature, frequency penalty, presence penalty, etc.,
    on these requests to the newly fine-tuned model as well.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: No engine is specified on these requests. This is the intended design and something
    that OpenAI plans on standardizing across other API endpoints in the future.
  prefs: []
  type: TYPE_NORMAL
- en: For more information, refer to OpenAI’s [fine-tuning documentation](https://oreil.ly/dSZao).
  prefs: []
  type: TYPE_NORMAL
- en: Tokens
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before diving deeper into how different prompts consume tokens, let’s look more
    closely at what a token is.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve told you that tokens are numerical representations of words or characters.
    Using tokens as a standard measure, GPT-3 can handle training prompts from a few
    words to entire documents.
  prefs: []
  type: TYPE_NORMAL
- en: For regular English text, *1 token consists of approximately 4 characters*.
    It translates to roughly three-quarters of a word, so for one hundred tokens there
    will be approximately 75 words. As a point of reference, the collected works of
    Shakespeare consist of about 900,000 words, which roughly translates to 1.2 million
    tokens.
  prefs: []
  type: TYPE_NORMAL
- en: To maintain the latency of API calls, OpenAI imposes a limit of 2,048 tokens
    (approximately 1,500 words) for prompts and completions.
  prefs: []
  type: TYPE_NORMAL
- en: To further understand how tokens are calculated and consumed in the context
    of GPT-3 and to stay within the limits set by the API, let’s walk through the
    ways you can measure the token count.
  prefs: []
  type: TYPE_NORMAL
- en: In the Playground, as you enter text into the interface, you can see the token
    count update in real time in the footer at the bottom right. It displays the number
    of tokens that will be consumed by the text prompt after hitting the Generate
    button. You can use it to monitor your token consumption every time you interact
    with the Playground (see [Figure 2-10](#token_count_in_the_playground)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gpt3_0210.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-10\. Token count in the Playground
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The other way to measure the consumption of tokens is by using the GPT-3 Tokenizer
    tool ([Figure 2-11](#tokenizer_tool_by_openai)) that lets you visualize the formation
    of tokens from characters. You can interact with the Tokenizer via a simple text
    box where you type the prompt text and Tokenizer shows you the token and character
    counts along with a detailed visualization.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gpt3_0211.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-11\. Tokenizer tool by OpenAI
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For integrating the token count metric in your API calls to different endpoints,
    you can patch the logprobs and echo attributes along with the API request to get
    the full list of tokens consumed.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section we will cover how tokens are priced based on the different
    execution engines.
  prefs: []
  type: TYPE_NORMAL
- en: Pricing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last section we talked about tokens, which is the smallest fungible unit
    used by OpenAI to determine the pricing for API calls. Tokens allow greater flexibility
    than measuring the number of words or sentences used in the training prompt, and
    due to the granularity of tokens, they can be easily processed and used to measure
    the pricing for a wide range of training prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Every time you call the API from either the Playground or programmatically,
    behind the scenes the API calculates the number of tokens used in the training
    prompt along with the generated completion and charges each call on the basis
    of the total number of tokens used.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI generally charges a flat fee per 1,000 tokens, with the fee depending
    on the execution engine used in the API call. Davinci is the most powerful and
    expensive, while Curie, Babbage, and Ada are cheaper and faster.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 2-2](#model_pricing) shows the pricing for the various API engines at
    the time this chapter was written (December 2021).'
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-2\. Model pricing
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Price per 1,000 tokens |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Davinci (most powerful) | $0.0600 |'
  prefs: []
  type: TYPE_TB
- en: '| Curie | $0.0060 |'
  prefs: []
  type: TYPE_TB
- en: '| Babbage | $0.0012 |'
  prefs: []
  type: TYPE_TB
- en: '| Ada (fastest) | $0.0008 |'
  prefs: []
  type: TYPE_TB
- en: The company works on the cloud pricing model of “pay as you go.” For updated
    pricing check the [online pricing schedule](https://oreil.ly/2yKos).
  prefs: []
  type: TYPE_NORMAL
- en: Instead of monitoring the tokens for each API call, OpenAI provides a [reporting
    dashboard](https://oreil.ly/rvMM9) to monitor daily cumulative token usage. Depending
    on your usage, it may look something like [Figure 2-12](#api_usage_dashboard).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gpt3_0212.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-12\. API usage dashboard
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In [Figure 2-12](#api_usage_dashboard) you can see a bar graph showing the daily
    token consumption. The dashboard helps you monitor token usage and costs for your
    organization, so that you can regulate API usage and stay within your budget.
    There is also an option to monitor cumulative usage and get a breakdown of token
    count per API call. This should give you enough flexibility to create policies
    around token consumption and pricing for your organization. Now that you understand
    the ins and outs of the Playground and the API, we will take a look at GPT-3’s
    performance on typical language modeling tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Beginners who have just started with GPT-3 can find it hard to wrap their heads
    around token consumption. Many users enter prompt texts that are too long, which
    leads to the overuse of credits, followed by unexpected fees. To avoid this, during
    your initial days, use the API usage dashboard to observe the number of tokens
    consumed and see how the length of prompts and completions affect token usage.
    It can help prevent uncontrolled use of credits and keep everything within budget.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3’s Performance on Conventional NLP Tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'GPT-3 is a highly advanced and sophisticated successor to the NLP field, built
    and trained using the core NLP approaches and deep neural networks. For any AI-based
    modeling approach, the model performance is evaluated in the following way: First
    you train the model for a specific task (like classification, question and answer,
    text generation, etc.) on training data; then you verify the model performance
    using the test data (new, previously unseen data).'
  prefs: []
  type: TYPE_NORMAL
- en: In a similar way, there is a standard set of NLP benchmarks for evaluating the
    performance of NLP models and coming up with a relative model ranking or comparison.
    This comparison, or *relative ranking*, allows you to pick and choose the best
    model for a specific NLP task (business problem).
  prefs: []
  type: TYPE_NORMAL
- en: In this section we will discuss the performance of GPT-3 on some standard NLP
    tasks as seen in [Figure 2-13](#conventional_nlp_tasks) and compare it with the
    performance of similar models on the respective NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gpt3_0213.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-13\. Conventional NLP tasks
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Text Classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Text classification* is the process of categorizing text into organized groups.
    By using NLP, text classification can automatically analyze text and then assign
    a set of predefined tags or categories based on its context.'
  prefs: []
  type: TYPE_NORMAL
- en: Text classification involves analyzing the text provided as input and assigning
    it a label, score, or other attribute that characterizes it. Some common examples
    of text classification are sentiment analysis, topic labeling, and intent detection.
    You can use a number of approaches to get GTP-3 to classify text, again ranging
    from zero-shot classification (where you don’t give any examples to the model)
    to single-shot and few-shot classification (where you show some examples to the
    model).
  prefs: []
  type: TYPE_NORMAL
- en: Zero-shot classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Modern artificial intelligence has long aimed to develop models that can perform
    predictive functions on data they have never seen before. This important research
    area is called zero-shot learning. Similarly, a *zero-shot classification* is
    a classification task where no prior training or fine-tuning on labeled data is
    required for the model to classify a piece of text. GPT-3 currently produces results
    for unseen data that are either better than or on par with state-of-the-art AI
    models fine-tuned for that specific purpose. To perform zero-shot classification
    with GPT-3, we must provide it with a compatible prompt. Here is an example of
    a zero-shot classification where the goal is to perform a fact-check analysis
    to determine if the information included in the tweet is correct or incorrect.
    [Figure 2-14](#zero_shot_classification_example) shows a pretty impressive information
    correctness classification result based on a zero-shot example.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gpt3_0214.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-14\. Zero-shot classification example
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'And here is our prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'And the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Single-shot and few-shot classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The other approach to text classification is via fine-tuning an AI model on
    a single or a few training examples, known as single-shot or few-shot text classification,
    respectively. When you provide examples of how to classify text, the model can
    learn information about the object categories based on those examples. This is
    a superset of zero-shot classification that allows you to classify text by providing
    the model with three to four diversified examples. This can be useful specifically
    for downstream use cases, which require some level of context setting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the following example of few-shot classification. We are asking
    the model to perform a tweet sentiment analysis classification and giving it three
    tweet examples to illustrate each of the possible labels: positive, neutral, and
    negative. As you can see in [Figure 2-15](#few_shot_classification_example), the
    model, equipped with such a detailed context based on a few examples, is able
    to very easily perform the sentiment analysis of the next tweet.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When you recreate prompt examples from the book, or create your own, make sure
    to have adequate line spacing in your prompt. An additional line after a paragraph
    can result in a very different outcome, so you’ll want to play with that and see
    what works best for you.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gpt3_0215.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-15\. Few-shot classification example
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here is our prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'And the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Batch classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After understanding the few-shot classification with GPT-3, let’s dive deeper
    into classification with batch classification, which enables you to classify input
    samples in batches in a single API call instead of classifying just one example
    per API call. It is suitable for applications where you want to classify multiple
    examples in a single go, just like the tweet sentiment analysis task we examined,
    but analyzing a few tweets in a row.
  prefs: []
  type: TYPE_NORMAL
- en: As with few-shot classification, you want to provide enough context for the
    model to achieve the desired result but in a batch configuration format. Here,
    we define the different categories of tweet sentiment classification using various
    examples in the batch configuration format (Figures [2-16](#batch_classification_example_left_pare)
    and [2-17](#batch_classification_example_left_paren)). Then we ask the model to
    analyze the next batch of tweets.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gpt3_0216.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-16\. Batch-classification example (part 1)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![](Images/gpt3_0217.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-17\. Batch-classification example (part 2)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here is our prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'And the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the model recreated the batch sentiment analysis format and
    classified the tweets successfully. Now let’s move on to see how it performs with
    named entity recognition tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Named Entity Recognition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Named entity recognition (NER) is an information extraction task that seeks
    to locate and classify named entities mentioned in unstructured text into predefined
    categories such as person names, organizations, locations, expressions of time,
    quantities, monetary values, percentages, etc.
  prefs: []
  type: TYPE_NORMAL
- en: NER helps to make the responses more personalized and relevant but the current
    state-of-the-art approaches require massive amounts of data for training before
    you even start with the prediction. GPT-3, on the other hand, can recognize general
    entities like people, places, and organizations out of the box without humans
    providing even a single training example.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example we use a davinci-instruct-series version of the model
    that was in beta at the time of writing this book, and the model gathers prompts
    to train and improve the future OpenAI API models. We give it a simple task: to
    extract contact information from an example email. It successfully completes the
    task on the first attempt ([Figure 2-18](#ner_example)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gpt3_0218.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-18\. NER example
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here is our input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'And the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Text Summarization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Text summarization* is a technique for generating a concise and exact summary
    of lengthy texts while focusing on the sections that convey useful information,
    without losing the overall meaning. GPT-3-based text summarization aims to transform
    lengthy pieces of tl;dr^([2](ch02.xhtml#ch01fn4)) texts into their condensed versions.
    Such tasks are generally difficult and costly to accomplish manually. With GPT-3,
    it is a matter of a single input and a few seconds!'
  prefs: []
  type: TYPE_NORMAL
- en: NLP models can be trained to comprehend documents and identify the sections
    that convey important facts and information before producing the summarized texts.
    However, such models need a large amount of training samples before they can learn
    the context and start summarizing unseen input.
  prefs: []
  type: TYPE_NORMAL
- en: 'GPT-3’s abstractive summarization is the key to solving the problem of information
    extraction. By producing summaries instead of merely extracting key information,
    GPT-3 can provide a more comprehensive and accurate understanding of the text.
    It uses a zero-shot or few-shot approach toward text summarization, making it
    useful for a variety of use cases. With GPT-3 there are multiple ways you can
    go about summarizing the text depending on your use case: basic summaries, one-line
    summaries, or grade-level summaries. Let’s have a quick walk-through of these
    approaches.'
  prefs: []
  type: TYPE_NORMAL
- en: Most of the time the model is able to generate decent results in the form of
    a review summary, but sometimes it can output irrelevant results depending on
    the prior context. To avoid the problem of getting unwanted results, you can set
    the “best of” parameter to 3, which will always give you the best of three results
    generated by the API. In the example shown in [Figure 2-19](#text_summarization_example),
    after a few tries and minor parameter tweaking, we got decent results.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gpt3_0219.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-19\. Text summarization example
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here is our prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'And the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Since many people spend hours per day reading and writing emails, summarizing
    them is a widely desired use case for GPT-3\. Let’s see how GPT-3 does with summarizing
    a three-paragraph email into one crisp line ([Figure 2-20](#email_summarization_example)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gpt3_0220.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-20\. Email summarization example
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To achieve the desired result, we pasted the full email and then simply added
    “one-sentence summary:” at the end. We also included a “.” stop sequence to tell
    the model that it should stop its summary generation after a single sentence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'And the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Text Generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before the introduction of GPT-3, there was a general understanding that AI
    models were capable of carrying on short conversations with humans that answered
    specific questions or handled specific tasks. However, the models were not sophisticated
    enough to handle complicated text generation tasks, and they started to lose track
    of the conversation whenever they encountered something complex or abstract.
  prefs: []
  type: TYPE_NORMAL
- en: In the complicated world of natural language generation, GPT-3 has shaken the
    notion of language models being limited to trivial tasks. Text generation is the
    greatest strength of GPT-3\. It is capable of generating textual content that
    is almost indistinguishable from human-written text. GPT-3 is trained on billions
    of words from the training dataset to generate text in response to a variety of
    prompts. It generates an average of 4.5 billion words per day, [according to OpenAI](https://oreil.ly/fbyhM).
  prefs: []
  type: TYPE_NORMAL
- en: In the next two examples, we experiment with using GPT-3 to create content for
    a personal productivity app start-up and social media posts. We give the model
    only minimal context, and it generates many of the responses in Figures [2-21](#article_generation_example)
    and [2-22](#social_media_post_generation_example) on the first take.
  prefs: []
  type: TYPE_NORMAL
- en: Article generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](Images/gpt3_0221.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-21\. Article generation example
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Our prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'And the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Social media post generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](Images/gpt3_0222.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-22\. Social media post generation example
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here is our prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'And the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we have covered the OpenAI Playground, prompt engineering, and
    the different components of the OpenAI API, followed by Playground examples covering
    the major NLP tasks. By now, you should have an understanding of how the API works
    in tandem with different components and how to use the Playground as the base
    to design and experiment with different training prompts.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll walk you through how to use GPT-3 with different
    programming languages to integrate the API in your product or build a completely
    new application from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch02.xhtml#ch01fn3-marker)) For more than two hundred documents, OpenAI
    offers a [beta API](https://oreil.ly/cY0Z6).
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch02.xhtml#ch01fn4-marker)) A longstanding internet abbreviation for “too
    long; didn’t read.”
  prefs: []
  type: TYPE_NORMAL
