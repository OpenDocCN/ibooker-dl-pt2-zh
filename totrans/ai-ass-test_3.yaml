- en: 3 AI, automation, and testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Outlining the value of good testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appreciating how tools help in testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying when AI tools could be of use in testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before we begin our journey into the use of LLMs in testing, we need to take
    a short detour and ask ourselves the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the purpose and value of testing?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can tooling help us what that purpose?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When is it appropriate to use AI tools?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It may feel like asking these fundamental questions is an unnecessary activity.
    But if you are someone who sees testing merely as a confirmatory exercise, as
    in executing test cases to confirm a requirement is correct, then your mileage
    from the subsequent chapters will be limited. Understanding the value and performance
    of testing is critical for determining how tools can be used in a useful way.
    So in this chapter, we’re going to explore why a deeper understanding of testing
    can help us to utilize tools effectively. That said, if you are someone who already
    has that deep understanding of testing, feel free to skim this chapter and move
    on. For the rest of us, let’s go back to square one by asking the question, why
    do we test?
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 The value of testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To help us appreciate having a strong foundation of why we need testing in
    software development, let’s return to the common perspective that testing is a
    confirmatory exercise. By this we mean that testing is viewed as something that
    is done to confirm these conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: Written requirements have been met.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All critical paths have been covered in a system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The system has no bugs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Teams that have this perspective tend to overly rely on the use of test cases/scripts
    that contain explicit instructions for a human, or machine, to follow and confirm
    whether an expected outcome has been met. The issue with this mindset and approach
    is less that it’s utilizing test scripts and more that it only uses test scripts
    and nothing else. Many biases come from an over-reliance on test scripts, but
    if we bring it back to the usage of LLMs in testing, then it limits our appreciation
    of how these tools can help us. When tools like ChatGPT grew in popularity, a
    large majority of demonstrations and debates around the use of LLMs in testing
    were focused on one thing: Test scripts. People would demonstrate how LLMs could
    generate test scripts that could manually be executed by either a human or a test
    automation tool.'
  prefs: []
  type: TYPE_NORMAL
- en: Though initially these might have some use, the options for what else could
    be done with LLMs to help testing began to dry out. On the surface, this appears
    to be a limitation of the tooling in question, but instead, the real issue is
    that limited idea of what testing is and how it can help. So, if we are to expand
    our use of LLMs in testing, we have to first expand our understanding of what
    testing is and how it works.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1 A different way of thinking about testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To help us establish a deeper understanding, let’s explore a model of testing
    that I use to define what I believe testing is for and how it helps, which is
    shown in Figure 3.1.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.1 A model of testing to help describe the value and purpose of testing
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The model, based upon one created by James Lyndsay in his paper ''Exploration
    and Strategy'', [https://www.workroom-productions.com/why-exploration-has-a-place-in-any-strategy](www.workroom-productions.com.html),
    is made up of two circles. The left circle represents imagination, or what it
    is that we *want* in a product, and the right circle represents implementation,
    or what it is that we *have* in a product. The purpose of testing is to learn
    as much about what''s going on in each of these circles by carrying out testing
    activities. The more we test in these two circles, the more we learn. We can then:'
  prefs: []
  type: TYPE_NORMAL
- en: Discover potential issues that might impact the quality.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Overlap these two circles of information, ensuring we have the understanding
    and confidence that we are building the product or service we want to build.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To help describe this further, let's look at an example in which a team is delivering
    a hypothetical search feature that we want to ensure is delivered to a high degree
    of quality.
  prefs: []
  type: TYPE_NORMAL
- en: Imagination
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The Imagination circle represents what it is that we want from our product,
    and that includes expectations that are both explicit and implicit. So in this
    circle, our testing is focused on learning as much about those explicit and implicit
    expectations. By doing this, we not only learn what has been explicitly stated
    in writing or verbally shared but also dig down into the details and remove ambiguity
    over terms and ideas. Let''s say a representative of the business or a user, such
    as a product owner, has shared this requirement with their team:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The explicit information shared here tells us that the product owner wants
    search results, ordered by relevance. However, a lot of implied information can
    be uncovered by testing the ideas and concepts behind what is being asked. This
    might come in the form of a series of questions we could ask, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: What is meant by relevant results?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relevant to whom?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What information is shared?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we order by relevancy?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What data should we use?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By asking these questions, we form a fuller picture of what is wanted, remove
    any misunderstandings in our team's thinking and identify potential risks that
    could impact those expectations. If we know more about what we are being asked
    to build, then we're more likely to build the right thing the first time.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'By testing the imagination, we get a stronger sense of what it is we are being
    asked to build. But, just because we might know what to build doesn''t mean we
    end up with a product that matches those expectations. This is why we also test
    the Implementation to learn:'
  prefs: []
  type: TYPE_NORMAL
- en: Whether the product matches our expectations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How the product might not match our expectations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Both goals are of equal importance. We want to ensure that we have built the
    right thing, but there will always be side effects, such as unintended behavior,
    vulnerabilities, missed expectations and downright weirdness that might appear
    in our products. With our search results example, we could not only test that
    the feature delivers results in the relevant order but also ask the product these
    questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What if I use different search terms?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What if the relevant results don't match the behavior of other search tools?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What if part of the service is down when I search?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What if I request results 1000 times in less than 5 seconds?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What happens if there are no results?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By exploring beyond our expectations, we become more aware of what is going
    on in our product—warts and all. This ensures we don't end up making incorrect
    assumptions about how our product behaves and releasing a poor-quality product.
    It also means if we find unexpected behavior, we have the choice to attempt to
    remove or readjust our expectations.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 A holistic approach to testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: What the model of testing the imagination and implementation demonstrates is
    that the testing goes beyond a simple confirmation of expectations and lays out
    a more holistic approach to testing. By implementing different activities that
    focus on the imagination and implementation spaces, we learn more through the
    testing we execute about what we want to build and what we have built. The more
    we learn in these two areas, the more they align with one another. And the more
    they align, the more accurate our perception of quality becomes.
  prefs: []
  type: TYPE_NORMAL
- en: A team that is well-informed about their work has a better idea of the quality
    of their product. We are then also better equipped to decide what steps to take
    to improve quality. This enables us to decide to focus our attention on specific
    risks, make changes in our product to align with users’ expectations or determine
    what issues we want to invest time in to fix and which to leave alone. This is
    the value of good testing—to help teams get into a position where they can make
    these informed decisions and feel confident in the steps they are taking to develop
    a high-quality product.
  prefs: []
  type: TYPE_NORMAL
- en: To help us better appreciate this model, let’s consider a sample context in
    which testing is required. For our example, we are responsible for the delivery
    of a fast food ordering system. Users log on, find the restaurant they want to
    order from, place their order (which is sent to the restaurant) and then track
    the delivery of their order from within the product. A product like this one would
    need to be easy to use, highly available and secure. So to deliver a high-quality
    product, we might need to utilize different testing activities to focus on different
    types of risks, as shown in Figure 3.2.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.2 The imagination/implementation model with sample activities shown
    within it
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image003.png)'
  prefs: []
  type: TYPE_IMG
- en: Within the model, we can see a range of various activities that are placed in
    specific areas because they are focused on specific types of risks. For example,
    on the imagination side, we might be concerned with risks that impact the usability
    of a site. So we employ testing activities that focus on user experience testing
    and collaborative design. On the implementation side, we want to ensure the product
    is stable and minimize risks that might impact the availability and usage of the
    product. So we employ activities such as exploratory testing and performance testing.
    Finally, notice how in the area that overlaps, we have added in test scripts.
    These are useful to us because they mitigate risks around unexpected changes appearing
    in the product as it grows and morphs over time, demonstrating that test scripting
    and automated checks are of use, but are only one part of the holistic whole.
  prefs: []
  type: TYPE_NORMAL
- en: Each of these activities has different implementations, different challenges
    and different ways in which tooling can help. But we wouldn’t be able to easily
    identify these usages if we didn’t have that understanding of testing being an
    exercise in information gathering and knowledge sharing. With this model in place,
    and with an appreciation of the many different risks our work faces and the testing
    activities that help mitigate them, we can begin to start drilling deeper into
    how tooling plays a part in testing.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 How tools help with testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You’ll likely hear a tester say (or you may even have said it yourself) that
    there is never enough time to test everything. It will likely be repeated in this
    book a few times. Teams are always limited by time, budgets, meetings, staffing
    and other factors, so in order to implement and execute effective, modern testing,
    we have to rely on tools to help us. Tools are essential to testing, but they
    bring us to our next misconception around testing—that a tool, or machine, can
    test like a human can.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 Automation bias
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To appreciate how machines and humans differ in regard to testing, let’s consider
    an example where both are used to test a feature on a website. The feature is
    a full-width banner for an e-commerce website with an image and some text to highlight
    the deal of the day. Initially, we test it manually, or in a human-led way, and
    we observe the feature is working correctly: the image is shown and all the text
    associated with it is correct. Then we decide to use tools to automate this testing.
    We create code that will open the browser and assert that element A, which is
    where the deal of the day is loaded, exists. We run the automated test, and it
    passes. And then one day, after another successful release in which all our automated
    tests pass, an end-user raises a bug and informs us they can’t see the deal of
    the day. All they see is an empty white box at the top of the page.'
  prefs: []
  type: TYPE_NORMAL
- en: 'What happened? During the process of creating the automated test, we have transferred
    our knowledge, which is built up from implicit information based on mental heuristics
    and oracles, and made it explicit. We have stripped a complex understanding of
    a feature down to a single instruction: element A should exist on a web page.
    So when the latest release of our product went out with a fault function to retrieve
    the deal of the day, or the CSS was incorrect or broken, the automated test still
    passes, because element A still exists. However, it takes a human a matter of
    seconds to see something is wrong.'
  prefs: []
  type: TYPE_NORMAL
- en: The moral of this story isn’t that tools are bad or unnecessary, but that they
    are often misused or misinterpreted. This behavior is a type of *automation bias*
    that creeps into our perspective on the value of tooling, where the issue is that
    we ascribe more value to the output of a tool than what it is relaying to us.
    That is, when we designed our automated test to look for element A, we assumed
    that all we as humans were doing was looking for element A. But we were considering
    many other factors, even if we weren’t doing it consciously.
  prefs: []
  type: TYPE_NORMAL
- en: If we fall prey to automation bias, we run the risk of selecting and implementing
    tools that we believe can reveal and report information in the same way as a human
    can—when in fact they don’t, leading to misguided overconfidence in the products
    we’re delivering, or generating a level of workload to have tools emulate human
    behavior that is unsustainable for a modern project. Tools cannot replace testing
    activities, and subscribing to that notion will ultimately lead to issues with
    quality and an increase in risks to a project. So instead, we have to shift our
    thinking more towards how tools can help support our testing.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Being selective with tooling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Success with tools comes from situations in which some thought has been put
    into the problem we want to solve and what tools could potentially help. To help
    us better understand this, let’s return to our deal of the day feature and look
    closer at what a human is doing when they test a feature like this.
  prefs: []
  type: TYPE_NORMAL
- en: First, we consider different ways of testing the feature. We use our current
    understanding around said feature to formulate test ideas and select what we want
    to test first. Next, we need to set up our test. This might include setting up
    an environment or creating/updating the necessary test data. (We might need to
    create a deal of the day to observe as well as test users to administer and view
    the deal.) With everything set up, we then need to execute our test, loading the
    browser or perhaps multiple browsers to verify that the deal renders correctly.
    Then once we’ve observed the results, we take notes or report our findings to
    our team, all of which would update our understanding of the feature, ready for
    us to start the process again. This flow can be summarized in a visualization
    like the one shown in Figure 3.3.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.3 A visualization of the process of testing the deal of the day feature
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image005.png)'
  prefs: []
  type: TYPE_IMG
- en: This cycle may be something that happens rapidly—for example, in an exploratory
    testing session. Or it may take place in a longer form, such as performance testing,
    in which each step has many details to consider. Regardless of the type of activity,
    to carry out the loop successfully, we need tools to complete the process. We
    likely would need to use tools such as database clients, test data managers or
    infrastructure tools to set up state management. We would use note-taking tools,
    screenshot applications and project management tools to report what has been learned.
    We can summarize these uses of tools in testing by updating our initial visualization,
    as shown in Figure 3.4.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.4 A visualization of the process of testing, this time with the addition
    of tools
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image007.png)'
  prefs: []
  type: TYPE_IMG
- en: The visualization demonstrates how modern testing utilizes a range of tools
    for various tasks that occur in testing—rather than attempt to wrap the whole
    testing performance into the use of one tool. This is because, as the visualization
    demonstrates, there are many different activities at play when testing is carried
    out, and typically a successful tool is one that does one job well, as opposed
    to a tool that does many things poorly.
  prefs: []
  type: TYPE_NORMAL
- en: What is so interesting about this thinking is that when we take the time to
    consider it, it seems obvious to us. We all use tools to help us with distinct
    tasks that make up a larger activity. However, most of us do it without conscious
    or deliberate thought. Although we know that the use of tools in specific tasks
    is sensible, we need to develop the skills to start choosing and using tools with
    intention. This means familiarizing ourselves with tools and being more in tune
    with what we’re doing daily in our testing so that we can pick the right tool
    or, in the case of LLMs, the right prompt or enhancement. This is what the rest
    of this book will be focusing on, giving us the skills needed to know when LLMs
    can help with specific tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Knowing when to use LLMs in testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve now developed an understanding of how effective tool use in testing is
    about looking to enhance tasks within testing rather than replicate broad testing
    activities. But where do LLMs fit into this? Given their broad applications, let’s
    look at some of the common ways in which LLMs are currently being used and see
    how each can be used to support testing.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1 Generative capabilities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The label of generative AI has been applied to LLMs because they offer the ability
    to generate novel natural language outputs. By this we mean if we asked an LLM
    to generate a poem about the wonders of software testing, it would, although it
    may run the risk of repeating material it had been trained on—and would probably
    be a terrible poem. But putting quality to one side for a moment, the ability
    to have LLMs generate new outputs based on its model and the prompts it has been
    given is what has captured a lot of attention.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, we must remind ourselves that how an LLM generates outputs is based
    on a complex model of probabilities. This means it will generate material in a
    way that differs from how we as humans might generate ideas. A good example of
    this is the use of LLMs to generate test cases. If you recall in Chapter 1 we
    explored the concept that when sending a prompt like the one below to an LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'It might respond with something similar to this (shortened for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: How an LLM has generated these test cases is impressive, but they lack context
    and specificity to the feature or product we are testing. So instead, when working
    with LLMs, we need to make sure that how we prompt them or extend them gives them
    enough context to generate outputs that are valuable when used.
  prefs: []
  type: TYPE_NORMAL
- en: The consequence of ensuring that we provide sufficient context is that it’s
    easier to use LLMs to generate outputs for very specific and targeted tasks. The
    alternative means we would have to provide a massive amount of input data that
    would result in a prompt that is expensive to build and maintain. For example,
    imagine the amount of context you would have to put into an LLM to get a test
    strategy that was relevant to our working context.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, we can get more value from LLMs if we focus on using them to help
    with tasks such as these:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Test data generation** - LLMs when given explicit rules around data sets
    can be used to generate rapid sets of data for use in a range of testing activities
    from exploratory to performance testing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Suggestions for risks and test ideas** - We should always avoid letting the
    output of an LLM be the sole arbiter of what to test and what not to test. We
    can use them to suggest test ideas and risks that can be used as jumping-off points
    for new ideas or factored into our existing work.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Code snippets** - Similar to the test case example earlier, we gain little
    value from LLMs if we ask them to generate complete automated tests or frameworks.
    However, using them to generate smaller parts of automation or scripts that are
    used to support testing activities like exploratory testing can be advantageous.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3.3.2 Transformation capabilities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another benefit that LLMs offer is the ability to transform natural language
    from one structure to another. A common example of LLM transformation is language
    translation. Suppose that we sent something like this to an LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Then it will return a response such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a useful way to illustrate how LLMs transform data, but we shouldn’t
    be restricted to just spoken language. LLMs are capable of transforming all types
    of data from one abstraction to another. Here are some examples that can help
    with testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Transforming test data** - Using LLMs to rapidly transform data from one
    structure to another can help speed up testing. For example, we might ask an LLM
    to convert plain-text test data into SQL statements, or to convert SQL statements
    into helper functions that are called in test automation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Converting code** - LLMs can convert functions, classes and other data into
    new iterations of code. What makes this valuable is that LLMs can transform code
    into different languages but still keep the logic and flow of the original code
    in the newly translated output (although we should always test it to be sure).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Summarizing notes** - Though not as direct a conversion of data as, say,
    converting a code snippet from one language to another, we can use LLMs to transform
    and summarize at the same time. We can use LLMs to take raw testing notes from
    testing activities such as exploratory testing or shift-left testing sessions,
    and have them converted into summary notes to be shared with others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3.3.3 Enhancing capabilities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Finally, we can use LLMs to enhance and expand existing material. This usually
    means providing an LLM with a snippet of data and prompting the LLM to expand
    upon it. This has some overlap with the generative capabilities because we’re
    asking LLMs to generate a certain degree of new output, but in this situation,
    we’re providing a lot more upfront context and instructing it to focus on existing
    material, as opposed to prompting an LLM to generate something completely new.
    This means we can use this ability to help us with testing tasks such as these:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reviewing code** - Not all who work in testing are confident coders, and
    even those of us who are comfortable with reading code can struggle at times to
    make sense of the code we are required to analyze or test. LLMs can enhance our
    understanding by taking code snippets and providing a natural language breakdown
    of how said code works, which can help with risk analysis, test design and more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Descriptive code** - Similar to reviewing code, we can use LLMs to help improve
    the descriptiveness of code—for example, rapidly creating code comments that can
    be easily created and maintained. This can be especially useful for automated
    testing where communicating what our automation code is doing is important for
    maintenance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Expanding analysis** - We can also use LLMs to expand our analysis activities
    such as risk analysis and shift left testing (where we ask questions about requirements
    before the feature is built). By providing it with our current analysis data,
    we can ask LLMs to review and expand upon it, suggesting new ideas that we can
    either incorporate into our analysis or ignore.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3.3.4 LLMs in use in testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To put these different abilities of an LLM into context, let’s return to our
    visualization of testing with the support of tools, as shown in Figure 3.5.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.5 A visualization of the process of testing, this time with the addition
    of LLMs
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image009.png)'
  prefs: []
  type: TYPE_IMG
- en: Looking through this updated visualization, we can see how LLMs can be inserted
    into distinct and specific tasks within the wider testing lifecycle. It brings
    us back to our area of effect model that we covered in Chapter 1\. Instead of
    attempting to use LLMs to replicate the full gamut of testing activities that
    exist within a lifecycle, we prioritize the best of our abilities as humans and
    the value we bring to testing. Then we choose to add LLMs in select areas to expand
    our work so that we can move faster, learn more and help ensure our teams are
    better informed so that they can build higher-quality products.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If we have a limited understand of what testing is, then the use of tools will
    also be limited.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing is not a confirmatory exercise, but rather a collection of different
    activities that help those seeking to learn about the imagination and implementation
    of a product.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The imagination relates to our understanding of what we want to build.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The implementation relates to our understanding of what we have built.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As our understanding of both aspects increases, they align, helping us to deliver
    a higher quality product.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We run many different types of testing activities to focus on different types
    of risk and how they impact both imagination and implementation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tools are an essential component of successful testing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can be used to generate, transform and enhance outputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use LLMs with smaller, specific tasks to generate outputs that provide value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can generate content for us that can help with specific tasks or create
    suggested content.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can transform data to convert raw data into useful formats or summarize
    it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can also enhance existing material, adding new suggestions or expanding
    details.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can insert LLMs into many distinct and specific testing tasks, which reflects
    the area of effect model we learned about in chapter 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
