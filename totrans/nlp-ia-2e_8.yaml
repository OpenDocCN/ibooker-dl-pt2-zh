- en: 8 Reduce, Reuse, Recycle Your Words (RNNs and LSTMs)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 减少、重用和回收单词（RNN 和 LSTMs）
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖的内容
- en: Unrolling recursion so you can understand how to use it for NLP
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积递归展开，以便您可以了解如何将其用于 NLP。
- en: Implementing word and character-based RNNs in PyTorch
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 PyTorch 中实现基于单词和字符的 RNN。
- en: Identifying applications where RNNs are your best option
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别 RNN 是您最好的选择的应用程序。
- en: Re-engineering your datasets for training RNNs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新设计您的数据集以进行 RNN 训练。
- en: Customizing and tuning your RNN structure for your NLP problems
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定制和调整您的 RNN 结构以解决 NLP 问题。
- en: Understanding backprop (backpropagation) in time
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解时间反向传播（backprop）。
- en: Combining long and short-term memory mechanisms to make your RNN smarter
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将长期和短期记忆机制相结合，使您的 RNN 变得更加智能。
- en: An *RNN* (Recurrent Neural Network) recycles tokens. Why would you want to recycle
    and reuse your words? To build a more sustainable NLP pipeline of course! ;) *Recurrence*
    is just another word for recycling. An RNN uses recurrence to allow it to remember
    the tokens it has already read and reuse that understanding to predict the target
    variable. And if you use RNNs to predict the next word, RNNs can generate, going
    on and on and on, until you tell them to stop. This sustainability or regenerative
    ability of RNNs is their superpower.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 *RNN*（递归神经网络）会重复使用词汇。为什么要重复和重用您的单词？当然是为了构建更可持续的 NLP 管道！;) *递归* 只是另一个词汇，用于循环利用。RNN
    使用递归来记住它已经阅读过的标记，并重复利用这种理解来预测目标变量。如果您使用 RNN 来预测下一个单词，RNN 可以生成一直生成，直到你告诉它停止。 RNN
    的这种可持续性或再生能力是它们的超级能力。
- en: It turns out that your NLP pipeline can predict the next tokens in a sentence
    much better if it remembers what it has already read and understood. But, wait,
    didn’t a CNN "remember" the nearby tokens with a kernel or filter of weights?
    It did! But a CNN can only *remember* a limited window, that is a few words long.
    By recycling the machine’s understanding of each token before moving to the next
    one, an RNN can remember something about *all* of the tokens it has read. This
    makes your machine reader much more sustainable, it can keep reading and reading
    and reading…​for as long as you like.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 原来，您的 NLP 管道可以更好地预测句子中的下一个标记，如果它记得它已经阅读并理解了什么。但是，等一下，之前的 CNN 是用一组权重来“记住”附近的标记的吗？是的！但是
    CNN 只能 *记住* 有限的窗口，即几个单词长。通过在转到下一个标记之前循环利用机器对每个标记的理解，RNN 可以记住关于它阅读过的 *所有* 标记的内容。这使得您的机器阅读器更具可持续性，它可以不停地读下去……您喜欢多久它就能读多久。
- en: But wait, isn’t recursion dangerous? If that’s the first thought that came to
    you when you read recurrence, you’re not alone. Anyone who has taken an algorithms
    class has probably broken a function, an entire program, or even taken down an
    entire web server, but using recurrence the wrong way. The key to doing recurrence
    correctly and safely is that you must always make sure your algorithm is *reducing*
    the amount of work it has to do with each recycling of the input. This means you
    need to delete something from the input before you call the function again with
    that input. For your NLP RNN, this comes naturally as you *pop* (remove) a token
    off of the *stack* (the text string) before you feed that input back into your
    network.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 但是等一下，递归不危险吗？如果您在读到递归时第一个想到的是危险，那么您并不孤单。任何学过算法的人都可能使用不正确的递归方式，破坏了函数、整个程序，甚至拖垮了整个网络服务器。正确和安全地使用递归的关键是您必须始终确保您的算法在每次输入回收时*减少*它必须进行的工作量。这意味着您需要在再次使用该输入之前从输入中删除一些内容。对于您的
    NLP RNN，这是自然而然的，因为您会在将输入馈送回网络之前，*弹出*（删除）堆栈（文本字符串）上的一个标记。
- en: Note
  id: totrans-12
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注
- en: Technically "recurrence" and "recursion" are two different things. ^([[1](#_footnotedef_1
    "View footnote.")]) But most mathematicians and computer scientists use both words
    to explain the same concept - recycling a portion of the output back into the
    input to perform an operation repeatedly in sequence. ^([[2](#_footnotedef_2 "View
    footnote.")]) But as with all natural language words, the concepts are fuzzy and
    it can help to understand them both when building *Recurrent* Neural Networks.
    As you’ll see in the code for this chapter, an RNN doesn’t have a function that
    calls itself recursively the way you normally think of recursion. The `.forward(x)`
    method is called in a `for` loop that is outside of the RNN itself.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，“循环”和“递归”是两个不同的概念。^[[1]](#_footnotedef_1) 但是大多数数学家和计算机科学家使用这两个词来解释相同的概念
    - 将部分输出循环回输入，以便重复执行序列中的操作。^[[2]](#_footnotedef_2) 但是像所有自然语言词汇一样，这些概念是模糊的，当构建 *循环*
    神经网络时理解它们可能会有所帮助。正如你将在本章的代码中看到的那样，RNN 并没有像你通常认为的那样具有调用自身的递归函数。`.forward(x)` 方法是在
    RNN 本身之外的 `for` 循环中调用的。
- en: RNNs are *neuromorphic*. This is a fancy way of saying that researchers are
    mimicking how they think brains work when they design artificial neural nets such
    as RNNs. You can use what you know about how your own brain works to come up with
    ideas for how to process text with artificial neurons. And your brain is recurrently
    processing the tokens that you are reading right now. So recurrence must be a
    smart, efficient way to use your brain resources to understand text.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 是 *类神经形* 的。这是一种花哨的说法，意思是研究人员在设计诸如 RNN 这样的人工神经网络时模仿了他们认为大脑如何工作的方式。你可以利用你对自己大脑运作方式的了解来想出如何使用人工神经元处理文本的方法。你的大脑正在循环处理你正在阅读的标记。所以循环必定是一种聪明、高效的利用大脑资源来理解文本的方式。
- en: As you read this text you are recycling what you already know about the previous
    words before updating your prediction of what’s going to happen next. And you
    don’t stop predicting until you reach the end of a sentence or paragraph or whatever
    you’re trying to understand. Then you can pause at the end of a text and process
    all of what you’ve just read. Just like the RNNs in this chapter, the RNN in your
    brain uses that pause at the end to encode, classify, and *get something out of*
    the text. And because RNNs are always predicting, you can use them to predict
    words that your NLP pipeline should say. So RNNs are great not only for reading
    text data but also for tagging and writing text.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当你阅读这段文字时，你会利用你已经了解的先前单词的知识来更新你对接下来会发生什么的预测。并且在你达到句子、段落或者你试图理解的任何东西的结尾之前，你不会停止预测。然后你可以在文本的结尾停顿一下，处理你刚刚阅读过的所有内容。就像本章中的
    RNN 一样，你大脑中的 RNN 利用这个结尾的停顿来对文本进行编码、分类和 *得到一些* 信息。由于 RNN 总是在预测，你可以用它们来预测你的 NLP
    流水线应该说的单词。所以 RNN 不仅适用于阅读文本数据，还适用于标记和撰写文本。
- en: RNNs are a game changer for NLP. They have spawned an explosion of practical
    applications and advancements in deep learning and AI.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 对 NLP 是一个颠覆性的改变。它们引发了深度学习和人工智能的实际应用和进步的爆炸性增长。
- en: 8.1 What are RNNs good for?
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 RNN 适用于什么？
- en: The previous deep learning architectures you’ve learned about are great for
    processing short bits of text - usually individual sentences. RNNs promise to
    break through that text length barrier and allow your NLP pipeline to ingest an
    infinitely long sequence of text. And not only can they process unending text,
    but they can also *generate* text for as long as you like. RNNs open up a whole
    new range of applications like generative conversational chatbots and text summarizers
    that combine concepts from many different places within your documents.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经学过的先前的深度学习架构对处理短小的文本片段非常有效 - 通常是单个句子。 RNN 承诺打破文本长度的限制，并允许你的自然语言处理流水线摄入无限长的文本序列。它们不仅可以处理无穷尽的文本，还可以
    *生成* 你喜欢的文本。RNN 打开了一系列全新的应用，如生成式对话聊天机器人和将来自文档的许多不同地方的概念结合起来的文本摘要器。
- en: '| **Type** | **Description** | **Applications** |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| **类型** | **描述** | **应用** |'
- en: '| One to Many | One input tensor used to generate a sequence of output tensors
    | Generate chat messages, answer questions, describe images |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 一对多 | 一个输入张量用于生成一系列输出张量 | 生成聊天消息、回答问题、描述图像 |'
- en: '| Many to One | sequence of input tensors gathered up into a single output
    tensor | Classify or tag text according to its language, intent, or other characteristics
    |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 多对一 | 被收集成单个输出张量的输入张量序列 | 根据语言、意图或其他特征对文本进行分类或标记 |'
- en: '| Many to Many | a sequence of input tensors used to generate a sequence of
    output tensors | Translate, tag, or anonymize the tokens within a sequence of
    tokens, answer questions, participate in a conversation |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 多对多 | 一系列输入张量用于生成一系列输出张量 | 在一系列标记中翻译、标记或匿名化标记，回答问题，参与对话 |'
- en: This is the superpower of RNNs, they process sequences of tokens or vectors.
    You are no longer limited to processing a single, fixed-length vector. So you
    don’t have to truncate and pad your input text to make your round text the right
    shape to fit into a square hole. And an RNN can generate text sequences that go
    on and on forever if you like. You don’t have to stop or truncate the output at
    some arbitrary maximum length that you decide ahead of time. Your code can dynamically
    decide when enough is enough.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 RNNs 的超能力，它们处理标记或向量的序列。你不再受限于处理单个、固定长度的向量。因此，你不必截断和填充输入文本，使你的圆形文本形状适合于方形洞。如果愿意，RNN
    可以生成永无止境的文本序列。你不必在预先决定的任意最大长度处停止或截断输出。你的代码可以在足够的时候动态决定什么是足够的。
- en: Figure 8.1 Recycling tokens creates endless options
  id: totrans-24
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.1 回收标记创建了无尽的选项
- en: '![rnn unrolled many to many drawio](images/rnn-unrolled-many-to-many_drawio.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![rnn unrolled many to many drawio](images/rnn-unrolled-many-to-many_drawio.png)'
- en: You can use RNNs to achieve state-of-the-art performance on many of the tasks
    you’re already familiar with, even when your text is shorter than infinity `;)`.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 RNNs 在许多你已经熟悉的任务上取得最先进的性能，即使你的文本比无穷小 `;)` 还要短。
- en: translation
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 翻译
- en: summarization
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 摘要
- en: classification
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类
- en: question answering
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问答
- en: 'And RNNs are one of the most efficient and accurate ways to accomplish some
    new NLP tasks that you will learn about in this chapter:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs 是实现一些新的 NLP 任务最有效和准确的方法之一，你将在本章中了解到：
- en: generating new text such as paraphrases, summaries or even answers to questions
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成新的文本，如释义、摘要甚至是问题的答案
- en: tagging individual tokens
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对单个标记进行标记
- en: diagramming the grammar of sentences, as you did in English class
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绘制句子的语法框图，就像你在英语课上做的那样
- en: creating language models that predict the next token
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建预测下一个标记的语言模型
- en: If you read through the RNNs that are at the top of the leader board on Papers
    with Code ^([[3](#_footnotedef_3 "View footnote.")]) you can see that RNNs are
    the most efficient approach for many applications.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你阅读过论文榜上的 RNNs，你会发现 RNNs 是许多应用中最有效的方法。
- en: 'RNNs aren’t just for researchers and academics. Let’s get real. In the real
    world, people are using RNNs to:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs 不仅仅是给研究人员和学者使用的。让我们来认真对待。在现实世界中，人们正在使用 RNNs 来：
- en: spell checking and correction
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拼写检查和更正
- en: autocompletion of natural language or programming language expressions
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言或编程语言表达的自动补全
- en: classify sentences for grammar checking or FAQ chatbots
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对句子进行语法检查或 FAQ 聊天机器人进行分类
- en: classify questions or generate answers to those questions
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对问题进行分类或生成这些问题的答案
- en: generate entertaining conversational text for chatbots
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为聊天机器人生成有趣的对话文本
- en: named entity recognition (NER) and extraction
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命名实体识别（NER）和提取
- en: classify, predict, or generate names for people, babies, and businesses
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对人、婴儿和企业进行分类、预测或生成名称
- en: classify or predict subdomain names (for security vulnerability scanning
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类或预测子域名（用于安全漏洞扫描）
- en: You can probably guess what most of those applications are about, but you’re
    probably curious about that last one (subdomain prediction). A subdomain is that
    first part of a domain name in a URL, the `www` in `www.lesswrong.com` or `en`
    in `en.wikipedia.org`. Why would anyone want to predict or guess subdomains? Dan
    Meisler did a talk on the critical role that subdomain guessers play in his cybersecurity
    toolbox.^([[4](#_footnotedef_4 "View footnote.")]) Once you know a subdomain,
    a hacker or pentester can scan the domain to find vulnerabilities in the server
    security.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能可以猜到这些应用的大部分内容，但你可能对最后一个应用（子域名预测）感到好奇。子域名是 URL 中域名的第一部分，比如 `www.lesswrong.com`
    中的 `www` 或 `en.wikipedia.org` 中的 `en`。为什么有人要预测或猜测子域名？Dan Meisler 在他的网络安全工具箱中讨论了子域名猜测器发挥的关键作用。一旦你知道一个子域名，黑客或渗透测试人员就可以扫描域名，找出服务器安全的漏洞。
- en: And once you will soon be comfortable using RNNs to generate completely new
    words, phrases, sentences, paragraphs and even entire pages of text. It can be
    so much fun playing around with RNNs that you could find yourself accidentally
    creating applications that open up opportunities for completely new businesses.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你很快就能熟练地使用RNNs生成全新的单词、短语、句子、段落，甚至整页的文字。使用RNNs玩耍可以非常有趣，你可能会不经意间创造出开启全新业务机会的应用程序。
- en: suggest company, product or domain names ^([[5](#_footnotedef_5 "View footnote.")])
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建议公司、产品或领域名称 ^([[5](#_footnotedef_5 "查看脚注。")])
- en: suggest baby names
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建议婴儿姓名
- en: sentence labeling and tagging
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子标注和标记
- en: autocomplete for text fields
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本字段的自动完成
- en: paraphrasing and rewording sentences
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对句子进行释义和改写
- en: inventing slang words and phrases
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发明俚语词汇和短语
- en: 8.1.1 RNNs can handle any sequence
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.1 RNNs可以处理任何序列
- en: In addition to NLP, RNNs are useful for any sequence of numerical data, such
    as time series. You just need to represent the objects in your sequence as numerical
    vectors. For natural language words this is often the word embedding. But you
    can also see how a city government might represent daily or hourly electric scooter
    rentals, freeway traffic, or weather conditions as vectors. And often they will
    want to predict all of this simultaneously in one vector.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 除了NLP之外，RNNs对于任何数值数据序列都是有用的，比如时间序列。你只需要将序列中的对象表示为数值向量。对于自然语言词汇，这通常是词嵌入。但你也可以看到一个城市政府如何将每日或每小时的电动滑板车租赁、高速公路交通或天气条件表示为向量。而且通常他们希望一次性在一个向量中预测所有这些。
- en: 'Because RNNs can output something for each and every element in a sequence,
    you can create an RNN that outputs a prediction for "tomorrow" — the next sequence
    element after the one you currently know. You can then use that prediction to
    predict the one after that, recursively. This means that once you master backpropagation
    through time, you will be able to use RNNs to predict things such as:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 因为RNNs可以为序列中的每个元素输出结果，所以你可以创建一个RNN，用于预测“明天”——当前已知元素之后的下一个序列元素。然后，你可以使用该预测来预测下一个预测，递归地进行。这意味着一旦你掌握了时序反向传播，你就能够使用RNNs来预测诸如：
- en: The next day’s weather
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 明天的天气
- en: The next minute’s web traffic volume
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下一分钟的网站流量量
- en: The next second’s Distributed Denial of Services (DDOS) web requests
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下一秒的分布式拒绝服务（DDOS）网络请求
- en: The action an automobile driver will take over the next 100 milliseconds
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 汽车驾驶员在接下来的100毫秒内将采取的动作
- en: The next image in a sequence of frames in a video clip
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视频剪辑序列中的下一帧图像
- en: As soon as you have a prediction of the target variable you can measure the
    error - the difference between the model’s output and the desired output. This
    usually happens at the last time step in whatever sequence of events you are processing.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你对目标变量有了预测，你就可以衡量错误——模型输出与期望输出之间的差异。这通常发生在你正在处理的事件序列中的最后一个时间步骤。
- en: 8.1.2 RNNs remember everything you tell them
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.2 RNNs会记住你告诉它们的一切
- en: Have you ever accidentally touched wet paint and found yourself "reusing" that
    paint whenever you touched something? And as a child, you might have fancied yourself
    an impressionistic painter as you shared your art with the world by finger-painting
    the walls around you. You’re about to learn how to build a more mindful impressionistic
    word painter. In chapter 7 you imagined a lettering stencil as an analogy for
    processing text with CNNs. Well now, instead of sliding a word stencil across
    the words in a sentence you are going to roll a paint roller across them…​ while
    they’re still wet!
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否曾经不小心触摸到潮湿的油漆，并发现自己在触碰到东西时“重复使用”那种油漆？小时候，你可能会想象自己是一位印象派画家，通过用手指在周围的墙壁上涂油彩的方式与世界分享你的艺术。你将要学会如何建造一个更加专注的印象派文字画家。在第7章中，你想象了一个字母模板作为用CNNs处理文本的类比。现在，与其在句子中滑动一个单词模板，不如在它们还潮湿的时候用油漆辊滚动它们...​！
- en: Imagine painting the letters of a sentence with slow-drying paint and laying
    it on thick. And let’s create a diverse rainbow of colors in your text. Maybe
    you’re even supporting LBGTQ pride week by painting the crosswalks and bike lanes
    in North Park.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，用干得慢的颜料给句子的字母涂上厚厚的一层。让我们在文本中创造出多样化的彩虹颜色。也许你甚至正在支持北公园的LBGTQ自豪周，给人行道和自行车道涂上斑马线。
- en: Figure 8.2 A rainbow of meaning
  id: totrans-66
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.2 意义的彩虹
- en: '![wet paint rainbow lettering drawio](images/wet-paint-rainbow-lettering_drawio.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![潮湿的油漆彩虹字母绘制图](images/wet-paint-rainbow-lettering_drawio.png)'
- en: Now, pick up a clean paint roller and roll it across the letters of the sentence
    from the beginning of the sentence to the end. Your roller would pick up the paint
    from one letter and recycle it to lay it back down on top of the previous letters.
    Depending on how big your roller is, a small number of letters (or parts of letters)
    would be rolled on top of the letters to the right. All the letters after the
    first one would be smeared together to create a smudgy stripe that only vaguely
    resembles the original sentence.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，拿起一个干净的油漆滚筒，将其从句子的开头滚到结尾的字母上。你的滚筒会从一个字母上取下油漆，并将其重新放在先前字母的顶部。根据你的滚筒大小，少量的字母（或字母部分）会被滚到右边的字母上。第一个字母后的所有字母都会被涂抹在一起，形成一个模糊的条纹，只能模糊地反映出原始句子。
- en: Figure 8.3 Pot of gold at the end of the rainbow
  id: totrans-69
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.3 彩虹尽头的一锅金币
- en: '![wet paint rainbow lettering smudged drawio](images/wet-paint-rainbow-lettering-smudged_drawio.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![湿油漆彩虹字母被涂抹的drawio](images/wet-paint-rainbow-lettering-smudged_drawio.png)'
- en: The smudge gathers up all the paint from the previous letters into a single
    compact representation of the original text. But is it a useful, meaningful representation?
    For a human reader, all you’ve done is create a multicolored mess. It wouldn’t
    communicate much meaning to the humans reading it. This is why humans don’t use
    this *representation* of the meaning of the text for themselves. However, if you
    think about the smudge of characters you might be able to imagine how a machine
    might interpret it. And for a machine, it is certainly much more dense and compact
    than the original sequence of characters.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 涂抹将先前字母的所有油漆汇集成原始文本的一个紧凑表示。但这是一个有用的、有意义的表示吗？对于人类读者来说，你所做的只是创建了一个多彩的混乱。它对于阅读它的人类来说并不传达多少意义。这就是为什么人类不会为自己使用这种*文本含义*的表示方式。然而，如果你考虑一下字符的涂抹，也许你能想象出机器是如何解释它的。对于机器来说，它肯定比原始字符序列要密集和紧凑得多。
- en: In NLP we want to create compact, dense vector representations of text. Fortunately,
    that representation we’re looking for is hidden on your paint roller! As your
    fresh clean roller got smeared with the letters of your text it gathered up a
    *memory* of all the letters you rolled it across. This is analogous to the word
    embeddings you created in Chapter 6\. But this embedding approach would work on
    much longer pieces of text. You could keep rolling the roller forever across more
    and more text, if you like, squeezing more and more text into the compact representation.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理中，我们希望创建文本的紧凑、密集的向量表示。幸运的是，我们正在寻找的那种表示隐藏在你的油漆滚筒上！当你的干净的新滚筒被文本的字母涂抹时，它收集了你滚过的所有字母的*记忆*。这类似于你在第6章创建的词嵌入。但这种嵌入方法可以用于更长的文本片段。如果你愿意，你可以不停地滚动滚筒，不断地将更多的文本压缩成紧凑的表示。
- en: In previous chapters, your tokens were mostly words or word n-grams. You need
    to expand your idea of a token to include individual characters. The simplest
    RNNs use characters rather than words as tokens. This is called a character-based
    RNN. Just as you had word and token embeddings in previous chapters you can think
    of characters having meaning too. Now does it make more sense how this smudge
    at the end of the "Wet Paint!" lettering represents an embedding of all the letters
    of the text?
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在以前的章节中，你的标记主要是单词或单词 n-gram。你需要扩展你对标记的理解，将个别字符包括在内。最简单的RNN使用字符而不是单词作为标记。这被称为基于字符的RNN。就像你在之前的章节中有单词和标记嵌入一样，你也可以认为字符也有意义。现在，你能理解这个在"Wet
    Paint!"字母末尾的涂抹如何表示文本所有字母的嵌入吗？
- en: One last imaginary step might help you bring out the hidden meaning in this
    thought experiment. In your mind, check out that embedding on your paint roller.
    In your mind roll it out on a fresh clean piece of paper. Keep in mind the paper
    and your roller are only big enough to hold a single letter. That will *output*
    a compact representation of the paint roller’s memory of the text. And that output
    is hidden inside your roller until you decide to use it for something. That’s
    how the text embeddings work in an RNN. The embeddings are *hidden* inside your
    RNN until you decide to output them or combine them with something else to reuse
    them. In fact, this vector representation of your text is stored in a variable
    called `hidden` in many implementations of RNNs.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个想象中的步骤可能会帮助你揭示这个思想实验中的隐藏含义。在你的脑海中，检查一下你的油漆滚筒上的嵌入。在你的脑海中，将其在一张干净的纸上滚开。记住，纸和你的滚筒只大到能容纳一个单独的字母。这将
    *输出* 滚筒对文本的记忆的紧凑表示。而这个输出隐藏在你的滚筒里，直到你决定用它做点什么。这就是 RNN 中文本嵌入的工作方式。嵌入被隐藏在你的 RNN 中，直到你决定输出它们或与其他东西结合以重用它们。事实上，在许多
    RNN 实现中，文本的这种向量表示存储在名为 `hidden` 的变量中。
- en: Important
  id: totrans-75
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 重要
- en: RNN embeddings are different from the word and document embeddings you learned
    about in Chapters 6 and 7\. An RNN is gathering up meaning over time or text position.
    An RNN encodes meaning into this vector for you to reuse with subsequent tokens
    in the text. This is like the Python `str.encode()` function for creating a multi-byte
    representation of Unicode text characters. The order in which the sequence of
    tokens is processed matters a lot to the end result, the encoding vector. So you
    probably want to call RNN embeddings "encodings", "encoding vectors" or "encoding
    tensors." This vocabulary shift was encouraged by Garrett Lander on a project
    to do NLP on extremely long and complex documents, such as patient medical records
    or The Meuller Report.^([[6](#_footnotedef_6 "View footnote.")]) This new vocabulary
    made it a lot easier for his team to develop a shared mental model of the NLP
    pipeline.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 嵌入与你在第 6 章和第 7 章学到的单词和文档嵌入不同。RNN 在时间或文本位置上聚集意义。RNN 将意义编码到这个向量中，以便你可以在文本中重复使用后续的标记。这就像
    Python 的 `str.encode()` 函数，用于创建 Unicode 文本字符的多字节表示。标记序列处理的顺序对最终结果，即编码向量，至关重要。所以你可能想把
    RNN 嵌入称为 "编码"、"编码向量" 或 "编码张量"。这种词汇转变是在 Garrett Lander 的一个项目中受到鼓励的，该项目是对非常长且复杂的文档进行自然语言处理，例如患者病历或《穆勒报告》。[[6](#_footnotedef_6
    "查看脚注。")] 这种新的词汇使他的团队更容易发展起自然语言处理管道的共享心理模型。
- en: Keep your eye out for the hidden layer later in this chapter. The activation
    values are stored in the variable `h` or `hidden`. These activation values within
    this tensor are your embeddings up to that point in the text. It’s overwritten
    with new values each time a new token is processed as your NLP pipeline is gathering
    up the meaning of the tokens it has read so far. In figure [8.4](#ch8_best_figure)
    you can see how this blending of meaning in an embedding vector is much more compact
    and blurry than the original text.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章后面要密切关注隐藏层。激活值存储在变量 `h` 或 `hidden` 中。这个张量内的这些激活值是文本中到目前为止的嵌入。每次处理一个新标记时，它都会被新值覆盖，因为你的自然语言处理管道正在汇总它到目前为止已读取的标记的含义。在图
    [8.4](#ch8_best_figure) 中，你可以看到这种在嵌入向量中汇集含义的混合要比原始文本更加紧凑和模糊。
- en: Figure 8.4 Gather up meaning into one spot
  id: totrans-78
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.4 汇集含义到一个点中
- en: '![wet paint rainbow lettering smudged encoding drawio](images/wet-paint-rainbow-lettering-smudged-encoding_drawio.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![湿油漆彩虹字母污迹编码 drawio](images/wet-paint-rainbow-lettering-smudged-encoding_drawio.png)'
- en: You could read into the paint smudge something of the meaning of the original
    text, just like in a Rorschach inkblot test. Rorschach inkblots are smudges of
    ink or paint on flashcards used to spark people’s memories and test their thinking
    or mental health.^([[7](#_footnotedef_7 "View footnote.")]) Your smudge of paint
    from the paint roller is a vague, impressionistic representation of the original
    text. And it’s a much more compact representation of the text. This is exactly
    what you were trying to achieve, not just creating a mess. You could clean your
    roller, rinse and repeat this process on a new line of text to get a different
    smudge with a different *meaning* for your neural network. Soon you’ll see how
    each of these steps is analogous to the actual mathematical operations going on
    in an RNN layer of neurons.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从油漆印迹中读出一些原始文本的含义，就像罗夏克墨点测试一样。罗夏克墨点是指用在纸牌上的墨水或油漆印迹，用于激发人们的记忆并测试他们的思维或心理健康^([[7](#_footnotedef_7
    "查看脚注")])。你油漆辊上的油漆印迹是原始文本的模糊、印象派式的呈现。这是你要达成的目标，而不仅是制造一团糟。你可以清洁你的辊子，冲洗并重复这个过程，得到不同的油漆印迹，这些印迹代表了你的神经网络的不同*含义*。很快你就会看到，这些步骤与RNN神经元层中的实际数学操作是相似的。
- en: Your paint roller has smeared many of the letters at the end of the sentence
    so that the last exclamation point at the end is almost completely unintelligible.
    But that unintelligible bit at the end is exactly what your machine needs to understand
    the entire sentence within the limited surface area of the paint roller. You have
    smudged all the letters of the sentence together onto the surface of your roller.
    And if you want to see the message embedded in your paint roller, you just roll
    it out onto a clean piece of paper.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 你的油漆辊沾污了句子末尾的许多字母，以至于末尾的感叹号几乎完全无法辨认。但正是这不可理解的部分，使你的机器能够在油漆辊的有限表面积内理解整个句子。你已经把句子的所有字母都涂到油漆辊的表面上了。如果你想看到油漆辊嵌入的信息，只需把它滚到一张干净的纸上即可。
- en: In your RNN you can accomplish this by outputting the hidden layer activations
    after you’ve rolled your RNN over the tokens of some text. The encoded message
    probably won’t say much to you as a human, but it gives your paint roller, the
    machine, a hint at what the entire sentence said. Your paint roller gathered an
    impression of the entire sentence. We even use the word "gather" to express understanding
    of something someone says, as in "I gather from what you just said, that rolling
    paint rollers over wet paint are analogous to RNNs."
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的RNN中，你可以在将RNN滚动文本标记后输出隐藏层激活。对于人类来讲，编码信息可能不会有很多意义，但它给了你的油漆辊，即机器，整个句子的暗示。你的油漆辊收集了整个句子的印象。我们甚至使用“收集”这个词来表达对某人说的话的理解，就像“我从你刚刚说的话中收集到，将湿漆辊辊在湿漆上与RNN是相似的。”
- en: Your paint roller has compressed, or encoded, the entire sentence of letters
    into a short smudgy impressionistic stripe of paint. In an RNN this smudge is
    a vector or tensor of numbers. Each position or dimension in the encoding vector
    is like a color in your paint smudge. Each encoding dimension holds an aspect
    of meaning that your RNN has been designed to keep track of. The impressions that
    the paint made on your roller (the hidden layer activations) were continuously
    recycled till you got to the end of the text. And then you reused all those smudges
    on your roller to create a new impression of the entire sentence.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 你的油漆辊已将整个字母句子压缩或编码成一个短小的、模糊印象派风格的油漆条纹。在RNN中，这个印迹是一个由数字组成的向量或张量。编码向量中的每个位置或维度就像你的油漆印迹中的一个颜色。每个编码维度都保留着一个意义方面，你的RNN被设计成跟踪这些方面的含义。油漆在辊子上留下的印象（隐藏层激活）被持续回收，直到文本的末尾。接着，将所有这些印迹再次应用在油漆辊的新位置上，创建一个整个句子的新印象。
- en: 8.1.3 RNNs hide their understanding
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.3 RNNs隐藏他们的理解
- en: The key change for an RNN is that it maintains a hidden embedding by recycling
    the meaning of each token as it reads them one at a time. This hidden vector of
    weights contains everything the RNN has understood up to the point in the text
    it is reading. This means you can’t run the network all at once on the entire
    text you’re processing. In previous chapters, your model learns a function that
    maps one input to one output. But, as you’ll soon see, an RNN learns a *program*
    that keeps running on your text until it’s done. An RNN needs to read your text
    one token at a time.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 RNN 来说，一个关键的改变是通过逐个读取令牌来重复使用每个令牌的含义而维护一个隐藏嵌入。这个包含了 RNN 所理解的一切的权重隐藏向量包含在它所读取的文本点中。这意味着你不能一次性运行整个你正在处理的文本的网络。在先前的章节中，你的模型学习了将一个输入映射到一个输出的函数。但是，接下来你将看到，RNN
    会学习一个程序，在你的文本上不断运行，直到完成。RNN 需要逐个读取你的文本的令牌。
- en: An ordinary feedforward neuron just multiplies the input vector by a bunch of
    weights to create an output. No matter how long your text is, a CNN or feedforward
    neural network will have to do the exact same number of multiplications to compute
    the output prediction. The neurons of a linear neural network all work together
    to compose a new vector to represent your text. You can see in Figure [8.5](#ordinary-feedforward-neuron)
    that a normal feedforward neural network takes in a vector input (`x`), multiplies
    it by a matrix of weights (`W`), applies an activation function, and then outputs
    a transformed vector (`y`). Feedforward network layers transform can only transform
    one vector into another.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一个普通的前馈神经元只是将输入向量乘以一堆权重来创建输出。无论你的文本有多长，CNN 或者前馈神经网络都必须执行相同数量的乘法来计算输出预测。线性神经网络的神经元一起工作，组合出一个新的向量来表示你的文本。
    在图[8.5](#ordinary-feedforward-neuron)中可以看到，一个普通的前馈神经网络接受一个向量输入(`x`)，将其乘以一组权重矩阵(`W`)，应用激活函数，然后输出一个转换过的向量(`y`)。前馈网络层只能将一个向量转换为另一个向量。
- en: Figure 8.5 Ordinary feedforward neuron
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.5 普通的前馈神经元
- en: '![neuron feedforward drawio](images/neuron-feedforward_drawio.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![neuron feedforward drawio](images/neuron-feedforward_drawio.png)'
- en: With RNNs, your neuron never gets to see the vector for the entire text. Instead,
    an RNN must process your text one token at a time. To keep track of the tokens
    it has already read it records a hidden vector (`h`) that can be passed along
    to its future self - the exact same neuron that produced the hidden vector in
    the first place. In computer science terminology this hidden vector is called
    a *state*. That’s why Andrej Karpathy and other deep learning researchers get
    so excited about the effectiveness of RNNs. RNNs enable machines to finally learn
    Turing complete programs rather than just isolated functions.^([[8](#_footnotedef_8
    "View footnote.")])
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 RNNs 时，你的神经元不会看到整个文本的向量。相反，RNN 必须逐个令牌处理你的文本。为了跟踪已经读取的令牌，它记录一个隐藏向量(`h`)，可以传递给未来自己——产生隐藏向量的完全相同的神经元。在计算机科学术语中，这个隐藏向量被称为
    *状态*。这就是为什么 Andrej Karpathy 和其他深度学习研究人员对 RNNs 的效果如此兴奋的原因。RNNs 使得机器终于能够学习 Turing
    完备程序而不只是孤立的函数.^[[8](#_footnotedef_8 "View footnote.")]
- en: Figure 8.6 A neuron with recurrence
  id: totrans-90
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.6 循环神经元
- en: '![neuron with recurrence drawio](images/neuron-with-recurrence_drawio.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![neuron with recurrence drawio](images/neuron-with-recurrence_drawio.png)'
- en: If you unroll your RNN it begins to look a lot like a chain…​ a Markov Chain,
    in fact. But this time your window is only one token wide and you’re reusing the
    output from the previous token, combined with the current token before rolling
    forward to the next token in your text. Fortunately, you started doing something
    similar to this when you slid the CNN window or kernel across the text in Chapter
    7.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你展开你的 RNN，它开始看起来像一个链……实际上是一个马尔可夫链。但这一次，你的窗口只有一个标记的宽度，并且您重用了先前标记的输出，结合当前标记，然后向前滚动到文本的下一个标记。庆幸的是，当你在第7章中滑动
    CNN 窗口或卷积核时，已经开始做类似的事情。
- en: How can you implement neural network recurrence in Python? Luckily, you don’t
    have to try to wrap around a recursive function call like you may have encountered
    in coding interviews. Instead, all you have to do is create a variable to store
    the hidden state separate from the inputs and outputs. And you need to have a
    separate matrix of weights to use for computing that hidden tensor. Listing [8.1](#listing-recurrence-pytorch)
    implements a minimal RNN from scratch, without using PyTorch’s `RNNBase` class.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何在 Python 中实现神经网络的递归？幸运的是，你不必像在编程面试中遇到的那样尝试使用递归函数调用。相反，你只需创建一个变量来存储与输入和输出分开的隐藏状态，并且你需要有一个单独的权重矩阵用于计算隐藏张量。[列表
    8.1](#listing-recurrence-pytorch) 实现了一个最小的 RNN，从头开始，而不使用 PyTorch 的 `RNNBase` 类。
- en: Listing 8.1 Recurrence in PyTorch
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.1 PyTorch 中的递归
- en: '[PRE0]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You can see how this new RNN neuron now outputs more than one thing. Not only
    do you need to return the output or prediction, but you also need to output the
    hidden state tensor to be reused by the "future self" neuron.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到这个新的 RNN 神经元现在输出不止一件事。你不仅需要返回输出或预测，而且需要输出隐藏状态张量以供“未来自己”神经元重用。
- en: Of course, the PyTorch implementation has many more features. PyTorch RNNs can
    even be trained from left to right and right to left simultaneously! This is called
    a bidirectional language model. Of course, your problem needs to be "noncausal"
    for a bidirectional language model to be of any use. A noncausal model in NLP
    for English just means that you want your language model to predict words that
    occur before (to the left of) other words that you already know. A common noncausal
    application is to predict interior words that have been masked out intentionally
    or accidentally corrupted during OCR (Optical Character Recognition). If you’re
    curious about bidirectional RNNs, all of the PyTorch RNN models (RNNs, GRUs, LSTMs,
    and even Transformers) include an option to turn on bidirectional recurrence.^([[9](#_footnotedef_9
    "View footnote.")]) For question-answering models and other difficult problems,
    you will often see a 5-10% improvement in the accuracy of bidirectional models
    when you compare them to the default forward direction (causal) language models.
    This is simply because their embeddings of a bidirectional language model are
    more balanced, forgetting as much about the beginning of the text as they forget
    about the end of the text.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，PyTorch 实现有许多其他特性。PyTorch 中的 RNNs 甚至可以同时从左到右和从右到左训练！这被称为双向语言模型。当然，你的问题需要是“非因果”的，才能使用双向语言模型。在英语
    NLP 中，非因果模型意味着你希望语言模型预测你已经知道的其他单词之前（左边）出现的单词。一个常见的非因果应用是预测在 OCR（光学字符识别）期间有意或无意地被屏蔽或损坏的内部单词。如果你对双向
    RNNs 感兴趣，所有的 PyTorch RNN 模型（RNNs、GRUs、LSTMs，甚至 Transformers）都包括一个选项来启用双向递归。对于问答模型和其他困难的问题，与默认的向前方向（因果）语言模型相比，双向模型的准确率通常会提高
    5-10%。这仅仅是因为双向语言模型的嵌入更加平衡，忘记了文本开头和结尾的内容一样多。
- en: 8.1.4 RNNs remember everything you tell them
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.4 RNNs 记得你告诉它们的一切
- en: To see how RNNs retain a memory of all the tokens of a document you can unroll
    the neuron diagram in Figure 8.7\. You create copies of the neuron to show the
    "future selves" in the `for` loop that is iterating through your tokens. This
    is like unrolling a `for` loop, when you just copy and paste the lines of code
    within the loop the appropriate number of times.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解 RNNs 如何保留文档中所有标记的记忆，你可以展开图 8.7 中的神经元图。你可以创建神经元的副本，来展示“未来自己”在循环中遍历你的标记。这就像展开一个
    for 循环，当你只需复制并粘贴循环内的代码行适当次数时。
- en: Figure 8.7 Unroll an RNN to reveal its hidden secrets
  id: totrans-100
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.7 展开 RNN 以揭示它的隐藏秘密
- en: '![rnn unrolled drawio](images/rnn-unrolled_drawio.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![rnn 展开图](images/rnn-unrolled_drawio.png)'
- en: Figure 8.7 shows an RNN passes the hidden state along to the next "future self"
    neuron, sort of like Olympic relay runners passing the baton. But this baton is
    imprinted with more and more memories as it is recycled over and over again within
    your RNN. You can see how the tensors for the input tokens are modified many,
    many times before the RNN finally sees the last token in the text.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.7 显示了一个 RNN 将隐藏状态传递给下一个“未来自己”神经元，有点像奥运接力选手传递接力棒。但是这个接力棒在被 RNN 反复回收利用时印上了越来越多的记忆。你可以看到在
    RNN 最终看到文本的最后一个标记之前，输入标记的张量被修改了许多许多次。
- en: Another nice feature of RNNs is that you can tap into an output tensor anywhere
    along the way. This means you can tackle challenges like machine translation,
    named entity recognition, anonymization and deanonymization of text, and even
    unredaction of government documents.^([[10](#_footnotedef_10 "View footnote.")])
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs 的另一个好处是你可以在任何位置取出输出张量。这意味着你可以解决像机器翻译、命名实体识别、文本匿名化和去匿名化、甚至政府文件开放化等挑战。^([[10](#_footnotedef_10
    "查看脚注。")])
- en: These two features are what make RNNs unique.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个特点是 RNNs 独有的特点。
- en: You can process as many tokens as you like in one text document.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以在一个文档中处理任意数量的 token。
- en: You can output anything you need after each token is processed.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个 token 处理完之后，你可以输出任何你需要的内容。
- en: That first feature is not such a big deal. As you saw with CNNs, if you want
    to process long text, you just need to make room for them in your max input tensor
    size. In fact, the most advanced NLP models to date, *transformers*, create a
    max length limit and pad the text just like CNNs.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个特点其实并不是什么大不了的事情。正如你在 CNN 中看到的那样，如果你想处理长文本，只需要在输入张量的最大尺寸里面留出空间就可以了。事实上，到目前为止最先进的
    NLP 模型——*transformers*，也是创建了最大长度限制并像 CNN 一样填充文本的。
- en: However, that second feature of RNNs is a really big deal. Imagine all the things
    you can do with a model that labels each and every token in a sentence. Linguists
    spend a lot of time diagramming sentences and labeling tokens. RNNs and deep learning
    have revolutionized the way linguistics research is done. Just look at some of
    the linguistic features that SpaCy can identify for each word in some example
    "hello world" text in listing [8.2](#figure-spacy-tags-tokens).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，RNNs 的第二大特点真的很重要。想象一下，你可以用一个标记每个句子中每一个词汇的模型做出哪些事情。语言学家花费很多时间对话语进行图解并标记 token。RNNs
    和深度学习已经改变了语言学研究的方式。只要看一下 SpaCy 可以在清单 [8.2](#figure-spacy-tags-tokens) 中识别一些“hello
    world”文本中每个单词的语言学特征，就可以想象一下。
- en: Listing 8.2 SpaCy tags tokens with RNNs
  id: totrans-109
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 8.2 SpaCy 用 RNNs 标记 token
- en: '[PRE1]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: It’s all well and good to have all that information - all that output whenever
    you want it. And you’re probably excited to try out RNNs on really long text to
    see how much it can actually remember.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有所有信息、在你需要的时候输出所有结果都是很好的。你可能很兴奋地想要在真正长的文本上尝试 RNNs，看看它到底能记住多少。
- en: 8.2 Predict someone’s nationality from only their last name
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 只使用姓氏预测一个人的国籍
- en: To get you up to speed quickly on recycling, you’ll start with the simplest
    possible token — the lowly character (letter or punctuation). You are going to
    build a model that can predict the nationality of last names, also called "surnames"
    using only the letters in the names to guide the predictions. This kind of model
    may not sound all that useful to you. You might even be worried that it could
    be used to harm individuals from particular cultures.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 为了快速让你掌握再循环利用，你将从最简单的 token（字母或标点符号）开始。你要建立一个模型，只使用名字中的字母来指导预测，可以预测出一个人的国籍，也叫“姓氏”。这种模型可能对你来说并不那么有用。你可能甚至担心它可能会被用于伤害某些特定文化的人。
- en: Like you, the authors' LinkedIn followers were suspicious when we mentioned
    we were training a model to predict the demographic characteristics of names.
    Unfortunately, businesses and governments do indeed use models like this to identify
    and target particular groups of people, often with harmful consequences. But these
    models can also be used for good. We use them to help our nonprofit and government
    customers anonymize their conversational AI datasets. Volunteers and open-source
    contributors can then train NLP models from these anonymized conversation datasets
    to identify healthcare or education content that can be helpful for users, while
    simultaneously protecting user privacy.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 就像你一样，作者的 LinkedIn 关注者们也对当我们提到正在训练一个模型来预测姓名的人口学特征时，感到怀疑。不幸的是，企业和政府确实使用这样的模型来识别和定位特定群体的人，这往往会产生有害的后果。但这些模型也可以用于好处。我们使用它们来帮助我们的非营利组织和政府客户将他们的对话
    AI 数据集匿名化。然后志愿者和开源贡献者可以从这些经过匿名处理的对话数据库中训练 NLP 模型，根据用户的需求，同时保护用户的隐私，识别出有用的医疗保健或教育内容。
- en: This multilingual dataset will give you a chance to learn how to deal with diacritics
    and other embellishments that are common for non-English words. To keep it interesting,
    you will remove these character embellishments and other giveaways in the Unicode
    characters of multilingual text. That way your model can learn the patterns you
    really care about rather than "cheating" based on this leakage. The first step
    in processing this dataset is to *asciify* it - convert it to pure ASCII characters.
    For example, the Unicode representation of the Irish name "O’Néàl" has an "acute
    accent" over the "e" and a "grave accent" over the "a" in this name. And the apostrophe
    between the "O" and "N" can be a special directional apostrophe that could unfairly
    clue your model into the nationality of the name, if you don’t *asciify* it. You
    will also need to remove the cedilla embellishment that is often added to the
    letter "C" in Turkish, Kurdish, Romance and other alphabets.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这个多语言数据集将让你有机会学习如何处理非英语单词常见的变音符号和其他装饰。为了保持趣味性，你将删除这些字符装饰和其他泄漏的Unicode字符。这样你的模型就可以学习你真正关心的模式，而不是基于这种泄漏而“作弊”。处理这个数据集的第一步是将其*ASCII化*
    - 将其转换为纯ASCII字符。例如，爱尔兰名字“O’Néàl”的Unicode表示中，“e”上有一个“重音符号”，在这个名字的“a”上有一个“重音符号”。而“O”和“N”之间的撇号可能是一个特殊的方向撇号，如果你不将其*ASCII化*，它可能会不公平地提示你的模型该名字的国籍。你还需要删除经常添加到土耳其语、库尔德语、罗曼语和其他字母表的字母“C”上的西迪拉装饰。
- en: '[PRE2]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now that you have a pipeline that "normalizes" the alphabet for a broad range
    of languages, your model will generalize better. Your model will be useful for
    almost any Latin script text, even text transliterated into Latin script from
    other alphabets. You can use this exact same model to classify any string in almost
    any language. You just need to label a few dozen examples in each language you
    are interested in "solving" for.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你有了一个可以为广泛语言规范化字母表的流水线，你的模型会更好地泛化。你的模型几乎可以用于任何拉丁字母文字，甚至是从其他字母表转写为拉丁字母文字的文字。你可以使用完全相同的模型来对几乎任何语言的任何字符串进行分类。你只需要在你感兴趣的每种语言中标记几十个例子来“解决”。
- en: 'Now let’s see if you’ve created a *solvable problem*. A solvable machine learning
    problem is one where:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看你是否已经创建了一个*可解决的问题*。一个可解决的机器学习问题是指：
- en: You can imagine a human answering those same questions
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以想象一个人类回答这些同样的问题
- en: There exists a correct answer for the vast majority of "questions" you want
    to ask your model
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于你想问你的模型的绝大多数“问题”，存在一个正确的答案
- en: You don’t expect a machine to achieve accuracy much better than a well-trained
    human expert
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你不指望机器的准确度会比训练有素的人类专家高得多
- en: Think about this problem of predicting the country or dialect associated with
    a surname. Remember we’ve removed a lot of the clues about the language, like
    the characters and embellishments that are unique to non-English languages. Is
    it solvable?
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 想一想这个预测与姓氏相关的国家或方言的问题。记住，我们已经删除了很多关于语言的线索，比如独特于非英语语言的字符和装饰。这是一个可解决的问题吗？
- en: Start with the first question above. Can you imagine a human could identify
    a person’s nationality from their asciified surname alone? Personally, I often
    guess wrong when I try to figure out where one of my students is from, based on
    their surname. I will never achieve 100% accuracy in real life and neither will
    a machine. So as long as you’re OK with an imperfect model, this is a solvable
    problem. And if you build a good pipeline, with lots of labeled data, you should
    be able to create an RNN model that is at least as accurate as humans like you
    or I. It may even be more accurate than a well-trained linguistics expert, which
    is pretty amazing when you think about it. This is where the concept of AI comes
    from, if a machine or algorithm can do intelligent things, we call it AI.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的第一个问题开始。你能想象一个人类仅从他们的姓氏的ASCII化就能确定一个人的国籍吗？就我个人而言，当我试图根据他们的姓氏猜测我的学生来自哪里时，我经常猜错。在现实生活中，我永远不会达到100%的准确率，机器也不会。所以只要你能接受一个不完美的模型，这就是一个可解决的问题。如果你建立一个良好的管道，有大量标记的数据，你应该能够创建一个至少与你我一样准确的RNN模型。当你考虑到这一点时，它甚至可能比训练有素的语言学家更准确，这是相当令人惊讶的。这就是AI概念的来源，如果一台机器或算法能够做出智能的事情，我们就称之为AI。
- en: Think about what makes this problem hard. There is no one-to-one mapping between
    surnames and countries. Even though surnames are generally shared between parents
    and children for generations, people tend to move around. And people can change
    their nationality, culture, and religion. All these things affect the names that
    are common for a particular country. And sometimes individuals or whole families
    decide to change their last name, especially immigrants, expats and spies. People
    have a lot of different reasons for wanting to blend in.^([[11](#_footnotedef_11
    "View footnote.")]) That blending of culture and language is what makes humans
    so awesome at working together to achieve great things, including AI. RNNs will
    give your nationality prediction model the same flexibility. And if you want to
    change your name, this model can help you craft it so that it invokes the nationality
    that you want people (and machines) to perceive of you.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 想想这个问题之所以难的原因。姓氏和国家之间没有一对一的映射。尽管姓氏通常在几代人之间被父母和子女共享，但人们倾向于四处迁移。而且人们可以改变自己的国籍、文化和宗教信仰。所有这些因素都会影响某个特定国家常见的姓名。有时个人或整个家庭决定改姓，尤其是移民、外国人和间谍。人们有很多不同的原因想要融入[^11]。文化和语言的融合是使人类在共同努力实现伟大事业方面如此出色的原因，包括人工智能。RNNs会给你的国籍预测模型带来同样的灵活性。如果你想改名，这个模型可以帮助你设计，使其唤起你想要人（和机器）感知到的国籍。
- en: Take a look at some random names from this dataset to see if you can find any
    character patterns that are reused in multiple countries.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 浏览一些来自这个数据集的随机姓名，看看是否可以找到在多个国家中重复使用的字符模式。
- en: Listing 8.3 Load the
  id: totrans-126
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单8.3 加载
- en: '[PRE3]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Take a quick look at the data before diving in. It seems the Dutch like their
    family names (surnames) to be at the beginning of the roll call. Several Dutch
    surnames begin with "Aa." In the US there are a lot of business names that start
    with "AAA" for similar reasons. And it seems that Moroccan, Dutch, and Finnish
    languages and cultures tend to encourage the use of the trigram "Aar" at the beginning
    of words. So you can expect some confusion among these nationalities. Don’t expect
    to achieve 90% accuracy on a classifier.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究之前先快速查看一下数据。看起来荷兰人喜欢把他们的姓氏（姓氏）放在点名表的开头。一些荷兰姓氏以“Aa”开头。在美国，有很多企业名称以“AAA”开头，原因类似。而且似乎摩洛哥、荷兰和芬兰的语言和文化倾向于鼓励在词语开头使用三字母组“Aar”。所以你可以预料到这些国籍之间会有一些混淆。不要期望分类器达到90%的准确率。
- en: You also want to count up the unique categories in your dataset so you know
    how many options your model will have to choose from.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 你还想要统计一下数据集中唯一类别的数量，这样你就知道你的模型将有多少选择。
- en: Listing 8.4 Unique nationalities in the dataset
  id: totrans-130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单8.4 数据集中的唯一国籍
- en: '[PRE4]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In listing [8.4](#listing-unique-nationalities-in-the-dataset) you can see the
    thirty-seven unique nationalities and language categories that were collected
    from multiple sources. This is what makes this problem difficult. It’s like a
    multiple-choice question where there are 36 wrong answers and only one correct
    answer. And these region or language categories often overlap. For example, Algerian
    is considered to be an Arabic language, and Brazilian is a dialect of Portuguese.
    There are several names that are shared across these nationality boundaries. So
    the model can’t get the correct answer for all of the names. It can only try to
    return the right answer as often as possible.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在清单[8.4](#listing-unique-nationalities-in-the-dataset)中，你可以看到从多个来源收集到的三十七个独特的国籍和语言类别。这就是这个问题的难点所在。这就像是一个多项选择题，有36个错误答案，只有一个正确答案。而且这些地区或语言类别经常重叠。例如，阿尔及利亚人被认为是阿拉伯语的一种，巴西人是葡萄牙语的一种方言。有几个姓名跨越了这些国籍边界。所以模型不能为所有姓名都得到正确答案。它只能尽可能地返回正确答案。
- en: The diversity of nationalities and data sources helped us do name substitution
    to anonymize messages exchanged within our multilingual chatbots. That way can
    share conversation design datasets in open-source projects like the chatbots discussed
    in Chapter 12 of this book. RNN models are great for anonymization tasks, such
    as named entity recognition and generation of fictional names. They can even be
    used to generate fictional, but realistic social security numbers, telephone numbers,
    and other PII (Personally Identifiable Information). To build this dataset we
    augmented the PyTorch RNN tutorial dataset with names scraped from public APIs
    that contained data for underrepresented countries in Africa, South and Central
    America, and Oceania.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 各种国籍和数据源的多样性帮助我们进行名称替换，以匿名化我们多语言聊天机器人中交换的消息。这样可以在开源项目中共享会话设计数据集，例如本书第 12 章讨论的聊天机器人。递归神经网络模型非常适用于匿名化任务，例如命名实体识别和虚构名称的生成。它们甚至可以用来生成虚构但逼真的社会安全号码、电话号码和其他个人身份信息（PII）。为了构建这个数据集，我们使用了从公共
    API 中抓取的包含非洲、南美和中美洲以及大洋洲少数族裔国家数据的 PyTorch RNN 教程数据集。
- en: When we were building this dataset during our weekly mob programming on Manning’s
    Twitch channel, Rochdi Khalid pointed out that his last name is Arabic. And he
    lives in Casablanca, Morocco where Arabic is an official language, along side
    French and Berber. This dataset is a mashup of data from a variety of sources.^([[12](#_footnotedef_12
    "View footnote.")]) some of which create labels based on broad language labels
    such as "Arabic" and others are labeled with their specific nationality or dialect,
    such as Moroccan, Algerian, Palestinian, or Malaysian.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们每周在 Manning 的 Twitch 频道上进行集体编程时，Rochdi Khalid 指出他的姓氏是阿拉伯语。他住在摩洛哥的卡萨布兰卡，在那里阿拉伯语是官方语言，与法语和柏柏尔语并存。这个数据集是从各种来源汇编而成的。[[12](#_footnotedef_12
    "查看脚注。")]) 其中一些基于广泛的语言标签（如"Arabic"）创建标签，而其他一些则以特定的国籍或方言为标签，如摩洛哥、阿尔及利亚、巴勒斯坦或马来西亚。
- en: Dataset bias is one of the most difficult biases to compensate for unless you
    can find data for the groups you want to elevate. Besides public APIs, you can
    also mine your internal data for names. Our anonymization scripts strip out names
    from multilingual chatbot dialog. We added those names to this dataset to ensure
    it is a representative sample of the kinds of users that interact with our chatbots.
    You can use this dataset for your own projects where you need a truly global slice
    of names from a variety of cultures.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集偏见是最难弥补的偏见之一，除非你能找到要提升的群体的数据。除了公共 API，你还可以从内部数据中挖掘名称。我们的匿名化脚本从多语言聊天机器人对话中剥离出名称。我们将这些名称添加到了这个数据集中，以确保它是与我们的聊天机器人互动的用户种类的代表性样本。你可以在需要从各种文化中获得真正全球化的名称片段的自己的项目中使用这个数据集。
- en: Diversity has its challenges. As you might imagine some spellings of these transliterated
    names are reused across national borders and even across languages. Translation
    and transliteration are two separate NLP problems that you can solve with RNNs.
    The word "नमस्कार" can be *translated* to the English word "hello". But before
    your RNN would attempt to translate a Nepalese word it would *transliterate* the
    Nepalese word "नमस्कार" into the word "namaskāra" which uses only the Latin character
    set. Most multilingual deep learning pipelines utilize the Latin character set
    (Romance script alphabet) to represent words in all languages.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 多样性也带来了挑战。你可以想象到，这些音译名称的拼写可能跨越国界甚至跨越语言。翻译和音译是两个不同的自然语言处理问题，你可以使用递归神经网络来解决。词语
    "नमस्कार" 可以*翻译*成英语单词 "hello"。但在你的递归神经网络尝试翻译尼泊尔语单词之前，它将会*音译*尼泊尔语单词 "नमस्कार" 成为使用拉丁字符集的单词
    "namaskāra"。大多数多语言深度学习流程都使用拉丁字符集（罗马脚本字母）来表示所有语言中的单词。
- en: Note
  id: totrans-137
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Transliteration is when you translate the characters and spellings of words
    from one language’s alphabet to another, making it possible to represent words
    using the Latin character set (Romance script alphabet) used in Europe and the
    Americas. A simple example is the removal or adding of the acute accent from the
    French character "é", as in "resumé" (resume) and "école" (school). Transliteration
    is a lot harder for non-Latin alphabets such as Nepalese.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 音译是将一个语言的字母和拼写翻译成另一种语言的字母，从而可以使用在欧洲和美洲使用的拉丁字符集（罗马脚本字母）表示单词。一个简单的例子是将法语字符 "é"
    的重音去除或添加，例如 "resumé"（简历）和 "école"（学校）。对于非拉丁字母表，如尼泊尔语，音译要困难得多。
- en: Here’s how you can calculate just how much overlap there is within each of your
    categories (nationalities).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何计算每个类别（国籍）内重叠程度的方法。
- en: '[PRE5]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In addition to the overlap *across* nationalities, the PyTorch tutorial dataset
    contained many duplicated names within nationalities. More than 94% of the Arabic
    names were duplicates, some of which are shown in listing [8.5](#listing-surname-oversampling).
    Other nationalities and languages such as English, Korean, and Scottish appear
    to have been deduplicated. Duplicates in your training set make your model fit
    more closely to common names than to less frequently occurring names. Duplicating
    entries in your datasets is a brute-force way of "balancing" your dataset or enforcing
    statistics about the frequency of phrases to help it predict popular names and
    heavily populated countries more accurately. This technique is sometimes referred
    to as "oversampling the minority class" because it boosts the frequency and accuracy
    of underrepresented classes in your dataset.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 除了跨国家的重叠之外，PyTorch 教程数据集中还包含了许多重复的名称。超过 94% 的阿拉伯语名称是重复的，其中一些在第 [8.5](#listing-surname-oversampling)
    节中显示出来。其他国籍和语言，如英语、韩语和苏格兰语，似乎已经去重了。在你的训练集中重复条目使你的模型更紧密地适应于常见名称而不是不太频繁出现的名称。在数据集中复制条目是一种“平衡”数据集或强制统计短语频率的方法，以帮助准确预测流行名称和人口稠密国家。这种技术有时被称为“过度抽样少数类”，因为它增加了数据集中未被充分代表的类别的频率和准确性。
- en: If you’re curious about the original surname data check out the PyTorch "RNN
    Classification Tutorial".^([[13](#_footnotedef_13 "View footnote.")]) There were
    only 108 unique Arabic surnames among the 2000 Arabic examples in Arabic.txt.^([[14](#_footnotedef_14
    "View footnote.")])
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对原始的姓氏数据感兴趣，请查看 PyTorch 的“RNN 分类教程”。^([[13](#_footnotedef_13 "查看脚注。")]) 在
    Arabic.txt 中的 2000 个阿拉伯示例中，只有 108 个独特的阿拉伯姓氏。^([[14](#_footnotedef_14 "查看脚注。")])
- en: Listing 8.5 Surname oversampling
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第 8.5 节 姓氏过度抽样
- en: '[PRE6]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This means that even a relatively simple model (like the one shown in the PyTorch
    tutorial) should be able to correctly label popular names like Abadi and Zogby
    as Arabic. And you can anticipate your model’s confusion matrix statistics by
    counting up the number of nationalities associated with each name in the dataset.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着即使是一个相对简单的模型（比如 PyTorch 教程中展示的模型），也应该能够正确地将像 Abadi 和 Zogby 这样的流行名称标记为阿拉伯语。通过计算数据集中与每个名称关联的国籍数量，你可以预期模型的混淆矩阵统计数据。
- en: You are going to use a deduplicated dataset that you loaded in listing [8.5](#listing-surname-oversampling).
    We have counted up the duplicates to give you the statistics for these duplicates
    without burdening you with downloading a bloated dataset. And you will use a balanced
    sampling of countries to encourage your model to treat all categories and names
    equally. This means your model will predict rare names and rare countries just
    as accurately as popular names from popular countries. This balanced dataset will
    encourage your RNN to generalize from the linguistic features it sees in names.
    Your model will be more likely to recognize patterns of letters that are common
    among many different names, especially those that help the RNN distinguish between
    countries. We’ve included information on how to obtain accurate usage frequency
    statistics for names in the `nlpia2` repository on GitLab.^([[15](#_footnotedef_15
    "View footnote.")]) You’ll need to keep this in mind if you intend to use this
    model in the real world on a more random sample of names.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用在第 [8.5](#listing-surname-oversampling) 节中加载的去重数据集。我们已经计算了重复项，为你提供了这些重复项的统计信息，而不会让你下载一个庞大的数据集。你将使用平衡抽样的国家数据，以鼓励你的模型平等对待所有类别和名称。这意味着你的模型将像准确预测流行国家的流行名称一样准确地预测罕见名称和罕见国家。这个平衡的数据集将鼓励你的
    RNN 从它在名称中看到的语言特征中归纳出一般规律。你的模型更有可能识别出许多不同名称中常见的字母模式，尤其是那些帮助 RNN 区分国家的模式。我们在 `nlpia2`
    仓库的 GitLab 上包含了关于如何获取准确的名称使用频率统计信息的信息。^([[15](#_footnotedef_15 "查看脚注。")]) 如果你打算在更随机的名称样本上在真实世界中使用这个模型，你需要记住这一点。
- en: Listing 8.6 Name nationality overlap
  id: totrans-147
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第 8.6 节 名称国籍重叠
- en: '[PRE7]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: To help diversify this dataset and make it a little more representative of real-world
    statistics, we added some names from India and Africa. And we compressed the dataset
    by counting the duplicates. The resulting dataset of surnames combines data from
    the PyTorch RNN tutorial with anonymized data from multilingual chatbots.^([[16](#_footnotedef_16
    "View footnote.")]) In fact, we use this name classification and generation model
    to anonymize names in our chatbot logs. This allows us to *default to open* with
    both NLP datasets as well as software.^([[17](#_footnotedef_17 "View footnote.")])
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助使这个数据集多样化，并使其更具代表性，我们添加了一些来自印度和非洲的姓名。并且通过计算重复项来压缩数据集。由此产生的姓氏数据集将 PyTorch
    RNN 教程的数据与多语言聊天机器人的匿名化数据结合起来。事实上，我们使用这个姓名分类和生成模型来匿名化我们聊天机器人日志中的姓名。这使我们能够在 NLP
    数据集和软件方面“默认开放”。
- en: Important
  id: totrans-150
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 重要提示
- en: A great way to find out if a machine learning pipeline has a chance of solving
    your problem, pretend you are the machine. Give yourself training on a few of
    the examples in your training set. Then try to answer a few of the "questions"
    in your test set without looking at the correct label. Your NLP pipeline should
    probably be able to solve your problem almost as well as you could. And in some
    cases, you might find machines are much better than you because they can balance
    many patterns in their head more accurately than you can.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 要找出机器学习流水线是否有可能解决您的问题，假装自己是机器。对训练集中的一些示例进行训练。然后尝试回答一些测试集中的“问题”，而不查看正确的标签。你的
    NLP 流水线应该能够几乎和你一样好地解决你的问题。在某些情况下，你可能会发现机器比你更好，因为它们可以更准确地在脑海中平衡许多模式。
- en: By computing the most popular nationality for each name in the dataset, it is
    possible to create a confusion matrix, using the most common nationality as the
    "true" label for a particular name. This can reveal several quirks in the dataset
    that should influence what the model learns and how well it can perform this task.
    There is no confusion at all for Arabic names because there are very few unique
    Arabic names and none of them are included in the other nationalities. And a significant
    overlap exists between Spanish, Portuguese, Italian and English names. Interestingly,
    for the 100 Scottish names in the dataset, None of them are most commonly labeled
    as Scottish. Scottish names are more often labeled as English and Irish names.
    This is because there are thousands of English and Irish names, but only 100 Scottish
    names in the original PyTorch tutorial dataset.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 通过计算数据集中每个名称的最流行国籍，可以创建一个混淆矩阵，使用最常见的国籍作为特定名称的“真实”标签。这可以揭示数据集中的几个怪癖，应该影响模型学习的内容以及其执行此任务的效果如何。对于阿拉伯名字，根本没有混淆，因为阿拉伯名字非常少，而且没有一个被包含在其他国籍中。西班牙、葡萄牙、意大利和英国名字之间存在显著的重叠。有趣的是，在数据集中有100个苏格兰名字，其中没有一个最常被标记为苏格兰名字。苏格兰名字更常被标记为英国和爱尔兰名字。这是因为原始的
    PyTorch 教程数据集中有成千上万个英国和爱尔兰名字，但只有100个苏格兰名字。
- en: Figure 8.8 The dataset is confused even before training
  id: totrans-153
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.8 在训练之前数据集就产生了混淆
- en: '![confusion pytorch tutorial](images/confusion-pytorch-tutorial.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![混淆 pytorch 教程](images/confusion-pytorch-tutorial.png)'
- en: We’ve added 26 more nationalities to the original PyTorch dataset. This creates
    much more ambiguity or overlap in the class labels. Many names are common in multiple
    different regions of the world. An RNN can deal with this ambiguity quite well,
    using the statistics of patterns in the character sequences to guide its classification
    decisions.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在原始 PyTorch 数据集中添加了 26 个国籍。这在类标签中创建了更多的歧义或重叠。许多名称在世界多个不同地区都很常见。RNN 可以很好地处理这种歧义，使用字符序列中模式的统计数据来指导其分类决策。
- en: 8.2.1 Build an RNN from scratch
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.1 从头开始构建 RNN
- en: Here’s the heart of your `RNN` class in listing [8.7](#listing-heart-rnn) Like
    all Python classes, a PyTorch Module class has an `*init*()` method where you
    can set some configuration values that control how the rest of the class works.
    For an RNN you can use the `*init*()` method to set the hyperparameters that control
    the number of neurons in the hidden vector as well as the size of the input and
    output vectors.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这是您的`RNN`类的核心代码，见列表[8.7](#listing-heart-rnn)。像所有 Python 类一样，PyTorch Module 类有一个`*init*()`方法，您可以在其中设置一些配置值，以控制类的其余部分的工作方式。对于
    RNN，您可以使用`*init*()`方法设置控制隐藏向量中的神经元数量以及输入和输出向量大小的超参数。
- en: For an NLP application that relies on tokenizers, it’s a good idea to include
    the tokenizer parameters within the init method to make it easier to instantiate
    again from data saved to disk. Otherwise, you’ll find that you end up with several
    different models saved on your disk. And each model may use a different vocabulary
    or dictionary to tokenize and vectorize your data. Keeping all those models and
    tokenizers connected is a challenge if they aren’t stored together in one object.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 对于依赖于分词器的自然语言处理应用程序，将分词器参数包含在init方法中是个好主意，这样可以更容易地从保存到磁盘的数据中再次实例化。否则，你会发现你在磁盘上保存了几个不同的模型。每个模型可能使用不同的词汇表或字典来对你的数据进行分词和向量化。如果它们没有在一个对象中一起存储，那么保持所有这些模型和分词器的连接是一种挑战。
- en: The same goes for the vectorizers in your NLP pipeline. Your pipeline must be
    consistent about where it stores each word for your vocabulary. And you also have
    to be consistent about the ordering of your categories if your output is a class
    label. You can easily get confused if you aren’t exactly consistent with the ordering
    of your category labels each time you reuse your model. The output will be garbled
    nonsense labels if the numerical values used by your model aren’t consistently
    mapped to human-readable names for those categories. If you store your vectorizers
    in your model class (see listing [8.7](#listing-heart-rnn)), it will know exactly
    which category labels it wants to apply to your data.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的自然语言处理流水线中，向量化器也是如此。你的流水线必须一致地确定每个词汇的存储位置。如果你的输出是一个类别标签，你还必须一致地确定类别的排序。如果在每次重用模型时，你的类别标签的排序不完全一致，你很容易感到困惑。如果你的模型使用的数值值与这些类别的人类可读名称不一致地映射，输出将是一些混乱的无意义标签。如果你将向量化器存储在你的模型类中（见清单
    [8.7](#listing-heart-rnn)），它将确切地知道要将哪些类别标签应用于你的数据。
- en: Listing 8.7 Heart of an RNN
  id: totrans-160
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 8.7 RNN的核心
- en: '[PRE8]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Technically, your model doesn’t need the full `char2i` vocabulary. It just needs
    the size of the one-hot token vectors you plan to input into it during training
    and inference. Likewise for the category labels. Your model only really needs
    to know the number of categories. The names of those categories are meaningless
    to the machine. But by including the category labels within your model you can
    print them to the console whenever you want to debug the internals of your model.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，你的模型不需要完整的`char2i`词汇表。它只需要你计划在训练和推断期间输入的一个独热令牌向量的大小。类别标签也是如此。你的模型只需要知道类别的数量。这些类别的名称对机器来说毫无意义。但是通过在你的模型中包含类别标签，你可以在需要调试模型内部时随时将它们打印到控制台。
- en: 8.2.2 Training an RNN, one token at a time
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.2 逐个令牌训练RNN
- en: The dataset of 30000+ surnames for 37+ countries in the `nlpia2` project is
    manageable, even on a modest laptop. So you should be able to train it using the
    in a reasonable amount of time. If your laptop has 4 or more CPU cores and 6 GB
    or more RAM, the training will take about 30 minutes. And if you limit yourself
    to only 10 countries, 10000 surnames, and get lucky (or smart) with your choice
    of learning rate, you can train a good model in two minutes.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '`nlpia2`项目中包含30000多个姓氏的数据集，涵盖了37个以上的国家，即使在一台普通的笔记本电脑上也是可管理的。因此，你应该能够在合理的时间内使用`nlpia2`来训练它。如果你的笔记本电脑有4个或更多的CPU核心和6GB或更多的RAM，训练将花费大约30分钟。如果你限制自己只使用10个国家、10000个姓氏，并且在学习率的选择上有一些幸运（或聪明），你可以在两分钟内训练出一个好的模型。'
- en: Rather than using the built-in `torch.nn.RNN` layer you can build your first
    RNN from scratch using plain old `Linear` layers. This will generalize your understanding
    so you can design your own RNNs for almost any application.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是使用内置的`torch.nn.RNN`层，你可以使用普通的`Linear`层从头开始构建你的第一个RNN。这样可以让你的理解更加泛化，这样你就可以为几乎任何应用设计自己的RNN。
- en: Listing 8.8 Training on a single sample must loop through the characters
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 8.8 对单个样本进行训练必须循环遍历字符
- en: '[PRE9]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The `nlpia2` package contains a script to orchestrate the training process and
    allow you to experiment with different hyperparameters.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '`nlpia2`包包含一个脚本，用于编排训练过程，并允许你尝试不同的超参数。'
- en: '[PRE10]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Tip
  id: totrans-170
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: You want to use the `%run` magic command within the iPython console rather than
    running your machine learning scripts in the terminal using the `python` interpreter.
    The ipython console is like a debugger. It allows you to inspect all the global
    variables and functions after your script finishes running. And if you cancel
    the run or if there is an error that halts the script, you will still be able
    to examine the global variables without having to start over from scratch.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该在 iPython 控制台中使用 `%run` 魔术命令，而不是在终端中使用 `python` 解释器运行机器学习脚本。ipython控制台类似于调试器。它允许您在脚本运行完成后检查所有全局变量和函数。如果取消运行或遇到停止脚本的错误，您仍然能够检查全局变量，而无需从头开始。
- en: Once you launch the `classify_name_nationality.py` script it will prompt you
    with several questions about the model’s hyperparameters. This is one of the best
    ways to develop an instinct about deep learning models. And this is why we chose
    a relatively small dataset and small problem that can be successfully trained
    in a reasonable amount of time. This allows you to try many different hyperparameter
    combinations and fine tune your intuitions about NLP while fine tuning your model.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您启动 `classify_name_nationality.py` 脚本，它将提示您关于模型超参数的几个问题。这是培养关于深度学习模型直觉的最佳方式之一。这也是为什么我们选择了一个相对较小的数据集和小问题，可以在合理的时间内成功训练。这使您可以尝试许多不同的超参数组合，并在微调模型时微调您对
    NLP 的直觉。
- en: Listing [8.9](#listing-interactive-prompts-hyperparameters) shows some hyperparameter
    choices that will give you pretty good results. But we’ve left you room to explore
    the "hyperspace" of options on your own. Can you find a set of hyperparameters
    that can identify a broader set of nationalities with better accuracy?
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 列表[8.9](#listing-interactive-prompts-hyperparameters)展示了一些超参数的选择，可以获得很好的结果。但我们给您留了足够的空间来自行探索各种选项的“超空间”。您能否找到一组超参数，以更高的准确率识别更广泛的国籍？
- en: Listing 8.9 Interactive prompts so you can play with hyperparameters
  id: totrans-174
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.9 可交互的提示，以便您可以调整超参数。
- en: '[PRE11]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Even this simplified RNN model with only 128 neurons and 1500 epochs takes several
    minutes to converge to a decent accuracy. This example was trained on a laptop
    with a 4-core (8-thread) i7 Intel processor and 64 GB of RAM. If your computing
    resources are more limited, you can train a simpler model on only 10 nationalities
    and it should converge much more quickly. Keep in mind that many names were assigned
    to multiple nationalities. And some of the nationality labels were more general
    language labels like "Arabic" that apply to many many countries. So you don’t
    expect to get very high accuracy, especially when you give the model many nationalities
    (categories) to choose from.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 即使只有128个神经元和1500个周期的简化 RNN 模型，也需要几分钟才能收敛到一个合理的精确度。此示例在一台配备4核心（8线程）i7 Intel处理器和64GB内存的笔记本上进行训练。如果您的计算资源更有限，您可以在只有10个国籍的简化模型上进行训练，它应该会更快地收敛。请记住，许多名称被分配给多个国籍。有些国籍标签是更常见的语言标签，比如“阿拉伯语”，适用于很多很多国家。因此，您不应期望获得非常高的精确度，特别是当您给模型许多国籍（类别）选择时。
- en: Listing 8.10 Training output log
  id: totrans-177
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.10 训练输出日志
- en: '[PRE12]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Looks like the RNN achieved 57% accuracy on the training set and 29% accuracy
    on the validation set. This is an unfair measure of the model’s usefulness. Because
    the dataset was deduplicated before splitting into training and validation sets,
    there is only one row in the dataset for each name-nationality combination. This
    means that a name that is associated with one nationality in the training set
    will likely be associated with a *different* nationality in the validation set.
    This is why the PyTorch tutorial doesn’t create test or validation datasets in
    the official docs. They don’t want to confuse you.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来 RNN 在训练集上达到了57%的准确率，在验证集上达到了29%的准确率。这是对模型有用性的一种不公平的衡量方式。因为在将数据集拆分成训练和验证集之前，数据集已经去重，每个姓名-国籍组合只有一行数据。这意味着在训练集中与一个国籍相关联的姓名可能在验证集中与*不同的*国籍相关联。这就是为什么
    PyTorch 教程在官方文档中没有创建测试或验证数据集的原因。他们不想让您感到困惑。
- en: Now that you understand the ambiguity in the dataset you can see how hard the
    problem is and that this RNN does a really good job of generalizing from the patterns
    it found in the character sequences. It generalizes to the validation set much
    better than random chance. Random guesses would have achieved 4% accuracy on 25
    categories (`1/25 == .04`) even if there was no ambiguity in the nationality associated
    with each name.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你了解了数据集中的歧义，你可以看到这个问题有多困难，而且这个RNN在字符序列中找到的模式上的泛化能力非常强。它在验证集上的泛化能力比随机猜测要好得多。即使每个名字关联的国籍没有歧义，随机猜测也只能在25个类别中获得4%的准确率（`1/25
    == .04`）。
- en: Let’s try it on some common surnames that are used in many countries. An engineer
    named Rochdi Khalid helped create one of the diagrams in this chapter. He lives
    and works in Casablanca, Morrocco. Even though Morocco isn’t the top prediction
    for "Khalid", Morocco is in second place!
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试试一些在许多国家都使用的常见姓氏。一个叫Rochdi Khalid的工程师帮助创建了本章中的一个图表。他生活和工作在摩洛哥的卡萨布兰卡。尽管摩洛哥不是"Khalid"的最高预测，但摩洛哥位居第二！
- en: '[PRE13]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The top 3 predictions are all for Arabic-speaking countries. I don’t think there
    are expert linguists that could do this prediction as fast or as accurately as
    this RNN model did.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 前三个预测都是阿拉伯语国家。我认为没有专家语言学家能够像这个RNN模型那样快速或准确地进行这种预测。
- en: Now it’s time to dig deeper and examine some more predictions to see if you
    can figure out how only 128 neurons can predict someone’s nationality so well.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候深入挖掘，检查一些更多的预测，看看你是否能够弄清楚只有128个神经元如何能够如此成功地预测某人的国籍。
- en: 8.2.3 Understanding the results
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.3 理解结果
- en: To use a model like this in the real world you will need to be able to explain
    how it works to your boss. Germany, Finland, and the Netherlands (and soon in
    all of the EU) are regulating how AI can be used, to force businesses to explain
    their AI algorithms so users can protect themselves.^([[18](#_footnotedef_18 "View
    footnote.")]) Businesses won’t be able to hide their exploitative business practices
    within algorithms for long.^([[19](#_footnotedef_19 "View footnote.")]) ^([[20](#_footnotedef_20
    "View footnote.")]) You can imagine how governments and businesses might use a
    nationality prediction algorithm for evil. Once you understand how this RNN works
    you’ll be able to use that knowledge to trick algorithms into doing what’s right,
    elevating rather than discriminating against historically disadvantaged groups
    and cultures.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 要在现实世界中使用这样的模型，你需要能够向老板解释它是如何工作的。德国、芬兰和荷兰（以及很快在整个欧盟）正在规范AI的使用，迫使企业解释他们的AI算法，以便用户能够保护自己。企业将无法长时间隐藏他们在算法中的剥削性商业行为。你可以想象政府和企业可能如何利用国籍预测算法进行邪恶用途。一旦你了解了这个RNN的工作原理，你就能利用这些知识来欺骗算法做正确的事情，提升而不是歧视历史上处于劣势的群体和文化。
- en: Perhaps the most important piece of an AI algorithm is the metric you used to
    train it. You used `NLLLoss` for the PyTorch optimization training loop in listing
    [8.8](#training-on-a-single-sample). The `NLL` part stands for "Negative Log Likelihood".
    You should already know how to invert the `log()` part of that expression. Try
    to guess what the mathematical function and Python code is to invert the `log()`
    function before checking out the code snippet below. As with most ML algorithms,
    `log` means natural log, sometimes written as *ln* or *log to the base e*.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 也许AI算法中最重要的部分是你用来训练它的指标。你在PyTorch优化训练循环中使用了`NLLLoss`来训练，这在列表[8.8](#training-on-a-single-sample)中已经提到。`NLL`部分代表“负对数似然”。你应该已经知道如何求反`log()`这个表达式的部分了。在查看下面的代码片段之前，试着猜测如何求反`log()`函数的数学函数和Python代码是什么。像大多数ML算法一样，`log`表示自然对数，有时写作*ln*或*以e为底的对数*。
- en: '[PRE14]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This means that the model is only 31% confident that Rochdi is Algerian. These
    probabilities (likelihoods) can be used to explain how confident your model is
    to your boss or teammates or even your users.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着模型仅有31%的信心认为Rochdi是阿尔及利亚人。这些概率（可能性）可以用来解释你的模型对老板、队友甚至用户有多自信。
- en: If you’re a fan of "debug by print" you can modify your model to print out anything
    you’re interested in about the math the model uses to make predictions. PyTorch
    models can be instrumented with print statements whenever you want to record some
    of the internal goings on. If you do decide to use this approach, you only need
    to `.detach()` the tensors from the GPU or CPU where they are located to bring
    them back into your working RAM for recording in your model class.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是"通过打印调试"的粉丝，你可以修改你的模型来打印出你对模型使用的数学的任何感兴趣的内容。PyTorch 模型可以在你想要记录一些内部过程时用打印语句进行仪器化。如果你决定使用这种方法，你只需要将张量从它们所在的
    GPU 或 CPU 上`.detach()`，将它们带回你的工作 RAM 中进行记录在你的模型类中。
- en: A nice feature of RNNs is that the predictions are built up step by step as
    your `forward()` method is run on each successive token. This means you may not
    even need to add print statements or other instrumentation to your model class.
    Instead, you can just make predictions of the hidden and output tensors for parts
    of the input text.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 的一个很好的特性是，预测是逐步建立的，当你的`forward()`方法在每个连续的标记上运行时。这意味着你甚至可能不需要添加打印语句或其他仪器到你的模型类中。相反，你可以为输入文本的部分进行隐藏和输出张量的预测。
- en: You may want to add some `predict_*` convenience functions for your model class
    to make it easier to explore and explain the model’s predictions. If you remember
    the `LogisticRegression` model in Scikit-Learn, it has a `predict_proba` method
    to predict probabilities in addition to the `predict` method used to predict the
    category. An RNN has an additional hidden state vector you may sometimes want
    to examine for clues as to how the network is making predictions. So you can create
    a `predict_hidden` method to output the 128-D hidden tensor and a `predict_proba`
    to show you the predicted probabilities for each of the target categories (nationalities).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想要为你的模型类添加一些`predict_*`便利函数，以便更容易地探索和解释模型的预测。如果你还记得 Scikit-Learn 中的`LogisticRegression`模型，它有一个`predict_proba`方法用于预测概率，除了用于预测类别的`predict`方法。一个
    RNN 有一个额外的隐藏状态向量，有时你可能想要检查这个向量，以了解网络是如何进行预测的。因此，你可以创建一个`predict_hidden`方法来输出128维的隐藏张量，以及一个`predict_proba`来显示每个目标类别（国籍）的预测概率。
- en: '[PRE15]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This `predict_hidden` convenience method converts the text (surname) into a
    tensor before iterating through the one-hot tensors to run the forward method
    (or just the model’s `self`).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`predict_hidden`便利方法将文本（姓氏）转换为张量，然后通过一个热编码张量迭代运行前向方法（或者只是模型的`self`）。
- en: '[PRE16]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This `predict_hidden` method gives you access to the most interesting part of
    the model where the "logic" of the predictions is taking place. The hidden layer
    evolves as it learns more and more about the nationality of a name with each character.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`predict_hidden`方法让你访问模型最有趣的部分，即预测逻辑正在发生的地方。随着每个字符的学习，隐藏层会不断演化，越来越多地了解姓名的国籍。
- en: Finally, you can use a `predict_category` convenience method to run the model’s
    forward pass predictions to predict the nationality of a name.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可以使用一个`predict_category`便利方法来运行模型的前向传递预测，以预测一个姓名的国籍。
- en: '[PRE17]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The key thing to recognize is that for all of these methods, you don’t necessarily
    have to input the entire string for the surname. It is perfectly fine to reevaluate
    the first part of the surname text over and over again, as long as you reset the
    hidden layer each time.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 要认识到的关键一点是，对于所有这些方法，你不一定需要输入姓氏的整个字符串。重复评估姓氏文本的前部分是完全可以的，只要每次重置隐藏层即可。
- en: If you input an expanding window of text you can see how the predictions and
    hidden layer evolve in their understanding of the surname. During mob programming
    sessions with other readers of this book, we noticed that nearly all names started
    out with predictions of "Chinese" as the nationality for a name until after the
    3rd or 4th character. This is perhaps because so many Chinese surnames contain
    4 (or fewer) characters.^([[21](#_footnotedef_21 "View footnote.")])
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你输入一个不断扩展的文本窗口，你可以看到预测和隐藏层在对姓氏的理解上是如何演变的。在与本书其他读者的集体编程会议期间，我们注意到几乎所有的名字最初都被预测为"中国"，直到第三或第四个字符之后。这可能是因为很多中国姓氏只包含4个（或更少）字符。
- en: Now that you have helper functions you can use them to record the hidden and
    category predictions as the RNN is run on each letter in a name.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你有了辅助函数，你可以用它们来记录隐藏层和类别预测，当 RNN 在姓名的每个字母上运行时。
- en: '[PRE18]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: And you can create a 128 x 6 matrix of all the hidden layer values in a 6-letter
    name. The list of PyTorch tensors can be converted to a list of lists and then
    a DataFrame to make it easier to manipulate and explore.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 而且，您可以创建一个128 x 6的矩阵，其中包含6个字母名称中的所有隐藏层值。 PyTorch张量列表可以转换为列表，然后转换为DataFrame，以便更容易地操作和探索。
- en: '[PRE19]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This wall of numbers contains everything your RNN "thinks" about the name as
    it is reading through it.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这堵数字墙包含了您的RNN在阅读名称时的所有“想法”。
- en: Tip
  id: totrans-206
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: There are some Pandas display options that will help you get a feel for the
    numbers in a large DataFrame without TMI ("too much information"). Here are some
    of the settings that helped improve the printouts of tables in this book
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些Pandas显示选项可以帮助您对大型DataFrame中的数字有所了解，而不会出现TMI（“太多信息”）。以下是本书中提高表格打印质量的一些设置。
- en: 'To display only 2 decimal places of precision for floating point values try:
    `pd.options.display.float_format = ''{:.2f}''`.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 要仅显示浮点值的2个小数位精度，请尝试：`pd.options.display.float_format = '{:.2f}'`。
- en: 'To display a maximum of 12 columns and 7 rows of data from your DataFrame:
    `pd.options.display.max_columns = 12` and `pd.options.display.max_rows = 7`'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 要从DataFrame显示最多12列和7行的数据：`pd.options.display.max_columns = 12`和`pd.options.display.max_rows
    = 7`
- en: These only affect the displayed representation of your data, not the internal
    values used when you do addition or multiplication.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这些选项仅影响数据的显示表示，而不是进行加法或乘法时使用的内部值。
- en: As you’ve probably done with other large tables of numbers, it’s often helpful
    to find patterns by correlating it with other numbers that are interesting to
    you. For example, you may want to find out if any of the hidden weights are keeping
    track of the RNN’s position within the text - how many characters it is from the
    beginning or end of the text.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您可能用其他大量数字的表格所做的那样，通过将其与您感兴趣的其他数字相关联，通常可以找到模式。例如，您可能想发现隐藏权重中是否有任何一个正在跟踪RNN在文本中的位置-即它距离文本的开头或结尾有多少个字符。
- en: '[PRE20]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Interestingly our hidden layer has room in its hidden memory to record the position
    in many different places. And the strongest correlation seems to be negative.
    These are likely helping the model to estimate the likelihood of the current character
    being the last character in the name. When we looked at a wide range of example
    names, the predictions only seemed to converge on the correct answer at the very
    last character or two. Andrej Karpathy experimented with several more ways to
    glean insight from the weights of your RNN model in his blog post "The unreasonable
    effectiveness of RNNs" in the early days of discovering RNNs. ^([[22](#_footnotedef_22
    "View footnote.")])]
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，我们的隐藏层在其隐藏内存中有空间来记录许多不同地方的位置。而且最强的相关性似乎是负相关。这些可能有助于模型估计当前字符是名字中最后一个字符的可能性。当我们观察了各种各样的示例名称时，预测似乎只在最后一个或两个字符处收敛到正确的答案。安德烈·卡尔帕西在他的博客文章《RNN的不合理有效性》中尝试了几种从RNN模型的权重中获得见解的方法，这是在发现RNN时期的早期。^([[22](#_footnotedef_22
    "View footnote.")])]
- en: 8.2.4 Multiclass classifiers vs multi-label taggers
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.4 多类别分类器与多标签标记器
- en: How can you deal with the ambiguity of multiple different correct nationalities
    for surnames? The answer is multi-label classification or tagging rather than
    the familiar multiclass classification. Because the terms "multiclass classification"
    and "multi-label classification" sound so similar and are easily confused, you
    probably want to use the term "multi-label tagging" or just "tagging" instead
    of "multi-label classification." And if you’re looking for the `sklearn` models
    suited to this kind of problem you want to search for "multi-output classification."
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 怎样应对姓氏的多个不同正确国籍的歧义性？答案是多标签分类或标记，而不是熟悉的多类别分类。因为“多类分类”和“多标签分类”这些术语听起来相似且容易混淆，您可能想使用“多标签标记”或仅使用“标记”而不是“多标签分类”这个术语。如果您正在寻找适用于这种问题的`sklearn`模型，则要搜索“多输出分类”。
- en: Multi-label taggers are made for ambiguity. In NLP intent classification and
    tagging is full of intent labels that have fuzzy overlapping boundaries. We aren’t
    talking about a graffiti war between Banksy and Bario Logan street artists when
    we say "taggers". We’re talking about a kind of machine learning model that can
    assign multiple discrete labels to an object in your dataset.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 多标签标记器是用于模棱两可的任务的。在NLP意图分类和标记中，标签充满了具有模糊重叠边界的意图标签。当我们说“标记器”时，我们不是在谈论Banksy和Bario
    Logan街头艺术家之间的涂鸦之争，而是在谈论一种机器学习模型，可以为您数据集中的对象分配多个离散标签。
- en: A multiclass classifier has multiple different categorical labels that are matched
    to objects, one label for each object. A categorical variable takes on only one
    of several mutually exclusive classes or categories. For example, if you wanted
    to predict both the language and the gender associated with first names (given
    names), then that would require a multiclass classifier. But if you want to label
    a name with all the relevant nationalities and genders that are appropriate, then
    you would need a tagging model.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 多类分类器具有多个不同的分类标签，这些标签与对象匹配，每个对象对应一个标签。分类变量只取几个相互排斥的类别中的一个。例如，如果您想要预测名字（给定名字）的语言和性别，那么就需要一个多类分类器。但是，如果您想要为名字标记所有相关的适当国籍和性别，那么您就需要一个标记模型。
- en: This may seem like splitting hairs to you, but it’s much more than just semantics.
    It’s the semantics (meaning) of the text that you are processing that is getting
    lost in the noise of bad advice on the Internet. David Fischer at ReadTheDocs.com
    (RTD) and the organizer for San Diego Python ran into these misinformed blog posts
    when he started learning about NLP to build a Python package classifier. Ultimately
    he ended up building a tagger, which gave RTD advertisers more effective placements
    for their ads and gave developers reading documentation more relevant advertisements.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这对您来说可能是在纠结细节，但这绝不仅仅是语义。在互联网上错误建议的噪音中，正在丢失您处理的文本的语义（含义）。当David Fischer在ReadTheDocs.com（RTD）和圣地亚哥Python组织者开始学习NLP以构建Python包分类器时，他遇到了这些误导的博客文章。最终，他建立了一个标记器，为RTD广告商提供了更有效的广告位置，并为阅读文档的开发人员提供了更相关的广告。
- en: Tip
  id: totrans-219
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: To turn any multi-class classifier into a multi-label tagger you must change
    your activation function from `softmax` to an element-wise `sigmoid` function.
    A softmax creates a probability distribution across all the mutually exclusive
    categorical labels. A sigmoid function allows every value to take on any value
    between zero and one, such that each dimension in your multi-label tagging output
    represents the independent binary probability of that particular label applying
    to that instance.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 要将任何多类分类器转换为多标签标记器，您必须将激活函数从`softmax`更改为逐元素的`sigmoid`函数。Softmax在所有相互排斥的分类标签上创建一个概率分布。Sigmoid函数允许每个值取零到一之间的任意值，以便您多标签标记输出中的每个维度表示该特定标签适用于该实例的独立二进制概率。
- en: 8.3 Backpropagation through time
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 通过时间的反向传播
- en: Backpropagation for RNNs is a lot more work than for CNNs. The reason training
    an RNN is so computationally expensive is that it must perform the forward and
    backward calculations many times for each text example - once for each token in
    the text. And then it has to do all that again for the next layer in the RNN.
    And this sequence of operations is really important because the computation for
    one token depends on the previous one. You are recycling the output and hidden
    state tensors back into the calculation for the next token. For CNNs and fully
    connected neural networks, the forward and backward propagation calculations could
    run all at once on the entire layer. The calculations for each token in your text
    did not affect the calculation for the neighboring tokens in the same text. RNNs
    do forward and backward propagation in time, from one token in the sequence to
    the next.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 对于RNN来说，反向传播比对CNN来说要复杂得多。训练RNN之所以如此计算密集，是因为它必须为每个文本示例的每个标记执行前向和后向计算多次。然后，它必须再次为RNN中的下一层执行所有这些操作。这一系列操作非常重要，因为一个标记的计算取决于前一个标记。您正在将输出和隐藏状态张量循环回到下一个标记的计算中。对于CNN和完全连接的神经网络，前向和后向传播计算可以同时在整个层上运行。您文本中每个标记的计算不会影响同一文本中相邻标记的计算。RNN在时间上进行前向和后向传播，从序列中的一个标记到下一个标记。
- en: But you can see in the unrolled RNN in Figure 8.7 that your training must propagate
    the error back through all the weight matrix multiplications. Even though the
    weight matrices are the same, or `tied` for all the tokens in your data, they
    must work on each and every token in each of your texts. So your training loop
    will need to loop through all the tokens backward to ensure that the error at
    each step of the way is used to adjust the weights.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 但是您可以在图8.7中的展开的循环神经网络中看到，您的训练必须将错误通过所有权重矩阵乘法传播回去。即使权重矩阵对于数据中的所有标记都是相同的，或者`tied`，它们也必须作用于每个文本中的每个标记。因此，您的训练循环将需要向后循环遍历所有标记，以确保每一步的错误都被用来调整权重。
- en: The initial error value is the distance between the final output vector and
    the "true" vector for the label appropriate for that sample of text. Once you
    have that difference between the truth and the predicted vector, you can work
    your way back through time (tokens) to propagate that error to the previous time
    step (previous token). The PyTorch package will use something very similar to
    the chain rule that you used in algebra or calculus class to make this happen.
    PyTorch calculates the gradients it needs during forward propagation and then
    multiplies those gradients by the error for each token to decide how much to adjust
    the weights and improve the predictions.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 初始误差值是最终输出向量与适用于该文本样本的“真实”向量之间的距离。一旦你得到了真实向量和预测向量之间的差异，你就可以通过时间（标记）向后传播该误差，将该误差传播到上一个时间步（上一个标记）。PyTorch
    包将使用与您在代数或微积分课程中使用的链式法则非常相似的东西来实现这一点。PyTorch 在正向传播过程中计算它需要的梯度，然后将这些梯度乘以每个标记的误差，以决定调整权重的量并改善预测。
- en: And once you’ve adjusted the weights for all the tokens in one layer you do
    the same thing again for all the tokens on the next layer. Working your way from
    the output of the network all the way back to the inputs (tokens) you will eventually
    have to "touch" or adjust all of the weights many times for each text example.
    Unlike backpropagation through a linear layer or CNN layer, the backpropagation
    on an RNN must happen serially, one token at a time.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你为一层中的所有标记调整了权重，你就可以为下一层中的所有标记做同样的事情。从网络的输出一直回到输入（标记），你最终将不得不多次“触及”或调整每个文本示例的所有权重。与通过线性层或
    CNN 层的反向传播不同，RNN 上的反向传播必须按顺序进行，一次一个标记。
- en: An RNN is just a normal feedforward neural network "rolled up" so that the Linear
    weights are multiplied again and again for each token in your text. If you unroll
    it you can see all the weight matrices that need to be adjusted. And like the
    CNN, many of the weight matrices are shared across all of the tokens in the unrolled
    view of the neural network computational graph. An RNN is one long kernel that
    reuses "all" of the weights for each text document. The weights of an RNN are
    one long, giant kernel. At each time step, it is the *same* neural network, just
    processing a different input and output at that location in the text.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 一个循环神经网络（RNN）只是一个普通的前馈神经网络，被“卷起来”，以便线性权重被为文本中的每个标记再次相乘。如果展开它，你可以看到所有需要调整的权重矩阵。而且像卷积神经网络一样，许多权重矩阵在神经网络计算图的展开视图中对所有标记共享。RNN
    是一个长的内核，它重用了每个文本文档中的“所有”权重。RNN 的权重是一个长而巨大的内核。在每个时间步长，它是*相同的*神经网络，只是在文本中的那个位置处理不同的输入和输出。
- en: Tip
  id: totrans-227
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: In all of these examples, you have been passing in a single training example,
    the *forward pass*, and then backpropagating the error. As with any neural network,
    this forward pass through your network can happen after each training sample,
    or you can do it in batches. And it turns out that batching has benefits other
    than speed. But for now, think of these processes in terms of just single data
    samples, single sentences, or documents.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些示例中，你一直在传递一个单一的训练示例，*前向传播*，然后反向传播错误。与任何神经网络一样，你的网络中的这个前向传播可以在每个训练样本之后发生，或者你可以批处理。而且批处理除了速度之外还有其他好处。但是现在，把这些过程看作单个数据样本、单个句子或文档。
- en: In chapter 7 you learned how to process a string all at once with a CNN. CNNs
    can recognize patterns of meaning in text using kernels (matrices of weights)
    that represent those patterns. CNNs and the techniques of previous chapters are
    great for most NLU tasks such as text classification, intent recognition, and
    creating embedding vectors to represent the meaning of text in a vector. CNNs
    accomplish this with overlapping windows of weights that can detect almost any
    pattern of meaning in text.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 7 章中，你学会了如何使用 CNN 一次处理一个字符串。CNN 可以使用代表这些模式的内核（权重矩阵）识别文本中的意义模式。CNN 和前几章的技术非常适用于大多数
    NLU 任务，如文本分类、意图识别和创建表示文本意义的嵌入向量。CNN 通过可以检测文本中几乎任何意义模式的重叠窗口权重来实现这一点。
- en: Figure 8\. 9\. 1D convolution with embeddings
  id: totrans-230
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8\. 9\. 使用嵌入进行 1D 卷积
- en: '![cnn stride text words are sacred transparent drawio](images/cnn-stride-text-words-are-sacred_transparent_drawio.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![cnn stride text words are sacred transparent drawio](images/cnn-stride-text-words-are-sacred_transparent_drawio.png)'
- en: In Chapter 7 you imagined striding the kernel window over your text, one step
    at a time. But in reality, the machine is doing all the multiplications in parallel.
    The order of operations doesn’t matter. For example, the convolution algorithm
    can do the multiplication on the pair of words and then hop around to all the
    other possible locations for the window. It just needs to compute a bunch of dot
    products and then sum them all up or pool them together at the end. Addition is
    commutative (order doesn’t matter). And none of the convolution dot products depend
    on any of the others. In fact, on a GPU these matrix multiplications (dot products)
    are all happening *in parallel* at approximately the *same* time.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 7 章中，您想象将内核窗口跨越文本，一次一步地进行滑动。 但实际上，机器是在并行进行所有乘法。 操作的顺序并不重要。 例如，卷积算法可以对词对进行乘法，然后跳到窗口的所有其他可能位置。
    它只需要计算一堆点积，然后在最后将它们全部相加或汇总在一起。 加法是可交换的（顺序无关紧要）。 实际上，在 GPU 上，这些矩阵乘法（点积）几乎同时并行进行。
- en: But an RNN is different. With an RNN you’re recycling the output of one token
    back into the dot product you’re doing on the next token. So even though we talked
    about RNNs working on any length text, to speed things up, most RNN pipelines
    truncate and pad the text to a fixed length. This unrolls the RNN matrix multiplications
    so that And you need two matrix multiplications for an RNN compared to one multiplication
    for a CNN. You need one matrix of weights for the hidden vector and another for
    the output vector.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 但是 RNN 不同。 使用 RNN 时，您将一个标记的输出重新循环到您对下一个标记执行的点积中。 因此，即使我们讨论过 RNN 可以处理任意长度的文本，为了加快速度，大多数
    RNN 流水线会将文本截断和填充到固定长度。 这样会展开 RNN 的矩阵乘法，这样和你需要为 RNN 需要两次矩阵乘法，而 CNN 需要一次乘法相比速度更快。
    您需要一个用于隐藏向量的权重矩阵和另一个用于输出向量的权重矩阵。
- en: If you’ve done any signal processing or financial modeling you may have used
    an RNN without knowing it. The recurrence part of a CNN is called 'auto-regression"
    in the world of signal processing and quantitative financial analysis. An *auto-regressive
    moving average* (ARMA) model is an RNN in disguise.^([[23](#_footnotedef_23 "View
    footnote.")])
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您进行过任何信号处理或金融建模，您可能已经使用了 RNN 而不自知。 CNN 中的回归部分在信号处理和定量金融分析领域被称为“自回归”。 自回归移动平均模型是一个伪装的
    RNN。^([[23](#_footnotedef_23 "查看脚注。")])
- en: In this chapter, you are learning about a new way to structure the input data.
    Just as in a CNN, each token is associated with a time (`t`) or position within
    the text. The variable `t` is just another name for the index variable in your
    sequence of tokens.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您正在了解一种新的结构化输入数据的方式。 就像在 CNN 中一样，每个标记都与文本中的时间（`t`）或位置相关联。 变量 `t` 只是您标记序列中的索引变量的另一个名称。
- en: You will even see places where you use the integer value of `t` to retrieve
    a particular token in the sequence of tokens with an expression such as `token
    = tokens[t]`. So when you see `t-1` or `tokens[t-1]` you know that it is referring
    to the preceding time step or token. And `t+1` and `tokens[t+1]` refers to the
    next time step or token. In past chapters, you may have seen that we sometimes
    used `i` for this index value.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 甚至会看到您使用 `t` 的整数值来检索序列中的特定标记，例如 `token = tokens[t]`。 因此，当您看到 `t-1` 或 `tokens[t-1]`
    时，您知道它是指前一个时间步或标记。 而 `t+1` 和 `tokens[t+1]` 则是指下一个时间步或标记。 在过去的章节中，您可能已经看到我们有时将
    `i` 用于此索引值。
- en: 'Now you will use multiple different indexes to keep track of what has been
    passed into the network and is being output by the network:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您将使用多个不同的索引来跟踪输入到网络中的内容以及网络输出的内容：
- en: '`t` or `token_num`: time step or token position for the current tensor being
    input to the network'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`t` 或 `token_num`：当前输入到网络中的张量的时间步或标记位置'
- en: '`k` or `sample_num`: sample number within a batch for the text example being
    trained on'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`k` 或 `sample_num`：正在训练的文本示例的批次中的样本号'
- en: '`b` or `batch_num`: batch number of the set of samples being trained'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`b` 或 `batch_num`：正在训练的样本集的批次号'
- en: '`epoch_num`: number of epochs that have passed since the start of training'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epoch_num`: 训练开始后经过的周期数'
- en: Figure 8.10 Data fed into a recurrent network
  id: totrans-242
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.10 输入到循环网络的数据
- en: '![rnn input](images/rnn_input.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![rnn input](images/rnn_input.png)'
- en: This 2-D tensor representation of a document is similar to the "player piano"
    representation of text in chapter 2\. Only this time you are creating a dense
    representation of each token using word embeddings.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这种文档的二维张量表示类似于第 2 章中文本的“自动钢琴”表示。只不过这一次，您将使用词嵌入来创建每个标记的密集表示。
- en: For an RNN you no longer need to process each text sample all at once. Instead,
    you process text one token at a time.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 RNN，您不再需要一次处理每个文本样本。相反，您逐个标记地处理文本。
- en: In your recurrent neural net, you pass in the word vector for the first token
    and get the network’s output. You then pass in the second token, but you also
    pass in the output from the first token! And then pass in the third token along
    with the output from the second token. And so on. The network has a concept of
    before and after, cause and effect - some vague notion of time (see Figure 8.8).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的循环神经网络中，你传入第一个标记的词向量，并获得网络的输出。然后传入第二个标记，但同时也传入了第一个标记的输出！然后传入第三个标记以及第二个标记的输出。依此类推。网络有一个关于前后、因果关系的概念
    - 一些关于时间的模糊概念（见图 8.8）。
- en: 8.3.1 Initializing the hidden layer in an RNN
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.1 初始化 RNN 中的隐藏层
- en: There’s a chicken-and-egg problem with the hidden layer when you restart the
    training of an RNN on each new document. For each text string you want to process,
    there is no "previous" token or previous hidden state vector to recycle back into
    the network. You don’t have anything to prime the pump with and start the recycling
    (recurrence) loop. Your model’s `forward()` method needs a vector to concatenate
    with the input vector so that it will be the right size for multiplying by `W_c2h`
    and `W_c2o`.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 当您在每个新文档上重新启动 RNN 的训练时，隐藏层存在一个鸡生蛋的问题。对于您要处理的每个文本字符串，都没有“先前”的标记或先前的隐藏状态向量可供重新循环回网络中。您没有任何东西可以引导并启动循环（递归）循环。您的模型的
    `forward()` 方法需要一个向量与输入向量连接，以便将其调整为与 `W_c2h` 和 `W_c2o` 相乘的正确大小。
- en: The most obvious approach is to set the initial hidden state to all zeroes and
    allow the biases and weights to quickly ramp up to the best values during the
    training on each sample. This can be great for any of the neurons that are keeping
    track of time, the position in the token sequence that is currently (recurrently)
    being processed. But there are also neurons trying to predict how far from the
    end of the sequence you are. And your network has a defined polarity with 0 for
    off and 1 for on. So you may want your network to start with a mix of zeros and
    ones for your hidden state vector. Better yet you can use some gradient or pattern
    of values between zero and 1 which is your particular "secret sauce", based on
    your experience with similar problems.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 最明显的方法是将初始隐藏状态设置为全零，并在训练每个样本时快速将偏差和权重增加到最佳值。这对于任何正在跟踪时间的神经元（当前（递归）正在处理的标记序列中的位置）可能非常有用。但是，还有一些神经元试图预测您离序列末尾有多远。而且您的网络有一个定义明确的极性，0
    表示关闭，1 表示打开。因此，您可能希望您的网络以零和一的混合值开始隐藏状态向量。最好的做法是使用一些介于零和 1 之间的梯度或值模式，这是您特定的“秘密配方”，基于您处理类似问题的经验。
- en: Getting creative and being consistent with your initialization of deep learning
    networks has the added benefit of creating more "explainable" AI. You will often
    create a predictable structure in your weights. And by doing it the same way each
    time you will know where to look within all the layers. For example, you will
    know which positions in the hidden state vector are keeping track of position
    (time) within the text.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始化深度学习网络时，变得有创意并保持一致，还有一个额外的好处，那就是创造更多可“解释”的人工智能。您经常会在权重中创建可预测的结构。通过每次都以相同的方式进行，您将知道在所有层中查找的位置。例如，您将知道隐藏状态向量中的哪些位置在跟踪文本中的位置（时间）。
- en: To get the full benefit of this consistency in your initialization values you
    will also need to be consistent with the ordering of your samples used during
    training. You can sort your texts by their lengths, as you did with CNNs in Chapter
    7\. But many texts will have the same length, so you will also need a sort algorithm
    that consistently orders the samples with the same length. Alphabetizing is an
    obvious option, but this will tend to trap your model in local minima as it’s
    trying to find the best possible predictions for your data. It would get really
    good at the "A" names but do poorly on "Z" names. So don’t pursue this advanced
    seeding approach until you’ve fully mastered the random sampling and shuffling
    that has proven so effective.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 要充分利用初始化值的一致性，您还需要在训练期间使用的样本的排序上保持一致。 您可以按其长度对文本进行排序，就像您在第 7 章中使用 CNN 时所做的那样。
    但是，许多文本将具有相同的长度，因此您还需要一种排序算法，该算法可以一致地对具有相同长度的样本进行排序。 字母顺序是一个明显的选择，但这会倾向于使您的模型陷入局部最小值，因为它试图找到数据的最佳可能预测。
    它会在“A”名称上表现得非常好，但在“Z”名称上表现不佳。 因此，在完全掌握已被证明非常有效的随机抽样和洗牌之前，请不要追求这种高级初始化方法。
- en: As long as you are consistent throughout the training process, your network
    will learn the biases and weights that your network needs to layer on top of these
    initial values. And that can create a recognizable structure in your neural network
    weights.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 只要您在整个训练过程中保持一致，您的网络将学习您的网络需要在这些初始值之上叠加的偏差和权重。 这可以在您的神经网络权重中创建一个可识别的结构。
- en: Tip
  id: totrans-253
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: In some cases, it can help to seed your neural networks with an initial hidden
    state other than all zeros. Johnathon Frankle and Michael Carbin found that being
    intentional about reuse of good initialization values can be key to helping a
    network find the *global minimum* loss achievable for a particular dataset "Lottery
    Ticket Hypothesis" paper, ^([[24](#_footnotedef_24 "View footnote.")]) Their approach
    is to initialize all weights and biases using a random seed that can be reused
    in subsequent training.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，使用初始隐藏状态而不是全零状态可能有助于启动您的神经网络。 Johnathon Frankle 和 Michael Carbin 发现，有意识地重用良好的初始化值可能是帮助网络找到特定数据集的全局最小损失的关键^（参见脚注
    [24]） Their approach is to initialize all weights and biases using a random seed
    that can be reused in subsequent training.
- en: Now your network is remembering something! Well, sort of. A few things remain
    for you to figure out. For one, how does backpropagation even work in a structure
    like this?
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您的网络记住了某些东西！ 嗯，有点像。 还有一些事情需要您解决。 首先，这样的结构中反向传播工作是如何进行的？
- en: Another approach that is popular in the Keras community is to retain the hidden
    layer from a previous batch of documents. This "pre-trained" hidden layer embedding
    gives your language model information about the context of the new document -
    the text that came before it. However, this only makes sense if you’ve maintained
    the order of your documents within the batches and across the batches that you
    are training. In most cases, you shuffle and reshuffle your training examples
    with each epoch. You do this when you want your model to work equally well at
    making predictions "cold" without any priming by reading similar documents or
    nearby passages of text.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 社区中另一种流行的方法是保留来自先前批处理的隐藏层。 这种“预训练”的隐藏层嵌入给出了您的语言模型有关新文档上下文的信息 - 即它之前的文本。
    但是，只有在训练中保持了文档的顺序才有意义。 在大多数情况下，您会在每个 epoch 中对训练示例进行洗牌和重洗牌。 当您希望您的模型在没有通过阅读类似文档或附近文本段落进行任何引导的情况下同样出色地进行预测时，您会这样做。
- en: So unless you are trying to squeeze out every last bit of accuracy you can for
    a really difficult problem you should probably just reset it to zeros every time
    to start feeding a new document into your model. And if you do use this *stateful*
    approach to training an RNN, make sure you will be able to warm up your model
    on context documents for each prediction it needs to make in the real world (or
    on your test set). And make sure you prepare your documents in a consistent order
    and can reproduce this document ordering for a new set of documents that you need
    to make prediction on with your model.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 所以除非您试图挤出您对一个非常困难的问题的每一点准确性，否则您可能只需在每次将新文档输入模型时将其重置为零即可。 如果您确实使用了这种 *stateful*
    方法来训练 RNN，请确保您能够在真实世界中（或者在您的测试集上）对每个预测需要的上下文文档进行热身，并确保您以一致的顺序准备文档，并且可以为需要对模型进行预测的新文档集合重现此文档排序。
- en: 8.4 Remembering with recurrent networks
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.4 使用递归网络记忆
- en: An RNN remembers previous words in the text they are processing and can keep
    adding more and more patterns to its memory as it processes a theoretically limitless
    amount of text. This can help it understand patterns that span the entire text
    and recognize the difference between two texts that have dramatically different
    meanings depending on where words occur.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络记住了它们正在处理的文本中的前面单词，并且可以在处理理论上无限量的文本时不断地向其记忆中添加更多模式。这可以帮助它理解跨越整个文本的模式，并且识别出两个具有截然不同含义的文本之间的区别，这取决于单词出现的位置。
- en: '*I apologize for the lengthy letter. I didn’t have time to write a shorter
    one.*'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '*抱歉这封信太长了。我没有时间写一封更短的。*'
- en: '*I apologize for the short letter. I didn’t have time to write a lengthy one.*'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '*抱歉这封信太短了。我没有时间写一封更长的。*'
- en: Swapping the words "short" and "lengthy", flips the meaning of this Mark Twain
    quote. Knowing Mark Twain’s dry sense of humor and passion for writing, can you
    tell which quote is his? It’s the one where he apologizes for the lengthy letter.
    He’s making light of the fact that editing and writing concisely is hard work.
    It’s something that smart humans can still do better than even the smartest AI.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 交换“短”和“长”这两个词，会改变这个马克·吐温的引用的含义。了解马克·吐温幽默的干燥的幽默感和对写作的热情，你能分辨出哪个是他的引用吗？是他为长信道歉的那个。他在轻松地谈论编辑和简洁写作是一项艰苦的工作。这是一件即使是最聪明的人类也比最聪明的人工智能做得更好的事情。
- en: The CNNs you learned about in Chapter 7 would have a hard time making the connection
    between these two sentences about lengthy and short letters, whereas RNNs make
    this connection easily. This is because CNNs have a limited window of text that
    they can recognize patterns within. To make sense of an entire paragraph, you
    would have to build up layers of CNNs with overlapping kernels or windows of text
    that they understand. RNNs do this naturally. RNNs remember something about every
    token in the document they’ve read. They remember everything you’ve input into
    them until you tell them you are done with that document. This makes them better
    at summarizing lengthy Mark Twain letters and makes them better at understanding
    his long sophisticated jokes.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 您在第 7 章学到的卷积神经网络会很难在关于长信和短信的这两个句子之间建立联系，而循环神经网络却可以轻松地做到这一点。这是因为卷积神经网络在识别模式时有一个有限的文本窗口。要理解整个段落，您必须构建具有重叠核或文本窗口的
    CNN 图层。循环神经网络可以自然地做到这一点。循环神经网络记住了它们读取的文档中的每个标记的一些信息。在您告诉它们您已完成该文档之前，它们会记住您输入到其中的所有内容。这使它们更擅长摘要马克·吐温的长信，并使它们更擅长理解他的长而复杂的笑话。
- en: Mark Twain was right. Communicating things concisely requires skill, intelligence
    and attention to detail. In the paper "Attention is All You Need" Ashish Vaswani
    revealed how transformers can add an attention matrix that allows RNNs to accurately
    understand much longer documents.^([[25](#_footnotedef_25 "View footnote.")])
    In chapter 9 you’ll see this attention mechanism at work, as well as the other
    tricks that make the transformer approach to RNNs the most successful and versatile
    deep learning architecture so far.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 马克·吐温是对的。简洁地传达事物需要技巧、智慧和对细节的关注。在论文“注意力就是一切”中，阿希什·瓦斯瓦尼揭示了变换器如何添加注意力矩阵，使循环神经网络能够准确理解更长的文档。在第九章中，您将看到这种注意机制的运作，以及使变换器方法成为迄今为止最成功和最灵活的深度学习架构的其他技巧。
- en: Summarization of lengthy text is still an unsolved problem in NLP. Even the
    most advanced RNNs and transformers make elementary mistakes. In fact, The Hutter
    Prize for Artificial Intelligence will give you 5000 Euros for each one percent
    improvement in the compression (lossless summarization) of Wikipedia.^([[26](#_footnotedef_26
    "View footnote.")]) The Hutter Prize focuses on the compression of the symbols
    within Wikipedia. You’re going to learn how to compress the meaning of text. That’s
    even harder to do well. And it’s hard to measure how well you’ve done it.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 长文本的摘要仍然是自然语言处理中未解决的问题。即使是最先进的循环神经网络和变换器也会犯初级错误。事实上，人工智能的赫特奖将为维基百科压缩（无损摘要）每提高一百分之一而奖励您
    5000 欧元。赫特奖专注于压缩维基百科中的符号。您将学习如何压缩文本的含义。这甚至更难做到。很难衡量您做得有多好。
- en: You will have to develop generally intelligent machines that understand common
    sense logic and can organize and manipulate memories and symbolic representations
    of those memories. That may seem hopeless, but it’s not. The RNNs you’ve built
    so far can remember everything in one big hidden representation of their understanding.
    Can you think of a way to give some structure to that memory, so that your machine
    can organize its thoughts about text a bit better? What if you gave your machine
    a separate way to maintain both short-term memories and long-term memories? This
    would give it a working memory that it could then store in long-term memory whenever
    it ran across a concept that was important to remember.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 你将不得不开发通用智能机器，它们能够理解常识逻辑，并能够组织和操作记忆以及这些记忆的符号表示。这可能看起来无望，但事实并非如此。到目前为止，你构建的 RNN
    可以记住其理解的一切，都存储在一个大的隐藏表示中。你能想到一种方法来给这个记忆结构一些结构，让你的机器可以更好地组织关于文本的思维吗？如果你让你的机器有一种单独的方式来维持短期记忆和长期记忆怎么样？这将给它一个工作记忆，当它遇到需要记住的重要概念时，它可以将其存储在长期记忆中。
- en: 8.4.1 Word-level Language Models
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.1 单词级语言模型
- en: All the most impressive language models that you’ve read about use words as
    their tokens, rather than individual characters. So, before you jump into GRUs
    and LSTMs you will need to rearrange your training data to contain sequences of
    word IDs rather than character (letter) IDs. And you’re going to have to deal
    with much longer documents than just surnames, so you will want to `batchify`
    your dataset to speed it up.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 所有你听说过的最令人印象深刻的语言模型都使用单词作为它们的令牌，而不是单个字符。所以，在你跳入 GRU 和 LSTM 之前，你需要重新安排你的训练数据，以包含单词
    ID 的序列，而不是字符（字母）ID。而且你将不得不处理比只有姓氏长得多的文档，所以你会想要对你的数据集进行 `batchify` 以加快速度。
- en: Take a look at the Wikitext-2 dataset and think about how you will preprocess
    it to create a sequence of token IDs (integers).
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 看一看维基文本-2 数据集，并思考如何预处理它以创建一系列令牌 ID（整数）。
- en: '[PRE21]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Oh wow, this is going to be an interesting dataset. Even the English language
    version of Wikipedia contains a lot of other natural languages in it, such as
    Japanese in this first article. If you use your tokenization and vocabulary-building
    skills from previous chapters you should be able to create a Corpus class like
    the one used in the RNN examples coming up.^([[27](#_footnotedef_27 "View footnote.")])
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 哇哦，这将是一个有趣的数据集。即使是英文版的维基百科也包含很多其他的自然语言，比如这篇第一篇文章中的日语。如果你使用前面章节的分词和词汇构建技能，你应该能够创建一个类似于即将出现的
    RNN 示例中使用的 Corpus 类。
- en: '[PRE22]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'And you always want to make sure that your vocabulary has all the info you
    need to generate the correct words from the sequence of word IDs:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 并且你总是希望确保你的词汇量包含了你需要从单词 ID 序列中生成正确单词的所有信息：
- en: '[PRE23]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Now, during training your RNN will have to read each token one at a time. That
    can be pretty slow. What if you could train it on multiple passages of text simultaneously?
    You can do this by splitting your text into batches or *batchifying* your data.
    These batches can each become columns or rows in a matrix that PyTorch can more
    efficiently perform math on within a *GPU* (Graphics Processing Unit).
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在训练过程中，你的 RNN 将不得不逐个读取每个令牌。这可能相当慢。如果你能同时训练它在多个文本段落上呢？你可以通过将文本拆分成批次或 *batchifying*
    你的数据来实现这一点。这些批次可以成为 PyTorch 中可以更有效地执行数学运算的矩阵中的列或行，在 *GPU*（图形处理单元）内。
- en: In the `nlpia2.ch08.data` module you’ll find some functions for batchifying
    long texts.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `nlpia2.ch08.data` 模块中，你会找到一些批量化长文本的函数。
- en: '[PRE24]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: One last step and your data is ready for training. You need to `stack` the tensors
    within this list so that you have one large tensor to iterate through during your
    training.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步，你的数据已经准备好进行训练了。你需要 `stack` 这个列表中的张量，这样你就可以在训练过程中迭代一个大张量。
- en: '[PRE26]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 8.4.2 Gated Recurrent Units (GRUs)
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.2 门控循环单元（GRUs）
- en: For short text, ordinary RNNs with a single activation function for each neuron
    works well. All your neurons need to do is recycle and reuse the hidden vector
    representation of what they have read so far in the text. But ordinary RNNs have
    a short attention span that limits their ability to understand longer texts. The
    influence of the first token in a string fades over time as your machine reads
    more and more of the text. That’s the problem that GRU (Gated Recurrent Unit)
    and LSTM (Long and Short Term Memory) neural networks aim to fix.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 对于短文本，具有单个激活函数的普通循环神经网络效果很好。所有神经元所需做的就是循环和重复利用它们迄今为止在文本中所读取的隐藏向量表示。但是普通循环神经网络的注意力集中范围有限，限制了它们理解较长文本的能力。随着您的机器阅读越来越多的文本，字符串中第一个令牌的影响会随着时间的推移而减弱。这就是门控循环单元（GRU）和长短期记忆（LSTM）神经网络试图解决的问题。
- en: How do you think you could counteract fading memory of early tokens in a text
    string? How could you stop the fading, but just for a few important tokens at
    the beginning of a long text string? What about adding an `if` statement to record
    or emphasize particular words in the text. That’s what GRUs do. GRUs add `if`
    statements, called *logic gates* (or just "gates"), to RNN neurons.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 您认为如何抵消文本字符串中早期令牌的记忆衰减？您如何阻止衰减，但只针对长文本字符串开头的几个重要令牌？在记录或强调文本中的特定单词方面，您怎么想？这就是
    GRU 所做的。GRU 添加了称为*逻辑门*（或只是“门”）的`if`语句到 RNN 神经元中。
- en: The magic of machine learning and backpropagation will take care of the if statement
    conditions for you, so you don’t have to adjust logic gate thresholds manually.
    Gates in an RNN learn the best thresholds by adjusting biases and weights that
    affect the level of a signal that triggers a zero or 1 output (or something in
    between). And the magic of back-propagation in time will train the LSTM gates
    to let important signals (aspects of token meaning) pass through and get recorded
    in the hidden vector and cell state vector.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习和反向传播的魔法会替您处理 if 语句条件，因此您不必手动调整逻辑门阈值。RNN 中的门通过调整影响触发零或1输出（或介于两者之间的某种输出）的信号水平的偏置和权重来学习最佳阈值。而时间上的反向传播的魔法将训练
    LSTM 门让重要信号（令牌含义的方面）通过并记录在隐藏向量和单元状态向量中。
- en: But wait, you probably thought we already had if statements in our network.
    After all, each neuron has a nonlinear activation function that acts to squash
    some outputs to zero and push others up close to 1\. So the key isn’t that LSTMs
    add gates (activation functions) to your network. The key is that the new gates
    are *inside* the neuron and connected in a way that creates a structure to your
    neural network that wouldn’t naturally just emerge from a normal linear, fully-connected
    layer of neurons. And that structure was intentionally designed with a purpose,
    reflecting what researchers thing would help RNN neurons deal with this long-term
    memory problem.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 但是等等，您可能认为我们的网络中已经有了 if 语句。毕竟，每个神经元都有一个非线性激活函数，作用是将一些输出压缩到零并将其他输出推向接近1。因此，关键不是
    LSTM 向网络添加门（激活函数）。关键在于新门是*在*神经元内部并以一种连接方式连接的，这种连接方式创建了一个结构，您的神经网络不会自然地从一个正常的线性、全连接的神经元层中出现。这种结构是有意设计的，目的是反映研究人员认为将有助于
    RNN 神经元解决这个长期记忆问题的内容。
- en: In addition to the original RNN output gate, GRUs add two new logic gates or
    activation functions within your recurrent unit.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 除了原始 RNN 输出门之外，GRU 还在您的循环单元中添加了两个新的逻辑门或激活函数。
- en: 'Reset gate: What parts of the hidden layer should be blocked because they are
    no longer relevant to the current output.'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 复位门：应该阻止隐藏层的哪些部分，因为它们对当前输出不再相关。
- en: 'Update gate: What parts of the hidden layer should matter to the current output
    (now, at time `t`).'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新门：隐藏层的哪些部分应该与当前输出（现在，在时间`t`）相关。
- en: You already had an activation function on the output of your RNN layer. This
    output logic gate is called the "new" logic gate in a GRU.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经在 RNN 层的输出上有了一个激活函数。这个输出逻辑门在 GRU 中被称为“新”逻辑门。
- en: '[PRE27]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: So when you are thinking about how many units to add to your neural network
    to solve a particular problem, each LSTM or GRU unit gives your network a capacity
    similar to 2 "normal" RNN neurons or hidden vector dimensions. A unit is just
    a more complicated, higher-capacity neuron, and you can see this if you count
    up the number of "learned parameters" in your LSTM model and compare it to those
    of an equivalent RNN.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当你考虑向你的神经网络添加多少单元来解决特定问题时，每个 LSTM 或 GRU 单元都给你的网络一个类似于 2 个 "普通" RNN 神经元或隐藏向量维度的容量。一个单元只是一个更复杂、更高容量的神经元，如果你数一数你的
    LSTM 模型中的 "学习参数" 的数量，并将其与等效的 RNN 比较，你会看到这一点。
- en: Note
  id: totrans-292
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注
- en: You’re probably wondering why we started using the word "unit" rather than "neuron"
    for the elements of this neural net. Researchers use the terms "unit" or "cell"
    to describe the basic building blocks of an LSTM or GRU neural network because
    they are a bit more complicated than a neuron. Each unit or cell in an LSTM or
    GRU contains internal gates and logic. This gives your GRU or LSTM units more
    capacity for learning and understanding text, so you will probably need fewer
    of them to achieve the same performance as an ordinary RNN.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道为什么我们开始使用"单元"这个词而不是"神经元"来描述这个神经网络的元素。研究人员使用"单元"或"细胞"来描述 LSTM 或 GRU 神经网络的基本构建块，因为它们比神经元稍微复杂一些。每个
    LSTM 或 GRU 中的单元或细胞都包含内部门和逻辑。这使得你的 GRU 或 LSTM 单元具有更多的学习和理解文本的能力，因此你可能需要比普通 RNN
    更少的单元来达到相同的性能。
- en: The *reset*, *update*, and *new* logic gates are implemented with the fully-connected
    linear matrix multiplications and nonlinear activation functions you are familiar
    with from Chapter 5\. What’s new is that they are implemented on each token recurrently
    and they are implemented on the hidden and input vectors in parallel. Figure 8.12
    shows how the input vector and hidden vector for a single token flow through the
    logic gates and output the prediction and hidden state tensors.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '*重置*、*更新* 和 *新* 逻辑门是使用你在第五章熟悉的全连接线性矩阵乘法和非线性激活函数实现的。新的地方在于它们在每个标记上是递归实现的，并且它们是在隐藏向量和输入向量上并行实现的。图
    8.12 显示了单个标记的输入向量和隐藏向量如何通过逻辑门流过并输出预测和隐藏状态张量。'
- en: Figure 8.11 GRUs add capacity with logic gates
  id: totrans-295
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.11 GRU 通过逻辑门增加容量
- en: '![gru drawio](images/gru_drawio.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![gru drawio](images/gru_drawio.png)'
- en: 'If you have gotten good at reading data flow diagrams like Figure 8.12 you
    may be able to see that the GRU *update* and *relevance* logic gates are implementing
    the following two functions: ^([[28](#_footnotedef_28 "View footnote.")])'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你擅长阅读数据流图，比如图 8.12，你可能会看到 GRU *更新* 和 *相关性* 逻辑门实现了以下两个功能：
- en: '[PRE28]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Looking at these two lines of code you can see that inputs to the formula are
    exactly the same. Both the hidden and input tensors are multiplied by weight matrices
    in both formulas. And if you remember your linear algebra and matrix multiplication
    operations, you might be able to simplify the And you may notice in the block
    diagram (figure 8.12) that the input and hidden tensors are concatenated together
    before the matrix multiplication by W_reset, the reset weight matrix.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 观察这两行代码，你会发现公式的输入完全相同。在这两个公式中，隐藏张量和输入张量都被权重矩阵相乘。如果你记得线性代数和矩阵乘法操作，你可能能简化这个过程。并且你可能会注意到在方块图（图
    8.12）中，输入张量和隐藏张量在被重置权重矩阵 W_reset 进行矩阵乘法之前会被连接在一起。
- en: Once you add GRUs to your mix of RNN model architectures, you’ll find that they
    are much more efficient. A GRU will achieve better accuracy with fewer learned
    parameters and less training time and less data. The gates in a GRU give structure
    to the neural network that creates more efficient mechanisms for remembering important
    bits of meaning in the text. To measure efficiency you’ll need some code to count
    up the learned (trainable) parameters in your models. This is the number of weight
    values that your model must adjust to optimize the predictions. The requires_grad
    attribute is an easy way to check whether a particular layer contains learnable
    parameters or not.^([[29](#_footnotedef_29 "View footnote.")])
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你将 GRU 添加到你的 RNN 模型架构中，你会发现它们更加高效。GRU 将以更少的学习参数、更少的训练时间和更少的数据获得更好的准确性。GRU
    中的门给神经网络带来了结构，创造了更有效的机制来记住文本中的重要含义。为了衡量效率，你需要一些代码来计算模型中的学习（可训练）参数。这是你的模型必须调整以优化预测的权重值的数量。requires_grad
    属性是一个简单的方法，用来检查特定层是否包含可学习参数。
- en: '[PRE29]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The more weights or learned parameters there are, the greater the capacity of
    your model to learn more things about the data. But the whole point of all the
    clever ideas, like convolution and recurrence, is to create neural networks that
    are efficient. By choosing the right combination of algorithms, sizes and types
    of layers, you can reduce the number of weights or parameters your model must
    learn while simultaneously creating smarter models with greater capacity to make
    good predictions.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 权重或学习参数越多，您的模型就能够学习有关数据的更多信息。但是，所有巧妙的想法（例如卷积和递归）的整个目的是创建高效的神经网络。通过选择正确的算法组合、大小和层类型，您可以减少模型必须学习的权重或参数的数量，并同时创建更智能的模型，具有更大的能力做出良好的预测。
- en: If you experiment with a variety of GRU hyperparameters using the `nlpia2/ch08/rnn_word/hypertune.py`
    script you can aggregate all the results with your RNN results to compare them
    all together.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用`nlpia2/ch08/rnn_word/hypertune.py`脚本尝试各种GRU超参数，则可以将所有结果与RNN结果聚合在一起，以进行比较。
- en: '[PRE30]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: You can see from these experiments that GRUs are your best bet for creating
    language models that understand text well enough to predict the next word. Surprisingly
    GRUs do not need as many layers as other RNN architectures to achieve the same
    accuracy. And they take less time to train than RNNs to achieve comparable accuracy.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些实验结果可以看出，GRU是创建语言模型以足够准确地预测下一个单词的最佳选择。令人惊讶的是，与其他RNN架构相比，GRU不需要使用更多的层数来实现相同的准确性。并且，与RNN相比，它们所需的训练时间更少，以实现可比较的准确性。
- en: 8.4.3 Long and Short-Term Memory (LSTM)
  id: totrans-307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.3 长短期记忆（LSTM）
- en: An LSTM neuron adds two more internal gates in an attempt to improve both the
    long-term and the short-term memory capacity of an RNN. An LSTM retains the update
    and relevance gates but adds new gates for forgetting and the output gate. four
    internal gates, each with a different purpose. The first one is just the normal
    activation function that you are familiar with.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM神经元增加了两个内部门，以试图提高RNN的长期和短期记忆容量。 LSTM保留了更新和相关性门，但添加了新的门来进行遗忘和输出门，共四个内部门，每个具有不同的目的。第一个门只是您熟悉的普通激活函数。
- en: 'Forgetting gate (`f`): Whether to completely ignore some element of the hidden
    layer to make room in memory for future more important tokens.'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遗忘门(`f`)：是否完全忽略隐藏层的某个元素，以为将来更重要的令牌腾出空间。
- en: 'Input or update gate (`i`): What parts of the hidden layer should matter to
    the current output (now, at time `t`).'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入或更新门(`i`)：隐藏层的哪些部分应该对当前输出（现在，在时间`t`）起作用。
- en: 'Relevance or cell gate (`i`): What parts of the hidden layer should be blocked
    because they are not longer relevant to the current output.'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 相关性或细胞门(`i`)：应该阻塞哪些隐藏层的部分，因为它们与当前输出不再相关。
- en: 'Output gate (`o`): What parts of the hidden layer should be output, both to
    the neurons output as well as to the hidden layer for the next token in the text.'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出门(`o`)：隐藏层的哪些部分应输出，既输出到神经元输出，也输出到文本中下一个令牌的隐藏层。
- en: But what about that unlabeled `tanh` activation function at the upper right
    of Figure 8.12? That’s just the original output activation used to create the
    hidden state vector from the cell state. The hidden state vector holds information
    about the most recently processed tokens; it’s the short-term memory of the LSTM.
    The cell state vector holds a representation of the meaning of the text over the
    long term, since the beginning of a document.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，图8.12右上角的未标注的`tanh`激活函数是什么？这只是用来从细胞状态创建隐藏状态向量的原始输出激活。隐藏状态向量保存了关于最近处理的令牌的信息；它是LSTM的短期记忆。细胞状态向量保存了关于文档自开始以来文本含义的表示，即长期记忆。
- en: In Figure 8.13 you can see how these four logic gates fit together. The various
    weights and biases required for each of the logic gates are hidden to declutter
    the diagram. You can imagine the weight matrix multiplications happening within
    each of the activation functions that you see in the diagram. Another thing to
    notice is that the hidden state is not the only recurrent input and output. You’ve
    now got another encoding or state tensor called the *cell state*. As before, you
    only need the hidden state to compute the output at each time step. But the new
    cell state tensor is where the long and short-term memories of past patterns are
    encoded and stored to be reused on the next token.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 8.13 中，你可以看到这四个逻辑门是如何配合使用的。每个逻辑门所需的各种权重和偏差都被隐藏起来，以精简图表。你可以想象在图表中看到的每个激活函数内进行的权重矩阵乘法。
    另一个要注意的事情是隐藏状态不是唯一的循环输入和输出。你现在有了另一个编码或状态张量，称为*单元状态*。与以前一样，你只需要隐藏状态来计算每个时间步的输出。但是，新的单元状态张量是过去模式的长期和短期记忆所编码和存储的地方，以在下一个标记上重复使用。
- en: Figure 8.12 LSTMs add a forgetting gate and a cell output
  id: totrans-315
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.12 LSTM 添加了一个遗忘门和一个单元输出
- en: '![lstm drawio](images/lstm_drawio.png)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![lstm drawio](images/lstm_drawio.png)'
- en: One thing in this diagram that you’ll probably only see in the smartest blog
    posts is the explicit linear weight matrix needed to compute the output tensor.^([[30](#_footnotedef_30
    "View footnote.")]) Even the PyTorch documentation glosses over this tidbit. You’ll
    need to add this fully connected linear layer yourself at whichever layer you
    are planning to compute predictions based on your hidden state tensor.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在图中，你可能只会在最聪明的博客文章中看到所需的显式线性权重矩阵，用于计算输出张量。^([[30](#_footnotedef_30 "View footnote.")])即使是
    PyTorch 文档也会忽略这个琐事。你需要在计划根据隐藏状态张量计算预测的哪一层添加这个全连接的线性层。
- en: You’re probably saying to yourself "Wait, I thought all hidden states (encodings)
    were the same, why do we have this new *cell state* thing?" Well, that’s the long-term
    memory part of an LSTM. The cell state is maintained separately so the logic gates
    can remember things and store them there, without having to mix them in with the
    shorter-term memory of the hidden state tensor. And the cell state logic is a
    bit different from the hidden state logic. It’s designed to be selective in the
    things it retrains to keep room for things it learns about the text long before
    it reaches the end of the string.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想：“等等，我以为所有隐藏状态（编码）都是相同的，为什么我们有这个新的*单元状态*？”好吧，这就是 LSTM 的长期记忆部分。单元状态是单独维护的，因此逻辑门可以记住事物并将它们存储在那里，而无需将它们混合在隐藏状态张量的较短期记忆中。单元状态逻辑与隐藏状态逻辑有些不同。它的设计是有选择性地保留它所保留的东西，为学习文本长期到达字符串结尾之前的东西保留空间。
- en: The formulas for computing the LSTM logic gates and outputs are very similar
    to those for the GRU. The main difference is the addition of 3 more functions
    to compute all the signals you need. And some of the signals have been rerouted
    to create a more complicated network for storing more complex patterns of connections
    between long and short-term memory of the text. It’s this more complicated interaction
    between hidden and cell states that creates more "capacity" or memory and computation
    in one cell. Because an LSTM cell contains more nonlinear activation functions
    and weights it has more information processing capacity.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 计算 LSTM 逻辑门和输出的公式与计算 GRU 的公式非常相似。主要区别在于添加了另外 3 个函数，以计算所需的所有信号。一些信号已被重新路由以创建更复杂的网络，以存储短期和长期记忆之间的更复杂的连接模式。在隐藏状态和单元状态之间的这种更复杂的交互创造了更多的“容量”或内存和计算能力。因为
    LSTM 单元包含更多的非线性激活函数和权重，所以它具有更多的信息处理能力。
- en: '[PRE32]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 8.4.4 Give your RNN a tuneup
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.4 给你的 RNN 进行调谐
- en: As you learned in Chapter 7, hyperparameter tuning becomes more and more important
    as your neural networks get more and more complicated. Your intuitions about layers,
    network capacity and training time will get fuzzier and fuzzier as the models
    get complicated. RNNs are particularly intuitive. To jumpstart your intuition
    we’ve trained dozens of different basic RNNs with different combinations of hyperparameters
    such as the number of layers and number of hidden units in each layer. You can
    explore all the hyperparameters that you are curious about using the code in `nlpia2/ch08`.^([[31](#_footnotedef_31
    "View footnote.")])
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: It’s a really exciting thing to explore the hyperspace of options like this
    and discover surprising tricks for building accurate models. Surprisingly, for
    this RNN language model trained on a small subset of Wikipedia, you can get great
    results without maximizing the size and capacity of the model. You can achieve
    better accuracy with a 3-layer RNN than with a 5-layer RNN. You just need to start
    with an aggressive learning rate and keep the dropout to a minimum. And the fewer
    layers you have the faster the model will train.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-326
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Experiment often, and always document what things you tried and how well the
    model worked. This kind of hands-on work provides the quickest path toward an
    intuition that speeds up your model building and learning. Your lifelong goal
    is to train your mental model to predict which hyperparameter values will produce
    the best results in any given situation.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: If you feel the model is overfitting the training data but you can’t find a
    way to make your model simpler, you can always try increasing the `Dropout(percentage)`.
    This is a sledgehammer that reduces overfitting while allowing your model to have
    as much complexity as it needs to match the data. If you set the dropout percentage
    much above 50%, the model starts to have a difficult time learning. Your learning
    will slow and the validation error may bounce around a lot. But 20% to 50% is
    a pretty safe range for a lot of RNNs and most NLP problems.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: If you’re like Cole and me when we were getting started in NLP, you’re probably
    wondering what a "unit" is. All the previous deep learning models have used "neurons"
    as the fundamental unit of computation within a neural network. Researchers use
    the more general term "unit" to describe the elements of an LSTM or GRU that contain
    internal gates and logic. So when you are thinking about how many units to add
    to your neural network to solve a particular problem, each LSTM or GRU unit gives
    your network a capacity similar to two "normal" RNN neurons or hidden vector dimensions.
    A unit is just a more complicated, higher-capacity neuron, and you can see this
    if you count up the number of "learned parameters" in your LSTM model and compare
    it to those of an equivalent RNN.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: 8.5 Predicting
  id: totrans-330
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The word-based RNN language model you trained for this chapter used the `WikiText-2`
    corpus.^([[32](#_footnotedef_32 "View footnote.")]) The nice thing about working
    with this corpus is that it is often used by researchers to benchmark their language
    model accuracy. And the Wikipedia article text has already been tokenized for
    you. Also, the uninteresting sections such as the References at the end of the
    articles have been removed.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, the PyTorch version of the WikiText-2 includes "<unk>" tokens
    that randomly replace, or mask, 2.7% of the tokens. That means that your model
    will never get very high accuracy unless there is some predictable pattern that
    determines which tokens were masked with "<unk>". But if you download the original
    raw text without the masking tokens you can train your language model on it and
    get a quick boost in accuracy.^([[33](#_footnotedef_33 "View footnote.")]) And
    you can compare the accuracy of your LSTM and GRU models to those of the experts
    that use this benchmark data.^([[34](#_footnotedef_34 "View footnote.")])
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: Here is an example paragraph at the end of the masked training dataset `train.txt`.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: It seems that the last Wikipedia article in the WikiText-2 benchmark corpus
    is about the common starling (a small bird in Europe). And from the article, it
    seems that the starling appears to be good at mimicking human speech, just as
    your RNN can.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: What about those "<unk>" tokens? These are designed to test machine learning
    models. Language models are trained with the goal of predicting the words that
    were replaced with the "<unk>" (unknown) tokens. Because you have a pretty good
    English language model in your brain you can probably predict the tokens that
    have been masked out with all those "<unk>" tokens.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: But if the machine learning model you are training thinks these are normal English
    words, you may confuse it. The RNN you are training in this chapter is trying
    to discern the *meaning* of the meaningless "<unk>" token, and this will reduce
    its understanding of all other words in the corpus.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-339
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: If you want to avoid this additional source of error and confusion, you can
    try training your RNN on the unofficial raw text for the `wikitext-2` benchmark.
    There is a one-to-one correspondence between the tokens of the official wikitext-2
    corpus and the unofficial raw version in the nlpia2 repository. ^([[35](#_footnotedef_35
    "View footnote.")])
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: So how many "<eos>" and "<unk>" tokens are there in this training set?
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: So 2.6% of the tokens have been replaced with the meaningless "<unk>" token.
    And the "<eos>" token marks the newlines in the original text, which is typically
    the end of a paragraph in a Wikipedia article.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: So let’s see how well it does at writing new sentences similar to those in the
    WikiText-2 dataset, including the "<unk>" tokens. We’ll prompt the model to start
    writing with the word "The" to find out what’s on the top of its "mind".
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The first line in the training set is "= Valkyria Chronicles III =" and the
    last article in the training corpus is titled "= Common Starling =". So this GRU
    remembers how to generate text similar to text at the beginning and end of the
    text passages it has read. So it surely seems to have both long and short-term
    memory capabilities. This is exciting, considering we only trained a very simple
    model on a very small dataset. But this GRI doesn’t yet seem to have the capacity
    to store all of the English language patterns that it found in the two-million-token-long
    sequence. And it certainly isn’t going to do any sense-making any time soon.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-348
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Sense-making is the way people give meaning to the experiences that they share.
    When you try to explain to yourself why others are doing what they are doing,
    you are doing sense-making. And you don’t have to do it alone. A community can
    do it as a group through public conversation mediated by social media apps and
    even conversational virtual assistants. That’s why it’s often called "collective
    sense-making." Startups like DAOStack are experimenting with chatbots that bubble
    up the best ideas of a community and use them for building knowledge bases and
    making decisions. ^([[36](#_footnotedef_36 "View footnote.")])
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: You now know how to train a versatile NLP language model that you can use on
    word-level or character-level tokens. You can use these models to classify text
    or even generate modestly interesting new text. And you didn’t have to go crazy
    on expensive GPUs and servers.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: 8.6 Test yourself
  id: totrans-351
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What are some tricks to improve "retention" for reading long documents with
    an RNN?
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are some "unreasonably effective" applications for RNNs in the real world?
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How could you use a name classifier for good? What are some unethical uses of
    a name classifier?
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are some ethical and prosocial AI uses for a dataset with millions of username-password
    pairs such as Mark Burnett’s password dataset? ^([[37](#_footnotedef_37 "View
    footnote.")])
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train an rnn_word model on the raw text, unmasked text for the Wikitext-2 dataset
    the proportion of tokens that are "<unk>". Did this improve the accuracy of your
    word-level RNN language model?
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modify the dataset to label each name with a multi-hot tensor indicating all
    the nationalities for each name.^([[38](#_footnotedef_38 "View footnote.")]) ^([[39](#_footnotedef_39
    "View footnote.")]) How should you measure accuracy? Does your accuracy improve?
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 8.7 Summary
  id: totrans-358
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In natural language token sequences, an RNN can remember everything it has read
    up to that point, not just a limited window.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Splitting a natural language statement along the dimension of time (tokens)
    can help your machine deepen its understanding of natural language.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can backpropagate errors back in time (token) as well as in the layers of
    a deep learning network.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because RNNs are particularly deep neural nets, RNN gradients are particularly
    temperamental, and they may disappear or explode.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Efficiently modeling natural language character sequences wasn’t possible until
    recurrent neural nets were applied to the task.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weights in an RNN are adjusted in aggregate across time for a given sample.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use different methods to examine the output of recurrent neural nets.
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can model the natural language sequence in a document by passing the sequence
    of tokens through an RNN backward and forward in time simultaneously.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[1]](#_footnoteref_1) Mathematics forum StackExchange question about recurrence
    and recursion ( [https://math.stackexchange.com/questions/931035/recurrence-vs-recursive](931035.html))'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '[[2]](#_footnoteref_2) MIT Open Courseware lectures for CS 6.005 "Software
    Construction" ( [https://ocw.mit.edu/ans7870/6/6.005/s16/classes/10-recursion/](10-recursion.html))'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: '[[3]](#_footnoteref_3) Papers with Code query for RNN applications ( [https://proai.org/pwc-rnn](proai.org.html))'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: '[[4]](#_footnoteref_4) Daniel Miessler’s Unsupervised Learning podcast #340
    ( [https://mailchi.mp/danielmiessler/unsupervised-learning-no-2676196](danielmiessler.html))
    and the RNN source code ( [https://github.com/JetP1ane/Affinis](JetP1ane.html))'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '[[5]](#_footnoteref_5) Ryan Stout’s ( [https://github.com/ryanstout](github.com.html))
    BustAName app ( [https://bustaname.com/blog_posts](bustaname.com.html))'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: '[[6]](#_footnoteref_6) Garrett Lander, Al Kari, and Chris Thompson contributed
    to our project to unredact the Meuller report ( [https://proai.org/unredact](proai.org.html))'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: '[[7]](#_footnoteref_7) Rorsharch test Wikipedia article ( [https://en.wikipedia.org/wiki/Rorschach_test](wiki.html))'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: '[[8]](#_footnoteref_8) "The unreasonable effectiveness of RNNs" ( [https://karpathy.github.io/2015/05/21/rnn-effectiveness](21.html))'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: '[[9]](#_footnoteref_9) PyTorch `RNNBase` class source code ( [https://github.com/pytorch/pytorch/blob/75451d3c81c88eebc878fb03aa5fcb89328989d9/torch/nn/modules/rnn.py#L44](modules.html))'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: '[[10]](#_footnoteref_10) Portland Python User Group presentation on unredacting
    the Meuller Report ( [https://proai.org/unredact](proai.org.html))'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: '[[11]](#_footnoteref_11) Lex Fridman interview with ex-spy Andrew Bustamante
    ( [https://lexfridman.com/andrew-bustamante](lexfridman.com.html))'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: '[[12]](#_footnoteref_12) There’s more info and data scraping code in the nlpia2
    package ( [https://proai.org/nlpia-ch08-surnames](proai.org.html))'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: '[[13]](#_footnoteref_13) PyTorch RNN Tutorial by Sean Robertson ( [https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html](intermediate.html))'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: '[[14]](#_footnoteref_14) The original PyTorch RNN Tutorial surname dataset
    with duplicates ( [https://download.pytorch.org/tutorial/data.zip](tutorial.html))'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: '[[15]](#_footnoteref_15) iPython `history` log in the `nlpia2` repository on
    GitLab with examples for scraping surname data ( [https://proai.org/nlpia-ch08-surnames](proai.org.html))'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: '[[16]](#_footnoteref_16) PyTorch character-based RNN tutorial ( [https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html](intermediate.html))'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: '[[17]](#_footnoteref_17) Qary ( [https://docs.qary.ai](.html)) combines technology
    and data from all our multilingual chatbots ( [https://tangibleai.com/our-work](tangibleai.com.html))'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: '[[18]](#_footnoteref_18) AI algorithm registry launched in Amsterdam in 2020
    ( [https://algoritmeregister.amsterdam.nl/en/ai-register/](ai-register.html))'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: '[[19]](#_footnoteref_19) EU Artificial Intelligence Act website ( [https://artificialintelligenceact.eu/](artificialintelligenceact.eu.html))'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: '[[20]](#_footnoteref_20) The accepted ''OECD AI Council'' recommendations (
    [https://legalinstruments.oecd.org/en/instruments/OECD-LEGAL-0449](instruments.html)
    )'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: '[[21]](#_footnoteref_21) Thank you Tiffany Kho for pointing this out.'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '[[22]](#_footnoteref_22) footnote:["The unreasonable effectiveness of RNNs"
    by Andrej Karpathy ( [https://karpathy.github.io/2015/05/21/rnn-effectiveness](21.html))'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: '[[23]](#_footnoteref_23) ARMA model explanation ( [https://en.wikipedia.org/wiki/Autoregressive_model](wiki.html))'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: '[[24]](#_footnoteref_24) [https://arxiv.org/pdf/1803.03635.pdf](pdf.html)'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: '[[25]](#_footnoteref_25) "Attention Is All You Need" by Ashish Vaswani et al
    ( [https://arxiv.org/abs/1706.03762](abs.html))'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: '[[26]](#_footnoteref_26) [https://en.wikipedia.org/wiki/Hutter_Prize](wiki.html)'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: '[[27]](#_footnoteref_27) The full source code is in the nlpia2 package ( [https://gitlab.com/tangibleai/nlpia2/-/blob/main/src/nlpia2/ch08/rnn_word/data.py](rnn_word.html))'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: '[[28]](#_footnoteref_28) PyTorch docs for GRU layers ( [https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU](generated.html))'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: '[[29]](#_footnoteref_29) PyTorch docs discussion about counting up learned
    parameters ( [https://discuss.pytorch.org/t/how-do-i-check-the-number-of-parameters-of-a-model/4325/9](4325.html)
    )'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: '[[30]](#_footnoteref_30) Thank you Rian Dolphin for "LSTM Networks | A Detailed
    Explanation" ( [http://archive.today/8YD7k](archive.today.html)).'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: '[[31]](#_footnoteref_31) The `hypertune.py` script in the `ch08/rnn_word` module
    within the `nlpia2` Python package [https://gitlab.com/tangibleai/nlpia2/-/blob/main/src/nlpia2/ch08/rnn_word/hypertune.py](rnn_word.html)'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: '[[32]](#_footnoteref_32) PyTorch `torchtext` dataset ( [https://pytorch.org/text/0.8.1/datasets.html#wikitext-2](0.8.1.html))'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: '[[33]](#_footnoteref_33) Raw, unmasked text with "answers" for all the "unk"
    tokens ( [https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip](wikitext.html))'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: '[[34]](#_footnoteref_34) AI researchers( [https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/](the-wikitext-dependency-language-modeling-dataset.html))'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: '[[35]](#_footnoteref_35) `nlpia2` package with code and data for the rnn_word
    model code and datasets used in this chapter ( [https://gitlab.com/tangibleai/nlpia2/-/tree/main/src/nlpia2/ch08/rnn_word/data/wikitext-2](data.html))'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: '[[36]](#_footnoteref_36) DAOStack platform for decentralized governance ( [https://daostack.io/deck/DAOstack-Deck-ru.pdf](deck.html))'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: '[[37]](#_footnoteref_37) Alexander Fishkov’s analysis ( [https://www.city-data.com/blog/1424-passwords-on-the-internet-publicly-available-dataset/](1424-passwords-on-the-internet-publicly-available-dataset.html))
    of Mark Burnett’s ten million passwords ( [https://archive.is/cDki7](archive.is.html))
    - torrent magnet link at the bottom of the article.'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: '[[38]](#_footnoteref_38) PyTorch community multi-label (tagging) data format
    example ( [https://discuss.pytorch.org/t/multi-label-classification-in-pytorch/905/45](905.html))'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: '[[39]](#_footnoteref_39) Example `torchtext`` Dataset class multi-label text
    classification ( [https://discuss.pytorch.org/t/how-to-do-multi-label-classification-with-torchtext/11571/3](11571.html))'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
