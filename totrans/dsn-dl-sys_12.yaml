- en: Appendix B. Survey of existing solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning systems are huge undertakings when implemented from scratch. In
    some situations, special requirements may warrant spending the extra effort to
    build a deep learning system from scratch. In other cases, given finite resources
    and time, it may make sense to use existing components, or even systems as a whole,
    and tailor them to your own needs.
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of this appendix is to examine a few deep learning systems that
    have been implemented by different cloud vendors and open source communities.
    These operations range from serverless deployment to custom service container
    deployment. You will gain a sense of which pieces of these operations you can
    use to design your own project by comparing them with our reference architecture
    and highlighting their similarities and differences.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to see a quick summarized comparison of every solution that we will
    cover, feel free to skip ahead to section B.5\. Also, for your convenience, the
    reference architecture introduced in section 1.2.1 is reposted in figure B.1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/B-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure B.1 An overview of a typical deep learning system that includes basic
    components to support a deep learning development cycle. This reference architecture
    can be used as a starting point and further tailored.
  prefs: []
  type: TYPE_NORMAL
- en: B.1 Amazon SageMaker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Amazon SageMaker is the umbrella term for its collection of artificial intelligence
    products that can be used together to form a complete deep learning system. In
    this section, we will review the suite of products and see how they compare with
    our key components. As mentioned at the beginning of this section, we make these
    comparisons so that you will learn what product will best help to build your own
    system.
  prefs: []
  type: TYPE_NORMAL
- en: B.1.1 Dataset management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Amazon SageMaker does not offer a dataset management component that provides
    a unified interface to help manage the complex interaction of data preparation
    with the different types of users of a deep learning system. Amazon, however,
    does provide a collection of data storage, transformation, and querying solutions
    that can be used to build a data management component.
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to build a data management component that collects raw data for
    Amazon S3, an object storage product. Metadata tagging can be backed by AWS Glue
    Data Catalog, which can be used by AWS Glue ETL for further processing into datasets
    that can be used for training. After reading chapter 2, you should be able to
    identify how you can use these Amazon products to build your own data management
    component.
  prefs: []
  type: TYPE_NORMAL
- en: B.1.2 Model training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Amazon SageMaker supports both built-in algorithms and externally provided custom
    code for training deep learning models. It also supports containers for training
    runs. It exposes an API that can be called to launch training jobs on demand.
    This is largely similar to the compute backend that powers the training component
    of a deep learning system, which is covered in this book. To implement the resource
    management portion of the training component, you may use the existing tools provided
    by Amazon, such as assigning resource limits and policies to different AWS Identity
    and Access Management (IAM) users or roles. If your organization requires extra
    control or sophistication or already has an identity provider implementation,
    you may need to spend more time building a custom solution. After reading chapters
    3 and 4, you should be able to figure out how you can build your own training
    component with existing Amazon tools.
  prefs: []
  type: TYPE_NORMAL
- en: B.1.3 Model serving
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Amazon SageMaker, in its most basic form, supports deploying trained models
    as web services that are accessible over the internet. For scaling out to host
    multiple models without deploying each of them to separate endpoints, SageMaker
    provides a multimodel endpoint that also comes with configurable model-caching
    behavior. These tools can come in handy if they fit your bill. As of the time
    of this writing, SageMaker supports multi-container endpoints and serial inference
    pipelines, which are similar to the serving architecture and DAG support described
    in this book. Chapters 6 and 7 review model-serving principles so that you will
    understand what existing tools you can use and how you can build your own when
    you encounter limitations with existing tools.
  prefs: []
  type: TYPE_NORMAL
- en: B.1.4 Metadata and artifacts store
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As the component that centers around trained models, it is not surprising to
    see cloud vendors make products that do just that. The SageMaker Model Registry
    provides functionalities that map to many key concepts of the metadata and artifacts
    store of a deep learning system. For example, metadata, such as training metrics
    of a model and model versions, can be tracked using Model Registry. It does not,
    however, provide a storage solution for artifacts in the same component. You can
    easily build an interface on top of Model Registry and other Amazon storage products
    to provide the artifact storage aspect of this component.
  prefs: []
  type: TYPE_NORMAL
- en: Another important type of metadata that is tracked between artifacts is their
    lineage information. SageMaker provides ML Lineage Tracking as a separate feature
    that keeps tabs on this information automatically.
  prefs: []
  type: TYPE_NORMAL
- en: In chapter 8, we will discuss key concerns in building the metadata and artifacts
    store. After reading the chapter, you will understand the design principles behind
    this component and how existing products can help you to build your own quickly.
  prefs: []
  type: TYPE_NORMAL
- en: B.1.5 Workflow orchestration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: On Amazon SageMaker, you can use the Model Building Pipelines product to manage
    your workflows, or *pipelines* (which is SageMaker terminology). Using this product,
    you can execute a set of actions, such as data preparation steps, training steps,
    and model validation steps, as one unit with a predefined order in an arbitrary
    fashion. To allow multiple types of users to work on the same problem, SageMaker
    also provides a Project product to help organize relationships between workflows,
    code versions, lineage information, and different access permissions for each
    user type.
  prefs: []
  type: TYPE_NORMAL
- en: In chapter 9, we review how to use a workflow manager to enable different modes
    of training. After reading the chapter, you will understand the reasoning behind
    the design and utility of a workflow manager in a deep learning system, as well
    as its role in an enterprise environment.
  prefs: []
  type: TYPE_NORMAL
- en: B.1.6 Experimentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Amazon SageMaker provides a feature called Experiments that tags experimental
    runs with relevant tracking information and metrics. Indeed, this type of tracking
    information is also a kind of metadata, which is important to users of a deep
    learning system who need to evaluate the performance of different combinations
    of data input, training algorithms, and hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: B.2 Google Vertex AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Google Vertex AI, a combination of Googleâ€™s AI platform offering and its AutoML
    product, provides a collection of tools and services that can be used as a deep
    learning system. In this section, we will review its offerings and compare them
    with key components introduced in this book.
  prefs: []
  type: TYPE_NORMAL
- en: B.2.1 Dataset management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Google Vertex AI provides a simple API to manage datasets, though you must first
    upload your object data to Google Cloud Storage and then upload metadata and annotation
    files that reference object data in Google Cloud Storage via the Vertex AI API.
    The dataset API is similar across different types of datasets (images, text, video,
    etc.) that provide a unified experience to developers. The API, however, does
    not provide versioning information and other lineage tracking information. In
    chapter 2, we explore core data management principles. After reading the chapter,
    you will be able to compare existing solutions and extend them or build from scratch
    for your own needs.
  prefs: []
  type: TYPE_NORMAL
- en: B.2.2 Model training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Google Vertex AI supports training with Docker containers. It provides prebuilt
    training containers for those who do not need further customization and also supports
    custom-built training containers for those who require more than what the prebuilt
    flavor provides. Its training service exposes an interface that allows the launching
    of training runs on either a single node or on multiple nodes for distributed
    training. When running distributed training, Vertex AI provides additional support
    with reduction to accelerate training. In chapters 3 and 4, we explore these features
    and the principles behind them. After reading these chapters, you will be able
    to determine what existing offerings you can use, how to extend them if you need
    more, and how to build it from scratch if you have more specific requirements.
  prefs: []
  type: TYPE_NORMAL
- en: B.2.3 Model serving
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Google Vertex AI supports serving online inference requests to trained models,
    either with prebuilt inference containers or custom inference containers. Trained
    models are decoupled from containers and must be deployed with compute resources
    to form an endpoint that can serve online inference requests. Vertex AI supports
    deploying one model to multiple endpoints and supports deploying multiple models
    to a single endpoint. Different from other solutions that support various model
    types, deploying multiple models to a single endpoint in Vertex AI is primarily
    used for canarying new model versions using split traffic patterns. In Vertex
    AI, if you train a Vertex AI video model, it cannot be made to serve online inference
    requests.
  prefs: []
  type: TYPE_NORMAL
- en: In chapters 6 and 7, we learn the fundamental principles behind model serving.
    After completing these chapters, you will have a good understanding of model serving
    and will be able to decide whether existing solutions are sufficient for your
    needs. You will be able to build your own, as well as understand how to operate
    a model server efficiently and at scale.
  prefs: []
  type: TYPE_NORMAL
- en: B.2.4 Metadata and artifacts store
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Vertex ML Metadata is Googleâ€™s metadata store solution that can be used in a
    deep learning system. It uses a graph to describe relationships between artifacts
    such as datasets, training runs, and trained models. Each node and edge in the
    graph can be tagged with a list of key-value pairs to describe any metadata. When
    used properly, this can provide comprehensive lineage information for everything
    in a deep learning system.
  prefs: []
  type: TYPE_NORMAL
- en: Artifacts are not stored directly in Vertex ML Metadata. Artifacts are stored
    in Google Cloud Storage. Vertex ML Metadata uses a URI reference to point to these
    artifacts.
  prefs: []
  type: TYPE_NORMAL
- en: In chapter 8, we will explore a similar approach in building a metadata and
    artifacts store, where both can be managed through a single, unified interface.
    After reading the chapter, you will be able to tell how to leverage and extend
    existing solutions for your needs.
  prefs: []
  type: TYPE_NORMAL
- en: B.2.5 Workflow orchestration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With Google, you can use Vertex Pipelines to manage and operate your deep learning
    workflows. You can represent data preparation and training operations as steps
    in a pipeline. In Vertex Pipelines, steps are organized as nodes in a directed
    acyclic graph. Each step of the pipeline is implemented by a container. A run
    of a pipeline is in fact an orchestration of executions of containers.
  prefs: []
  type: TYPE_NORMAL
- en: In chapter 9, we review how to use a workflow manager to enable different modes
    of training. After reading the chapter, you will understand the reasoning behind
    the design and utility of a workflow manager in a deep learning system and its
    role in an enterprise environment.
  prefs: []
  type: TYPE_NORMAL
- en: B.2.6 Experimentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Google Vertex AI Experiments provides a unified UI to create, track, and manage
    experiments. The Vertex AI SDK provides autologging support for model training
    code to record hyperparameters, metrics, and data lineage. When paired with Vertex
    ML Metadata, you can get a complete overview of all your model training experiment
    runs.
  prefs: []
  type: TYPE_NORMAL
- en: B.3 Microsoft Azure Machine Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Different from the classic ML Studio offering from Microsoft that focuses on
    a GUI approach to machine learning, Azure Machine Learning is a new suite of tools
    and services that also supports a wide variety of customization using code and
    established open source frameworks. In this section, we will compare their offerings
    to key components that are described in this book. After completing this section,
    you will gain a sense of what you can use as is, what you can extend, and what
    you need to build from scratch to fulfill your requirements.
  prefs: []
  type: TYPE_NORMAL
- en: B.3.1 Dataset management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: On Azure Machine Learning, datasets are first-class objects that are inputs
    and outputs of data processing and training tasks. Datasets are defined as a collection
    of metadata associated with the raw data of the dataset. The dataset references
    its raw data via a URI reference to underlying data storage. Once a dataset is
    created, it becomes immutable. The underlying data, however, does not have the
    same guarantee, and it is up to you to manage its immutability.
  prefs: []
  type: TYPE_NORMAL
- en: Once datasets are defined, data processing and training codes can access them
    through a unified client API. Data can either be downloaded for local access or
    mounted as network storage for direct access. After reading chapter 2, you will
    be able to identify similarities between this paradigm and the one that is described
    in the book. You will learn how you can use this existing product as is and how
    you can extend it for your own needs.
  prefs: []
  type: TYPE_NORMAL
- en: B.3.2 Model training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Azure Machine Learning supplies prebuilt containers with Python distributions
    and allows users to define a custom-base image that conforms to specific requirements.
    As of this writing, only Python is supported for defining custom training code.
    To launch a training run, you need to specify a runtime container and a reference
    to training code that conforms to a certain convention. If you need something
    other than this setup, you will need to build your own training service. Chapters
    3 and 4 will show you the key principles of a training service and an example
    that you can use as a starting point for your own training service.
  prefs: []
  type: TYPE_NORMAL
- en: B.3.3 Model serving
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: On Azure Machine Learning v2, an endpoint can be created to serve online inference
    requests. The endpoint can either be configured to load a certain model and use
    a Python script to produce inferences or be configured to use a completely custom
    container imageâ€”such as TensorFlow Servingâ€”to produce inferences. Azure Machine
    Learning also integrates with NVIDIA Triton Inference Server, which provides additional
    performance gain when GPU is used to produce inferences.
  prefs: []
  type: TYPE_NORMAL
- en: If you need to deploy multiple models to a single endpoint or manage models
    and inference production on edge devices, you will need to build your own. In
    chapters 6 and 7, we discuss model serving in depth. After completing these chapters,
    you will be able to build your own model server should you require additional
    features that existing offerings do not support.
  prefs: []
  type: TYPE_NORMAL
- en: B.3.4 Metadata and artifacts store
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In Azure Machine Learning, metadata can be tagged along many objects, such as
    models, training runs, etc., in the form of tags. While not a standalone product,
    the model registration capability supports additional metadata when registering
    a model. The interface receives the metadata as well as the model file (artifact)
    at the same time during registration, taking one less step when compared to other
    solutions that require models to be registered to already exist in their cloud
    storage.
  prefs: []
  type: TYPE_NORMAL
- en: As of this writing, a preview feature called registry can be used to centralize
    ML related metadata to one place. If you want to track lineage between different
    artifacts, though you may need to build your own solution.
  prefs: []
  type: TYPE_NORMAL
- en: After reading chapter 8, you will gain an in-depth understanding of the metadata
    and artifacts store. You will learn its fundamentals and will be able to quickly
    build one yourself.
  prefs: []
  type: TYPE_NORMAL
- en: B.3.5 Workflow orchestration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Azure Machine Learning provides a feature called *ML pipelines* that allows
    you to define data, training, and other tasks as steps. These steps are put together
    programmatically to form a pipeline that can be executed periodically based on
    a schedule or trigger or be launched once manually. Compute resources, execution
    environment, and access permissions can be configured programmatically when the
    pipeline is defined.
  prefs: []
  type: TYPE_NORMAL
- en: In chapter 9, we review how to use a workflow manager to enable different modes
    of training. After reading the chapter, you will understand the reasoning behind
    the design and utility of a workflow manager in a deep learning system and its
    role in an enterprise environment.
  prefs: []
  type: TYPE_NORMAL
- en: B.3.6 Experimentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Azure Machine Learning provides a feature for defining and tracking experiments.
    When model training is being performed as part of an experiment, metrics can be
    logged from the training code and visualized through the web interface. It also
    supports arbitrary tagging and parent-child relationships between experiment runs
    for a hierarchical organization and lookup.
  prefs: []
  type: TYPE_NORMAL
- en: B.4 Kubeflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubeflow is an open source suite of tools that provides many useful components
    for building a deep learning system without being locked into a particular cloud
    vendor. In this section, we walk through the list of key components that are introduced
    in this book and compare them with similar components provided by Kubeflow.
  prefs: []
  type: TYPE_NORMAL
- en: B.4.1 Dataset management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kubeflowâ€™s vision is to not reinvent any existing tools, so it should not be
    a surprise that it does not come with a data management component, as other open
    source solutions exist. In chapter 2, we review some open source data management
    solutions and explore how they can be further extended to implement key principles
    described in that chapter.
  prefs: []
  type: TYPE_NORMAL
- en: B.4.2 Model training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kubeflow, being a suite of tools that is based on Kubernetes, has the luxury
    of being backed by a sophisticated resource scheduler. Unlike cloud vendors that
    provide prebuilt model training containers, you must build your own and manage
    their launches. In chapters 3 and 4, we talk about the principles of a training
    service and how it helps to abstract the complexity in resource assignment and
    scheduling. We go over a reference training service, and you learn how to build
    one yourself for your requirements.
  prefs: []
  type: TYPE_NORMAL
- en: B.4.3 Model serving
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As of this writing, Kubeflow provides a KServe component that can be used to
    deploy trained models as an inference service, which serves inference requests
    over the network. It is an interface that sits on top of existing serving frameworks
    such as TensorFlow Serving, PyTorch TorchServe, and NVIDIA Triton Inference Server.
    The main benefit of using KServe is the additional abstraction of operational
    complexity such as autoscaling, health checks, and auto-recovery. Because it is
    an open source solution, it is possible to host either one model or multiple models
    with the same endpoint. In chapters 6 and 7, we will go through model serving
    principles so that you will understand the reason behind the design of popular
    serving interfaces and how you can customize them to fit your own needs.
  prefs: []
  type: TYPE_NORMAL
- en: B.4.4 Metadata and artifacts store
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Starting with Kubeflow version 1.3, metadata and artifacts become an integral
    part of Kubeflow Pipelines. Kubeflow Pipelines consist of a graph of pipeline
    components. Between each component, parameters and artifacts can be passed along.
    Similar to the description in this book, artifacts encapsulate any kind of data
    that is the side effect of a deep learning system, such as the model itself, training
    metrics, and data distribution metrics. Metadata is any data that describes pipeline
    components and artifacts. With these constructs, you can deduce the lineage between
    input training datasets, trained models, experiment results, and served inferences.
  prefs: []
  type: TYPE_NORMAL
- en: In chapter 8, we discuss key concerns for building the metadata and artifacts
    store. After reading the chapter, you will understand the design principles behind
    this component and how existing products can help you build your own quickly.
  prefs: []
  type: TYPE_NORMAL
- en: B.4.5 Workflow orchestration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Also described in the previous section, Kubeflow Pipelines can be used to help
    manage deep learning data preparation and training workflows. Metadata and versioning
    are built into pipelines, and native users and access permissions from Kubernetes
    can be used to restrict access.
  prefs: []
  type: TYPE_NORMAL
- en: In chapter 9, we review how a workflow manager enables different modes of training.
    After reading the chapter, you will understand the reasoning behind the design
    and utility of a workflow manager in a deep learning system.
  prefs: []
  type: TYPE_NORMAL
- en: B.4.6 Experimentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kubeflow Pipelines provides the Experiment construct in which multiple training
    runs can be organized into a logical group, where it provides additional visualization
    tools for differences between each experimental run. This fits well with offline
    experimentation. If you need to perform online experimentation, you will need
    to roll your own solution.
  prefs: []
  type: TYPE_NORMAL
- en: B.5 Side-by-side comparison
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We think it will be handy to provide a summarized overview table of every solution
    grouped by components that we have covered previously. We hope that table B.1
    will make it easier for you to pick the right solution.
  prefs: []
  type: TYPE_NORMAL
- en: Table B.1 Side-by-side comparison
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Amazon SageMaker | Google Vertex AI | Microsoft Azure Machine Learning
    | Kubeflow |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Comparing dataset management solutions | AWS components, such as S3, Glue
    Data Catalog, and Glue ETL, can be used to build a dataset management component.
    | APIs for managing datasets are ready to use. Data content upload and metadata
    tagging are separate operations. | Datasets are first-class objects and are immutable
    once created. A unified client API is provided for training jobs to access training
    datasets. | Does not provide a dataset management solution. Other open source
    alternatives are readily available. |'
  prefs: []
  type: TYPE_TB
- en: '| Comparing model training solutions | Supports built-in algorithms, externally
    provided custom code, and custom containers for training. Exposes an API for launching
    training jobs on demand. | Provides prebuilt training containers that can be used
    as is. Supports custom training containers. Exposes an API that supports launching
    training containers on multiple nodes. | Provides prebuilt training containers
    with Python that can be customized. Training containers must conform to a certain
    convention. | Has native access to Kubernetes scheduling capabilities. No prebuilt
    training containers are provided. |'
  prefs: []
  type: TYPE_TB
- en: '| Comparing model serving solutions | Models can be deployed as web endpoints.
    Multiple models can be deployed to the same endpoint for better utilization, with
    some limitations when GPUs are used. Configurable model caching behavior. | Models
    and inference containers are decoupled. They must be deployed together to form
    a web endpoint for serving. Custom inference containers are supported. Multiple
    models per endpoint are primarily used for canarying new versions of models. Video
    models are not supported. | Endpoints can be deployed to serve models over the
    web. Endpoints are configured to use a particular model with a custom Python script
    for producing inferences. NVIDIA Triton Inference Server integration is available.
    | KServe is the Kubeflow component that serves models. It provides a serverless
    inferencing abstraction on top of popular serving frameworks such as TensorFlow
    Serving, PyTorch TorchServe, and NVIDIA Triton Inference Server. |'
  prefs: []
  type: TYPE_TB
- en: '| Comparing metadata and artifacts store solutions | SageMaker Model Registry
    provides a central metadata store solution. Artifacts are stored separately in
    Amazonâ€™s object store. | Vertex ML Metadata provides a central metadata store
    solution. Metadata is stored as graphs that can describe complex relationships.
    Artifacts are stored in Googleâ€™s object store. | A preview feature called registry
    can be used to centralize ML metadata. Metadata exists as tags of different objects
    (training runs, models, etc.), and objects can be artifacts. Lineage information
    can be deduced using these object tags. | Does not have a central repository of
    metadata or artifacts. Metadata and artifacts are integral parts of Kubeflow Pipelines.
    Each stage in the pipeline can be annotated with metadata and produce artifacts
    that can be tracked. Lineage information can be deduced from this information
    that can be retrieved from the Pipelines API. |'
  prefs: []
  type: TYPE_TB
- en: '| Comparing workflow orchestration solutions | Model Building Pipelines can
    be used to build and manage deep learning workflows. | Vertex ML Metadata provides
    a central metadata store solution. Metadata is stored as graphs that can describe
    complex relationships. Artifacts are stored in Googleâ€™s object store. | A preview
    feature called registry can be used to centralize ML metadata. Metadata exists
    as tags of different objects (training runs, models, etc.), and objects can be
    artifacts. Lineage information can be deduced using these object tags. | Does
    not have a central repository of metadata or artifacts. Metadata and artifacts
    are integral parts of Kubeflow Pipelines. Each stage in the pipeline can be annotated
    with metadata and produce artifacts that can be tracked. Lineage information can
    be deduced from the details that can be retrieved from the Pipelines API. |'
  prefs: []
  type: TYPE_TB
- en: '| Comparing experimentation solutions | The Experiments feature provides grouping
    and tracking for training runs. | Provides Vertex AI Experiments for tracking
    and visualizing experiment setups and run results. | Provides features for defining
    and tracking experiments. Experiments can be associated with a parent-child relationship.
    The web interface supports visualization. | Provides an Experiment construct for
    logical grouping of Kubeflow Pipelines that belong to the same experiment group.
    Visualization tools are provided to highlight differences between each pipeline
    run in the same experiment. |'
  prefs: []
  type: TYPE_TB
