- en: 5 Managing Data with GitHub Copilot and Copilot Chat
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Persisting our data into a relational database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming our data using Apache Kafka
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incorporating event-driven principles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing our data to monitor the location using Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The last chapter laid the foundation for our Information Technology Asset Management
    system. However, this application will not fulfill our requirements without data.
    Data is the life’s blood of every application. That is what this chapter is all
    about: the various ways that we can use Generative AIs to create data, stream
    data, transform data, react to data, and learn from data.'
  prefs: []
  type: TYPE_NORMAL
- en: Perceptive individuals might have noticed in the last chapter that our data
    access pattern would not have worked as it was incomplete. The opening section
    of this chapter will address this. After that, we will set up our database, fix
    the classes which access this data, and load some sample data for us to use in
    the rest of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Amassing our data set
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our first task will be to construct a substantial corpus of data to assist
    our experimentation in the remainder of the chapter. First, we will use GitHub
    Copilot to generate one thousand rows of asset information. We will soon find,
    however, that this may not be the tool most suited to this task. One key driver
    behind using these tools is the idea of discovery: Testing their boundaries, pushing
    against them, and occasionally, pushing back. But the journey is often where the
    joy is found. Once we have found this edge, we will be introduced to a new, previously
    unseen tool: GitHub Copilot Chat. Finally, once we have created our list of assets,
    we will add location information for those assets, again using GitHub Copilot
    Chat.'
  prefs: []
  type: TYPE_NORMAL
- en: We need to get our database running before building our initial dataset. Docker
    makes this task trivial, allowing us to quickly spin up an empty Postgres (or
    other RDBMS/NoSQL server) with minimal effort. Have you forgotten the command
    to do this? No worries, we can ask Copilot. Open a new file called data/initial_data_laod.sql.
    and enter the following prompt at the top of your newly minted SQL file.
  prefs: []
  type: TYPE_NORMAL
- en: Snippet 5.1 A prompt to have GitHub Copilot provide us with a Docker command
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Copilot will slowly reveal the Docker command: `--Answer: docker run --name
    itam_db -e POSTGRES_PASSWORD=postgres -d -p 5432:5432 postgres.` Once you run
    this command at your terminal or command line, we can build out our dataset. You
    should be able to connect to the locally running database. You should notice that
    there is a database called itam_db running in it. However, this database has no
    schema, tables, or data. Let’s first set up a new schema.'
  prefs: []
  type: TYPE_NORMAL
- en: In our initial_data_file.sql, we will add a prompt to have Copilot draft the
    schema creation command. The following prompt (and response from Copilot) will
    allow you to create a new schema called itam if executed from within your database
    client application (e.g., DataGrip, SQuirreL, pdAdmin, or even using the Docker
    exec command `docker exec -i itam_db psql -U postgres -c "create schema itam"`)
  prefs: []
  type: TYPE_NORMAL
- en: Snippet 5.2 A prompt to have Copilot create a new schema
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Next, we should add a user for use within our application. This user will be
    able to perform *CRUD* (Create, Read, Update, Delete) operations on our data but
    will not be able to affect the structure of the database tables or procedures.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The lines that start with double dashes are comments in SQL. Commenting out
    these lines are optional from Copilot’s perspective, as it will generate solutions
    without the comments; it makes it easier to copy and paste the code directly into
    our database tool of choice.
  prefs: []
  type: TYPE_NORMAL
- en: While we are at it, we will also add an administrative account to perform the
    operations that our read-write users cannot, such as creating or dropping tables.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.1 A prompt to create new users
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will grant ownership of this schema to the itam_admin account. Transfering
    this ownership will ensure that only this account can change the table structure:
    the data definition.'
  prefs: []
  type: TYPE_NORMAL
- en: Snippet 5.3 A prompt to transfer schema ownership to the admin account
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'With the set-up, account creation, and worship of the system out of the way,
    we can start to focus on the data. We will begin by adding the reference data,
    the data that supports the assets: the depreciation strategies. This data is more
    static in nature; it changes less frequently, if at all. Next, we will define
    and store these strategies.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.2 A prompt to create the depreciation_strategy table
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We will use a sequence as this table's primary key. While this would not be
    strictly necessary for a table that will not be very large and with known values
    that we could and will manually enter, adding this sequence will allow us to work
    with Copilot more and have it make some suggestions. Moreover, it is amusing to
    ask Copilot questions and have Copilot answer within a text file.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.3 A prompt to create a sequence for use as the primary key of the
    depreciation_strategy table
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Naturally, with the sequence in our proverbial hand, we need to know how to
    associate the sequence with the primary key column of the `depreciation_stategy`
    table. Luckily, Copilot has the answer.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.4 Asking Copilot how to associate the sequence with the primary key
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we shall complete this table by inserting the following static entries
    into the table. We will only use two depreciation strategies for now: straight-line
    and double declining balance.'
  prefs: []
  type: TYPE_NORMAL
- en: Lising 5.5 Adding the static entries to the depreciation_strategy table
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Next, we shall move on the funding_details table. This information tells us
    how we financed our equipment, the resale value, and instructions for what should
    be done with our asset once its useful life is over. The sequence of steps will
    be identical for what we did for the depreciation strategies, with the exception
    that we will not be added static entries, as this data is directly related to
    an individual asset. We will define the table, create the sequence, and apply
    said sequence to the table, functioning as the primary key.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.6 Complete code listing for the funding_details table
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The final information that we will define and generate are the assets themselves.
    This listing, too, is redundant but included for completeness. Finally, we create
    the table, make the sequence, and use it as the primary key.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.7 Complete code listing for the assets table
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: With the tables defined and created, we will now focus on creating the data.
    In our text file, we instruct Copilot with parameters for the dataset we are looking
    for. Copilot will likely attempt to assist you in outlining the attributes surrounding
    your new dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.8 Creating a dataset for the assets table
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The solution that Copilot provides is novel. It builds a large series using
    a Postgres built-in function, meaning that this would not be a portable solution.
    However, given that this is the database we will use, this is an appropriate enough
    solution. The resultant dataset is refined. We would have gotten better results
    if we had used Python and asked for Copilot’s assistance in coding a script to
    generate a file to load into Postgres. However, given, that this dataset is only
    for playing out with the application, we do not need to be overly concerned with
    the data quality for now. Although, in the real world, data quality is everything.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 5.9 Copilot’s response: An insert statement built off of a series'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: If we switch back to ChatGPT for just minute, we can get a second opinion as
    to how to create such a dataset. ChatGPT suggests the Python library `faker`.
    The `faker` package is used to generate fake data, like common English first names.
    `numpy` is used to generate the random float values for cost, useful life, and
    salvage value. `pandas` is used to manage the data in a `DataFrame` (the table).
    Additionally, we could save the `DataFrame` to a CSV file, using the method `df.to_csv('assets.csv',
    index=False)`.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.10 ChatGPT suggests Faker to generate the fake dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'For each of these assets, we will need funding details as well; how they were
    financed (purchased in this case), and the depreciation details. Unsurprisingly,
    we get a similar solution from Copilot: generate a series of entries using a similar
    prompt as what we used for the assets. We will need to ensure that for each of
    the asset identifiers (1-1000), we have a corresponding funding details entry.
    Otherwise, we would risk getting null pointers when running out code.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.11 Creating a dataset for the `funding_details` table
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: With the dataset generated and stored in the database, we should be able to
    wire up the remainder of our application to store and display assets using the
    REST APIs. However, since we had previously stripped out all of the metadata for
    SQLAlchemy during our build phase (see previous chapter), we need a way to wire
    this metadata with our adapters differently.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this, we have reached the edge of Copilot’s capabilities. We are perplexed
    by what comes next; how we can solve our most recent dilemma. Tempting as it is,
    we cannot give up and go home. Therefore, it is time to introduce the most recent
    addition to the Copilot product suite: Copilot Chat. Copilot Chat is an embedded
    GPT-4 model in your IDE (only currently supported by Visual Studio Code). We shall
    open the chat dialog and ask how to keep our business model clean but still use
    SQLAlchemy’s ORM (Object Relational Model) features.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.1 GitHub Copilot Chat’s solution for how to solve our most recent quandary
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![A screenshot of a computer Description automatically generated with low confidence](images/05image002.png)'
  prefs: []
  type: TYPE_IMG
- en: Copilot Chat suggests that we create a separate Data Access Layer. This approach
    maps nicely onto the Ports and Adapters approach we have used thus far. In addition,
    Copilot Chat recommended modeling these classes similarly to the domain classes
    but including the metadata required for ORM functionality to work correctly. The
    resultant code is in Listing 5.12.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.12 ORM support outside of the domain classes.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Now that the external model classes have been created, we must map these ORM
    instances to our domain model before returning them to the system's core. While
    this might seem like over-engineered code for such a simple application, this
    gives us great flexibility in how our domain model can operate. For example, our
    model can perform complex operations beyond just CRUD. We would be limited to
    these operations if we kept our domain model identity to the model used in the
    Data Access Layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will use Copilot and Copilot Chat to explore incorporating event-driven
    ideas into our application. Event-driven concepts will allow us to track our IT
    assets in real-time: their location, status, and market value, for example.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Monitoring our assets in real time with Kafka
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will monitor our assets in real-time to motivate our exploration of using
    Generative AIs in conjunction with event-driven architecture. We shall take it
    as a given that some system external to the Information Security Asset Management
    system fires events as our assets move from one location to another.
  prefs: []
  type: TYPE_NORMAL
- en: To delve into ITAM events, we will need to configure a few additional services.
    In this case, we will use Apache Kafka. Apache Kafka is a distributed streaming
    platform that is used for building real-time data pipelines and streaming apps.
    It's designed to handle data streams from multiple sources and deliver them to
    multiple consumers, effectively acting as a middleman for our real-time data.
  prefs: []
  type: TYPE_NORMAL
- en: To start we will ask Copilot Chat how to run Kafka locally, using Docker. Apache
    Kafka has an undeserved reputation for being difficult to install and configure.
    Running in Docker will allow us to side-step is this controversy. Using Copilot
    Chat we can produce a docker compose file. However, as is often the case, the
    versions are very old to the point of not supporting some hardware. Listing 5.13
    is an updated listing from Confluent’s (the company that offers commercial support
    for Kafka) official GitHub repository. Notice that the docker-compose file's content
    includes both Kafka and Zookeeper. Zookeeper is a distributed coordination service
    that Kafka uses to manage and coordinate the brokers within the cluster, at least
    for now. Future versions aim to remove dependency on Zookeeper.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.13 Docker-Compose file to launch Kafka with Zookeeper
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: With our new Kafka instance running, we now need a consumer that will pull the
    updated locations off the topic `asset_location` (which we share create shortly)
    and update the inventory in the database. Again, we can ask Copilot Chat to provide
    us with a suggestion as to how to do this.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.14 Prompt to Copilot Chat asking how to use Python to subscribe to
    the topic
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The code that Copilot Chat generates creates a consumer, listens to the topic,
    and uses a reference to the `AssetManager` class to update the location of the
    affected asset.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.15 Copilot Chat code to monitor assets’ locations
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'However, should the adapter be aware of the `AssetManager`? While the `AssetManager`
    does not directly sit inside the domain model, it is a critical intermediary.
    This coupling level between the `AssetManager` and the adapter could arguably
    violate the hexagon in the hexagonal architecture model. Let’s have Copilot Chat
    weigh in. We can ask it: “Given that this project uses hexagonal architecture,
    is it a good idea to have an adapter aware of the `AssetManger`?”'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.2 GitHub Copilot Chat’s acceptance that it is, in fact, imperfect
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![A screenshot of a computer Description automatically generated with medium
    confidence](images/05image003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It would appear to be the case that Copilot Chat agrees that this would indeed
    be considered a breach of the proper contraction responsibility of an adapter.
    We could add a new port to our `AssetManager` class that could use the Kakfa port.
    However, let’s see if Copilot Chat has any other suggestions: “We do not want
    the Kafka consumer to interact directly with the `AssetManager`. Are there any
    ways to accomplish this?” Copilot Chat suggests that we apply the `Mediator` pattern,
    which resembles a port and adapter combination.'
  prefs: []
  type: TYPE_NORMAL
- en: Mediator Pattern
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The Mediator design pattern is a behavioral pattern that promotes loose coupling
    between objects by encapsulating their interactions within a mediator object.
    The mediator object acts as a central hub that facilitates communication between
    objects without the objects having to know about each other.
  prefs: []
  type: TYPE_NORMAL
- en: Let us first take a look at the Mediator class that Copilot Chat generated.
    The class would sit between the `AssetManager` and the Kafka consumer.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.16 The Mediator class that will mediate the interaction between the
    AssetManager and the Kafka consumer
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This implementation closely resembles a message queue or message publisher,
    which is precisely the point: decoupling.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Eagle-eyed readers would likely notice that we are playing a little fast and
    loose with the distinction between an Asset and an asset of type Hardware. In
    the original domain model, only Hardware had a location. Generally, one does not
    think of Software as having a location. Of course, you could say that the software
    is installed in its location, but it is arguably how convincing one finds this
    argument. However, as this project continues, the reason for the flattening of
    the domain model is for simplicity’s sake, as polymorphic structures in a persistence
    layer is a distractingly complex topic.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a strategy to decouple the Kafka consumer with the AssetManager,
    we should update the Kafka consumer to take advantage of it. We will need to pass
    the mediator into the class in its constructor. This way the AssetManager and
    the consumer will have access to the same instance, and messages can freely flow
    back and forth; or rather, in this case, the flow will be unidirectional. You
    should note that we intend to read and write JSON on this topic, so we will need
    to have our value deserializer understand this.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.17 Incorporating the mediator into the Kafka consumer class
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Next, we shall examine the changes that the `AssetManager` class require to
    incorporate the ability to track these locations. You should note that to get
    this project to run in its entirely, you would need to modify the `AssetManager`,
    `SQLAlchemyAssetRepository`, and `Asset` classes, as well as create a new table
    in your database called `itam.asset_locations`. The complete and updated source
    code is available in the GitHub repository for this book. For now, we shall just
    focus on the changes needed to get the events flowing through our system and use
    the repository for reference if the reader so chooses.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.3 AssetManager requires the additional of another constructor parameter
    and a method to handle the updates to its locations objects
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![A picture containing text, screenshot, font, receipt Description automatically
    generated](images/05image004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are two required changes for the `AssetManager` class: First, we need
    to add the `AssetLocationMediator` added to the constructor, registering it to
    handle the `AssetLocationUpdated` event. And secondly, we need to add a method
    that will handle this event. In this case, we call the method `update_asset_location`.
    The abridged code is below.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.18 The updated constructor and an event handler for the `AssetManager`
    class
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The `add_location` method of the Asset class merely appends the new Location
    to the end of a list of Locations. More sophisticated domain models might include
    a `current_location` attribute, relegating the rest to a list of historical locations;
    however, given that we are trying to get our events flowing through the system,
    it behooves us to keep things simple.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is only one final item on our todo list: create the topic. How does one
    create a topic? That is a good question. Thankfully, all the tools we need are
    available in our running Docker container. So, let’s log into our Kafka Docker
    instance. We use the following command (assuming that your Docker instance is
    named kafka): `docker exec -it kafka /bin/bash`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing that should check is if any topics are already created. We
    can do that with the following command: `kafka-topics --list --bootstrap-server
    localhost:9092.` This command would list all of the existing topics, running on
    this Kafka cluster. As you can see, there aren’t any.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the need for a topic, let’s create it. You would use the following command:
    `kafka-topics --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic asset_location.`
    If you re-run the `kafka-topics --list` command again, you will see the new topic.
    The partitions and replication-factor instructions we included in the create topic
    command inform Kafka that we want one partition and a replication factor of one.
    If we were setting this up for production, or any purpose other than testing,
    we would likely want them to be greater than that to ensure availability of data.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.1 Summary of Kafka console commands
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Action | Command |'
  prefs: []
  type: TYPE_TB
- en: '| Create |'
  prefs: []
  type: TYPE_TB
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Read |'
  prefs: []
  type: TYPE_TB
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Write |'
  prefs: []
  type: TYPE_TB
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Delete |'
  prefs: []
  type: TYPE_TB
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| List |'
  prefs: []
  type: TYPE_TB
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now comes the fun part, observing the application in action. Kafka comes with
    a console produce that will allow us to publish messages to Kafka from standard
    in. To do this, we should launch the console producer with the follow command:
    `kafka-console-producer --broker-list localhost:9092 --topic asset_location`.
    You will enter an interactive session allowing you to publish a message with every
    line. Let’s publish a few messages simulating our asset moving around or near
    Chicago.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.19 Entries for the Kafka console producer
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: As you enter these messages, you should see the output from your application
    indicating that the location has been updated.
  prefs: []
  type: TYPE_NORMAL
- en: 'For completeness’s sake, there is one more command that you should be aware
    of: you might make a mistake when entering these messages. An invalid message
    could potentially break your consumer. One possible solution is to delete the
    topic. Deleting a topic might sound dramatic, but it will solve the issue. So
    here is that command: `kafka-topics --delete --topic asset_location --bootstrap-server
    localhost:9092.`'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have added the ability to see changes in location of our
    `Assets` in real-time tracking using Apache Kafka. In the final section of this
    chapter, we will work with Copilot Chat to extend the capacity by monitoring our
    `Assets` in real-time, attempting to determine if they are where they should be.
    Again, we will explore using Spark and Kafka together to accomplish this analysis.
    Once completed, we will win the thanks for our InfoSec team who fear that too
    much of our core business and intellectual property exists on and within these
    `Assets`.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Analyzing, Learning, and Tracking with Apache Spark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Real-time tracking of assets is a business-critical function. Your IT assets
    contain sensitive business data, client lists, sales figures, Profits and Loss
    (PnL) projections, and sales strategies, amongst many other items. A lost asset
    can be an existential event for a company. Therefore, careful management and monitoring
    is priority one for many InfoSec professionals. In this section, we aim to make
    their jobs substantially easier. Modern data platforms make it trivial to track
    your assets in real time and send notifications should questionable conditions
    arise. Let’s get into it.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark is a powerful, open-source data processing engine built around
    speed, ease of use, and sophisticated analytics. It was developed to provide an
    improved alternative to MapReduce for processing big data sets and can handle
    batch and real-time analytics. Spark provides APIs for Scala, Java, Python, and
    R and a built-in module for SQL queries. Its core data structure, the Resilient
    Distributed Dataset (RDD), enables fault-tolerant operation and allows data to
    be processed in parallel across a cluster of computers.
  prefs: []
  type: TYPE_NORMAL
- en: Spark also includes several libraries to broaden its capabilities, including
    MLlib for machine learning, Spark Streaming for processing live data streams,
    and Spark SQL and DataFrames for processing structured data. These tools make
    it well-suited for tasks ranging from machine learning to real-time data streaming
    and batch processing. Its in-memory processing capabilities make Spark significantly
    faster than its predecessor, making it a popular choice for big data processing.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will ask Copilot Chat to recommend a strategy for using Apache Spark
    to track our assets.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.20 The prompt to ask Copilot Chat how best to track out `Assets` in
    real-time
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Copilot Chat generates the class that you should be able to put into a file
    called asset_location_spark_adapter.py in the infrastructure package. Helpfully,
    it also includes comments for each line, so you should find the generated code
    easy to follow. The import statements include the Spark libraries, as well as
    the geopy.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.21 The imports required to run Spark
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The class begins with an overstuffed constructor that defines the schema Spark
    will use when it translates the JSON to a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The `AssetLocationSparkAdapter`, as defined, is a blocking process. Therefore,
    your FastAPI application will not “fully” boot until the Spark process has been
    killed. You would want this to be a standalone process, or you would need to introduce
    an asynchronous framework to have these two processes run concomitantly.
  prefs: []
  type: TYPE_NORMAL
- en: Next, it will start up a local Spark instance/session that will allow Spark
    to connect to the Kafka topic and continuously stream in the records.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.22 The `AssessLocationSparkAdapter`, which is responsible for processing
    the Kafka topic and generating notifications
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The final section of the `AssetLocationSparkAdapter` class will calculate the
    distance from the asset’s current location to Chicago’s. If the difference is
    greater than 25 miles, then it will send the result set to the console. Additionally,
    it provides a method to start and stop the adapter.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.23 The `AssessLocationSparkAdapter` calculating the distance from
    the current `Asset’s` location from Chicago
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The calculate_distance method takes the logitude and latitude of the asset’s
    location and determines the distance from Chicaogo using the geopy.distance function.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.24 The function that Spark uses to calculate the distance between
    Chi-town and your `Asset`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'In this instance, the code that Copilot Chat produced had some issues preventing
    it from running locally. After running it locally, encountering these issues,
    and trolling Stack Overflow, you would have found a solution to the two main issues
    with the code: a missing environmental variable for running locally, and failing
    to register your UDF (User Defined Function). Thankfully, you do not need to do
    the testing and research, as a solution is provided in listing 5.23.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.25 Edits required to run the application locally
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Finally, to run your Spark application, you would update `main.py` with the
    following code in the `main` function.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.26 Updates to the `main` function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: As you enter locations for your asset into the Kafka console producer that are
    further than twenty-five miles from downtown Chicago, you will notice entries
    get written to the console. It would be trivial to update the class to output
    these results to Twilio’s SMS API or an email service such as SendGrid.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.27 The streaming output from your asset location
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Congratulations, you are tracking your assets in real-time and sending real-time
    alerts should your corporate resources grow legs and walk away.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GitHub Copilot Chat is an innovative tool that brings together the comprehensive
    language understanding of ChatGPT and the handy features of Copilot. It's a noteworthy
    development in the realm of programming assistance, particularly for providing
    detailed and contextually relevant suggestions in real-time, fostering a more
    efficient coding experience.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Mediator design pattern is a distinct behavioral pattern that facilitates
    a high level of decoupling between objects, thus enhancing the modularity of your
    code. By encompassing the interactions between objects within a mediator object,
    objects can communicate indirectly, which reduces dependencies and promotes code
    reusability and ease of modification.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Kafka serves as a robust, distributed streaming platform engineered for
    creating real-time data pipelines and streaming applications. It can effectively
    handle data streams from a multitude of sources and transmit them to various consumers,
    making it an ideal solution for use cases that require handling substantial volumes
    of real-time or near-real-time data. It's important to remember that Kafka is
    optimized for append-only, immutable data and not for use cases that need record
    updates or deletions, or complex querying.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Spark stands as a high-performance, distributed data processing engine
    renowned for its speed, user-friendliness, and advanced analytics capabilities.
    It's highly suitable for scenarios necessitating real-time data processing or
    for operations on enormous datasets. However, for simpler tasks such as basic
    analytics or straightforward aggregations, a traditional relational database could
    be a more appropriate choice.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative AI, despite its rapid evolution, is not infallible. It's crucial
    to meticulously review all generated output to ensure it aligns with your specific
    requirements and quality standards. While generative AI is not a substitute for
    deep domain knowledge or coding expertise, it significantly enhances productivity
    by providing valuable insights and reducing the time spent on routine tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
