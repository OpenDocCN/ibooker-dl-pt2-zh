- en: Chapter 11\. Basics of deep reinforcement learning
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第11章。深度强化学习的基础知识
- en: '*This chapter covers*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章内容*'
- en: How reinforcement learning differs from the supervised learning discussed in
    the previous chapters
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习与前面几章讨论的监督学习有什么不同
- en: 'The basic paradigm of reinforcement learning: agent, environment, action, and
    reward, and the interactions between them'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习的基本范例：智能体、环境、行动和奖励以及它们之间的交互
- en: 'The general ideas behind two major approaches to solving reinforcement-learning
    problems: policy-based and value-based methods'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决强化学习问题的两种主要方法背后的一般思想：基于策略和基于值的方法
- en: 'Up to this point in this book, we have focused primarily on a type of machine
    learning called *supervised learning*. In supervised learning, we train a model
    to give us the correct answer given an input. Whether it’s assigning a class label
    to an input image ([chapter 4](kindle_split_015.html#ch04)) or predicting future
    temperature based on past weather data ([chapters 8](kindle_split_020.html#ch08)
    and [9](kindle_split_021.html#ch09)), the paradigm is the same: mapping a static
    input to a static output. The sequence-generating models we visited in [chapters
    9](kindle_split_021.html#ch09) and [10](kindle_split_022.html#ch10) were slightly
    more complicated in that the output is a sequence of items instead of a single
    item. But those problems can still be reduced to one-input-one-output mapping
    by breaking the sequences into steps.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本书中，我们主要关注一种叫做*监督学习*的机器学习方法。在监督学习中，我们通过给出一个输入来训练模型给我们正确的答案。无论是给输入图像赋予一个类别标签（[第4章](kindle_split_015.html#ch04)）还是根据过去的天气数据预测未来温度（[第8章](kindle_split_020.html#ch08)和[第9章](kindle_split_021.html#ch09)），这种模式都是一样的：将静态输入映射到静态输出。在我们访问的[第9章](kindle_split_021.html#ch09)和[第10章](kindle_split_022.html#ch10)中生成序列的模型要稍微复杂一些，因为输出是一系列项而不是单个项。但是通过将序列拆分成步骤，这些问题仍然可以归结为一对一的输入输出映射。
- en: In this chapter, we will look at a very different type of machine learning called
    *reinforcement learning* (RL). In RL, our primary concern is not a static output;
    instead, we train a model (or an *agent* in RL parlance) to take actions in an
    environment with the goal of maximizing a metric of success called a *reward*.
    For example, RL can be used to train a robot to navigate the interior of a building
    and collect trash. In fact, the environment doesn’t have to be a physical one;
    it can be any real or virtual space that an agent takes actions in. The chess
    board is the environment in which an agent can be trained to play chess; the stock
    market is the environment in which an agent can be trained to trade stocks. The
    generality of the RL paradigm makes it applicable to a wide range of real-world
    problems ([figure 11.1](#ch11fig01)). Also, some of the most spectacular advances
    in the deep-learning revolution involve combining the power of deep learning with
    RL. These include bots that can beat Atari games with superhuman skill and algorithms
    that can beat world champions at the games of Go and chess.^([[1](#ch11fn1)])
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍一种非常不同的机器学习类型，称为*强化学习*（RL）。在强化学习中，我们的主要关注点不是静态输出；相反，我们训练一个模型（或者在强化学习术语中称为*智能体*）在一个环境中采取行动，目的是最大化称为*奖励*的成功指标。例如，RL可以用于训练一个机器人在建筑物内航行并收集垃圾。实际上，环境不一定是物理环境；它可以是任何一个智能体采取行动的真实或虚拟空间。国际象棋棋盘是训练智能体下棋的环境；股票市场是训练智能体交易股票的环境。强化学习范式的普遍性使其适用于广泛的实际问题（[图11.1](#ch11fig01)）。另外，深度学习革命中一些最为引人瞩目的进展涉及将深度学习的能力与强化学习相结合。这包括可以以超人的技巧打败Atari游戏的机器人和可以在围棋和国际象棋游戏中击败世界冠军的算法^([[1](#ch11fn1)])。
- en: ¹
  id: totrans-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: David Silver et al., “Mastering Chess and Shogi by Self-Play with a General
    Reinforcement Learning Algorithm,” submitted 5 Dec. 2017, [https://arxiv.org/abs/1712.01815](https://arxiv.org/abs/1712.01815).
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: David Silver等人，“通过自我对弈用通用强化学
- en: 'Figure 11.1\. Example real-world applications of reinforcement learning. Top
    left: Solving board games such as chess and Go. Top right: algorithmic trading
    of stocks. Bottom left: automated resource management in data centers. Bottom
    right: control and action planning in robotics. All images are free license and
    downloaded from [www.pexels.com](http://www.pexels.com).'
  id: totrans-10
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.1。强化学习的实际应用示例。左上：解决象棋和围棋等棋类游戏。右上：进行算法交易。左下：数据中心的自动资源管理。右下：机器人的控制和行动规划。所有图像均为免费许可证，并从[www.pexels.com](http://www.pexels.com)下载。
- en: '![](11fig01a_alt.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![11fig01a_alt.jpg](11fig01a_alt.jpg)'
- en: The fascinating topic of RL differs from the supervised-learning problems we
    saw in the previous chapters in some fundamental ways. Unlike learning input-output
    mappings in supervised learning, RL is about discovering optimal decision-making
    processes by interacting with an environment. In RL, we are not given labeled
    training datasets; instead, we are given different types of environments to explore.
    In addition, time is an indispensable and foundational dimension in RL problems,
    unlike in many supervised-learning problems, which either lack a time dimension
    or treat time more or less like a spatial dimension. As a result of RL’s unique
    characteristics, this chapter will involve a vocabulary and way of thinking very
    different from the previous chapters. But don’t worry. We will use simple and
    concrete examples to illustrate the basic concepts and approaches. In addition,
    our old friends, deep neural networks and their implementations in TensorFlow.js,
    will still be with us. They will form an important pillar (albeit not the only
    one!) of the RL algorithms that we’ll encounter in this chapter.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 引人入胜的强化学习话题在一些基本方式上与我们在前几章中看到的监督学习问题有所不同。与监督学习中学习输入-输出映射不同，强化学习是通过与环境交互来发现最优决策过程。在强化学习中，我们没有给定标记的训练数据集；相反，我们被提供了不同类型的环境来探索。此外，时间是强化学习问题中不可或缺且基础性的维度，与许多监督学习问题不同，后者要么缺乏时间维度，要么将时间更多地视为空间维度。由于强化学习的独特特征，本章将涉及一种与前几章非常不同的词汇和思维方式。但不要担心。我们将使用简单而具体的例子来说明基本概念和方法。此外，我们的老朋友，深度神经网络及其在
    TensorFlow.js 中的实现，将仍然与我们同在。它们将构成本章中我们将遇到的强化学习算法的重要支柱（尽管不是唯一的！）。
- en: By the end of the chapter, you should be familiar with the basic formulation
    of RL problems, understand the basic ideas underlying two commonly used types
    of neural networks in RL (policy networks and Q-networks), and know how to train
    such networks using the API of TensorFlow.js.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，您应该熟悉强化学习问题的基本公式化，理解强化学习中两种常用神经网络（策略网络和 Q 网络）背后的基本思想，并知道如何使用 TensorFlow.js
    的 API 对这些网络进行训练。
- en: 11.1\. The formulation of reinforcement-learning problems
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1\. 强化学习问题的制定
- en: '[Figure 11.2](#ch11fig02) lays out the major components of an RL problem. The
    agent is what we (the RL practitioners) have direct control over. The agent (such
    as a robot collecting trash in a building) interacts with the environment in three
    ways:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11.2](#ch11fig02) 描绘了强化学习问题的主要组成部分。代理是我们（强化学习从业者）直接控制的对象。代理（例如在建筑物中收集垃圾的机器人）以三种方式与环境交互：'
- en: At each step, the agent takes an *action*, which changes the state of the environment.
    In the context of our trash-collecting robot, for instance, the set of actions
    to choose from may be `{go forward, go backward, turn left, turn right, grab trash,
    dump trash into container}`.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每一步中，代理程序采取一种 *行动*，这改变了环境的状态。例如，在我们的垃圾收集机器人的背景下，可供选择的行动集可能是`{前进，后退，左转，右转，抓取垃圾，将垃圾倒入容器}`。
- en: Once in a while, the environment provides the agent with a *reward*, which can
    be understood in anthropomorphic terms as a measurement of instantaneous pleasure
    or fulfillment. But in more abstract terms, a reward (or rather, a sum of rewards
    over time, as we’ll see later) is a number that the agent tries to maximize. It
    is an important numeric value that guides RL algorithms in a way similar to how
    loss values guide supervised-learning algorithms. A reward can be positive or
    negative. In the example of our trash-collecting robot, a positive reward can
    be given when a bag of trash is dumped successfully into the robot’s trash container.
    In addition, a negative reward should be given when the robot knocks over a trash
    can, bumps into people or furniture, or dumps trash outside its container.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偶尔，环境会向代理程序提供一个 *奖励*，在人性化的术语中，可以理解为即时愉悦或满足的衡量。但更抽象地说，奖励（或者，如我们稍后将看到的，一段时间内的奖励总和）是一个代理试图最大化的数字。它是一个重要的数值，以类似于损失值引导监督学习算法的方式引导强化学习算法。奖励可以是正的或负的。在我们的垃圾收集机器人的例子中，当一袋垃圾成功倒入机器人的垃圾容器时，可以给予正奖励。此外，当机器人撞倒垃圾桶，撞到人或家具，或者在容器外倒垃圾时，应给予负奖励。
- en: Apart from the reward, the agent can observe the state of the environment through
    another channel, namely, *observation*. This can be the full state of the environment
    or only the part of it visible to the agent, possibly distorted through a certain
    imperfect channel. For our trash-collecting robot, observations are the streams
    of images and signals from cameras and various types of sensors on its body.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了奖励外，代理还可以通过另一个渠道观察环境的状态，即*观察*。这可以是环境的完整状态，也可以只是代理可见的部分，可能通过某个不完美的渠道而失真。对于我们的垃圾收集机器人来说，观察包括来自其身体上的相机和各种传感器的图像和信号流。
- en: Figure 11.2\. A schematic diagram of the basic formulation of RL problems. At
    each time step, an agent selects an action from the set of possible actions, which
    causes a change in the state of the environment. The environment provides the
    agent with a reward according to its current state and the action selected. The
    state of the environment is fully or partially observed by the agent, which will
    use that state to make decisions about future actions.
  id: totrans-19
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.2：强化学习问题的基本公式的示意图。在每个时间步，代理从可能的行动集合中选择一个行动，从而导致环境状态的变化。环境根据其当前状态和选择的行动向代理提供奖励。代理可以部分或完全观察到环境的状态，并将使用该状态来决定未来的行动。
- en: '![](11fig01_alt.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](11fig01_alt.jpg)'
- en: The formulation just defined is somewhat abstract. Let’s look at some concrete
    RL problems and get a sense of the range of possibilities the formulation encompasses.
    In this process, we will also glance at the taxonomy of all the RL problems out
    there. First let’s consider actions. The space from which the agent can choose
    its actions can be discrete or continuous. For example, RL agents that play board
    games usually have discrete action spaces because in such problems, there are
    only a finite set of moves to choose from. However, an RL problem that involves
    controlling a virtual humanoid robot to walk bipedally^([[2](#ch11fn2)]) involves
    a continuous action space because torques on the joints are continuous-varying
    quantities. The example problems we’ll cover in this chapter will be about discrete
    action spaces. Note that in some RL problems, continuous action spaces can be
    turned into discrete ones through discretization. For example, DeepMind’s StarCraft
    II game agent divides the high-resolution 2D screen into coarser rectangles to
    determine where to move units or launch attacks.^([[3](#ch11fn3)])
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 刚定义的公式有些抽象。让我们看看一些具体的强化学习问题，并了解公式所涵盖的可能范围。在此过程中，我们还将浏览所有强化学习问题的分类。首先让我们考虑一下行动。代理可以选择的行动空间可以是离散的，也可以是连续的。例如，玩棋盘游戏的强化学习代理通常有离散的行动空间，因为在这种问题中，只有有限的移动选择。然而，一个涉及控制虚拟类人机器人的强化学习问题需要在双足行走时使用连续的行动空间，因为关节上的扭矩是连续变化的。在本章中，我们将介绍关于离散行动空间的示例问题。请注意，在某些强化学习问题中，可以通过离散化将连续的行动空间转化为离散的。例如，DeepMind
    的《星际争霸 II》游戏代理将高分辨率的 2D 屏幕划分成较粗的矩形，以确定将单位移动到哪里或在哪里发起攻击。
- en: ²
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ²
- en: ''
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'See the Humanoid environment in OpenAI Gym: [https://gym.openai.com/envs/Humanoid-v2/](https://gym.openai.com/envs/Humanoid-v2/).'
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 查看 OpenAI Gym 中的 Humanoid 环境：[https://gym.openai.com/envs/Humanoid-v2/](https://gym.openai.com/envs/Humanoid-v2/)。
- en: ³
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³
- en: ''
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Oriol Vinyals et al., “StarCraft II: A New Challenge for Reinforcement Learning,”
    submitted 16 Aug. 2017, [https://arxiv.org/abs/1708.04782](https://arxiv.org/abs/1708.04782).'
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Oriol Vinyals 等，“星际争霸 II：强化学习的新挑战”，提交日期：2017 年 8 月 16 日，[https://arxiv.org/abs/1708.04782](https://arxiv.org/abs/1708.04782)。
- en: Rewards, which play a centric role in RL problems, also show variations. First,
    some RL problems involve only positive rewards. For example, as we’ll see shortly,
    an RL agent whose goal is to balance a pole on a moving cart gets only positive
    rewards. It gets a small positive reward for every time step it keeps the pole
    standing. However, many RL problems involve a mix of positive and negative rewards.
    Negative rewards can be thought of as “penalties” or “punishment.” For instance,
    an agent that learns to shoot a basketball at the hoop should receive positive
    rewards for goals and negative ones for misses.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励在强化学习问题中起着核心作用，但也呈现出多样性。首先，有些强化学习问题仅涉及正奖励。例如，正如我们稍后将看到的，一个强化学习代理的目标是使一个杆保持在移动的推车上，则它只会获得正奖励。每次它保持杆竖立时，它都会获得少量正奖励。然而，许多强化学习问题涉及正负奖励的混合。负奖励可以被看作是“惩罚”或“处罚”。例如，一个学习向篮筐投篮的代理应该因进球而获得正奖励，而因投篮失误而获得负奖励。
- en: 'Rewards can also vary in the frequency of occurrence. Some RL problems involve
    a continuous flow of rewards. Take the aforementioned cart-pole problem, for example:
    as long as the pole is still standing, the agent receives a (positive) reward
    at each and every time step. On the other hand, consider a chess-playing RL agent—the
    reward comes only at the end, when the outcome of the game (win, lose, or draw)
    is determined. There are also RL problems between these two extremes. For instance,
    our trash-collecting robot may receive no reward at all in the steps between two
    successful trash dumps—that is, when it’s just moving from place A to place B.
    Also, an RL agent trained to play the Atari game Pong doesn’t receive a reward
    at every step (frame) of the video game; instead, it is rewarded positively once
    every few steps, when the bat it controls hits the ball and bounces back toward
    the opponent. The example problems we’ll visit in this chapter contain a mix of
    RL problems with high and low reward frequencies of occurrence.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励的发生频率也可能不同。一些强化学习问题涉及连续的奖励流。比如前文提到的倒立摆问题：只要杆子还没倒下，智能体每一个时间步长都会获得（正面的）奖励。而对于下棋的强化学习智能体，则只有在游戏结束（胜利、失败或平局）时才会获得奖励。两种极端之间还有其他强化学习问题。例如，我们的垃圾收集机器人在两次成功垃圾转移之间可能完全没有任何奖励——也就是在从
    A 点到 B 点的移动过程中。此外，训练打 Atari 游戏 Pong 的强化学习智能体也不会在电子游戏的每一步（帧）都获得奖励；相反，在球拍成功击中乒乓球并将其反弹到对手处时，才会每隔几步（帧）获得正面的奖励。本章我们将介绍一些奖励频率高低不同的强化学习问题。
- en: Observation is another important factor in RL problems. It is a window through
    which the agent can glance at the state of the environment and form a basis on
    which to make decisions apart from any reward. Like actions, observations can
    be discrete (such as in a board or card game) or continuous (as in a physical
    environment). One question you might want to ask is why our RL formulation separates
    observation and reward into two entities, even though they can both be viewed
    as feedback provided by the environment to the agent. The answer is conceptual
    clarity and simplicity. Although the reward can be regarded as an observation,
    it is what the agent ultimately “cares” about. Observation may contain both relevant
    and irrelevant information, which the agent needs to learn to filter and make
    smart use of.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 观察是强化学习问题中的另一个重要因素。它是一个窗口，通过它智能体可以看到环境的状态，并且基于这个状态做出决策，而不仅仅是依据任何奖励。像动作一样，观察可以是离散的（例如在棋盘游戏或者扑克游戏中），也可以是连续的（例如在物理环境中）。你可能会问：为什么我们的强化学习公式将观察和奖励分开，即使它们都可以被看作是环境向智能体提供反馈的形式？答案是为了概念上的清晰和简单易懂。尽管奖励可以被视为观察，但它是智能体最终“关心”的。而观察可以包含相关和无关的信息，智能体需要学会过滤并巧妙地使用。
- en: Some RL problems reveal the entire state of the environment to the agent through
    observation, while others make available only parts of their states. Examples
    of the first kind include board games such as chess and Go. For the latter kind,
    good examples are card games like poker, in which you cannot see your opponent’s
    hand, as well as stock trading. Stock prices are determined by many factors, such
    as the internal operations of the companies and the mindset of other stock traders
    on the market. But very few of these states are directly observable by the agent.
    As a result, the agent’s observations are limited to the moment-by-moment history
    of stock prices, perhaps in addition to publicly available information such as
    financial news.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一些强化学习问题通过观察向智能体揭示环境的整个状态，而另一些问题则仅向智能体提供部分状态信息。第一类问题的例子包括棋类游戏（如棋类和围棋）。对于后一类问题，德州扑克等纸牌游戏是一个很好的例子，在这种游戏中你无法看到对手的手牌，而股票交易也是其中的一个例子。股票价格受许多因素的影响，例如公司的内部运营和市场上其他股票交易者的想法。但是，智能体只能观察到股票价格的逐时历史记录，可能还加上公开的信息，如财经新闻。
- en: 'This discussion sets up the playground in which RL happens. An interesting
    thing worth pointing out about this formulation is that the flow of information
    between the agent and the environment is bidirectional: the agent acts on the
    environment; the environment, in turn, provides the agent with rewards and state
    information. This distinguishes RL from supervised learning, in which the flow
    of information is largely unidirectional: the input contains enough information
    for an algorithm to predict the output, but the output doesn’t act on the input
    in any significant way.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这个讨论建立了强化学习发生的平台。关于这个表述值得指出的一个有趣的事情是，代理与环境之间的信息流是双向的：代理对环境进行操作；环境反过来提供给代理奖励和状态信息。这使得强化学习与监督学习有所不同，监督学习中信息流主要是单向的：输入包含足够的信息，使得算法能够预测输出，但输出并不会以任何重要的方式影响输入。
- en: Another interesting and unique fact about RL problems is that they must happen
    along the time dimension in order for the agent-environment interaction to consist
    of multiple rounds or steps. Time can be either discrete or continuous. For instance,
    RL agents that solve board games usually operate on a discrete time axis because
    such games are played out in discrete turns. The same applies to video games.
    However, an RL agent that controls a physical robotic arm to manipulate objects
    is faced with a continuous time axis, even though it may still choose to take
    actions at discrete points in time. In this chapter, we will focus on discrete-time
    RL problems.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习问题的另一个有趣而独特的事实是，它们必须沿着时间维度发生，以便代理-环境交互由多个轮次或步骤组成。时间可以是离散的或连续的。例如，解决棋盘游戏的RL代理通常在离散的时间轴上操作，因为这类游戏是在离散的回合中进行的。视频游戏也是如此。然而，控制物理机器人手臂操纵物体的RL代理面临着连续的时间轴，即使它仍然可以选择在离散的时间点采取行动。在本章中，我们将专注于离散时间RL问题。
- en: This theoretical discussion of RL should be enough for now. In the next section,
    we will start exploring some actual RL problems and algorithms hands-on.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这个关于强化学习的理论讨论暂时就够了。在下一节中，我们将开始亲手探索一些实际的强化学习问题和算法。
- en: '11.2\. Policy networks and policy gradients: The cart-pole example'
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2\. 策略网络和策略梯度：车杆示例
- en: The first RL problem we’ll solve is a simulation of a physical system in which
    a cart carrying a pole moves on a one-dimensional track. Aptly named the *cart-pole*
    problem, it was first proposed by Andrew Barto, Richard Sutton, and Charles Anderson
    in 1983.^([[4](#ch11fn4)]) Since then, it has become a benchmark problem for control-systems
    engineering (somewhat analogous to the MNIST digit-recognition problem for supervised
    learning), owing to its simplicity and well-formulated physics and math, as well
    as to the fact that it is not entirely trivial to solve. In this problem, the
    agent’s goal is to control the movement of a cart by exerting leftward or rightward
    forces in order to keep a pole standing in balance for as long as possible.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将解决的第一个强化学习问题是模拟一个物理系统，在该系统中，一个装有杆的小车在一维轨道上移动。这个问题被恰如其名地称为*车杆*问题，它是由安德鲁·巴托（Andrew
    Barto）、理查德·萨顿（Richard Sutton）和查尔斯·安德森（Charles Anderson）在1983年首次提出的。自那时以来，它已经成为控制系统工程的基准问题（在某种程度上类似于MNIST数字识别问题用于监督学习），因为它的简单性和良好构建的物理学和数学，以及解决它并非完全微不足道。在这个问题中，代理的目标是通过施加左右方向的力来控制小车的运动，以尽可能长时间地保持杆的平衡。
- en: ⁴
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁴
- en: ''
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Andrew G. Barto, Richard S. Sutton, and Charles W. Anderson, “Neuronlike Adaptive
    Elements that Can Solve Difficult Learning Control Problems,” *IEEE Transactions
    on Systems, Man, and Cybernetics*, Sept./Oct. 1983, pp. 834–846, [http://mng.bz/Q0rG](http://mng.bz/Q0rG).
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 安德鲁·G·巴托（Andrew G. Barto）、理查德·S·萨顿（Richard S. Sutton）和查尔斯·W·安德森（Charles W. Anderson），“可以解决困难学习控制问题的类神经自适应元件”，*IEEE系统、人类和控制论交易*，1983年9月/10月，页码834–846，[http://mng.bz/Q0rG](http://mng.bz/Q0rG)。
- en: 11.2.1\. Cart-pole as a reinforcement-learning problem
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.2.1\. 作为强化学习问题的车杆
- en: Before going further, you should play with the cart-pole example to get an intuitive
    understanding of the problem. The cart-pole problem is simple and lightweight
    enough that we perform the simulation and training entirely in the browser. [Figure
    11.3](#ch11fig03) offers a visual depiction of the cart-pole problem, which you
    can find in the page opened by the `yarn watch` command. To checkout and run the
    example, use
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在进一步探讨之前，你应该通过玩车杆示例来直观地理解这个问题。车杆问题简单轻便，我们完全可以在浏览器中进行模拟和训练。[图 11.3](#ch11fig03)
    提供了车杆问题的可视化描述，你可以在通过`yarn watch`命令打开的页面中找到。要查看和运行示例，请使用
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Figure 11.3\. Visual rendering of the cart-pole problem. Panel A: four physical
    quantities (cart position *x*, cart velocity *x*′, pole tilt angle θ, and pole
    angular velocity θ′) make up the environment state and observation. At each time
    step, the agent may choose a leftward-force action or a rightward-force one, which
    will change the environment state accordingly. Panels B and C: the two conditions
    that will cause a game to end—either the cart goes too much to the left or to
    the right (B) or the pole tilts too much from the upright position (C).'
  id: totrans-43
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.3\. 小车杆问题的视觉渲染。A 面板：四个物理量（小车位置*x*，小车速度*x*′，杆倾角 θ 和杆角速度 θ'）构成环境状态和观察。在每个时间步长，代理可以选择向左施加力或向右施加力的行动，这将相应地改变环境状态。B
    和 C 面板：导致游戏结束的两个条件——要么小车向左或向右移动太多（B），要么杆从垂直位置倾斜太多（C）。
- en: '![](11fig02_alt.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](11fig02_alt.jpg)'
- en: Click the Create Model button and then the Train button. You should then see
    an animation at the bottom of the page showing an untrained agent performing the
    cart-pole task. Since the agent’s model has its weights initialize random values
    (more on the model later), it will perform quite poorly. All time steps from the
    beginning of a game to the end are sometimes referred to collectively as an *episode*
    in RL terminology. We will use the terms *game* and *episode* interchangeably
    here.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 点击“创建模型”按钮，然后再点击“训练”按钮。然后您应该在页面底部看到一个动画，显示一个未经训练的代理执行车杆任务。由于代理模型的权重被初始化为随机值（关于模型的更多信息稍后再说），它的表现会非常糟糕。从游戏开始到结束的所有时间步有时在
    RL 术语中称为一个*episode*。我们在这里将术语*game*和*episode*互换使用。
- en: As panel A in [figure 11.3](#ch11fig03) shows, the position of the cart along
    the track at any time step is captured by a variable called *x*. Its instantaneous
    velocity is denoted *x*'. In addition, the tilt angle of the pole is captured
    by another variable called θ. The angular velocity of the pole (how fast θ changes
    and in what direction) is denoted θ'. Together, the four physical quantities (*x*,
    *x*', θ, and θ') are completely observed by the agent at every step and constitute
    the observation part of this RL problem.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 正如[图 11.3](#ch11fig03) 中的 A 面板所示，任何时间步中小车沿轨道的位置由称为*x*的变量捕获。它的瞬时速度表示为*x*'。此外，杆的倾斜角由另一个称为
    θ 的变量捕获。杆的角速度（θ 变化的速度和方向）表示为 θ'。因此，这四个物理量（*x*，*x*'，θ 和 θ'）每一步都由代理完全观察到，并构成此 RL
    问题的观察部分。
- en: 'The simulation ends when either of two conditions is met:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟在满足以下任一条件时结束：
- en: The value of *x* goes out of a prespecified bound, or, in physical terms, the
    cart bumps into one of the walls on the two ends of the track (panel B in [figure
    11.3](#ch11fig03)).
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x* 的值超出预先指定的边界，或者从物理角度来说，小车撞到轨道两端的墙壁之一（[图 11.3](#ch11fig03) 的 B 面板）。'
- en: The absolute value of θ exceeds a certain threshold, or, in physical terms,
    the pole tilts too much away from the upright position (panel C in [figure 11.3](#ch11fig03)).
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当 θ 的绝对值超过一定阈值时，或者从物理角度来说，杆过于倾斜，偏离了垂直位置（[图 11.3](#ch11fig03) 的 C 面板）。
- en: The environment also terminates an episode after the 500th simulation step.
    This prevents the game from lasting too long (which can happen once the agent
    gets very good at the game through learning). This upper bound on the number of
    steps is adjustable in the UI. Until the game ends, the agent gets a reward of
    a unit (`1`) at every step of the simulation. Therefore, in order to achieve a
    higher cumulative reward, the agent needs to find a way to keep the pole standing.
    But how does the agent control the cart-pole system? This brings us to the action
    part of this RL problem.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 环境还在第 500 个模拟步骤后终止一个 episode。这样可以防止游戏持续时间过长（一旦代理通过学习变得非常擅长游戏，这种情况可能会发生）。步数的上限在用户界面中是可以调整的。直到游戏结束，代理在模拟的每一步都获得一个单位的奖励（`1`）。因此，为了获得更高的累积奖励，代理需要找到一种方法来保持杆站立。但是代理如何控制小车杆系统呢？这就引出了这个
    RL 问题的行动部分。
- en: 'As the Force arrows in panel A of [figure 11.3](#ch11fig03) show, the agent
    is limited to two possible actions at every step: exerting a force to the left
    or to the right on the cart. The agent must choose one of the two force directions.
    The magnitude of the force is fixed. Once the force is exerted, the simulation
    will enact a set of mathematical equations to compute the next state (new values
    of *x*, *x*'', θ, and θ'') of the environment. The details involve familiar Newtonian
    mechanics. We won’t cover the detailed equations, as understanding them is not
    essential here, but they are available in the cart-pole/cart_pole.js file under
    the cart-pole directory if you are interested.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图 11.3](#ch11fig03) A 面板中的力箭头所示，智能体在每一步只能执行两种可能的动作：在小车上施加向左或向右的力。智能体必须选择其中一种力的方向。力的大小是固定的。一旦施加了力，模拟将执行一组数学方程来计算环境的下一个状态（*x*、*x*'、θ
    和 θ' 的新值）。详细内容涉及熟悉的牛顿力学。我们不会详细介绍这些方程，因为在这里理解它们并不重要，但是如果您感兴趣，可以在 cart-pole 目录下的
    cart-pole/cart_pole.js 文件中找到它们。
- en: 'Likewise, the code that renders the cart-pole system in an HTML canvas can
    be found in cart-pole/ui.js. This code underlines an advantage of writing RL algorithms
    in JavaScript (in particular, in TensorFlow.js): the UI and the learning algorithm
    can be conveniently written in the same language and be tightly integrated with
    each other. This facilitates the visualization and intuitive understanding of
    the problem and speeds up the development process. To summarize the cart-pole
    problem, we can describe it in the canonical RL formulation (see [table 11.1](#ch11table01)).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，渲染小车摆杆系统的 HTML 画布的代码可以在 cart-pole/ui.js 中找到。这段代码突显了使用 JavaScript（特别是 TensorFlow.js）编写
    RL 算法的优势：UI 和学习算法可以方便地用同一种语言编写，并且彼此紧密集成。这有助于可视化和直观理解问题，并加速开发过程。为了总结小车摆杆问题，我们可以用经典强化学习框架来描述它（参见[table
    11.1](#ch11table01)）。
- en: Table 11.1\. Describing the cart-pole problem in the canonical RL formulation
  id: totrans-53
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 11.1\. 在经典强化学习框架中描述了小车摆杆问题
- en: '| Abstract RL concept | Realization in the cart-pole problem |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 抽象 RL 概念 | 在小车摆杆问题中的实现 |'
- en: '| --- | --- |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Environment | A cart carrying a pole and moving on a one-dimensional track.
    |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 环境 | 一个运载杆子并在一维轨道上移动的小车。'
- en: '| Action | (Discrete) Binary choice between a leftward force and a rightward
    one at each step. The magnitude of the force is fixed. |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 动作 | （离散）在每一步中，在左侧施加力和右侧施加力之间进行二进制选择。力的大小是固定的。'
- en: '| Reward | (Frequent and positive-only) For each step of the game episode,
    the agent receives a fixed reward (1). The episode ends as soon as the cart hits
    a wall at one end of the track, or the pole tilts too much from the upright position.
    |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 奖励 | （频繁且仅为正值）对于游戏每一步，智能体会收到固定的奖励（1）。当小车撞到轨道一端的墙壁，或者杆子从直立位置倾斜得太厉害时，该情节就会结束。'
- en: '| Observation | (Complete state, continuous) At each step, the agent can access
    the full state of the cart-pole system, including the cart position (*x*) and
    velocity (*x*''), in addition to the pole tilt angle (θ) and angular velocity
    (θ''). |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 观测 | （完整状态，连续）每一步，智能体可以访问小车摆杆系统的完整状态，包括小车位置（*x*）和速度（*x*''），以及杆倾斜角（θ）和角速度（θ''）。'
- en: 11.2.2\. Policy network
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.2.2\. 策略网络
- en: 'Now that the cart-pole RL problem is laid out, let’s look at how to solve it.
    Historically, control theorists have devised ingenious solutions to this problem.
    Their solutions are based on the underlying physics of this system.^([[5](#ch11fn5)])
    That’s *not* how we will approach the problem in this book. In the context of
    this book, doing that would be somewhat analogous to writing heuristics to parse
    edges and corners in MNIST images in order to classify the digits. Instead, we
    will ignore the physics of the system and let our agent learn through repeated
    trial and error. This jibes with the spirit of the rest of this book: instead
    of hard-coding an algorithm or manually engineering features based on human knowledge,
    we design an algorithm that allows the model to learn on its own.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在小车摆杆强化学习问题已经描述完毕，让我们看看如何解决它。从历史上看，控制理论家们曾经为这个问题设计过巧妙的解决方案。他们的解决方案基于这个系统的基本物理原理。[[5](#ch11fn5)]
    但是在本书的背景下，我们不会这样来解决这个问题。在本书的背景下，这样做有点类似于编写启发式算法来解析 MNIST 图像中的边缘和角落，以便对数字进行分类。相反，我们将忽略系统的物理特性，让我们的智能体通过反复试错来学习。这符合本书其余部分的精神：我们不是在硬编码算法，也不是根据人类知识手动设计特征，而是设计了一种允许模型自主学习的算法。
- en: ⁵
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁵
- en: ''
  id: totrans-63
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'If you are interested in the traditional, non-RL approach to the cart-pole
    problem and are not scared of the math, you can read the open courseware of a
    control-theory course at MIT by Russ Tedrake: [http://mng.bz/j5lp](http://mng.bz/j5lp).'
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果您对传统的、非RL方法解决小车-杆问题感兴趣，并且不怕数学，可以阅读麻省理工学院Russ Tedrake的控制理论课程的开放课程Ware：[http://mng.bz/j5lp](http://mng.bz/j5lp)。
- en: How can we let the agent decide the action (leftward versus rightward force)
    to take at each step? Given the observations available to the agent and the decision
    the agent has to make at every step, this problem can be reformulated as a simple
    input-output mapping problem like the ones in supervised learning. A natural solution
    is to build a neural network to select an action based on the observation. This
    is the basic idea behind the *policy network*.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何让代理在每一步决定动作（向左还是向右的力）？鉴于代理可用的观察和代理每一步需要做出的决定，这个问题可以被重新制定为一个简单的输入输出映射问题，就像在监督学习中那样。一个自然的解决方案是构建一个神经网络，根据观察来选择一个动作。这是*策略网络*背后的基本思想。
- en: This neural network takes a length-4 observation vector (*x*, *x*', θ, and θ')
    and outputs a number that can be translated into a left-versus-right decision.
    The network architecture is similar to the binary classifier we built for the
    phishing websites in [chapter 3](kindle_split_014.html#ch03). Abstractly, at each
    step, we will look at the environment and use our network to decide which action
    to take. By letting our network play a number of rounds, we will collect some
    data with which to evaluate those decisions. Then, we will invent a way to assign
    quality to those decisions so that we can adjust the weights of our network so
    that it will make decisions more like the “good” ones and less like the “bad”
    ones in the future.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这个神经网络将一个长度为4的观察向量（*x*，*x*'，θ和θ'）作为输入，并输出一个可以转化为左右决定的数字。该网络架构类似于我们在[第3章](kindle_split_014.html#ch03)中为仿冒网站构建的二元分类器。抽象地说，每一步，我们将查看环境，并使用我们的网络决定采取哪些行动。通过让我们的网络玩一些回合，我们将收集一些数据来评价那些决定。然后，我们将发明一种方法来给这些决定分配质量，以便我们可以调整我们的网络的权重，使其在将来做出更像“好”的决定，而不像“坏”的决定。
- en: 'The details of this system are different from our previous classifier work
    in the following aspects:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 该系统的细节与我们之前的分类器工作在以下方面有所不同：
- en: The model is invoked many times in the course of a game episode (at every time
    step).
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型在游戏过程中多次被调用（在每个时间步长）。
- en: The model’s output (the output from the Policy Network box in [figure 11.4](#ch11fig04))
    is logits instead of probability scores. The logits are subsequently converted
    into probability scores through a sigmoid function. The reason we don’t include
    the sigmoid nonlinearity directly in the last (output) layer of the policy network
    is that we need the logits for training, as we’ll see shortly.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的输出（[图11.4](#ch11fig04)中的策略网络框中的输出）是对数而不是概率分数。通过S形函数将对数值转换为概率分数。我们之所以不直接在策略网络的最后（输出）层中包含S形非线性，是因为我们需要对数值进行训练，我们很快就会看到原因。
- en: Figure 11.4\. How the policy network fits into our solution to the cart-pole
    problem. The policy network is a TensorFlow.js model that outputs the probability
    of the leftward-force action by using the observation vector (*x*, *x*′, θ, and
    θ′) as the input. The probability is converted to an actual action through random
    sampling.
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.4。策略网络如何融入我们解决小车-杆问题的解决方案。策略网络是一个TensorFlow.js模型，通过使用观察向量（*x*，*x*'，θ和θ'）作为输入，输出左向力动作的概率。通过随机抽样将概率转换为实际行动。
- en: '![](11fig03_alt.jpg)'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](11fig03_alt.jpg)'
- en: The probability output by the sigmoid function must be converted to a concrete
    action (left versus right). This is done through the random-sampling `tf.multinomial()`
    function call. Recall that we used `tf.multinomial()` in the lstm-text-generation
    example in [chapter 10](kindle_split_022.html#ch10), when we sampled the next
    character using softmax probabilities over letters of the alphabet to sample the
    next character. The situation here is slightly simpler because there are only
    two choices.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由S形函数输出的概率必须转换为具体的动作（向左还是向右）。这是通过随机抽样`tf.multinomial()`函数调用完成的。回想一下，在[lstm-text-generation
    example](kindle_split_022.html#ch10)中，我们使用`tf.multinomial()`来对字母表上的softmax概率进行抽样以抽取下一个字符。在这里的情况稍微简单一些，因为只有两个选择。
- en: The last point has deeper implications. Consider the fact that we *could* convert
    the output of the `tf.sigmoid()` function directly into an action by applying
    a threshold (for example, selecting the left action when the network’s output
    is greater than 0.5 and the right action otherwise). Why do we prefer the more
    complicated random-sampling approach with `tf.multinomial()` over this simpler
    approach? The answer is that we *want* the randomness that comes with `tf.multinomial()`.
    In the early phase of the training, the policy network is clueless about how to
    select the direction of the force because its weights are initialized randomly.
    By using random sampling, we encourage it to try random actions and see which
    ones work better. Some of the random trials will end up being bad, while others
    will give good results. Our algorithm will remember the good choices and make
    more of them in the future. But these good choices won’t become available unless
    the agent is allowed to try randomly. If we had chosen the deterministic threshold
    approach, the model would be stuck with its initial choices.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一点有着更深层次的含义。考虑到我们*可以*直接将 `tf.sigmoid()` 函数的输出通过应用阈值（例如，当网络的输出大于 0.5 时选择左侧动作，否则选择右侧动作）转换为一个动作。为什么我们更倾向于使用
    `tf.multinomial()` 的更复杂的随机抽样方法，而不是这种更简单的方法？答案是我们*希望*`tf.multinomial()` 带来的随机性。在训练的早期阶段，策略网络对于如何选择力的方向一无所知，因为其权重是随机初始化的。通过使用随机抽样，我们鼓励它尝试随机动作并查看哪些效果更好。一些随机试验将会失败，而另一些则会获得良好的结果。我们的算法会记住这些良好的选择，并在将来进行更多这样的选择。但是除非允许代理随机尝试，否则这些良好的选择将无法实现。如果我们选择了确定性的阈值方法，模型将被困在其初始选择中。
- en: This brings us to a classical and important topic in RL called *exploration
    versus exploitation*. *Exploration* refers to random tries; it is the basis on
    which good actions are discovered by the RL agent. *Exploitation* means making
    the optimal solutions that the agent has learned in order to maximize the reward.
    The two are incompatible with each other. Finding a good balance between them
    is critical to designing working RL algorithms. In the beginning, we want to explore
    a diverse array of possible strategies, but as we converge on better strategies,
    we want to fine-tune those strategies. So, there is generally a gradual ramping
    down of exploration with training in many algorithms. In the cart-pole problem,
    the ramping is implicit in the `tf.multinomial()` sampling function because it
    gives more and more deterministic outcomes when the model’s confidence level increases
    with training.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这将我们带入了强化学习中一个经典而重要的主题，即*探索与利用*。*探索*指的是随机尝试；这是 RL 代理发现良好行动的基础。*利用*意味着利用代理已学到的最优解以最大化奖励。这两者是相互不兼容的。在设计工作
    RL 算法时，找到它们之间的良好平衡非常关键。起初，我们想要探索各种可能的策略，但随着我们逐渐收敛于更好的策略，我们希望对这些策略进行微调。因此，在许多算法中，训练过程中的探索通常会逐渐减少。在
    cart-pole 问题中，这种减少是隐含在 `tf.multinomial()` 抽样函数中的，因为当模型的置信水平随着训练增加时，它会给出越来越确定的结果。
- en: '[Listing 11.1](#ch11ex01) (excerpted from cart-pole/index.js) shows the TensorFlow.js
    calls that create the policy network. The code in [listing 11.2](#ch11ex02) (also
    excerpted from cart-pole/index.js) converts the policy network’s output into the
    agent’s action, in addition to returning the logits for training purposes. Compared
    to the supervised-learning models we encountered in the previous chapters, the
    model-related code here is not much different.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 11.1](#ch11ex01)（摘自 cart-pole/index.js）展示了创建策略网络的 TensorFlow.js 调用。[清单
    11.2](#ch11ex02) 中的代码（同样摘自 cart-pole/index.js）将策略网络的输出转换为代理的动作，并返回用于训练目的的对数概率。与我们在前几章遇到的监督学习模型相比，这里的模型相关代码并没有太大不同。'
- en: However, what’s fundamentally different here is the fact that we don’t have
    a set of labeled data that can be used to teach the model which action choices
    are good and which are bad. If we had such a dataset, we could simply call `fit()`
    or `fitDataset()` on the policy network in order to solve the problem, like we
    did for the models in the previous chapters. But the fact is that we don’t, so
    the agent has to figure out which actions are good by playing the game and looking
    at the rewards it gets. In other words, it has to “learn swimming by swimming,”
    a key feature of RL problems. Next, we’ll look at how that’s done in detail.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这里根本不同的是，我们没有一组可以用来教模型哪些动作选择是好的，哪些是坏的标记数据集。如果我们有这样的数据集，我们可以简单地在策略网络上调用 `fit()`
    或 `fitDataset()` 来解决问题，就像我们在前几章中对模型所做的那样。但事实是我们没有，所以智能体必须通过玩游戏并观察到的奖励来弄清楚哪些动作是好的。换句话说，它必须“通过游泳学会游泳”，这是
    RL 问题的一个关键特征。接下来，我们将详细看一下如何做到这一点。
- en: 'Listing 11.1\. Policy network MLP: selecting actions based on observations'
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 策略网络 MLP：基于观察选择动作
- en: '[PRE1]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '***1*** hiddenLayerSize controls the sizes of all the policy network’s layers
    except the last one (output layer).'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: hiddenLayerSize 控制策略网络除最后一层（输出层）之外的所有层的大小。
- en: '***2*** inputShape is needed only for the first layer.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: inputShape 仅在第一层需要。
- en: '***3*** The last layer is hard-coded to have one unit. The single output number
    will be converted to a probability of selecting the leftward-force action.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后一层被硬编码为一个单元。单个输出数字将被转换为选择左向力动作的概率。
- en: Listing 11.2\. Getting the logits and actions from the output of the policy
    network
  id: totrans-82
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 从策略网络输出获取 logit 和动作的方法示例
- en: '[PRE2]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '***1*** Converts the logits to the probability values of the leftward action'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 logit 转换为左向动作的概率值
- en: '***2*** Calculates the probability values for both actions, as they are required
    by tf.multinomial()'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算两个动作的概率值，因为 tf.multinomial() 需要它们。
- en: '***3*** Randomly samples actions based on the probability values. The four
    arguments are probability values, number of samples, random seed (unused), and
    a flag that indicates that the probability values are normalized.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据概率值随机抽样动作。四个参数分别是概率值、抽样数量、随机种子（未使用），以及一个指示概率值是否归一化的标志。
- en: '11.2.3\. Training the policy network: The REINFORCE algorithm'
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练策略网络：REINFORCE 算法
- en: Now the key question becomes how to calculate which actions are good and which
    are bad. If we can answer this question, we’ll be able to update the weights of
    the policy network to make it more likely to pick the good actions in the future,
    in a way similar to supervised learning. What quickly comes to mind is that we
    can use the reward to measure how good the actions are. But the cart-pole problem
    involves rewards that 1) always have a fixed value (`1`) and 2) happen at every
    step as long as the episode hasn’t ended. So, we can’t simply use the step-by-step
    reward as a metric, or we’ll end up labeling all actions as equally good. We need
    to take into account how long each episode lasts.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在关键问题是如何计算哪些动作是好的，哪些是坏的。如果我们能回答这个问题，我们就能够更新策略网络的权重，使其在未来更有可能选择好的动作，这与监督学习类似。很快能想到的是我们可以使用奖励来衡量动作的好坏。但是车杆问题涉及奖励：1）总是有一个固定值（`1`）；2）只要剧集没有结束，就会在每一步发生。所以，我们不能简单地使用逐步奖励作为度量标准，否则所有动作都会被标记为同样好。我们需要考虑每个剧集持续的时间。
- en: A naive approach is to sum all the rewards in an episode, which gives us the
    length of the episode. But can the sum be a good assessment of the actions? It
    is not hard to realize that it won’t work. The reason is the steps at the end
    of an episode. Suppose in a long episode, the agent balances the cart-pole system
    quite well all the way until near the end, when it makes a few bad choices that
    cause the episode to finally end. The naive summing approach will assign equally
    good assessment to the bad actions at the end and the good ones from before. Instead,
    we want to assign higher scores to the actions in the early and middle parts of
    the episode and assign lower ones to the actions near the end.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的方法是在一个剧集中求所有奖励的总和，这给了我们剧集的长度。但是总和能否成为对动作的良好评估？很容易意识到这是不行的。原因在于剧集末尾的步骤。假设在一个长剧集中，智能体一直很好地平衡车杆系统，直到接近结束时做了一些不好的选择，导致剧集最终结束。简单的总和方法会将最后的不良动作和之前的良好动作平等评估。相反，我们希望将更高的分数分配给剧集早期和中间部分的动作，并将较低的分配给靠近结尾的动作。
- en: This brings us to the idea of *reward discounting*, a simple but important idea
    in RL that the value of a certain step should equal the immediate reward plus
    the reward that is expected for the future. The future reward may be equally as
    important as the immediate reward, or it may be less important. The relative balance
    can be quantified with a discounting factor called γ (gamma). γ is usually set
    to a value close to but slightly less than 1, such as 0.95 or 0.99\. We write
    this in a mathematical equation as
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这引出了 *奖励折扣* 的概念，一个简单但在 RL 中非常重要的概念：某一步的价值应等于即时奖励加上预期未来奖励。未来奖励可能与即时奖励同等重要，也可能不那么重要。可以通过折扣系数
    γ 来量化相对平衡。γ 通常设置为接近但略小于 1 的值，如 0.95 或 0.99。我们可以用公式表示：
- en: equation 11.1\.
  id: totrans-91
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 公式 11.1。
- en: '![](11fig04.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](11fig04.jpg)'
- en: In [equation 11.1](#ch11equ01), *v[i]* is the total discounted reward of the
    state at step *i*, which can be understood as the value of that particular state.
    It is equal to the immediate reward given to the agent at that step (*r[i]*),
    plus the reward from the next step (*r[i]*[+1]) discounted by γ, plus a further
    discounted reward from two steps later, and so forth, up to the end of the episode
    (step *N*).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [公式 11.1](#ch11equ01) 中，*v[i]* 表示第 *i* 步状态的总折扣奖励，可以理解为该特定状态的价值。它等于在该步骤给予智能体的即时奖励
    (*r[i]*)，加上下一步奖励 (*r[i]*[+1]) 乘以折扣系数 γ，再加上再后两步的折扣奖励，以此类推，直到该事件结束（第 *N* 步）。
- en: To illustrate reward discounting, we show how this equation transforms our original
    rewards to a more useful value metric in [figure 11.5](#ch11fig05). The top plot
    in panel A displays the original rewards from all four steps from a short episode.
    The bottom plot shows the discounted rewards (based on [equation 11.1](#ch11equ01)).
    Panel B shows the original and discounted total rewards from a longer episode
    (length = 20) for comparison. From the two panels, we can see that the discounted
    total reward value is higher in the beginning and lower at the end, which makes
    sense because we want to assign lower values to actions toward the end of an episode,
    which causes the game to end. Also, the values at the beginning and middle parts
    of the longer episode (panel B) are higher than those at the beginning of the
    shorter one (panel A). This also makes intuitive sense because we want to assign
    higher values to the actions that lead to longer episodes.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明奖励折扣，我们展示了这个公式如何将原始奖励转换为更有用的价值度量方式，如 [图 11.5](#ch11fig05) 所示。面板 A 的顶部图显示了来自一段短情节的所有四步原始奖励。底部图显示了根据
    [公式 11.1](#ch11equ01) 计算的折扣奖励。为了比较，面板 B 显示了来自长度为 20 的更长情节的原始和折扣总奖励。从两个面板可以看出，折扣总奖励值在开头较高，在结尾较低，这是有意义的，因为我们要为一个游戏结束的动作分配较低的值。此外，长情节的开头和中段的值（面板
    B）高于短情节的开头（面板 A）。这也是有意义的，因为我们要为导致更长情节的动作分配更高的值。
- en: 'Figure 11.5\. Panel A: applying reward discounting ([equation 11.1](#ch11equ01))
    on rewards from an episode with four steps. Panel B: same as panel A, but from
    an episode with 20 steps (that is, five times longer than the one in panel A).
    As a result of the discounting, higher values are assigned to the actions in the
    beginning of each episode compared to the actions near the end.'
  id: totrans-95
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.5。面板 A：对四步情节的奖励进行奖励折扣（[公式 11.1](#ch11equ01)）。面板 B：与面板 A 相同，但来自一个包含 20 步的情节（即比面板
    A 的情节长五倍）。由于折扣，与靠近结尾的动作相比，为每个情节的开始动作分配更高的值。
- en: '![](11eqa01_alt.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](11eqa01_alt.jpg)'
- en: The reward-discounting equation gives us a set of values that make more sense
    than the naive summing before. But we are still faced with the question of how
    to use these discounted reward values to train the policy network. For that, we
    will use an algorithm called REINFORCE, invented by Ronald Williams in 1992.^([[6](#ch11fn6)])
    The basic idea behind REINFORCE is to adjust the weights of the policy network
    to make it more likely to make good choices (choices assigned higher discounted
    rewards) and less likely to make bad choices (the ones assigned lower discounted
    rewards).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励折扣公式为我们提供了一组比单纯地求和更有意义的值。但我们仍然面临着如何使用这些折扣奖励价值来训练策略网络的问题。为此，我们将使用一种名为 REINFORCE
    的算法，该算法由罗纳德·威廉姆斯在 1992 年发明。^([[6](#ch11fn6)]) REINFORCE 的基本思想是调整策略网络的权重，使其更有可能做出良好的选择（选择分配更高的折扣奖励）并减少做出不良选择（分配更低的折扣奖励）。
- en: ⁶
  id: totrans-98
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁶
- en: ''
  id: totrans-99
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Ronald J. Williams, “Simple Statistical Gradient-Following Algorithms for Connectionist
    Reinforcement Learning,” *Machine Learning*, vol. 8, nos. 3–4, pp. 229–256, [http://mng.bz/WOyw](http://mng.bz/WOyw).
  id: totrans-100
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Ronald J. Williams，“Simple Statistical Gradient-Following Algorithms for Connectionist
    Reinforcement Learning,” *Machine Learning*, vol. 8, nos. 3–4, pp. 229–256, [http://mng.bz/WOyw](http://mng.bz/WOyw).
- en: To this end, we need to calculate the direction in which to change the parameters
    to make an action more likely given the observation inputs. This is done with
    the code in [listing 11.3](#ch11ex03) (excerpted from cart-pole/index.js). The
    function `getGradientsAndSaveActions()` is invoked at every step of the game.
    It compares the logits (unnormalized probability scores) and the actual action
    selected at the step and returns the gradient of the discrepancy between the two
    with respect to the policy network’s weights. This may sound complicated, but
    intuitively, it’s fairly straightforward. The returned gradient tells the policy
    network how to change its weights so as to make the choices more like the choices
    that were actually selected. The gradients, together with the rewards from the
    training episodes, form the basis of our RL method. This is why this method belongs
    to the family of RL algorithms called *policy gradients*.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为了达到此目的，我们需要计算改变参数的方向，以使给定观察输入更有可能进行动作。这是通过 [代码清单 11.3](#ch11ex03)（摘自 cart-pole/index.js）实现的。函数
    `getGradientsAndSaveActions()` 在游戏的每个步骤中被调用。它比较逻辑回归（未归一化的概率得分）和该步骤选择的实际动作，并返回相对于策略网络权重的两者不一致性的梯度。这可能听起来很复杂，但直观上是相当简单的。返回的梯度告诉策略网络如何更改其权重，以使选择更类似于实际选择。这些梯度与训练集的奖励一起构成了我们强化学习方法的基础。这就是为什么该方法属于被称为
    *策略梯度* 的强化学习算法家族的原因。
- en: Listing 11.3\. Comparing logits and actual actions to obtain gradients for weights
  id: totrans-102
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 代码清单 11.3 通过比较逻辑回归和实际动作来获取权重的梯度。
- en: '[PRE3]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '***1*** getLogitsAndActions() is defined in [listing 11.2](#ch11ex02).'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** `getLogitsAndActions()` 在 [代码清单 11.2](#ch11ex02) 中定义。'
- en: '***2*** The sigmoid cross-entropy loss quantifies the discrepancy between the
    actual action made during the game and the policy network’s output logits.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** sigmoid 交叉熵损失量化其在游戏中实际执行的动作与策略网络输出的逻辑回归之间的差异。'
- en: '***3*** Calculates the gradient of the loss with respect to the policy network’s
    weights'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 计算损失相对于策略网络权重的梯度。'
- en: 'During training, we let the agent play a number of games (say, *N* games) and
    collect all the discounted rewards according to [equation 11.1](#ch11equ01), as
    well as the gradients from all the steps. Then, we combine the discounted rewards
    and gradients by multiplying the gradients with a normalized version of the discounted
    rewards. The reward normalization here is an important step. It linearly shifts
    and scales all the discounted rewards from the *N* games so that they have an
    overall mean value of 0 and overall standard deviation of 1\. An example of applying
    this normalization on the discounted rewards is shown in [figure 11.6](#ch11fig06).
    It illustrates the normalized, discounted rewards from a short episode (length
    = 4) and a longer one (length = 20). From this figure, it should be clear what
    steps are favored by the REINFORCE algorithm: they are the actions made in the
    early and middle parts of longer episodes. By contrast, all the steps from the
    shorter (length-4) episode are assigned *negative* values. What does a negative
    normalized reward mean? It means that when it is used to update the policy network’s
    weights later, it will steer the network *away* from making a similar choice of
    actions given similar state inputs in the future. This is in contrast to a positive
    normalized reward, which will steer the policy network *toward* choosing similar
    actions given similar inputs in the future.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练期间，我们让代理对象玩一些游戏（比如 *N* 个游戏），并根据 [方程式 11.1](#ch11equ01) 收集所有折扣奖励以及所有步骤中的梯度。然后，我们通过将梯度与折扣奖励的归一化版本相乘来结合折扣奖励和梯度。奖励归一化在这里是一个重要的步骤。它线性地转移和缩放了
    *N* 个游戏中所有折扣奖励，使得它们的总体均值为 0 和总体标准偏差为 1。[图 11.6](#ch11fig06) 显示了在折扣奖励上应用此归一化的示例。它说明了短剧集（长度为
    4）和较长剧集（长度为 20）的归一化、折扣奖励。从这张图中可以明确 REINFORCE 算法所偏向的步骤是什么：它们是较长剧集的早期和中间部分的动作。相比之下，所有来自较短（长度为
    4）剧集的步骤都被赋予 *负* 值。负的归一化奖励意味着什么？这意味着当它用于稍后更新策略网络的权重时，它将使网络远离未来给定相似状态输入时进行类似动作的选择。这与正的归一化奖励相反，后者将使策略网络向未来在类似的输入条件下做出相似的动作方向
- en: Figure 11.6\. Normalizing the discounted rewards from the two episodes with
    lengths 4 (panel A) and 20 (panel B). We can see that the normalized, discounted
    rewards have the highest values at the beginning of the length-20 episode. The
    policy gradient method will use these discounted reward values to update the weights
    of the policy network, which will make the network less likely to make the action
    choices that resulted in the bad rewards in the first case (length = 4) and more
    likely to make the choices that resulted in the good rewards in the beginning
    part of the second case (length = 20) (given the same state inputs as before,
    that is).
  id: totrans-108
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.6\. 对两个长度为 4（面板 A）和 20（面板 B）的情节中的折现奖励进行归一化。我们可以看到，归一化的折现奖励在长度为 20 的情节开始部分具有最高值。策略梯度方法将使用这些折现奖励值来更新策略网络的权重，这将使网络更不可能选择导致第一个情节（长度
    = 4）中不良奖励的动作选择，并且更有可能选择导致第二个情节开始部分（长度 = 20）中良好奖励的选择（在相同的状态输入下，即）。
- en: '![](11fig05_alt.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](11fig05_alt.jpg)'
- en: The code for the normalization of the discounted rewards, and using it to scale
    the gradients, is somewhat tedious but not complicated. It is in the `scaleAndAverageGradients()`
    function in cart-pole/index.js, which is not listed here for the sake of brevity.
    The scaled gradients are used to update the policy network’s weights. With the
    weights updated, the policy network will output higher logits for the actions
    from the steps assigned higher discounted rewards and lower logits for the actions
    from the steps assigned lower ones.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 对折现奖励进行归一化，并使用它来缩放梯度的代码有些冗长但不复杂。它在 cart-pole/index.js 中的 `scaleAndAverageGradients()`
    函数中，由于篇幅限制这里不列出。缩放后的梯度用于更新策略网络的权重。随着权重的更新，策略网络将对从分配了更高折现奖励的步骤中的动作输出更高的 logits，并对从分配了较低折现奖励的步骤中的动作输出较低的
    logits。
- en: 'That is basically how the REINFORCE algorithm works. The core training logic
    of the cart-pole example, which is based on REINFORCE, is shown in [listing 11.4](#ch11ex04).
    It is a reiteration of the steps described previously:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上就是 REINFORCE 算法的工作原理。基于 REINFORCE 的 cart-pole 示例的核心训练逻辑显示在 [列表 11.4](#ch11ex04)
    中。它是前面描述的步骤的重述：
- en: Invoke the policy network to get logits based on current agent observation.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用策略网络以基于当前代理观察获得 logits。
- en: Randomly sample an action based on the logits.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于 logits 随机采样一个动作。
- en: Update the environment using the sampled action.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用采样的动作更新环境。
- en: 'Remember the following for updating weights later (step 7): the logits and
    the selected action, as well as the gradients of the loss function with respect
    to the policy network’s weights. These gradients are referred to as *policy gradients*.'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记住以下内容以备后续更新权重（步骤 7）：logits 和所选动作，以及损失函数相对于策略网络权重的梯度。这些梯度被称为 *策略梯度*。
- en: Receive a reward from the environment, and remember it for later (step 7).
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从环境中接收奖励，并将其记住以备后用（步骤 7）。
- en: Repeat steps 1–5 until `numGames` episodes are completed.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤 1–5 直到完成 `numGames` 情节。
- en: Once all `numGames` episodes have ended, discount and normalize the rewards
    and use the results to scale the gradients from step 4\. Then update the policy
    network’s weights using the scaled gradients. (This is where the policy network’s
    weights get updated.)
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦所有 `numGames` 情节结束，对奖励进行折扣和归一化，并使用结果来缩放步骤 4 中的梯度。然后使用缩放后的梯度来更新策略网络的权重。（这是策略网络的权重被更新的地方。）
- en: (Not shown in [listing 11.4](#ch11ex04)) Repeat steps 1–7 `numIterations` times.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: （未在 [列表 11.4](#ch11ex04) 中显示）重复步骤 1–7 `numIterations` 次。
- en: Compare these steps with the code in the listing (excerpted from cart-pole/index.js)
    to make sure you can see the correspondence and follow the logic.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些步骤与代码中的步骤进行比较（从 cart-pole/index.js 中摘录），以确保您能够看到对应关系并按照逻辑进行。
- en: Listing 11.4\. Cart-pole example’s training loop implementing the REINFORCE
    algorithm
  id: totrans-121
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.4\. Cart-pole 示例中实现 REINFORCE 算法的训练循环
- en: '[PRE4]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '***1*** Loops over specified number of episodes'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 循环指定次数的情节'
- en: '***2*** Randomly initializes a game episode'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 随机初始化一个游戏情节'
- en: '***3*** Loops over steps of the game'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 循环游戏的步骤'
- en: '***4*** Keeps track of the gradients from every step for later REINFORCE training'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 跟踪每步的梯度以备后续 REINFORCE 训练'
- en: '***5*** The agent takes an action in the environment.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 代理在环境中采取一个动作。'
- en: '***6*** As long as the game hasn’t ended, the agent receives a unit reward
    per step.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 只要游戏尚未结束，代理每步都会获得一个单位奖励。'
- en: '***7*** Discounts and normalizes the rewards (key step of REINFORCE)'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7*** 对奖励进行折扣和归一化（REINFORCE 的关键步骤）'
- en: '***8*** Updates the policy network’s weights using the scaled gradients from
    all steps'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8*** 使用来自所有步骤的缩放梯度更新策略网络的权重'
- en: To see the REINFORCE algorithm in action, specify 25 epochs on the demo page
    and click the Train button. By default, the state of the environment is displayed
    in real time during training so that you can see repeated tries by the learning
    agent. To speed up the training, uncheck the Render During Training check box.
    Twenty-five epochs of training will take a few minutes on a reasonably up-to-date
    laptop and should be sufficient to achieve ceiling performance (500 steps per
    game episode in the default setting). [Figure 11.7](#ch11fig07) shows a typical
    training curve, which plots the average episode length as a function of the training
    iteration. Notice that the training progress shows some dramatic fluctuation,
    with the mean number of steps changing in a nonmonotonic and highly noisy fashion
    over the iterations. This type of fluctuation is not uncommon in RL training jobs.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 要看到 REINFORCE 算法的运行情况，请在演示页面上指定 25 个时期，并单击“训练”按钮。默认情况下，训练期间实时显示环境的状态，以便您可以看到学习代理的重复尝试。要加快训练速度，请取消选中“训练期间渲染”复选框。在合理更新的笔记本电脑上，25
    个时期的训练需要几分钟，并且应足以达到天花板性能（默认设置下游戏每轮 500 步）。[图 11.7](#ch11fig07) 显示了典型的训练曲线，该曲线将平均每轮长度作为训练迭代的函数绘制出来。请注意，训练进度显示出一些戏剧性的波动，平均步数随着迭代次数以非单调和高度嘈杂的方式变化。这种波动在强化学习训练工作中并不罕见。
- en: Figure 11.7\. A curve showing the average number of steps the agent survives
    in the cart-pole episodes as a function of the number of training iterations.
    The perfect score (500 steps in this case) is attained at around iteration 20\.
    This result is obtained with a hidden layer size of 128\. The highly nonmonotonic
    and fluctuating shape of the curve is not uncommon among RL problems.
  id: totrans-132
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.7\. 一条曲线显示了智能体在车杆问题的每个训练迭代中生存的平均步数与训练迭代次数的关系。在约第 20 次迭代时达到完美分数（在本例中为 500
    步）。这个结果是在隐藏层大小为 128 的情况下获得的。曲线的高度非单调和波动形状在强化学习问题中并不罕见。
- en: '![](11fig06_alt.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](11fig06_alt.jpg)'
- en: After the training completes, click the Test button, and you should see the
    agent do a good job keeping the cart-pole system balanced over many steps. Since
    the testing phase doesn’t involve a maximum number of steps (500 by default),
    it is possible that the agent can keep the episode going for over 1,000 steps.
    If it continues too long, you can click the Stop button to terminate the simulation.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，单击“测试”按钮，您应该会看到代理在许多步骤上很好地保持车杆系统平衡的表现。由于测试阶段不涉及最大步数（默认为 500 步），因此代理可以保持游戏进行超过
    1,000 步。如果持续时间过长，您可以单击“停止”按钮终止模拟。
- en: To wrap up this section, [figure 11.8](#ch11fig08) recapitulates the formulation
    of the problem and the role of the REINFORCE policy-gradient algorithm. All major
    parts of the solution are depicted in this figure. At each step, the agent uses
    a neural network called the *policy network* to estimate the likelihood that the
    leftward action (or, equivalently, the rightward one) is the better choice. This
    likelihood is converted into an actual action through a random sampling process
    that encourages the agent to explore early on and obeys the certainty of the estimates
    later. The action drives the cart-pole system in the environment, which in turn
    provides the agent with rewards until the end of the episode. This process repeats
    a number of episodes, during which the REINFORCE algorithm remembers the reward,
    the action, and the policy network’s estimate at every step. When it’s time for
    REINFORCE to update the policy network, it distinguishes good estimates from the
    network from bad ones through reward discounting and normalization, and then uses
    the results to nudge the network’s weights in the direction of making better estimates
    in the future. This process iterates a number of times until the end of the training
    (for instance, when the agent reaches a threshold performance).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 总结这一节，[图11.8](#ch11fig08)概括了问题的表述以及REINFORCE策略梯度算法的作用。这张图展示了解决方案的所有主要部分。在每个步骤中，代理使用一个名为*策略网络*的神经网络来估计向左行动（或等效地，向右行动）是更好的选择的可能性。这种可能性通过一个随机抽样过程转换为实际行动，该过程鼓励代理早期探索并在后期遵守估计的确定性。行动驱动环境中的车杆系统，该系统反过来为代理提供奖励，直到本集的结束。这个过程重复了多个集，期间REINFORCE算法记住了每一步的奖励、行动和策略网络的估计。当REINFORCE需要更新策略网络时，它通过奖励折现和归一化区分网络中的好估计和坏估计，然后使用结果来推动网络的权重朝着未来做出更好的估计。这个过程迭代了多次，直到训练结束（例如，当代理达到阈值性能时）。
- en: Figure 11.8\. A schematic diagram illustrating the REINFORCE algorithm-based
    solution to the cart-pole problem. This diagram is an expanded view of the diagram
    in [figure 11.4](#ch11fig04).
  id: totrans-136
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.8\. 展示了基于REINFORCE算法的解决方案对车杆问题的示意图。该图是[图11.4](#ch11fig04)中图示的扩展视图。
- en: '![](11fig07_alt.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](11fig07_alt.jpg)'
- en: 'All the elegant technical details aside, let’s take a step back and look at
    the bigger picture of RL embodied in this example. The RL-based approach has clear
    advantages over non-machine-learning methods such as traditional control theory:
    the generality and the economy of human effort. In cases where the system has
    complex or unknown characteristics, the RL approach may be the only viable solution.
    If the characteristics of the system change over time, we won’t have to derive
    new mathematical solutions from scratch: we can just re-run the RL algorithm and
    let the agent adapt itself to the new situation.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 抛开所有优雅的技术细节，让我们退后一步，看一看这个例子中体现的RL的大局。基于RL的方法相对于非机器学习方法（如传统控制理论）具有明显的优势：普适性和人力成本的经济性。在系统具有复杂或未知特性的情况下，RL方法可能是唯一可行的解决方案。如果系统的特性随时间变化，我们不必从头开始推导新的数学解：我们只需重新运行RL算法，让代理适应新情况。
- en: The disadvantage of the RL approach, which is still an unsolved question in
    the field of RL research, is that it requires many repeated trials in the environment.
    In the case of the cart-pole example, it took about 400 game episodes to reach
    the target level of proficiency. Some traditional, non-RL approaches may require
    no trial at all. Implement the control-theory-based algorithm, and the agent should
    be able to balance the pole from episode 1\. For a problem like cart-pole, RL’s
    hunger for repeated trials is not a major problem because the computer simulation
    of the environment is simple, fast, and cheap. However, in more realistic problems,
    such as self-driving cars and object-manipulating robot arms, this problem of
    RL becomes a more acute and pressing challenge. No one can afford crashing a car
    or breaking a robotic arm hundreds or thousands of times in order to train an
    agent, not to mention the prohibitively long time it would take to run the RL
    training algorithm in such real-world problems.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: RL 方法的劣势，这仍然是 RL 研究领域中一个未解决的问题，是它需要在环境中进行许多次重复试验。在车杆示例中，大约需要 400 个游戏回合才能达到目标水平的熟练程度。一些传统的、非
    RL 方法可能根本不需要试验。实施基于控制理论的算法，代理应该能够从第 1 个回合就平衡杆子。对于像车杆这样的问题，RL 对于重复试验的渴望并不是一个主要问题，因为计算机对环境的模拟是简单、快速和廉价的。然而，在更现实的问题中，比如自动驾驶汽车和物体操纵机器臂，RL
    的这个问题就变得更加尖锐和紧迫。没有人能承担在训练代理时多次撞车或者摧毁机器臂的成本，更不用说在这样的现实问题中运行 RL 训练算法将需要多么长的时间。
- en: This concludes our first RL example. The cart-pole problem has some special
    characteristics that don’t hold in other RL problems. For example, many RL environments
    don’t provide a positive reward to the agent at every step. In some situations,
    the agent may need to make dozens of decisions, if not more, before it can be
    rewarded positively. In the gaps between the positive rewards, there may be no
    reward, or there may be only negative rewards (it can be argued that many real-world
    endeavors, such as studying, exercising, and investing, are like that!). In addition,
    the cart-pole system is “memoryless” in the sense that the dynamics of the system
    don’t depend on what the agent did in the past. Many RL problems are more complex
    than that, in that the agent’s action changes certain aspects of the environment.
    The RL problem we’ll study in the next section will show both sparse positive
    rewards and an environment that changes with action history. To tackle the problem,
    we’ll introduce another useful and popular RL algorithm, called *deep* *Q-learning*.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们的第一个 RL 示例。车杆问题具有一些特殊的特征，在其他 RL 问题中不适用。例如，许多 RL 环境并不会在每一步向代理提供正面奖励。在某些情况下，代理可能需要做出几十个甚至更多的决策，才能获得积极的奖励。在正面奖励之间的空隙中，可能没有奖励，或者只有负面奖励（可以说很多现实生活中的努力，比如学习、锻炼和投资，都是如此！）。此外，车杆系统在“无记忆”方面是“无记忆”的，即系统的动态不取决于代理过去的行为。许多
    RL 问题比这更复杂，因为代理的行为改变了环境的某些方面。我们将在下一节中研究的 RL 问题将展示稀疏的正面奖励和一个随着行动历史而变化的环境。为了解决这个问题，我们将介绍另一个有用且流行的
    RL 算法，称为 *deep* *Q-learning*。
- en: '11.3\. Value networks and Q-learning: The snake game example'
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3\. 价值网络和 Q 学习：蛇游戏示例
- en: We will use the classic action game called *snake* as our example problem to
    cover deep Q-learning. As we did in the last section, we’ll first describe the
    RL problem and the challenge it poses. In doing so, we’ll also discuss why policy
    gradients and REINFORCE won’t be very effective on this problem.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用经典的动作游戏 *snake* 作为我们深度 Q 学习的示例问题。就像我们在上一节中所做的那样，我们将首先描述 RL 问题及其带来的挑战。在这样做的过程中，我们还将讨论为什么策略梯度和
    REINFORCE 对这个问题不会非常有效。
- en: 11.3.1\. Snake as a reinforcement-learning problem
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.3.1\. 蛇作为一个强化学习问题
- en: First appearing in 1970s arcade games, snake has become a well-known video game
    genre. The snake-dqn directory in tfjs-examples contains a JavaScript implementation
    of a simple variant of it. You can check out the code with
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 蛇游戏首次出现在 1970 年代的街机游戏中，已经成为一个广为人知的视频游戏类型。tfjs-examples 中的 snake-dqn 目录包含一个简单变体的
    JavaScript 实现。您可以通过以下代码查看：
- en: '[PRE5]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In the web page opened by the `yarn watch` command, you can see a board of the
    snake game. You can load a pretrained and hosted deep Q-network (DQN) model and
    observe it play the game. Later, we’ll talk about how you can train such a model
    from scratch. For now, you should be able to get an intuitive sense of how this
    game works through observing. In case you aren’t already familiar with the snake
    game, its settings and rules can be summarized as follows.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在由`yarn watch`命令打开的网页中，你可以看到贪吃蛇游戏的棋盘。你可以加载一个预先训练并托管的深度Q网络（DQN）模型，并观察它玩游戏。稍后，我们将讨论如何从头开始训练这样的模型。现在，通过观察，你应该能直观地感受到这款游戏是如何运行的。如果你还不熟悉贪吃蛇游戏，它的设置和规则可以总结如下。
- en: 'First, all actions happen in a 9 × 9 grid world (see an example in [figure
    11.9](#ch11fig09)). The world (or board) can made be larger, but 9 × 9 is the
    default size in our example. There are three types of squares on the board: the
    snake, the fruit, and the empty space. The snake is represented by blue squares,
    except the head, which is colored orange with a semicircle representing the snake’s
    mouth. The fruit is represented by a green square with a circle inside. The empty
    squares are white. The game happens in steps—or, in video game terminology, *frames*.
    At each frame, the agent must choose from three possible actions for the snake:
    go straight, turn left, or turn right (staying put is not an option). The agent
    is rewarded positively when the head of the snake comes into contact with a fruit
    square, in which case the fruit square will disappear (get “eaten” by the snake),
    and the length of the snake will increase by one at the tail. A new fruit will
    appear in one of the empty squares. The agent will be rewarded negatively if it
    doesn’t eat a fruit at a step. The game terminates (the snake “dies”) when the
    head of the snake goes out of bounds (as in panel B of [figure 11.9](#ch11fig09))
    or runs into its own body (as in panel C).'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，所有动作发生在一个9×9的网格世界中（参见[图11.9](#ch11fig09)的例子）。世界（或棋盘）可以设得更大，但在我们的例子中，9×9是默认大小。棋盘上有三种类型的方块：蛇、果子和空白。蛇由蓝色方块表示,
    只有头部是橙色的，并带有半圆形代表蛇的嘴巴。果子由内部有圆圈的绿色方块表示。空白方块是白色的。游戏按步骤进行，或者按视频游戏术语来说是*帧*。在每一帧中，代理必须从三个可能的动作中为蛇选择：直行、左转或右转（原地不动不是选项）。当蛇的头部与果子方块接触时，代理被奖励呈积极反应，这种情况下果子方块将消失（被蛇“吃掉”），蛇的长度会在尾部增加一个。一个新的果子将出现在空白方块中。如果代理在某一步没有吃到果子，它将受到负奖励。游戏终止（蛇“死亡”）是指当蛇的头部离开边界（如[图11.9](#ch11fig09)的面板B）或撞到自己的身体时。
- en: 'Figure 11.9\. The snake game: a grid world in which the player controls a snake
    to eat fruit. The snake’s “goal” is to eat as many fruits as possible through
    an efficient movement pattern (panel A). The length of the snake grows by 1 every
    time a fruit is eaten. The game ends (the snake “dies”) as soon as the snake goes
    out of bounds (panel B) or bumps into its own body (panel C). Note that in panel
    B, the snake’s head reaches the edge position, and then an upward motion (a go-straight
    action) ensues that causes the game to terminate. Simply reaching the edge squares
    with the snake’s head won’t result in termination. Eating every fruit leads to
    a large positive reward. Moving one square without eating a fruit incurs a negative
    reward that is smaller in magnitude. Game termination (the snake dying) also incurs
    a negative reward.'
  id: totrans-148
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.9\. 贪吃蛇游戏：一个网格世界, 玩家控制蛇吃果子。蛇的“目标”是通过有效的移动模式尽可能多地吃果子（面板A）。每次吃一个果子蛇的长度增加1。游戏结束（蛇“死掉”）是当蛇离开边界（面板B）或撞到自己的身体（面板C）时。注意，在面板B中，蛇的头部到达边缘位置，然后发生了向上的运动（直行动作），导致游戏终止。简单到达边缘方块并不会导致终止。吃掉每个果子会导致一个很大的正奖励。在没有吃果子的情况下移动一个方块会导致一个较小幅度的负奖励。游戏终止（蛇死亡）也会导致一个负奖励。
- en: '![](11fig08_alt.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](11fig08_alt.jpg)'
- en: One key challenge in the snake game is the snake’s growth. If not for this rule,
    the game would be much simpler. Simply navigate the snake to the fruit over and
    over, and there’s no limit to the reward the agent can get. With the length-growth
    rule, however, the agent must learn to avoid bumping into its own body, which
    gets harder as the snake eats more fruit and grows longer. This is the nonstatic
    aspect of the snake RL problem that the cart-pole environment lacks, as we mentioned
    at the end of the last section.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 蛇游戏中的一个关键挑战是蛇的增长。如果没有这个规则，游戏会简单得多。只需一遍又一遍地将蛇导航到水果，智能体可以获得无限的奖励。然而，有了长度增长规则，智能体必须学会避免撞到自己的身体，随着蛇吃更多的水果和变得更长，这变得更加困难。这是蛇
    RL 问题的非静态方面，推车杆环境所缺乏的，正如我们在上一节末尾提到的。
- en: '[Table 11.2](#ch11table02) describes the snake problem in the canonical RL
    formulation. Compared with the formulation of the cart-pole problem ([table 11.1](#ch11table01)),
    the biggest difference is in the reward structure. In the snake problem, the positive
    rewards (+10 for each fruit eaten) come infrequently—that is, only after a number
    of negative rewards due to the movement the snake needs to reach the fruit. Given
    the size of the board, two positive rewards may be spaced out by as much as 17
    steps even if the snake moves in the most efficient manner. The small negative
    reward is a penalty that encourages the snake to move in a more straightforward
    path. Without this penalty, the snake can move in a meandering and indirect way
    and still receive the same rewards, which will make the gameplay and training
    process unnecessarily long. This sparse and complex reward structure is also the
    main reason why the policy gradient and REINFORCE method will not work well on
    this problem. The policy-gradient method works better when the rewards are frequent
    and simple, as in the cart-pole problem.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[表11.2](#ch11table02) 在经典 RL 表述中描述了蛇问题。与推车杆问题的表述（[表11.1](#ch11table01)）相比，最大的区别在于奖励结构。在蛇问题中，正奖励（每吃一颗水果+10）出现不频繁——也就是说，只有在蛇移动到达水果后，经历了一系列负奖励后才会出现。考虑到棋盘的大小，即使蛇以最有效的方式移动，两个正奖励之间的间隔也可能长达17步。小的负奖励是一个惩罚，鼓励蛇走更直接的路径。没有这个惩罚，蛇可以以蜿蜒的间接方式移动，并且仍然获得相同的奖励，这将使游戏和训练过程不必要地变长。这种稀疏而复杂的奖励结构也是为什么策略梯度和
    REINFORCE 方法在这个问题上效果不佳的主要原因。策略梯度方法在奖励频繁且简单时效果更好，就像推车杆问题一样。'
- en: Table 11.2\. Describing the snake-game problem in the canonical RL formulation
  id: totrans-152
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表11.2\. 在经典 RL 表述中描述蛇游戏问题
- en: '| Abstract RL concept | Realization in the snake problem |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 抽象 RL 概念 | 在蛇问题中的实现 |'
- en: '| --- | --- |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Environment | A grid world that contains a moving snake and a self-replenishing
    fruit. |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 环境 | 一个包含移动蛇和自我补充水果的网格世界。 |'
- en: '| Action | (Discrete) Ternary choice: go straight, turn left, or turn right.
    |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 动作 | (离散) 三元选择：直行，左转，或右转。 |'
- en: '| Reward | (Frequent, mixed positive negative rewards)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '| 奖励 | (频繁，混合正负奖励)'
- en: Eating fruit—Large positive reward (+10)
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吃水果——大正奖励 (+10)
- en: Moving without eating fruit—Small negative reward (–0.2)
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移动而不吃水果——小负奖励 (–0.2)
- en: Dying—Large negative reward (–10)
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 死亡——大负奖励 (–10)
- en: '|'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Observation | (Complete state, discrete) At each step, the agent can access
    the full state of the game: that is, what is in every square of the board. |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 观测 | (完整状态，离散) 每一步，智能体可以访问游戏的完整状态：即棋盘上每个方块的内容。 |'
- en: The JavaScript API of snake
  id: totrans-163
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 蛇的 JavaScript API
- en: 'Our JavaScript implementation of snake can be found in the file snake-dqn/snake_
    game.js. We will describe only the API of the `SnakeGame` class and spare you
    the implementation details, which you can study at your own pleasure if they are
    of interest to you. The constructor of the `SnakeGame` class has the following
    syntax:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 JavaScript 实现可以在文件 snake-dqn/snake_ game.js 中找到。我们只会描述`SnakeGame`类的 API，并略过实现细节，如果你感兴趣，可以自行学习。`SnakeGame`类的构造函数具有以下语法：
- en: '[PRE6]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here, the size parameters of the board, `height` and `width`, have default values
    of 9\. `numFruits` is the number of fruits present on the board at any given time;
    it has a default value of 1\. `initLen`, the initial length of the snake, defaults
    to 2.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，棋盘的大小参数，`height`和`width`，默认值为9。`numFruits`是棋盘上任意给定时间存在的水果数量，默认值为1。`initLen`，蛇的初始长度，默认值为2。
- en: 'The `step()` method exposed by the `game` object allows the caller to play
    one step in the game:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '`game`对象暴露的`step()`方法允许调用者在游戏中执行一步：'
- en: '[PRE7]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The argument to the `step()` method represents the action: 0 for going straight,
    1 for turning left, and 2 for turning right. The return of the `step()` value
    has the following fields:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`step()` 方法的参数表示动作：0 表示直行，1 表示向左转，2 表示向右转。`step()` 方法的返回值具有以下字段：'
- en: '`state`—The new state of the board immediately after the action, represented
    as a plain JavaScript object with two fields:'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`state`—动作后立即棋盘的新状态，表示为具有两个字段的普通 JavaScript 对象：'
- en: '`s`—The squares occupied by the snake, as an array of `[x, y]` coordinates.
    The elements of this array are ordered such that the first element corresponds
    to the head and the last element to the tail.'
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`s`—蛇占据的方块，以 `[x, y]` 坐标数组形式表示。此数组的元素按照头部对应第一个元素，尾部对应最后一个元素的顺序排列。'
- en: '`f`—The `[x, y]` coordinates of the square(s) occupied by the fruit(s).Note
    that this representation of the game state is designed to be efficient, which
    is necessitated by the Q-learning algorithm’s storage of a large number (for example,
    tens of thousands) of such state objects, as we will soon see. An alternative
    is to use an array or nested array to record the status of every square of the
    board, including the empty ones. This would be much less space-efficient.'
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`f`—水果占据的方块的 `[x, y]` 坐标。请注意，此游戏状态的表示设计为高效，这是由 Q 学习算法存储大量（例如，成千上万）这样的状态对象所必需的，正如我们很快将看到的。另一种方法是使用数组或嵌套数组来记录棋盘上每个方块的状态，包括空的方块。这将是远不及空间高效的方法。'
- en: '`reward`—The reward given to the snake at the step, immediately after the action
    takes place. This is a single number.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reward`—蛇在步骤中立即执行动作后获得的奖励。这是一个单一数字。'
- en: '`done`—A Boolean flag indicating whether the game is over immediately after
    the action takes place.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`done`—一个布尔标志，指示游戏在动作发生后是否立即结束。'
- en: '`fruitEaten`—A Boolean flag indicating whether a fruit was eaten by the snake
    in the step as a result of the action. Note that this field is partly redundant
    with the `reward` field because we can infer from `reward` whether a fruit was
    eaten. It is included for simplicity and to decouple the exact values of the rewards
    (which may be tunable hyperparameters) from the binary event of fruit eaten versus
    fruit not eaten.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fruitEaten`—一个布尔标志，指示蛇在动作中是否吃到了水果。请注意，这个字段部分冗余于 `reward` 字段，因为我们可以从 `reward`
    推断出是否吃到了水果。它包含在内是为了简单起见，并将奖励的确切值（可能是可调节的超参数）与水果被吃与未被吃的二进制事件解耦。'
- en: As we will see later, the first three fields (`state`, `reward`, and `done`)
    will play important roles in the Q-learning algorithm, while the last field (`fruitEaten`)
    is mainly for monitoring.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将在稍后看到的，前三个字段（`state`、`reward` 和 `done`）在 Q 学习算法中将发挥重要作用，而最后一个字段（`fruitEaten`）主要用于监视。
- en: 11.3.2\. Markov decision process and Q-values
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.3.2\. 马尔可夫决策过程和 Q 值
- en: 'To explain the deep Q-learning algorithm we will apply on the snake problem,
    it is necessary to first go a little abstract. In particular, we will introduce
    the *Markov decision process* (MDP) and its underlying math at a basic level.
    Don’t worry: we’ll use simple and concrete examples and tie the concepts to the
    snake problem we have at hand.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 要解释我们将应用于蛇问题的深度 Q 学习算法，首先需要有点抽象。特别是，我们将以基本水平介绍*马尔可夫决策过程*（MDP）及其基本数学。别担心：我们将使用简单具体的示例，并将概念与我们手头的蛇问题联系起来。
- en: 'From the viewpoint of MDP, the history of an RL environment is a sequence of
    transitions through a finite set of discrete states. In addition, the transitions
    between the states follow a particular type of rule:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 从 MDP 的视角看，RL 环境的历史是通过有限数量的离散状态的一系列转换。此外，状态之间的转换遵循一种特定类型的规则：
- en: '*The state of the environment at the next step is determined completely by
    the state and the action taken by the agent at the current step.*'
  id: totrans-180
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*下一步环境的状态完全由代理在当前步骤采取的状态和动作决定。*'
- en: 'The key is that the next state depends on *only* two things: the current state
    and the action taken, and nothing more. In other words, MDP assumes that your
    history (how you got to your current state) is irrelevant to deciding what you
    should do next. It is a powerful simplification that makes the problem more tractable.
    What is a *non-Markov decision process*? That would be a case in which the next
    state depends on not only the current state and the current action but also the
    states or actions at earlier steps, potentially going all the way back to the
    beginning of the episode. In the non-Markov scenario, the math would be much more
    complex, and a much greater amount of computational resources would be required
    to solve the math.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 关键是下一个状态仅取决于两件事：当前状态和采取的动作，而不是其他。换句话说，MDP 假设你的历史（你如何到达当前状态）与决定下一步该做什么无关。这是一个强大的简化，使问题更易处理。什么是
    *非马尔可夫决策过程*？这将是一种情况，即下一个状态不仅取决于当前状态和当前动作，还取决于先前步骤的状态或动作，可能一直追溯到情节开始。在非马尔可夫情况下，数学会变得更加复杂，解决数学问题需要更多的计算资源。
- en: The MDP requirement makes intuitive sense for a lot of RL problems. A game of
    chess is a good example of this. At any step of the game, the board configuration
    (plus which player’s turn it is) fully characterizes the game state and provides
    all the information the player needs for calculating the next move. In other words,
    it is possible to resume a chess game from the board configuration without knowing
    the previous moves. (Incidentally, this is why newspapers can post chess puzzles
    in a very space-efficient way.) Video games such as snake are also consistent
    with the MDP formulation. The positions of the snake and the fruit on the board
    fully characterize the game state and are all it takes to resume the game from
    that point or for the agent to decide the next action.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多强化学习问题来说，马尔可夫决策过程的要求是直观的。象棋游戏是一个很好的例子。在游戏的任何一步中，棋盘配置（以及轮到哪个玩家）完全描述了游戏状态，并为玩家提供了计算下一步移动所需的所有信息。换句话说，可以从棋盘配置恢复棋局而不知道先前的移动。
    （顺便说一句，这就是为什么报纸可以以非常节省空间的方式发布国际象棋谜题的原因。）像贪吃蛇这样的视频游戏也符合马尔可夫决策过程的公式化。蛇和食物在棋盘上的位置完全描述了游戏状态，这就足以从那一点恢复游戏或代理决定下一步行动。
- en: 'Even though problems such as chess and snake are fully compatible with MDP,
    they each involve an astronomical number of possible states. In order to present
    MDP in an intuitive and visual fashion, we need a simpler example. In [figure
    11.10](#ch11fig10), we show a very simple MDP problem in which there are only
    seven possible states and two possible agent actions. The transition between the
    states is governed by the following rules:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管诸如国际象棋和贪吃蛇等问题与马尔可夫决策过程完全兼容，但它们都涉及天文数字级别的可能状态。为了以直观和视觉的方式呈现马尔可夫决策过程，我们需要一个更简单的例子。在
    [图 11.10](#ch11fig10) 中，我们展示了一个非常简单的马尔可夫决策过程问题，其中只有七种可能的状态和两种可能的代理动作。状态之间的转换受以下规则管理：
- en: The initial state is always *s*[1].
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始状态始终为 *s*[1]。
- en: From state s[1], if the agent takes action *a*[1], the environment will enter
    state *s*[2]. If the agent takes action *a*[2], the environment will enter state
    *s*[3].
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从状态 s[1] 开始，如果代理采取动作 *a*[1]，环境将进入状态 *s*[2]。如果代理采取动作 *a*[2]，环境将进入状态 *s*[3]。
- en: From each of the states *s*[2] and *s*[3], the transition into the next state
    follows a similar set of bifurcating rules.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从每个状态 *s*[2] 和 *s*[3]，进入下一个状态的转换遵循一组类似的分叉规则。
- en: 'States *s*[4], *s*[5], *s*[6], and *s*[7] are terminal states: if any of the
    states is reached, the episode ends.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态 *s*[4]、*s*[5]、*s*[6] 和 *s*[7] 是终止状态：如果达到任何一个状态，那么该情节结束。
- en: Figure 11.10\. A very simple concrete example of the Markov decision process
    (MDP). States are represented as gray circles labeled with *s[n]*, while actions
    are represented as gray circles labeled with *a[m]*. The reward associated with
    each state transition caused by an action is labeled with *r* = *x*.
  id: totrans-188
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.10\. 马尔可夫决策过程（MDP）的一个非常简单具体的例子。状态表示为标有 *s[n]* 的灰色圆圈，而动作表示为标有 *a[m]* 的灰色圆圈。由动作引起的每个状态转换的奖励标有
    *r* = *x*。
- en: '![](11fig09_alt.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](11fig09_alt.jpg)'
- en: So, each episode in this RL problem lasts exactly three steps. How should the
    agent in this RL problem decide what action to take at the first and second steps?
    Given that we are dealing with an RL problem, the question makes sense only if
    the rewards are considered. In MDP, each action not only causes a state transition
    but also leads to a reward. In [figure 11.10](#ch11fig10), the rewards are depicted
    as the arrows that connect actions with the next states, labeled with `r = <reward_value>`.
    The agent’s goal is, of course, to maximize the total reward (discounted by a
    factor). Now imagine we are the agent at the first step. Let’s examine the thought
    process through which we’ll decide which of *a*[1] or *a*[2] is the better choice.
    Let’s suppose the reward discount factor (γ) has a value of 0.9.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这个强化学习问题中，每个阶段都恰好持续三个步骤。在这个强化学习问题中，代理应该如何决定在第一步和第二步采取什么行动？考虑到我们正在处理一个强化学习问题，只有在考虑奖励时，这个问题才有意义。在马尔可夫决策过程中，每个动作不仅引起状态转移，而且还导致奖励。在
    [图 11.10](#ch11fig10) 中，奖励被描述为将动作与下一个状态连接的箭头，标记为 `r = <reward_value>`。代理的目标当然是最大化总奖励（按比例折现）。现在想象一下我们是第一步的代理。让我们通过思考过程来确定我们将选择
    *a*[1] 还是 *a*[2] 更好的选择。假设奖励折现因子（γ）的值为0.9。
- en: 'The thought process goes like this. If we pick action *a*[1], we will get an
    immediate reward of –3 and transition to state *s*[2]. If we pick action *a*[2],
    we will get an immediate reward of 3 and transition to state *s*[3]. Does that
    mean *a*[2] is a better choice because 3 is greater than –3? The answer is no,
    because 3 and –3 are just the immediate rewards, and we haven’t taken into account
    the rewards from the following steps. We should look at the *best possible* outcome
    from each of *s*[2] and *s*[3]. What is the best outcome from *s*[2]? It is the
    outcome engendered by action *a*[2], which gives a reward of 11\. That leads to
    the best discounted reward we can expect if we take the action *a*[1] from state
    *s*[1]:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 思考过程如下。如果我们选择动作 *a*[1]，我们将获得–3 的即时奖励并转移到状态 *s*[2]。如果我们选择动作 *a*[2]，我们将获得3 的即时奖励并转移到状态
    *s*[3]。这是否意味着 *a*[2] 是更好的选择，因为3 大于 –3？答案是否定的，因为3 和 –3 只是即时奖励，并且我们还没有考虑以下步骤的奖励。我们应该看看每个
    *s*[2] 和 *s*[3] 的最佳可能结果是什么。从 *s*[2] 得到的最佳结果是通过采取动作 *a*[2] 而产生的结果，该动作获得了11 的奖励。这导致我们从状态
    *s*[1] 采取动作 *a*[1] 可以期望的最佳折现奖励：
- en: '| Best reward from state *s*[1] taking action *a*[1] | = immediate reward +
    discounted future reward |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: 当从状态 *s*[1] 采取动作 *a*[1] 时的最佳奖励 = 即时奖励 + 折现未来奖励 |
- en: '|   | = –3 + γ * 10 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '|   | = –3 + γ * 10 |'
- en: '|   | = –3 + 0.9 * 10 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '|   | = –3 + 0.9 * 10 |'
- en: '|   | = 6 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '|   | = 6 |'
- en: Similarly, the best outcome from state *s*[3] is if we take action *a*[1], which
    gives us a reward of –4\. Therefore, if we take action *a*[2] from state *s*[1],
    the best discounted reward for us is
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，从状态 *s*[3] 得到的最佳结果是我们采取动作 *a*[1]，这给我们带来了–4 的奖励。因此，如果我们从状态 *s*[1] 采取动作 *a*[2]，我们可以得到的最佳折现奖励是
- en: '| Best reward from state *s*[1] taking action *a*[2] | = immediate reward +
    discounted future reward |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 当从状态 *s*[1] 采取动作 *a*[2] 时的最佳奖励 = 即时奖励 + 折现未来奖励 |'
- en: '|   | = 3 + γ * –4 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '|   | = 3 + γ * –4 |'
- en: '|   | = 3 + 0.9 * –4 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '|   | = 3 + 0.9 * –4 |'
- en: '|   | = 0.6 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '|   | = 0.6 |'
- en: The discounted rewards we calculated here are examples of what we refer to as
    a *Q-values*. A Q-value is the expected total cumulative reward (with discounting)
    for an action at a given state. From these Q-values, it is clear that *a*[1] is
    the better choice at state *s*[1]—a different conclusion from what we’d reach
    if we considered only the immediate reward caused by the first action. Exercise
    3 at the end of the chapter guides you through the Q-value calculation for more
    realistic scenarios of MDP that involve stochasticity.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里计算的折现奖励是我们所说的 *Q-values* 的示例。 Q-value 是给定状态的动作的预期总累积奖励（按比例折现）。从这些 Q-values
    中，很明显 *a*[1] 是在状态 *s*[1] 下更好的选择——这与仅考虑第一个动作造成的即时奖励的结论不同。本章末尾的练习 3 将指导您完成涉及随机性的更现实的
    MDP 情景的 Q-value 计算。
- en: The example thought process described may seem trivial. But it leads us to an
    abstraction that plays a central role in Q-learning. A Q-value, denoted *Q*(*s*,
    *a*), is a function of the current state (*s*) and the action (*a*). In other
    words, *Q*(*s*, *a*) is a function that maps a state-action pair to the estimated
    value of taking the particular action at the particular state. This value is farsighted,
    in the sense that it accounts for best future rewards, under the assumption of
    optimal actions at all future steps.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 描述的示例思考过程可能看起来微不足道。但它引导我们得到一个在 Q 学习中起着核心作用的抽象。Q 值，表示为 *Q*(*s*, *a*)，是当前状态 (*s*)
    和动作 (*a*) 的函数。换句话说，*Q*(*s*, *a*) 是一个将状态-动作对映射到在特定状态采取特定动作的估计值的函数。这个值是长远眼光的，因为它考虑了最佳未来奖励，在所有未来步骤中都选择最优动作的假设下。
- en: Thanks to its farsightedness, *Q*(*s*, *a*) `is all we need to decide on the
    best action at any given state. In particular, given that we know what *Q*(*s*,
    *a*) is, the best action is the one that gives us the highest Q-value among all
    possible actions:`
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它的长远眼光，*Q*(*s*, *a*) `是我们在任何给定状态决定最佳动作的全部内容。特别是，鉴于我们知道 *Q*(*s*, *a*) 是什么，最佳动作是在所有可能动作中给出最高
    Q 值的动作：`
- en: equation 11.2\.
  id: totrans-204
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 11.2\.
- en: '![](11fig10_alt.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](11fig10_alt.jpg)'
- en: where *N* is the number of all possible actions. If we have a good estimate
    of *Q*(*s*, *a*), we can simply follow this decision process at every step, and
    we’ll be guaranteed to get the highest possible cumulative reward. Therefore,
    the RL problem of finding the best decision-making process is reduced to learning
    the function *Q*(*s*, *a*). This is why this learning algorithm is called Q-learning.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *N* 是所有可能动作的数量。如果我们对 *Q*(*s*, *a*) 有一个很好的估计，我们只需在每一步简单地遵循这个决策过程，我们就能保证获得最高可能的累积奖励。因此，找到最佳决策过程的强化学习问题被简化为学习函数
    *Q*(*s*, *a*)。这就是为什么这个学习算法被称为 Q 学习的原因。
- en: Let’s stop for a moment and look at how Q-learning differs from the policy-gradient
    method we saw in the cart-pole problem. Policy gradient is about predicting the
    best action; Q-learning is about predicting the values of all possible actions
    (Q-values). While policy gradient tells us which action to choose directly, Q-learning
    requires an additional “pick-the-maximum” step and is hence slightly more indirect.
    The benefit afforded by this indirection is that it makes it easier to form a
    connection between the rewards and values of successive steps, which facilitates
    learning in problems that involve sparse positive rewards like snake.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们停下来，看看 Q 学习与我们在小车杆问题中看到的策略梯度方法有何不同。策略梯度是关于预测最佳动作的；Q 学习是关于预测所有可能动作的值（Q 值）。虽然策略梯度直接告诉我们选择哪个动作，但
    Q 学习需要额外的“选择最大值”的步骤，因此稍微间接一些。这种间接性带来的好处是，它使得在涉及稀疏正奖励（如蛇）的问题中更容易形成奖励和连续步骤值之间的连接，从而促进学习。
- en: What are the connections between rewards and values of successive steps? We
    have already gotten a glimpse of this when solving the simple MDP problem in [figure
    11.10](#ch11fig10). This connection can be written mathematically as
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励与连续步骤的值之间有什么联系？当我们解决简单的 MDP 问题时，我们已经窥见了这一点，见[图 11.10](#ch11fig10)。这种连接可以用数学方式表示为：
- en: equation 11.3\.
  id: totrans-209
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 11.3\.
- en: '![](11eqa02_alt.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](11eqa02_alt.jpg)'
- en: where *s*[next] is the state we’ll reach after choosing action `a` from state
    *s[i]*. This equation, known as the *Bellman equation*,^([[7](#ch11fn7)]) is an
    abstraction for how we got the numbers 6 and –0.6 for the actions *a*[1] and *a*[2]
    in the simple earlier example. In plain words, the equation says
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '*s*[next] 是我们在状态 *s[i]* 中选择动作 `a` 后到达的状态。这个方程被称为*贝尔曼方程*，是我们在简单的早期示例中得到数字 6
    和 -0.6 的抽象。简单来说，这个方程表示：'
- en: ⁷
  id: totrans-212
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁷
- en: ''
  id: totrans-213
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Attributed to American applied mathematician Richard E. Bellman (1920–1984).
    See his book *Dynamic Programming*, Princeton University Press, 1957.
  id: totrans-214
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 归因于美国应用数学家理查德·E·贝尔曼（1920–1984）。参见他的书 *Dynamic Programming*，普林斯顿大学出版社，1957 年。
- en: '*The Q-value of taking action a at state s[i] is a sum of two terms:*'
  id: totrans-215
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*在状态 s[i] 采取动作 a 的 Q 值是两个术语的总和：*'
- en: ''
  id: totrans-216
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*The immediate reward due to `a, and`*'
  id: totrans-217
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*由于 a 而产生的即时奖励，以及*'
- en: '*The best possible Q-value from that next state multiplied by a discounting
    factor (“best” in the sense of optimal choice of action at the next state)*'
  id: totrans-218
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*从下一个状态中获得的最佳 Q 值乘以一个折扣因子（“最佳”是指在下一个状态选择最优动作的意义上）*'
- en: 'The Bellman equation is what makes Q-learning possible and is therefore important
    to understand. The programmer in you will immediately notice that the Bellman
    equation ([equation 11.3](#ch11equ03)) is recursive: all the Q-values on the right-hand
    side of the equation can be expanded further using the equation itself. The example
    in [figure 11.10](#ch11fig10) we worked through ends after two steps, while real
    MDP problems usually involve a much larger number of steps and states, potentially
    even containing cycles in the state-action-transition graph. But the beauty and
    power of the Bellman equation is that it allows us to turn the Q-learning problem
    into a supervised learning problem, even for large state spaces. We’ll explain
    why that’s the case in the next section.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔曼方程是使Q学习成为可能的因素，因此很重要理解。你作为程序员会立即注意到贝尔曼方程（[方程11.3](#ch11equ03)）是递归的：方程右侧的所有Q值都可以使用方程本身进一步展开。我们在[图11.10](#ch11fig10)中解释的示例在两步之后结束，而真实的MDP问题通常涉及更多步骤和状态，甚至可能包含状态-动作-转换图中的循环。但贝尔曼方程的美丽和力量在于，它允许我们将Q学习问题转化为一个监督学习问题，即使对于大状态空间也是如此。我们将在下一节解释为什么会这样。
- en: 11.3.3\. Deep Q-network
  id: totrans-220
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.3.3。深度Q网络
- en: Hand-crafting the function *Q*(*s*, *a*) can be difficult, so we will instead
    let the function be a deep neural network (the DQN mentioned earlier in the section)
    and train its parameters. This DQN receives an input tensor that represents the
    complete state of the environment—that is, the snake board configuration—which
    is available to the agent as the observation. As [figure 11.11](#ch11fig11) shows,
    the tensor has a shape `[9, 9, 2]` (excluding the batch dimension). The first
    two dimensions correspond to the height and width of the game board. Hence, the
    tensor can be viewed as a bitmap representation of all squares on the board. The
    last dimension (2) is two channels that represent the snake and the fruit, respectively.
    In particular, the snake is encoded in the first channel, with the head labeled
    as 2 and the body labeled as 1\. The fruit is encoded in the second channel, with
    a value 1\. In both channels, empty squares are represented by 0s. Note that these
    pixel values and the number of channels are more or less arbitrary. Other value
    arrangements (such as 100 for the snake’s head and 50 for the snake’s body, or
    separating the snake’s head and body into two channels) will likely also work,
    as long as they keep the three types of entities (snake head, snake body, and
    fruit) distinct.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 手工制作函数*Q*(*s*, *a*)可能很困难，因此我们将让函数成为一个深度神经网络（在本节中之前提到的DQN），并训练其参数。这个DQN接收一个表示环境完整状态的输入张量——也就是蛇板配置，这个张量作为观察结果提供给智能体。正如[图11.11](#ch11fig11)所示，该张量的形状为`[9,
    9, 2]`（不包括批次维度）。前两个维度对应于游戏板的高度和宽度。因此，张量可以被视为游戏板上所有方块的位图表示。最后一个维度（2）是代表蛇和水果的两个通道。特别地，蛇被编码在第一个通道中，头部标记为2，身体标记为1。水果被编码在第二个通道中，值为1。在两个通道中，空方块用0表示。请注意，这些像素值和通道数目是或多或少任意的。其他值排列（例如，蛇头为100，蛇身为50，或者将蛇头和蛇身分成两个通道）也可能有效，只要它们保持三种实体（蛇头、蛇身和水果）是不同的。
- en: Figure 11.11\. How the snake game’s board state is represented as a 3D tensor
    of shape `[9, 9, 2]`
  id: totrans-222
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.11。蛇游戏的板状态如何表示为形状为`[9, 9, 2]`的三维张量
- en: '![](11eqa03_alt.jpg)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](11eqa03_alt.jpg)'
- en: Note that this tensor representation of the game state is much less space-efficient
    than the JSON representation consisting of the fields `s` and `f` that we described
    in the previous section, because it always includes all the squares of the board
    regardless of how long the snake is. This inefficient representation is used only
    when we use back-propagation to update the DQN’s weights. In addition, only a
    small number (`batchSize`) of game states are present in this way at any given
    time, due to the batch-based training paradigm we will soon visit.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这种游戏状态的张量表示比我们在上一节中描述的由字段`s`和`f`组成的JSON表示要不太空间有效，因为它总是包含板上的所有方块，无论蛇有多长。这种低效的表示仅在我们使用反向传播来更新DQN的权重时使用。此外，在任何给定时间，由于我们即将访问的基于批次的训练范式，这种方式下只存在一小部分（`batchSize`）游戏状态。
- en: The code that converts an efficient representation of the board state into the
    kind of tensors illustrated in [figure 11.11](#ch11fig11) can be found in the
    `getStateTensor()` function in snake-dqn/snake_game.js. This function will be
    used a lot during the DQN’s training, but we omit its details here because it
    is just mechanically assigning values to the elements of a tensor buffer based
    on where the snake and fruit are.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 将有效表示的棋盘状态转换为[图11.11](#ch11fig11)中所示张量的代码可以在snake-dqn/snake_game.js的`getStateTensor()`函数中找到。这个函数在DQN的训练过程中会被频繁使用，但我们这里忽略其细节，因为它只是根据蛇和水果的位置机械地为张量缓冲区的元素赋值。
- en: You might have noticed that this `[height, width, channel]` input format is
    exactly what convnets are designed to process. The DQN we use is of the familiar
    convnet architecture. The code that defines the topology of the DQN can be found
    in [listing 11.5](#ch11ex05) (excerpted from snake-dqn/dqn.js, with some error-checking
    code removed for clarity). As the code and the diagram in [figure 11.12](#ch11fig12)
    show, the network consists of a stack of conv2d layers followed by an MLP. Additional
    layers including batchNormalization and dropout are inserted to increase the generalization
    power of the DQN. The output of the DQN has a shape of `[3]` (excluding the batch
    dimension). The three elements of the output are the predicted Q-values of the
    corresponding actions (turn left, going straight, and turn right). Thus our model
    of *Q*(*s*, *a*) is a neural network that takes a state as the input and outputs
    the Q-values for all possible actions given that state.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到这种`[height, width, channel]`的输入格式恰好是卷积神经网络设计来处理的。我们使用的DQN是熟悉的卷积神经网络架构。定义DQN拓扑的代码可以在[列表11.5](#ch11ex05)中找到（从snake-dqn/dqn.js中摘录，为了清晰起见删除了一些错误检查代码）。正如代码和[图11.12](#ch11fig12)中的图示所示，网络由一组conv2d层后跟一个MLP组成。额外的层包括batchNormalization和dropout被插入以增加DQN的泛化能力。DQN的输出形状为`[3]`（排除批次维度）。输出的三个元素是对应动作（向左转，直行和向右转）的预测Q值。因此，我们对*Q*(*s*,
    *a*)的模型是一个神经网络，它以状态作为输入，并输出给定该状态的所有可能动作的Q值。
- en: Figure 11.12\. A schematic illustration of the DQN that we use as an approximation
    to the function *Q*(*s*, *a*) for the snake problem. In the “Online DQN” box,
    “BN” stands for BatchNormalization.
  id: totrans-227
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.12 作为蛇问题中*Q*(*s*, *a*)函数的近似所使用的DQN的图示示意图。在“Online DQN”框中，“BN”代表BatchNormalization。
- en: '![](11fig11_alt.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](11fig11_alt.jpg)'
- en: Listing 11.5\. Creating the DQN for the snake problem
  id: totrans-229
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.5 创建蛇问题的DQN
- en: '[PRE8]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '***1*** The DQN has a typical convnet architecture: it begins with a stack
    of conv2d layers.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** DQN具有典型的卷积神经网络架构：它始于一组conv2d层。'
- en: '***2*** The input shape matches the tensor representation of the agent’s observation,
    as shown in [figure 11.11](#ch11fig11).'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 输入形状与代理观察的张量表示相匹配，如[图11.11](#ch11fig11)所示。'
- en: '***3*** batchNormalization layers are added to counter overfitting and improve
    generalization'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** batchNormalization层被添加以防止过拟合并提高泛化能力'
- en: '***4*** The MLP portion of the DQN begins with a flatten layer.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** DQN的MLP部分以一个flatten层开始。'
- en: '***5*** Like batchNormalization, the dropout layer is added to counter overfitting.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 与batchNormalization类似，dropout层被添加以防止过拟合。'
- en: 'Let’s pause for a moment and think about why it makes sense to use a neural
    network as the function *Q*(*s*, *a*) in this problem. The snake game has a discrete
    state space, unlike the continuous state space in the cart-pole problem, which
    consisted of four floating-point numbers. Therefore, the *Q*(*s*, *a*) function
    could in principle be implemented as a lookup table—that is, one that maps every
    single possible combination of board configuration and action into a value of
    *Q*. So why do we prefer a DQN over such a lookup table? The reason: there are
    far too many possible board configurations with even the relatively small board
    size (9 × 9),^([[8](#ch11fn8)]) which leads to two major shortcomings of the lookup
    table approach. First, the system RAM is unable to hold such a huge lookup table.
    Second, even if we manage to build a system with sufficient RAM, it will take
    a prohibitively long time for the agent to visit all the states during RL. The
    DQN addresses the first (memory space) problem thanks to its moderate size (about
    1 million parameters). It addresses the second (state-visit time) problem because
    of neural networks’ generalization power. As we’ve seen ample evidence for in
    the previous chapters, a neural network doesn’t need to see all the possible inputs;
    it learns to interpolate between training examples through generalization. Therefore,
    by using DQN, we kill two birds with one stone.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们停下来思考一下为什么在这个问题中使用神经网络作为函数*Q*(*s*, *a*)是有意义的。蛇游戏具有离散状态空间，不像连续状态空间的车杆问题，后者由四个浮点数组成。因此，*Q*(*s*,
    *a*)函数原则上可以实现为查找表，即将每个可能的棋盘配置和动作组合映射为*Q*的值。那么为什么我们更喜欢DQN而不是这样的查找表呢？原因在于，即使是相对较小的棋盘尺寸（9×9），可能的棋盘配置也太多了，导致了查找表方法的两个主要缺点。首先，系统RAM无法容纳如此庞大的查找表。其次，即使我们设法构建了具有足够RAM的系统，代理在RL期间访问所有状态也需要耗费非常长的时间。DQN通过其适度大小（约100万参数）解决了第一个（内存空间）问题。它通过神经网络的泛化能力解决了第二个（状态访问时间）问题。正如我们在前面章节中已经看到的大量证据所示，神经网络不需要看到所有可能的输入；它通过泛化学习来插值训练示例。因此，通过使用DQN，我们一举解决了两个问题。
- en: ⁸
  id: totrans-237
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁸
- en: ''
  id: totrans-238
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A back-of-the-envelope calculation leads to the rough estimate that the number
    of possible board configurations is on the order of at least 10^(15), even if
    we limit the snake length to 20\. For example, consider the particular snake length
    of 20\. First, pick a location for the head of the snake, for which there are
    9 * 9 = 81 possibilities. Then there are four possible locations for the first
    segment of the body, followed by three possible locations for the second segment,
    and so forth. Of course, in some body-pose configurations, there will be fewer
    than three possibilities, but that shouldn’t significantly alter the order of
    magnitude. Hence, we can estimate the number of possible body configurations of
    a length-20 snake to be approximately 81 * 4 * 3^(18) ≈ 10^(12). Considering that
    there are 61 possible fruit locations for each body configuration, the estimate
    for possible joint snake-fruit configurations goes up to 10^(14). Similar estimations
    can be applied to shorter snake lengths, from 2 to 19\. Summing all the estimated
    numbers from the lengths from 2 to 20 gives us the order of magnitude of 10^(15).
    Video games such as Atari 2600 games involve a much larger number of pixels compared
    to the number of squares on our snake board, and are therefore even less amenable
    to the lookup-table approach. This is one of the reasons why DQNs are a suitable
    technique for solving such video games using RL, as demonstrated in the landmark
    2015 paper by DeepMind’s Volodymyr Mnih and colleagues.
  id: totrans-239
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一个粗略的估算表明，即使我们将蛇的长度限制为20，可能的棋盘配置数量也至少达到10^(15)数量级。例如，考虑蛇长度为20的特定情况。首先，为蛇头选择一个位置，共有81种可能性（9
    * 9 = 81）。然后第一段身体有四个可能的位置，接着第二段有三个可能的位置，依此类推。当然，在一些身体姿势的配置中，可能的位置会少于三个，但这不应显著改变数量级。因此，我们可以估算出长度为20的蛇可能的身体配置数量约为81
    * 4 * 3^(18) ≈ 10^(12)。考虑到每种身体配置有61种可能的水果位置，关节蛇-水果配置的估算增加到了10^(14)。类似的估算可以应用于更短的蛇长度，从2到19。将从长度2到20的估算数字求和得到了10^(15)数量级。与我们的蛇棋盘上的方块数量相比，视频游戏如Atari
    2600游戏涉及更多的像素，因此更不适合查找表方法。这就是为什么DQNs是解决这类视频游戏使用RL的适当技术之一，正如DeepMind的Volodymyr
    Mnih及其同事在2015年的里程碑式论文中所示的那样。
- en: 11.3.4\. Training the deep Q-network
  id: totrans-240
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.3.4\. 训练深度Q网络
- en: 'Now we have a DQN that estimates the Q-values of all three possible actions
    at every step of the snake game. To achieve the greatest possible cumulative reward,
    all we have to do is run the DQN using the observation at every step and pick
    the action with the highest Q-value. Are we done yet? No, because the DQN is not
    trained yet! Without proper training, the DQN will contain only randomly initialized
    weights, and the actions it gives us will be no better than random guesses. Now
    the snake RL problem has been reduced to the question of how to train the DQN,
    a topic we’ll cover in this section. The process is somewhat involved. But don’t
    worry: we’ll use plenty of diagrams, accompanied by code excerpts, to spell out
    the training algorithm step-by-step.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有了一个 DQN，可以在蛇游戏的每一步估计出三个可能行动的 Q 值。为了实现尽可能大的累积奖励，我们只需要在每一步运行 DQN，并选择具有最高
    Q 值的动作即可。我们完成了吗？并没有，因为 DQN 还没有经过训练！没有适当的训练，DQN 只会包含随机初始化的权重，它给出的动作不会比随机猜测更好。现在，蛇的强化学习问题已经被减少为如何训练
    DQN 的问题，这是我们在本节中要讨论的主题。这个过程有些复杂。但别担心：我们将使用大量的图表以及代码摘录，逐步详细说明训练算法。
- en: Intuition behind the deep Q-network’s training
  id: totrans-242
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 深度 Q 网络训练的直觉
- en: We will train our DQN by pressuring it to match the Bellman equation. If all
    goes well, this means that our DQN will reflect both the immediate rewards and
    the optimal discounted future rewards.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过迫使 DQN 与贝尔曼方程相匹配来训练我们的 DQN。如果一切顺利，这意味着我们的 DQN 将同时反映即时奖励和最优折现未来奖励。
- en: How can we do that? What we will need is many samples of input-output pairs,
    the input being the state and action actually taken and the output being the “correct”
    (target) value of Q. Computing samples of input requires the current state *s[i]*
    and the action we took at that state, *a[j]*, both of which are directly available
    in the game history. Computing the target value of Q requires the immediate reward
    *r[i]* and the next state *s[i]*[+1], which are also available from game history.
    We can use *r[i]* and *s[i]*[+1] to compute the target Q-value by applying the
    Bellman equation, the details of which will be covered shortly. We will then calculate
    the difference between Q-value predicted by the DQN and the target Q-value from
    the Bellman equation and call that our loss. We will reduce the loss (in a least-squares
    sense) using standard backpropagation and gradient descent. The machinery making
    this possible and efficient is somewhat complicated, but the intuition is rather
    straightforward. We want an estimate of the Q function so we can make good decisions.
    We know our estimate of Q must match the environmental rewards and the Bellman
    equation, so we will use gradient descent to make it so. Simple!
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们该如何做到这一点呢？我们需要的是许多输入-输出对的样本，其中输入是实际采取的状态和动作，而输出是 Q 的“正确”（目标）值。计算输入样本需要当前状态
    *s[i]* 和我们在该状态下采取的动作 *a[j]*，这两者都可以直接从游戏历史中获取。计算目标 Q 值需要即时奖励 *r[i]* 和下一个状态 *s[i]*[+1]，这两者也可以从游戏历史中获取。我们可以使用
    *r[i]* 和 *s[i]*[+1]，通过应用贝尔曼方程来计算目标 Q 值，其细节将很快涉及到。然后，我们将计算由 DQN 预测的 Q 值与贝尔曼方程中的目标
    Q 值之间的差异，并将其称为我们的损失。我们将使用标准的反向传播和梯度下降来减少损失（以最小二乘的方式）。使这成为可能和高效的机制有些复杂，但基本的直觉却相当简单。我们想要一个
    Q 函数的估计值，以便能做出良好的决策。我们知道我们对 Q 的估计必须与环境奖励和贝尔曼方程相匹配，因此我们将使用梯度下降来实现。简单！
- en: 'Replay memory: A rolling dataset for the DQN’s training'
  id: totrans-245
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 回放内存：用于 DQN 训练的滚动数据集
- en: 'Our DQN is a familiar convnet implemented as an instance of `tf.LayersModel`
    in TensorFlow.js. With regard to how to train it, the first thing that comes to
    mind is to call its `fit()` or `fitDataset()` method. However, we can’t use that
    usual approach here because we don’t have a labeled dataset that contains observed
    states and the corresponding Q-values. Consider this: before the DQN is trained,
    there is no way to know the Q-values. If we had a method that gave us the true
    Q-values, we would just use it in our Markov decision process and be done with
    it. So, if we confine ourselves to the traditional supervised-learning approach,
    we will face a chicken-and-egg problem: without a trained DQN, we can’t estimate
    the Q-values; without a good estimate of Q-values, we can’t train the DQN. The
    RL algorithm we are about to introduce will help us solve this chicken-and-egg
    problem.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 DQN 是一个熟悉的卷积网络，在 TensorFlow.js 中作为 `tf.LayersModel` 的一个实例实现。关于如何训练它，首先想到的是调用其
    `fit()` 或 `fitDataset()` 方法。然而，我们在这里不能使用常规方法，因为我们没有一个包含观察到的状态和相应 Q 值的标记数据集。考虑这样一个问题：在
    DQN 训练之前，没有办法知道 Q 值。如果我们有一个给出真实 Q 值的方法，我们就会在马尔科夫决策过程中使用它并完成。因此，如果我们局限于传统的监督学习方法，我们将面临一个先有鸡还是先有蛋的问题：没有训练好的
    DQN，我们无法估计 Q 值；没有良好的 Q 值估计，我们无法训练 DQN。我们即将介绍的强化学习算法将帮助我们解决这个先有鸡还是先有蛋的问题。
- en: 'Specifically, our method is to let the agent play the game randomly (at least
    initially) and remember what happened at every step of the game. The random-play
    part is easily achieved using a random-number generator. The remembering part
    is achieved with a data structure known as *replay memory*. [Figure 11.13](#ch11fig13)
    illustrates how the replay memory works. It stores five items for every step of
    the game:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们的方法是让代理者随机玩游戏（至少最初是如此），并记住游戏的每一步发生了什么。随机游戏部分很容易通过随机数生成器实现。记忆部分则通过一种称为*重放内存*的数据结构实现。[图
    11.13](#ch11fig13)展示了重放内存的工作原理。它为游戏的每一步存储五个项目：
- en: '*s[i]*, observation of the current state at step *i* (the board configuration).'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*s[i]*，第 *i* 步的当前状态观察（棋盘配置）。'
- en: '*a[i]*, action actually performed at the current step (selected either by the
    DQN as depicted in [figure 11.12](#ch11fig12) or through random selection).'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*a[i]*，当前步骤实际执行的动作（可以是 DQN 选择的，如[图 11.12](#ch11fig12)，也可以是随机选择）。'
- en: '*r[i]*, the immediate reward received at this step.'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*r[i]*，在此步骤接收到的即时奖励。'
- en: '*d[i]*, a Boolean flag indicating whether the game ends immediately after the
    current step. From this, you can see the fact that the replay memory is not just
    for a single episode of the game. Instead, it concatenates the results from multiple
    game episodes. Once a previous game is over, the training algorithm simply starts
    a new one and keeps appending the new records to the replay memory.'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*d[i]*，一个布尔标志，指示游戏在当前步骤后立即结束。由此可见，重放内存不仅仅是为了一个游戏回合。相反，它将来自多个游戏回合的结果连接在一起。一旦前一场游戏结束，训练算法就会简单地开始新的游戏，并将新记录追加到重放内存中。'
- en: '*s[i]*[+1], the observation from the next step if *d[i]* is false. (If *d[i]*
    is true, a null is stored as the placeholder.)'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*s[i]*[+1]，如果 *d[i]* 为假，则是下一步的观察。（如果 *d[i]* 为真，则存储 null 作为占位符。）'
- en: Figure 11.13\. The replay memory used during the training of the DQN. Five pieces
    of data are pushed to the end of the replay memory at every step. These data are
    sampled during the DQN’s training.
  id: totrans-253
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.13\. 在 DQN 训练过程中使用的重放内存。每一步都将五条数据推到重放内存的末尾。在 DQN 的训练过程中对这些数据进行抽样。
- en: '![](11fig12_alt.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](11fig12_alt.jpg)'
- en: 'These pieces of data will go into the backpropagation-based training of the
    DQN. The replay memory can be thought of as a “dataset” for the DQN’s training.
    However, it’s different from the kind of datasets in supervised learning, in the
    sense that it keeps getting updated as the training goes on. The replay memory
    has a fixed length *M* (*M* = 10,000 by default in the example code). When a record
    (*s[i]*, *a[i]*, *r[i]*, *d[i]*, *s[i]*[+1]) is pushed to its end after a new
    game step, an old record is popped out from its beginning, which maintains a fixed
    replay-memory length. This ensures that the replay memory keeps track of what
    happened in the most recent *M* steps of the training, in addition to avoiding
    out-of-memory problems. It is beneficial to always train the DQN using the latest
    game records. Why? Consider the following: once the DQN has been trained for a
    while and starts to “get the hang of” the game, we won’t want to teach it using
    old game records like the ones from the beginning of the training because those
    may contain naive moves that are no longer relevant or conducive to the further
    training of the network.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据片段将用于DQN的基于反向传播的训练。回放记忆可以被视为DQN训练的“数据集”。然而，它不同于监督学习中的数据集，因为它会随着训练的进行而不断更新。回放记忆有一个固定长度*M*（在示例代码中默认为10,000）。当一个记录(*s[i]*,
    *a[i]*, *r[i]*, *d[i]*, *s[i]*[+1])被推到其末尾时，一个旧的记录会从其开始弹出，这保持了一个固定的回放记忆长度。这确保了回放记忆跟踪了训练中最近*M*步的发生情况，除了避免内存不足的问题。始终使用最新的游戏记录训练DQN是有益的。为什么？考虑以下情况：一旦DQN训练了一段时间并开始“熟悉”游戏，我们将不希望使用旧的游戏记录来教导它，比如训练开始时的记录，因为这些记录可能包含不再相关或有利于进一步网络训练的幼稚移动。
- en: 'The code that implements the replay memory is very simple and can be found
    in the file snake-dqn/replay_memory.js. We won’t describe the details of the code,
    except its two public methods, `append()` and `sample()`:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 实现回放记忆的代码非常简单，可以在文件snake-dqn/replay_memory.js中找到。我们不会描述代码的详细信息，除了它的两个公共方法，`append()`和`sample()`：
- en: '`append()` allows the caller to push a new record to the end of the replay
    memory.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`append()`允许调用者将新记录推送到回放记忆的末尾。'
- en: '`sample(batchSize)` selects `batchSize` records from the replay memory randomly.
    The records are sampled completely uniformly and will in general include records
    from multiple different episodes. The `sample()` method will be used to extract
    training batches during the calculation of the loss function and the subsequent
    backpropagation, as we will see shortly.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sample(batchSize)`从回放记忆中随机选择`batchSize`个记录。这些记录完全均匀地抽样，并且通常包括来自多个不同情节的记录。`sample()`方法将用于在计算损失函数和随后的反向传播期间提取训练批次，我们很快就会看到。'
- en: 'The epsilon-greedy algorithm: Balancing exploration and exploitation'
  id: totrans-259
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: epsilon-greedy算法：平衡探索和利用
- en: An agent that keeps trying random things will stumble onto some good moves (eat
    a fruit or two in a snake game) by pure luck. This is useful for kickstarting
    the agent’s early learning process. In fact, it is the only way because the agent
    is never told the rules of the game. But if the agent keeps behaving randomly,
    it won’t make it very far in the learning process, both because random choices
    lead to accidental deaths and because some advanced states can be achieved only
    through streaks of good moves.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 一个不断尝试随机事物的智能体将凭借纯运气偶然发现一些好的动作（在贪吃蛇游戏中吃几个水果）。这对于启动智能体的早期学习过程是有用的。事实上，这是唯一的方法，因为智能体从未被告知游戏的规则。但是，如果智能体一直随机行为，它在学习过程中将无法走得很远，因为随机选择会导致意外死亡，而且一些高级状态只能通过一连串良好的动作达到。
- en: This is the manifestation of the exploration-versus-exploitation dilemma in
    the snake game. We’ve seen this dilemma in the cart-pole example, where the policy-gradient
    method addresses the problem thanks to the gradual increase in the determinism
    of the multinomial `sampling with training. In the snake game, we do not have
    this luxury because our action selection is based not on `tf.multinomial()` but
    on selecting the maximum Q-value among the actions. The way in which we address
    the dilemma is by parameterizing the randomness of the action-selection process
    and gradually reducing the parameter of randomness. In particular, we use the
    so-called *epsilon-greedy policy*. This policy can be expressed in pseudo-code
    as`
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是蛇游戏中探索与开发的两难境地的体现。我们在平衡摇摆杆的示例中看到了这个两难境地，其中的策略梯度方法通过逐渐增加训练过程中的多项式采样的确定性来解决这个问题。在蛇游戏中，我们没有这个便利，因为我们的动作选择不是基于
    `tf.multinomial()`，而是选择具有最大 Q 值的动作。我们解决这个问题的方式是通过参数化动作选择过程的随机性，并逐渐减小随机性参数。特别地，我们使用所谓的*epsilon-greedy
    策略*。该策略可以用伪代码表示为`
- en: '[PRE9]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This logic is applied at every step of the training. The larger the value of
    epsilon (the closer it is to 1), the more likely the action will be chosen at
    random. Conversely, a smaller value of epsilon (closer to 0) leads to a higher
    probability of choosing the action based on the Q-values predicted by the DQN.
    Choosing actions at random can be viewed as exploring the environment (“epsilon”
    stands for “exploration”), while choosing actions to maximize the Q-value is referred
    to as *greedy*. Now you understand where the name *epsilon-greedy* comes from.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 这个逻辑在训练的每一步都会应用。epsilon 的值越大（接近 1），选择动作的随机性越高。相反，epsilon 的值越小（接近 0），基于 DQN 预测的
    Q 值选择动作的概率越高。随机选择动作可以看作是对环境的探索（"epsilon" 代表 "exploration"），而选择最大 Q 值的动作被称为*贪心*。现在你明白了
    "epsilon-greedy" 这个名字的来历。
- en: As shown in [listing 11.6](#ch11ex06), the actual TensorFlow.js code that implements
    the epsilon-greedy algorithm in the snake-dqn example has a close one-to-one correspondence
    with the previous pseudo-code. This code is excerpted from snake-dqn/agent.js.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [代码清单 11.6](#ch11ex06) 所示，实现蛇 DQN 示例中 epsilon-greedy 算法的实际 TensorFlow.js 代码与之前的伪代码具有密切的一对一对应关系。该代码摘自
    snake-dqn/agent.js。
- en: Listing 11.6\. The part of snake-dqn code that implements the epsilon-greedy
    algorithm
  id: totrans-265
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 代码清单 11.6。实现 epsilon-greedy 算法的部分蛇 DQN 代码
- en: '[PRE10]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '***1*** Exploration: picks actions randomly'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 探索：随机选择动作'
- en: '***2*** Represents the game state as a tensor'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 将游戏状态表示为张量'
- en: '***3*** Greedy policy: gets predicted Q-values from the DQN and finds the index
    of the action that corresponds to the highest Q-value'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 贪心策略：从 DQN 获取预测的 Q 值，并找到对应于最高 Q 值的动作的索引'
- en: The epsilon-greedy policy balances the early need for exploration and later
    need for stable behavior. It does so through gradually ramping down the value
    of epsilon from a relative large value to a value close to (but not exactly) zero.
    In our snake-dqn example, epsilon is ramped down in a linear fashion from 0.5
    to 0.01 over the first 1 × 105 steps of the training. Note that we don’t decrease
    the epsilon all the way to zero because we need a moderate degree of exploration
    even at advanced stages of the agent’s training in order to help the agent discover
    smart new moves. In RL problems based on the epsilon-greedy policy, the initial
    and final values of epsilon are tunable hyperparameters, and so is the time course
    of epsilon’s down-ramping.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: epsilon-greedy 策略在早期需要探索和后期需要稳定行为之间保持平衡。它通过逐渐减小 epsilon 的值，从一个相对较大的值逐渐减小到接近（但不完全等于）零。在我们的蛇
    DQN 示例中，epsilon 在训练的前 1 × 105 步中以线性方式逐渐减小从 0.5到 0.01。请注意，我们没有将 epsilon 减小到零，因为在智能体的训练的高级阶段，我们仍然需要适度的探索程度来帮助智能体发现新的智能举动。在基于
    epsilon-greedy 策略的 RL 问题中，epsilon 的初始值和最终值都是可调节的超参数，epsilon 的降低时间也是如此。
- en: With the backdrop of our deep Q-learning algorithm set by the epsilon-greedy
    policy, next let’s examine the details of how the DQN is trained.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在 epsilon-greedy 策略设定下的深度 Q 学习算法背景下，接下来让我们详细了解 DQN 的训练细节。
- en: Extracting predicted Q-values
  id: totrans-272
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提取预测的 Q 值
- en: 'Although we are using a new approach to attack the RL problem, we still want
    to mold our algorithm into supervised learning because that will allow us to use
    the familiar backpropagation approach to update the DQN’s weights. Such a formulation
    requires three things:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们正在使用一种新方法来解决RL问题，但我们仍然希望将我们的算法塑造成监督学习，因为这样可以让我们使用熟悉的反向传播方法来更新DQN的权重。这样的制定需要三个要素：
- en: Predicted Q-values.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测的Q值。
- en: “True” Q-values. Note that the word “true” is in quotes here because there isn’t
    really a way to obtain the ground truths for Q-values. These values are merely
    the best estimates of *Q*(*s*, *a*) that we can come up with at a given stage
    of the training algorithm. For this reason, we’ll refer to them as the target
    Q-values instead.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “真实”的Q值。请注意，在这里，“真实”一词带有引号，因为实际上并没有办法获得Q值的基本真实值。这些值只是我们在训练算法的给定阶段能够得到的*Q*(*s*,
    *a*)的最佳估计值。因此，我们将其称为目标Q值。
- en: A loss function that takes the predicted and target Q-values and outputs a number
    that quantifies the mismatch between the two.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个损失函数，它以预测和目标Q值作为输入，并输出一个量化两者之间不匹配的数字。
- en: In this subsection, we’ll look at how the predicted Q-values can be obtained
    from the replay memory. The following two subsections will talk about how to obtain
    the target Q-values and the loss function. Once we have all three, our snake RL
    problem will basically become a straightforward backpropagation problem.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个小节中，我们将看看如何从回放记忆中获取预测的Q值。接下来的两个小节将讨论如何获取目标Q值和损失函数。一旦我们有了这三个要素，我们的蛇RL问题基本上就变成了一个简单的反向传播问题。
- en: '[Figure 11.14](#ch11fig14) illustrates how the predicted Q-values are extracted
    from the replay memory in a step of the DQN’s training. The diagram should be
    viewed in conjunction with the implementing code in [listing 11.7](#ch11ex07)
    to facilitate understanding.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.14](#ch11fig14)说明了如何从回放记忆中提取预测的Q值的DQN训练步骤。应该将这个图表与实现代码[清单11.7](#ch11ex07)一起查看，以便更好地理解。'
- en: Figure 11.14\. How the predicted Q-values are obtained from the replay memory
    and the online DQN. This is the first of the two parts that go into the supervised-learning
    portion of the DQN training algorithm. The result of this workflow, `actionQs`—that
    is, the Q-values predicted by the DQN—is one of the two arguments that will go
    into the calculation of the MSE loss together with `targetQs`. See [figure 11.15](#ch11fig15)
    for the workflow in which `targetQs` is calculated.
  id: totrans-279
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.14\. 如何从回放记忆和在线DQN中获取预测的Q值。这是DQN训练算法中监督学习部分的两个部分中的第一个部分。这个工作流的结果，即DQN预测的Q值`actionQs`，是将与`targetQs`一起用于计算MSE损失的两个参数之一。查看[图11.15](#ch11fig15)以了解计算`targetQs`的工作流程。
- en: '![](11fig13_alt.jpg)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![](11fig13_alt.jpg)'
- en: In particular, we sample `batchSize` (`N = 128` by default) records randomly
    from the replay memory. As described before, each record has five items. For the
    purpose of getting the predicted Q-values, we need only the first two. The first
    items, consisting of the *N* state observations, are converted together into a
    tensor. This batched observation tensor is processed by the online DQN, which
    gives the predicted Q-values (`qs` in both the diagram and the code). However,
    `qs` includes the Q-values for not only the actually selected actions but also
    the nonselected ones. For our training, we want to ignore the Q-values for the
    nonselected actions because there isn’t a way to know their target Q-values. This
    is where the second replay-memory item comes in.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，我们从回放记忆中随机抽取`batchSize`（默认为`N = 128`）条记录。正如之前所描述的，每条记录都有五个项目。为了获得预测的Q值，我们只需要前两个。第一个项目，包括*N*个状态观察，一起转换成一个张量。这个批处理的观察张量由在线DQN处理，它给出了预测的Q值（在图表和代码中都是`qs`）。然而，`qs`包含的Q值不仅包括实际选择的动作，还包括未选择的动作。对于我们的训练，我们希望忽略未选择动作的Q值，因为没有办法知道它们的目标Q值。这就是第二个回放记忆项发挥作用的地方。
- en: 'The second items contain the actually selected actions. They are formatted
    into a tensor representation (`actionTensor` in the diagram and code). `actionTensor`
    is then used to select the elements of `qs` that we want. This step, illustrated
    in the box labeled Select Actual Actions in the diagram, is achieved using three
    TensorFlow.js functions: `tf.oneHot()`, `mul()`, and `sum()` (see the last line
    in [listing 11.7](#ch11ex07)). This is slightly more complex than slicing a tensor
    because different actions can be selected at different game steps. The code in
    [listing 11.7](#ch11ex07) is excerpted from the `SnakeGameAgent.trainOnReplayBatch()`
    method in snake-dqn/agent.js, with minor omissions for clarity.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 第二项包含实际选择的动作。它们被格式化成张量表示（图和代码中的 `actionTensor`）。然后使用 `actionTensor` 选择我们想要的
    `qs` 元素。这一步骤在图中标记为选择实际动作的框中完成，使用了三个 TensorFlow.js 函数：`tf.oneHot()`、`mul()` 和 `sum()`（参见[清单
    11.7](#ch11ex07) 中的最后一行）。这比切片张量稍微复杂一些，因为在不同的游戏步骤可以选择不同的动作。[清单 11.7](#ch11ex07)
    中的代码摘自 snake-dqn/agent.js 中的 `SnakeGameAgent.trainOnReplayBatch()` 方法，为了清晰起见进行了些许省略。
- en: Listing 11.7\. Extracting a batch of predicted Q-values from the replay memory
  id: totrans-283
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 11.7\. 从回放内存中提取一批预测的 Q 值
- en: '[PRE11]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '***1*** Gets a batch of batchSize randomly chosen game records from the replay
    memory'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 从回放内存中随机选择一批大小为 batchSize 的游戏记录'
- en: '***2*** The first element of every game record is the agent’s state observation
    (see [figure 11.13](#ch11fig13)). It is converted from a JSON object into a tensor
    by the getStateTensor() function (see [figure 11.11](#ch11fig11)).'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 每个游戏记录的第一个元素是代理的状态观察（参见[图 11.13](#ch11fig13)）。它由 getStateTensor() 函数（参见[图
    11.11](#ch11fig11)）将其从 JSON 对象转换为张量。'
- en: '***3*** The second element of the game record is the actually selected action.
    It’s represented as a tensor as well.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 游戏记录的第二个元素是实际选择的动作。它也被表示为张量。'
- en: '***4*** The apply() method is similar to the predict() method, but the “training:
    true” flag is specified explicitly to enable backpropagation.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** apply() 方法与 predict() 方法类似，但显式指定了“training: true”标志以启用反向传播。'
- en: '***5*** We use tf.oneHot(), mul(), and sum() to isolate the Q-values for only
    the actually selected actions and discard the ones for the actions not selected.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 我们使用 tf.oneHot()、mul() 和 sum() 来隔离仅针对实际选择的动作的 Q 值，并丢弃未选择的动作的 Q 值。'
- en: These operations give us a tensor called `actionQs`, which has a shape of `[N]`,
    `N` being the batch size. This is the predicted Q-value that we sought—that is,
    the predicted *Q*(*s*, *a*) for the state *s* we were in and the action *a* we
    actually took. Next, we’ll examine how the target Q-values are obtained.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 这些操作给了我们一个名为 `actionQs` 的张量，其形状为 `[N]`，其中 `N` 是批次大小。这就是我们寻找的预测的 Q 值，即我们所处的状态
    *s* 和我们实际采取的动作 *a* 的预测 *Q*(*s*, *a*)。接下来，我们将探讨如何获取目标 Q 值。
- en: 'Extracting target Q-values: Using the Bellman equation'
  id: totrans-291
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提取目标 Q 值：使用贝尔曼方程
- en: 'It is slightly more involved to obtain the target Q-values than the predicted
    ones. This is where the theoretical Bellman equation will be put to practical
    use. Recall that the Bellman equation describes the Q-value of a state-action
    pair in terms of two things: 1) the immediate reward and 2) the maximum Q-value
    available from the next step’s state (discounted by a factor). The former is easy
    to obtain. It is directly available as the third item of the replay memory. The
    `rewardTensor` in [figure 11.15](#ch11fig15) illustrates this schematically.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 获取目标 Q 值比获取预测值稍微复杂一些。这是理论上的贝尔曼方程将被实际应用的地方。回想一下，贝尔曼方程用两个因素描述了状态-动作对的 Q 值：1) 即时奖励和
    2) 下一步状态可用的最大 Q 值（通过一个因子折现）。前者很容易获得。它直接作为回放内存的第三项可得到。[图 11.15](#ch11fig15) 中的
    `rewardTensor` 用示意图的方式说明了这一点。
- en: Figure 11.15\. How the target Q-values (`targetQs`) are obtained from the replay
    memory and the target DQN. This figure shares the replay-memory and batch-sampling
    parts with [figure 11.14](#ch11fig14). It should be examined in conjunction with
    the code in [listing 11.8](#ch11ex08). This is the second of the two parts that
    goes into the supervised-learning portion of the DQN training algorithm. `targetQs`
    plays a role similar to the truth labels in supervised-learning problems seen
    in the previous chapters (for example, known true labels in the MNIST examples
    or known true future temperature values in the Jena-weather example). The Bellman
    equation plays a critical role in the calculation of `targetQs`. Together with
    the target DQN, the equation allows us to calculate the values of `targetQs` through
    forming a connection between the Q-values of the current step and the Q-values
    of the ensuing step.
  id: totrans-293
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.15\. 如何从重播记忆和目标DQN获取目标Q值（`targetQs`）。此图与[图11.14](#ch11fig14)共享重播记忆和批量采样部分。应该与[列表11.8](#ch11ex08)中的代码一起检查。这是进入DQN训练算法的监督学习部分的两个部分之一。`targetQs`在计算中起着类似于前几章中监督学习问题中的真实标签的作用（例如，MNIST示例中的已知真实标签或Jena-weather示例中的已知真实未来温度值）。贝尔曼方程在计算`targetQs`中起着关键作用。与目标DQN一起，该方程允许我们通过形成当前步骤的Q值和随后步骤的Q值之间的连接来计算`targetQs`的值。
- en: '![](11fig14_alt.jpg)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![](11fig14_alt.jpg)'
- en: To calculate the latter (maximum next-step Q-value), we need the state observation
    from the next step. Luckily, the next-step observation is stored in the replay
    memory as the fifth item. We take the next-step observation of the randomly sampled
    batch, convert it to a tensor, and run it through a copy of the DQN called the
    *target DQN* (see [figure 11.15](#ch11fig15)). This gives us the estimated Q-values
    for the next-step states. Once we have these, we perform a `max()` call along
    the last (actions) dimension, which leads to the maximum Q-values achievable from
    the next-step state (represented as `nextMaxQTensor` in [listing 11.8](#ch11ex08)).
    Following the Bellman equation, this maximum value is multiplied by the discount
    factor (γ in [figure 11.15](#ch11fig15) and `gamma` in [listing 11.8](#ch11ex08))
    and combined with the immediate reward, which yields the target Q-values (`targetQs`
    in both the diagram and the code).
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算后者（最大的下一步Q值），我们需要来自下一步的状态观察。幸运的是，下一步观察被存储在重播记忆中的第五项。我们取随机抽样批次的下一步观察，将其转换为张量，并通过名为*目标DQN*的DQN的副本运行它（见[图11.15](#ch11fig15)）。这给了我们下一步状态的估计Q值。一旦我们有了这些值，我们沿着最后（动作）维度进行`max()`调用，这导致从下一步状态中获得的最大Q值（在[列表11.8](#ch11ex08)中表示为`nextMaxQTensor`）。遵循贝尔曼方程，这个最大值乘以折扣因子（[图11.15](#ch11fig15)中的γ和[列表11.8](#ch11ex08)中的`gamma`）并与即时奖励相结合，产生目标Q值（在图和代码中均为`targetQs`）。
- en: Note that the next-step Q-value exists only when the current step is not the
    last step of a game episode (that is, it doesn’t cause the snake to die). If it
    is, then the right-hand side of the Bellman equation will include only the immediate-reward
    term, as shown in [figure 11.15](#ch11fig15). This corresponds to the `doneMask`
    tensor in [listing 11.8](#ch11ex08). The code in this listing is excerpted from
    the `SnakeGameAgent.trainOnReplayBatch()` method in snake-dqn/agent.js, with minor
    omissions for clarity.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，只有当当前步骤不是游戏剧集的最后一步时（即，它不会导致蛇死亡），下一步Q值才存在。如果是，那么贝尔曼方程的右侧将仅包括即时奖励项，如[图11.15](#ch11fig15)所示。这对应于[列表11.8](#ch11ex08)中的`doneMask`张量。此列表中的代码摘自snake-dqn/agent.js中的`SnakeGameAgent.trainOnReplayBatch()`方法，为了清晰起见做了一些小的省略。
- en: Listing 11.8\. Extracting a batch of target (“true”) Q-values from the replay
    memory
  id: totrans-297
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.8\. 从重播记忆中提取一批目标（“真实”）Q值
- en: '[PRE12]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '***1*** The third item of a replay record contains the immediate reward value.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 重播记录的第三项包含即时奖励值。'
- en: '***2*** The fifth item of a record contains the next-state observation. It’s
    transformed into a tensor representation.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 记录的第五项包含下一状态观察。它被转换为张量表示。'
- en: '***3*** The target DQN is used on the next-state tensor, which yields the Q-values
    for all actions at the next step.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 目标DQN用于下一个状态张量，它产生下一步所有动作的Q值。'
- en: '***4*** Uses the max() function to extract the highest possible reward at the
    next step. This is on the right-hand side of the Bellman equation.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 使用`max()`函数提取下一步可能的最高奖励。这在贝尔曼方程的右侧。'
- en: '***5*** doneMask has the value 0 for the steps that terminate the game and
    1 for other steps.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** doneMask 在终止游戏的步骤上具有值 0，并在其他步骤上具有值 1。'
- en: '***6*** Uses the Bellman equation to calculate the target Q-values.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 使用贝尔曼方程来计算目标 Q 值。'
- en: As you may have noticed, an important trick in the deep Q-learning algorithm
    here is the use of two instances of DQNs. They are called the *online* DQN and
    the *target* DQN, respectively. The online DQN is responsible for calculating
    the predicted Q-values (see [figure 11.14](#ch11fig14) in the previous subsection).
    It is also the DQN that we use to choose the snake’s action when the epsilon-greedy
    algorithm decides on the greedy (no-exploration) approach. This is why it’s called
    the “online” network. By contrast, the target DQN is used only to calculate the
    target Q-values, as we’ve just seen. This is why it’s called the “target” DQN.
    Why do we use two DQNs instead of one? To break up undesirable feedback loops,
    which can cause instabilities in the training process.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能已经注意到的，在深度 Q 学习算法中的一个重要技巧是使用两个 DQN 实例。它们分别被称为 *在线* DQN 和 *目标* DQN。在线 DQN
    负责计算预测的 Q 值（参见上一小节的 [图 11.14](#ch11fig14)）。它也是我们在 epsilon-greedy 算法决定采用贪婪（无探索）方法时选择蛇行动的
    DQN。这就是为什么它被称为“在线”网络。相比之下，目标 DQN 仅用于计算目标 Q 值，就像我们刚刚看到的那样。这就是为什么它被称为“目标”DQN。为什么我们使用两个
    DQN 而不是一个？为了打破不良反馈循环，这可能会导致训练过程中的不稳定性。
- en: The online DQN and target DQN are created by the same `createDeepQNetwork()`
    function ([listing 11.5](#ch11ex05)). They are two deep convnets with identical
    topologies. Therefore, they have exactly the same set of layers and weights. The
    weight values are copied from the online DQN to the target one periodically (every
    1,000 steps in the default setting of snake-dqn). This keeps the target DQN up-to-date
    with the online DQN. Without this synchronization, the target DQN will go out-of-date
    and hamper the training process by producing poor estimates of the best next-step
    Q-values in the Bellman equation.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在线 DQN 和目标 DQN 是由相同的 `createDeepQNetwork()` 函数创建的（[清单 11.5](#ch11ex05)）。它们是两个具有相同拓扑结构的深度卷积网络。因此，它们具有完全相同的层和权重集。权重值周期性地从在线
    DQN 复制到目标 DQN（在默认设置的 snake-dqn 中每 1,000 步）。这使目标 DQN 与在线 DQN 保持同步。没有这种同步，目标 DQN
    将过时，并通过产生贝尔曼方程中最佳下一步 Q 值的劣质估计来阻碍训练过程。
- en: Loss function for Q-value prediction and backpropagation
  id: totrans-307
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Q 值预测和反向传播的损失函数
- en: With both predicted and target Q-values at hand, we use the familiar `meanSquaredError`
    loss function to compute the discrepancy between the two ([figure 11.16](#ch11fig16)).
    At this point, we’ve managed to turn our DQN training process into a regression
    problem, not unlike previous examples such as Boston-housing and Jena-weather.
    The error signal from the `meanSquareError` loss drives the backpropagation; the
    resulting weight updates are used to update the online DQN.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 有了预测和目标 Q 值，我们使用熟悉的 `meanSquaredError` 损失函数来计算两者之间的差异（[图 11.16](#ch11fig16)）。在这一点上，我们已经成功将我们的
    DQN 训练过程转化为一个回归问题，类似于以前的例子，如波士顿房屋和耶拿天气。来自 `meanSquaredError` 损失的误差信号驱动反向传播；由此产生的权重更新用于更新在线
    DQN。
- en: Figure 11.16\. Putting the `actionQs` and `targetQs` together in order to calculate
    the online DQN’s `meanSquaredError` prediction error and thereby use backpropagation
    to update its weights. Most parts of this diagram have already been shown in [figures
    11.12](#ch11fig12) and [11.13](#ch11fig13). The newly added parts are the `meanSquaredError`
    loss function and the backpropagation step based on it, located in the bottom-right
    part of the diagram.
  id: totrans-309
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.16\. 将 `actionQs` 和 `targetQs` 结合在一起，以计算在线 DQN 的 `meanSquaredError` 预测误差，从而使用反向传播来更新其权重。这张图的大部分部分已经在
    [图 11.12](#ch11fig12) 和 [11.13](#ch11fig13) 中展示过。新添加的部分是 `meanSquaredError` 损失函数和基于它的反向传播步骤，位于图的右下部分。
- en: '![](11fig15_alt.jpg)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![11fig15_alt.jpg](11fig15_alt.jpg)'
- en: The schematic diagram in [figure 11.16](#ch11fig16) includes parts we’ve already
    shown in [figures 11.12](#ch11fig12) and [11.13](#ch11fig13). It puts those parts
    together and adds the new boxes and arrows for the `meanSquaredError` loss and
    the backpropagation based on it (see the bottom-right of the diagram). This completes
    the full picture of the deep Q-learning algorithm we use to train our snake-game
    agent.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11.16](#ch11fig16) 中的示意图包括我们已经在 [图 11.12](#ch11fig12) 和 [11.13](#ch11fig13)
    中展示过的部分。它将这些部分放在一起，并添加了新的框和箭头，用于 `meanSquaredError` 损失和基于它的反向传播（见图的右下部分）。这完成了我们用来训练蛇游戏代理的深度
    Q 学习算法的完整图景。'
- en: The code in [listing 11.9](#ch11ex09) has a close correspondence with the diagram
    in [figure 11.16](#ch11fig16). It is the `trainOnReplayBatch()` method of the
    `SnakeGameAgent` class in snake-dqn/agent.js, which plays a central role in our
    RL algorithm. The method defines a loss function that calculates the `meanSquaredError`
    between the predicted and target Q-values. It then calculates the gradients of
    the `meanSquaredError` with respect to the online DQN’s weights using the `tf.variableGrads()`
    function ([appendix B](kindle_split_030.html#app02), section B.4 contains a detailed
    discussion of TensorFlow.js’s gradient-computing functions such as `tf.variableGrads()`).
    The calculated gradients are used to update the DQN’s weights with the help of
    an optimizer. This nudges the online DQN in the direction of making more accurate
    estimates of the Q-values. Repeated over millions of iterations, this leads to
    a DQN that can guide the snake to a decent performance. For the following listing,
    the part of the code responsible for calculating the target Q-values (`targetQs`)
    has already been shown in [listing 11.8](#ch11ex08).
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 代码清单11.9中的代码与[图11.16](#ch11fig16)中的图表紧密对应。这是在蛇DQN / agent.js中的SnakeGameAgent类的trainOnReplayBatch()方法，它在我们的强化学习算法中发挥着核心作用。该方法定义了一个损失函数，该函数计算预测Q值和目标Q值之间的meanSquaredError。然后，它使用`tf.variableGrads()`函数（[附录B](kindle_split_030.html#app02)，第B.4节包含了有关TensorFlow.js的梯度计算函数（如`tf.variableGrads()`）的详细讨论）计算在线DQN权重相对于meanSquaredError的梯度。通过优化器使用计算出的梯度来更新DQN的权重。这将促使在线DQN朝着更准确的Q值估计方向移动。重复数百万次后，这将导致DQN能够引导蛇达到不错的性能。对于下面的列表，已经展示了负责计算目标Q值（`targetQs`）的代码部分（参见[代码清单11.8](#ch11ex08)）。
- en: Listing 11.9\. The core function that trains the DQN
  id: totrans-313
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 代码清单11.9。训练DQN的核心函数
- en: '[PRE13]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '***1*** Gets a random batch of examples from the replay buffe'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 从重播缓冲区中获取一组随机示例'
- en: '***2*** lossFunction returns a scalar and will be used for backpropagation.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** lossFunction返回标量，将用于反向传播。'
- en: '***3*** The predicted Q-values'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 预测的Q值'
- en: '***4*** The target Q-values calculated by applying the Bellman equation'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 通过应用贝尔曼方程计算的目标Q值'
- en: '***5*** Uses MSE as a measure of the discrepancy between the predicted and
    target Q-values'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 使用均方误差(MSE)作为预测和目标Q值之间差距的度量'
- en: '***6*** Calculates the gradient of lossFunction with respect to weights of
    the online DQN'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 计算损失函数相对于在线DQN权重的梯度'
- en: '***7*** Updates the weights using the gradients through an optimizer'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7*** 通过优化器使用梯度更新权重'
- en: 'That’s it for the internal details of the deep Q-learning algorithm. The training
    based on this algorithm can be started with the following command in the Node.js
    environment:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '至此，深度Q学习算法的内部细节就介绍完了。在Node.js环境中，可以使用以下命令开始基于这个算法的训练:'
- en: '[PRE14]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Add the `--gpu` flag to the command to speed up the training if you have a
    CUDA-enabled GPU. This `--logDir` flag lets the command log the following metrics
    to the TensorBoard log directory during training: 1) the running average of the
    cumulative rewards from the 100 most recent game episodes (`cumulativeReward100`);
    2) the running average of the number of fruits eaten in the 100 most recent episodes
    (`eaten100`); 3) the value of the exploration parameter (`epsilon`); and 4) the
    training speed in number of steps per second (`framesPerSecond`). These logs can
    be viewed by launching TensorBoard with the following commands and navigating
    to the HTTP URL of the TensorBoard frontend (by default: http://localhost:6006):'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有支持CUDA的GPU，请将`--gpu`标志添加到命令中，以加快训练速度。此`--logDir`标志让该命令在训练过程中将以下指标记录到TensorBoard日志目录中:1）最近100个游戏周期内累计奖励的运行平均值（cumulativeReward100）；2）最近100个周期内食用水果数量的运行平均值（eaten100）；3）探索参数的值（epsilon）；4）每秒钟处理的步数（framesPerSecond）的训练速度。这些日志可以通过使用以下命令启动TensorBoard并导航到TensorBoard前端的HTTP
    URL（默认为：http://localhost:6006）进行查看：
- en: '[PRE15]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[Figure 11.17](#ch11fig17) shows a set of typical log curves from the training
    process. As seen frequently in RL training, the `cumulativeReward100` and `eaten100`
    curves both show fluctuation. After a few hours of training, the model is able
    to reach a best `cumulativeReward100` of 70–80 and a best `eaten100` of about
    12.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.17](#ch11fig17)展示了一组训练过程中典型的对数曲线。在强化学习中，cumulativeReward100和eaten100曲线都经常展现出波动。经过几个小时的训练，模型可以达到cumulativeReward100的最佳成绩为70-80，eaten100的最佳成绩约为12。'
- en: Figure 11.17\. Example logs from a snake-dqn training process in tfjs-node.
    The panels show 1) `cumulativeReward100`, a moving average of the cumulative reward
    obtained in the most recent 100 games; 2) `eaten100`, a moving average of the
    number of fruits eaten in the most recent 100 games; 3) `epsilon`, the value of
    epsilon, from which you can see the time course of the epsilon-greedy policy;
    and 4) `framesPerSecond`, a measure of the training speed.
  id: totrans-327
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.17：tfjs-node 中蛇的强化学习训练过程的示例日志。面板显示：1）`cumulativeReward100`，最近 100 场游戏的累积奖励的移动平均；2）`eaten100`，最近
    100 场游戏中水果被吃的移动平均；3）`epsilon`，epsilon 的值，您可以从中看到 epsilon-greedy 策略的时间进程；以及 4）`framesPerSecond`，训练速度的度量。
- en: '![](11fig16_alt.jpg)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.18](11fig16_alt.jpg)'
- en: The training script also saves the model to the relative path ./models/dqn every
    time a new best `cumulativeReward100` value has been achieved. The saved model
    is served from the web frontend when the `yarn watch` command is invoked. The
    frontend displays the Q-values predicted by the DQN at every step of the game
    (see [figure 11.18](#ch11fig18)). The epsilon-greedy policy used during training
    is replaced with the “always-greedy” policy during the post-training gameplay.
    The action that corresponds to the highest Q-value (for example, 33.9 for going
    straight in [figure 11.18](#ch11fig18)) is always chosen as the snake’s action.
    This gives you an intuitive understanding of how the trained DQN plays the game.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 训练脚本还会在每次达到新的最佳 `cumulativeReward100` 值时，将模型保存到相对路径`./models/dqn`。在调用 `yarn
    watch` 命令时，从 web 前端加载保存的模型。前端会在游戏的每一步显示 DQN 预测的 Q 值（参见[图 11.18](#ch11fig18)）。在训练期间使用的
    epsilon-greedy 策略在训练后的游戏中被“始终贪婪”的策略所取代。蛇的动作总是选择对应于最高 Q 值的动作（例如，在[图 11.18](#ch11fig18)中，直行的
    Q 值为 33.9）。这可以直观地了解训练后的 DQN 如何玩游戏。
- en: Figure 11.18\. The Q-values estimated by a trained DQN are displayed as numbers
    and overlaid as different shades of green in the game’s frontend.
  id: totrans-330
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.18：经过训练的 DQN 估计的 Q 值以数字形式显示，并以不同的绿色叠加在游戏的前端。
- en: '![](11fig17.jpg)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.17](11fig17.jpg)'
- en: There are a couple of interesting observations from the snake’s behavior. First,
    the number of fruits actually eaten by the snake in the frontend demo (~18) is
    on average greater than the `eaten100` curve from the training logs (~12). This
    is because of the removal of the epsilon-greedy policy, which abolishes random
    actions in the gameplay. Recall that epsilon is maintained as a small but nonzero
    value throughout the late stage of the DQN’s training (see the third panel of
    [figure 11.17](#ch11fig17)). The random actions caused by this lead to premature
    deaths occasionally, and this is the cost of exploratory behavior. Second, the
    snake has developed an interesting strategy of going to the edges and corners
    of the board before approaching the fruit, even when the fruit is located near
    the center of the board. This strategy is effective in helping the snake reduce
    the likelihood of bumping into itself when its length is moderately large (for
    example, in the range of 10–18). This is not bad, but it is not perfect either
    because there are smarter strategies that the snake hasn’t developed. For example,
    the snake frequently traps itself in a circle when its length gets above 20\.
    This is as far as the algorithm in the snake-dqn can take us. To improve the snake
    agent further, we need to tweak the epsilon-greedy algorithm to encourage the
    snake to explore better moves when its length is long.^([[9](#ch11fn9)]) In the
    current algorithm, the degree of exploration is too low once the snake grows to
    a length that calls for skillful maneuvering around its own body.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 从蛇的行为中有几个有趣的观察。首先，在前端演示中，蛇实际吃到的水果数量（约为 18）平均要大于训练日志中的 `eaten100` 曲线（约为 12）。这是因为
    epsilon-greedy 策略的移除，这消除了游戏过程中的随机动作。请记住，epsilon 在 DQN 训练的后期维持为一个小但非零的值（参见[图 11.17](#ch11fig17)的第三个面板）。由此引起的随机动作偶尔会导致蛇的提前死亡，这就是探索性行为的代价。其次，蛇在靠近水果之前会经过棋盘的边缘和角落，即使水果位于棋盘的中心附近。这种策略对于帮助蛇在长度适中（例如，10-18）时减少碰到自己的可能性是有效的。这并不是坏事，但也不是完美的，因为蛇尚未形成更聪明的策略。例如，蛇在长度超过
    20 时经常陷入一个循环。这就是蛇的强化学习算法能够带给我们的。为了进一步改进蛇的智能体，我们需要调整 epsilon-greedy 算法，以鼓励蛇在长度较长时探索更好的移动方式。[[9](#ch11fn9)]
    在当前的算法中，一旦蛇的长度需要在其自身周围熟练操纵时，探索的程度太低。
- en: ⁹
  id: totrans-333
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁹
- en: ''
  id: totrans-334
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For example, see [https://github.com/carsonprindle/OpenAIExam2018](https://github.com/carsonprindle/OpenAIExam2018).
  id: totrans-335
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 例如，查看[https://github.com/carsonprindle/OpenAIExam2018](https://github.com/carsonprindle/OpenAIExam2018)。
- en: This concludes our tour of the DQN technique for RL. Our algorithm is modeled
    after the 2015 paper “Human-Level Control through Deep Reinforcement Learning,”^([[10](#ch11fn10)])
    in which researchers at DeepMind demonstrated for the first time that combining
    the power of deep neural networks and RL enables machines to solve many Atari
    2600-style video games. The snake-dqn solution we’ve demonstrated is a simplified
    version of DeepMind’s algorithm. For instance, our DQN looks at the observation
    from only the current step, while DeepMind’s algorithm combines the current observation
    with observations from the previous several steps as the input to the DQN. But
    our example captures the essence of the groundbreaking technique—namely, using
    a deep convnet as a powerful function approximator to estimate the state-dependent
    values of actions, and training it using MDP and the Bellman equation. Subsequent
    feats by RL researchers, such as conquering the games of Go and chess, are based
    on a similar wedding between deep neural networks and traditional non-deep-learning
    RL methods.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们对 DQN 技术的介绍结束了。我们的算法是基于2015年的论文“通过深度强化学习实现人类水平的控制”，[[10](#ch11fn10)]，在该论文中，DeepMind
    的研究人员首次证明，结合深度神经网络和强化学习的力量使得机器能够解决许多类似 Atari 2600 的视频游戏。我们展示的 snake-dqn 解决方案是
    DeepMind 算法的简化版本。例如，我们的 DQN 仅查看当前步骤的观察，而 DeepMind 的算法将当前观察与前几个步骤的观察结合起来作为 DQN
    的输入。但我们的示例捕捉到了这一划时代技术的本质——即使用深度卷积网络作为强大的函数逼近器来估计动作的状态相关值，并使用 MDP 和贝尔曼方程进行训练。强化学习研究人员的后续成就，如征服围棋和国际象棋等游戏，都基于类似的深度神经网络和传统非深度学习强化学习方法的结合。
- en: ^(10)
  id: totrans-337
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(10)
- en: ''
  id: totrans-338
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Volodymyr Mnih et al., “Human-Level Control through Deep Reinforcement Learning,”
    *Nature*, vol. 518, 2015, pp. 529–533, [www.nature.com/articles/nature14236/](http://www.nature.com/articles/nature14236/).
  id: totrans-339
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Volodymyr Mnih 等人，《深度强化学习实现人类水平的控制》，*自然*, vol. 518, 2015, pp. 529–533，[www.nature.com/articles/nature14236/](http://www.nature.com/articles/nature14236/)。
- en: Materials for further reading
  id: totrans-340
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 进一步阅读材料
- en: 'Richard S. Sutton and Andrew G. Barto, *Reinforcement Learning: An Introduction*,
    A Bradford Book, 2018.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Richard S. Sutton 和 Andrew G. Barto，《强化学习导论》，A Bradford 书籍，2018。
- en: 'David Silver’s lecture notes on reinforcement learning at University College
    London: [http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html).'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: David Silver 在伦敦大学学院的强化学习讲座笔记：[http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html)。
- en: Alexander Zai and Brandon Brown, *Deep Reinforcement Learning in Action*, Manning
    Publications, in press, [www.manning.com/books/deep-reinforcement-learning-in-action](http://www.manning.com/books/deep-reinforcement-learning-in-action).
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alexander Zai 和 Brandon Brown，《深度强化学习实战》，Manning 出版社，即将出版，[www.manning.com/books/deep-reinforcement-learning-in-action](http://www.manning.com/books/deep-reinforcement-learning-in-action)。
- en: 'Maxim Laplan, *Deep Reinforcement Learning Hands-On: Apply Modern RL Methods,
    with Deep Q-networks, Value Iteration, Policy Gradients, TRPO, AlphaGo Zero, and
    More*, Packt Publishing, 2018.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maxim Laplan，《深度强化学习实战：应用现代强化学习方法，包括深度 Q 网络，值迭代，策略梯度，TRPO，AlphaGo Zero 等》，Packt
    出版社，2018。
- en: Exercises
  id: totrans-345
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习
- en: In the cart-pole example, we used a policy network consisting of a hidden dense
    layer with 128 units, as it was the default setting. How does this hyperparameter
    affect the policy-gradient-based training? Try changing it to a small value such
    as 4 or 8 and comparing the resulting learning curve (mean steps per game versus
    iteration curve) with the one from the default hidden-layer size. What does that
    tell you about the relation between model capacity and its effectiveness in estimating
    the best action?
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在小车摆杆示例中，我们使用了一个策略网络，其中包含一个带有128个单元的隐藏密集层，因为这是默认设置。这个超参数如何影响基于策略梯度的训练？尝试将其更改为小值，如4或8，并将结果的学习曲线（每游戏平均步数与迭代曲线）与默认隐藏层大小的曲线进行比较。这对模型容量和其估计最佳动作的有效性之间的关系告诉了你什么？
- en: We mentioned that one of the advantages of using machine learning to solve a
    problem like cart-pole is the economy of human effort. Specifically, if the environment
    unexpectedly changes, we don’t need to figure out *how* it has really changed
    and rework the physical equations. Instead, we can just let the agent re-learn
    the problem on its own. Prove to yourself that this is the case by following these
    steps. First, make sure that the cart-pole example is launched from source code
    and not the hosted web page. Train a working cart-pole policy network using the
    regular approach. Second, edit the value of `this.gravity` in cart-pole/cart_pole.js
    and change it to a new value (say, 12, if you want to pretend that we’ve moved
    the cart-pole setup to an exoplanet with a higher gravity than Earth!). Launch
    the page again, load the policy network you’ve trained in the first step, and
    test it. Can you confirm that it performs significantly worse than before, just
    because of the gravity change? Finally, train the policy network a few more iterations.
    Can you see the policy getting better at the game again (adapting to the new environment)?
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们提到使用机器学习解决类似倒立摆的问题的一个优点是人力经济性。具体来说，如果环境意外改变，我们不需要弄清楚它是如何真正改变的并重新确定物理方程，而是可以让代理人自行重新学习问题。通过以下步骤向自己证明这一点。首先，确保倒立摆示例是从源代码而不是托管的网页启动的。使用常规方法训练一个有效的倒立摆策略网络。其次，编辑
    cart-pole/cart_pole.js 中的 `this.gravity` 的值，并将其更改为一个新值（例如，如果您要假装我们将倒立摆的配置移到一个比地球更高重力的外行星上，可以将其改为
    12）。再次启动页面，加载您在第一步训练的策略网络，并对其进行测试。你能确认它因为重力的改变而表现明显更差吗？最后，再多训练几次策略网络。你能看到策略又逐渐适应新环境而在游戏中表现越来越好吗？
- en: (Exercise on MDP and Bellman equation) The example of MDP we presented in [section
    11.3.2](#ch11lev2sec5) and [figure 11.10](#ch11fig10) was simple in the sense
    that it was fully deterministic because there is no randomness in the state transitions
    and the associated rewards. But many real-world problems are better described
    as stochastic (random) MDPs. In a stochastic MDP, the state the agent will end
    up in and the reward it will get after taking an action follows a probabilistic
    distribution. For instance, as [figure 11.19](#ch11fig19) shows, if the agent
    takes action *A*[1] at state *S*[1], it will end up in state *S*[2] with a probability
    of 0.5 and in state *S*[3] with a probability of 0.5\. The rewards associated
    with the two state transitions are different. In such stochastic *MDPs*, the agent
    must take into account the randomness by calculating the *expected* future reward.
    The expected future reward is a weighted average of all possible rewards, with
    weights being the probabilities. Can you apply this probabilistic approach and
    estimate the Q-values for *a*[1] and *a*[2] at *s*[1] in the figure? Based on
    the answer, is *a*[1] or *a*[2] the better action at state s1?
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: （有关MDP和贝尔曼方程的练习）我们在 [第11.3.2节](#ch11lev2sec5) 和 [图11.10](#ch11fig10) 中提供的MDP示例在一定程度上是简单的，因为状态转移和相关奖励没有随机性。但是，许多现实世界的问题更适合描述为随机MDP。在随机MDP中，代理人在采取行动后将进入的状态和获得的奖励遵循概率分布。例如，如
    [图11.19](#ch11fig19)所示，如果代理人在状态*S*[1]采取行动*A*[1]，它将以0.5的概率进入状态*S*[2]，以0.5的概率进入状态*S*[3]。与这两个状态转换关联的奖励是不同的。在这种随机MDP中，代理人必须计算*预期*未来奖励，以考虑随机性。预期未来奖励是所有可能奖励的加权平均值，权重为概率。你能应用这种概率方法并在图中估计*s*[1]的*a*[1]和*a*[2]的Q值吗？根据答案，在状态*s*[1]时，*a*[1]和*a*[2]哪个是更好的行动？
- en: Figure 11.19\. The diagram for the MDP in the first part of exercise 3
  id: totrans-349
  prefs:
  - PREF_IND
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.19. 练习3第一部分的MDP图表
- en: '![](11fig18.jpg)'
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![11fig18.jpg](11fig18.jpg)'
- en: Now let’s look at a slightly more complicated stochastic MDP, one that involves
    more than one step (see [figure 11.20](#ch11fig20)). In this slightly more complex
    case, you need to apply the recursive Bellman equation in order to take into account
    the best possible future rewards after the first action, which are themselves
    stochastic. Note that sometimes the episode ends after the first step, and sometimes
    it will last another step. Can you decide which action is better at *s*[1]? For
    this problem, you can use a reward discount factor of 0.9.
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在让我们看一个稍微复杂一点的随机MDP，其中涉及多个步骤（参见 [图11.20](#ch11fig20)）。在这种稍微复杂的情况下，您需要应用递归的贝尔曼方程，以考虑第一步之后的可能的最佳未来奖励，这些奖励本身也是随机的。请注意，有时在第一步之后，该情节结束，而有时它将持续进行另一步。你能决定在*s*[1]时哪个行动更好吗？对于这个问题，您可以使用奖励折扣因子0.9。
- en: Figure 11.20\. The diagram for the MDP in the second part of exercise 3
  id: totrans-352
  prefs:
  - PREF_IND
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.20。练习3第二部分中MDP的图表
- en: '![](11fig19_alt.jpg)'
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](11fig19_alt.jpg)'
- en: In the snake-dqn example, we used the epsilon-greedy policy to balance the needs
    for exploration and exploitation. The default setting decreases epsilon from an
    initial value of 0.5 to a final value of 0.01 and holds it there. Try changing
    the final epsilon value to a large value (such as 0.1) or a smaller one (such
    as 0), and observe the effects on how well the snake agent learns. Can you explain
    the resulting difference in terms of the role epsilon plays?
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在贪吃蛇-dqn示例中，我们使用ε-贪心策略来平衡探索和利用的需求。默认设置将ε从初始值0.5减小到最终值0.01，并将其保持在那里。尝试将最终ε值更改为较大的值（例如0.1）或较小的值（例如0），并观察蛇代理学习效果的影响。您能解释ε扮演的角色造成的差异吗？
- en: Summary
  id: totrans-355
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: As a type of machine learning, RL is about learning to make optimal decisions.
    In an RL problem, an agent learns to select actions in an environment to maximize
    a metric called the *cumulative reward*.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为一种机器学习类型，强化学习是关于学习如何做出最优决策。在强化学习问题中，代理学习在环境中选择行动以最大化称为*累积奖励*的指标。
- en: Unlike supervised learning, there are no labeled training datasets in RL. Instead,
    the agent must learn what actions are good under different circumstances by trying
    out random actions.
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与监督学习不同，RL中没有标记的训练数据集。相反，代理必须通过尝试随机动作来学习在不同情况下哪些动作是好的。
- en: 'We explored two commonly used types of RL algorithms: policy-based methods
    (using the cart-pole example) and Q-value-based methods (using the snake example).'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们探讨了两种常用的强化学习算法类型：基于策略的方法（以倒立摆为例）和基于Q值的方法（以贪吃蛇为例）。
- en: A policy is an algorithm by which the agent picks an action based on the current
    state observation. A policy can be encapsulated in a neural network that takes
    state observation as its input and produces an action selection as its output.
    Such a neural network is called a *policy network*. In the cart-pole problem,
    we used policy gradients and the REINFORCEMENT method to update and train a policy
    network.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略是一种算法，代理根据当前状态观察选择动作。策略可以封装在一个神经网络中，该网络将状态观察作为输入并产生动作选择作为输出。这样的神经网络称为*策略网络*。在倒立摆问题中，我们使用策略梯度和REINFORCEMENT方法来更新和训练策略网络。
- en: Unlike the policy-based methods, Q-learning uses a model called *Q-network*
    to estimate the values of actions under a given observed state. In the snake-dqn
    example, we demonstrated how a deep convnet can serve as the Q-network and how
    it can be trained by using the MDP assumption, the Bellman equation, and a construct
    called *replay* *memory*.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与基于策略的方法不同，Q学习使用一种称为*Q网络*的模型来估算在给定观察状态下行动的值。在贪吃蛇-dqn示例中，我们演示了深度卷积网络如何作为Q网络以及如何通过使用MDP假设、贝尔曼方程和一种称为*回放*
    *记忆*的结构来训练它。
