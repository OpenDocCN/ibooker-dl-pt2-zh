- en: Chapter 4\. Text Generation with GPT Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the first chapters of this book, we have taken our first steps into the world
    of Large Language Models (LLMs). We delved into various applications, such as
    classification and semantic search, employing models that focus on representing
    text, like BERT and its derivatives.
  prefs: []
  type: TYPE_NORMAL
- en: As we progressed, we used models trained primarily for text generation, models
    that are often referred to as Generative Pre-trained Transformers (GPT). These
    models have the remarkable ability to generate text in response to *prompts* from
    the user. Through *prompt engineering*, we can design these prompts in a way that
    enhances the quality of the generated text.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will explore these generative models in more detail and
    dive into the realm of prompt engineering, reasoning with generative models, verification
    and even evaluating their output.
  prefs: []
  type: TYPE_NORMAL
- en: Using Text Generation Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we start with the fundamentals of prompt engineering, it is essential
    to explore the basics of utilizing a text generation model. How do we select the
    model to use? Do we use a proprietary or open-source model? How can we control
    the generated output? These questions will serve as our stepping stones into using
    text generation models.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing a Text Generation Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Choosing a text generation model starts with choosing between proprietary models
    or open-source models. Although proprietary models are generally more performant,
    we focus in this book more on open-source models as they offer more flexibility
    and are free to use.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4-1](#fig_1_foundation_models) shows a small selection of impactful
    foundation models, LLMs that have been pre-trained on vast amounts of text data
    and are often fine-tuned for specific applications.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Foundation models ](assets/text_generation_with_gpt_models_479875_01.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-1\. Foundation models
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: From those foundation models, hundreds if not thousands of models have been
    fine-tuned, one more suitable for certain tasks than another. Choosing the model
    to use can be a daunting task!
  prefs: []
  type: TYPE_NORMAL
- en: We generally advise starting out with a small and recently released foundation
    model, like Llama 2 or Mistral 7B in the illustration in [Figure 4-1](#fig_1_foundation_models)
    This allows for quick iteration and as a result a thorough understanding of whether
    the model is suitable for your use case. Moreover, a smaller model requires less
    GPU memory (VRAM) which makes it easier and faster to run if you do not have a
    large GPU. Scaling up tends to be a nicer experience than scaling down.
  prefs: []
  type: TYPE_NORMAL
- en: In the examples throughout this chapter, we will employ a model from the Zephyr
    family, namely [Zephyr 7B-beta](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta).
    They are models fine-tuned on Mistral 7B, a relatively small but quite capable
    open-source LLM.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re taking your first steps in generative AI, it’s important to start
    with a smaller model. This provides a great introduction and lays a solid foundation
    for progressing to larger models.
  prefs: []
  type: TYPE_NORMAL
- en: Loading a Text Generation Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: “How to load a text generation model” can actually be a chapter by itself. There
    are dozens of packages out there each with their compression and inference strategies
    to squeeze out performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most straightforward method of doing so is through the well-known HuggingFace
    Transformers library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: To use the model, we will have to take a closer look at its prompt template.
    Any LLM requires a specific template so that it can differentiate between recent
    and older query/answer pairs.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate, let us ask the LLM to make a joke about chickens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Aside from our main prompt, we also generated a system prompt that provides
    context or guidance to an LLM for generating the response. As illustrated in [Figure 4-2](#fig_2_the_template_zephyr_expects_when_interacting_with),
    the prompt template helps the LLM understand the difference between types of prompts
    and also between text generated by the LLM and the user.
  prefs: []
  type: TYPE_NORMAL
- en: '![The template Zephyr expects when interacting with the model.](assets/text_generation_with_gpt_models_479875_02.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-2\. The template Zephyr expects when interacting with the model.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Using that prompt, we can let the LLM give an answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Which outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now that we know how to create a prompt using a chat template, let us explore
    how we can control the output of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Controlling the Model Output
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Other than prompt engineering, we can control the kind of output that we want
    by adjusting the model parameters. In our previous example, you might have noticed
    that we used several parameters in the `pipe` function, including `temperature`
    and `top_p`.
  prefs: []
  type: TYPE_NORMAL
- en: These parameters control the randomness of the output. A part of what makes
    LLMs exciting technology is that it can generate different responses for the exact
    same prompt. Each time an LLM needs to generate a token, it assigns a likelihood
    number to each possible token.
  prefs: []
  type: TYPE_NORMAL
- en: As illustrated in [Figure 4-3](#fig_3_the_model_chooses_the_next_token_to_generate_based),
    in the sentence “*I am driving a…*” the likelihood of that sentence being followed
    by tokens like “*car*” or “*truck*” is generally higher than a token like “elephant”.
    However, there is still a possibility of “*elephant*” being generated but it is
    much lower.
  prefs: []
  type: TYPE_NORMAL
- en: '![The model chooses the next token to generate based on their likelihood scores.](assets/text_generation_with_gpt_models_479875_03.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-3\. The model chooses the next token to generate based on their likelihood
    scores.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Temperature
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `temperature` controls the randomness or the creativity of the text generated.
    It defines how likely it is to choose tokens that are less probable. The underlying
    idea is that a temperature of 0 generates the same response every time because
    it always chooses the most likely word. As illustrated in [Figure 4-4](#fig_4_a_higher_temperature_increases_the_likelihood_that),
    a higher value allows less probable words to be generated.
  prefs: []
  type: TYPE_NORMAL
- en: '![A higher temperature increases the likelihood that less probable tokens are
    generated  and vice versa.](assets/text_generation_with_gpt_models_479875_04.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-4\. A higher temperature increases the likelihood that less probable
    tokens are generated, and vice versa.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As a result, a higher temperature (e.g., 0.8) generally results in a more diverse
    output while a lower temperature (e.g., 0.2) creates a more deterministic output.
  prefs: []
  type: TYPE_NORMAL
- en: top_p
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`top_p`, also known as nucleus sampling, is a sampling technique that controls
    which subset of tokens (the nucleus) the LLM can consider. It will consider tokens
    until it reaches their cumulative probability. If we set top_p to 0.1, it will
    consider tokens until it reaches that value. If we set `top_p` to 1, it will consider
    all tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: As shown in [Figure 4-5](#fig_5_a_higher_top_p_increases_the_number_of_tokens_that),
    by lowering the value, it will consider fewer tokens and generally give less “creative”
    output whilst increasing the value allows the LLM to choose from more tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '![A higher top_p increases the number of tokens that can be selected to generate  and
    vice versa.](assets/text_generation_with_gpt_models_479875_05.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-5\. A higher top_p increases the number of tokens that can be selected
    to generate, and vice versa.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Similarly, the `top_k` parameter controls exactly how many tokens the LLM can
    consider. If you change its value to 100, the LLM will only consider the top 100
    most probable tokens.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in Table 5-1, these parameters allow the user to have a sliding scale
    between being creative (high `temperature` and `top_p`) and being predictable
    (lower `temperature` and `top_p`).
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](assets/text_generation_with_gpt_models_479875_06.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4-6\. Examples of use cases when selecting values for `temperature` and
    `top_p`.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Intro to Prompt Engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An essential part of working with text-generative LLMs is prompt engineering.
    By carefully designing our prompts we can guide the LLM to generate desired responses.
    Whether the prompts are questions, statements, or instructions, the main goal
    of prompt engineering is to elicit a useful response from the model.
  prefs: []
  type: TYPE_NORMAL
- en: However, prompt engineering is also more than just designing effective prompts.
    It can be used as a tool to evaluate the output of a model, design safeguards,
    and safety mitigation methods. This is an iterative process of prompt optimization
    and requires experimentation. There is not and unlikely will ever be a perfect
    prompt design.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will go through common methods for prompt engineering, and
    small tips and tricks to understand what the effect is of certain prompts. These
    skills allow us to understand the capabilities of LLMs and lie at the foundation
    of interfacing with these kinds of models.
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin by answering the question: What should be in a prompt?'
  prefs: []
  type: TYPE_NORMAL
- en: The Basic Ingredients of a Prompt
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An LLM is a prediction machine. Based on a certain input, the prompt, it tries
    to predict the words that might follow it. At its core, and as illustrated in
    [Figure 4-7](#fig_6_a_basic_example_of_a_prompt_no_instruction_is_giv), the prompt
    does not need to be more than just a few words to elicit a response from the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: '![A basic example of a prompt. No instruction is given so the LLM will simply
    try to complete the sentence.](assets/text_generation_with_gpt_models_479875_07.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-7\. A basic example of a prompt. No instruction is given so the LLM
    will simply try to complete the sentence.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: However, although the illustration works as a basic example, it fails to complete
    a specific task. Instead, we generally approach prompt engineering by asking a
    specific question or task the LLM should complete. To elicit the desired response,
    we need a more structured prompt.
  prefs: []
  type: TYPE_NORMAL
- en: For example, and as shown in [Figure 4-8](#fig_7_two_components_of_a_basic_instruction_prompt_the),
    we could ask the LLM to classify a sentence into either having positive or negative
    sentiment.
  prefs: []
  type: TYPE_NORMAL
- en: '![Two components of a basic instruction prompt  the instruction itself and
    the data it refers to.](assets/text_generation_with_gpt_models_479875_08.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-8\. Two components of a basic instruction prompt, the instruction itself
    and the data it refers to.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This extends the most basic prompt to one consisting of two components–the instruction
    itself and the data that relates to the instruction.
  prefs: []
  type: TYPE_NORMAL
- en: More complex use cases might require more components to be necessary in a prompt.
    For instance, to make sure the model only outputs “negative” or “positive” we
    can introduce output indicators that help guide the model. In [Figure 4-9](#fig_8_extending_the_prompt_with_an_output_indicator_whic),
    we prefix the sentence with “Text:” and add “Sentiment:” to prevent the model
    from generating a complete sentence. Instead, this structure indicates that we
    expect either “negative” or “positive”.
  prefs: []
  type: TYPE_NORMAL
- en: '![Extending the prompt with an output indicator which allows for a specific
    output.](assets/text_generation_with_gpt_models_479875_09.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-9\. Extending the prompt with an output indicator which allows for
    a specific output.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can continue adding or updating the elements of a prompt until we elicit
    the response we were looking for. We could add additional examples, describe the
    use case in more detail, provide additional context, etc. These components are
    merely examples and are not a limited set of possibilities. The creativity that
    comes with designing these components is key.
  prefs: []
  type: TYPE_NORMAL
- en: Although a prompt is a single piece of text it is tremendously helpful to think
    of prompts as pieces of a larger puzzle. Have I described the context of my question?
    Does the prompt have an example of the output?
  prefs: []
  type: TYPE_NORMAL
- en: Instruction-based Prompting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although prompting comes in many flavors, from discussing philosophy with the
    LLM to role-playing with your favorite superhero, prompting is often used to have
    the LLM answer a specific question or resolve a certain task. This is referred
    to as instruction-based prompting.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4-10](#fig_9_examples_of_use_cases_that_employ_instruction_base) illustrates
    a number of use cases in which instruction-based prompting plays an important
    role. We already did one of these in the previous example, namely supervised classification.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Examples of use cases that employ instruction based prompting.](assets/text_generation_with_gpt_models_479875_10.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-10\. Examples of use cases that employ instruction-based prompting.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Each of these tasks requires different formats of prompting and more specifically,
    different questions to be asked of the LLM. Asking the LLM to summarize a piece
    of text will not suddenly result in classification. To illustrate, examples of
    prompts for some of these use cases can be found in [Figure 4-11](#fig_10_prompt_examples_of_common_use_cases_notice_how_wi).
  prefs: []
  type: TYPE_NORMAL
- en: '![Prompt examples of common use cases. Notice how within a use case  the structure
    and location of the instruction can be changed.](assets/text_generation_with_gpt_models_479875_11.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-11\. Prompt examples of common use cases. Notice how within a use case,
    the structure and location of the instruction can be changed.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Although these tasks require different instructions, there is actually a lot
    of overlap in the prompting techniques used to improve the quality of the output.
    A non-exhaustive list of these techniques includes:'
  prefs: []
  type: TYPE_NORMAL
- en: Specificity
  prefs: []
  type: TYPE_NORMAL
- en: Accurately describe what you want to achieve. Instead of asking the LLM to “Write
    a description for a product.” ask it to “Write a description for a product in
    less than two sentences and use a formal tone.”.
  prefs: []
  type: TYPE_NORMAL
- en: Hallucination
  prefs: []
  type: TYPE_NORMAL
- en: LLMs may generate incorrect information confidently, which is referred to as
    hallucination. To reduce its impact, we can ask the LLM to only generate an answer
    if it knows the answer. If it does not know the answer, respond with “I don’t
    know”.
  prefs: []
  type: TYPE_NORMAL
- en: Order
  prefs: []
  type: TYPE_NORMAL
- en: Either begin or end your prompt with the instruction. Especially with long prompts,
    information in the middle is often forgotten. LLMs tend to focus on information
    either at the beginning of a prompt (primacy effect) or the end of a prompt (recency
    effect).
  prefs: []
  type: TYPE_NORMAL
- en: Here, specificity is arguably the most important aspect. An LLM does not know
    what you want unless you are specific in what you want to achieve and why.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced Prompt Engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: On the surface, creating a good prompt might seem straightforward. Ask a specific
    question, be accurate, add some examples and you are done! However, prompting
    can grow complex quite quickly and as a result is an often underestimated component
    of leveraging LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we will go through several advanced techniques for building up your prompts,
    starting with the iterative workflow of building up complex prompts all the way
    to using LLMs sequentially to get improved results. Eventually, we will even build
    up to advanced reasoning techniques.
  prefs: []
  type: TYPE_NORMAL
- en: The Potential Complexity of a Prompt
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we explored in the intro to prompt engineering, a prompt generally consists
    of multiple components. In our very first example, our prompt consisted of instruction,
    data, and output indicators. As we mentioned before, no prompt is limited to just
    these three components and you can build it up as complex as you want.
  prefs: []
  type: TYPE_NORMAL
- en: 'These advanced components can quickly make a prompt quite complex. Some common
    components are:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Persona*'
  prefs: []
  type: TYPE_NORMAL
- en: Describe what role the LLM should take on. For example, use *“You are an expert
    in astrophysics.”* if you want to ask a question about astrophysics.
  prefs: []
  type: TYPE_NORMAL
- en: '*Instruction*'
  prefs: []
  type: TYPE_NORMAL
- en: The task itself. Make sure this is as specific as possible. We do not want to
    leave much room for interpretation.
  prefs: []
  type: TYPE_NORMAL
- en: '*Context*'
  prefs: []
  type: TYPE_NORMAL
- en: Additional information describing the context of the problem or task. It answers
    questions like *“What is the reason for the instruction?”*.
  prefs: []
  type: TYPE_NORMAL
- en: '*Format*'
  prefs: []
  type: TYPE_NORMAL
- en: The format the LLM should use to output the generated text. Without it, the
    LLM will come up with a format itself which is troublesome in automated systems.
  prefs: []
  type: TYPE_NORMAL
- en: '*Audience*'
  prefs: []
  type: TYPE_NORMAL
- en: For whom the generated text should be. This also describes the level of the
    generated output. For education purposes, it is often helpful to use ELI5 (*“Explain
    it like I’m 5.”*)
  prefs: []
  type: TYPE_NORMAL
- en: '*Tone*'
  prefs: []
  type: TYPE_NORMAL
- en: The tone of voice the LLM should use in the generated text. If you are writing
    a formal email to your boss, you might not want to use an informal tone of voice.
  prefs: []
  type: TYPE_NORMAL
- en: '*Data*'
  prefs: []
  type: TYPE_NORMAL
- en: The main data related to the task itself.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate, let us extend the classification prompt that we had earlier and
    use all of the above components. This is demonstrated in [Figure 4-12](#fig_11_an_example_of_a_complex_prompt_with_many_component).
  prefs: []
  type: TYPE_NORMAL
- en: '![An example of a complex prompt with many components.](assets/text_generation_with_gpt_models_479875_12.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-12\. An example of a complex prompt with many components.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This complex prompt demonstrates the modular nature of prompting. We can add
    and remove components freely and judge their effect on the output. As illustrated
    in [Figure 4-13](#fig_12_iterating_over_modular_components_is_a_vital_part), we
    can slowly build up our prompt and explore the effect of each change.
  prefs: []
  type: TYPE_NORMAL
- en: '**![Iterating over modular components is a vital part of prompt engineering.
    ](assets/text_generation_with_gpt_models_479875_13.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4-13\. Iterating over modular components is a vital part of prompt engineering.**  **The
    changes are not limited to simply introducing or removing components. Their order,
    as we saw before with the recency and primacy effects, can affect the quality
    of the LLM’s output.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In other words, experimentation is vital when finding the best prompt for your
    use case. With prompting, we essentially have ourselves in an iterative cycle
    of experimentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Try it out yourself! Use the complex prompt to add and/or remove parts to observe
    its impact on the generated prompts. You will quickly notice when pieces of the
    puzzle are worth keeping. You can use your own data by adding it to the `data`
    variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Almost weekly there are new components of a prompt that might increase the accuracy
    of the output. There are all manners of components that we could add and creative
    components like using emotional stimuli (e.g., “This is very important for my
    career.”) are discovered on a weekly basis.
  prefs: []
  type: TYPE_NORMAL
- en: Part of the fun in prompt engineering is that you can be as creative as possible
    to figure out which combination of prompt components contribute to your use case.
    There are few constraints to develop a format that works for you.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, note that some prompts work better for certain models compared to
    others as their training data might be different or if they are trained for different
    purposes.**  **## In-Context Learning: Providing Examples'
  prefs: []
  type: TYPE_NORMAL
- en: In the previous sections, we tried to accurately describe what the LLM should
    do. Although accurate and specific descriptions help the LLM to understand the
    use case, we can go one step further.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of describing the task, why do we not just show the task?
  prefs: []
  type: TYPE_NORMAL
- en: We can provide the LLM with examples of exactly the thing that we want to achieve.
    This is often referred to as in-context learning, where we provide the model with
    correct examples.
  prefs: []
  type: TYPE_NORMAL
- en: As illustrated in [Figure 4-14](#fig_13_an_example_of_a_complex_prompt_with_many_component),
    this comes in a number of forms depending on how many examples you show the LLM.
    Zero-shot prompting does not leverage examples, one-shot prompts use a single
    example, and few-shot prompts use two or more examples.
  prefs: []
  type: TYPE_NORMAL
- en: '![An example of a complex prompt with many components.](assets/text_generation_with_gpt_models_479875_14.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-14\. An example of a complex prompt with many components.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Adopting the original phrase, we believe that “an example is worth a thousand
    words”. These examples provide a direct example of what and how the LLM should
    achieve.
  prefs: []
  type: TYPE_NORMAL
- en: We can illustrate this method with a simple example taken from the original
    paper describing this method. The goal of the prompt is to generate a sentence
    with a made-up word. To improve the quality of the resulting sentence, we can
    show the generative model an example of what a proper sentence with a made-up
    word would be.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do so, we will need to differentiate between our question (`user`) and the
    answers that were provided by the model (`assistant`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The prompt illustrates the need to differentiate between the user and assistant.
    If we did not, it would seem as if we were talking to ourselves:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use this prompt to run our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is a proper sentence using the made-up word “screeg”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As with all prompt components, one or few shot-prompting is not the be-all and
    end-all of prompt engineering. We can use it as one piece of the puzzle to further
    enhance the descriptions that we gave it. The model can still “choose”, through
    random sampling, to ignore the instructions
  prefs: []
  type: TYPE_NORMAL
- en: 'Chain Prompting: Breaking up the Problem'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In previous examples, we explored splitting up prompts into modular components
    to improve the performance of LLMs. Although this works well for many use cases,
    this might not be feasible for highly complex prompts or use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of breaking the problem within a prompt, we can do so between prompts.
    Essentially, we take the output of one prompt and use it as input for the next.
    Thereby creating a continuous chain of interactions that solves our problem.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate, let us say we want to use an LLM to create a product name, slogan,
    and sales pitch for us based on a number of product features. Although we can
    ask the LLM to do this in one go, we can instead break the problem up into pieces.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, and as illustrated in [Figure 4-16](#fig_15_chain_of_thought_prompting_uses_reasoning_examples),
    we get a sequential pipeline that first creates the product name, uses that with
    the product features as input to create the slogan, and finally, uses the features,
    product name, and slogan to create the sales pitch.
  prefs: []
  type: TYPE_NORMAL
- en: '![Using a description of a product s features  chain prompts to create a suitable
    name  slogan  and sales pitch.](assets/text_generation_with_gpt_models_479875_15.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-15\. Using a description of a product’s features, chain prompts to
    create a suitable name, slogan, and sales pitch.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This technique of chaining prompts allows the LLM to spend more time on each
    individual question instead of tackling the whole problem.
  prefs: []
  type: TYPE_NORMAL
- en: Let us illustrate this with a small example.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we ask the model first to create a name and slogan. Then, we
    can use the output to ask for a good sales pitch based on the product’s characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: 'This gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Although we need two calls to the model, a major benefit is that we can give
    each call different parameters. For instance, the number of tokens created was
    relatively small for the name and slogan whereas the pitch can be much longer.
  prefs: []
  type: TYPE_NORMAL
- en: 'It can be used for a variety of use cases, including:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Response validation*'
  prefs: []
  type: TYPE_NORMAL
- en: Asking the LLM to double-check previously generated outputs
  prefs: []
  type: TYPE_NORMAL
- en: '*Parallel prompts*'
  prefs: []
  type: TYPE_NORMAL
- en: Create multiple prompts in parallel and do a final pass to merge them. For example,
    ask multiple LLMs to generate multiple recipes in parallel and use the combined
    result to create a shopping list.
  prefs: []
  type: TYPE_NORMAL
- en: '*Writing stories*'
  prefs: []
  type: TYPE_NORMAL
- en: Leverage the LLM to write books or stories by breaking the problem down into
    components. For example, by first writing a summary, develop characters and build
    the story beats before diving into creating the dialogue.
  prefs: []
  type: TYPE_NORMAL
- en: In Chapter 6, we will go beyond chaining LLMs and chain other pieces of technology
    together, like memory, search, and more! Before that, this idea of prompt chaining
    will be explored further in the next sections describing more complex prompt chaining
    methods like self-consistency, chain-of-thought, and tree-of-thought.**  **# Reasoning
    with Generative Models
  prefs: []
  type: TYPE_NORMAL
- en: In the previous sections, we focused mostly on the modular component of prompts,
    building them up through iteration. These advanced prompt engineering techniques,
    like prompt chaining, proved to be the first step toward enabling complex reasoning
    with generative models.
  prefs: []
  type: TYPE_NORMAL
- en: To allow for this complex reasoning, it is a good moment to step back and explore
    what reasoning entails. To simplify, our methods of reasoning can be divided into
    system 1 and 2 thinking processes, as illustrated in Figure 5-X.
  prefs: []
  type: TYPE_NORMAL
- en: System 1 thinking represents automatic, intuitive, and near-instantaneous. It
    shares similarities with generative models that automatically generate tokens
    without any self-reflective behavior. In contrast, systems 2 thinking is a conscious,
    slow, and logical process, akin to brainstorming and self-reflection.
  prefs: []
  type: TYPE_NORMAL
- en: If we could give a generative model the ability of self-reflection, we would
    essentially be emulating the system 2 way of thinking which tends to produce more
    thoughtful responses than system 1 thinking.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will explore several techniques that attempt to mimic these
    kinds of thought processes of human reasoners with the aim of improving the output
    of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Chain-of-Thought: Think Before Answering'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first and major step towards complex reasoning in generative models was
    through a method called Chain-of-Thought (CoT). CoT aims to have the generative
    model “think” first rather than answering the question directly without any reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: As illustrated in [Figure 4-16](#fig_15_chain_of_thought_prompting_uses_reasoning_examples),
    it provides examples in a prompt that demonstrate the reasoning the model should
    do before generating its response. These reasoning processes are referred to as
    “thoughts”. This helps tremendously for tasks that involve a higher degree of
    complexity, like mathematical questions. Adding this reasoning step allows the
    model to distribute more compute over the reasoning process.
  prefs: []
  type: TYPE_NORMAL
- en: '![Chain of Thought prompting uses reasoning examples to persuade the generative
    model to use reasoning in its answer.](assets/text_generation_with_gpt_models_479875_16.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-16\. Chain-of-Thought prompting uses reasoning examples to persuade
    the generative model to use reasoning in its answer.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We will use the example they used in their paper to demonstrate this phenomenon.
    To start with, let’s explore the output of a standard prompt without CoT. Instead
    of providing a single query, we differentiate between the user and the assistant
    when providing examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the incorrect answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead, we will use CoT to have the model present its reasoning before giving
    the answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This time, we got the correct response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This reasoning process is especially helpful because the model does so before
    generating the answer. By doing so, it can leverage the knowledge it has generated
    thus far to compute the correct answer.
  prefs: []
  type: TYPE_NORMAL
- en: Zero-shot Chain-of-Thought
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although CoT is a great method for enhancing the output of a generative model,
    it does require one or more examples of reasoning in the prompt which the user
    might not have access to.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of providing examples, we can simply ask the generative model to provide
    the reasoning. There are many different forms that work but a common and effective
    method is to use the phrase “Let’s think step-by-step” which is illustrated in
    [Figure 4-17](#fig_16_chain_of_thought_prompting_without_using_examples).
  prefs: []
  type: TYPE_NORMAL
- en: '![Chain of Thought prompting without using examples. Instead  it uses the phrase  Let
    s think step by step  to prime reasoning in its answer. ](assets/text_generation_with_gpt_models_479875_17.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-17\. Chain-of-Thought prompting without using examples. Instead, it
    uses the phrase “Let’s think step-by-step” to prime reasoning in its answer.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Using the example we used before, we can simply append that phrase to the prompt
    to enable CoT-like reasoning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, we got the correct response but now without needing to provide examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This is why it is so important to “show your work” when doing calculations.
    By addressing the reasoning process we can justify the answer and be more sure
    of the answer.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Although the prompt “*Let’s think step-by-step*” can improve the output, you
    are not constrained by this exact formulation. Alternatives exist like “*Take
    a deep breath and think step-by-step*” and “*Let’s work through this problem step-by-step*”.
    The authors demonstrated the usefulness of coming up with alternative formulations.
  prefs: []
  type: TYPE_NORMAL
- en: '**Self-Consistency: Sampling Outputs**'
  prefs: []
  type: TYPE_NORMAL
- en: Using the same prompt multiple times can lead to different results if we allow
    for a degree of creativity through parameters like `temperature` and `top_p`.
    As a result, the quality of the output might improve or degrade depending on the
    random selection of tokens. In other words, luck!
  prefs: []
  type: TYPE_NORMAL
- en: To counteract this degree of randomness and improve the performance of generative
    models, self-consistency was introduced. This method asks the generative model
    the same prompt multiple times and takes the majority result as the final answer.
    During this process, each answer can be affected by different `temperature` and
    `top_p` values to increase the diversity of sampling.
  prefs: []
  type: TYPE_NORMAL
- en: As illustrated in [Figure 4-18](#fig_17_by_sampling_from_multiple_reasoning_paths_we_can),
    this method can further be improved by adding Chain-of-Thought prompting to improve
    its reasoning whilst only using the answer for the voting procedure.
  prefs: []
  type: TYPE_NORMAL
- en: '![By sampling from multiple reasoning paths  we can use majority voting to
    extract the most likely answer.](assets/text_generation_with_gpt_models_479875_18.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-18\. By sampling from multiple reasoning paths, we can use majority
    voting to extract the most likely answer.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Although this method works quite well to improve the output, it does require
    a single question to be asked multiple times. As a result, although the method
    can improve performance, it becomes *n* times slower where *n* is the number of
    output samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tree-of-Thought: Exploring Intermediate Steps'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ideas of Chain-of-Thought and Self-consistency are meant to enable more
    complex reasoning. By sampling from multiple “thoughts” and making them more thoughtful,
    we aim to improve the output of generative models.
  prefs: []
  type: TYPE_NORMAL
- en: These techniques scratch only the surface of what is currently being done to
    enable this complex reasoning. An improvement to these approaches can be found
    in Tree-of-Thought which allows for an in-depth exploration of several ideas.
  prefs: []
  type: TYPE_NORMAL
- en: The method works as follows. When faced with a problem that requires multiple
    reasoning steps, it often helps to break it down into pieces. At each step, and
    as illustrated in [Figure 4-19](#fig_18_by_leveraging_a_tree_based_structure_generative_m),
    the generative model is prompted to explore different solutions to the problem
    at hand. It then votes for the best solution and then continues to the next step.
  prefs: []
  type: TYPE_NORMAL
- en: '![By leveraging a tree based structure  generative models can generate intermediate
    thoughts to be rated. The most promising thoughts are kept and the lowest are
    pruned.](assets/text_generation_with_gpt_models_479875_19.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-19\. By leveraging a tree-based structure, generative models can generate
    intermediate thoughts to be rated. The most promising thoughts are kept and the
    lowest are pruned.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This method is tremendously helpful when needing to consider multiple paths,
    like when writing a story or coming up with creative ideas.
  prefs: []
  type: TYPE_NORMAL
- en: A disadvantage of this method is that it requires many calls to the generative
    models which slows the application significantly. Fortunately, there has been
    a successful attempt to convert the Tree-of-Thought framework into a simple prompting
    technique.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of calling the generative model multiple times, we ask the model to
    mimic that behavior by emulating a conversation between multiple experts. These
    experts will question each other until they reach a consensus. An example of a
    Tree-of-Thought prompt is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use this prompt to explore how an LLM might respond to complex questions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, it generated the correct answer by leveraging the discussion between
    multiple experts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: It is interesting to see such an elaborate conservation between “experts” and
    demonstrates the creativity that comes with prompt engineering.
  prefs: []
  type: TYPE_NORMAL
- en: Output Verification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Systems and applications built with generative models might eventually end up
    in production. When that happens, it is important that we verify and control the
    output of the model to prevent breaking the application and to create a robust
    generative AI application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reasons for validating the output might include:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Structured output*'
  prefs: []
  type: TYPE_NORMAL
- en: By default, most generative models create free-form text without adhering to
    specific structures other than those defined by natural language. Some use cases
    require their output to be structured in certain formats, like JSON.
  prefs: []
  type: TYPE_NORMAL
- en: '*Valid output*'
  prefs: []
  type: TYPE_NORMAL
- en: Even if we allow the model to generate structured output, it still has the capability
    to freely generate its content. For instance, when a model is asked to output
    either one of two choices, it should not come up with a third.
  prefs: []
  type: TYPE_NORMAL
- en: '*Ethics*'
  prefs: []
  type: TYPE_NORMAL
- en: Some open-source generative models have no guardrails and will generate outputs
    that do not consider safety or ethical considerations. For instance, use cases
    might require the output to be free of profanity, personally identifiable information
    (PII), bias, cultural stereotypes, etc.
  prefs: []
  type: TYPE_NORMAL
- en: '*Accuracy*'
  prefs: []
  type: TYPE_NORMAL
- en: Many use cases require the output to adhere to certain standards or performance.
    The aim is to double-check whether the generated information is factually accurate,
    coherent, or free from hallucination.
  prefs: []
  type: TYPE_NORMAL
- en: Controlling the output of a generative model, as we explored with parameters
    like `top_p` and `temperature`, is not an easy feat. These models require help
    to generate consistent output conforming to certain guidelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally, there are three ways of controlling the output of a generative model:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Examples*'
  prefs: []
  type: TYPE_NORMAL
- en: Provide a number of examples of the expected output.
  prefs: []
  type: TYPE_NORMAL
- en: '*Grammar*'
  prefs: []
  type: TYPE_NORMAL
- en: Control the token selection process.
  prefs: []
  type: TYPE_NORMAL
- en: '*Fine-tuning*'
  prefs: []
  type: TYPE_NORMAL
- en: Tune a model on data that contains the expected output
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will go through the first two methods. The third, fine-tuning
    a model, is left for Chapter 12 where we will in-depth into fine-tuning methods.
  prefs: []
  type: TYPE_NORMAL
- en: Providing Examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A simple and straightforward method to fix the output is to provide the generative
    model with examples of what the output should look like. As we explored before,
    few-shot learning is a helpful technique that guides the output of the generative
    model. This method can be generalized to guide the structure of the output as
    well.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let us consider an example where we want the generative model
    to create a character profile for an RPG game. We start by using no examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following structure which we truncated to prevent overly
    long descriptions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Although this is valid JSON, we might not want certain attributes like “strength”
    or “age”. Instead, we can provide the model with a number of examples that indicate
    the expected format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following which we again truncated to prevent overly long
    descriptions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The model perfectly followed the example we gave it which allows for more consistent
    behavior. This also demonstrates the importance of leveraging few-shot learning
    to improve the structure of the output and not only its content.
  prefs: []
  type: TYPE_NORMAL
- en: An important note here is that it is still up to the model whether it will adhere
    to your suggested format or not. Some models are better than others at following
    instructions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Grammar: Constrained Sampling'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Few-shot learning has a big disadvantage: we cannot explicitly prevent certain
    output from being generated. Although we guide the model and give it instructions,
    it might still not follow it entirely.'
  prefs: []
  type: TYPE_NORMAL
- en: Instead, packages have been rapidly to constrain and validate the output of
    generative models, like Guidance, Guardrails, and LMQL. In part, they leverage
    generative models to validate their own output, as illustrated in [Figure 4-20](#fig_19_use_an_llm_to_check_whether_the_output_correctly_f).
    The generative models retrieve the output as new prompts and attempt to validate
    it based on a number of predefined guardrails.
  prefs: []
  type: TYPE_NORMAL
- en: '![Use an LLM to check whether the output correctly follows our rules.](assets/text_generation_with_gpt_models_479875_20.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-20\. Use an LLM to check whether the output correctly follows our rules.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Similarly, as illustrated in [Figure 4-21](#fig_20_use_an_llm_to_generate_only_the_pieces_of_informat),
    it can also be used to control the formatting of the output by generating parts
    of its format ourselves as we already know how it should be structured.
  prefs: []
  type: TYPE_NORMAL
- en: '![Use an LLM to generate only the pieces of information we do not know beforehand.
    ](assets/text_generation_with_gpt_models_479875_21.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-21\. Use an LLM to generate only the pieces of information we do not
    know beforehand.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This process can be taken one step further and instead of validating the output
    we can already perform validation during the token sampling process. When sampling
    tokens, we can define a number of grammars or rules that the LLM should adhere
    to when choosing its next token. For instance, if we ask the model to either return
    “positive”, “negative” or “neutral” when performing sentiment classification,
    it might still return something else. As illustrated in [Figure 4-22](#fig_21_constrain_the_token_selection_to_only_three_possib),
    by constraining the sampling process, we can have the LLM only output what we
    are interested in.
  prefs: []
  type: TYPE_NORMAL
- en: '![Constrain the token selection to only three possible tokens   positive    neutral   and  negative
    .](assets/text_generation_with_gpt_models_479875_22.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-22\. Constrain the token selection to only three possible tokens:
    “positive”, “neutral”, and “negative”.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that this is still affected by parameters such as `top_p` and `temperature`
    and the illustrated is quite constrained.
  prefs: []
  type: TYPE_NORMAL
- en: Let us illustrate this phenomenon with llama-cpp-python, which is a library,
    like transformers, that we can use to load in our language model. It is generally
    used to efficiently load and use compressed models (through quantization; see
    Chapter 13).
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by downloading the quantized version of the model by running the following
    in your terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25] import httpx from llama_cpp.llama import Llama, LlamaGrammar # We load
    the JSON grammar from the official llama.cpp repository grammar = httpx.get(     "https://raw.githubusercontent.com/ggerganov/llama.cpp/master/grammars/json_arr.gbnf"
    ) grammar = LlamaGrammar.from_string(grammar.text) # Load a pre-quantized LLM
    llm = Llama("zephyr-7b-beta.Q4_K_M.gguf") [PRE26] import json # Run the generative
    model and ask it to create a character in JSON format response = llm(     "Create
    a warrior for an RPG in JSON format.",     max_tokens=-1,     grammar=grammar
    ) # Print the output in nicely-formatted JSON print(json.dumps(json.loads(response[''choices''][0][''text'']),
    indent=4)) [PRE27] [     {         "name": "Swordmaster",         "level": 10,
            "health": 250,         "mana": 100,         "strength": 18,         "dexterity":
    16,         "intelligence": 10,         "armor": 75,         "weapon": "Two-Handed
    Sword",         "specialty": "One-handed Swords"     } ] [PRE28]`  `# Summary    In
    this chapter, we explored the basics of using generative models through prompt
    engineering and output verification. We focused on the creativity and potential
    complexity that comes with prompt engineering. We discovered that the components
    of a prompt are key in generating the output that is right for our use case. As
    a result, experimentation is vital when prompt engineering.    In the next chapter,
    we explore advanced techniques for leveraging generative models. These techniques
    go beyond prompt engineering and are meant to enhance the capabilities of these
    models. From giving a model external memory to using external tools, we aim to
    give a generative model superpowers!`**'
  prefs: []
  type: TYPE_NORMAL
