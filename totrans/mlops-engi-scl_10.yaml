- en: 8 Scaling out with distributed training
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 使用分布式训练进行扩展
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容包括：
- en: Understanding distributed data parallel gradient descent
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解分布式数据并行梯度下降
- en: Using gradient accumulation in gradient descent for out-of-memory data sets
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在梯度下降中使用梯度累积以处理内存不足的数据集
- en: Evaluating parameter server versus ring-based approaches for distributed gradient
    descent
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对比参数服务器和基于环结构的分布式梯度下降方法
- en: Understanding reduce-scatter and all-gather phases of ring-based gradient descent
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解基于环结构的梯度下降的reduce-scatter和all-gather阶段
- en: Implementing a single node version of ring-based gradient descent using Python
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Python实现基于环结构的分布式梯度下降的单节点版本
- en: In chapter 7, you learned about scaling up your machine learning implementation
    to make the most of the compute resources available in a single compute node.
    For example, you saw examples for taking advantage of the more powerful processors
    in GPU devices. However, as you will discover by launching a machine learning
    system in production, the rate of growth in the number of training examples and
    the size of the training data sets can outpace the compute capacity of even the
    most capable servers and workstations. Although with contemporary public cloud
    infrastructure scaling up by upgrading to a more powerful processor or by adding
    more memory or more GPU devices can get you far, you should have a better plan
    for the long run.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在第7章中，您了解了如何将机器学习实现扩展到单个计算节点上，以充分利用可用的计算资源。例如，您可以看到如何利用GPU设备中更强大的处理器。然而，当您在生产中启动一个机器学习系统时，训练示例的数量增长速度和训练数据集的规模可能会超过即使是最强大的服务器和工作站的计算能力。尽管借助现代公共云基础设施的升级，例如通过升级到更强大的处理器，增加内存或GPU设备，可以获得很大的扩展性，但您应该有一个更好的长远计划。
- en: '*Distributed data parallel* (DDP) training is a category of machine learning
    model training that relies on scaling out rather than scaling up. With scaling
    out, as the training data set size grows, you scale by partitioning and performing
    the computational workload involved in model training across a cluster of networked
    compute servers, or nodes. Here, a *node* is a virtual or physical server on a
    network connecting the nodes into a cluster. Instead of upgrading to the more
    capable (and often more expensive) compute nodes to perform machine learning model
    training (the scale-up approach), with scale out you can network a cluster of
    less powerful, even commodity compute nodes, and open the possibility of finishing
    training sooner by distributing and doing work across the nodes in parallel. In
    effect, scaling out to larger training data sets means adding more nodes to a
    cluster.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*分布式数据并行*（DDP）训练是一种依靠扩展而不是升级的机器学习模型训练方法。随着训练数据集的增大，通过将模型训练涉及的计算工作负载划分并在网络计算服务器（节点）集群上进行，可以进行扩展。这里的“节点”是连接成集群的网络上的虚拟或物理服务器。与采用更高性能（也往往更昂贵）的计算节点来执行机器学习模型训练（即扩展方法）不同，通过扩展，您可以将一组较弱甚至是普通的计算节点组成一个网络，并通过在节点之间分布和并行执行工作的方式，可能更早地完成训练。事实上，将训练数据集扩展到更大规模意味着向集群中添加更多的节点。'
- en: DDP model training is more than just scaling out by adding nodes to a cluster.
    The “data parallel” aspect of DDP describes that, in the cluster, every node computes
    gradients using only independent and mutually exclusive partitions (also known
    as *shards*) of the training data set. Typically, the number of training examples
    in a shard is selected to ensure that the shard can fit in the memory of each
    of the nodes in the cluster. Although in a DDP approach every training node in
    the cluster uses a distinct shard of the data set for every iteration of gradient
    descent, within the scope of the iteration, all nodes must use an identical copy
    of the model in training to compute the model parameter gradients. Hence, after
    the nodes compute the gradients based on the training data set (or a batch of
    training examples), the nodes must all synchronize to an updated version of the
    model parameters using the computed gradients.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: DDP模型训练不仅仅是通过向集群中添加节点来进行扩展。“数据并行”方面的DDP描述了在集群中，每个节点仅使用训练数据集的独立且互斥的划分（也称为“分片”）来计算梯度。通常，每个分片中的训练示例数量都被选定为确保每个节点的内存可以容纳该分片。虽然在DDP方法中，集群中的每个训练节点在梯度下降的每次迭代中都使用数据集的不同分片，但在迭代的范围内，所有节点必须使用相同的模型副本进行训练以计算模型参数梯度。因此，在节点根据训练数据集（或一批训练示例）计算梯度后，节点必须同步到更新后的模型参数版本。
- en: In this chapter, you will learn about alternative approaches to distributed
    gradient descent and how DDP gradient descent implementations can help you efficiently
    scale out training across an arbitrary number of nodes while using practical nodes
    with limited compute, memory, storage, and bandwidth resources.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: 8.1 What if the training data set does not fit in memory?
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section and its subsections provide a step-by-step introduction to gradient
    accumulation and the role that gradient accumulation plays in gradient descent
    to enable support for out-of-memory training data sets.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.1 Illustrating gradient accumulation
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section demonstrates gradient accumulation using the autograd library of
    PyTorch. While the example in this section is based on using autograd with a trivial
    function, later sections apply gradient accumulation to more realistic examples.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: When using gradient descent with reverse mode accumulating autodiff it is necessary
    to clear out the values of the tensor’s gradients after performing an optimization
    step of gradient descent.[¹](#pgfId-1011924) In PyTorch, this is done by setting
    the tensor’s gradients to None or using a torch.optim.Optimizer helper method,
    zero_grad. Unless the gradients are zeroed (cleared) out, calls to the backward
    method of the tensor produced by the loss function can result in accumulation
    of the gradient values in the model’s tensors. This behavior is shown in the following
    listing.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.1 Illustration of gradient accumulation for repeated calls to backward
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Use requires_grad=True to enable differentiation of y with respect to x.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Set retain_graph=True to prevent PyTorch from de-allocating memory.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: This outputs
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Based on five repeated calls to backward accumulates the value for ![008-01_EQ01](Images/008-01_EQ01.png)
    of *y* = *x*² at 3, which is 6. As the result of accumulation, the output of x.grad
    skips counts by 6 for each of the 5 iterations of the for-loop. Although gradient
    accumulation may seem like an inconvenient side effect of autodiff, it can serve
    a useful purpose when scaling gradient descent to out-of-memory data sets and
    distributed clusters.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.2 Preparing a sample model and data set
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section describes how to prepare a sample model and a training data set
    to illustrate the role that gradient accumulation plays in scaling to out-of-memory
    data sets. In the next section, you will learn how the model and the data set
    can be used in gradient descent.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you are working with a training data set of 1,000 structured records
    and the compute node executing your gradient descent algorithm has just enough
    memory to hold 250 examples at a time. Of course, modern compute environments
    can scale to much larger data sets; however, this choice of numbers will prove
    useful as an illustration. Let’s look at a gradient accumulation with a made-up
    data set that does fit in memory before jumping directly into the complexity of
    a real-world out-of-memory data set.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.2 Preparing a sample multivariate linear regression data set
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.2 准备一个样本多元线性回归数据集
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Set the pseudo-random number seed for reproducibility.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 设置伪随机数种子以实现可重现性。
- en: ❷ Create a data set for a multivariate linear regression problem.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建用于多元线性回归问题的数据集。
- en: ❸ Use 1,000 records (rows) in the training examples data set.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在训练示例数据集中使用 1,000 条记录（行）。
- en: ❹ Use multivariate_normal to generate the synthetic training data set.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用 multivariate_normal 生成合成训练数据集。
- en: ❺ Use different means for the independent variables.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用不同的均值来作为独立变量。
- en: ❻ Specify that the independent variables should be uncorrelated.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 指定独立变量应该不相关。
- en: ❼ Multiply the features in X_train by coefficients.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 将 X_train 中的特征与系数相乘。
- en: 'The listing created a training data set with four features (independent variables)
    and a dependent variable (label) based on four coefficients, 1, 2, 3, 4, for each
    of the four features. For example, assuming that you used seed value 42 when generating
    the X_train values, the value of the y_train[0] is computed from X_train[0, :]:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 此列表创建了一个训练数据集，其中有四个特征（自变量）和一个因变量（标签），基于每个特征的四个系数 1、2、3、4。例如，假设在生成 X_train 值时使用了种子值
    42，则 y_train[0] 的值是从 X_train[0,:] 计算的：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: which should output
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 应输出
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You can also confirm the expected shapes of the training data set tensors X_train
    and y_train by printing
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过打印来确认训练数据集张量 X_train 和 y_train 的预期形状
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'which should output the following based on the values of 1,000 and 4 for the
    TRAINING_ DATASET_SIZE and FEATURES, respectively:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 应基于 TRAINING_DATASET_SIZE 和 FEATURES 的值输出如下：
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: With the training data set tensors in place, you can prepare a linear regression
    model and the supporting methods to train the model using gradient descent. The
    model w is initialized with random values sampled from a standard normal distribution.
    Also, since the w model parameter tensor is created with requires_grad=True, the
    tensor has the initial gradient values set to None.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 有了训练数据集张量的准备，您可以准备一个线性回归模型和支持方法，使用梯度下降来训练模型。模型 w 是用从标准正态分布中抽取的随机值初始化的。此外，由于模型参数张量
    w 被创建为 requires_grad=True，因此张量的初始梯度值设置为 None。
- en: Listing 8.3 Defining a model w and utility methods for gradient descent
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.3 定义模型 w 和梯度下降的实用方法
- en: '[PRE7]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Create the model for the multivariate linear regression problem.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建多元线性回归问题的模型。
- en: ❷ Implement the forward step of gradient descent based on the model w.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 基于模型 w 实现梯度下降的前向步骤。
- en: ❸ Compute the errors (residuals) of the target (y).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 计算目标（y）的误差（残差）。
- en: ❹ Return the value of the mean squared error.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 返回均方误差的值。
- en: Although you could have initialized w using a more sophisticated technique,[²](#pgfId-1013157)
    in this case the multivariate linear regression problem is simple enough that
    the added complexity is not warranted.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管您可以使用更复杂的技术来初始化 w，但在这种情况下，多元线性回归问题足够简单，不需要增加复杂性。
- en: 8.1.3 Understanding gradient descent using out-of-memory data shards
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.3 理解使用超出内存的数据片段的梯度下降
- en: In this section, the model and the data set prepared in section 8.1.2 are used
    with gradient descent to use the gradient accumulation feature of autodiff to
    scale to out-of-memory data sets.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，使用第 8.1.2 节准备的模型和数据集，使用梯度下降利用 autodiff 的梯度累积特性来扩展到超出内存的数据集。
- en: By relying on gradient accumulation, gradient descent can compute the gradients
    based on the entire training data set (i.e., an epoch of gradient descent) using
    the approach illustrated in figure 8.1\. Be careful not to confuse the shards
    shown in figure 8.1 with batches as used in mini-batch gradient descent; the difference
    is clarified in the following paragraphs.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 通过依赖梯度累积，梯度下降可以使用图 8.1 中所示的方法基于整个训练数据集（即梯度下降的一个时期）来计算梯度。注意不要将图 8.1 中显示的分片与 mini-batch
    梯度下降中使用的批次混淆；区别在下面的段落中进行了澄清。
- en: '![08-01](Images/08-01.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![08-01](Images/08-01.png)'
- en: Figure 8.1 Gradient accumulation re-uses shard memory to enable scaling to out-of-memory
    data sets.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 梯度累积重新使用分片内存以实现对超出内存的数据集的扩展。
- en: The left side of figure 8.1 shows the shards of the training dataset using [0:250][0]
    to represent the first shard of 250 examples (records) out of the 1,000 in the
    training data set, [0:250][1], for the second shard of records 250 up to 500,
    and so on. Here, the Python slicing notation (e.g., [0:250]) is used to specify
    which of the 1,000 examples in the training data set are included in a shard.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 的左侧显示了使用 [0:250][0] 表示训练数据集中的前 250 个示例（记录）的第一个分片，[0:250][1] 表示第二个分片，即记录从
    250 到 500，依此类推。在这里，使用 Python 切片表示法（例如，[0:250]）来指定训练数据集中的哪些 1,000 个示例包含在一个分片中。
- en: Notice that in figure 8.1 each shard is processed (in the forward and backward
    steps of gradient descent) using the same model w, or, more precisely, using identical
    values of the w model parameters. Although the model parameter values are identical
    in the four sequential steps of gradient accumulation in figure 8.1, since each
    shard contains a distinct collection of the training examples, the gradients computed
    for each shard are also distinct and are shard-specific. In the figure, this relationship
    between the shard and its corresponding gradient is denoted using the subscript
    so that the shard [0:250][0] produces the gradient *g*[0], and so on.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在图 8.1 中，每个分片都使用相同的模型w进行处理（在梯度下降的前向和后向步骤中），或者更准确地说，使用相同的w模型参数值。虽然图 8.1 中梯度积累的四个顺序步骤中模型参数值是相同的，但由于每个分片包含训练示例的不同集合，因此为每个分片计算的梯度也是不同的，并且是特定于分片的。在图中，使用下标表示分片及其相应的梯度之间的关系，以便分片
    [0:250][0] 产生梯度*g*[0]，依此类推。
- en: Once the gradients are computed per shard’s worth of training examples (listing
    8.4), the shard gradients are not used to update the model parameters. Instead,
    the gradients are left to accumulate in the model’s tensors. Thus, after the second
    shard of the training examples is processed by the forward method and then the
    backward method computes the corresponding gradient *g*[1], the model’s tensor
    w.grad contains the sum (accumulation) of the gradients *g*[0] + *g*[1].
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦每个分片的训练样本计算出梯度（见清单 8.4），则不会使用分片梯度来更新模型参数。相反，梯度被保留在模型张量中累积。因此，在第二个训练示例分片通过前向方法处理，然后后向方法计算相应的梯度*g*[1]之后，模型张量w.grad
    包含梯度*g*[0]+*g*[1]的总和（累积）。
- en: Note that computation with shards is different from computation with batches
    in mini-batch gradient descent, where the gradients computed from each batch are
    used to update the model parameters and are subsequently cleared out. It is useful
    to distinguish batches from shards because both can be used with gradient descent;
    for example, a shard can be a partition of a batch in cases where a batch of data
    does not fit into node memory. A shard can also consist of multiple batches so
    that gradient descent can be sped up by processing multiple batches stored in
    a memory of a node. Although shards can be used with mini-batch gradient descent,
    this section focuses on explaining a more basic example of using shards with ordinary
    gradient descent, where the gradients are computed based on an entire set of training
    examples.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，使用分片进行计算与小批量梯度下降中的批量计算不同，其中来自每个批次的梯度用于更新模型参数，然后清除。将批次与分片区分开很有用，因为两者都可以与梯度下降一起使用；例如，分片可以是批次的分区，在数据批次不适合节点内存的情况下。分片还可以由多个批次组成，以便通过处理存储在节点内存中的多个批次来加速梯度下降。虽然可以将分片与小批量梯度下降一起使用，但本节重点介绍使用分片与普通梯度下降的更基本示例，其中根据整个训练示例集计算梯度。
- en: Only after processing the entire training data set, one shard at a time, does
    the algorithm illustrated in figure 8.1 perform an optimization step of gradient
    descent based on the accumulated gradient *g*[0] + *g*[1] + *g*[2] + *g*[3].
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 仅在处理完整个训练数据集后，一次处理一个分片，图 8.1 中的算法才执行基于累积梯度*g*[0]+*g*[1]+*g*[2]+*g*[3]的梯度下降的优化步骤。
- en: Listing 8.4 Gradient descent using IN_MEMORY_SHARD_SIZE examples
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 8.4 使用 IN_MEMORY_SHARD_SIZE 示例的梯度下降
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Perform TRAINING_DATASET_SIZE // IN_MEMORY_SHARD_SIZE iterations per epoch.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 每个周期执行 TRAINING_DATASET_SIZE // IN_MEMORY_SHARD_SIZE 次迭代。
- en: ❷ Assign training examples to y_shard and X_shard.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将训练示例分配给y_shard 和 X_shard。
- en: ❸ Perform the forward step of gradient descent.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 执行梯度下降的前向步骤。
- en: ❹ Compute the shard-size adjusted training loss.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 计算调整后的分片大小训练损失。
- en: ❺ Perform back prop and accumulation of the gradients
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 执行反向传播和梯度累积
- en: ❻ Perform the gradient descent optimization step.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 执行梯度下降优化步骤。
- en: ❼ Clear out the gradients of the model’s tensors.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Once the code is executed, the print statement
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: should output
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: which confirms that the gradient descent correctly recovered the coefficients
    [1.0000, 2.0000, 3.0000, 4.0000] used in listing 8.2 to create the training data
    set made up of y_train and X_train.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: The fraction (IN_MEMORY_SHARD_SIZE / TRAINING_DATASET_SIZE) used in the calculation
    of the loss in listing 8.4 is subtle but important. Recall that the listing is
    intended to compute the gradient for the entire epoch of training examples, or,
    more precisely, TRAINING_DATASET_SIZE examples. The default implementation of
    the mse method, which computes the mean squared error of the model’s estimates
    y_est, assumes IN_MEMORY_SHARD_SIZE of examples during the computation. In other
    words, every iteration of the inner for-loop in the listing computes mse by calculating
    ![08-01_EQ02](Images/08-01_EQ02.png), or equivalently in PyTorch using
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: which returns the mean squared error per IN_MEMORY_DATASET_SIZE examples. The
    (IN_MEMORY_SHARD_SIZE / TRAINING_DATASET_SIZE) fraction used in ❹ rescales the
    mean squared error to the TRAINING_DATASET_SIZE examples.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: With this multiplication, expressed in terms of an equation, notice that rescaling
    amounts to IN_MEMORY_DATASET_SIZE, which cancels out in the numerator and denominator
    of
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '![08-01_EQ03](Images/08-01_EQ03.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
- en: When the inner for-loop finishes, the w.grad contains the sum of the training
    example gradients, so the code w.data -= LEARNING_RATE * w.grad computes the optimization
    step for the entire epoch of shards. In other words, in the gradient descent implementation
    in listing 8.4, the gradient optimization step is performed once per every epoch
    of a training example. This confirms that the implementation in listing 8.4 is
    not a mini-batch gradient descent.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 'Although the approach illustrated in figure 8.1 enables scaling to out-of-memory
    data sets while using arbitrary shard sizes, it suffers from a significant algorithm
    complexity problem: the inner for-loop is sequential, which changes the big-zero
    performance of the gradient descent implementation from *O*(EPOCHS) to *O*(EPOCHS
    * SHARDS).'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Distributing the inner for-loop from listing 8.4 across a cluster of parallel
    worker nodes can return the implementation to the original *O*(EPOCHS) worst-case
    performance. But how can this be implemented efficiently?
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 Parameter server approach to gradient accumulation
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section introduces parameter server-based implementation of distributed
    gradient descent and explains the role that gradient accumulation plays in the
    implementation. This section clarifies the limitations of the parameter server-based
    approach and motivates a more efficient, ring-based implementation.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Legacy machine learning frameworks like TensorFlow 1.x popularized the parameter
    server-based approach to distributing gradient descent across multiple nodes in
    a cluster. The parameter server approach illustrated in figure 8.2 is straightforward
    to understand and implement.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 像 TensorFlow 1.x 这样的传统机器学习框架普及了基于参数服务器的方法，以在集群的多个节点之间分布梯度下降。 图 8.2 中描绘的参数服务器方法易于理解和实现。
- en: '![08-02](Images/08-02.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![08-02](Images/08-02.png)'
- en: Figure 8.2 Gradient descent distributed across worker and parameter servers
    to support scaling out
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2 梯度下降在工作节点和参数服务器之间进行分布以支持扩展
- en: In the figure, each of the worker nodes (shown using dashed lines) performs
    the forward and backward steps of gradient descent (e.g., the steps from the inner
    for-loop in listing 8.4) based on a single shard of the training data set to compute
    shard-specific gradients of the loss function. Notice that in figure 8.2, the
    gradients have a subscript that corresponds to the subscript of the shard used
    to compute the gradient just as in figure 8.1.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在图中，每个工作节点（使用虚线表示）根据训练数据集的单个分片执行梯度下降的前向和后向步骤（例如清单 8.4 中的内部循环中的步骤），以计算损失函数的分片特定梯度。请注意，在图
    8.2 中，梯度具有与用于计算梯度的分片的下标对应的下标，就像图 8.1 中一样。
- en: Once the worker node computes its gradient, it sends the gradient to a parameter
    server (or a cluster of parameter servers) for processing. The parameter server(s)
    (right side of figure 8.2) waits to accumulate the gradients from the worker nodes
    and uses the accumulated gradients to perform an optimization step of gradient
    descent, which computes the model parameters for the next iteration of gradient
    descent. The next version of the model based on the newly computed model parameters
    (shown as w' in figure 8.2) is then sent to the worker nodes, replacing the previous
    model parameters (shown as w in figure 8.2) and ensuring that every node computes
    the next iteration of gradient descent using the identical, updated copy of the
    model.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦工作节点计算出其梯度，它就将梯度发送到参数服务器（或参数服务器集群）进行处理。参数服务器（图 8.2 的右侧）等待累积来自工作节点的梯度，并使用累积的梯度执行梯度下降的优化步骤，计算下一次梯度下降的模型参数。然后，基于新计算的模型参数（在图
    8.2 中表示为 w'），将下一个版本的模型发送到工作节点，取代以前的模型参数（在图 8.2 中表示为 w），确保每个节点使用相同和更新的模型的下一个梯度下降迭代。
- en: The parameter server implementation of distributed gradient descent from figure
    8.2 is a kind of distributed data parallel (defined in the introduction to this
    chapter) approach to gradient descent. In a distributed data parallel approach,
    the training data set is partitioned (sharded) into independent and mutually exclusive
    subsets such that there exists a one-to-one relationship between the training
    data set shard and a worker node. Next, every worker node computes gradients using
    a shard and an identical copy of the model parameters.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2 中的分布式梯度下降的参数服务器实现是一种分布式数据并行（在本章的介绍中定义）方法。在分布式数据并行方法中，训练数据集被划分为独立且互不重复的子集，以便训练数据集分片和工作节点之间存在一对一的关系。接下来，每个工作节点使用一个分片和一个相同的模型参数副本计算梯度。
- en: 'Unlike alternative distributed data parallel approaches (explained in the remainder
    of the chapter), parameter server implementation of distributed gradient descent
    suffers from a significant scalability issue: the network connectivity between
    the worker and parameter servers is a communications bottleneck. Specifically,
    the limited bandwidth available to communicate between the worker and parameter
    server nodes is saturated in both communication phases of the implementation:
    during the many-to-one (or many-to-few) communication of the gradients from the
    workers to the parameter servers, as well as during the one-to-many (or few-to-many)
    communication of the updated model parameters from the parameter server(s) to
    the worker nodes.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 与替代的分布式数据并行方法（在本章的其余部分中讲解）不同，分布式梯度下降的参数服务器实现存在重大的可伸缩性问题：工作节点和参数服务器之间的网络连通性是通信瓶颈。具体而言，在实现的两个通信阶段中都存在通信带宽受限的问题：在从工作节点到参数服务器的梯度的多到一（或多到少）通信期间，以及在从参数服务器（多个参数服务器）到工作节点的更新模型参数的一到多（或少到多）通信期间。
- en: 8.3 Introducing logical ring-based gradient descent
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 引入逻辑环形梯度下降
- en: This section introduces the fundamental concepts of nodes communicating in a
    logical ring network. Instead of provisioning actual nodes and having them communicate
    over a network, this section explains the networking concepts using a simple Python
    program running in a single-node environment. Once you have a firm grasp of the
    concepts, you will apply them to the more complex, distributed, multi-node environments.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了在逻辑环网络中通信的节点的基本概念。本节不是为了提供实际节点并使其通过网络通信，而是使用在单节点环境中运行的简单Python程序来解释网络概念。一旦你对概念有了牢固的掌握，你将把它们应用到更复杂的、分布式的、多节点环境中。
- en: 'As opposed to relying on a centralized cluster of parameter servers (the approach
    illustrated in section 8.2), logical ring-based distributed data parallel algorithms
    (e.g., Horovod; [https://github.com/horovod/horovod](https://github.com/horovod/horovod))
    avoid one-to-many and many-to-one communications bottlenecks and rely on nodes
    communicating just with the two logical neighbors in a ring: the predecessor and
    the successor nodes.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 与依赖于集中式参数服务器集群（第8.2节中所示方法）相反，基于逻辑环的分布式数据并行算法（例如Horovod；[https://github.com/horovod/horovod](https://github.com/horovod/horovod)）避免了一对多和多对一通信的瓶颈，并且仅依赖于在环中与两个逻辑邻居通信的节点：前趋节点和后继节点。
- en: 'The illustration in figure 8.3 (left side) shows four nodes, each shown using
    dashed lines and denoted as nodes *n*[0] through *n*[3] that are organized in
    a logical ring. Note that with contemporary virtual networks in public cloud environments
    the nodes do not have to be physically connected to each other in a ring: standard
    ethernet networking is sufficient. However, in the logical ring network shown
    in the figure, every node is constrained such that it communicates only with its
    predecessor and successor nodes. As you will learn in section 8.4, this helps
    limit the networking bandwidth needed per iteration of the distributed gradient
    descent.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3（左侧）的图示显示了四个节点，每个节点使用虚线表示，并表示为节点*n*[0]到*n*[3]，这些节点组织在一个逻辑环中。请注意，在公共云环境中的当代虚拟网络中，节点不必物理连接到彼此形成环：标准以太网网络足够。但是，在图中显示的逻辑环网络中，每个节点都受到限制，仅与其前趋节点和后继节点通信。正如您将在第8.4节中了解到的那样，这有助于限制分布式梯度下降每次迭代所需的网络带宽。
- en: '![08-03](Images/08-03.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![08-03](Images/08-03.png)'
- en: Figure 8.3 A logical networking ring (left) explained using sample values (right)
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 逻辑网络环（左）使用示例值解释
- en: For a node with an identifier *n*[i], the identifier of the successor node is
    defined as *n*[(i+1) %] *NODES*, where NODES is the total number of nodes in the
    logical ring. The modulo operation ensures that the communication pattern forms
    a ring by having the node with the highest identifier (which is always *n[NODES-1]*)
    communicate with the node with the identifier 0, and vice versa. In ring networking,
    as described in this chapter, every node *sends* data only to the successor node.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有标识符*n*[i]的节点，后继节点的标识符被定义为*n*[(i+1) %] *NODES*，其中NODES是逻辑环中节点的总数。模运算确保通信模式形成一个环，其中具有最高标识符（始终为*n*[NODES-1]）的节点与标识符为0的节点进行通信，反之亦然。在环形网络中，如本章所述，每个节点只向后继节点*发送*数据。
- en: Using similar logic, the identifier of the predecessor node is defined as *n*[(i-1)
    %] *NODES* for the ring-based networking so that node 0 can communicate with both
    node 1 and the node with the highest identifier value, (NODES - 1). Every node
    in the ring network used in this chapter *receives* data only from the predecessor
    node.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 使用类似的逻辑，基于环形网络的前趋节点的标识符被定义为*n*[(i-1) %] *NODES*，以便节点0可以与节点1和具有最高标识符值（NODES -
    1）的节点进行通信。本章使用的环网络中的每个节点只从前趋节点*接收*数据。
- en: As with the parameter server-based approach explained in section 8.2, the nodes
    in figure 8.3 process independent shards of the training data set such that *g*[0](*n*[0])
    represents the gradient values computed from a shard having an index of 0 by the
    node *n*[0]. Continuing with the example from section 8.2, if [0:250]0 is the
    first of four shards, then *g*[0](*n*[0]) denotes the gradient values from the
    first shard computed by the node *n*[0], using model parameter values w. Hence,
    just like with the parameter server-based approach, the ring-based approach is
    data-parallel distributed.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 就像第8.2节中解释的基于参数服务器的方法一样，图8.3中的节点处理训练数据集的独立碎片，以便*g*[0](*n*[0])表示由节点*n*[0]计算的具有索引0的碎片的梯度值。继续使用第8.2节的示例，如果[0:250]0是四个碎片中的第一个碎片，那么*g*[0](*n*[0])表示由节点*n*[0]计算的第一个碎片的梯度值，使用模型参数值w。因此，就像基于参数服务器的方法一样，基于环的方法也是数据并行分布式的。
- en: In the ring-based distributed data parallel implementation, the dedicated parameter
    servers do not exist. Instead, after every node in the cluster completes the forward
    and backward steps of an iteration of gradient descent, the nodes communicate
    in the logical ring network so that the gradients from all the shards are accumulated
    on every node in the ring.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于环的分布式数据并行实现中，不存在专用的参数服务器。相反，在集群中的每个节点完成梯度下降迭代的前向和后向步骤后，节点在逻辑环网络中通信，以便所有碎片的梯度在环中的每个节点上累积。
- en: What kind of information needs to be communicated between the nodes to ensure
    that model parameter values and the accumulated gradient values are perfectly
    synchronized and identical? In the logical ring, since every node can send data
    only to the successor node, a node can receive accumulated gradients only from
    a series of iterations of gradient send/receive operations from the predecessor
    nodes. For instance, in order for the node *n*[0] to accumulate gradients from
    nodes *n*[1] through *n*[3] (right-most side of figure 8.4), three iterations
    of the send/receive operations are required. These three iterations are shown
    in sequence from the left side to the right side of figure 8.4\. As you will observe
    throughout this chapter, it takes (NODES - 1) iterations of send/receive communication
    in a multi-node cluster consisting of NODES number of nodes.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 需要在节点之间通信什么样的信息，以确保模型参数值和累积梯度值完全同步和相同？在逻辑环中，由于每个节点只能向后继节点发送数据，因此节点只能从前驱节点的一系列梯度发送/接收操作中接收累积梯度。例如，为了让节点*n*[0]从节点*n*[1]到*n*[3]（图8.4的最右侧）累积梯度，需要三次迭代的发送/接收操作。这三次迭代从图8.4的左侧到右侧按顺序显示。正如您将在本章中观察到的那样，在由NODES个节点组成的多节点集群中，需要（NODES
    - 1）次发送/接收通信迭代。
- en: '![08-04](Images/08-04.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![08-04](Images/08-04.png)'
- en: Figure 8.4 Reduce (sum) gradients to node 0 in a ring of four nodes, a reduce-all
    algorithm to distribute gradients across nodes
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 在一个由四个节点组成的环中将梯度（求和）减少到节点0，这是一个分布梯度的全局规约算法，用于在节点之间分发梯度。
- en: The source code from listing 8.5 provides the Python pseudocode implementation
    of the logic described by figure 8.4\. In the implementation, the variable NODES
    is defined using the relationship between the number of the training examples
    in the training data set (the value of the constant TRAINING_DATASET_SIZE) floor
    divided by the number of training examples that fit in memory of a node in the
    multi-node cluster (the value of IN_MEMORY_SHARD_SIZE). The floor division operation
    // is used to ensure that the value of the NODES constant is set to be an integer
    value since it is later used as an argument to the Python range operation.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.5中的源代码提供了图8.4描述的逻辑的Python伪代码实现。在实现中，使用了NODES变量，该变量是使用训练数据集中训练示例的数量（常量TRAINING_DATASET_SIZE的值）与多节点集群中一个节点的内存中适合的训练示例的数量（IN_MEMORY_SHARD_SIZE的值）之间的关系定义的。使用地板除法运算符//以确保NODES常量的值被设置为整数值，因为它稍后将用作Python范围操作的参数。
- en: Listing 8.5 Python pseudocode to illustrate reduction of the gradient to node
    0
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.5 Python伪代码，以说明梯度减少到节点0
- en: '[PRE12]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Calculate the number of the NODES needed for the training data set.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 计算训练数据集所需的节点数量。
- en: ❷ Assign arbitrary GRADIENT values, one per node for an illustration.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 为演示分配任意的GRADIENT值，每个节点一个。
- en: ❸ Create a dictionary to track the gradient computed by a node.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 创建一个字典来跟踪节点计算的梯度。
- en: ❹ Perform NODES - 1 iterations of communication.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 执行NODES - 1次通信迭代。
- en: ❺ Start with node iter+1 so that after NODES-1 . . .
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 从节点iter+1开始，以便在NODES-1后...
- en: ❻ . . . iterations, node 0 accumulates the gradients.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ❻……迭代，节点0累积梯度。
- en: ❼ The identifier of the next node closes the ring.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ❼下一个节点的标识符结束了环。
- en: ❽ Accumulate the gradient in node_to_gradient.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❽在节点对梯度进行累积。
- en: Once the code is done executing, printing the value of the node_to_gradients
    dictionary
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦代码执行完毕，打印node_to_gradients字典的值。
- en: '[PRE13]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'outputs the result:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果：
- en: '[PRE14]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: where the entry for the key 0 corresponds to the expected gradient computed
    for *n*[0], with the value of 11 based on the accumulated gradient 5 + 3 + 2 +
    1. Also, notice that since figure 8.4 does not include accumulation of gradients
    to any nodes other than *n*[0], the gradients for nodes *n*[1] through *n*[3]
    remain unchanged. Upcoming sections explain how to ensure that identical gradients
    are accumulated on all nodes in the ring.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 其中键0对应于预期梯度，计算的 *n*[0]，值为11，基于累积梯度5+3+2+1。此外，请注意，由于图8.4不包括对*n*[0]以外的任何节点的梯度累积，因此
    *n*[1]到*n*[3]的梯度保持不变。即将介绍的部分将解释如何确保在环中的所有节点上累积相同的梯度。
- en: 'During the first (shown with a zero-based index in figure 8.4 as Iteration
    0) of the three (NODES - 1) iterations, node *n*[1] sends and node *n*[2] receives
    the gradient values *g*[1](*n*[1]) computed by node *n*[1] prior to the start
    of the iteration 0. Since the purpose of the communication in the ring is to arrive
    to the accumulated gradient, upon receiving the *g*[1](*n*[1]) gradient values,
    the node *n*[2] can accumulate (add to) the gradient values directly to the memory
    occupied by the gradient *g*[2](*n*[2]), which ensures the re-use of memory to
    store the accumulated gradient values: *g*[1](*n*[1]) + *g*[2](*n*[2]). For example,
    if each of the gradient tensors on each of the nodes is 400 MB, then 400 MB’ worth
    of data is communicated between the nodes in the ring, and 400 MB’ worth of memory
    is consumed by each of the nodes to store the accumulated gradient values. By
    the conclusion of the iteration 0, node *n*[2] accumulates the added (i.e., reduced
    using a sum operation) gradients.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在三（节点-1）次迭代的第一次（在图8.4中以基于零的索引显示为迭代0）中，节点*n*[1]发送并且节点*n*[2]接收节点*n*[1]在开始迭代0之前计算的梯度值
    *g*[1](*n*[1])。由于在环中的通信目的是为了到达累积梯度，一旦接收 *g*[1](*n*[1])梯度值，*n*[2]节点可以直接累积（加到）梯度值，以确保重用内存来存储累积梯度值：*g*[1](*n*[1])+*g*[2](*n*[2])。例如，如果每个节点上的每个梯度张量都是400
    MB，那么在环中的节点之间传输400 MB的数据，并且每个节点消耗400 MB的内存来存储累积梯度值。到迭代0结束时，节点*n*[2]累积了添加（即使用求和操作减少的）梯度。
- en: Hence, during the second iteration (labeled as Iteration 1 in figure 8.4), the
    accumulated gradient values are sent from node *n*[2] to node *n*[3], resulting
    in the gradient values *g*[1](*n*[1]) + *g*[2](*n*[2]) + *g*[3](*n*[3]) accumulating
    on node *n*[3] at the end of the second iteration.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在第二次迭代（在图8.4中标记为迭代1）期间，累积的梯度值从节点*n*[2]发送到节点*n*[3]，导致在第二次迭代结束时在节点 *n*[3]上累积的梯度值*g*[1](*n*[1])+*g*[2](*n*[2])+*g*[3](*n*[3])。
- en: The last and final iteration in this example (labeled Iteration 2 in figure
    8.4) completes the accumulation of the gradients on the node *n*[0], adding the
    gradient computed on the node *g*[0](*n*[0]) to the accumulated gradient received
    from *n*[3] during this iteration. The resulting gradient, consisting of *g*[0](*n*[0])
    + *g*[1](*n*[1]) + *g*[2](*n*[2]) + *g*[3](*n*[3]), is sufficient for *n*[0] to
    compute the model parameter values for the next optimization step of gradient
    descent to be performed by every node in the cluster.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中的最后一次迭代（在图8.4中标记为迭代2）完成了对节点*n*[0]上的梯度的累积，将在这次迭代中计算的节点*n*[0]上的梯度* g*[0](*n*[0])加到从*n*
    [3]收到的累积梯度上。由此得到的梯度，包括*g*[0](*n*[0])+*g*[1](*n*[1])+*g*[2](*n*[2])+*g*[3](*n*[3])，足以让*n*[0]计算出下一个优化步骤的模型参数值，这个步骤是由集群中的每个节点执行的梯度下降过程。
- en: 'While the three iterations illustrated in figure 8.4 and listing 8.5 achieved
    accumulation (reduce step) of the gradients to a single node, for the distributed
    data parallel gradient descent to work, every node in the ring must have access
    to the entire accumulated gradient: *g*[0](*n*[0]) + *g*[1](*n*[1]) + *g*[2](*n*[2])
    + *g*[3](*n*[3]). Unless the accumulated gradient is available for every node,
    the nodes are unable to perform the gradient descent step of changing the values
    of the model parameters using the accumulated gradient. The upcoming sections
    build on the reduce steps from listing 8.5 to explain the reduce-all phase of
    the entire distributed gradient descent algorithm.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 8.4 Understanding ring-based distributed gradient descent
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the naive ring-based reduce operation described in section 8.3 can eliminate
    the need for parameter servers and ensure that the values of the gradients are
    reduced (accumulated) on the individual compute nodes in the ring-based multi-node
    cluster, it suffers from several disadvantages. As the training data set size
    grows (which is to be expected), the number of the nodes in the cluster must grow
    to keep up. This also means that the total bandwidth the cluster needs must grow
    along with the number of the nodes since each node must send the entire gradient
    to the next node in the ring during each iteration. In this section, you will
    learn about how ring-based distributed data parallel algorithms (e.g., the well-known
    Horovod algorithm) help with an efficient use of bandwidth in scale-out situations
    where both the number of training nodes and the training examples grow.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'The Horovod algorithm can support the growth in the training data set (and
    the number of the nodes in the cluster) while keeping the bandwidth demands constant
    or even lowering the bandwidth requirements. To support this, Horovod relies on
    two separate and district phases of ring-based communication: (1) reduce-scatter
    and (2) all-gather. In both phases, instead of sending/receiving the entire gradient’s
    worth of data between the nodes, Horovod communicates just a single segment of
    the gradient such that by default the size of the segment is the size of the gradient
    times ![08-04_EQ04](Images/08-04_EQ04.png), where *NODES* is the number of the
    worker nodes in the ring cluster. Hence, increasing the number of the worker nodes
    to scale with the training data set size reduces the bandwidth requirements in
    node-to-node communication.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: So what is a *segment* of the gradient? You can consider each segment a logical
    partition of the gradient, as illustrated in figure 8.5\. In the figure, the gradient
    *g*[0] computed by node *n*[0], based on the training data set shard [0:250][0]
    (where [0:250] is the Python slicing notation), is in turn partitioned into NODES
    segments, such that by default, a roughly equivalent number of the gradient values
    exists per segment. Continuing with an earlier example where the gradient occupied
    400 MB’ worth of data (for example 4 bytes per 100,000,000 of 32-bit floating
    point gradient values of the model parameters), each segment is 100 MB of the
    mutually exclusive logical partitions of the segment. Note that in this case,
    since the shard is computed by the node *n*[0], each *i* of the four segments
    is annotated using *s[i]*(*n*[0]).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '![08-05](Images/08-05.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 Gradient segments used by Horovod for node-to-node communication
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Also notice that while the segments cannot be accumulated (added) along the
    horizontal axis of the frame in figure 8.5, it is possible to accumulate the segments
    along the vertical axis. Further, the segments *s[i]* shown below the segment
    frame in figure 8.5 correspond to the accumulation of the corresponding segments
    computed by each node. For example, *s[0]* is equal to *s[0]*(*n*[0]) + *s[1]*(*n*[1])
    + *s[2]*(*n*[2]) + *s[3]*(*n*[3]). Hence, the segments *s*[0]*s*[1]*s*[2]*s*[3]
    shown below the frame in figure 8.5 are equivalent to a logical partition into
    segments of the accumulated gradient *g*[0] + *g*[1] + *g*[2] + *g*[3] needed
    to perform the optimization step of gradient descent.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: As with an introduction to the ring-based reduce steps in listing 8.5, the rest
    of this chapter uses Python pseudocode to explain the Horovod algorithm. Recall
    that for a distributed data parallel algorithm (such as Horovod) to work correctly,
    every node in the ring cluster must be initialized with an identical copy of the
    model parameters. In listing 8.6, the Python list of tensors W is used to represent
    the identical models. Notice that every tensor in W is initialized using the values
    from w_src, a tensor of pseudorandom values sampled from a standard normal distribution.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.6 W storing the identical copies of the model tensor
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In order to re-use the training data set tensors X_train and y_train from listing
    8.4, the following explanation of the Horovod algorithm creates a PyTorch DataLoader,
    which partitions the training data set into shards of IN_MEMORY_SHARD_SIZE records
    each. Do not be confused by the batch_size argument to the DataLoader in listing
    8.7; although this argument is used to shard the source TensorDataset, the individual
    shards are not used as batches to update the parameters of the model.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.7 A step of gradient descent using PyTorch DataLoader for sharding
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Once the code is done executing, the expression
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: should output
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: representing the tensors of the model gradients, one per node in the ring cluster.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Note that after the forward and backward steps of gradient descent are performed
    on each node using the code in the for-loop in listing 8.7, the Horovod algorithm
    must perform two phases of ring-based networks in order to communicate the accumulated
    gradient to every node in the ring. The first phase, known as *reduce-scatter*,
    is explained in section 8.5, and the second phase, known as *all-gather*, is explained
    in section 8.6\.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '8.5 Phase 1: Reduce-scatter'
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section explains the reduce-scatter phase of Horovod, under the assumption
    that every node in the ring-based cluster is initialized with an identical copy
    of the model parameters. The section continues with the example from listing 8.7,
    where the identical copies of the model parameters are stored in W[node] and every
    node completed the forward and backward steps of gradient descent, with the resulting
    gradient values saved in W[node].grad. By the end of this section, you will learn
    how the reduce-scatter phase of Horovod ensures that every node in the ring ends
    up with a distinct segment of the accumulated gradient *g*[0] + *g*[1] + *g*[2]
    + *g*[3].
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: The first phase of Horovod, known as reduce-scatter, starts after each of the
    nodes is done computing a gradient based on the node-specific shard of the data
    set. As explained in the previous section, each node logically partitions the
    computed gradient into NODES segments. The first iteration (of a total of three
    iterations) of this phase is shown in figure 8.6, where the top side of the figure
    illustrates that, at the start of the phase, each node *n*[i] stores the shard-specific
    segments, *s*[0](*n*[i]) through *s[NODES-1]*(*n*[i]).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '![08-06](Images/08-06.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 The first iteration of the reduce-scatter phase initiates gradient
    segment transfer across nodes.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Since reduce-scatter sends just a segment’s worth of data to the successor node
    at every iteration, during the first iteration (shown using arrows on the bottom
    side of figure 8.6), a node *n[i]* forwards a segment *s*[(i - 1)] % NODES(*n*[i])
    to the successor node. By the conclusion of the first iteration (bottom side of
    figure 8.6), each node *n[i]* accumulates a segment *s*[(i - t - 1) % NODES](*n*[(i
    - 1) % NODES]) + *s*[(i - t - 1) % NODES](*n*[i]), where t=1 represents the first
    iteration.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'In subsequent iterations, each node sends the segment that was accumulated
    in the previous iteration to the successor node. For example, in the second iteration
    (shown in figure 8.7), node *n*[1] sends the segment *s*[3](*n*[0] + *n*[1]),
    node *n*[2] sends the segment *s*[0](*n*[1] + *n*[2]), and in general, for iteration
    t, node *n*[i] sends the accumulated segment *s*[(i - t)] % NODES(*n*[i]). Since
    in the example with four nodes only three iterations are needed to reduce-scatter
    the segments, the bottom side of figure 8.7 shows that, by the conclusion of the
    second iteration, only one part of each segment is missing on each of the nodes:
    the segment specified by *s*[(i + 1) % NODES](*n*[i]).'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '![08-07](Images/08-07.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 The second reduce-scatter iteration propagates accumulated gradients.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: This missing part is filled in the third and final iteration of the example,
    whereby at the conclusion of the iteration (bottom side of figure 8.8), every
    node *n[i]* accumulates the entire segment *s[i]*. For example, notice that in
    figure 8.8, *n*[0] concludes the final iteration of this phase with *s*[0], node
    *n*[1] with *s*[1], and so forth.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '![08-08](Images/08-08.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 The third reduce-scatter iteration finishes gradient accumulation
    for a four-node ring.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.8 Python pseudocode for the reduce-scatter phase
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ The first segment is accumulated on the first node.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Retrieve the gradient values corresponding to node and segment seg.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Accumulate gradient segment value on the next_node in the ring.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: After the code from listing 8.8 finishes executing, you can output the resulting
    gradients using
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: which should print out
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Notice that, as expected, the gradient values are scattered across the nodes
    such that *n*[0] stores the segment *s*[0] of the accumulated gradient, *n*[1]
    stores the segment *s*[1], and so on. In general, the accumulated segments of
    the gradient after reduce-scatter can be printed out using
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'which outputs the values of accumulated segment on each node:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The illustration in figure 8.9 summarizes the code from listing 8.8 for the
    case when the reduce-scatter ring consists of four nodes.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '![08-09a](Images/08-09a.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
- en: Figure 8.9a Iterations of reduce-scatter
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '![08-09b](Images/08-09b.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
- en: Figure 8.9b Iterations of reduce-scatter
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '8.6 Phase 2: All-gather'
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section explains the second and the final phase of the Horovod algorithm:
    all-gather. In this section, you can observe how the scattered segments of the
    accumulated gradient from the reduce-scatter phase are gathered, or sent around
    the ring, so that by the conclusion of the phase, every node stores the entire
    accumulated gradient *g*[0] + *g*[1] + *g*[2] + *g*[3]. This means that at the
    conclusion of this phase, every node in the logical ring can perform the optimization
    step of gradient descent and compute the next iteration of the model parameters
    for further training.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Given that the reduce-scatter phase performs the nuanced steps of selectively
    accumulating (reducing) the gradient segment values, the implementation of all-gather,
    the second and the last phase, is easier to follow. Using an approach introduced
    with the reduce-all algorithm, this phase involves simply sending the accumulated
    segments from one node to the next. As with the reduce-scatter phase of the Horovod
    algorithm, the all-gather phase takes NODES - 1 iterations of node-to-node communication
    in the logical ring network of the cluster.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: The three iterations for the four nodes in figure 8.10 are shown as the upper-left,
    upper-right, and lower-right quadrants of the figure. The lower-left corner of
    the figure shows the final state of the nodes in the cluster after the nodes have
    completed all the steps of the Horovod algorithm. Note that the gradient segments
    on each node (shown as *s*[0] through *s*[3]) store the entire accumulated gradient
    (shown as *g*[0] + *g*[1] + *g*[2] + *g*[3]) computed from the corresponding shards
    of the training data set.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '![08-10a](Images/08-10a.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10a Iterations of all-gather
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '![08-10b](Images/08-10b.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10b Iterations of all-gather
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: The upper-left quadrant of the figure indicates that at the beginning of the
    first iteration the state of the four nodes in the example is such that *n*[0]
    stores segment *s*[0] of the accumulated gradient, and so forth. During the first
    iteration of the phase (upper-left quadrant), each node sends only the accumulated
    segment that it stores to the successor node in the ring, overwriting and replacing
    any previous segment values stored in the successor node.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.9 Python pseudocode for the all-gather phase
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ❶ Start with the first node on the first iteration.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Store the gradient values of the segment on the next node in the ring.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: At the beginning of the second iteration (upper-right quadrant in figure 8.10),
    every node stores exactly two complete segments of the accumulated gradient. During
    this and the remaining iterations, every node sends the segment received during
    the previous iteration (e.g., *s*[3] in case of *n*[0] during the second iteration)
    to the successor node in the ring. The last iteration (lower-right quadrant) completes
    the transfer of the remaining segments of the gradient to the nodes in the cluster.
    At the conclusion of this phase (lower-left quadrant) the accumulated gradient
    *g*[0] + *g*[1] + *g*[2] + *g*[3] is available on every node in the ring cluster.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: At this point, printing the gradients of the model on each node,
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'outputs four identical gradient values for every node in the ring:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Listing 8.10 Horovod ring-based distributed gradient descent algorithm
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This should output the recovered multivariable linear regression coefficients:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Summary
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Distributed data parallel training is an approach to distributed gradient descent
    where each node in a scale-out cluster uses an identical copy of the trained model
    but a dedicated shard of the training data set.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The gradient accumulation feature of the reverse-mode accumulating autodiff
    enables gradient descent to scale down to limited memory nodes or scale up to
    out-of-memory data sets.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Legacy parameter server-based approaches to distributed data parallel gradient
    descent require expensive, broadcast-style networking operations and do not scale
    well under bandwidth constraints.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Horovod is a scalable and bandwidth-efficient algorithm for distributed data
    parallel gradient descent based on two phases of ring-based networking operations:
    reduce-scatter and all-gather.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ^(1.)This feature of autodiff is covered in detail in chapter 5.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '^(2.)For example, many deep learning models are initialized using Kaiming initialization:
    [http://mng.bz/5K47](http://mng.bz/5K47).'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
