- en: 5 Generative AI and large language models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Generative artificial intelligence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reflections on human communication and speech
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The benefits, limitations, and risks of generative AI and large language models
    such as ChatGPT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Differences between human and generative AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Artificial intelligence has witnessed numerous ups and downs, but the release
    of ChatGPT represents a pivotal moment in the field of AI for several compelling
    reasons. First, it signifies a significant leap forward in natural language understanding
    and generation, demonstrating the remarkable progress AI has made in processing
    and generating human-like text. ChatGPT’s ability to engage in coherent and contextually
    relevant conversations with users across a wide range of topics showcases the
    potential for AI to be integrated into various applications, from customer support
    to content creation. Furthermore, ChatGPT embodies the power of large-scale pretrained
    models. Its capabilities highlight the potential for AI to augment human endeavors,
    improving efficiency and offering valuable insights across industries.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, ChatGPT is the first product that led to democratizing the use
    of AI by providing a user-friendly interface that enables people without extensive
    technical expertise to harness the benefits of AI and integrate it into their
    work and daily lives. This democratization of AI usage fosters innovation, creativity,
    and collaboration across diverse fields and industries.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will introduce generative AI, a remarkable technology that
    offers a multitude of benefits across various domains and holds great potential
    for revolutionizing many industries. We will also examine its limitations and
    the potential risks associated with its use.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Generative artificial intelligence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generative AI represents a cutting-edge branch of AI that uses deep learning
    algorithms on extensive datasets. Its primary function is to create entirely new
    content across various mediums, ranging from textual narratives to realistic images,
    audio, and even lifelike video. What distinguishes generative AI is its remarkable
    capacity to produce outputs of astonishing realism, often blurring the lines between
    machine-generated and human creativity. This remarkable success is achieved by
    the AI system through a process of learning and discerning patterns within existing
    data and extrapolating them to produce novel and distinctive creations. This innovation
    has its roots in the long-standing use of generative models within statistical
    frameworks, primarily employed for the exploration of numerical data.
  prefs: []
  type: TYPE_NORMAL
- en: However, the true transformative leap occurred with the advent of deep learning,
    enabling its applications to extend well beyond numbers and into images, speech,
    and other unstructured data types. One of the most impressive facets of generative
    AI is its adaptability. It can be trained to mimic the style of a specific artist,
    write in the voice of a particular author, or generate music reminiscent of a
    favorite composer. These models have found applications in a wide range of domains,
    from art and entertainment to natural language processing (NLP) and numerous other
    fields.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Large language models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large language models (LLMs) comprise a subset of generative AI designed to
    understand and generate human-like text. These models are characterized by their
    immense scale, with billions of parameters, which act as their knowledge base
    for language understanding and generation. LLMs undergo a two-step training process,
    starting with pretraining, where they learn the fundamentals of language, including
    grammar, syntax, and semantics by analyzing vast amounts of text data from the
    internet. Afterward, they are fine-tuned on specific tasks to tailor their language
    skills for specific applications.
  prefs: []
  type: TYPE_NORMAL
- en: One prominent capability of LLMs is text generation. These models can produce
    human-like text in various forms, from articles and essays to creative works like
    poetry. For instance, LLMs can generate poetry that rhymes and evokes emotions,
    showcasing their creative potential. They excel in producing text that reads convincingly
    as if written by a human, making them valuable for content creation and storytelling.
    Additionally, LLMs are adept at language translation and text summarization. They
    can accurately translate text from one language to another, breaking down language
    barriers and facilitating global communication. Moreover, LLMs can summarize lengthy
    documents, a skill particularly useful in tasks such as news article summarization,
    enabling readers to quickly grasp the key points of a story. They can also provide
    coding assistance by generating code snippets in various programming languages,
    simplifying coding tasks. For example, an LLM can generate Python code to perform
    specific operations, enhancing efficiency and productivity. Furthermore, these
    models can answer questions using the knowledge they gained during training, responding
    accurately to factual queries or complex questions.
  prefs: []
  type: TYPE_NORMAL
- en: The pivotal moment of large language models happened when OpenAI introduced
    GPT-3 to select partners and developers in June 2020, followed by broader availability
    to the public in November 2022\. This release generated significant excitement
    on the internet because of GPT-3’s remarkable ability to mimic human-like conversation.
    GPT-3 was a computational powerhouse, boasting an impressive 175 billion parameters.
    It had undergone extensive training on a diverse dataset sourced from various
    online resources. The model’s proficiency in engaging in coherent, context-aware
    conversations made it appear exceptionally intelligent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Like many groundbreaking technologies, ChatGPT-3’s debut was accompanied by
    hype and exaggeration. The media, often drawn to sensationalism, didn’t miss the
    opportunity to capitalize on the excitement. In April 2023, a pair of Fox News
    journalists ran a headline that raised alarm with its dramatic assertion: “VERY
    SCARY: AI Bot Lays Out Plans to Destroy Humanity” [1]. This headline, coupled
    with sensationalist reporting, contributed to public anxiety about AI. The conversation
    between the two journalists and the guest expert voiced a fearful perspective,
    linking AI and ChatGPT to negative human traits and citing an incident involving
    a chatbot generating content related to nuclear devices. The news segment heightened
    the drama by featuring footage of Boston Dynamics’ dancing humanoid robots, misleadingly
    implying that the AI threats were associated with human-like, superintelligent
    robots with malevolent intentions. This sensationalized style of reporting stands
    as a notable illustration of how misinformation and fearmongering can skew the
    public’s understanding of AI. Even individuals with significant expertise occasionally
    make statements that are challenging to substantiate, thereby inflating the capabilities
    of AI algorithms. As an illustration, Sam Altman, the CEO of OpenAI, the entity
    behind ChatGPT, issued the following statement on March 16, 2021:'
  prefs: []
  type: TYPE_NORMAL
- en: In the next five years, computer programs that can think will read legal documents
    and give medical advice. In the next decade, they will do assembly-line work and
    maybe even become companions. And in the decades after that, they will do almost
    everything, including making new scientific discoveries that will expand our concept
    of everything. This technological revolution is unstoppable. [2]
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In the preceding chapters, I alluded to comparable exaggerated statements from
    the past that ultimately proved untrue.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 ChatGPT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Throughout the history of AI, there has been a recurring theme: the aspiration
    to empower machines with the ability to engage in meaningful conversations using
    natural language. The complexity of natural language is a formidable obstacle
    that AI researchers have faced since the inception of the discipline. In 2020,
    OpenAI unveiled GPT-3 [3]. Alongside GPT-3, other LLMs like BERT [4], T5 [5],
    and OPT [6] are often seen as significant advancements in the field of NLP. OpenAI
    researchers [7] stated:'
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3 excels in various NLP tasks, including translation, question-answering,
    cloze tasks, as well as tasks involving on-the-fly reasoning or domain adaptation,
    such as unscrambling words, using a new word in a sentence, or performing 3-digit
    arithmetic.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'OpenAI offers four primary GPT-3 models: Davinci, Curie, Babbage, and ADA [8].
    These models are characterized by different power levels and suitability for various
    tasks. For instance, Davinci, while more resource-intensive and slower than the
    others, is considered the most capable and is recommended for applications requiring
    deep understanding, such as generating creative content or summarizing existing
    content for specific audiences.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recent years have witnessed substantial improvements in NLP applications, thanks
    to the utilization of large text corpora during model training and the fine-tuning
    of models for specific tasks. As of the time of writing, OpenAI’s ChatGPT stands
    as the most advanced AI language generator and chatbot. The free version was made
    accessible to the public in November 2022, with over 1 million users having utilized
    it. On March 13, 2023, OpenAI introduced GPT-4, the latest milestone in its journey
    to scale up deep learning. GPT-4 is a large multimodal model capable of accepting
    both image and text inputs and generating text outputs. While it falls short of
    human-level performance in many real-world scenarios, it demonstrates human-level
    performance on various professional and academic benchmarks [9]. Concerning benchmarks,
    the GPT-4 technical report [10] states:'
  prefs: []
  type: TYPE_NORMAL
- en: We tested GPT-4 on a diverse set of benchmarks, including simulating exams originally
    designed for humans. We did not specifically train the model for these exams.
    A minority of the problems in the exams were encountered by the model during training.
    For each exam, we ran a variant with these questions removed and reported the
    lower score of the two.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Since late 2022, the media’s excitement about ChatGPT has spurred the release
    of several significant language models.
  prefs: []
  type: TYPE_NORMAL
- en: In my experience, ChatGPT-4’s capabilities have been truly impressive. Its ability
    to produce text that closely resembles human writing is remarkable. This proficiency
    creates a fascinating challenge in distinguishing between content generated by
    the machine and content crafted by a human hand.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1 How ChatGPT creates human-like text
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Language models like ChatGPT operate on the fundamental principle of predicting
    the next word or token in a sequence of text based on the preceding words. This
    predictive capability is the result of training on vast amounts of textual data
    from the internet. Let’s explore further the workings of this process.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine we start with a text prompt, such as “The benefits of electric vehicles
    are …” The model, in essence, plays a game of probability, attempting to guess
    the most likely word or token to follow. To make this prediction, it learns from
    the patterns in language found across the internet. The model creates a ranked
    list of potential words that could logically follow *are*. In this example, it
    might suggest words like *plentiful*, *numerous*, *economic*, *clear*, or *not*.
    Importantly, the model doesn’t just deal with complete words but also with tokens,
    which are sequences of characters or word fragments. Tokens can include not only
    whole words but also sub-words and trailing spaces. A useful rule of thumb is
    that one token typically represents about four characters of standard English
    text. This tokenization process allows the model to process and analyze text efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: What sets language models like ChatGPT apart is their ability to generate responses
    that aren’t solely driven by the highest probability choice. Instead, they sometimes
    introduce a creative touch by selecting words with lower probabilities. This creative
    element often results in responses that feel more human-like and less formulaic.
    One remarkable aspect of these language models is their capacity to understand
    context. They achieve this by learning from a vast and diverse range of texts,
    encompassing billions of sources. This learning includes exposure to text where
    certain words are omitted or paraphrased, allowing the model to predict semantically
    similar text. This mimics an understanding of context and meaning in human language.
  prefs: []
  type: TYPE_NORMAL
- en: The training process for models like ChatGPT is extensive and meticulous. It
    begins with training on a large dataset comprising internet text. During training,
    the model’s predictions for the next token are compared to human-written text.
    The model then adjusts its internal structure and performance to minimize discrepancies
    and improve its predictive capabilities. For instance, GPT-2, a predecessor of
    ChatGPT, featured 1.5 billion parameters and was trained on 40 GB of internet
    text. The subsequent iteration, GPT-3, took a substantial leap in complexity with
    175 billion parameters.
  prefs: []
  type: TYPE_NORMAL
- en: The training process for AI models is a complex and essential step in their
    development. It involves the iterative adjustment of the numerous weights and
    parameters within the model through the application of deep learning algorithms.
    This fine-tuning process allows the model to learn from vast datasets and improve
    its performance over time. One key factor in the recent advancements in AI training
    is the utilization of modern GPUs (graphics processing units) and hardware enhancements.
    These powerful computational tools have revolutionized the field by enabling models
    to process millions of training examples simultaneously, significantly accelerating
    the training process. This parallel processing capability is particularly crucial
    because it allows AI researchers and engineers to train increasingly large and
    complex models efficiently. Moreover, the use of distributed computing and specialized
    hardware has further enhanced the speed and efficiency of AI model training. These
    advancements have opened the door to solving complex problems that were once considered
    computationally infeasible.
  prefs: []
  type: TYPE_NORMAL
- en: In essence, language models such as ChatGPT represent a remarkable advancement
    in the field of NLP. These models are powered by deep learning algorithms and
    have been meticulously trained on vast and diverse datasets sourced from the internet.
    At their core, these models employ a form of predictive intelligence that enables
    them to understand and generate text with a level of proficiency that was once
    the exclusive domain of human authors. They not only follow the rules of grammar
    but also have the capacity to mimic the nuanced style of human-written text. This
    predictive prowess is a result of their ability to capture complex patterns and
    relationships within language. For example, when presented with a sentence fragment
    like “The sun is shining, and the birds are,” these models can accurately predict
    that the next word might be “singing” or “flying,” depending on the context. This
    capacity to simulate human-like language use makes them invaluable in a wide array
    of language-related applications. For instance, they serve as the backbone for
    chatbots, providing users with responses that are not only grammatically correct
    but also contextually relevant. They are adept at adjusting to different conversational
    tones, styles, and domains, making them versatile tools for tasks such as customer
    support, where they can handle a range of inquiries and issues while maintaining
    a consistent and human-like tone. Language models are equally proficient in summarizing
    lengthy documents and facilitating efficient information retrieval. With the ability
    to generate creative content, they can write articles, generate poetry, and compose
    music lyrics, all while adhering to the desired tone and style.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2 ChatGPT hallucination
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Like any other deep learning model, ChatGPT lacks the capacity to grasp its
    own statements or the meanings of the words it employs. Its primary function is
    to generate words in response to given inputs, and its proficiency in predicting
    word combinations does not guarantee the accuracy of the generated text. In the
    following examples, I will illustrate that ChatGPT is incapable of comprehending
    human language and struggles with its complexities. To commence, I posed factual
    questions related to subjects I am well-versed in, specifically, myself and Brighterion,
    the company I established after relocating to the United States. On July 7, 2023,
    I inquired, “Who is Akli Adjaoute?” To my astonishment, the response identified
    me as the founder of “Brighter IA,” a privacy-focused video company (figure 5.1).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 ChatGPT prompt, which resulted in incorrectly identifying the author
    as the founder and CEO of a company he was not affiliated with.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: I became intrigued by how ChatGPT generated its responses, so I closed the session
    and initiated a new one, asking the same question. The answer I received was different,
    yet still incorrect. It contained numerous fabricated details, including the claim
    that I had received the prestigious Innovators Under 35 award from *MIT Technology
    Review*. Perplexed, I closed the session and started a new one once again. On
    my third attempt with the same question, I received yet another set of invented
    facts. This time, it included claims about me holding a PhD from the University
    of Illinois and having held positions at prominent technology companies and research
    institutions like IBM and Yahoo. It struck me as odd to receive three distinct
    fictitious responses.
  prefs: []
  type: TYPE_NORMAL
- en: I continued to ask the same question, and on the fifth attempt, I finally received
    an accurate response (figure 5.2). It left me wondering how anyone could rely
    on a program that consistently generated random and incorrect responses to the
    same question.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 ChatGPT prompt and output required five attempts to generate correct
    information.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: It’s worth noting that my question was not obscure; it pertained to Brighterion,
    a well-known company that offers enterprise AI applications for payment service
    providers, financial institutions, healthcare payers, and merchants. Furthermore,
    Brighterion has been extensively covered in the media, so information on the subject
    is widely available. As for myself, while I’m not a public figure, my first and
    last names are both highly unusual and unlikely to be confused with someone else.
    Moreover, I’ve been credited as the founder of Brighterion in numerous reputable
    sources, including the *Wall Street Journal*, *Forbes*, *USA TODAY*, CNBC, and
    various others. The point is to emphasize that this information is neither obscure
    nor difficult to find.
  prefs: []
  type: TYPE_NORMAL
- en: On April 9, 2023, during another set of tests, ChatGPT made an incorrect statement
    suggesting that I played football for Marseille in 1994\. When I mentioned my
    affiliation with FC Rouen, ChatGPT insisted that its records confirmed this erroneous
    claim (figure 5.3).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 ChatGPT insisted that the author played for a football team that
    he was never a part of.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The amalgamation of factual information with elements of fiction can render
    these programs significantly dangerous when utilized as sources of verifiable
    facts. This danger arises from the potential for such programs to blur the lines
    between reality and imagination, which, in turn, can have profound consequences
    on individuals and society as a whole. When truth and fiction intertwine within
    these programs, they often create a distorted version of reality that can easily
    mislead and confuse consumers of the content. This distortion can range from relatively
    harmless misinformation to more harmful disinformation, with the latter intentionally
    spreading false narratives to manipulate public perception or achieve specific
    agendas. One of the primary concerns is that when individuals encounter such content,
    they may unknowingly accept the fictional elements as truth, leading to misinformed
    beliefs and decisions. This can manifest in various ways, such as forming inaccurate
    opinions about current events, making misguided health choices, and even affecting
    political and social discourse. When people can’t distinguish between genuine
    and fabricated content, they become increasingly skeptical of news organizations,
    government bodies, and scientific authorities. Furthermore, the spread of false
    information can erode social cohesion and sow division within communities. It
    has the potential to amplify existing fault lines and create new ones, leading
    to polarization and animosity among different groups.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Bard
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: During the 2023 I/O event, Google introduced Bard, a competitor to ChatGPT.
    (Please note that Google has recently released a new version named Gemini.) On
    July 8 of that year, I initiated a prompt to Bard with the phrase “Akli Adjaoute
    20 awards.” Unfortunately, much like with ChatGPT, a substantial portion of the
    Bard’s response was found to be fictional (figure 5.4).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-4.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 5.4 Bard attributed a combination of factual and fictional awards
    to the author, rendering the output unreliable.**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As previously mentioned, my first and last names are quite uncommon. Nevertheless,
    the AI consistently confused me with other individuals and attributed their accomplishments
    and careers to me. I have never been honored with the IEEE Longuet-Higgins Prize,
    the ACM Paris Kanellakis Award, or the IEEE Computer Society Technical Achievement
    Award.
  prefs: []
  type: TYPE_NORMAL
- en: Following an initial unreliable response, I terminated the session and rephrased
    my question with “12” instead of “20.” To my dismay, I received a different but
    equally dubious response. The AI provided a new list of accolades I have never
    earned, and to compound matters, it now falsely asserted that I hold the position
    of a computer science professor at Stanford University and am a co-founder of
    the Stanford Center for Artificial Intelligence in Finance.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout my career, I have been an entrepreneur and, as such, have not authored
    over 100 papers in esteemed academic journals. My research has never received
    funding from the National Science Foundation, the Department of Defense, or the
    European Union.
  prefs: []
  type: TYPE_NORMAL
- en: I have never been recognized as one of the “Top 50 AI Leaders in the World”
    by the *MIT Technology Review*, and I am not affiliated with the Association for
    the Advancement of Artificial Intelligence (AAAI) or the Institute of Electrical
    and Electronics Engineers (IEEE).
  prefs: []
  type: TYPE_NORMAL
- en: Not only did the AI make inaccurate assertions and misrepresent facts, but it
    also attributed its errors to an irrelevant primary source. Such misinformation
    raises concerns about the reliability of citations, which are fundamental to academic
    work (figure 5.5).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-5.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 AI made inaccurate assertions attributed to an irrelevant citation.
    This information raises concerns about the reliability of citations.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Distinguishing between facts and falsehoods can be challenging, especially for
    those who lack familiarity with a subject. Accepting fabricated information can
    lead to significant and harmful consequences. For instance, a mayor in Australia
    named Brian Hood is threatening to file a defamation lawsuit against OpenAI’s
    ChatGPT for falsely suggesting that he went to prison for a bribery scandal involving
    a subsidiary of the Reserve Bank of Australia (figure 5.6). Hood was, in fact,
    a whistleblower in this case [11].
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 Australian mayor Brian Hood is threatening a lawsuit over ChatGPT
    falsely stating that he went to prison for bribery when, in fact, he was the whistleblower,
    not the criminal.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Software capable of generating fabricated information poses a significant threat
    to both individuals and society as a whole. To address these challenges, it is
    imperative that we demand that technology be developed and utilized in ways that
    prioritize accuracy, transparency, and responsible information sharing. This means
    holding tech companies accountable for the content on their platforms and implementing
    mechanisms to verify the authenticity of information.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Humans vs. LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In October 2011, Apple unveiled the iPhone 4S, which marked the debut of Siri,
    a virtual personal assistant. Apple acquired the technology behind Siri in 2010
    through its acquisition of a startup with the same name. This startup originated
    from the SRI International Artificial Intelligence Center, an institution tracing
    its roots back to the Stanford Research Institute (SRI). The name “Siri” is essentially
    a phonetic representation of “SRI.”
  prefs: []
  type: TYPE_NORMAL
- en: Siri gained recognition for its hands-free functionality, which allows it to
    continuously monitor the microphone of the host device. Using a deep neural network,
    it analyzes sounds from its environment. When it identifies the phrase “Hey Siri”
    with a high level of confidence, the full application becomes active and processes
    and responds to subsequent sounds as questions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In October 2017, the Siri development team provided a detailed explanation
    of this functionality in an article titled “Deep Learning for Siri’s Voice: On-Device
    Deep Mixture Density Networks for Hybrid Unit Selection Synthesis” [12]:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The microphone in an iPhone or Apple Watch turns your voice into a stream of
    instantaneous waveform samples, at a rate of 16,000 per second. A spectrum analysis
    stage converts the waveform sample stream to a sequence of frames, each describing
    the sound spectrum of approximately 0.01 sec. About 20 of these frames at a time
    (0.2 sec of audio) are fed to the acoustic model, a Deep Neural Network (DNN),
    which converts each of these acoustic patterns into a probability distribution
    over a set of speech sound classes: those used in the “Hey Siri” phrase, plus
    silence and other speech, for a total of about 20 sound classes. The DNN consists
    mainly of matrix multiplications and logistic nonlinearities. The training process
    adjusts the weights using standard Backpropagation and stochastic gradient descent.
    … Next time you say, “Hey Siri,” you may think of all that goes on to make responding
    to that phrase happen, but we hope that it “just works!”'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'This article should serve as an indispensable reference point for any educational
    material on artificial intelligence. In particular, it sheds light on the formidable
    challenge faced by personal assistant programs when attempting to engage in genuinely
    meaningful conversations. Although these AI systems may initially appear clever,
    they function much like well-trained parrots, lacking a genuine understanding
    of the words they process or produce. This deficiency becomes evident with even
    a modest degree of experimentation. Consider the scenario in which you ask Siri
    to recommend a restaurant. If you repeatedly respond with, “No, I don’t like that,”
    Siri will persist in offering new suggestions. However, if you veer off-topic
    by asking an unrelated question and then return to your restaurant inquiry, Siri
    will present the same list of options in the same order despite your prior expressions
    of dissatisfaction. Such rigid behavior is in stark contrast to the adaptability
    and comprehension exhibited by humans in similar situations. The lesson here is
    clear: communication transcends mere words.'
  prefs: []
  type: TYPE_NORMAL
- en: The gift of language endows us with the ability to share an endlessly diverse
    array of complex ideas and profound emotions with others. Through language, we
    can convey what we know and extract knowledge by posing questions. When someone
    speaks to us, our brains perform an almost magical exploit by instantaneously
    translating auditory sounds into coherent concepts, emotions, and vivid sensory
    experiences. For example, when the word *book* is uttered in a conversation, our
    minds swiftly decipher its meaning from its context and conjure up imagery of
    bound pages filled with text. It is indeed remarkable that we can communicate
    at all when each word within a sentence may possess numerous possible meanings.
    Take, for instance, the sentence, “The mining equipment at this gold mine is mine!”
    In this case, there is no ambiguity in the intended meaning, as our brains seamlessly
    distinguish whether *mine* functions as an adjective, a noun, or a possessive
    pronoun. Yet, the complexity of language comprehension extends far beyond these
    apparent challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Words possess the remarkable ability to evoke sensations and emotions through
    association. Take, for instance, the phrase “ocean breeze.” Even without explicitly
    mentioning the coldness of the sea air or the sound of crashing waves, these words
    conjure up a vivid sense of freshness, tranquility, and perhaps even a hint of
    adventure. Similarly, the phrase “crimson autumn leaves” transports us to a scene
    of vibrant foliage, crisp air, and the cozy embrace of fall, all without directly
    detailing the temperature or scenery. Moreover, words can be used metaphorically
    to express things that would make no sense if we only processed literal meaning.
    Consider the phrase “time flies.” This expression doesn’t imply that time possesses
    wings or takes flight like a bird; instead, it captures the concept of how time
    seems to pass swiftly and imperceptibly, emphasizing its fleeting nature. Likewise,
    when we say someone is “walking on air,” it doesn’t mean they are defying gravity
    but suggests an overwhelming sense of happiness and euphoria that seems almost
    weightless.
  prefs: []
  type: TYPE_NORMAL
- en: Even more astonishing is our capacity to employ words symbolically to communicate
    memories and experiences, with certain words holding a deeply personal significance
    known only to those involved. Take, for example, the phrase “olive oil,” as it
    resonates with my memories of Kabylia. Mentioned in a conversation, it instantly
    transports me back to the rolling hills and picturesque groves of olive trees,
    where the very air seemed to be infused with the rich, earthy aroma of freshly
    harvested olives. The scent alone carries the essence of that place and time,
    a sensory time machine that brings back the warm, sun-drenched days spent amidst
    the olive orchards. But it’s not just the scent; it’s the sense of community and
    tradition that olive oil represents for me. The olive harvest in Kabylia was a
    magical communion among people of all ages, coming together to gather the precious
    fruits of the land. I remember the laughter of children as they scampered between
    the trees, the wisdom shared by the elders about the art of olive harvesting,
    and the bonds that formed as we worked side by side, all united in the age-old
    ritual of collecting olives. In those moments, “olive oil” embodies not just a
    culinary ingredient but the collective spirit of a community, a tradition passed
    down through generations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to navigating the complexities of multiple meanings, associations,
    metaphors, and symbolism, our minds also possess the unique ability to anticipate
    and infer meaning from unspoken words. For instance, imagine yourself in a grocery
    aisle, and someone is obstructing your path to the item you seek. By uttering
    the phrase “Can I jus… .” with the appropriate tone, you convey not just your
    intention to retrieve an item but also your politeness, acknowledgment of their
    presence, and assurance that you’ll be quick. These unspoken nuances of communication
    are remarkably clear, with no confusion over whether you are asking for a dollar
    or attempting to take a nap where they stand. The grocery store scenario also
    illustrates another intriguing phenomenon: subtext and our capacity to “read between
    the lines.” A subtle shift in the tone of your voice while using the same words,
    “Can I jus… .,” can dramatically alter their meaning. Instead of conveying politeness
    and cooperation, a harsh tone can signal frustration and impatience, effectively
    demanding that the other person move aside. Their failure to comply could result
    in you uttering a polite-sounding “Thank you,” masking the true message conveyed
    by your tone and body language, which is quite the opposite of gratitude. Moreover,
    our ability to infer substantial information beyond the explicit words spoken
    is equally remarkable. For instance, if someone mentions having a “café au lait
    and scrambled eggs” for breakfast over a phone call, it offers a plethora of insights.
    This snippet of conversation hints at the time of day, the use of a cup, the presence
    of utensils, and proximity to a kitchen and even provides a sense of taste and
    aroma.'
  prefs: []
  type: TYPE_NORMAL
- en: Human communication is a remarkably multifaceted phenomenon, capable of conveying
    messages through not only words but also nonverbal cues, gestures, and various
    forms of expression. In fact, our ability to communicate extends beyond the literal
    words we use, often involving subtle nuances and indirect strategies. This indirect
    communication can serve various purposes, such as safeguarding someone’s feelings
    or imparting valuable lessons. For instance, consider a scenario in a corporate
    setting where a manager is working with an employee who needs to improve their
    time management skills. Instead of directly telling the employee to be more punctual
    and organized, the manager might employ a more indirect approach. They could share
    a story about a successful colleague known for their exceptional time management
    and how it positively affected their career. The manager might mention books or
    articles on effective time management techniques and suggest that the employee
    could find them interesting. By doing so, the manager guides the employee toward
    the realization that improved time management is crucial for career growth, encouraging
    them to work on it independently. In this way, the manager has effectively communicated
    a message without explicitly stating it.
  prefs: []
  type: TYPE_NORMAL
- en: Communication itself extends to a wide array of nonverbal cues and signals.
    In business presentations, the use of visuals like graphs and charts can convey
    complex data and ideas more effectively than words alone. In negotiations, a firm
    handshake or a well-timed pause during a conversation can communicate confidence
    and control. Even something as simple as the choice of attire in a professional
    setting can send signals about one’s professionalism and attention to detail without
    the need for explicit statements.
  prefs: []
  type: TYPE_NORMAL
- en: Machines and artificial intelligence have not reached the level of human proficiency.
    Perhaps the most compelling evidence of AI’s limitations in communication is its
    inability to engage in meaningful conversation for extended periods. The Loebner
    Prize, a competition resembling the Turing test, ran for nearly three decades,
    challenging AI systems to simulate human conversation via text and audiovisual
    input. Despite significant advancements in AI, no system convincingly passed as
    human during these tests. Even the most advanced chatbots reveal their limitations
    after brief interactions, highlighting the vast gap between machine learning and
    true understanding. Alan Turing, the pioneer of computer science, would likely
    be disappointed by our limited progress in this area, considering the substantial
    investments made in AI development.
  prefs: []
  type: TYPE_NORMAL
- en: To facilitate effective communication, machines must not only recognize the
    individual meanings of words but also interpret the complex structure of sentences,
    consider the contextual nuances, and discern the underlying intentions and objectives
    behind the communication. Achieving this level of comprehension and proficiency
    in language understanding has proven to be a Herculean task. Despite decades of
    relentless efforts, it’s important to recognize that although there has been significant
    advancement in NLP, it has yet to bridge the gap in communication. True understanding
    remains the elusive key to meaningful communication, a milestone that AI has yet
    to achieve.
  prefs: []
  type: TYPE_NORMAL
- en: 5.6 AI does not understand
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s take a moment to revisit the analogy between the game of chess and the
    complexity of language. Chess, a classic board game, comprises various pieces,
    a structured playing board, and a set of well-defined rules that dictate how players
    can move from one state of the game to another. Similarly, language, the cornerstone
    of human communication, involves a diverse set of linguistic units (the alphabet)
    and sophisticated syntactical rules that govern how we construct meaningful sentences.
    It’s tempting to draw parallels between these two seemingly disparate domains
    and question whether the alphabet could be akin to chess pieces and syntax rules
    could serve as our strategic moves in forming coherent sentences. Indeed, all
    languages, much like games, adhere to their own unique sets of rules, even if
    languages are not just about following syntactical rules as it demands a profound
    understanding of the meaning conveyed by those words and sentences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the remarkable achievements of AI, such as IBM’s Deep Blue defeating
    the world chess champion or AlphaGo outwitting the Go champion. While these accomplishments
    are undoubtedly impressive, they don’t signify the presence of genuine intelligence
    or understanding in the machines. Rather, they demonstrate the application of
    brute-force computing power and specialized AI algorithms to large datasets. These
    algorithms excel in specific tasks, like mathematical calculations or data sorting,
    surpassing human capabilities. In the same vein, LLMs trained on vast datasets
    from the internet exhibit the ability to generate coherent text that appears sensible.
    Yet, they also lack a genuine understanding as language understanding transcends
    mere pattern recognition or rule adherence. True understanding involves the capacity
    to conceptually associate words with objects, actions, and events in the real
    world. Consider these illustrative examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Punctuation can drastically alter meaning*. “Let’s eat Grandma” and “Let’s
    eat, Grandma.” Here, a simple comma has the power to save lives by clarifying
    the intended meaning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Language can have implicit meanings*. When a person says, “I waited an hour
    for you last night in the restaurant” to their date, it’s not just about the words
    spoken. The true essence of this statement lies in the unspoken desperation and
    emotion, a level of nuance that eludes current AI systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Word order changes meaning*. “Jim was furious, and Jenny ended the call” conveys
    a different narrative than “Jenny ended the call, and Jim was furious.” In both
    cases, the same words are used, but the sequencing of these words fundamentally
    alters the meaning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The analogy between chess and language serves as a thought-provoking starting
    point to explore the capabilities and limitations of AI in understanding language.
    While AI systems, including LLMs, exhibit remarkable prowess in generating text,
    they are fundamentally pattern-matching tools, and genuine language understanding
    remains an elusive frontier to current artificial intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: Attempting to attribute understanding to algorithms that primarily learn patterns
    to produce text is analogous to attributing mathematical understanding to a pocket
    calculator that rapidly performs arithmetic calculations.
  prefs: []
  type: TYPE_NORMAL
- en: The two questions that follow could have been easily answered if ChatGPT possessed
    the ability to understand their meaning and relate them to real-world knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: 'Question 1: A hunter shoots a branch with three birds and kills one. How many
    are left? (figure 5.7)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![figure](../Images/5-7.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 AI missed the subtlety of the question and returned an incorrect
    response.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Although the answer to this question is available online, ChatGPT’s response
    was “If the hunter shoots a branch with three birds and kills one, then there
    will be two birds left on the branch.” However, the correct answer is zero, as
    other birds would likely fly away due to the noise of the gunshot. ChatGPT’s response
    illustrates its reliance on pattern-matching and a lack of the ability to infer
    context as a human would. Furthermore, providing the correct response to ChatGPT
    does not necessarily lead to genuine learning or understanding on the AI’s part.
    While it may increase the probability of the correct response being generated
    in similar situations, it does not indicate true comprehension or reasoning. This
    highlights a challenge in AI development, where models like ChatGPT can generate
    seemingly plausible answers without a deep understanding of the content or context.
  prefs: []
  type: TYPE_NORMAL
- en: 'Question 2: Suppose I’m on the 10th floor, and I ask someone, could you please
    take this bucketful of water and run to the reception area on the first floor?
    What happens?'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I submitted the query “running with a bucket full of water splash” to Google
    and obtained more than 16 million search results (figure 5.8). Consequently, it’s
    reasonable to assume that ChatGPT had access to ample information to deduce the
    fundamental outcomes associated with running while carrying a bucket full of water
    (figure 5.9).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-8.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.8 Illustration of information readily available in Google (16.2 million
    results)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![figure](../Images/5-9.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 AI returns results unrelated to the query, making it evident that
    the response was computer-generated.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The reply is self-explanatory. Although the algorithm appeared to recognize
    the concept of sloshing, its emphasis on gravity and acceleration was completely
    unrelated, making it evident that the response was computer generated. Substituting
    the term “bucketful” with “glass” only made the response appear even more bizarre
    (figure 5.10).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-10.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.10 AI returns an unclear and logically incoherent response.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Once more, the algorithm’s reply remains disconnected from the subject, unclear,
    and devoid of logical coherence. Human understanding encompasses a substantial
    nonlinguistic component, which AI struggles to encompass, given the vast complexity
    of reality and its myriad subtleties. In our human interactions, we possess an
    understanding of our intended message, carefully selecting words in response to
    the context and considering the potential consequences and reactions. We convey
    not only the dictionary definitions of our words but also imply deeper nuances
    and infer meanings beyond their literal interpretations.
  prefs: []
  type: TYPE_NORMAL
- en: 5.7 Benefits of LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generative AI stands as a remarkable technological advancement with the potential
    to bring about a transformative effect across a multitude of industries. Its far-reaching
    implications signify a significant milestone in the evolution of AI algorithms,
    offering the promise of enhanced productivity and innovation in various domains.
    One of the most compelling aspects of generative AI is its capacity to revolutionize
    content creation. This technology empowers creative professionals, such as designers,
    writers, and artists, by enabling the generation of highly realistic images, videos,
    and text. By automating the creative process, it streamlines workflows, potentially
    freeing up valuable time for these creators to focus on refining their ideas rather
    than grappling with the complexity of content generation. Furthermore, generative
    AI is transforming natural language understanding and generation. Chatbots and
    virtual assistants powered by these models can have more context-aware and human-like
    conversations, improving customer support and user interaction across various
    industries.
  prefs: []
  type: TYPE_NORMAL
- en: Education is another domain where generative AI has the potential to have a
    substantial effect. Students can benefit from quick access to synthesized information
    on various subjects, significantly reducing the time needed to acquire knowledge.
    For instance, asking questions about history or geography can yield summarized
    information drawn from numerous sources.
  prefs: []
  type: TYPE_NORMAL
- en: In the field of computer programming, generative AI could potentially eliminate
    the need for extensive reference manuals. These systems can use their extensive
    knowledge base to swiftly generate code solutions, a process that might take a
    junior programmer hours of trial and error. By learning from specialized resources
    like StackOverflow, LLMs can become invaluable tools for developers seeking to
    solve complex coding challenges efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, in the business of customer support, generative AI can be harnessed
    to provide procedural responses to common user inquiries. These responses can
    be derived from extensive training with software documentation, ensuring consistency
    and efficiency in addressing user needs. In conclusion, generative AI represents
    a paradigm shift in how we approach creativity, communication, education, and
    problem-solving.
  prefs: []
  type: TYPE_NORMAL
- en: 5.8 LLM limits
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While LLMs exhibit remarkable capabilities in pattern recognition, they also
    exhibit certain limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Dependence on input quality* —The quality of LLM-generated outputs is heavily
    reliant on the quality of the input data. Concerns about the “garbage-in, garbage-out”
    (GIGO) principle persist, as large datasets may lack diversity, and online data
    often contains negative or false information. Consequently, there is a risk that
    biased or erroneous training data can influence the LLM’s outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Computational intensity* —LLMs demand significant computational resources
    for both training and operation due to their reliance on vast datasets and substantial
    computing power.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hallucinations and incoherent text* —LLMs can generate text that lacks coherence
    and context, often producing content that doesn’t make sense due to their limited
    understanding of human language.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Repetition and lack of creativity* —LLM-generated content can be repetitive
    and lack creativity, resulting in outputs that feel formulaic or uninspired.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Lack of interpretability* —Since LLMs are based on deep learning, a technology
    that doesn’t provide insights into decision-making processes or output generation,
    their results are challenging to interpret or explain.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Security concerns* —LLMs can be harnessed to create harmful content, such
    as deepfakes and disinformation, posing significant security risks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Limited learning capacity* —LLMs have a constrained learning capacity because
    they do not possess inherent expertise. As demonstrated in chapter 3, many domains,
    including medicine, involve implicit knowledge that isn’t readily captured by
    LLMs, highlighting the limits of their learning capabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 5.9 Generative AI and intellectual property
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Concerns have arisen among artists, authors, and news publishers regarding the
    utilization of their intellectual property in the training of generative AI (figure
    5.11). On July 7, 2023, ChatGPT4 acknowledged that its training procedure incorporated
    an extensive corpus of text data sourced from diverse internet outlets, encompassing
    books, articles, websites, and assorted written content.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-11.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.11 ChatGPT’s answer to a query about the source of the data it uses
    raises concerns about IP rights and accuracy.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Robert Thomson, the CEO of News Corp, issued a warning regarding the potential
    jeopardy to intellectual property rights. He stated [13]:'
  prefs: []
  type: TYPE_NORMAL
- en: To begin with, our content is being collected, scraped, and otherwise absorbed
    for the purpose of training AI systems. Secondly, individual stories may emerge
    prominently in specific searches. And, thirdly, our content could be synthesized
    and presented as original when, in reality, it is an extraction of editorial essence.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The concerns surrounding intellectual property are well-founded, as some AI
    companies derive their value from the utilization and repurposing of creative
    works produced by countless individuals. Without access to such content, the existence
    of generative AI models like ChatGPT might be doubtful. It presents a paradox
    that companies reusing the contributions of artists, advertisers, and writers
    could inadvertently jeopardize the livelihoods of these very creators.
  prefs: []
  type: TYPE_NORMAL
- en: 5.10 Risks of generative AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generative AI programs pose a significant risk due to their propensity for fabricating
    information. A recent case reported by the BBC highlights this concern, where
    a New York lawyer is facing a lawsuit for utilizing ChatGPT in his legal research.
    The lawyer’s firm submitted a brief referencing various past court cases, six
    of which were later identified as fictitious, complete with fabricated quotes
    and citations. ChatGPT generated this false information. Attached screenshots
    from the conversation between the lawyer and ChatGPT reveal the lawyer inquiring,
    “Is Varghese a real case?” to which ChatGPT affirmed, and when pressed for its
    source, ChatGPT reiterated that the case was genuine and accessible on legal databases
    like LexisNexis and Westlaw. The authenticity of the other five made-up cases
    was similarly asserted. The lawyer informed the court that he was “unaware that
    its content could be false” but now faces potential disciplinary action from the
    judge.
  prefs: []
  type: TYPE_NORMAL
- en: A second instance of this risk was reported by *The Wall Street Journal* [14]
    regarding a chatbot designed to assist individuals with eating disorders like
    anorexia. Following an update with generative AI, the bot started advocating for
    dieting and calorie reduction. Clearly, such recommendations could be highly detrimental
    to individuals struggling with eating disorders, prompting the National Eating
    Disorders Association to quickly remove the bot from its platform.
  prefs: []
  type: TYPE_NORMAL
- en: The present inability of generative AI to reason or comprehend can lead to adverse
    outcomes in various ways. For instance, despite incorporating explicit safety
    protocols and impressive generative capabilities, ChatGPT was manipulated into
    displaying instructions for constructing explosive devices, a feature learned
    from its training data [15]. While the ChatGPT team swiftly addressed this issue
    by implementing safeguards, it’s essential to acknowledge the resourcefulness
    of ill-intentioned individuals in devising new methods to exploit such programs,
    akin to the sophistication demonstrated by certain cybercriminals who continually
    innovate hacking techniques and fraudulent schemes. Given ChatGPT’s inability
    to discern that disseminating instructions for creating a harmful device contradicts
    its safety mandate, it may be prudent to reassess our optimism regarding some
    of its other achievements.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is also the risk of individuals unquestioningly accepting all generative
    AI outputs at face value. In his 1976 book *Computer Power and Human Reason* [16],
    Joseph Weizenbaum, the creator of the first chatbot, ELIZA in 1966 emphasized
    this concern:'
  prefs: []
  type: TYPE_NORMAL
- en: ELIZA created the most remarkable illusion of having understood in the minds
    of the many people who conversed with it. People who knew very well that they
    were conversing with a machine soon forgot that fact, just as theatergoers, in
    the grip of suspended disbelief, soon forget that the action they are witnessing
    is not “real.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Weizenbaum further pointed out that individuals with limited or no knowledge
    of computers tended to be particularly susceptible to this illusion. They frequently
    insisted on engaging with the system in private, and despite any objections from
    the program’s creator, they would firmly maintain that they had been understood.
  prefs: []
  type: TYPE_NORMAL
- en: Increasingly, both institutions and individuals are becoming reliant on automated
    systems and AI in their daily operations. It’s common for people to unquestionably
    accept the results generated by their computers, assuming that AI will always
    provide truthful information. However, our world is plagued by the proliferation
    of fake news, and the rise of generative AI technology poses a serious concern.
    It has the potential to flood the internet with text, images, and videos that
    are nearly indistinguishable from genuine content, making it difficult to discern
    what is real and what is fake. The dangers extend to the manipulation of public
    opinion through propaganda, which often relies on repeated messaging.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the catastrophic consequences that could arise from the fusion of disinformation
    and AI-generated content. Imagine someone using generative AI to fabricate a video
    depicting a group of individuals burning a sacred book, potentially inciting extreme
    reactions from fanatical groups. Furthermore, there have been instances where
    LLMs have provided fabricated responses and inappropriate suggestions, raising
    concerns about their use in critical applications such as emergency response or
    law enforcement. Relying on a system that consistently generates nonsensical answers
    is clearly untenable. Imagine employing such a system for medical recommendations;
    the risks are evident. Another worrisome aspect is the possibility that students
    may turn to generative AI tools for their academic assignments. Since these programs
    can only regurgitate information that has already been conceived, written, or
    created by humans, there is a legitimate concern that we might foster a generation
    of students who mindlessly echo “Chat GPT told me so” in unison.
  prefs: []
  type: TYPE_NORMAL
- en: 'An article in the British newspaper *The Guardian*, titled “The Stupidity of
    AI” [17], succinctly summarizes the risks associated with placing unwavering trust
    in this type of technology:'
  prefs: []
  type: TYPE_NORMAL
- en: The belief in this kind of AI as actually knowledgeable or meaningful is actively
    dangerous. It risks poisoning the well of collective thought, and of our ability
    to think at all. If, as is being proposed by technology companies, the results
    of ChatGPT queries will be provided as answers to those seeking knowledge online,
    and if, as has been proposed by some commentators, ChatGPT is used in the classroom
    as a teaching aide, then its hallucinations will enter the permanent record, effectively
    coming between us and more legitimate, testable sources of information, until
    the line between the two is so blurred as to be invisible.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 5.11 LLMs and the Illusion of Understanding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLM technology stands as a testament to the ever-expanding horizons of AI, consistently
    pushing the boundaries of what was once thought to be exclusive to human capabilities.
    Nevertheless, the current fervor surrounding LLMs echoes the excitement witnessed
    in 2011 when IBM’s Watson outperformed two of Jeopardy’s top champions. At that
    time, Watson gained acclaim for its adept handling of complex questions and optimal
    answer-finding abilities [18]. Bold declarations hailed Watson as a game-changer,
    with assertions that it could revolutionize information-rich sectors, particularly
    those inundated with vast volumes of unstructured and semi-structured data, such
    as healthcare, banking, insurance, and telecommunications. Have these predictions
    come to fruition? Not entirely, and similar outcomes are expected for numerous
    ambitious predictions regarding LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Much like other AI programs, LLMs essentially execute a predetermined set of
    logical operations. Earlier iterations of neural networks and algorithms, like
    backpropagation, were characterized by limited layers, parameters, and data-processing
    capabilities. Present-day deep learning algorithms rely on supercomputers to train
    on extensive datasets, yet the core outcome remains fairly consistent: they discern
    patterns without genuine reasoning or understanding. To draw a comparison, consider
    the evolution of pocket calculators over the past half-century. The initial models
    could solely perform basic arithmetic operations like addition, subtraction, multiplication,
    and division. However, modern calculators rely on enhanced memory and powerful
    hardware to perform complex functions such as logarithms, trigonometric operations,
    and hyperbolic functions. Some can even generate colorful 3D surface graphs.'
  prefs: []
  type: TYPE_NORMAL
- en: It is well-established that the English language contains dictionaries with
    fewer than 500,000 entries, and most English speakers employ only a fraction of
    those words in their daily communication. Consequently, achieving a competent
    command of English seems to be a relatively modest objective, given the millions
    who achieve it annually. Additionally, human-generated text exhibits a high degree
    of correlation, making it easily predictable. The patterns and statistical relationships
    are inherent in the data, and individuals can accurately anticipate sentence completions
    based on these cues. In stark contrast, LLMs function as advanced predictive text
    systems, analyzing preceding words to generate the most probable following word.
    While this automated process might appear to produce intelligent outputs, it is
    essentially an artificial imitation.
  prefs: []
  type: TYPE_NORMAL
- en: Human communication relies on innate knowledge encompassing space, time, and
    various fundamental aspects of the world. For instance, when someone types “The
    sky is blue,” we comprehend it due to our familiarity with the sensations and
    meanings associated with the words *sky* and *blue*. In contrast, LLMs glean their
    understanding from conversations, comments, books, and websites, where the co-occurrence
    of certain words is observed. The essence of human text, which comprises nuanced
    context, eludes deep learning algorithms, regardless of their extensive parameter
    count and impressive pattern recognition capabilities. Generative AI lacks the
    capacity to introduce entirely novel concepts or ideas; it merely rearranges existing
    elements.
  prefs: []
  type: TYPE_NORMAL
- en: In my perspective, embracing these systems carries the risk of transitioning
    from a world where knowledge is cumulative and rigorously validated to one where
    knowledge is approximated and challenging to verify. I believe that LLMs serve
    as a striking example of the “AI illusions” alluded to in the title of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generative AI relies on vast datasets and substantial computing resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs like ChatGPT often produce repetitive text and can generate content that
    isn’t accurate, a phenomenon known as *hallucination*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative AI programs are rooted in deep learning, lacking the capability to
    explain their output-generation process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can be manipulated to generate harmful content, including deepfakes and
    various forms of disinformation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI communication is notably limited compared to human interaction, as AI lacks
    a genuine understanding of text, audio, or images.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Human communication encompasses more than words; it relies heavily on context
    and concepts such as metaphor, analogy, and sarcasm, which are challenging for
    AI to grasp.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While generative AI marks significant progress, it is still subject to numerous
    limitations and associated risks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Like other AI algorithms, the quality of generative AI output is heavily influenced
    by the quality of the input data and prompts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Despite advancements, AI lacks the human ability to generalize and adapt to
    diverse contexts effectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
