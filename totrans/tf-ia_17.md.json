["```py\nimport tensorflow_datasets as tfds\n\n# Construct a tf.data.Dataset\nfashion_ds = tfds.load('fashion_mnist')\n```", "```py\nprint(fashion_ds)\n```", "```py\n{'test': <PrefetchDataset shapes: {image: (28, 28, 1), label: ()}, types: \n➥ {image: tf.uint8, label: tf.int64}>, 'train': <PrefetchDataset shapes: \n➥ {image: (28, 28, 1), label: ()}, types: {image: tf.uint8, label: \n➥ tf.int64}>}\n```", "```py\ndef get_train_valid_test_datasets(fashion_ds, batch_size, \n➥ flatten_images=False):\n\n    train_ds = fashion_ds[\"train\"].shuffle(batch_size*20).map(\n        lambda xy: (xy[\"image\"], tf.reshape(xy[\"label\"], [-1]))         ❶\n    )\n    test_ds = fashion_ds[\"test\"].map(\n        lambda xy: (xy[\"image\"], tf.reshape(xy[\"label\"], [-1]))         ❷\n    )\n\n    if flatten_images:\n        train_ds = train_ds.map(lambda x,y: (tf.reshape(x, [-1]), y))   ❸\n        test_ds = test_ds.map(lambda x,y: (tf.reshape(x, [-1]), y))     ❸\n\n    valid_ds = train_ds.take(10000).batch(batch_size)                   ❹\n\n    train_ds = train_ds.skip(10000).batch(batch_size)                   ❺\n\n    return train_ds, valid_ds, test_ds\n```", "```py\nid2label_map = {\n    0: \"T-shirt/top\",\n    1: \"Trouser\",\n    2:\"Pullover\",\n    3: \"Dress\",\n    4: \"Coat\",\n    5: \"Sandal\",\n    6: \"Shirt\",\n    7: \"Sneaker\",\n    8: \"Bag\",\n    9: \"Ankle boot\"\n}\n```", "```py\nlog_datetimestamp_format = \"%Y%m%d%H%M%S\"\n\nlog_datetimestamp = datetime.strftime(datetime.now(), \n➥ log_datetimestamp_format)\nimage_logdir = \"./logs/data_{}/train\".format(log_datetimestamp)\n```", "```py\nimage_writer = tf.summary.create_file_writer(image_logdir)\n```", "```py\nwith image_writer.as_default():\n    for data in fashion_ds[\"train\"].batch(1).take(10):\n        tf.summary.image(\n            id2label_map[int(data[\"label\"].numpy())], \n            data[\"image\"], \n            max_outputs=10, \n            step=0\n        )\n\n# Write a batch of 20 images at once\nwith image_writer.as_default():\n    for data in fashion_ds[\"train\"].batch(20).take(1):\n        pass\n    tf.summary.image(\"A training data batch\", data[\"image\"], max_outputs=20, step=0)\n```", "```py\n%load_ext tensorboard\n%tensorboard --logdir ./logs --port 6006\n```", "```py\nfrom tensorflow.keras import layers, models\n\ndense_model = models.Sequential([\n    layers.Dense(512, activation='relu', input_shape=(784,)),\n    layers.Dense(256, activation='relu'),\n    layers.Dense(10, activation='softmax')\n])\n\ndense_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n```", "```py\nlog_datetimestamp_format = \"%Y%m%d%H%M%S\"\nlog_datetimestamp = datetime.strftime(\n    datetime.now(), log_datetimestamp_format\n)\n\ndense_log_dir = os.path.join(\"logs\",\"dense_{}\".format(log_datetimestamp))\n```", "```py\n./logs/dense/run_2021-05-27-03-14-21_lr=0.01\n./logs/dense/run_2021-05-27-09-02-52_lr=0.001\n./logs/dense/run_2021-05-27-10-12-09_lr=0.001\n./logs/dense/run_2021-05-27-14-43-12_lr=0.0005\n```", "```py\n./logs/dense/lr=0.01/2021-05-27-03-14-21\n./logs/dense/lr=0.001/2021-05-27-09-02-52\n./logs/dense/lr=0.001/2021-05-27-10-12-09\n./logs/dense/lr=0.0005/2021-05-27-14-43-12\n```", "```py\nbatch_size = 64\ntr_ds, v_ds, ts_ds = get_train_valid_test_datasets(\n    fashion_ds, batch_size=batch_size, flatten_images=True\n)\n```", "```py\ntb_callback = tf.keras.callbacks.TensorBoard(\n    log_dir=dense_log_dir, profile_batch=0\n)\n```", "```py\ntf.keras.callbacks.TensorBoard(\n    log_dir='logs', histogram_freq=0, write_graph=True,\n    write_images=False, write_steps_per_second=False, update_freq='epoch',\n    profile_batch=2, embeddings_freq=0, embeddings_metadata=None, \n)\n```", "```py\ndense_model.fit(tr_ds, validation_data=v_ds, epochs=10, callbacks=[tb_callback])\n```", "```py\nconv_model = models.Sequential([\n    layers.Conv2D(\n       filters=32, \n       kernel_size=(5,5), \n       strides=(2,2), \n       padding='same', \n       activation='relu', \n       input_shape=(28,28,1)\n    ),\n    layers.Conv2D(\n        filters=16, \n        kernel_size=(3,3), \n        strides=(1,1), \n        padding='same', \n        activation='relu'\n    ),\n    layers.Flatten(),\n    layers.Dense(10, activation='softmax')\n])\n\nconv_model.compile(\n    loss=\"sparse_categorical_crossentropy\", optimizer='adam', \n➥ metrics=['accuracy']\n)\nconv_model.summary()\n```", "```py\nlog_datetimestamp_format = \"%Y%m%d%H%M%S\"\nlog_datetimestamp = datetime.strftime(\n    datetime.now(), log_datetimestamp_format\n)\n\nconv_log_dir = os.path.join(\"logs\",\"conv_{}\".format(log_datetimestamp))\n\nbatch_size = 64\ntr_ds, v_ds, ts_ds = get_train_valid_test_datasets(\n    fashion_ds, batch_size=batch_size, flatten_images=False\n)\n\ntb_callback = tf.keras.callbacks.TensorBoard(\n    log_dir=conv_log_dir, histogram_freq=2, profile_batch=0\n)\n\nconv_model.fit(\n    tr_ds, validation_data=v_ds, epochs=10, callbacks=[tb_callback]\n)\n```", "```py\nfrom tensorflow.keras import layers, models\nimport tensorflow.keras.backend as K\n\nK.clear_session()\ndense_model = models.Sequential([\n    layers.Dense(512, activation='relu', input_shape=(784,)),    \n    layers.Dense(256, activation='relu', name='log_layer'),    \n    layers.Dense(10, activation='softmax')\n])\n\ndense_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n```", "```py\ndense_model_bn = models.Sequential([\n    layers.Dense(512, activation='relu', input_shape=(784,)),\n    layers.BatchNormalization(),\n    layers.Dense(256, activation='relu', name='log_layer_bn'),\n    layers.BatchNormalization(),\n    layers.Dense(10, activation='softmax')\n])\n\ndense_model_bn.compile(\n    loss=\"sparse_categorical_crossentropy\", optimizer='adam', \n➥ metrics=['accuracy']\n)\n```", "```py\ndef train_model(model, dataset, log_dir, log_layer_name, epochs):    \n\n    writer = tf.summary.create_file_writer(log_dir)                        ❶\n    step = 0\n\n    with writer.as_default():                                              ❷\n\n        for e in range(epochs):\n            print(\"Training epoch {}\".format(e+1))\n            for batch in tr_ds:\n\n                model.train_on_batch(*batch)                               ❸\n\n                weights = model.get_layer(log_layer_name).get_weights()[0] ❹\n\n                tf.summary.scalar(\"mean_weights\",np.mean(np.abs(weights)), ❺\n➥ step=step)                                                              ❺\n                tf.summary.scalar(\"std_weights\", np.std(np.abs(weights)),  ❺\n➥ step=step)                                                              ❺\n\n                writer.flush()                                             ❻\n\n                step += 1\n            print('\\tDone')\n\n    print(\"Training completed\\n\")\n```", "```py\nbatch_size = 64\ntr_ds, _, _ = get_train_valid_test_datasets(\n    fashion_ds, batch_size=batch_size, flatten_images=True\n)\ntrain_model(dense_model, tr_ds, exp_log_dir + '/standard', \"log_layer\", 5)\n\ntr_ds, _, _ = get_train_valid_test_datasets(\n    fashion_ds, batch_size=batch_size, flatten_images=True\n)\ntrain_model(dense_model_bn, tr_ds, exp_log_dir + '/bn', \"log_layer_bn\", 5)\n```", "```py\ndef get_cnn_model():\n\n    conv_model = models.Sequential([                                ❶\n        layers.Conv2D(                                              ❷\n            filters=64, \n            kernel_size=(5,5), \n            strides=(1,1), \n            padding='same', \n            activation='relu', \n            input_shape=(64,64,3)\n        ),\n        layers.BatchNormalization(),                                ❸\n        layers.MaxPooling2D(pool_size=(3,3), strides=(2,2)),        ❹\n        layers.Conv2D(                                              ❺\n            filters=128, \n            kernel_size=(3,3), \n            strides=(1,1), \n            padding='same', \n            activation='relu'\n        ),\n        layers.BatchNormalization(),                                ❺\n        layers.Conv2D(                                              ❺\n            filters=256, \n            kernel_size=(3,3), \n            strides=(1,1), \n            padding='same', \n            activation='relu'\n        ),\n        layers.BatchNormalization(),                                ❺\n        layers.Conv2D(                                              ❺\n            filters=512, \n            kernel_size=(3,3), \n            strides=(1,1), \n            padding='same', \n            activation='relu'\n        ),\n        layers.BatchNormalization(),                                ❺\n        layers.AveragePooling2D(pool_size=(2,2), strides=(2,2)),    ❻\n        layers.Flatten(),                                           ❼\n        layers.Dense(512),                                          ❽\n        layers.LeakyReLU(),                                         ❽\n        layers.LayerNormalization(),                                ❽\n        layers.Dense(256),                                          ❽\n        layers.LeakyReLU(),                                         ❽\n        layers.LayerNormalization(),                                ❽\n        layers.Dense(17),                                           ❽\n        layers.Activation('softmax', dtype='float32')               ❽\n    ])\n    return conv_model\n```", "```py\ndef get_flower_datasets(image_dir, batch_size, flatten_images=False):\n\n    # Get the training dataset, shuffle it, and output a tuple of (image, \n➥ label)\n    dataset = tf.data.Dataset.list_files(\n        os.path.join(image_dir,'*.jpg'), shuffle=False\n    )\n\n    def get_image_and_label(file_path):\n\n        tokens = tf.strings.split(file_path, os.path.sep)\n        label = (\n            tf.strings.to_number(\n                tf.strings.split(\n                    tf.strings.split(tokens[-1],'.')[0], '_'\n                )[-1]\n            ) -1\n        )//80\n\n        # load the raw data from the file as a string\n        img = tf.io.read_file(file_path)\n        img = tf.image.decode_jpeg(img, channels=3)\n\n        return tf.image.resize(img, [64, 64]), label\n\n    dataset = dataset.map(get_image_and_label).shuffle(400)\n\n    # Make the validation dataset the first 10000 data\n    valid_ds = dataset.take(250).batch(batch_size)\n    # Make training dataset the rest\n    train_ds = dataset.skip(250).batch(batch_size)\n    )\n\n    return train_ds, valid_ds\n```", "```py\nbatch_size = 32\ntr_ds, v_ds = get_flower_datasets(\n    os.path.join(\n        'data', '17flowers','jpg'), batch_size=batch_size, \n➥ flatten_images=False\n)\n\ntb_callback = tf.keras.callbacks.TensorBoard(\n    log_dir=profile_log_dir, profile_batch=[10, 20]\n)\n\nconv_model.fit(\n    tr_ds, validation_data=v_ds, epochs=2, callbacks=[tb_callback]\n)\n```", "```py\ndef get_flower_datasets(image_dir, batch_size, flatten_images=False):\n\n    dataset = tf.data.Dataset.list_files(\n        os.path.join(image_dir,'*.jpg'), shuffle=False          ❶\n    )\n\n    def get_image_and_label(file_path):                         ❷\n\n        tokens = tf.strings.split(file_path, os.path.sep)       ❸\n        label = (tf.strings.to_number(\n            tf.strings.split(\n                tf.strings.split(tokens[-1],'.')[0], '_')[-1]   ❸\n            ) - 1\n        )//80\n\n        img = tf.io.read_file(file_path)                        ❹\n        img = tf.image.decode_jpeg(img, channels=3)             ❹\n\n        return tf.image.resize(img, [64, 64]), label\n\n    dataset = dataset.map(\n        get_image_and_label,\n        *num_parallel_calls=tf.data.AUTOTUNE        *             ❺\n    ).shuffle(400)\n\n    # Make the validation dataset the first 10000 data\n    valid_ds = dataset.take(250).batch(batch_size)\n    # Make training dataset the rest\n    train_ds = dataset.skip(250).batch(batch_size).prefetch(\n        tf.data.experimental.AUTOTUNE                           ❻\n )\n\n    return train_ds, valid_ds\n```", "```py\nfrom tensorflow.keras import mixed_precision\npolicy = mixed_precision.Policy('mixed_float16')\nmixed_precision.set_global_policy(policy)\n```", "```py\nconv_model = get_cnn_model()\n```", "```py\nprint(\"Input to the layers have the data type: {}\".format(\n    conv_model.get_layer(\"conv2d_1\").input.dtype)\n)\nprint(\"Variables in the layers have the data type: {}\".format(\n    conv_model.get_layer(\"conv2d_1\").trainable_variables[0].dtype)\n)\nprint(\"Output of the layers have the data type: {}\".format(\n    conv_model.get_layer(\"conv2d_1\").output.dtype)\n)\n```", "```py\nInput to the layers have the data type: <dtype: 'float16'>\nVariables in the layers have the data type: <dtype: 'float32'>\nOutput of the layers have the data type: <dtype: 'float16'>\n```", "```py\nbatch_size = 32\ntr_ds, v_ds = get_flower_datasets(\n    os.path.join('data', '17flowers','jpg'), batch_size=batch_size, \n➥ flatten_images=False\n)\n\n# This tensorboard call back does the following\n# 1\\. Log loss and accuracy\n# 2\\. Profile the model memory/time for 10  batches\ntb_callback = tf.keras.callbacks.TensorBoard(\n    log_dir=profile_log_dir, profile_batch=[10, 20]\n)\n\nconv_model.fit(\n    tr_ds, validation_data=v_ds, epochs=2, callbacks=[tb_callback]\n)\n```", "```py\ndf = pd.read_csv(\n    os.path.join('data', 'glove.6B.50d.txt'), \n    header=None, \n    index_col=0, \n    sep=None, \n    error_bad_lines=False, \n    encoding='utf-8'\n)\ndf.head()\n```", "```py\nreview_ds = tfds.load('imdb_reviews')\ntrain_review_ds = review_ds[\"train\"]\n```", "```py\ncorpus = []\nfor data in train_review_ds:      \n    txt = str(np.char.decode(data[\"text\"].numpy(), encoding='utf-8')).lower()\n    corpus.append(str(txt))\n```", "```py\nfrom collections import Counter\n\ncorpus = \" \".join(corpus)\n\ncnt = Counter(corpus.split())\nmost_common_words = [w for w,_ in cnt.most_common(5000)]\nprint(cnt.most_common(100))\n```", "```py\n[('the', 322198), ('a', 159953), ('and', 158572), ('of', 144462), ('to', \n➥ 133967), ('is', 104171), ('in', 90527), ('i', 70480), ('this', 69714), \n➥ ('that', 66292), ('it', 65505), ('/><br', 50935), ('was', 47024), \n➥ ('as', 45102), ('for', 42843), ('with', 42729), ('but', 39764), ('on', \n➥ 31619), ('movie', 30887), ('his', 29059), \n➥ ... ,\n➥ ('other', 8229), ('also', 8007), ('first', 7985), ('its', 7963), \n➥ ('time', 7945), ('do', 7904), (\"don't\", 7879), ('me', 7722), ('great', \n➥ 7714), ('people', 7676), ('could', 7594), ('make', 7590), ('any', \n➥ 7507), ('/>the', 7409), ('after', 7118), ('made', 7041), ('then', \n➥ 6945), ('bad', 6816), ('think', 6773), ('being', 6390), ('many', 6388), \n➥ ('him', 6385)]\n```", "```py\ndf_common = df.loc[df.index.isin(most_common_words)]\n```", "```py\nfrom tensorboard.plugins import projector\n\nlog_dir=os.path.join('logs', 'embeddings')\nweights = tf.Variable(df_common.values)                          ❶\n\ncheckpoint = tf.train.Checkpoint(embedding=weights)              ❷\ncheckpoint.save(os.path.join(log_dir, \"embedding.ckpt\"))         ❷\n\nwith open(os.path.join(log_dir, 'metadata.tsv'), 'w') as f:      ❸\n    for w in df_common.index:\n        f.write(w+'\\n')\n\nconfig = projector.ProjectorConfig()                             ❹\nembedding = config.embeddings.add()\nembedding.metadata_path = 'metadata.tsv'                         ❺\nprojector.visualize_embeddings(log_dir, config)\n```", "```py\n%tensorboard --logdir logs/embeddings/ --port 6007\n```", "```py\nimage_writer = tf.summary.create_file_writer(image_logdir)\n\nwith image_writer.as_default():\n    for bi, batch in enumerate(steps_image_batches):\n        tf.summary.image(\n            “batch_{}”.format(bi), \n            batch, \n            max_outputs=10, \n            step=bi\n        )\n```", "```py\nlog_dir = \"./logs \"\n\nclassif_model.compile(\n    loss=’binary_crossentropy', \n    optimizer=’adam’, \n    metrics=[tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n)\n\ntb_callback = tf.keras.callbacks.TensorBoard(\n    log_dir=log_dir, histogram_freq=1, profile_batch=0\n)\n\nclassif_model.fit(tr_ds, validation_data=v_ds, epochs=10, callbacks=[tb_callback])\n```", "```py\n writer = tf.summary.create_file_writer(log_dir)\n\n x_n_minus_1 = 1\n x_n_minus_2 = 0\n\n with writer.as_default():        \n     for i in range(100):\n         x_n = x_n_minus_1 + x_n_minus_2\n         x_n_minus_1 = x_n\n      x_n_minus_2 = x_n_minus_1\n\n      tf.summary.scalar(\"fibonacci\", x_n, step=i)\n\n      writer.flush()\n```", "```py\nlog_dir=os.path.join('logs', 'embeddings')\n\nweights = tf.Variable(df_common.values)\n   checkpoint = tf.train.Checkpoint(embedding=weights)\ncheckpoint.save(os.path.join(log_dir, \"embedding.ckpt\"))\n\nwith open(os.path.join(log_dir, 'metadata.tsv'), 'w') as f:\n    for i, w in enumerate(df_common.index):\n        f.write(w+'; '+str(i)+'\\n')\n```"]