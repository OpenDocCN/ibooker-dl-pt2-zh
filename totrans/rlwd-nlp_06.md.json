["```py\nresult = (v(\"john\") + v(\"loves\") + v(\"mary\") + v(\".\")) / \n```", "```py\nstate = init_state()\nstate = update(state, v(\"john\"))\nstate = update(state, v(\"loves\"))\nstate = update(state, v(\"mary\"))\nstate = update(state, v(\".\"))\n```", "```py\ndef rnn(words):\n    state = init_state()\n    for word in words:\n        state = update(state, word)\n    return state\n```", "```py\ndef update_simple(state, word):\n    return f(w1 * state + w2 * word + b)\n```", "```py\ndef update_simple_linear(state, word):\n    return w1 * state + w2 * word + b\n```", "```py\nw1 * w2 * x1 + w2 * x2 + w1 * b + b\n```", "```py\ndef rnn(sentence):\n    word1, word2, word3, word4, word5, word6 = sentence\n    state = init_state()\n\n    state = update(state, word1)\n    state = update(state, word2)\n    state = update(state, word3)\n    state = update(state, word4)\n    state = update(state, word5)\n    state = update(state, word6)\n\n    return state\n```", "```py\ndef rnn_simple(sentence):\n    word1, word2, word3, word4, word5, word6 = sentence\n    state = init_state()\n\n    state = f(w1 * f(w1 * f(w1 * f(w1 * f(w1 * f(w1 * state + w2 * word1 + b) + w2 * word2 + b) + w2 * word3 + b) + w2 * word4 + b) + w2 * word5 + b) + w2 * word6 + b)\n    return state\n```", "```py\ndef is_grammatical(sentence):\n    word1, word2, word3, word4, word5, word6 = sentence\n    state = init_state()\n\n    state = process_main_verb(w1 *\n        process_adverb(w1 *\n            process_relative_clause_verb(w1 *\n                process_relative_clause_subject(w1 *\n                    process_main_subject(w1 *\n                        process_article(w1 * state + w2 * word1 + b) +\n                    w2 * word2 + b) +\n                w2 * word3 + b) +\n            w2 * word4 + b) + \n        w2 * word5 + b) + \n    w2 * word6 + b)\n\n    return state\n```", "```py\ndef update_lstm(state, word):\n    cell_state, hidden_state = state\n\n    cell_state *= forget(hidden_state, word)\n    cell_state += add(hidden_state, word)\n\n    hidden_state = update_hidden(hidden_state, cell_state, word)\n\n    return (cell_state, hidden_state)\n```", "```py\ndef update_gru(state, word): \n    new_state = update_hidden(state, word)\n\n    switch = get_switch(state, word)\n\n    state = swtich * new_state + (1 - switch) * state\n\n    return state\n```", "```py\nfrom itertools import chain\nfrom typing import Dict\n\nimport numpy as np\nimport torch\nimport torch.optim as optim\nfrom allennlp.data.data_loaders import MultiProcessDataLoader\nfrom allennlp.data.samplers import BucketBatchSampler\nfrom allennlp.data.vocabulary import Vocabulary\nfrom allennlp.models import Model\nfrom allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\nfrom allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\nfrom allennlp.modules.token_embedders import Embedding\nfrom allennlp.nn.util import get_text_field_mask\nfrom allennlp.training.metrics import CategoricalAccuracy, F1Measure\nfrom allennlp.training.trainer import GradientDescentTrainer\nfrom allennlp_models.classification.dataset_readers.stanford_sentiment_tree_bank import \\\n    StanfordSentimentTreeBankDatasetReader\n```", "```py\nInstance({'tokens': TextField(tokens),\n          'label': LabelField(sentiment)})\n```", "```py\nreader = StanfordSentimentTreeBankDatasetReader()\n\ntrain_dataset = reader.read('path/to/sst/dataset/train.txt')\ndev_dataset = reader.read('path/to/sst/dataset/dev.txt')\n\nfor inst in train_dataset + dev_dataset:\n    print(inst)\n```", "```py\nreader = StanfordSentimentTreeBankDatasetReader()\n\nsampler = BucketBatchSampler(batch_size=32, sorting_keys=[\"tokens\"])\ntrain_data_loader = MultiProcessDataLoader(\n    reader, train_path, batch_sampler=sampler)\ndev_data_loader = MultiProcessDataLoader(\n    reader, dev_path, batch_sampler=sampler)\n```", "```py\nvocab = Vocabulary.from_instances(chain(train_data_loader.iter_instances(), \n                                        dev_data_loader.iter_instances()),\n                                  min_count={'tokens': 3})\n```", "```py\ntrain_data_loader.index_with(vocab)\ndev_data_loader.index_with(vocab)\n```", "```py\ntoken_embedding = Embedding(\n    num_embeddings=vocab.get_vocab_size('tokens'),\n    embedding_dim=EMBEDDING_DIM)\n```", "```py\nencoder = PytorchSeq2VecWrapper(\n    torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\n```", "```py\n@Model.register(\"lstm_classifier\")\nclass LstmClassifier(Model):                                   ❶\n    def __init__(self,\n                 embedder: TextFieldEmbedder,\n                 encoder: Seq2VecEncoder,\n                 vocab: Vocabulary,\n                 positive_label: str = '4') -> None:\n        super().__init__(vocab)\n        self.embedder = embedder\n        self.encoder = encoder\n\n        self.linear = torch.nn.Linear(                         ❷\n            in_features=encoder.get_output_dim(),\n            out_features=vocab.get_vocab_size('labels'))\n\n        positive_index = vocab.get_token_index(\n            positive_label, namespace='labels')\n        self.accuracy = CategoricalAccuracy()\n        self.f1_measure = F1Measure(positive_index)            ❸\n\n        self.loss_function = torch.nn.CrossEntropyLoss()       ❹\n\n    def forward(self,                                          ❺\n                tokens: Dict[str, torch.Tensor],\n                label: torch.Tensor = None) -> torch.Tensor:\n        mask = get_text_field_mask(tokens)\n\n        embeddings = self.embedder(tokens)\n        encoder_out = self.encoder(embeddings, mask)\n        logits = self.linear(encoder_out)\n\n        output = {\"logits\": logits}                            ❻\n        if label is not None:\n            self.accuracy(logits, label)\n            self.f1_measure(logits, label)\n            output[\"loss\"] = self.loss_function(logits, label)\n\n        return output\n\n    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n        return {'accuracy': self.accuracy.get_metric(reset),   ❼\n                **self.f1_measure.get_metric(reset)}\n```", "```py\nEMBEDDING_DIM = 128\nHIDDEN_DIM = 128\n\nreader = StanfordSentimentTreeBankDatasetReader()\n\ntrain_path = 'path/to/sst/dataset/train.txt'\ndev_path = 'path/to/sst/dataset/dev.txt'\n\nsampler = BucketBatchSampler(batch_size=32, sorting_keys=[\"tokens\"])\ntrain_data_loader = MultiProcessDataLoader(                            ❶\n    reader, train_path, batch_sampler=sampler)\ndev_data_loader = MultiProcessDataLoader(\n    reader, dev_path, batch_sampler=sampler)\n\nvocab = Vocabulary.from_instances(chain(train_data_loader.iter_instances(), \n                                        dev_data_loader.iter_instances()),\n                                  min_count={'tokens': 3})\n\ntrain_data_loader.index_with(vocab)\ndev_data_loader.index_with(vocab)\n\ntoken_embedding = Embedding(\n    num_embeddings=vocab.get_vocab_size('tokens'),\n    embedding_dim=EMBEDDING_DIM)\n\nword_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n\nencoder = PytorchSeq2VecWrapper(\n    torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\n\nmodel = LstmClassifier(word_embeddings, encoder, vocab)               ❷\n\noptimizer = optim.Adam(model.parameters())                            ❸\n\ntrainer = GradientDescentTrainer(                                     ❹\n    model=model,\n    optimizer=optimizer,\n    data_loader=train_data_loader,\n    validation_data_loader=dev_data_loader,\n    patience=10,\n    num_epochs=20,\n    cuda_device=-1)\n\ntrainer.train()\n```", "```py\n\"dataset_reader\": {\n    \"type\": \"sst_tokens\"\n  },\n  \"train_data_path\": \"https:/./s3.amazonaws.com/realworldnlpbook/data/stanfordSentimentTreebank/trees/train.txt\",\n  \"validation_data_path\": \"https:/./s3.amazonaws.com/realworldnlpbook/data/stanfordSentimentTreebank/trees/dev.txt\"\n```", "```py\n@DatasetReader.register(\"sst_tokens\")\nclass StanfordSentimentTreeBankDatasetReader(DatasetReader): \n    ...\n```", "```py\n\"model\": {\n    \"type\": \"lstm_classifier\",\n\n    \"embedder\": {\n      \"token_embedders\": {\n        \"tokens\": {\n          \"type\": \"embedding\",\n          \"embedding_dim\": embedding_dim\n        }\n      }\n    },\n\n    \"encoder\": {\n      \"type\": \"lstm\",\n      \"input_size\": embedding_dim,\n      \"hidden_size\": hidden_dim\n    }\n}\n```", "```py\n@Model.register(\"lstm_classifier\")\nclass LstmClassifier(Model):\n    def __init__(self,\n                 embedder: TextFieldEmbedder,\n                 encoder: Seq2VecEncoder,\n                 vocab: Vocabulary,\n                 positive_label: str = '4') -> None:\n```", "```py\n  \"data_loader\": {\n    \"batch_sampler\": {\n      \"type\": \"bucket\",\n      \"sorting_keys\": [\"tokens\"],\n      \"padding_noise\": 0.1,\n      \"batch_size\" : 32\n    }\n  },\n  \"trainer\": {\n    \"optimizer\": \"adam\",\n    \"num_epochs\": 20,\n    \"patience\": 10\n  }\n```", "```py\nallennlp train examples/sentiment/sst_classifier.jsonnet \\\n    --serialization-dir sst-model \\\n    --include-package examples.sentiment.sst_classifier\n```", "```py\n$ allennlp serve \\ \n    --archive-path sst-model/model.tar.gz \\\n    --include-package examples.sentiment.sst_classifier \\\n    --predictor sentence_classifier_predictor \\\n    --field-name sentence\n```", "```py\npor De entre os designers, ele escolheu um jovem ilustrador e deu-lhe a tarefa.\npor A apresentação me fez chorar.\ntur Bunu denememize gerek var mı?\ntur O korkutucu bir parçaydı.\nber Tebḍamt aɣrum-nni ɣef sin, naɣ?\nber Ad teddud ad twalid taqbuct n umaḍal n tkurt n uḍar deg Brizil?\neng Tom works at Harvard.\neng They fixed the flat tire by themselves.\nhun Az arca hirtelen elpirult.\nhun Miért aggodalmaskodsz? Hiszen még csak egy óra van!\nepo Sidiĝu sur la benko.\nepo Tiu ĉi kutime funkcias.\nfra Vu d'avion, cette île a l'air très belle.\nfra Nous boirons à ta santé.\ndeu Das Abnehmen fällt ihm schwer.\ndeu Tom war etwas besorgt um Maria.\nita Sono rimasto a casa per potermi riposare.\nita Le due più grandi invenzioni dell'uomo sono il letto e la bomba atomica: il primo ti tiene lontano dalle noie, la seconda le elimina.\nspa He visto la película.\nspa Has hecho los deberes.\n```", "```py\nfrom typing import Dict\n\nimport numpy as np\nimport torch\nimport torch.optim as optim\nfrom allennlp.common.file_utils import cached_path\nfrom allennlp.data.data_loaders import MultiProcessDataLoader\nfrom allennlp.data.dataset_readers import DatasetReader\nfrom allennlp.data.fields import LabelField, TextField\nfrom allennlp.data.instance import Instance\nfrom allennlp.data.samplers import BucketBatchSampler\nfrom allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\nfrom allennlp.data.tokenizers.character_tokenizer import CharacterTokenizer\nfrom allennlp.data.vocabulary import Vocabulary\nfrom allennlp.modules.seq2vec_encoders import PytorchSeq2VecWrapper\nfrom allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\nfrom allennlp.modules.token_embedders import Embedding\nfrom allennlp.training import GradientDescentTrainer\nfrom overrides import overrides\n\nfrom examples.sentiment.sst_classifier import LstmClassifier\n```", "```py\nclass TatoebaSentenceReader(DatasetReader):                    ❶\n    def __init__(self,\n                 token_indexers: Dict[str, TokenIndexer]=None):\n        super().__init__()\n        self.tokenizer = CharacterTokenizer()                  ❷\n        self.token_indexers = token_indexers or {'tokens': SingleIdTokenIndexer()}\n\n    @overrides\n    def text_to_instance(self, tokens, label=None):            ❸\n        fields = {}\n\n        fields['tokens'] = TextField(tokens, self.token_indexers)\n        if label:\n            fields['label'] = LabelField(label)\n\n        return Instance(fields)\n\n    @overrides\n    def _read(self, file_path: str):\n        file_path = cached_path(file_path)                     ❹\n        with open(file_path, \"r\") as text_file:\n            for line in text_file:\n                lang_id, sent = line.rstrip().split('\\t')\n\n                tokens = self.tokenizer.tokenize(sent)\n\n                yield self.text_to_instance(tokens, lang_id)   ❺\n```", "```py\nEMBEDDING_DIM = 16\nHIDDEN_DIM = 16\n\nreader = TatoebaSentenceReader()\ntrain_path = 'https:/./s3.amazonaws.com/realworldnlpbook/data/tatoeba/sentences.top10langs.train.tsv'\ndev_path = 'https:/./s3.amazonaws.com/realworldnlpbook/data/tatoeba/sentences.top10langs.dev.tsv'\n\nsampler = BucketBatchSampler(batch_size=32, sorting_keys=[\"tokens\"])\ntrain_data_loader = MultiProcessDataLoader(\n    reader, train_path, batch_sampler=sampler)\ndev_data_loader = MultiProcessDataLoader(\n    reader, dev_path, batch_sampler=sampler)\n\nvocab = Vocabulary.from_instances(train_data_loader.iter_instances(),\n                                  min_count={'tokens': 3})\ntrain_data_loader.index_with(vocab)\ndev_data_loader.index_with(vocab)\n\ntoken_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n                            embedding_dim=EMBEDDING_DIM)\nword_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\nencoder = PytorchSeq2VecWrapper(\n    torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\n\nmodel = LstmClassifier(word_embeddings,\n                       encoder,\n                       vocab,\n                       positive_label='eng')\n\ntrain_dataset.index_with(vocab)\ndev_dataset.index_with(vocab)\n\noptimizer = optim.Adam(model.parameters())\n\ntrainer = GradientDescentTrainer(\n    model=model,\n    optimizer=optimizer,\n    data_loader=train_data_loader,\n    validation_data_loader=dev_data_loader,\n    patience=10,\n    num_epochs=20,\n    cuda_device=-1)\n\ntrainer.train()\n```", "```py\naccuracy: 0.9461, precision: 0.9481, recall: 0.9490, f1_measure: 0.9485, loss: 0.1560\n```", "```py\ndef classify(text: str, model: LstmClassifier):\n    tokenizer = CharacterTokenizer()\n    token_indexers = {'tokens': SingleIdTokenIndexer()}\n\n    tokens = tokenizer.tokenize(text)\n    instance = Instance({'tokens': TextField(tokens, token_indexers)})\n    logits = model.forward_on_instance(instance)['logits']\n    label_id = np.argmax(logits)\n    label = model.vocab.get_token_from_index(label_id, 'labels')\n\n    print('text: {}, label: {}'.format(text, label))\n```", "```py\ntext: Take your raincoat in case it rains., label: fra\ntext: Tu me recuerdas a mi padre., label: spa\ntext: Wie organisierst du das Essen am Mittag?, label: deu\ntext: Il est des cas où cette règle ne s'applique pas., label: fra\ntext: Estou fazendo um passeio em um parque., label: por\ntext: Ve, postmorgaŭ jam estas la limdato., label: epo\ntext: Credevo che sarebbe venuto., label: ita\ntext: Nem tudja, hogy én egy macska vagyok., label: hun\ntext: Nella ur nli qrib acemma deg tenwalt., label: ber\ntext: Kurşun kalemin yok, deǧil mi?, label: tur\n```"]