["```py\nimport time\nfrom gensim.models import FastText, KeyedVectors\n\nstart=time.time()\nFastText_embedding = KeyedVectors.load_word2vec_format(\"../input/jigsaw/wiki.en.vec\")   ❶\nend = time.time()\nprint(\"Loading the embedding took %d seconds\"%(end-start))\n```", "```py\ndef load_data(path):\n    data, sentiments = [], []\n    for folder, sentiment in (('neg', 0), ('pos', 1)):\n        folder = os.path.join(path, folder)\n        for name in os.listdir(folder):                  ❶\n            with open(os.path.join(folder, name), 'r') as reader:\n                  text = reader.read()\n            text = tokenize(text)                        ❷\n            text = stop_word_removal(text)\n            text = reg_expressions(text)\n            data.append(text)\n            sentiments.append(sentiment)                ❸\n    data_np = np.array(data)                            ❹\n    data, sentiments = unison_shuffle_data(data_np, sentiments)\n\n    return data, sentiments\n\ntrain_path = os.path.join('aclImdb', 'train')           ❺\nraw_data, raw_header = load_data(train_path)\n```", "```py\ndef handle_out_of_vocab(embedding,in_txt):\n    out = None\n    for word in in_txt:                                  ❶\n        try:\n            tmp = embedding[word]                        ❷\n            tmp = tmp.reshape(1,len(tmp))\n\n            if out is None:                              ❸\n                out = tmp\n            else:\n                out = np.concatenate((out,tmp),axis=0)   ❹\n        except:                                          ❺\n            pass\n\n    return out\n```", "```py\ndef assemble_embedding_vectors(data):\n    out = None\n    for item in data:                                         ❶\n        tmp = handle_out_of_vocab(FastText_embedding,item)    ❷\n        if tmp is not None:\n            dim = tmp.shape[1]\n            if out is not None:\n                vec = np.mean(tmp,axis=0)                     ❸\n                vec = vec.reshape((1,dim))\n                out = np.concatenate((out,vec),axis=0)        ❹\n            else:\n                out = np.mean(tmp,axis=0).reshape((1,dim))                                            \n        else:\n            pass                                              ❺\n\n    return out\n```", "```py\nEmbeddingVectors = assemble_embedding_vectors(data_train)\n```", "```py\npip install git+https:/ /github.com/epfml/sent2vec\n```", "```py\nimport time\nimport sent2vec\n\nmodel = sent2vec.Sent2vecModel()\nstart=time.time()\nmodel.load_model('../input/sent2vec/wiki_unigrams.bin')     ❶\nend = time.time()\nprint(\"Loading the sent2vec embedding took %d seconds\"%(end-start))\n```", "```py\ndef assemble_embedding_vectors(data):\n    out = None\n    for item in data:                                   ❶\n        vec = model.embed_sentence(\" \".join(item))      ❷\n        if vec is not None:                             ❸\n            if out is not None:\n                out = np.concatenate((out,vec),axis=0)\n            else:\n                out = vec\n        else:\n            pass\n\n    return out\n```", "```py\nEmbeddingVectors = assemble_embedding_vectors(data_train)\n```", "```py\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Dropout\n\ninput_shape = (len(train_x[0]),)\nsent2vec_vectors = Input(shape=input_shape)                  ❶\ndense = Dense(512, activation='relu')(sent2vec_vectors)      ❷\ndense = Dropout(0.3)(dense)                                  ❸\noutput = Dense(1, activation='sigmoid')(dense)               ❹\nmodel = Model(inputs=sent2vec_vectors, outputs=output)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \nhistory = model.fit(train_x, train_y, validation_data=(test_x, test_y), batch_size=32, nb_epoch=10, shuffle=True)\n```", "```py\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Dropout\nfrom keras.layers.merge import concatenate\n\ninput1_shape = (len(train_x[0]),)\ninput2_shape = (len(train_x2[0]),)\nsent2vec_vectors1 = Input(shape=input1_shape)\nsent2vec_vectors2 = Input(shape=input2_shape)\ncombined = concatenate([sent2vec_vectors1,sent2vec_vectors2])             ❶\ndense1 = Dense(512, activation='relu')(combined)                          ❷\ndense1 = Dropout(0.3)(dense1)\noutput1 = Dense(1, activation='sigmoid',name='classification1')(dense1)   ❸\noutput2 = Dense(1, activation='sigmoid',name='classification2')(dense1)\nmodel = Model(inputs=[sent2vec_vectors1,sent2vec_vectors2], outputs=[output1,output2])\n```", "```py\nmodel.compile(loss={'classification1': 'binary_crossentropy',              ❶\n                    'classification2': 'binary_crossentropy'},\n              optimizer='adam', metrics=['accuracy'])\nhistory = model.fit([train_x,train_x2],[train_y,train_y2],\n                    validation_data=([test_x,test_x2],[test_y,test_y2]),   ❷\n        batch_size=8,nb_epoch=10, shuffle=True)\n```", "```py\ndef parse_MDSD(data):\n    out_lst = []\n    for i in range(len(data)):\n    txt = \"\"\n    if(data[i]==\"<review_text>\\n\"):              ❶\n                    j=i\n            while(data[j]!=\"</review_text>\\n\"):\n                txt = txt+data[j]\n                j = j+1\n            text = tokenize(txt)\n            text = stop_word_removal(text)\n            text = remove_reg_expressions(text)\n            out_lst.append(text)\n\n    return out_lst\n\ninput_file_path = \\\n\"../input/multi-domain-sentiment-dataset-books-and-dvds/books.negative.review\"\nwith open (input_file_path, \"r\", encoding=\"latin1\") as myfile:   \n    data=myfile.readlines()\nneg_books = parse_MDSD(data)                     ❷\n\ninput_file_path = \\\n\"../input/multi-domain-sentiment-dataset-books-and-dvds/books.positive.review\"\nwith open (input_file_path, \"r\", encoding=\"latin1\") as myfile:\n    data=myfile.readlines()\npos_books = parse_MDSD(data)\n\nheader = [0]*len(neg_books)                      ❸\nheader.extend([1]*len(pos_books))\nneg_books.extend(pos_books)                      ❹\nMDSD_data = np.array(neg_books)\ndata, sentiments = unison_shuffle_data(np.array(MDSD_data), header)\nEmbeddingVectors = assemble_embedding_vectors(data)\n```", "```py\nprint(model.evaluate(x=EmbeddingVectors,y=sentiments))\n```", "```py\nencoding_dim = 30\n\ninput_shape = (len(train_x[0]),)                                  ❶\nsent2vec_vectors = Input(shape=input_shape)\nencoder = Dense(encoding_dim, activation='relu')(sent2vec_vectors)\ndropout = Dropout(0.1)(encoder)                                   ❷\ndecoder = Dense(encoding_dim, activation='relu')(dropout)\ndropout = Dropout(0.1)(decoder)\noutput = Dense(len(train_x[0]))(dropout)                          ❸\nautoencoder = Model(inputs=sent2vec_vectors, outputs=output)\n```", "```py\nautoencoder.compile(optimizer='adam',loss='mse',metrics=[\"mse\",\"mae\"])\nautoencoder.fit(train_x,train_x,validation_data=(test_x, test_x),\nbatch_size=32,nb_epoch=50, shuffle=True)\n```", "```py\nEmbeddingVectorsScaledProjected = autoencoder.predict(EmbeddingVectors)\nprint(model.evaluate(x=EmbeddingVectorsScaledProjected,y=sentiments))\n```"]