["```py\nimport os\nimport requests\nimport tarfile\n\nimport shutil\n\nif not os.path.exists(os.path.join('data', 'csv', 'forestfires.csv')):    ❶\n    url = \"http:/ /archive.ics.uci.edu/ml/machine-learning-databases/forest-\n➥ fires/forestfires.csv\"\n    r = requests.get(url)                                                 ❷\n\n    if not os.path.exists(os.path.join('data', 'csv')):                   ❸\n        os.makedirs(os.path.join('data', 'csv'))                          ❸\n\n    with open(os.path.join('data', 'csv', 'forestfires.csv'), 'wb') as f: ❸\n        f.write(r.content)                                                ❸\nelse:\n    print(\"The forestfires.csv file already exists.\")\n\nif not os.path.exists(os.path.join('data', 'forestfires.names')):         ❹\n\n    url = \"http:/ /archive.ics.uci.edu/ml/machine-learning-databases/forest-\n➥ fires/forestfires.names\"\n    r = requests.get(url)                                                 ❹\n\n    if not os.path.exists('data'):                                        ❺\n        os.makedirs('data')                                               ❺\n\n    with open(os.path.join('data', 'forestfires.names'), 'wb') as f:      ❺\n        f.write(r.content)                                                ❺\n\nelse:\n    print(\"The forestfires.names file already exists.\")\n```", "```py\nimport pandas as pd\n\ndf = pd.read_csv(\n    os.path.join('data', 'csv', 'forestfires.csv'), index_col=None, \n➥ header=0\n)\ntrain_df = df.sample(frac=0.95, random_state=random_seed)\ntest_df = df.loc[~df.index.isin(train_df.index), :]\n\ntrain_path = os.path.join('data','csv','train')\nos.makedirs(train_path, exist_ok=True)\ntest_path = os.path.join('data','csv','test')\nos.makedirs(test_path, exist_ok=True)\n\ntrain_df.to_csv(\n    os.path.join(train_path, 'forestfires.csv'), index=False, header=True\n)\ntest_df.to_csv(\n    os.path.join(test_path, 'forestfires.csv'), index=False, header=True\n)\n```", "```py\n_pipeline_root = os.path.join(\n    os.getcwd(), 'pipeline', 'examples', 'forest_fires_pipeline'\n)\n```", "```py\nabsl.logging.set_verbosity(absl.logging.INFO)\n```", "```py\nfrom tfx.orchestration.experimental.interactive.interactive_context import \n➥ InteractiveContext\n\ncontext = InteractiveContext(\n    pipeline_name = \"forest_fires\", pipeline_root=_pipeline_root\n)\n```", "```py\nfrom tfx.components import CsvExampleGen\n\nexample_gen = CsvExampleGen(input_base=os.path.join('data', 'csv', 'train'))\n```", "```py\ncontext.run(example_gen)\n```", "```py\n\"split_config\": { \n    \"splits\": [ \n        { \"hash_buckets\": 2, \"name\": \"train\" }, \n        { \"hash_buckets\": 1, \"name\": \"eval\" } \n    ] \n} \n```", "```py\nartifact = example_gen.outputs['examples'].get()[0]\n\nprint(\"Artifact split names: {}\".format(artifact.split_names))\nprint(\"Artifact URI: {}\".format(artifact.uri)\n```", "```py\nArtifact split names: [\"train\", \"eval\"]\nArtifact URI: <path to project>/Ch15-TFX-for-MLOps-in-\n➥ TF2/tfx/pipeline/examples/forest_fires_pipeline/CsvExampleGen/examples/1\n```", "```py\ntrain_uri = os.path.join(\n    example_gen.outputs['examples'].get()[0].uri, 'Split-train'       ❶\n) \n\ntfrecord_filenames = [\n    os.path.join(train_uri, name) for name in os.listdir(train_uri)   ❷\n]\n\ndataset = tf.data.TFRecordDataset(\n    tfrecord_filenames, compression_type=\"GZIP\"\n)                                                                     ❸\n\nfor tfrecord in dataset.take(2):                                      ❹\n  serialized_example = tfrecord.numpy()                               ❺\n  example = tf.train.Example()                                        ❻\n  example.ParseFromString(serialized_example)                         ❼\n  print(example)                                                      ❽\n```", "```py\nfeatures {\n  feature {\n    key: \"DC\"\n    value {\n      float_list {\n        value: 605.7999877929688\n      }\n    }\n  }\n  ...\n  feature {\n    key: \"RH\"\n    value {\n      int64_list {\n        value: 43\n      }\n    }\n  }\n  feature {\n    key: \"X\"\n    value {\n      int64_list {\n        value: 5\n      }\n    }\n  }\n  ...\n  feature {\n    key: \"area\"\n    value {\n      float_list {\n        value: 2.0\n      }\n    }\n  }\n  feature {\n    key: \"day\"\n    value {\n      bytes_list {\n        value: \"tue\"\n      }\n    }\n  }\n  ...\n}\n\n...\n```", "```py\nfrom tfx.components import StatisticsGen\n\nstatistics_gen = StatisticsGen(\n    examples=example_gen.outputs['examples'])\n\ncontext.run(statistics_gen)\n```", "```py\ncontext.show(statistics_gen.outputs['statistics'])\n```", "```py\nfrom tfx.components import SchemaGen\n\nschema_gen = SchemaGen(\n    statistics=statistics_gen.outputs[‘statistics’],\n    infer_feature_shape=False)\n\ncontext.run(schema_gen)\n```", "```py\ncontext.show(schema_gen.outputs['schema'])\n```", "```py\n%%writefile forest_fires_constants.py                              ❶\n\nVOCAB_FEATURE_KEYS = ['day','month']                               ❷\n\nMAX_CATEGORICAL_FEATURE_VALUES = [7, 12]                           ❸\n\nDENSE_FLOAT_FEATURE_KEYS = [\n    'DC', 'DMC', 'FFMC', 'ISI', 'rain', 'temp', 'wind', 'X', 'Y'   ❹\n]\n\nBUCKET_FEATURE_KEYS = ['RH']                                       ❺\n\nBUCKET_FEATURE_BOUNDARIES = [(33, 66)]                             ❻\n\nLABEL_KEY = 'area'                                                 ❼\n\ndef transformed_name(key):                                         ❽\n\n    return key + '_xf'\n```", "```py\n%%writefile forest_fires_transform.py                                      ❶\n\nimport tensorflow as tf\nimport tensorflow_transform as tft\n\nimport forest_fires_constants                                              ❷\n\n_DENSE_FLOAT_FEATURE_KEYS = forest_fires_constants.DENSE_FLOAT_FEATURE_KEYS❸\n_VOCAB_FEATURE_KEYS = forest_fires_constants.VOCAB_FEATURE_KEYS            ❸\n_BUCKET_FEATURE_KEYS = forest_fires_constants.BUCKET_FEATURE_KEYS          ❸\n_BUCKET_FEATURE_BOUNDARIES = \n➥ forest_fires_constants.BUCKET_FEATURE_BOUNDARIES                        ❸\n_LABEL_KEY = forest_fires_constants.LABEL_KEY                              ❸\n_transformed_name = forest_fires_constants.transformed_name                ❸\n\ndef preprocessing_fn(inputs):                                              ❹\n\n  outputs = {}\n\n  for key in _DENSE_FLOAT_FEATURE_KEYS:                                    ❺\n    outputs[_transformed_name(key)] = tft.scale_to_z_score(                ❻\n        sparse_to_dense(inputs[key])                                       ❼\n    )\n\n  for key in _VOCAB_FEATURE_KEYS:\n    outputs[_transformed_name(key)] = tft.compute_and_apply_vocabulary(    ❽\n        sparse_to_dense(inputs[key]),\n        num_oov_buckets=1)\n\n  for key, boundary in zip(_BUCKET_FEATURE_KEYS,                           ❾\n➥ _BUCKET_FEATURE_BOUNDARIES):                                            ❾\n    outputs[_transformed_name(key)] = tft.apply_buckets(                   ❾\n        sparse_to_dense(inputs[key]), bucket_boundaries=[boundary]         ❾\n    )                                                                      ❾\n\n  outputs[_transformed_name(_LABEL_KEY)] = \n➥ sparse_to_dense(inputs[_LABEL_KEY])                                     ❿\n\n  return outputs\n\ndef sparse_to_dense(x):                                                    ⓫\n\n    return tf.squeeze(\n        tf.sparse.to_dense(\n            tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1])\n        ),\n        axis=1\n    )\n```", "```py\nfrom tfx.components import Transform\n\ntransform = Transform(\n    examples=example_gen.outputs['examples'],\n    schema=schema_gen.outputs['schema'],\n    module_file=os.path.abspath('forest_fires_transform.py'),\n)\n\ncontext.run(transform)\n```", "```py\nimport forest_fires_constants\n\n_DENSE_FLOAT_FEATURE_KEYS = forest_fires_constants.DENSE_FLOAT_FEATURE_KEYS\n_VOCAB_FEATURE_KEYS = forest_fires_constants.VOCAB_FEATURE_KEYS\n_BUCKET_FEATURE_KEYS = forest_fires_constants.BUCKET_FEATURE_KEYS\n_LABEL_KEY = forest_fires_constants.LABEL_KEY\n\n# Get the URI of the output artifact representing the training examples, which is a directory\ntrain_uri = os.path.join(\n    transform.outputs['transformed_examples'].get()[0].uri, 'Split-train'\n)\ntfrecord_filenames = [\n    os.path.join(train_uri, name) for name in os.listdir(train_uri)        ❶\n]\n\ndataset = tf.data.TFRecordDataset(\n    tfrecord_filenames, compression_type=\"GZIP\"\n)                                                                          ❷\n\nexample_records = []                                                       ❸\nfloat_features = [\n    _transformed_name(f) for f in _DENSE_FLOAT_FEATURE_KEYS + [_LABEL_KEY] ❹\n]\nint_features = [\n    _transformed_name(f) for f in _BUCKET_FEATURE_KEYS + \n➥ _VOCAB_FEATURE_KEYS                                                     ❹\n]\nfor tfrecord in dataset.take(5):                                           ❺\n  serialized_example = tfrecord.numpy()                                    ❻\n  example = tf.train.Example()                                             ❻\n  example.ParseFromString(serialized_example)                              ❻\n  record = [\n    example.features.feature[f].int64_list.value for f in int_features     ❼\n  ] + [\n    example.features.feature[f].float_list.value for f in float_features   ❼\n  ]\n  example_records.append(record)                                           ❽\n  print(example)\n  print(\"=\"*50)\n```", "```py\nfeatures {\n  feature {\n    key: \"DC_xf\"\n    value {\n      float_list {\n        value: 0.4196213185787201\n      }\n    }\n  }\n\n  ...\n\n  feature {\n    key: \"RH_xf\"\n    value {\n      int64_list {\n        value: 0\n      }\n    }\n  }\n\n  ...\n\n  feature {\n    key: \"area_xf\"\n    value {\n      float_list {\n        value: 2.7699999809265137\n      }\n    }\n  }\n\n  ...\n}\n```", "```py\ndef _build_keras_model() -> tf.keras.Model:                     ❶\n\n  real_valued_columns = [                                       ❷\n      tf.feature_column.numeric_column(key=key, shape=(1,))\n      for key in _transformed_names(_DENSE_FLOAT_FEATURE_KEYS)\n  ]\n\n  categorical_columns = [                                       ❸\n      tf.feature_column.indicator_column(\n          tf.feature_column.categorical_column_with_identity(\n              key, \n              num_buckets=len(boundaries)+1\n          )\n      ) for key, boundaries in zip(\n          _transformed_names(_BUCKET_FEATURE_KEYS),\n          _BUCKET_FEATURE_BOUNDARIES\n      )\n  ]\n\n  categorical_columns += [                                      ❹\n      tf.feature_column.indicator_column(\n          tf.feature_column.categorical_column_with_identity( \n              key,\n              num_buckets=num_buckets,\n              default_value=num_buckets-1\n          )\n      ) for key, num_buckets in zip(\n              _transformed_names(_VOCAB_FEATURE_KEYS),\n              _MAX_CATEGORICAL_FEATURE_VALUES\n      )      \n  ]\n\n  model = _dnn_regressor(                                       ❺\n      columns=real_valued_columns+categorical_columns,          ❻\n      dnn_hidden_units=[128, 64]                                ❼\n  )\n\n  return model\n```", "```py\na = tf.feature_column.numeric_column(\"a\")\nx = tf.keras.layers.DenseFeatures(a)({'a': [0.5, 0.6]})\nprint(x)\n```", "```py\ntf.Tensor(\n[[0.5]\n [0.6]], shape=(2, 1), dtype=float32)\n```", "```py\nb = tf.feature_column.indicator_column(\n    tf.feature_column.categorical_column_with_identity('b', num_buckets=10)\n)\ny = tf.keras.layers.DenseFeatures(b)({'b': [5, 2]})\nprint(y)\n```", "```py\ntf.Tensor(\n[[0\\. 0\\. 0\\. 0\\. 0\\. 1\\. 0\\. 0\\. 0\\. 0.]\n [0\\. 0\\. 1\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0.]], shape=(2, 10), dtype=float32)\n```", "```py\ndef _dnn_regressor(columns, dnn_hidden_units):                            ❶\n\n  input_layers = {\n      colname: tf.keras.layers.Input(\n          name=colname, shape=(), dtype=tf.float32\n      )                                                                   ❷\n      for colname in _transformed_names(_DENSE_FLOAT_FEATURE_KEYS)\n  }\n  input_layers.update({\n      colname: tf.keras.layers.Input(\n          name=colname, shape=(), dtype='int32'\n      )                                                                   ❸\n      for colname in _transformed_names(_VOCAB_FEATURE_KEYS)\n  })\n  input_layers.update({\n      colname: tf.keras.layers.Input(\n          name=colname, shape=(), dtype='int32'\n      )                                                                   ❹\n      for colname in _transformed_names(_BUCKET_FEATURE_KEYS)\n  })  \n\n  output = tf.keras.layers.DenseFeatures(columns)(input_layers)           ❺\n  for numnodes in dnn_hidden_units:\n    output = tf.keras.layers.Dense(numnodes, activation='tanh')(output)   ❻\n\n  output = tf.keras.layers.Dense(1)(output)                               ❼\n\n  model = tf.keras.Model(input_layers, output)                            ❽\n  model.compile(\n      loss='mean_squared_error',                                          ❾\n      optimizer=tf.keras.optimizers.Adam(lr=0.001)\n  )\n  model.summary(print_fn=absl.logging.info)                               ❿\n\n  return model\n```", "```py\ndef _build_keras_model() -> tf.keras.Model:\n```", "```py\ndef run_fn(fn_args: tfx.components.FnArgs):\n```", "```py\nfrom typing import List, Text                                ❶\n\ndef _input_fn(file_pattern: List[Text],                      ❷\n              data_accessor: tfx.components.DataAccessor,    ❸\n              tf_transform_output: tft.TFTransformOutput,    ❹\n              batch_size: int = 200) -> tf.data.Dataset:     ❺\n\n  return data_accessor.tf_dataset_factory(\n      file_pattern,\n      tfxio.TensorFlowDatasetOptions(\n          batch_size=batch_size, label_key=_transformed_name(_LABEL_KEY)),\n      tf_transform_output.transformed_metadata.schema)\n```", "```py\ndef run_fn(fn_args: tfx.components.FnArgs):                        ❶\n\n  absl.logging.info(\"=\"*50)\n  absl.logging.info(\"Printing the tfx.components.FnArgs object\")   ❷\n  absl.logging.info(fn_args)                                       ❷\n  absl.logging.info(\"=\"*50)\n\n  tf_transform_output = tft.TFTransformOutput(\n    fn_args.transform_graph_path\n  )                                                                ❸\n\n  train_dataset = _input_fn(\n    fn_args.train_files, fn_args.data_accessor, tf_transform_output, \n➥ 40                                                              ❹\n  )\n  eval_dataset = _input_fn(\n    fn_args.eval_files, fn_args.data_accessor, tf_transform_output, \n➥ 40                                                              ❹\n  )\n  model = _build_keras_model()                                     ❺\n\n  csv_write_dir = os.path.join(\n    fn_args.model_run_dir,'model_performance'\n)                                                                  ❻\n  os.makedirs(csv_write_dir, exist_ok=True)\n\n  csv_callback = tf.keras.callbacks.CSVLogger(\n    os.path.join(csv_write_dir, 'performance.csv'), append=False   ❼\n  )\n\n  model.fit(                                                       ❽\n      train_dataset,\n      steps_per_epoch=fn_args.train_steps,\n      validation_data=eval_dataset,\n      validation_steps=fn_args.eval_steps,\n      epochs=10,\n      callbacks=[csv_callback]\n  )\n\n  signatures = {                                                   ❾\n      'serving_default':\n          _get_serve_tf_examples_fn(\n              model, tf_transform_output\n          ).get_concrete_function(\n              tf.TensorSpec(\n                  shape=[None],\n                  dtype=tf.string,\n                  name='examples'\n              )\n          ),\n\n  }\n  model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)                                        ❿\n```", "```py\ndef _get_serve_tf_examples_fn(model, tf_transform_output):            ❶\n\n  model.tft_layer = tf_transform_output.transform_features_layer()    ❷\n\n  @tf.function\n  def serve_tf_examples_fn(serialized_tf_examples):                   ❸\n    \"\"\"Returns the output to be used in the serving signature.\"\"\"\n    feature_spec = tf_transform_output.raw_feature_spec()             ❹\n    feature_spec.pop(_LABEL_KEY)                                      ❺\n    parsed_features = tf.io.parse_example(serialized_tf_examples, \n➥ feature_spec)                                                      ❻\n    transformed_features = model.tft_layer(parsed_features)           ❼\n    return model(transformed_features)                                ❽\n\n  return serve_tf_examples_fn                                         ❾\n```", "```py\n{\n  'DC': VarLenFeature(dtype=tf.float32), \n  'DMC': VarLenFeature(dtype=tf.float32),\n  'RH': VarLenFeature(dtype=tf.int64), \n  ...\n  'X': VarLenFeature(dtype=tf.int64), \n  'area': VarLenFeature(dtype=tf.float32), \n  'day': VarLenFeature(dtype=tf.string), \n  'month': VarLenFeature(dtype=tf.string)\n}\n```", "```py\nsignatures = {\n      'serving_default':\n          _get_serve_tf_examples_fn(\n              model, tf_transform_output\n          ).get_concrete_function(\n              tf.TensorSpec(\n                  shape=[None],\n                  dtype=tf.string,\n                  name='examples'\n              )\n          ),    \n  }\n```", "```py\nfrom tfx.components import Trainer\nfrom tfx.proto import trainer_pb2\nimport tensorflow.keras.backend as K\n\nK.clear_session()\n\nn_dataset_size = df.shape[0]\nbatch_size = 40\n\nn_train_steps_mod = 2*n_dataset_size % (3*batch_size)\nn_train_steps = int(2*n_dataset_size/(3*batch_size))\nif n_train_steps_mod != 0:\n    n_train_steps += 1\n\nn_eval_steps_mod = n_dataset_size % (3*batch_size)\nn_eval_steps = int(n_dataset_size/(3*batch_size))\nif n_eval_steps != 0:\n    n_eval_steps += 1\n\ntrainer = Trainer(\n    module_file=os.path.abspath(\"forest_fires_trainer.py\"),\n    transformed_examples=transform.outputs['transformed_examples'],\n    schema=schema_gen.outputs['schema'],\n    transform_graph=transform.outputs['transform_graph'],\n    train_args=trainer_pb2.TrainArgs(num_steps=n_train_steps),\n    eval_args=trainer_pb2.EvalArgs(num_steps=n_eval_steps))\n\ncontext.run(trainer)\n```", "```py\nINFO:absl:Generating ephemeral wheel package for \n➥ '/home/thushv89/code/manning_tf2_in_action/Ch15-TFX-for-MLOps-in-\n➥ TF2/tfx/forest_fires_trainer.py' (including modules: \n➥ ['forest_fires_constants', 'forest_fires_transform', \n➥ 'forest_fires_trainer']).\n\n...\n\nINFO:absl:Training model.\n\n...\n\n43840.0703WARNING:tensorflow:11 out of the last 11 calls to <function \n➥ recreate_function.<locals>.restored_function_body at 0x7f53c000ea60> \n➥ triggered tf.function retracing. Tracing is expensive and the excessive \n➥ number of tracings could be due to (1) creating @tf.function repeatedly \n➥ in a loop, (2) passing tensors with different shapes, (3) passing \n➥ Python objects instead of tensors. \n\nINFO:absl:____________________________________________________________________________\nINFO:absl:Layer (type)                    Output Shape         Param #    \n➥ Connected to                     \nINFO:absl:=================================================================\n➥ ===========\n\n...\n\nINFO:absl:dense_features (DenseFeatures)  (None, 31)           0           \n➥ DC_xf[0][0]                      \nINFO:absl:                                                                \n➥ DMC_xf[0][0]                     \nINFO:absl:                                                               \n➥ FFMC_xf[0][0]                    \n...\nINFO:absl:                                                               \n➥ temp_xf[0][0]                    \nINFO:absl:                                                               \n➥ wind_xf[0][0]                    \nINFO:absl:_________________________________________________________________\n➥ ___________\n\n...\n\nINFO:absl:Total params: 12,417\n\n...\n\nEpoch 1/10\n9/9 [==============================] - ETA: 3s - loss: 43840.070 - 1s \n➥ 32ms/step - loss: 13635.6658 - val_loss: 574.2498\nEpoch 2/10\n9/9 [==============================] - ETA: 0s - loss: 240.241 - 0s \n➥ 10ms/step - loss: 3909.4543 - val_loss: 495.7877\n...\nEpoch 9/10\n9/9 [==============================] - ETA: 0s - loss: 42774.250 - 0s \n➥ 8ms/step - loss: 15405.1482 - val_loss: 481.4183\nEpoch 10/10\n9/9 [==============================] - 1s 104ms/step - loss: 1104.7073 - \n➥ val_loss: 456.1211\n...\n\nINFO:tensorflow:Assets written to: \n➥ /home/thushv89/code/manning_tf2_in_action/Ch15-TFX-for-MLOps-in-\n➥ TF2/tfx/pipeline/examples/forest_fires_pipeline/Trainer/model/5/Format-\n➥ Serving/assets\nINFO:absl:Training complete. Model written to \n➥ /home/thushv89/code/manning_tf2_in_action/Ch15-TFX-for-MLOps-in-\n➥ TF2/tfx/pipeline/examples/forest_fires_pipeline/Trainer/model/5/Format-\n➥ Serving. ModelRun written to \n➥ /home/thushv89/code/manning_tf2_in_action/Ch15-TFX-for-MLOps-in-\n➥ TF2/tfx/pipeline/examples/forest_fires_pipeline/Trainer/model_run/5\nINFO:absl:Running publisher for Trainer\nINFO:absl:MetadataStore with DB connection initialized\n```", "```py\nout of the last x calls to <function \n➥ recreate_function.<locals>.restored_function_body at 0x7f53c000ea60> \n➥ triggered tf.function retracing. Tracing is expensive and the excessive \n➥ number of tracings could be due to\n```", "```py\nINFO:absl:==================================================\nINFO:absl:Printing the tfx.components.FnArgs object\nINFO:absl:FnArgs(\n    working_dir=None, \n    train_files=['.../Transform/transformed_examples/16/Split-train/*'], \n    eval_files=['.../Transform/transformed_examples/16/Split-eval/*'], \n    train_steps=100, \n    eval_steps=100, \n    schema_path='.../SchemaGen/schema/15/schema.pbtxt', \n    schema_file='.../SchemaGen/schema/15/schema.pbtxt', \n    transform_graph_path='.../Transform/transform_graph/16', \n    transform_output='.../Transform/transform_graph/16', \n    data_accessor=DataAccessor(\n        tf_dataset_factory=<function \n➥ get_tf_dataset_factory_from_artifact.<locals>.dataset_factory at \n➥ 0x7f7a56329a60>, \n        record_batch_factory=<function \n➥ get_record_batch_factory_from_artifact.<locals>.record_batch_factory at \n➥ 0x7f7a563297b8>, \n        data_view_decode_fn=None\n    ), \n    serving_model_dir='.../Trainer/model/17/Format-Serving', \n    eval_model_dir='.../Trainer/model/17/Format-TFMA', \n    model_run_dir='.../Trainer/model_run/17', \n    base_model=None, \n    hyperparameters=None, \n    custom_config=None\n)\nINFO:absl:==================================================\n```", "```py\nisi_feature = tfdv.get_feature(schema, 'ISI')\nisi_feature.float_domain.max = 30.0\n```", "```py\ndocker pull tensorflow/serving:2.6.3-gpu\n```", "```py\ndocker images \n```", "```py\ndocker run \\\n  --rm \\\n  -it \\\n  --gpus all \\\n  -p 8501:8501 \\\n  --user $(id -u):$(id -g) \\\n  -v ${PWD}/tfx/forest-fires-pushed:/models/forest_fires_model \\\n  -e MODEL_NAME=forest_fires_model \\\n  tensorflow/serving:2.6.3-gpu\n```", "```py\n2.6.3-gpu: Pulling from tensorflow/serving\nDigest: \n➥ sha256:e55c44c088f6b3896a8f66d8736f38b56a8c5687c105af65a58f2bfb0bf90812\nStatus: Image is up to date for tensorflow/serving:2.6.3-gpu\ndocker.io/tensorflow/serving:2.6.3-gpu\n2021-07-16 05:59:37.786770: I\ntensorflow_serving/model_servers/server.cc:88] Building single TensorFlow \n➥ model file config: model_name: forest_fires_model model_base_path: \n➥ /models/forest_fires_model\n2021-07-16 05:59:37.786895: I\ntensorflow_serving/model_servers/server_core.cc:464] Adding/updating \n➥ models.\n2021-07-16 05:59:37.786915: I\ntensorflow_serving/model_servers/server_core.cc:587]  (Re-)adding model: \n➥ forest_fires_model\n2021-07-16 05:59:37.787498: W\ntensorflow_serving/sources/storage_path/file_system_storage_path_source.cc:\n➥ 267] No versions of servable forest_fires_model found under base path \n➥ /models/forest_fires_model. Did you forget to name your leaf directory \n➥ as a number (eg. '/1/')?\n...\n```", "```py\nfrom tfx.components import InfraValidator\nfrom tfx.proto import infra_validator_pb2\n\ninfra_validator = InfraValidator(\n    model=trainer.outputs['model'],                                        ❶\n\n    examples=example_gen.outputs['examples'],                              ❷\n    serving_spec=infra_validator_pb2.ServingSpec(                          ❸\n        tensorflow_serving=infra_validator_pb2.TensorFlowServing(          ❹\n            tags=['2.6.3-gpu']\n        ),\n        local_docker=infra_validator_pb2.LocalDockerConfig(),              ❺\n    ),\n    request_spec=infra_validator_pb2.RequestSpec(                          ❻\n        tensorflow_serving=infra_validator_pb2.TensorFlowServingRequestSpec(❼\n            signature_names=['serving_default']\n        ),\n        num_examples=5                                                     ❽\n    )\n)\n\ncontext.run(infra_validator)\n```", "```py\nfrom tfx import v1 as tfx\n\nmodel_resolver = tfx.dsl.Resolver(\n      strategy_class=tfx.dsl.experimental.LatestBlessedModelStrategy,\n      model=tfx.dsl.Channel(type=tfx.types.standard_artifacts.Model),\n      model_blessing=tfx.dsl.Channel(\n          type=tfx.types.standard_artifacts.ModelBlessing\n      )\n).with_id('latest_blessed_model_resolver')\n\ncontext.run(model_resolver)\n```", "```py\nimport tensorflow_model_analysis as tfma\n\neval_config = tfma.EvalConfig(\n    model_specs=[\n        tfma.ModelSpec(label_key='area')                                  ❶\n    ],\n    metrics_specs=[\n        tfma.MetricsSpec(\n            metrics=[                                                     ❷\n                tfma.MetricConfig(class_name='ExampleCount'),             ❸\n                tfma.MetricConfig(\n                    class_name='MeanSquaredError',                        ❹\n                    threshold=tfma.MetricThreshold(                       ❺\n                       value_threshold=tfma.GenericValueThreshold(\n                           upper_bound={'value': 200.0}\n                       ),\n                       change_threshold=tfma.GenericChangeThreshold(      ❻\n                           direction=tfma.MetricDirection.LOWER_IS_BETTER,\n                           absolute={'value': 1e-10}\n                       )\n                   )\n               )\n           ]\n        )\n    ],\n    slicing_specs=[                                                       ❼\n        tfma.SlicingSpec(),                                               ❽\n        tfma.SlicingSpec(feature_keys=['month'])                          ❾\n    ])\n```", "```py\nfrom tfx.components import Evaluator\n\nevaluator = Evaluator(\n    examples=example_gen.outputs['examples'],\n    model=trainer.outputs['model'],\n    baseline_model=model_resolver.outputs['model'],\n    eval_config=eval_config)\ncontext.run(evaluator)\n```", "```py\nINFO:absl:Evaluation complete. Results written to \n➥ .../pipeline/examples/forest_fires_pipeline/Evaluator/evaluation/14.\nINFO:absl:Checking validation results.\nINFO:absl:Blessing result False written to \n➥ .../pipeline/examples/forest_fires_pipeline/Evaluator/blessing/14.\n```", "```py\nimport tensorflow_model_analysis as tfma\n\nvalidation_path = os.path.join(\n    evaluator.outputs['evaluation']._artifacts[0].uri, \"validations\"\n)\nvalidation_res = tfma.load_validation_result(validation_path)\n\nprint('='*20, \" Output stored in validations file \", '='*20)\nprint(validation_res)\nprint(\"=\"*75)\n```", "```py\nmetric_validations_per_slice {\n  slice_key {\n    single_slice_keys {\n      column: \"month\"\n      bytes_value: \"sep\"\n    }\n  }\n  failures {\n    metric_key {\n      name: \"mean_squared_error\"\n    }\n    metric_threshold {\n      value_threshold {\n        upper_bound {\n          value: 200.0\n        }\n      }\n    }\n    metric_value {\n      double_value {\n        value: 269.11712646484375\n      }\n    }\n  }\n}\nvalidation_details {\n  slicing_details {\n    slicing_spec {\n    }\n    num_matching_slices: 12\n  }\n}\n```", "```py\nmetrics_path = os.path.join(\n    evaluator.outputs['evaluation']._artifacts[0].uri, \"metrics\"\n)\nmetrics_res = tfma.load_metrics(metrics_path)\n\nprint('='*20, \" Output stored in metrics file \", '='*20)\nfor r in metrics_res:\n    print(r)\n    print('-'*75)\nprint(\"=\"*75)\n```", "```py\nslice_key {\n  single_slice_keys {\n    column: \"month\"\n    bytes_value: \"sep\"\n  }\n}\nmetric_keys_and_values {\n  key {\n    name: \"loss\"\n  }\n  value {\n    double_value {\n      value: 269.11712646484375\n    }\n  }\n}\nmetric_keys_and_values {\n  key {\n    name: \"mean_squared_error\"\n  }\n  value {\n    double_value {\n      value: 269.11712646484375\n    }\n  }\n}\nmetric_keys_and_values {\n  key {\n    name: \"example_count\"\n  }\n  value {\n    double_value {\n      value: 52.0\n    }\n  }\n}\n\n---------------------------------------------------------------------------\nslice_key {\n}\nmetric_keys_and_values {\n  key {\n    name: \"loss\"\n  }\n  value {\n    double_value {\n      value: 160.19691467285156\n    }\n  }\n}\nmetric_keys_and_values {\n  key {\n    name: \"mean_squared_error\"\n  }\n  value {\n    double_value {\n      value: 160.19691467285156\n    }\n  }\n}\nmetric_keys_and_values {\n  key {\n    name: \"example_count\"\n  }\n  value {\n    double_value {\n      value: 153.0\n    }\n  }\n}\n...\n```", "```py\nINFO:absl:Evaluation complete. Results written to \n➥ .../pipeline/examples/forest_fires_pipeline/Evaluator/evaluation/15.\nINFO:absl:Checking validation results.\nINFO:absl:Blessing result True written to \n➥ .../pipeline/examples/forest_fires_pipeline/Evaluator/blessing/15.\n```", "```py\nfrom tfx.components import Pusher\nfrom tfx.proto import pusher_pb2\n\npusher = Pusher(\n  model=trainer.outputs['model'],\n  model_blessing=evaluator.outputs['blessing'],\n  infra_blessing=infra_validator.outputs['blessing'],\n  push_destination=pusher_pb2.PushDestination(\n    filesystem=pusher_pb2.PushDestination.Filesystem(\n        base_directory=os.path.join('forestfires-model-pushed'))\n  )\n)\ncontext.run(pusher)\n```", "```py\nimport base64\nimport json\nimport requests\n\nreq_body = {\n  \"signature_name\": \"serving_default\",\n\n  \"instances\": \n    [\n            str(base64.b64encode(\n                b\"{\\\"X\\\": 7,\\\"Y\\\": \n➥ 4,\\\"month\\\":\\\"oct\\\",\\\"day\\\":\\\"fri\\\",\\\"FFMC\\\":60,\\\"DMC\\\":30,\\\"DC\\\":200,\\\n➥ \"ISI\\\":9,\\\"temp\\\":30,\\\"RH\\\":50,\\\"wind\\\":10,\\\"rain\\\":0}]\")\n               )\n    ]\n\n}\n\ndata = json.dumps(req_body)\n\njson_response = requests.post(\n    'http:/ /localhost:8501/v1/models/forest_fires_model:predict', \n    data=data, \n    headers={\"content-type\": \"application/json\"}\n)\npredictions = json.loads(json_response.text)\n```", "```py\n{'predictions': [[2.77522683]]}\n```", "```py\n  outputs = {}\n\n  # Treating dense features\n  outputs[_transformed_name('DC')] = tft.scale_to_0_1(\n        sparse_to_dense(inputs['DC'])\n    )\n\n  # Treating bucketized features\n  outputs[_transformed_name('temp')] = tft.apply_buckets(\n        sparse_to_dense(inputs['temp']), bucket_boundaries=[(20, 30)])\n```", "```py\ncategorical_columns = [\n      tf.feature_column.embedding_column(\n          tf.feature_column.categorical_column_with_identity( \n              key,\n              num_buckets=num_buckets,\n              default_value=0\n          ),\n          dimension=32\n      ) for key, num_buckets in zip(\n              _transformed_names(_VOCAB_FEATURE_KEYS),\n              _MAX_CATEGORICAL_FEATURE_VALUES\n      )\n```", "```py\ndocker run -v /tmp/inputs:/data -p 5000:5000 tensorflow/tensorflow:2.5.0\n```", "```py\nreq_body = {\n  \"signature_name\": \"serving_default\",\n\n  \"instances\": \n    [\n        str(base64.b64encode(\n            b\"{\\\"X\\\": 9,\\\"Y\\\": \n➥ 6,\\\"month\\\":\\\"aug\\\",\\\"day\\\":\\\"fri\\\",\\\"FFMC\\\":91,\\\"DMC\\\":248,\\\"DC\\\":553,\n➥ \\\"ISI\\\":6,\\\"temp\\\":20.5,\\\"RH\\\":58,\\\"wind\\\":3,\\\"rain\\\":0}]\")\n        ),\n        str(base64.b64encode(\n            b\"{\\\"X\\\": 7,\\\"Y\\\": \n➥ 4,\\\"month\\\":\\\"aug\\\",\\\"day\\\":\\\"fri\\\",\\\"FFMC\\\":91,\\\"DMC\\\":248,\\\"DC\\\":553,\n➥ \\\"ISI\\\":6,\\\"temp\\\":20.5,\\\"RH\\\":20,\\\"wind\\\":0,\\\"rain\\\":0}]\")\n        ),\n\n    ]\n\n}\n```"]