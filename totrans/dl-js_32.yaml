- en: Glossary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Activation function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The function at the last stage of a neural network layer. For example, a rectified
    linear unit (relu) function may be applied on the result of the matrix multiplication
    to generate the final output of a dense layer. An activation function can be linear
    or nonlinear. Nonlinear activation functions can be used to increase the representational
    power (or capacity) of a neural network. Examples of nonlinear activations include
    sigmoid, hyperbolic tangent (tanh), and the aforementioned relu.
  prefs: []
  type: TYPE_NORMAL
- en: Area under the curve (AUC)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A single number used to quantify the shape of an ROC curve. It is defined as
    the definite integral under the ROC curve, from false positive rate 0 to 1\. See
    *ROC curve*.
  prefs: []
  type: TYPE_NORMAL
- en: Axis
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the context of TensorFlow.js, when we talk about a *tensor*, an axis (plural
    *axes*) is one of the independent keys indexing into the tensor. For example,
    a rank-3 tensor has three axes; an element of a rank-3 tensor is identified by
    three integers that correspond to the three axes. Also known as a *dimension*.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The algorithm that traces back from the loss value of a differentiable machine-learning
    model to the gradients on the weight parameters. It is based on the chain rule
    of differentiation and forms the basis of training for most neural networks in
    this book.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation through time (BPTT)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A special form of backpropagation in which the steps are not over the operations
    for the successive layers of a model, but instead over the operations for the
    successive time steps. It underlies the training of recurrent neural networks
    (RNNs).
  prefs: []
  type: TYPE_NORMAL
- en: Balance (dataset)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A quality of a dataset with categorical labels. The more equal the numbers of
    examples from different categories are, the more balanced a dataset is.
  prefs: []
  type: TYPE_NORMAL
- en: Batch
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: During the training of neural networks, multiple input examples are often aggregated
    to form a single tensor, which is used to calculate the gradients and updates
    to the network’s weights. Such an aggregation is called a *batch*. The number
    of examples in the batch is called the *batch size*.
  prefs: []
  type: TYPE_NORMAL
- en: Bellman equation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In reinforcement learning, a recursive equation that quantifies the value of
    a state-action pair as a sum of two terms: 1) the reward the agent is expected
    to get immediately after the action and 2) the best expected reward the agent
    can get in the next state, discounted by a factor. The second term assumes optimal
    selection of action in the next state. It forms the basis of reinforcement-learning
    algorithms such as deep Q-learning.'
  prefs: []
  type: TYPE_NORMAL
- en: Binary classification
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A classification task in which the target is the answer to a yes/no question,
    such as whether a certain X-ray image indicates pneumonia or whether a credit
    card transaction is legitimate or fraudulent.
  prefs: []
  type: TYPE_NORMAL
- en: Broadcasting
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: TensorFlow allows for pairwise operations between tensors with different but
    compatible shapes. For instance, it is possible to add a tensor of shape `[5]`
    to a tensor of shape `[13, 5]`. In effect, the smaller tensor will be repeated
    13 times to compute the output. The details for the rules of when broadcasting
    is allowed are in [info box 2.4](kindle_split_013.html#ch02sb04) in [chapter 2](kindle_split_013.html#ch02).
  prefs: []
  type: TYPE_NORMAL
- en: Capacity
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The range of input-output relations that a machine-learning model is capable
    of learning. For example, a neural network with a hidden layer with a nonlinear
    activation function has a greater capacity than a linear-regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Class activation map
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: An algorithm that can visualize the relative importance of different parts of
    an input image for the classification output of a convolutional neural network.
    It is based on computing the gradient of the final probability score of the winning
    class with respect to the output of the last internal convolutional layer of the
    network. It is discussed in detail in [section 7.2.3](kindle_split_019.html#ch07lev2sec5).
  prefs: []
  type: TYPE_NORMAL
- en: Computer vision
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The study of how computers can understand images and videos. It is an important
    part of machine learning. In the context of machine learning, common computer-vision
    tasks include image recognition, segmentation, captioning, and object detection.
  prefs: []
  type: TYPE_NORMAL
- en: Confusion matrix
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A square matrix (a 2D tensor) of the shape `[numClasses, numClasses]`. In multiclass
    classification, a confusion matrix is used to quantify how many times examples
    of a given truth class are classified as each of the possible classes. The element
    at indices `[i, j]` is the number of times examples from the true class `i` are
    classified as class `j`. The elements on the diagonal line correspond to correct
    classification results.
  prefs: []
  type: TYPE_NORMAL
- en: Constant folding
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A type of computation-graph optimization in which a subgraph that contains only
    predetermined constant nodes and deterministic operations among them is reduced
    to a single constant node. The `GraphModel` conversion technique in TensorFlow.js
    leverages constant folding.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional kernel
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In convolution operations, a tensor that operates on the input tensor to generate
    the output tensor. Take image tensors, for example: the kernel is usually smaller
    in its height and width dimensions compared to the input image. It is “slided”
    over the height and width dimensions of the input image and undergoes a dot product
    (multiply and add) at every sliding position. For a convolutional layer of TensorFlow.js
    (such as conv2d), the kernel is its key weight.'
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The process of generating more training data from existing training samples
    (x, y) by creating mutations of the training samples via a family of programmatic
    transformations that yield valid inputs x' without changing the target. This helps
    expose the model to more aspects of the data and thus generalize better without
    the engineer having to manually build invariance to these types of transformations
    into the model.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The study and application of deep neural networks (that is, using a large number
    of successive representational transformations to solve machine-learning problems).
  prefs: []
  type: TYPE_NORMAL
- en: Deep neural network
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A neural network with a large number (anywhere between two and thousands) of
    layers.
  prefs: []
  type: TYPE_NORMAL
- en: Dimension
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the context of a tensor, synonymous with *axis*. See *axis*.
  prefs: []
  type: TYPE_NORMAL
- en: Dot product
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: See *inner product*.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In deep learning, a representation of a certain piece of data in an *n*-dimensional
    vector space (*n* being a positive integer). In other words, it is a representation
    of a piece of data as an ordered, length-*n* array of floating-point numbers.
    Embedding representations can be created for many types of data: images, sounds,
    words, and items from a closed set. An embedding is usually from an intermediate
    layer of a trained neural network.'
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble learning
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The practice of training a number of individual machine-learning models and
    using them together for inference on the same problem. Even though each individual
    model may not be very accurate, the ensemble model can have a much higher accuracy.
    Ensemble models are often used by the winning entries of data science competitions,
    such as Kaggle competitions.
  prefs: []
  type: TYPE_NORMAL
- en: Epoch
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: When training a model, one complete pass through the training data.
  prefs: []
  type: TYPE_NORMAL
- en: Epsilon-greedy policy
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In reinforcement learning, an action-selection method that parametrizes the
    balance between random exploratory behavior and optimal behavior on the part of
    the agent. The value of epsilon is constrained between 0 and 1\. The higher it
    is, the more likely the agent is to select random actions.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the context of machine learning, an individual instance of input data (for
    example, an image of the appropriate size for a computer-vision model), for which
    a machine-learning model will generate an output prediction (such as a label for
    the image).
  prefs: []
  type: TYPE_NORMAL
- en: Feature
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'One aspect of the input data for a machine-learning model. A feature can be
    in any of the following forms:'
  prefs: []
  type: TYPE_NORMAL
- en: A number (for example, the monetary amount of a credit card transaction)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A string from an open set (name of transaction)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A piece of categorical information (such as the brand name of the credit card)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A one- or multidimensional array of numbers (for instance, a grayscale image
    of the credit card customer’s signature represented as a 2D array)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other types of information (for example, date-time)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An input example can consist of one or multiple features.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The process of transforming the original features in input data into a representation
    more amenable to solving the machine-learning problem. Before deep learning, feature
    engineering was performed by engineers with domain-specific knowledge through
    trial and error. It was often a labor-intensive and brittle process, without any
    guarantee of finding the optimal solution. Deep learning has largely automated
    feature engineering.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In transfer learning, a phase of model training during which the weights in
    some layers of the base model are allowed to be updated. It usually follows an
    initial phase of model training during which all weights in the base model are
    frozen to prevent large initial gradients from perturbing the pretrained weights
    too much. When used properly, fine-tuning can boost the capacity of the transfer-learning
    model, thereby achieving superior accuracy while consuming significantly less
    computation resources than training an entire model from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Generative adversarial network (GAN)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A type of generative machine-learning model that involves two parts called the
    *discriminator* and the *generator*. The discriminator is trained to distinguish
    real examples from a training set from fake ones, while the generator is trained
    to output examples that cause the discriminator to output high realness scores
    (that is, to “fool” the discriminator into “thinking” that the fake examples are
    real). After proper training, the generator is capable of outputting highly realistic
    fake examples.
  prefs: []
  type: TYPE_NORMAL
- en: Golden value
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the context of testing a machine-learning system, the correct output a model
    should generate for a given input. An example is the “classical” label for a neural
    network that classifies audio recordings into genres of music when given a recording
    of Beethoven’s Fifth Symphony.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The process of minimizing the numerical output value of a system by iteratively
    changing the parameters of the system along the direction of the gradients (that
    is, derivatives of the parameters with respect to the output value). It is the
    primary way in which neural networks are trained. In the context of neural network
    training, the system is formed by the neural network and a loss function selected
    by the engineer. The parameters of the system are the weights of the neural network’s
    layers. The iteration process happens batch-by-batch over the training data.
  prefs: []
  type: TYPE_NORMAL
- en: Graphics processing unit (GPU)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Parallel-computing chips equipped with a much larger number (hundreds or thousands)
    of cores than typical CPUs. GPUs were originally designed to accelerate the computation
    and rendering of 2D and 3D graphics. But they turned out to be useful for the
    kind of parallel computing involved in running deep neural networks as well. GPUs
    are an important contributing factor to the deep-learning revolution and continue
    to play critical roles in the research and applications of deep learning today.
    TensorFlow.js harnesses the parallel-computing power of GPUs through two conduits:
    1) the WebGL API of the web browser and 2) binding to the TensorFlow CUDA kernels
    in Node.js.'
  prefs: []
  type: TYPE_NORMAL
- en: GraphModel
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In TensorFlow.js, a model converted from TensorFlow (Python) and loaded into
    JavaScript. `GraphModel` has the potential to undergo TensorFlow-internal performance
    optimizations such as Grappler’s arithmetic optimization and op fusion (see [section
    12.2.2](kindle_split_025.html#ch12lev2sec5) for details).
  prefs: []
  type: TYPE_NORMAL
- en: Hidden layer
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A neural network that consists of a layer whose output is not exposed as an
    output of the network but is instead consumed only by other layers of the network.
    For example, in a neural network defined as a TensorFlow.js sequential model,
    all layers except the last one are hidden layers.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter optimization
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Sometimes also called *hyperparameter tuning*; the process of searching for
    the set of hyperparameters that gives the lowest validation loss on a given machine-learning
    task.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameters
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Tunable parameters of the model and optimizer that are not tunable with backpropagation.
    Typically, the learning rate and model structure are common example hyperparameters.
    Hyperparameters may be tuned by grid search or more sophisticated hyperparameter-tuning
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Hypothesis space
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the context of machine learning, the set of possible solutions to a machine-learning
    problem. The training process involves searching for a good solution in such a
    space. The hypothesis space is determined by the type and the architecture of
    the machine-learning model chosen to solve the problem.
  prefs: []
  type: TYPE_NORMAL
- en: ImageNet
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A large-scale public dataset of labeled colored images. It is an important training
    set and benchmark for computer-vision-oriented deep neural networks. ImageNet
    was instrumental in ushering in the beginning of the deep-learning revolution.
  prefs: []
  type: TYPE_NORMAL
- en: Imputation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A technique for filling in missing values from a dataset. For instance, if we
    had a dataset of cars, and some cars were missing their “weight” feature, we might
    simply guess the average weight for those features. More sophisticated imputation
    techniques are also possible.
  prefs: []
  type: TYPE_NORMAL
- en: Inception
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A type of deep convolutional neural network featuring a large number of layers
    and a complex network structure.
  prefs: []
  type: TYPE_NORMAL
- en: Independent and identically distributed (IID)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A statistical property of data samples. If we assume that data is sampled from
    an underlying distribution, then the samples are identically distributed if each
    sample comes from the same distribution. Samples are independent if knowing the
    value of one sample gives you no additional information about the next sample.
  prefs: []
  type: TYPE_NORMAL
- en: A sample of dice rolls is an example of an IID collection of samples. If the
    dice rolls are sorted, the samples are identically distributed but not independent.
    Training data should be IID, or there is likely to be convergence or other issues
    during training.
  prefs: []
  type: TYPE_NORMAL
- en: Inference
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Using a machine-learning model on input data to generate an output. It is the
    ultimate purpose of training the model.
  prefs: []
  type: TYPE_NORMAL
- en: Inner product
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Also known as *dot product*; a mathematical operation on two vectors of equivalent
    shape, yielding a single scalar value. To calculate the inner product between
    vectors `a` and `b`, sum up all `a[i] * b[i]` for all valid values of `i`. In
    geometric terms, the inner product of two vectors is equal to the product of their
    magnitudes and the cosine of the angle between them.
  prefs: []
  type: TYPE_NORMAL
- en: Keras
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A popular library for deep learning. Today, it is the most frequently used deep-learning
    library in Kaggle competitions. François Chollet, currently a software engineer
    at Google, is its original author. Keras is a Python library. The high-level API
    of TensorFlow.js, which is a main focus of this book, is modeled after and compatible
    with Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Label
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The desired answer for an input example given the task at hand. A label can
    be a Boolean (yes/no) answer, a number, a text string, a category among a number
    of possible categories, a sequence of numbers, or more complex data types. In
    supervised machine learning, a model aims at generating outputs that closely match
    the labels.
  prefs: []
  type: TYPE_NORMAL
- en: Layer
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In the context of neural networks, a transformation of the data representation.
    It behaves like a mathematical function: given an input, it emits an output. A
    layer can have state captured by its weights. The weights can be altered during
    the training of the neural network.'
  prefs: []
  type: TYPE_NORMAL
- en: LayersModel
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A model built using the Keras-like high-level API of TensorFlow.js. It can also
    be loaded from a converted Keras (Python) model. A `LayersModel` supports inference
    (with its `predict()` method) and training (with its `fit()` and `fitDataset()`
    methods).
  prefs: []
  type: TYPE_NORMAL
- en: Learning rate
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: During gradient descent, model weights are modified to reduce loss. The exact
    change in the weights is a function not only of the gradient of the loss but also
    of a parameter. In the standard gradient-descent algorithm, the weight update
    is calculated by multiplying the gradient by the learning rate, which is typically
    a small positive constant. The default learning rate for the `'sgd'` optimizer
    in tensorflow.js is 0.01.
  prefs: []
  type: TYPE_NORMAL
- en: Local minimum
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: When optimizing the parameters of a model, a setting of the parameters for which
    any sufficiently small change in the parameters always increases the loss. Similar
    to a marble at the bottom of a bowl, there is no small movement that is even lower.
    A local minimum is distinguished from a *global minimum* in that a local minimum
    is the lowest point in the local neighborhood, but the global minimum is the lowest
    point overall.
  prefs: []
  type: TYPE_NORMAL
- en: Logit
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In machine learning, an unnormalized probability value. Unlike probabilities,
    logits are not limited to the [0, 1] interval or required to sum to 1\. Hence,
    they can be more easily output by a neural network layer. A set of logits can
    be normalized to probability values through an operation called *softmax*.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A subfield of artificial intelligence (AI) that automates the discovery of rules
    for solving complex problems by using data labeled with the desired answers. It
    differs from classical programming in that no handcrafting of the rules is involved.
  prefs: []
  type: TYPE_NORMAL
- en: Markov decision process (MDP)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In reinforcement learning, a decision process in which the current state and
    the action selected by the agent completely determine the next state that the
    agent will end up with and the reward the agent will receive at the step. It is
    an important simplification that enables learning algorithms such as Q-learning.
  prefs: []
  type: TYPE_NORMAL
- en: Model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In machine learning and deep learning, an object that transforms input data
    (such as an image) into the desired output (such as a text label for the image)
    through a number of successive mathematical operations. A model has parameters
    (called *weights*) that can be tuned during training.
  prefs: []
  type: TYPE_NORMAL
- en: Model adaptation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The process of training a pretrained model or a part of it in order to make
    the model achieve better accuracy during inference on the input data from a specific
    user or specific use case. It is a type of transfer learning, one in which the
    types of the input features and the type of the target don’t differ from the original
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Model deployment
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The process of packaging a trained model to the place where it can be used for
    making predictions. Similar to “pushing to production” for other software stacks,
    deployment is how users can get to use models “for real.”
  prefs: []
  type: TYPE_NORMAL
- en: MobileNet
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A pretrained deep convolutional neural network. It is typically trained on the
    ImageNet image-classification dataset and can be used for transfer learning. Among
    similar pretrained convolutional neural networks, it has a relatively small size
    and involves less computation to perform inference, and is therefore more suitable
    to run in a resource-restricted environment such as the web browser, with TensorFlow.js.
  prefs: []
  type: TYPE_NORMAL
- en: Multiclass classification
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A classification problem in which the target may take more than two discrete
    labels. Examples are what kind of animal a picture contains or what (natural)
    language a web page is in given its content.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-hot encoding
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A way to represent the words in a sentence (or, in general, the items in a sequence)
    as a vector by setting the elements that correspond to the words to 1 and leaving
    the rest as 0\. This can be viewed as a generalization of *one-hot encoding*.
    It discards the information regarding the order of the words.
  prefs: []
  type: TYPE_NORMAL
- en: Multilayer perceptron (MLP)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A neural network consisting of feedforward topology and at least one hidden
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: Natural language processing
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The subfield of computer science that studies how to use computers to process
    and understand natural language, most prominently text and speech. Deep learning
    finds many applications in natural language processing.
  prefs: []
  type: TYPE_NORMAL
- en: Neural network
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A category of machine-learning models inspired by the layered organization seen
    in biological neural systems. The layers of a neural network perform multistep,
    separable transformations of the data representation.
  prefs: []
  type: TYPE_NORMAL
- en: Nonlinearity
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: An input-output relation that does not meet the definition of linearity (linear
    combinations of inputs lead to a linear combination of the outputs, up to a constant-term
    difference). In neural networks, nonlinear relations (such as sigmoid and relu
    activations in layers) and the cascading of multiple such relations can increase
    the capacity of the neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Object detection
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A computer-vision task that involves detecting certain classes of objects and
    their location in an image.
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encoding
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The scheme of encoding categorical data as a vector of length *N* consisting
    of all zeros except at the index that corresponds to the actual class.
  prefs: []
  type: TYPE_NORMAL
- en: Op fusion
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A computation-graph optimization technique in which multiple operations (or
    ops) are replaced with a single equivalent op. Op fusion reduces the op-dispatching
    overhead and can lead to more opportunities for further intra-op memory and performance
    optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Out-of-vocabulary (OOV)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the context of deep learning, when a *vocabulary* is used on a set of discrete
    items, the vocabulary sometimes doesn’t include all possible items. When an item
    outside the vocabulary is encountered, it is mapped to a special index called
    out-of-vocabulary, which can then be mapped to a special element in the one-hot
    encoding or embedding representation. See *vocabulary*.
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: When a model is fit to the training data in such a way that the model has sufficient
    capacity to memorize the training data, we see the training loss continue to go
    down, but the testing or validation loss starts to rise. Models with this property
    begin to lose their ability to generalize and perform well only on the exact samples
    in the training data. We say models in this circumstance are overfit.
  prefs: []
  type: TYPE_NORMAL
- en: Policy gradients
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A type of reinforcement-learning algorithm that computes and utilizes the gradients
    of certain measures (such as logits) of selected actions with respect to the weights
    of a policy network in order to cause the policy network to gradually select better
    actions.
  prefs: []
  type: TYPE_NORMAL
- en: Precision
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A metric of a binary classifier, defined as the ratio of the examples labeled
    by the classifier as positive that are actually positive. See *recall*.
  prefs: []
  type: TYPE_NORMAL
- en: Pseudo examples
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Additional examples based on known valid mutations of input training examples,
    used to supplement the training data. For instance, we might take the MNIST digits
    and apply small rotations and skews. These transformations do not change the image
    label.
  prefs: []
  type: TYPE_NORMAL
- en: Q-network
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In reinforcement learning, a neural network that predicts the Q-values of all
    possible actions given the current state observation. The Q-learning algorithm
    is about training a Q-network using data from the agent’s experience.
  prefs: []
  type: TYPE_NORMAL
- en: Q-value
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In reinforcement learning, the expected total future cumulative reward for taking
    an action at a given state. Hence a Q-value is a function of action and state.
    It guides the selection of actions in Q-learning.
  prefs: []
  type: TYPE_NORMAL
- en: Random initialization
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Before a model is fit, the process of assigning the weights an initial value
    as a starting point. There is much literature on what, exactly, are good distributions
    to choose from for the initial values based on the layer type, size, and task.
  prefs: []
  type: TYPE_NORMAL
- en: Recall
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A metric of a binary classifier, defined as the ratio of the actual examples
    that are labeled by the classifier as positive. See *precision*.
  prefs: []
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A type of learning problem where the desired output (or label) is a number or
    list of numbers. Making predictions that are numerically closer to the expected
    output is better.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In machine learning, the process of imposing various modifications to the loss
    function or the training process in order to counteract overfitting. There are
    several ways to perform regularization, the most frequently used of which are
    L1 and L2 regularization of weights.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning (RL)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A type of machine learning that involves learning optimal decisions that maximize
    a metric called a *reward* through interacting with an environment. [Chapter 11](kindle_split_023.html#ch11)
    of this book covers the basics of RL and how to solve simple RL problems using
    deep-learning techniques.
  prefs: []
  type: TYPE_NORMAL
- en: ResNet
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Short for *residual network*; a popular convolutional network widely used in
    computer vision, featuring residual connections—that is, connections that skip
    layers.
  prefs: []
  type: TYPE_NORMAL
- en: ROC curve
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A way to visualize the trade-off between the true positive rate (recall) and
    the false positive rate (false-alarm rate) of a binary classifier. The name of
    the curve (the *receiver operating characteristics curve*) originated from the
    early days of radar technology. See *area under the curve* (AUC).
  prefs: []
  type: TYPE_NORMAL
- en: Spectrogram
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'An image-like 2D representation of 1D time signals such as sounds. A spectrogram
    has two dimensions: time and frequency. Each element represents the intensity
    or power the sound contains in a given frequency range at a given moment in time.'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The paradigm of training a machine-learning model using labeled examples. The
    internal parameters of the model are altered in a way that minimizes the difference
    between the model’s output for the examples and the corresponding actual labels.
  prefs: []
  type: TYPE_NORMAL
- en: Symbolic tensor
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In TensorFlow.js, an object of the `SymbolicTensor` class that is a specification
    for the shape and data type (dtype) of a tensor. Unlike a tensor, a `SymbolicTensor`
    object is not associated with concrete values. Instead, it is used as a placeholder
    for the input or output of a layer or a model.
  prefs: []
  type: TYPE_NORMAL
- en: Tensor
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'A data structure for holding data elements, usually numbers. Tensors can be
    thought of as *n*-dimensional grids, where each position in the grid holds exactly
    one element. The number of dimensions and size of each dimension is called the
    tensor’s *shape*. For instance, a 3 × 4 matrix is a tensor with shape `[3, 4]`.
    A vector of length 10 is a 1D tensor with shape `[10]`. Each tensor instance holds
    only one type of element. Tensors are designed this way because it allows for
    convenient, highly efficient implementations of common operations necessary for
    deep learning: for instance, matrix dot products.'
  prefs: []
  type: TYPE_NORMAL
- en: TensorBoard
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A monitoring and visualization tool for TensorFlow. It allows users to visualize
    model structure and training performance in the browser. TensorFlow.js can write
    training logs in a data format compatible with TensorBoard.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: An open source Python library for accelerated machine learning, with a focus
    on deep neural networks. It was released by Google’s Brain team in November 2015\.
    Its API forms a blueprint for that of TensorFlow.js.
  prefs: []
  type: TYPE_NORMAL
- en: Training
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The process of altering a machine-learning model’s internal parameters (weights)
    to make the model’s outputs more closely match the desired answers.
  prefs: []
  type: TYPE_NORMAL
- en: Training data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The data that is used to train a machine-learning model. Training data consists
    of individual examples. Each example is structured information (for example, images,
    audio, or text) in conjunction with the expected answer (the label).
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The practice of taking a machine-learning model previously trained for one task,
    retraining it with a relatively small amount of data (compared to the original
    training dataset) for a new task, and using it for inference on the new task.
  prefs: []
  type: TYPE_NORMAL
- en: Underfitting
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: When a model is trained for too few optimization steps, or a model has an insufficient
    representational power (capacity) to learn the patterns in the training data,
    which results in a model that does not reach a decent level of quality, we say
    that the model is underfit.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The paradigm of machine learning that uses unlabeled data. It is opposed to
    supervised learning, which uses labeled data. Examples of unsupervised learning
    include clustering (discovering distinct subsets of examples in the dataset) and
    anomaly detection (determining if a given example is sufficiently different from
    the examples in the training set).
  prefs: []
  type: TYPE_NORMAL
- en: Validation data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Data that is set apart from training data for the tuning of hyperparameters,
    such as the learning rate or the number of units in a dense layer. Validation
    data allows us to tune our learning algorithm, possibly running training many
    times. Since validation data is also separate from testing data, we can still
    rely on the result from the test data to give us an unbiased estimate of how our
    model will perform on new, unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: Vanishing-gradient problem
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A classic problem in training deep neural networks in which the gradients on
    the weight parameter get increasingly smaller as the number of layers gets larger,
    and the weight parameters get farther and farther apart from the loss function
    as a result. In modern deep learning, this problem is mitigated through improved
    activation functions, proper initialization of weights, and other tricks.
  prefs: []
  type: TYPE_NORMAL
- en: Vectorization
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The process of turning a piece of nonnumerical data into a representation as
    an array of numbers (such as a vector). For example, text vectorization involves
    turning characters, words, or sentences into vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Visor
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In tfjs-vis (a visualization library tightly integrated with TensorFlow.js),
    a collapsible region that can be created with a single function call on the side
    of the web page to hold surfaces for visualization. Multiple tabs can be created
    within a visor to organize the surfaces. See [section 8.1](kindle_split_020.html#ch08lev1sec1)
    for details.
  prefs: []
  type: TYPE_NORMAL
- en: Vocabulary
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the context of deep learning, a set of discrete, unique items that may be
    used as the input to or output from a neural network. Typically, each item of
    the vocabulary can be mapped to an integer index, which can then be turned into
    a one-hot or embedding-based representation.
  prefs: []
  type: TYPE_NORMAL
- en: Weight
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A tunable parameter of a neural network layer. Changing the weights changes
    the numerical details of how the input is transformed into the output. The training
    of a neural network is primarily about updating weight values in a systematic
    way.
  prefs: []
  type: TYPE_NORMAL
- en: Weight quantization
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A technique for reducing the serialized and on-the-wire size of a model. It
    involves storing the weight parameters of the model at a lower numeric precision.
  prefs: []
  type: TYPE_NORMAL
- en: Word embedding
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: One way to vectorize words in text-related neural networks. A word is mapped
    onto a 1D tensor (or vector) via an embedding lookup process. Unlike one-hot encoding,
    the word embedding involves nonsparse vectors in which the element values are
    continuous-varying numbers instead of 0s and 1s.
  prefs: []
  type: TYPE_NORMAL
- en: '![](ifc_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![](ibc_alt.jpg)'
  prefs: []
  type: TYPE_IMG
