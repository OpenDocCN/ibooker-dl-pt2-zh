- en: Glossary
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 术语表
- en: Activation function
  id: totrans-1
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 激活函数
- en: The function at the last stage of a neural network layer. For example, a rectified
    linear unit (relu) function may be applied on the result of the matrix multiplication
    to generate the final output of a dense layer. An activation function can be linear
    or nonlinear. Nonlinear activation functions can be used to increase the representational
    power (or capacity) of a neural network. Examples of nonlinear activations include
    sigmoid, hyperbolic tangent (tanh), and the aforementioned relu.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络层的最后阶段的函数。例如，可以在矩阵乘法的结果上应用修正线性单元（relu）函数，以生成密集层的最终输出。激活函数可以是线性或非线性的。非线性激活函数可用于增加神经网络的表示能力（或容量）。非线性激活函数的示例包括
    sigmoid、双曲正切（tanh）和前面提到的 relu。
- en: Area under the curve (AUC)
  id: totrans-3
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 曲线下面积（AUC）
- en: A single number used to quantify the shape of an ROC curve. It is defined as
    the definite integral under the ROC curve, from false positive rate 0 to 1\. See
    *ROC curve*.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 用于量化 ROC 曲线形状的单个数字。它被定义为 ROC 曲线下的定积分，从假阳性率 0 到 1。见 *ROC 曲线*。
- en: Axis
  id: totrans-5
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 轴
- en: In the context of TensorFlow.js, when we talk about a *tensor*, an axis (plural
    *axes*) is one of the independent keys indexing into the tensor. For example,
    a rank-3 tensor has three axes; an element of a rank-3 tensor is identified by
    three integers that correspond to the three axes. Also known as a *dimension*.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow.js 的上下文中，当我们谈论一个 *张量* 时，一个轴（复数 *轴*）是张量中独立索引的一个关键。例如，一个秩为 3 的张量有三个轴；秩为
    3 的张量的一个元素由三个整数标识，这些整数对应于三个轴。也称为 *维度*。
- en: Backpropagation
  id: totrans-7
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 反向传播
- en: The algorithm that traces back from the loss value of a differentiable machine-learning
    model to the gradients on the weight parameters. It is based on the chain rule
    of differentiation and forms the basis of training for most neural networks in
    this book.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 将可微分机器学习模型的损失值追溯到权重参数梯度的算法。它基于微分的链式法则，并构成了本书中大多数神经网络的训练基础。
- en: Backpropagation through time (BPTT)
  id: totrans-9
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 时间反向传播（BPTT）
- en: A special form of backpropagation in which the steps are not over the operations
    for the successive layers of a model, but instead over the operations for the
    successive time steps. It underlies the training of recurrent neural networks
    (RNNs).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一种特殊形式的反向传播，在其中步骤不是在模型的连续层的操作上进行，而是在连续时间步骤上进行的。它构成了递归神经网络（RNNs）的训练基础。
- en: Balance (dataset)
  id: totrans-11
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 平衡（数据集）
- en: A quality of a dataset with categorical labels. The more equal the numbers of
    examples from different categories are, the more balanced a dataset is.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 具有分类标签的数据集的一种质量。不同类别的示例数量越平衡，数据集就越平衡。
- en: Batch
  id: totrans-13
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 批次
- en: During the training of neural networks, multiple input examples are often aggregated
    to form a single tensor, which is used to calculate the gradients and updates
    to the network’s weights. Such an aggregation is called a *batch*. The number
    of examples in the batch is called the *batch size*.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络的训练过程中，通常将多个输入示例聚合成一个张量，该张量用于计算梯度和对网络权重的更新。这样的聚合称为 *批次*。批次中示例的数量称为 *批次大小*。
- en: Bellman equation
  id: totrans-15
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 贝尔曼方程
- en: 'In reinforcement learning, a recursive equation that quantifies the value of
    a state-action pair as a sum of two terms: 1) the reward the agent is expected
    to get immediately after the action and 2) the best expected reward the agent
    can get in the next state, discounted by a factor. The second term assumes optimal
    selection of action in the next state. It forms the basis of reinforcement-learning
    algorithms such as deep Q-learning.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，一种递归方程，将状态-动作对的价值量化为两项之和：1）代理在采取行动后立即获得的奖励；2）代理在下一个状态中可以获得的最佳期望奖励，乘以一个折现因子。第二项假定在下一个状态中选择的行动是最优的。它构成了强化学习算法（如深度
    Q 学习）的基础。
- en: Binary classification
  id: totrans-17
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 二元分类
- en: A classification task in which the target is the answer to a yes/no question,
    such as whether a certain X-ray image indicates pneumonia or whether a credit
    card transaction is legitimate or fraudulent.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一个分类任务，其中目标是回答是/否问题，例如某个 X 射线图像是否表明肺炎，或者信用卡交易是否合法或欺诈性。
- en: Broadcasting
  id: totrans-19
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 广播
- en: TensorFlow allows for pairwise operations between tensors with different but
    compatible shapes. For instance, it is possible to add a tensor of shape `[5]`
    to a tensor of shape `[13, 5]`. In effect, the smaller tensor will be repeated
    13 times to compute the output. The details for the rules of when broadcasting
    is allowed are in [info box 2.4](kindle_split_013.html#ch02sb04) in [chapter 2](kindle_split_013.html#ch02).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 允许对具有不同但兼容形状的张量进行成对操作。例如，可以将形状为 `[5]` 的张量添加到形状为 `[13, 5]` 的张量中。实际上，较小的张量将重复
    13 次以计算输出。关于何时允许广播的规则的详细信息在 [章节 2.4 的信息框](kindle_split_013.html#ch02sb04) 中有说明。
- en: Capacity
  id: totrans-21
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 容量
- en: The range of input-output relations that a machine-learning model is capable
    of learning. For example, a neural network with a hidden layer with a nonlinear
    activation function has a greater capacity than a linear-regression model.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型能够学习的输入-输出关系的范围。例如，具有具有非线性激活函数的隐藏层的神经网络比线性回归模型具有更大的容量。
- en: Class activation map
  id: totrans-23
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 类激活图
- en: An algorithm that can visualize the relative importance of different parts of
    an input image for the classification output of a convolutional neural network.
    It is based on computing the gradient of the final probability score of the winning
    class with respect to the output of the last internal convolutional layer of the
    network. It is discussed in detail in [section 7.2.3](kindle_split_019.html#ch07lev2sec5).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一种算法，可以可视化输入图像的不同部分对卷积神经网络分类输出的相对重要性。它基于计算网络的最后一个内部卷积层的输出对获胜类别的最终概率得分的梯度。详细讨论见
    [第 7.2.3 节](kindle_split_019.html#ch07lev2sec5)。
- en: Computer vision
  id: totrans-25
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 计算机视觉
- en: The study of how computers can understand images and videos. It is an important
    part of machine learning. In the context of machine learning, common computer-vision
    tasks include image recognition, segmentation, captioning, and object detection.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机如何理解图像和视频的研究。这是机器学习的重要组成部分。在机器学习的背景下，常见的计算机视觉任务包括图像识别、分割、字幕生成和目标检测。
- en: Confusion matrix
  id: totrans-27
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 混淆矩阵
- en: A square matrix (a 2D tensor) of the shape `[numClasses, numClasses]`. In multiclass
    classification, a confusion matrix is used to quantify how many times examples
    of a given truth class are classified as each of the possible classes. The element
    at indices `[i, j]` is the number of times examples from the true class `i` are
    classified as class `j`. The elements on the diagonal line correspond to correct
    classification results.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一个形状为 `[numClasses, numClasses]` 的方阵（2D 张量）。在多类分类中，混淆矩阵用于量化给定真实类别的示例被分类为每个可能类别的次数。索引
    `[i, j]` 处的元素是真实类别 `i` 的示例被分类为类别 `j` 的次数。对角线上的元素对应于正确分类的结果。
- en: Constant folding
  id: totrans-29
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 常量折叠
- en: A type of computation-graph optimization in which a subgraph that contains only
    predetermined constant nodes and deterministic operations among them is reduced
    to a single constant node. The `GraphModel` conversion technique in TensorFlow.js
    leverages constant folding.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一种计算图优化类型，其中包含仅由预定常量节点和它们之间的确定性操作组成的子图被减少为单个常量节点。TensorFlow.js 中的 `GraphModel`
    转换技术利用了常量折叠。
- en: Convolutional kernel
  id: totrans-31
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 卷积核
- en: 'In convolution operations, a tensor that operates on the input tensor to generate
    the output tensor. Take image tensors, for example: the kernel is usually smaller
    in its height and width dimensions compared to the input image. It is “slided”
    over the height and width dimensions of the input image and undergoes a dot product
    (multiply and add) at every sliding position. For a convolutional layer of TensorFlow.js
    (such as conv2d), the kernel is its key weight.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积操作中，一个张量对输入张量进行操作以生成输出张量。以图像张量为例：与输入图像相比，卷积核通常在其高度和宽度维度上较小。它在输入图像的高度和宽度维度上“滑动”，并在每个滑动位置上进行点积（乘法和加法）。对于
    TensorFlow.js 的卷积层（例如 conv2d），卷积核是其关键权重。
- en: Data augmentation
  id: totrans-33
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据增强
- en: The process of generating more training data from existing training samples
    (x, y) by creating mutations of the training samples via a family of programmatic
    transformations that yield valid inputs x' without changing the target. This helps
    expose the model to more aspects of the data and thus generalize better without
    the engineer having to manually build invariance to these types of transformations
    into the model.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 通过创建训练样本（x，y）的变异来从现有训练样本中生成更多训练数据的过程，通过产生有效输入x'的一系列编程转换来暴露模型更多数据的方面，从而更好地泛化，而不需要工程师手动将这些转换类型的不变性构建到模型中。
- en: Deep learning
  id: totrans-35
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 深度学习
- en: The study and application of deep neural networks (that is, using a large number
    of successive representational transformations to solve machine-learning problems).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络的研究和应用（即使用大量连续的表示转换来解决机器学习问题）。
- en: Deep neural network
  id: totrans-37
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 深度神经网络
- en: A neural network with a large number (anywhere between two and thousands) of
    layers.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 具有大量层（从两个到数千个）的神经网络。
- en: Dimension
  id: totrans-39
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 维度
- en: In the context of a tensor, synonymous with *axis*. See *axis*.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在张量的上下文中，与*轴*同义。 参见*轴*。
- en: Dot product
  id: totrans-41
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 点积
- en: See *inner product*.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 参见*内积*。
- en: Embedding
  id: totrans-43
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 嵌入
- en: 'In deep learning, a representation of a certain piece of data in an *n*-dimensional
    vector space (*n* being a positive integer). In other words, it is a representation
    of a piece of data as an ordered, length-*n* array of floating-point numbers.
    Embedding representations can be created for many types of data: images, sounds,
    words, and items from a closed set. An embedding is usually from an intermediate
    layer of a trained neural network.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，将某个数据片段表示为*n*维向量空间（*n*为正整数）的表示。 换句话说，它是将数据片段表示为有序的，长度为*n*的浮点数数组。 可以为许多类型的数据创建嵌入表示：图像，声音，单词以及来自封闭集的项目。
    嵌入通常来自训练的神经网络的中间层。
- en: Ensemble learning
  id: totrans-45
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 集成学习
- en: The practice of training a number of individual machine-learning models and
    using them together for inference on the same problem. Even though each individual
    model may not be very accurate, the ensemble model can have a much higher accuracy.
    Ensemble models are often used by the winning entries of data science competitions,
    such as Kaggle competitions.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一些个体机器学习模型并在同一问题上一起使用它们进行推断的实践。 即使每个单独的模型可能不太准确，但集成模型的准确性可能会更高。 集成模型经常被数据科学竞赛的获奖作品使用，例如Kaggle竞赛。
- en: Epoch
  id: totrans-47
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 纪元
- en: When training a model, one complete pass through the training data.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型时，对训练数据的完整通过。
- en: Epsilon-greedy policy
  id: totrans-49
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Epsilon-贪婪策略
- en: In reinforcement learning, an action-selection method that parametrizes the
    balance between random exploratory behavior and optimal behavior on the part of
    the agent. The value of epsilon is constrained between 0 and 1\. The higher it
    is, the more likely the agent is to select random actions.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，一种参数化代理方的随机探索行为与最优行为之间平衡的动作选择方法。 epsilon的值受到0和1之间的约束。 它越高，代理选择随机动作的可能性就越大。
- en: Example
  id: totrans-51
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例
- en: In the context of machine learning, an individual instance of input data (for
    example, an image of the appropriate size for a computer-vision model), for which
    a machine-learning model will generate an output prediction (such as a label for
    the image).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习的上下文中，输入数据的个体实例（例如，适用于计算机视觉模型的图像），机器学习模型将为其生成输出预测（例如，图像的标签）。
- en: Feature
  id: totrans-53
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 特征
- en: 'One aspect of the input data for a machine-learning model. A feature can be
    in any of the following forms:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型的输入数据的一个方面。 特征可以采用以下任何形式：
- en: A number (for example, the monetary amount of a credit card transaction)
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数字（例如信用卡交易的货币金额）
- en: A string from an open set (name of transaction)
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自开放集合的字符串（交易名称）
- en: A piece of categorical information (such as the brand name of the credit card)
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类信息的一部分（例如信用卡品牌名称）
- en: A one- or multidimensional array of numbers (for instance, a grayscale image
    of the credit card customer’s signature represented as a 2D array)
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个或多维数组（例如，信用卡客户签名的灰度图像表示为2D数组）
- en: Other types of information (for example, date-time)
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他类型的信息（例如日期时间）
- en: An input example can consist of one or multiple features.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 输入示例可以由一个或多个特征组成。
- en: Feature engineering
  id: totrans-61
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 特征工程
- en: The process of transforming the original features in input data into a representation
    more amenable to solving the machine-learning problem. Before deep learning, feature
    engineering was performed by engineers with domain-specific knowledge through
    trial and error. It was often a labor-intensive and brittle process, without any
    guarantee of finding the optimal solution. Deep learning has largely automated
    feature engineering.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 原始特征数据的转化过程，使其更易于解决机器学习问题。在深度学习之前，通过领域专家进行试错的特征工程。这通常是一个耗时且脆弱的过程，没有找到最优解的保证。深度学习在很大程度上自动化了特征工程。
- en: Fine-tuning
  id: totrans-63
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 微调
- en: In transfer learning, a phase of model training during which the weights in
    some layers of the base model are allowed to be updated. It usually follows an
    initial phase of model training during which all weights in the base model are
    frozen to prevent large initial gradients from perturbing the pretrained weights
    too much. When used properly, fine-tuning can boost the capacity of the transfer-learning
    model, thereby achieving superior accuracy while consuming significantly less
    computation resources than training an entire model from scratch.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在迁移学习中，模型训练的一个阶段，在该阶段，基本模型中某些层的权重可以更新。通常，在全部基本模型的权重都被冻结以防止大的初始梯度干扰预训练权重的初始阶段之后。如果使用得当，微调可以增强迁移学习模型的能力，从而在消耗比完全从头开始训练模型少得多的计算资源的同时实现更高的准确性。
- en: Generative adversarial network (GAN)
  id: totrans-65
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 生成式对抗网络（GAN）
- en: A type of generative machine-learning model that involves two parts called the
    *discriminator* and the *generator*. The discriminator is trained to distinguish
    real examples from a training set from fake ones, while the generator is trained
    to output examples that cause the discriminator to output high realness scores
    (that is, to “fool” the discriminator into “thinking” that the fake examples are
    real). After proper training, the generator is capable of outputting highly realistic
    fake examples.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式机器学习模型的一种类型，包括两个部分：鉴别器和生成器。鉴别器被训练为区分真实样本和训练集中的假样本，而生成器被训练为输出能使鉴别器给出高真实度评分的样本（即“欺骗”鉴别器，“使其认为”这些假样本是真实的）。经过适当的训练，生成器能够输出高度逼真的假样本。
- en: Golden value
  id: totrans-67
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 金值
- en: In the context of testing a machine-learning system, the correct output a model
    should generate for a given input. An example is the “classical” label for a neural
    network that classifies audio recordings into genres of music when given a recording
    of Beethoven’s Fifth Symphony.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试机器学习系统时，模型对于给定输入应该生成的正确输出。例如，当给出贝多芬第五交响曲的录音时，神经网络将其分类为音乐流派时的“标准标签”就是一个例子。
- en: Gradient descent
  id: totrans-69
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 梯度下降
- en: The process of minimizing the numerical output value of a system by iteratively
    changing the parameters of the system along the direction of the gradients (that
    is, derivatives of the parameters with respect to the output value). It is the
    primary way in which neural networks are trained. In the context of neural network
    training, the system is formed by the neural network and a loss function selected
    by the engineer. The parameters of the system are the weights of the neural network’s
    layers. The iteration process happens batch-by-batch over the training data.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降法是通过沿着系统参数的梯度方向（即与输出值相关的参数的导数）反复改变系统参数，将系统数值输出最小化的过程。这是神经网络训练的主要方法。在神经网络训练的上下文中，系统由工程师选择的神经网络和损失函数组成。系统的参数是神经网络层的权重，迭代过程逐批次地在训练数据上进行。
- en: Graphics processing unit (GPU)
  id: totrans-71
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图形处理单元（GPU）
- en: 'Parallel-computing chips equipped with a much larger number (hundreds or thousands)
    of cores than typical CPUs. GPUs were originally designed to accelerate the computation
    and rendering of 2D and 3D graphics. But they turned out to be useful for the
    kind of parallel computing involved in running deep neural networks as well. GPUs
    are an important contributing factor to the deep-learning revolution and continue
    to play critical roles in the research and applications of deep learning today.
    TensorFlow.js harnesses the parallel-computing power of GPUs through two conduits:
    1) the WebGL API of the web browser and 2) binding to the TensorFlow CUDA kernels
    in Node.js.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 配备比典型CPU核心多得多（数百或数千个）的并行计算芯片。 GPU最初被设计用于加速2D和3D图形的计算和渲染。但它们也被证明对于运行深度神经网络所涉及的并行计算非常有用。GPU是深度学习革命的重要因素，并且在今天深度学习的研究和应用中继续发挥关键作用。TensorFlow.js通过两个渠道利用GPU的并行计算能力：1）web浏览器的WebGL
    API，2）在Node.js中绑定到TensorFlow CUDA核心。
- en: GraphModel
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: GraphModel
- en: In TensorFlow.js, a model converted from TensorFlow (Python) and loaded into
    JavaScript. `GraphModel` has the potential to undergo TensorFlow-internal performance
    optimizations such as Grappler’s arithmetic optimization and op fusion (see [section
    12.2.2](kindle_split_025.html#ch12lev2sec5) for details).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow.js中，从TensorFlow（Python）转换并加载到JavaScript中的模型。 `GraphModel` 有潜力进行TensorFlow内部性能优化，比如Grappler的算术优化和op融合（详见[第12.2.2节](kindle_split_025.html#ch12lev2sec5)）。
- en: Hidden layer
  id: totrans-75
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 隐藏层
- en: A neural network that consists of a layer whose output is not exposed as an
    output of the network but is instead consumed only by other layers of the network.
    For example, in a neural network defined as a TensorFlow.js sequential model,
    all layers except the last one are hidden layers.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 由一个输出不作为网络的输出暴露，而是只被网络的其他层消耗的层组成的神经网络。例如，在一个定义为TensorFlow.js顺序模型的神经网络中，除了最后一个层外，所有层都是隐藏层。
- en: Hyperparameter optimization
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 超参数优化
- en: Sometimes also called *hyperparameter tuning*; the process of searching for
    the set of hyperparameters that gives the lowest validation loss on a given machine-learning
    task.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 有时也称为*超参数调优*；搜索在给定机器学习任务上给出最低验证损失的超参数集的过程。
- en: Hyperparameters
  id: totrans-79
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 超参数
- en: Tunable parameters of the model and optimizer that are not tunable with backpropagation.
    Typically, the learning rate and model structure are common example hyperparameters.
    Hyperparameters may be tuned by grid search or more sophisticated hyperparameter-tuning
    algorithms.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 模型和优化器的可调参数，这些参数不能通过反向传播进行调整。通常学习率和模型结构是常见的超参数示例。超参数可以通过网格搜索或更复杂的超参数调优算法进行调整。
- en: Hypothesis space
  id: totrans-81
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 假设空间
- en: In the context of machine learning, the set of possible solutions to a machine-learning
    problem. The training process involves searching for a good solution in such a
    space. The hypothesis space is determined by the type and the architecture of
    the machine-learning model chosen to solve the problem.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习的上下文中，机器学习问题的可能解集。训练过程涉及在这样的空间中搜索一个良好的解。假设空间由解决问题选择的机器学习模型的类型和架构决定。
- en: ImageNet
  id: totrans-83
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ImageNet
- en: A large-scale public dataset of labeled colored images. It is an important training
    set and benchmark for computer-vision-oriented deep neural networks. ImageNet
    was instrumental in ushering in the beginning of the deep-learning revolution.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 由标记的彩色图像组成的大规模公共数据集。这是计算机视觉导向的深度神经网络的重要训练集和基准。ImageNet在引领深度学习革命的开端方面发挥了重要作用。
- en: Imputation
  id: totrans-85
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 填补
- en: A technique for filling in missing values from a dataset. For instance, if we
    had a dataset of cars, and some cars were missing their “weight” feature, we might
    simply guess the average weight for those features. More sophisticated imputation
    techniques are also possible.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一种从数据集中填补缺失值的技术。例如，如果我们有一个汽车数据集，有些汽车缺少“重量”特征，我们可以简单地猜测这些特征的平均重量。也可以使用更复杂的填补技术。
- en: Inception
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 形式
- en: A type of deep convolutional neural network featuring a large number of layers
    and a complex network structure.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 一种有大量层和复杂网络结构的深度卷积神经网络。
- en: Independent and identically distributed (IID)
  id: totrans-89
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 独立同分布（IID）
- en: A statistical property of data samples. If we assume that data is sampled from
    an underlying distribution, then the samples are identically distributed if each
    sample comes from the same distribution. Samples are independent if knowing the
    value of one sample gives you no additional information about the next sample.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 数据样本的统计属性。如果我们假设数据是从一个潜在的分布中抽样得到的，那么如果每个样本来自相同的分布，则样本是相同分布的。如果知道一个样本的值不会给你关于下一个样本的额外信息，那么样本是独立的。
- en: A sample of dice rolls is an example of an IID collection of samples. If the
    dice rolls are sorted, the samples are identically distributed but not independent.
    Training data should be IID, or there is likely to be convergence or other issues
    during training.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 掷骰子的样本是IID（独立同分布）样本集的一个示例。如果骰子的结果被排序，那么样本是相同分布的但不是独立的。训练数据应该是IID，否则在训练过程中可能会出现收敛或其他问题。
- en: Inference
  id: totrans-92
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 推断
- en: Using a machine-learning model on input data to generate an output. It is the
    ultimate purpose of training the model.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对输入数据使用机器学习模型生成输出。这是训练模型的最终目的。
- en: Inner product
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 内积
- en: Also known as *dot product*; a mathematical operation on two vectors of equivalent
    shape, yielding a single scalar value. To calculate the inner product between
    vectors `a` and `b`, sum up all `a[i] * b[i]` for all valid values of `i`. In
    geometric terms, the inner product of two vectors is equal to the product of their
    magnitudes and the cosine of the angle between them.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 也称为*点积*；是两个形状相同的向量上的数学运算，得到一个单一的标量值。要计算向量`a`和`b`之间的内积，对所有有效值的`i`求和`a[i] * b[i]`。从几何角度来看，两个向量的内积等于它们的大小乘积和它们之间的余弦值。
- en: Keras
  id: totrans-96
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Keras
- en: A popular library for deep learning. Today, it is the most frequently used deep-learning
    library in Kaggle competitions. François Chollet, currently a software engineer
    at Google, is its original author. Keras is a Python library. The high-level API
    of TensorFlow.js, which is a main focus of this book, is modeled after and compatible
    with Keras.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一种流行的深度学习库。今天，在 Kaggle 竞赛中它是使用最频繁的深度学习库。弗朗索瓦·肖莱（François Chollet）是其原始作者，目前是
    Google 的软件工程师。Keras 是一个 Python 库。TensorFlow.js 的高级 API，这是本书的重点，是基于 Keras 建模并与之兼容的。
- en: Label
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 标签
- en: The desired answer for an input example given the task at hand. A label can
    be a Boolean (yes/no) answer, a number, a text string, a category among a number
    of possible categories, a sequence of numbers, or more complex data types. In
    supervised machine learning, a model aims at generating outputs that closely match
    the labels.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 根据手头的任务给定输入示例的期望答案。标签可以是布尔（是/否）答案、数字、文本字符串、一系列可能的类别中的一个、一系列数字或更复杂的数据类型。在监督式机器学习中，模型的目标是生成与标签尽可能匹配的输出。
- en: Layer
  id: totrans-100
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 层
- en: 'In the context of neural networks, a transformation of the data representation.
    It behaves like a mathematical function: given an input, it emits an output. A
    layer can have state captured by its weights. The weights can be altered during
    the training of the neural network.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络的上下文中，数据表示的转换。它的行为类似于数学函数：给定一个输入，它产生一个输出。一个层可以有由它的权重捕获的状态。这些权重可以在神经网络的训练过程中被改变。
- en: LayersModel
  id: totrans-102
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: LayersModel
- en: A model built using the Keras-like high-level API of TensorFlow.js. It can also
    be loaded from a converted Keras (Python) model. A `LayersModel` supports inference
    (with its `predict()` method) and training (with its `fit()` and `fitDataset()`
    methods).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 TensorFlow.js 的类似 Keras 的高级 API 构建的模型。它也可以从转换后的 Keras（Python）模型加载。`LayersModel`支持推断（使用其`predict()`方法）和训练（使用其`fit()`和`fitDataset()`方法）。
- en: Learning rate
  id: totrans-104
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 学习率
- en: During gradient descent, model weights are modified to reduce loss. The exact
    change in the weights is a function not only of the gradient of the loss but also
    of a parameter. In the standard gradient-descent algorithm, the weight update
    is calculated by multiplying the gradient by the learning rate, which is typically
    a small positive constant. The default learning rate for the `'sgd'` optimizer
    in tensorflow.js is 0.01.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在梯度下降期间，模型权重被修改以减少损失。权重的确切变化不仅取决于损失的梯度，还取决于一个参数。在标准梯度下降算法中，通过将梯度乘以学习率来计算权重更新，学习率通常是一个小正常数。tensorflow.js
    中 `'sgd'` 优化器的默认学习率是 0.01。
- en: Local minimum
  id: totrans-106
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 局部最小值
- en: When optimizing the parameters of a model, a setting of the parameters for which
    any sufficiently small change in the parameters always increases the loss. Similar
    to a marble at the bottom of a bowl, there is no small movement that is even lower.
    A local minimum is distinguished from a *global minimum* in that a local minimum
    is the lowest point in the local neighborhood, but the global minimum is the lowest
    point overall.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在优化模型参数时，参数的设置使得参数的任何足够小的变化都会增加损失。类似于碗底的大理石，没有任何更低的小运动。局部最小值与*全局最小值*有所区别，局部最小值是局部邻域中的最低点，而全局最小值是整体上的最低点。
- en: Logit
  id: totrans-108
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 对数几率
- en: In machine learning, an unnormalized probability value. Unlike probabilities,
    logits are not limited to the [0, 1] interval or required to sum to 1\. Hence,
    they can be more easily output by a neural network layer. A set of logits can
    be normalized to probability values through an operation called *softmax*.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，一个未标准化的概率值。与概率不同，对数几率不限于[0,1]区间，也不要求总和为1。因此，它们可以更轻松地由神经网络层输出。一组对数几率可以通过称为*softmax*的操作标准化为概率值。
- en: Machine learning
  id: totrans-110
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 机器学习
- en: A subfield of artificial intelligence (AI) that automates the discovery of rules
    for solving complex problems by using data labeled with the desired answers. It
    differs from classical programming in that no handcrafting of the rules is involved.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 一种人工智能（AI）的子领域，通过使用带有所需答案标签的数据自动发现解决复杂问题的规则。它与经典编程不同，因为它不涉及规则的手工制作。
- en: Markov decision process (MDP)
  id: totrans-112
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程（MDP）
- en: In reinforcement learning, a decision process in which the current state and
    the action selected by the agent completely determine the next state that the
    agent will end up with and the reward the agent will receive at the step. It is
    an important simplification that enables learning algorithms such as Q-learning.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，一种决策过程，其中智能体选择的当前状态和动作完全决定了智能体将结束的下一个状态以及智能体将在该步骤中获得的奖励。这是一个重要的简化，使得像Q-learning这样的学习算法成为可能。
- en: Model
  id: totrans-114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型
- en: In machine learning and deep learning, an object that transforms input data
    (such as an image) into the desired output (such as a text label for the image)
    through a number of successive mathematical operations. A model has parameters
    (called *weights*) that can be tuned during training.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习和深度学习中，将输入数据（例如图像）转换为所需输出（例如图像的文本标签）的对象，通过一系列连续的数学操作。模型具有可以在训练期间调整的参数（称为*权重*）。
- en: Model adaptation
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型适应
- en: The process of training a pretrained model or a part of it in order to make
    the model achieve better accuracy during inference on the input data from a specific
    user or specific use case. It is a type of transfer learning, one in which the
    types of the input features and the type of the target don’t differ from the original
    model.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 训练预训练模型或其部分的过程，以使模型在特定用户或特定用例的输入数据上进行推理时达到更好的准确性。这是一种迁移学习的类型，其中输入特征的类型和目标的类型与原始模型不同。
- en: Model deployment
  id: totrans-118
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型部署
- en: The process of packaging a trained model to the place where it can be used for
    making predictions. Similar to “pushing to production” for other software stacks,
    deployment is how users can get to use models “for real.”
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 将训练好的模型打包到可以用于进行预测的地方的过程。类似于其他软件堆栈的“推向生产”，部署是用户可以真正使用模型的方式。
- en: MobileNet
  id: totrans-120
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: MobileNet
- en: A pretrained deep convolutional neural network. It is typically trained on the
    ImageNet image-classification dataset and can be used for transfer learning. Among
    similar pretrained convolutional neural networks, it has a relatively small size
    and involves less computation to perform inference, and is therefore more suitable
    to run in a resource-restricted environment such as the web browser, with TensorFlow.js.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 一个预训练的深度卷积神经网络。通常在ImageNet图像分类数据集上进行训练，并可用于迁移学习。在类似的预训练卷积神经网络中，它具有相对较小的尺寸，并且在执行推理时涉及的计算较少，因此更适合在资源受限的环境（如Web浏览器）中运行，使用TensorFlow.js。
- en: Multiclass classification
  id: totrans-122
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 多类分类
- en: A classification problem in which the target may take more than two discrete
    labels. Examples are what kind of animal a picture contains or what (natural)
    language a web page is in given its content.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 一个分类问题，其中目标可能具有两个以上的离散标签。例如，一张图片包含什么样的动物或给定内容的网页所使用的（自然）语言是什么。
- en: Multi-hot encoding
  id: totrans-124
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 多热编码
- en: A way to represent the words in a sentence (or, in general, the items in a sequence)
    as a vector by setting the elements that correspond to the words to 1 and leaving
    the rest as 0\. This can be viewed as a generalization of *one-hot encoding*.
    It discards the information regarding the order of the words.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一种通过将与单词对应的元素设置为1并将其余元素保持为0来表示句子中的单词（或一般情况下，序列中的项目）的方法。这可以看作是*独热编码*的泛化。它丢弃了关于单词顺序的信息。
- en: Multilayer perceptron (MLP)
  id: totrans-126
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 多层感知器（MLP）
- en: A neural network consisting of feedforward topology and at least one hidden
    layer.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 由前馈拓扑和至少一个隐藏层组成的神经网络。
- en: Natural language processing
  id: totrans-128
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 自然语言处理
- en: The subfield of computer science that studies how to use computers to process
    and understand natural language, most prominently text and speech. Deep learning
    finds many applications in natural language processing.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学的一个子领域，研究如何利用计算机来处理和理解自然语言，最突出的是文本和语音。深度学习在自然语言处理中有许多应用。
- en: Neural network
  id: totrans-130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 神经网络
- en: A category of machine-learning models inspired by the layered organization seen
    in biological neural systems. The layers of a neural network perform multistep,
    separable transformations of the data representation.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 一类受生物神经系统中分层组织启发的机器学习模型。神经网络的层对数据表示进行多步、可分离的转换。
- en: Nonlinearity
  id: totrans-132
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 非线性
- en: An input-output relation that does not meet the definition of linearity (linear
    combinations of inputs lead to a linear combination of the outputs, up to a constant-term
    difference). In neural networks, nonlinear relations (such as sigmoid and relu
    activations in layers) and the cascading of multiple such relations can increase
    the capacity of the neural networks.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 一个输入输出关系，不符合线性定义（输入的线性组合导致输出的线性组合，最多存在一个常数项的差异）。在神经网络中，非线性关系（例如在层中的sigmoid和relu激活）以及多个这样的关系的级联可以增加神经网络的容量。
- en: Object detection
  id: totrans-134
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 目标检测
- en: A computer-vision task that involves detecting certain classes of objects and
    their location in an image.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 一种计算机视觉任务，涉及在图像中检测某些类别的对象及其位置。
- en: One-hot encoding
  id: totrans-136
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 独热编码
- en: The scheme of encoding categorical data as a vector of length *N* consisting
    of all zeros except at the index that corresponds to the actual class.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 将分类数据编码为长度为*N*的向量的方案，该向量由除了对应于实际类别的索引之外的所有零组成。
- en: Op fusion
  id: totrans-138
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 操作融合
- en: A computation-graph optimization technique in which multiple operations (or
    ops) are replaced with a single equivalent op. Op fusion reduces the op-dispatching
    overhead and can lead to more opportunities for further intra-op memory and performance
    optimization.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 一种计算图优化技术，其中多个操作（或ops）被替换为一个等效的操作。操作融合减少了操作分派开销，并可以为进一步的操作内存和性能优化提供更多机会。
- en: Out-of-vocabulary (OOV)
  id: totrans-140
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 超出词汇表（OOV）
- en: In the context of deep learning, when a *vocabulary* is used on a set of discrete
    items, the vocabulary sometimes doesn’t include all possible items. When an item
    outside the vocabulary is encountered, it is mapped to a special index called
    out-of-vocabulary, which can then be mapped to a special element in the one-hot
    encoding or embedding representation. See *vocabulary*.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习的上下文中，当一个*词汇表*用于一组离散项时，该词汇表有时不包括所有可能的项。当遇到词汇表之外的项时，它被映射到一个称为超出词汇表的特殊索引，然后可以将其映射到独热编码或嵌入表示中的特殊元素。见*词汇表*。
- en: Overfitting
  id: totrans-142
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 过拟合
- en: When a model is fit to the training data in such a way that the model has sufficient
    capacity to memorize the training data, we see the training loss continue to go
    down, but the testing or validation loss starts to rise. Models with this property
    begin to lose their ability to generalize and perform well only on the exact samples
    in the training data. We say models in this circumstance are overfit.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型被适配到训练数据中，以至于模型具有足够的容量来记忆训练数据时，我们会看到训练损失继续下降，但测试或验证损失开始上升。具有这种属性的模型开始失去泛化能力，仅在训练数据中的确切样本上表现良好。我们称处于这种情况下的模型为过拟合。
- en: Policy gradients
  id: totrans-144
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 策略梯度
- en: A type of reinforcement-learning algorithm that computes and utilizes the gradients
    of certain measures (such as logits) of selected actions with respect to the weights
    of a policy network in order to cause the policy network to gradually select better
    actions.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 一种强化学习算法，它计算并利用选定动作的某些度量（如对数几率）相对于策略网络的权重的梯度，以使策略网络逐渐选择更好的动作。
- en: Precision
  id: totrans-146
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 精确度
- en: A metric of a binary classifier, defined as the ratio of the examples labeled
    by the classifier as positive that are actually positive. See *recall*.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 一个二元分类器的度量，定义为分类器标记为正且实际为正的比率。参见 *召回率*。
- en: Pseudo examples
  id: totrans-148
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 伪示例
- en: Additional examples based on known valid mutations of input training examples,
    used to supplement the training data. For instance, we might take the MNIST digits
    and apply small rotations and skews. These transformations do not change the image
    label.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 基于已知有效的输入训练示例的突变的附加示例，用于补充训练数据。例如，我们可以取MNIST数字并应用小的旋转和倾斜。这些变换不会改变图像标签。
- en: Q-network
  id: totrans-150
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Q网络
- en: In reinforcement learning, a neural network that predicts the Q-values of all
    possible actions given the current state observation. The Q-learning algorithm
    is about training a Q-network using data from the agent’s experience.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，预测给定当前状态观察到所有可能行动的Q值的神经网络。Q学习算法是关于使用代理经验数据训练Q网络的过程。
- en: Q-value
  id: totrans-152
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Q值
- en: In reinforcement learning, the expected total future cumulative reward for taking
    an action at a given state. Hence a Q-value is a function of action and state.
    It guides the selection of actions in Q-learning.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，以给定状态下采取行动的预期总未来累积奖励。因此，一个Q值是一个行动和状态的函数。它指导了在Q学习中选择行动的过程。
- en: Random initialization
  id: totrans-154
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 随机初始化
- en: Before a model is fit, the process of assigning the weights an initial value
    as a starting point. There is much literature on what, exactly, are good distributions
    to choose from for the initial values based on the layer type, size, and task.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型拟合之前，为权重分配初始值作为起点的过程。关于什么，具体来说，是好的分布选择以获取初始值的文献基于层类型、大小和任务有很多。
- en: Recall
  id: totrans-156
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 回想
- en: A metric of a binary classifier, defined as the ratio of the actual examples
    that are labeled by the classifier as positive. See *precision*.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 一个二元分类器的度量，定义为分类器标记为正的实际示例的比率。参见 *精度*。
- en: Regression
  id: totrans-158
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 回归
- en: A type of learning problem where the desired output (or label) is a number or
    list of numbers. Making predictions that are numerically closer to the expected
    output is better.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 一种学习问题，期望输出（或标签）是一个数字或数字列表。预测越接近期望输出，越好。
- en: Regularization
  id: totrans-160
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 正则化
- en: In machine learning, the process of imposing various modifications to the loss
    function or the training process in order to counteract overfitting. There are
    several ways to perform regularization, the most frequently used of which are
    L1 and L2 regularization of weights.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，对损失函数或训练过程进行各种修改以抵消过度拟合的过程。有几种方式可以执行正则化，其中最常用的是L1和L2正则化。
- en: Reinforcement learning (RL)
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 强化学习（RL）
- en: A type of machine learning that involves learning optimal decisions that maximize
    a metric called a *reward* through interacting with an environment. [Chapter 11](kindle_split_023.html#ch11)
    of this book covers the basics of RL and how to solve simple RL problems using
    deep-learning techniques.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 一种涉及通过与环境交互来学习最优决策以最大化一个叫做*奖励*的度量的机器学习方法。本书的第11章介绍了RL的基础知识以及如何使用深度学习技术解决简单的RL问题。
- en: ResNet
  id: totrans-164
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ResNet
- en: Short for *residual network*; a popular convolutional network widely used in
    computer vision, featuring residual connections—that is, connections that skip
    layers.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Residual Network的缩写；一种广泛用于计算机视觉的流行卷积网络，具有残差连接，即跳过层之间的连接。
- en: ROC curve
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ROC曲线
- en: A way to visualize the trade-off between the true positive rate (recall) and
    the false positive rate (false-alarm rate) of a binary classifier. The name of
    the curve (the *receiver operating characteristics curve*) originated from the
    early days of radar technology. See *area under the curve* (AUC).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化二元分类器真正的阳性率（召回率）和假阳性率（误报率）之间的权衡的方法。该曲线的名称（接受者操作特征曲线）源于雷达技术的早期阶段。参见 *曲线下面积*（AUC）。
- en: Spectrogram
  id: totrans-168
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 频谱图
- en: 'An image-like 2D representation of 1D time signals such as sounds. A spectrogram
    has two dimensions: time and frequency. Each element represents the intensity
    or power the sound contains in a given frequency range at a given moment in time.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 一种类似于图像的二维表示，用于表示一维时间信号（例如声音）。频谱图有两个维度：时间和频率。每个元素表示在给定时间内在给定频率范围内所包含声音的强度或功率。
- en: Supervised learning
  id: totrans-170
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 监督学习
- en: The paradigm of training a machine-learning model using labeled examples. The
    internal parameters of the model are altered in a way that minimizes the difference
    between the model’s output for the examples and the corresponding actual labels.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 使用标记示例训练机器学习模型的范例。模型的内部参数被改变，以最小化模型对示例的输出与相应实际标签之间的差异。
- en: Symbolic tensor
  id: totrans-172
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 符号张量
- en: In TensorFlow.js, an object of the `SymbolicTensor` class that is a specification
    for the shape and data type (dtype) of a tensor. Unlike a tensor, a `SymbolicTensor`
    object is not associated with concrete values. Instead, it is used as a placeholder
    for the input or output of a layer or a model.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow.js 中，`SymbolicTensor` 类的对象，它是张量的形状和数据类型（dtype）的规范。与张量不同，`SymbolicTensor`
    对象没有与具体值相关联。相反，它被用作层或模型的输入或输出的占位符。
- en: Tensor
  id: totrans-174
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 张量
- en: 'A data structure for holding data elements, usually numbers. Tensors can be
    thought of as *n*-dimensional grids, where each position in the grid holds exactly
    one element. The number of dimensions and size of each dimension is called the
    tensor’s *shape*. For instance, a 3 × 4 matrix is a tensor with shape `[3, 4]`.
    A vector of length 10 is a 1D tensor with shape `[10]`. Each tensor instance holds
    only one type of element. Tensors are designed this way because it allows for
    convenient, highly efficient implementations of common operations necessary for
    deep learning: for instance, matrix dot products.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 一种用于保存数据元素的数据结构，通常是数字。张量可以被视为 *n* 维网格，其中网格中的每个位置恰好保存一个元素。张量的维数和每个维度的大小被称为张量的
    *形状*。例如，一个 3 × 4 矩阵是一个形状为 `[3, 4]` 的张量。长度为 10 的向量是一个形状为 `[10]` 的一维张量。每个张量实例只保存一种类型的元素。张量是这样设计的，因为它允许方便、高效的实现深度学习中常见操作：例如，矩阵点积。
- en: TensorBoard
  id: totrans-176
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: TensorBoard
- en: A monitoring and visualization tool for TensorFlow. It allows users to visualize
    model structure and training performance in the browser. TensorFlow.js can write
    training logs in a data format compatible with TensorBoard.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 一个用于 TensorFlow 的监控和可视化工具。它允许用户在浏览器中可视化模型结构和训练性能。TensorFlow.js 可以将训练日志写入与 TensorBoard
    兼容的数据格式。
- en: TensorFlow
  id: totrans-178
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: TensorFlow
- en: An open source Python library for accelerated machine learning, with a focus
    on deep neural networks. It was released by Google’s Brain team in November 2015\.
    Its API forms a blueprint for that of TensorFlow.js.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 一个用于加速机器学习的开源 Python 库，重点放在深度神经网络上。它由 Google 的 Brain 团队于 2015 年 11 月发布。其 API
    构成了 TensorFlow.js 的蓝图。
- en: Training
  id: totrans-180
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 训练
- en: The process of altering a machine-learning model’s internal parameters (weights)
    to make the model’s outputs more closely match the desired answers.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 改变机器学习模型的内部参数（权重），使模型的输出更接近期望的答案的过程。
- en: Training data
  id: totrans-182
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 训练数据
- en: The data that is used to train a machine-learning model. Training data consists
    of individual examples. Each example is structured information (for example, images,
    audio, or text) in conjunction with the expected answer (the label).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练机器学习模型的数据。训练数据由各个示例组成。每个示例都是结构化信息（例如，图像、音频或文本），与预期答案（标签）一起。
- en: Transfer learning
  id: totrans-184
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 迁移学习
- en: The practice of taking a machine-learning model previously trained for one task,
    retraining it with a relatively small amount of data (compared to the original
    training dataset) for a new task, and using it for inference on the new task.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 将之前针对一个任务训练过的机器学习模型，重新训练它以适应一个新任务的相对较少的数据量（与原始训练数据集相比），并在新任务上进行推断的实践。
- en: Underfitting
  id: totrans-186
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 欠拟合
- en: When a model is trained for too few optimization steps, or a model has an insufficient
    representational power (capacity) to learn the patterns in the training data,
    which results in a model that does not reach a decent level of quality, we say
    that the model is underfit.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个模型经过太少的优化步骤训练，或者一个模型的表示能力（容量）不足以学习训练数据中的模式时，导致模型不能达到一个体面的质量水平时，我们称该模型为欠拟合。
- en: Unsupervised learning
  id: totrans-188
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 无监督学习
- en: The paradigm of machine learning that uses unlabeled data. It is opposed to
    supervised learning, which uses labeled data. Examples of unsupervised learning
    include clustering (discovering distinct subsets of examples in the dataset) and
    anomaly detection (determining if a given example is sufficiently different from
    the examples in the training set).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 使用未标记数据的机器学习范式。与使用标记数据的监督学习相对。无监督学习的示例包括聚类（在数据集中发现不同子集的示例）和异常检测（确定给定示例与训练集中的示例是否足够不同）。
- en: Validation data
  id: totrans-190
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 验证数据
- en: Data that is set apart from training data for the tuning of hyperparameters,
    such as the learning rate or the number of units in a dense layer. Validation
    data allows us to tune our learning algorithm, possibly running training many
    times. Since validation data is also separate from testing data, we can still
    rely on the result from the test data to give us an unbiased estimate of how our
    model will perform on new, unseen data.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 用于调整超参数的训练数据以外的数据，例如学习率或密集层中的单元数。验证数据允许我们调整学习算法，可能需要多次运行训练。由于验证数据与测试数据是分开的，我们仍然可以依靠测试数据的结果来提供无偏的模型在新的、未见过的数据上的性能估计。
- en: Vanishing-gradient problem
  id: totrans-192
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 梯度消失问题
- en: A classic problem in training deep neural networks in which the gradients on
    the weight parameter get increasingly smaller as the number of layers gets larger,
    and the weight parameters get farther and farther apart from the loss function
    as a result. In modern deep learning, this problem is mitigated through improved
    activation functions, proper initialization of weights, and other tricks.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练深度神经网络中的一个经典问题是，随着层数的增加，权重参数上的梯度越来越小，结果导致权重参数与损失函数之间的距离越来越远。在现代深度学习中，通过改进的激活函数、适当初始化权重和其他技巧来解决这个问题。
- en: Vectorization
  id: totrans-194
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 矢量化
- en: The process of turning a piece of nonnumerical data into a representation as
    an array of numbers (such as a vector). For example, text vectorization involves
    turning characters, words, or sentences into vectors.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 将非数字数据转化为数字数组(如向量)的过程。例如，文本向量化涉及将字符、单词或句子转换为向量。
- en: Visor
  id: totrans-196
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 可视化工具
- en: In tfjs-vis (a visualization library tightly integrated with TensorFlow.js),
    a collapsible region that can be created with a single function call on the side
    of the web page to hold surfaces for visualization. Multiple tabs can be created
    within a visor to organize the surfaces. See [section 8.1](kindle_split_020.html#ch08lev1sec1)
    for details.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在与TensorFlow.js紧密集成的可视化库tfjs-vis中，可以通过在网页的一侧进行单个函数调用来创建一个可折叠的区域，以容纳可视化用的表面。可以在可视化工具中创建多个标签页来组织各个表面。详见[第8.1节](kindle_split_020.html#ch08lev1sec1)。
- en: Vocabulary
  id: totrans-198
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 词汇表
- en: In the context of deep learning, a set of discrete, unique items that may be
    used as the input to or output from a neural network. Typically, each item of
    the vocabulary can be mapped to an integer index, which can then be turned into
    a one-hot or embedding-based representation.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习的背景下，一组离散的、独特的项目，可以用作神经网络的输入或输出。通常，词汇表的每个项目可以映射到一个整数索引，然后可以将其转换为一种one-hot或基于嵌入的表示。
- en: Weight
  id: totrans-200
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 权重
- en: A tunable parameter of a neural network layer. Changing the weights changes
    the numerical details of how the input is transformed into the output. The training
    of a neural network is primarily about updating weight values in a systematic
    way.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络层的一个可调参数。改变权重会改变输入如何转换为输出的数值细节。神经网络的训练主要是关于以系统性的方式更新权重值。
- en: Weight quantization
  id: totrans-202
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 权重量化
- en: A technique for reducing the serialized and on-the-wire size of a model. It
    involves storing the weight parameters of the model at a lower numeric precision.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 一种用于减少模型序列化和传输时大小的技术。它涉及将模型的权重参数以较低的数值精度存储。
- en: Word embedding
  id: totrans-204
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 词嵌入模型
- en: One way to vectorize words in text-related neural networks. A word is mapped
    onto a 1D tensor (or vector) via an embedding lookup process. Unlike one-hot encoding,
    the word embedding involves nonsparse vectors in which the element values are
    continuous-varying numbers instead of 0s and 1s.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 文本相关的神经网络中将词向量化的一种方式。通过嵌入查找过程，将单词映射到1D张量(或向量)。与one-hot编码不同，词嵌入涉及到非稀疏向量，其中元素值是连续变化的数字，而不是0和1。
- en: '![](ifc_alt.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](ifc_alt.jpg)'
- en: '![](ibc_alt.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](ibc_alt.jpg)'
