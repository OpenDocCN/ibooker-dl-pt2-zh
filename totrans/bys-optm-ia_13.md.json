["```py\ndef objective(x):                                       ❶\n    y = -((x + 1) ** 2) * torch.sin(2 * x + 2) /\n    ➥5 + 1 + x / 3                                     ❶\n    return y                                            ❶\n\nlb = -5                                                 ❷\nub = 5                                                  ❷\nbounds = torch.tensor([[lb], [ub]], dtype=torch.float)  ❷\n```", "```py\ntensor([[ 0.],   ❶\n        [ 3.],   ❷\n        [-4.]])  ❸\n```", "```py\ntensor([[0, 1],   ❶\n        [0, 2]])  ❷\n```", "```py\ndef compare(y):\n    assert y.numel() == 2                     ❶\n\n    if y.flatten()[0] > y.flatten()[1]:       ❷\n        return torch.tensor([[0, 1]]).long()  ❷\n    else:                                     ❷\n        return torch.tensor([[1, 0]]).long()  ❷\n```", "```py\ntorch.manual_seed(0)                                              ❶\ntrain_x = bounds[0] + (bounds[1] - bounds[0]) * torch.rand(2, 1)  ❷\n```", "```py\ntensor([[-0.0374],\n        [ 2.6822]])\n```", "```py\ntrain_y = objective(train_x)    ❶\ntrain_comp = compare(train_y)   ❷\n```", "```py\ntensor([[0, 1]])\n```", "```py\n    def observe_and_append_data(x_next, f, x_train, comp_train, tol=1e-3):\n        x_next = x_next.to(x_train)              ❶\n        y_next = f(x_next)                       ❶\n        comp_next = compare(y_next)              ❶\n\n        n = x_train.shape[-2]                    ❷\n        new_x_train = x_train.clone()            ❷\n        new_comp_next = comp_next.clone() + n    ❷\n    ```", "```py\n    n_dups = 0\n\n      dup_ind = torch.where(                                   ❶\n          torch.all(torch.isclose(x_train, x_next[0],\n          ➥atol=tol), axis=1)                                 ❶\n      )[0]                                                     ❶\n      if dup_ind.nelement() == 0:                              ❷\n          new_x_train = torch.cat([x_train, x_next[0]\n          ➥.unsqueeze(-2)])                                   ❷\n      else:                                                    ❸\n          new_comp_next = torch.where(                         ❸\n              new_comp_next == n, dup_ind, new_comp_next - 1   ❸\n          )                                                    ❸\n          n_dups += 1\n\n      dup_ind = torch.where(                                   ❹\n          torch.all(torch.isclose(new_x_train, x_next[1],\n          ➥atol=tol), axis=1)                                 ❹\n      )[0]                                                     ❹\n      if dup_ind.nelement() == 0:                              ❷\n          new_x_train = torch.cat([new_x_train, x_next[1]\n          ➥.unsqueeze(-2)])                                   ❷\n      else:                                                    ❺\n          new_comp_next = torch.where(                         ❺\n              new_comp_next == n + 1 - n_dups, dup_ind,\n              ➥new_comp_next                                  ❺\n          )                                                    ❺\n\n      new_comp_train = torch.cat([comp_train,\n      ➥new_comp_next])                                        ❻\n      return new_x_train, new_comp_train                       ❻\n    ```", "```py\nfrom botorch.models.pairwise_gp import PairwiseGP,        ❶\n➥ PairwiseLaplaceMarginalLogLikelihood                   ❶\nfrom botorch.fit import fit_gpytorch_mll                  ❶\n\nmodel = PairwiseGP(train_x, train_comp)                   ❷\nmodel.covar_module.raw_outputscale.requires_grad_(False)  ❸\nmll = PairwiseLaplaceMarginalLogLikelihood(model)         ❹\nfit_gpytorch_mll(mll);                                    ❺\n```", "```py\nNumericalWarning: A not p.d., added jitter of 1.0e-06 to the diagonal\n  warnings.warn(\n```", "```py\ntrain_x = torch.tensor([[0.], [3.], [-4.], [4.]])\n```", "```py\ntrain_comp = torch.tensor(\n    [\n        [0, 1],    ❶\n        [0, 2],    ❷\n        [3, 0],    ❸\n    ]\n)\n```", "```py\nmodel = PairwiseGP(train_x, train_comp)              ❶\nmll = PairwiseLaplaceMarginalLogLikelihood(model)    ❷\nfit_gpytorch_mll(mll)                                ❸\n```", "```py\npolicy = UpperConfidenceBound(model, beta=2)   ❶\n\nchallenger, acq_val = optimize_acqf(           ❷\n    policy,                                    ❷\n    bounds=bounds,                             ❷\n    q=1,                                       ❷\n    num_restarts=50,                           ❷\n    raw_samples=100,                           ❷\n)                                              ❷\n```", "```py\npolicy = qNoisyExpectedImprovement(model, train_x)  ❶\n\nchallenger, acq_val = optimize_acqf(                ❷\n    policy,                                         ❷\n    bounds=bounds,                                  ❷\n    q=1,                                            ❷\n    num_restarts=50,                                ❷\n    raw_samples=100,                                ❷\n)                                                   ❷\n```", "```py\nchallenger = bounds[0] + (bounds[1] - bounds[0]) * torch.rand(1, 1)   ❶\n```", "```py\nincumbent_ind = train_y.argmax()                   ❶\n\nnext_x = torch.vstack([train_x[incumbent_ind,\n➥:], challenger])                                 ❷\n\ntrain_x, train_comp = observe_and_append_data(     ❸\n    next_x, objective, train_x, train_comp         ❸\n)                                                  ❸\ntrain_y = objective(train_x)                       ❸\n```"]