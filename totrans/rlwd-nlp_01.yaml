- en: front matter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: preface
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having worked at the intersection of machine learning (ML), natural language
    processing (NLP), and education for the last two decades, I have always been passionate
    about education and helping people learn new technologies. That’s why I didn’t
    think twice when I heard about the opportunity of publishing a book on NLP.
  prefs: []
  type: TYPE_NORMAL
- en: The field of artificial intelligence (AI) went through a lot of changes over
    the past several years, including the explosive popularization of neural network-based
    methods and the advent of large, pretrained language models. This change made
    advanced language technologies possible, many of which you interact with daily—voice-based
    virtual assistants, speech recognition, and machine translation, to name a few.
    However, the “technology stack” of NLP, characterized by the use of pretrained
    models and transfer learning, has finally stabilized in the last few years and
    is expected to remain so, at least for the next couple of years. This is why I
    think now is a good time to start learning about NLP.
  prefs: []
  type: TYPE_NORMAL
- en: Developing a book on AI is never easy. It feels like you are chasing a moving
    target that doesn’t slow down and wait for you. When I started writing this book,
    the Transformer had just been published, and BERT did not yet exist. Over the
    course of writing, AllenNLP, the main NLP framework we use in this book, went
    through two major updates. Few people were using Hugging Face Transformer, a widely
    popular deep NLP library currently used by many practitioners all over the world.
    Within two years, the landscape of the NLP field changed completely, due to the
    advent of the Transformer and pretrained language models such as BERT. The good
    news is that the basics of modern machine learning, including word and sentence
    embeddings, RNNs, and CNNs, have not become obsolete and remain important. This
    book intends to capture this “core” of ideas and concepts that help you build
    real-world NLP applications.
  prefs: []
  type: TYPE_NORMAL
- en: Many great books about ML and deep learning in general are on the market, but
    some of them focus heavily on math and theories. There’s a gap between what’s
    taught in books and what the industry needs. I hope this book will serve to bridge
    this gap.
  prefs: []
  type: TYPE_NORMAL
- en: acknowledgments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This book would not be possible without the help of many people. I must start
    by thanking Karen Miller, the development editor at Manning Publications. Thank
    you for your support and patience during the development of this book. I’m also
    grateful for the rest of the Manning team: technical development editor Mike Shepard,
    review editor Adriana Sabo, production editor Deirdre Hiam, copy editor Pamela
    Hunt, proofreader Keri Hales, and technical proofreader Mayur Patil. Denny ([http://www.designsonline.id/](http://www.designsonline.id/))
    also created some of the high-quality illustrations you see in this book.'
  prefs: []
  type: TYPE_NORMAL
- en: 'I’d also like to thank the reviewers who gave valuable feedback after reading
    the manuscript of this book: Al Krinker, Alain Lompo, Anutosh Ghosh, Brian S.
    Cole, Cass Petrus, Charles Soetan, Dan Sheikh, Emmanuel Medina Lopez, Frédéric
    Flayol, George L. Gaines, James Black, Justin Coulston, Lin Chen, Linda Ristevski,
    Luis Moux, Marc-Anthony Taylor, Mike Rosencrantz, Nikos Kanakaris, Ninoslav Čerkez,
    Richard Vaughan, Robert Diana, Roger Meli, Salvatore Campagna, Shanker Janakiraman,
    Stuart Perks, Taylor Delehanty, and Tom Heiman.'
  prefs: []
  type: TYPE_NORMAL
- en: I’d like to acknowledge the AllenNLP team at the Allen Institute for Artificial
    Intelligence. I’ve had great discussions with the team, namely, Matt Gardner,
    Mark Neumann, and Michael Schmitz. I always look up to their great work that makes
    deep NLP technologies easy and accessible to the world.
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least, I’d like to thank my awesome wife, Lynn. She not only helped
    me choose the right cover image for this book but has also been understanding
    and supportive of my work throughout the development of this book.
  prefs: []
  type: TYPE_NORMAL
- en: about this book
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Real-World Natural Language Processing* is not a typical NLP textbook. We
    focus on building real-world NLP applications. *Real-world*’s meaning here is
    twofold: first, we pay attention to what it takes to build real-world NLP applications.
    As a reader, you will learn not just how to train NLP models but also how to design,
    develop, deploy, and monitor them. Along the way, you will also learn the basic
    building blocks of modern NLP models, as well as recent developments in the NLP
    field that are useful for building NLP applications. Second, unlike most introductory
    books, we take a top-down approach to teaching. Instead of a bottom-up approach,
    spending page after page showing neural network theories and mathematical formulae,
    we focus on quickly building NLP applications that “just work.” We then dive deeper
    into individual concepts and models that make up NLP applications. You’ll also
    learn how to build end-to-end custom NLP applications tailored to your needs using
    these basic building blocks.'
  prefs: []
  type: TYPE_NORMAL
- en: Who should read this book
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This book is written mainly for software engineers and programmers who are looking
    to learn the basics of NLP and how to build NLP applications. We assume that you,
    the reader, have basic programming and software engineering skills in Python.
    This book also comes in handy if you are already working on machine learning but
    would like to move into the NLP field. Either way, you don’t need any prior knowledge
    of ML or NLP. You don’t need any math knowledge to read this book, although basic
    understanding of linear algebra might be helpful. There is not a single mathematical
    formula in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'How this book is organized: A roadmap'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This book consists of three parts that span a total of 11 chapters. Part 1 covers
    the basics of NLP, where we learn how to quickly build an NLP application with
    AllenNLP for basic tasks such as sentiment analysis and sequence labeling.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 1 begins by introducing the “what” and “why” of NLP—what is NLP, what
    is not NLP, how NLP technologies are used, and how NLP is related to other fields
    of AI.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 2 demonstrates how to build your very first NLP application, a sentiment
    analyzer, and introduces the basics of modern NLP models—word embeddings and recurrent
    neural networks (RNNs)—along the way.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 3 introduces two important building blocks of NLP applications, word
    and sentence embeddings, and demonstrates how to use and train them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 4 discusses one of the simplest but most important NLP tasks, sentence
    classification, and how to use RNNs for this task.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 5 covers sequence labeling tasks such as part-of-speech tagging and
    named entity extraction. It also touches upon a related technique, language modeling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 2 covers advanced NLP topics including sequence-to-sequence models, the
    Transformer, and how to leverage transfer learning and pretrained language models
    to build powerful NLP applications.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 6 introduces sequence-to-sequence models, which transform one sequence
    into another. We build a simple machine translation system and a chatbot within
    an hour.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 7 discusses another type of popular neural network architecture, convolutional
    neural networks (CNNs).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 8 provides a deep dive into the Transformer, one of the most important
    NLP models today. We’ll demonstrate how to build an improved machine translation
    system and a spell-checker using the Transformer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 9 builds upon the previous chapter and discusses transfer learning,
    a popular technique in modern NLP, with pretrained language models such as BERT.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 3 covers topics that become relevant when you develop NLP applications
    that are robust to real-world data, and deploy and serve them.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 10 details best practices when developing NLP applications, including
    batching and padding, regularization, and hyperparameter optimization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 11 concludes the book by covering how to deploy and serve NLP models.
    It also covers how to explain and interpret ML models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: About the code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This book contains many examples of source code both in numbered listings and
    in line with normal text. In both cases, source code is formatted in a fixed-width
    font like this to separate it from ordinary text. Sometimes code is also **in
    bold** to highlight code that has changed from previous steps in the chapter,
    such as when a new feature adds to an existing line of code.
  prefs: []
  type: TYPE_NORMAL
- en: In many cases, the original source code has been reformatted; we’ve added line
    breaks and reworked indentation to accommodate the available page space in the
    book. In rare cases, even this was not enough, and listings include line-continuation
    markers (➥). Additionally, comments in the source code have often been removed
    from the listings when the code is described in the text. Code annotations accompany
    many of the listings, highlighting important concepts.
  prefs: []
  type: TYPE_NORMAL
- en: The code for the examples in this book is available for download from the Manning
    website at [https://www.manning.com/books/real-world-natural-language-processing](https://www.manning.com/books/real-world-natural-language-processing)
    and from GitHub at [https://github.com/mhagiwara/realworldnlp](https://github.com/mhagiwara/realworldnlp).
  prefs: []
  type: TYPE_NORMAL
- en: Most of the code can also be run on Google Colab, which is a free web-based
    platform where you can run your machine learning code on hardware accelerators,
    including GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: liveBook discussion forum
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Purchase of Real-World Natural Language Processing includes free access to a
    private web forum run by Manning Publications where you can make comments about
    the book, ask technical questions, and receive help from the author and from other
    users. To access the forum, go to [https://livebook.manning.com/book/real-world-natural
    -language-processing/discussion](https://livebook.manning.com/book/real-world-natural-language-processing/discussion).
    You can also learn more about Manning’s forums and the rules of conduct at [https://livebook.manning.com/#!/discussion](https://livebook.manning.com/#!/discussion).
  prefs: []
  type: TYPE_NORMAL
- en: Manning’s commitment to our readers is to provide a venue where a meaningful
    dialogue between individual readers and between readers and the author can take
    place. It is not a commitment to any specific amount of participation on the part
    of the author, whose contribution to the forum remains voluntary (and unpaid).
    We suggest you try asking the author some challenging questions lest his interest
    stray! The forum and the archives of previous discussions will be accessible from
    the publisher’s website as long as the book is in print.
  prefs: []
  type: TYPE_NORMAL
- en: Other online resources
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The two NLP frameworks we use heavily in this book, AllenNLP and Hugging Face
    Transformers, both have great online courses ([https://guide.allennlp.org/](https://guide.allennlp.org/)
    and [https://huggingface.co/course](https://huggingface.co/course)) where you
    can learn the basics of NLP and how to use the libraries to solve a variety of
    NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: about the author
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| ![Hagiwara](../Images/Hagiwara.jpg) | Masato Hagiwaraspan> received a PhD
    in computer science from Nagoya University in 2009, focusing on natural language
    processing and machine learning. He has interned at Google and Microsoft Research
    and worked at Baidu, Rakuten Institute of Technology, and Duolingo, as an engineer
    and a researcher. He now runs his own research and consultancy company, Octanove
    Labs, focusing on educational applications of NLP. |'
  prefs: []
  type: TYPE_TB
- en: about the cover illustration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The figure on the cover of *Real-World Natural Language Processing* is captioned
    “Bulgare,” or a man from Bulgaria. The illustration is taken from a collection
    of dress costumes from various countries by Jacques Grasset de Saint-Sauveur (1757–1810),
    titled *Costumes de Différents Pays*, published in France in 1797\. Each illustration
    is finely drawn and colored by hand. The rich variety of Grasset de Saint-Sauveur’s
    collection reminds us vividly of how culturally apart the world’s towns and regions
    were just 200 years ago. Isolated from each other, people spoke different dialects
    and languages. In the streets or in the countryside, it was easy to identify where
    they lived and what their trade or station in life was just by their dress.
  prefs: []
  type: TYPE_NORMAL
- en: The way we dress has changed since then and the diversity by region, so rich
    at the time, has faded away. It is now hard to tell apart the inhabitants of different
    continents, let alone different towns, regions, or countries. Perhaps we have
    traded cultural diversity for a more varied personal life—certainly for a more
    varied and fast-paced technological life.
  prefs: []
  type: TYPE_NORMAL
- en: At a time when it is hard to tell one computer book from another, Manning celebrates
    the inventiveness and initiative of the computer business with book covers based
    on the rich diversity of regional life of two centuries ago, brought back to life
    by Grasset de Saint-Sauveur’s pictures.
  prefs: []
  type: TYPE_NORMAL
