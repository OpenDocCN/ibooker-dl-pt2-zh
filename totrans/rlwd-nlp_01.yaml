- en: front matter
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前言
- en: preface
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前言
- en: Having worked at the intersection of machine learning (ML), natural language
    processing (NLP), and education for the last two decades, I have always been passionate
    about education and helping people learn new technologies. That’s why I didn’t
    think twice when I heard about the opportunity of publishing a book on NLP.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去二十年里，我一直在机器学习（ML）、自然语言处理（NLP）和教育的交叉领域工作，我一直热衷于教育和帮助人们学习新技术。这就是为什么当我听说有机会出版一本关于NLP的书时，我毫不犹豫地接受了。
- en: The field of artificial intelligence (AI) went through a lot of changes over
    the past several years, including the explosive popularization of neural network-based
    methods and the advent of large, pretrained language models. This change made
    advanced language technologies possible, many of which you interact with daily—voice-based
    virtual assistants, speech recognition, and machine translation, to name a few.
    However, the “technology stack” of NLP, characterized by the use of pretrained
    models and transfer learning, has finally stabilized in the last few years and
    is expected to remain so, at least for the next couple of years. This is why I
    think now is a good time to start learning about NLP.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 过去几年，人工智能（AI）领域经历了许多变化，包括基于神经网络的方法的爆炸式普及和大规模预训练语言模型的出现。这一变化使得许多先进的语言技术成为可能，其中包括你每天都会与之交互的语音虚拟助手、语音识别和机器翻译等。然而，NLP的“技术堆栈”，以预训练模型和迁移学习为特征，最近几年已经稳定下来，并预计将保持稳定，至少在未来几年内。这就是为什么我认为现在是开始学习NLP的好时机。
- en: Developing a book on AI is never easy. It feels like you are chasing a moving
    target that doesn’t slow down and wait for you. When I started writing this book,
    the Transformer had just been published, and BERT did not yet exist. Over the
    course of writing, AllenNLP, the main NLP framework we use in this book, went
    through two major updates. Few people were using Hugging Face Transformer, a widely
    popular deep NLP library currently used by many practitioners all over the world.
    Within two years, the landscape of the NLP field changed completely, due to the
    advent of the Transformer and pretrained language models such as BERT. The good
    news is that the basics of modern machine learning, including word and sentence
    embeddings, RNNs, and CNNs, have not become obsolete and remain important. This
    book intends to capture this “core” of ideas and concepts that help you build
    real-world NLP applications.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 编写一本关于AI的书绝非易事。感觉就像你在追逐一个不会减速等待你的移动目标。当我开始写这本书时，Transformer刚刚发布，BERT还不存在。在写作过程中，我们在这本书中使用的主要NLP框架AllenNLP经历了两次重大更新。很少有人使用Hugging
    Face Transformer，这是一款广受欢迎的深度NLP库，目前被全球许多实践者使用。在两年内，由于Transformer和预训练语言模型（如BERT）的出现，NLP领域的格局发生了彻底的变化。好消息是，现代机器学习的基础，包括单词和句子嵌入、RNN和CNN，尚未过时，并且仍然重要。本书旨在捕捉帮助您构建真实世界NLP应用程序的思想和概念的“核心”。
- en: Many great books about ML and deep learning in general are on the market, but
    some of them focus heavily on math and theories. There’s a gap between what’s
    taught in books and what the industry needs. I hope this book will serve to bridge
    this gap.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 市场上有许多关于ML和深度学习的优秀书籍，但其中一些过分强调数学和理论。书籍教授的内容与行业需求存在差距。我希望这本书能填补这一差距。
- en: acknowledgments
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: 'This book would not be possible without the help of many people. I must start
    by thanking Karen Miller, the development editor at Manning Publications. Thank
    you for your support and patience during the development of this book. I’m also
    grateful for the rest of the Manning team: technical development editor Mike Shepard,
    review editor Adriana Sabo, production editor Deirdre Hiam, copy editor Pamela
    Hunt, proofreader Keri Hales, and technical proofreader Mayur Patil. Denny ([http://www.designsonline.id/](http://www.designsonline.id/))
    also created some of the high-quality illustrations you see in this book.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 没有许多人的帮助，这本书是不可能完成的。我必须首先感谢Manning出版社的开发编辑Karen Miller。在编写这本书的过程中，感谢你的支持和耐心。我还要感谢Manning团队的其他成员：技术开发编辑Mike
    Shepard、审稿编辑Adriana Sabo、制作编辑Deirdre Hiam、副本编辑Pamela Hunt、校对员Keri Hales和技术校对员Mayur
    Patil。Denny（[http://www.designsonline.id/](http://www.designsonline.id/)）还为本书创作了一些高质量的插图。
- en: 'I’d also like to thank the reviewers who gave valuable feedback after reading
    the manuscript of this book: Al Krinker, Alain Lompo, Anutosh Ghosh, Brian S.
    Cole, Cass Petrus, Charles Soetan, Dan Sheikh, Emmanuel Medina Lopez, Frédéric
    Flayol, George L. Gaines, James Black, Justin Coulston, Lin Chen, Linda Ristevski,
    Luis Moux, Marc-Anthony Taylor, Mike Rosencrantz, Nikos Kanakaris, Ninoslav Čerkez,
    Richard Vaughan, Robert Diana, Roger Meli, Salvatore Campagna, Shanker Janakiraman,
    Stuart Perks, Taylor Delehanty, and Tom Heiman.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我还要感谢在阅读本书手稿后提供宝贵反馈的审稿人：Al Krinker、Alain Lompo、Anutosh Ghosh、Brian S. Cole、Cass
    Petrus、Charles Soetan、Dan Sheikh、Emmanuel Medina Lopez、Frédéric Flayol、George
    L. Gaines、James Black、Justin Coulston、Lin Chen、Linda Ristevski、Luis Moux、Marc-Anthony
    Taylor、Mike Rosencrantz、Nikos Kanakaris、Ninoslav Čerkez、Richard Vaughan、Robert
    Diana、Roger Meli、Salvatore Campagna、Shanker Janakiraman、Stuart Perks、Taylor Delehanty和Tom
    Heiman。
- en: I’d like to acknowledge the AllenNLP team at the Allen Institute for Artificial
    Intelligence. I’ve had great discussions with the team, namely, Matt Gardner,
    Mark Neumann, and Michael Schmitz. I always look up to their great work that makes
    deep NLP technologies easy and accessible to the world.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我要感谢Allen Institute for Artificial Intelligence的AllenNLP团队。我与该团队的Matt Gardner、Mark
    Neumann和Michael Schmitz进行了很好的讨论。我一直钦佩他们的优秀工作，使深度NLP技术易于访问并普及于世界。
- en: Last but not least, I’d like to thank my awesome wife, Lynn. She not only helped
    me choose the right cover image for this book but has also been understanding
    and supportive of my work throughout the development of this book.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，但同样重要的是，我要感谢我的出色妻子Lynn。她不仅帮助我选择了本书的正确封面图像，而且在整本书的编写过程中一直理解和支持我的工作。
- en: about this book
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于本书
- en: '*Real-World Natural Language Processing* is not a typical NLP textbook. We
    focus on building real-world NLP applications. *Real-world*’s meaning here is
    twofold: first, we pay attention to what it takes to build real-world NLP applications.
    As a reader, you will learn not just how to train NLP models but also how to design,
    develop, deploy, and monitor them. Along the way, you will also learn the basic
    building blocks of modern NLP models, as well as recent developments in the NLP
    field that are useful for building NLP applications. Second, unlike most introductory
    books, we take a top-down approach to teaching. Instead of a bottom-up approach,
    spending page after page showing neural network theories and mathematical formulae,
    we focus on quickly building NLP applications that “just work.” We then dive deeper
    into individual concepts and models that make up NLP applications. You’ll also
    learn how to build end-to-end custom NLP applications tailored to your needs using
    these basic building blocks.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*实战自然语言处理*不是一本典型的NLP教材。我们专注于构建实际的NLP应用程序。这里的*实战*有两层含义：首先，我们关注构建实际NLP应用程序所需的内容。作为读者，您将学习不仅如何训练NLP模型，还将学习如何设计、开发、部署和监控它们。在这一过程中，您还将了解到现代NLP模型的基本构建模块，以及对构建NLP应用程序有用的NLP领域的最新发展。其次，与大多数入门书籍不同，我们采用自顶向下的教学方法。我们不是采用自下而上的方法，一页页地展示神经网络理论和数学公式，而是专注于快速构建“只管用”的NLP应用程序。然后，我们深入研究构成NLP应用程序的各个概念和模型。您还将学习如何使用这些基本构建模块构建符合您需求的端到端定制NLP应用程序。'
- en: Who should read this book
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 谁应该阅读本书
- en: This book is written mainly for software engineers and programmers who are looking
    to learn the basics of NLP and how to build NLP applications. We assume that you,
    the reader, have basic programming and software engineering skills in Python.
    This book also comes in handy if you are already working on machine learning but
    would like to move into the NLP field. Either way, you don’t need any prior knowledge
    of ML or NLP. You don’t need any math knowledge to read this book, although basic
    understanding of linear algebra might be helpful. There is not a single mathematical
    formula in this book.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本书主要面向希望学习NLP基础知识以及如何构建NLP应用程序的软件工程师和程序员。我们假设您，读者，在Python中具有基本的编程和软件工程技能。如果您已经从事机器学习工作，但希望转入NLP领域，本书也会很有用。无论哪种情况，您都不需要任何ML或NLP的先前知识。您不需要任何数学知识来阅读本书，尽管对线性代数的基本理解可能会有所帮助。本书中没有一个数学公式。
- en: 'How this book is organized: A roadmap'
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本书的组织方式：路线图
- en: This book consists of three parts that span a total of 11 chapters. Part 1 covers
    the basics of NLP, where we learn how to quickly build an NLP application with
    AllenNLP for basic tasks such as sentiment analysis and sequence labeling.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本书共分三部分，共包括11章。第一部分涵盖了自然语言处理（NLP）的基础知识，在这里我们学习如何使用AllenNLP快速构建NLP应用程序，包括情感分析和序列标注等基本任务。
- en: Chapter 1 begins by introducing the “what” and “why” of NLP—what is NLP, what
    is not NLP, how NLP technologies are used, and how NLP is related to other fields
    of AI.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第1章从介绍自然语言处理的“什么”和“为什么”开始——什么是自然语言处理，什么不是自然语言处理，自然语言处理技术如何被使用，以及自然语言处理与其他人工智能领域的关系。
- en: Chapter 2 demonstrates how to build your very first NLP application, a sentiment
    analyzer, and introduces the basics of modern NLP models—word embeddings and recurrent
    neural networks (RNNs)—along the way.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第2章演示了如何构建您的第一个自然语言处理应用程序，即情感分析器，并介绍了现代自然语言处理模型的基础——词嵌入和循环神经网络（RNNs）。
- en: Chapter 3 introduces two important building blocks of NLP applications, word
    and sentence embeddings, and demonstrates how to use and train them.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第3章介绍了自然语言处理应用程序的两个重要构建块，即词嵌入和句子嵌入，并演示了如何使用和训练它们。
- en: Chapter 4 discusses one of the simplest but most important NLP tasks, sentence
    classification, and how to use RNNs for this task.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第4章讨论了最简单但最重要的自然语言处理任务之一，即句子分类，以及如何使用循环神经网络（RNNs）来完成此任务。
- en: Chapter 5 covers sequence labeling tasks such as part-of-speech tagging and
    named entity extraction. It also touches upon a related technique, language modeling.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第5章涵盖了诸如词性标注和命名实体提取之类的序列标注任务。它还涉及到一种相关技术，即语言建模。
- en: Part 2 covers advanced NLP topics including sequence-to-sequence models, the
    Transformer, and how to leverage transfer learning and pretrained language models
    to build powerful NLP applications.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 第2部分涵盖了包括序列到序列模型、Transformer以及如何利用迁移学习和预训练语言模型来构建强大的自然语言处理应用在内的高级自然语言处理主题。
- en: Chapter 6 introduces sequence-to-sequence models, which transform one sequence
    into another. We build a simple machine translation system and a chatbot within
    an hour.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第6章介绍了序列到序列模型，它将一个序列转换为另一个序列。我们在一个小时内构建了一个简单的机器翻译系统和一个聊天机器人。
- en: Chapter 7 discusses another type of popular neural network architecture, convolutional
    neural networks (CNNs).
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第7章讨论了另一种流行的神经网络架构，卷积神经网络（CNNs）。
- en: Chapter 8 provides a deep dive into the Transformer, one of the most important
    NLP models today. We’ll demonstrate how to build an improved machine translation
    system and a spell-checker using the Transformer.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第8章深入探讨了Transformer，这是当今最重要的自然语言处理模型之一。我们将演示如何使用Transformer构建一个改进的机器翻译系统和一个拼写检查器。
- en: Chapter 9 builds upon the previous chapter and discusses transfer learning,
    a popular technique in modern NLP, with pretrained language models such as BERT.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第9章在上一章的基础上展开，并讨论了迁移学习，这是现代自然语言处理中的一种流行技术，使用预训练的语言模型如BERT。
- en: Part 3 covers topics that become relevant when you develop NLP applications
    that are robust to real-world data, and deploy and serve them.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 第3部分涵盖了在开发对真实世界数据具有鲁棒性、并进行部署和提供的自然语言处理应用程序时变得相关的主题。
- en: Chapter 10 details best practices when developing NLP applications, including
    batching and padding, regularization, and hyperparameter optimization.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第10章详细介绍了开发自然语言处理应用程序时的最佳实践，包括批处理和填充、正则化以及超参数优化。
- en: Chapter 11 concludes the book by covering how to deploy and serve NLP models.
    It also covers how to explain and interpret ML models.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第11章通过讨论如何部署和提供自然语言处理模型来结束本书。它还涵盖了如何解释和解释机器学习模型。
- en: About the code
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于代码
- en: This book contains many examples of source code both in numbered listings and
    in line with normal text. In both cases, source code is formatted in a fixed-width
    font like this to separate it from ordinary text. Sometimes code is also **in
    bold** to highlight code that has changed from previous steps in the chapter,
    such as when a new feature adds to an existing line of code.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本书包含许多源代码示例，既有编号列表，也有与普通文本一样的行内代码。在这两种情况下，源代码都以像这样的固定宽度字体格式化，以使其与普通文本分开。有时代码也会**加粗**，以突出显示与章节中的先前步骤有所不同的代码，例如当一个新功能添加到现有代码行时。
- en: In many cases, the original source code has been reformatted; we’ve added line
    breaks and reworked indentation to accommodate the available page space in the
    book. In rare cases, even this was not enough, and listings include line-continuation
    markers (➥). Additionally, comments in the source code have often been removed
    from the listings when the code is described in the text. Code annotations accompany
    many of the listings, highlighting important concepts.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，原始源代码已经被重新格式化；我们已经添加了换行符并重新安排了缩进以适应书中可用的页面空间。在罕见情况下，即使这样做还不够，列表中也包括了行继续标记（➥）。此外，当文本中描述代码时，源代码中的注释通常会被从列表中移除。代码注释伴随着许多列表，突出显示重要概念。
- en: The code for the examples in this book is available for download from the Manning
    website at [https://www.manning.com/books/real-world-natural-language-processing](https://www.manning.com/books/real-world-natural-language-processing)
    and from GitHub at [https://github.com/mhagiwara/realworldnlp](https://github.com/mhagiwara/realworldnlp).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 本书示例中的代码可从 Manning 网站（[https://www.manning.com/books/real-world-natural-language-processing](https://www.manning.com/books/real-world-natural-language-processing)）和
    GitHub（[https://github.com/mhagiwara/realworldnlp](https://github.com/mhagiwara/realworldnlp)）下载。
- en: Most of the code can also be run on Google Colab, which is a free web-based
    platform where you can run your machine learning code on hardware accelerators,
    including GPUs.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分代码也可以在 Google Colab 上运行，这是一个免费的基于网络的平台，您可以在其中运行您的机器学习代码，包括 GPU 硬件加速器。
- en: liveBook discussion forum
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: liveBook 讨论论坛
- en: Purchase of Real-World Natural Language Processing includes free access to a
    private web forum run by Manning Publications where you can make comments about
    the book, ask technical questions, and receive help from the author and from other
    users. To access the forum, go to [https://livebook.manning.com/book/real-world-natural
    -language-processing/discussion](https://livebook.manning.com/book/real-world-natural-language-processing/discussion).
    You can also learn more about Manning’s forums and the rules of conduct at [https://livebook.manning.com/#!/discussion](https://livebook.manning.com/#!/discussion).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 购买《实用自然语言处理》包含免费访问 Manning Publications 运行的私人网络论坛，您可以在该论坛上对书籍发表评论，提出技术问题，并从作者和其他用户那里获得帮助。要访问论坛，请转到
    [https://livebook.manning.com/book/real-world-natural -language-processing/discussion](https://livebook.manning.com/book/real-world-natural-language-processing/discussion)。您还可以在
    [https://livebook.manning.com/#!/discussion](https://livebook.manning.com/#!/discussion)
    上了解更多关于 Manning 论坛和行为规则的信息。
- en: Manning’s commitment to our readers is to provide a venue where a meaningful
    dialogue between individual readers and between readers and the author can take
    place. It is not a commitment to any specific amount of participation on the part
    of the author, whose contribution to the forum remains voluntary (and unpaid).
    We suggest you try asking the author some challenging questions lest his interest
    stray! The forum and the archives of previous discussions will be accessible from
    the publisher’s website as long as the book is in print.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Manning 对我们的读者的承诺是提供一个场所，让个体读者和读者与作者之间进行有意义的对话。这不是对作者参与的任何具体数量的承诺，作者对论坛的贡献仍然是自愿的（且无偿的）。我们建议您尝试向作者提出一些具有挑战性的问题，以免他的兴趣减退！只要本书在印刷状态下，论坛和之前讨论的存档将可以从出版商的网站上访问到。
- en: Other online resources
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 其他在线资源
- en: The two NLP frameworks we use heavily in this book, AllenNLP and Hugging Face
    Transformers, both have great online courses ([https://guide.allennlp.org/](https://guide.allennlp.org/)
    and [https://huggingface.co/course](https://huggingface.co/course)) where you
    can learn the basics of NLP and how to use the libraries to solve a variety of
    NLP tasks.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本书中大量使用的两个自然语言处理框架，AllenNLP 和 Hugging Face Transformers，都有很棒的在线课程（[https://guide.allennlp.org/](https://guide.allennlp.org/)
    和 [https://huggingface.co/course](https://huggingface.co/course)），您可以在这些课程中学习自然语言处理的基础知识以及如何使用这些库来解决各种自然语言处理任务。
- en: about the author
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于作者
- en: '| ![Hagiwara](../Images/Hagiwara.jpg) | Masato Hagiwaraspan> received a PhD
    in computer science from Nagoya University in 2009, focusing on natural language
    processing and machine learning. He has interned at Google and Microsoft Research
    and worked at Baidu, Rakuten Institute of Technology, and Duolingo, as an engineer
    and a researcher. He now runs his own research and consultancy company, Octanove
    Labs, focusing on educational applications of NLP. |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| ![Hagiwara](../Images/Hagiwara.jpg) | **萩原真人**于2009年从名古屋大学获得计算机科学博士学位，专注于自然语言处理和机器学习。他曾在谷歌和微软研究院实习，并在百度、乐天技术研究所和Duolingo工作过，担任工程师和研究员。他现在经营自己的研究和咨询公司Octanove
    Labs，专注于自然语言处理在教育应用中的应用。 |'
- en: about the cover illustration
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于封面插图
- en: The figure on the cover of *Real-World Natural Language Processing* is captioned
    “Bulgare,” or a man from Bulgaria. The illustration is taken from a collection
    of dress costumes from various countries by Jacques Grasset de Saint-Sauveur (1757–1810),
    titled *Costumes de Différents Pays*, published in France in 1797\. Each illustration
    is finely drawn and colored by hand. The rich variety of Grasset de Saint-Sauveur’s
    collection reminds us vividly of how culturally apart the world’s towns and regions
    were just 200 years ago. Isolated from each other, people spoke different dialects
    and languages. In the streets or in the countryside, it was easy to identify where
    they lived and what their trade or station in life was just by their dress.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 封面上的图案*Real-World Natural Language Processing*的标题是“Bulgare”，或者来自保加利亚的人。 这幅插图摘自雅克·格拉塞·德·圣索维尔（1757–1810）的各国服装收藏品，该收藏品名为*Costumes
    de Différents Pays*，于1797年在法国出版。 每幅插图都是精细绘制和手工上色的。 格拉塞·德·圣索维尔收藏的丰富多样性生动地提醒我们，仅200年前世界各地的城镇和地区在文化上是多么独立。
    人们相互隔离，使用不同的方言和语言。 在街头或乡间，仅凭着他们的服装就可以轻易辨别他们住在哪里，以及他们的职业或生活地位。
- en: The way we dress has changed since then and the diversity by region, so rich
    at the time, has faded away. It is now hard to tell apart the inhabitants of different
    continents, let alone different towns, regions, or countries. Perhaps we have
    traded cultural diversity for a more varied personal life—certainly for a more
    varied and fast-paced technological life.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 自那时以来，我们的着装方式已经发生了变化，那时如此丰富的地区多样性已经消失。 现在很难区分不同大陆的居民，更不用说不同的城镇、地区或国家了。 或许我们已经用文化多样性换取了更丰富多彩的个人生活——当然也换来了更丰富多样和快节奏的技术生活。
- en: At a time when it is hard to tell one computer book from another, Manning celebrates
    the inventiveness and initiative of the computer business with book covers based
    on the rich diversity of regional life of two centuries ago, brought back to life
    by Grasset de Saint-Sauveur’s pictures.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在很难辨认出一本计算机书籍与另一本书籍之时，曼宁通过基于两个世纪前区域生活丰富多样性的书籍封面，庆祝计算机业的创造力和主动性，这些生活被格拉塞·德·圣索维尔的图片重新唤起。
