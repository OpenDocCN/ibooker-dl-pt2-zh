- en: Appendix A. A “hello world” deep learning system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This book is about teaching design principles for building a deep learning system
    that fits your own situation. But you may be wondering what a deep learning system
    even looks like. Or how, why, and when people can use a system like this in practice.
    These are all great questions at this stage.
  prefs: []
  type: TYPE_NORMAL
- en: We believe the best method for learning new ideas, skills, and approaches is
    by doing—by getting your hands on some examples and seeing what you can do with
    them. To help you, we built a mini deep learning system and a code lab for you
    to play with. Playing around in this “hello world” deep learning system should
    help you build the knowledge base for understanding the concepts and principles
    introduced in this book. To make this sample system easy to digest, we focus on
    the key components of a deep learning system, such as dataset management (DM)
    and model training and serving. The entire mini system can be set up easily on
    your local system with a bash script, and its components are discussed in detail
    in the later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: In this appendix, we will first take a tour of our sample system and then run
    the lab exercise to let you experience the most common user activities in a deep
    learning system, including dataset ingestion, model training, and model serving.
    Although our sample system is extremely simplified, it covers all the basics of
    a deep learning system. By reading this appendix, you will not only gain a practical
    understanding of how a basic deep learning system is organized and how it functions,
    but you will also have a holistic view of the sample services that are discussed
    in the rest of the book.
  prefs: []
  type: TYPE_NORMAL
- en: A.1 Introducing the “hello world” deep learning system
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The quickest way to understand a software system is from the user’s perspective.
    So in this introduction section, we will first look at the users of a deep learning
    system: the personas and their responsibilities. Then we will dive into system
    design, the major components, and the user workflows.'
  prefs: []
  type: TYPE_NORMAL
- en: A.1.1 Personas
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To keep the complexity to a minimum, our sample mini deep learning system only
    has four *personas*, or *roles*: data engineer, data scientist/researcher, system
    developer, and deep learning application developer. We pick these four roles because
    they are the minimum personnel needed to keep a deep learning system functioning.
    The role definition and job description of each persona in this “hello world”
    system are listed in the following sections.'
  prefs: []
  type: TYPE_NORMAL
- en: Note The role responsibilities described here are oversimplified because we
    want to concentrate on the most fundamental workflows of a deep learning system.
    For more detailed definitions of the personas and roles involved in a deep learning
    system, please refer to section 1.1.
  prefs: []
  type: TYPE_NORMAL
- en: A.1.2 Data engineers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data engineers are responsible for collecting, processing, and storing the raw
    data for deep learning training. We have a DM service in this mini system to store
    datasets for model training. Data engineers will use this service to upload the
    raw training data. In our lab exercise, we prepared some intent classification
    datasets for you to experience this process.
  prefs: []
  type: TYPE_NORMAL
- en: A.1.3 Data scientists/researchers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data scientists or researchers develop training algorithms and produce models
    that meet business requirements. They are the *customers* of the model training
    infrastructure in a deep learning system.
  prefs: []
  type: TYPE_NORMAL
- en: Our example system contains a training service to run model training code. In
    the lab, we prebuilt an intent classification training code for you to experience
    the model training execution.
  prefs: []
  type: TYPE_NORMAL
- en: A.1.4 System developer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: System developers build the entire deep learning system and maintain it to make
    sure all the machine learning activities are functioning well. Their activities
    include dataset uploading, model training, and model serving.
  prefs: []
  type: TYPE_NORMAL
- en: A.1.5 Deep learning application developers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep learning application developers utilize deep learning models to build commercial
    products, such as chatbots, self-driving software, and facial recognition mobile
    apps. These applications are the most important customers of any deep learning
    system because they create the business impact (revenue) for the models produced
    by the system. In our lab, you will have the opportunity to imagine yourself as
    a chatbot customer by running a script to send requests to the prediction service
    and categorize your message.
  prefs: []
  type: TYPE_NORMAL
- en: A.1.6 Sample system overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our mini deep learning system consists of four services and one storage system:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Data service*—Designed for storing and fetching dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model training service*—Designed for running model training code'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Metadata store service*—Designed for storing model metadata, such as model
    name, model version, and model algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Prediction service*—Designed to execute models to process customer’s prediction
    requests'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*MinIO storage*—Designed to run on your local computer as an object storage
    similar to Amazon S3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Almost all these services have their own chapters in this book, so we will be
    able to study them in more detail. For now, we just want to provide you with the
    high-level overview you’ll need to comprehend the user scenarios and lab exercises
    later in the book. Figure A.1 illustrates the major components of the sample system
    (the four services and the storage system) along with their interdependencies.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/A-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure A.1 The design overview of the sample deep learning system
  prefs: []
  type: TYPE_NORMAL
- en: 'Along with the four services and storage (in rectangular boxes), you’ll notice
    lots of directed arrows between these boxes. These arrows show the interdependencies
    of the services inside the sample system. Here are the explanations for these
    dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: The DM service saves the dataset to MinIO storage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model training service queries DM to prepare the training dataset and obtain
    the training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model training service downloads training data from MinIO.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model training service saves model files to MinIO.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model training service saves model metadata to the metadata store service.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The prediction service queries the metadata store to determine which model to
    use.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The prediction service downloads model files from MinIO to serve the prediction
    request.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A.1.7 User workflows
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have introduced the personas and the main services, let’s look at
    the user workflows. Figure A.2 illustrates the user workflow for each persona.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/A-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure A.2 Four different workflows are enabled in the system for DM, training,
    serving, and system maintenance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure A.2 shows each persona using the mini deep learning system to perform
    its own task by utilizing the services introduced in figure A.1\. Let’s review
    each of these workflows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Scenario A*—Data engineers call the DM service to upload raw data; DM ingests
    the data and saves it to MinIO storage in a training dataset format.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Scenario B*—Data scientists first write the training code and then submit
    a training request to the training service. The training service executes the
    training code and produces the model. It then saves model metadata to the metadata
    store and saves model files to MinIO storage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Scenario C*—Application developers build applications to call the prediction
    service to consume the model trained in scenario B.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Scenario D*—System developers build and maintain this system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A.2 Lab demo
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now it’s time for you to start learning by doing. In this lab exercise, you
    will engage in the user scenarios mentioned in section A.1.3 on your local computer.
    To make this exercise more vivid, we introduced some characters so you will not
    only know how to work with a deep learning system but also understand who handles
    each different job. The fictitious characters include Ivan, a data scientist;
    Feng, a data engineer; Tang, a system developer; and Johanna, an application developer.
  prefs: []
  type: TYPE_NORMAL
- en: A.2.1 Demo steps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this demo scenario, we will have Tang, Feng, Ivan, and Johanna work together
    to train an intent classification model and use the model to classify arbitrary
    text messages. This scenario simulates the four fundamental steps of a typical
    model development workflow: system setup, dataset building, model training, and
    model serving.'
  prefs: []
  type: TYPE_NORMAL
- en: To make the lab easy to run, we Dockerized all the microservices and built shell
    scripts to automate the lab setup and demo scenarios. By following the instructions
    in the README file ([https://github.com/orca3/MiniAutoML#lab](https://github.com/orca3/MiniAutoML#lab))
    at our GitHub repo ([https://github.com/orca3/MiniAutoML](https://github.com/orca3/MiniAutoML)),
    you can complete the lab by running four shell scripts.
  prefs: []
  type: TYPE_NORMAL
- en: Note You can find the lab demo scripts in the scripts folder ([https://github.com/orca3/MiniAutoML/tree/main/scripts](https://github.com/orca3/MiniAutoML/tree/main/scripts))
    at our GitHub repo ([https://github.com/orca3/MiniAutoML](https://github.com/orca3/MiniAutoML)).
    The scripts folder contains the demo scripts for the entire book. The files starting
    with *lab-* are used in this demo—for example, the `lab-001-start-all.sh` ([http://mng.bz/zmv1](http://mng.bz/zmv1))
    can be set up on your local system. In the case of future updates, and to execute
    the lab scripts successfully, please always refer to the instructions in the GitHub
    repo.
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 1 is the system setup.* Run `scripts/lab-001-start-all.sh` ([http://mng.bz/zmv1](http://mng.bz/zmv1)).'
  prefs: []
  type: TYPE_NORMAL
- en: Tang (system developer) lights up the mini deep learning system by running the
    `scripts/lab-001-start-all.sh` script. The script will download the prebuilt Docker
    images of the demo services and execute them.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the script finishes, the mini deep learning system is up and running.
    You can use the following command to list all the local running Docker containers
    to verify that all the services are running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The Docker containers required to run the lab are provided in the following
    listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing A.1 Verifying all system components are running
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Once Tang verifies that all the microservices are running, the system is ready
    to use. He informs Ivan and Feng to start their work.
  prefs: []
  type: TYPE_NORMAL
- en: note If you read the `lab-001-start-all.sh`, you will find that most of the
    services—such as data management and model training—in the system (except predictors)
    are packaged into one Docker image (`orca3/services`). This is not a recommended
    pattern for production use cases, but it fits our demo needs here since it uses
    less disk space and it’s simple to execute.
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 2 is building a training dataset.* Run `scripts/lab-002-upload-data.sh`
    ([http://mng.bz/0yqJ](http://mng.bz/0yqJ)).'
  prefs: []
  type: TYPE_NORMAL
- en: Feng (data engineer) first downloads raw data from the internet and applies
    some modifications for training (see `scripts/prepare_data.py` at [http://mng.bz/KlKX](http://mng.bz/KlKX)).
    Feng then uploads the processed data to the DM service. Once the dataset upload
    completes, DM returns a unique dataset ID for future reference.
  prefs: []
  type: TYPE_NORMAL
- en: We automated Feng’s work in the `scripts/lab-002-upload-data.sh` script. After
    executing this script, a dataset is created. You can see a JSON object being printed
    out by the DM service in your terminal. This JSON object presents the metadata
    of a dataset; see the following example.
  prefs: []
  type: TYPE_NORMAL
- en: Listing A.2 A sample dataset metadata in DM service
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Dataset identifier
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Dataset name
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Dataset type
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Dataset audit history
  prefs: []
  type: TYPE_NORMAL
- en: Dataset metadata is discussed in detail in chapter 2\. For now, we can ignore
    most attributes of the metadata JSON object and only pay attention to the `dataset_id`
    attribute. *Dataset ID* is a unique identifier for a dataset; you need to pass
    this ID to the training service for model training in step 3\. Once the dataset
    is ready, Feng informs Ivan to start model training with `dataset_id="1"`.
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 3 is model training.* Run `scripts/lab-003-first-training.sh` ([http://mng.bz/vnra](http://mng.bz/vnra)).'
  prefs: []
  type: TYPE_NORMAL
- en: Ivan (the data scientist) first builds the intent classification training code
    (`training-code/text-classification` at [https://github.com/orca3/MiniAutoML/tree/main/training-code/text-classification](https://github.com/orca3/MiniAutoML/tree/main/training-code/text-classification))
    and packages it into a Docker image ([http://mng.bz/WA5g](http://mng.bz/WA5g)).
    Next, Ivan submits a training request to the model training service to create
    a model training job. In the training request, he specifies which dataset (dataset
    ID) and training algorithm (Docker image name) to use in training.
  prefs: []
  type: TYPE_NORMAL
- en: note In this lab, we use a hardcoded dataset ID `"1"`. To test other datasets,
    please feel free to set any other dataset ID into the training request.
  prefs: []
  type: TYPE_NORMAL
- en: Once the training service receives the training request, it will launch a Docker
    container to run the intent classification training code provided by Ivan. In
    our demo, the Docker image is `orca3/intent-classification` ([http://mng.bz/WA5g](http://mng.bz/WA5g)).
    See code listing A.3 for a sample gRPC training request. Please run the lab script
    (`scripts/lab-003-first-training.sh` at [http://mng.bz/916j](http://mng.bz/916j))
    to kick off a model training job, which sets up the dependencies and parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Listing A.3 Submitting a training job to a training service
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Training the Docker image name
  prefs: []
  type: TYPE_NORMAL
- en: ❷ The ID of the dataset to train
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Dataset version
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Training hyperparameters
  prefs: []
  type: TYPE_NORMAL
- en: Once the training job starts, the training service will keep monitoring the
    training execution status and return a job ID for tracking purposes. With the
    job ID, Ivan can fetch the latest training job status and training metadata by
    querying the `GetTrainingStatus` API of the training service and the `GetRunStatus`
    API of the metastore service. See the sample query requests as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Listing A.4 Querying the model training job status and model metadata
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Model ID, also the training job ID
  prefs: []
  type: TYPE_NORMAL
- en: 'The training service can return training execution status in real time; see
    the sample response as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Training completes
  prefs: []
  type: TYPE_NORMAL
- en: 'Because the training Docker container reports real-time metrics, such as training
    accuracy, to the metadata store during training execution, the metadata store
    service can return real-time training metrics. See the sample model training metrics
    from the metadata store service as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Training status
  prefs: []
  type: TYPE_NORMAL
- en: ❷ The last message from the training container
  prefs: []
  type: TYPE_NORMAL
- en: ❸ The training job ID, as well as the model ID
  prefs: []
  type: TYPE_NORMAL
- en: ❹ The dataset identifier
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Training metric per epoch
  prefs: []
  type: TYPE_NORMAL
- en: After training is completed, Ivan informs Johanna that the model is ready to
    consume. In our lab, he passes the model ID (job ID = `"1"`) to Johanna so she
    knows which model to use.
  prefs: []
  type: TYPE_NORMAL
- en: Note All the API requests described in code listings A.3 and A.4 are automated
    in `scripts/lab-003-first-training.sh`; you can execute them in one go. In chapters
    3 and 4, we discuss in detail how the training service works.
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 4 is model serving.* Run `scripts/lab-004-model-serving.sh` ([http://mng.bz/815K](http://mng.bz/815K)).'
  prefs: []
  type: TYPE_NORMAL
- en: Johanna (application developer) is building a chatbot, so she wants to consume
    the newly trained intent classification model to categorize customers’ questions.
    When Ivan tells Johanna that the model is ready to use, Johanna sends a prediction
    request to the prediction service to test the newly trained model.
  prefs: []
  type: TYPE_NORMAL
- en: In the prediction request, Johanna specifies the model ID (`runId`) and document,
    where the document is the text message that is being categorized. And the sample
    prediction service will load the model requested in the prediction request automatically.
    You can see a sample gRPC prediction request in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing A.5 A sample model prediction gRPC request
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Model ID, as well as the training job ID
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Request body (text)
  prefs: []
  type: TYPE_NORMAL
- en: 'After executing the query (code listing A.5) or `scripts/lab-004-model-serving.sh`
    in the terminal, you will see output—a predicted category (label) from the intent
    classification model for the given text—from the model serving service as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: ❶ The predict category is “joy.”
  prefs: []
  type: TYPE_NORMAL
- en: note If you encounter any issues while trying to complete the lab, please check
    the latest instructions in the lab section ([https://github.com/orca3/MiniAutoML#lab](https://github.com/orca3/MiniAutoML#lab))
    of our GitHub repo’s README file. We aim to keep these instructions updated if
    the sample system is modified.
  prefs: []
  type: TYPE_NORMAL
- en: A.2.2 An exercise to do on your own
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we walked you through a completed model development cycle, it’s homework
    time. Imagine after a successful chatbot release, Johanna’s chatbot service needs
    to support a new category, *optimism*. This new requirement means the current
    intent classification model needs to be retrained to recognize the optimism type
    of text messages.
  prefs: []
  type: TYPE_NORMAL
- en: Feng and Ivan need to work together to build a new intent classification model.
    Feng needs to collect more training data with the “optimism” label and add it
    to the current dataset. Although Ivan doesn’t need to change the training code,
    he does need to trigger a training job in the training service with the updated
    dataset to build a new model.
  prefs: []
  type: TYPE_NORMAL
- en: By following the sample queries and scripts in section A.2.1, you should be
    able to complete the tasks for Feng and Ivan. If you want to check your results
    or get help completing these tasks, you can find our solution in the `scripts/lab-005-second-training.sh`
    file. We encourage you to experiment—or play—with this problem yourself before
    checking our solution.
  prefs: []
  type: TYPE_NORMAL
