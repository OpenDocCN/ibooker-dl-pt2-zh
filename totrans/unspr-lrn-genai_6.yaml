- en: 6 Dimensionality reduction (advanced)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: “Life is really simple, but we insist on making it complicated - Confucius”
  prefs: []
  type: TYPE_NORMAL
- en: Simplicity is a virtue. Both in life and in data science. We have discussed
    a lot of algorithms so far – a few of them are simple enough and some of them
    are a bit complicated. In Part one of the book, we studied simpler clustering
    algorithms and in the last chapter, we examined advanced clustering algorithms.
    Similarly, we studied a few dimensionality algorithms like PCA in chapter 3\.
    Continuing on the same note, we will study two advanced dimensionality reduction
    techniques in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The advanced topics we are covering this part and the next part of the book
    are meant to prepare you for complex problems. Whilst you can apply these advanced
    solutions, it is always advisable to start with the classical solution like PCA
    for dimensionality reduction. And if the solution achieved it not at par, then
    you can try the advanced solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction is one of the most sought-after solution particularly
    when we have a large number of variables. Recall “Curse of Dimensionality” we
    discussed in chapter 3\. You are advised to refresh chapter 3 before moving forward.
    We will cover t-distributed Stochastic Neighbour Embedding (t-SNE) and Multidimensional
    Scaling (MDS) in this chapter. This chapter will have some mathematical concepts
    which create the foundation of the advanced techniques we are going to discuss.
    As always, we will have the concept discussion followed by Python implementation.
    We will have a short case study at the end of the chapter. And, in this chapter
    we are developing a solution using images dataset too!
  prefs: []
  type: TYPE_NORMAL
- en: There can be a dilemma in your mind. What is the level of mathematics required
    and is an in-depth statistical knowledge is a pre-requisite? The answer is both
    Yes and No. Whilst, having a mathematical understanding will allow you to understand
    the algorithms and appreciate the process in greater depth; at the same time for
    real-world business implementation sometimes one might want to skip the mathematics
    and directly move to the implementation in Python. We would suggest to have at
    least more than basic understanding of the mathematics to fully grasp the concept.
    In this book, we are providing that level of mathematical support without going
    in too much depth – an optimal mix of practical world and mathematical concepts.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this sixth chapter of the book, we are going to cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: t-distributed Stochastic Neighbour Embedding (t-SNE)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Multidimensional Scaling (MDS)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Python implementations of the algorithms
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Case study
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Welcome to the sixth chapter and all the very best!
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Technical toolkit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will continue to use the same version of Python and Jupyter notebook as we
    have used so far. The codes and datasets used in this chapter have been checked-in
    at [https://github.com/vverdhan/UnsupervisedLearningWithPython/tree/main/Chapter%206](main.html)
    location.
  prefs: []
  type: TYPE_NORMAL
- en: You would need to install Keras as an additional Python libraries in this chapter
    . Along with this we will need the regular libraries – numpy, pandas, matplotlib,
    seaborn, sklearn. Using libraries, we can implement the algorithms very quickly.
    Otherwise, coding these algorithms is quite a time-consuming and painstaking task.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started with Chapter 6 of the book!
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Multidimensional Scaling (MDS)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I love to travel. Unfortunately, due to the COVID pandemic, the travelling has
    taken a hit. As you know, maps prove to be quite handy while travelling. Now,
    imagine you are given a task. You receive distances between some cities around
    the world. For example, between London and New York, London and Paris, Paris and
    New Delhi and so forth. And then we ask you to recreate the map from which these
    distances have been derived. If we have to recreate that two-dimensional map,
    that will be through trial and error, we will make some assumptions and move ahead
    with the process. It will surely be a tiring exercise prone to error and quite
    time consuming indeed. MDS can do this task easily for us.
  prefs: []
  type: TYPE_NORMAL
- en: While thinking of the above example, ignore the fact that earth is not flat.
    And assume that the distance measurement metric is constant. For example, there
    is no confusion in miles or kilometres.
  prefs: []
  type: TYPE_NORMAL
- en: As an illustration, consider the Figure 6-1.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6-1 Illustration of distance between the cities and if they are represented
    on a map. The figure is only to help develop an understanding and does not represent
    the actual results.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![06_01](images/06_01.png)'
  prefs: []
  type: TYPE_IMG
- en: Formally put, if we have x data points, multidimensional scaling (MDS) can help
    us in converting the information of the pairwise distance between these x points
    to a configuration of points in a Cartesian space. Or simply put, MDS transforms
    a large dimensional dataset into a lower dimensional one and in the process keeping
    the distance or the similarity between the points same.
  prefs: []
  type: TYPE_NORMAL
- en: To simplify, consider the image below. Here we have three points - A, B and
    C. We are representing these points in a 3D space. And then we are representing
    the three points in a 2D space and finally they are represented in a 1D space.
    The distance between the points is not up to scale in the diagrams below in Figure
    6-2\. The example shown in Figure 6-2 represents the meaning of lowering the number
    of dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6-2 Representation of three points – first we are showing three points
    in a three-dimensional space. Then they are being represented in a 2D space and
    then finally in a single dimensional space.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![06_02](images/06_02.png)'
  prefs: []
  type: TYPE_IMG
- en: Hence, in MDS a multidimensional data is reduced to lower number of dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: We can have three types of MDS algorithms
  prefs: []
  type: TYPE_NORMAL
- en: Classical MDS,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Metric multidimensional scaling and
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Non-metric multidimensional scaling.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will examine metric MDS process in detail in the book while we will cover
    the classical and non-metric briefly.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine we have two points – i and j. Let us assume that the original distance
    between two points is d[ij] and the corresponding distance in the lower dimensional
    space is d[ij].
  prefs: []
  type: TYPE_NORMAL
- en: In classical MDS, the distances between the points are treated as Euclidean
    distances and the original and fitted distances are represented in the same metric.
    It means that if the original distances in higher dimensional space are calculated
    using Euclidean method, the fitted distances in lower dimensional are also calculated
    using Euclidean distance. We already know how to calculate Euclidean distances.
    For example, we have to find the distance between points i and j and let’s say
    the distance is d[ij]. The distance can be given by the Euclidean distance formula
    given by Equation 6-1.
  prefs: []
  type: TYPE_NORMAL
- en: '![06_02a](images/06_02a.png)'
  prefs: []
  type: TYPE_IMG
- en: Recall in earlier chapters, we have discussed other distance functions like
    Manhattan, Hamming distance etc. You are advised to refresh them.
  prefs: []
  type: TYPE_NORMAL
- en: We will now come to non-metric MDS. We just now noted that Euclidean distance
    can be used to calculate the distance between two points. Sometimes it is not
    possible to take the actual values of the distances like when d[ij] is the result
    of an experiment where subjective assessments were made. Or in other words, where
    a rank was allocated to the various data parameters. For example, if the distance
    between point 2 and 5 was at rank 4 in the original data, in such a scenario,
    it will not be wise enough to use absolute values of d[ij] and hence relative
    values or *rank-values* have to be used. This is the process in non-metric MDS.
    For example, imagine we have four points – A, B, C and D. We wish to rank the
    respective distances between these four points. The respective combinations of
    points can be – A and B, A and C, A and D, B and C, B and D, and final C and D.
    Their distances can be ranked as shown in the Table 6-1.
  prefs: []
  type: TYPE_NORMAL
- en: Table 6-1 Representing the respective distance between four points and the ranks
    of the distances
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Pair of the points | Distance | Ranks of the respective distances |'
  prefs: []
  type: TYPE_TB
- en: '| A and B | 100 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| A and C | 105 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| A and D | 95 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| B and C | 205 | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| B and D | 150 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| C and D | 55 | 1 |'
  prefs: []
  type: TYPE_TB
- en: So, in non-metric MDS method, instead of using the actual distances we use the
    respective ranks of the distance. We will now move to metric MDS method.
  prefs: []
  type: TYPE_NORMAL
- en: We know that in classical MDS, the original and fitted distances are represented
    in the same metric. In *metric MDS*, it is assumed that the values of d[ij] can
    be transformed into Euclidean distances by employing some parametric transformation
    on the datasets. In some articles, you might find classical and metric MDS to
    be used interchangeably.
  prefs: []
  type: TYPE_NORMAL
- en: In MDS, as a first step, the respective distances between the points are calculated.
    Once the respective distances have been calculated, then MDS will try to represent
    the higher dimensional data point into lower dimensional space. To perform this,
    an optimization process has to be carried so that the most optimum number of resultant
    dimensions can be chosen. And hence, a loss function or cost function has to be
    optimized.
  prefs: []
  type: TYPE_NORMAL
- en: If you do not know what is a cost function, go through this section below.
  prefs: []
  type: TYPE_NORMAL
- en: Cost function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We use algorithms to predict the values of a variable. For example, we might
    use some algorithm to predict the expected demand of a product next year. We would
    want the algorithm to predict as much accurate as possible. Cost functions are
    a simple method to check the performance of the algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Cost function is a simple technique to measure the effectiveness of our algorithms.
    It is the most common method used to gauge the performance of a predictive model.
    It compares the original values and the predicted values by the algorithm and
    calculates how wrong the model is in its prediction.
  prefs: []
  type: TYPE_NORMAL
- en: As you would imagine, in an ideal solution, we would want the predicted values
    to be the same as the actual values, which is very difficult to achieve. If the
    predicted values differ a lot from the actual values, the output of a cost function
    is higher. If the predicted values are closer to the actual values, then the value
    of a cost function is lower. A robust solution is one which has a lower value
    of the cost function. Hence, the objective to optimize any algorithm will be to
    minimize value of the cost function. Cost function is also referred as *loss function*,
    these two terms can be used interchangeably.
  prefs: []
  type: TYPE_NORMAL
- en: 'In metric MDS, we can also call the cost function as **Stress**. The formula
    for Stress is given by Equation 6-2 as given below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![06_02b](images/06_02b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s understand the equation now:'
  prefs: []
  type: TYPE_NORMAL
- en: Term Stress[D] is the value MDS function has to minimize.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The data points with the new set of coordinates in a lower dimensional space
    are represented by x[, x[2], x[3]…. x[N].]
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The term ||x[i] – x[j]|| is the distance between two points in their lower dimensional
    space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The term d[ij] is original distance between the two points in the original multi-dimensional
    space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By looking at the equation, we can clearly understand that if the values of
    ||x[i] – x[j]|| and d[ij] are close to each other, the value of the resultant
    stress will be small.
  prefs: []
  type: TYPE_NORMAL
- en: Minimizing the value of stress is the objective of the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: To optimize this loss function, multiple approaches can be used. One of the
    most famous method is using a gradient descent which was originally proposed by
    Kruskal and Wish in 1978\. The gradient descent method is very simple to understand
    and can be explained using a simple analogy.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you are standing on top of a mountain and you want to get down. While
    doing so, you want to choose the fastest path because you want to get down as
    fast as possible (no, you cannot jump!). So, to take the first step, you will
    look around and whichever is the steepest path, you can take a step in that direction
    and you will reach a new point. And then again, you will take a step in the steepest
    direction. We are showing that process in Figure 6-3(i).
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6-3 (i) The first figure is of a person standing on top of a mountain
    and trying to get down. The process of gradient descent follows this method (ii)
    The actual process of optimization of a cost function in gradient descent process.
    Note that at the point of convergence, the value of the cost function is minimum.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![06_03](images/06_03.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, if an algorithm has to achieve the similar feat, the process can be represented
    in Figure 6-3 (ii), wherein a loss function starts at a point and finally reaches
    the Point of convergence. At this point of convergence, the cost function is minimum.
  prefs: []
  type: TYPE_NORMAL
- en: MDS differs from the other dimensionality reduction techniques. As compared
    to techniques like PCA, MDS does not make any assumptions about the data set and
    hence can be used for a larger type of datasets. Moreover, MDS allows to use any
    distance measurement metric. Unlike PCA, MDS is not an eigenvalue-eigenvector
    technique. Recall in PCA, the first axis captures the maximum amount of variance,
    second axis has the next best variance and so on. In MDS, there is no such condition.
    The axes in MDS can be inverted or rotated as per the need. Next, in most of the
    other dimensional reduction methods used, the algorithms do calculate a lot of
    axes but they cannot be viewed. In MDS, smaller number of dimensions are explicitly
    chosen at the start. And hence, there is less ambiguity in the solution. Further,
    in other solutions generally there is only one unique solution whereas MDS tries
    to iteratively find the acceptable solution. It means in MDS there can be multiple
    solutions for the same dataset.
  prefs: []
  type: TYPE_NORMAL
- en: But at the same time, the computation time required for MDS is higher for bigger
    datasets. And there is a catch in the Gradient Descent method used for optimization.
    Refer to Figure 6-4\. Let’s refer to the mountain example we covered in the last
    section. Imagine that while you are coming down from the top of the mountain.
    The starting point is A and the bottom of the mountain is at point C. While you
    are coming down, you reached point B. As you can see in Figure 6-4(i), there is
    a slight elevation around point B. At this point B, you might incorrectly conclude
    that you have reached the bottom of the mountain. In other words, you will think
    that you have finished your task. This is the precise problem of the local minima.
  prefs: []
  type: TYPE_NORMAL
- en: It is a possibility that instead of global minima, the loss function might be
    stuck in a local minima. The algorithm might think that it has reached the point
    of convergence, while the complete convergence might not have been achieved still
    and we are at local minima.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6-4 While the first figure is the point of convergence and represents
    the gradient descent method, note that in the second figure the global minima
    is somewhere else, while the algorithm can be stuck at a local minima. The algorithm
    might believe that it has optimized the cost function and reached the point of
    global minima whereas, it has only reached the local minima.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![06_04](images/06_04.png)'
  prefs: []
  type: TYPE_IMG
- en: There is still a question to be answered about the efficacy of the MDS solution.
    How can we measure the effectiveness of the solution? In the original paper, Kruskal
    has recommended about the stress values to measure the goodness-of-fit of the
    solution which are shown in Table 6-1\. The recommendations are mostly based on
    empirical experience of Kruskal. These stress values are based on Kruskal’s experience.
  prefs: []
  type: TYPE_NORMAL
- en: '| Stress Values | Goodness-of-fit |'
  prefs: []
  type: TYPE_TB
- en: '| 0.200 | Poor |'
  prefs: []
  type: TYPE_TB
- en: '| 0.100 | Fair |'
  prefs: []
  type: TYPE_TB
- en: '| 0.050 | Good |'
  prefs: []
  type: TYPE_TB
- en: '| 0.025 | Excellent |'
  prefs: []
  type: TYPE_TB
- en: '| 0.000 | Perfect |'
  prefs: []
  type: TYPE_TB
- en: The next logical question is – how many final dimensions we should choose? Scree
    plot provides the answer as shown in Figure 6-5\. Recall, in chapter 2 we used
    the similar elbow method to choose the optimal number of clusters in kmeans clustering.
    For MDS too, we can have the elbow method to determine the most optimal number
    of components to represent the data.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6-5  Scree plot to find the most optimal number of components. It is
    similar to the kmeans solution we have discussed in the earlier chapters; we have
    to look for the elbow in the plot.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![06_05](images/06_05.png)'
  prefs: []
  type: TYPE_IMG
- en: This concludes our discussion on MDS. We will now move to the Python implementation
    of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '![](images/tgt.png) POP QUIZ – answer these question to check your understanding..
    Answers at the end of the book'
  prefs: []
  type: TYPE_NORMAL
- en: 1.   What is the difference between metric and non-metric MDS algorithm?
  prefs: []
  type: TYPE_NORMAL
- en: 2.   Gradient descent is used to maximise the cost. True or False.
  prefs: []
  type: TYPE_NORMAL
- en: 3.   Explain gradient descent method using a simple example.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.1     Python implementation of MDS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will now have the Python implementation of MDS method. We will use the famous
    Iris dataset which we have used previously too. The implementation of the algorithm
    is quite simple, thanks to the libraries available in the scikit learn package.
  prefs: []
  type: TYPE_NORMAL
- en: The implementation is generally simple as the heavy lifting is done by the libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: We will first load the libraries. The usual suspects are `sklearn`,
    `matplotlib`, `numpy` and we also load `MDS` from `sklearn`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 2: Load the data set now. Iris dataset is available in the `sklearn` library
    so we need not import excel or .csv file here.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 3: A requirement for MDS is that the dataset should be scaled before the
    actual visualization is done. We are using `MixMaxScalar()` function to achieve
    the same. MinMax scaling simply scales the data using the formula below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![06_F01](images/06_F01.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As an output of this step, the data is scaled and ready for the next step of
    modelling.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: We now invoke the MDS method from sklearn library. The random_state
    value allows us to reproduce the results. We have decided the number of components
    as 3 for the example.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 5: We will now fit the scaled data created earlier using the MDS model.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 6: We are now declaring the colors we wish to use for visualization. And
    next, the data points are visualized in a scatter plot.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the code above can be shown below in Figure 6-6:'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6-6 Output for the IRIS data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![06_06](images/06_06.png)'
  prefs: []
  type: TYPE_IMG
- en: The above example of Python implementation is a visualization of the IRIS data.
    It is quite simple example but it does not involve *stress* and optimization for
    the number of components. *We* will now work on a curated dataset to implement
    MDS.
  prefs: []
  type: TYPE_NORMAL
- en: '![06_06a](images/06_06a.png)'
  prefs: []
  type: TYPE_IMG
- en: Let us assume we have five cities and the respective distance between them is
    given in Table 6-2.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: We have already imported the libraries in the last code.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 2: Let’s now create the dataset. We are creating the dataset here, but
    in real business scenarios it will be in the form of distances only.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![06_06b](images/06_06b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Step 3: We will now use the MinMaxScalar() function to scale the dataset as
    we did in the last coding exercise.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 4: Now, let’s work towards finding the most optimal number of components.
    We will iterate for different values of number of components. For each of the
    value of number of components, we will get the value of stress. And at a point,
    where a kink is observed, that is the most optimal number of components.'
  prefs: []
  type: TYPE_NORMAL
- en: As a first step, we will declare an empty dataframe which can be used to store
    the values of number of components and corresponding stress values. Then, we are
    iterating from 1 to 10 in a for loop. And finally, for each of the values of components
    (1 to 10), we get the respective values of stress.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 5: We have got the values of stress. We will now plot these values in
    a graph. The respective labels for each of the axes are also given. Look at the
    kink at values 2 and 3\. This can be the values of optimal values of number of
    components.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Figure 6-7 Scree plot to select the optimized number of components
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![06_07](images/06_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Step 6: We will now run the solution for number of components = 3\. If we look
    at the values of stress, number of components = 3, it generates the minimum values
    of stress as 0.00665.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Figure 6-8 Output for the MDS dataset, representation of the 5 cities in a plot
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![06_08](images/06_08.png)'
  prefs: []
  type: TYPE_IMG
- en: This concludes our section on MDS algorithm. We discussed the foundation and
    concepts, pros and cons, algorithm assessment and Python implementation of MDS.
    It is a great solution for visualization and dimensionality reductions. It is
    one of the non-linear dimensionality reductions methods.
  prefs: []
  type: TYPE_NORMAL
- en: We will now move to t-SNE, second dimensionality reduction methods in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 t-distributed stochastic neighbor embedding (t-SNE)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If a data set is really high dimensional, the analysis becomes cumbersome. The
    visualization is even more confusing. We have covered that in great detail in
    Curse of Dimensionality section in Chapter 2\. You are advised to revisit the
    concept before proceeding.
  prefs: []
  type: TYPE_NORMAL
- en: One such really high-dimensional dataset can be image data. We find it difficult
    to comprehend such data which is really high-dimensional.
  prefs: []
  type: TYPE_NORMAL
- en: You would have used facial recognition software in your smartphones. For such
    solutions, facial images have to analyzed and machine learning models have to
    be trained. Look at the pictures below in Figure 6-9– we have a human face, a
    bike, a vacuum cleaner and screen capture of a game.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6-9 Images are quite complex to decipher by an algorithm. Images can
    be of any form and can be of a person, or an equipment or even any game screen.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![06_09](images/06_09.png)'
  prefs: []
  type: TYPE_IMG
- en: Image is a complex data point. Each image is made up of pixels, and each pixel
    can be made up of RGB (red, green, blue) values. And values for each of the red,
    green, blue can range from 0 to 255\. The resulting dataset will be a very high-dimensional
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Now, recall Principal Component Analysis (PCA) we studied in Chapter 3\. PCA
    is a linear algorithm. And being a linear algorithm, its capability is limited
    to resolve non-linear and complex polynomial functions. Moreover, when a *high-dimensional*
    dataset has to represented in a low-dimensional space the algorithm should keep
    similar datapoints close to each other, which can be challenge in linear algorithms.
    PCA being a linear dimension reduction technique, it tries to separate the different
    data points as far away from each other as PCA is trying to maximize the variance
    between the data points. The resulting analysis is not robust and might not be
    best suited for further usage and visualization. Hence, we have non-linear algorithms
    like t-SNE to help.
  prefs: []
  type: TYPE_NORMAL
- en: Formally put, t-SNE is a *non-linear dimensionality reduction technique* which
    is quite handy for high dimensional data. It is based on Stochastic Neighbor Embedding
    which was developed by Sam Roweis and Geoffrey Hinton. The t-distributed variant
    was proposed by Lauren van der Maaten. So, t-SNE is an improvement on the SNE
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: At a high level, SNE measures the similarity between instances pairs in a high-dimensional
    space and in a low dimensional space. A good solution is where the difference
    between these similarity measures is the least and hence SNE then optimizes these
    similarity measure using a cost function.
  prefs: []
  type: TYPE_NORMAL
- en: We will examine the step-by-step process of t-SNE now. The process described
    below is a little heavy on mathematics.
  prefs: []
  type: TYPE_NORMAL
- en: Consider we have a high-dimensional space and we have some points in this high-dimensional
    space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will now measure the similarities between the various points. For a point
    x[i], we will then create a Gaussian distribution centered at that point. We have
    already studied Gaussian or normal distribution is the last chapters of the book.
    The Gaussian distribution is shown in Figure 6-10.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 6-10 Gaussian or normal distribution which we have already studied earlier.
    Image has been taken from Wikipedia.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![06_10](images/06_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we will measure the density of points (let’s say x[j]) which fall under
    that Gaussian Distribution and then we renormalize them to get the respective
    conditional probabilities (p[j|i]). For the points which are nearby and hence
    similar, this conditional probability will be high and for the points which are
    far and dissimilar, the value of conditional probabilities (p[j|i]) will be very
    small. These values of probabilities are the ones in high-dimensional space. For
    the curious ones, the mathematical formula for this conditional probability is
    Equation 6-3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![06_10a](images/06_10a.png)'
  prefs: []
  type: TYPE_IMG
- en: where σ is the variance of the Gaussian Distribution centered at x[i]. The mathematical
    proof is beyond the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Now we will measure one more set of probabilities in the low-dimensional space.
    For this set of measurements, we use *Cauchy Distribution*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cauchy Distribution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Cauchy distribution, belongs to the family of continuous probability distributions.
    Though there is a resemblance with the normal distribution, as we have represented
    in Figure 6-11, the Cauchy distribution has narrower peak and spreads out more
    slowly. It means that as compared to a normal distribution, the probability of
    obtaining values far from the peaks are higher. Sometimes, Cauchy distribution
    is also known as *Lorentz distribution.* It is interesting to note that Cauchy
    does not a well-defined mean but the median is the center of symmetry.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 6-11 Comparison of Gaussian distribution vs Cauchy distribution. (Image
    source: Quora)'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![06_11](images/06_11.png)'
  prefs: []
  type: TYPE_IMG
- en: Consider we get y[i] and y[j] as the low-dimensional counterparts for the high-dimensional
    data points x[i] and x[j]. So, we can calculate the probability score like we
    did in the last step. Using Cauchy distribution, we can get second set of probabilities
    q[j|i] too. The mathematical formula is shown below in Equation 6-4.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![06_11a](images/06_11a.png)'
  prefs: []
  type: TYPE_IMG
- en: So far, we have calculated two set of probabilities (p[j|i]) and (q[j|i]). In
    this step, we compare the two distributions and measure the difference between
    the two. In other words, while calculating (p[j|i]) we measured the probability
    of similarity in a high-dimensional space whereas for (q[j|i]) we did the same
    in a low-dimensional space. Ideally, if the mapping of the two spaces to be similar
    and for that there should be not be any difference between (p[j|i]) and (q[j|i]).
    So, the SNE algorithm tries to minimize the difference in the conditional probabilities
    (p[j|i]) and (q[j|i]).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The difference between the two probability distributions is done using Kullback-Liebler
    divergence or KL divergence, which we will explore here.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: KL divergence
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: KL divergence or relative entropy is used to measure the difference between
    two probability distributions – usually one probability distribution is the data
    or the measured scores. The second probability distribution is an approximation
    or the prediction of the original probability distribution. For example, if the
    original probability distribution is X and the approximated one is Y. KL divergence
    can be used to measure the difference between X and Y probability distributions.
    In absolute terms if the value is 0 then it means that the two distributions are
    similar. The KL divergence is applicable for neurosciences, statistics and fluid
    mechanics.
  prefs: []
  type: TYPE_NORMAL
- en: To minimize the KL cost function, we use the gradient descent approach. We have
    already discussed the gradient descent approach in the section where we discussed
    MDS algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There is one more important point we should be aware while we work on t-SNE,
    an important hyperparameter called *perplexity*. Perplexity is a hyperparameter
    which allows us to control and optimize the number of close neighbors each of
    the data point has.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As per the official paper, a typical value for perplexity lies between 5 and
    50.
  prefs: []
  type: TYPE_NORMAL
- en: There can be one additional nuance – the output of a t-SNE algorithm might never
    be same on successive runs. We have to optimize the values of the hyperparameters
    to receive the best output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](images/tgt.png) POP QUIZ – answer these question to check your understanding..
    Answers at the end of the book'
  prefs: []
  type: TYPE_NORMAL
- en: 1.   Explain Cauchy distribution is your own words.
  prefs: []
  type: TYPE_NORMAL
- en: 2.   PCA is a non-linear algorithm. True or False.
  prefs: []
  type: TYPE_NORMAL
- en: 3.   KL divergence is used to measure the difference between two probability
    distributions. True or False
  prefs: []
  type: TYPE_NORMAL
- en: We will now proceed to the Python implementation of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.1 Python implementation of t-SNE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will use two datasets in this example. The first one is the already known
    IRIS dataset, which we have already used more than once in this book. The second
    dataset is quite an interesting one. It is MNIST dataset which is a database of
    handwritten digits. It is one of the most famous datasets used to train image
    processing solutions and generally is considered “Hello World” program for image
    detection solutions. An image representation is shown below in ().
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6-12 MNIST dataset- it is a collection of handwritten images of digits.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![06_12](images/06_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Step 1: We will first import the necessary libraries. Note that we have imported
    MNIST dataset from keras library.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 2: First we will work with the IRIS dataset. We will load the IRIS dataset.
    The dataset comprises of two parts – one is the “data” and second is the respective
    label or “target” for it. It means that “data” is the description of the data
    and “target” is the type of IRIS. We are printing the features and the labels
    using a piece of code.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 3: The next step is invoking the tSNE algorithm. We are using the number
    of components = 2 and random_state =5 to reproduce the results. And then the algorithm
    is used to fit the data.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![06_12a](images/06_12a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Step 4: We are now plotting the data. This step allows us to visualize the
    data fitted by the algorithm in the last step.'
  prefs: []
  type: TYPE_NORMAL
- en: First, we will initiate an empty dataframe. We will add three columns one at
    a time. We will start with iris_target, followed by tSNE_first_component and tSNE_second_component.
    tSNE_first_component is the first column of the fitted_data dataframe and hence
    the index is 0\. tSNE_second_component is the second column of the fitted_data
    dataframe and hence the index is 1\. Finally, we are representing the data in
    a scatterplot.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Figure 6-13 tSNE projection of the IRIS dataset. Note how we are getting three
    separate clusters for the three classes we have in the dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![06_13](images/06_13.png)'
  prefs: []
  type: TYPE_IMG
- en: Now we will implement the algorithm for MNIST dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: the libraries are already loaded in the last code example. Then we
    load the dataset. The dataset requires reshape which is done here'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Figure 6-14 Output of tSNE for the 10 classes of digits represented in different
    colors.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![06_14](images/06_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are a few important points which you should keep in mind while running
    tSNE:'
  prefs: []
  type: TYPE_NORMAL
- en: Run the algorithm with different values of hyperparameters before finalizing
    a solution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ideally, perplexity should be between 5 and 50 and for an optimized solution,
    the value of perplexity should be less than the number of points.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: tSNE guess the number of close neighbors for each of the point. And because
    of this reason, a dataset which is denser will require a much higher perplexity
    value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Particularly, we should note that perplexity is the hyper parameter which balances
    the attention given to both the local and global aspects of the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: tSNE is one of the widely popular algorithms. It is used for studying topology
    of an area, but a single tSNE cannot be used for making a final assessment. Instead,
    multiple tSNE plots should be created to make any final recommendation. Sometimes,
    there are complaints that tSNE is a black box algorithm. This might be true to
    a certain extent. What makes the adoption of tSNE harder is that it does not generate
    same results in successive iterations. Hence, you might find tSNE being recommended
    only for exploratory analysis.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our discussion on tSNE. We will now move to the case study.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Case study
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recall from chapter 3 where we explored a case study for telecom industry employing
    dimensionality. In this chapter, we will examine a small case study wherein tSNE
    or MDS can be utilized for dimensionality reduction.
  prefs: []
  type: TYPE_NORMAL
- en: Have you heard about hyperspectral images? As you know, we humans see the colors
    of visible light in mostly three bands – long wavelengths, medium ones and short
    wavelengths. The long wavelengths are perceived as red color, medium are green
    and short ones are perceived as blue color. Spectral imagining on the other hand,
    divides the spectrum into many greater numbers of bands and this technique can
    be extended beyond the visible ones and hence is of usage across biology, physics,
    geoscience, astronomy, agriculture and many more avenues.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperspectral imaging collects and processes information from across the electromagnetic
    spectrum. It obtains the spectrum for each of the pixel in the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6-15 Hyperspectral image of "sugar end" potato strips shows invisible
    defects (Image source: Wikipedia)'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![06_15](images/06_15.png)'
  prefs: []
  type: TYPE_IMG
- en: One such dataset can be the Pavia University Dataset. It is acquired by ROSIS
    sensor on Pavia, northern Italy. The details of the dataset are given below and
    the dataset can be downloaded from ([http://www.ehu.eus/ccwintco/uploads/e/ee/PaviaU.mat](ee.html)
    [http://www.ehu.eus/ccwintco/uploads/5/50/PaviaU_gt.mat](50.html))
  prefs: []
  type: TYPE_NORMAL
- en: In this dataset the spectral bands are 103, HIS size is 610*340 pixels and it
    contains 9 classes. Now, such a type of data can be used for crop analysis, mineral
    examining and explorations etc. Since this data contains information about the
    geological patterns, it is quite useful for scientific purpose. Before developing
    any image recognition solutions, we have to reduce the number of dimensions for
    this dataset. Moreover, the computation cost will be much higher if we have a
    large number of dimensions. Hence, it is obvious to have lesser number of representative
    number of dimensions. We are showing a few example bands of below. You are advised
    to download the dataset (which is also checked-in at the git Hub repo) and use
    the various dimensionality reduction techniques on the dataset to reduce the number
    of dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6-16 Example of bands in the dataset. These are only random examples,
    you are advised to load the dataset and run dimensionality reduction algorithms.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![06_16](images/06_16.png)'
  prefs: []
  type: TYPE_IMG
- en: There can be many other image datasets and complex business problems where tSNE
    and MDS can be of pragmatic usage. Some of such datasets have been listed in the
    next steps.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dimensionality reduction is quite an interesting and useful solution. It makes
    the machine learning less expensive and time consuming. Imagine that you have
    a dataset with thousands of attributes or features. You do not know the data very
    well; the business understanding is quite less and at the same time you have to
    find the patterns in the dataset. You are not even sure that if these variables
    are all relevant or just random noise. At such a moment, when we have to quickly
    reduce the number of dimensions in the dataset, make it less complex to crack
    and reduce the time – dimensionality reduction is the solution.
  prefs: []
  type: TYPE_NORMAL
- en: We covered dimensionality reduction techniques earlier in the book. This chapter
    covers two advanced techniques – tSNE and MDS. Both of these techniques should
    not be considered a substitute to the other easier techniques we discussed. Rather,
    they are two be used if we are not getting meaningful results. It is always advised
    to use PCA first, then try tSNE or MDS.
  prefs: []
  type: TYPE_NORMAL
- en: We are increasing the complexity in the book. This chapter started with images
    – we have only wet our toe though. In the next chapter, we are dealing with text
    data, perhaps you will find it very interesting and useful.
  prefs: []
  type: TYPE_NORMAL
- en: Practical next steps and suggested readings
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Use the vehicles dataset used in the Chapter2 for clustering and implement MDS
    on it. Compare the performance on clustering before and after implementing MDS.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get the datasets used in Chapter 2 for Python examples and use them for implementing
    MDS.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For MDS, you can refer to the following research papers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Dimensionality reduction: a comparative review by Lauren van der Maaten, Eric
    Postma and H. Japp Van Den Herik [https://www.researchgate.net/publication/228657549_Dimensionality_Reduction_A_Comparative_Review](publication.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Multidimensional scaling -based data dimension reduction method for application
    in short term traffic flow prediction for urban road network by Satish V. Ukkusuri
    and Jian Lu https://www.hindawi.com/journals/jat/2018/3876841/
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get tSNE research papers from the links below and study them
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visualizing data using t-SNE by Laurens van der Maaten and Geoffrey Hinton https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The art of using t-SNE for single cell transcriptomics [https://www.nature.com/articles/s41467-019-13056-x](articles.html)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There is one more paper which might be of interest- Performance evaluation of
    t-SNE and MDS dimensionality reduction techniques with KNN, SNN and SVM classifiers
    [https://arxiv.org/pdf/2007.13487.pdf](2007.html)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
