- en: 6 Dimensionality reduction (advanced)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 维度降低（高级）
- en: “Life is really simple, but we insist on making it complicated - Confucius”
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: “生活很简单，但我们坚持把它变复杂了 - 孔子”
- en: Simplicity is a virtue. Both in life and in data science. We have discussed
    a lot of algorithms so far – a few of them are simple enough and some of them
    are a bit complicated. In Part one of the book, we studied simpler clustering
    algorithms and in the last chapter, we examined advanced clustering algorithms.
    Similarly, we studied a few dimensionality algorithms like PCA in chapter 3\.
    Continuing on the same note, we will study two advanced dimensionality reduction
    techniques in this chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 简单是一种美德，无论是在生活中还是在数据科学中都是如此。到目前为止，我们已经讨论了很多算法 - 其中一些足够简单，而另一些则有些复杂。在本书的第一部分，我们学习了更简单的聚类算法，在最后一章中，我们考察了高级聚类算法。同样，在第3章中，我们学习了几个维度算法，如PCA。在同样的基础上，我们将在本章中学习两种高级降维技术。
- en: The advanced topics we are covering this part and the next part of the book
    are meant to prepare you for complex problems. Whilst you can apply these advanced
    solutions, it is always advisable to start with the classical solution like PCA
    for dimensionality reduction. And if the solution achieved it not at par, then
    you can try the advanced solutions.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本书本部分和下一部分涵盖的高级主题旨在为您准备复杂问题。虽然您可以应用这些高级解决方案，但始终建议从经典解决方案（如PCA进行维度降低）开始。如果获得的解决方案不够优秀，则可以尝试高级方案。
- en: Dimensionality reduction is one of the most sought-after solution particularly
    when we have a large number of variables. Recall “Curse of Dimensionality” we
    discussed in chapter 3\. You are advised to refresh chapter 3 before moving forward.
    We will cover t-distributed Stochastic Neighbour Embedding (t-SNE) and Multidimensional
    Scaling (MDS) in this chapter. This chapter will have some mathematical concepts
    which create the foundation of the advanced techniques we are going to discuss.
    As always, we will have the concept discussion followed by Python implementation.
    We will have a short case study at the end of the chapter. And, in this chapter
    we are developing a solution using images dataset too!
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们有很多变量时，降维是最受追捧的解决方案之一。请回忆一下第3章中讨论过的“维度诅咒”。在继续之前，建议您回顾第3章。我们将在本章中涵盖t分布随机邻居嵌入（t-SNE）和多维缩放（MDS）。本章将涉及一些创建我们将讨论的高级技术基础的数学概念。像往常一样，我们将有概念讨论，接着是Python实现。本章末尾将有一个简短的案例研究。此外，在本章中，我们也将使用图像数据集开发解决方案！
- en: There can be a dilemma in your mind. What is the level of mathematics required
    and is an in-depth statistical knowledge is a pre-requisite? The answer is both
    Yes and No. Whilst, having a mathematical understanding will allow you to understand
    the algorithms and appreciate the process in greater depth; at the same time for
    real-world business implementation sometimes one might want to skip the mathematics
    and directly move to the implementation in Python. We would suggest to have at
    least more than basic understanding of the mathematics to fully grasp the concept.
    In this book, we are providing that level of mathematical support without going
    in too much depth – an optimal mix of practical world and mathematical concepts.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 您的脑海中可能会有一个困惑。需要什么水平的数学知识，深入的统计知识是先决条件吗？答案既是肯定也是否定。虽然，具备数学理解能让您更深入地理解算法并欣赏过程；同时，对于实际业务实现，有时可能想跳过数学直接转向Python实现。我们建议您至少具有超过基本的数学理解以充分把握概念。在本书中，我们提供这种数学支持水平而不过度深入
    - 实践世界和数学概念的最佳混合。
- en: 'In this sixth chapter of the book, we are going to cover the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的第六章中，我们将介绍以下主题：
- en: t-distributed Stochastic Neighbour Embedding (t-SNE)
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: t分布随机邻居嵌入（t-SNE）
- en: Multidimensional Scaling (MDS)
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 多维缩放（MDS）
- en: Python implementations of the algorithms
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 算法的Python实现
- en: Case study
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 案例研究
- en: Welcome to the sixth chapter and all the very best!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到第六章，并祝你好运！
- en: 6.1 Technical toolkit
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 技术工具箱
- en: We will continue to use the same version of Python and Jupyter notebook as we
    have used so far. The codes and datasets used in this chapter have been checked-in
    at [https://github.com/vverdhan/UnsupervisedLearningWithPython/tree/main/Chapter%206](main.html)
    location.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用与迄今为止相同版本的Python和Jupyter笔记本。本章中使用的代码和数据集已在[https://github.com/vverdhan/UnsupervisedLearningWithPython/tree/main/Chapter%206](main.html)位置检查过。
- en: You would need to install Keras as an additional Python libraries in this chapter
    . Along with this we will need the regular libraries – numpy, pandas, matplotlib,
    seaborn, sklearn. Using libraries, we can implement the algorithms very quickly.
    Otherwise, coding these algorithms is quite a time-consuming and painstaking task.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您需要安装 Keras 作为额外的 Python 库。除此之外，我们还需要常规的库 - numpy、pandas、matplotlib、seaborn、sklearn。使用库，我们可以非常快速地实现算法。否则，编写这些算法是非常耗时和痛苦的任务。
- en: Let’s get started with Chapter 6 of the book!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始学习本书的第六章吧！
- en: 6.2 Multidimensional Scaling (MDS)
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 多维缩放（MDS）
- en: I love to travel. Unfortunately, due to the COVID pandemic, the travelling has
    taken a hit. As you know, maps prove to be quite handy while travelling. Now,
    imagine you are given a task. You receive distances between some cities around
    the world. For example, between London and New York, London and Paris, Paris and
    New Delhi and so forth. And then we ask you to recreate the map from which these
    distances have been derived. If we have to recreate that two-dimensional map,
    that will be through trial and error, we will make some assumptions and move ahead
    with the process. It will surely be a tiring exercise prone to error and quite
    time consuming indeed. MDS can do this task easily for us.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢旅行。不幸的是，由于 COVID 大流行，旅行受到了打击。正如您所知，地图在旅行中非常方便。现在，想象一下你被分配了一个任务。你收到了世界各地一些城市之间的距离。例如，伦敦和纽约之间，伦敦和巴黎之间，巴黎和新德里之间等等。然后我们要求您重新创建生成这些距离的地图。如果我们必须重新创建那个二维地图，那将通过试错，我们将做出一些假设并继续进行该过程。这肯定是一项令人疲惫的练习，容易出错，而且确实非常耗时。MDS
    可以轻松地为我们完成这项任务。
- en: While thinking of the above example, ignore the fact that earth is not flat.
    And assume that the distance measurement metric is constant. For example, there
    is no confusion in miles or kilometres.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑上述例子时，请忽略地球不是平的这一事实。并假设距离测量度量是恒定的。例如，英里或公里之间没有混淆。
- en: As an illustration, consider the Figure 6-1.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 作为例证，考虑图 6-1。
- en: Figure 6-1 Illustration of distance between the cities and if they are represented
    on a map. The figure is only to help develop an understanding and does not represent
    the actual results.
  id: totrans-20
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6-1 表示城市之间的距离以及它们在地图上的表示。该图仅用于帮助理解，并不代表实际结果。
- en: '![06_01](images/06_01.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![06_01](images/06_01.png)'
- en: Formally put, if we have x data points, multidimensional scaling (MDS) can help
    us in converting the information of the pairwise distance between these x points
    to a configuration of points in a Cartesian space. Or simply put, MDS transforms
    a large dimensional dataset into a lower dimensional one and in the process keeping
    the distance or the similarity between the points same.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 正式地说，如果我们有 x 个数据点，多维缩放（MDS）可以帮助我们将这些 x 点之间的成对距离的信息转换为笛卡尔空间中点的配置。或者简单地说，MDS 将一个大的维度数据集转换为一个较低维度的数据集，并在这个过程中保持点之间的距离或相似度不变。
- en: To simplify, consider the image below. Here we have three points - A, B and
    C. We are representing these points in a 3D space. And then we are representing
    the three points in a 2D space and finally they are represented in a 1D space.
    The distance between the points is not up to scale in the diagrams below in Figure
    6-2\. The example shown in Figure 6-2 represents the meaning of lowering the number
    of dimensions.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，考虑下面的图片。这里有三个点 - A、B 和 C。我们将这些点表示在一个三维空间中。然后我们在二维空间中表示这三个点，最后它们在一维空间中表示。在下图
    6-2 中，图中的点之间的距离不是按比例的。图 6-2 中的示例表示降低维度数量的含义。
- en: Figure 6-2 Representation of three points – first we are showing three points
    in a three-dimensional space. Then they are being represented in a 2D space and
    then finally in a single dimensional space.
  id: totrans-24
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6-2 表示三个点 - 首先我们展示三个点在三维空间中。然后它们被表示在二维空间中，最后在一维空间中。
- en: '![06_02](images/06_02.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![06_02](images/06_02.png)'
- en: Hence, in MDS a multidimensional data is reduced to lower number of dimensions.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在 MDS 中，多维数据被降低到较低维度。
- en: We can have three types of MDS algorithms
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以有三种类型的 MDS 算法
- en: Classical MDS,
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 经典 MDS，
- en: Metric multidimensional scaling and
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 度量多维缩放和
- en: Non-metric multidimensional scaling.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 非度量多维缩放。
- en: We will examine metric MDS process in detail in the book while we will cover
    the classical and non-metric briefly.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在书中详细讨论度量 MDS 过程，同时我们将简要介绍经典的和非度量的方法。
- en: Imagine we have two points – i and j. Let us assume that the original distance
    between two points is d[ij] and the corresponding distance in the lower dimensional
    space is d[ij].
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下我们有两个点 – i 和 j。假设两点之间的原始距离是 d[ij]，而低维空间中的相应距离是 d[ij]。
- en: In classical MDS, the distances between the points are treated as Euclidean
    distances and the original and fitted distances are represented in the same metric.
    It means that if the original distances in higher dimensional space are calculated
    using Euclidean method, the fitted distances in lower dimensional are also calculated
    using Euclidean distance. We already know how to calculate Euclidean distances.
    For example, we have to find the distance between points i and j and let’s say
    the distance is d[ij]. The distance can be given by the Euclidean distance formula
    given by Equation 6-1.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在经典 MDS 中，点之间的距离被视为欧几里得距离，原始距离和拟合距离在相同的度量中表示。这意味着，如果在高维空间中使用欧几里得方法计算原始距离，那么在低维空间中计算的拟合距离也是使用欧几里得距离计算的。我们已经知道如何计算欧几里得距离了。例如，我们要找到点
    i 和 j 之间的距离，假设距离为 d[ij]。距离可以用方程式 6-1 给出的欧几里得距离公式给出。
- en: '![06_02a](images/06_02a.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![06_02a](images/06_02a.png)'
- en: Recall in earlier chapters, we have discussed other distance functions like
    Manhattan, Hamming distance etc. You are advised to refresh them.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在早些章节中，我们已经讨论过其他距离函数，如曼哈顿距离、汉明距离等。建议你进行复习。
- en: We will now come to non-metric MDS. We just now noted that Euclidean distance
    can be used to calculate the distance between two points. Sometimes it is not
    possible to take the actual values of the distances like when d[ij] is the result
    of an experiment where subjective assessments were made. Or in other words, where
    a rank was allocated to the various data parameters. For example, if the distance
    between point 2 and 5 was at rank 4 in the original data, in such a scenario,
    it will not be wise enough to use absolute values of d[ij] and hence relative
    values or *rank-values* have to be used. This is the process in non-metric MDS.
    For example, imagine we have four points – A, B, C and D. We wish to rank the
    respective distances between these four points. The respective combinations of
    points can be – A and B, A and C, A and D, B and C, B and D, and final C and D.
    Their distances can be ranked as shown in the Table 6-1.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将来到非度量 MDS。刚才我们注意到欧几里得距离可以用来计算两点之间的距离。有时候无法采用距离的实际值，比如当 d[ij] 是一个实验的结果，主观评价被做出时。换句话说，各个数据参数被分配了一个排名。例如，如果点
    2 和 5 之间的距离在原始数据中排名第 4，那么在这种情况下，使用 d[ij] 的绝对值就不明智了，因此必须使用相对值或*排名值*。这就是非度量 MDS
    的过程。例如，想象一下我们有四个点 – A、B、C 和 D。我们希望排列这四个点之间的相应距离。点的相应组合可以是 – A 和 B、A 和 C、A 和 D、B
    和 C、B 和 D，最后是 C 和 D。它们的距离可以按照表 6-1 中所示的排名进行排列。
- en: Table 6-1 Representing the respective distance between four points and the ranks
    of the distances
  id: totrans-37
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 6-1 代表着四个点之间的相应距离以及距离的排名
- en: '| Pair of the points | Distance | Ranks of the respective distances |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 点对 | 距离 | 相应距离的排名 |'
- en: '| A and B | 100 | 3 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| A 和 B | 100 | 3 |'
- en: '| A and C | 105 | 4 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| A 和 C | 105 | 4 |'
- en: '| A and D | 95 | 2 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| A 和 D | 95 | 2 |'
- en: '| B and C | 205 | 6 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| B 和 C | 205 | 6 |'
- en: '| B and D | 150 | 5 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| B 和 D | 150 | 5 |'
- en: '| C and D | 55 | 1 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| C 和 D | 55 | 1 |'
- en: So, in non-metric MDS method, instead of using the actual distances we use the
    respective ranks of the distance. We will now move to metric MDS method.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在非度量 MDS 方法中，我们不是使用实际距离，而是使用距离的相应排名。现在我们将转向度量 MDS 方法。
- en: We know that in classical MDS, the original and fitted distances are represented
    in the same metric. In *metric MDS*, it is assumed that the values of d[ij] can
    be transformed into Euclidean distances by employing some parametric transformation
    on the datasets. In some articles, you might find classical and metric MDS to
    be used interchangeably.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道在经典的 MDS 中，原始距离和拟合距离在相同的度量中表示。在*度量 MDS*中，假设通过在数据集上应用一些参数变换，d[ij]的值可以转换为欧几里得距离。在一些文章中，你可能会发现经典
    MDS 和度量 MDS 被互换使用。
- en: In MDS, as a first step, the respective distances between the points are calculated.
    Once the respective distances have been calculated, then MDS will try to represent
    the higher dimensional data point into lower dimensional space. To perform this,
    an optimization process has to be carried so that the most optimum number of resultant
    dimensions can be chosen. And hence, a loss function or cost function has to be
    optimized.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在MDS中，作为第一步，计算点之间的相应距离。一旦计算出相应的距离，MDS将尝试将更高维的数据点表示为更低维的空间。为了执行此操作，必须进行优化过程，以便选择最合适的结果维数。因此，必须优化一个损失函数或成本函数。
- en: If you do not know what is a cost function, go through this section below.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不知道什么是成本函数，请看下面这一节。
- en: Cost function
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 成本函数
- en: We use algorithms to predict the values of a variable. For example, we might
    use some algorithm to predict the expected demand of a product next year. We would
    want the algorithm to predict as much accurate as possible. Cost functions are
    a simple method to check the performance of the algorithms.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用算法来预测变量的值。例如，我们可能会使用某种算法来预测下一年产品的预期需求。我们希望算法尽可能准确地预测。成本函数是检查算法性能的一种简单方法。
- en: Cost function is a simple technique to measure the effectiveness of our algorithms.
    It is the most common method used to gauge the performance of a predictive model.
    It compares the original values and the predicted values by the algorithm and
    calculates how wrong the model is in its prediction.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 成本函数是衡量我们算法效果的一种简单技术。它是衡量预测模型性能的最常见方法。它比较算法预测的原始值和预测值，并计算模型在预测中的错误程度。
- en: As you would imagine, in an ideal solution, we would want the predicted values
    to be the same as the actual values, which is very difficult to achieve. If the
    predicted values differ a lot from the actual values, the output of a cost function
    is higher. If the predicted values are closer to the actual values, then the value
    of a cost function is lower. A robust solution is one which has a lower value
    of the cost function. Hence, the objective to optimize any algorithm will be to
    minimize value of the cost function. Cost function is also referred as *loss function*,
    these two terms can be used interchangeably.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所想象的，在理想解决方案中，我们希望预测值与实际值相同，这是非常难以实现的。如果预测值与实际值相差很大，成本函数的输出就会更高。如果预测值接近实际值，则成本函数的值较低。一个健壮的解决方案是具有较低成本函数值的解决方案。因此，优化任何算法的目标将是最小化成本函数的值。成本函数也称为*损失函数*，这两个术语可以互换使用。
- en: 'In metric MDS, we can also call the cost function as **Stress**. The formula
    for Stress is given by Equation 6-2 as given below:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在度量MDS中，我们也可以将成本函数称为**应力**。应力的公式由下面给出的方程式 6-2 给出：
- en: '![06_02b](images/06_02b.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![06_02b](images/06_02b.png)'
- en: 'Let’s understand the equation now:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来了解方程式：
- en: Term Stress[D] is the value MDS function has to minimize.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 术语 Stress[D] 是MDS函数必须最小化的值。
- en: The data points with the new set of coordinates in a lower dimensional space
    are represented by x[, x[2], x[3]…. x[N].]
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在较低维空间中具有新坐标集的数据点由 x[, x[2], x[3]…. x[N].] 表示。
- en: The term ||x[i] – x[j]|| is the distance between two points in their lower dimensional
    space.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 术语 ||x[i] – x[j]|| 是它们在较低维空间中的两点之间的距离。
- en: The term d[ij] is original distance between the two points in the original multi-dimensional
    space.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 术语 d[ij] 是原始多维空间中两点之间的原始距离。
- en: By looking at the equation, we can clearly understand that if the values of
    ||x[i] – x[j]|| and d[ij] are close to each other, the value of the resultant
    stress will be small.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 通过观察方程式，我们可以清楚地理解，如果||x[i] – x[j]||和d[ij]的值彼此接近，结果应力的值将较小。
- en: Minimizing the value of stress is the objective of the loss function.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化应力值是损失函数的目标。
- en: To optimize this loss function, multiple approaches can be used. One of the
    most famous method is using a gradient descent which was originally proposed by
    Kruskal and Wish in 1978\. The gradient descent method is very simple to understand
    and can be explained using a simple analogy.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了优化这个损失函数，可以使用多种方法。其中最著名的方法之一是使用梯度下降法，最初由Kruskal和Wish于1978年提出。梯度下降法非常简单易懂，并可以用一个简单的类比来解释。
- en: Imagine you are standing on top of a mountain and you want to get down. While
    doing so, you want to choose the fastest path because you want to get down as
    fast as possible (no, you cannot jump!). So, to take the first step, you will
    look around and whichever is the steepest path, you can take a step in that direction
    and you will reach a new point. And then again, you will take a step in the steepest
    direction. We are showing that process in Figure 6-3(i).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下你站在山顶上，想要下来。在这样做的同时，你想选择最快的路径，因为你想尽快下来（不，你不能跳！）。所以，为了迈出第一步，你会四处看看，无论哪条路最陡峭，你都可以朝那个方向迈出一步，然后你会到达一个新的点。然后再次，你会朝最陡峭的方向迈出一步。我们在图
    6-3(i) 中展示了这个过程。
- en: Figure 6-3 (i) The first figure is of a person standing on top of a mountain
    and trying to get down. The process of gradient descent follows this method (ii)
    The actual process of optimization of a cost function in gradient descent process.
    Note that at the point of convergence, the value of the cost function is minimum.
  id: totrans-64
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6-3 (i) 第一幅图是一个站在山顶上并试图下来的人。梯度下降过程遵循这种方法 (ii) 梯度下降过程中成本函数的实际优化过程。注意，在收敛点，成本函数的值是最小的。
- en: '![06_03](images/06_03.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![06_03](images/06_03.png)'
- en: Now, if an algorithm has to achieve the similar feat, the process can be represented
    in Figure 6-3 (ii), wherein a loss function starts at a point and finally reaches
    the Point of convergence. At this point of convergence, the cost function is minimum.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果一个算法要达到类似的效果，过程可以表示为图 6-3 (ii)，其中损失函数从一个点开始，最终到达收敛点。在这个收敛点，成本函数是最小的。
- en: MDS differs from the other dimensionality reduction techniques. As compared
    to techniques like PCA, MDS does not make any assumptions about the data set and
    hence can be used for a larger type of datasets. Moreover, MDS allows to use any
    distance measurement metric. Unlike PCA, MDS is not an eigenvalue-eigenvector
    technique. Recall in PCA, the first axis captures the maximum amount of variance,
    second axis has the next best variance and so on. In MDS, there is no such condition.
    The axes in MDS can be inverted or rotated as per the need. Next, in most of the
    other dimensional reduction methods used, the algorithms do calculate a lot of
    axes but they cannot be viewed. In MDS, smaller number of dimensions are explicitly
    chosen at the start. And hence, there is less ambiguity in the solution. Further,
    in other solutions generally there is only one unique solution whereas MDS tries
    to iteratively find the acceptable solution. It means in MDS there can be multiple
    solutions for the same dataset.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: MDS与其他降维技术不同。与诸如 PCA 等技术相比，MDS 不对数据集做任何假设，因此可以用于更大类型的数据集。此外，MDS 允许使用任何距离测量度量。与
    PCA 不同，MDS 不是一种特征值-特征向量技术。回想一下，在 PCA 中，第一轴捕获最大的方差，第二轴具有下一个最佳方差，依此类推。在 MDS 中，没有这样的条件。MDS
    中的轴可以根据需要翻转或旋转。接下来，在大多数其他使用的降维方法中，算法确实计算了许多轴，但无法查看。在 MDS 中，开始时明确选择较小数量的维度。因此，解决方案中的歧义较少。此外，在其他解决方案中通常只有一个唯一的解决方案，而
    MDS 尝试迭代地找到可接受的解决方案。这意味着在 MDS 中，对于相同的数据集可能会有多个解决方案。
- en: But at the same time, the computation time required for MDS is higher for bigger
    datasets. And there is a catch in the Gradient Descent method used for optimization.
    Refer to Figure 6-4\. Let’s refer to the mountain example we covered in the last
    section. Imagine that while you are coming down from the top of the mountain.
    The starting point is A and the bottom of the mountain is at point C. While you
    are coming down, you reached point B. As you can see in Figure 6-4(i), there is
    a slight elevation around point B. At this point B, you might incorrectly conclude
    that you have reached the bottom of the mountain. In other words, you will think
    that you have finished your task. This is the precise problem of the local minima.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 但与此同时，MDS所需的计算时间对于更大的数据集要求更高。在用于优化的梯度下降方法中存在一个陷阱。参考图 6-4。让我们参考上一节中提到的山的例子。想象一下，当你从山顶下来时。起点是
    A，山的底部在 C 点。当你下来时，你到达了 B 点。正如图 6-4(i) 中所示，B 点周围有一点高度。在 B 点，你可能会错误地得出结论，认为自己已经到达了山底。换句话说，你会认为自己完成了任务。这就是局部最小值的确切问题。
- en: It is a possibility that instead of global minima, the loss function might be
    stuck in a local minima. The algorithm might think that it has reached the point
    of convergence, while the complete convergence might not have been achieved still
    and we are at local minima.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 有可能损失函数会陷入局部极小值而不是全局极小值。算法可能会认为已经达到收敛点，而实际上并没有完全收敛，我们仍然处于局部极小值。
- en: Figure 6-4 While the first figure is the point of convergence and represents
    the gradient descent method, note that in the second figure the global minima
    is somewhere else, while the algorithm can be stuck at a local minima. The algorithm
    might believe that it has optimized the cost function and reached the point of
    global minima whereas, it has only reached the local minima.
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6-4 虽然第一幅图是收敛点并且代表梯度下降法，但请注意第二幅图中全局极小值在其他地方，而算法可能会陷入局部极小值。该算法可能认为它已经优化了成本函数并达到了全局极小值，而实际上它只达到了局部极小值。
- en: '![06_04](images/06_04.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![06_04](images/06_04.png)'
- en: There is still a question to be answered about the efficacy of the MDS solution.
    How can we measure the effectiveness of the solution? In the original paper, Kruskal
    has recommended about the stress values to measure the goodness-of-fit of the
    solution which are shown in Table 6-1\. The recommendations are mostly based on
    empirical experience of Kruskal. These stress values are based on Kruskal’s experience.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 关于MDS解决方案的有效性仍有个问题有待解答。我们如何衡量解决方案的有效性？在原始论文中，Kruskal推荐使用应力值来衡量解决方案的拟合度，这些值显示在表6-1中。这些建议大多基于Kruskal的经验经验。这些应力值基于Kruskal的经验。
- en: '| Stress Values | Goodness-of-fit |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 应力值 | 拟合度 |'
- en: '| 0.200 | Poor |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 0.200 | 较差 |'
- en: '| 0.100 | Fair |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 0.100 | 中等 |'
- en: '| 0.050 | Good |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 0.050 | 良好 |'
- en: '| 0.025 | Excellent |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 0.025 | 优秀 |'
- en: '| 0.000 | Perfect |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 0.000 | 完美 |'
- en: The next logical question is – how many final dimensions we should choose? Scree
    plot provides the answer as shown in Figure 6-5\. Recall, in chapter 2 we used
    the similar elbow method to choose the optimal number of clusters in kmeans clustering.
    For MDS too, we can have the elbow method to determine the most optimal number
    of components to represent the data.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个逻辑问题是 - 我们应该选择多少个最终维度？Scree图提供了答案，如第6-5图所示。回想一下，在第2章，我们使用了类似的肘部法选择了kmeans聚类中的最优聚类数。对于MDS，我们也可以使用肘部法来确定代表数据的最优组件数。
- en: Figure 6-5  Scree plot to find the most optimal number of components. It is
    similar to the kmeans solution we have discussed in the earlier chapters; we have
    to look for the elbow in the plot.
  id: totrans-80
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6-5  Scree图来找出最优组件数。与我们在前几章讨论过的kmeans解决方案类似；我们需要在图表中找到肘部。
- en: '![06_05](images/06_05.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![06_05](images/06_05.png)'
- en: This concludes our discussion on MDS. We will now move to the Python implementation
    of the algorithm.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对MDS的讨论。现在我们将转向算法的Python实现。
- en: '![](images/tgt.png) POP QUIZ – answer these question to check your understanding..
    Answers at the end of the book'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '![](images/tgt.png) 答题 - 回答这些问题来检查你的理解。答案在书的末尾处'
- en: 1.   What is the difference between metric and non-metric MDS algorithm?
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 1.  度量和非度量MDS算法有什么区别？
- en: 2.   Gradient descent is used to maximise the cost. True or False.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 2.  梯度下降法用于最大化成本。是True还是False。
- en: 3.   Explain gradient descent method using a simple example.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 3.  使用一个简单的例子解释梯度下降法。
- en: 6.2.1     Python implementation of MDS
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.1  MDS的Python实现
- en: We will now have the Python implementation of MDS method. We will use the famous
    Iris dataset which we have used previously too. The implementation of the algorithm
    is quite simple, thanks to the libraries available in the scikit learn package.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将采用Python实现MDS方法。我们将使用我们之前使用过的著名的鸢尾花数据集。由于scikit learn包中提供的库，算法的实现相当简单。
- en: The implementation is generally simple as the heavy lifting is done by the libraries.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 由于库提供了重要的工作，因此实现通常很简单。
- en: 'Step 1: We will first load the libraries. The usual suspects are `sklearn`,
    `matplotlib`, `numpy` and we also load `MDS` from `sklearn`.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤1：我们首先加载库。通常会用到`sklearn`，`matplotlib`，`numpy`，并且还从`sklearn`加载`MDS`。
- en: '[PRE0]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Step 2: Load the data set now. Iris dataset is available in the `sklearn` library
    so we need not import excel or .csv file here.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤2：现在加载数据集。鸢尾花数据集在`sklearn`库中可用，因此我们无需在此处导入excel或.csv文件。
- en: '[PRE1]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Step 3: A requirement for MDS is that the dataset should be scaled before the
    actual visualization is done. We are using `MixMaxScalar()` function to achieve
    the same. MinMax scaling simply scales the data using the formula below:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 3：MDS的要求是在实际可视化之前对数据集进行缩放。我们使用`MixMaxScalar（）`函数来实现相同的效果。 MinMax缩放只是使用以下公式对数据进行缩放：
- en: '![06_F01](images/06_F01.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![06_F01](images/06_F01.png)'
- en: '[PRE2]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As an output of this step, the data is scaled and ready for the next step of
    modelling.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 作为此步骤的输出，数据已经被缩放，准备进行下一步的建模。
- en: 'Step 4: We now invoke the MDS method from sklearn library. The random_state
    value allows us to reproduce the results. We have decided the number of components
    as 3 for the example.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 4：我们现在从sklearn库中调用MDS方法。 random_state值允许我们重现结果。我们已经决定示例中的组件数量为3。
- en: '[PRE3]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Step 5: We will now fit the scaled data created earlier using the MDS model.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 5：我们现在将使用MDS模型拟合之前创建的缩放数据。
- en: '[PRE4]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Step 6: We are now declaring the colors we wish to use for visualization. And
    next, the data points are visualized in a scatter plot.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 6：我们现在声明我们希望用于可视化的颜色。接下来，数据点在散点图中可视化。
- en: '[PRE5]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output of the code above can be shown below in Figure 6-6:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下图6-6所示：
- en: Figure 6-6 Output for the IRIS data
  id: totrans-105
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6-6 IRIS数据的输出
- en: '![06_06](images/06_06.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![06_06](images/06_06.png)'
- en: The above example of Python implementation is a visualization of the IRIS data.
    It is quite simple example but it does not involve *stress* and optimization for
    the number of components. *We* will now work on a curated dataset to implement
    MDS.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 上述Python实现示例是IRIS数据的可视化。这是一个相当简单的例子，但不涉及*压力*和组件数量的优化。*我们*现在将在一个精心策划的数据集上实施MDS。
- en: '![06_06a](images/06_06a.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![06_06a](images/06_06a.png)'
- en: Let us assume we have five cities and the respective distance between them is
    given in Table 6-2.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设我们有五个城市，它们之间的距离分别在表6-2中给出。
- en: 'Step 1: We have already imported the libraries in the last code.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 1：我们已经在上一个代码中导入了库。
- en: '[PRE6]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Step 2: Let’s now create the dataset. We are creating the dataset here, but
    in real business scenarios it will be in the form of distances only.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 2：现在让我们创建数据集。我们在这里创建数据集，但在实际业务场景中，它将仅以距离的形式存在。
- en: '[PRE7]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![06_06b](images/06_06b.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![06_06b](images/06_06b.png)'
- en: 'Step 3: We will now use the MinMaxScalar() function to scale the dataset as
    we did in the last coding exercise.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 3：我们现在将使用MinMaxScalar（）函数来缩放数据集，就像我们在上一个编码练习中所做的那样。
- en: '[PRE8]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Step 4: Now, let’s work towards finding the most optimal number of components.
    We will iterate for different values of number of components. For each of the
    value of number of components, we will get the value of stress. And at a point,
    where a kink is observed, that is the most optimal number of components.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 4：现在，让我们朝着找到最优组件数量的方向前进。我们将迭代不同组件数量的值。对于每个组件数量的值，我们将获得压力的值。当观察到一个拐点时，那就是最优的组件数量。
- en: As a first step, we will declare an empty dataframe which can be used to store
    the values of number of components and corresponding stress values. Then, we are
    iterating from 1 to 10 in a for loop. And finally, for each of the values of components
    (1 to 10), we get the respective values of stress.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一步，我们将声明一个空数据框，用于存储组件数量和相应压力值的值。然后，我们在for循环中从1到10进行迭代。最后，对于每个组件值（1到10），我们获取相应的压力值。
- en: '[PRE9]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Step 5: We have got the values of stress. We will now plot these values in
    a graph. The respective labels for each of the axes are also given. Look at the
    kink at values 2 and 3\. This can be the values of optimal values of number of
    components.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 5：我们已经得到了压力值。我们现在将这些值绘制在图表中。每个轴的相应标签也给出了。观察值为2和3的拐点。这可以是最优组件数量的值。
- en: '[PRE10]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Figure 6-7 Scree plot to select the optimized number of components
  id: totrans-122
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6-7 屏幕图以选择优化的组件数量
- en: '![06_07](images/06_07.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![06_07](images/06_07.png)'
- en: 'Step 6: We will now run the solution for number of components = 3\. If we look
    at the values of stress, number of components = 3, it generates the minimum values
    of stress as 0.00665.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 6：我们现在将运行组件数量= 3的解决方案。如果我们查看压力值，组件数量= 3，它将生成压力值的最小值为0.00665。
- en: '[PRE11]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Figure 6-8 Output for the MDS dataset, representation of the 5 cities in a plot
  id: totrans-126
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6-8 MDS数据集的输出，在绘图中表示了5个城市
- en: '![06_08](images/06_08.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![06_08](images/06_08.png)'
- en: This concludes our section on MDS algorithm. We discussed the foundation and
    concepts, pros and cons, algorithm assessment and Python implementation of MDS.
    It is a great solution for visualization and dimensionality reductions. It is
    one of the non-linear dimensionality reductions methods.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们关于 MDS 算法的部分。我们讨论了基础和概念，优缺点，算法评估以及 MDS 的 Python 实现。它是可视化和降维的一个很好的解决方案。它是非线性降维方法之一。
- en: We will now move to t-SNE, second dimensionality reduction methods in this chapter.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将转向 t-SNE，在本章中的第二个降维方法。
- en: 6.3 t-distributed stochastic neighbor embedding (t-SNE)
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 t-分布随机邻居嵌入（t-SNE）
- en: If a data set is really high dimensional, the analysis becomes cumbersome. The
    visualization is even more confusing. We have covered that in great detail in
    Curse of Dimensionality section in Chapter 2\. You are advised to revisit the
    concept before proceeding.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据集真的是高维的，分析就变得很麻烦。可视化甚至更加混乱。我们在第二章的维度诅咒部分中对此进行了详细的讨论。建议您在继续之前重新查看这个概念。
- en: One such really high-dimensional dataset can be image data. We find it difficult
    to comprehend such data which is really high-dimensional.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这样一个真正高维的数据集可以是图像数据。我们发现很难理解这种真正高维的数据。
- en: You would have used facial recognition software in your smartphones. For such
    solutions, facial images have to analyzed and machine learning models have to
    be trained. Look at the pictures below in Figure 6-9– we have a human face, a
    bike, a vacuum cleaner and screen capture of a game.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能在智能手机上使用过人脸识别软件。对于这样的解决方案，必须分析面部图像并训练机器学习模型。看一下下面的图片，图 6-9 中我们有一个人脸，一辆自行车，一台吸尘器和一个游戏的屏幕截图。
- en: Figure 6-9 Images are quite complex to decipher by an algorithm. Images can
    be of any form and can be of a person, or an equipment or even any game screen.
  id: totrans-134
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6-9 图像对算法来说相当复杂，很难解释。图像可以是任何形式的，可以是一个人，或者一台设备，甚至是任何游戏屏幕。
- en: '![06_09](images/06_09.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![06_09](images/06_09.png)'
- en: Image is a complex data point. Each image is made up of pixels, and each pixel
    can be made up of RGB (red, green, blue) values. And values for each of the red,
    green, blue can range from 0 to 255\. The resulting dataset will be a very high-dimensional
    dataset.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图像是一个复杂的数据点。每个图像由像素组成，每个像素可以由 RGB（红色，绿色，蓝色）值组成。而每个红色、绿色、蓝色的值都可以从 0 到 255。结果数据集将是一个非常高维的数据集。
- en: Now, recall Principal Component Analysis (PCA) we studied in Chapter 3\. PCA
    is a linear algorithm. And being a linear algorithm, its capability is limited
    to resolve non-linear and complex polynomial functions. Moreover, when a *high-dimensional*
    dataset has to represented in a low-dimensional space the algorithm should keep
    similar datapoints close to each other, which can be challenge in linear algorithms.
    PCA being a linear dimension reduction technique, it tries to separate the different
    data points as far away from each other as PCA is trying to maximize the variance
    between the data points. The resulting analysis is not robust and might not be
    best suited for further usage and visualization. Hence, we have non-linear algorithms
    like t-SNE to help.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，回顾一下我们在第三章学习的主成分分析（PCA）。PCA 是一种线性算法。作为一种线性算法，它的能力受到限制，无法解决非线性和复杂多项式函数。此外，当一个*高维*数据集必须表示在一个低维空间时，算法应该将类似的数据点保持在彼此附近，这对于线性算法来说可能是一个挑战。PCA
    是一种线性降维技术，它试图将不同的数据点分开得尽可能远，因为 PCA 正在尝试最大化数据点之间的方差。结果分析不够稳健，可能不适合进一步使用和可视化。因此，我们有非线性算法如
    t-SNE 来帮助。
- en: Formally put, t-SNE is a *non-linear dimensionality reduction technique* which
    is quite handy for high dimensional data. It is based on Stochastic Neighbor Embedding
    which was developed by Sam Roweis and Geoffrey Hinton. The t-distributed variant
    was proposed by Lauren van der Maaten. So, t-SNE is an improvement on the SNE
    algorithm.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上来说，t-SNE 是一种*非线性降维技术*，非常适用于高维数据。它基于由 Sam Roweis 和 Geoffrey Hinton 开发的随机邻居嵌入。t-分布变体是由
    Lauren van der Maaten 提出的。因此，t-SNE 是对 SNE 算法的改进。
- en: At a high level, SNE measures the similarity between instances pairs in a high-dimensional
    space and in a low dimensional space. A good solution is where the difference
    between these similarity measures is the least and hence SNE then optimizes these
    similarity measure using a cost function.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在高水平上，SNE 测量高维空间和低维空间中实例对之间的相似性。一个好的解决方案是这些相似度测量之间的差异最小，因此 SNE 通过成本函数优化这些相似度测量。
- en: We will examine the step-by-step process of t-SNE now. The process described
    below is a little heavy on mathematics.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将逐步检验t-SNE的过程。下面描述的过程在数学上有些繁重。
- en: Consider we have a high-dimensional space and we have some points in this high-dimensional
    space.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设我们有一个高维空间，在这个高维空间中有一些点。
- en: We will now measure the similarities between the various points. For a point
    x[i], we will then create a Gaussian distribution centered at that point. We have
    already studied Gaussian or normal distribution is the last chapters of the book.
    The Gaussian distribution is shown in Figure 6-10.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将度量各个点之间的相似性。对于一个点x[i]，我们将创建一个以该点为中心的高斯分布。我们在书的前几章已经学习了高斯或正态分布。高斯分布如图6-10所示。
- en: Figure 6-10 Gaussian or normal distribution which we have already studied earlier.
    Image has been taken from Wikipedia.
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6-10 高斯或正态分布，我们之前已经学过。图片来源：维基百科。
- en: '![06_10](images/06_10.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![06_10](images/06_10.png)'
- en: 'Now we will measure the density of points (let’s say x[j]) which fall under
    that Gaussian Distribution and then we renormalize them to get the respective
    conditional probabilities (p[j|i]). For the points which are nearby and hence
    similar, this conditional probability will be high and for the points which are
    far and dissimilar, the value of conditional probabilities (p[j|i]) will be very
    small. These values of probabilities are the ones in high-dimensional space. For
    the curious ones, the mathematical formula for this conditional probability is
    Equation 6-3:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将测量在高斯分布下落在该点（比如x[j]）的点的密度，然后重新归一化它们以获得相应的条件概率(p[j|i])。对于附近和相似的点，这个条件概率会很高，而对于远离和不相似的点，条件概率(p[j|i])的值会非常小。这些概率值是在高维空间中的。对于好奇的人，这个条件概率的数学公式是等式6-3：
- en: '![06_10a](images/06_10a.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![06_10a](images/06_10a.png)'
- en: where σ is the variance of the Gaussian Distribution centered at x[i]. The mathematical
    proof is beyond the scope of this book.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 其中σ是以x[i]为中心的高斯分布的方差。这个条件概率的数学证明超出了本书的范围。
- en: Now we will measure one more set of probabilities in the low-dimensional space.
    For this set of measurements, we use *Cauchy Distribution*.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将在低维空间测量另一组概率。对于这组测量，我们使用*柯西分布*。
- en: Cauchy Distribution
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 柯西分布
- en: Cauchy distribution, belongs to the family of continuous probability distributions.
    Though there is a resemblance with the normal distribution, as we have represented
    in Figure 6-11, the Cauchy distribution has narrower peak and spreads out more
    slowly. It means that as compared to a normal distribution, the probability of
    obtaining values far from the peaks are higher. Sometimes, Cauchy distribution
    is also known as *Lorentz distribution.* It is interesting to note that Cauchy
    does not a well-defined mean but the median is the center of symmetry.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 柯西分布，属于连续概率分布家族。虽然与正态分布有些相似，正如我们在图6-11中表示的，柯西分布的峰值较窄，扩散速度较慢。这意味着，与正态分布相比，远离峰值的值的概率更高。有时，柯西分布也被称为*洛伦兹分布*。有趣的是，柯西分布没有定义良好的均值，但中位数是对称中心。
- en: 'Figure 6-11 Comparison of Gaussian distribution vs Cauchy distribution. (Image
    source: Quora)'
  id: totrans-151
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6-11 高斯分布与柯西分布的比较。（图片来源：Quora）
- en: '![06_11](images/06_11.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![06_11](images/06_11.png)'
- en: Consider we get y[i] and y[j] as the low-dimensional counterparts for the high-dimensional
    data points x[i] and x[j]. So, we can calculate the probability score like we
    did in the last step. Using Cauchy distribution, we can get second set of probabilities
    q[j|i] too. The mathematical formula is shown below in Equation 6-4.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设我们得到y[i]和y[j]作为高维数据点x[i]和x[j]的低维对应物。因此，我们可以像上一步那样计算概率分数。使用柯西分布，我们也可以得到第二组概率q[j|i]。数学公式如下所示，见等式6-4。
- en: '![06_11a](images/06_11a.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![06_11a](images/06_11a.png)'
- en: So far, we have calculated two set of probabilities (p[j|i]) and (q[j|i]). In
    this step, we compare the two distributions and measure the difference between
    the two. In other words, while calculating (p[j|i]) we measured the probability
    of similarity in a high-dimensional space whereas for (q[j|i]) we did the same
    in a low-dimensional space. Ideally, if the mapping of the two spaces to be similar
    and for that there should be not be any difference between (p[j|i]) and (q[j|i]).
    So, the SNE algorithm tries to minimize the difference in the conditional probabilities
    (p[j|i]) and (q[j|i]).
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经计算了两组概率（p[j|i]）和（q[j|i]）。在这一步中，我们比较这两个分布并测量两者之间的差异。换句话说，当计算（p[j|i]）时，我们在高维空间中测量了相似性的概率，而对于（q[j|i]），我们在低维空间中进行了相同的操作。理想情况下，如果两个空间的映射相似，那么（p[j|i]）和（q[j|i]）之间就不应该有任何差异。因此，SNE算法试图最小化条件概率（p[j|i]）和（q[j|i]）之间的差异。
- en: The difference between the two probability distributions is done using Kullback-Liebler
    divergence or KL divergence, which we will explore here.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用Kullback-Liebler散度或KL散度来衡量两个概率分布之间的差异，我们将在这里探讨。
- en: KL divergence
  id: totrans-157
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: KL散度
- en: KL divergence or relative entropy is used to measure the difference between
    two probability distributions – usually one probability distribution is the data
    or the measured scores. The second probability distribution is an approximation
    or the prediction of the original probability distribution. For example, if the
    original probability distribution is X and the approximated one is Y. KL divergence
    can be used to measure the difference between X and Y probability distributions.
    In absolute terms if the value is 0 then it means that the two distributions are
    similar. The KL divergence is applicable for neurosciences, statistics and fluid
    mechanics.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: KL散度或相对熵用于衡量两个概率分布之间的差异 - 通常一个概率分布是数据或测量得分。第二个概率分布是原始概率分布的近似值或预测值。例如，如果原始概率分布是X，近似概率分布是Y。KL散度可用于测量X和Y概率分布之间的差异。绝对来说，如果值为0，则意味着两个分布相似。KL散度适用于神经科学、统计学和流体力学。
- en: To minimize the KL cost function, we use the gradient descent approach. We have
    already discussed the gradient descent approach in the section where we discussed
    MDS algorithm.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了最小化KL成本函数，我们使用梯度下降方法。我们已经在讨论MDS算法的部分中讨论过梯度下降方法。
- en: There is one more important point we should be aware while we work on t-SNE,
    an important hyperparameter called *perplexity*. Perplexity is a hyperparameter
    which allows us to control and optimize the number of close neighbors each of
    the data point has.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们研究t-SNE时，还有一点非常重要，即一个重要的超参数称为*困惑度*。困惑度是一个超参数，允许我们控制和优化每个数据点的邻近数量。
- en: As per the official paper, a typical value for perplexity lies between 5 and
    50.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 根据官方文件，困惑度的典型值在5和50之间。
- en: There can be one additional nuance – the output of a t-SNE algorithm might never
    be same on successive runs. We have to optimize the values of the hyperparameters
    to receive the best output.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 还可能存在一个额外的细微差别 - t-SNE算法的输出在连续运行中可能永远不会相同。我们必须优化超参数的值以获得最佳输出。
- en: '![](images/tgt.png) POP QUIZ – answer these question to check your understanding..
    Answers at the end of the book'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '![](images/tgt.png) 流行测验 - 回答这些问题以检查您的理解.. 答案在本书的结尾'
- en: 1.   Explain Cauchy distribution is your own words.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 解释自己的话语中的柯西分布。
- en: 2.   PCA is a non-linear algorithm. True or False.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 2. PCA是非线性算法。是还是不是。
- en: 3.   KL divergence is used to measure the difference between two probability
    distributions. True or False
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 3. KL散度用于衡量两个概率分布之间的差异。是还是不是
- en: We will now proceed to the Python implementation of the algorithm.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将继续讨论算法的Python实现。
- en: 6.3.1 Python implementation of t-SNE
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.1 t-SNE的Python实现
- en: We will use two datasets in this example. The first one is the already known
    IRIS dataset, which we have already used more than once in this book. The second
    dataset is quite an interesting one. It is MNIST dataset which is a database of
    handwritten digits. It is one of the most famous datasets used to train image
    processing solutions and generally is considered “Hello World” program for image
    detection solutions. An image representation is shown below in ().
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用两个数据集。第一个是已知的IRIS数据集，在本书中我们已经多次使用过。第二个数据集非常有趣。它是MNIST数据集，这是一个手写数字的数据库。它是用来训练图像处理解决方案的最著名的数据集之一，通常被认为是图像检测解决方案的“Hello
    World”程序。下面显示了图像表示（）。
- en: Figure 6-12 MNIST dataset- it is a collection of handwritten images of digits.
  id: totrans-170
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6-12 MNIST 数据集-这是一组手写数字的图像。
- en: '![06_12](images/06_12.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![06_12](images/06_12.png)'
- en: 'Step 1: We will first import the necessary libraries. Note that we have imported
    MNIST dataset from keras library.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 第 1 步：我们将首先导入必要的库。请注意，我们已从 keras 库导入 MNIST 数据集。
- en: '[PRE12]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Step 2: First we will work with the IRIS dataset. We will load the IRIS dataset.
    The dataset comprises of two parts – one is the “data” and second is the respective
    label or “target” for it. It means that “data” is the description of the data
    and “target” is the type of IRIS. We are printing the features and the labels
    using a piece of code.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 第 2 步：首先我们将使用 IRIS 数据集。我们将加载 IRIS 数据集。数据集包括两部分 - 一部分是“数据”，另一部分是相应的标签或“目标”。这意味着“数据”是数据的描述，“目标”是
    IRIS 的类型。我们使用一小段代码打印特征和标签。
- en: '[PRE13]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Step 3: The next step is invoking the tSNE algorithm. We are using the number
    of components = 2 and random_state =5 to reproduce the results. And then the algorithm
    is used to fit the data.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 第 3 步：下一步是调用 tSNE 算法。我们使用的组件数量为 2，随机状态为 5，以重现结果。然后使用算法拟合数据。
- en: '[PRE14]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![06_12a](images/06_12a.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![06_12a](images/06_12a.png)'
- en: 'Step 4: We are now plotting the data. This step allows us to visualize the
    data fitted by the algorithm in the last step.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 第 4 步：现在我们将绘制数据。此步骤允许我们可视化算法在上一步中拟合的数据。
- en: First, we will initiate an empty dataframe. We will add three columns one at
    a time. We will start with iris_target, followed by tSNE_first_component and tSNE_second_component.
    tSNE_first_component is the first column of the fitted_data dataframe and hence
    the index is 0\. tSNE_second_component is the second column of the fitted_data
    dataframe and hence the index is 1\. Finally, we are representing the data in
    a scatterplot.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将初始化一个空的数据框架。我们将逐一添加三列。我们将从 iris_target 开始，然后是 tSNE_first_component 和 tSNE_second_component。tSNE_first_component
    是 fitted_data 数据框架的第一列，因此索引为 0。tSNE_second_component 是 fitted_data 数据框架的第二列，因此索引为
    1。最后，我们将数据表示为散点图。
- en: '[PRE15]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Figure 6-13 tSNE projection of the IRIS dataset. Note how we are getting three
    separate clusters for the three classes we have in the dataset
  id: totrans-182
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6-13 IRIS 数据集的 tSNE 投影。请注意，我们为数据集中的三个类别得到了三个单独的聚类
- en: '![06_13](images/06_13.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![06_13](images/06_13.png)'
- en: Now we will implement the algorithm for MNIST dataset.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将为 MNIST 数据集实现算法。
- en: 'Step 1: the libraries are already loaded in the last code example. Then we
    load the dataset. The dataset requires reshape which is done here'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 第 1 步：库已经在上一个代码示例中加载。然后我们加载数据集。数据集需要进行 reshape，这在这里完成
- en: '[PRE16]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Figure 6-14 Output of tSNE for the 10 classes of digits represented in different
    colors.
  id: totrans-187
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6-14 tSNE 对表示为不同颜色的 10 类数字的输出。
- en: '![06_14](images/06_14.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![06_14](images/06_14.png)'
- en: 'There are a few important points which you should keep in mind while running
    tSNE:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行 tSNE 时，请记住以下几个重要点：
- en: Run the algorithm with different values of hyperparameters before finalizing
    a solution.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在最终确定解决方案之前，用不同的超参数值运行算法。
- en: Ideally, perplexity should be between 5 and 50 and for an optimized solution,
    the value of perplexity should be less than the number of points.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理想情况下，perplexity 应该在 5 和 50 之间，对于优化的解决方案，perplexity 的值应该小于数据点的数量。
- en: tSNE guess the number of close neighbors for each of the point. And because
    of this reason, a dataset which is denser will require a much higher perplexity
    value.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: tSNE 猜测每个点的近邻数。因此，密集的数据集将需要更高的 perplexity 值。
- en: Particularly, we should note that perplexity is the hyper parameter which balances
    the attention given to both the local and global aspects of the data.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特别需要注意的是 perplexity 是一个超参数，它平衡了对数据的局部和全局方面的关注。
- en: tSNE is one of the widely popular algorithms. It is used for studying topology
    of an area, but a single tSNE cannot be used for making a final assessment. Instead,
    multiple tSNE plots should be created to make any final recommendation. Sometimes,
    there are complaints that tSNE is a black box algorithm. This might be true to
    a certain extent. What makes the adoption of tSNE harder is that it does not generate
    same results in successive iterations. Hence, you might find tSNE being recommended
    only for exploratory analysis.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: tSNE 是广泛流行的算法之一。它用于研究区域的拓扑，但单个 tSNE 不能用于做出最终评估。相反，应创建多个 tSNE 图来做出任何最终推荐。有时会有人抱怨
    tSNE 是一个黑箱算法。在某种程度上这可能是真的。使 tSNE 的采用变得更加困难的是，在连续迭代中它不会生成相同的结果。因此，你可能只会发现 tSNE
    被推荐用于探索性分析。
- en: This concludes our discussion on tSNE. We will now move to the case study.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们关于 tSNE 的讨论。现在我们将转向案例研究。
- en: 6.4 Case study
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 案例研究
- en: Recall from chapter 3 where we explored a case study for telecom industry employing
    dimensionality. In this chapter, we will examine a small case study wherein tSNE
    or MDS can be utilized for dimensionality reduction.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 请回忆第 3 章，我们探讨了运用降维技术在电信业的案例研究。在本章中，我们将研究一个小案例，应用 tSNE 或 MDS 进行降维。
- en: Have you heard about hyperspectral images? As you know, we humans see the colors
    of visible light in mostly three bands – long wavelengths, medium ones and short
    wavelengths. The long wavelengths are perceived as red color, medium are green
    and short ones are perceived as blue color. Spectral imagining on the other hand,
    divides the spectrum into many greater numbers of bands and this technique can
    be extended beyond the visible ones and hence is of usage across biology, physics,
    geoscience, astronomy, agriculture and many more avenues.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 你听说过高光谱图像吗？如你所知，我们人类主要看到可见光的颜色有长波长、中波长和短波长三个波段。长波长被感知为红色，中波长为绿色，短波长为蓝色。而频谱成像则将光谱分成许多更多的波段，这种技术可以扩展到可见光以外的波段，因此在生物学、物理学、地球科学、天文学、农业等许多领域都有用途。
- en: Hyperspectral imaging collects and processes information from across the electromagnetic
    spectrum. It obtains the spectrum for each of the pixel in the image.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 高光谱成像收集并处理来自整个电磁波谱的信息。它获取图像中每个像素的光谱。
- en: 'Figure 6-15 Hyperspectral image of "sugar end" potato strips shows invisible
    defects (Image source: Wikipedia)'
  id: totrans-200
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6-15《糖端》土豆条的高光谱图像显示了看不见的缺陷（图片来源：维基百科）
- en: '![06_15](images/06_15.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![06_15](images/06_15.png)'
- en: One such dataset can be the Pavia University Dataset. It is acquired by ROSIS
    sensor on Pavia, northern Italy. The details of the dataset are given below and
    the dataset can be downloaded from ([http://www.ehu.eus/ccwintco/uploads/e/ee/PaviaU.mat](ee.html)
    [http://www.ehu.eus/ccwintco/uploads/5/50/PaviaU_gt.mat](50.html))
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 一个这样的数据集可能是 Pavia 大学数据集。这个数据集是由 ROSIS 传感器在意大利北部的 Pavia 获取的。数据集的详细信息如下，并且可以从（[http://www.ehu.eus/ccwintco/uploads/e/ee/PaviaU.mat](ee.html)
    [http://www.ehu.eus/ccwintco/uploads/5/50/PaviaU_gt.mat](50.html)）下载。
- en: In this dataset the spectral bands are 103, HIS size is 610*340 pixels and it
    contains 9 classes. Now, such a type of data can be used for crop analysis, mineral
    examining and explorations etc. Since this data contains information about the
    geological patterns, it is quite useful for scientific purpose. Before developing
    any image recognition solutions, we have to reduce the number of dimensions for
    this dataset. Moreover, the computation cost will be much higher if we have a
    large number of dimensions. Hence, it is obvious to have lesser number of representative
    number of dimensions. We are showing a few example bands of below. You are advised
    to download the dataset (which is also checked-in at the git Hub repo) and use
    the various dimensionality reduction techniques on the dataset to reduce the number
    of dimensions.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在此数据集中，光谱波段为 103，HIS 尺寸为 610*340 像素，包含 9 类。这种类型的数据可以用于作物分析、矿物检查和勘探等。由于此数据包含有关地质图案的信息，因此对于科学目的非常有用。在开发任何图像识别解决方案之前，我们必须减少此数据集的维度数量。此外，如果维度数量较大，计算成本将会更高。因此，显然应该有较少的代表性维度数量。以下显示了一些示例波段。建议您下载数据集（也已在
    git Hub repo 中检入）并在数据集上使用各种降维技术以减少维度数量。
- en: Figure 6-16 Example of bands in the dataset. These are only random examples,
    you are advised to load the dataset and run dimensionality reduction algorithms.
  id: totrans-204
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6-16 数据集中波段的例子。这只是随机的例子，建议您加载数据集并运行降维算法。
- en: '![06_16](images/06_16.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![06_16](images/06_16.png)'
- en: There can be many other image datasets and complex business problems where tSNE
    and MDS can be of pragmatic usage. Some of such datasets have been listed in the
    next steps.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: tSNE 和 MDS 可以在许多其他图像数据集和复杂的商业问题中发挥实际作用。下一步列出了一些此类数据集。
- en: 6.5 Summary
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.5 总结
- en: Dimensionality reduction is quite an interesting and useful solution. It makes
    the machine learning less expensive and time consuming. Imagine that you have
    a dataset with thousands of attributes or features. You do not know the data very
    well; the business understanding is quite less and at the same time you have to
    find the patterns in the dataset. You are not even sure that if these variables
    are all relevant or just random noise. At such a moment, when we have to quickly
    reduce the number of dimensions in the dataset, make it less complex to crack
    and reduce the time – dimensionality reduction is the solution.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 维度缩减是一种非常有趣且有用的解决方案。它使得机器学习变得更加经济高效。想象一下，你有一个包含成千上万个属性或特征的数据集。你对数据了解不多；业务理解很少，同时你需要在数据集中找到模式。你甚至不确定这些变量是否都是相关的，还是只是随机噪声。在这样一个时刻，当我们需要快速降低数据集的维度，使其更加易于处理并减少时间时，维度缩减就是解决方案。
- en: We covered dimensionality reduction techniques earlier in the book. This chapter
    covers two advanced techniques – tSNE and MDS. Both of these techniques should
    not be considered a substitute to the other easier techniques we discussed. Rather,
    they are two be used if we are not getting meaningful results. It is always advised
    to use PCA first, then try tSNE or MDS.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在书中较早地介绍了维度缩减技术。本章涵盖了两种高级技术——tSNE和MDS。这两种技术不应被视为我们讨论的其他更简单技术的替代品。相反，如果我们没有得到有意义的结果，可以尝试使用这两种技术。建议先使用PCA，然后再尝试tSNE或MDS。
- en: We are increasing the complexity in the book. This chapter started with images
    – we have only wet our toe though. In the next chapter, we are dealing with text
    data, perhaps you will find it very interesting and useful.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在增加书中的复杂性。这一章以图像开始——我们只是初探了一下。在下一章中，我们将处理文本数据，也许你会觉得非常有趣和有用。
- en: Practical next steps and suggested readings
  id: totrans-211
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实际下一步和建议阅读
- en: Use the vehicles dataset used in the Chapter2 for clustering and implement MDS
    on it. Compare the performance on clustering before and after implementing MDS.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用第二章中使用的车辆数据集进行聚类，并在其上实施MDS。比较在实施MDS之前和之后的聚类性能。
- en: Get the datasets used in Chapter 2 for Python examples and use them for implementing
    MDS.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取第二章中用于Python示例的数据集，并将其用于实施MDS。
- en: 'For MDS, you can refer to the following research papers:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于MDS，您可以参考以下研究论文：
- en: 'Dimensionality reduction: a comparative review by Lauren van der Maaten, Eric
    Postma and H. Japp Van Den Herik [https://www.researchgate.net/publication/228657549_Dimensionality_Reduction_A_Comparative_Review](publication.html)'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Lauren van der Maaten、Eric Postma和H. Japp Van Den Herik撰写的《维度缩减：一项比较性回顾》[维度缩减：一项比较性回顾](https://www.researchgate.net/publication/228657549_Dimensionality_Reduction_A_Comparative_Review)
- en: Multidimensional scaling -based data dimension reduction method for application
    in short term traffic flow prediction for urban road network by Satish V. Ukkusuri
    and Jian Lu https://www.hindawi.com/journals/jat/2018/3876841/
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Satish V. Ukkusuri 和 Jian Lu提出了一种基于多维缩放的数据维度缩减方法，用于城市道路网络的短期交通流预测。[多维缩放方法在城市道路网络短期交通流预测中的应用](https://www.hindawi.com/journals/jat/2018/3876841/)
- en: Get tSNE research papers from the links below and study them
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从下面的链接获取tSNE研究论文并研究它们。
- en: Visualizing data using t-SNE by Laurens van der Maaten and Geoffrey Hinton https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Laurens van der Maaten和Geoffrey Hinton撰写的《使用t-SNE可视化数据》[使用t-SNE可视化数据](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf)
- en: The art of using t-SNE for single cell transcriptomics [https://www.nature.com/articles/s41467-019-13056-x](articles.html)
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 《单细胞转录组学中使用t-SNE的艺术》[单细胞转录组学中使用t-SNE的艺术](https://www.nature.com/articles/s41467-019-13056-x)
- en: There is one more paper which might be of interest- Performance evaluation of
    t-SNE and MDS dimensionality reduction techniques with KNN, SNN and SVM classifiers
    [https://arxiv.org/pdf/2007.13487.pdf](2007.html)
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 还有一篇可能感兴趣的论文-《使用KNN、SNN和SVM分类器对t-SNE和MDS维度缩减技术的性能评估》[t-SNE和MDS维度缩减技术的性能评估](https://arxiv.org/pdf/2007.13487.pdf)
