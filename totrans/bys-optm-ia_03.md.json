["```py\nimport torch\nimport matplotlib.pyplot as plt\n```", "```py\ndef forrester_1d(x):\n    y = -((x + 1) ** 2) * torch.sin(2 * x + 2) / 5 + 1\n    return y.squeeze(-1)\n```", "```py\nxs = torch.linspace(-3, 3, 101).unsqueeze(1)\nys = forrester_1d(xs)\n\ntorch.manual_seed(0)\ntrain_x = torch.rand(size=(3, 1)) * 6 - 3\ntrain_y = forrester_1d(train_x)\n\nplt.figure(figsize=(8, 6))\n\nplt.plot(xs, ys, label=\"objective\", c=\"r\")\nplt.scatter(train_x, train_y, marker=\"x\", c=\"k\", label=\"observations\")\n\nplt.legend(fontsize=15);\n```", "```py\nimport gpytorch\n\nclass BaseGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        ...\n\n    def forward(self, x):\n        ...\n```", "```py\ndef __init__(self, train_x, train_y, likelihood):\n        super().__init__(train_x, train_y, likelihood)\n        self.mean_module = gpytorch.means.ZeroMean()\n        self.covar_module = gpytorch.kernels.RBFKernel()\n```", "```py\ndef forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n```", "```py\nlikelihood = gpytorch.likelihoods.GaussianLikelihood()\n```", "```py\nmodel = BaseGPModel(None, None, likelihood)\n```", "```py\nlengthscale = 1\nnoise = 1e-4\n\nmodel.covar_module.lengthscale = lengthscale\nmodel.likelihood.noise = noise\n\nmodel.eval()\nlikelihood.eval()\n```", "```py\nwith torch.no_grad():\n    predictive_distribution = likelihood(model(xs))\n```", "```py\npredictive_mean = predictive_distribution.mean\n```", "```py\npredictive_lower, predictive_upper =\n    ➥    predictive_distribution.confidence_region()\n```", "```py\ntorch.manual_seed(0)\n    samples = predictive_distribution.sample(torch.Size([5]))\n```", "```py\nplt.plot(xs, predictive_mean.detach(), label=\"mean\")\n```", "```py\nplt.fill_between(\n    xs.flatten(),\n    predictive_upper,\n    predictive_lower,\n    alpha=0.3,\n    label=\"95% CI\"\n)\n```", "```py\nfor i in range(samples.shape[0]):\n    plt.plot(xs, samples[i, :], alpha=0.5)\n```", "```py\nmodel = BaseGPModel(train_x, train_y, likelihood)\n```", "```py\n# training data\ntrain_x = torch.tensor(\n    [\n        [0., 0.],\n        [1., 2.],\n        [-1., 1.]\n    ]\n)\n\ntrain_y = torch.tensor([0., -1., 0.5])\n\n# test data\ngrid_x = torch.linspace(-3, 3, 101)               ❶\n\ngrid_x1, grid_x2 = torch.meshgrid(grid_x, grid_x,\n➥indexing=\"ij\")                                  ❷\nxs = torch.vstack([grid_x1.flatten(), grid_x2.flatten()]).transpose(-1, -2)\n```", "```py\npredictive_stddev = predictive_distribution.stddev\n```", "```py\nfig, ax = plt.subplots(1, 2)\n\nax[0].imshow(\n    predictive_mean.detach().reshape(101, 101).transpose(-1, -2),\n    origin=\"lower\",\n    extent=[-3, 3, -3, 3]\n)                           ❶\n\nax[1].imshow(\n    predictive_stddev.detach().reshape(101, 101).transpose(-1, -2),\n    origin=\"lower\",\n    extent=[-3, 3, -3, 3]\n)                           ❷\n```", "```py\n    tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n            [0.0000, 0.0100, 0.0000, 0.0000],\n            [0.0000, 0.0200, 0.0000, 0.0000],\n            ...,\n            [1.0000, 0.9800, 0.0000, 0.0000],\n            [1.0000, 0.9900, 0.0000, 0.0000],\n            [1.0000, 1.0000, 0.0000, 0.0000]])\n    ```"]