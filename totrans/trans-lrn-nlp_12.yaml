- en: 9 ULMFiT and knowledge distillation adaptation strategies
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9 ULMFiT和知识蒸馏适应策略
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包括
- en: Implementing the strategies of *discriminative fine-tuning* and *gradual unfreezing*
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施*判别微调*和*逐步解冻*等策略。
- en: Executing *knowledge distillation* between *teacher* and *student* BERT models
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在*教师*和*学生*BERT模型之间执行*知识蒸馏*
- en: In this chapter and the following chapter, we will cover some adaptation strategies
    for the deep NLP transfer learning modeling architectures that we have covered
    so far. In other words, given a pretrained architecture such as ELMo, BERT, or
    GPT, how can we carry out transfer learning more efficiently? We can employ several
    measures of efficiency here. We choose to focus on *parameter efficiency*, where
    the goal is to yield a model with the fewest parameters possible while suffering
    minimal reduction in performance. The purpose of this is to make the model smaller
    and easier to store, which would make it easier to deploy on smartphone devices,
    for instance. Alternatively, smart adaptation strategies may be required just
    to get to an acceptable level of performance in some difficult transfer cases.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章和下一章中，我们将介绍迄今为止已涵盖的深度*NLP*迁移学习建模架构的一些适应策略。换句话说，鉴于预训练架构如ELMo、BERT或GPT，我们如何更有效地进行迁移学习？我们可以在这里采取几种效率措施。我们选择关注*参数效率*，即目标是在减少性能的同时产生尽可能少的参数模型。这样做的目的是使模型更小、更容易存储，从而更容易在智能手机设备上部署。另外，智能适应策略可能需要在某些困难的迁移情况下达到可接受的性能水平。
- en: In chapter 6, we described the method ULMFiT,[¹](#pgfId-1110444) which stands
    for Universal Language Model Fine-Tuning. This method introduced the concepts
    of *discriminative fine-tuning* and *gradual unfreezing*. As a brief reminder,
    gradual unfreezing progressively increases the number of sublayers of the network
    that are *unfrozen*, or fine-tuned. Discriminative fine-tuning, on the other hand,
    specifies a variable learning rate for each layer in the network, also leading
    to more effective transfer. We did not implement these methods in the code in
    chapter 6 because, as adaptation strategies, we felt that they would best fit
    in this chapter. In this chapter, we employ the *fast.ai* library, written by
    the ULMFiT authors, to demonstrate the concepts on a pretrained recurrent neural
    network (RNN)-based language model.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在第6章中，我们描述了ULMFiT[¹](#pgfId-1110444)方法，即通用语言模型微调。该方法引入了*判别微调*和*逐步解冻*的概念。简而言之，逐步解冻逐渐增加网络中*解冻*或微调的子层的数量。另一方面，判别微调为网络中的每一层指定了可变的学习率，从而实现更有效的迁移。我们在第6章的代码中没有实施这些方法，因为作为适应策略，我们认为它们最适合放在本章中。在本章中，我们使用ULMFiT作者编写的*fast.ai*库来演示预训练的循环神经网络（RNN）语言模型的概念。
- en: A few model-compression methods have been employed generally on large neural
    networks to decrease their size. Some prominent methods include weight pruning
    and quantizing. Here, we will focus on the adaptation strategy knows as *knowledge
    distillation*, due to its recent prominence in the NLP field. This process essentially
    attempts to mimic the output from the larger *teacher* model using the significantly
    smaller *student* model. In particular, we use an implementation of the method
    DistilBERT[²](#pgfId-1110458) in the transformers library to demonstrate that
    the size of a BERT can be more than halved by this approach.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 一些模型压缩方法通常被应用于大型神经网络以减小其大小。一些著名的方法包括权重修剪和量化。在这里，我们将重点关注适应策略，即*NLP*领域最近备受关注的*知识蒸馏*。该过程本质上试图使用显著较小的*学生*模型模拟来自较大的*教师*模型的输出。特别是，我们使用变压器库中的DistilBERT[²](#pgfId-1110458)方法的实现来演示通过这种方法可以将BERT的大小减半以上。
- en: Let’s begin with ULMFiT in the next section.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从下一节开始ULMFiT。
- en: 9.1 Gradual unfreezing and discriminative fine-tuning
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 逐步解冻和判别微调
- en: In this section, we will implement in code the ULMFiT method for adapting language
    models to new data domains and tasks. We first discussed this method conceptually
    at the end of chapter 6 because, historically, it was first introduced in the
    context of recurrent neural networks (RNNs). However, we delayed the actual coding
    exercise until now, to underscore that at its core, ULMFiT is an architecture-agnostic
    set of adaptation techniques. This means that they could also be applied to transformer-based
    models. We carry out the exercise in the context of an RNN-based language model,
    however, for consistency with the source material. We focus the coding exercise
    on the fake news detection example we looked at in chapter 6.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: As a reminder, discriminative fine-tuning specifies a variable learning rate
    for each layer in the network. Additionally, the learning rates are not constant
    during learning. Instead they are *slanted triangular*—linearly increasing in
    the beginning up to a point and then linearly decaying. In other words, this involves
    increasing the learning rate rapidly until it reaches the maximum rate and then
    decreasing it at a slower pace. This concept was illustrated in figure 6.8, and
    we duplicate it here for your convenience.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '![06_08](../Images/06_08.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 (Duplicated from Chapter 6) Suggested slanted triangular ULMFiT rate
    schedule for the case of 10,000 total iterations. The rate increases linearly
    for 10% of the total number of iterations (i.e., 1,000) up to a maximum of 0.01
    and then decreases linearly afterward to 0.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Note that the point in the figure labeled as “maximum learning rate” will be
    something different in our case (not 0.01). The total number of iterations will
    also be different from the 10,000 cases shown in the figure. This schedule results
    in more effective transfer and a more generalizable model.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Gradual unfreezing, on the other hand, progressively increases the number of
    sublayers of the network that are *unfrozen*, which reduces overfitting and also
    results in a more effective transfer and a more generalizable model. All these
    techniques were discussed in detail in the final section of chapter 6, and it
    may be beneficial to review that discussion briefly before tackling the rest of
    this section.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the illustrative example from section 5.2—the fact-checking example—here
    as well. Recall that this dataset contains more than 40,000 articles divided into
    two categories: “fake” and “true.” The true articles were collected from reuters.com,
    a reputable news website. The fake articles, on the other hand, were collected
    from a variety of sources flagged by PolitiFact—a fact-checking organization—as
    unreliable. In section 6.2, we trained a binary classifier on top of feature vectors
    derived from a pretrained ELMo model. This classifier predicts whether a given
    article is true (1) or fake (0). An accuracy of 98%+ was achieved with a dataset
    composed of 2,000 articles, 1,000 from each category. Here, we will see if we
    can do better with the ULMFiT method.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we split the method into two subsections. The first subsection
    addresses the first ULMFiT stage of fine-tuning a pretrained language model on
    the target task data. The slanted triangular learning rates come into play here,
    as well as the idea of discriminative fine-tuning. Some data preprocessing and
    model architecture discussions are naturally woven into this first subsection
    as well. The second subsection covers the second stage, involving fine-tuning
    the target task classifier—which sits on top of the fine-tuned language model—on
    the target task data. The effectiveness of the gradual unfreezing procedure is
    thereby demonstrated.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Please note that the code presented in this section is written in version 1
    syntax of fast.ai. The reason for this choice is that version 2 of the library
    changed the handling of input data, providing internal functions to split it into
    train and validation sets, rather than allowing you to specify your own. For consistency
    with our work in the previous chapters, where we split the data ourselves, we
    stick to version 1 here. We also provide an equivalent fast.ai version 2 syntax
    code in a Kaggle notebook,[³](#pgfId-1110478) which you should also run and compare
    with the version 1 code presented here. Finally, note that version 1 documentation
    is hosted at [https://fastai1.fast.ai/](https://fastai1.fast.ai/) whereas version
    2 documentation is hosted at [https://docs.fast.ai/](https://docs.fast.ai/).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 9.1.1 Pretrained language model fine-tuning
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Section 5.2 already described the initial data preprocessing steps that we need
    to carry out on the fact-checking example dataset. In particular, we shuffle the
    article text data and load it into the NumPy arrays `train_x` and `test_x`. We
    also construct the corresponding label NumPy arrays `train_y` and `test_y`, containing
    1 whenever the corresponding article is true and 0 otherwise. Sticking with 1,000
    samples and a test/validation ratio of 30%, as in section 5.2, yields train arrays—`train_x`,
    `train_y`—of length 1,400 and test arrays—`test_x`, `test_y`—of length 600.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing we need to do is prepare data in the form that the fast.ai
    library expects. One such data format is a two-column Pandas DataFrame, with the
    first column containing labels and the second column containing the data. We can
    construct training and testing/validation DataFrames accordingly as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的第一件事是准备 fast.ai 库所期望的数据形式。其中一种数据格式是一个两列的 Pandas DataFrame，第一列包含标签，第二列包含数据。我们可以相应地构建训练和测试/验证数据框，如下所示：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'These DataFrames should have 1,400 rows and 600 rows, respectively—one for
    each article in the corresponding data sample—and it is a good idea to check this
    with the usual `.shape` command before continuing, as shown here:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据框应该分别有1,400行和600行，每个都对应于相应数据样本中的每篇文章，并且在继续之前，最好用通常的`.shape`命令检查一下，如下所示：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The expected outputs are `(1400, 2)` and `(600, 2)`, respectively.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 预期输出分别为`(1400, 2)`和`(600, 2)`。
- en: 'Data in fast.ai is consumed by language models using the `TextLMDataBunch`
    class, instances of which can be constructed from the DataFrame format we just
    prepared using the following command:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: fast.ai 中的数据使用`TextLMDataBunch`类进行消耗，这些实例可以使用我们刚刚准备的 DataFrame 格式构建，使用以下命令：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'On the other hand, data in fast.ai is consumed by a task-specific classifier
    using the `TextClasDataBunch` class. We construct an instance of this class, in
    preparation for the next subsection, from our DataFrames using the following analogous
    command:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，fast.ai 中的数据由一个特定于任务的分类器使用`TextClasDataBunch`类进行消耗。我们构建此类的一个实例，准备进入下一小节，使用以下类似的命令从我们的数据框中：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We are now ready to fine-tune our language model on the target data! To do
    this, we need to create an instance of the `language_model_learner` fast.ai class
    with the following command:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备在目标数据上微调我们的语言模型！为此，我们需要使用以下命令创建`language_model_learner` fast.ai 类的一个实例：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Initializes a pretrained weight-dropped LSTM with a weight drop probability
    of 30%. This is pretrained on the WikiText-103 benchmark dataset.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用30%的权重丢失率初始化预训练的权重丢失 LSTM。这是在 WikiText-103 基准数据集上预训练的。
- en: Here, `AWD_LSTM` stands for *ASGD weight-dropped LSTM*.[⁴](#pgfId-1110507) This
    is just the usual LSTM architecture in which some weights have been randomly dropped,
    like what the usual dropout layer does with neural network activations, as opposed
    to weights. It is the most similar architecture choice to what was done in the
    original ULMFiT paper[⁵](#pgfId-1110512) in the fast.ai library. Also, if you
    check the execution log of the previous command, you should be able to confirm
    that it is also loading pretrained weights from a checkpoint trained on the WikiText-103
    benchmark.[⁶](#pgfId-1110515) This dataset, officially called the “WikiText long-term
    dependency language modeling dataset,” is a collection of Wikipedia articles judged
    to be “good” by humans. It is a nice, clean source of unsupervised data that has
    been used by a large number of NLP papers for benchmarking.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`AWD_LSTM`代表*ASGD权重丢失LSTM*。[⁴](#pgfId-1110507)这只是通常的LSTM架构，其中一些权重已被随机丢弃，就像通常的dropout层对神经网络激活所做的那样，与权重相反。这是最类似于
    fast.ai 库中原始ULMFiT论文中所做的架构选择。此外，如果您检查上一个命令的执行日志，您应该能够确认它还从在 WikiText-103 基准数据集上训练的检查点加载预训练权重。[⁶](#pgfId-1110515)这个数据集，官方称为“WikiText长期依赖语言建模数据集”，是一组由人类判断为“好”的维基百科文章。这是一个很好的、干净的无监督数据来源，已被许多自然语言处理论文用于基准测试。
- en: 'Now that we have loaded an instance of the model and some pretrained weights,
    we are going to try to determine the best or optimal learning rate for fine-tuning
    our language model. A nifty utility method in fast.ai called `lr_find` can do
    this for us automatically. It iterates through a number of learning rates and
    detects the point at which the loss function is dropping the fastest on the resulting
    loss versus learning rate curve. Equivalently, this is where the loss gradient
    is the smallest.[⁷](#pgfId-1110520) We can carry it out quickly using our language
    model learner `learn` as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经加载了一个模型实例和一些预训练权重，我们将尝试确定用于微调语言模型的最佳或最优学习率。fast.ai 中一个称为`lr_find`的巧妙实用方法可以自动为我们完成这项工作。它会迭代一系列学习率，并检测结果损失与学习率曲线上损失函数下降最快的点。等价地，这是损失梯度最小的地方。[⁷](#pgfId-1110520)我们可以使用我们的语言模型学习器`learn`快速进行如下操作：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Finds best/optimal learning rate
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 寻找最佳/最优学习率
- en: ❷ Plots it
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 绘制它
- en: The resulting loss versus learning rate curve, with the optimal rate highlighted,
    is shown in figure 9.1.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '![09_01](../Images/09_01.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 The result of the optimal learning rate-finding procedure from the
    fast.ai library for the language model fine-tuning step on the fake news detection
    example. Several learning rates are iterated through, and the optimal one is selected
    as the point where the loss is dropping fastest on the curve.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: 'We can programmatically retrieve this learning rate and display it using the
    following commands:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Retrieves the best/optimal rate
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Displays it
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: In our execution of the code, the optimal learning rate returned was approximately
    4.0e-2.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: 'Having found the optimal rate, we can now fine-tune our pretrained weight-dropped
    LSTM model with a slanted triangular learning rate using the `fit_one_cycle` fast.ai
    command as shown next:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ This command uses the slanted triangular learning rate under the hood. It
    takes the number of epochs and the desired maximum learning rate as inputs.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Executing the command yields an accuracy of 0.334 in about 26 seconds of fine-tuning
    on a single Kaggle GPU.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'Having obtained that baseline value, we would like to find out whether discriminative
    fine-tuning can lead to an improvement. We do this by first unfreezing all layers
    with the `unfreeze` command and then using the `slice` method to specify the upper
    and lower bounds for the learning rate range. This command sets the maximum learning
    rate of the layer closest to the output to the upper bound and geometrically decreases—via
    division by a constant factor—each subsequent layer’s maximum learning rate toward
    the lower bound. The exact code for doing this is shown next:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Makes sure all layers are unfrozen for fine-tuning
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Varies the maximum learning rate geometrically between the optimal rate in
    the final layer and a value that is two orders of magnitude smaller
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: As can be seen from the code, we arbitrarily chose to vary the learning rate
    from the maximum optimal value down to a value that is two orders of magnitude
    smaller. The intuition behind this schedule is that the subsequent layers contain
    information that is more general and less task-specific, and thus it should learn
    less from this target-specific dataset than the layers closest to the output.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'Executing the discriminative fine-tuning code presented yields an accuracy
    score of 0.353, a clear improvement over the 0.334 value we obtained without it.
    Save the fine-tuned language model for later use with the following command:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Having tuned our pretrained language model via slanted triangular learning rates
    and discriminative fine-tuning, let’s see how good a target task classifier—that
    is, a fake news detector—we can get. We fine-tune a classifier on top of the fine-tuned
    language model in the next subsection.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 9.1.2 Target task classifier fine-tuning
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Recall that in the previous subsection, we created an object for consumption
    of data by the target task classifier. We called this variable `data_clas`. As
    a next step for fine-tuning our target task classifier, we need to instantiate
    an instance of a classifier learner, with the method aptly named `text_classifier_learner`
    in fast.ai. This is done with the following code:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 请回想在前一小节中，我们创建了一个用于目标任务分类器的数据消费对象。我们将这个变量称为`data_clas`。作为微调我们的目标任务分类器的下一步，我们需要实例化一个分类器学习器的实例，方法恰当地命名为`text_classifier_learner`，在fast.ai中。下面的代码完成了这一步：
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Instantiates an instance of the target task classifier learning. Uses the
    same settings as the language model we fine-tuned, so we can load without problems.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 实例化目标任务分类器学习的一个实例。使用我们微调过的语言模型相同的设置，因此我们可以无问题地加载。
- en: ❷ Loads our fine-tuned language model
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 载入我们微调过的语言模型
- en: 'As the next step, we again employ the utility fast.ai method `lr_find` to find
    the optimal learning rate, with the following code:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 作为下一步，我们再次使用实用的fast.ai方法`lr_find`来找到最佳学习率，使用以下代码：
- en: '[PRE11]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Finds the best rate
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 寻找最佳速率
- en: ❷ Plots it
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 绘制它
- en: Executing the code yields the loss versus learning rate curve shown in figure
    9.2.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 执行该代码得到的是图9.2中显示的损失与学习率曲线。
- en: '![09_02](../Images/09_02.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![09_02](../Images/09_02.png)'
- en: Figure 9.2 The result of the optimal learning rate-finding procedure from the
    fast.ai library for the target task classifier fine-tuning step on the fake news
    detection example. Several learning rates are iterated through, and the optimal
    one is selected as the point where the loss is dropping fastest on the curve.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2 从fast.ai库获取目标任务分类器微调步骤中用于找到最佳学习率的结果的过程。通过几个学习率进行迭代，并选择最佳学习率，即在曲线上损失下降最快的点。
- en: 'We see that the optimal rate is about 7e-4\. We train the classifier learner
    for one epoch, using slanted triangular learning rates, via the following code:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到最佳速率约为7e-4。我们使用倾斜三角形学习率，通过以下代码对分类器学习器进行一轮训练：
- en: '[PRE12]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Extracts the optimal maximum learning rate
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 提取最佳的最大学习率
- en: ❷ Fine-tunes the target task classifier using the determined maximum learning
    rate in the slanted triangular learning rate schedule
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用确定的最大学习率在倾斜三角形学习率计划中微调目标任务分类器
- en: Executing the code yields an accuracy of about 99.5%. This is already better
    than the result of 98%+ we obtained by training a classifier on top of the ELMo
    embedding in chapter 6 (section 6.2). Is there anything else we can do to improve
    it even further?
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 执行该代码得到的准确率约为99.5%。这已经比我们在第6章（第6.2节）通过在ELMo嵌入之上训练分类器得到的98%+的结果更好了。我们还能做些什么来进一步提高它呢？
- en: 'Luckily, we have one more trick up our sleeve: gradual unfreezing. As yet another
    reminder, this is when we unfreeze just one layer, fine-tune it, unfreeze an additional
    lower layer, fine-tune it, and repeat this process for a fixed number of steps.
    The ULMFiT authors found that applying this method during the fine-tuning of the
    target task classifier stage significantly improves results. As a simple illustration,
    to execute this procedure up to a layer depth of 2, we would need the following
    bit of code:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们还有一个底牌：渐进式解冻。再次提醒，这是当我们仅解冻一层，微调它，解冻一个额外的较低层，微调它，并重复此过程一定次数时。ULMFiT的作者发现，在目标任务分类器阶段应用此方法显着改善了结果。举个简单的例子，要执行此过程直到2层深度，我们需要以下代码：
- en: '[PRE13]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ We execute gradual unfreezing up to two unfrozen layers only.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们仅执行渐进式解冻，直到解冻两个层为止。
- en: ❷ Unfreezes progressively more layers, first one and then two, training for
    one epoch each time with slanted triangular learning rates
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 逐渐解冻更多层，首先一个，然后两个，每次使用倾斜三角形学习率进行一轮训练
- en: ❸ This command unfreezes the top i layers.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 此命令解冻了顶部i层。
- en: ❹ Slanted triangular learning rate execution for one epoch, as already introduced
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 执行一次倾斜三角形学习率，如已经介绍的
- en: Note that the command `learn.freeze_to``(-i)` unfreezes the top `i` layers and
    is critical for the recipe. When we executed the code on our fake news detection
    example, we found the accuracy reached 99.8% in the first step, hitting the stunning
    score of 100% at the second step when the top two layers were unfrozen. These
    results speak for themselves and appear to suggest that the ULMFiT method is an
    extremely useful set of techniques to have in one’s toolbox. Note that we could
    have continued unfreezing layers to greater depths—3, 4, and so on—if we found
    it necessary.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，命令 `learn.freeze_to``(-i)` 冻结前 `i` 层对于本次操作至关重要。在我们对虚假新闻检测示例上执行代码时，我们发现在第一步中准确性达到了99.8％，当解冻了前两层时，准确性达到了惊人的100％。这些结果充分说明了自己，似乎表明ULMFiT方法是一套非常有用的技术。请注意，如果有必要，我们可以继续解冻更深层次的层次——第3层，第4层等等。
- en: Fascinating stuff! It appears that being clever about how we adapt models to
    new scenarios can lead to significant benefits! In the next section, we will touch
    on another method for doing this—knowledge distillation.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 奇妙的事情！看来在我们适应新场景时，聪明地调整模型可以带来显著的好处！在接下来的章节中，我们将介绍另一种实现这一点的方法——知识蒸馏。
- en: 9.2 Knowledge distillation
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2 知识蒸馏
- en: Knowledge distillation is a neural network compression method that seeks to
    teach a smaller student model the knowledge contained within a larger teacher
    model. This strategy, which has become popular recently in the NLP community,
    essentially attempts to mimic the output from the teacher by the student. This
    approach is also model-agnostic—the teacher and student could be transformer-based,
    RNN-based, or some other architecture, and can be completely different from each
    other.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏是一种神经网络压缩方法，旨在教授一个较小的学生模型大型教师模型所包含的知识。这种方法近年来在 NLP 社区中变得流行，本质上是试图通过学生来模仿教师的输出。此方法也与模型无关——教师和学生可以是基于变压器的、基于循环神经网络的或其他结构，并且彼此之间可以完全不同。
- en: Initial applications of this method in NLP were driven by questions about the
    representation power of bidirectional LSTMs (bi-LSTMs) compared to transformer-based
    architectures.[⁸](#pgfId-1110609) The authors wanted to know how much of BERT’s
    information could be captured by a single bi-LSTM layer. Surprisingly, researchers
    found that in some cases, pretrained transformer-based language models could be
    reduced in parameter size by a factor of 100, with an inference time lower by
    a factor of 15, while not sacrificing in the standard performance metrics. This
    is a huge reduction in size and time that can make the difference between whether
    or not these methods can be practically deployed! The process of knowledge distillation
    is briefly summarized in figure 9.3.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在 NLP 领域中，对此方法的最初应用是由于对双向LSTM（bi-LSTMs）的表示能力与基于变压器的架构之间的比较的疑问。[⁸](#pgfId-1110609)
    作者想要知道单个bi-LSTM层是否能够捕捉到BERT的多少信息。令人惊讶的是，研究人员发现，在某些情况下，预训练的基于变压器的语言模型的参数数量可以减少100倍，推理时间可以减少15倍，同时不损失标准性能指标。这是一个巨大的尺寸和时间上的减少，可以决定这些方法是否可以实际部署！知识蒸馏的过程在图9.3中简要概述。
- en: '![09_03](../Images/09_03.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![09_03](../Images/09_03.png)'
- en: Figure 9.3 An illustration of the general process of knowledge distillation.
    “Soft” labels produced by the teacher model are used to encourage the student
    model to behave similarly via the distillation loss. At the same time, the student
    loss is trained to behave similarly to ground truth via the student loss.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 是知识蒸馏的一般过程的示意图。教师模型产生的“软”标签被用于通过蒸馏损失鼓励学生模型表现出类似的行为。同时，学生损失被训练成与通过学生损失的标准地面真实情况行为类似。
- en: As can be seen in the figure, traditionally, labels produced by the teacher
    are used to compute “soft” labels, which determine the distillation loss by comparison
    with the student output. This loss encourages the student model to track the teacher
    model output. Additionally, the student is simultaneously taught the “hard” ground
    truth labels via the student loss. We will show you how to use this idea quickly
    via an implementation in the *transformers* library by Hugging Face.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，传统上，教师产生的标签被用于计算“软”标签，通过与学生的输出进行比较来确定蒸馏损失。这种损失促使学生模型跟踪教师模型的输出。此外，学生还通过学生损失同时学习“硬”的真实标签。我们将通过
    Hugging Face 的 *transformers* 库来快速展示如何使用这个想法实现。
- en: Several architectures have been proposed to reduce the size of pretrained NLP
    language models, including TinyBERT [⁹](#pgfId-1110619) and DistilBERT.[^(10)](#pgfId-1110622)
    We choose to focus on DistilBERT due to its ready availability within the transformers
    library. DistilBERT was developed by Hugging Face, which is the same team that
    wrote the transformers library. As before, our coverage of this topic is not meant
    to be exhaustive but rather illustrative. Keeping up with further development
    and literature in a fast-moving field like this one remains important. It is our
    hope that the coverage presented here will set you up to do that.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 已经提出了几种架构来减小预训练的 NLP 语言模型的尺寸，包括 TinyBERT [⁹](#pgfId-1110619) 和 DistilBERT. [^(10)](#pgfId-1110622)
    我们选择专注于 DistilBERT，因为它在 transformers 库中已经准备就绪。 DistilBERT 是由 Hugging Face 开发的，这是与编写
    transformers 库相同的团队。 与以前一样，我们对这个主题的覆盖并不意味着是全面的，而是举例说明。 在像这样快速发展的领域中保持进一步开发和文献的更新仍然很重要。
    我们希望这里所呈现的内容能让您做到这一点。
- en: The goal of the DistilBERT study was to generate a smaller version of the BERT
    model specifically. The student architecture was chosen to be the same as BERT—stacked
    transformer encoders as described in chapters 7 and 8\. The number of the layers
    in the student was reduced by a factor of two, yielding a model of just six layers.
    This is where most of the size savings comes from. The authors found variations
    in internal hidden dimensions to have little impact on efficiency in this framework,
    and as such, those are all similar between the teacher and the student. An important
    part of the process is student initialization to an appropriate set of weights
    from which convergence will be reasonably quick. Because the dimensions of all
    the layers are similar between the teacher and the student, the authors could
    simply use the pretrained teacher weights in corresponding layers to initialize
    the student and found that to work well.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: DistilBERT 研究的目标是特别生成 BERT 模型的较小版本。 学生架构被选择为与 BERT 相同-在第7和第8章中描述的堆叠变压器编码器。 学生的层数减少了一半，只有六层的模型。
    这是大部分尺寸节省的地方。 作者发现在这种框架中，内部隐藏维度的变化对效率几乎没有影响，因此，在教师和学生之间都是相似的。 过程的一个重要部分是将学生初始化到适当的一组权重，从中收敛会相对较快。
    因为教师和学生的所有层的尺寸都是相似的，作者可以简单地使用对应层中的预训练教师权重来初始化学生，并发现这样做效果良好。
- en: The authors did extensive experiments on benchmarks such as GLUE, which we will
    look at later in the next chapter, and SQuAD. They found that performance on the
    GLUE benchmark by the resulting DistilBERT model retained 97% of the performance
    of the BERT teacher with 40% of the number of parameters. It is also 60% faster,
    in terms of inference time on a CPU, and 71% faster on mobile devices such as
    the iPhone. As you can see, the improvement is significant.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 作者对 GLUE 等基准进行了广泛的实验证明，我们将在下一章中看到，并在 SQuAD 上进行了实验证明。 他们发现，由结果产生的 DistilBERT
    模型在 GLUE 基准上的性能保持了 BERT 教师模型的97% ，但参数个数只有教师的40%. 它在 CPU 上的推理时间也快了60%，而在 iPhone
    等移动设备上快了71%。 如您所见，这是一项明显的改进。
- en: Scripts for performing the actual distillation are available in the official
    transformers repository.[^(11)](#pgfId-1110628) To train your own DistilBERT model,
    you create a file with one text sample per line and execute a sequence of commands
    provided on that page, which prepare the data and distill the model. Because the
    authors already made a variety of checkpoints available for direct loading—all
    of the checkpoints are listed on that page—and our focus here is on transfer learning,
    we do not repeat the training from scratch steps here. Instead, we work with a
    checkpoint analogous to the mBERT checkpoint we used in chapter 8 for our cross-lingual
    transfer learning experiment. This allows us to directly compare the performance
    and benefits of using the distilled architecture over the original mBERT, while
    also teaching you how to start using this architecture in your own projects. It
    also gives you another opportunity to work on fine-tuning a pretrained transformer-based
    model on a custom corpus—directly modifying the code with a different architecture,
    pretrained checkpoint, and custom dataset should work for your own use cases.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: More specifically, we will be repeating the experiment we carried out in section
    8.2.2, where we transferred the knowledge contained in mBERT—trained on over 100
    languages simultaneously—to a monolingual Twi scenario by fine-tuning on a corpus
    from the JW300 dataset. We perform the variant of the experiment that uses the
    pretrained tokenizer included with the checkpoint, rather than training a new
    one from scratch, for simplicity.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.1 Transfer DistilmBERT to monolingual Twi data with pretrained tokenizer
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our goal in this subsection is to generate a DistilBERT model for the Ghanaian
    language Twi from a model trained on more than 100 languages, not including Twi,
    simultaneously. The multilingual equivalent of BERT is called mBERT; therefore,
    the multilingual equivalent of DistilBERT is predictably called DistilmBERT. This
    DistilmBERT model is directly analogous to the mBERT model we experimented with
    in chapter 8\. We found then that starting with this checkpoint was beneficial,
    even though Twi was not included in the original training. Here, we will essentially
    replicate the same sequence of steps, replacing every instance of mBERT with DistilmBERT.
    This allows us to compare the two directly and thereby get an intuition for the
    benefits of knowledge distillation, while also learning how to use DistilBERT
    in your own projects. As before, we fine-tune the model on the monolingual Twi
    subset of the JW300 datatset.[^(12)](#pgfId-1110637)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by initializing a DistilBERT tokenizer to the pretrained checkpoint
    from the DistilmBERT model. We use the cased version this time, as shown by the
    following code:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ This is just a faster version of DistilBertTokenizer, which you could use
    instead.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Uses the pretrained DistilmBERT tokenizer
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: 'Having prepared the tokenizer, load the DistilmBERT checkpoint into a DistilBERT
    masked language model, and display the number of parameters as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好 tokenizer 后，将 DistilmBERT 检查点加载到 DistilBERT 掩码语言模型中，并按照以下方式显示参数的数量：
- en: '[PRE15]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Uses masked language modeling
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用掩码语言建模
- en: ❷ Initializes to an mBERT checkpoint
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 初始化为 mBERT 检查点
- en: The output indicates that the model has 135.5 million parameters versus the
    178.6 million parameters we found BERT has in chapter 8\. Thus, our DistilBERT
    model is just 76% the size of the equivalent BERT model.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 输出表明，与我们在第 8 章中发现的 BERT 模型的 178.6 百万个参数相比，该模型具有 1.355 亿个参数。 因此，DistilBERT 模型的大小仅为等效
    BERT 模型的 76％。
- en: 'Next, build the dataset from the monolingual Twi text, using the convenient
    `LineByLineTextDataset` method included with transformers, shown next:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用 transformers 中方便的 `LineByLineTextDataset` 方法从单语 Twi 文本构建数据集，具体方法如下所示：
- en: '[PRE16]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ The English to Twi JW300 dataset we introduced in section 8.2.1
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们在第 8.2.1 节中介绍的英语到 Twi JW300 数据集
- en: ❷ How many lines to read at a time
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 一次读取多少行
- en: 'Subsequently, define a “data collator”—a helper method that creates a special
    object out of a batch of sample data lines (of length `block_size`)—as shown in
    the next code snippet. This special object is consummable by PyTorch for neural
    network training:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，按照下面的代码片段中所示的方式定义“数据集整理器”——这是一个帮助程序，它将一批样本数据行（长度为 `block_size`）创建成一个特殊对象，这个特殊对象可以被
    PyTorch 用于神经网络训练：
- en: '[PRE17]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Uses masked language modeling, and masks words with a probability of 0.15
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用掩码语言建模，并掩码单词的概率为 0.15
- en: Here, we used masked language modeling—15% of the words are randomly masked
    in our input data, and the model is asked to predict them during training.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了掩码语言建模的方法——将我们输入数据中的 15% 的单词随机掩码，要求模型在训练过程中进行预测。
- en: 'Next, define standard training arguments, such as the output directory (which
    we select to be `twidistilmbert`) and training batch size, as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，按照以下方式定义标准的训练参数，例如输出目录（我们选择为 `twidistilmbert`）和训练批次大小：
- en: '[PRE18]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Then, use training arguments with the previously defined dataset and collator
    to define a “trainer” for one training epoch across the data, as shown here. Remember
    the Twi data contains over 600,000 lines, so one pass across all of it is a significant
    amount of training!
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用已定义的数据集和数据整理器定义“训练器”，并在数据上进行一个训练时代，具体方法如下。请记住，Twi 数据包含超过 600,000 行，因此在所有数据上进行一遍训练是相当费力的！
- en: '[PRE19]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Finally, train and time how long training takes as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，按照以下方式进行训练并计算训练所需的时间：
- en: '[PRE20]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'As always, be sure to save the model:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往地，一定要保存模型：
- en: '[PRE21]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We found that the model takes about 2 hours and 15 minutes to complete the epoch,
    versus the 3 hours it took for the equivalent teacher to complete the epoch in
    chapter 8\. Thus, the student training time is just 75% of the teacher. Significant
    improvement!
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，与第 8 章中等效教师完成每个时代所需的 3 小时相比，该模型花费了大约 2 小时和 15 分钟完成该时代。 因此，学生的训练时间只有老师的
    75％。 显著提高！
- en: Moreover, the loss reaches a value of about 0.81, whereas the mBERT equivalent
    reached a loss of about 0.77 in chapter 8\. In absolute terms, the difference
    in performance can then be roughly quantified as approximately 5%—we see DistilBERT
    reach 95% of the performance of BERT. This is pretty close to the benchmark number
    of 97% reported by the DistilBERT authors in their paper.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，损失函数的值达到了约 0.81，而 mBERT 的等效模型在第 8 章中的损失为约 0.77。就绝对值而言，性能差异可以粗略地量化为大约 5％——我们看到
    DistilBERT 达到了 BERT 性能的 95％。 这非常接近 DistilBERT 作者在论文中报告的基准数字 97％。
- en: 'As the last step, take the following sentence from the corpus: “Eyi de *ɔ*haw
    k*ɛ*se baa sukuu h*ɔ.”* Drop/mask one word, sukuu (which means “school” in Twi),
    and then apply the pipelines API to predict the dropped word as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步，从语料库中取出以下句子：“Eyi de *ɔ*haw k*ɛ*se baa sukuu h*ɔ。” 掩盖一个单词，sukuu（在 Twi 中表示“学校”），然后将管道
    API 应用于以下预测所删除的单词：
- en: '[PRE22]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ Defines the fill-in-the-blanks pipeline
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义了填空管道
- en: ❷ Predicts the masked token
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 预测掩码标记
- en: 'This yields the following output:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '[PRE23]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: These are indeed plausible completions. What is interesting to note here is
    that the religious bias in the outcome that we saw in section 8.2.2 appears to
    have been alleviated in the model. Completions such as “Israel” and “Eden,” which
    were suggested by the mBERT equivalent model in section 8.2.2, are no longer present.
    This can be explained, in retrospect, by the significant difference in the number
    of parameters between the two. DistilBERT is less likely to overfit due to this,
    whereas BERT is much more likely to do so.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: And now you know how to use DistilBERT in your own projects! We again stress
    that the exercise you just carried out taught you how to fine-tune a pretrained
    transformer-based model on a custom corpus—simply modify the code with a different
    architecture, pretrained checkpoint, and custom dataset to apply it to your own
    use cases.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: In the first section of the next chapter, we will get another opportunity to
    fine-tune a transformer-based model on a custom corpus, this time in English!
    We will discuss the adaptation ideas behind the architecture ALBERT—A Lite BERT—and
    fine-tune it on some reviews from the Multi-Domain Sentiment Dataset.[^(13)](#pgfId-1110728)
    Recall that we played with this dataset in chapter 4\. It is a dataset of reviews
    of 25 product categories on Amazon, of which we will focus on book reviews, as
    in chapter 4.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ULMFiT strategies of slanted triangular learning rates, *discriminative fine-tuning,*
    and *gradual unfreezing* can lead to noticeably more effective transfer.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Executing *knowledge distillation* of a larger *teacher* BERT model yields a
    significantly *smaller student* BERT model with minimal sacrifice of performance.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1. [http://nlp.fast.ai/ulmfit](http://nlp.fast.ai/ulmfit)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '2. V. Sanh et al., “DistilBERT, a Distilled Version of BERT: Smaller, Faster,
    Cheaper and Lighter,” EMC^2: 5th Edition Co-located with NeurIPS (2019).'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 3. [https://www.kaggle.com/azunre/tlfornlp-chapter9-ulmfit-adaptation-fast-aiv2](https://www.kaggle.com/azunre/tlfornlp-chapter9-ulmfit-adaptation-fast-aiv2)
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: 4. S. Merity et al., “Regularizing and Optimizing LSTM Language Models,” ICLR
    (2018).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 5. [http://nlp.fast.ai/ulmfit](http://nlp.fast.ai/ulmfit)
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: 6. [https://www.salesforce.com/ca/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/](https://www.salesforce.com/ca/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/)
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '7. L. Smith et al., “A Disciplined Approach to Neural Network Hyper-Parameters:
    Part 1—Learning Rate, Batch Size, Momentum, and Weight Decay,” arXiv (2018).'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 8. R. Tang et al., “Distilling Task-Specific Knowledge from BERT into Simple
    Neural Networks,” arXiv (2018).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '9. X. Jiao et al., “TinyBERT: Distilling BERT for Natural Language Understanding,”
    arXiv (2020).'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '10. V. Sanh et al., “DistilBERT, a Distilled Version of BERT: Smaller, Faster,
    Cheaper and Lighter,” EMC^2: 5th Edition Co-located with NeurIPS (2019).'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 11. [https://github.com/huggingface/transformers/blob/master/examples/research_projects/distillation](https://github.com/huggingface/transformers/blob/master/examples/research_projects/distillation)
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 11. [https://github.com/huggingface/transformers/blob/master/examples/research_projects/distillation](https://github.com/huggingface/transformers/blob/master/examples/research_projects/distillation)
- en: 12. [http://opus.nlpl.eu/JW300.php](http://opus.nlpl.eu/JW300.php)
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 12. [http://opus.nlpl.eu/JW300.php](http://opus.nlpl.eu/JW300.php)
- en: 13. [https://www.cs.jhu.edu/~mdredze/datasets/sentiment/](https://www.cs.jhu.edu/~mdredze/datasets/sentiment/)
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 13. [https://www.cs.jhu.edu/~mdredze/datasets/sentiment/](https://www.cs.jhu.edu/~mdredze/datasets/sentiment/)
