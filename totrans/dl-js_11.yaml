- en: Chapter 4\. Recognizing images and sounds using convnets
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第4章。使用卷积神经网络识别图像和声音
- en: '*This chapter covers*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章内容涵盖*'
- en: How images and other perceptual data, such as audio, are represented as multidimensional
    tensors
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像和其他知觉数据（例如音频）如何表示为多维张量
- en: What convnets are, how they work, and why they are especially suitable for machine-learning
    tasks involving images
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络是什么、如何工作以及为什么它们特别适用于涉及图像的机器学习任务
- en: How to write and train a convnet in TensorFlow.js to solve the task of classifying
    hand-written digits
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何编写和训练一个TensorFlow.js中的卷积神经网络来解决手写数字分类的任务
- en: How to train models in Node.js to achieve faster training speeds
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在Node.js中训练模型以实现更快的训练速度
- en: How to use convnets on audio data for spoken-word recognition
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在音频数据上使用卷积神经网络进行口语识别
- en: The ongoing deep-learning revolution started with breakthroughs in image-recognition
    tasks such as the ImageNet competition. There is a wide range of useful and technically
    interesting problems that involve images, from recognizing the contents of images
    to segmenting images into meaningful parts, and from localizing objects in images
    to synthesizing images. This subarea of machine learning is sometimes referred
    to as *computer vision*.^([[1](#ch04fn1)]) Computer-vision techniques are often
    transplanted to areas that have nothing to do with vision or images (such as natural
    language processing), which is one more reason why it is important to study deep
    learning for computer vision.^([[2](#ch04fn2)]) But before delving into computer-vision
    problems, we need to discuss the ways in which images are represented in deep
    learning.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 持续进行的深度学习革命始于图像识别任务的突破，比如ImageNet比赛。涉及图像的有许多有用且技术上有趣的问题，包括识别图像的内容、将图像分割成有意义的部分、在图像中定位对象以及合成图像。这个机器学习的子领域有时被称为*计算机视觉*^([[1](#ch04fn1)])。计算机视觉技术经常被移植到与视觉或图像无关的领域（如自然语言处理），这也是为什么学习计算机视觉的深度学习至关重要的另一个原因^([[2](#ch04fn2)])。但在深入讨论计算机视觉问题之前，我们需要讨论图像在深度学习中的表示方式。
- en: ¹
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Note that computer vision is itself a broad field, some parts of which use non-machine-learning
    techniques beyond the scope of this book.
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 需要注意的是，计算机视觉本身是一个广泛的领域，其中一些部分使用了本书范围以外的非机器学习技术。
- en: ²
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ²
- en: ''
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Readers who are especially interested in deep learning in computer vision and
    want to dive deeper into the topic can check out Mohamed Elgendy’s, *Grokking
    Deep Learning for Computer Vision*, Manning Publications, in press.
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 对于对计算机视觉深度学习特别感兴趣并希望深入了解该主题的读者，可以参考Mohamed Elgendy的《*深度学习图像处理入门*》，Manning出版社，即将出版。
- en: '4.1\. From vectors to tensors: Representing images'
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1。从向量到张量：图像的表示
- en: In the previous two chapters, we looked at machine-learning tasks involving
    numerical inputs. For example, the download-duration prediction problem in [chapter
    2](kindle_split_013.html#ch02) took a single number (file size) as the input.
    The input in the Boston-housing problem was an array of 12 numbers (number of
    rooms, crime rate, and so on). What these problems have in common is the fact
    that each input example can be represented as a flat (non-nested) array of numbers,
    which corresponds to a 1D tensor in TensorFlow.js. Images are represented differently
    in deep learning.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两章中，我们讨论了涉及数值输入的机器学习任务。例如，[第2章](kindle_split_013.html#ch02)中的下载时长预测问题将单个数字（文件大小）作为输入。波士顿房价问题的输入是一个包含12个数字的数组（房间数量、犯罪率等）。这些问题的共同点是，每个输入示例都可以表示为一维（非嵌套）数字数组，对应于TensorFlow.js中的一维张量。图像在深度学习中的表示方式有所不同。
- en: To represent an image, we use a 3D tensor. The first two dimensions of the tensor
    are the familiar height and width dimensions. The third one is the color channel.
    For example, color is often encoded as RGB values. In this case, each of the three
    colors is a channel, which leads to a size of 3 along the third dimension. If
    we have an RGB-encoded color image of size 224 × 224 pixels, we can represent
    it as a 3D tensor of size `[224, 224, 3]`. The images in some computer-vision
    problems are noncolor (for example, grayscale). In those cases, there is only
    one channel, which, if represented as a 3D tensor, will lead to a tensor shape
    of `[height, width, 1]` (see [figure 4.1](#ch04fig01) for an example).^([[3](#ch04fn3)])
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了表示一张图片，我们使用一个三维张量。张量的前两个维度是熟悉的高度和宽度维度。第三个是颜色通道。例如，颜色通常被编码为 RGB 值。在这种情况下，三个颜色值分别是通道，导致第三个维度的大小为
    3。如果我们有一张尺寸为 224 × 224 像素的 RGB 编码颜色图像，我们可以将其表示为一个尺寸为 `[224, 224, 3]` 的三维张量。某些计算机视觉问题中的图像是无颜色的（例如灰度图像）。在这种情况下，只有一个通道，如果将其表示为三维张量，将导致张量形状为
    `[height, width, 1]` （参见[图 4.1](#ch04fig01)）。^([[3](#ch04fn3)])
- en: ³
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³
- en: ''
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An alternative is to “flatten” all the pixels of the image and their associated
    color values into a 1D tensor (a flat array of numbers). But doing so makes it
    hard to exploit the association between the color channels of each pixel and the
    2D spatial relations between pixels.
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 另一种选择是将图像的所有像素及其关联的颜色值展开为一个一维张量（一个由数字组成的扁平数组）。但是这样做很难利用每个像素的颜色通道与像素间的二维空间关系之间的关联。
- en: Figure 4.1\. Representing an MNIST image as tensors in deep learning. For the
    sake of visualization, we downsized the MNIST image from 28 × 28 to 8 × 8\. The
    image is a grayscale one, which leads to a height-width-channel (HWC) shape of
    `[8, 8, 1]`. The single color channel along the last dimension is omitted in this
    diagram.
  id: totrans-20
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.1\. 在深度学习中使用张量表示一个 MNIST 图像。为了可视化，我们将 MNIST 图像从 28 × 28 缩小到 8 × 8。这张图片是一个灰度图像，它的高度-宽度-通道(HWC)形状为`[8,
    8, 1]`。这个图示中省略了最后一维的单个颜色通道。
- en: '![](04fig01a_alt.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](04fig01a_alt.jpg)'
- en: This mode of encoding an image is referred to as *height-width-channel (HWC)*.
    To perform deep learning on images, we often combine a set of images into a batch
    for efficient parallelized computation. When batching images, the dimension of
    individual images is always the first dimension. This is similar to how we combined
    1D tensors into a batched 2D tensor in [chapters 2](kindle_split_013.html#ch02)
    and [3](kindle_split_014.html#ch03). Therefore, a batch of images is a 4D tensor,
    with the four dimensions being image number (N), height (H), width (W), and color
    channel (C), respectively. This format is referred to as *NHWC*. There is an alternative
    format, resulting from a different ordering of the four dimensions. It is called
    *NCHW*. As its name suggests, NCHW puts the channel dimension ahead of the height
    and width dimensions. TensorFlow.js can handle both the NHWC and NCHW formats.
    But we will only use the default NHWC format in this book, for consistency.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这种编码图像的方式被称为 *高度-宽度-通道(HWC)*。为了在图像上进行深度学习，我们经常将一组图像合并成一个批次以便进行高效的并行计算。当批量处理图像时，各个图像的维度总是第一维。这与我们在
    [第2章](kindle_split_013.html#ch02) 和 [第3章](kindle_split_014.html#ch03) 中将一维张量组合成批量化的二维张量的方式类似。因此，图像的批次是一个四维张量，它的四个维度分别是图像数量（N）、高度（H）、宽度（W）和颜色通道（C）。这个格式被称为
    *NHWC*。还有另一种格式，它由四个维度的不同排序得出。它被称为 *NCHW*。顾名思义，NCHW 将通道维度放在高度和宽度维度之前。TensorFlow.js可以处理
    NHWC 和 NCHW 两种格式。但是我们在本书中只使用默认的 NHWC 格式，以保持一致性。
- en: 4.1.1\. The MNIST dataset
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1\. MNIST 数据集
- en: The computer-vision problem we will focus on in this chapter is the MNIST^([[4](#ch04fn4)])
    handwritten-digit dataset. This is such an important and frequently used dataset
    that it is often referred to as the “hello world” for computer vision and deep
    learning. The MNIST dataset is older and smaller than most datasets you will find
    in deep learning. Yet it is good to be familiar with it because it is widely used
    as an example and often serves as a first test for novel deep-learning techniques.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将着眼于计算机视觉问题中的 MNIST^([[4](#ch04fn4)])手写数字数据集。这个数据集非常重要并且频繁使用，通常被称为计算机视觉和深度学习的“Hello
    World”。MNIST 数据集比大多数深度学习数据集都要旧，也要小。然而熟悉它是很好的，因为它经常被用作示例并且经常用作新颖深度学习技术的第一个测试。
- en: ⁴
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁴
- en: ''
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: MNIST stands for Modified NIST. The “NIST” part of the name comes from the fact
    that the dataset originated from the US National Institute of Standards and Technology
    around 1995\. The “modified” part of the name reflects the modification made to
    the original NIST dataset, which included 1) normalizing images into the same
    uniform 28 × 28 pixel raster with anti-aliasing to make the training and test
    subsets more homogeneous and 2) making sure that the sets of writers are disjoint
    between the training and test subsets. These modifications made the dataset easier
    to work with and more amenable to objective evaluation of model accuracy.
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: MNIST 代表 Modified NIST。名称中的“NIST”部分源自数据集约于 1995 年起源于美国国家标准技术研究所。名称中的“modified”部分反映了对原始
    NIST 数据集所做的修改，包括 1）将图像标准化为相同的均匀 28 × 28 像素光栅，并进行抗锯齿处理，以使训练和测试子集更加均匀，以及 2）确保训练和测试子集之间的作者集是不相交的。这些修改使数据集更易于处理，并更有利于模型准确性的客观评估。
- en: Each example in the MNIST dataset is a 28 × 28 grayscale image (see [figure
    4.1](#ch04fig01) for an example). These images were converted from real handwriting
    of the 10 digits 0 through 9\. The image size of 28 × 28 is sufficient for reliable
    recognition of these simple shapes, although it is smaller than the image sizes
    seen in typical computer-vision problems. Each image is accompanied by a definitive
    label, which indicates which of the 10 possible digits the image actually is.
    As we have seen in the download-time prediction and Boston-housing datasets, the
    data is divided into a training set and a test set. The training set consists
    of 60,000 images, while the test contains 10,000 images. The MNIST dataset^([[5](#ch04fn5)])
    is approximately balanced, meaning that there are approximately equal numbers
    of examples for the 10 categories (that is, the 10 digits).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST 数据集中的每个示例都是一个 28 × 28 的灰度图像（参见[图 4.1](#ch04fig01)作为示例）。这些图像是从 0 到 9 的
    10 个数字的真实手写转换而来的。28 × 28 的图像尺寸足以可靠地识别这些简单形状，尽管它比典型的计算机视觉问题中看到的图像尺寸要小。每个图像都附有一个明确的标签，指示图像实际上是
    10 个可能数字中的哪一个。正如我们在下载时间预测和波士顿房屋数据集中看到的那样，数据被分为训练集和测试集。训练集包含 60,000 个图像，而测试集包含
    10,000 个图像。MNIST 数据集^([[5](#ch04fn5)])大致平衡，这意味着这 10 个类别的示例大致相等（即 10 个数字）。
- en: ⁵
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁵
- en: ''
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See Yann LeCun, Corinna Cortes, and Christopher J.C. Burges, “The MNIST Database
    of Handwritten Digits,” [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/).
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请参阅 Yann LeCun、Corinna Cortes 和 Christopher J.C. Burges 的《手写数字 MNIST 数据库》[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)。
- en: 4.2\. Your first convnet
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 您的第一个卷积网络
- en: Given the representation of the image data and the labels, we know what kind
    of input a neural network that solves the MNIST dataset should take and what kind
    of output it should generate. The input to the neural network is a tensor of the
    NHWC-format shape `[null, 28, 28, 1]`. The output is a tensor of shape `[null,
    10]`, where the second dimension corresponds to the 10 possible digits. This is
    the canonical one-hot encoding of multiclass-classification targets. It is the
    same as the one-hot encoding of the species of iris flowers we saw in the iris
    example in [chapter 3](kindle_split_014.html#ch03). With this knowledge, we can
    dive into the details of convnets (which, as a reminder, is short for convolutional
    networks), the method of choice for image-classification tasks such as MNIST.
    The “convolutional” part of the name may sound scary. It is just a type of mathematical
    operation, and we will explain it in detail.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于图像数据和标签的表示，我们知道解决 MNIST 数据集的神经网络应该采用何种输入，以及应该生成何种输出。神经网络的输入是形状为 `[null, 28,
    28, 1]` 的 NHWC 格式张量。输出是形状为 `[null, 10]` 的张量，其中第二个维度对应于 10 个可能的数字。这是多类分类目标的经典一热编码。这与我们在[第
    3 章](kindle_split_014.html#ch03)中看到的鸢尾花种类的一热编码相同。有了这些知识，我们可以深入了解卷积网络（作为提醒，卷积网络是图像分类任务（如
    MNIST）的选择方法）的细节。名称中的“卷积”部分可能听起来很吓人。这只是一种数学运算，我们将详细解释它。
- en: 'The code is in the mnist folder of tfjs-examples. Like the previous examples,
    you can access and run the code as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 代码位于 tfjs-examples 的 mnist 文件夹中。与前面的示例一样，您可以按以下方式访问和运行代码：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[Listing 4.1](#ch04ex01) is an excerpt from the main index.js code file in
    the mnist example. It is a function that creates the convnet we use to solve the
    MNIST problem. The number of layers in this sequential model (seven) is significantly
    greater than in the examples we have seen so far (between one and three layers).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单4.1](#ch04ex01)是mnist示例中主要index.js代码文件的摘录。这是一个函数，用于创建我们用来解决MNIST问题的convnet。此顺序模型的层数（七层）明显多于我们到目前为止看到的示例（一到三层之间）。'
- en: Listing 4.1\. Defining a convolutional model for the MNIST dataset
  id: totrans-37
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单4.1。为MNIST数据集定义卷积模型
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '***1*** First conv2d layer'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 第一个conv2d层'
- en: '***2*** Pooling after convolution'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 卷积后进行池化'
- en: '***3*** Repeating “motif” of conv2d-maxPooling2d'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** conv2d-maxPooling2d的重复“模式”'
- en: '***4*** Flattens tensor to prepare for dense layers'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 将张量展平以准备进行密集层'
- en: '***5*** Uses softmax activation for multiclass classification problem'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 用于多类分类问题的softmax激活函数'
- en: '***6*** Prints a text summary of model'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 打印模型的文本摘要'
- en: The sequential model constructed by the code in [listing 4.1](#ch04ex01) consists
    of seven layers, created one by one through the `add()` method calls. Before we
    examine the detailed operations performed by each layer, let’s look at the model’s
    overall architecture, which is shown in [figure 4.2](#ch04fig02). As the diagram
    shows, the model’s first five layers include a repeating pattern of conv2d-maxPooling2d
    layer groups, followed by a flatten layer. The groups of conv2d-maxPooling2d layers
    are the working horse of feature extraction. Each of the layers transforms an
    input image into an output one. A conv2d layer operates through a *convolutional
    kernel*, which is “slid” over the height and width dimensions of the input image.
    At each sliding position, it is multiplied with the input pixels, and the products
    are summed and fed through a nonlinearity. This yields a pixel in the output image.
    The maxPooling2d layers operate in a similar fashion but without a kernel. By
    passing the input image data through the successive layers of convolution and
    pooling, we get tensors that become smaller and smaller in size and more and more
    abstract in the feature space. The output of the last pooling layer is transformed
    into a 1D tensor through flattening. The flattened 1D tensor then goes into the
    dense layer (not shown in the diagram).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 代码中的顺序模型由[清单 4.1](#ch04ex01)中的代码构建，由`add()`方法调用逐个创建七个层。在我们查看每个层执行的详细操作之前，让我们先看一下模型的整体架构，如[图
    4.2](#ch04fig02)所示。如图所示，模型的前五层包括一组conv2d-maxPooling2d层的重复模式，后跟一个flatten层。conv2d-maxPooling2d层组是特征提取的主力军。每个层将输入图像转换为输出图像。conv2d层通过“卷积核”操作，该核在输入图像的高度和宽度维度上“滑动”。在每个滑动位置，它与输入像素相乘，然后将产品相加并通过非线性传递。这产生输出图像中的像素。maxPooling2d层以类似的方式操作，但没有核。通过将输入图像数据通过连续的卷积和池化层，我们得到越来越小且在特征空间中越来越抽象的张量。最后一个池化层的输出通过展平变成一个1D张量。展平的1D张量然后进入密集层（图中未显示）。
- en: Figure 4.2\. A high-level overview of the architecture of a simple convnet of
    the kind constructed by the code in [listing 4.1](#ch04ex01). In this figure,
    the sizes of the images and intermediate tensors are made smaller than the actual
    sizes in the model defined by [listing 4.1](#ch04ex01) for illustration’s sake.
    So are the sizes of the convolutional kernels. Also note that this diagram shows
    a single channel in each intermediate 4D tensor, whereas the intermediate tensors
    in the actual model have multiple channels.
  id: totrans-46
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.2。简单convnet架构的高级概述，类似于[清单4.1](#ch04ex01)中的代码构建的模型。在此图中，图像和中间张量的大小比实际模型中定义的大小要小，以进行说明。卷积核的大小也是如此。还要注意，该图显示每个中间4D张量中仅有一个通道，而实际模型中的中间张量具有多个通道。
- en: '![](04fig01_alt.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](04fig01_alt.jpg)'
- en: 'You can think of the convnet as an MLP built on top of convolutional and pooling
    preprocessing. The MLP is exactly the same type as what we’ve seen in the Boston-housing
    and phishing-detection problems: it’s simply made of dense layers with nonlinear
    activations. What’s different in the convnet here is that the input to the MLP
    is the output of the cascaded conv2d and maxPooling2d layers. These layers are
    specifically designed for image inputs to extract useful features from them. This
    architecture was discovered through years of research in neural networks: it leads
    to an accuracy significantly better than feeding the pixel values of the images
    directly into an MLP.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将convnet看作是建立在卷积和池化预处理之上的MLP。MLP与我们在波士顿房屋和网络钓鱼问题中看到的完全相同：它仅由具有非线性激活的稠密层构成。在这里，convnet的不同之处在于MLP的输入是级联conv2d和maxPooling2d层的输出。这些层专门设计用于图像输入，以从中提取有用的特征。这种架构是通过多年的神经网络研究发现的：它的准确性明显优于直接将图像的像素值馈入MLP。
- en: With this high-level understanding of the MNIST convnet, let’s now dive deeper
    into the internal workings of the model’s layers.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 有了对MNIST convnet的高层次理解，现在让我们更深入地了解模型层的内部工作。
- en: 4.2.1\. conv2d layer
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1\. conv2d层
- en: The first layer is a conv2d layer, which performs 2D convolution. This is the
    first convolutional layer you see in this book. What does it do? conv2d is an
    image-to-image transform—it transforms a 4D (NHWC) image tensor into another 4D
    image tensor, possibly with a different height, width, and number of channels.
    (It may seem strange that conv2d operates on 4D tensors, but keep in mind that
    there are two extra dimensions, one for batch examples and one for channels.)
    Intuitively, it can be understood as a group of simple “Photoshop filters”^([[6](#ch04fn6)])
    that lead to image effects such as blurring and sharpening. These effects are
    done with 2D convolution, which involves sliding a small patch of pixels (the
    *convolutional kernel*, or simply *kernel*) over the input image. At each sliding
    position, the kernel is multiplied with the small patch of the input image that
    it overlaps with, pixel by pixel. Then the pixel-by-pixel products are summed
    to form pixels in the resulting image.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 第一层是conv2d层，它执行2D卷积。这是本书中看到的第一个卷积层。它的作用是什么？conv2d是一个图像到图像的转换-它将一个4D（NHWC）图像张量转换为另一个4D图像张量，可能具有不同的高度、宽度和通道数量。(conv2d操作4D张量可能看起来有些奇怪，但请记住这里有两个额外的维度，一个是用于批处理示例，一个是用于通道。)
    直观地看，它可以被理解为一组简单的“Photoshop滤镜”^([[6](#ch04fn6)])，它会产生图像效果，如模糊和锐化。这些效果是通过2D卷积实现的，它涉及在输入图像上滑动一个小的像素块（卷积核，或简称核），并且像素与输入图像的小块重叠时，核与输入图像逐像素相乘。然后逐像素的乘积相加形成结果图像的像素。
- en: ⁶
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁶
- en: ''
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'We owe this analogy to Ashi Krishnan’s talk titled “Deep Learning in JS” at
    JSConf EU 2018: [http://mng.bz/VPa0](http://mng.bz/VPa0).'
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们将这种类比归功于Ashi Krishnan在JSConf EU 2018上的名为“JS中的深度学习”的演讲：[http://mng.bz/VPa0](http://mng.bz/VPa0)。
- en: Compared to a dense layer, a conv2d layer has more configuration parameters.
    `kernelSize` and `filters` are two key parameters of the conv2d layer. To understand
    their meaning, we need to describe how 2D convolution works on a conceptual level.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 与稠密层相比，conv2d层具有更多的配置参数。`kernelSize`和`filters`是conv2d层的两个关键参数。为了理解它们的含义，我们需要在概念层面上描述2D卷积是如何工作的。
- en: '[Figure 4.3](#ch04fig03) illustrates 2D convolution in greater detail. Here,
    we suppose the input image (top left) tensor consists of a simple example so that
    we can draw it easily on paper. We suppose the conv2d operation is configured
    as `kernelSize = 3` and `filters = 3`. Due to the fact that the input image has
    two color channels (a somewhat unusual number of channels just for illustration
    purposes), the kernel is a 3D tensor of shape `[3, 3, 2, 3]`. The first two numbers
    (3 and 3) are the height and width of the kernel, determined by `kernelSize`.
    The third dimension (2) is the number of input channels. What is the fourth dimension
    (3)? It is the number of filters, which equals the last dimension of conv2d’s
    output tensor.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4.3](#ch04fig03)更详细地说明了2D卷积。在这里，我们假设输入图像（左上角）张量由一个简单的例子组成，以便我们可以在纸上轻松地绘制它。我们假设conv2d操作配置为`kernelSize
    = 3`和`filters = 3`。由于输入图像具有两个颜色通道（仅仅是为了图示目的而具有的相当不寻常的通道数），卷积核是一个形状为`[3, 3, 2,
    3]`的3D张量。前两个数字（3和3）是由`kernelSize`确定的核的高度和宽度。第三个维度（2）是输入通道的数量。第四个维度（3）是什么？它是滤波器的数量，等于conv2d输出张量的最后一个维度。'
- en: Figure 4.3\. How a conv2D layer works, with an example. For simplicity, it is
    assumed that the input tensor (top left) contains only one image and is therefore
    a 3D tensor. Its dimensions are height, width, and depth (color channels). The
    batch dimension is omitted for simplicity. The depth of the input image tensor
    is set as 2 for simplicity. Note that the height and width of the image (4 and
    5) are much smaller than those of a typical real image. The depth (2) is less
    than the more typical value of 3 or 4 (for example, for RGB or RGBA). Assuming
    that the `filters` property (the number of filters) of the conv2D layer is 3,
    `kernelSize` is `[3, 3]`, and `strides` is `[1, 1]`, the first step in performing
    the 2D convolution is to slide through the height and width dimensions and extract
    small patches of the original image. Each patch has a height of 3 and a width
    of 3, matching the layer’s `filterSize`; it also has the same depth as the original
    image. In the second step, a dot product is calculated between every 3 × 3 × 2
    patch and the convolutional kernel (that is, “filters”). [Figure 4.4](#ch04fig04)
    gives more details on each dot-product operation. The kernel is a 4D tensor and
    consists of three 3D filters. The dot product between the image patch with the
    filter occurs separately for the three filters. The image patch is multiplied
    with the filter pixel-by-pixel, and the products are summed, which leads to a
    pixel in the output tensor. Because there are three filters in the kernel, each
    image patch is converted to a stack of three pixels. This dot-product operation
    is performed over all image patches, and the resulting stacks of three pixels
    are merged as the output tensor, which has a shape of `[2, 3, 3]` in this case.
  id: totrans-57
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.3\. 卷积层的工作原理，并附有一个示例。为简化起见，假设输入张量（左上角）仅包含一幅图像，因此是一个 3D 张量。其维度为高度、宽度和深度（色道）。为简单起见，批次维度被省略。输入图像张量的深度为
    2。注意图像的高度和宽度（4 和 5）远小于典型真实图像的高宽。深度（2）也低于更典型的 3 或 4 的值（例如 RGB 或 RGBA）。假设 conv2D
    层的 `filters` 属性（滤波器数量）为 3，`kernelSize` 为 `[3, 3]`，`strides` 为 `[1, 1]`，进行 2D 卷积的第一步是沿着高度和宽度维度滑动，并提取原始图像的小块。每个小块的高度为
    3，宽度为 3，与层的 `filterSize` 匹配；它的深度与原始图像相同。第二步是计算每个 3 × 3 × 2 小块与卷积核（即“滤波器”）的点积。[图
    4.4](#ch04fig04) 更详细地说明了每个点积操作。卷积核是一个 4D 张量，由三个 3D 滤波器组成。对三个滤波器分别进行图像小块与滤波器之间的点积。图像小块与滤波器逐像素相乘，然后求和，这导致输出张量中的一个像素。由于卷积核中有三个滤波器，每个图像小块被转换为一个三个像素的堆叠。这个点积操作在所有图像小块上执行，产生的三个像素堆叠被合并为输出张量，这种情况下形状为
    `[2, 3, 3]`。
- en: '![](04fig02_alt.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](04fig02_alt.jpg)'
- en: If the output is regarded as an image tensor (a totally valid way of looking
    at this!), then filters can be understood as the number of channels in the output.
    Unlike the input image, the channels in the output tensor don’t actually have
    to do with colors. Instead, they represent different visual features of the input
    image, learned from the training data. For example, some filters may be sensitive
    to straight-line boundaries between bright and dark regions at a certain orientation,
    while others may be sensitive to corners formed by a brown color, and so forth.
    More on that later.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将输出视为图像张量（这是一个完全有效的观察方式！），那么滤波器可以理解为输出中通道的数量。与输入图像不同，输出张量中的通道实际上与颜色无关。相反，它们代表从训练数据中学到的输入图像的不同视觉特征。例如，一些滤波器可能对在特定方向上明亮和暗区域之间的直线边界敏感，而其他滤波器可能对由棕色形成的角落敏感，依此类推。稍后再详细讨论。
- en: The “sliding” action mentioned previously is represented as extracting small
    patches from the input image. Each patch has height and width equal to `kernelSize`
    (3 in this case). Since the input image has a height of 4, there are only two
    possible sliding positions along the height dimension because we need to make
    sure that the 3 × 3 window does not fall outside the bounds of the input image.
    Similarly, the width (5) of the input image gives us three possible sliding positions
    along the width dimension. Hence, we end up with `2 × 3 = 6` image patches extracted.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 先前提到的“滑动”动作表示从输入图像中提取小块。每个小块的高度和宽度都等于 `kernelSize`（在这个例子中为 3）。由于输入图像的高度为 4，沿着高度维度只有两种可能的滑动位置，因为我们需要确保
    3 × 3 窗口不会超出输入图像的边界。同样，输入图像的宽度（5）给出了三个可能的宽度维度滑动位置。因此，我们最终提取出 `2 × 3 = 6` 个图像小块。
- en: At each sliding-window position, a dot-product operation occurs. Recall that
    the convolutional kernel has a shape of `[3, 3, 2, 3]`. We can break up the 4D
    tensor along the last dimension into three separate 3D tensors, each of which
    has a shape of `[3, 3, 2]`, as shown by the hash lines in [figure 4.3](#ch04fig03).
    We take the image patch and one of the 3D tensors, multiply them together pixel-by-pixel,
    and sum all the `3 * 3 * 2 = 18` values to get a pixel in the output tensor. [Figure
    4.4](#ch04fig04) illustrates the dot-product step in greater detail. It is not
    a coincidence that the image patch and the slice of the convolutional kernel have
    exactly the same shape—we extracted the image patches based on the kernel’s shape!
    This multiply-and-add operation is repeated for all three slices of the kernel,
    which gives a set of three numbers. Then this dot-product operation is repeated
    for the remaining image patches, which gives the six columns of three cubes in
    the figure. These columns are finally combined to form the output, which has an
    HWC shape of `[2, 3, 3]`.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个滑动窗口位置，都会进行一次点积操作。回想一下，卷积核的形状为`[3, 3, 2, 3]`。我们可以沿着最后一个维度将4D张量分解为三个单独的3D张量，每个张量的形状为`[3,
    3, 2]`，如[图4.3](#ch04fig03)中的哈希线所示。我们取图像块和三维张量之一，将它们逐像素相乘，并将所有`3 * 3 * 2 = 18`个值求和以获得输出张量中的一个像素。[图4.4](#ch04fig04)详细说明了点积步骤。图像块和卷积核切片具有完全相同的形状并非巧合——我们基于内核的形状提取了图像块！这个乘加操作对所有三个内核切片重复进行，得到一组三个数字。然后，该点积操作对其余的图像块重复进行，得到图中六列三个立方体。这些列最终被组合成输出，其HWC形状为`[2,
    3, 3]`。
- en: Figure 4.4\. An illustration of the dot-product (that is, multiply-and-add)
    operation in the 2D convolution operation, a step in the full workflow outlined
    in [figure 4.3](#ch04fig03). For the sake of illustration, it is assumed that
    the image patch (x) contains only one color channel. The image patch has a shape
    of `[3, 3, 1]`, that is, the same as the size of the convolutional kernel slice
    (K). The first step is element-by-element multiplication, which yields another
    `[3, 3, 1]` tensor. The elements of the new tensor are added together (represented
    by the σ), and the sum is the result.
  id: totrans-62
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.4\. 在2D卷积操作中的点积（即乘加）操作的示意图，这是[图4.3](#ch04fig03)中概述的完整工作流程中的一步。为了说明方便，假设图像块（x）只包含一个颜色通道。图像块的形状为`[3,
    3, 1]`，与卷积核切片（K）的大小相同。第一步是逐元素相乘，产生另一个`[3, 3, 1]`张量。新张量的元素被加在一起（由σ表示），和即为结果。
- en: '![](04fig03_alt.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](04fig03_alt.jpg)'
- en: 'Like a dense layer, a conv2d layer has a bias term, which is added to the result
    of the convolution. Also, a conv2d layer is usually configured to have a nonlinear
    activation function. In this example, we use relu. Recall that in the [chapter
    3](kindle_split_014.html#ch03) section “[Avoiding the fallacy of stacking layers
    without nonlinearity](kindle_split_014.html#ch03lev3sec2),” we warned that stacking
    two dense layers without nonlinearity is mathematically equivalent to using a
    single dense layer. A similar cautionary note applies to conv2d layers: stacking
    two such layers without a nonlinear activation is mathematically equivalent to
    using a single conv2d layer with a larger kernel and is hence an inefficient way
    of constructing a convnet that should be avoided.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 像密集层一样，conv2d层有一个偏置项，它被加到卷积结果中。此外，conv2d层通常配置为具有非线性激活函数。在这个例子中，我们使用relu。回想一下，在[第3章](kindle_split_014.html#ch03)的“[避免堆叠层而不使用非线性函数的谬论](kindle_split_014.html#ch03lev3sec2)”一节中，我们警告说堆叠两个没有非线性的密集层在数学上等价于使用单个密集层。类似的警告也适用于conv2d层：堆叠两个这样的层而没有非线性激活在数学上等价于使用一个具有更大内核的单个conv2d层，因此这是一种应该避免的构建卷积网络的低效方式。
- en: Whew! That’s it for the details of how conv2d layers work. Let’s take a step
    back and look at what conv2d actually achieves. In a nutshell, it is a special
    way to transform an input image into an output image. The output image will usually
    have smaller height and width compared to the input. The reduction in size is
    dependent on the `kernelSize` configuration. The output image may have fewer,
    more, or the same channels than the input, which is determined by the `filters`
    configuration.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！关于conv2d层如何工作的细节就是这些。让我们退后一步，看看conv2d实际上实现了什么。简而言之，这是一种将输入图像转换为输出图像的特殊方式。输出图像通常比输入图像具有较小的高度和宽度。尺寸的减小取决于`kernelSize`配置。输出图像可能具有比输入更少、更多或相同的通道数，这取决于`filters`配置。
- en: 'So conv2d is an image-to-image transformation. Two key features of the conv2d
    transformation are locality and parameter sharing:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: conv2d 是一种图像到图像的转换。conv2d 转换的两个关键特性是局部性和参数共享：
- en: '*Locality* refers to the property that the value of a given pixel in the output
    image is affected by only a small patch of the input image, instead of all the
    pixels in the input image. The size of that patch is `kernelSize`. This is what
    makes conv2d distinct from dense layers: in a dense layer, every output element
    is affected by every input element. In other words, the input elements and output
    elements are “densely connected” in a dense layer (hence its name). So, we can
    say that a conv2d layer is “sparsely connected.” While dense layers learn global
    patterns in the input, convolutional layers learn local patterns—patterns within
    the small window of the kernel.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*局部性* 指的是输出图像中给定像素的值仅受到输入图像中一个小区域的影响，而不是受到输入图像中所有像素的影响。该区域的大小为`kernelSize`。这就是conv2d与密集层的不同之处：在密集层中，每个输出元素都受到每个输入元素的影响。换句话说，在密集层中，输入元素和输出元素在“密集连接”（因此称为密集层）；而conv2d层是“稀疏连接”的。虽然密集层学习输入中的全局模式，但卷积层学习局部模式——卷积核的小窗口内的模式。'
- en: '*Parameter sharing* refers to the property that the way in which output pixel
    A is affected by its small input patch is exactly the same as the way in which
    output pixel B is affected by its input patch. This is because the dot product
    at every sliding position uses the same convolutional kernel ([figure 4.3](#ch04fig03)).'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*参数共享* 指的是输出像素A受其小输入区域影响的方式与输出像素B受其输入区域影响的方式完全相同。这是因为每个滑动位置的点积都使用相同的卷积核（[图4.3](#ch04fig03)）。'
- en: Due to locality and parameter sharing, a conv2d layer is a highly efficient,
    image-to-image transform in terms of the number of parameters required. In particular,
    the size of the convolutional kernel does not change with the height or width
    of the input image. Coming back to the first conv2d layer in [listing 4.1](#ch04ex01),
    the kernel has a shape of `[kernelSize, kernelSize, 1, filter]` (that is, `[5,
    5, 1, 8]`), and therefore a total of 5 * 5 * 1 * 8 = 200 parameters, regardless
    of whether the input MNIST images are 28 × 28 or much larger. The output of the
    first conv2d layer has a shape of `[24, 24, 8]` (omitting the batch dimension).
    So, the conv2d layer transforms a tensor consisting of 28 * 28 * 1 = 784 elements
    into another tensor of 24 * 24 * 8 = 4,608 elements. If we were to implement this
    transform with a dense layer, how many parameters will be involved? The answer
    is 784 * 4,608 = 3,612,672 (not including the bias), which is about 18 thousand
    times more than the conv2d layer! This thought experiment shows the efficiency
    of convolutional layers.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 由于局部性和参数共享，conv2d 层在所需参数数量方面是高效的图像到图像变换。特别地，卷积核的大小不随输入图像的高度或宽度而变化。回到[列表4.1](#ch04ex01)中的第一个conv2d层，核的形状为`[kernelSize,
    kernelSize, 1, filter]`（即`[5, 5, 1, 8]`），因此总共有5 * 5 * 1 * 8 = 200个参数，不管输入的MNIST图像是28
    × 28还是更大。第一个conv2d层的输出形状为`[24, 24, 8]`（省略批次维度）。所以，conv2d层将由28 * 28 * 1 = 784个元素组成的张量转换为由24
    * 24 * 8 = 4,608个元素组成的另一个张量。如果我们要用密集层来实现这个转换，将涉及多少参数？答案是784 * 4,608 = 3,612,672（不包括偏差），这约是conv2d层的18千倍！这个思想实验展示了卷积层的效率。
- en: 'The beauty of conv2d’s locality and parameter sharing is not only in its efficiency,
    but also in the fact that it mimics (in a loose fashion) how biological visual
    systems work. Consider neurons in the retina. Each neuron is affected by only
    a small patch in the eye’s field of view, called the *receptive field*. Two neurons
    located at different locations of the retina respond to light patterns in their
    respective receptive fields in pretty much the same way, which is analogous to
    the parameter sharing in a conv2d layer. What’s more, conv2d layers prove to work
    well for computer-vision problems, as we will soon appreciate in this MNIST example.
    conv2d is a neat neural network layer that has it all: efficiency, accuracy, and
    relevance to biology. No wonder it is so widely used in deep learning.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: conv2d 的局部性和参数共享之美不仅在于其效率，还在于它（以松散的方式）模拟了生物视觉系统的工作方式。考虑视网膜上的神经元。每个神经元只受到眼睛视野中的一个小区域的影响，称为*感受野*。位于视网膜不同位置的两个神经元对其各自感受野中的光模式的响应方式几乎相同，这类似于
    conv2d 层中的参数共享。更重要的是，conv2d 层在计算机视觉问题中表现良好，正如我们将在这个 MNIST 示例中很快看到的那样。conv2d 是一个很棒的神经网络层，它集效率、准确性和与生物学相关性于一身。难怪它在深度学习中被如此广泛地使用。
- en: 4.2.2\. maxPooling2d layer
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2\. maxPooling2d 层
- en: Having examined the conv2d layer, let’s look at the next layer in the sequential
    model—a maxPooling2d layer. Like conv2d, maxPooling2d is a kind of image-to-image
    transform. But the maxPooling2d transform is simpler compared to conv2d. As [figure
    4.5](#ch04fig05) shows, it simply calculates the maximum pixel values in small
    image patches and uses them as the pixel values in the output. The code that defines
    and adds the maxPooling2d layer is
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在研究了 conv2d 层之后，让我们看一下顺序模型中的下一层——maxPooling2d 层。像 conv2d 一样，maxPooling2d 是一种图像到图像的转换。但与
    conv2d 相比，maxPooling2d 转换更简单。正如[图4.5](#ch04fig05)所示，它简单地计算小图像块中的最大像素值，并将它们用作输出中的像素值。定义并添加
    maxPooling2d 层的代码为
- en: '[PRE2]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Figure 4.5\. An example of how a maxPooling2D layer works. This example uses
    a tiny 4 × 4 image and assumes that the maxPooling2D layer is configured to have
    a `poolSize` of `[2, 2]` and `strides` of `[2, 2]`. The depth dimension is not
    shown, but the max-pooling operation occurs independently over the dimensions.
  id: totrans-74
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.5\. maxPooling2D 层的工作原理示例。此示例使用一个小的 4 × 4 图像，并假设 maxPooling2D 层配置为`poolSize`为`[2,
    2]`和`strides`为`[2, 2]`。深度维度未显示，但 max-pooling 操作独立地在各维度上进行。
- en: '![](04fig04.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](04fig04.jpg)'
- en: 'In this particular case, the image patches have a height and width of 2 × 2
    because of the specified `poolSize` value of `[2, 2]`. The patches are extracted
    every two pixels, along both dimensions. This spacing between patches results
    from the `strides` value we use here: `[2, 2]`. As a result, the output image,
    with an HWC shape of `[12, 12, 8]`, is half the height and half the width of the
    input image (shape `[24, 24, 8]`) but has the same number of channels.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种特定情况下，由于指定的`poolSize`值为`[2, 2]`，图像块的高度和宽度为2 × 2。沿着两个维度，每隔两个像素提取图像块。这些图像块之间的间隔是由我们在此处使用的`strides`值决定的：`[2,
    2]`。因此，输出图像的 HWC 形状为`[12, 12, 8]`，高度和宽度是输入图像（形状为`[24, 24, 8]`）的一半，但具有相同数量的通道。
- en: A maxPooling2d layer serves two main purposes in a convnet. First, it makes
    the convnet less sensitive to the exact location of key features in the input
    image. For example, we want to be able to recognize the digit “8” regardless of
    whether it has shifted to the left or the right from the center in the 28 × 28
    input image (or shifted up or down, for that matter), a property called *positional
    invariance*. To understand how the maxPooling2d layer enhances positional invariance,
    realize the fact that within each image patch that maxPooling2d operates on, it
    doesn’t matter where the brightest pixel is, as long as it falls into that patch.
    Admittedly, a single maxPooling2d layer can do only so much in making the convnet
    insensitive to shifts because its pooling window is limited. However, when multiple
    maxPooling2d layers are used in the same convnet, they work together to achieve
    considerably greater positional invariance. This is exactly what is done in our
    MNIST model—as well as in virtually all practical convnets—which contains two
    maxPooling2d layers.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: maxPooling2d层在卷积网络中有两个主要目的。首先，它使卷积网络对输入图像中关键特征的确切位置不那么敏感。例如，我们希望能够识别出数字“8”，无论它是否从28
    × 28输入图像的中心向左或向右移动（或者从上到下移动），这种特性称为*位置不变性*。要理解maxPooling2d层如何增强位置不变性，需要意识到maxPooling2d在操作的每个图像块内部，最亮的像素位于何处并不重要，只要它落入该图像块内即可。诚然，单个maxPooling2d层在使卷积网络不受位移影响方面能做的事情有限，因为它的池化窗口是有限的。然而，当在同一个卷积网络中使用多个maxPooling2d层时，它们共同努力实现了更大程度的位置不变性。这正是我们MNIST模型中所做的事情——以及几乎所有实际卷积网络中所做的事情——其中包含两个maxPooling2d层。
- en: 'As a thought experiment, consider what happens when two conv2d layers (called
    conv2d_1 and conv2d_2) are stacked directly on top of each other without an intermediate
    maxPooling2d layer. Suppose each of the two conv2d layers has a `kernelSize` of
    3; then each pixel in conv2d_2’s output tensor is a function of a 5 × 5 region
    in the original input to conv2d_1\. We can say that each “neuron” of the conv2d_2
    layer has a receptive field of size 5 × 5\. What happens when there is an intervening
    maxPooling2d layer between the two conv2d layers (as is the case in our MNIST
    convnet)? The receptive field of conv2d_2’s neurons becomes larger: 11 × 11\.
    This is due to the pooling operation, of course. When multiple maxPooling2d layers
    are present in a convnet, layers at higher levels can have broad receptive fields
    and positional invariance. In short, they can see wider!'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个思想实验，考虑当两个conv2d层（称为conv2d_1和conv2d_2）直接叠加在一起而没有中间的maxPooling2d层时会发生什么。假设这两个conv2d层的`kernelSize`都为3；那么conv2d_2输出张量中的每个像素都是原始输入到conv2d_1的5
    × 5区域的函数。我们可以说conv2d_2层的每个“神经元”具有5 × 5的感受野。当两个conv2d层之间存在一个maxPooling2d层时会发生什么（就像我们的MNIST卷积网络中的情况一样）？conv2d_2层的神经元的感受野变得更大：11
    × 11。当卷积网络中存在多个maxPooling2d层时，较高层次的层可以具有广泛的感受野和位置不变性。简而言之，它们可以看得更广！
- en: Second, a maxPooling2d layer also shrinks the size of the height and width dimensions
    of the input tensor, significantly reducing the amount of compute required in
    subsequent layers and in the entire convnet overall. For example, the output from
    the first conv2d layer has an output tensor of shape `[26, 26, 16]`. After passing
    through the maxPooling2d layer, the tensor shape becomes `[13, 13, 16]`, which
    reduces the number of tensor elements by a factor of 4\. The convnet contains
    another maxPooling2d layer, which further shrinks the size of the weights in subsequent
    layers and the number of elementwise mathematical operations in those layers.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，一个maxPooling2d层也会使输入张量的高度和宽度尺寸缩小，大大减少了后续层次和整个卷积网络中所需的计算量。例如，第一个conv2d层的输出张量形状为`[26,
    26, 16]`。经过maxPooling2d层后，张量形状变为`[13, 13, 16]`，将张量元素数量减少了4倍。卷积网络包含另一个maxPooling2d层，进一步缩小了后续层次的权重尺寸和这些层次中的逐元素数学运算的数量。
- en: 4.2.3\. Repeating motifs of convolution and pooling
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3\. 卷积和池化的重复模式
- en: 'Having examined the first maxPooling2d layer, let’s focus our attention on
    the next two layers of the convnet, defined by these lines in [listing 4.1](#ch04ex01):'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在审查了第一个maxPooling2d层后，让我们将注意力集中在卷积网络的接下来的两层上，这两层由[list 4.1](#ch04ex01)中的这些行定义：
- en: '[PRE3]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'These two layers are an exact repeat of the previous two layers (except that
    the conv2d layer has a larger value in its `filters` configuration and does not
    possess an `inputShape` field). This type of an almost-repeating “motif” consisting
    of a convolutional layer and a pooling layer is seen frequently in convnets. It
    performs a critical role: hierarchical extraction of features. To understand what
    it means, consider a convnet trained for the task of classifying animals in images.
    At early stages of the convnet, the filters (that is, channels) in a convolutional
    layer may encode low-level geometric features, such as straight lines, curved
    lines, and corners. These low-level features are transformed into more complex
    features, such as a cat’s eye, nose, and ear (see [figure 4.6](#ch04fig06)). At
    the top level of the convnet, a layer may have filters that encode the presence
    of a whole cat. The higher the level, the more abstract the representation and
    the more removed from the pixel-level values the features are. But those abstract
    features are exactly what is required to achieve good accuracy on the convnet’s
    task—for instance, detecting a cat when it is present in the image. Moreover,
    these features are not handcrafted but are instead extracted from the data in
    an automatic fashion through supervised learning. This is a quintessential example
    of the kind of layer-by-layer representational transformation that we described
    as the essence of deep learning in [chapter 1](kindle_split_011.html#ch01).'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个层与前面的两个层完全相同（除了 conv2d 层的`filters`配置有一个更大的值并且没有`inputShape`字段）。这种几乎重复的“基本图案”由一个卷积层和一个池化层组成，在
    convnets 中经常见到。它发挥了关键作用：分层特征提取。要理解这意味着什么，考虑一个用于图像中动物分类任务的 convnet。在 convnet 的早期阶段，卷积层中的滤波器（即通道）可能编码低级几何特征，如直线、曲线和角落。这些低级特征被转换成更复杂的特征，如猫的眼睛、鼻子和耳朵（见
    [图 4.6](#ch04fig06)）。在 convnet 的顶层，一层可能具有编码整个猫的滤波器。层级越高，表示越抽象，与像素级值越远。但这些抽象特征正是在
    convnet 任务上取得良好准确率所需要的特征，例如在图像中存在猫时检测出猫。此外，这些特征不是手工制作的，而是通过监督学习以自动方式从数据中提取的。这是我们在
    [第一章](kindle_split_011.html#ch01) 中描述的深度学习的本质，即逐层表示变换的典型示例。
- en: Figure 4.6\. Hierarchical extraction of features from an input image by a convnet,
    using a cat image as an example. Note that in this example, the input to the neural
    network is at the bottom, and the output is at the top.
  id: totrans-84
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.6\. 通过 convnet 从输入图像中分层提取特征，以猫图像为例。请注意，在此示例中，神经网络的输入在底部，输出在顶部。
- en: '![](04fig05_alt.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](04fig05_alt.jpg)'
- en: 4.2.4\. Flatten and dense layers
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.4\. 压扁和稠密层
- en: After the input tensor has passed through the two groups of conv2d-maxPooling2d
    transformations, it becomes a tensor of the HWC shape `[4, 4, 16]` (without the
    batch dimension). The next layer in the convnet is a flatten layer. This layer
    forms a bridge between the previous conv2d-maxPooling2d layers and the following
    layers of the sequential model.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在输入张量通过两组 conv2d-maxPooling2d 变换后，它变成了一个形状为`[4, 4, 16]`的 HWC 形状的张量（不包括批次维度）。在
    convnet 中的下一层是一个压扁层。这一层在前面的 conv2d-maxPooling2d 层和顺序模型的后续层之间形成了一个桥梁。
- en: 'The code for the flatten layer is simple, as the constructor doesn’t require
    any configuration parameters:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Flatten 层的代码很简单，因为构造函数不需要任何配置参数：
- en: '[PRE4]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: A flatten layer “squashes” a multidimensional tensor into a 1D tensor, preserving
    the total number of elements. In our case, the 3D tensor of shape `[3, 3, 32]`
    is flattened into a 1D tensor `[288]` (without the batch dimension). An obvious
    question for the squashing operation is how to order the elements, because there
    is no intrinsic order in the original 3D space. The answer is, we order the elements
    such that if you go down the elements in the flattened 1D tensor and look at how
    their original indices (from the 3D tensor) change, the last index changes the
    fastest, the second-to-last index changes the second fastest, and so forth, while
    the first index changes the slowest. This is illustrated in [figure 4.7](#ch04fig07).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Flatten 层将多维张量“压缩”成一个一维张量，保留元素的总数。在我们的例子中，形状为`[3, 3, 32]`的三维张量被压扁成一个一维张量`[288]`（不包括批次维度）。对于压扁操作一个明显的问题是如何排序元素，因为原始的三维空间没有固有的顺序。答案是，我们按照这样的顺序排列元素：当你在压扁后的一维张量中向下移动并观察它们的原始索引（来自三维张量）如何变化时，最后一个索引变化得最快，倒数第二个索引变化得次快，依此类推，而第一个索引变化得最慢。这在
    [图 4.7](#ch04fig07) 中有所说明。
- en: Figure 4.7\. How a flatten layer works. A 3D tensor input is assumed. For the
    sake of simplicity, we let each dimension have a small size of 2\. The indices
    of the elements are shown on the “faces” of the cubes that represent the elements.
    The flatten layer transforms the 3D tensor into a 1D tensor while preserving the
    total number of elements. The ordering of the elements in the flattened 1D tensor
    is such that when you go down the elements of the output 1D tensor and examine
    their original indices in the input tensor, the last dimension is the one that
    changes the fastest.
  id: totrans-91
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.7\.flatten层的工作原理。假设输入是一个3D张量。为了简单起见，我们让每个维度的大小都设为2。方块表示元素，元素的索引显示在方块的“面”上。flatten层将3D张量转换成1D张量，同时保持元素的总数不变。在展平的1D张量中，元素的顺序是这样安排的：当您沿着输出1D张量的元素向下走，并检查它们在输入张量中的原始索引时，最后一个维度变化得最快。
- en: '![](04fig06_alt.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](04fig06_alt.jpg)'
- en: What purpose does the flatten layer serve in our convnet? It sets the stage
    for the subsequent dense layers. As we learned in [chapters 2](kindle_split_013.html#ch02)
    and [3](kindle_split_014.html#ch03), a dense layer usually takes a 1D tensor (excluding
    the batch dimension) as its input due to how a dense layer works ([section 2.1.4](kindle_split_013.html#ch02lev2sec4)).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: flatten层在我们的卷积网络中起到什么作用呢？它为随后的密集层做好了准备。就像我们在[第2章](kindle_split_013.html#ch02)和[第3章](kindle_split_014.html#ch03)学到的那样，由于密集层的工作原理（[第2.1.4节](kindle_split_013.html#ch02lev2sec4)），它通常需要一个1D张量（不包括批次维度）作为其输入。
- en: 'The next two lines of the code in [listing 4.1](#ch04ex01) add two dense layers
    to the convnet:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 代码中的下两行，将两个密集层添加到卷积网络中。
- en: '[PRE5]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Why two dense layers and not just one? The same reason as in the Boston-housing
    example and phishing-URL-detection example we saw in [chapter 3](kindle_split_014.html#ch03):
    adding layers with nonlinear activation increases the network’s capacity. In fact,
    you can think of the convnet as consisting of two models stacked on top of each
    other:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么要使用两个密集层而不只是一个？与波士顿房产示例和[第3章](kindle_split_014.html#ch03)中的网络钓鱼URL检测示例的原因相同：添加具有非线性激活的层增加了网络的容量。实际上，您可以将卷积网络视为在此之上堆叠了两个模型：
- en: A model that contains conv2d, maxPooling2d, and flatten layers, which extracts
    visual features from the input images
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含conv2d、maxPooling2d和flatten层的模型，从输入图像中提取视觉特征
- en: An MLP with two dense layers that uses the extracted features to make digit-class
    predictions—this is essentially what the two dense layers are for
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有两个密集层的多层感知器（MLP），使用提取的特征进行数字分类预测——这本质上就是两个密集层的用途。
- en: In deep learning, many models show this pattern of feature-extraction layers
    followed by MLPs for final predictions. We will see more examples like this throughout
    the rest of this book, in models ranging from audio-signal classifiers to natural
    language processing.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，许多模型都显示了这种特征提取层后面跟着MLPs进行最终预测的模式。在本书的其余部分中，我们将看到更多类似的示例，从音频信号分类器到自然语言处理的模型都会有。
- en: 4.2.5\. Training the convnet
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.5\. 训练卷积网络
- en: Now that we’ve successfully defined the topology of the convnet, the next step
    is to train it and evaluate the result of the training. This is what the code
    in the next listing is for.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经成功定义了卷积网络的拓扑结构，下一步是训练并评估训练结果。下一个清单中的代码就是用来实现这个目的的。
- en: Listing 4.2\. Training and evaluating the MNIST convnet
  id: totrans-102
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单4.2\. 训练和评估MNIST卷积网络
- en: '[PRE6]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '***1*** Uses callbacks to plot accuracy and loss during training'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 使用回调函数来绘制训练期间的准确率和损失图'
- en: '***2*** Evaluates the model’s accuracy using data the model hasn’t seen'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 使用模型没有看见的数据来评估模型的准确性'
- en: 'Much of the code here is about updating the UI as the training progresses,
    for instance, to plot how the loss and accuracy values change. This is useful
    for monitoring the training process but not strictly essential for model training.
    Let’s highlight the parts essential for training:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 很多这里的代码都是关于在训练过程中更新用户界面的，例如绘制损失和准确率值的变化。这对于监控训练过程很有用，但对于模型训练来说并不是必需的。让我们来强调一下训练所必需的部分：
- en: '`trainData.xs` (the first argument to `model.fit()`) contains the input MNIST
    images represented as a tensor of NHWC shape `[N, 28, 28, 1]`'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`trainData.xs`（`model.fit()`的第一个参数）包含作为形状为`[N, 28, 28, 1]`的NHWC张量的输入MNIST图像表示'
- en: '`trainData.labels` (the second argument to `model.fit()`). This includes the
    input labels, represented as a one-hot encoded 2D tensor of shape `[N, 10]`.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`trainData.labels`（`model.fit()`的第二个参数）。这包括作为形状为`[N, '
- en: The loss function used in the `model.compile()` call, `categoricalCrossentropy`,
    which is appropriate for multiclass-classification problems like MNIST. Recall
    that we used the same loss function for the iris-flower-classification problem
    in [chapter 3](kindle_split_014.html#ch03).
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 `model.compile()` 调用中使用的损失函数 `categoricalCrossentropy`，适用于诸如 MNIST 的多类分类问题。回想一下，我们在[第
    3 章](kindle_split_014.html#ch03)中也使用了相同的损失函数来解决鸢尾花分类问题。
- en: 'The metric function specified in the `model.compile()` call: `''accuracy''`.
    This function measures what fraction of the examples are classified correctly,
    given that the prediction is made based on the largest element among the 10 elements
    of the convnet’s output. Again, this is exactly the same metric we used for the
    newswire problem. Recall the difference between the cross-entropy loss and the
    accuracy metric: cross-entropy is differentiable and hence makes backpropagation-based
    training possible, whereas the accuracy metric is not differentiable but is more
    easily interpretable.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 `model.compile()` 调用中指定的度量函数：`'accuracy'`。该函数衡量了多大比例的示例被正确分类，假设预测是基于卷积神经网络输出的
    10 个元素中最大的元素。再次强调，这与我们在新闻线问题中使用的度量标准完全相同。回想一下交叉熵损失和准确度度量之间的区别：交叉熵可微分，因此可以进行基于反向传播的训练，而准确度度量不可微分，但更容易解释。
- en: The `batchSize` parameter specified for the `model.fit()` call. In general,
    the benefit of using larger batch sizes is that it produces a more consistent
    and less variable gradient update to the model’s weights than a smaller batch
    size. But the larger the batch size, the more memory is required during training.
    You should also keep in mind that given the same amount of training data, a larger
    batch size leads to a small number of gradient updates per epoch. So, if you use
    a larger batch size, be sure to increase the number of epochs accordingly so you
    don’t inadvertently decrease the number of weight updates during training. Thus,
    there is a trade-off. Here, we use a relatively small batch size of 64 because
    we need to make sure that this example works on a wide range of hardware. Like
    other parameters, you can modify the source code and refresh the page so as to
    experiment with the effect of using different batch sizes.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 `model.fit()` 调用中指定的 `batchSize` 参数。一般来说，使用较大的批量大小的好处是，它会产生对模型权重的更一致和 less
    变化的梯度更新，而不是较小的批量大小。但是，批量大小越大，训练过程中所需的内存就越多。您还应该记住，给定相同数量的训练数据，较大的批量大小会导致每个 epoch
    中的梯度更新数量减少。因此，如果使用较大的批量大小，请务必相应地增加 epoch 的数量，以免在训练过程中意外减少权重更新的数量。因此，存在一种权衡。在这里，我们使用相对较小的批量大小
    64，因为我们需要确保这个示例适用于各种硬件。与其他参数一样，您可以修改源代码并刷新页面，以便尝试使用不同批量大小的效果。
- en: The `validationSplit` used in the `model.fit()` call. This lets the training
    process leave out the last 15% of `trainData.xs` and `trainData.labels` for validation
    during training. As you learned with the previous nonimage models, monitoring
    validation loss and accuracy is important during training. It gives you an idea
    of whether and when the model is *overfitting*. What is overfitting? Put simply,
    it is a state in which the model pays too much attention to the fine details of
    the data it has seen during training—so much so that its prediction accuracy on
    data not seen during training is negatively affected. It is a critical concept
    in supervised machine learning. Later in the book ([chapter 8](kindle_split_020.html#ch08)),
    we will devote an entire chapter to how to spot and counteract overfitting.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 `model.fit()` 调用中使用的 `validationSplit`。这使得训练过程中排除了 `trainData.xs` 和 `trainData.labels`
    的最后 15% 以供验证。就像你在之前的非图像模型中学到的那样，监控验证损失和准确度在训练过程中非常重要。它让你了解模型是否过拟合。什么是过拟合？简单地说，这是指模型过于关注训练过程中看到的数据的细节，以至于其在训练过程中没有看到的数据上的预测准确性受到负面影响。这是监督式机器学习中的一个关键概念。在本书的后面章节（[第
    8 章](kindle_split_020.html#ch08)）中，我们将专门讨论如何发现和对抗过拟合。
- en: '`model.fit()` is an async function, so we need to use `await` on it if subsequent
    actions depend on the completion of the `fit()` call. This is exactly what’s done
    here, as we need to perform an evaluation on the model using a test dataset after
    the model is trained. The evaluation is performed using the `model.evaluate()`
    method, which is synchronous. The data fed to `model.evaluate()` is `testData`,
    which has the same format as the `trainData` mentioned earlier, but has a smaller
    number of examples. These examples were never seen by the model during the `fit()`
    call, ensuring that the test dataset does not leak into the evaluation result
    and that the result of the evaluation is an objective assessment of the model’s
    quality.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`model.fit()` 是一个异步函数，因此如果后续操作依赖于 `fit()` 调用的完成，则需要在其上使用 `await`。这正是这里所做的，因为我们需要在模型训练完成后使用测试数据集对模型进行评估。评估是使用
    `model.evaluate()` 方法进行的，该方法是同步的。传递给 `model.evaluate()` 的数据是 `testData`，其格式与前面提到的
    `trainData` 相同，但包含较少数量的示例。这些示例在 `fit()` 调用期间模型从未见过，确保测试数据集不会影响评估结果，并且评估结果是对模型质量的客观评估。'
- en: With this code, we let the model train 10 epochs (specified in the input box),
    which gives us the loss and accuracy curves in [figure 4.8](#ch04fig08). As shown
    by the plots, the loss converges toward the end of the training epochs, and so
    does the accuracy. The validation loss and accuracy values do not deviate from
    their training counterparts too much, which indicates that there is no significant
    overfitting in this case. The final `model.evaluate()` call gives an accuracy
    in the neighborhood of 99.0% (the actual value you get will vary slightly from
    run to run, owing to the random initialization of weights and the implicit random
    shuffling of examples during training).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这段代码，我们让模型训练了 10 个 epoch（在输入框中指定），这给我们提供了 [figure 4.8](#ch04fig08) 中的损失和准确度曲线。如图所示，损失在训练
    epochs 结束时收敛，准确度也是如此。验证集的损失和准确度值与其训练集对应值相差不大，这表明在这种情况下没有明显的过拟合。最终的 `model.evaluate()`
    调用给出了约为 99.0% 的准确度（实际值会因权重的随机初始化和训练过程中示例的随机洗牌而略有变化）。
- en: 'Figure 4.8\. The MNIST convnet’s training curves. Ten epochs of training are
    performed, with each epoch consisting of approximately 800 batches. Left: loss
    value. Right: accuracy value. The values from the training and validation sets
    are shown by the different colors, line widths, and marker symbols. The validation
    curves contain fewer data points than the training ones because, unlike the training
    batches, validation is performed only at the end of every epoch.'
  id: totrans-115
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.8\. MNIST 卷积神经网络的训练曲线。进行了十个 epoch 的训练，每个 epoch 大约包含 800 个批次。左图：损失值。右图：准确度值。训练集和验证集的数值由不同颜色、线宽和标记符号表示。验证曲线的数据点比训练数据少，因为验证是在每个
    epoch 结束时进行，而不像训练批次那样频繁。
- en: '![](04fig07_alt.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](04fig07_alt.jpg)'
- en: How good is 99.0%? It is passable from a practical point of view, but it is
    certainly not the state of the art. With more convolutional layers, it is possible
    to achieve an accuracy reaching 99.5% by increasing the number of convolutional
    and pooling layers and the number of filters in the model. However, training those
    larger convnets take significantly longer in the browser—so long that it makes
    sense to do the training in a less resource-constrained environment like Node.js.
    We will show you exactly how to do that in [section 4.3](#ch04lev1sec3).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 99.0% 的准确度如何？从实际角度来看，这是可以接受的，但肯定不是最先进的水平。通过增加卷积层和池化层以及模型中的滤波器数量，可以达到准确率达到 99.5%
    的可能性。然而，在浏览器中训练这些更大的卷积神经网络需要更长的时间，以至于最好在像 Node.js 这样资源不受限制的环境中进行训练。我们将在 [section
    4.3](#ch04lev1sec3) 中准确地介绍如何做到这一点。
- en: From a theoretical point of view, remember MNIST is a 10-way classification
    problem. So, the chance-level (pure guessing) accuracy is 10%; 99.0% is way better
    than that. But chance level is not a very high bar. How do we show the value of
    the conv2d and maxPooling2d layers in the model? Would we have done as well if
    we stuck with the good old dense layers?
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 从理论角度来看，记住 MNIST 是一个 10 类别分类问题。因此，偶然猜测的准确率是 10%；而 99.0% 远远超过了这个水平。但偶然猜测并不是一个很高的标准。我们如何展示模型中的
    conv2d 和 maxPooling2d 层的价值？如果我们坚持使用传统的全连接层，我们会做得像这样吗？
- en: To answer these questions, we can do an experiment. The code in index.js contains
    another function for model creation called `createDenseModel()`. Unlike the `createConvModel()`
    function we saw in [listing 4.1](#ch04ex01), `createDenseModel()` creates a sequential
    model made of only flatten and dense layers, that is, without using the new layer
    types we learned in this chapter. `createDenseModel()` makes sure that the total
    number of parameters is approximately equal between the dense model it creates
    and the convnet we just trained—approximately 33,000, so it will be a fairer comparison.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 要回答这些问题，我们可以进行一个实验。index.js 中的代码包含了另一个用于模型创建的函数，名为 `createDenseModel()`。与我们在
    [列表 4.1](#ch04ex01) 中看到的 `createConvModel()` 函数不同，`createDenseModel()` 创建的是仅由展平和密集层组成的顺序模型，即不使用本章学习的新层类型。`createDenseModel()`
    确保它所创建的密集模型和我们刚刚训练的卷积网络之间的总参数数量大约相等 —— 约为 33,000，因此这将是一个更公平的比较。
- en: Listing 4.3\. A flatten-and-dense-only model for MNIST, for comparison with
    convnet
  id: totrans-120
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.3\. 用于与卷积网络进行比较的 MNIST 的展平-仅密集模型
- en: '[PRE7]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The summary of the model defined in [listing 4.3](#ch04ex03) is as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 4.3](#ch04ex03) 中定义的模型概述如下：'
- en: '[PRE8]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Using the same training configuration, we obtain training results as shown
    in [figure 4.9](#ch04fig09) from the nonconvolutional model. The final evaluation
    accuracy we get after 10 training epochs is about 97.0%. The difference of two
    percentage points may seem small, but in terms of error rate, the nonconvolutional
    model is three times worse than the convnet. As a hands-on exercise, try increasing
    the size of the nonconvolutional model by increasing the `units` parameter of
    the hidden (first) dense layer in the `createDenseModel()` function. You will
    see that even with greater sizes, it is impossible for the dense-only model to
    achieve an accuracy on par with the convnet. This shows you the power of a convnet:
    through parameter sharing and exploiting the locality of visual features, convnets
    can achieve superior accuracy on computer-vision tasks with an equal or fewer
    number of parameters than nonconvolutional neural networks.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 使用相同的训练配置，我们从非卷积模型中获得的训练结果如 [图 4.9](#ch04fig09) 所示。经过 10 次训练周期后，我们获得的最终评估准确率约为
    97.0%。两个百分点的差异可能看起来很小，但就误差率而言，非卷积模型比卷积网络差三倍。作为一项动手练习，尝试通过增加 `createDenseModel()`
    函数中的隐藏（第一个）密集层的 `units` 参数来增加非卷积模型的大小。你会发现，即使增加了更大的尺寸，仅有密集层的模型也无法达到与卷积网络相当的准确性。这向你展示了卷积网络的强大之处：通过参数共享和利用视觉特征的局部性，卷积网络可以在计算机视觉任务中实现优越的准确性，其参数数量相等或更少于非卷积神经网络。
- en: Figure 4.9\. Same as [figure 4.8](#ch04fig08), but for a nonconvolutional model
    for the MNIST problem, created by the `createDenseModel()` function in [listing
    4.3](#ch04ex03)
  id: totrans-125
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.9\. 与 [图 4.8](#ch04fig08) 相同，但用于 MNIST 问题的非卷积模型，由 [列表 4.3](#ch04ex03) 中的
    `createDenseModel()` 函数创建
- en: '![](04fig08_alt.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](04fig08_alt.jpg)'
- en: 4.2.6\. Using a convnet to make predictions
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.6\. 使用卷积网络进行预测
- en: Now we have a trained convnet. How do we use it to actually classify images
    of hand-written digits? First, you need to get hold of the image data. There are
    a number of ways through which image data can be made available to TensorFlow.js
    models. We will list them and describe when they are applicable.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个训练好的卷积网络。我们如何使用它来实际分类手写数字的图像呢？首先，你需要获得图像数据。有许多方法可以将图像数据提供给 TensorFlow.js
    模型。我们将列出它们并描述它们何时适用。
- en: Creating image tensors from TypedArrays
  id: totrans-129
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 从 TypedArrays 创建图像张量
- en: In some cases, the image data you want are already stored as JavaScript `TypedArray`s.
    This is the case in the tfjs-example/mnist example we are focusing on. The details
    are in the data.js file, and we will not elaborate on the detailed machinery.
    Given a `Float32Array` representing an MNIST of the correct length (say, a variable
    named `imageDataArray`), we can convert it into a 4D tensor of the shape expected
    by our model with^([[7](#ch04fn7)])
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，您想要的图像数据已经存储为 JavaScript 的 `TypedArray`。这就是我们专注于的 tfjs-example/mnist
    示例中的情况。详细信息请参见 data.js 文件，我们不会详细说明其中的机制。假设有一个表示正确长度的 MNIST 的 `Float32Array`（例如，一个名为
    `imageDataArray` 的变量），我们可以将其转换为我们的模型所期望的形状的 4D 张量，如下所示^([[7](#ch04fn7)])：
- en: ⁷
  id: totrans-131
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁷
- en: ''
  id: totrans-132
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See [appendix B](kindle_split_030.html#app02) for a more comprehensive tutorial
    on how to create tensors using the low-level API in TensorFlow.js.
  id: totrans-133
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见[附录 B](kindle_split_030.html#app02)了解如何使用 TensorFlow.js 中的低级 API 创建张量的更全面教程。
- en: '[PRE9]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The second argument in the `tf.tensor4d()` call specifies the shape of the
    tensor to be created. It is necessary because a `Float32Array` (or a `TypedArray`
    in general) is a flat structure with no information regarding the image’s dimensions.
    The size of the first dimension is 1 because we are dealing with a single image
    in `imageDataArray`. As in previous examples, the model always expects a batch
    dimension during training, evaluation, and inference, no matter whether there
    is only one image or more than one. If the `Float32Array` contains a batch of
    multiple images, it can also be converted into a single tensor, where the size
    of the first dimension equals the number of images:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.tensor4d()` 调用中的第二个参数指定要创建的张量的形状。这是必需的，因为 `Float32Array`（或一般的 `TypedArray`）是一个没有关于图像尺寸信息的平坦结构。第一个维度的大小为
    1，因为我们正在处理 `imageDataArray` 中的单个图像。与之前的示例一样，在训练、评估和推断期间，模型始终期望有一个批次维度，无论是一个图像还是多个图像。如果
    `Float32Array` 包含多个图像的批次，则还可以将其转换为单个张量，其中第一个维度的大小等于图像数量：'
- en: '[PRE10]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'tf.browser.fromPixels: Getting image tensors from HTML img, canva- as, or video
    elements'
  id: totrans-137
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: tf.browser.fromPixels：从 HTML img、canvas 或 video 元素获取图像张量
- en: The second way to get image tensors in the browser is to use the TensorFlow.js
    function `tf.browser.fromPixels()` on HTML elements that contain image data—this
    includes `img`, `canvas`, and `video` elements.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 浏览器中获取图像张量的第二种方法是在包含图像数据的 HTML 元素上使用 TensorFlow.js 函数 `tf.browser.fromPixels()`
    ——这包括 `img`、`canvas` 和 `video` 元素。
- en: For example, suppose a web page contains an `img` element defined as
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设网页包含一个如下定义的 `img` 元素
- en: '[PRE11]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'You can obtain the image data displayed in the `img` element with one line:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以用一行代码获取显示在 `img` 元素中的图像数据：
- en: '[PRE12]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This generates a tensor of shape `[height, width, 3]`, where the three channels
    are for RGB color encoding. The `asType0` call at the end is necessary because
    `tf.browser.fromPixels()` returns a int32-type tensor, but the convnet expects
    float32-type tensors as inputs. The height and width are determined by the size
    of the `img` element. If it doesn’t match the height and width expected by the
    model, you can either change the height and width attributes of the `img` element
    (if that doesn’t make the UI look bad, of course) or resize the tensor from `tf.browser.fromPixels()`
    by using one of the two image-resizing methods provided by TensorFlow.js, `tf.image.resizeBilinear()`
    or `tf.image.resizeNearestNeigbor()`:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成形状为 `[height, width, 3]` 的张量，其中三个通道用于 RGB 颜色编码。末尾的 `asType0` 调用是必需的，因为 `tf.browser.fromPixels()`
    返回一个 int32 类型的张量，但 convnet 期望输入为 float32 类型的张量。高度和宽度由 `img` 元素的大小确定。如果它与模型期望的高度和宽度不匹配，您可以通过使用
    TensorFlow.js 提供的两种图像调整方法之一 `tf.image.resizeBilinear()` 或 `tf.image.resizeNearestNeigbor()`
    来改变 `tf.browser.fromPixels()` 返回的张量大小：
- en: '[PRE13]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`tf.image.resizeBilinear()` and `tf.image.resizeNearestNeighbor()` have the
    same syntax, but they perform image resizing with two different algorithms. The
    former uses bilinear interpolation to form pixel values in the new tensor, while
    the latter performs nearest-neighbor sampling and is usually less computationally
    intensive than bilinear interpolation.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.image.resizeBilinear()` 和 `tf.image.resizeNearestNeighbor()` 具有相同的语法，但它们使用两种不同的算法进行图像调整。前者使用双线性插值来生成新张量中的像素值，而后者执行最近邻采样，通常比双线性插值计算量小。'
- en: Note that the tensor created by `tf.browser.fromPixels()` does not include a
    batch dimension. So, if the tensor is to be fed into a TensorFlow.js model, it
    must be dimension-expanded first; for example,
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`tf.browser.fromPixels()` 创建的张量不包括批次维度。因此，如果张量要被馈送到 TensorFlow.js 模型中，必须首先进行维度扩展；例如，
- en: '[PRE14]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`expandDims()` takes a dimension argument in general. But in this case, the
    argument can be omitted because we are expanding the first dimension, which is
    the default for that argument.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`expandDims()` 通常需要一个维度参数。但在这种情况下，可以省略该参数，因为我们正在扩展默认为该参数的第一个维度。'
- en: In addition to `img` elements, `tf.browser.fromPixels()` works on `canvas` and
    `video` elements in the same way. Applying `tf.browser.fromPixels()` on `canvas`
    elements is useful for cases in which the user can interactively alter the content
    of a canvas before the content is used by a TensorFlow.js model. For example,
    imagine an online handwriting-recognition app or an online hand-drawn-shape-recognition
    app. Apart from static images, applying `tf.browser.fromPixels()` on `video` elements
    is useful for obtaining frame-by-frame image data from a webcam. This is exactly
    what’s done in the Pac-Man demo that Nikhil Thorat and Daniel Smilkov gave during
    the initial TensorFlow.js announcement (see [http://mng.bz/xl0e](http://mng.bz/xl0e)),
    the PoseNet demo,^([[8](#ch04fn8)]) and many other TensorFlow.js-based web apps
    that use a webcam. You can read the source code on GitHub at [http://mng.bz/ANYK](http://mng.bz/ANYK).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 `img` 元素外，`tf.browser.fromPixels()` 也适用于 `canvas` 和 `video` 元素。在 `canvas`
    元素上应用 `tf.browser.fromPixels()` 对于用户可以交互地改变 canvas 内容然后使用 TensorFlow.js 模型的情况非常有用。例如，想象一下在线手写识别应用或在线手绘形状识别应用。除了静态图像外，在
    `video` 元素上应用 `tf.browser.fromPixels()` 对于从网络摄像头获取逐帧图像数据非常有用。这正是在 Nikhil Thorat
    和 Daniel Smilkov 在最初的 TensorFlow.js 发布中展示的 Pac-Man 演示中所做的（参见 [http://mng.bz/xl0e](http://mng.bz/xl0e)），PoseNet
    演示，^([[8](#ch04fn8)]) 以及许多其他使用网络摄像头的基于 TensorFlow.js 的网络应用程序。你可以在 GitHub 上查看源代码
    [http://mng.bz/ANYK](http://mng.bz/ANYK)。
- en: ⁸
  id: totrans-150
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁸
- en: ''
  id: totrans-151
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Dan Oved, “Real-time Human Pose Estimation in the Browser with TensorFlow.js,”
    Medium, 7 May 2018, [http://mng.bz/ZeOO](http://mng.bz/ZeOO).
  id: totrans-152
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Dan Oved，“使用 TensorFlow.js 在浏览器中进行实时人体姿势估计”，Medium，2018 年 5 月 7 日，[http://mng.bz/ZeOO](http://mng.bz/ZeOO)。
- en: 'As we have seen in the previous chapters, great care should be taken to avoid
    *skew* (that is, mismatch) between the training data and the inference data. In
    this case, our MNIST convnet is trained with image tensors normalized to the range
    between 0 and 1\. Therefore, if the data in the `x` tensor has a different range,
    say 0–255, as is common in HTML-based image data, we should normalize the data:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的章节中所看到的，应该非常小心避免训练数据和推断数据之间的 *偏差*（即不匹配）。在这种情况下，我们的 MNIST 卷积网络是使用范围在
    0 到 1 之间的图像张量进行训练的。因此，如果 `x` 张量中的数据范围不同，比如常见的 HTML 图像数据范围是 0 到 255，那么我们应该对数据进行归一化：
- en: '[PRE15]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: With the data at hand, we are now ready to call `model.predict()` to get the
    predictions. See the following listing.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 有了手头的数据，我们现在准备调用 `model.predict()` 来获取预测结果。请参见下面的清单。
- en: Listing 4.4\. Using the trained convnet for inference
  id: totrans-156
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 4.4。使用训练好的卷积网络进行推断
- en: '[PRE16]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '***1*** Uses tf.tidy() to prevent WebGL memory leaks'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 使用 tf.tidy() 避免 WebGL 内存泄漏'
- en: '***2*** Calls argMax() to get the class with the largest probability'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 调用 argMax() 函数获取概率最大的类别'
- en: The code is written with the assumption that the batch of images for prediction
    is already available in a single tensor, namely, `examples.xs`. It has a shape
    of `[100,` `28, 28,` `1]` (including the batch dimension), where the first dimension
    reflects the fact that there are 100 images we are running a prediction on. `model.predict()`
    returns an output 2D tensor of shape `[100,` `10]`. The first dimension of the
    output corresponds to the examples, while the second dimension corresponds to
    the 10 possible digits. Every row of the output tensor includes the probability
    values assigned to the 10 digits for a given image input. To determine the prediction,
    we need to find out the indices of the maximum probability values, image by image.
    This is done with the lines
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码假设用于预测的图像批已经以一个单一张量的形式可用，即 `examples.xs`。它的形状是 `[100,` `28, 28,` `1]`（包括批处理维度），其中第一个维度反映了我们要运行预测的
    100 张图像。`model.predict()` 返回一个形状为 `[100,` `10]` 的输出二维张量。输出的第一个维度对应于示例，而第二个维度对应于
    10 个可能的数字。输出张量的每一行包含为给定图像输入分配的 10 个数字的概率值。为了确定预测结果，我们需要逐个图像找出最大概率值的索引。这是通过以下代码完成的
- en: '[PRE17]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The `argMax()` function returns the indices of the maximum values along a given
    axis. In this case, this axis is the second dimension, `const axis = 1`. The return
    value of `argMax()` is a tensor of shape `[100, 1]`. By calling `dataSync()`,
    we convert the `[100, 1]`- shaped tensor into a length-100 `Float32Array`. Then
    `Array.from()` converts the `Float32Array` into an ordinary JavaScript array consisting
    of 100 integers between 0 and 9\. This predictions array has a very straightforward
    meaning: it is the classification results made by the model for the 100 input
    images. In the MNIST dataset, the target labels happen to match the output index
    exactly. Therefore, we don’t even need to convert the array into string labels.
    The predictions array is consumed by the next line, which calls a UI function
    that renders the results of the classification alongside the test images (see
    [figure 4.10](#ch04fig10)).'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`argMax()` 函数返回沿指定轴的最大值的索引。在这种情况下，这个轴是第二维，`const axis = 1`。`argMax()` 的返回值是一个形状为
    `[100, 1]` 的张量。通过调用 `dataSync()`，我们将 `[100, 1]` 形状的张量转换为长度为 100 的 `Float32Array`。然后
    `Array.from()` 将 `Float32Array` 转换为由 100 个介于 0 和 9 之间的整数组成的普通 JavaScript 数组。这个预测数组有一个非常直观的含义：它是模型对这
    100 个输入图像的分类结果。在 MNIST 数据集中，目标标签恰好与输出索引完全匹配。因此，我们甚至不需要将数组转换为字符串标签。下一行消耗了预测数组，调用了一个
    UI 函数，该函数将分类结果与测试图像一起呈现（见 [图 4.10](#ch04fig10)）。'
- en: Figure 4.10\. A few examples of predictions made by the model after training,
    shown alongside the input MNIST images
  id: totrans-163
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.10\. 训练后模型进行预测的几个示例，显示在输入的 MNIST 图像旁边
- en: '![](04fig09_alt.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](04fig09_alt.jpg)'
- en: '4.3\. Beyond browsers: Training models faster using Node.js'
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 超越浏览器：使用 Node.js 更快地训练模型
- en: 'In the previous section, we trained a convnet in the browser, and it reached
    a test accuracy of 99.0%. In this section, we will create a more powerful convnet
    that will give us a higher test accuracy: around 99.5%. The improved accuracy
    comes at a cost, though: a greater amount of memory and computation consumed by
    the model during both training and inference. The increase in cost is more pronounced
    during training because training involves backpropagation, which is more computationally
    intensive compared to the forward runs that inference entails. The larger convnet
    will be too heavy and too slow to train in most web browser environments.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们在浏览器中训练了一个卷积网络，测试准确率达到了 99.0%。在本节中，我们将创建一个更强大的卷积网络，它将给我们更高的测试准确率：大约
    99.5%。然而，提高的准确性也伴随着代价：模型在训练和推断期间消耗的内存和计算量更多。成本的增加在训练期间更为显著，因为训练涉及反向传播，这与推断涉及的前向运行相比，需要更多的计算资源。较大的卷积网络将过重且速度过慢，在大多数
    web 浏览器环境下训练。
- en: 4.3.1\. Dependencies and imports for using tfjs-node
  id: totrans-167
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1\. 使用 tfjs-node 的依赖项和导入
- en: Enter the Node.js version of TensorFlow.js! It runs in a backend environment,
    unhindered by any resource restriction like that of a browser tab. The CPU version
    of Node.js of TensorFlow (*tfjs-node* for short hereafter) directly uses the multithreaded
    math operations written in C++ and used by the main Python version of TensorFlow.
    If you have a CUDA-enabled GPU installed on your machine, tfjs-node can also use
    the GPU-accelerated math kernels written in CUDA, achieving even greater gains
    in speed.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 进入 TensorFlow.js 的 Node.js 版本！它在后端环境中运行，不受像浏览器标签那样的任何资源限制。TensorFlow.js 的 CPU
    版本（此处简称 tfjs-node）直接使用了在 C++ 中编写的多线程数学运算，这些数学运算也被 TensorFlow 的主 Python 版本使用。如果您的计算机安装了支持
    CUDA 的 GPU，tfjs-node 还可以使用 CUDA 编写的 GPU 加速数学核心，实现更大的速度提升。
- en: 'The code for our enhanced MNIST convnet is in the mnist-node directory of tfjs-examples.
    As in the examples we have seen, you can use the following commands to access
    the code:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们增强的 MNIST 卷积神经网络的代码位于 tfjs-examples 的 mnist-node 目录中。与我们所见的示例一样，您可以使用以下命令访问代码：
- en: '[PRE18]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: What’s different from the previous examples is that the mnist-node example will
    run in a terminal instead of a web browser. To download the dependencies, use
    the `yarn` command.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的示例不同之处在于，mnist-node 示例将在终端而不是 web 浏览器中运行。要下载依赖项，请使用 `yarn` 命令。
- en: If you examine the package.json file, you can see the dependency `@tensorflow/tfjs-node`.
    With `@tensorflow/tfjs-node` declared as a dependency, `yarn` will automatically
    download the C++ shared library (with the name libtensorflow.so, libtensorflw
    .dylib, or libtensorflow.dll on Linux, Mac, or Windows systems, respectively)
    into your node_ modules directory for use by TensorFlow.js.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你检查 package.json 文件，你会看到依赖项 `@tensorflow/tfjs-node`。通过将 `@tensorflow/tfjs-node`
    声明为依赖项，`yarn` 将自动下载 C++ 共享库（在 Linux、Mac 或 Windows 系统上分别命名为 libtensorflow.so、libtensorflw
    .dylib 或 libtensorflow.dll）到你的 node_modules 目录，以供 TensorFlow.js 使用。
- en: Once the `yarn` command has finished running, you can kick off the model training
    with
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 `yarn` 命令运行完毕，你就可以开始模型训练了
- en: '[PRE19]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We assume that the node binary is available on your path since you have already
    installed yarn (see [appendix A](kindle_split_027.html#app01) if you need more
    information on this).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设你的路径上已经有了节点二进制文件，因为你已经安装了 yarn（如果你需要更多关于这个的信息，请参见[附录A](kindle_split_027.html#app01)）。
- en: 'The workflow just described will allow you to train the enhanced convnet on
    your CPU. If your workstation and laptop have a CUDA-enabled GPU inside, you can
    also train the model on your GPU. The steps involved are as follows:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 刚刚描述的工作流将允许你在 CPU 上训练增强的 convnet。如果你的工作站和笔记本电脑内置了 CUDA 启用的 GPU，你也可以在 GPU 上训练模型。所涉及的步骤如下：
- en: Install the correct versions of the NVIDIA driver for your GPU.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装正确版本的 NVIDIA 驱动程序以适配你的 GPU。
- en: Install the NVIDIA CUDA toolkit. This is the library that enables general-purpose
    parallel computing on NVIDIA’s line of GPUs.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 NVIDIA CUDA 工具包。这是一个库，可以在 NVIDIA 的 GPU 系列上实现通用并行计算。
- en: Install CuDNN, NVIDIA’s library for high-performance, deep-learning algorithms
    built on top of CUDA (see [appendix A](kindle_split_027.html#app01) for more details
    on steps 1–3).
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 CuDNN，这是基于 CUDA 构建的高性能深度学习算法的 NVIDIA 库（有关步骤 1-3 的更多详细信息，请参见[附录A](kindle_split_027.html#app01)）。
- en: In package.json, replace the `@tensorflow/tfjs-node` dependency with `@-tensor-flow/tfjs-node-gpu`,
    but keep the same version number because the two packages have synchronized releases.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 package.json 中，将 `@tensorflow/tfjs-node` 依赖项替换为 `@-tensor-flow/tfjs-node-gpu`，但保持相同的版本号，因为这两个软件包的发布是同步的。
- en: Run `yarn` again, which will download the shared library that contains the CUDA
    math operations for TensorFlow.js use.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次运行 `yarn`，这将下载包含 TensorFlow.js 用于 CUDA 数学运算的共享库。
- en: In main.js, replace the line
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 main.js 中，将该行替换为
- en: '[PRE20]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: with
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用
- en: '[PRE21]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Start the training again with
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次开始训练
- en: '[PRE22]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: If the steps are done correctly, your model will be roaring ahead on your CUDA
    GPU, training at a speed that is typically five times the speed you can get with
    the CPU version (tfjs-node). Training with either the CPU or GPU version of tfjs-node
    is significantly faster compared to training the same model in the browser.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如果步骤正确完成，你的模型将在 CUDA GPU 上迅猛地训练，在速度上通常是 CPU 版本（tfjs-node）的五倍。与在浏览器中训练相同模型相比，使用
    tfjs-node 的 CPU 或 GPU 版本训练速度都显著提高。
- en: Training an enhanced convnet for MNIST in tfjs-node
  id: totrans-189
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在 tfjs-node 中为 MNIST 训练增强的 convnet
- en: Once the training is complete in 20 epochs, the model should show a final test
    (or evaluation) accuracy of approximately 99.6%, which beats the previous result
    of 99.0% we achieved in [section 4.2](#ch04lev1sec2). So, what are the differences
    between this node-based model and the browser-based model that lead to this boost
    in accuracy? After all, if you train the same model in tfjs-node and the browser
    version of TensorFlow.js using the training data, you should get the same results
    (except the effects or random weights initialization.) To answer this question,
    let’s look at the definition of the node-based model. The model is constructed
    in the file model.js, which is imported by main.js.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦在 20 个周期内完成训练，模型应该显示出约 99.6% 的最终测试（或评估）准确度，这超过了我们在[section 4.2](#ch04lev1sec2)中取得的
    99.0% 的先前结果。那么，导致准确度提高的是这个基于节点的模型和基于浏览器的模型之间的区别是什么呢？毕竟，如果你使用训练数据在 tfjs-node 和
    TensorFlow.js 的浏览器版本中训练相同的模型，你应该得到相同的结果（除了随机权重初始化的影响）。为了回答这个问题，让我们来看看基于节点的模型的定义。模型是在文件
    model.js 中构建的，这个文件由 main.js 导入。
- en: Listing 4.5\. Defining a larger convnet for MNIST in Node.js
  id: totrans-191
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.5\. 在 Node.js 中定义一个更大的 MNIST convnet
- en: '[PRE23]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '***1*** Adds dropout layers to reduce overfitting'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 添加了 dropout 层以减少过拟合'
- en: 'The summary of the model is as follows:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的摘要如下：
- en: '[PRE24]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'These are the key differences between our tfjs-node model and the browser-based
    model:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我们的 tfjs-node 模型和基于浏览器的模型之间的关键区别：
- en: The node-based model has four conv2d layers, one more compared to the browser-based
    model.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于节点的模型具有四个conv2d层，比基于浏览器的模型多一个。
- en: The hidden dense layer in the node-based model has more units (512) compared
    to the counterpart in the browser-based model (100).
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于节点的模型的hidden dense层比基于浏览器的模型的对应层单元更多（512与100）。
- en: Overall, the node-based model has about 18 times as many weight parameters as
    the browser-based model.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总体而言，基于节点的模型的权重参数约为基于浏览器的模型的18倍。
- en: The node-based model has two *dropout* layers inserted between the flatten and
    dense layers.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于节点的模型在flatten层和dense层之间插入了两个*dropout*层。
- en: The first three differences in this list give the node-based model a higher
    capacity than the browser-based model. They are also what make the node-based
    model too memory- and computation-intensive to be trained with acceptable speed
    in the browser. As we learned in [chapter 3](kindle_split_014.html#ch03), with
    greater model capacity comes a greater risk of overfitting. The increased risk
    of overfitting is ameliorated by the fourth difference, namely, the inclusion
    of dropout layers.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 列表中的前三个差异使基于节点的模型具有比基于浏览器的模型更高的容量。这也是使基于节点的模型在浏览器中训练速度无法接受的原因。正如我们在[第3章](kindle_split_014.html#ch03)中学到的那样，更大的模型容量意味着更大的过拟合风险。第四个差异增加了过拟合风险的减轻，即包括dropout层。
- en: Reducing overfitting with dropout layers
  id: totrans-202
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用dropout层来减少过拟合。
- en: 'Dropout is yet another new TensorFlow.js layer type you have encountered in
    this chapter. It is one of the most effective and widely used ways to reduce overfitting
    in deep neural networks. Its functionality can be described simply:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout是你在本章中遇到的另一个新的TensorFlow.js层类型之一。它是减少深层神经网络过拟合最有效和广泛使用的方式之一。它的功能可以简单地描述为：
- en: 'During the training phase (during `Model.fit()` calls), it randomly sets a
    fraction of the elements in the input tensor as zero (or “dropped”), and the result
    is the output tensor of the dropout layer. For the purpose of this example, a
    dropout layer has only one configuration parameter: the dropout rate (for example,
    the two `rate` fields as shown in [listing 4.5](#ch04ex05)). For example, suppose
    a dropout layer is configured to have a dropout rate of 0.25, and the input tensor
    is a 1D tensor of value `[0.7, -0.3, 0.8, -0.4]`; the output tensor may be `[0.7,
    -0.3, 0.0, 0.4]`—with 25% of the input tensor’s elements selected at random and
    set to the value 0\. During backpropagation, the gradient tensor on a dropout
    layer is affected similarly by this random zeroing-out.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练阶段（在`Model.fit()`调用期间），它随机将输入张量的一部分作为零（或“丢失”），并且其输出张量是dropout层的输出张量。对于本示例来说，dropout层只有一个配置参数：dropout比例（例如，如[列表4.5](#ch04ex05)所示的两个`rate`字段）。例如，假设一个dropout层被配置为有一个0.25的dropout比例，并且输入张量是值为`[0.7,
    -0.3, 0.8, -0.4]` 的1D张量，则输出张量可以是`[0.7, -0.3, 0.0, 0.4]`，其中25％的输入张量元素随机选择并设置为值0。在反向传播期间，dropout层的梯度张量也会受到类似的归0影响。
- en: During the inference phase (during `Model.predict()` and `Model.evaluate()`
    calls), a dropout layer does *not* randomly zero-out elements in the input tensor.
    Instead, the input is simply passed through as the output without change (that
    is, an identity mapping).
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在推理期间（在`Model.predict()`和`Model.evaluate()`调用期间），dropout层不会随机将输入张量中的元素置零。相反，输入会简单地作为输出传递，不会发生任何改变（即，一个恒等映射）。
- en: '[Figure 4.11](#ch04fig11) shows an example of how a dropout layer with a 2D
    input tensor works at training time and testing time.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4.11](#ch04fig11)展示了一个带有二维输入张量的dropout层在训练和测试时的工作示例。'
- en: Figure 4.11\. An example of how a dropout layer works. In this example, the
    input tensor is 2D and has a shape of `[4, 2]`. The dropout layer has its rate
    configured as 0.25, which leads to 25% (that is, two out of eight) elements of
    the input tensor being randomly selected and set to zero during the training phase.
    During the inference phase, the layer acts as a trivial passthrough.
  id: totrans-207
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.11. dropout层的一个示例。在这个示例中，输入张量是2D的，形状为`[4, 2]`。dropout层的比例被配置为0.25，导致在训练阶段随机选择输入张量中25％（即8个中的两个）的元素并将它们设置为零。在推理阶段，该层充当一个简单的传递层。
- en: '![](04fig10.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](04fig10.jpg)'
- en: It might seem strange that such a simple algorithm is one of the most effective
    ways of fighting overfitting. Why does it work? Geoff Hinton, the inventor of
    the dropout algorithm (among many other things in neural networks) says he was
    inspired by a mechanism used by some banks to prevent fraud by employees. In his
    own words,
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这种简单算法是对抗过拟合最有效的方法之一似乎很奇怪。为什么它有效呢？Geoff Hinton，是 dropout 算法的发明者（神经网络中的许多其他内容也是他的创造），他说他受到了一些银行用于防止员工欺诈的机制的启发。用他自己的话说，
- en: '*I went to my bank. The tellers kept changing, and I asked one of them why.
    He said he didn’t know, but they got moved around a lot. I figured it must be
    because it would require cooperation between employees to successfully defraud
    the bank. This made me realize that randomly removing a different subset of neurons
    on each example would prevent conspiracies and thus reduce overfitting.*'
  id: totrans-210
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*我去了我的银行。出纳员不停地换，我问其中一个为什么。他说他不知道，但他们经常被调动。我想这一定是因为需要员工之间的合作才能成功欺诈银行。这让我意识到，随机移除每个示例上的不同子集神经元将防止共谋，从而减少过拟合。*'
- en: To put this into the lingo of deep learning, introducing noise in the output
    values of a layer breaks up happenstance patterns that aren’t significant with
    regard to the true patterns in the data (what Hinton refers to as “conspiracies”).
    In exercise 3 at the end of this chapter, you should try removing the two dropout
    layers from the node-based convnet in model.js, train the model again, and see
    how the training, validation, and evaluation accuracies change as a result.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 将这种深度学习的术语引入，向层的输出值引入噪声会破坏不重要的偶然模式，这些模式与数据中的真实模式不相关（Hinton 称之为“共谋”）。在本章末尾的练习
    3 中，您应该尝试从 model.js 中的基于节点的卷积网络中移除两个 dropout 层，然后再次训练模型，并查看由此导致的训练、验证和评估准确度的变化。
- en: '[Listing 4.6](#ch04ex06) shows the key code we use to train and evaluate the
    enhanced convnet. If you compare the code here with that in [listing 4.2](#ch04ex02),
    you can appreciate the similarity between the two chunks of code. Both are centered
    around `Model.fit()` and `Model.evaluate()` calls. The syntax and style are identical,
    except with regard to how the loss value, accuracy value, and training progress
    are rendered or displayed on different user interfaces (terminal versus browser).'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 4.6](#ch04ex06) 显示了我们用于训练和评估增强型卷积网络的关键代码。如果您将此处的代码与 [清单 4.2](#ch04ex02)
    中的代码进行比较，您就会欣赏到这两个代码块之间的相似之处。两者都围绕着 `Model.fit()` 和 `Model.evaluate()` 调用。语法和样式相同，只是在如何呈现或显示损失值、准确度值和训练进度上有所不同（终端与浏览器）。'
- en: 'This shows an important feature of TensorFlow.js, a JavaScript deep-learning
    framework that straddles the frontend and the backend:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这展示了 TensorFlow.js 的一个重要特性，这是一个跨越前端和后端的 JavaScript 深度学习框架：
- en: '*As far as the creation and training of models is concerned, the code you write
    in TensorFlow.js is the same regardless of whether you are working with the web
    browser or with Node.js.*'
  id: totrans-214
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*就创建和训练模型而言，在 TensorFlow.js 中编写的代码与您是在 web 浏览器还是在 Node.js 中工作无关。*'
- en: Listing 4.6\. Training and evaluating the enhanced convnet in tfjs-node
  id: totrans-215
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.6\. 在 tfjs-node 中训练和评估增强型卷积网络
- en: '[PRE25]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '***1*** Evaluates the model using data the model hasn’t seen'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 使用模型未见过的数据评估模型'
- en: 4.3.2\. Saving the model from Node.js and loading it in the browser
  id: totrans-218
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2\. 从 Node.js 保存模型并在浏览器中加载
- en: Training your model consumes CPU and GPU resources and takes some time. You
    don’t want to throw away the fruit of training. Without saving the model, you
    would have to start from scratch the next time you run main.js. This section shows
    how to save the model after training and export the saved model as files on the
    disk (called a *checkpoint* or an *artifact*). We will also show you how to import
    the checkpoint in the browser, reconstitute it as a model, and use it for inference.
    The final part of the `main()` function in main.js consists of the model-saving
    code in the following listing.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型会消耗 CPU 和 GPU 资源，并需要一些时间。您不希望浪费训练的成果。如果不保存模型，下次运行 main.js 时，您将不得不从头开始。本节展示了如何在训练后保存模型，并将保存的模型导出为磁盘上的文件（称为
    *检查点* 或 *工件*）。我们还将向您展示如何在浏览器中导入检查点，将其重新构建为模型，并用于推断。main.js 中 `main()` 函数的最后一部分是以下清单中的保存模型代码。
- en: Listing 4.7\. Saving the trained model to the file system in tfjs-node
  id: totrans-220
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.7\. 在 tfjs-node 中将训练好的模型保存到文件系统中
- en: '[PRE26]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The `save()` method of the `model` object is used to save the model to a directory
    on your file system. The method takes a single argument, which is a URL string
    that begins with the scheme file://. Note that it is possible to save the model
    on the file system because we are using tfjs-node. The browser version of TensorFlow.js
    also provides the `model.save()` API but cannot access the machine’s native file
    system directly because the browser forbids that for security reasons. Non-file-system
    saving destinations (such as the browser’s local storage and IndexedDB) will have
    to be used if we are using TensorFlow.js in the browser. Those correspond to URL
    schemes other than file://.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '`model`对象的`save()`方法用于将模型保存到文件系统上的目录中。该方法接受一个参数，即以file://开头的URL字符串。请注意，由于我们使用的是tfjs-node，所以可以将模型保存在文件系统上。TensorFlow.js的浏览器版本也提供了`model.save()`API，但不能直接访问机器的本地文件系统，因为浏览器出于安全原因禁止了这样做。如果我们在浏览器中使用TensorFlow.js，则必须使用非文件系统保存目标（例如浏览器的本地存储和IndexedDB）。这些对应于file://以外的URL方案。'
- en: '`model.save()` is an asynchronous function because it involves file or network
    input-output in general. Therefore, we use await on the `save()` call. Suppose
    `modelSavePath` has a value /tmp/tfjs-node-mnist; after the `model.save()` call
    completes, you can examine the content of the directory,'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '`model.save()`是一个异步函数，因为它通常涉及文件或网络输入输出。因此，在`save()`调用上使用`await`。假设`modelSavePath`的值为/tmp/tfjs-node-mnist；在`model.save()`调用完成后，您可以检查目录的内容，'
- en: '[PRE27]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'which may print a list of files like the following:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能打印出类似以下的文件列表：
- en: '[PRE28]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'There, you can see two files:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在那里，你可以看到两个文件：
- en: model.json is a JSON file that contains the model’s saved topology. What’s referred
    to as “topology” here includes the types of layers that form the model, their
    respective configuration parameters (such as `filters` for a conv2d layer and
    `rate` for a dropout layer), as well as the way in which the layers connect to
    each other. The connections are simple for the MNIST convnet because it is a sequential
    model. We will see models with less trivial connection patterns, which can also
    be saved to disk with `model.save()`.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: model.json是一个包含模型保存拓扑的JSON文件。这里所说的“拓扑”包括形成模型的层类型、它们各自的配置参数（例如卷积层的`filters`和dropout层的`rate`），以及层之间的连接方式。对于MNIST卷积网络来说，连接是简单的，因为它是一个顺序模型。我们将看到连接模式不太平凡的模型，这些模型也可以使用`model.save()`保存到磁盘上。
- en: 'In addition to the model topology, model.json also contains a manifest of the
    model’s weights. That part lists the names, shapes, and data types of all the
    model’s weights, in addition to the locations at which the weight values are stored.
    This brings us to the second file: weights.bin. As its name indicates, weights.bin
    is a binary file that stores all the model’s weight values. It is a flat binary
    stream without demarcation of where the individual weights begin and end. That
    “metainformation” is available in the weights-manifest part of the JSON object
    in model.json.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了模型拓扑，model.json还包含模型权重的清单。该部分列出了所有模型权重的名称、形状和数据类型，以及权重值存储的位置。这将我们带到第二个文件：weights.bin。正如其名称所示，weights.bin是一个存储所有模型权重值的二进制文件。它是一个没有标记的平面二进制流，没有标明个体权重的起点和终点。这些“元信息”在model.json中的权重清单部分中可用于。
- en: 'To load the model using tfjs-node, you can use the `tf.loadLayersModel()` method,
    pointing to the location of the model.json file (not shown in the example code):'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用tfjs-node加载模型，您可以使用`tf.loadLayersModel()`方法，指向model.json文件的位置（未在示例代码中显示）：
- en: '[PRE29]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '`tf.loadLayersModel()` reconstitutes the model by deserializing the saved topology
    data in model.json. Then, `tf.loadLayersModel()` reads the binary weight values
    in weights.bin using the manifest in model.json and force-sets the model’s weight
    to those values. Like `model.save()`, `tf.loadLayersModel()` is asynchronous,
    so we use `await` when calling it here. Once the call returns, the `loadedModel`
    object is, for all intents and purposes, equivalent to the model created and trained
    using the JavaScript code in [listings 4.5](#ch04ex05) and [4.6](#ch04ex06). You
    can print a summary of the model by calling its `summary()` method, use it to
    perform inference by calling its `predict()` method, evaluate its accuracy by
    using the `evaluate()` method, or even retrain it using the `fit()` method. If
    so desired, the model can also be saved again. The workflow of retraining and
    resaving a loaded model will be relevant when we talk about transfer learning
    in [chapter 5](kindle_split_016.html#ch05).'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.loadLayersModel()`通过反序列化`model.json`中保存的拓扑数据来重建模型。然后，`tf.loadLayersModel()`使用`model.json`中的清单读取`weights.bin`中的二进制权重值，并将模型的权重强制设置为这些值。与`model.save()`一样，`tf.loadLayersModel()`是异步的，所以我们在这里调用它时使用`await`。一旦调用返回，`loadedModel`对象在所有意图和目的上等同于使用[listings
    4.5](#ch04ex05)和[4.6](#ch04ex06)中的JavaScript代码创建和训练的模型。你可以通过调用其`summary()`方法打印模型的摘要，通过调用其`predict()`方法执行推理，通过使用`evaluate()`方法评估其准确性，甚至通过使用`fit()`方法重新训练它。如果需要，也可以再次保存模型。重新训练和重新保存加载的模型的工作流程将在我们讨论[第5章](kindle_split_016.html#ch05)的迁移学习时相关。'
- en: 'What’s said in the previous paragraph applies to the browser environment as
    well. The files you saved can be used to reconstitute the model in a web page.
    The reconstituted model supports the full `tf.LayersModel()` workflow, with the
    caveat that, if you retrain the entire model, it will be especially slow and inefficient
    due to the large size of the enhanced convnet. The only thing that’s fundamentally
    different between loading a model in tfjs-node and in the browser is that you
    should use a URL scheme other than file:// in the browser. Typically, you can
    put the model.json and weights.bin files as static asset files on an HTTP server.
    Suppose your hostname is localhost and your files are seen under the server path
    my/models/; you can use the following line to load the model in the browser:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段中所说的内容同样适用于浏览器环境。你保存的文件可以用来在网页中重建模型。重建后的模型支持完整的`tf.LayersModel()`工作流程，但有一个警告，如果你重新训练整个模型，由于增强卷积网络的尺寸较大，速度会特别慢且效率低下。在tfjs-node和浏览器中加载模型唯一根本不同的是，你在浏览器中应该使用除`file://`之外的其他URL方案。通常，你可以将`model.json`和`weights.bin`文件作为静态资产文件放在HTTP服务器上。假设你的主机名是`localhost`，你的文件在服务器路径`my/models/`下可见；你可以使用以下行在浏览器中加载模型：
- en: '[PRE30]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'When handling HTTP-based model loading in the browser, `tf.loadLayersModel()`
    calls the browser’s built-in fetch function under the hood. Therefore, it has
    the following features and properties:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在浏览器中处理基于HTTP的模型加载时，`tf.loadLayersModel()`在底层调用浏览器内置的fetch函数。因此，它具有以下特性和属性：
- en: Both http:// and https:// are supported.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持`http://`和`https://`。
- en: 'Relative server paths are supported. In fact, if a relative path is used, the
    http:// or https:// part of the URL can be omitted. For example, if your web page
    is at the server path my/index.html, and your model’s JSON file is at my/models/model.json,
    you can use the relative path model/model.json:'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持相对服务器路径。事实上，如果使用相对路径，则可以省略URL的`http://`或`https://`部分。例如，如果你的网页位于服务器路径`my/index.html`，你的模型的JSON文件位于`my/models/model.json`，你可以使用相对路径`model/model.json`：
- en: '[PRE31]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: To specify additional options for the HTTP/HTTPS requests, the `tf.io.browserHTTPRequest()`
    method should be used in lieu of the string argument. For example, to include
    credentials and headers during model loading, you can do something like
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 若要为HTTP/HTTPS请求指定额外选项，应该使用`tf.io.browserHTTPRequest()`方法代替字符串参数。例如，在模型加载过程中包含凭据和标头，你可以这样做：
- en: '[PRE32]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '4.4\. Spoken-word recognition: Applying convnets on audio data'
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4\. 语音识别：在音频数据上应用卷积网络
- en: So far, we have shown you how to use convnets to perform computer-vision tasks.
    But human perception is not just vision. Audio is an important modality of perceptual
    data and is accessible via browser APIs. How to recognize the content and meaning
    of speech and other kinds of sounds? Remarkably, convnets not only work for computer
    vision, but also help audio-related machine learning in a significant way.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经向您展示了如何使用卷积网络执行计算机视觉任务。但是人类的感知不仅仅是视觉。音频是感知数据的一个重要模态，并且可以通过浏览器 API
    进行访问。如何识别语音和其他类型声音的内容和意义？值得注意的是，卷积网络不仅适用于计算机视觉，而且在音频相关的机器学习中也以显著的方式发挥作用。
- en: In this section, you will see how we can solve a relatively simple audio task
    with a convnet similar to the one we built for MNIST. The task is to classify
    short snippets of speech recordings into 20 or so word categories. This task is
    simpler than the kind of speech recognition you may see in devices such as Amazon
    Echo and Google Home. In particular, those speech-recognition systems involve
    larger vocabularies than the one used in this example. Also, they process connected
    speech consisting of multiple words spoken in succession, whereas our example
    deals with words spoken one at a time. Therefore, our example doesn’t qualify
    as a “speech recognizer;” instead, it is more accurately described as a “word
    recognizer” or “speech-command recognizer.” However, our example still has practical
    uses (such as hands-free UIs and accessibility features). Also, the deep-learning
    techniques embodied in this example actually form the basis of more advanced speech-recognition
    systems.^([[9](#ch04fn9)])
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将看到我们如何使用类似于我们为 MNIST 构建的卷积网络来解决一个相对简单的音频任务。该任务是将短语音片段分类到 20 多个单词类别中。这个任务比您可能在亚马逊
    Echo 和 Google Home 等设备中看到的语音识别要简单。特别是，这些语音识别系统涉及比本示例中使用的词汇量更大的词汇。此外，它们处理由多个词连续发音组成的连续语音，而我们的示例处理逐个单词发音。因此，我们的示例不符合“语音识别器”的条件；相反，更准确地描述它为“单词识别器”或“语音命令识别器”。然而，我们的示例仍然具有实际用途（如无需手动操作的用户界面和可访问性功能）。此外，本示例中体现的深度学习技术实际上是更高级语音识别系统的基础。^([[9](#ch04fn9)])
- en: ⁹
  id: totrans-244
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁹
- en: ''
  id: totrans-245
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Ronan Collobert, Christian Puhrsch, and Gabriel Synnaeve, “Wav2Letter: An End-to-End
    ConvNet-based Speech Recognition System,” submitted 13 Sept. 2016, [https://arxiv.org/abs/1609.03193](https://arxiv.org/abs/1609.03193).'
  id: totrans-246
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'Ronan Collobert、Christian Puhrsch 和 Gabriel Synnaeve，“Wav2Letter: 一种基于端到端卷积网络的语音识别系统”，2016
    年 9 月 13 日提交，[https://arxiv.org/abs/1609.03193](https://arxiv.org/abs/1609.03193)。'
- en: '4.4.1\. Spectrograms: Representing sounds as images'
  id: totrans-247
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.1\. 声谱图：将声音表示为图像
- en: As in any deep-learning application, if you want to understand how the model
    works, you need to understand the data first. To understand how audio convnets
    work, we need to first look at how sound is represented as tensors. Recall from
    high school physics that sounds are patterns of air-pressure changes. A microphone
    picks up the air-pressure changes and converts them to electrical signals, which
    can in turn be digitized by a computer’s sound card. Modern web browsers feature
    the *WebAudio* API, which talks to the sound card and provides real-time access
    to the digitized audio signals (with permission granted by the user). So, from
    a JavaScript programmer’s point of view, sounds are arrays of real-valued numbers.
    In deep learning, such arrays of numbers are usually represented as 1D tensors.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何深度学习应用一样，如果您想要理解模型的工作原理，首先需要了解数据。要理解音频卷积网络的工作原理，我们需要首先查看声音是如何表示为张量的。请回忆高中物理课上的知识，声音是空气压力变化的模式。麦克风捕捉到空气压力变化并将其转换为电信号，然后计算机的声卡可以将其数字化。现代
    Web 浏览器提供了*WebAudio* API，它与声卡通信并提供对数字化音频信号的实时访问（在用户授权的情况下）。因此，从 JavaScript 程序员的角度来看，声音就是一组实值数字的数组。在深度学习中，这种数字数组通常表示为
    1D 张量。
- en: You might be wondering, how can the kinds of convnets we have seen so far work
    on 1D tensors? Aren’t they supposed to operate on tensors that are at least 2D?
    The key layers of a convnet, including conv2d and maxPooling2d, exploit spatial
    relations in 2D spaces. It turns out that sounds *can* be represented as special
    types of images called *spectrograms*. Spectrograms not only make it possible
    to apply convnets on sounds but also have theoretical justifications beyond deep
    learning.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，迄今为止我们见过的这种卷积网络是如何在 1D 张量上工作的？它们不是应该操作至少是 2D 的张量吗？卷积网络的关键层，包括 conv2d 和
    maxPooling2d，利用了 2D 空间中的空间关系。事实证明声音*可以*被表示为称为*声谱图*的特殊类型的图像。声谱图不仅使得可以在声音上应用卷积网络，而且在深度学习之外还具有理论上的解释。
- en: As [figure 4.12](#ch04fig12) shows, a spectrogram is a 2D array of numbers,
    which can be shown as grayscale images pretty much in the same way as MNIST images.
    The horizontal dimension is time, and the vertical one is frequency. Each vertical
    slice of a spectrogram is the *spectrum* of the sound inside a short time window.
    A spectrum is a decomposition of a sound into different frequency components,
    which can be roughly understood as different “pitches.” Just as light can be divided
    by a prism into multiple colors, sound can be decomposed by a mathematical operation
    called *Fourier transform* into multiple frequencies. In a nutshell, a spectrogram
    describes how the frequency content of sound changes over a number of successive,
    short time windows (usually on the order of 20 milliseconds).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [图 4.12](#ch04fig12) 所示，频谱图是一个二维数组，可以以与 MNIST 图像基本相同的方式显示为灰度图像。水平维度是时间，垂直维度是频率。频谱图的每个垂直切片是一个短时间窗口内的声音的*频谱*。频谱是将声音分解为不同频率分量的过程，可以粗略地理解为不同的“音高”。就像光可以通过棱镜分解成多种颜色一样，声音可以通过称为*傅里叶变换*的数学操作分解为多个频率。简而言之，频谱图描述了声音的频率内容如何在一系列连续的短时间窗口（通常约为20毫秒）内变化。
- en: Figure 4.12\. Example spectrograms of the isolated spoken words “zero” and “yes.”
    A spectrogram is a joint time-frequency representation of sound. You can think
    of a spectrogram as a sound represented as an image. Each slice along the time
    axis (a column of the image) is a short moment (frame) in time; each slice along
    the frequency axis (a row of the image) corresponds to a particular narrow range
    of frequency (pitch). The value at each pixel of the image represents the relative
    energy of the sound in the given frequency bin at a given moment in time. The
    spectrograms in this figure are rendered such that a darker shade of gray corresponds
    to a higher amount of energy. Different speech sounds have different defining
    features. For example, sibilant consonants like “z” and “s” are characterized
    by a quasi-steady-state energy concentrated at frequencies above 2–3 kHz; vowel
    sounds like “e” and “o” are characterized by horizontal stripes (energy peaks)
    in the low end of the spectrum (< 3 kHz). These energy peaks are called *formants*
    in acoustics. Different vowels have different formant frequencies. All these distinctive
    features of different speech sounds can be used by a deep convnet for word recognition.
  id: totrans-251
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.12\. “zero” 和 “yes” 这两个孤立口语单词的示例频谱图。频谱图是声音的联合时间-频率表示。你可以将频谱图视为声音的图像表示。沿着时间轴的每个切片（图像的一列）都是时间的短时刻（帧）；沿着频率轴的每个切片（图像的一行）对应于特定的窄频率范围（音调）。图像的每个像素的值表示给定时间点上给定频率区段的声音相对能量。本图中的频谱图被渲染为较暗的灰色，对应着较高的能量。不同的语音有不同的特征。例如，类似于“z”和“s”这样的咝音辅音以在2–3
    kHz以上频率处集中的准稳态能量为特征；像“e”和“o”这样的元音以频谱的低端（< 3 kHz）中的水平条纹（能量峰值）为特征。在声学中，这些能量峰值被称为*共振峰*。不同的元音具有不同的共振峰频率。所有这些不同语音的独特特征都可以被深度卷积神经网络用于识别单词。
- en: '![](04fig11_alt.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![](04fig11_alt.jpg)'
- en: 'Spectrograms are a suitable representation of sound for the following reasons.
    First, they save space: the number of float numbers in a spectrogram is usually
    a few times less than the number of float values in the raw waveform. Second,
    in a loose sense, spectrograms correspond to how hearing works in biology. An
    anatomical structure inside the inner ear called the cochlea essentially performs
    the biological version of Fourier transform. It decomposes sounds into different
    frequencies, which are then picked up by different sets of auditory neurons. Third,
    spectrogram representation of speech sounds makes different types of speech sounds
    easier to distinguish from each other. This is shown by the example speech spectrograms
    in [figure 4.12](#ch04fig12): vowels and consonants all have different defining
    patterns in their spectrograms. Decades ago, prior to the wide adoption of machine
    learning, people working on speech recognition actually tried to handcraft rules
    that detect different vowels and consonants from spectrograms. Deep learning saves
    us the trouble and tears of such handcrafting.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 谱图对于以下原因是声音的合适表示。首先，它们节省空间：谱图中的浮点数通常比原始波形中的浮点值少几倍。其次，在宽泛的意义上，谱图对应于生物学中的听力工作原理。内耳内部的一种名为耳蜗的解剖结构实质上执行了傅里叶变换的生物版本。它将声音分解成不同的频率，然后被不同组听觉神经元接收。第三，谱图表示可以更容易地区分不同类型的语音。这在
    [图4.12](#ch04fig12)的示例语音谱图中可以看到：元音和辅音在谱图中都有不同的特征模式。几十年前，在机器学习被广泛应用之前，从谱图中检测不同的元音和辅音的人们实际上尝试手工制作规则。深度学习为我们节省了这种手工制作的麻烦和泪水。
- en: Let’s stop and think for a moment. Looking at the MNIST images in [figure 4.1](#ch04fig01)
    and the speech spectrograms in [figure 4.12](#ch04fig12), you should be able to
    appreciate the similarity between the two datasets. Both datasets contain patterns
    in a 2D feature space, which a pair of trained eyes should be able to distinguish.
    Both datasets show some randomness in the detailed location, size, and details
    of the features. Finally, both are multicategory classification tasks. While MNIST
    contains 10 possible classes, our speech-commands dataset contains 20 (the 10
    digits from 0 to 9, “up,” “down,” “left,” “right,” “go,” “stop,” “yes,” and “no,”
    in addition to the category of “unknown” words and background noise). It is exactly
    these similarities in the essence of the datasets that make convnets suitable
    for the speech-command-recognition task.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们停下来思考一下。看一看 [图4.1](#ch04fig01)中的MNIST图片和 [图4.12](#ch04fig12)中的声音谱图，你应该能够理解这两个数据集之间的相似之处。两个数据集都包含在二维特征空间中的模式，一双经过训练的眼睛应该能够区分出来。两个数据集都在特征的具体位置、大小和细节上呈现一定的随机性。最后，两个数据集都是多类别分类任务。虽然MNIST有10个可能的类别，我们的声音命令数据集有20个类别（从0到9的10个数字，“上”，“下”，“左”，“右”，“前进”，“停止”，“是”，“否”，以及“未知”词和背景噪音的类别）。正是这些数据集本质上的相似性使得卷积神经网络非常适用于声音命令识别任务。
- en: But there are also some noticeable differences between the two datasets. First,
    the audio recordings in the speech-command dataset are somewhat noisy, as you
    can see from the speckles of dark pixels that don’t belong to the speech sound
    in the example spectrograms in [figure 4.12](#ch04fig12). Second, every spectrogram
    in the speech-command dataset has a size of 43 × 232, which is significantly larger
    compared to the 28 × 28 size of the individual MNIST images. The size of the spectrogram
    is asymmetric between the time and frequency dimensions. These differences will
    be reflected by the convnet we will use on the audio dataset.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 但是这两个数据集也有一些显著的区别。首先，声音命令数据集中的音频录音有些噪音，可以从 [图4.12](#ch04fig12)中的示例谱图中看到不属于语音声音的黑色像素点。其次，声音命令数据集中的每个谱图的尺寸为43×232，与单个MNIST图像的28×28大小相比显著较大。谱图的尺寸在时间和频率维度之间是不对称的。这些差异将体现在我们将在音频数据集上使用的卷积神经网络中。
- en: 'The code that defines and trains the speech-commands convnet lives in the tfjs-models
    repo. You can access the code with the following commands:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 定义和训练声音命令卷积神经网络的代码位于tfjs-models存储库中。您可以使用以下命令访问代码：
- en: '[PRE33]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The creation and compilation of the model is encapsulated in the `createModel()`
    function in model.ts.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的创建和编译封装在model.ts中的`createModel()`函数中。
- en: Listing 4.8\. Convnet for classifying spectrograms of speech commands
  id: totrans-259
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**4.8章节的声音命令谱图分类的卷积神经网络**'
- en: '[PRE34]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '***1*** Repeating motifs of conv2d+maxPooling2d'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** conv2d+maxPooling2d 的重复模式'
- en: '***2*** Multilayer perceptron begins'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 多层感知器开始'
- en: '***3*** Uses dropout to reduce overfitting'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 使用 dropout 减少过拟合'
- en: '***4*** Configures loss and metric for multicategory classification'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 配置多类别分类的损失和指标'
- en: The topology of our audio convnet looks a lot like the MNIST convnets. The sequential
    model begins with several repeating motifs of conv2d layers paired with maxPooling2d
    layers. The convolution-pooling part of the model ends at a flatten layer, on
    top of which an MLP is added. The MLP has two dense layers. The hidden dense layer
    has a relu activation, and the final (output) one has a softmax activation that
    suits the classification task. The model is compiled to use `categoricalCrossentropy`
    as the loss function and emit the accuracy metric during training and evaluation.
    This is exactly the same as the MNIST convnets because both datasets involve multicategory
    classification. The audio convnet also shows some interesting differences from
    the MNIST one. In particular, the `kernelSize` properties of the conv2d layers
    are rectangular (for instance, `[2, 8]`) instead of square-shaped. These values
    are selected to match the nonsquare shape of the spectrograms, which have a larger
    frequency dimension than the temporal dimension.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的音频卷积网络的拓扑结构看起来很像 MNIST 卷积网络。顺序模型以多个重复的 conv2d 层与 maxPooling2d 层组合开始。模型的卷积
    - 池化部分在一个展平层结束，在其上添加了 MLP。MLP 有两个密集层。隐藏的密集层具有 relu 激活，最终（输出）层具有适合分类任务的 softmax
    激活。模型编译为在训练和评估期间使用 `categoricalCrossentropy` 作为损失函数并发出准确度指标。这与 MNIST 卷积网络完全相同，因为两个数据集都涉及多类别分类。音频卷积网络还显示出与
    MNIST 不同的一些有趣之处。特别是，conv2d 层的 `kernelSize` 属性是矩形的（例如，`[2, 8]`）而不是方形的。这些值被选择为与频谱图的非方形形状匹配，该频谱图的频率维度比时间维度大。
- en: 'To train the model, you need to download the speech-command dataset first.
    The dataset originated from the speech-commands dataset collected by Pete Warden,
    an engineer on the Google Brain team (see [www.tensorflow.org/tutorials/sequences/audio_recognition](http://www.tensorflow.org/tutorials/sequences/audio_recognition)).
    It has been converted to a browser-specific spectrogram format:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练模型，首先需要下载语音命令数据集。该数据集源自谷歌 Brain 团队工程师 Pete Warden 收集的语音命令数据集（请参阅 [www.tensorflow.org/tutorials/sequences/audio_recognition](http://www.tensorflow.org/tutorials/sequences/audio_recognition)）。它已经转换为浏览器特定的频谱图格式：
- en: '[PRE35]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'These commands will download and extract the browser version of the speech-command
    dataset. Once the data has been extracted, you can kick off the training process
    with this command:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 这些命令将下载并提取语音命令数据集的浏览器版本。一旦数据被提取，您就可以使用以下命令启动训练过程：
- en: '[PRE36]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The first argument to the `yarn train` command points at the location of the
    training data. The following arguments specify the path at which the model’s JSON
    file will be saved, together with the weight file and the metadata JSON file.
    Just like when we trained the enhanced MNIST convnet, the training of the audio
    convnet happens in tfjs-node, with the potential to utilize GPUs. Because the
    sizes of the dataset and the model are larger than the MNIST convnet, the training
    will take longer (on the order of a few hours). You can get a significant speedup
    of the training if you have a CUDA GPU and change the command slightly to use
    tfjs-node-gpu instead of the default tfjs-node (which runs on a CPU only). To
    do that, just add the flag `--gpu` to the previous command:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '`yarn train` 命令的第一个参数指向训练数据的位置。以下参数指定了模型的 JSON 文件将保存的路径，以及权重文件和元数据 JSON 文件的路径。就像我们训练增强的
    MNIST 卷积网络时一样，音频卷积网络的训练也发生在 tfjs-node 中，有可能利用 GPU。由于数据集和模型的大小都比 MNIST 卷积网络大，训练时间会更长（大约几个小时）。如果您有
    CUDA GPU 并且稍微更改命令以使用 tfjs-node-gpu 而不是默认的 tfjs-node（仅在 CPU 上运行），您可以显著加快训练速度。要做到这一点，只需在上一个命令中添加标志
    `--gpu`：'
- en: '[PRE37]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: When the training ends, the model should achieve a final evaluation (test) accuracy
    of approximately 94%.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练结束时，模型应该达到约 94% 的最终评估（测试）准确率。
- en: The trained model is saved at the path specified by the flag in the previous
    command. Like the MNIST convnet we trained with tfjs-node, the saved model can
    be loaded in the browser for serving. However, you need to be familiar with the
    WebAudio API to be able to acquire data from the microphone and preprocess it
    into a format that can be used by the model. For your convenience, we wrote a
    wrapper class that not only loads a trained audio convnet, but also takes care
    of the data ingestion and preprocessing. If you are interested in the mechanisms
    of the audio data input pipeline, you can study the underlying code in the tfjs-model
    Git repository, in the folder speech-commands/src. The wrapper is available via
    npm under the name @tensorflow-models/speech-commands. [Listing 4.9](#ch04ex09)
    shows a minimal example of how the wrapper class can be used to perform online
    recognition of speech-command words in the browser.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过的模型保存在上一个命令中指定的路径中。与我们用 tfjs-node 训练的 MNIST 卷积网络一样，保存的模型可以在浏览器中加载以提供服务。然而，您需要熟悉
    WebAudio API，以便能够从麦克风获取数据并将其预处理为模型可用的格式。为了方便起见，我们编写了一个封装类，不仅可以加载经过训练的音频卷积网络，还可以处理数据输入和预处理。如果您对音频数据输入流水线的机制感兴趣，可以在
    tfjs-model Git 仓库中的 speech-commands/src 文件夹中研究底层代码。这个封装类可以通过 npm 的 @tensorflow-models/speech-commands
    名称使用。[Listing 4.9](#ch04ex09)展示了如何使用封装类在浏览器中进行在线语音命令识别的最小示例。
- en: 'In the speech-commands/demo folder of the tfjs-models repo, you can find a
    less barebones example of how to use the package. To clone and run the demo, run
    the following commands under the speech-commands directory:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在 tfjs-models 仓库的 speech-commands/demo 文件夹中，您可以找到一个相对完整的示例，该示例展示了如何使用该软件包。要克隆并运行该演示，请在
    speech-commands 目录下运行以下命令：
- en: '[PRE38]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The `yarn watch` command will automatically open a new tab in your default web
    browser. In order to see the speech-command recognizer in action, make sure your
    machine has a microphone ready (which most laptops do). Each time a word within
    the vocabulary is recognized, it will be displayed on the screen along with the
    one-second-long spectrogram that contains the word. So, this is browser-based,
    single-word recognition in action, powered by the WebAudio API and a deep convnet.
    Surely it doesn’t have the ability to recognize connected speech with grammar?
    That will require help from other types of neural network building blocks capable
    of processing sequential information. We will visit those in [chapter 8](kindle_split_020.html#ch08).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '`yarn watch` 命令将自动在默认的网页浏览器中打开一个新的标签页。要看到语音命令识别器的实际效果，请确保您的计算机已准备好麦克风（大多数笔记本电脑都有）。每次识别到词汇表中的一个单词时，屏幕上将显示该单词以及包含该单词的一秒钟的频谱图。所以，这是基于浏览器的单词识别，由
    WebAudio API 和深度卷积网络驱动。当然，它没有能力识别带有语法的连接语音？这将需要其他类型的能够处理序列信息的神经网络模块的帮助。我们将在[第8章](kindle_split_020.html#ch08)中介绍这些模块。'
- en: Listing 4.9\. Example usage of the @tensorflow-models/speech-commands module
  id: totrans-277
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Listing 4.9\. @tensorflow-models/speech-commands 模块的示例用法
- en: '[PRE39]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '***1*** Imports the speech-commands module. Make sure it is listed as a dependency
    in package.json.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 导入 speech-commands 模块。确保它在 package.json 中列为依赖项。'
- en: '***2*** Creates an instance of the speech-command recognizer that uses the
    browser’s built-in fast Fourier transform (FFT)'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 创建一个使用浏览器内置快速傅里叶变换（FFT）的语音命令识别器实例'
- en: '***3*** You can examine what word labels (including the “background-noise”
    and “unknown” labels) the model is capable of recognizing.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 您可以查看模型能够识别的单词标签（包括“background-noise”和“unknown”标签）。'
- en: '***4*** Starts online streaming recognition. The first argument is a callback,
    which will be invoked anytime a non-background-noise, non-unknown word is recognized
    with a probability above the threshold (0.75 in this case).'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 启动在线流式识别。第一个参数是一个回调函数，每当识别到概率超过阈值（本例为0.75）的非背景噪声、非未知单词时，都会调用该函数。'
- en: '***5*** result.scores contains the probability scores that correspond to recognizer.wordLabels().'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** result.scores 包含与 recognizer.wordLabels() 对应的概率得分。'
- en: '***6*** Finds the index of the word with the highest score'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 找到具有最高得分的单词的索引'
- en: '***7*** Stops the online streaming recognition in 10 seconds'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7*** 在10秒内停止在线流式识别'
- en: Exercises
  id: totrans-286
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习
- en: 'The convnet for classifying MNIST images in the browser ([listing 4.1](#ch04ex01))
    has two groups of conv2d and maxPooling2d layers. Modify the code to reduce the
    number to only one group. Answer the following questions:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用于在浏览器中对 MNIST 图像进行分类的卷积网络（[listing 4.1](#ch04ex01)）有两组 conv2d 和 maxPooling2d
    层。修改代码，将该数量减少到只有一组。回答以下问题：
- en: How does that affect the total number of trainable parameters of the convnet?
  id: totrans-288
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这会影响卷积网络可训练参数的总数吗？
- en: How does that affect the training speed?
  id: totrans-289
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这会影响训练速度吗？
- en: How does that affect the final accuracy obtained by the convnet after training?
  id: totrans-290
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这会影响训练后卷积网络获得的最终准确率吗？
- en: This exercise is similar to exercise 1\. But instead of playing with the number
    of conv2d-maxPooling2d layer groups, experiment with the number of dense layers
    in the MLP part of the convnet in [listing 4.1](#ch04ex01). How do the total number
    of parameters, training speed, and final accuracy change if you remove the first
    dense layer and keep only the second (output) one?
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '这个练习与练习1相似。但是，与其调整conv2d-maxPooling2d层组的数量，不如在卷积网络的MLP部分中尝试调整密集层的数量。如果去除第一个密集层，只保留第二个（输出）层，总参数数量、训练速度和最终准确率会发生什么变化，见[列表
    4.1](#ch04ex01)。 '
- en: Remove dropout from the convnet in mnist-node ([listing 4.5](#ch04ex05)), and
    see what happens to the training process and the final test accuracy. Why does
    that happen? What does that show?
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从mnist-node中的卷积网络（[列表 4.5](#ch04ex05)）中移除dropout，并观察训练过程和最终测试准确率的变化。为什么会发生这种情况？这说明了什么？
- en: 'As practice using the `tf.browser.fromPixels()` method to pull image data from
    image- and video-related elements of a web page, try the following:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`tf.browser.fromPixels()`方法练习从网页中图像和视频相关元素中提取图像数据，尝试以下操作：
- en: Use `tf.browser.fromPixels()` to get a tensor representing a color JPG image
    by using an `img` tag.
  id: totrans-294
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`tf.browser.fromPixels()`通过`img`标签获取表示彩色JPG图像的张量。
- en: What are the height and width of the image tensor returned by `tf.browser.fromPixels()`?
    What determines the height and width?
  id: totrans-295
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tf.browser.fromPixels()`返回的图像张量的高度和宽度是多少？是什么决定了高度和宽度？'
- en: Resize the image to a fixed dimension of 100 × 100 (height × width) using `tf.image.resizeBilinear().`
  id: totrans-296
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`tf.image.resizeBilinear()`将图像调整为固定尺寸100 × 100（高 × 宽）。
- en: Repeat the previous step, but using the alternative resizing function `tf.image.resizeNearestNeighbor()`
    instead. Can you spot any differences between the results of these two resizing
    functions?
  id: totrans-297
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复上一步，但使用替代的调整大小函数`tf.image.resizeNearestNeighbor()`。你能发现这两种调整大小函数的结果之间有什么区别吗？
- en: Create an HTML canvas and draw some arbitrary shapes in it using functions such
    as `rect()`. Or, if you wish, you can use more advanced libraries such as d3.js
    or three.js to draw more complicated 2D and 3D shapes in it. Then, get the image
    tensor data from the canvas using `tf.browser.fromPixels()`.
  id: totrans-298
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个HTML画布并在其中绘制一些任意形状，例如使用`rect()`函数。或者，如果愿意，也可以使用更先进的库，如d3.js或three.js，在其中绘制更复杂的2D和3D形状。然后，使用`tf.browser.fromPixels()`从画布获取图像张量数据。
- en: Summary
  id: totrans-299
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Convnets extract 2D spatial features from input images with a hierarchy of stacked
    conv2d and maxPooling2d layers.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积网络通过堆叠的conv2d和maxPooling2d层的层次结构从输入图像中提取2D空间特征。
- en: conv2d layers are multichannel, tunable spatial filters. They have the properties
    of locality and parameter sharing, which make them powerful feature extractors
    and efficient representational transforms.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: conv2d层是多通道、可调节的空间滤波器。它们具有局部性和参数共享的特性，使它们成为强大的特征提取器和高效的表示转换器。
- en: maxPooling2d layers reduce the size of the input image tensor by calculating
    the maximum within fixed-size windows, achieving better positional invariance.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: maxPooling2d层通过在固定大小的窗口内计算最大值来减小输入图像张量的大小，从而实现更好的位置不变性。
- en: The conv2d-maxPooling2d “tower” of a convnet usually ends in a flatten layer,
    which is followed by an MLP made of dense layers for classification or regression
    tasks.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积网络的conv2d-maxPooling2d“塔”通常以一个flatten层结束，其后是由密集层组成的MLP，用于分类或回归任务。
- en: With its restricted resources, the browser is only suitable for training small
    models. To train larger models, it is recommended you use tfjs-node, the Node.js
    version of TensorFlow.js; tfjs-node can use the same CPU- and GPU-parallelized
    kernels used by the Python version of TensorFlow.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 受限于其资源，浏览器只适用于训练小型模型。要训练更大的模型，建议使用tfjs-node，即TensorFlow.js的Node.js版本；tfjs-node可以使用与Python版本的TensorFlow相同的CPU和GPU并行化内核。
- en: With greater model capacities comes greater risks of overfitting. Overfitting
    can be ameliorated by adding dropout layers in a convnet. Dropout layers randomly
    zero-out a given fraction of the input elements during training.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型容量增加会增加过拟合的风险。通过在卷积网络中添加dropout层可以缓解过拟合。在训练期间，dropout层会随机将给定比例的输入元素归零。
- en: Convnets are useful not only for computer vision tasks. When audio signals are
    represented as spectrograms, convnets can be applied on them to achieve good classification
    accuracies as well.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Convnets 不仅对计算机视觉任务有用。当音频信号被表示为频谱图时，卷积神经网络也可以应用于它们，以实现良好的分类准确性。
