- en: 4 More exploratory data analysis and data preparation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 更多的探索性数据分析和数据准备
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖内容
- en: Analyzing summary statistics of the DC taxi data set
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析华盛顿特区出租车数据集的摘要统计信息
- en: Evaluating alternative data set sizes for machine learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估用于机器学习的替代数据集大小
- en: Using statistical measures to choose the right machine learning data set size
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用统计量选择合适的机器学习数据集大小
- en: Implementing data set sampling in a PySpark job
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 PySpark 作业中实现数据集抽样
- en: In the last chapter, you started with the analysis of the DC taxi fare data
    set. After the data set was converted to an analysis-friendly Apache Parquet format,
    you crawled the data schema and used the Athena interactive querying service to
    explore the data. These first steps of data exploration surfaced numerous data
    quality issues, motivating you to establish a rigorous approach to deal with the
    garbage in, garbage out problem in your machine learning project. Next, you learned
    about the VACUUM principles for data quality along with several case studies illustrating
    the real-world relevance of the principles. Finally, you applied VACUUM to the
    DC taxi data set to “clean” it and prepare a data set of sufficient quality to
    proceed with sampling from the data set for machine learning.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，您开始分析了华盛顿特区出租车费用数据集。在将数据集转换为适合分析的 Apache Parquet 格式后，您检查了数据架构，并使用 Athena
    交互式查询服务来探索数据。数据探索的初步步骤揭示了许多数据质量问题，促使您建立严谨的方法来解决机器学习项目中的垃圾进、垃圾出问题。接下来，您了解了用于数据质量的
    VACUUM 原则，并通过几个案例研究说明了这些原则的现实相关性。最后，您对华盛顿特区出租车数据集应用了 VACUUM 进行了“清洁”，准备了一个足够质量的数据集，以便从中进行机器学习的抽样。
- en: This chapter picks up with using the VACUUM-ed data set for a more in-depth
    data exploration. In this chapter, you will analyze the summary statistics (arithmetic
    means, standard deviations, and more) of the data set in order to make a better-informed
    decision about the sizes of the training, validation, and test data sets for machine
    learning. You’ll compare common approaches for the selection of the data set sizes
    (e.g., using a 70/15/15% split) to an approach that takes the statistics of the
    data sets into account to pick the right size. You will learn about using statistical
    measures such as the standard error of the mean, z-scores, and p-values to help
    you evaluate alternative data set sizes and how to implement data-driven experiments
    for selecting the right size using PySpark.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章继续使用经过 VACUUM 处理的数据集进行更深入的数据探索。在本章中，您将分析数据集的摘要统计信息（算术平均值、标准差等等），以便更明智地确定用于机器学习的训练、验证和测试数据集的大小。您将比较常见的数据集大小选择方法（例如，使用
    70/15/15% 的划分）和根据数据集统计信息选择合适大小的方法。您将了解如何使用统计量，如均值标准误、Z 分数和 P 值，来帮助评估替代数据集大小，并学习如何使用
    PySpark 实现基于数据的实验来选择合适的大小。
- en: 4.1 Getting started with data sampling
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 数据采样入门
- en: This section introduces you to a more rigorous, data-driven, and reusable approach
    for choosing the right training, validation, and test data set split sizes for
    your data set. Using examples from the DC taxi data, you are going to explore
    the key statistical measures important for choosing the right data set size and
    then implement a PySpark job using a data set size-selection approach that can
    be re-used for other data sets.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本节向您介绍了一种更严谨、基于数据驱动且可重复使用的方法，用于选择适合您数据集的正确训练、验证和测试数据集分割大小。利用华盛顿特区出租车数据的示例，您将探索选择正确数据集大小所需的关键统计量，然后使用一个可以重复使用于其他数据集的数据集大小选择方法来实现一个
    PySpark 作业。
- en: One of the most common questions I hear from junior machine learning practitioners
    is about the selection of the data set size for training, validation, and test
    data sets. This should not come as a surprise since online courses, blogs, and
    machine learning tutorials often use numbers like 70/15/15%, meaning that 70%
    of the project data set should be allocated to training, 15% to validation, and
    15% to held-out test data. Some courses argue for 80/10/10% splits or 98/1/1%
    for “big data” data sets. The well-known Netflix Prize used a roughly 97.3/1.35/1.35%
    split for a data set on the order of 100 million records, but with less than 1
    GB in volume, should it qualify as “big data?”
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我经常听到初级机器学习从业者提出的一个最常见的问题是关于训练、验证和测试数据集的数据集大小的选择。这应该不足为奇，因为在线课程、博客和机器学习教程经常使用像
    70/15/15% 这样的数字，意味着项目数据集的 70% 应该分配给训练，15% 分配给验证，15% 分配给留出测试数据。一些课程主张使用 80/10/10%
    的分割或 98/1/1% 的“大数据”数据集。著名的 Netflix Prize 使用了大约 97.3/1.35/1.35% 的分割来处理大约 1 亿条记录的数据集，但体积不到
    1 GB，它应该被视为“大数据”吗？
- en: 4.1.1 Exploring the summary statistics of the cleaned-up data set
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.1 探索清理后数据集的汇总统计
- en: In this section, you are going to load the cleaned-up data set metadata as a
    pandas DataFrame and explore the summary statistics (including counts, arithmetic
    means, standard deviations, and more) of the data set.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将将清理后的数据集元数据加载为 pandas DataFrame 并探索数据集的汇总统计（包括计数、算术平均数、标准差等）。
- en: 'At the conclusion of chapter 3, along with the cleaned-up version of the data
    set, the dctaxi_parquet_vacuum.py PySpark job used a save_stats_metadata function
    to save some metadata information with the statistical description for the data
    set, including the total number of rows, means, standard deviations, minimums,
    and maximums for every column of values. To read this information into a pandas
    DataFrame named df, execute the following code:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 3 章结束时，除了清理后的数据集之外，dctaxi_parquet_vacuum.py PySpark 作业使用 save_stats_metadata
    函数保存了一些带有数据集统计描述的元数据信息，包括每列值的总行数、均值、标准差、最小值和最大值。要将此信息读入名为 df 的 pandas DataFrame
    中，请执行以下代码：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Install the Python packages needed by pandas to read from S3.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 安装 pandas 读取 S3 所需的 Python 包。
- en: ❷ Read the metadata into a pandas DataFrame.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将元数据读入 pandas DataFrame。
- en: The code installs the s3fs library in your environment to access data from S3
    using pandas read_csv API. The rest of the code lists the objects from the parquet/vacuum/.meta/stats/*
    subfolder of your S3 bucket and reads the contents of the CSV file from the folder
    into a pandas DataFrame.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码在您的环境中安装了 s3fs 库，以使用 pandas read_csv API 访问来自 S3 的数据。代码的其余部分列出了 S3 存储桶的 parquet/vacuum/.meta/stats/*
    子文件夹中的对象，并从该文件夹中的 CSV 文件读取内容到 pandas DataFrame 中。
- en: The output of the info method of the data frame reports the schema of the data
    stored as well as the amount of memory consumed by the data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 数据帧的 info 方法的输出报告存储的数据的模式以及数据消耗的内存量。
- en: Listing 4.1 Output of df.info() for dctaxi_parquet_vacuum.py metadata
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 4.1 dctaxi_parquet_vacuum.py 元数据的 df.info() 输出
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Note that the schema in listing 4.1 aligns with the schema used by SQL queries
    from chapter 2, with a few minor changes: instead of DOUBLE, the data frame uses
    float64 and object instead of STRING. Also, there is a new summary column that
    did not exist in the DC taxi data set in S3\. The summary column was created by
    the describe method of the dctaxi_parquet_vacuum.py PySpark job from chapter 3
    and is used to store the name of the statistical functions, such as mean and count,
    for each row in the metadata table.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，清单 4.1 中的模式与第 2 章中的 SQL 查询使用的模式保持一致，只有一些小的变化：数据帧使用 float64 而不是 DOUBLE，并且使用
    object 代替 STRING。此外，DC 出租车数据集中没有 summary 列。summary 列是通过第 3 章的 dctaxi_parquet_vacuum.py
    PySpark 作业的 describe 方法创建的，并用于存储每行在元数据表中的统计函数的名称，如平均值和计数。
- en: To get started, you can index the data frame using the summary column and look
    at the result
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，您可以使用 summary 列索引数据帧并查看结果
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: which produces
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生
- en: '![04_table_4-1color](Images/04_table_4-1color.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![04_table_4-1color](Images/04_table_4-1color.png)'
- en: 'Let’s save the size of the data set (i.e., in terms of the number of the values
    per column) to a separate variable ds_size, which is going to be used later:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将数据集的大小（即每列的值的数量）保存到一个单独的变量 ds_size 中，稍后将使用它：
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Once executed, this prints 14262196.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 执行后，这将打印 14262196。
- en: The code to obtain the size of the data set relies on the max method to find
    the largest count of values across all the columns in the data set. In the case
    of the cleaned-up DC taxi data set, all columns return an identical count because
    none contain NULL, None, or NaN values. Although for the DC taxi data set max
    is unnecessary, it is good practice to keep using the function to correctly count
    the largest number of rows needed to store the data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 用于获取数据集大小的代码依赖于max方法，在数据集的所有列中找到最大值。对于经过清理的DC出租车数据集，所有列都返回相同的计数，因为它们都不包含NULL、None或NaN值。尽管对于DC出租车数据集来说max是不必要的，但继续使用该函数来正确计算存储数据所需的最大行数是一个好的做法。
- en: Since the upcoming part of the chapter is focused on sampling from the data,
    create two separate series to gather the data set mean (mu)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 由于接下来的章节将重点涉及从数据中采样，所以创建两个单独的序列来收集数据集的均值（mu）
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: which should output
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: and the standard deviation (sigma) statistics
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 和标准差（sigma）统计数据
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'which print the following:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 打印如下所示：
- en: '[PRE7]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 4.1.2 Choosing the right sample size for the test data set
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.2 为测试数据集选择合适的样本大小
- en: In this section, you are going to explore the effectiveness of using machine
    learning “rules of thumb” for choosing data set sizes and decide the appropriate
    sizes for the DC taxi data set. Although this section uses the DC taxi data set
    as an example, you will learn about an approach for choosing the right sizes for
    the training, test, and validation data sets regardless of the actual data set
    you are using.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将探索使用机器学习“经验法则”选择数据集大小的有效性，并决定DC出租车数据集的合适大小。尽管本节以DC出租车数据集为例，但您将学习一种在使用实际数据集时选择正确大小的方法。
- en: Now that you know about the average values of the numeric columns in the cleaned-up
    data set, you are prepared to tackle the question of how many records from the
    data set should be allocated to training your machine learning model and how many
    should be held out for the test and validation data sets. When preparing training,
    validation, and test data sets, many machine learning practitioners rely on rules
    of thumb or heuristics to decide on the sizes. Some argue for an 80/10/10% split
    across training, validation, and test, while others claim that the split should
    be 98/1/1% when the data set is large, without specifying what “large” means.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了清理数据集中数值列的平均值，您准备好回答将数据集中多少记录分配给机器学习模型训练，以及有多少记录保留给测试和验证数据集的问题了。在准备训练、验证和测试数据集时，许多机器学习从业者依赖于经验法则或启发式方法来确定各个数据集的大小。有些人主张使用80/10/10%的训练、验证和测试划分，而其他人则声称当数据集很大时，划分应为98/1/1%，而不指定“大”是什么意思。
- en: When approaching the issues of the number of records to allocate to training,
    validation, and test data sets, it is valuable to recall the basic rationale behind
    them. What makes the selection of the right percentage for the training versus
    test data set difficult is that they are fundamentally in opposition to one another.
    On one hand, the percentage of the data set used for the machine learning model
    training should be as large as possible. On the other hand, the percentage of
    the data set held out for the test should be large enough so that the performance
    of a trained machine learning model on the test data set is a meaningful estimate
    for how the model is expected to perform on unseen samples from the population
    data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理分配给训练、验证和测试数据集的记录数量时，回顾它们的基本原理是有价值的。选择训练数据集和测试数据集的合适百分比之间存在困难的原因是它们本质上是相互对立的。一方面，用于机器学习模型训练的数据集的百分比应尽可能大。另一方面，用于测试的数据集的百分比应足够大，以便训练后的机器学习模型在测试数据集上的性能是对该模型在未知样本中的预期表现的有意义的估计。
- en: Test and validation data sets
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 测试和验证数据集
- en: The test data set described in this book will not be used to check the model
    for overfitting. Although some machine learning literature uses test data sets
    to ensure that the model is generalizing, this book will use a separate validation
    data set for this purpose. The approach used in this book is illustrated in this
    figure.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中描述的测试数据集将不会用于检查模型是否过拟合。虽然一些机器学习文献使用测试数据集来确保模型的泛化，但本书将使用一个单独的验证数据集来实现此目的。本书使用的方法如下图所示。
- en: '![004-UN01](Images/004-UN01.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![004-UN01](Images/004-UN01.png)'
- en: The cleaned-up project data set is split into development and test sets in this
    chapter. Upcoming chapters cover further splits of the development data set into
    training and validation data sets.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中将项目的清理数据集分为开发集和测试集。接下来的章节将涵盖将开发数据集进一步分为训练集和验证数据集。
- en: You can use some basic results from statistics to help you with the choice of
    the size. The idea is to ensure that the test data set is large enough so that
    it is statistically similar to the entire data set.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用统计学的一些基本结果来帮助您选择大小。思路是确保测试数据集足够大，以便在统计上与整个数据集相似。
- en: To start, consider the upper and the lower bounds for the fraction of the data
    set to use for test data. With respect to the upper bound, when using 70% for
    training, you can allocate 15% for validation and test. On the lower bound, you
    may consider allocating as little as 1% for test and validation. To better illustrate
    the idea of a lower bound, let’s consider a more extreme case of allocating just
    0.5% of data for testing.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，考虑用于测试数据的数据集的上限和下限。对于上限，在训练时使用70％，您可以分配15％进行验证和测试。在下限方面，您可以考虑将仅1％用于测试和验证。为更好地说明下限的概念，让我们考虑分配0.5％的数据进行测试的更极端情况。
- en: You can obtain the number of records for various percentages (fractions) using
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下内容获取各种百分比（分数）的记录数：
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: which returns
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 它的返回值为
- en: '[PRE9]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: When working with sample sizes, it helps to put the sizes in terms of powers
    of two. This is helpful for several reasons. When computing statistics from samples
    (e.g., standard error of the mean of samples), you are going to find that exponential
    changes in the size of the data set are needed to achieve linear changes to the
    values of the statistics. Also, in statistical formulas it is common to take square
    roots of the sample size, and starting with powers of two simplifies the calculations.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理样本大小时，将其转化为2的幂次方会很有帮助。这是有帮助的原因有几个。当从样本计算统计学（例如，样本均值的标准误差）时，您会发现需要指数级地改变数据集的大小才能实现统计量的线性变化。此外，在统计公式中，取样本大小的平方根很常见，而从2的幂次方开始会简化计算。
- en: 'To find the power of two estimates for the fractions of the data set, you can
    use the following code:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 要找出数据集分数的二次幂估计值，可以使用以下代码：
- en: '[PRE10]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note that the code takes the base 2 logarithm of the actual number of records
    that correspond to the fractions from 30% to 0.5% of the data set. Since the value
    of the logarithm can be a non-integer value, the floor function returns the data
    set size at the power of two that can store up an approximated fraction of the
    data set.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，该代码以30％到0.5％的近似数据集分数的实际记录数的基2对数为基础。由于对数值可以是非整数值，因此floor函数返回以2的幂次方存储近似数据集分数的数据集大小。
- en: Hence, the output of the code
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，代码的输出为
- en: '[PRE11]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: corresponds to a range from 2^(22) = 4,194,304 to 2^(16) = 65,536.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对应于从2^(22) = 4,194,304到2^(16) = 65,536的范围。
- en: Although data sets in this range can easily fit in the memory of a modern laptop,
    let’s attempt an experiment to identify the smallest fraction of the data set
    that can be sampled and still be used to report an accurate performance metric
    for a machine learning model. The valuable part of the experiment isn’t the finding,
    but rather the illustration of the process for finding the right sample size.
    The process is valuable because it can be repeated even with larger data sets.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管此范围内的数据集可以轻松适应现代笔记本电脑的内存，但让我们尝试进行实验，以确定可以对数据集进行抽样并仍然用于报告机器学习模型准确性能指标的最小数据集。实验的有价值之处不在于发现，而在于说明寻找正确样本大小的过程。该过程有价值，因为即使在更大的数据集中也可以重复。
- en: 'For the experiment, let’s continue using the upper part of the range as the
    largest sample size, 2^(22) = 4,194,304, but start with a much smaller part of
    the range of 2^(15) = 32,768:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实验中，让我们继续使用上部范围作为最大样本大小，2^(22) = 4,194,304，但从范围较小的2^(15) = 32,768开始：
- en: '[PRE12]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The maximum and the minimum values returned by the code are as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 代码返回的最大和最小值如下：
- en: '[PRE13]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Given the range, you can figure out how well it approximates fractions of the
    data set by running
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 给定范围，您可以通过运行以下内容来计算其近似数据集分数的程度：
- en: '[PRE14]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: which results in
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 它的结果为
- en: '[PRE15]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: which shows that a test data set size of 2^(15) covers only about 0.23% of the
    data set while a test data size of 2^(22) covers roughly 29.4%.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 它显示2^(15)的测试数据集大小仅覆盖约0.23%的数据集，而测试数据大小为2^(22)则覆盖约29.4%。
- en: 4.1.3 Exploring the statistics of alternative sample sizes
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.3 探索替代样本大小的统计信息
- en: This section describes how to use a standard error of the mean statistic along
    with diminishing returns (a marginal) to produce candidate sizes (in terms of
    the number of records) for the test data set. In the following listing, the function
    sem_over_range from listing 4.2 computes a pandas DataFrame specifying the standard
    error of the mean (SEM) for every column in the data set and every sample size
    from sample_ size_lower to sample_size_upper. In this example, the range corresponds
    to values from 32,768 to 4,194,304.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了如何使用均值的标准误差统计量以及收益递减（边际）来生成候选大小（以记录数表示）的测试数据集。在下面的清单中，清单4.2中的sem_over_range函数计算了一个pandas
    DataFrame，该DataFrame指定了数据集中每一列和每个样本大小从sample_size_lower到sample_size_upper的标准误差（SEM）。在本例中，范围对应于从32,768到4,194,304的值。
- en: Listing 4.2 SEM for every column across candidate sample sizes
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对每个候选样本大小的每列进行SEM（标准误差）。
- en: '[PRE16]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ The sem_over_range function uses the sample ranges along with the data sets
    mu and sigma.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ sem_over_range函数使用样本范围以及数据集的mu和sigma。
- en: ❷ Convert the sample ranges to a pandas Series.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将样本范围转换为pandas Series。
- en: ❸ Create a pandas DataFrame by computing standard error of the mean for each
    sample size and column sigma.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 通过计算每个样本大小和列σ的平均值标准误差来创建一个pandas DataFrame。
- en: The function sem_over_range from listing 4.2 computes a pandas DataFrame specifying
    the standard error of the mean (SEM) for every column in the data set and every
    sample size from sample_size_lower to sample_size_upper. In this example, the
    range corresponds to values from 32,768 to 4,194,304.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 清单4.2中的sem_over_range函数计算了一个pandas DataFrame，该DataFrame指定了数据集中每一列和每个样本大小从sample_size_lower到sample_size_upper的标准误差（SEM）。在本例中，范围对应于从32,768到4,194,304的值。
- en: Recall that for any of the columns in the data set, given its population standard
    deviation (σ) and the number of records (observations) in the column (*n*), the
    SEM is defined as ![004-UN01_EQ01](Images/004-UN01_EQ01.png).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，对于数据集中的任何一列，给定其总体标准差（σ）和列中的记录数（观测值）（*n*），SEM定义为![004-UN01_EQ01](Images/004-UN01_EQ01.png)。
- en: Since the raw SEM values returned in the sem_df DataFrame from listing 4.2 are
    not easily interpretable, it is valuable to plot a graph to illustrate the overall
    trend of change in the SEM as the sample size grows. You can display this trend
    using the matplotlib library, plotting the average SEM values in the sem_df data
    frame across columns using
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 由于清单4.2中的sem_df DataFrame返回的原始SEM值不易解释，因此绘制图形以说明随着样本大小增长SEM的总体变化趋势是有价值的。您可以使用matplotlib库显示此趋势，绘制sem_df数据框中各列的平均SEM值，如下所示
- en: '[PRE17]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: which results in figure 4.1.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致图4.1。
- en: '![04-01](Images/04-01.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![04-01](Images/04-01.png)'
- en: 'Figure 4.1 Exponential increase in sample size is expensive: larger samples
    require exponentially more memory, disk space, and compute while yielding less
    improvement in terms of the reduction in the standard error of the mean.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1 样本大小呈指数增长是昂贵的：更大的样本需要指数级的内存、磁盘空间和计算量，同时在标准误差减小方面产生的改进较少。
- en: The plot in figure 4.1 uses powers of two for the annotations on the horizontal
    axis to describe the sample sizes in the data frame. Notice that the plot captures
    the trend of diminishing returns on the increase in the sample size. Although
    the sample size grows exponentially, the slope (the instantaneous rate of change
    in the SEM given a sample size) of the average SEM flattens with every doubling.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1中的绘图使用二的幂次方作为水平轴上的注释，描述数据框中的样本大小。请注意，该图捕获了样本大小增加时的收益递减趋势。尽管样本大小呈指数增长，但平均SEM的斜率（给定样本大小的SEM的瞬时变化率）随着每倍增长而趋于平缓。
- en: Since it is valuable to allocate as much data as possible to the training data
    set, you can take advantage of the diminishing returns heuristic to discover a
    lower bound size for the test data set. The idea is to find a sample size such
    that if it were any larger, the improvements to the SEM would yield diminishing
    returns.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 由于将尽可能多的数据分配给训练数据集是有价值的，您可以利用收益递减启发式方法发现测试数据集的下限大小。思路是找到一个样本大小，以便如果它更大，那么SEM的改善将产生收益递减。
- en: 'To identify the point of diminishing returns (also known as the *marginal*
    ) on the doubling of the sample size, you can start by looking at the total reduction
    in the SEM for each increase in the sample size. This is computed using sem_df.cumsum()
    in the following code snippet. Then, to obtain a single aggregate measure for
    each sample size, the mean(axis = 1) computes the average of the total reduction
    in SEM across the columns in the data set:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 要确定样本大小加倍的边际收益点（也称为 *边际* ），您可以从每次样本大小增加时 SEM 的总减少开始。这是使用以下代码片段中的 sem_df.cumsum()
    计算的。然后，为了获得每个样本大小的单个聚合度量，mean(axis = 1) 计算数据集中列之间 SEM 总减少的平均值：
- en: '[PRE18]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: which produces
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 生成
- en: '[PRE19]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The values of the agg_change pandas series are plotted in figure 4.2\. Notice
    that the sample size highlighted with an arrow corresponds to the sample size
    of 220 and is also the point where the SEM reduction due to increase in the sample
    size begins to yield diminishing returns.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: agg_change pandas 系列的值在图 4.2 中被绘制出来。请注意，箭头突出显示的样本大小对应于 220 的样本大小，也是由于增加样本大小而导致
    SEM 减少开始产生边际收益的点。
- en: '![04-02](Images/04-02.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![04-02](Images/04-02.png)'
- en: Figure 4.2 The marginal sample size corresponds to the largest sample size before
    the point of diminishing returns.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 边际样本大小对应于边际收益点之前的最大样本大小。
- en: 'This point, the marginal, can be computed in Python using the following marginal
    function:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，边际可以使用以下边际函数在 Python 中计算：
- en: '[PRE20]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ Create a NumPy array of data points with sample sizes on the x-axis and SEM
    values on the y-axis.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个 NumPy 数组，其中数据点在 x 轴上的样本大小，SEM 值在 y 轴上。
- en: ❷ Compute the distances from the data points to an imaginary line connecting
    the largest and the smallest sample size data points.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 计算数据点到连接最大和最小样本大小数据点的虚拟线的距离。
- en: Here, the marginal is computed by looking at data points of sample sizes against
    cumulative reduction in SEM, drawing an imaginary line between the smallest and
    largest sample sizes (dashed line in figure 4.2) and identifying the data point
    with the furthest distance at the right angle to the imaginary line.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，通过查看样本大小的数据点与 SEM 累积减少之间的关系，绘制连接最小和最大样本大小的虚线（图 4.2 中的虚线）并识别与虚拟线右角最远距离的数据点来计算边际。
- en: 'When applied to the DC taxi data set, the marginal function computes the following:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当应用于 DC 出租车数据集时，边际函数计算如下内容：
- en: '[PRE21]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Here, the marginal test sample size chosen by the diminishing returns heuristic
    corresponds to 1,048,576 records, or roughly 7% of the data set.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，通过边际收益启发法选择的边际测试样本大小对应于 1,048,576 条记录，或者大约是数据集的 7%。
- en: If it were possible to use a sample of any 1,048,576 records as a test data
    set, it would be valuable for maximizing the amount of data available to machine
    learning model training. However, the SEM measure is designed to identify a *lower*
    bound for the sample size and does not indicate that an arbitrary data set of
    this size is the right one to use for the test data set.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可能的话，使用任意 1,048,576 条记录的样本作为测试数据集将有助于最大化可用于机器学习模型训练的数据量。然而，SEM 测量旨在确定样本大小的
    *下限* ，并不表示这种大小的任意数据集都适合用作测试数据集。
- en: 'You can use p-values of a sample of 1,048,576 records to establish confidence
    in the sample by answering the fundamental question of statistical hypothesis
    testing: what is the degree of certainty that the sample comes from the population?'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 1,048,576 条记录的 p 值来建立对样本的置信度，从而回答统计假设检验的基本问题：样本来自总体的确信度是多少？
- en: 4.1.4 Using a PySpark job to sample the test set
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.4 使用 PySpark 作业对测试集进行抽样
- en: In this section, you are going to experiment by randomly sampling 1,048,576
    records (the size identified in the previous section) using a PySpark job in order
    to create the test data set. Once the test set is sampled, the remaining records
    are persisted to a separate DC taxi development data set. Both development and
    test data sets are also analyzed to compute p-values as well as other summary
    statistics.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将通过使用 PySpark 作业随机采样 1,048,576 条记录（在上一节中确定的大小）来创建测试数据集进行实验。一旦采样了测试集，剩余的记录将持久保存到一个单独的
    DC 出租车开发数据集中。开发和测试数据集还被分析以计算 p 值以及其他摘要统计信息。
- en: Since the implementation of the entire PySpark job is roughly 90 lines of code,
    in this section the job is introduced as a series of code snippets. The preamble
    to the job, shown in listing 4.3, resembles the PySpark jobs in chapters 2 and
    3\. As in the earlier chapters, this part of the job implementation imports the
    relevant libraries and resolves job arguments.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 由于整个 PySpark 作业的实现大约有 90 行代码，在本节中，作业被介绍为一系列代码片段。作业的前文部分，在列表 4.3 中显示的，类似于第 2
    和第 3 章中的 PySpark 作业。与早期章节一样，作业的这一部分导入相关库并解析作业参数。
- en: Listing 4.3 PySpark DataFrame reading code in dctaxi_dev_test.py
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在 dctaxi_dev_test.py 中的第 4.3 节代码中读取的 PySpark DataFrame。
- en: '[PRE22]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ Construct a pandas DataFrame df based on the BUCKET_SRC_PATH parameter.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 根据 BUCKET_SRC_PATH 参数构建一个 pandas DataFrame df。
- en: The implementation related to sampling of the test set from the cleaned-up DC
    taxi data set begins in listing 4.4 where the sample fraction of the entire data
    set size is computed and saved to the variable sample_frac. In order to compute
    the summary statistics of the cleaned-up data set in PySpark, the implementation
    relies on the Kaen library PySpark utility function spark_df_to_stats_pandas_df,
    which returns a pandas DataFrame from the PySpark DataFrame instance named df.
    The pandas summary_df in turn provides standard pandas DataFrame API access to
    data set averages (mu) and standard deviations (sigma) for each of the columns
    in the cleaned-up data set.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 与从清理后的 DC 出租车数据集中抽样有关的实现始于列表 4.4，其中计算整个数据集大小的样本分数，并将其保存到变量 sample_frac 中。为了在
    PySpark 中计算清理后数据集的摘要统计信息，实现依赖于 Kaen 库的 PySpark 实用函数 spark_df_to_stats_pandas_df，该函数从名为
    df 的 PySpark DataFrame 实例返回 pandas DataFrame。然后，pandas summary_df 提供了对清理后数据集中每列的平均值（mu）和标准差（sigma）的标准
    pandas DataFrame API 访问。
- en: Listing 4.4 PySpark DataFrame reading code in dctaxi_dev_test.py
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在 dctaxi_dev_test.py 中的第 4.4 节代码中读取的 PySpark DataFrame。
- en: '[PRE23]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ The sample size in terms of a fraction as needed for Spark randomSplit method
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 根据 Spark 的 randomSplit 方法所需的样本大小，以分数的形式表示。
- en: ❷ Import Spark and pandas utilities from the kaen package.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 从 kaen 包中导入 Spark 和 pandas 实用工具。
- en: ❸ Create the pandas DataFrame with statistics of the Spark DataFrame.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 创建包含 Spark DataFrame 统计信息的 pandas DataFrame。
- en: ❹ Save the data set mean as mu.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将数据集的平均值保存为 mu。
- en: ❺ Save the data set standard deviation as sigma.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将数据集的标准差保存为 sigma。
- en: The summary statistics along with the sample_frac values are used in listing
    4.5 to perform random sampling. The PySpark randomSplit method partitions the
    cleaned-up DC taxi data set into the test_df consisting of a maximum of SAMPLE_SIZE
    rows and totaling roughly sample_frac of the entire data set from the df data
    frame.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 汇总统计信息以及 sample_frac 值在列表 4.5 中用于执行随机抽样。PySpark 的 randomSplit 方法将经过清理的 DC 出租车数据集分割为
    test_df，其中包含最多 SAMPLE_SIZE 行，并且总计来自 df 数据帧的 sample_frac 的整个数据集。
- en: Listing 4.5 PySpark DataFrame reading code in dctaxi_dev_test.py
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在 dctaxi_dev_test.py 中的第 4.5 节代码中读取的 PySpark DataFrame。
- en: '[PRE24]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ❶ Use the SEED to initialize pseudorandom number generators.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用 SEED 初始化伪随机数生成器。
- en: ❷ Work around a poor choice (p-value < 0.05) for a SEED value using at most
    SAMPLE_COUNT samples.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 通过使用最多 SAMPLE_COUNT 个样本，解决了选择不佳（p 值 < 0.05）的 SEED 值的问题。
- en: ❸ Sample the test data set into Spark test_df DataFrame, the rest to dev_df.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将测试数据集抽样到 Spark 的 test_df DataFrame 中，其余抽样到 dev_df。
- en: ❹ Use the sample_frac fraction of the records in the df for the test data set.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用 df 中记录的 sample_frac 分数作为测试数据集。
- en: ❺ Ensure that the test_df is limited to at most SAMPLE_SIZE records.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 确保 test_df 最多仅包含 SAMPLE_SIZE 条记录。
- en: ❻ Create a pandas test_stats_df DataFrame with summary statistics of test_df.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 创建一个包含 test_df 摘要统计信息的 pandas test_stats_df DataFrame。
- en: ❼ Sample again in case of a poor sample (pvalue < 0.05), up to SAMPLE_COUNT
    times.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 在出现不良样本（p 值 < 0.05）的情况下再次抽样，最多抽样 SAMPLE_COUNT 次。
- en: 'The part of the job implementation shown in listing 4.6 is responsible for
    saving the development (dev_df) and the test (test_df) data sets to S3\. For each
    of the data sets, Spark saves the records in a CSV format to the BUCKET_DST_PATH
    with the header information. Also, for both development and test the implementation
    saves additional metadata (which is shown later in this section) to the BUCKET_DST_PATH
    subfolders: .meta/stats and .meta/shards.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.6 中显示的作业实现部分负责将开发（dev_df）和测试（test_df）数据集保存到 S3。对于每个数据集，Spark 将记录保存为 CSV
    格式，带有标头信息，保存到 BUCKET_DST_PATH 中。此外，对于开发和测试，该实现还将其他元数据（稍后在本节中显示）保存到 BUCKET_DST_PATH
    的子文件夹中：.meta/stats 和 .meta/shards。
- en: The stats subfolder stores a CSV file with the summary statistics, including
    the count, mean, p-values, and others. The shards subfolder is stored to facilitate
    processing of the data set during training and store the metadata about the number
    of CSV part files and the number of records per part file used to save the data
    sets in S3.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: stats 子文件夹存储一个包含摘要统计信息的 CSV 文件，包括计数、均值、p 值等。 shards 子文件夹被存储以便在训练期间处理数据集，并存储关于用于将数据集保存在
    S3 中的 CSV 部分文件数和每个部分文件中的记录数的元数据。
- en: Listing 4.6 PySpark DataFrame reading code in dctaxi_dev_test.py
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.6 dctaxi_dev_test.py 中的 PySpark DataFrame 读取代码
- en: '[PRE25]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: For convenience, the full implementation of the PySpark job as it should be
    persisted in a file named dctaxi_dev_test.py is shown next.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便起见，下面展示了 PySpark 作业的完整实现，它应该被保存在一个名为 dctaxi_dev_test.py 的文件中。
- en: Listing 4.7 PySpark dctaxi_dev_test.py job to sample dev and test data sets
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.7 PySpark dctaxi_dev_test.py 作业以抽样开发和测试数据集
- en: '[PRE26]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Before the PySpark job in the dctaxi_dev_test.py file can be executed in AWS
    Glue, you need to configure several environment variables. The SAMPLE_SIZE and
    SAMPLE_ COUNT operating system environment variables should be set using the values
    of the corresponding Python variables:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在 dctaxi_dev_test.py 文件中执行 PySpark 作业之前，你需要配置几个环境变量。应使用相应 Python 变量的值设置 SAMPLE_SIZE
    和 SAMPLE_COUNT 操作系统环境变量：
- en: '[PRE27]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'As in the previous chapter, the PySpark job is executed using the convenience
    functions from the utils.sh script. Start by downloading the script to your local
    environment using the following command in your bash shell:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 与上一章节类似，PySpark 作业使用 utils.sh 脚本中的便捷函数执行。首先，在你的 bash shell 中使用以下命令下载该脚本到你的本地环境：
- en: '[PRE28]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Once the utils.sh script is downloaded, you can use it to launch and monitor
    the PySpark job implemented in the dctaxi_dev_test.py file. Launch the job by
    running the following in your shell environment:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 utils.sh 脚本被下载，你可以使用它来启动和监视 dctaxi_dev_test.py 文件中实现的 PySpark 作业。在你的 shell
    环境中运行以下命令来启动该作业：
- en: '[PRE29]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Note that the job is going to read the Parquet files saved in chapter 3 from
    the parquet/vacuum subfolder and save the development and test data set under
    the csv/dev and csv/test subfolders in your S3 bucket. This job should take about
    eight minutes to finish on AWS Glue. Assuming it completes successfully, it should
    produce an output resembling the following:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，该作业将要读取第 3 章保存在 parquet/vacuum 子文件夹中的 Parquet 文件，并将开发和测试数据集保存在你的 S3 存储桶的
    csv/dev 和 csv/test 子文件夹下。该作业在 AWS Glue 上应该需要大约八分钟才能完成。假设它成功完成，它应该会产生以下类似的输出：
- en: '[PRE30]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Since the PySpark job persists metadata about the data sets, you can use pandas
    to review the contents of the metadata. To preview the statistical summary of
    the test set, execute the following Python code:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 PySpark 作业保留了关于数据集的元数据，你可以使用 pandas 预览元数据的内容。为了预览测试集的统计摘要，请执行以下 Python 代码：
- en: '[PRE31]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Assuming the PySpark job executed correctly, the printout of the test_stats_df
    for the test data set should resemble the following:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 PySpark 作业执行正确，对于测试数据集的 test_stats_df 的打印输出应该类似于以下内容：
- en: '![04_table_4-2color](Images/04_table_4-2color.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![04_table_4-2color](Images/04_table_4-2color.png)'
- en: The metadata about the CSV part files (shards) of the development data set should
    have been saved to the csv/dev/.meta/shards subfolder of your S3 bucket. If you
    preview this metadata in a pandas DataFrame using the following code
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 关于开发数据集的 CSV 部分文件（shards）的元数据应该已保存到你的 S3 存储桶的 csv/dev/.meta/shards 子文件夹中。如果你使用以下代码预览此元数据中的
    pandas DataFrame
- en: '[PRE32]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'the output should consist of a three-column table, where the id column stores
    the ID of the CSV part file from the csv/dev subfolder in S3 and the corresponding
    entry in the count column specifies the number of the rows in the part file. The
    contents of the data frame should resemble the following:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该包含一个三列表，其中 id 列存储来自 S3 中 csv/dev 子文件夹的 CSV 部分文件的 ID，而 count 列中的相应条目指定了部分文件中的行数。数据框的内容应该类似于以下内容：
- en: '|  | id | count |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '|  | id | count |'
- en: '| 39 | 0 | 165669 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 39 | 0 | 165669 |'
- en: '| 3 | 1 | 165436 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 1 | 165436 |'
- en: '| 56 | 2 | 165754 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 56 | 2 | 165754 |'
- en: '| 53 | 3 | 165530 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 53 | 3 | 165530 |'
- en: '| 63 | 4 | 165365 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 63 | 4 | 165365 |'
- en: '...'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '...'
- en: '| 72 | 75 | 164569 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 72 | 75 | 164569 |'
- en: '| 59 | 76 | 164729 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 59 | 76 | 164729 |'
- en: '| 2 | 77 | 164315 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 77 | 164315 |'
- en: '| 11 | 78 | 164397 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 78 | 164397 |'
- en: '| 22 | 79 | 164406 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 22 | 79 | 164406 |'
- en: Summary
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Using a fixed percentage—based heuristic to pick the size of a held-out test
    data set can waste valuable machine learning model training data.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用固定百分比的启发式方法来选择保留的测试数据集的大小可能会浪费宝贵的机器学习模型训练数据。
- en: Measuring diminishing results from increasing the size of a data set can help
    with choosing the lower bound for the size of the test and validation data sets.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测量数据集大小增加导致递减的结果，有助于选择测试和验证数据集大小的下限。
- en: Ensuring that a test data set has sufficient z-score and p-values can prevent
    choosing a data set size that’s too small for machine learning.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保测试数据集具有足够的z分数和p值，可以防止选择机器学习时数据集大小过小。
- en: Serverless PySpark jobs can be used to evaluate alternative test data sets and
    report on their statistical summaries.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无服务器 PySpark 作业可用于评估替代测试数据集，并报告它们的统计摘要。
