["```py\n>>> def logistic(x, w=1., phase=0, gain=1):\n...    return gain / (1\\. + np.exp(-w * (x - phase)))\n```", "```py\n>>> import pandas as pd\n>>> import numpy as np\n>>> import seaborn as sns\n>>> sns.set_style()\n\n>>> xy = pd.DataFrame(np.arange(-50, 50) / 10., columns=['x'])\n>>> for w, phase in zip([1, 3, 1, 1, .5], [0, 0, 2, -1, 0]):\n...    kwargs = dict(w=w, phase=phase)\n...    xy[f'{kwargs}'] = logistic(xy['x'], **kwargs)\n>>> xy.plot(grid=\"on\", ylabel=\"y\")\n```", "```py\n>>> from collections import Counter\n\n>>> np.random.seed(451)\n>>> tokens = \"green egg egg ham ham ham spam spam spam spam\".split()\n>>> bow = Counter(tokens)\n>>> x = pd.Series(bow)\n>>> x\ngreen    1\negg      2\nham      3\nspam     4\n```", "```py\n>>> x1, x2, x3, x4 = x\n>>> x1, x2, x3, x4\n(1, 2, 3, 4)\n```", "```py\n>>> w0 = np.round(.1 * np.random.randn(), 2)\n>>> w0\n0.07\n>>> w1, w2, w3, w4 = (.1 * np.random.randn(len(x))).round(2)\n>>> w1, w2, w3, w4\n(0.12, -0.16, 0.03, -0.18)\n```", "```py\n>>> x = np.array([1, x1, x2, x3, x4])  # #1\n>>> w = np.array([w0, w1, w2, w3, w4])  # #2\n>>> y = np.sum(w * x)  # #3\n>>> y\n-0.76\n```", "```py\n>>> threshold = 0.0\n>>> y = int(y > threshold)\n```", "```py\n>>> y = logistic(x)\n```", "```py\ny = m * x + b\n```", "```py\ny = slope * x + intercept\n```", "```py\nw0 * 1.0 + w1 * x1 + ... + (x_n * w_n)\n```", "```py\n>>> def neuron(x, w):\n...    z = sum(wi * xi for xi, wi in zip(x, w))  # #1\n...    return z > 0  # #2\n```", "```py\n>>> def neuron(x, w):\n...    z = np.array(wi).dot(w)\n...    return z > 0\n```", "```py\n>>> import pandas as pd\n>>> import numpy as np\n>>> pd.options.display.max_rows = 7\n```", "```py\n>>> np.random.seed(451)\n>>> df = pd.read_csv(  # #1\n...     'https://proai.org/baby-names-us.csv.gz')\n>>> df.to_csv(  # #2\n...     'baby-names-us.csv.gz', compression='gzip')\n>>> df = df.sample(10_000)  # #3\n>>> df.shape\n(10000, 6)\n```", "```py\n>>> df.groupby(['name', 'sex'])['count'].sum()[('Timothy',)]\nsex\nF       5\nM    3538\n```", "```py\n>>> df = df.set_index(['name', 'sex'])\n>>> groups = df.groupby(['name', 'sex'])\n>>> counts = groups['count'].sum()\n>>> counts\nname    sex\nAaden   M      51\nAahana  F      26\nAahil   M       5\n               ..\nZvi     M       5\nZya     F       8\nZylah   F       5\n```", "```py\n>>> from sklearn.feature_extraction.text import TfidfVectorizer\n>>> vectorizer = TfidfVectorizer(\n...     use_idf=False,  # #1\n...     analyzer='char',\n...     ngram_range=(1, 3)  # #2\n...     )\n>>> vectorizer\nTfidfVectorizer(analyzer='char', ngram_range=(1, 3), use_idf=False)\n```", "```py\n>>> df = pd.DataFrame([list(tup) for tup in counts.index.values],\n...                   columns=['name', 'sex'])\n>>> df['count'] = counts.values\n>>> df\n        name sex  counts\n0      Aaden   M      51\n1     Aahana   F      26\n2      Aahil   M       5\n...      ...  ..     ...\n4235     Zvi   M       5\n4236     Zya   F       8\n4237   Zylah   F       5\n\n[4238 rows x 3 columns]\n```", "```py\n>>> df['istrain'] = np.random.rand(len(df)) < .9\n>>> df\n        name sex  counts  istrain\n0      Aaden   M      51     True\n1     Aahana   F      26     True\n2      Aahil   M       5     True\n...      ...  ..     ...      ...\n4235     Zvi   M       5     True\n4236     Zya   F       8     True\n4237   Zylah   F       5     True\n```", "```py\n>>> df.index = pd.MultiIndex.from_tuples(\n...     zip(df['name'], df['sex']), names=['name_', 'sex_'])\n>>> df\n               name sex  count  istrain\nname_  sex_\nAaden  M      Aaden   M     51     True\nAahana F     Aahana   F     26     True\nAahil  M      Aahil   M      5     True\n...             ...  ..    ...      ...\nZvi    M        Zvi   M      5     True\nZya    F        Zya   F      8     True\nZylah  F      Zylah   F      5     True\n```", "```py\n>>> df_most_common = {}  # #1\n>>> for name, group in df.groupby('name'):\n...     row_dict = group.iloc[group['count'].argmax()].to_dict()  # #2\n...     df_most_common[(name, row_dict['sex'])] = row_dict\n>>> df_most_common = pd.DataFrame(df_most_common).T  # #3\n```", "```py\n>>> df_most_common['istest'] = ~df_most_common['istrain'].astype(bool)\n>>> df_most_common\n            name sex count istrain  istest\nAaden  M   Aaden   M    51    True   False\nAahana F  Aahana   F    26    True   False\nAahil  M   Aahil   M     5    True   False\n...          ...  ..   ...     ...     ...\nZvi    M     Zvi   M     5    True   False\nZya    F     Zya   F     8    True   False\nZylah  F   Zylah   F     5    True   False\n\n[4025 rows x 5 columns]\n```", "```py\n>>> df['istest'] = df_most_common['istest']\n>>> df['istest'] = df['istest'].fillna(False)\n>>> df['istrain'] = ~df['istest']\n>>> istrain = df['istrain']\n>>> df['istrain'].sum() / len(df)\n0.9091...  # #1\n>>> df['istest'].sum() / len(df)\n0.0908...  # #2\n>>> (df['istrain'].sum() + df['istest'].sum()) / len(df)\n1.0\n```", "```py\n>>> unique_names = df['name'][istrain].unique()\n>>> unique_names = df['name'][istrain].unique()\n>>> vectorizer.fit(unique_names)\n>>> vecs = vectorizer.transform(df['name'])\n>>> vecs\n<4238x2855 sparse matrix of type '<class 'numpy.float64'>'\n    with 59959 stored elements in Compressed Sparse Row format>\n```", "```py\n>>> vecs = pd.DataFrame(vecs.toarray())\n>>> vecs.columns = vectorizer.get_feature_names_out()\n>>> vecs.index = df.index\n>>> vecs.iloc[:,:7]\n               a        aa  aac       aad       aah  aak  aal\nAaden   0.175188  0.392152  0.0  0.537563  0.000000  0.0  0.0\nAahana  0.316862  0.354641  0.0  0.000000  0.462986  0.0  0.0\nAahil   0.162303  0.363309  0.0  0.000000  0.474303  0.0  0.0\n...          ...       ...  ...       ...       ...  ...  ...\nZvi     0.000000  0.000000  0.0  0.000000  0.000000  0.0  0.0\nZya     0.101476  0.000000  0.0  0.000000  0.000000  0.0  0.0\nZylah   0.078353  0.000000  0.0  0.000000  0.000000  0.0  0.0\n```", "```py\n>>> vectorizer = TfidfVectorizer(analyzer='char',\n...    ngram_range=(1, 3), use_idf=False, lowercase=False)\n>>> vectorizer = vectorizer.fit(unique_names)\n>>> vecs = vectorizer.transform(df['name'])\n>>> vecs = pd.DataFrame(vecs.toarray())\n>>> vecs.columns = vectorizer.get_feature_names_out()\n>>> vecs.index = df.index\n>>> vecs.iloc[:,:5]\n                    A        Aa       Aad       Aah  Aal\nname_  sex_\nAaden  M     0.193989  0.393903  0.505031  0.000000  0.0\nAahana F     0.183496  0.372597  0.000000  0.454943  0.0\nAahil  M     0.186079  0.377841  0.000000  0.461346  0.0\n...               ...       ...       ...       ...  ...\nZvi    M     0.000000  0.000000  0.000000  0.000000  0.0\nZya    F     0.000000  0.000000  0.000000  0.000000  0.0\nZylah  F     0.000000  0.000000  0.000000  0.000000  0.0\n```", "```py\n>>> import pandas as pd\n>>> import re\n\n>>> dfs = pd.read_html('https://en.wikipedia.org/wiki/'\n...     + 'Comparison_of_deep-learning_software')\n>>> tabl = dfs[0]\n```", "```py\n>>> bincols = list(tabl.loc[:, 'OpenMP support':].columns)\n>>> bincols += ['Open source', 'Platform', 'Interface']\n>>> dfd = {}\n>>> for i, row in tabl.iterrows():\n...    rowd = row.fillna('No').to_dict()\n...    for c in bincols:\n...        text = str(rowd[c]).strip().lower()\n...        tokens = re.split(r'\\W+', text)\n...        tokens += '\\*'\n...        rowd[c] = 0\n...        for kw, score in zip(\n...                'yes via roadmap no linux android python \\*'.split(),\n...                [1, .9, .2, 0, 2, 2, 2, .1]):\n...            if kw in tokens:\n...                rowd[c] = score\n...                break\n...    dfd[i] = rowd\n```", "```py\n>>> tabl = pd.DataFrame(dfd).T\n>>> scores = tabl[bincols].T.sum()  # #1\n>>> tabl['Portability'] = scores\n>>> tabl = tabl.sort_values('Portability', ascending=False)\n>>> tabl = tabl.reset_index()\n>>> tabl[['Software', 'Portability']][:10]\n              Software Portability\n0              PyTorch        14.9\n1         Apache MXNet        14.2\n2           TensorFlow        13.2\n3       Deeplearning4j        13.1\n4                Keras        12.2\n5                Caffe        11.2\n6              PlaidML        11.2\n7         Apache SINGA        11.2\n8  Wolfram Mathematica        11.1\n9              Chainer          11\n```", "```py\n>>> import torch\n>>> class LogisticRegressionNN(torch.nn.Module):\n\n...    def __init__(self, num_features, num_outputs=1):\n...         super().__init__()\n...         self.linear = torch.nn.Linear(num_features, num_outputs)\n\n...    def forward(self, X):\n...        return torch.sigmoid(self.linear(X))\n\n>>> model = LogisticRegressionNN(num_features=vecs.shape[1], num_outputs=1)\n>>> model\nLogisticRegressionNN(\n  (linear): Linear(in_features=3663, out_features=1, bias=True)\n)\n```", "```py\n>>> loss_func_train = torch.nn.BCELoss(\n...     weight=torch.Tensor(df[['count']][istrain].values))\n>>> loss_func_test = torch.nn.BCELoss(  # #1\n...     weight=torch.Tensor(df[['count']][~istrain].values))\n>>> loss_func_train\nBCELoss()\n```", "```py\n>>> from torch.optim import SGD\n>>> hyperparams = {'momentum': 0.001, 'lr': 0.02}  # #1\n>>> optimizer = SGD(\n...     model.parameters(), **hyperparams)  # #2\n>>> optimizer\nSGD (\nParameter Group 0\n    dampening: 0\n    differentiable: False\n    foreach: None\n    lr: 0.02\n    maximize: False\n    momentum: 0.001\n    nesterov: False\n    weight_decay: 0\n)\n```", "```py\n>>> X = vecs.values\n>>> y = (df[['sex']] == 'F').values\n>>> X_train = torch.Tensor(X[istrain])\n>>> X_test = torch.Tensor(X[~istrain])\n>>> y_train = torch.Tensor(y[istrain])\n>>> y_test = torch.Tensor(y[~istrain])\n```", "```py\n>>> from tqdm import tqdm\n>>> num_epochs = 200\n>>> pbar_epochs = tqdm(range(num_epochs), desc='Epoch:', total=num_epochs)\n\n>>> for epoch in pbar_epochs:\n...      optimizer.zero_grad()  # #1\n...      outputs = model(X_train)\n...      loss_train = loss_func_train(outputs, y_train)  # #2\n...      loss_train.backward()  # #3\n...      optimizer.step()  # #4\n\n...      Epoch:: 100%|█████████████████████████| 200/200 [00:02<00:00,\n 96.26it/s]\n```", "```py\n>>> def make_array(x):\n...     if hasattr(x, 'detach'):\n...         return torch.squeeze(x).detach().numpy()\n...     return x\n```", "```py\n>>> def measure_binary_accuracy(y_pred, y):\n...     y_pred = make_array(y_pred).round()\n...     y = make_array(y).round()\n...     num_correct = (y_pred == y).sum()\n...     return num_correct / len(y)\n```", "```py\nfor epoch in range(num_epochs):\n    optimizer.zero_grad()  # #1\n    outputs = model(X_train)\n    loss_train = loss_func_train(outputs, y_train)\n    loss_train.backward()\n    epoch_loss_train = loss_train.item()\n    optimizer.step()\n    outputs_test = model(X_test)\n    loss_test = loss_func_test(outputs_test, y_test).item()\n    accuracy_test = measure_binary_accuracy(outputs_test, y_test)\n    if epoch % 20 == 19:  # #2\n        print(f'Epoch {epoch}:'\n            f' loss_train/test: {loss_train.item():.4f}/{loss_test:.4f},'\n            f' accuracy_test: {accuracy_test:.4f}')\n```", "```py\nEpoch 19: loss_train/test: 80.1816/75.3989, accuracy_test: 0.4275\nEpoch 39: loss_train/test: 75.0748/74.4430, accuracy_test: 0.5933\nEpoch 59: loss_train/test: 71.0529/73.7784, accuracy_test: 0.6503\nEpoch 79: loss_train/test: 67.7637/73.2873, accuracy_test: 0.6839\nEpoch 99: loss_train/test: 64.9957/72.9028, accuracy_test: 0.6891\nEpoch 119: loss_train/test: 62.6145/72.5862, accuracy_test: 0.6995\nEpoch 139: loss_train/test: 60.5302/72.3139, accuracy_test: 0.7073\nEpoch 159: loss_train/test: 58.6803/72.0716, accuracy_test: 0.7073\nEpoch 179: loss_train/test: 57.0198/71.8502, accuracy_test: 0.7202\nEpoch 199: loss_train/test: 55.5152/71.6437, accuracy_test: 0.7280\n```", "```py\n>>> X = vectorizer.transform(\n...     ['John', 'Greg', 'Vishvesh',  # #1\n\n...         ...      'Ruby', 'Carlana', 'Sarah'])  # #2\n>>> model(torch.Tensor(X.todense()))\ntensor([[0.0196],\n        [0.1808],\n        [0.3729],\n        [0.4964],\n        [0.8062],\n        [0.8199]], grad_fn=<SigmoidBackward0>)\n```"]