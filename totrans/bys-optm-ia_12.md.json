["```py\ndef objective(x):                                               ❶\n    y = -((x + 1) ** 2) * torch.sin(2 * x + 2) / 5 + 1 + x / 3  ❶\n    return y                                                    ❶\n\ndef approx_objective(x):                                        ❷\n    return 0.5 * objective(x) + x / 4 + 2                       ❷\n\nlb = -5                                                         ❸\nub = 5                                                          ❸\nbounds = torch.tensor([[lb], [ub]], dtype=torch.float)          ❸\n```", "```py\nfidelities = torch.tensor([0.5, 1.0])\n```", "```py\nn = 10                                                             ❶\n\ntorch.manual_seed(0)                                               ❷\ntrain_x = bounds[0] + (bounds[1] - bounds[0]) * torch.rand(n, 1)   ❸\n```", "```py\ntrain_f = fidelities[torch.randint(2, (n, 1))]         ❶\ntrain_x_full = torch.cat([train_x, train_f], dim=1)    ❷\n```", "```py\ntensor([[-0.0374,  1.0000],   ❶\n        [ 2.6822,  0.5000],   ❷\n        ...\n```", "```py\ndef evaluate_all_functions(x_full):\n    y = []\n    for x in x_full:                             ❶\n        if torch.isclose(x[-1], torch.ones(1)):  ❷\n            y.append(objective(x[:-1]))          ❷\n        else:                                    ❸\n            y.append(approx_objective(x[:-1]))   ❸\n\n    return torch.tensor(y).unsqueeze(-1)         ❹\n```", "```py\nfrom botorch.models.gp_regression_fidelity\n➥import SingleTaskMultiFidelityGP         ❶\n\nmodel = SingleTaskMultiFidelityGP(\n➥train_x_full, train_y, data_fidelity=1)  ❷\n```", "```py\nfrom gpytorch.mlls.exact_marginal_log_likelihood import  ❶\n➥ExactMarginalLogLikelihood                             ❶\nfrom botorch.fit import fit_gpytorch_mll                 ❶\n\nmll = ExactMarginalLogLikelihood(model.likelihood,\n➥model)                                                 ❷\nfit_gpytorch_mll(mll);                                   ❸\n```", "```py\nUserWarning: The model inputs are of type torch.float32\\. It is strongly \nrecommended to use double precision in BoTorch, as this improves both \nprecision and stability and can help avoid numerical errors. See \nhttps:/ /github.com/pytorch/botorch/discussions/1444\n  warnings.warn(\n```", "```py\ntorch.set_default_dtype(torch.double)\n```", "```py\nInputDataWarning: Input data is not \n contained to the unit cube. Please consider min-max scaling the input data.\n  warnings.warn(msg, InputDataWarning)\nInputDataWarning: Input data is not standardized. Please consider scaling \nthe input to zero mean and unit variance.\n  warnings.warn(msg, InputDataWarning)\n```", "```py\nxs = torch.linspace(−5, 5, 201)\n```", "```py\nwith torch.no_grad():                                             ❶\n    pred_dist = model(torch.vstack([xs, torch.ones_like(xs)]).T)  ❷\n    pred_mean = pred_dist.mean                                    ❸\n    pred_lower, pred_upper = pred_dist.confidence_region()        ❹\n```", "```py\ntrain_f = torch.ones_like(train_x) * fidelities[0]     ❶\ntrain_x_full = torch.cat([train_x, train_f], dim=1)    ❷\n```", "```py\nfrom botorch.models.cost import AffineFidelityCostModel\n\ncost_model = AffineFidelityCostModel(\n  fixed_cost=0.0,              ❶\n  fidelity_weights={1: 1.0},   ❷\n)\n```", "```py\ndef forward(self, X: Tensor) -> Tensor:\n    lin_cost = torch.einsum(                                      ❶\n        \"...f,f\", X[..., self.fidelity_dims], self.weights.to(X)  ❶\n    )                                                             ❶\n    return self.fixed_cost + lin_cost.unsqueeze(-1)               ❷\n```", "```py\n    from botorch.acquisition.cost_aware import InverseCostWeightedUtility\n\n    cost_aware_utility = InverseCostWeightedUtility(cost_model=cost_model)\n    ```", "```py\n    torch.manual_seed(0)                                   ❶\n\n    sobol = SobolEngine(1, scramble=True)                  ❷\n    candidate_x = sobol.draw(1000)                         ❷\n\n    candidate_x = bounds[0] + (bounds[1] - bounds[0]) *\n    ➥candidate_x                                          ❸\n\n    candidate_x = torch.cat([candidate_x, torch.ones_like(\n    ➥candidate_x)], dim=1)                                ❹\n    ```", "```py\nfrom botorch.acquisition.utils import project_to_target_fidelity\n\npolicy = qMultiFidelityMaxValueEntropy(\n    model,\n    candidate_x,                             ❶\n    num_fantasies=128,\n    cost_aware_utility=cost_aware_utility,   ❷\n    project=project_to_target_fidelity,      ❸\n)\n```", "```py\nfrom botorch.optim.optimize import optimize_acqf_mixed\n\nnext_x, acq_val = optimize_acqf_mixed(\n    policy,\n    bounds=torch.cat(                                                ❶\n        [bounds, torch.tensor([0.5, 1.0]).unsqueeze(-1)], dim=1      ❶\n    ),                                                               ❶\n    fixed_features_list=[{1: cost.item()} for cost in fidelities],   ❷\n    q=1,\n    num_restarts=20,\n    raw_samples=50,\n)\n```", "```py\nfrom botorch.acquisition.fixed_feature\n➥import FixedFeatureAcquisitionFunction\nfrom botorch.acquisition import PosteriorMean\n\npost_mean_policy = FixedFeatureAcquisitionFunction(\n    acq_function=PosteriorMean(model),   ❶\n    d=2,                                 ❷\n    columns=[1],                         ❸\n    values=[1],                          ❹\n)\n```", "```py\nfinal_x, _ = optimize_acqf(\n    post_mean_policy,       ❶\n    bounds=bounds,          ❷\n    q=1,                    ❷\n    num_restarts=20,        ❷\n    raw_samples=50,         ❷\n)\n```", "```py\ndef get_final_recommendation(model):\n    post_mean_policy = FixedFeatureAcquisitionFunction(...)  ❶\n    final_x, _ = optimize_acqf(...)                          ❷\n\n    return torch.cat([final_x, torch.ones(1, 1)], dim=1)     ❸\n```", "```py\nbudget_limit = 10                                  ❶\n\nrecommendations = []                               ❷\nspent_budget = []                                  ❸\n\n...                                                ❹\n\ncurrent_budget = 0\n\nwhile current_budget < budget_limit:\n    ...                                            ❺\n\n    rec_x = get_final_recommendation(model)        ❻\n    recommendations.append(evaluate_all_functions  ❻\n    ➥(rec_x).item())                              ❻\n    spent_budget.append(current_budget)            ❻\n\n    ...                                            ❼\n\n    current_budget += cost_model(next_x).item()    ❽\n\n    ...                                            ❾\n```", "```py\npolicy = FixedFeatureAcquisitionFunction(\n  acq_function=qMaxValueEntropy(model, candidate_x, num_fantasies=128), ❶\n  d=2,\n  columns=[1],                                                          ❷\n  values=[1],                                                           ❷\n)\n\nnext_x, acq_val = optimize_acqf(...)                                    ❸\n```", "```py\n    problem = AugmentedBranin()                              ❶\n\n    def objective(X):\n        X_copy = X.detach().clone()                          ❷\n        X_copy[..., :-1] = X_copy[..., :-1] * 15 - 5         ❷\n        X_copy[..., -2] = X_copy[..., -2] + 5                ❷\n        return (-problem(X_copy) / 500 + 0.9).unsqueeze(-1)  ❷\n    ```"]