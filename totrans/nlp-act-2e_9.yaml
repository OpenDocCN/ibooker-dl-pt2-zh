- en: 9 Stackable deep learning (Transformers)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Understanding what makes transformers so powerful
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seeing how transformers enable limitless "stacking" options for NLP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encoding text to create meaningful vector representations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decoding semantic vectors to generate text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finetuning transformers (BERT, GPT) for your application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying transformers to extractive and abstraction summarization of long documents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating grammatically correct and interesting text with transformers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimating the information capacity of a transformer network required for a
    particular problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Transformers* are changing the world. The increased intelligence that transformers
    bring to AI is transforming culture, society, and the economy. For the first time,
    transformers are making us question the long-term economic value of human intelligence
    and creativity. And the ripple effects of transformers go deeper than just the
    economy. Transformers are changing not only how we work and play, but even how
    we think, communicate, and create. Within less than a year, transformer-enabled
    AI known as Large Language Models (LLMs) created whole new job categories such
    as *prompt engineering* and real-time content curation and fact-checking (grounding).
    Tech companies are racing to recruit engineers that can design effective LLM prompts
    and incorporate LLMs into their workflows. Transformers are automating and accelerating
    productivity for information economy jobs that previously required a level of
    creativity and abstraction out of reach for machines.'
  prefs: []
  type: TYPE_NORMAL
- en: As transformers automate more and more information economy tasks, workers are
    reconsidering whether their jobs are as essential to their employers as they thought.
    For example, influential CyberSecurity experts are bragging about augmenting their
    thinking, planning, and creativity with the help of dozens of ChatGPT suggestions
    every day.^([[1](#_footnotedef_1 "View footnote.")]) Microsoft News and the MSN.com
    website laid off its journalists in 2020, replacing them with transformer models
    capable of curating and summarizing news articles automatically. This race to
    the bottom (of the content quality ladder) probably won’t end well for media companies
    or their advertisers and employees.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn how to use transformers to *improve* the accuracy
    and thoughtfulness of natural language text. Even if your employer tries to program
    away your job, you will know how to program transformers to create new opportunities
    for yourself. Program or be programmed. Automate or be automated.
  prefs: []
  type: TYPE_NORMAL
- en: And transformers are your best choice not only for natural language generation,
    but also for natural language understanding. Any system that relies on a vector
    representation of meaning can benefit from transformers.
  prefs: []
  type: TYPE_NORMAL
- en: At one point Replika used GPT-3 to generate more than 20% of its replies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qary uses BERT to generate open domain question answers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google uses models based on BERT to improve search results and query a knowledge
    graph
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nboost` uses transformers to create a semantic search proxy for ElasticSearch'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: aidungeon.io uses GPT-3 to generate an endless variety of rooms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most vector databases for semantic search rely on transformers.^([[2](#_footnotedef_2
    "View footnote.")])
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even if you only want to get good at *prompt engineering*, your understanding
    of transformers will help you design prompts for LLMs that avoid the holes in
    LLM capabilities. And LLMs are so full of holes that engineers and statisticians
    often use the *swiss cheese model* when thinking about how LLMs fail. ^([[3](#_footnotedef_3
    "View footnote.")]) The conversational interface of LLMs makes it easy to learn
    how to cajole the snarky conversational AI systems into doing valuable work. People
    that understand how LLMs work and can fine tune them for their own applications,
    those people will have their hands at the helm of a powerful machine. Imagine
    how sought-after you’d be if you could build a "TutorGPT" that can help students
    solve arithmetic and math word problems. Shabnam Aggarwal at Rising Academies
    in Kigali is doing just that with her Rori.AI WhatsApp math tutor bot for middle
    school students.^([[4](#_footnotedef_4 "View footnote.")]) ^([[5](#_footnotedef_5
    "View footnote.")]) And Vishvesh Bhat did this for college math students as a
    passion project.^([[6](#_footnotedef_6 "View footnote.")]) ^([[7](#_footnotedef_7
    "View footnote.")])
  prefs: []
  type: TYPE_NORMAL
- en: 9.1 Recursion vs recurrence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Transformers are the latest big leap forward in auto-regressive NLP models.
    Auto-regressive models predict one discrete output value at a time, usually a
    token or word in natural language text. An autoregressor recycles the output to
    reuse it as an input for predicting the next output so auto-regressive neural
    networks are *recursive*. The word "recursive" is a general term for any recycling
    of outputs back into the input, a process that can continue indefinitely until
    an algorithm or computation "terminates." A recursive function in computer science
    will keep calling itself until it achieves the desired result.
  prefs: []
  type: TYPE_NORMAL
- en: But transformers are recursive in a bigger and more general way than *Recurrent*
    Neural Networks. Transformers are called *recursive* NNs rather than *recurrent*
    NNs because *recursive* is a more general term for any system that recycles the
    input.^([[8](#_footnotedef_8 "View footnote.")]) The term *recurrent* is used
    exclusively to describe RNNs such as LSTMs and GRUs where the individual neurons
    recycle their outputs into the same neuron’s input for each step through the sequence
    tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers are a *recursive* algorithm but do not contain *recurrent* neurons.
    As you learned in Chapter 8, recurrent neural networks recycle their output within
    each individual neuron or RNN *unit*. But transformers wait until the very last
    layer to output a token embedding that can be recycled back into the input. The
    entire transformer network, both the encoder and the decoder, must be run to predict
    each token so that token can be used to help it predict the next one. In the computer
    science world, you can see that a transformer is one big recursive function calling
    a series of nonrecursive functions inside. The whole transformer is run *recursively*
    to generate one token at a time.
  prefs: []
  type: TYPE_NORMAL
- en: '![transformer recursion drawio](images/transformer_recursion_drawio.png)'
  prefs: []
  type: TYPE_IMG
- en: Because there is no recurrence within the inner guts of the transformer it doesn’t
    need to be "unrolled." This gives transformers a huge advantage over RNNs. The
    individual neurons and layers in a transformer can be run in parallel all at once.
    For an RNN, you had to run the functions for the neurons and layers one at a time
    in sequence. *Unrolling* all these recurrent function calls takes a lot of computing
    power and it must be performed in order. You can’t skip around or run them in
    parallel. They must be run sequentially all the way through the entire text. A
    transformer breaks the problem into a much smaller problem, predicting a single
    token at a time. This way all the neurons of a transformer can be run in parallel
    on a GPU or multi-core CPU to dramatically speed up the time it takes to make
    a prediction.
  prefs: []
  type: TYPE_NORMAL
- en: They use the last predicted output as the input to predict the next output.
    But transformers are *recursive* not *recurrent*. Recurrent neural networks (RNNs)
    include variational autoencoders, RNNs, LSTMs, and GRUs. When researchers combine
    five NLP ideas to create the transformer architecture, they discovered a total
    capability that was much greater than the sum of its parts. Let’s looks at these
    ideas in detail.
  prefs: []
  type: TYPE_NORMAL
- en: 9.1.1 Attention is NOT all you need
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Byte pair encoding (BPE)*:: Tokenizing words based on character sequence statistics
    rather than spaces and punctuation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Attention*:: Connecting important word patterns together across long stretches
    of text using a connection matrix (attention)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Positional encoding*:: Keeping track of where each token or pattern is located
    within the token sequence'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Byte pair encoding (BPE) is an often overlooked enhancement of transformers.
    BPE was originally invented to encode text in a compressed binary (byte sequence)
    format. But BPE really came into its own when it was used as a tokenizer in NLP
    pipelines such as search engines. Internet search engines often contain millions
    of unique words in their vocabulary. Imagine all the important names a search
    engine is expected to understand and index. BPE can efficiently reduce your vocabulary
    by several orders of magnitude. The typical transformer BPE vocabulary size is
    only 5000 tokens. And when you’re storing a long embedding vector for each of
    your tokens, this is a big deal. A BPE vocabulary trained on the entire Internet
    can easily fit in the RAM of a typical laptop or GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Attention gets most of the credit for the success of transformers because it
    made the other parts possible. The attention mechanism is a much simpler approach
    than the complicated math (and computational complexity) of CNNs and RNNs. The
    attention mechanism removes the recurrence of the encoder and decoder networks.
    So a transformer has neither the *vanishing gradients* nor the *exploding gradients*
    problem of an RNN. Transformers are limited in the length of text they can process
    because the attention mechanism relies on a fixed-length sequence of embeddings
    for both the inputs and outputs of each layer. The attention mechanism is essentially
    a single CNN kernel that spans the entire sequence of tokens. Instead of rolling
    across the text with convolution or recurrence, the attention matrix is simply
    multiplied once by the entire sequence of token embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: The loss of recurrence in a transformer creates a new challenge because the
    transformer operates on the entire sequence all at once. A transformer is *reading*
    the entire token sequence all at once. And it outputs the tokens all at once as
    well, making bi-directional transformers an obvious approach. Transformers do
    not care about the normal causal order of tokens while it is reading or writing
    text. To give transformers information about the causal sequence of tokens, positional
    encoding was added. And it doesn’t even require additional dimensions within the
    vector embedding, positional encoding is spread out over the entire embedding
    sequence by multiplying them by the sine and cosine functions. Positional encoding
    enables nuanced adjustment to a transformer’s understanding of tokens depending
    on their location in a text. With positional encoding, the word "sincerely" at
    the beginning of an email has a different meaning than it does at the end of an
    email.
  prefs: []
  type: TYPE_NORMAL
- en: 'Limiting the token sequence length had a cascading effect of efficiency improvements
    that give transformers an unexpectedly powerful advantage over other architectures:
    *scalability*. BPE plus *attention* plus positional encoding combine together
    to create unprecedented scalability. These three innovations and simplifications
    of neural networks combined to create a network that is both much more stackable
    and much more parallelizable.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Stackability*:: The inputs and outputs of a transformer layer have the exact
    same structure so they can be stacked to increase capacity'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Parallelizability*:: The cookie cutter transformer layers all rely heavily
    on large matrix multiplications rather than complex recurrence and logical switching
    gates'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This stackability of transformer layers combined with the parallelizablity of
    the matrix multiplication required for the attention mechanism creates unprecedented
    scalability. And when researchers tried out their large-capacity transformers
    on the largest datasets they could find (essentially the entire Internet), they
    were taken aback. The extremely large transformers trained on extremely large
    datasets were able to solve NLP problems previously thought to be out of reach.
    Smart people are beginning to think that world-transforming conversational machine
    intelligence (AGI) may only be years away, if it isn’t already upon us.
  prefs: []
  type: TYPE_NORMAL
- en: 9.1.2 Much attention about everything
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You might think that all this talk about the power of attention is much ado
    about nothing. Surely transformers are more than just a simple matrix multiplication
    across every token in the input text. Transformers combine many other less well-known
    innovations such as BPE, self-supervised training, and positional encoding. The
    attention matrix was the connector between all these ideas that helped them work
    together effectively. And the attention matrix enables a transformer to accurately
    model the connections between *all* the words in a long body of text, all at once.
  prefs: []
  type: TYPE_NORMAL
- en: As with CNNs and RNNs (LSTMs & GRUs), each layer of a transformer gives you
    a deeper and deeper representation of the *meaning* or *thought* of the input
    text. But unlike CNNs and RNNs, the transformer layer outputs an encoding that
    is the exact same size and shape as the previous layers. Likewise for the decoder,
    a transformer layer outputs a fixed-size sequence of embeddings representing the
    semantics (meaning) of the output token sequence. The outputs of one transformer
    layer can be directly input into the next transformer layer making the layers
    even more *stackable* than CNN’s. And the attention matrix within each layer spans
    the entire length of the input text, so each transformer layer has the same internal
    structure and math. You can stack as many transformer encoder and decoder layers
    as you like creating as deep a neural network as you need for the information
    content of your data.
  prefs: []
  type: TYPE_NORMAL
- en: Every transformer layer outputs a consistent *encoding* with the same size and
    shape. Encodings are just embeddings but for token sequences instead of individual
    tokens. In fact, many NLP beginners use the terms "encoding" and embedding" interchangeably,
    but after this chapter, you will understand the difference. The word "embedding",
    used as a noun, is 3 times more popular than "encoding", but as more people catch
    up with you in learning about transformers that will change.^([[9](#_footnotedef_9
    "View footnote.")]) If you don’t need to make it clear which ones you are talking
    about you can use "semantic vector", a term you learned in Chapter 6.
  prefs: []
  type: TYPE_NORMAL
- en: Like all vectors, encodings maintain a consistent structure so that they represent
    the meaning of your token sequence (text) in the same way. And transformers are
    designed to accept these encoding vectors as part of their input to maintain a
    "memory" of the previous layers' understanding of the text. This allows you to
    stack transformer layers with as many layers as you like if you have enough training
    data to utilize all that capacity. This "scalability" allows transformers to break
    through the diminishing returns ceiling of RNNs.
  prefs: []
  type: TYPE_NORMAL
- en: And because the attention mechanism is just a connection matrix, it can be implemented
    as a matrix multiplication with a PyTorch `Linear` layer. Matrix multiplications
    are parallelized when you run your PyTorch network on a GPU or multicore CPU.
    This means that much larger transformers can be parallelized and these much larger
    models can be trained much faster. *Stackability* plus *Parallelizablity* equals
    *Scalability*.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer layers are designed to have inputs and outputs with the same size
    and shape so that the transformer layers can be stacked like Lego bricks that
    all have the same shape. The transformer innovation that catches most researchers'
    attention is the *attention mechanism*. Start there if you want to understand
    what makes transformers so exciting to NLP and AI researchers. Unlike other deep
    learning NLP architectures that use recurrence or convolution, the transformer
    architecture uses stacked blocks of attention layers which are essentially fully-connected
    feedforward layers with the same.
  prefs: []
  type: TYPE_NORMAL
- en: In Chapter 8, you used RNNs to build encoders and decoders to transform text
    sequences. In encoder-decoder (*transcoder* or *transduction*) networks,^([[10](#_footnotedef_10
    "View footnote.")]) the encoder processes each element in the input sequence to
    distill the sentence into a fixed-length thought vector (or *context vector*).
    That thought vector can then be passed on to the decoder where it is used to generate
    a new sequence of tokens.
  prefs: []
  type: TYPE_NORMAL
- en: The encoder-decoder architecture has a big limitation — it can’t handle longer
    texts. If a concept or thought is expressed in multiple sentences or a long complex
    sentence, then the encoded thought vector fails to accurately encapsulate *all*
    of that thought. The *attention mechanism* presented by Bahdanau et al ^([[11](#_footnotedef_11
    "View footnote.")]) to solve this issue is shown to improve sequence-to-sequence
    performance, particularly on long sentences, however it does not alleviate the
    time sequencing complexity of recurrent models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The introduction of the *transformer* architecture in "Attention Is All You
    Need" ^([[12](#_footnotedef_12 "View footnote.")]) propelled language models forward
    and into the public eye. The transformer architecture introduced several synergistic
    features that worked together to achieve as yet impossible performance:'
  prefs: []
  type: TYPE_NORMAL
- en: The most widely recognized innovation in the transformer architecture is *self-attention*.
    Similar to the memory and forgetting gates in a GRU or LSTM, the attention mechanism
    creates connections between concepts and word patterns within a lengthy input
    string.
  prefs: []
  type: TYPE_NORMAL
- en: In the next few sections, you’ll walk through the fundamental concepts behind
    the transformer and take a look at the architecture of the model. Then you will
    use the base PyTorch implementation of the Transformer module to implement a language
    translation model, as this was the reference task in "Attention Is All You Need",
    to see how it is both powerful and elegant in design.
  prefs: []
  type: TYPE_NORMAL
- en: Self-attention
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When we were writing the first edition of this book, Hannes and Cole (the first
    edition coauthors) were already focused on the attention mechanism. It’s now been
    6 years and attention is still the most researched topic in deep learning. The
    attention mechanism enabled a leap forward in capability for problems where LSTMs
    struggled:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Conversation* — Generate plausible responses to conversational prompts, queries,
    or utterances.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Abstractive summarization or paraphrasing*:: Generate a new shorter wording
    of a long text summarization of sentences, paragraphs, and even several pages
    of text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Open domain question answering*:: Answering a general question about anything
    the transformer has ever read.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Reading comprehension question answering*:: Answering questions about a short
    body of text (usually less than a page).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Encoding*:: A single vector or sequence of embedding vectors that represent
    the meaning of body of text in a vector space — sometimes called *task-independent
    sentence embedding*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Translation and code generation* — Generating plausible software expressions
    and programs based on plain English descriptions of the program’s purpose.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Self-attention is the most straightforward and common way to implement attention.
    It takes the input sequence of embedding vectors and puts them through linear
    projections. A linear projection is merely a dot product or matrix multiplication.
    This dot product creates key, value and query vectors. The query vector is used
    along with the key vector to create a context vector for the words' embedding
    vectors and their relation to the query. This context vector is then used to get
    a weighted sum of values. In practice, all these operations are done on sets of
    queries, keys, and values packed together in matrices, *Q*, *K*, and *V*, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two ways to implement the linear algebra of an attention algorithm:
    *additive attention* or *dot-product attention*. The one that was most effective
    in transformers is a scaled version of dot-production attention. For dot-product
    attention, the scalar products between the query vectors *Q* and the key vectors
    *K*, are scaled down based on how many dimensions there are in the model. This
    makes the dot product more numerically stable for large dimensional embeddings
    and longer text sequences. Here’s how you compute the self-attention outputs for
    the query, key, and value matrices *Q*, *K*, and *V*.'
  prefs: []
  type: TYPE_NORMAL
- en: Equation 9.1 Self-attention outputs
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: \[Attention(Q, K, V ) = softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V\]
  prefs: []
  type: TYPE_NORMAL
- en: The high dimensional dot products create small gradients in the softmax due
    to the law of large numbers. To counteract this effect, the product of the query
    and key matrices is scaled by \(\frac{1}{\sqrt{d_{k}}}\). The softmax normalizes
    the resulting vectors so that they are all positive and sum to 1\. This "scoring"
    matrix is then multiplied with the values matrix to get the weighted values matrix
    in figure [9.1](#figure-scaled-dot-product-attention).^([[13](#_footnotedef_13
    "View footnote.")]) ^([[14](#_footnotedef_14 "View footnote.")])
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.1 Scaled dot product attention
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![transformer attention](images/transformer_attention.png)'
  prefs: []
  type: TYPE_IMG
- en: Unlike RNNs, where there is recurrence and shared weights, in self-attention
    all of the vectors used in the query, key, and value matrices come from the input
    sequences' embedding vectors. The entire mechanism can be implemented with highly
    optimized matrix multiplication operations. And the *Q* *K* product forms a square
    matrix that can be understood as the connection between words in the input sequence.
    A toy example is shown in figure [9.2](#figure-attention-matrix-illustration).
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.2 Encoder attention matrix as connections between words
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![attention heatmap](images/attention_heatmap.png)'
  prefs: []
  type: TYPE_IMG
- en: Multi-Head Self-Attention
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Multi-head self-attention is an expansion of the self-attention approach to
    creating multiple attention heads that each attend to different aspects of the
    words in a text. So if a token has multiple meanings that are all relevant to
    the interpretation of the input text, they can each be accounted for in the separate
    attention heads. You can think of each attention head as another dimension of
    the encoding vector for a body of text, similar to the additional dimensions of
    an embedding vector for an individual token (see Chapter 6). The query, key, and
    value matrices are multiplied *n* (*n_heads*, the number of attention heads) times
    by each different \(d_q\) , \(d_k\), and \(d_v\) dimension, to compute the total
    attention function output. The *n_heads* value is a hyperparameter of the transformer
    architecture that is typically small, comparable to the number of transformer
    layers in a transformer model. The \(d_v\)-dimensional outputs are concatenated
    and again projected with a \(W^o\) matrix as shown in the next equation.
  prefs: []
  type: TYPE_NORMAL
- en: Equation 9.2 Multi-Head self-attention
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: \[MultiHeadAttention(Q, K, V ) = Concat(head_1, ..., head_n) W^o\\ where\ head_i
    = Attention(QW_i^Q, KW_i^K, VW_i^V)\]
  prefs: []
  type: TYPE_NORMAL
- en: The multiple heads allow the model to focus on different positions, not just
    ones centered on a single word. This effectively creates several different vector
    subspaces where the transformer can encode a particular generalization for a subset
    of the word patterns in your text. In the original transformers paper, the model
    uses *n*=8 attention heads such that \(d_k = d_v = \frac{d_{model}}{n} = 64\).
    The reduced dimensionality in the multi-head setup is to ensure the computation
    and concatenation cost is nearly equivalent to the size of a full-dimensional
    single-attention head.
  prefs: []
  type: TYPE_NORMAL
- en: If you look closely you’ll see that the attention matrices (attention heads)
    created by the product of *Q* and *K* all have the same shape, and they are all
    square (same number of rows as columns). This means that the attention matrix
    merely rotates the input sequence of embeddings into a new sequence of embeddings,
    without affecting the shape or magnitude of the embeddings. And this makes it
    possible to explain a bit about what the attention matrix is doing for a particular
    example input text.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.3 Multi-Head Self-Attention
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![multi head attention drawio](images/multi-head-attention_drawio.png)'
  prefs: []
  type: TYPE_IMG
- en: It turns out, the multi-head attention mechanism is just a fully connected linear
    layer under the hood. After all is said and done, the deepest of the deep learning
    models turned to be nothing more than a clever stacking of what is essentially
    linear and logistic regressions. This is why it was so surprising that transformers
    were so successful. And this is why it was so important for you to understand
    the basics of linear and logistic regression described in earlier chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2 Filling the attention gaps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The attention mechanism compensates for some problems with RNNs and CNNs of
    previous chapters but creates some additional challenges. Encoder-decoders based
    on RNNs don’t work very well for longer passages of text where related word patterns
    are far apart. Even long sentences are a challenge for RNNs doing translation.^([[15](#_footnotedef_15
    "View footnote.")]) And the attention mechanism compensates for this by allowing
    a language model to pick up important concepts at the beginning of a text and
    connect them to text that is towards the end. The attention mechanism gives the
    transformer a way to reach back to any word it has ever seen. Unfortunately, adding
    the attention mechanism forces you to remove all recurrence from the transformer.
  prefs: []
  type: TYPE_NORMAL
- en: CNNs are another way to connect concepts that are far apart in the input text.
    A CNN can do this by creating a hierarchy of convolution layers that progressively
    "necks down" the encoding of the information within the text it is processing.
    And this hierarchical structure means that a CNN has information about the large-scale
    position of patterns within a long text document. Unfortunately, the outputs and
    the inputs of a convolution layer usually have different shapes. So CNNs are not
    stackable, making them tricky to scale up for greater capacity and larger training
    datasets. So to give a transformer the uniform data structure it needs for stackability,
    transformers use byte pair encoding and positional encoding to spread the semantic
    and position information uniformly across the encoding tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.1 Positional encoding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Word order in the input text matters, so you need a way to bake in some positional
    information into the sequence of embeddings that is passed along between layers
    in a transformer. A positional encoding is simply a function that adds information
    about the relative or absolute position of a word in a sequence to the input embeddings.
    The encodings have the same dimension, \(d_{model}\), as the input embeddings
    so they can be summed with the embedding vectors. The paper discusses learned
    and fixed encodings and proposes a sinusoidal function of sin and cosine with
    different frequencies, defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: Equation 9.3 Positional encoding function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: \[PE_{(pos, 2i)} = sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}})\\ PE_{(pos,
    2i+1)} = cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})\]
  prefs: []
  type: TYPE_NORMAL
- en: This mapping function was chosen because for any offset *k*, \(PE_{(pos+k)}\)
    can be represented as a linear function of \(PE_{pos}\). In short, the model should
    be able to learn to attend to relative positions easily.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at how this can be coded in Pytorch. The official Pytorch Sequence-to-Sequence
    Modeling with `nn.Transformer` tutorial ^([[16](#_footnotedef_16 "View footnote.")])
    provides an implementation of a PositionEncoding nn.Module based on the previous
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.1 Pytorch PositionalEncoding
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You will use this module in the translation transformer you build. However,
    first, we need to fill in the remaining details of the model to complete your
    understanding of the architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.2 Connecting all the pieces
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that you’ve seen the hows and whys of BPE, embeddings, positional encoding,
    and multi-head self-attention, you understand all the elements of a transformer
    layer. You just need a lower dimensional linear layer at the output to collect
    all those attention weights together to create the output sequence of embeddings.
    And the linear layer output needs to be scaled (normalized) so that the layers
    all have the same scale. These linear and normalization layers are stacked on
    top of the attention layers to create reusable stackable transformer blocks as
    shown in figure [9.4](#figure-transformer-architecture).
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.4 Transformer architecture
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![transformer original](images/transformer_original.png)'
  prefs: []
  type: TYPE_IMG
- en: In the original transformer, both the encoder and decoder are comprised of *N*
    = 6 stacked identical encoder and decoder layers, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Encoder
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The encoder is composed of multiple encoder layers. Each encoder layer has
    two sub-layers: a multi-head attention layer and a position-wise fully connected
    feedforward network. A residual connection is made around each sub-layer. And
    each encoder layer has its output normalized so that all the values of the encodings
    passed between layers range between zero and one. The outputs of all sub-layers
    in a transformer layer (PyTorch module) that are passed between layers all have
    dimension \(d_{model}\). And the input embedding sequences to the encoder are
    summed with the positional encodings before being input into the encoder.'
  prefs: []
  type: TYPE_NORMAL
- en: Decoder
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The decoder is nearly identical to the encoder in the model but has three sublayers
    instead of one. The new sublayer is a fully connected layer similar to the multi-head
    self-attention matrix but contains only zeros and ones. This creates a *masking*
    of the output sequences that are to the right of the current target token (in
    a left-to-right language like English). This ensures that predictions for position
    *i* can depend only on previous outputs, for positions less than *i*. In other
    words, during training, the attention matrix is not allowed to "peek ahead" at
    the subsequent tokens that it is supposed to be generating in order to minimize
    the loss function. This prevents *leakage* or "cheating" during training, forcing
    the transformer to attend only to the tokens it has already seen or generated.
    Masks are not required within the decoders for an RNN, because each token is only
    revealed to the network one at a time. But transformer attention matrices have
    access to the entire sequence all at once during training.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.5 Connections between encoder and decoder layers
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![encoder decoder drawio](images/encoder_decoder_drawio.png)'
  prefs: []
  type: TYPE_IMG
- en: 9.2.3 Transformer Translation Example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Transformers are suited for many tasks. The "Attention Is All You Need" paper
    showed off a transformer that achieved better translation accuracy than any preceding
    approach. Using `torchtext`, you will prepare the Multi30k dataset for training
    a Transformer for German-English translation using the `torch.nn.Transformer`
    module. In this section, you will customize the decoder half of the `Transformer`
    class to output the self-attention weights for each sublayer. You use the matrix
    of self-attention weights to explain how the words in the input German text were
    combined together to create the embeddings used to produce the English text in
    the output. After training the model you will use it for inference on a test set
    to see for yourself how well it translates German text into English.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the Data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You can use the Hugging Face datasets package to simplify bookkeeping and ensure
    your text is fed into the Transformer in a predictable format compatible with
    PyTorch. This is one of the trickiest parts of any deep learning project, ensuring
    that the structure and API for your dataset matches what your PyTorch training
    loop expects. Translation datasets are particularly tricky unless you use Hugging
    Face:'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.2 Load a translation dataset in Hugging Face format
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Not all Hugging Face datasets have predefined test and validation splits of
    the data. But you can always create your own splits using the `train_test_split`
    method as in listing [9.3](#listing-translation-dataset-split).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.3 Load a translation dataset in Hugging Face format
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: It’s always a good idea to examine some examples in your dataset before you
    start a long training run. This can help you make sure the data is what you expect.
    The `opus_books` doesn’t contain many books. So it’s not a very diverse (representative)
    sample of German. It has been segmented into only 50,000 aligned sentence pairs.
    Imagine having to learn German by having only a few translated books to read.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: If you would like to use a custom dataset of your own creation, it’s always
    a good idea to comply with an open standard like the Hugging Face datasets package
    shown in listing [9.2](#listing-hugging-face-translation-datasets) gives you a
    "best practice" approach to structuring your datasets. Notice that a translation
    dataset in Hugging Face contains an array of paired sentences with the language
    code in a dictionary. The `dict` keys of a translation example are the two-letter
    language code (from ISO 639-2)^([[17](#_footnotedef_17 "View footnote.")]). The
    `dict` values of an example text are the sentences in each of the two languages
    in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: You’ll avoid insidious, sometimes undetectable bugs if you resist the urge to
    invent your own data structure and instead use widely recognized open standards.
  prefs: []
  type: TYPE_NORMAL
- en: If you have access to a GPU, you probably want to use it for training transformers.
    Transformers are made for GPUs with their matrix multiplication operations for
    all the most computationally intensive parts of the algorithm. CPUs are adequate
    for most pre-trained Transformer models (except LLMs), but GPUs can save you a
    lot of time for training or fine-tuning a transformer. For example, GPT2 required
    3 days to train with a relatively small (40 MB) training dataset on a 16-core
    CPU. It trained in 2 hours for the same dataset on a 2560-core GPU (40x speedup,
    160x more cores). Listing [9.4](#listing-torch-gpu) will enable your GPU if one
    is available.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.4 Enable any available GPU
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'To keep things simple you can tokenize your source and target language texts
    separately with specialized tokenizers for each. If you use the Hugging Face tokenizers
    they will keep track of all of the special tokens that you’ll need for a transformer
    to work on almost any machine learning task:'
  prefs: []
  type: TYPE_NORMAL
- en: '**start-of-sequence token**::typically `"<SOS>"` or `"<s>"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**end-of-sequence token**::typically `"<EOS>"` or `"</s>"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**out-of-vocabulary (unknown) token**::typically `"<OOV>"`, `"<unk>"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mask token**::typically `"<mask>"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**padding token**::typically `"<pad>"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *start-of-sequence token* is used to trigger the decoder to generate a token
    that is suitable for the first token in a sequence. And many generative problems
    will require you to have an *end-of-sequence token*, so that the decoder knows
    when it can stop recursively generating more tokens. Some datasets use the same
    token for both the *start-of-sequence* and the *end-of-sequence* marker. They
    do not need to be unique because your decoder will always "know" when it is starting
    a new generation loop. The padding token is used to fill in the sequence at the
    end for examples shorter than the maximum sequence length. The mask token is used
    to intentionally hide a known token for training task-independent encoders such
    as BERT. This is similar to what you did in Chapter 6 for training word embeddings
    using skip grams.
  prefs: []
  type: TYPE_NORMAL
- en: You can choose any tokens for these marker (special) tokens, but you want to
    make sure that they are not words used within the vocabulary of your dataset.
    So if you are writing a book about natural language processing and you don’t want
    your tokenizer to trip up on the example SOS and EOS tokens, you may need to get
    a little more creative to generate tokens not found in your text.
  prefs: []
  type: TYPE_NORMAL
- en: Create a separate Hugging Face tokenizer for each language to speed up your
    tokenization and training and avoid having tokens leak from your source language
    text examples into your generated target language texts. You can use any language
    pair you like, but the original AIAYN paper demo examples usually translate from
    English (source) to German (target).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `ByteLevel` part of your BPE tokenizer ensures that your tokenizer will
    never miss a beat (or byte) as it is tokenizing your text. A byte-level BPE tokenizer
    can always construct any character by combining one of the 256 possible single-byte
    tokens available in its vocabulary. This means it can process any language that
    uses the Unicode character set. A byte-level tokenizer will just fall back to
    representing the individual bytes of a Unicode character if it hasn’t seen it
    before or hasn’t included it in its token vocabulary. A byte-level tokenizer will
    need an average of 70% more tokens (almost double the vocabulary size) to represent
    a new text containing characters or tokens that it hasn’t been trained on.
  prefs: []
  type: TYPE_NORMAL
- en: Character-level BPE tokenizers have their disadvantages too. A character-level
    tokenizer must hold each one of the multibyte Unicode characters in its vocabulary
    to avoid having any meaningless OOV (out-of-vocabulary) tokens. This can create
    a huge vocabulary for a multilingual transformer expected to handle most of the
    161 languages covered by Unicode characters. There are 149,186 characters with
    Unicode code points for both historical (Egyptian hieroglyphs for example) and
    modern written languages. That’s about 10 times the memory to store all the embeddings
    and tokens in your transformer’s tokenizer. In the real world, it is usually practical
    to ignore historical languages and some rare modern languages when optimizing
    your transformer BPE tokenizer for memory and balancing that with your transformer’s
    accuracy for your problem.
  prefs: []
  type: TYPE_NORMAL
- en: Important
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The BPE tokenizer is one of the five key "superpowers" of transformers that
    makes them so effective. And a `ByteLevel` BPE tokenizer isn’t quite as effective
    at representing the meaning of words even though it will never have OOV tokens.
    So in a production application, you may want to train your pipeline on both a
    character-level BPE tokenizer as well as a byte-level tokenizer. That way you
    can compare the results and choose the approach that gives you the best performance
    (accuracy and speed) for *your* application.
  prefs: []
  type: TYPE_NORMAL
- en: You can use your English tokenizer to build a preprocessing function that *flattens*
    the `Dataset` structure and returns a list of lists of token IDs (without padding).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: TranslationTransformer Model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: At this point, you have tokenized the sentences in the Multi30k data and converted
    them to tensors consisting of indexes into the vocabularies for the source and
    target languages, German and English, respectively. The dataset has been split
    into separate training, validation and test sets, which you have wrapped with
    iterators for batch training. Now that the data is prepared you turn your focus
    to setting up the model. Pytorch provides an implementation of the model presented
    in "Attention Is All You Need", `torch.nn.Transformer`. You will notice the constructor
    takes several parameters, familiar amongst them are `d_model=512`, `nhead=8`,
    `num_encoder_layers=6`, and `num_decoder_layers=6`. The default values are set
    to the parameters employed in the paper. Along with several other parameters for
    the feedforward dimension, dropout, and activation, the model also provides support
    for a `custom_encoder` and `custom_decoder`. To make things interesting, create
    a custom decoder that additionally outputs a list of attention weights from the
    multi-head self-attention layer in each sublayer of the decoder. It might sound
    complicated, but it’s actually fairly straightforward if you simply subclass `torch.nn.TransformerDecoderLayer`
    and `torch.nn.TransformerDecoder` and augment the *forward()* methods to return
    the auxiliary outputs - the attention weights.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.5 Extend torch.nn.TransformerDecoderLayer to additionally return multi-head
    self-attention weights
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Listing 9.6 Extend torch.nn.TransformerDecoder to additionally return list of
    multi-head self-attention weights
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The only change to `.forward()` from the parent’s version is to cache weights
    in the list member variable, `attention_weights`.
  prefs: []
  type: TYPE_NORMAL
- en: To recap, you have extended the `torch.nn.TransformerDecoder` and its sublayer
    component, `torch.nn.TransformerDecoderLayer`, mainly for exploratory purposes.
    That is, you save the multi-head self-attention weights from the different decoder
    layers in the Transformer model you are about to configure and train. The *forward()*
    methods in each of these classes copy the one in the parent nearly verbatim, with
    the exception of the changes called out to save the attention weights.
  prefs: []
  type: TYPE_NORMAL
- en: The `torch.nn.Transformer` is a somewhat bare-bones version of the sequence-to-sequence
    model containing the main secret sauce, the multi-head self-attention in both
    the encoder and decoder. If one looks at the source code for the module ^([[18](#_footnotedef_18
    "View footnote.")]), the model does not assume the use of embedding layers or
    positional encodings. Now you will create your *TranslationTransformer* model
    that uses the custom decoder components, by extending `torch.nn.Transformer` module.
    Begin with defining the constructor, which takes parameters `src_vocab_size` for
    a source embedding size, and `tgt_vocab_size` for the target, and uses them to
    initialize a basic `torch.nn.Embedding` for each. Notice a `PositionalEncoding`
    member, `pos_enc`, is created in the constructor for adding the word location
    information.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.7 Extend nn.Transformer for translation with a CustomDecoder
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Note the import of `rearrange` from the `einops` ^([[19](#_footnotedef_19 "View
    footnote.")]) package. Mathematicians like it for tensor reshaping and shuffling
    because it uses a syntax common in graduate level applied math courses. To see
    why you need to `rearrange()` your tensors refer to the `torch.nn.Transformer`
    documentation ^([[20](#_footnotedef_20 "View footnote.")]). If you get any one
    of the dimensions of any of the tensors wrong it will mess up the entire pipeline,
    sometimes invisibly.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.8 torch.nn.Transformer "shape" and dimension descriptions
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The datasets you created using `torchtext` are batch-first. So, borrowing the
    nomenclature in the Transformer documentation, your source and target tensors
    have shape *(N, S)* and *(N, T)*, respectively. To feed them to the `torch.nn.Transformer`
    (i.e. call its `forward()` method), the source and target must be reshaped. Also,
    you want to apply the embeddings plus the positional encoding to the source and
    target sequences. Additionally, a *padding key mask* is needed for each and a
    *memory key mask* is required for the target. Note, you can manage the embeddings
    and positional encodings outside the class, in the training and inference sections
    of the pipeline. However, since the model is specifically set up for translation,
    you make a stylistic/design choice to encapsulate the source and target sequence
    preparation within the class. To this end, you define `prepare_src()` and `prepare_tgt()`
    methods for preparing the sequences and generating the required masks.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.9 TranslationTransformer prepare_src()
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The `make_key_padding_mask()` method returns a tensor set to 1’s in the position
    of the padding token in the given tensor, and zero otherwise. The `prepare_src()`
    method generates the padding mask and then rearranges the `src` to the shape that
    the model expects. It then applies the positional encoding to the source embedding
    multiplied by the square root of the model’s dimension. This is taken directly
    from "Attention Is All You Need". The method returns the `src` with positional
    encoding applied, and the key padding mask for it.
  prefs: []
  type: TYPE_NORMAL
- en: The `prepare_tgt()` method used for the target sequence is nearly identical
    to `prepare_src()`. It returns the `tgt` adjusted for positional encodings, and
    a target key padding mask. However, it also returns a "subsequent" mask, `tgt_mask`,
    which is a triangular matrix for which columns (ones) in a row that are permitted
    to be observed. To generate the subsequent mask you use `Transformer.generate_square_subsequent_mask()`
    method defined in the base class as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.10 TranslationTransformer prepare_tgt()
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: You put `prepare_src()` and `prepare_tgt()` to use in the model’s `forward()`
    method. After preparing the inputs, it simply invokes the parent’s `forward()`
    and feeds the outputs through a Linear reduction layer after transforming from
    (T, N, E) back to batch first (N, T, E). We do this for consistency in our training
    and inference.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.11 TranslationTransformer forward()
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Also, define an `init_weights()` method that can be called to initialize the
    weights of all submodules of the Transformer. Xavier initialization is commonly
    used for Transformers, so use it here. The Pytorch `nn.Module` documentation ^([[21](#_footnotedef_21
    "View footnote.")]) describes the `apply(fn)` method that recursively applies
    `fn` to every submodule of the caller.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.12 TranslationTransformer init_weights()
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The individual components of the model have been defined and the complete model
    is shown in the next listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.13 TranslationTransformer complete model definition
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Finally, you have a complete transformer all your own! And you should be able
    to use it for translating between virtually any pair of languages, even character-rich
    languages such as traditional Chinese and Japanese. And you have explicit access
    to all the hyperparameters that you might need to tune your model for your problem.
    For example, you can increase the vocabulary size for the target or source languages
    to efficiently handle *character-rich* languages such as traditional Chinese and
    Japanese.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Traditional Chinese and Japanese (kanji) are called *character-rich* because
    they have a much larger number of unique characters that European languages. Chinese
    and Japanese languages use logograph characters. Logograph characters look a bit
    like small pictographs or abstract hieroglyphic drawings. For example, the kanji
    character "日" can mean day and it looks a little like the day block you might
    see on a calendar. Japanese logographic characters are roughly equivalent to word
    pieces somewhere between morphemes and words in the English language. This means
    that you will have many more unique characters in logographic languages than in
    European languages. For instance, traditional Japanese uses about 3500 unique
    kanji characters.^([[22](#_footnotedef_22 "View footnote.")]) English has roughly
    7000 unique syllables within the most common 20,000 words.
  prefs: []
  type: TYPE_NORMAL
- en: You can even change the number of layers in the encoder and decoder sides of
    the transformer, depending on the source (encoder) or target (decoder) language.
    You can even create a translation transformer that simplifies text for explaining
    complex concepts to five-year-olds, or adults on Mastodon server focused on ELI5
    ("explain it like I’m 5") conversations. If you reduce the number of layers in
    the decoder this will create a "capacity" bottleneck that can force your decoder
    to simplify or compress the concepts coming out of the encoder. Similarly, the
    number of attention heads in the encoder or decoder layers can be adjusted to
    increase or decrease the capacity (complexity) of your transformer.
  prefs: []
  type: TYPE_NORMAL
- en: Training the TranslationTransformer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now let’s create an instance of the model for our translation task and initialize
    the weights in preparation for training. For the model’s dimensions you use the
    defaults, which correlate to the sizes of the original "Attention Is All You Need"
    transformer. Know that since the encoder and decoder building blocks comprise
    duplicate, stackable layers, you can configure the model with any number of these
    layers.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.14 Instantiate a TranslationTransformer
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: PyTorch creates a nice `_\_str\_\_` representation of your model. It displays
    all the layers and their inner structure including the shapes of the inputs and
    outputs. You may even be able to see the parallels between the layers of your
    models and the diagrams of tranformers that you see in this chapter or online.
    From the first half of the text representation for your transformer, you can see
    that all of the encoder layers have exactly the same structure. The inputs and
    outputs of each `TransformerEncoderLayer` have the same shape, so this ensures
    that you can stack them without reshaping linear layers between them. Transformer
    layers are like the floors of a skyscraper or a child’s stack of wooden blocks.
    Each level has exactly the same 3D shape.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Notice that you set the sizes of your source and target vocabularies in the
    constructor. Also, you pass the indices for the source and target padding tokens
    for the model to use in preparing the source, targets, and associated masking
    sequences. Now that you have the model defined, take a moment to do a quick sanity
    check to make sure there are no obvious coding errors before you set up the training
    and inference pipeline. You can create "batches" of random integer tensors for
    the sources and targets and pass them to the model, as demonstrated in the following
    listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.15 Quick model sanity check with random tensors
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We created two tensors, `src` and `tgt`, each with random integers between
    1 and 100 distributed uniformly. Your model accepts tensors having batch-first
    shape, so we made sure that the batch sizes (10 in this case) were identical -
    otherwise we would have received a runtime error on the forward pass, that looks
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: It may be obvious, the source and target sequence lengths do not have to match,
    which is confirmed by the successful call to *model(src, tgt)*.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: When setting up a new sequence-to-sequence model for training, you may want
    to initially use smaller tunables in your setup. This includes limiting max sequence
    lengths, reducing batch sizes, and specifying a smaller number of training loops
    or epochs. This will make it easier to debug issues in your model and/or pipeline
    to get your program executing end-to-end more quickly. Be careful not to draw
    any conclusions on the capabilities/accuracy of your model at this "bootstrapping"
    stage; the goal is simply to get the pipeline to run.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you feel confident the model is ready for action, the next step is
    to define the optimizer and criterion for training. "Attention Is All You Need"
    used Adam optimizer with a warmup period in which the learning rate is increased
    followed by a decreasing rate for the duration of training. You will use a static
    rate, 1e-4, which is smaller than the default rate 1e-2 for Adam. This should
    provide for stable training as long as you are patient to run enough epochs. You
    can play with learning rate scheduling as an exercise if you are interested. Other
    Transformer based models you will look at later in this chapter use a static learning
    rate. As is common for this type of task, you use `torch.nn.CrossEntropyLoss`
    for the criterion.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.16 Optimizer and Criterion
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Ben Trevett contributed much of the code for the Pytorch Transformer Beginner
    tutorial. He, along with colleagues, has written an outstanding and informative
    Jupyter notebook series for their Pytorch Seq2Seq tutorial ^([[23](#_footnotedef_23
    "View footnote.")]) covering sequence-to-sequence models. Their Attention Is All
    You Need ^([[24](#_footnotedef_24 "View footnote.")]) notebook provides a from-scratch
    implementation of a basic transformer model. To avoid re-inventing the wheel,
    the training and evaluation driver code in the next sections is borrowed from
    Ben’s notebook, with minor changes.
  prefs: []
  type: TYPE_NORMAL
- en: The `train()` function implements a training loop similar to others you have
    seen. Remember to put the model into `train` mode before the batch iteration.
    Also, note that the last token in the target, which is the EOS token, is stripped
    from `trg` before passing it as input to the model. We want the model to predict
    the end of a string. The function returns the average loss per iteration.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.17 Model training function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The `evaluate()` function is similar to `train()`. You set the model to `eval`
    mode and use the `with torch.no_grad()` paradigm as usual for straight inference.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.18 Model evaluation function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Next a straightforward utility function `epoch_time()`, used for calculating
    the time elapsed during training, is defined as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.19 Utility function for elapsed time
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s proceed to setup the training. You set the number of epochs to 15,
    to give the model enough opportunities to train with the previously selected learning
    rate of 1e-4\. You can experiment with different combinations of learning rates
    and epoch numbers. In a future example, you will use an early stopping mechanism
    to avoid over-fitting and unnecessary training time. Here you declare a filename
    for `BEST_MODEL_FILE` and after each epoch, if the validation loss is an improvement
    over the previous best loss, the model is saved and the best loss is updated as
    shown.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.20 Run the TranslationTransformer model training and save the **best**
    model to file
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we could have probably run a few more epochs given that validation
    loss was still decreasing prior to exiting the loop. Let’s see how the model performs
    on a test set by loading the *best* model and running the `evaluate()` function
    on the test set.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.21 Load *best* model from file and perform evaluation on test data
    set
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Your translation transformer achieves a log loss of about 1.6 on the test set.
    For a translation model trained on such a small dataset, this is not too bad.
    Log loss of 1.59 corresponds to a 20% probability (`exp(-1.59)`) of generating
    the correct token and the exact position it was provided in the test set. Because
    there are many different correct English translations for a given German text,
    this is a reasonable accuracy for a model that can be trained on a commodity laptop.
  prefs: []
  type: TYPE_NORMAL
- en: TranslationTransformer Inference
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You are now convinced your model is ready to become your personal German-to-English
    interpreter. Performing translation requires only slightly more work to set up,
    which you do in the `translate_sentence()` function in the next listing. In brief,
    start by tokenizing the source *sentence* if it has not been tokenized already
    and end-capping it with the *<sos>* and *<eos>* tokens. Next, you call the `prepare_src()`
    method of the model to transform the *src* sequence and generate the source key
    padding mask as was done in training and evaluation. Then run the prepared `src`
    and `src_key_padding_mask` through the model’s encoder and save its output (in
    `enc_src`). Now, here is the fun part, where the target sentence (the translation)
    is generated. Start by initializing a list, `trg_indexes`, to the SOS token. In
    a loop - while the generated sequence has not reached a maximum length - convert
    the current prediction, *trg_indexes*, to a tensor. Use the model’s *prepare_tgt()*
    method to prepare the target sequence, creating the target key padding mask, and
    the target sentence mask. Run the current decoder output, the encoder output,
    and the two masks through the decoder. Get the latest predicted token from the
    decoder output and append it to *trg_indexes*. Break out of the loop if the prediction
    was an *<eos>* token (or if maximum sentence length is reached). The function
    returns the target indexes converted to tokens (words) and the attention weights
    from the decoder in the model.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.22 Define *translate_sentence()* for performing inference
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Your `translate_sentence()` wraps up your big transformer into a handy package
    you can use to translate whatever German sentence you run across.
  prefs: []
  type: TYPE_NORMAL
- en: TranslationTransformer Inference Example 1
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now you can use your `translate_sentence()` function on an example text. Since
    you probably do not know German, you can use a random example from the test data.
    Try it for the sentence "Eine Mutter und ihr kleiner Sohn genießen einen schönen
    Tag im Freien." In the OPUS dataset the character case was folded so that the
    text you feed into your transformer should be "eine mutter und ihr kleiner sohn
    genießen einen schönen tag im freien." And the correct translation that you’re
    looking for is: "A mother and her little [or young] son are enjoying a beautiful
    day outdoors."'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.23 Load sample at *test_data* index 10
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: It looks like the OPUS dataset is not perfect - the target (translated) token
    sequence is missing the verb "are" between "song" and "enjoying". And, the German
    word "kleiner" can be translated as little or young, but OPUS dataset example
    only provides one possible "correct" translation. And what about that "young song,"
    that seems odd. Perhaps that’s a typo in the OPUS test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Now you can run the `src` token sequence through your translator to see how
    it deals with that ambiguity.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.24 Translate the test data sample
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Interestingly, it appears there is a typo in the translation of the German word
    for "son" ("sohn") in the OPUS dataset. The dataset incorrectly translates "sohn"
    in German to "song" in English. Based on context, it appears the model did well
    to infer that a mother is (probably) with her young (little) "son". The model
    gives us the adjective "little" instead of "young", which is acceptable, given
    that the direct translation of the German word "kleiner" is "smaller".
  prefs: []
  type: TYPE_NORMAL
- en: Let’s focus our attention on, um, *attention*. In your model, you defined a
    *CustomDecoder* that saves the average attention weights for each decoder layer
    on each forward pass. You have the *attention* weights from the translation. Now
    write a function to visualize self-attention for each decoder layer using `matplotlib`.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.25 Function to visualize self-attention weights for decoder layers
    of the TranslationTransformer
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The function plots the attention values at each index in the sequence with the
    original sentence on the x-axis and the translation along the y-axis. We use the
    *gist_yarg* color map since it’s a gray-scale scheme that is printer-friendly.
    Now you display the attention for the "mother and son enjoying the beautiful day"
    sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.26 Visualize the self-attention weights for the test example translation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Looking at the plots for the initial two decoder layers we can see that an area
    of concentration is starting to develop along the diagonal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.6 Test Translation Example: Decoder Self-Attention Layers 1 and 2'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![translation attention 1 2](images/translation_attention_1_2.png)'
  prefs: []
  type: TYPE_IMG
- en: In the subsequent layers, three and four, the focus is appearing to become more
    refined.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.7 Test Translation Example: Decoder Self-Attention Layers 3 and 4'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![translation attention 3 4](images/translation_attention_3_4.png)'
  prefs: []
  type: TYPE_IMG
- en: In the final two layers, we see the attention is strongly weighted where direct
    word-to-word translation is done, along the diagonal, which is what you likely
    would expect. Notice the shaded clusters of article-noun and adjective-noun pairings.
    For example, "son" is clearly weighted on the word "sohn", yet there is also attention
    given to "kleiner".
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.8 Test Translation Example: Decoder Self-Attention Layers 5 and 6'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![translation attention 5 6](images/translation_attention_5_6.png)'
  prefs: []
  type: TYPE_IMG
- en: You selected this example arbitrarily from the test set to get a sense of the
    translation capability of the model. The attention plots appear to show that the
    model is picking up on relations in the sentence, but the word importance is still
    strongly positional in nature. By that, we mean the German word at the current
    position in the original sentence is generally translated to the English version
    of the word at the same or similar position in the target output.
  prefs: []
  type: TYPE_NORMAL
- en: TranslationTransformer Inference Example 2
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Have a look at another example, this time from the validation set, where the
    ordering of clauses in the input sequence and the output sequence are different,
    and see how the attention plays out. Load and print the data for the validation
    sample at index 25 in the next listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.27 Load sample at *valid_data* index 25
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Even if your German comprehension is not great, it seems fairly obvious that
    the *orange toy* ("orangen spielzeug") is at the end of the source sentence, and
    the *in the tall grass* is in the middle. In the English sentence, however, "in
    tall grass" completes the sentence, while "with an orange toy" is the direct recipient
    of the "play" action, in the middle part of the sentence. Translate the sentence
    with your model.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.28 Translate the validation data sample
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: This is a pretty exciting result for a model that took about 15 minutes to train
    (depending on your computing power). Again, plot the attention weights by calling
    the *display_attention()* function with the *src*, *translation* and *attention*.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.29 Visualize the self-attention weights for the validation example
    translation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Here we show the plots for the last two layers (5 and 6).
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.9 Validation Translation Example: Decoder Self-Attention Layers 5
    and 6'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![translation attention validation 5 6](images/translation_attention_validation_5_6.png)'
  prefs: []
  type: TYPE_IMG
- en: This sample excellently depicts how the attention weights can break from the
    position-in-sequence mold and actually attend to words later or earlier in the
    sentence. It truly shows the uniqueness and power of the multi-head self-attention
    mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: To wrap up the section, you will calculate the BLEU (bilingual evaluation understudy)
    score for the model. The `torchtext` package supplies a function, *bleu_score*,
    for doing the calculation. You use the following function, again from Mr. Trevett’s
    notebook, to do inference on a dataset and return the score.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Calculate the score for your test data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: To compare to Ben Trevett’s tutorial code, a convolutional sequence-to-sequence
    model ^([[25](#_footnotedef_25 "View footnote.")]) achieves a 33.3 BLEU and the
    smaller-scale Transformer scores about 35\. Your model uses the same dimensions
    of the original "Attention Is All You Need" Transformer, hence it is no surprise
    that it performs well.
  prefs: []
  type: TYPE_NORMAL
- en: 9.3 Bidirectional backpropagation and "BERT"
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes you want to predict something in the middle of a sequence — perhaps
    a masked-out word. Transformers can handle that as well. And the model doesn’t
    need to be limited to reading your text from left to right in a "causal" way.
    It can read the text from right to left on the other side of the mask as well.
    When generating text, the unknown word your model is trained to predict is at
    the end of the text. But transformers can also predict an interior word, for example,
    if you are trying to unredact the secret blacked-out parts of the Mueller Report.
  prefs: []
  type: TYPE_NORMAL
- en: When you want to predict an unknown word *within* your example text you can
    take advantage of the words before and *after* the masked word. A human reader
    or an NLP pipeline can start wherever they like. And for NLP you always have a
    particular piece of text, with finite length, that you want to process. So you
    could start at the end of the text or the beginning…​ or *both*! This was the
    insight that BERT used to create task-independent embeddings of any body of text.
    It was trained on the general task of predicting masked-out words, similar to
    how you learned to train word embeddings using skip-grams in Chapter 6\. And,
    just as in word embedding training, BERT created a lot of useful training data
    from unlabeled text simply by masking out individual words and training a bidirectional
    transformer model to restore the masked word.
  prefs: []
  type: TYPE_NORMAL
- en: In 2018, researchers at Google AI unveiled a new language model they call BERT,
    for "Bi-directional Encoder Representations from Transformers" ^([[26](#_footnotedef_26
    "View footnote.")]). The "B" in "BERT" is for "bidirectional." It isn’t named
    for a Sesame Street character it means "Bidirectional Encoder Representations
    from Transformers" - basically just a bidirectional transformer. Bidirectional
    transformers were a huge leap forward for machine-kind. In the next chapter, chapter
    10, you will learn about the three tricks that helped Transformers (souped-up
    RNNs) reach the top of the leaderboard for many of the hardest NLP problems. Giving
    RNNs the ability to read in both directions simultaneously was one of these innovative
    tricks that helped machines surpass humans at reading comprehension tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The BERT model, which comes in two flavors (configurations) — BERT *BASE* and
    BERT *LARGE* — is comprised of a stack of encoder transformers with feedforward
    and attention layers. Different from transformer models that preceded it, like
    OpenAI GPT, BERT uses masked language modeling (MLM) objective to train a deep
    bi-directional transformer. MLM involves randomly masking tokens in the input
    sequence and then attempting to predict the actual tokens from context. More powerful
    than typical left-to-right language model training, the MLM objective allows BERT
    to better generalize language representations by joining the left and right context
    of a token in all layers. The BERT models were pre-trained in a semi-unsupervised
    fashion on the English Wikipedia sans tables and charts (2500M words), and the
    BooksCorpus (800M words and upon which GPT was also trained). With simply some
    tweaks to inputs and the output layer, the models can be fine tuned to achieve
    state-of-the-art results on specific sentence-level and token-level tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.1 Tokenization and Pre-training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The input sequences to BERT can ambiguously represent a single sentence or
    a pair of sentences. BERT uses WordPiece embeddings with the first token of each
    sequence always set as a special *[CLS]* token. Sentences are distinguished by
    a trailing separator token, *[SEP]*. Tokens in a sequence are further distinguished
    by a separate segment embedding with either sentence A or B assigned to each token.
    Additionally, a positional embedding is added to the sequence, such that each
    position the input representation of a token is formed by summation of the corresponding
    token, segment, and positional embeddings as shown in the figure below (from the
    published paper):'
  prefs: []
  type: TYPE_NORMAL
- en: '![bert inputs](images/bert_inputs.png)'
  prefs: []
  type: TYPE_IMG
- en: During pre-training a percentage of input tokens are masked randomly (with a
    *[MASK]* token) and the model the model predicts the actual token IDs for those
    masked tokens. In practice, 15% of the WordPiece tokens were selected to be masked
    for training, however, a downside of this is that during fine-tuning there is
    no *[MASK]* token. To work around this, the authors came up with a formula to
    replace the selected tokens for masking (the 15%) with the *[MASK]* token 80%
    of the time. For the other 20%, they replace the token with a random token 10%
    of the time and keep the original token 10% of the time. In addition to this MLM
    objective pre-training, secondary training is done for Next Sentence Prediction
    (NSP). Many downstream tasks, such as Question Answering (QA), depend upon understanding
    the relationship between two sentences, and cannot be solved with language modeling
    alone. For the NSP wave of training, the authors generated a simple binarized
    NSP task by selecting pairs of sentences A and B for each sample and labeling
    them as *IsNext* and *NotNext*. Fifty percent of the samples for the pre-training
    had selections where sentence B followed sentence A in the corpus, and for the
    other half sentence B was chosen at random. This plain solution shows that sometimes
    one need not overthink a problem.
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.2 Fine-tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For most BERT tasks, you will want to load the BERT[BASE] or BERT[LARGE] model
    with all its parameters initialized from the pre-training and fine tune the model
    for your specific task. The fine-tuning should typically be straightforward; one
    simply plugs in the task-specific inputs and outputs and then commences training
    all parameters end-to-end. Compared to the initial pre-training, the fine-tuning
    of the model is much less expensive. BERT is shown to be more than capable on
    a multitude of tasks. For example, at the time of its publication, BERT outperformed
    the current state-of-the-art OpenAI GPT model on the General Language Understanding
    Evaluation (GLUE) benchmark. And BERT bested the top-performing systems (ensembles)
    on the Stanford Question Answering Dataset (SQuAD v1.1), where the task is to
    select the text span from a given Wikipedia passage that provides the answer to
    a given question. Unsurprisingly, BERT was also best at a variation of this task,
    SQuAD v2.0, where it is allowed that a short answer for the problem question in
    the text might not exist.
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.3 Implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Borrowing from the discussion on the original transformer earlier in the chapter,
    for the BERT configurations, *L* denotes the number of transformer layers. The
    hidden size is *H* and the number of self-attention heads is *A*. BERT[BASE] has
    dimensions *L*=12, *H*=768, and *A*=12, for a total of 110M parameters. BERT[LARGE]
    has *L*=24, *H*=1024, and *A*=16 for 340M total parameters! The large model outperforms
    the base model on all tasks, however depending on hardware resources available
    to you, you may find working with the base model more than adequate. There are
    are *cased* and *uncased* versions of the pre-trained models for both, the base
    and large configurations. The *uncased* version had the text converted to all
    lowercase before pre-training WordPiece tokenization, while there were no changes
    made to the input text for the *cased* model.
  prefs: []
  type: TYPE_NORMAL
- en: The original BERT implementation was open-sourced as part of the TensorFlow
    *tensor2tensor* library ^([[27](#_footnotedef_27 "View footnote.")]) A *Google
    Colab* notebook ^([[28](#_footnotedef_28 "View footnote.")]) demonstrating how
    to fine tune BERT for sentence-pair classification tasks was published by the
    TensorFlow Hub authors circa the time the BERT academic paper was released. Running
    the notebook requires registering for access to Google Cloud Platform Compute
    Engine and acquiring a Google Cloud Storage bucket. At the time of this writing,
    it appears Google continues to offer monetary credits for first-time users, but
    generally, you will have to pay for access to computing power once you have exhausted
    the initial trial offer credits.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As you go deeper into NLP models, especially with the use of models having deep
    stacks of transformers, you may find that your current computer hardware is insufficient
    for computationally expensive tasks of training and/or fine-tuning large models.
    You will want to evaluate the costs of building out a personal computer to meet
    your workloads and weigh that against pay-per-use cloud and virtual computing
    offerings for AI. We reference basic hardware requirements and compute options
    in this text, however, discussion of the "right" PC setup or providing an exhaustive
    list of competitive computing options are outside the scope of this book. In addition
    to the Google Compute Engine, just mentioned, the appendix has instructions for
    setting up Amazon Web Services (AWS) GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Pytorch versions of BERT models were implemented in the `pytorch-pretrained-bert`
    library ^([[29](#_footnotedef_29 "View footnote.")]) and then later incorporated
    in the indispensable HuggingFace *transformers* library ^([[30](#_footnotedef_30
    "View footnote.")]). You would do well to spend some time reading the "Getting
    Started" documentation and the summaries of the transformer models and associated
    tasks on the site. To install the transformers library, simply use `pip install
    transformers`. Once installed, import the BertModel from transformers using the
    `BertModel.from_pre-trained()` API to load one by name. You can print a summary
    for the loaded "bert-base-uncased" model in the listing that follows, to get an
    idea of the architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.30 Pytorch summary of BERT architecture
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: After import a BERT model you can display its string representation to get a
    summary of its structure. This is a good place to start if you are considering
    designing your own custom bidirectional transformer. But in most cases, you can
    use BERT directly to create encodings of English text that accurately represent
    the meaning of most text. A pretrained BERT model is all you may need for applications
    such as chatbot intent labeling (classification or tagging), sentiment analysis,
    social media moderation, semantic search, and FAQ question answering. And if you’re
    considering storing embeddings in a vector database for semantic search, vanilla
    BERT encodings are your best bet.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you’ll see an example of how to use a pretrained BERT model
    to identify toxic social media messages. And then you will see how to fine tune
    a BERT model for your application by training it for additional epochs on your
    dataset. You will see that fine tuning BERT can significantly improve your toxic
    comment classification accuracy without overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.4 Fine-tuning a pre-trained BERT model for text classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In 2018, the Conversation AI ^([[31](#_footnotedef_31 "View footnote.")]) team
    (a joint venture between Jigsaw and Google) hosted a Kaggle competition to develop
    a model to detect various types of toxicity in a online social media posts. At
    the time, LSTM’s and Convolutional Neural Networks were the state of the art.
    Bi-directional LSTMs with attention achieved the best scores in this competition.
    The promise of BERT is that it can simultaneously learn word context from words
    both left and right of the current word being processed by the transformer. This
    makes it especially useful for creating multipurpose encoding or embedding vectors
    for use in classification problems like detecting toxic social media comments.
    And because BERT is pre-trained on a large corpus, you don’t need a huge dataset
    or supercomputer to be able to fine tune a model that achieves good performance
    using the power of *transfer learning*.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you will use the library to quickly fine tune a pre-trained
    BERT model for classifying toxic social media posts. After that, you will make
    some adjustments to improve the model in your quest to combat bad behavior and
    rid the world of online trolls.
  prefs: []
  type: TYPE_NORMAL
- en: A toxic dataset
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You can download the "Toxic Comment Classification Challenge" dataset (`archive.zip`)
    from kaggle.com. ^([[32](#_footnotedef_32 "View footnote.")]) You can put the
    data in your `$HOME/.nlpia2-data/` directory with all the other large datasets
    from the book, if you like. When you unzip the `archive.zip` file you’ll see it
    contains the training set (`train.csv`) and test set (`test.csv`) as separate
    CSV files. In the real world you would probably combine the training and test
    sets to create your own sample of validation and test examples. But to make your
    results comparable to what you see on the competition website you will first only
    work with the training set.
  prefs: []
  type: TYPE_NORMAL
- en: Begin by loading the training data using pandas and take a look at the first
    few entries as shown in the next listing. Normally you would wan to take a look
    at examples from the dataset to get a feel for the data and see how it is formatted.
    It’s usually helpful to try to do the same task that you are asking the model
    to do, to see if it’s a reasonable problem for NLP. Here are the first five examples
    in the training set. Fortunately the dataset is sorted to contain the nontoxic
    posts first, so you won’t have to read any toxic comments until the very end of
    this section. If you have a grandmother named "Terri" you can close your eyes
    at the last line of code in the last code block of in this section `;-)`.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.31 Load the toxic comments dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Whew, luckily none of the first five comments are obscene, so they’re fit to
    print in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Spend time with the data.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Typically at this point you would explore and analyze the data, focusing on
    the qualities of the text samples and the accuracy of the labels and perhaps ask
    yourself questions about the data. How long are the comments in general? Does
    sentence length or comment length have any relation to toxicity? Consider focusing
    on some of the *severe_toxic* comments. What sets them apart from the merely *toxic*
    ones? What is the class distribution? Do you need to potentially account for a
    class imbalance in your training techniques?
  prefs: []
  type: TYPE_NORMAL
- en: You want to get to the training, so let’s split the data set into training and
    validation (evaluation) sets. With almost 160,000 samples available for model
    tuning, we elect to use an 80-20 train-test split.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.32 Split data into training and validation sets
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Now you have your data in a Pandas DataFrame with descriptive column names you
    can use to interpret the test results for your model.
  prefs: []
  type: TYPE_NORMAL
- en: There’s one last ETL task for you to deal with, you need a wrapper function
    to ensure the batches of examples passed to your transformer have the right shape
    and content. You are going to use the `simpletransformers` library which provides
    wrappers for various Hugging Face models designed for classification tasks including
    multilabel classification, not to be confused with multiclass or multioutput classification
    models. ^([[33](#_footnotedef_33 "View footnote.")]) The Scikit-Learn package
    also contains a `MultiOutputClassifier` wrapper that you can use to create multiple
    estimators (models), one for each possible target label you want to assign to
    your texts.
  prefs: []
  type: TYPE_NORMAL
- en: Important
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A multilabel classifier is a model that outputs multiple different predicted
    discrete classification labels ('toxic', 'severe', and 'obscene') for each input.
    This allows your text to be given multiple different labels. Like a fictional
    family in Tolstoy’s *Anna_Karenina*, a toxic comment can be toxic in many different
    ways, all at the same time. You can think of a multilabel classifier as applying
    hashtags or emojis to a text. To prevent confusion you can call your models "taggers"
    or "tagging models" so others don’t misunderstand you.
  prefs: []
  type: TYPE_NORMAL
- en: Since each comment can be assigned multiple labels (zero or more) the `MultiLabelClassificationModel`
    is your best bet for this kind of problem. According to the documentation,^([[34](#_footnotedef_34
    "View footnote.")]) the `MultiLabelClassificationModel` model expects training
    samples in the format of `["text", [label1, label2, label3, …​]]`. This keeps
    the outer shape of the dataset the same no matter how many different kinds of
    toxicity you want to keep track of. The Hugging Face `transformers` models can
    handle any number of possible labels (tags) with this data structure, but you
    need to be consistent within your pipeline, using the same number of possible
    labels for each example. You need a *multihot* vector of zeros and ones with a
    constant number of dimensions so your model knows where to put the predictions
    for each kind of toxicity. The next listing shows how you can arrange the batches
    of data within a wrapper function that you run during training and evaluation
    of you model.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.33 Create datasets for model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: You can now see that this dataset has a pretty low bar for toxicity if mothers
    and grandmothers are the target of bullies' insults. This means this dataset may
    be helpful even if you have extremely sensitive or young users that you are trying
    to protect. If you are trying to protect modern adults or digital natives that
    are used to experiencing cruelty online, you can augment this dataset with more
    extreme examples from other sources.
  prefs: []
  type: TYPE_NORMAL
- en: Detect toxic comments with `simpletransformers`
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You now have a function for passing batches of labeled texts to the model and
    printing some messages to monitor your progress. So it’s time to choose a BERT
    model to download. You need to set up just a few basic parameters and then you
    will be ready to load a pre-trained BERT for multi-label classification and kick
    off the fine-tuning (training).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.34 Setup training parameters
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: In the listing below you load the pre-trained `bert-base-cased` model configured
    to output the number of labels in our toxic comment data (6 total) and initialized
    for training with your `model_args` dictionary.^([[35](#_footnotedef_35 "View
    footnote.")])
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.35 Load pre-trained model and fine tune
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The `train_model()` is doing the heavy lifting for you. It loads the pre-trained
    `BertTokenizer` for the pre-trained *bert-base-cased* model you selected and uses
    it to tokenize the `train_df['text']` to inputs for training the model. The function
    combines these inputs with the `train_df[labels]` to generate a `TensorDataset`
    which it wraps with a PyTorch `DataLoader`, that is then iterated over in batches
    to comprise the training loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words, with just a few lines of code and one pass through your data
    (one epoch) you’ve fine tuned a 12-layer transformer with 110 million parameters!
    The next question is: did it help or hurt the model’s translation ability? Let’s
    run inference on your evaluation set and check the results.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.36 Evaluation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The ROC (Receiver Operating Characteristic curve) AUC (Area Under the Curve)
    metric balances all the different ways a classifier can be wrong by computing
    the integral (area) under the precision vs recall plot (curve) for a classifier.
    This ensures that models that are confidently wrong are penalized more than models
    that are closer to the truth with their predicted probability values. And the
    `roc_auc_score` within this `simpletransformers` package will give you the micro
    average of all the examples and all the different labels it could have chosen
    for each text.
  prefs: []
  type: TYPE_NORMAL
- en: The ROC AUC micro average score is essentially the sum of all the `predict_proba`
    error values, or how far the predicted probability values are from the 0 or 1
    values that each example was given by a human labeler. It’s always a good idea
    to have that mental model in mind when your are measuring model accuracy. Accuracy
    is just how close to what your human labelers thought the correct answer was,
    not some absolute truth about the meaning or intent or effects of the words that
    are being labeled. Toxicity is a very subjective quality.
  prefs: []
  type: TYPE_NORMAL
- en: A `roc_auc_score` of 0.981 is not too bad out of the gate. While it’s not going
    to win you any accolades ^([[36](#_footnotedef_36 "View footnote.")]), it does
    provide encouraging feedback that your training simulation and inference is setup
    correctly.
  prefs: []
  type: TYPE_NORMAL
- en: The implementations for `eval_model()` and `train_model()` are found in the
    base class for both `MultiLabelClassificationModel` and `ClassificationModel`.
    The evaluation code will look familiar to you, as it uses the `with torch.no_grad()`
    context manager for doing inference, as one would expect. Taking the time to look
    at the method implementations is suggested. Particularly, `train_model()` is helpful
    for viewing exactly how the configuration options you select in the next section
    are employed during training and evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: A better BERT
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now that you have a first cut at a model you can do some more fine tuning to
    help your BERT-based model do better. And "better" in this case simply means having
    a higher AUC score. Just like in the real world, you’ll need to decide what better
    is in your particular case. So don’t forget to pay attention to how the model’s
    predictions are affecting the user experience for the people or businesses using
    your model. If you can find a better metric that more directly measures what "better"
    means for your users you should use that in place of the AUC score for your application
    you should substitute it in this code.
  prefs: []
  type: TYPE_NORMAL
- en: Building upon the training code you executed in the previous example, you’ll
    work on improving your model’s accuracy. Cleaning the text a bit with some preprocessing
    is fairly straightforward. The book’s example source code comes with a utility
    `TextPreprocessor` class we authored to replace common misspellings, expand contractions
    and perform other miscellaneous cleaning such as removing extra white-space characters.
    Go ahead and rename the `comment_text` column to `original_text` in the loaded
    *train.csv* dataframe. Apply the preprocessor to the original text and store the
    refined text back in a `comment_text` column.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.37 Preprocessing the comment text
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: With the text cleaned, turn your focus to tuning the model initialization and
    training parameters. In your first training run, you accepted the default input
    sequence length (128) as an explicit value for `max_sequence_length` was not provided
    to the model. The BERT-base model can handle sequences of a maximum length of
    512\. As you increase `max_sequence_length` you may need to decrease `train_batch_size`
    and `eval_batch_size` to fit tensors into GPU memory, depending on the hardware
    available to you. You can do some exploration on the lengths of the comment text
    to find an optimal max length. Be mindful that at some point you’ll get diminishing
    returns, where longer training and evaluation times incurred by using larger sequences
    do not yield a significant improvement in model accuracy. For this example pick
    a `max_sequence_length` of 300, which is between the default of 128 and the model’s
    capacity. Also explicitly select `train_batch_size` and `eval_batch_size` to fit
    into GPU memory.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: You’ll quickly realize your batch sizes are set too large if a GPU memory exception
    is displayed shortly after training or evaluation commences. And you don’t necessarily
    want to maximize the batch size based on this warning. The warning may only appear
    late in your training runs and ruin a long running training session. And larger
    isn’t always better for the `batch_size` parameter. Sometimes smaller batch sizes
    will help your training be a bit more stochastic (random) in its gradient descent.
    Being more random can sometimes help your model jump over ridges and saddle points
    in your the high dimensional nonconvex error surface it is trying to navigate.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that in your first fine-tuning run, the model trained for exactly one
    epoch. Your hunch that the model could have trained longer to achieve better results
    is likely correct. You want to find the sweet spot for the amount of training
    to do before the model overfits on the training samples. Configure options to
    enable evaluation during training so you can also set up the parameters for early
    stopping. The evaluation scores during training are used to inform early stopping.
    So set `evaluation_during_training=True` to enable it, and set `use_early_stopping=True`
    also. As the model learns to generalize, we expect oscillations in performance
    between evaluation steps, so you don’t want to stop training just because the
    accuracy declined from the previous value in the latest evaluation step. Configure
    the *patience* for early stopping, which is the number of consecutive evaluations
    without improvement (defined to be greater than some delta) at which to terminate
    the training. You’re going to set `early_stopping_patience=4` because you’re somewhat
    patient but you have your limits. Use `early_stopping_delta=0` because no amount
    of improvement is too small.
  prefs: []
  type: TYPE_NORMAL
- en: Saving these transformers models to disk repeatedly during training (e.g. after
    each evaluation phase or after each epoch) takes time and disk space. For this
    example, you’re looking to keep the *best* model generated during training, so
    specify `best_model_dir` to save your best-performing model. It’s convenient to
    save it to a location under the `output_dir` so all your training results are
    organized as you run more experiments on your own.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.38 Setup parameters for evaluation during training and early stopping
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Train the model by calling `model.train_model()`, as you did previously. One
    change is that you are now going to `evaluate_during_training` so you need to
    include an `eval_df` (your validation data set). This allows your training routine
    to estimate how well your model will perform in the real world while it is still
    training the model. If the validation accuracy starts to degrade for several (`early_stoping_patience`)
    epochs in a row, your model will stop the training so it doesn’t continue to get
    worse.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.39 Load pre-trained model and fine tune with early stopping
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Your *best* model was saved during training in the `best_model_dir`. It should
    go without saying that this is the model you want to use for inference. The evaluation
    code segment is updated to load the model by passing `best_model_dir` for the
    `model_name` parameter in the model class' constructor.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.40 Evaluation with the best model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Now that’s looking better. A 0.989 accuracy puts us in contention with the top
    challenge solutions of early 2018\. And perhaps you think that 98.9% accuracy
    may be a little too good to be true. You’d be right. Someone fluent in German
    would need to dig into several of the translations to find all the translation
    errors your model is making. And the false negatives — test examples incorrectly
    marked as correct — would be even harder to find.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re like me, you probably don’t have a fluent German translator lying
    around. So here’s a quick example of a more English-focused translation application
    that you may be able to appreciate more, grammar checking and correcting. And
    even if you are still an English learner, you can appreciate the benefit of having
    a personalized tool to help you write. A personalized grammar checker may be your
    personal killer app that helps you develop strong communication skills and advance
    your NLP career.
  prefs: []
  type: TYPE_NORMAL
- en: 9.4 Test Yourself
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How is the input and output dimensionality of a transformer layer different
    from any other deep learning layer like CNN, RNN, or LSTM layers?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How could you expand the information capacity of a transformer network like
    BERT or GPT-2?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a rule of thumb for estimating the information capacity required to
    get high accuracy on a particular labeled dataset?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a good measure of the relative information capacity of 2 deep learning
    networks?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are some techniques for reducing the amount of labeled data required to
    train a transformer for a problem like summarization?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you measure the accuracy or loss of a summarizer or translator where
    there can be more than one right answer?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 9.5 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By keeping the inputs and outputs of each layer consistent, transformers gained
    their key superpower — infinite stackabilty
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Transformers combine three key innovations to achieve world-changing NLP power:
    BPE tokenization, multi-head attention, and positional encoding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The GPT transformer architecture is the best choice for most text generation
    tasks such as translation and conversational chatbots
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Despite being more than 5 years old (when this book was released) the BERT transformer
    model is still the right choice for most NLU problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you chose a pretrained model that is efficient, you can fine-tune it to achieve
    competitive results for many difficult Kaggle problems, using only affordable
    hardware such as a laptop or free online GPU resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[1]](#_footnoteref_1) For months following ChatGPT’s public release, Dan Miessler
    spent almost half of his "Unsupervised Learning" podcasts discussing transformer-based
    tools such as InstructGPT, ChatGPT, Bard and Bing ( [https://danielmiessler.com/](danielmiessler.com.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[2]](#_footnoteref_2) PineCone.io, Milvus.io, Vespa.ai, Vald.vdaas.org use
    transformers'
  prefs: []
  type: TYPE_NORMAL
- en: '[[3]](#_footnoteref_3) "Swiss cheese model" on Wikipedia ( [https://en.wikipedia.org/wiki/Swiss_cheese_model](wiki.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[4]](#_footnoteref_4) Sebastian Larson, an actual middle schooler, won our
    competition to develop Rori’s `mathtext` NLP algorithm ( [https://gitlab.com/tangibleai/community/team/-/tree/main/exercises/2-mathtext](exercises.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[5]](#_footnoteref_5) All of Rori.AI’s NLP code is open source and available
    on Huggingface ( [https://huggingface.co/spaces/TangibleAI/mathtext-fastapi](TangibleAI.html)).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[6]](#_footnoteref_6) Vish built an transformer-based teaching assistant called
    Clevrly (clevrly.io)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[7]](#_footnoteref_7) Some of Vish’s fine tuned transformers are available
    on Huggingface ( [https://huggingface.co/clevrly](huggingface.co.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[8]](#_footnoteref_8) Stats Stack Exchange answer ( [https://stats.stackexchange.com/a/422898/15974](422898.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[9]](#_footnoteref_9) N-Gram Viewer query "embedding_NOUN" / "encoding_NOUN"
    ( [https://books.google.com/ngrams/graph?content=embedding_NOUN+%2F+encoding_NOUN&year_start=2010&year_end=2019&corpus=en-2019&smoothing=3](ngrams.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[10]](#_footnoteref_10) "Gentle Introduction to Transduction in Machine Learning"
    blog post on *Machine Learning Mastery* by Jason Brownlee 2017 ( [https://machinelearningmastery.com/transduction-in-machine-learning/](transduction-in-machine-learning.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[11]](#_footnoteref_11) Neural Machine Translation by Jointly Learning to
    Align and Translate: [https://arxiv.org/abs/1409.0473](abs.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[12]](#_footnoteref_12) "Attention Is All You Need" by Vaswani, Ashish et
    al. 2017 at Google Brain and Google Research ( [https://arxiv.org/abs/1706.03762](abs.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[13]](#_footnoteref_13) "Scaled dot product attention from scratch" by Jason
    Brownlee ( [https://machinelearningmastery.com/how-to-implement-scaled-dot-product-attention-from-scratch-in-tensorflow-and-keras/](how-to-implement-scaled-dot-product-attention-from-scratch-in-tensorflow-and-keras.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[14]](#_footnoteref_14) "Attention is all you Need" by Ashish Vaswani et al
    2017 ( [https://arxiv.org/abs/1706.03762](abs.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[15]](#_footnoteref_15) [http://www.adeveloperdiary.com/data-science/deep-learning/nlp/machine-translation-using-attention-with-pytorch/](machine-translation-using-attention-with-pytorch.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[16]](#_footnoteref_16) Pytorch Sequence-to-Sequence Modeling With nn.Transformer
    Tutorial: [https://simpletransformers.ai/docs/multi-label-classification/](multi-label-classification.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[17]](#_footnoteref_17) List of ISO 639 language codes on Wikipedia ( [https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes](wiki.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[18]](#_footnoteref_18) Pytorch nn.Transformer source: [https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/transformer.py](modules.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[19]](#_footnoteref_19) einops: [https://github.com/arogozhnikov/einops](arogozhnikov.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[20]](#_footnoteref_20) Pytorch torch.nn.Transformer documentation: [https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html](generated.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[21]](#_footnoteref_21) Pytorch nn.Module documentation: [https://pytorch.org/docs/stable/generated/torch.nn.Module.html](generated.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[22]](#_footnoteref_22) Japanese StackExchange answer with counts of Japanese
    characters ( [https://japanese.stackexchange.com/a/65653/56506](65653.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[23]](#_footnoteref_23) Trevett,Ben - PyTorch Seq2Seq: [https://github.com/bentrevett/pytorch-seq2seq/](pytorch-seq2seq.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[24]](#_footnoteref_24) Trevett,Ben - Attention Is All You Need Jupyter notebook:
    [https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb/](6%20-%20Attention%20is%20All%20You%20Need.ipynb.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[25]](#_footnoteref_25) Trevett,Ben - Convolutional Sequence to Sequence Learning:
    [https://github.com/bentrevett/pytorch-seq2seq/blob/master/5%20-%20Convolutional%20Sequence%20to%20Sequence%20Learning.ipynb/](5%20-%20Convolutional%20Sequence%20to%20Sequence%20Learning.ipynb.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[26]](#_footnoteref_26) BERT: Pre-training of Deep Bidirectional Transformers
    for Language Understanding: [https://arxiv.org/abs/1810.04805/](1810.04805.html)
    (Devlin, Jacob et al. 2018)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[27]](#_footnoteref_27) "tensor2tensor" library on GitHub ( [https://github.com/tensorflow/tensor2tensor/](tensor2tensor.html)).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[28]](#_footnoteref_28) "BERT Fine-tuning With Cloud TPUS" Jupyter Notebook
    ( [https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb](colab.html)).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[29]](#_footnoteref_29) pytorch-pretrained-bert on PyPi ( [https://pypi.org/project/pytorch-pretrained-bert/](pytorch-pretrained-bert.html)).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[30]](#_footnoteref_30) Hugging Face transformer models on Hugging Face Hub
    - ( [https://huggingface.co/transformers/](transformers.html)).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[31]](#_footnoteref_31) Conversation AI: ( [https://conversationai.github.io/](conversationai.github.io.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[32]](#_footnoteref_32) Jigsaw toxic comment classification challenge on Kaggle
    ( [https://www.kaggle.com/datasets/julian3833/jigsaw-toxic-comment-classification-challenge](julian3833.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[33]](#_footnoteref_33) SciKit Learn documentation on multiclass and multioutput
    models( [https://scikit-learn.org/stable/modules/multiclass.html#multilabel-classification](modules.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[34]](#_footnoteref_34) Simpletransformers Multi-Label Classification documentation
    ( [https://simpletransformers.ai/docs/multi-label-classification/](multi-label-classification.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[35]](#_footnoteref_35) See "Configuring a Simple Transformers Model" section
    of the following webpage for full list of options and their defaults: [https://simpletransformers.ai/docs/usage/](usage.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[36]](#_footnoteref_36) Final leader board from the Kaggle Toxic Comment Classification
    Challenge: [https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/leaderboard](jigsaw-toxic-comment-classification-challenge.html)'
  prefs: []
  type: TYPE_NORMAL
