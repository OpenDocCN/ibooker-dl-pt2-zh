- en: 2 The mathematical building blocks of neural networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 神经网络的数学基础
- en: '*This chapter covers*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章内容涵盖*'
- en: A first example of a neural network
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络的第一个示例
- en: Tensors and tensor operations
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张量和张量操作
- en: How neural networks learn via backpropagation and gradient descent
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络如何通过反向传播和梯度下降进行学习
- en: 'Understanding deep learning requires familiarity with many simple mathematical
    concepts: *tensors, tensor operations, differentiation, gradient descent*, and
    so on. Our goal in this chapter will be to build up your intuition about these
    notions without getting overly technical. In particular, we’ll steer away from
    mathematical notation, which can introduce unnecessary barriers for those without
    any mathematics background and isn’t necessary to explain things well. The most
    precise, unambiguous description of a mathematical operation is its executable
    code.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 理解深度学习需要熟悉许多简单的数学概念：*张量、张量操作、微分、梯度下降*等等。本章的目标是在不过度技术化的情况下建立你对这些概念的直觉。特别是，我们将避开数学符号，这对那些没有数学背景的人来说可能会引入不必要的障碍，而且并不是必要的来解释事物。描述数学操作最精确、无歧义的方式是其可执行的代码。
- en: To provide sufficient context for introducing tensors and gradient descent,
    we’ll begin the chapter with a practical example of a neural network. Then we’ll
    go over every new concept that’s been introduced, point by point. Keep in mind
    that these concepts will be essential for you to understand the practical examples
    in the following chapters.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 为了为引入张量和梯度下降提供足够的背景，我们将在本章开始时介绍一个神经网络的实际示例。然后逐点地介绍每个新引入的概念。请记住，这些概念对你理解后面章节中的实际示例非常重要。
- en: After reading this chapter, you’ll have an intuitive understanding of the mathematical
    theory behind deep learning, and you’ll be ready to start diving into Keras and
    TensorFlow in chapter 3.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 读完本章后，你将对深度学习背后的数学理论有直观的理解，并准备好在第3章中深入研究Keras和TensorFlow。
- en: 2.1 A first look at a neural network
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 神经网络的首次尝试
- en: Let’s look at a concrete example of a neural network that uses Keras to learn
    to classify handwritten digits. Unless you already have experience with Keras
    or similar libraries, you won’t understand everything about this first example
    right away. That’s fine. In the next chapter, we’ll review each element in the
    example and explain them in detail. So don’t worry if some steps seem arbitrary
    or look like magic to you—we’ve got to start somewhere.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个具体的示例，一个使用Keras学习分类手写数字的神经网络。除非你已经有了使用Keras或类似库的经验，否则你不会立即完全理解这个第一个示例的全部内容。没关系，在下一章中，我们将逐个回顾示例中的每个元素，并详细解释它们。所以如果有些步骤看起来随意或者像魔术一样，别担心——我们得从某个地方开始。
- en: The problem we’re trying to solve here is to classify grayscale images of handwritten
    digits (28 × 28 pixels) into their 10 categories (0 through 9). We’ll use the
    *MNIST dataset*, a classic in the machine learning community, which has been around
    almost as long as the field itself and has been intensively studied. It’s a set
    of 60,000 training images, plus 10,000 test images, assembled by the National
    Institute of Standards and Technology (the NIST in MNIST) in the 1980s. You can
    think of “solving” MNIST as the “Hello World” of deep learning—it’s what you do
    to verify that your algorithms are working as expected. As you become a machine
    learning practitioner, you’ll see MNIST come up over and over again in scientific
    papers, blog posts, and so on. You can see some MNIST samples in [figure 2.1](#fig2-1).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要解决的问题是将手写数字（28×28像素）的灰度图像分类成它们的10个类别（0到9）。我们将使用*MNIST数据集*，这是机器学习界的经典数据集，它几乎和这个领域一样久了，并且得到了广泛的研究。它是由国家标准与技术研究所（MNIST中的NIST）在20世纪80年代编制的一组60,000个训练图像和10,000个测试图像。你可以把“解决”MNIST看作是深度学习的“Hello
    World”—用它来验证你的算法是否按预期工作。随着你成为一个机器学习实践者，你会在科学论文、博客文章等各种场合中一次又一次地看到MNIST。你可以在[图2.1](#fig2-1)中看到一些MNIST样本。
- en: '![Image](../images/f0027-01.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0027-01.jpg)'
- en: '**Figure 2.1 MNIST sample digits**'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**图2.1 MNIST样本数字**'
- en: In machine learning, a *category* in a classification problem is called a *class*.
    Data points are called *samples*. The class associated with a specific sample
    is called a *label*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，分类问题中的*类别*被称为*类*。数据点被称为*样本*。与特定样本相关联的类被称为*标签*。
- en: You don’t need to try to reproduce the example shown in the next code listing
    on your machine just now. If you wish to, you’ll first need to set up a deep learning
    workspace, which is covered in chapter 3\. The MNIST dataset comes preloaded in
    Keras, in the form of a set of four R arrays, organized into two lists named train
    and test.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在不需要在你的机器上尝试重现下面代码列表中显示的示例。如果你愿意，你首先需要设置一个深度学习工作空间，这在第三章中有介绍。MNIST 数据集在 Keras
    中预装，以四个 R 数组的形式组织成了两个名为 train 和 test 的列表。
- en: Listing 2.1 Loading the MNIST dataset in Keras
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.1 在 Keras 中加载 MNIST 数据集
- en: library(tensorflow)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: library(tensorflow)
- en: library(keras)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: library(keras)
- en: mnist <- dataset_mnist()
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: mnist <- dataset_mnist()
- en: train_images <- mnist$train$x
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: train_images <- mnist$train$x
- en: train_labels <- mnist$train$y
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: train_labels <- mnist$train$y
- en: test_images <- mnist$test$x
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: test_images <- mnist$test$x
- en: test_labels <- mnist$test$y
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: test_labels <- mnist$test$y
- en: 'train_images and train_labels form the training set, the data that the model
    will learn from. The model will then be tested on the test set, test_images and
    test_ labels. The images are encoded as R arrays, and the labels are an array
    of digits, ranging from 0 to 9\. The images and labels have a one-to-one correspondence.
    Let’s look at the training data, shown here:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: train_images 和 train_labels 构成了训练集，模型将从中学习。然后模型将在测试集 test_images 和 test_labels
    上进行测试。图像被编码为 R 数组，标签是一个从 0 到 9 的数字数组。图像和标签之间是一一对应的。让我们来看看训练数据，如下所示：
- en: str(train_images)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: str(train_images)
- en: int[1:60000, 1:28, 1:28] 0 0 0 0 0 0 0 0 0 0 …
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: int[1:60000, 1:28, 1:28] 0 0 0 0 0 0 0 0 0 0 …
- en: str(train_labels)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: str(train_labels)
- en: int [1:60000(1d)] 5 0 4 1 9 2 1 3 1 4 …
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: int [1:60000(1d)] 5 0 4 1 9 2 1 3 1 4 …
- en: 'And here’s the test data:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这是测试数据：
- en: str(test_images)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: str(test_images)
- en: int [1:10000, 1:28, 1:28] 0 0 0 0 0 0 0 0 0 0 …
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: int [1:10000, 1:28, 1:28] 0 0 0 0 0 0 0 0 0 0 …
- en: str(test_labels)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: str(test_labels)
- en: int [1:10000(1d)] 7 2 1 0 4 1 4 9 5 9 …
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: int [1:10000(1d)] 7 2 1 0 4 1 4 9 5 9 …
- en: 'The workflow will be as follows: first, we’ll feed the neural network the training
    data, train_images and train_labels. Then the network will learn to associate
    images and labels. Finally, we’ll ask the network to produce predictions for test_images,
    and we’ll verify whether these predictions match the labels from test_labels.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流程将如下：首先，我们将向神经网络提供训练数据 train_images 和 train_labels。然后网络将学会关联图像和标签。最后，我们将要求网络为
    test_images 生成预测，并验证这些预测是否与 test_labels 中的标签相匹配。
- en: Let’s build the network, as shown in the next listing. Again, remember that
    you aren’t expected to understand everything about this example yet.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建网络，如下面的示例所示。再次提醒，你现在不必理解这个示例的所有内容。
- en: Listing 2.2 The network architecture
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.2 网络架构
- en: model <- keras_model_sequential(list(
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model_sequential(list(
- en: layer_dense(units = 512, activation = "relu"),
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(units = 512, activation = "relu"),
- en: layer_dense(units = 10, activation = "softmax")
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(units = 10, activation = "softmax")
- en: ))
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ))
- en: 'The core building block of neural networks is the *layer*. You can think of
    a layer as a filter for data: some data goes in, and it comes out in a more useful
    form. Specifically, layers extract *representations* out of the data fed into
    them—hopefully, representations that are more meaningful for the problem at hand.
    Most of deep learning consists of chaining together simple layers that will implement
    a form of progressive *data distillation*. A deep learning model is like a sieve
    for data processing, made of a succession of increasingly refined data filters—the
    layers.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的核心构建块是*层*。你可以把层想象成数据的过滤器：一些数据进去，以更有用的形式出来。具体来说，层会从输入的数据中提取*表示*——希望这些表示对手头的问题更有意义。大部分深度学习都是将简单的层链接在一起，实现逐步*数据精炼*的形式。深度学习模型就像是用于数据处理的筛子，由一系列越来越精炼的数据过滤器——即层组成。
- en: Here, our model consists of a sequence of two Dense layers, which are densely
    connected (also called *fully connected*) neural layers. The second (and last)
    layer is a 10-way *softmax classification* layer, which means it will return an
    array of 10 probability scores (summing to 1). Each score will be the probability
    that the current digit image belongs to one of our 10 digit classes.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们的模型由两个 Dense 层的序列组成，这些层是密集连接的（也称为*全连接*）神经层。第二（也是最后）层是一个 10 通路的*softmax
    分类*层，这意味着它将返回一个包含 10 个概率分数的数组（总和为 1）。每个分数都是当前数字图像属于我们 10 个数字类别之一的概率。
- en: 'To make the model ready for training, we need to pick the following three things
    as part of the *compilation* step, shown in listing 2.3:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使模型准备好训练，我们需要在*编译*步骤中选择以下三个要素，如列表 2.3 所示：
- en: '*An optimizer*—The mechanism through which the model will update itself based
    on the training data it sees, so as to improve its performance.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*优化器*——模型将通过它看到的训练数据更新自身的机制，以改善其性能。'
- en: '*A loss function*—How the model will be able to measure its performance on
    the training data, and thus how it will be able to steer itself in the eright
    direction.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*损失函数*——模型如何在训练数据上测量其性能，以及如何使其朝着正确的方向调整自身。'
- en: '*Metrics to monitor during training and testing*—Here, we care only about accuracy
    (the fraction of the images that were correctly classified).'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*训练和测试期间要监控的指标*——在这里，我们只关心准确率（被正确分类的图像的比例）。'
- en: The exact purpose of the loss function and the optimizer will be made clear
    throughout the next two chapters.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数和优化器的确切目的将在接下来的两章中清晰地解释。
- en: Listing 2.3 The compilation step
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.3 编译步骤
- en: compile(model,
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 编译(model,
- en: optimizer = "rmsprop",
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: optimizer = "rmsprop",
- en: loss = "sparse_categorical_crossentropy",
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: loss = "稀疏分类交叉熵",
- en: metrics = "accuracy")
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 指标 = "准确度")
- en: Note that we don’t save the return value from compile() because the model is
    modified in place.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们不保存 compile() 的返回值，因为模型是就地修改的。
- en: Before training, we’ll preprocess the data by reshaping it into the shape the
    model expects and scaling it so that all values are in the [0, 1] interval, as
    shown next. Previously, our training images were stored in an array of shape (60000,
    28, 28) of type integer with values in the [0, 255] interval. We’ll transform
    it into a double array of shape (60000, 28 * 28) with values between 0 and 1.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练之前，我们将通过重塑数据并缩放数据来对数据进行预处理，以使其符合模型的期望形状，并使其所有值都在 [0, 1] 区间内，如下所示。之前，我们的训练图像存储在一个形状为
    (60000, 28, 28) 的整数数组中，值在 [0, 255] 区间内。我们将其转换为一个形状为 (60000, 28 * 28) 的双精度数组，值介于
    0 和 1 之间。
- en: Listing 2.4 Preparing the image data
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.4 准备图像数据
- en: train_images <- array_reshape(train_images, c(60000, 28 * 28))
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: train_images <- array_reshape(train_images, c(60000, 28 * 28))
- en: train_images <- train_images / 255
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: train_images <- train_images / 255
- en: test_images <- array_reshape(test_images, c(10000, 28 * 28))
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: test_images <- array_reshape(test_images, c(10000, 28 * 28))
- en: test_images <- test_images / 255
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: test_images <- test_images / 255
- en: Note that we use the array_reshape() function rather than the dim•() function
    to reshape the array. We’ll explain why later, when we talk about tensor reshaping.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用的是 array_reshape() 函数而不是 dim•() 函数来重塑数组。我们稍后会解释为什么，当我们谈论张量重塑时。
- en: We’re now ready to train the model, which in Keras is done via a call to the
    model’s fit() method—we *fit* the model to its training data.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好训练模型了，在 Keras 中，这是通过调用模型的 fit() 方法来完成的——我们将模型与其训练数据*拟合*起来。
- en: Listing 2.5 “Fitting” the model
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.5 “拟合”模型
- en: fit(model, train_images, train_labels, epochs = 5, batch_size = 128)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: fit(model, train_images, train_labels, epochs = 5, batch_size = 128)
- en: Epoch 1/5
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Epoch 1/5
- en: '60000/60000 [===========================] - 5s - loss: 0.2524 - acc:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '60000/60000 [===========================] - 5s - loss: 0.2524 - acc:'
- en: '![](../images/common01.jpg) 0.9273'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../images/common01.jpg) 0.9273'
- en: Epoch 2/5
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Epoch 2/5
- en: '51328/60000 [=====================>.....] - ETA: 1s - loss: 0.1035 -'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '51328/60000 [=====================>.....] - ETA: 1s - loss: 0.1035 -'
- en: '![](../images/common01.jpg) acc: 0.9692'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../images/common01.jpg) 准确率: 0.9692'
- en: 'Two quantities are displayed during training: the loss of the model over the
    training data, and the accuracy of the model over the training data. We quickly
    reach an accuracy of 0.989 (98.9%) on the training data.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '训练期间显示两个数量：模型在训练数据上的损失和模型在训练数据上的准确度。我们很快就能在训练数据上达到 0.989（98.9%）的准确率。 '
- en: Now that we have a trained model, we can use it to predict class probabilities
    for *new* digits—images that weren’t part of the training data, like those from
    the test set.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个经过训练的模型，我们可以用它来预测*新*数字的类别概率——那些不属于训练数据的图像，比如测试集中的图像。
- en: Listing 2.6 Using the model to make predictions
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.6 使用模型进行预测
- en: test_digits <- test_images[1:10, ]
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: test_digits <- test_images[1:10, ]
- en: predictions <- predict(model, test_digits)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: predictions <- predict(model, test_digits)
- en: str(predictions)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: str(predictions)
- en: num [1:10, 1:10] 3.10e-09 3.53e-11 2.55e-07 1.00 8.54e-07 …
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: num [1:10, 1:10] 3.10e-09 3.53e-11 2.55e-07 1.00 8.54e-07 …
- en: predictions[1, ]
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: predictions[1, ]
- en: '[1] 3.103298e-09 1.175280e-10 1.060593e-06 4.761311e-05 4.189971e-12'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 3.103298e-09 1.175280e-10 1.060593e-06 4.761311e-05 4.189971e-12'
- en: '[6] 4.062199e-08 5.244305e-16 9.999473e-01 2.753219e-07 3.826783e-06'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] 4.062199e-08 5.244305e-16 9.999473e-01 2.753219e-07 3.826783e-06'
- en: 'Each number of index i in that array (predictions[1, ]) corresponds to the
    probability that digit image test_digits[1, ] belongs to class i. This first test
    digit has the highest probability score (0.9999473, almost 1) at index 8, so according
    to our model, it must be a 7 (because we start counting at 0):'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 数组中索引 i 处的每个数字（predictions[1, ]）对应于数字图像 test_digits[1, ] 属于类 i 的概率。这第一个测试数字在索引
    8 处有最高的概率分数（0.9999473，接近 1），因此根据我们的模型，它必须是 7（因为我们从 0 开始计数）：
- en: which.max(predictions[1, ])
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: which.max(predictions[1, ])
- en: '[1] 8'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 8'
- en: predictions[1, 8]
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: predictions[1, 8]
- en: '[1] 0.9999473'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 0.9999473'
- en: 'We can check that the test label agrees:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以检查测试标签是否一致：
- en: test_labels[1]
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: test_labels[1]
- en: '[1] 7'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 7'
- en: On average, how good is our model at classifying such never-before-seen digits?
    Let’s check by computing average accuracy over the entire test set.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 平均而言，我们的模型在分类这样的全新数字时表现如何？让我们通过计算整个测试集的平均准确率来检查。
- en: Listing 2.7 Evaluating the model on new data
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.7 在新数据上评估模型
- en: metrics <- evaluate(model, test_images, test_labels)
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: metrics <- evaluate(model, test_images, test_labels)
- en: metrics["accuracy"]
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: metrics["accuracy"]
- en: accuracy
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率
- en: '0.9795'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '0.9795'
- en: 'The test set accuracy turns out to be 97.9%—that’s quite a bit lower than the
    training set accuracy (98.9%). This gap between training accuracy and test accuracy
    is an example of *overfitting*: the fact that machine learning models tend to
    perform worse on new data than on their training data. Overfitting is a central
    topic in chapter 3.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集准确率为 97.9%，比训练集准确率（98.9%）低得多。训练准确率和测试准确率之间的差距是*过拟合*的一个例子：机器学习模型在新数据上的表现往往不如在其训练数据上表现好。过拟合是第
    3 章的一个核心主题。
- en: This concludes our first example. You just saw how you can build and train a
    neural network to classify handwritten digits in fewer than 15 lines of R code.
    In this chapter and the next, we’ll go into detail about every moving piece we
    just previewed and clarify what’s going on behind the scenes. You’ll learn about
    tensors, the data-storing objects going into the model; tensor operations, which
    layers are made of; and gradient descent, which allows your model to learn from
    its training examples.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这就完成了我们的第一个示例。你刚刚看到了如何使用不到 15 行的 R 代码构建和训练一个神经网络来对手写数字进行分类。在本章和下一章中，我们将详细介绍我们刚刚预览的每个移动部件并澄清幕后发生的事情。你将了解张量，即输入模型的数据存储对象；张量操作，层由哪些组成；以及梯度下降，它允许您的模型从其训练示例中学习。
- en: 2.2 Data representations for neural networks
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 神经网络的数据表示
- en: In the previous example, we started from data stored in multidimensional arrays,
    also called *tensors*. In general, all current machine learning systems use tensors
    as their basic data structure. Tensors are fundamental to the field—so fundamental
    that TensorFlow was named after them. So, what’s a tensor?
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们从存储在多维数组中的数据开始，也称为*张量*。一般来说，所有当前的机器学习系统都使用张量作为其基本数据结构。张量对于该领域是基础性的—以至于
    TensorFlow 就是以它们命名的。那么，张量是什么？
- en: 'At its core, a tensor is a container for data—usually numerical data—so, it’s
    a container for numbers. You may be already familiar with matrices, which are
    rank 2 tensors: tensors are a generalization of matrices to an arbitrary number
    of *dimensions* (note that in the context of tensors, a dimension is often called
    an *axis*).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，张量是数据的容器—通常是数字数据—因此，它是数字的容器。你可能已经熟悉矩阵，它们是秩 2 的张量：张量是矩阵到任意数量的*维度*的泛化（请注意，在张量的上下文中，维度通常被称为*轴*）。
- en: 'R provides an implementation of tensors: array objects (constructed via base::
    array()) are tensors. In this section we are focused on defining the concepts
    around tensors, so we will stick to using R arrays. Later in the book (chapter
    3), we introduce another implementation of tensors (Tensorflow Tensors).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 'R 提供了张量的实现：数组对象（通过 base:: array() 构造）是张量。在本节中，我们专注于定义张量周围的概念，所以我们将继续使用 R 数组。在本书的后面（第
    3 章），我们介绍了张量的另一个实现（Tensorflow 张量）。'
- en: 2.2.1 Scalars (rank 0 tensors)
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.1 标量（秩 0 张量）
- en: A tensor that can contain only one number is called a *scalar* (or scalar tensor,
    or rank 0 tensor, or 0D tensor). R doesn’t have a data type to represent scalars
    (all numeric objects are vectors), but an R vector of length 1 is conceptually
    similar to a scalar.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 只能包含一个数字的张量称为*标量*（或标量张量，或秩 0 张量，或 0D 张量）。R 没有表示标量的数据类型（所有数字对象都是向量），但长度为 1 的
    R 向量在概念上类似于标量。
- en: 2.2.2 Vectors (rank 1 tensors)
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.2 向量（秩 1 张量）
- en: 'An array of numbers is called a *vector*, or rank 1 tensor, or 1D tensor. A
    rank 1 tensor is said to have exactly one axis. The following is a tensor vector:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 数组称为*向量*，或排名 1 张量，或 1D 张量。排名 1 张量被称为具有一个轴。以下是一个张量向量：
- en: x <- as.array(c(12, 3, 6, 14, 7))
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: x <- as.array(c(12, 3, 6, 14, 7))
- en: str(x)
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: str(x)
- en: num [1:5(1d)] 12 3 6 14 7
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: num [1:5(1d)] 12 3 6 14 7
- en: length(dim(x))
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: length(dim(x))
- en: '[1] 1'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 1'
- en: This vector has five entries and so is called a *five-dimensional vector*. Don’t
    confuse a 5D vector with a 5D tensor! A 5D vector has only one axis and has five
    dimensions along its axis, whereas a 5D tensor has five axes (and may have any
    number of dimensions along each axis). *Dimensionality* can denote either the
    number of entries along a specific axis (as in the case of our 5D vector) or the
    number of axes in a tensor (such as a 5D tensor), which can be confusing at times.
    In the latter case, it’s technically more correct to talk about a *tensor of rank
    5* (the rank of a tensor being the number of axes), but the ambiguous notation
    *5D tensor* is common regardless.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这个向量有五个条目，因此被称为*五维向量*。不要混淆 5D 向量和 5D 张量！ 5D 向量只有一个轴，并且沿着其轴有五个维度，而 5D 张量有五个轴（并且可以沿每个轴具有任意数量的维度）。*维度*既可以表示沿特定轴的条目数量（例如我们的
    5D 向量的情况），也可以表示张量中的轴数（例如 5D 张量），这有时可能会令人困惑。在后一种情况下，从技术上讲，谈论*排名 5 的张量*（张量的排名是轴的数量）更加正确，但是不明确的符号
    *5D 张量* 是常见的。
- en: 2.2.3 Matrices (rank 2 tensors)
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.3 矩阵（排名 2 张量）
- en: 'An array of vectors is a *matrix*, or rank 2 tensor, or 2D tensor. A matrix
    has two axes (often referred to as *rows* and *columns*). You can visually interpret
    a matrix as a rectangular grid of numbers:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 向量数组称为*矩阵*，或排名 2 张量，或 2D 张量。矩阵有两个轴（通常称为*行*和*列*）。您可以将矩阵直观地解释为数字的矩形网格：
- en: x <- array(seq(3 * 5), dim = c(3, 5))
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: x <- array(seq(3 * 5), dim = c(3, 5))
- en: x
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: x
- en: '![Image](../images/f0032-01.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0032-01.jpg)'
- en: dim(x)
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: dim(x)
- en: '[1] 3 5'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 3 5'
- en: The entries from the first axis are called the *rows*, and the entries from
    the second axis are called the *columns*. In the previous example, c(1, 4, 7,
    10, 13) is the first row of x, and c(1, 2, 3) is the first column.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 来自第一个轴的条目称为*行*，来自第二个轴的条目称为*列*。在上一个示例中，c(1, 4, 7, 10, 13) 是 x 的第一行，c(1, 2, 3)
    是第一列。
- en: 2.2.4 Rank 3 and higher-rank tensors
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.4 排名 3 和更高等级的张量
- en: 'If you supply a length 3 vector to dim, you obtain a rank 3 tensor (or 3D tensor),
    which you can visually interpret as a cube of numbers or a stack of rank 2 tensors:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您向 dim 提供一个长度为 3 的向量，则会获得排名为 3 的张量（或 3D 张量），您可以将其直观地解释为数字的立方体或排名为 2 的张量的堆叠：
- en: x <- array(seq(2 * 3 * 4), dim = c(2, 3, 4))
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: x <- array(seq(2 * 3 * 4), dim = c(2, 3, 4))
- en: str(x)
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: str(x)
- en: int [1:2, 1:3, 1:4] 1 2 3 4 5 6 7 8 9 10 …
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: int [1:2, 1:3, 1:4] 1 2 3 4 5 6 7 8 9 10 …
- en: length(dim(x))
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: length(dim(x))
- en: '[1] 3'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 3'
- en: By stacking rank 3 tensors, you can create a rank 4 tensor, and so on. In deep
    learning, you’ll generally manipulate tensors with ranks 0 to 4, although you
    may go up to 5 if you process video data.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 通过堆叠排名 3 的张量，您可以创建排名 4 的张量，依此类推。在深度学习中，您通常会处理排名为 0 到 4 的张量，尽管如果处理视频数据，您可能会升到
    5。
- en: 2.2.5 Key attributes
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.5 关键属性
- en: 'A tensor is defined by the following three key attributes:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 张量由以下三个关键属性定义：
- en: '*Number of axes (rank)*—For instance, a rank 3 tensor has three axes, and a
    matrix has two axes. This is available from length(dim(x)).'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*轴数（排名）*—例如，排名 3 的张量有三个轴，矩阵有两个轴。这可以从 length(dim(x)) 获取。'
- en: '*Shape*—This is an integer vector that describes how many dimensions the tensor
    has along each axis. For instance, the previous matrix example has shape (3, 5),
    and the rank 3 tensor example has shape (2, 3, 4). A vector has a shape with a
    single element, such as (5). R arrays don’t distinguish between 1D vectors and
    scalar tensors, but conceptually, tensors can also be scalar with shape ().'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*形状*—这是一个整数向量，描述张量沿每个轴具有多少维度。例如，前一个矩阵示例具有形状（3, 5），而排名为 3 的张量示例具有形状（2, 3, 4）。向量具有具有单个元素的形状，例如（5）。
    R 数组不区分 1D 向量和标量张量，但在概念上，张量也可以是形状为（）的标量。'
- en: '*Data type*—This is the type of the data contained in the tensor. R arrays
    have support for R’s built-in data types like double and integer. Conceptually,
    however, tensors can support any type of homogeneous data type, and other tensor
    implementations also provide support for types like like float16, float32, float64
    (corresponding to R’s double), int32 (R’s integer type), and so on. In TensorFlow,
    you are also likely to come across string tensors.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据类型* —— 这是张量中包含的数据的类型。R 数组支持 R 内置数据类型，如 double 和 integer。然而，从概念上讲，张量可以支持任何类型的同构数据类型，其他张量实现也提供了对诸如
    float16、float32、float64（对应于 R 的 double）、int32（R 的整数类型）等类型的支持。在 TensorFlow 中，你还可能遇到字符串张量。'
- en: 'To make this more concrete, let’s look back at the data we processed in the
    MNIST example. First, we load the MNIST dataset:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这更具体，让我们回顾一下在 MNIST 示例中处理的数据。首先，我们加载 MNIST 数据集：
- en: library(keras)
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: library(keras)
- en: mnist <- dataset_mnist()
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: mnist <- dataset_mnist()
- en: train_images <- mnist$train$x
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 训练图像 <- mnist$train$x
- en: train_labels <- mnist$train$y
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 训练标签 <- mnist$train$y
- en: test_images <- mnist$test$x
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 测试图像 <- mnist$test$x
- en: test_labels <- mnist$test$y
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 测试标签 <- mnist$test$y
- en: 'Next, we display the number of axes of the tensor train_images:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们显示了张量训练图像的轴数：
- en: length(dim(train_images))
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: length(dim(训练图像))
- en: '[1] 3'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 3'
- en: 'Here’s its shape:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这是它的形状：
- en: dim(train_images)
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: dim(训练图像)
- en: '[1] 60000 28 28'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 60000 28 28'
- en: 'And this is its R data type:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这是它的 R 数据类型：
- en: typeof(train_images)
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: typeof(训练图像)
- en: '[1] "integer"'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] "integer"'
- en: So what we have here is a rank 3 tensor of integers. More precisely, it’s a
    stack of 60,000 matrices of 28 × 28 integers. Each such matrix is a grayscale
    image, with coefficients between 0 and 255 of pixel intensity values.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们这里有一个整数的三阶张量。更准确地说，它是一个 60,000 个 28 × 28 整数矩阵的堆叠。每个这样的矩阵都是一个灰度图像，像素强度值介于
    0 到 255 之间。
- en: Let’s display the fifth digit in this rank 3 tensor (see [figure 2.2](#fig2-2)).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们显示这个三阶张量中的第五个数字（见 [图 2.2](#fig2-2)）。
- en: '![Image](../images/f0034-01.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0034-01.jpg)'
- en: '**Figure 2.2 The fifth sample in our dataset**'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.2 我们数据集中的第五个样本**'
- en: Listing 2.8 Displaying the fifth digit
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 2.8 显示第五个数字
- en: digit <- train_images[5, , ]
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 数字 <- 训练图像[5, , ]
- en: plot(as.raster(abs(255 - digit), max = 255))
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: plot(as.raster(abs(255 - 数字), max = 255))
- en: 'Naturally, the corresponding label is the integer 9:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 对应的标签自然是整数 9：
- en: train_labels[5]
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 训练标签[5]
- en: '[1] 9'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 9'
- en: 2.2.6 Manipulating tensors in R
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.6 在 R 中操作张量
- en: In the previous example, we selected a specific digit alongside the first axis
    using the syntax train_images[i, , ]. Selecting specific elements in a tensor
    is called *tensor slicing*. Let’s look at the tensor-slicing operations you can
    do on R arrays.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们使用语法 train_images[i, , ] 沿着第一个轴选择了一个特定的数字。在张量中选择特定元素称为 *张量切片*。让我们看看你可以在
    R 数组上进行的张量切片操作。
- en: NOTE TensorFlow Tensor‘s slicing is similar to R arrays but with some differences.
    In this section, we focus on R arrays and begin discussing TensorFlow Tensors
    in chapter 3.
  id: totrans-158
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意 TensorFlow 张量的切片与 R 数组类似，但存在一些差异。在本节中，我们将关注 R 数组，并在第 3 章开始讨论 TensorFlow 张量。
- en: 'The following example selects digits 10 to 99 and puts them in an array of
    shape (90, 28, 28):'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例选择数字 10 到 99，并将它们放入形状为 (90, 28, 28) 的数组中：
- en: my_slice <- train_images[10:99, , ]
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我的切片 <- 训练图像[10:99, , ]
- en: dim(my_slice)
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: dim(我的切片)
- en: '[1] 90 28 28'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 90 28 28'
- en: 'In general, you may select slices between any two indices along each tensor
    axis. For instance, to select 14 × 14 pixels in the bottom-right corner of all
    images, you would do this:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，你可以在每个张量轴上的任意两个索引之间选择切片。例如，要选择所有图像右下角的 14 × 14 像素，你可以这样做：
- en: my_slice <- train_images[, 15:28, 15:28]
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我的切片 <- 训练图像[, 15:28, 15:28]
- en: dim(my_slice)
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: dim(我的切片)
- en: '[1] 60000    14    14'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 60000    14    14'
- en: 2.2.7 The notion of data batches
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.7 数据批次的概念
- en: In general, the first axis in all data tensors you’ll come across in deep learning
    will be the *samples axis* (sometimes called the *samples dimension*). In the
    MNIST example, “samples” are images of digits.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在深度学习中，你将遇到的所有数据张量的第一个轴都是 *样本轴*（有时称为 *样本维度*）。在 MNIST 示例中，“样本”是数字的图像。
- en: 'In addition, deep learning models don’t process an entire dataset at once;
    rather, they break the data into small batches. Concretely, here’s one batch of
    our MNIST digits, with a batch size of 128:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，深度学习模型不会一次处理整个数据集；相反，它们将数据分成小批量。具体来说，这是我们 MNIST 数字的一个批量，批量大小为 128：
- en: batch <- train_images[1:128, , ]
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 批量 <- 训练图像[1:128, , ]
- en: 'And here’s the next batch:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是下一个批量：
- en: batch <- train_images[129:256, , ]
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 批量 <- 训练图像[129:256, , ]
- en: 'And the *n*th batch:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 第 *n* 个批量：
- en: n <- 3
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: n <- 3
- en: batch <- train_images[seq(to = 128 * n, length.out = 128), , ]
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: batch <- train_images[seq(to = 128 * n, length.out = 128), , ]
- en: When considering such a batch tensor, the first axis is called the *batch axis*
    or *batch dimension*. This is a term you’ll frequently encounter when using Keras
    and other deep learning libraries.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑这样一个批张量时，第一个轴被称为*批轴*或*批量维度*。这是在使用Keras和其他深度学习库时经常遇到的术语。
- en: 2.2.8 Real-world examples of data tensors
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.8 数据张量的真实示例
- en: 'Let’s make data tensors more concrete with a few examples similar to what you’ll
    encounter later. The data you’ll manipulate will almost always fall into one of
    the following categories:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一些类似于您将来会遇到的示例来使数据张量更加具体化。您要处理的数据几乎总是属于以下几���类别之一：
- en: '*Vector data*—Rank 2 tensors of shape (samples, features), where each sample
    is a vector of numerical attributes (“features”)'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*向量数据*—形状为（样本，特征）的等级2张量，其中每个样本是一个数字属性（“特征”）的向量'
- en: '*Times-series data or sequence data*—Rank 3 tensors of shape (samples, timesteps,
    features), where each sample is a sequence (of length timesteps) of feature vectors'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*时间序列数据或序列数据*—形状为（样本，时间步长，特征）的等级3张量，其中每个样本是一系列（长度为时间步长）的特征向量'
- en: '*Images*—Rank 4 tensors of shape (samples, height, width, channels), where
    each sample is a 2D grid of pixels, and each pixel is represented by a vector
    of values (“channels”)'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图像*—形状为（样本，高度，宽度，通道）的等级4张量，其中每个样本是一个2D像素网格，并且每个像素由一组值（“通道”）表示'
- en: '*Video*—Rank 5 tensors of shape (samples, frames, height, width, channels),
    where each sample is a sequence (of length frames) of images'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*视频*—形状为（样本，帧数，高度，宽度，通道）的等级5张量，其中每个样本是图像序列（长度为帧数）'
- en: 2.2.9 Vector data
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.9 向量数据
- en: This is one of the most common cases. In such a dataset, each single data point
    can be encoded as a vector, and thus a batch of data will be encoded as a rank
    2 tensor (that is, a matrix), where the first axis is the *samples axis* and the
    second axis is the *features axis*.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最常见的几种情况之一。在这样的数据集中，每个单个数据点可以被编码为一个向量，因此一个数据批将被编码为等级2张量（即矩阵），其中第一个轴是*样本轴*，第二个轴是*特征轴*。
- en: 'Let’s take a look at the next two examples:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看下面的两个示例：
- en: An actuarial dataset of people, where we consider each person’s age, gender,
    and income. Each person can be characterized as a vector of 3 values, and thus
    an entire dataset of 100,000 people can be stored in a rank 2 tensor of shape
    (100000, 3).
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个保险数据集，其中我们考虑每个人的年龄、性别和收入。每个人可以被描述为一个3个值的向量，因此整个涵盖100,000人的数据集可以存储在形状为（100000，3）的等级2张量中。
- en: A dataset of text documents, where we represent each document by the counts
    of how many times each word appears in it (out of a dictionary of 20,000 common
    words). Each document can be encoded as a vector of 20,000 values (one count per
    word in the dictionary), and thus an entire dataset of 500 documents can be stored
    in a tensor of shape (500, 20000).
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个文本文档数据集，我们通过每个单词出现的次数来表示每个文档（在一个包含20,000个常见单词的词典中）。每个文档可以被编码为一个包含20,000个值的向量（词典中每个单词的计数），因此整个包含500个文档的数据集可以存储在形状为（500,
    20000）的张量中。
- en: 2.2.10\. Time-series data or sequence data
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.10\. 时间序列数据或序列数据
- en: Whenever time matters in your data (or the notion of sequence order), it makes
    sense to store it in a rank 3 tensor with an explicit time axis. Each sample can
    be encoded as a sequence of vectors (a rank 2 tensor), and thus a batch of data
    will be encoded as a rank 3 tensor (see [figure 2.3](#fig2-3)).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 每当数据中涉及时间（或序列顺序的概念）时，将其存储在具有显式时间轴的等级3张量中是有意义的。每个样本可以被编码为一系列向量（等级2张量），因此数据批将被编码为等级3张量（参见[图2.3](#fig2-3)）。
- en: '![Image](../images/f0036-01.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0036-01.jpg)'
- en: '**Figure 2.3 A rank 3 time-series data tensor**'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '**图2.3 时间序列数据等级3张量**'
- en: 'The time axis is always the second axis by convention. Let’s look at a few
    examples:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 时间轴按照惯例始终是第二轴。让我们看几个示例：
- en: A dataset of stock prices. Every minute, we store the current price of the stock,
    the highest price in the past minute, and the lowest price in the past minute.
    Thus, every minute is encoded as a 3D vector, an entire day of trading is encoded
    as a matrix of shape (390, 3) (there are 390 minutes in a trading day), and 250
    days’ worth of data can be stored in a rank 3 tensor of shape (250, 390, 3). Here,
    each sample would be one day’s worth of data.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个股票价格数据集。每分钟，我们存储股票的当前价格，过去一分钟内的最高价格和最低价格。因此，每分钟被编码为一个3D向量，整个交易日被编码为形状为（390，3）的矩阵（一个交易日有390分钟），并且250天的数据可以存储在形状为（250，390，3）的等级3张量中。在这里，每个样本将是一天的数据。
- en: A dataset of tweets, where we encode each tweet as a sequence of 280 characters
    out of an alphabet of 128 unique characters. In this setting, each character can
    be encoded as a binary vector of size 128 (an all-zeros vector except for a 1
    entry at the index corresponding to the character). Then each tweet can be encoded
    as a rank 2 tensor of shape (280, 128), and a dataset of one million tweets can
    be stored in a tensor of shape (1000000, 280, 128).
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个推文数据集，其中我们将每个推文编码为由 128 个唯一字符组成的 280 个字符序列。在这种情况下，每个字符可以编码为大小为 128 的二进制向量（除了字符对应的索引处有一个
    1 之外，其余全为零）。然后每个推文可以编码为形状为 (280, 128) 的二阶张量，并且一个包含一百万个推文的数据集可以存储在形状为 (1000000,
    280, 128) 的张量中。
- en: 2.2.11\. Image data
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.11\. 图像数据
- en: 'Images typically have three dimensions: height, width, and color depth. Although
    grayscale images (like our MNIST digits) have only a single color channel and
    could thus be stored in rank 2 tensors, by convention, image tensors are always
    rank 3, with a one-dimensional color channel for grayscale images. A batch of
    128 grayscale images of size 256 × 256 could thus be stored in a tensor of shape
    (128, 256, 256, 1), and a batch of 128 color images could be stored in a tensor
    of shape (128, 256, 256, 3) (see [figure 2.4](#fig2-4)).'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图像通常具有三个维度：高度、宽度和颜色深度。尽管灰度图像（如我们的 MNIST 数字）只有一个颜色通道，因此可以存储在二阶张量中，但按照惯例，图像张量始终是三阶的，对于灰度图像有一个一维的颜色通道。因此，尺寸为
    256 × 256 的 128 个灰度图像批次可以存储在形状为 (128, 256, 256, 1) 的张量中，而尺寸为 256 × 256 的 128 个彩色图像批次可以存储在形状为
    (128, 256, 256, 3) 的张量中（参见[图 2.4](#fig2-4)）。
- en: '![Image](../images/f0037-01.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0037-01.jpg)'
- en: '**Figure 2.4 A rank 4 image data tensor**'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.4 一个四阶图像数据张量**'
- en: 'Two conventions for shapes of image tensors are the *channels-last* convention
    (which is standard in TensorFlow) and the *channels-first* convention (which is
    increasingly falling out of favor). The channels-last convention places the color-depth
    axis at the end: (samples, height, width, color_depth). Meanwhile, the channels-first
    convention places the color depth axis right after the batch axis: (samples, color_depth,
    height, width). With the channels-first convention, the previous examples would
    become (128, 1, 256, 256) and (128, 3, 256, 256). The Keras API provides support
    for both formats.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图像张量的形状有两种约定：*channels-last* 约定（在 TensorFlow 中是标准的）和 *channels-first* 约定（越来越不受青睐）。*channels-last*
    约定将颜色深度轴放在最后：(samples, height, width, color_depth)。与此同时，*channels-first* 约定将颜色深度轴放在批处理轴之后：(samples,
    color_depth, height, width)。使用 *channels-first* 约定，前面的例子将变为 (128, 1, 256, 256)
    和 (128, 3, 256, 256)。Keras API 支持这两种格式。
- en: 2.2.12\. Video data
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.12\. 视频数据
- en: Video data is one of the few types of real-world data for which you’ll need
    rank 5 tensors. A video can be understood as a sequence of frames, with each frame
    being a color image. Because each frame can be stored in a rank 3 tensor (height,
    width, color_depth), a sequence of frames can be stored in a rank 4 tensor (frames,
    height, width, color_depth), and thus a batch of different videos can be stored
    in a rank 5 tensor of shape (samples, frames, height, width, color_depth).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 视频数据是你需要使用五阶张量的少数几种真实世界数据之一。视频可以被理解为帧的序列，其中每一帧都是一幅彩色图像。因为每一帧可以存储在三阶张量中（高度、宽度、颜色深度），一系列帧可以存储在四阶张量中（帧、高度、宽度、颜色深度），因此不同视频的批次可以存储在形状为
    (samples, frames, height, width, color_depth) 的五阶张量中。
- en: For instance, a 60-second, 144 × 256 YouTube video clip sampled at 4 frames
    per second would have 240 frames. A batch of four such video clips would be stored
    in a tensor of shape (4, 240, 144, 256, 3). That’s a total of 106,168,320 values!
    If the data type of the tensor was R integers, each value would be stored in 32
    bits, so the tensor would represent 405 MB. Heavy! Videos you encounter in real
    life are much lighter, because they aren’t stored as R integers, and they’re typically
    compressed by a large factor (such as in the MPEG format).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个 60 秒长、144 × 256 像素的 YouTube 视频片段，以每秒 4 帧采样，将有 240 帧。四个这样的视频片段的批次将存储在形状为
    (4, 240, 144, 256, 3) 的张量中。这总共有 106,168,320 个值！如果张量的数据类型为 R 整数，每个值将以 32 位存储，因此张量将表示
    405 MB。太重了！你在现实生活中遇到的视频要轻得多，因为它们不是以 R 整数的形式存储的，而且通常被大幅压缩（例如 MPEG 格式）。
- en: '2.3 The gears of neural networks: Tensor operations'
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 神经网络的齿轮：张量操作
- en: 'As much as any computer program can be ultimately reduced to a small set of
    binary operations on binary inputs (AND, OR, NOR, and so on), all transformations
    learned by deep neural networks can be reduced to a handful of *tensor operations*
    (or *tensor functions*) applied to tensors of numeric data. For instance, it’s
    possible to add tensors, multiply tensors, and so on. In our initial example,
    we built our model by stacking Dense layers on top of each other. A Keras layer
    instance looks like this:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 就像任何计算机程序最终都可以归结为对二进制输入的少量二进制操作（AND、OR、NOR等）一样，所有深度神经网络学到的转换都可以归结为应用于数值数据张量的少数*张量操作*（或*张量函数*）。例如，可以对张量进行加法、乘法等操作。在我们的初始示例中，我们通过将密集层堆叠在一起来构建模型。Keras层实例如下所示：
- en: layer_dense(units = 512, activation = "relu")
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(units = 512, activation = "relu")
- en: <keras.layers.core.dense.Dense object at 0x7f7b0e8cf520>
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '`<keras.layers.core.dense.Dense object at 0x7f7b0e8cf520>`'
- en: 'This layer can be interpreted as a function, which takes as input a matrix
    and returns another matrix—a new representation for the input tensor. Specifically,
    the function is as follows (where W is a matrix and b is a vector, both properties
    of the layer):'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这一层可以被解释为一个函数，它以一个矩阵作为输入，并返回另一个矩阵——输入张量的一个新表示。具体来说，函数如下（其中W是一个矩阵，b是一个向量，都是该层的属性）：
- en: output <- relu(dot(W, input) + b)
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: output <- relu(dot(W, input) + b)
- en: 'Let’s unpack this. We have the following three tensor operations here:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来解释一下。我们在这里有以下三个张量操作：
- en: A dot product (dot) between the input tensor and a tensor named W
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在输入张量和命名为W的张量之间的点积（dot）操作
- en: An addition (+) between the resulting matrix and a vector b
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果矩阵和向量b之间的加法（+）
- en: 'A relu operation: relu(x) is an element-wise max(x, 0); *relu* stands for rectified
    linear unit'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个relu操作：relu(x)是逐元素max(x, 0)；*relu*代表修正线性单元
- en: Although this section deals entirely with linear algebra expressions, you won’t
    find any mathematical notation here. I’ve found that mathematical concepts can
    be more readily mastered by programmers with no mathematical background if they’re
    expressed as short code snippets instead of mathematical equations. So we’ll use
    R and TensorFlow code throughout.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本节完全涉及线性代数表达式，但你在这里找不到任何数学符号。我发现，如果将数学概念表达为简短的代码片段，而不是数学方程式，那么没有数学背景的程序员更容易掌握这些概念。因此，我们将在整个过程中使用R和TensorFlow代码。
- en: 2.3.1 Element-wise operations
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.1 逐元素操作
- en: 'The relu operation and addition are element-wise operations: operations that
    are applied independently to each entry in the tensors being considered. This
    means these operations are highly amenable to massively parallel implementations
    (*vectorized implementations*, a term that comes from the *vector processor* supercomputer
    architecture from the 1970–1990 period). If you want to write a naive R implementation
    of an element-wise operation, you use a for loop, as in the following naive implementation
    of an element-wise relu operation:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: relu操作和加法是逐元素操作：这些操作独立地应用于考虑的张量中的每个条目。这意味着这些操作非常适合于大规模并行实现（*向量化实现*，这个术语来自于20世纪70-90年代的*向量处理器*超级计算机架构）。如果你想编写一个逐元素操作的简单的R实现，你可以使用for循环，就像下面对逐元素relu操作的简单实现一样：
- en: naive_relu <- functsion(x) {
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: naive_relu <- functsion(x) {
- en: stopifnot(length(dim(x)) == 2)➊
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: stopifnot(length(dim(x)) == 2)➊
- en: for (i in 1:nrow(x))
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: for (i in 1:nrow(x))
- en: for (j in 1:ncol(x))
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: for (j in 1:ncol(x))
- en: x[i, j] <- max(x[i, j], 0)
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: x[i, j] <- max(x[i, j], 0)
- en: x
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: x
- en: '}'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ **x is a rank 2 tensor (a matrix).**
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **x是一个秩为2的张量（一个矩阵）。**
- en: 'You could do the same for addition:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以对加法做同样的操作：
- en: naive_add <- function(x, y) {
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: naive_add <- function(x, y) {
- en: stopifnot(length(dim(x)) == 2, dim(x) == dim(y))➊
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: stopifnot(length(dim(x)) == 2, dim(x) == dim(y))➊
- en: for (i in 1:nrow(x))
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: for (i in 1:nrow(x))
- en: for (j in 1:ncol(x))
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: for (j in 1:ncol(x))
- en: x[i, j] <- x[i, j] + y[i, j]
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: x[i, j] <- x[i, j] + y[i, j]
- en: x
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: x
- en: '}'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ **x and y are rank 2 tensors.**
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **x和y都是秩为2的张量。**
- en: On the same principle, you can do element-wise multiplication, subtraction,
    and so on.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 按照相同的原则，你可以进行逐元素乘法、减法等操作。
- en: 'In practice, when dealing with R arrays, these operations are available as
    well-optimized built-in R functions, which themselves delegate the heavy lifting
    to a Basic Linear Algebra Subprograms (BLAS) implementation. BLAS are low-level,
    highly parallel, efficient tensor-manipulation routines that are typically implemented
    in Fortran or C. So, in R, you can do the following element-wise operation, and
    it will be blazing fast:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，当处理 R 数组时，这些操作也作为优化良好的内置 R 函数可用，它们自己将重活交给了基本线性代数子程序（BLAS）实现。BLAS 是低级别、高度并行、高效的张量操作例程，通常用
    Fortran 或 C 实现。因此，在 R 中，您可以执行以下逐元素操作，速度非常快：
- en: z <- x + y➊
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: z <- x + y➊
- en: z[z < 0] <- 0➋
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: z[z < 0] <- 0➋
- en: ➊ **Element-wise addition**
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **逐元素相加**
- en: ➋ **Element-wise relu**
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **逐元素 relu**
- en: 'Let’s actually time the difference here:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实际计算一下这里的时间差异：
- en: random_array <- function(dim, min = 0, max = 1)
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: random_array <- function(dim, min = 0, max = 1)
- en: array(runif(prod(dim), min, max),
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 数组(runif(prod(dim), min, max),
- en: dim)
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: dim)
- en: x <- random_array(c(20, 100))
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: x <- random_array(c(20, 100))
- en: y <- random_array(c(20, 100))
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: y <- random_array(c(20, 100))
- en: system.time({
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: system.time({
- en: for (i in seq_len(1000)) {
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: for (i in seq_len(1000)) {
- en: z <- x + y
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: z <- x + y
- en: z[z < 0] <- 0
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: z[z < 0] <- 0
- en: '}'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '})[["elapsed"]]'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '})[["elapsed"]]'
- en: '[1] 0.009'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 0.009'
- en: 'This takes 0.009 seconds. Meanwhile, the naive version takes a stunning 0.72
    seconds:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 这需要 0.009 秒。与此同时，简单版本需要惊人的 0.72 秒：
- en: system.time({
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: system.time({
- en: for (i in seq_len(1000)) {
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: for (i in seq_len(1000)) {
- en: z <- naive_add(x, y)
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: z <- naive_add(x, y)
- en: z <- naive_relu(z)
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: z <- naive_relu(z)
- en: '}'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '})[["elapsed"]]'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '})[["elapsed"]]'
- en: '[1] 0.724'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 0.724'
- en: Likewise, when running TensorFlow code on a GPU, element-wise operations are
    executed via fully vectorized CUDA implementations that can best utilize the highly
    parallel GPU chip architecture.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，当在 GPU 上运行 TensorFlow 代码时，逐元素操作通过完全矢量化的 CUDA 实现执行，这些实现可以最佳地利用高度并行的 GPU 芯片架构。
- en: 2.3.2 Broadcasting
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.2 广播
- en: Our earlier naive implementation of naive_add supports only the addition of
    rank 2 tensors with identical shapes. But in the layer_dense() introduced earlier,
    we added a rank 2 tensor with a vector. What happens with addition when the shapes
    of the two tensors being added differ?
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们先前的简单实现 naive_add 仅支持具有相同形状的秩 2 张量的加法。但是在前面介绍的 layer_dense() 中，我们将秩 2 张量与向量相加。当正在添加的两个张量的形状不同时，加法会发生什么情况？
- en: 'What we’d like is for the smaller tensor to be *broadcast* to match the shape
    of the larger tensor. Broadcasting consists of the following two steps:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望的是较小的张量 *广播* 以匹配较大张量的形状。广播由以下两个步骤组成：
- en: '**1** Axes (called *broadcast axes*) are added to the smaller tensor to match
    the length(dim(x)) of the larger tensor.'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**1** 轴（称为 *广播轴*）被添加到较小的张量中，以匹配较大张量的 length(dim(x))。'
- en: '**2** The smaller tensor is repeated alongside these new axes to match the
    full shape of the larger tensor.'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**2** 较小的张量沿着这些新轴重复，以匹配较大张量的完整形状。'
- en: Note that Tensorflow Tensors, covered in chapter 3, have rich broadcasting functionality
    built in. Here, however, we are building up machine learning concepts from scratch
    using R arrays and are intentionally avoiding R’s implicit recycling behavior
    when operating on two arrays of different dimensions. We can implement our own
    recycling approach by building up the smaller tensor to match the shape of the
    larger tensor, at which point we are again back to doing a standard element-wise
    operation.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，Tensorflow Tensors，在第 3 章中介绍了丰富的广播功能。然而，在这里，我们正在使用 R 数组从头构建机器学习概念，并且故意避免了在操作两个不同维度数组时的
    R 隐式重复行为。我们可以通过构建较小的张量来匹配较大张量的形状来实现我们自己的重复使用方法，这样我们再次回到了执行标准逐元素操作的地步。
- en: 'Let’s look at a concrete example. Consider X with shape (32, 10) and y with
    shape (10):'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个具体的例子。考虑形状为 (32, 10) 的 X 和形状为 (10) 的 y：
- en: X <- random_array(c(32, 10))➊
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: X <- random_array(c(32, 10))➊
- en: y <- random_array(c(10))➋
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: y <- random_array(c(10))➋
- en: ➊ **X is a random matrix with shape (32, 10).**
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **X 是形状为 (32, 10) 的随机矩阵。**
- en: ➋ **y is a random vector with shape (10).**
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **y 是形状为 (10) 的随机向量。**
- en: 'First, we add a size 1 first axis to y, whose shape becomes (1, 10):'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们给 y 添加一个大小为 1 的第一个轴，其形状变为 (1, 10)：
- en: dim(y) <- c(1, 10)
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: dim(y) <- c(1, 10)
- en: str(y)➊
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: str(y)➊
- en: num [1, 1:10] 0.885 0.429 0.737 0.553 0.426 …
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: num [1, 1:10] 0.885 0.429 0.737 0.553 0.426 …
- en: ➊ **The shape of y is now (1, 10).**
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **y 的形状现在是 (1, 10)。**
- en: 'Then, we repeat y 32 times alongside this new axis, so that we end up with
    a tensor Y with shape (32, 10), where Y[i, ] == y for i in seq(32):'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们沿着这个新轴重复 y 32 次，使我们最终得到一个形状为 (32, 10) 的张量 Y，其中 Y[i, ] == y，对于 i 在 seq(32)
    中：
- en: Y <- y[rep(1, 32), ]➊
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: Y <- y[rep(1, 32), ]➊
- en: str(Y)
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: str(Y)
- en: num [1:32, 1:10] 0.885 0.885 0.885 0.885 0.885 …
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: num [1:32, 1:10] 0.885 0.885 0.885 0.885 0.885 …
- en: ➊ **Repeat y 32 times along axis 1 to obtain Y, which has shape (32, 10).**
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **沿轴 1 重复 y 32 次以获取 Y，其形状为 (32, 10)。**
- en: At this point, we can proceed to add X and Y, because they have the same shape.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以继续添加 X 和 Y，因为它们具有相同的形状。
- en: 'In terms of implementation, ideally we want no new rank 2 tensor to be created,
    because that is terribly inefficient. In most tensor implementations, including
    R and TensorFlow, the repetition operation is entirely virtual: it happens at
    the algorithmic level rather than at the memory level. However, be aware that
    R’s recycling and TensorFlow’s (and NumPy’s) broadcasting differ in their behavior
    (we go into details in chapter 3). Regardless, thinking of the vector being repeated
    10 times alongside a new axis is a helpful mental model. Here’s what a naive implementation
    would look like:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现方面，理想情况下我们不希望创建新的二维张量，因为那样非常低效。在大多数张量实现中，包括 R 和 TensorFlow，在算法级别上进行的是完全虚拟的重复操作，而不是在内存级别上。但是，请注意，R
    的循环利用和 TensorFlow（以及 NumPy）的广播在行为上有所不同（我们在第 3 章中会详细介绍）。不管怎样，将向量重复 10 次并伴随一个新轴是一个有用的心理模型。下面是一个简单实现的样子：
- en: naive_add_matrix_and_vector <- function(x, y) {
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: naive_add_matrix_and_vector <- 函数(x, y) {
- en: stopifnot(length(dim(x)) == 2,➊
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: stopifnot(length(dim(x)) == 2,➊
- en: length(dim(y)) == 1,➋
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: length(dim(y)) == 1,➋
- en: ncol(x) == dim(y))
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: ncol(x) == 维度(y))
- en: for (i in seq(dim(x)[1]))
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: for (i in seq(维度(x)[1]))
- en: for (j in seq(dim(x)[2]))
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: for (j in seq(维度(x)[2]))
- en: x[i, j] <- x[i, j] + y[j]
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: x[i, j] <- x[i, j] + y[j]
- en: x
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: x
- en: '}'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ **x is a rank 2 tensor.**
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **x 是一个二阶张量。**
- en: ➋ **y is a vector.**
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **y 是一个向量。**
- en: 2.3.3 Tensor product
  id: totrans-295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.3 张量积
- en: 'The *tensor product*, or *dot product* (not to be confused with an element-wise
    product, the * operator), is one of the most common, most useful tensor operations.
    In R, an element-wise product is done with the * operator, whereas dot products
    use the %*% operator:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '*张量积*或*点积*（不要与逐元素乘积，即*操作符混淆）是最常见、最有用的张量运算之一。在 R 中，逐元素乘积使用 * 操作符进行，而点积使用 %*%
    操作符进行：'
- en: x <- random_array(c(32))
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: x <- 随机数组(c(32))
- en: y <- random_array(c(32))
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: y <- 随机数组(c(32))
- en: z <- x %*% y
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: z <- x %*% y
- en: 'In mathematical notation, you’d note the operation with a dot (•):'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学表示中，你会用一个点（•）来表示这个操作：
- en: z = x • y
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: z = x • y
- en: 'Mathematically, what does the dot operation do? Let’s start with the dot product
    of two vectors, x and y. It’s computed as follows:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，点运算做什么？让我们从两个向量 x 和 y 的点积开始。它的计算方法如下：
- en: naive_vector_dot <- function(x, y) {
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: naive_vector_dot <- 函数(x, y) {
- en: stopifnot(length(dim(x)) == 1,➊
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: stopifnot(length(dim(x)) == 1,➊
- en: length(dim(y)) == 1,➊
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: length(dim(y)) == 1,➊
- en: dim(x) == dim(y))
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 维度(x) == 维度(y))
- en: z <- 0
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: z <- 0
- en: for (i in seq_along(x))
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: for (i in seq_along(x))
- en: z <- z + x[i] * y[i]
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: z <- z + x[i] * y[i]
- en: z
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: z
- en: '}'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ x and y are 1D vectors of the same size.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ x 和 y 是大小相同的一维向量。
- en: You’ll have noticed that the dot product between two vectors is a scalar and
    that only vectors with the same number of elements are compatible for a dot product.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，两个向量之间的点积是一个标量，只有元素数量相同的向量才能进行点积运算。
- en: 'You can also take the dot product between a matrix x and a vector y, which
    returns a vector where the coefficients are the dot products between y and the
    rows of x:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以对矩阵 x 和向量 y 进行点积运算，它返回一个向量，其中的系数是 y 和 x 的行之间的点积：
- en: naive_matrix_vector_dot <- function(x, y) {
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: naive_matrix_vector_dot <- 函数(x, y) {
- en: stopifnot(length(dim(x)) == 2,➊
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: stopifnot(length(dim(x)) == 2,➊
- en: length(dim(y)) == 1,➋
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: length(dim(y)) == 1,➋
- en: nrow(x) == dim(y))➌
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: nrow(x) == 维度(y))➌
- en: z <- array(0, dim = dim(y))➍
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: z <- 数组(0, 维度 = 维度(y))➍
- en: for (i in 1:nrow(x))
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: for (i in 1:nrow(x))
- en: for (j in 1:ncol(x))
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: for (j in 1:ncol(x))
- en: z[i] <- z[i] + x[i, j] * y[j]
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: z[i] <- z[i] + x[i, j] * y[j]
- en: z
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: z
- en: '}'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ **x is a 2D tensor (matrix).**
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **x 是一个二维张量（矩阵）。**
- en: ➋ **y is a 1D tensor (vector).**
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **y 是一个一维张量（向量）。**
- en: ➌ **The first dimension of x must be the same as the first dimension of y!**
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **x 的第一个维度必须与 y 的第一个维度相同！**
- en: ➍ **This operation returns a vector of zeros with the same shape as y.**
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **此操作返回一个与 y 形状相同的零向量。**
- en: 'You could also reuse the code we wrote previously, which highlights the relationship
    between a matrix-vector product and a vector product:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以重用我们之前编写的代码，这突出了矩阵-向量乘积和向量乘积之间的关系：
- en: naive_matrix_vector_dot <- function(x, y) {
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: naive_matrix_vector_dot <- 函数(x, y) {
- en: z <- array(0, dim = c(nrow(x)))
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: z <- 数组(0, 维度 = c(nrow(x)))
- en: for (i in 1:nrow(x))
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: for (i in 1:nrow(x))
- en: z[i] <- naive_vector_dot(x[i, ], y)
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: z[i] <- 通过求向量内积得到(x[i, ], y)
- en: z
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: z
- en: '}'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: Note that as soon as one of the two tensors has a length(dim(x)) greater than
    1, %*% is no longer *symmetric*, which is to say that x %*% y isn’t the same as
    y %*% x.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，一旦其中一个张量的 length(dim(x)) 大于 1，%*% 就不再是*对称的*，也就是说 x %*% y 不等于 y %*% x。
- en: 'Of course, a dot product generalizes to tensors with an arbitrary number of
    axes. The most common applications may be the dot product between two matrices.
    You can take the dot product of two matrices x and y (x %*% y) if and only if
    ncol(x) == nrow(y). The result is a matrix with shape (nrow(x), ncol(y)), where
    the coefficients are the vector products between the rows of x and the columns
    of y. The naive implementation is shown here:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，点积可以推广到具有任意数量轴的张量。最常见的应用可能是两个矩阵之间的点积。只有当ncol(x) == nrow(y)时，您才能取两个矩阵 x 和
    y 的点积（x %*% y）。结果是一个形状为(nrow(x), ncol(y))的矩阵，其中的系数是 x 的行和 y 的列之间的向量积。这里展示了朴素实现：
- en: naive_matrix_dot <- function(x, y) {
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: naive_matrix_dot <- function(x, y) {
- en: stopifnot(length(dim(x)) == 2,➊
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: stopifnot(length(dim(x)) == 2,➊
- en: length(dim(y)) == 2,
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: length(dim(y)) == 2,
- en: ncol(x) == nrow(y))➋
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: ncol(x) == nrow(y))➋
- en: z <- array(0, dim = c(nrow(x), ncol(y)))➌
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: z <- array(0, dim = c(nrow(x), ncol(y)))➌
- en: for (i in 1:nrow(x))➍
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: for (i in 1:nrow(x))➍
- en: for (j in 1:ncol(y)) {➎
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: for (j in 1:ncol(y)) {➎
- en: row_x <- x[i, ]
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: row_x <- x[i, ]
- en: column_y <- y[, j]
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: column_y <- y[, j]
- en: z[i, j] <- naive_vector_dot(row_x, column_y)
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: z[i, j] <- naive_vector_dot(row_x, column_y)
- en: '}'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: z
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: z
- en: '}'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ **x and y are 2D tensors (matrices).**
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **x 和 y 是 2D 张量（矩阵）。**
- en: ➋ **The first dimension of x must be the same as the first dimension of y!**
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **x 的第一个维度必须与 y 的第一个维度相同！**
- en: ➌ **This operation returns a matrix of zeros with a specific shape.**
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **此操作返回一个具有特定形状的零矩阵。**
- en: ➍ **Iterate over the rows of x…**
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **迭代 x 的行...**
- en: ➎ **… and over the columns of y.**
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **... 和 y 的列。**
- en: To understand dot-product shape compatibility, it helps to visualize the input
    and output tensors by aligning them as shown in [figure 2.5](#fig2-5).
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解点积形状兼容性，有助于通过对齐输入和输出张量来可视化它们，如[图 2.5](#fig2-5)所示。
- en: In the figure, x, y, and z are pictured as rectangles (literal boxes of coefficients).
    Because the rows of x and the columns of y must have the same size, it follows
    that the width of x must match the height of y. If you go on to develop new machine
    learning algorithms, you’ll likely be drawing such diagrams often.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 在图中，x、y 和 z 被描绘为矩形（系数的文字框）。由于 x 的行和 y 的列必须具有相同的大小，因此 x 的宽度必须与 y 的高度匹配。如果您继续开发新的机器学习算法，您可能经常会画出这样的图表。
- en: '![Image](../images/f0043-01.jpg)'
  id: totrans-358
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0043-01.jpg)'
- en: '**Figure 2.5 Matrix dot-product box diagram**'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.5 矩阵点积盒子图示**'
- en: 'More generally, you can take the dot product between higher-dimensional tensors,
    following the same rules for shape compatibility as outlined earlier for the 2D
    case:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般地，您可以按照与 2D 情况下相同的形状兼容性规则，取高维张量之间的点积：
- en: (a, b, c, d) • (d) -> (a, b, c)
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: (a, b, c, d) • (d) -> (a, b, c)
- en: (a, b, c, d) • (d, e) -> (a, b, c, e)
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: (a, b, c, d) • (d, e) -> (a, b, c, e)
- en: And so on.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 诸如此类。
- en: 2.3.4 Tensor reshaping
  id: totrans-364
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.4 张量重塑
- en: 'A third type of tensor operation that’s essential to understand is *tensor
    reshaping*. Although it wasn’t used in the layer_dense() in our first neural network
    example, we used it when we preprocessed the digits data before feeding it into
    our model, as shown next:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 理解的第三种张量操作是*张量重塑*。尽管它在我们第一个神经网络示例的layer_dense()中没有使用，但我们在将手写数字数据预处理并输入模型之前使用了它，如下所示：
- en: train_images <- array_reshape(train_images, c(60000, 28 * 28))
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: train_images <- array_reshape(train_images, c(60000, 28 * 28))
- en: Note that we use the array_reshape() function rather than the `dim<-`() function
    to reshape R arrays. This is so that the data is reinterpreted using row-major
    semantics (as opposed to R’s default column-major semantics), which is in turn
    compatible with the way the numerical libraries called by Keras (NumPy, TensorFlow,
    and so on) interpret array dimensions. You should always use the array_reshape()
    function when reshaping R arrays that will be passed to Keras.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用array_reshape()函数而不是`dim<-`()函数来重塑 R 数组。这样做是为了使用行优先语义（而不是 R 的默认列优先语义）重新解释数据，这与
    Keras 调用的数值库（NumPy、TensorFlow 等）解释数组维度的方式兼容。当重塑将传递给 Keras 的 R 数组时，应始终使用array_reshape()函数。
- en: 'Reshaping a tensor means rearranging its rows and columns to match a target
    shape. Naturally, the reshaped tensor has the same total number of coefficients
    as the initial tensor. Reshaping is best understood via simple examples:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 重塑张量意味着重新排列其行和列以匹配目标形状。显然，重塑后的张量具有与初始张量相同的总系数数。通过简单示例最好理解重塑：
- en: x <- array(1:6)
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: x <- array(1:6)
- en: x
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: x
- en: '[1] 1 2 3 4 5 6'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 1 2 3 4 5 6'
- en: array_reshape(x, dim = c(3, 2))
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: array_reshape(x, dim = c(3, 2))
- en: '[,1] [,2]'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '[,1] [,2]'
- en: '[1,]     1    2'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '[1,]     1    2'
- en: '[2,]     3    4'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '[2,]     3    4'
- en: '[3,]     5    6'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '[3,]     5    6'
- en: array_reshape(x, dim = c(2, 3))
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: array_reshape(x, dim = c(2, 3))
- en: '[,1] [,2] [,3]'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '[,1] [,2] [,3]'
- en: '[1,]     1    2     3'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '[1,]     1    2     3'
- en: '[2,]     4    5     6'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '[2,]     4    5     6'
- en: 'A special case of reshaping that’s commonly encountered is *transposition.
    Transposing* a matrix means exchanging its rows and its columns, so that x[i,
    ] becomes x[, i]. We can use the t() function to transpose a matrix:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 重塑的一个常见特例是*转置。 转置*矩阵意味着交换其行和列，以便 x[i, ] 变为 x[, i]。我们可以使用 t() 函数来转置矩阵：
- en: x <- array(1:6, dim = c(3, 2))
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: x <- array(1:6, dim = c(3, 2))
- en: x
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: x
- en: '[,1] [,2]'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '[,1] [,2]'
- en: '[1,]     1    4'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '[1,]     1    4'
- en: '[2,]     2    5'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '[2,]     2    5'
- en: '[3,]     3    6'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '[3,]     3    6'
- en: t(x)
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: t(x)
- en: '[,1] [,2] [,3]'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '[,1] [,2] [,3]'
- en: '[1,]     1    2     3'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '[1,]     1    2     3'
- en: '[2,]     4    5     6'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '[2,]     4    5     6'
- en: 2.3.5 Geometric interpretation of tensor operations
  id: totrans-392
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.5 张量操作的几何解释
- en: 'Because the contents of the tensors manipulated by tensor operations can be
    interpreted as coordinates of points in some geometric space, all tensor operations
    have a geometric interpretation. For instance, let’s consider addition. We’ll
    start with the following vector:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 因为张量操作中的张量的内容可以解释为某些几何空间中点的坐标，所以所有张量操作都有一个几何解释。例如，我们来考虑加法。我们将从以下向量开始：
- en: A = c(0.5, 1)
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: A = c(0.5, 1)
- en: It’s a point in a 2D space (see [figure 2.6](#fig2-6)). It’s common to picture
    a vector as an arrow linking the origin to the point, as shown in [figure 2.7](#fig2-7).
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 2D 空间中的一个点（见[图 2.6](#fig2-6)）。通常将向量描绘为将原点与点连接的箭头，如[图 2.7](#fig2-7)所示。
- en: Let’s consider a new point, B = c(1, 0.25), which we’ll add to the previous
    one. This is done geometrically by chaining together the vector arrows, with the
    resulting location being the vector representing the sum of the previous two vectors
    (see [figure 2.8](#fig2-8)). As you can see, adding a vector B to a vector A represents
    the action of copying point A in a new location, whose distance and direction
    from the original point A is determined by the vector B. If you apply the same
    vector addition to a group of points in the plane (an *object*), you would be
    creating a copy of the entire object in a new location (see [figure 2.9](#fig2-9)).
    Tensor addition thus represents the action of *translating an object* (moving
    the object without distorting it) by a certain amount in a certain direction.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们考虑一个新点，B = c(1, 0.25)，我们将其添加到之前的点上。这在几何上通过串联向量箭头来完成，结果位置是代表前两个向量之和的向量的位置（见[图
    2.8](#fig2-8)）。 如您所见，将向量 B 添加到向量 A 表示将点 A 复制到新位置，其距离和方向从原始点 A 决定的位置。 如果将相同的向量加法应用于平面上的一组点（一个*对象*），则将在新位置创建整个对象的副本（见[图
    2.9](#fig2-9)）。 因此，张量加法表示*平移对象*（将对象移动而不会扭曲）一定量的动作在某个方向上。 '
- en: '![Image](../images/f0044-01.jpg)'
  id: totrans-397
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0044-01.jpg)'
- en: '**Figure 2.6 A point in a 2D space**'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.6 2D 空间中的一个点** '
- en: '![Image](../images/f0044-02.jpg)'
  id: totrans-399
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0044-02.jpg)'
- en: '**Figure 2.7 A point in a 2D space pictured as an arrow**'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.7 2D 空间中的一个点被描绘为箭头**'
- en: 'In general, elementary geometric operations such as translation, rotation,
    scaling, skewing, and so on can be expressed as tensor operations. Here are a
    few examples:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，诸如平移、旋转、缩放、扭曲等基本几何操作都可以表达为张量操作。以下是一些示例：
- en: '![Image](../images/f0045-01.jpg)'
  id: totrans-402
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0045-01.jpg)'
- en: '**Figure 2.8 Geometric interpretation of the sum of two vectors**'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.8 两个向量之和的几何解释**'
- en: '*Translation*—As you just saw, adding a vector to a point will move the point
    by a fixed amount in a fixed direction. Applied to a set of points (such as a
    2D object), this is called a “translation” (see [figure 2.9](#fig2-9)).'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*平移*— 正如你刚才看到的，将向量添加到一个点将使该点沿着固定方向移动固定量。 应用于一组点（如二维对象），这称为“平移”（见[图 2.9](#fig2-9)）。'
- en: '![Image](../images/f0045-02.jpg)'
  id: totrans-405
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0045-02.jpg)'
- en: '**Figure 2.9 2D translation as a vector addition**'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.9 2D 平移作为向量加法**'
- en: '*Rotation*—A counterclockwise rotation of a 2D vector by an angle theta (see
    [figure 2.10](#fig2-10)) can be achieved via a dot product with a 2 × 2 matrix
    R = rbind(c(cos(theta), -sin(theta)), c(sin(theta), cos(theta)).'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*旋转*— 将 2D 向量逆时针旋转一个角度 theta（见[图 2.10](#fig2-10)）可以通过与 2 × 2 矩阵 R = rbind(c(cos(theta),
    -sin(theta)), c(sin(theta), cos(theta))）的点积来实现。'
- en: '![Image](../images/f0045-03.jpg)'
  id: totrans-408
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0045-03.jpg)'
- en: '**Figure 2.10 2D rotation (counterclock-wise) as a dot product**'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.10 2D 旋转（逆时针）作为点积**'
- en: '*Scaling*—A vertical and horizontal scaling of the image (see [figure 2.11](#fig2-11))
    can be achieved via a dot product with a 2 × 2 matrix S = rbind(c(horizontal_factor,
    0), c(0, vertical_factor)) (note that such a matrix is called a *diagonal matrix*,
    because it has only non-zero coefficients in its “diagonal,” going from the top
    left to the bottom right).'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*缩放*—通过与2×2矩阵S=rbind(c(horizontal_factor,0), c(0,vertical_factor))的点积实现图像的垂直和水平缩放（参见[图2.11](#fig2-11)）。请注意，这样的矩阵被称为*对角矩阵*，因为它只在其“对角线”（从左上到右下）上有非零系数。'
- en: '![Image](../images/f0046-01.jpg)'
  id: totrans-411
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0046-01.jpg)'
- en: '**Figure 2.11 2D scaling as a dot product**'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '**图2.11作为点积的二维缩放**'
- en: '*Linear transform*—A dot product with an arbitrary matrix implements a linear
    transform. Note that *scaling* and *rotation*, listed previously, are by definition
    linear transforms.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*线性变换*—与任意矩阵的点积实现线性变换。请注意，前面列出的*缩放*和*旋转*都是线性变换的定义。'
- en: '*Affine transform*—An affine transform (see [figure 2.12](#fig2-12)) is the
    combination of a linear transform (achieved via a dot product with some matrix)
    and a translation (achieved via a vector addition). As you have probably recognized,
    that’s exactly the y = W • x + b computation implemented by layer_dense()! A Dense
    layer without an activation function is an affine layer.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*仿射变换*—仿射变换（参见[图2.12](#fig2-12)）是线性变换（通过与某些矩阵的点积实现）和平移（通过矢量加法实现）的组合。正如您可能已经认识到的那样，这正是由layer_dense()实现的y=W•x+b计算！没有激活函数的Dense层就是一个仿射层。'
- en: '![Image](../images/f0046-02.jpg)'
  id: totrans-415
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0046-02.jpg)'
- en: '**Figure 2.12 Affine transform in the plane**'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '**图2.12平面中的仿射变换**'
- en: 'Dense *layer with* relu *activation*—An important observation about affine
    transforms is that if you apply many of them repeatedly, you still end up with
    an affine transform (so you could just have applied that one affine transform
    in the first place). Let’s try it with two: affine2(affine1(x)) = W2 • (W1 • x
    + b1) + b2 = (W2 • W1) • x + (W2 • b1 + b2). That’s an affine transform where
    the linear part is the matrix W2 • W1 and the translation part is the vector W2
    • b1 + b2. As a consequence, a multilayer neural network made entirely of Dense
    layers without activations would be equivalent to a single Dense layer. This “deep”
    neural network would just be a linear model in disguise! This is why we need activation
    functions, like relu (seen in action in [figure 2.13](#fig2-13)). Thanks to activation
    functions, a chain of Dense layers can be made to implement very complex, nonlinear
    geometric transformations, resulting in very rich hypothesis spaces for your deep
    neural networks. We’ll cover this idea in more detail in the next chapter.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关仿射变换的一个重要观察是，如果您重复应用许多仿射变换，您最终仍将得到一种仿射变换（因此，您可以从一开始就应用该一种仿射变换）。让我们尝试两个：affine2(affine1(x))=W2•(W1•x+b1)+b2=(W2•W1)•x+(W2•b1+b2)。其中，线性部分是矩阵W2•W1，平移部分是向量W2•b1+b2。因此，完全由Dense层组成且没有激活函数的多层神经网络等价于一个单一的Dense层。这种“深度”神经网络实际上只是一个线性模型的伪装！这就是为什么需要激活函数，例如ReLU（在[图2.13](#fig2-13)中展示）。由于激活函数，Dense层的链可以实现非常复杂的非线性几何变换，从而为深度神经网络提供非常丰富的假设空间。我们将在下一章中详细介绍这个想法。
- en: '![Image](../images/f0047-01.jpg)'
  id: totrans-418
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0047-01.jpg)'
- en: '**Figure 2.13 Affine transform followed by relu activation**'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '**图2.13仿射变换后的ReLU激活**'
- en: 2.3.6 A geometric interpretation of deep learning
  id: totrans-420
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.6深度学习的几何解释
- en: You just learned that neural networks consist entirely of chains of tensor operations,
    and that these tensor operations are just simple geometric transformations of
    the input data. It follows that you can interpret a neural network as a very complex
    geometric transformation in a high-dimensional space, implemented via a series
    of simple steps.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 您刚刚学习到神经网络完全由输入数据的简单几何变换组成，这些张量运算只是在高维空间中实现的非常复杂的几何变换系列。因此，您可以将神经网络解释为非常复杂的几何变换，由一系列简单步骤实现。
- en: 'In 3D, the following mental image may prove useful. Imagine two sheets of colored
    paper: one red and one blue. Put one on top of the other. Now crumple them together
    into a small ball. That crumpled paper ball is your input data, and each sheet
    of paper is a class of data in a classification problem. What a neural network
    is meant to do is figure out a transformation of the paper ball that would uncrumple
    it, so as to make the two classes cleanly separable again (see [figure 2.14](#fig2-14)).
    With deep learning, this would be implemented as a series of simple transformations
    of the 3D space, such as those you could apply on the paper ball with your fingers,
    one movement at a time.'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 在3D视图中，以下心理形象可能会有所帮助。想象两张彩色纸：一张红色，一张蓝色。将一张放在另一张上面。现在将它们揉成一个小球。那个揉皱的纸球就是你的输入数据，而每张纸是分类问题中的一类数据。神经网络的目的是找出一个能够将纸球展平的变换，使得两类数据重新变得清晰可分（参见[图2.14](#fig2-14)）。对于深度学习来说，这将被实现为对三维空间的一系列简单变换，就像你可以用手指在纸球上做的那样，一次移动一个轴。
- en: '![Image](../images/f0047-02.jpg)'
  id: totrans-423
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0047-02.jpg)'
- en: '**Figure 2.14 Uncrumpling a complicated manifold of data**'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '**图2.14 解开复杂的数据流形**'
- en: 'Uncrumpling paper balls is what machine learning is about: finding neat representations
    for complex, highly folded data *manifolds* in high-dimensional spaces (a manifold
    is a continuous surface, like our crumpled sheet of paper). At this point, you
    should have a pretty good intuition as to why deep learning excels at this: it
    takes the approach of incrementally decomposing a complicated geometric transformation
    into a long chain of elementary ones, which is pretty much the strategy a human
    would follow to uncrumple a paper ball. Each layer in a deep network applies a
    transformation that disentangles the data a little, and a deep stack of layers
    makes tractable an extremely complicated disentanglement process.'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 展平纸球正是机器学习的目标：在高维空间中为复杂的、高度折叠的数据*流形*找到整洁的表示（流形是一种连续的曲面，就像我们揉皱的纸）。此时，你应该对为什么深度学习在这方面表现出色有一个相当好的直觉：它采取了将复杂几何变换逐步分解为一长串基本变换的方法，这几乎就是人类用来展平纸球的策略。深度网络中的每一层都应用了一个轻微地解缠数据的变换，而深层的层叠则使得这个极其复杂的解缠过程变得可行。
- en: '2.4 The engine of neural networks: Gradient-based optimization'
  id: totrans-426
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 神经网络的引擎：基于梯度的优化
- en: 'As you saw in the previous section, each neural layer from our first model
    example transforms its input data as follows:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 如前几节所示，我们第一个模型示例中的每个神经层将其输入数据转换为以下形式：
- en: output <- relu(dot(input, W) + b)
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: output <- relu(dot(input, W) + b)
- en: In this expression, W and b are tensors that are attributes of the layer. They’re
    called the *weights* or *trainable parameters* of the layer (the kernel and bias
    attributes, respectively). These weights contain the information learned by the
    model from exposure to training data.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个表达式中，W 和 b 是层的属性的张量。通常称为层的*权重*或*可训练参数*（分别是 kernel 和 bias 属性）。这些权重包含了模型从训练数据中学到的信息。
- en: 'Initially, these weight matrices are filled with small random values (a step
    called *random initialization*). Of course, there’s no reason to expect that relu(dot(input,
    W) + b), when W and b are random, will yield any useful representations. The resulting
    representations are meaningless—but they’re a starting point. What comes next
    is to gradually adjust these weights, based on a feedback signal. This gradual
    adjustment, also called *training*, is the learning that machine learning is all
    about. This happens within what’s called a *training loop*, which works as follows.
    Repeat these steps in a loop, until the loss seems sufficiently low:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，这些权重矩阵被填充为小的随机值（这一步被称为*随机初始化*）。当然，没有理由指望当 W 和 b 是随机时，relu(dot(input, W) +
    b) 会产生任何有用的表示。由此产生的表示是毫无意义的，但它们是一个起点。接下来的步骤是根据反馈信号逐渐调整这些权重。这种逐渐调整，也称为*训练*，就是机器学习的学习过程。这发生在所谓的*训练循环*中，工作方式如下。重复以下步骤，直到损失似乎足够低：
- en: '**1** Draw a batch of training samples, x, and corresponding targets, y_true.'
  id: totrans-431
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**1** 随机选择一批训练样本 x，并对应的目标值 y_true。'
- en: '**2** Run the model on x (a step called the *forward pass*) to obtain predictions,
    y_pred.'
  id: totrans-432
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**2** 在 x 上运行模型（称为*前向传播*）以获得预测结果 y_pred。'
- en: '**3** Compute the loss of the model on the batch, a measure of the mismatch
    between y_pred and y_true.'
  id: totrans-433
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**3** 计算模型在批量数据上的损失，衡量 y_pred 和 y_true 之间的不匹配程度。'
- en: '**4** Update all weights of the model in a way that slightly reduces the loss
    on this batch.'
  id: totrans-434
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新模型的所有权重，以稍微减少此批次上的损失。
- en: 'You’ll eventually end up with a model that has a very low loss on its training
    data: a low mismatch between predictions, y_pred, and expected targets, y_true.
    The model has “learned” to map its inputs to correct targets. From afar, it may
    look like magic, but when you reduce it to elementary steps, it turns out to be
    simple.'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，您将得到一个模型，其训练数据上的损失非常低：预测y_pred和期望目标y_true之间的低不匹配。模型已经“学会”将其输入映射到正确的目标。从远处看，它可能看起来像是魔术，但当您将其简化为基本步骤时，它变得简单。
- en: 'Step 1 sounds easy enough—just I/O code. Steps 2 and 3 are merely the application
    of a handful of tensor operations, so you could implement these steps purely from
    what you learned in the previous section. The difficult part is step 4: updating
    the model’s weights. Given an individual weight coefficient in the model, how
    can you compute whether the coefficient should be increased or decreased, and
    by how much?'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 第1步听起来很容易——只是I/O代码。步骤2和步骤3仅仅是一些张量操作的应用，因此您可以纯粹地从前一节中学到的内容来实现这些步骤。困难的部分是步骤4：更新模型的权重。给定模型中的一个单独的权重系数，您如何计算系数是否应增加或减少，以及增加或减少多少？
- en: One naive solution would be to freeze all weights in the model except the one
    scalar coefficient being considered, and try different values for this coefficient.
    Let’s say the initial value of the coefficient is 0.3\. After the forward pass
    on a batch of data, the loss of the model on the batch is 0.5\. If you change
    the coefficient’s value to 0.35 and rerun the forward pass, the loss increases
    to 0.6\. But if you lower the coefficient to 0.25, the loss falls to 0.4\. In
    this case, it seems that updating the coefficient by –0.05 would contribute to
    minimizing the loss. This would have to be repeated for all coefficients in the
    model.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 一个天真的解决方案是冻结模型中的所有权重，除了正在考虑的一个标量系数，并尝试不同的值用于此系数。假设系数的初始值为0.3。在一批数据的前向传递之后，模型对该批次的损失为0.5。如果将系数的值更改为0.35并重新运行前向传递，则损失增加到0.6。但如果将系数降低到0.25，则损失降至0.4。在这种情况下，似乎通过-0.05更新系数将有助于最小化损失。这必须对模型中的所有系数重复进行。
- en: 'But such an approach would be horribly inefficient, because you’d need to compute
    two forward passes (which are expensive) for every individual coefficient (of
    which there are many—usually thousands and sometimes up to millions). Thankfully,
    there’s a much better approach: *gradient descent*.'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 但这样的方法将非常低效，因为您需要为每个单独的系数计算两次前向传递（这些操作很昂贵）（其中有许多系数——通常是数千个，有时甚至高达数百万个）。幸运的是，有一个更好的方法：梯度下降。
- en: 'Gradient descent is the optimization technique that powers modern neural networks.
    Here’s the gist of it: all of the functions used in our models (such as dot or
    +) transform their input in a smooth and continuous way. If you look at z = x
    + y, for instance, a small change in y results in only a small change in z, and
    if you know the direction of the change in y, you can infer the direction of the
    change in z. Mathematically, you’d say these functions are *differentiable*. If
    you chain together such functions, the bigger function you obtain is still differentiable.
    In particular, this applies to the function that maps the model’s coefficients
    to the loss of the model on a batch of data: a small change in the model’s coefficients
    results in a small, predictable change in the loss value. This enables you to
    use a mathematical operator called the *gradient* to describe how the loss varies
    as you move the model’s coefficients in different directions. If you compute this
    gradient, you can use it to move the coefficients (all at once in a single update,
    rather than one at a time) in a direction that decreases the loss.'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是驱动现代神经网络的优化技术。这是它的要点：我们模型中使用的所有函数（如点或+）都以平滑连续的方式转换其输入。例如，如果您查看z = x + y，小的y变化仅导致z的小变化，并且如果您知道y变化的方向，您可以推断z变化的方向。数学上，您会说这些函数是可微的。如果您链接这样的函数，您得到的较大函数仍然是可微的。特别是，这适用于将模型的系数映射到一批数据上的模型损失的函数：模型的系数的微小变化导致损失值的微小且可预测的变化。这使您能够使用称为*梯度*的数学运算符描述损失如何随着您在不同方向上移动模型的系数而变化。如果计算此梯度，则可以使用它将系数（一次性一次性更新，而不是逐个更新）向减少损失的方向移动。
- en: If you already know what *differentiable* means and what a *gradient* is, you
    can skip to section 2.4.3\. Otherwise, the following two sections will help you
    understand these concepts.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经知道*可微分*的含义和*梯度*是什么，可以直接跳到第 2.4.3 节。否则，接下来的两节将帮助您理解这些概念。
- en: 2.4.1 What’s a derivative?
  id: totrans-441
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.1 什么是导数？
- en: Consider a continuous, smooth function f(x) = y, mapping a number, x, to a new
    number, y. We can use the function in [figure 2.15](#fig2-15) as an example.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个连续且平滑的函数 f(x) = y，将一个数 x 映射到一个新数 y。我们可以用 [图 2.15](#fig2-15) 中的函数作为示例。
- en: 'Because the function is *continuous*, a small change in x can only result in
    a small change in y—that’s the intuition behind *continuity*. Let’s say you increase
    x by a small factor, epsilon_x: this results in a small epsilon_y change to y,
    as shown in [figure 2.16](#fig2-16).'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 因为函数是*连续*的，x 的微小变化只能导致 y 的微小变化——这就是*连续性*背后的直觉。假设您将 x 增加一个小因子 epsilon_x：这将导致
    y 发生一个小的 epsilon_y 变化，如 [图 2.16](#fig2-16) 所示。
- en: 'In addition, because the function is *smooth* (its curve doesn’t have any abrupt
    angles), when epsilon_x is small enough, around a certain point p, it’s possible
    to approximate f as a linear function of slope a, so that epsilon_y becomes a
    * epsilon_x:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于函数是*平滑*的（其曲线没有任何突然的角度），当 epsilon_x 足够小，围绕某一点 p 时，可以将 f 近似为斜率为 a 的线性函数，使得
    epsilon_y 变为 * epsilon_x：
- en: f(x + epsilon_x) = y + a * epsilon_x
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: f(x + epsilon_x) = y + a * epsilon_x
- en: Obviously, this linear approximation is valid only when x is close enough to
    p.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这种线性近似仅在 x 足够接近 p 时有效。
- en: '![Image](../images/f0049-01.jpg)'
  id: totrans-447
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0049-01.jpg)'
- en: '**Figure 2.15 A continuous, smooth function**'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.15 一个连续且平滑的函数**'
- en: '![Image](../images/f0049-02.jpg)'
  id: totrans-449
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0049-02.jpg)'
- en: '**Figure 2.16 With a continuous function, a small change in x results in a
    small change in y.**'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.16 对于一个连续函数，x 的微小变化会导致 y 的微小变化。**'
- en: '![Image](../images/f0050-01.jpg)'
  id: totrans-451
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0050-01.jpg)'
- en: '**Figure 2.17 Derivative of f in p**'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.17 在 p 点的 f 的导数**'
- en: The slope a is called the *derivative* of f in p. If a is negative, it means
    a small increase in x around p will result in a decrease of f(x) (as shown in
    [figure 2.17](#fig2-17)), and if a is positive, a small increase in x will result
    in an increase of f(x). Further, the absolute value of a (the *magnitude* of the
    derivative) tells you how quickly this increase or decrease will happen.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 斜率 a 被称为 p 点处 f 的*导数*。如果 a 是负的，意味着在 p 点周围 x 的微小增加会导致 f(x) 的减少（如 [图 2.17](#fig2-17)
    所示），如果 a 是正的，x 的微小增加会导致 f(x) 的增加。此外，a 的绝对值（导数的*大小*）告诉您这种增加或减少将发生的速度。
- en: 'For every differentiable function f(x) (*differentiable* means “can be differentiated
    to find the derivative”: for example, smooth, continuous functions can be differentiated),
    there exists a derivative function f’(x) that maps values of x to the slope of
    the local linear approximation of f in those points. For instance, the derivative
    of cos(x) is -sin(x), the derivative of f(x) = a * x is f’(x) = a, and so on.'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个可微分函数 f(x)（*可微分*意味着“可以求导以找到导数”：例如，平滑、连续函数可以求导），都存在一个导数函数 f’(x)，将 x 的值映射到这些点上
    f 的局部线性近似的斜率。例如，cos(x) 的导数是 -sin(x)，f(x) = a * x 的导数是 f’(x) = a，等等。
- en: 'Being able to differentiate functions is a very powerful tool when it comes
    to *optimization*, the task of finding values of x that minimize the value of
    f(x). If you’re trying to update x by a factor epsilon_x to minimize f(x), and
    you know the derivative of f, then your job is done: the derivative completely
    describes how f(x) evolves as you change x. If you want to reduce the value of
    f(x), you just need to move x a little in the opposite direction from the derivative.'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及*优化*时，能够对函数进行微分是一种非常强大的工具，优化是找到最小化 f(x) 值的 x 的任务。如果您尝试通过因子 epsilon_x 更新 x
    以最小化 f(x)，并且您知道 f 的导数，那么您的任务就完成了：导数完全描述了 f(x) 随着 x 变化的方式。如果您想要减少 f(x) 的值，您只需要将
    x 沿着导数的相反方向移动一点。
- en: '2.4.2 Derivative of a tensor operation: The gradient'
  id: totrans-456
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.2 张量运算的导数：梯度
- en: 'The function we were just looking at turned a scalar value x into another scalar
    value y: you could plot it as a curve in a 2D plane. Now imagine a function that
    turns a list of scalars (x, y) into a scalar value z: that would be a vector operation.
    You could plot it as a 2D *surface* in a 3D space (indexed by coordinates x, y,
    z). Likewise, you can imagine functions that take matrices as inputs, functions
    that take rank 3 tensors as inputs, and so on.'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚看的函数将一个标量值x转换为另一个标量值y：你可以将其绘制为二维平面上的曲线。现在想象一个将一组标量（x，y）转换为标量值z的函数：这将是一个矢量操作。你可以将其绘制为三维空间中的二维*表面*（由坐标x、y、z索引）。同样，你可以想象将矩阵作为输入的函数，将秩为3的张量作为输入的函数，依此类推。
- en: The concept of differentiation can be applied to any such function, as long
    as the surfaces they describe are continuous and smooth. The derivative of a tensor
    operation (or tensor function) is called a *gradient*. Gradients are just the
    generalization of the concept of derivatives to functions that take tensors as
    inputs. Remember how, for a scalar function, the derivative represents the *local
    slope* of the curve of the function? In the same way, the gradient of a tensor
    function represents the *curvature* of the multidimensional surface described
    by the function. It characterizes how the output of the function varies when its
    input parameters vary.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 差分的概念可以应用于任何这样的函数，只要它们描述的表面是连续的和光滑的。张量操作（或张量函数）的导数称为*梯度*。梯度只是将导数的概念推广到以张量作为输入的函数。记得标量函数的导数代表函数曲线的*局部斜率*吗？以同样的方式，张量函数的梯度代表函数描述的多维表面的*曲率*。它表征了函数的输出在其输入参数变化时的变化情况。
- en: 'Let’s look at an example grounded in machine learning. Consider the following:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个机器学习中的例子。考虑以下情况：
- en: An input vector, x (a sample in a dataset)
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个输入向量，x（数据集中的一个样本）
- en: A matrix, W (the weights of a model)
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个矩阵，W（模型的权重）
- en: A target, y_true (what the model should learn to associate to x)
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个目标，y_true（模型应该学习将其与x相关联）
- en: A loss function, loss_fn() (meant to measure the gap between the model’s current
    predictions and y_true)
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个损失函数，loss_fn()（用于衡量模型当前预测和y_true之间的差距）
- en: 'You can use W to compute a target candidate y_pred, and then compute the loss,
    or mismatch, between the target candidate y_pred and the target y_true:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用W计算一个目标候选y_pred，然后计算目标候选y_pred与目标y_true之间的损失或不匹配：
- en: y_pred <- dot(W, x)➊
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: y_pred <- dot(W, x)➊
- en: loss_value <- loss_fn(y_pred, y_true)➋
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: loss_value <- loss_fn(y_pred, y_true)➋
- en: ➊ **We use the model weights, W, to make a prediction for x.**
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **我们使用模型权重W对x进行预测。**
- en: ➋ **We estimate how far off the prediction was.**
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **我们估计预测的偏差有多大。**
- en: 'Now we’d like to use gradients to figure out how to update W so as to make
    loss_value smaller. How do we do that? Given fixed inputs x and y_true, the preceding
    operations can be interpreted as a function mapping values of W (the model’s weights)
    to loss values:'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们想使用梯度来找出如何更新W以使loss_value变小。我们该如何做？给定固定的输入x和y_true，前述操作可以解释为将W的值（模型的权重）映射到loss值的函数：
- en: loss_value <- f(W)➊
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: loss_value <- f(W)➊
- en: ➊ **f() describes the curve (or high-dimensional surface) formed by loss values
    when W varies.**
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **f()描述了当W变化时损失值形成的曲线（或高维表面）。**
- en: Let’s say the current value of W is W0. Then the derivative of f() at the point
    W0 is a tensor grad(loss_value, W0), with the same shape as W, where each coefficient
    grad(loss_ value, W0)[i, j] indicates the direction and magnitude of the change
    in loss_value you observe when modifying W0[i, j]. That tensor grad(loss_value,
    W0) is the gradient of the function f(W) = loss_value in W0, also called “gradient
    of loss_value with respect to W around W0.”
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 假设当前值为W0。那么f()在点W0处的导数是一个张量grad(loss_value, W0)，与W具有相同的形状，其中每个系数grad(loss_ value,
    W0)[i, j]指示修改W0[i, j]时观察到的loss_value变化的方向和大小。该张量grad(loss_value, W0)是函数f(W) =
    loss_value在W0处的梯度，也称为“关于W在W0附近的loss_value的梯度”。
- en: '**Partial derivatives**'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: '**偏导数**'
- en: The tensor operation grad(f(W), W) (which takes as input a matrix W) can be
    expressed as a combination of scalar functions, grad_ij(f(W), w_ij), each of which
    would return the derivative of loss_value = f(W) with respect to the coefficient
    W[i, j] of W, assuming all other coefficients are constant. grad_ij is called
    the *partial derivative* of f with respect to W[i, j].
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 张量操作grad(f(W), W)（将矩阵W作为输入）可以表示为标量函数grad_ij(f(W), w_ij)的组合，其中每个函数将返回loss_value
    = f(W)相对于W的系数W[i, j]的导数，假设所有其他系数都是常数。grad_ij称为f相对于W[i, j]的*偏导数*。
- en: Concretely, what does grad(loss_value, W0) represent? You saw earlier that the
    derivative of a function f(x) of a single coefficient can be interpreted as the
    slope of the curve of f(). Likewise, grad(loss_value, W0) can be interpreted as
    the tensor describing the *direction of steepest ascent* of loss_value = f(W)
    around W0, as well as the slope of this ascent. Each partial derivative describes
    the slope of f() in a specific direction.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，grad(loss_value, W0) 代表什么？你之前看到过，函数 f(x) 的导数可以解释为 f() 曲线的斜率。同样，grad(loss_value,
    W0) 可以解释为描述 loss_value = f(W) 在 W0 附近的 *最陡上升方向* 的张量，以及这种上升的斜率。每个偏导数描述了特定方向上 f()
    的斜率。
- en: 'For this reason, in much the same way that, for a function f(x), you can reduce
    the value of f(x) by moving x a little in the opposite direction from the derivative,
    with a function f(W) of a tensor, you can reduce loss_value = f(W) by moving W
    in the opposite direction from the gradient: for example, W1 = W0 – step * grad(f(W0),
    W0) (where step is a small scaling factor). That means going against the direction
    of steepest ascent of f, which intuitively should put you lower on the curve.
    Note that the scaling factor step is needed because grad(loss_value, W0) approximates
    the curvature only when you’re close to W0, so you don’t want to get too far from
    W0.'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 出于这个原因，就像对于函数 f(x)，你可以通过将 x 沿着导数相反的方向移动一点来减少 f(x) 的值一样，对于张量的函数 f(W)，你可以通过将 W
    沿着梯度相反的方向移动来减少 loss_value = f(W)：例如，W1 = W0 - step * grad(f(W0), W0)（其中 step 是一个小的缩放因子）。这意味着朝着
    f 的最陡上升方向反向移动，直观上应该使你在曲线上更低。注意，缩放因子 step 是必需的，因为 grad(loss_value, W0) 仅在接近 W0
    时近似曲率，所以你不希望离 W0 太远。
- en: 2.4.3 Stochastic gradient descent
  id: totrans-477
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.3 随机梯度下降
- en: 'Given a differentiable function, it’s theoretically possible to find its minimum
    analytically: it’s known that a function’s minimum is a point where the derivative
    is 0, so all you have to do is find all the points where the derivative goes to
    0 and check for which of these points the function has the lowest value.'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个可微分函数，理论上可以在分析上找到其最小值：众所周知，函数的最小值是导数为 0 的点，所以你所要做的就是找到导数为 0 的所有点，并检查这些点中哪个点函数具有最小值。
- en: Applied to a neural network, that means finding analytically the combination
    of weight values that yields the smallest possible loss function. This can be
    done by solving the equation grad(f(W), W) = 0 for W. This is a polynomial equation
    of N variables, where N is the number of coefficients in the model. Although it
    would be possible to solve such an equation for N = 2 or N = 3, doing so is intractable
    for real neural networks, where the number of parameters is never less than a
    few thousand and can often be several tens of millions.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 应用于神经网络，这意味着在理论上找到使损失函数可能最小的权重值组合。这可以通过求解方程 grad(f(W), W) = 0 来完成对 W。这是一个包含
    N 个变量的多项式方程，其中 N 是模型中的系数数目。尽管可以解决 N = 2 或 N = 3 的方程，但在实际的神经网络中，解决这样的方程是不可行的，因为参数的数量从来不少于几千，并且通常可以达到几千万。
- en: 'Instead, you can use the four-step algorithm outlined at the beginning of this
    section: modify the parameters little by little based on the current loss value
    for a random batch of data, as follows. Because you’re dealing with a differentiable
    function, you can compute its gradient, which gives you an efficient way to implement
    step 4\. If you update the weights in the opposite direction from the gradient,
    the loss will be a little less every time:'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，你可以使用本节开头概述的四步算法：根据当前数据的随机批次的当前损失值逐渐修改参数，如下所示。因为你正在处理一个可微分的函数，你可以计算其梯度，这给你了一个实现步骤
    4 的高效方法。如果你更新权重与梯度相反的方向，损失每次都会减少一点：
- en: '**1** Draw a batch of training samples, x, and corresponding targets, y_true.'
  id: totrans-481
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**1** 绘制一批训练样本 x 和相应的目标 y_true。'
- en: '**2** Run the model on x to obtain predictions, y_pred (this is called the
    *forward pass*).'
  id: totrans-482
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**2** 运行模型在 x 上以获得预测值，y_pred（这称为 *前向传播*）。'
- en: '**3** Compute the loss of the model on the batch, a measure of the mismatch
    between y_pred and y_true.'
  id: totrans-483
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**3** 计算模型在批处理上的损失，即 y_pred 和 y_true 之间的不匹配程度的度量。'
- en: '**4** Compute the gradient of the loss with regard to the model’s parameters
    (this is called the *backward pass*).'
  id: totrans-484
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**4** 计算损失相对于模型参数的梯度（这称为 *反向传播*）。'
- en: '**5** Move the parameters a little in the opposite direction from the gradient—for
    example, W = W – (learning_rate * gradient)—thus reducing the loss on the batch
    a bit. The *learning rate* (learning_rate here) would be a scalar factor modulating
    the “speed” of the gradient descent process.'
  id: totrans-485
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**5** 将参数稍微朝着与梯度相反的方向移动一点，例如，W = W - (learning_rate * gradient)，从而稍微减少批处理上的损失。*学习速率*（这里是learning_rate）将是调节梯度下降过程“速度”的标量因子。'
- en: Easy enough! What we just described is called *mini-batch stochastic gradient
    descent* (mini-batch SGD). The term *stochastic* refers to the fact that each
    batch of data is drawn at random (*stochastic* is a scientific synonym of *random*).
    [Figure 2.18](#fig2-18) illustrates what happens in 1D, when the model has only
    one parameter and you have only one training sample.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 很简单！我们刚刚描述的称为*小批量随机梯度下降*（小批量SGD）。术语*随机*是指每个数据批次都是随机绘制的（*随机*是*随机*的科学同义词）。[图2.18](#fig2-18)说明了在一维情况下的情况，当模型只有一个参数时，而你只有一个训练样本。
- en: As you can see, intuitively it’s important to pick a reasonable value for the
    learning_ rate factor. If it’s too small, the descent down the curve will take
    many iterations, and it could get stuck in a local minimum. If learning_rate is
    too large, your updates may end up taking you to completely random locations on
    the curve.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，直观地选择合理的学习率因子是很重要的。如果太小，曲线下降将需要很多次迭代，并且可能会陷入局部最小值。如果学习速率太大，你的更新可能会将你带到曲线上完全随机的位置。
- en: '![Image](../images/f0052-01.jpg)'
  id: totrans-488
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0052-01.jpg)'
- en: '**Figure 2.18 SGD down a 1D loss curve (one learnable parameter)**'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: '**图2.18 在一维损失曲线上的SGD（一个可学习参数）**'
- en: Note that a variant of the mini-batch SGD algorithm would be to draw a single
    sample and target at each iteration, rather than drawing a batch of data. This
    would be *true* SGD (as opposed to *mini-batch* SGD). Alternatively, going to
    the opposite extreme, you could run every step on *all* data available, which
    is called *batch gradient descent*. Each update would then be more accurate but
    far more expensive. The efficient compromise between these two extremes is to
    use mini-batches of reasonable size.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，小批量SGD算法的一种变体是在每次迭代中绘制单个样本和目标，而不是绘制一批数据。这将是*真正的*SGD（与*小批量*SGD相对）。或者，走向相反的极端，你可以在*所有*可用数据上运行每一步，这称为*批量梯度下降*。然后，每个更新将更加准确，但成本更高。在这两个极端之间的有效折衷是使用合理大小的小批量。
- en: 'Although [figure 2.18](#fig2-18) illustrates gradient descent in a 1D parameter
    space, in practice you’ll use gradient descent in highly dimensional spaces: every
    weight coefficient in a neural network is a free dimension in the space, and there
    may be tens of thousands or even millions of them. To help you build intuition
    about loss surfaces, you can also visualize gradient descent along a 2D loss surface,
    as shown in [figure 2.19](#fig2-19). But you can’t possibly visualize what the
    actual process of training a neural network looks like—you can’t represent a 1,000,000-dimensional
    space in a way that makes sense to humans. As such, it’s good to keep in mind
    that the intuitions you develop through these low-dimensional representations
    may not always be accurate in practice. This has historically been a source of
    issues in the world of deep learning research.'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然[图2.18](#fig2-18)说明了在一维参数空间中的梯度下降，但在实际中，你会在高维空间中使用梯度下降：神经网络中的每个权重系数都是空间中的自由维度，可能有成千上万甚至数百万个。为了帮助你对损失曲面建立直觉，你也可以将梯度下降可视化为沿着二维损失曲面的过程，如[图2.19](#fig2-19)所示。但你不可能可视化神经网络训练的实际过程——你无法用对人类有意义的方式表示一个100万维的空间。因此，要记住，你通过这些低维表示所形成的直觉在实践中可能并不总是准确的。这在深度学习研究领域历史上一直是一个问题的根源。
- en: '![Image](../images/f0053-01.jpg)'
  id: totrans-492
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0053-01.jpg)'
- en: '**Figure 2.19 Gradient descent down a 2D loss surface (two learnable parameters)**'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: '**图2.19 二维损失曲面上的梯度下降（两个可学习参数）**'
- en: 'Additionally, multiple variants of SGD exist that differ by taking into account
    previous weight updates when computing the next weight update, rather than just
    looking at the current value of the gradients. There is, for instance, SGD with
    momentum, as well as AdaGrad, RMSprop, and several others. Such variants are known
    as *optimization methods* or *optimizers*. In particular, the concept of *momentum*,
    which is used in many of these variants, deserves your attention. Momentum addresses
    two issues with SGD: convergence speed and local minima. Consider [figure 2.20](#fig2-20),
    which shows the curve of a loss as a function of a model parameter.'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，存在多种不同的 SGD 变种，其在计算下一个权重更新时，不仅仅考虑当前梯度值，还考虑之前的权重更新值。例如，动量 SGD，以及 AdaGrad、RMSprop
    等等。这些变种被称为*优化方法*或*优化器*。特别是，许多这些变种中使用的*动量*的概念值得关注。动量解决了 SGD 的两个问题：收敛速度和局部最小值。考虑[图
    2.20](#fig2-20)，它展示了一种模型参数的损失曲线。
- en: '![Image](../images/f0053-02.jpg)'
  id: totrans-495
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0053-02.jpg)'
- en: '**Figure 2.20 A local minimum and a global minimum**'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.20 局部最小值和全局最小值**'
- en: 'As you can see, around a certain parameter value, there is a *local minimum*:
    around that point, moving left would result in the loss increasing, but so would
    moving right. If the parameter under consideration were being optimized via SGD
    with a small learning rate, the optimization process could get stuck at the local
    minimum instead of making its way to the global minimum.'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，在某一参数值附近，存在一个*局部最小值*：在该点附近，向左移动会导致损失增加，向右移动也会导致损失增加。如果正在通过小学习率使用 SGD 来优化考虑的参数，优化过程可能会陷入局部最小值而无法到达全局最小值。
- en: 'You can avoid such issues by using momentum, which draws inspiration from physics.
    A useful mental image here is to think of the optimization process as a small
    ball rolling down the loss curve. If it has enough momentum, the ball won’t get
    stuck in a ravine and will end up at the global minimum. Momentum is implemented
    by moving the ball at each step based not only on the current slope value (current
    acceleration) but also on the current velocity (resulting from past acceleration).
    In practice, this means updating the parameter w based not only on the current
    gradient value but also on the previous parameter update, such as in this naive
    implementation:'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过使用动量来避免这样的问题，动量受到物理学的启发。一个有用的心理形象是将优化过程想象成一个小球沿着损失曲线滚动的过程。如果它有足够的动量，小球不会被困在沟壑中，并最终到达全局最小值。动量是通过根据当前斜率值（当前加速度）和当前速度（过去加速度的结果）来每步移动球实现的。实际上，这意味着更新参数w不仅基于当前梯度值，而且还基于先前的参数更新，如下面的简单实现所示：
- en: past_velocity <- 0
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 过去速度 <- 0
- en: momentum <- 0.1➊
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 动量 <- 0.1➊
- en: repeat {➋
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 重复 {➋
- en: p <- get_current_parameters()➌
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: p <- get_current_parameters()➌
- en: if (p$loss <= 0.01)
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 (p$loss <= 0.01)
- en: break
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 跳出
- en: velocity <- past_velocity * momentum + learning_rate * p$gradient w <- p$w +
    momentum * velocity - learning_rate * p$gradient
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 速度 <- 过去速度 * 动量 + 学习率 * p$gradient w <- p$w + 动量 * 速度 - 学习率 * p$gradient
- en: past_velocity <- velocity
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 过去速度 <- 速度
- en: update_parameter(w)
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 更新参数(w)
- en: '}'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ **Constant momentum factor**
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **常数动量因子**
- en: ➋ **Optimization loop**
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **优化循环**
- en: '➌ **p contains: w, loss, gradient**'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **p 包含：w，损失，梯度**
- en: '2.4.4 Chaining derivatives: The backpropagation algorithm'
  id: totrans-512
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.4 链式导数：反向传播算法
- en: In the preceding algorithm, we casually assumed that because a function is differentiable,
    we can easily compute its gradient. But is that true? How can we compute the gradient
    of complex expressions in practice? In the two-layer model we started the chapter
    with, how can we get the gradient of the loss with regard to the weights? That’s
    where the *backpropagation algorithm* comes in.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的算法中，我们随意地假设因为一个函数是可导的，我们可以很容易地计算它的梯度。但这是真的吗？我们如何在实践中计算复杂表达式的梯度？在我们本章一开始的两层模型中，我们如何得到损失对于权重的梯度？这就是**反向传播算法**的用武之地。
- en: THE CHAIN RULE
  id: totrans-514
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 链式法则
- en: 'Backpropagation is a way to use the derivatives of simple operations (such
    as addition, relu, or tensor product) to easily compute the gradient of arbitrarily
    complex combinations of these atomic operations. Crucially, a neural network consists
    of many tensor operations chained together, each of which has a simple, known
    derivative. For instance, the model defined in listing 2.2 can be expressed as
    a function parameterized by the variables W1, b1, W2, and b2 (belonging to the
    first and second Dense layers, respectively), involving the atomic operations
    dot, relu, softmax, and +, as well as our loss function loss, which are all easily
    differentiable:'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播是一种使用简单操作的导数（如加法、relu或张量乘积）来轻松计算这些原子操作的任意复合操作的梯度的方法。关键是，神经网络由许多链接在一起的张量操作组成，每个操作的导数都是简单且已知的。例如，例2.2中定义的模型可以表示为由变量
    W1、b1、W2 和 b2（分别属于第一和第二个密集层）参数化的函数，涉及原子操作 dot、relu、softmax 和 +，以及我们的损失函数 loss，这些都很容易可导：
- en: loss_value <- loss(y_true,
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: loss_value <- loss(y_true,
- en: softmax(dot(relu(dot(inputs, W1) + b1), W2) + b2))
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: softmax(dot(relu(dot(inputs, W1) + b1), W2) + b2))
- en: Calculus tells us that such a chain of functions can be derived using the following
    identity, called the *chain rule*.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 微积分告诉我们，这样的函数链可以使用以下恒等式进行求导，称为*链式法则*。
- en: 'Consider two functions f and g, as well as the composed function fg such that
    fg(x) == f(g(x)):'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑两个函数 f 和 g，以及复合函数 fg，使得 fg(x) == f(g(x))：
- en: fg <- function(x) {
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: fg <- function(x) {
- en: x1 <- g(x)
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: x1 <- g(x)
- en: y <- f(x1)
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: y <- f(x1)
- en: y
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: y
- en: '}'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: 'Then the chain rule states that grad(y, x) == grad(y, x1) * grad(x1, x). This
    enables you to compute the derivative of fg as long as you know the derivatives
    of f and g. The chain rule is named as it is because when you add more intermediate
    functions, it starts looking like a chain:'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 然后链式法则表明 grad(y, x) == grad(y, x1) * grad(x1, x)。只要你知道 f 和 g 的导数，就能计算出 fg 的导数。链式法则之所以这样命名是因为当你添加更多的中间函数时，它开始看起来像一个链条：
- en: fghj <- function(x) {
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: fghj <- function(x) {
- en: x1 <- j(x)
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: x1 <- j(x)
- en: x2 <- h(x1)
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: x2 <- h(x1)
- en: x3 <- g(x2)
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: x3 <- g(x2)
- en: y <- f(x3)
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: y <- f(x3)
- en: y
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: y
- en: '}'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: grad(y, x) == (grad(y, x3) * grad(x3, x2) *
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: grad(y, x) == (grad(y, x3) * grad(x3, x2) *
- en: '![Image](../images/common01.jpg) grad(x2, x1) * grad(x1, x))'
  id: totrans-534
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/common01.jpg) grad(x2, x1) * grad(x1, x))'
- en: Applying the chain rule to the computation of the gradient values of a neural
    network gives rise to an algorithm called *backpropagation*. Let’s see how that
    works, concretely.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 将链式法则应用于神经网络梯度值的计算会产生一种称为*反向传播*的算法。让我们具体了解一下它的工作原理。
- en: AUTOMATIC DIFFERENTIATION WITH COMPUTATION GRAPHS
  id: totrans-536
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用计算图的自动微分
- en: A useful way to think about backpropagation is in terms of *computation graphs*.
    A computation graph is the data structure at the heart of TensorFlow and the deep
    learning revolution in general. It’s a directed acyclic graph of operations—in
    our case, tensor operations. For instance, [figure 2.21](#fig2-21) shows the graph
    representation of our first model.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有用的思考反向传播的方式是使用*计算图*。计算图是 TensorFlow 和深度学习革命的核心数据结构。它是一个有向无环图的操作数据结构——在我们的情况下，张量操作。例如，[图2.21](#fig2-21)显示了我们第一个模型的图形表示。
- en: '![Image](../images/f0055-01.jpg)'
  id: totrans-538
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0055-01.jpg)'
- en: '**Figure 2.21 The computation graph representation of our two-layer model**'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: '**图2.21 一种两层模型的计算图表示**'
- en: 'Computation graphs have been an extremely successful abstraction in computer
    science because they enable us to *treat computation as data*: a computable expression
    is encoded as a machine-readable data structure that can be used as the input
    or output of another program. For instance, you could imagine a program that receives
    a computation graph and returns a new computation graph that implements a large-scale
    distributed version of the same computation. This would mean that you could distribute
    any computation without having to write the distribution logic yourself. Or imagine
    a program that receives a computation graph and can automatically generate the
    derivative of the expression it represents. It’s much easier to do these things
    if your computation is expressed as an explicit graph data structure rather than,
    say, lines of ASCII characters in a .R file.'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 计算图在计算机科学中是一个极其成功的抽象，因为它们使我们能够*将计算视为数据*：可计算的表达式被编码为一种机器可读的数据结构，可以用作另一个程序的输入或输出。例如，您可以想象一个接收计算图并返回实现相同计算的大规模分布式版本的新计算图的程序。这意味着您可以分发任何计算而不必自己编写分发逻辑。或者想象一个接收计算图并可以自动生成表示其代表的表达式的导数的程序。如果您的计算以明确的图形数据结构而不是比如说.R文件中的ASCII字符行表示，则执行这些操作要容易得多。
- en: 'To explain backpropagation clearly, let’s look at a really basic example of
    a computation graph (see [figure 2.22](#fig2-22)). We’ll consider a simplified
    version of [figure 2.21](#fig2-21), where we have only one linear layer and where
    all variables are scalar. We’ll take two scalar variables w and b, a scalar input
    x, and apply some operations to them to combine them into an output y. Finally,
    we’ll apply an absolute value error-loss function: loss_val = abs(y_true - y).
    Because we want to update w and b in a way that will minimize loss_val, we are
    interested in computing grad(loss_val, b) and grad(loss_val, w).'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 要清楚地解释反向传播，让我们看一个真正基本的计算图示例（见[图2.22](#fig2-22)）。我们将考虑一个简化版本的[图2.21](#fig2-21)，其中我们只有一个线性层，并且所有变量都是标量。我们将取两个标量变量w和b，一个标量输入x，并对它们进行一些操作以将它们组合成输出y。最后，我们将应用绝对值误差损失函数：loss_val
    = abs(y_true - y)。因为我们想要以最小化loss_val的方式更新w和b，所以我们对计算grad(loss_val, b)和grad(loss_val,
    w)感兴趣。
- en: '![Image](../images/f0056-01.jpg)'
  id: totrans-542
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0056-01.jpg)'
- en: '**Figure 2.22 A basic example of a computation graph**'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: '**图2.22 一个基本的计算图示例**'
- en: Let’s set concrete values for the “input nodes” in the graph, that is to say,
    the input x, the target y_true, w, and b. We’ll propagate these values to all
    nodes in the graph, from top to bottom, until we reach loss_val. This is the *forward
    pass* (see [figure 2.23](#fig2-23)).
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为图中的“输入节点”设置具体值，也就是说，输入x，目标y_true，w和b。我们将这些值传播到图中的所有节点，从上到下，直到达到loss_val。这是*正向传播*（见[图2.23](#fig2-23)）。
- en: 'Now let’s “reverse” the graph: for each edge in the graph going from A to B,
    we will create an opposite edge from B to A, and ask, how much does B vary when
    A varies? That is to say, what is grad(B, A)? We’ll annotate each inverted edge
    with this value. This backward graph represents the *backward pass* (see [figure
    2.24](#fig2-24)).'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们“反转”图表：对于图表中从A到B的每条边，我们将创建一个从B到A的相反边，并询问，当A变化时，B变化多少？也就是说，grad(B, A)是多少？我们将用这个值注释每条倒置边。这个反向图表代表*反向传播*（见[图2.24](#fig2-24)）。
- en: 'We have the following:'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有以下：
- en: grad(loss_val, x2) = 1, because as x2 varies by an amount epsilon, loss_val
    = abs (4 -x2) varies by the same amount.
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: grad(loss_val, x2) = 1，因为当x2变化一个量epsilon时，loss_val = abs (4 -x2)也变化相同的量。
- en: grad(x2, x1) = 1, because as x1 varies by an amount epsilon, x2 = x1 + b = x1
    + 1 varies by the same amount.
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: grad(x2, x1) = 1，因为当x1变化一个量epsilon时，x2 = x1 + b = x1 + 1也变化相同的量。
- en: '![Image](../images/f0056-02.jpg)'
  id: totrans-549
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0056-02.jpg)'
- en: '**Figure 2.23 Running a forward pass**'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: '**图2.23 运行正向传播**'
- en: '![Image](../images/f0057-01.jpg)'
  id: totrans-551
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0057-01.jpg)'
- en: '**Figure 2.24 Running a backward pass**'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: '**图2.24 运行反向传播**'
- en: grad(x2, b) = 1, because as b varies by an amount epsilon, x2 = x1 + b = 6 +
    b varies by the same amount.
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: grad(x2, b) = 1，因为当b变化一个量epsilon时，x2 = x1 + b = 6 + b也变化相同的量。
- en: grad(x1, w) = 2, because as w varies by an amount epsilon, x1 = x * w = 2 *
    w varies by 2 * epsilon.
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: grad(x1, w) = 2，因为当w变化一个量epsilon时，x1 = x * w = 2 * w变化为2 * epsilon。
- en: What the chain rule says about this backward graph is that you can obtain the
    derivative of a node with respect to another node by *multiplying the derivatives
    for each edge along the path linking the two nodes*, for instance, grad(loss_val,
    w) = grad(loss_val, x2) * grad(x2, x1) * grad(x1, w) (see [figure 2.25](#fig2-25)).
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这个反向图，链式法则告诉我们，可以通过*沿着连接两个节点的路径的每个边缘的导数相乘*来获得相对于另一个节点的节点的导数，例如，grad(loss_val,
    w) = grad(loss_val, x2) * grad(x2, x1) * grad(x1, w)（参见 [图2.25](#fig2-25)）。
- en: 'By applying the chain rule to our graph, we obtain what we we’re looking for:'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将链式法则应用于我们的图，我们获得了我们所要寻找的内容：
- en: grad(loss_val, w) = 1 * 1 * 2 = 2
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: grad(loss_val, w) = 1 * 1 * 2 = 2
- en: grad(loss_val, b) = 1 * 1 = 1
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: grad(loss_val, b) = 1 * 1 = 1
- en: If there are multiple paths linking the two nodes of interest, a and b, in the
    backward graph, we would obtain grad(b, a) by summing the contributions of all
    the paths.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在反向图中存在连接两个感兴趣节点 a 和 b 的多条路径，则通过求和所有路径的贡献来获得 grad(b, a)。
- en: 'And with that, you just saw backpropagation in action! Backpropagation is simply
    the application of the chain rule to a computation graph. There’s nothing more
    to it. Backpropagation starts with the final loss value and works backward from
    the top layers to the bottom layers, computing the contribution that each parameter
    had in the loss value. That’s where the name “backpropagation” comes from: we
    “back-propagate” the loss contributions of different nodes in a computation graph.'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 这样一来，你就看到了反向传播的实际应用！反向传播简单地将链式法则应用于计算图。没有更多了。反向传播从最终损失值开始，从顶层向底层逆向工作，计算每个参数在损失值中的贡献。这就是“反向传播”这个名字的由来：我们在计算图中“反向传播”不同节点的损失贡献。
- en: Nowadays people implement neural networks in modern frameworks that are capable
    of *automatic differentiation*, such as TensorFlow. Automatic differentiation
    is implemented with the kind of computation graph you’ve just seen. Automatic
    differentiation makes it possible to retrieve the gradients of arbitrary compositions
    of differentiable tensor operations without doing any extra work besides writing
    down the forward pass. When I (François) wrote my first neural networks in C in
    the 2000s, I had to write my gradients by hand. Now, thanks to modern automatic
    differentiation tools, you’ll never have to implement backpropagation yourself.
    Consider yourself lucky!
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，人们在现代框架中实现神经网络，这些框架能够进行*自动微分*，例如 TensorFlow。自动微分是使用刚刚看到的计算图实现的。自动微分使得可以检索可微张量操作的任意组合的梯度，而无需除了编写正向传递之外的任何额外工作。当我（弗朗索瓦）在
    2000 年代用 C 编写我的第一个神经网络时，我不得不手动编写我的梯度。现在，由于现代自动微分工具，你再也不必自己实现反向传播了。算你幸运！
- en: '![Image](../images/f0058-01.jpg)'
  id: totrans-562
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0058-01.jpg)'
- en: '**Figure 2.25 Path from loss_val to w in the backward graph**'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: '**图2.25 从 loss_val 到 w 的反向图路径**'
- en: THE GRADIENT TAPE IN TENSORFLOW
  id: totrans-564
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TensorFlow 中的梯度带
- en: 'The API through which you can leverage TensorFlow’s powerful automatic differentiation
    capabilities is the GradientTape(). It’s a context manager that will “record”
    the tensor operations that run inside its scope, in the form of a computation
    graph (sometimes called a “tape”). This graph can then be used to retrieve the
    gradient of any output with respect to any variable or set of variables (instances
    of the TensorFlow Variable class). A tf$Variable is a specific kind of tensor
    meant to hold mutable state—for instance, the weights of a neural network are
    always TensorFlow Variable instances:'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以利用 TensorFlow 强大的自动求导功能的 API 是 GradientTape()。它是一个上下文管理器，将记录其范围内运行的张量操作，以计算图的形式（有时称为“带子”）。然后可以使用此图检索任何输出相对于任何变量或一组变量（TensorFlow
    Variable 类的实例）的梯度。tf$Variable 是一种特定类型的张量，用于保存可变状态，例如，神经网络的权重始终是 TensorFlow Variable
    实例：
- en: library(tensorflow)
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: library(tensorflow)
- en: x <- tf$Variable(0)➊
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: x <- tf$Variable(0)➊
- en: with(tf$GradientTape() %as% tape, {➋
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: with(tf$GradientTape() %as% tape, {➋
- en: y <- 2 * x + 3➌
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: y <- 2 * x + 3➌
- en: '})➍'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: '})➍'
- en: grad_of_y_wrt_x <- tape$gradient(y, x)➎
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: grad_of_y_wrt_x <- tape$gradient(y, x)➎
- en: ➊ **Instantiate a scalar Variable with an initial value of 0.**
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **实例化一个初始值为 0 的标量变量。**
- en: ➋ **Open a GradientTape scope.**
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **打开一个 GradientTape 范围。**
- en: ➌ **Inside the scope, apply some tensor operations to our variable.**
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **在范围内，对我们的变量应用一些张量操作。**
- en: ➍ **Exit the scope.**
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **退出作用域。**
- en: ➎ **Use the tape to retrieve the gradient of the output y with respect to our
    variable x.**
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **使用带子检索输出 y 相对于我们的变量 x 的梯度。**
- en: 'The GradientTape() works with tensor operations as follows:'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: GradientTape() 与张量操作一起工作如下：
- en: x <- tf$Variable(array(0, dim = c(2, 2)))➊
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: x <- tf$Variable(array(0, dim = c(2, 2)))➊
- en: with(tf$GradientTape() %as% tape, {
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: with(tf$GradientTape() %as% tape, {
- en: y <- 2 * x + 3
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: y <- 2 * x + 3
- en: '})'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: '})'
- en: grad_of_y_wrt_x <- as.array(tape$gradient(y, x))➋
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: grad_of_y_wrt_x <- as.array(tape$gradient(y, x))➋
- en: ➊ **Instantiate a variable with shape (2, 2) and an initial value of all zeros.**
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **创建一个形状为 (2, 2) 的变量，并将其初始值设为全零。**
- en: ➋ **grad_of_y_wrt_x is a tensor of shape (2, 2) (like x) describing the curvature
    of y = 2 * a + 3 around x = array(0, dim = c(2, 2)).**
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **grad_of_y_wrt_x 是一个形状为 (2, 2)（与 x 相同）的张量，描述了 y = 2 * a + 3 在 x = array(0,
    dim = c(2, 2)) 周围的曲率。**
- en: 'Note that tape$gradient() returns a TensorFlow Tensor, which we convert to
    an R array with as.array(). GradientTape() also works with lists of variables:'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，tape$gradient() 返回一个 TensorFlow 张量，我们用 as.array() 将其转换为 R 数组。GradientTape()
    也可以用于变量列表：
- en: W <- tf$Variable(random_array(c(2, 2)))
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: W <- tf$Variable(random_array(c(2, 2)))
- en: b <- tf$Variable(array(0, dim = c(2)))
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: b <- tf$Variable(array(0, dim = c(2)))
- en: x <- random_array(c(2, 2))
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: x <- random_array(c(2, 2))
- en: with(tf$GradientTape() %as% tape, {
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: with(tf$GradientTape() %as% tape, {
- en: y <- tf$matmul(x, W) + b➊
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: y <- tf$matmul(x, W) + b➊
- en: '})'
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: '})'
- en: grad_of_y_wrt_W_and_b <- tape$gradient(y, list(W, b))
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: grad_of_y_wrt_W_and_b <- tape$gradient(y, list(W, b))
- en: str(grad_of_y_wrt_W_and_b)➋
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: str(grad_of_y_wrt_W_and_b)➋
- en: List of 2
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 从中可以学到更多有关梯度带的知识。
- en: '$ :<tf.Tensor: shape=(2, 2), dtype=float64, numpy=…>'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: '$ :<tf.Tensor: shape=(2, 2), dtype=float64, numpy=…>'
- en: '$ :<tf.Tensor: shape=(2), dtype=float64, numpy=array([2., 2.])>'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: '$ :<tf.Tensor: shape=(2), dtype=float64, numpy=array([2., 2.])>'
- en: ➊ **matmul is how you say "dot product" in TensorFlow.**
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **matmul 是 TensorFlow 中表示“点乘”的方法。**
- en: ➋ **grad_of_y_wrt_W_and_b is a list of two tensors with the same shapes as W
    and b, respectively.**
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **grad_of_y_wrt_W_and_b 是两个张量列表，分别具有与 W 和 b 相同的形状。**
- en: You will learn more about the gradient tape in the next chapter.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在下一章节学到更多关于梯度带的内容。
- en: 2.5 Looking back at our first example
  id: totrans-600
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 回顾我们的第一个示例
- en: 'You’re nearing the end of this chapter, and you should now have a general understanding
    of what’s going on behind the scenes in a neural network. What was a magical black
    box at the start of the chapter has turned into a clearer picture, as illustrated
    in [figure 2.26](#fig2-26): the model, composed of layers that are chained together,
    maps the input data to predictions. The loss function then compares these predictions
    to the targets, producing a loss value: a measure of how well the model’s predictions
    match what was expected. The optimizer uses this loss value to update the model’s
    weights.'
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 你已接近本章的结尾，现在你应该对神经网络背后的操作有了一般的理解。本章开始时的神奇黑盒已经变成了一个更清晰的图景，正如[图 2.26](#fig2-26)所示：模型由相互连接的层组成，将输入数据映射到预测结果。损失函数然后将这些预测与目标进行比较，产生一个损失值：衡量模型预测与预期值匹配程度的指标。优化器使用这个损失值来更新模型的权重。
- en: '![Image](../images/f0059-01.jpg)'
  id: totrans-602
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0059-01.jpg)'
- en: '**Figure 2.26 Relationship between the network, layers, loss function, and
    optimizer**'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.26 神经网络、层、损失函数和优化器之间的关系**'
- en: Let’s go back to the first example in this chapter and review each piece of
    it in the light of what you’ve learned since.
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到本章的第一个示例，并根据你学到的内容逐个审视它。
- en: 'This was the input data:'
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输入数据：
- en: library(keras)
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: library(keras)
- en: mnist <- dataset_mnist()
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: mnist <- dataset_mnist()
- en: train_images <- mnist$train$x
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: train_images <- mnist$train$x
- en: train_images <- array_reshape(train_images, c(60000, 28 * 28))
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: train_images <- array_reshape(train_images, c(60000, 28 * 28))
- en: train_images <- train_images / 255
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: train_images <- train_images / 255
- en: test_images <- mnist$test$x
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: test_images <- mnist$test$x
- en: test_images <- array_reshape(test_images, c(10000, 28 * 28))
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: test_images <- array_reshape(test_images, c(10000, 28 * 28))
- en: test_images <- test_images / 255
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: test_images <- test_images / 255
- en: train_labels <- mnist$train$y
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: train_labels <- mnist$train$y
- en: test_labels <- mnist$test$y
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: test_labels <- mnist$test$y
- en: Now you understand that the input images are stored in R arrays of shape (60000,
    784) (training data) and (10000, 784) (test data) respectively.
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你明白了输入图像以 R 数组的形式存储，形状分别为 (60000, 784)（训练数据）和 (10000, 784)（测试数据）。
- en: 'This was our model:'
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的模型：
- en: model <- keras_model_sequential(list(
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model_sequential(list(
- en: layer_dense(units = 512, activation = "relu"),
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(units = 512, activation = "relu"),
- en: layer_dense(units = 10, activation = "softmax")
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(units = 10, activation = "softmax")
- en: ))
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: List of 2
- en: Now you understand that this model consists of a chain of two Dense layers,
    that each layer applies a few simple tensor operations to the input data, and
    that these operations involve weight tensors. Weight tensors, which are attributes
    of the layers, are where the *knowledge* of the model persists.
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你明白了这个模型由两个 Dense 层的链组成，每个层对输入数据应用了几个简单的张量操作，这些操作涉及权重张量。权重张量是层的属性，它们是模型*知识*的存储位置。
- en: 'This was the model-compilation step:'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 这是模型编译步骤：
- en: compile(model,
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 编译(model,
- en: optimizer = "rmsprop",
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器 = "rmsprop",
- en: loss = "sparse_categorical_crossentropy",
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 损失 = "稀疏分类交叉熵",
- en: metrics = c("accuracy"))
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: 度量 = c("准确度"))
- en: Now you understand that sparse_categorical_crossentropy is the loss function
    that’s used as a feedback signal for learning the weight tensors and which the
    training phase will attempt to minimize. You also know that this reduction of
    the loss happens via mini-batch stochastic gradient descent. The exact rules governing
    a specific use of gradient descent are defined by the rmsprop optimizer passed
    as the first argument.
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你明白了，稀疏分类交叉熵是用于学习权重张量的反馈信号，训练阶段将尝试最小化这个损失。你还知道，这种损失的减少是通过小批量随机梯度下降来实现的。具体的梯度下降使用规则由传递为第一个参数的
    rmsprop 优化器定义。
- en: 'Finally, this was the training loop:'
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这是训练循环：
- en: fit(model, train_images, train_labels, epochs = 5, batch_size = 128)
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: 训练(model, train_images, train_labels, epochs = 5, batch_size = 128)
- en: 'Now you understand what happens when you call fit: the model will start to
    iterate on the training data in mini-batches of 128 samples, five times over (each
    iteration over all the training data is called an *epoch*). For each batch, the
    model will compute the gradient of the loss with regard to the weights (using
    the backpropagation algorithm, which derives from the chain rule in calculus)
    and move the weights in the direction that will reduce the value of the loss for
    this batch.'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你明白了调用 fit 时会发生什么：模型将开始以 128 个样本的迷你批次迭代训练数据，重复五次（对所有训练数据的每次迭代称为一个*epoch*）。对于每个批次，模型将计算损失相对于权重的梯度（使用反向传播算法，这来源于微积分中的链式法则），并使权重沿着能够减少该批次损失的方向移动。
- en: After these five epochs, the model will have performed 2,345 gradient updates
    (469 per epoch), and the loss of the model will be sufficiently low that the model
    will be capable of classifying handwritten digits with high accuracy.
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 在这五个 epochs 之后，模型将进行了 2,345 次梯度更新（每个 epoch 469 次），并且模型的损失将足够低，使得模型能够以高精度对手写数字进行分类。
- en: At this point, you already know most of what there is to know about neural networks.
    Let’s prove it by reimplementing a simplified version of that first example “from
    scratch” in TensorFlow, step by step.
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，你已经知道了关于神经网络的所有知识。让我们通过在 TensorFlow 中一步步从头重新实现第一个示例的简化版本来证明这一点。
- en: 2.5.1 Reimplementing our first example from scratch in TensorFlow
  id: totrans-634
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.1 在 TensorFlow 中从头重新实现我们的第一个示例
- en: 'What better demonstrates full, unambiguous understanding than implementing
    everything from scratch? Of course, what “from scratch” means here is relative:
    we won’t reimplement basic tensor operations, and we won’t implement backpropagation.
    But we’ll go to such a low level that we will barely use any Keras functionality
    at all.'
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 还有什么比从头开始实现一切更能充分、明确地展示理解力呢？当然，这里的“从头”是相对的：我们不会重新实现基本张量操作，也不会实现反向传播。但我们会深入到如此低的层次，以至于几乎不会使用任何
    Keras 功能。
- en: Don’t worry if you don’t understand every little detail in this example just
    yet. The next chapter will dive in more detail into the TensorFlow API. For now,
    just try to follow the gist of what’s going on—the intent of this example is to
    help crystallize your understanding of the mathematics of deep learning using
    a concrete implementation. Let’s go!
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: 不用担心如果你还没有完全理解这个例子的每一个细节。下一章将更深入地探讨 TensorFlow API。目前，只需尝试跟随正在发生的事情的要点即可——本例子的目的是通过具体的实现帮助你明确深度学习的数学原理。让我们开始吧！
- en: A SIMPLE DENSE CLASS
  id: totrans-637
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个简单的 DENSE 类
- en: 'You’ve learned earlier that the Dense layer implements the following input
    transformation, where W and b are model parameters, and activation() is an element-wise
    function (usually relu(), but it would be softmax() for the last layer):'
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 你之前学到过 Dense 层实现了如下输入变换，其中 W 和 b 是模型参数，激活函数()是一个逐元素函数（通常是 relu()，但在最后一层会是 softmax()）：
- en: output <- activation(dot(W, input) + b)
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: output <- 激活(dot(W, 输入) + b)
- en: 'Let’s implement a simple Dense layer as a plain R environment with a class
    attribute NaiveDense, two TensorFlow variables, W and b, and a call() method that
    applies the preceding transformation:'
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在 R 环境中实现一个简单的 Dense 层，带有 NaiveDense 类属性、两个 TensorFlow 变量 W 和 b，以及一个应用前述变换的
    call() 方法：
- en: layer_naive_dense <- function(input_size, output_size, activation) {
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: layer_naive_dense <- function(input_size, output_size, activation) {
- en: self <- new.env(parent = emptyenv())
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: self <- new.env(parent = emptyenv())
- en: attr(self, "class") <- "NaiveDense"
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: attr(self, "class") <- "NaiveDense"
- en: self$activation <- activation
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: self$activation <- activation
- en: w_shape <- c(input_size, output_size)
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: w_shape <- c(input_size, output_size)
- en: w_initial_value <- random_array(w_shape, min = 0, max = 1e-1)
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: w_initial_value <- random_array(w_shape, min = 0, max = 1e-1)
- en: self$W <- tf$Variable(w_initial_value)➊
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: self$W <- tf$Variable(w_initial_value)➊
- en: b_shape <- c(output_size)
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: b_shape <- c(output_size)
- en: b_initial_value <- array(0, b_shape)
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: b_initial_value <- array(0, b_shape)
- en: self$b <- tf$Variable(b_initial_value)➋
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: self$b <- tf$Variable(b_initial_value)➋
- en: self$weights <- list(self$W, self$b)➌
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: self$weights <- list(self$W, self$b)➌
- en: self$call <- function(inputs) {➍
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: self$call <- function(inputs) {➍
- en: self$activation(tf$matmul(inputs, self$W) + self$b)➎
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: self$activation(tf$matmul(inputs, self$W) + self$b)➎
- en: '}'
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: self
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: self
- en: '}'
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ **Create a matrix, W, of shape (input_size, output_size), initialized with
    random values.**
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **创建一个形状为(input_size, output_size)的矩阵W，用随机值进行初始化。**
- en: ➋ **Create a vector, b, of shape (output_size), initialized with zeros.**
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **创建一个形状为(output_size)的向量b，用零进行初始化。**
- en: ➌ **Convenience property for retrieving all the layer's weights**
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **方便地检索所有层的权重的属性。**
- en: ➍ **Apply the forward pass in a function named call.**
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **在一个名为call的函数中应用前向传播。**
- en: ➎ **We stick to TensorFlow operations in this function, so that GradientTape
    can track them. (We'll learn more about TensorFlow operations in chapter 3.)**
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **在这个函数中我们坚持使用TensorFlow操作，以便GradientTape可以追踪它们。（我们将在第3章学习更多关于TensorFlow操作的知识。）**
- en: A SIMPLE SEQUENTIAL CLASS
  id: totrans-662
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个简单的顺序类
- en: 'Now, let’s create a naive_model_sequential() to chain these layers, as shown
    in the next code snippet. It wraps a list of layers and exposes a call() method
    that simply calls the underlying layers on the inputs, in order. It also features
    a weights property to easily keep track of the layers’ parameters:'
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个naive_model_sequential()来链接这些层，如下一个代码片段所示。它包装了一个层的列表，并公开了一个call()方法，该方法简单地按顺序在输入上调用底层层。它还具有一个weights属性，用于轻松跟踪层的参数：
- en: naive_model_sequential <- function(layers) {
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: naive_model_sequential <- function(layers) {
- en: self <- new.env(parent = emptyenv())
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: self <- new.env(parent = emptyenv())
- en: attr(self, "class") <- "NaiveSequential"
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: attr(self, "class") <- "NaiveSequential"
- en: self$layers <- layers
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: self$layers <- layers
- en: weights <- lapply(layers, function(layer) layer$weights)
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: weights <- lapply(layers, function(layer) layer$weights)
- en: self$weights <- do.call(c, weights)➊
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: self$weights <- do.call(c, weights)➊
- en: self$call <- function(inputs) {
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: self$call <- function(inputs) {
- en: x <- inputs
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: x <- inputs
- en: for (layer in self$layers)
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: for (layer in self$layers)
- en: x <- layer$call(x)
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: x <- layer$call(x)
- en: x
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: x
- en: '}'
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: self
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: self
- en: '}'
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ **Flatten the nested list one level.**
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **展开嵌套列表一层。**
- en: 'Using this NaiveDense class and this NaiveSequential class, we can create a
    mock Keras model:'
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个NaiveDense类和这个NaiveSequential类，我们可以创建一个模拟Keras模型：
- en: model <- naive_model_sequential(list(
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: model <- naive_model_sequential(list(
- en: layer_naive_dense(input_size = 28 * 28, output_size = 512,
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: layer_naive_dense(input_size = 28 * 28, output_size = 512,
- en: activation = tf$nn$relu),
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: activation = tf$nn$relu),
- en: layer_naive_dense(input_size = 512, output_size = 10,
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: layer_naive_dense(input_size = 512, output_size = 10,
- en: activation = tf$nn$softmax)
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: activation = tf$nn$softmax)
- en: ))
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: ))
- en: stopifnot(length(model$weights) == 4)
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: stopifnot(length(model$weights) == 4)
- en: A BATCH GENERATOR
  id: totrans-687
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个批量生成器
- en: 'Next, we need a way to iterate over the MNIST data in mini-batches. This is
    easy:'
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要一种方法来在小批量上迭代MNIST数据。这很容易：
- en: new_batch_generator <- function(images, labels, batch_size = 128) {
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: new_batch_generator <- function(images, labels, batch_size = 128) {
- en: self <- new.env(parent = emptyenv())
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: self <- new.env(parent = emptyenv())
- en: attr(self, "class") <- "BatchGenerator"
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: attr(self, "class") <- "BatchGenerator"
- en: stopifnot(nrow(images) == nrow(labels))
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: stopifnot(nrow(images) == nrow(labels))
- en: self$index <- 1
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: self$index <- 1
- en: self$images <- images
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: self$images <- images
- en: self$labels <- labels self$batch_size <- batch_size
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: self$labels <- labels self$batch_size <- batch_size
- en: self$num_batches <- ceiling(nrow(images) / batch_size)
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: self$num_batches <- ceiling(nrow(images) / batch_size)
- en: self$get_next_batch <- function() {
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: self$get_next_batch <- function() {
- en: start <- self$index
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: start <- self$index
- en: if(start > nrow(images))
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: if(start > nrow(images))
- en: return(NULL)➊
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: return(NULL)➊
- en: end <- start + self$batch_size - 1
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: end <- start + self$batch_size - 1
- en: if(end > nrow(images))
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: if(end > nrow(images))
- en: end <- nrow(images)➋
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: end <- nrow(images)➋
- en: self$index <- end + 1
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: self$index <- end + 1
- en: indices <- start:end
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: indices <- start:end
- en: list(images = self$images[indices, ],
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: list(images = self$images[indices, ],
- en: labels = self$labels[indices])
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: labels = self$labels[indices])
- en: '}'
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: self
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: self
- en: '}'
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ **Generator is finished.**
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **生成器已完成。**
- en: ➋ **Last batch may be smaller.**
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **最后一批可能会较小。**
- en: 2.5.2 Running one training step
  id: totrans-713
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.2 运行一个训练步骤
- en: 'The most difficult part of the process is the “training step”: updating the
    weights of the model after running it on one batch of data. We need to do the
    following:'
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程中最困难的部分是“训练步骤”：在一批数据上运行模型后更新模型的权重。我们需要做以下工作：
- en: '**1** Compute the predictions of the model for the images in the batch.'
  id: totrans-715
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**1** 计算批量图像的模型预测。'
- en: '**2** Compute the loss value for these predictions, given the actual labels.'
  id: totrans-716
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**2** 计算这些预测的损失值，给定实际标签。'
- en: '**3** Compute the gradient of the loss with regard to the model’s weights.'
  id: totrans-717
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**3** 计算损失相对于模型权重的梯度。'
- en: '**4** Move the weights by a small amount in the direction opposite to the gradient.'
  id: totrans-718
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**4** 将权重朝着梯度相反的方向移动一个小量。'
- en: 'To compute the gradient, we will use the TensorFlow GradientTape object we
    introduced in section 2.4.4:'
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算梯度，我们将使用我们在第 2.4.4 节中介绍的 TensorFlow GradientTape 对象：
- en: one_training_step <- function(model, images_batch, labels_batch) {
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: one_training_step <- function(model, images_batch, labels_batch) {
- en: with(tf$GradientTape() %as% tape, {
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: with(tf$GradientTape() %as% tape, {
- en: predictions <- model$call(images_batch)➊
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: predictions <- model$call(images_batch)➊
- en: per_sample_losses <
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: 每个样本的损失 <
- en: loss_sparse_categorical_crossentropy(labels_batch, predictions)
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: loss_sparse_categorical_crossentropy(labels_batch, predictions)
- en: average_loss <- mean(per_sample_losses)
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: average_loss <- mean(per_sample_losses)
- en: '})'
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: '})'
- en: gradients <- tape$gradient(average_loss, model$weights)➋
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: gradients <- tape$gradient(average_loss, model$weights)➋
- en: update_weights(gradients, model$weights)➌
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: update_weights(gradients, model$weights)➌
- en: average_loss
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: average_loss
- en: '}'
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ **Run the forward pass (compute the model's predictions under a GradientTape
    scope).**
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **运行前向传播（在 GradientTape 范围内计算模型的预测）。**
- en: ➋ **Compute the gradient of the loss with regard to the weights. The output
    gradients is a list where each entry corresponds to a weight from the model$weights
    list.**
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **计算损失相对于权重的梯度。输出梯度是一个列表，其中每个条目对应于模型$weights 列表中的一个权重。**
- en: ➌ **Update the weights using the gradients (we will define this function shortly).**
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **使用梯度更新权重（我们将很快定义此函数）。**
- en: 'As you already know, the purpose of the “weight update” step (represented by
    the preceding update_weights() function) is to move the weights by “a bit” in
    a direction that will reduce the loss on this batch. The magnitude of the move
    is determined by the “learning rate,” typically a small quantity. The simplest
    way to implement this update_ weights() function is to subtract gradient * learning_rate
    from each weight:'
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你已经知道的，"weight update" 步骤的目的（由前面的 update_weights() 函数表示）是在一个方向上将权重移动“一点”，以减少这个批次上的损失。移动的大小由“学习率”确定，通常是一个小量。实现这个
    update_weights() 函数的最简单方法是从每个权重中减去梯度 * 学习率：
- en: learning_rate <- 1e-3
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率 <- 1e-3
- en: update_weights <- function(gradients, weights) {
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: update_weights <- function(gradients, weights) {
- en: stopifnot(length(gradients) == length(weights))
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: stopifnot(length(gradients) == length(weights))
- en: for (i in seq_along(weights))
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: for (i in seq_along(weights))
- en: weights[[i]]$assign_sub(➊
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: weights[[i]]$assign_sub(➊
- en: gradients[[i]] * learning_rate)
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: gradients[[i]] * learning_rate)
- en: '}'
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ **x$assign_sub(value) is the equivalent of x <- x - value for TensorFlow variables.**
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **x$assign_sub(value) 相当于 x <- x - value 对于 TensorFlow 变量。**
- en: 'In practice, you would almost never implement a weight update step like this
    by hand. Instead, you would use an Optimizer instance from Keras:'
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，你几乎不会手动实现像这样的权重更新步骤。而是会使用来自 Keras 的 Optimizer 实例：
- en: optimizer <- optimizer_sgd(learning_rate = 1e-3)
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
  zh: optimizer <- optimizer_sgd(learning_rate = 1e-3)
- en: update_weights <- function(gradients, weights)
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: update_weights <- function(gradients, weights)
- en: optimizer$apply_gradients(zip_lists(gradients, weights))
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: optimizer$apply_gradients(zip_lists(gradients, weights))
- en: 'zip_lists() is a helper function that we use to turn the lists of gradients
    and weights into a list of (gradient, weight) pairs. We use it to pair gradients
    with weights for the optimizer. For example:'
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
  zh: zip_lists() 是一个辅助函数，用于将梯度和权重列表转换为 (gradient, weight) 对的列表。我们用它将梯度与权重配对给优化器。例如：
- en: str(zip_lists(
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: str(zip_lists(
- en: gradients = list("grad_for_wt_1", "grad_for_wt_2", "grad_for_wt_3"),
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
  zh: gradients = list("grad_for_wt_1", "grad_for_wt_2", "grad_for_wt_3"),
- en: weights = list("weight_1", "weight_2", "weight_3")))
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: 权重 = list("weight_1", "weight_2", "weight_3")))
- en: List of 3
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: List of 3
- en: $ :List of 2
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: $ :List of 2
- en: '..$ gradients: chr "grad_for_wt_1"'
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
  zh: '..$ gradients: chr "grad_for_wt_1"'
- en: '..$ weights : chr "weight_1"'
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: '..$ weights : chr "weight_1"'
- en: $ :List of 2
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
  zh: $ :List of 2
- en: '..$ gradients: chr "grad_for_wt_2"'
  id: totrans-756
  prefs: []
  type: TYPE_NORMAL
  zh: '..$ gradients: chr "grad_for_wt_2"'
- en: '..$ weights : chr "weight_2"'
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: '..$ weights : chr "weight_2"'
- en: $ :List of 2
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: $ :List of 2
- en: '..$ gradients: chr "grad_for_wt_3"'
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: '..$ 梯度: chr "grad_for_wt_3"'
- en: '..$ weights : chr "weight_3"'
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: '..$ weights : chr "weight_3"'
- en: Now that our per-batch training step is ready, we can move on to implementing
    an entire epoch of training.
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的每批训练步骤已经准备好了，我们可以继续实现整个训练的一个 epoch。
- en: 2.5.3 The full training loop
  id: totrans-762
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.3 完整的训练循环
- en: 'An epoch of training simply consists of repeating the training step for each
    batch in the training data, and the full training loop is simply the repetition
    of one epoch:'
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: 训练的一个 epoch 简单地是重复训练数据中每个批次的训练步骤，而完整的训练循环简单地是一个 epoch 的重复：
- en: fit <- function(model, images, labels, epochs, batch_size = 128) {
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
  zh: fit <- function(model, images, labels, epochs, batch_size = 128) {
- en: for (epoch_counter in seq_len(epochs)) {
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: for (epoch_counter in seq_len(epochs)) {
- en: cat("Epoch ", epoch_counter, "\n")
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: cat("Epoch ", epoch_counter, "\n")
- en: batch_generator <- new_batch_generator(images, labels)
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: batch_generator <- new_batch_generator(images, labels)
- en: for (batch_counter in seq_len(batch_generator$num_batches)) {
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
  zh: for (batch_counter in seq_len(batch_generator$num_batches)) {
- en: batch <- batch_generator$get_next_batch()
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: batch <- batch_generator$get_next_batch()
- en: loss <- one_training_step(model, batch$images, batch$labels)
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
  zh: loss <- one_training_step(model, batch$images, batch$labels)
- en: if (batch_counter %% 100 == 0)
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: if (batch_counter %% 100 == 0)
- en: 'cat(sprintf("loss at batch %s: %.2f\n", batch_counter, loss))'
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
  zh: cat(sprintf("第 %s 批次的损失：%.2f\n", batch_counter, loss))
- en: '}'
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: 'Let’s test-drive it:'
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来试一试：
- en: mnist <- dataset_mnist()
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: mnist <- dataset_mnist()
- en: train_images <- array_reshape(mnist$train$x, c(60000, 28 * 28)) / 255
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: train_images <- array_reshape(mnist$train$x, c(60000, 28 * 28)) / 255
- en: test_images <- array_reshape(mnist$test$x, c(10000, 28 * 28)) / 255
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
  zh: test_images <- array_reshape(mnist$test$x, c(10000, 28 * 28)) / 255
- en: test_labels <- mnist$test$y
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: test_labels <- mnist$test$y
- en: train_labels <- mnist$train$y
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
  zh: train_labels <- mnist$train$y
- en: fit(model, train_images, train_labels, epochs = 10, batch_size = 128)
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: fit(model, train_images, train_labels, epochs = 10, batch_size = 128)
- en: Epoch 1
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: 第 1 轮
- en: 'loss at batch 100: 2.37'
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
  zh: 'batch 100: 2.37时的损失'
- en: 'loss at batch 200: 2.21'
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: 'batch 200: 2.21时的损失'
- en: 'loss at batch 300: 2.15'
  id: totrans-786
  prefs: []
  type: TYPE_NORMAL
  zh: 'batch 300: 2.15时的损失'
- en: 'loss at batch 400: 2.09'
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
  zh: 'batch 400: 2.09时的损失'
- en: Epoch 2
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
  zh: 第 2 轮
- en: 'loss at batch 100: 1.98'
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: 'batch 100: 1.98时的损失'
- en: 'loss at batch 200: 1.83'
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
  zh: 'batch 200: 1.83时的损失'
- en: 'loss at batch 300: 1.83'
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
  zh: 'batch 300: 1.83时的损失'
- en: 'loss at batch 400: 1.75'
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: 'batch 400: 1.75时的损失'
- en: …
  id: totrans-793
  prefs: []
  type: TYPE_NORMAL
  zh: …
- en: Epoch 9
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
  zh: 第 9 轮
- en: 'loss at batch 100: 0.85'
  id: totrans-795
  prefs: []
  type: TYPE_NORMAL
  zh: 'batch 100: 0.85时的损失'
- en: 'loss at batch 200: 0.68'
  id: totrans-796
  prefs: []
  type: TYPE_NORMAL
  zh: 'batch 200: 0.68时的损失'
- en: 'loss at batch 300: 0.83'
  id: totrans-797
  prefs: []
  type: TYPE_NORMAL
  zh: 'batch 300: 0.83时的损失'
- en: 'loss at batch 400: 0.76'
  id: totrans-798
  prefs: []
  type: TYPE_NORMAL
  zh: 'batch 400: 0.76时的损失'
- en: Epoch 10
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
  zh: 第 10 轮
- en: 'loss at batch 100: 0.80'
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
  zh: 'batch 100: 0.80时的损失'
- en: 'loss at batch 200: 0.63'
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
  zh: 'batch 200: 0.63时的损失'
- en: 'loss at batch 300: 0.78'
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
  zh: 'batch 300: 0.78时的损失'
- en: 'loss at batch 400: 0.72'
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
  zh: 'batch 400: 0.72时的损失'
- en: 2.5.4 Evaluating the model
  id: totrans-804
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.4 评估模型
- en: 'We can evaluate the model by taking the max.col() of its predictions over the
    test images, and comparing it to the expected labels:'
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过对测试图像的预测取其最大值所在列（max.col()），然后与预期标签进行比较来评估模型：
- en: predictions <- model$call(test_images)
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
  zh: predictions <- model$call(test_images)
- en: predictions <- as.array(predictions)➊
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
  zh: predictions <- as.array(predictions)➊
- en: predicted_labels <- max.col(predictions) - 1➋ ➌
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
  zh: predicted_labels <- max.col(predictions) - 1➋ ➌
- en: matches <- predicted_labels == test_labels
  id: totrans-809
  prefs: []
  type: TYPE_NORMAL
  zh: matches <- predicted_labels == test_labels
- en: 'cat(sprintf("accuracy: %.2f\n", mean(matches)))'
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
  zh: cat(sprintf("准确率：%.2f\n", mean(matches)))
- en: 'accuracy: 0.82'
  id: totrans-811
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率：0.82
- en: ➊ **Convert the TensorFlow Tensor to an R array.**
  id: totrans-812
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **将 TensorFlow 张量转换为 R 数组。**
- en: ➋ **max.col(x) is a vectorized implementation of apply(x, 1, which.max)).**
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **max.col(x) 是对 apply(x, 1, which.max) 的矢量化实现。**
- en: ➌ **Subtract 1 because positions are offset from labels by 1, for example, the
    first position corresponds to digit 0.**
  id: totrans-814
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **减 1 是因为位置与标签偏移 1，例如，第一个位置对应数字 0。**
- en: All done! As you can see, it’s quite a bit of work to do “by hand” what you
    can do in a few lines of Keras code. But because you’ve gone through these steps,
    you should now have a crystal-clear understanding of what goes on inside a neural
    network when you call fit(). Having this low-level mental model of what your code
    is doing behind the scenes will make you better able to leverage the high-level
    features of the Keras API.
  id: totrans-815
  prefs: []
  type: TYPE_NORMAL
  zh: 完成啦！正如你所见，手工完成“几行 Keras 代码所能完成的事情”是相当多的工作。但是因为你已经经历了这些步骤，现在应该对在调用 fit() 时神经网络内部发生的情况有一个清晰的理解。拥有这种低级别的心理模型，知道代码在幕后正在做什么，将使你更能够利用
    Keras API 的高级特性。
- en: Summary
  id: totrans-816
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概要
- en: Tensors form the foundation of modern machine learning systems. They come in
    various flavors of type, rank, and shape.
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张量构成现代机器学习系统的基础。它们具有各种类型、秩和形状的变体。
- en: You can manipulate numerical tensors via tensor operations (such as addition,
    tensor product, or element-wise multiplication), which can be interpreted as encoding
    geometric transformations. In general, everything in deep learning is amenable
    to a geometric interpretation.
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以通过张量操作（如加法、张量乘积或逐元素乘法）来操作数值张量，这可以解释为编码几何变换。一般来说，深度学习中的一切都可以被解释为几何的。
- en: Deep learning models consist of chains of simple tensor operations, parameterized
    by weights, which are themselves tensors. The weights of a model are where its
    “knowledge” is stored.
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习模型由简单的张量操作链组成，由权重参数化，这些权重本身也是张量。模型的权重是存储其“知识”的地方。
- en: Learning means finding a set of values for the model’s weights that minimizes
    a loss function for a given set of training data samples and their corresponding
    targets.
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习意味着找到模型权重的一组数值，以最小化给定一组训练数据样本及其对应目标的损失函数。
- en: Learning happens by drawing random batches of data samples and their targets
    and computing the gradient of the model parameters with respect to the loss on
    the batch. The model parameters are then moved a bit (the magnitude of the move
    is defined by the learning rate) in the opposite direction from the gradient.
    This is called mini-batch stochastic gradient descent.
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习通过绘制随机的数据样本批次及其目标，并计算模型参数相对于批次上的损失的梯度来进行。然后，模型参数按梯度相反的方向移动一点（移动的大小由学习率定义）。这称为小批量随机梯度下降。
- en: The entire learning process is made possible by the fact that all tensor operations
    in neural networks are differentiable, and thus it’s possible to apply the chain
    rule of derivation to find the gradient function mapping the current parameters
    and current batch of data to a gradient value. This is called back-propagation.
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整个学习过程之所以成为可能，是因为神经网络中的所有张量操作都是可微分的，因此可以应用求导的链式法则来找到将当前参数和当前数据批次映射到梯度值的梯度函数。这称为反向传播。
- en: 'Two key concepts you’ll see frequently in future chapters are *loss* and *optimizer*.
    These are the two things you need to define before you begin feeding data into
    a model:'
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未来的章节中，你将经常看到的两个关键概念是*损失*和*优化器*。在开始向模型输入数据之前，这两个概念是你需要定义的事物：
- en: The *loss* is the quantity you’ll attempt to minimize during training, so it
    should represent a measure of success for the task you’re trying to solve.
  id: totrans-824
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*损失*是你在训练过程中尝试最小化的量，因此它应该代表你试图解决的任务的成功度量。'
- en: 'The *optimizer* specifies the exact way in which the gradient of the loss will
    be used to update parameters: for instance, it could be the RMSprop optimizer,
    SGD with momentum, and so on.'
  id: totrans-825
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*优化器*指定了损失的梯度将如何用于更新参数的具体方式：例如，可以是RMSprop优化器、具有动量的随机梯度下降（SGD）等。'
