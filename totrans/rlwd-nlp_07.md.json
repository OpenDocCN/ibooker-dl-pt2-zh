["```py\ndef rnn_vec(words):\n    state = init_state()\n    for word in words:\n        state = update(state, word)\n    return state\n```", "```py\ndef rnn_seq(words):\n    state = init_state()\n    states = []\n    for word in words:\n        state = update(state, word)\n        states.append(state)\n    return states\n```", "```py\nstate = init_state()\nstates = []\nstate = update(state, v(\"time\"))\nstates.append(state)\nstate = update(state, v(\"flies\"))\nstates.append(state)\nstate = update(state, v(\"like\"))\nstates.append(state)\nstate = update(state, v(\"an\"))\nstates.append(state)\nstate = update(state, v(\"arrow\"))\nstates.append(state)\nstate = update(state, v(\".\"))\nstates.append(state)\n```", "```py\nencoder = PytorchSeq2VecWrapper(\n    torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\n```", "```py\nencoder = PytorchSeq2SeqWrapper(\n    torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\n```", "```py\nfrom itertools import chain\nfrom typing import Dict\n\nimport numpy as np\nimport torch\nimport torch.optim as optim\n\nfrom allennlp.data.data_loaders import MultiProcessDataLoader\nfrom allennlp.data.samplers import BucketBatchSampler\nfrom allennlp.data.vocabulary import Vocabulary\nfrom allennlp.models import Model\nfrom allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, PytorchSeq2SeqWrapper\nfrom allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\nfrom allennlp.modules.token_embedders import Embedding\nfrom allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\nfrom allennlp.training.metrics import CategoricalAccuracy\nfrom allennlp.training import GradientDescentTrainer\nfrom allennlp_models.structured_prediction.dataset_readers.universal_dependencies import UniversalDependenciesDatasetReader\n\nfrom realworldnlp.predictors import UniversalPOSPredictor\n```", "```py\nreader = UniversalDependenciesDatasetReader()\ntrain_path = ('https:/./s3.amazonaws.com/realworldnlpbook/data/'\n              'ud-treebanks-v2.3/UD_English-EWT/en_ewt-ud-train.conllu')\ndev_path = ('https:/./s3.amazonaws.com/realworldnlpbook/'\n            'data/ud-treebanks-v2.3/UD_English-EWT/en_ewt-ud-dev.conllu')\n```", "```py\nsampler = BucketBatchSampler(batch_size=32, sorting_keys=[\"words\"])\ntrain_data_loader = MultiProcessDataLoader(\n    reader, train_path, batch_sampler=sampler)\ndev_data_loader = MultiProcessDataLoader(\n    reader, dev_path, batch_sampler=sampler)\n\nvocab = Vocabulary.from_instances(chain(train_data_loader.iter_instances(),\n                                        dev_data_loader.iter_instances()))\ntrain_data_loader.index_with(vocab)\ndev_data_loader.index_with(vocab)\n```", "```py\nEMBEDDING_SIZE = 128\nHIDDEN_SIZE = 128\n\ntoken_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n                            embedding_dim=EMBEDDING_SIZE)\nword_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n\nlstm = PytorchSeq2SeqWrapper(\n    torch.nn.LSTM(EMBEDDING_SIZE, HIDDEN_SIZE, batch_first=True))\n```", "```py\nclass LstmTagger(Model):\n    def __init__(self,\n                 embedder: TextFieldEmbedder,\n                 encoder: Seq2SeqEncoder,\n                 vocab: Vocabulary) -> None:\n        super().__init__(vocab)\n        self.embedder = embedder\n        self.encoder = encoder\n\n        self.linear = torch.nn.Linear(\n            in_features=encoder.get_output_dim(),\n            out_features=vocab.get_vocab_size('pos'))\n\n        self.accuracy = CategoricalAccuracy()                    ❶\n\n    def forward(self,\n                words: Dict[str, torch.Tensor],\n                pos_tags: torch.Tensor = None,\n                **args) -> Dict[str, torch.Tensor]:              ❷\n        mask = get_text_field_mask(words)\n\n        embeddings = self.embedder(words)\n        encoder_out = self.encoder(embeddings, mask)\n        tag_logits = self.linear(encoder_out)\n\n        output = {\"tag_logits\": tag_logits}\n        if pos_tags is not None:\n            self.accuracy(tag_logits, pos_tags, mask)\n            output[\"loss\"] = sequence_cross_entropy_with_logits(\n                tag_logits, pos_tags, mask)                      ❸\n\n        return output\n\n    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n        return {\"accuracy\": self.accuracy.get_metric(reset)}\n```", "```py\nmodel = LstmTagger(word_embeddings, encoder, vocab)\n\noptimizer = optim.Adam(model.parameters())\n\ntrainer = GradientDescentTrainer(\n    model=model,\n    optimizer=optimizer,\n    data_loader=train_data_loader,\n    validation_data_loader=dev_data_loader,\n    patience=10,\n    num_epochs=10,\n    cuda_device=-1)\n\ntrainer.train()\n```", "```py\npredictor = UniversalPOSPredictor(model, reader)\ntokens = ['The', 'dog', 'ate', 'the', 'apple', '.']\nlogits = predictor.predict(tokens)['tag_logits']\ntag_ids = np.argmax(logits, axis=-1)\n\nprint([vocab.get_token_from_index(tag_id, 'pos') for tag_id in tag_ids])\n```", "```py\nencoder = PytorchSeq2SeqWrapper(\n    torch.nn.LSTM(\n        EMBEDDING_SIZE, HIDDEN_SIZE, num_layers=2, batch_first=True))\n```", "```py\nencoder = PytorchSeq2SeqWrapper(\n    torch.nn.LSTM(\n        EMBEDDING_SIZE, HIDDEN_SIZE, bidirectional=True, batch_first=True))\n```", "```py\nreader = NERDatasetReader('https:/./s3.amazonaws.com/realworldnlpbook/'\n                          'data/entity-annotated-corpus/ner_dataset.csv')\n```", "```py\nsampler = BucketBatchSampler(batch_size=16, sorting_keys=[\"tokens\"])\ntrain_data_loader = MultiProcessDataLoader(\n    reader, 'train', batch_sampler=sampler)\ndev_data_loader = MultiProcessDataLoader(\n    reader, 'dev', batch_sampler=sampler)\n```", "```py\nself.f1 = SpanBasedF1Measure(vocab, tag_namespace='labels')\n```", "```py\ndef get_metrics(self, reset: bool = False) -> Dict[str, float]:\n    f1_metrics = self.f1.get_metric(reset)\n    return {'accuracy': self.accuracy.get_metric(reset),\n            'prec': f1_metrics['precision-overall'],\n            'rec': f1_metrics['recall-overall'],\n            'f1': f1_metrics['f1-measure-overall']}\n```", "```py\ntokens = ['Apple', 'is', 'looking', 'to', 'buy', 'UK', 'startup',\n          'for', '$1', 'billion', '.']\nlabels = predict(tokens, model)\nprint(' '.join('{}/{}'.format(token, label)\n               for token, label in zip(tokens, labels)))\n```", "```py\nApple/B-org is/O looking/O to/O buy/O UK/O startup/O for/O $1/O billion/O ./O\n```", "```py\nfrom allennlp.data.tokenizers import CharacterTokenizer\n\ntokenizer = CharacterTokenizer()\ntokens = tokenizer.tokenize(text)\n```", "```py\nfrom allennlp.common.util import START_SYMBOL, END_SYMBOL\n\ntokens.insert(0, Token(START_SYMBOL))\ntokens.append(Token(END_SYMBOL))\n```", "```py\nfrom allennlp.data.fields import TextField\nfrom allennlp.data.instance import Instance\n\ninput_field = TextField(tokens[:-1], token_indexers)\noutput_field = TextField(tokens[1:], token_indexers)\ninstance = Instance({'input_tokens': input_field,\n                     'output_tokens': output_field})\n```", "```py\nfrom allennlp.data.token_indexers import TokenIndexer \n\ntoken_indexers = {'tokens': SingleIdTokenIndexer()}\n```", "```py\npredict('The trip to the beach was ruined by bad weather.', model)\n{'loss': 1.3882852}\n\npredict('The trip to the beach was ruined by bad dogs.', model)\n{'loss': 1.5099115}\n\npredict('by weather was trip my bad beach the ruined to.', model)\n{'loss': 1.8084583}\n```", "```py\ndef generate():\n    state = init_state()\n    token = <START>\n    tokens = [<START>]\n    while token != <END>:\n        state = update(state, token)\n        probs = softmax(linear(state))\n        token = sample(probs)\n        tokens.append(token)\n    return tokens\n```", "```py\nYou can say that you don't know it, and why decided of yourself.\nPike of your value is to talk of hubies.\nThe meeting despoit from a police?\nThat's a problem, but us?\nThe sky as going to send nire into better.\nWe'll be look of the best ever studented.\nThere's you seen anything every's redusention day.\nHow a fail is to go there.\nIt sad not distaples with money.\nWhat you see him go as famous to eat!\n```"]