- en: 6 Using information theory with entropy-based policies
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用基于熵的策略和信息论的知识
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容包括
- en: Entropy as an information-theoretic measure of uncertainty
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为衡量不确定性的信息论量度的熵
- en: Information gain as a method of reducing entropy
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过信息增益减少熵的方法
- en: BayesOpt policies that use information theory for their search
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用信息理论进行搜索的 BayesOpt 策略
- en: We saw in chapter 4 that by aiming to improve from the best value achieved so
    far, we can design improvement-based BayesOpt policies, such as Probability of
    Improvement (POI) and Expected Improvement (EI). In chapter 5, we used multi-armed
    bandit (MAB) policies to obtain Upper Confidence Bound (UCB) and Thompson sampling
    (TS), each of which uses a unique heuristic to balance exploration and exploitation
    in the search for the global optimum of the objective function.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第 4 章中看到，通过力争从迄今为止取得的最佳值改进，我们可以设计基于改进的 BayesOpt 策略，如改进概率 (POI) 和期望改进 (EI)。在第
    5 章中，我们使用多臂老虎机 (MAB) 策略获得了上限置信度 (UCB) 和汤普森抽样 (TS)，每种策略都使用独特的启发式方法来平衡在搜索目标函数全局最优解时的探索和开发。
- en: In this chapter, we learn about another heuristic to decision-making, this time
    using information theory to design BayesOpt policies we can use in our optimization
    pipeline. Unlike the heuristics we have seen (seeking improvement, optimism in
    the face of uncertainty, and random sampling), which might seem unique to optimization-related
    tasks, information theory is a major subfield of mathematics that has applications
    in a wide range of topics. As we discuss in this chapter, by appealing to information
    theory or, more specifically, *entropy*, a quantity that measures uncertainty
    in terms of information, we can design BayesOpt policies that seek to reduce our
    uncertainty about the objective function to be optimized in a principled and mathematically
    elegant manner.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了另一种启发式决策方法，这次是利用信息理论来设计我们可以在优化流程中使用的 BayesOpt 策略。与我们所见过的启发式方法（寻求改进、面对不确定性的乐观和随机抽样）不同，这些方法可能看起来独特于与优化相关的任务，信息理论是数学的一个主要子领域，其应用涵盖广泛的主题。正如我们在本章中讨论的，通过诉诸信息理论或更具体地说是*熵*，一种以信息量衡量不确定性的量，我们可以设计出以一种有原则和数学上优雅的方式来减少我们对待优化的目标函数不确定性的
    BayesOpt 策略。
- en: 'The idea behind entropy-based search is quite simple: we look at places where
    our information about a quantity of interest will most increase. As we cover later
    in the chapter, this is similar to looking for a lost remote control in the living
    room, where the TV is, as opposed to in the bathroom.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 基于熵的搜索背后的想法非常简单：我们看看我们关心的数量的信息将最大增加的地方。正如我们在本章后面所讨论的，这类似于在客厅寻找遥控器，而不是在浴室里。
- en: The first part of this chapter is a high-level exposition on information theory,
    entropy, and ways to maximize the amount of information we receive upon performing
    an action. This is done by reinterpreting the familiar example of binary search.
    Armed with the fundamentals of information theory, we then move on to discussing
    BayesOpt policies that maximize information about the global optimum of an objective
    function. These policies are the result of applying information theory to the
    task of BayesOpt. As always, we also learn how to implement these policies in
    Python.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的第一部分是对信息理论、熵以及在执行动作时最大化我们所接收的信息量的方法的高层级阐述。这是通过重新解释熟悉的二分查找示例来完成的。具备信息理论的基础知识后，我们继续讨论最大化关于目标函数全局最优解的信息的
    BayesOpt 策略。这些策略是将信息理论应用于 BayesOpt 任务的结果。与往常一样，我们还学习如何在 Python 中实现这些策略。
- en: By the end of the chapter, you will gain a working understanding of what information
    theory is, how entropy as a measure of uncertainty is quantified, and how entropy
    is translated to BayesOpt. This chapter adds another policy to our BayesOpt toolkit
    and concludes the second part of the book on BayesOpt policies.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '在本章结束时，你将对信息理论是什么、熵作为不确定性度量是如何量化的以及熵如何转化为 BayesOpt 有一个工作理解。本章为我们的 BayesOpt
    工具包增加了另一个策略，并结束了关于 BayesOpt 策略的本书第二部分。 '
- en: 6.1 Measuring knowledge with information theory
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用信息论测量知识
- en: '*Information theory* is a subfield of mathematics that studies how to best
    represent, quantify, and reason about information in a principled, mathematical
    manner. In this section, we introduce information theory on a high level and discuss
    how it is related to decision-making under uncertainty. We do this by reexamining
    the idea behind binary search, a popular algorithm in computer science, from the
    perspective of information theory. This discussion subsequently allows us to connect
    information theory to BayesOpt and motivates an information-theoretic policy for
    optimization.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*信息论*是数学的一个子领域，研究如何以原则性和数学性的方式最佳表示、量化和推理信息。在本节中，我们从信息论的角度重新审视了二分查找的思想，这是计算机科学中一个常用的算法。这次讨论随后允许我们将信息论与BayesOpt相连接，并为优化问题提出信息论策略。'
- en: 6.1.1 Measuring uncertainty with entropy
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.1 使用熵来度量不确定性
- en: Information theory is especially prevalent in computer science, where digital
    information is represented as bits (0s and 1s). You might remember calculating
    how many bits are required to represent a given number of integers—for example,
    a single bit is enough to represent two numbers, 0 and 1, while five bits are
    necessary to represent 32 (2 raised to the fifth power) different numbers. These
    calculations are examples of information theory in practice.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 信息论在计算机科学中特别常见，其中数字信息被表示为二进制（0和1）。你可能还记得计算表示给定数量所需的比特数的例子，例如，一个比特足以表示两个数字，0和1，而五个比特则需要表示32个（2的五次方）不同数字。这些计算是信息论在实践中的例子。
- en: '![](../../OEBPS/Images/06-00-unnumb-1.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/06-00-unnumb-1.png)'
- en: The information-theoretic concept of interest within the context of decision-making
    under uncertainty is *entropy*. Entropy measures the level of uncertainty we have
    in an unknown quantity. If this unknown quantity is modeled as a random variable,
    entropy measures the variability in the possible values of the random variable.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 决策不确定性下，信息论中的重要概念是*熵*。熵度量了我们对未知数量的不确定程度。如果将这个未知数量建模为随机变量，熵度量的是随机变量可能取值的变异性。
- en: Note This uncertainty measure, entropy, is similar to but not quite the same
    as what we’ve been calling *uncertainty* in the predictions made by a GP thus
    far, which is simply the standard deviation of the predictive distribution.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '注意: 这个不确定性度量，熵，与迄今为止我们所称的高斯过程预测中的不确定性有点相似，后者简单地是预测分布的标准差。'
- en: In this subsection, we learn more about entropy as a concept and how it is computed
    for a simple Bernoulli distribution for binary events. We show how entropy successfully
    quantifies uncertainty in an unknown quantity.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将进一步了解熵作为一个概念以及如何计算二元事件的伯努利分布的熵。我们展示熵如何成功地量化了对未知数量的不确定性。
- en: 'Let’s go back to the first example of any Introduction to Probability course:
    coin flipping. Say you are about to flip a biased coin that will land on heads
    with some probability *p* between 0 and 1, and you’d like to reason about the
    event that the coin will, indeed, land on heads. Denote *X* as the binary random
    variable that indicates whether the event happens (that is, *X* = 1 if the coin
    lands on heads and *X* = 0 otherwise). Then, we say that *X* follows a Bernoulli
    distribution with parameter *p*, and the probability that *X* = 1 is equal to
    *p*.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到第一个概率论课的例子：抛硬币。假设你准备抛一枚有偏差的硬币，硬币以概率*p*（介于0和1之间）正面朝上，你想要推理这个硬币正面朝上的事件。用二进制随机变量*X*表示这个事件是否发生（即，如果硬币正面朝上，*X*
    = 1，否则*X* = 0）。那么，我们说*X*符合参数为*p*的伯努利分布，并且*X* = 1的概率等于*p*。
- en: '![](../../OEBPS/Images/06-00-unnumb-2.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/06-00-unnumb-2.png)'
- en: 'Here, the entropy of *X* is defined as –*p* log *p* – (1 – *p*) log(1 – *p*),
    where *log* is the logarithmic function in base 2\. We see that this is a function
    of the probability of heads *p*. Figure 6.1 shows the function of entropy for
    *p* in (0, 1), from which we can gain some insights:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*X*的熵定义为–*p* log *p* – (1 – *p*) log(1 – *p*)，其中*log*是以2为底的对数函数。我们看到这是一个关于硬币正面概率*p*的函数。图6.1展示了*p*在(0,
    1)区间内熵函数的形状，从中我们可以得出一些见解：
- en: The entropy is always nonnegative.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熵始终为非负数。
- en: The entropy increases when *p* increases before 0.5, reaches its highest at
    *p* = 0.5, and then decreases afterwards.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当*p*小于0.5时，**熵**随*p*的增加而增加，在*p* = 0.5时达到最高点，然后逐渐下降。
- en: '![](../../OEBPS/Images/06-01.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/06-01.png)'
- en: Figure 6.1 Entropy of a Bernoulli random variable as a function of the success
    probability. The entropy is maximized (uncertainty is at its highest) when the
    success probability is at 0.5.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 伯努利随机变量的熵作为成功概率的函数。当成功概率为0.5时，熵最大化（不确定性达到最高）。
- en: 'Both insights are valuable when we study our uncertainty about whether or not
    the coin will land on heads. First, we shouldn’t have negative uncertainty about
    something, so it makes sense that the entropy is never negative. More importantly,
    entropy is maximized right at the middle, when *p* = 0.5\. This is quite reasonable:
    as *p* gets farther and farther away from 0.5, we become more certain about the
    outcome of the event—whether the coin will land on heads.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们研究我们对硬币是否落在正面的不确定性时，这两种见解都是宝贵的。首先，我们不应该对某事物有负面不确定性，因此熵永远不是负数。更重要的是，熵在中间处达到最大值，当
    *p* = 0.5 时。这是非常合理的：随着 *p* 离0.5越来越远，我们对事件结果的确定性越来越大—硬币是否落在正面。
- en: For instance, if *p* = 0.7, then we are more certain it will land on heads—our
    entropy is around 0.9 here. If *p* = 0.1, then we are even more certain about
    the outcome (this time that it will land on tails)—the entropy here is roughly
    0.5\. While entropy is not defined at the endpoints (due to the logarithmic function),
    entropy approaches zero as we get closer to either endpoint, indicating zero uncertainty.
    When *p* = 0.5, on the other hand, our uncertainty is at its maximum, as we are
    maximally unsure whether the coin will land on heads or tails. These calculations
    demonstrate that entropy is an appropriate measure of uncertainty.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果 *p* = 0.7，则我们更确定它会落在正面上—这里我们的熵约为0.9。如果 *p* = 0.1，则我们对结果更确定（这次是它会落在反面上）—这里的熵大约是0.5。虽然由于对数函数的原因，熵在端点处未定义，但当我们接近任一端点时，熵接近零，表示零不确定性。另一方面，当
    *p* = 0.5 时，我们的不确定性达到最大值，因为我们对硬币是落正面还是反面最不确定。这些计算表明熵是不确定性的合适度量。
- en: Entropy vs. standard deviation
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 熵 vs. 标准差
- en: When we used the term *uncertainty* in previous chapters, we were referring
    to the standard deviations of the predictive normal distributions produced by
    a GP. The *standard deviation* of a distribution, as the name suggests, measures
    how much the values within the distribution deviate from the mean and is, therefore,
    a valid measure of uncertainty.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在之前的章节中使用术语 *不确定性* 时，我们指的是由GP产生的预测正态分布的标准差。分布的 *标准差* 正如其名称所示，衡量了分布内的值与平均值偏离的程度，因此是一种有效的不确定性度量。
- en: Entropy, on the other hand, is motivated by concepts of information theory,
    and it is also a valid measure of uncertainty. In fact, it is a more elegant and
    general approach to quantify uncertainty and could more accurately model uncertainty
    in edge cases of many situations.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 熵，另一方面，是受到信息理论概念的启发，它也是一种有效的不确定性度量。事实上，它是一种更加优雅和通用的方法来量化不确定性，并且能更准确地模拟许多情况下的边缘情况中的不确定性。
- en: Definition For a given probability distribution, the entropy is defined to be
    –Σ*[i]* *p[i]* log *p[i]*, where we sum over the different possible events indexed
    by *i*.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 对于给定的概率分布，熵被定义为 –Σ*[i]* *p[i]* log *p[i]*，其中我们对不同可能的事件按 *i* 索引进行求和。
- en: We see that what we used for the preceding Bernoulli distribution is a special
    case of this formula. We also use this formula later in the chapter when we work
    with uniform distributions.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到我们用于前述伯努利分布的公式是这个公式的一个特殊情况。我们在本章后面处理均匀分布时也使用了这个公式。
- en: 6.1.2 Looking for a remote control using entropy
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.2 使用熵寻找遥控器
- en: As entropy measures how much uncertainty there is in our knowledge about a quantity
    or event of interest, it can inform our decisions, helping us most efficiently
    reduce our uncertainty about the quantity or event. We look at an example of this
    in this subsection, in which we decide where to best look for a lost remote control.
    Although simple, the example presents the information-theoretic reasoning we use
    in subsequent discussions, where entropy is used for more complex decision-making
    problems.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 由于熵衡量了我们对感兴趣的数量或事件的知识中有多少不确定性，因此它可以指导我们的决策，帮助我们最有效地减少我们对数量或事件的不确定性。我们在本小节中看一个例子，我们在其中决定在哪里最好地寻找丢失的遥控器。虽然简单，但这个例子呈现了我们在后续讨论中使用的信息论推理，在那里熵被用于更复杂的决策问题。
- en: Imagine that while trying to turn on the TV in your living room one day, you
    realize you can’t find the remote control on the table, which is where it usually
    is. You, therefore, decide to conduct a search for this remote. First, you reason
    that it should be in the living room somewhere, but you don’t have any idea about
    where the remote is within the living room, so all locations are equally likely.
    In the language of probabilistic inference that we've been using, you can say
    that the *distribution of the location of the remote* is uniform over the living
    room.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/06-02.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 A sample floor plan for the example of finding the remote control.
    The living room is uniformly shaded to indicate that the distribution of the location
    of the remote is uniform over the living room.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.2 visualizes this belief about the location of the remote control
    that you have, indicated by the shaded living room, which is where the remote
    is (according to your belief). Now, you might ask yourself this question: Where
    in this house should you look for the remote? It’s reasonable to think that you
    should look in the living room, as opposed to, say, the bathroom, because that’s
    where the TV is. But how does one quantifiably justify that choice?'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Information theory, specifically entropy, offers a way of doing that by allowing
    us to reason about how much entropy remains after a search for a remote in the
    living room versus in the bathroom. That is, it allows us to determine how much
    uncertainty about the location of the remote we have left after looking in the
    living room as opposed to after looking in the bathroom.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/06-03.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 Entropy of the location of the remote after a portion of the living
    room is searched. If the remote is found (upper right), then no uncertainty remains.
    Otherwise, entropy is still reduced (lower right), as the distribution of the
    location of the remote is now narrower.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.3 shows how entropy of the location of the remote decreases once the
    upper portion of the living room is searched. We can reason the following:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: If the remote is found within the searched region, then you will no longer have
    any uncertainty about its location. In other words, the entropy will be zero.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the remote is not found, then our posterior belief about the location of
    the remote is updated to the shaded region in the lower-right section. This distribution
    spans a smaller region than the one in figure 6.2, so there is less uncertainty
    (entropy).
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Either way, the entropy is reduced by looking in the specified portion of the
    living room. What would happen, then, if you decided to search for the remote
    in the bathroom? Figure 6.4 shows the corresponding reasoning:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: If the remote is found in the bathroom, then entropy will still drop to zero.
    However, this is unlikely to happen, according to your belief about the location
    of the remote.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the remote is not found in the bathroom, then your posterior belief about
    the location of the remote doesn’t change from figure 6.2, and the resulting entropy
    remains the same.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果遥控器在浴室中找不到，那么你对遥控器位置的后验信念不会从图6.2中改变，结果熵保持不变。
- en: '![](../../OEBPS/Images/06-04.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/06-04.png)'
- en: Figure 6.4 Entropy of the location of the remote after the bathroom is searched.
    As the remote cannot be found in the bathroom, the entropy in the posterior distribution
    of the location of the remote is unchanged.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 在搜索完浴室后，遥控器位置的熵。由于遥控器在浴室中找不到，遥控器位置的后验分布中的熵不变。
- en: Searching for and not finding the remote in the bathroom doesn’t reduce the
    entropy of the location of the remote. In other words, looking in the bathroom
    doesn’t provide any extra information about the location of the remote, so it
    is the suboptimal decision according to information theory.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在浴室里搜索而没有找到遥控器并不会减少遥控器位置的熵。换句话说，在浴室里寻找并不提供有关遥控器位置的任何额外信息，因此根据信息论，这是次优的决定。
- en: 'This comparison would not be as cut and dried if the prior distribution of
    the location of the remote (your initial guess about where it is) was over the
    entire house, not just the living room. After all, there is always a small probability
    that the remote got misplaced outside the living room. However, the procedure
    of determining where to look—that is, the portion of the house that will give
    you maximal information about the location of the remote—is still the same:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果遥控器位置的先验分布（关于它在哪里的你的初始猜测）涵盖整个房子而不仅仅是客厅，那么这种比较就不会那么明显了。毕竟，遥控器被误放在客厅外的概率总是很小的。然而，决定在哪里寻找的过程——即能够为你提供有关遥控器位置最大信息的房子部分——仍然是相同的：
- en: Consider the posterior distribution of the location of the remote if it is found,
    and compute the entropy of that distribution.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑如果找到了遥控器的话，遥控器位置的后验分布，并计算该分布的熵。
- en: Compute the same entropy if the remote is not found.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算如果遥控器没有找到时的熵。
- en: Compute the average entropy over the two cases.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算两种情况下的平均熵。
- en: Repeat this computation for all locations you are considering looking in, and
    pick the one that gives the lowest entropy.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为考虑寻找的所有位置重复此计算，并选择给出最低熵的位置。
- en: Entropy gives us a way to quantify our uncertainty about a quantity of interest,
    using its probabilistic distribution in an information-theoretic manner. This
    procedure uses entropy to identify the action that maximally reduces entropy.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 熵提供了一种用信息论方法量化我们对感兴趣的数量的不确定性的方法，利用其在概率分布中的信息。此过程使用熵来识别最大程度减少熵的行动。
- en: Note This is a mathematically elegant procedure applicable to many decision-making
    situations under uncertainty. We can think of this entropy-based search procedure
    as a kind of search for the truth in which we aim to take the action that takes
    us as close to the truth as possible by maximally reducing uncertainty.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 这是一个在许多不确定性下决策情况下适用的数学上优雅的过程。我们可以将这种基于熵的搜索过程看作是一种搜索真相的过程，我们的目标是通过最大程度地减少不确定性来尽可能地接近真相。
- en: 6.1.3 Binary search using entropy
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.3 利用熵进行二进制搜索
- en: 'To further understand entropy-based search, we now see how this procedure manifests
    itself in one of the classic algorithms in computer science: binary search. You
    are most likely already familiar with this algorithm, so we don’t go into much
    detail here. For an excellent and beginner-friendly explanation of binary search,
    I recommend chapter 1 of Aditya Bhargava’s *Grokking Algorithms* (Manning, 2016).
    On a high level, we employ binary search when we want to look for the position
    of a specific targeted number within a sorted list such that the elements in the
    list are increasing from the first to the last element.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步理解基于熵的搜索，我们现在看看这个过程如何在计算机科学中的经典算法之一：二分查找中体现。您很可能已经熟悉这个算法，所以我们在这里不会详细介绍。对于对二分查找有很好且适合初学者的解释，我推荐阅读
    Aditya Bhargava 的《Grokking Algorithms》（Manning，2016）的第1章。从高层次上来说，当我们想要在一个排序列表中查找特定目标数字的位置，使得列表中的元素从第一个到最后一个元素递增时，我们使用二分查找。
- en: Tip The idea behind binary search is to look at the middle element of the list
    and compare it to the target. If the target is smaller than the middle element,
    then we know to only look at the first half of the list; otherwise, we look at
    the second half. We repeat this process of halving the list until we find the
    target.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 提示 二分搜索的思想是查看列表的中间元素并将其与目标进行比较。 如果目标小于中间元素，则我们只查看列表的前一半；否则，我们查看后一半。 我们重复这个列表减半的过程，直到找到目标。
- en: Consider a concrete example in which we have a sorted list of 100 elements [*x*[1],
    *x*[2], ..., *x*[100]], and we’d like to find the location of a given target *z*,
    assuming *z* is, indeed, in the sorted list.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个具体的例子，我们有一个排序过的100个元素列表 [*x*[1]，*x*[2]，...，*x*[100]]，我们想要找到给定目标 *z* 的位置，假设
    *z* 确实在排序列表中。
- en: '![](../../OEBPS/Images/06-05.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/06-05.png)'
- en: Figure 6.5 An illustration of binary search on a 100-element list. At each iteration
    of the search, the target is compared against the middle element of the current
    list. Depending on the result of this comparison, we remove either the first or
    second half of the list from the search space.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 在100个元素列表上执行二分搜索的示例。 在搜索的每次迭代中，目标被与当前列表的中间元素进行比较。 根据这个比较的结果，我们将第一半或第二半列表从搜索空间中移除。
- en: 'As illustrated by figure 6.5, binary search works by dividing the list into
    two halves: the first 50 elements and the last 50 elements. Since we know the
    list is sorted, we know the following:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 正如图6.5所示，二分搜索通过将列表分为两半进行工作：前50个元素和后50个元素。 由于我们知道列表是排序过的，我们知道以下内容：
- en: If our target *z* is less than the 50th element *x*[50], then we only need to
    consider the first 50 elements, since the last 50 elements are all greater than
    target *z*.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们的目标 *z* 小于第50个元素 *x*[50]，那么我们只需要考虑前50个元素，因为最后50个元素都大于目标 *z*。
- en: If our target is greater than *x*[50], then we only need to look at the second
    half of the list.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们的目标大于 *x*[50]，那么我们只需要查看列表的后一半。
- en: Terminating the search
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 终止搜索
- en: For each comparison in figure 6.5, we omit the situation where *z* is equal
    to the number it’s being compared to, in which case we can simply terminate the
    search.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图6.5中的每次比较，我们忽略*z*等于被比较的数字的情况，这种情况下我们可以简单地终止搜索。
- en: On average, this procedure helps us find the location of *z* within the list
    much more quickly than sequentially searching through the list from one end to
    the other. Binary search is a realization of the goal of making the optimal decision
    based on information theory in this game of searching for the location of a number
    within a sorted list, if we were to tackle the problem from a probabilistic angle.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 平均而言，这个过程可以帮助我们更快地在列表中找到 *z* 的位置，比在列表中顺序搜索从一端到另一端要快得多。 如果我们从概率的角度来处理这个问题，二分搜索是在排序列表中的数字位置搜索游戏中基于信息理论作出最佳决策的实现目标的方法。
- en: Note Binary search strategy is the optimal solution for finding *z*, allowing
    us to locate it more quickly than any other strategy, on average.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 二分搜索策略是寻找 *z* 的最佳解决方案，使我们能够比其他任何策略更快地找到它，平均来看。
- en: First, let’s use the random variable *L* to denote the location of our target
    *z* within the sorted list. Here, we would like to use a distribution to describe
    our belief about the variable. Since from our perspective, any location within
    the list is equally likely to contain the value of *z*, we use a uniform distribution
    for modeling.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们用随机变量 *L* 表示我们目标 *z* 在排序列表中的位置。 这里，我们想要使用一个分布来描述我们对这个变量的信念。 由于从我们的角度来看，列表中的任何位置都同样可能包含
    *z* 的值，我们使用均匀分布进行建模。
- en: Figure 6.6 visualizes this distribution, which, again, represents our belief
    about the location of *z*. Since each location is equally as likely as any other,
    the probability that a given location contains *z* is uniformly 1 ÷ 100, or 1%.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6可视化了这种分布，再次表示我们对 *z* 位置的信念。 由于每个位置都与其他任何位置一样可能，特定位置包含 *z* 的概率是均匀的1 ÷ 100，即1%。
- en: '![](../../OEBPS/Images/06-06.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/06-06.png)'
- en: Figure 6.6 Prior distribution of the location of the target *z* within the 100-element
    list. Since each location is as equally likely as any other, the probability that
    a given location contains *z* is 1%.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 给出了100个元素列表中目标 *z* 位置的先验分布。 由于每个位置一样可能包含 *z*，特定位置包含 *z* 的概率为1%。
- en: Note Let’s try computing the entropy of this uniform distribution. Remember,
    the formula for the entropy is *–Σ[i]* *p[i]* log *p[i]*, where we sum over the
    different possible events indexed by *i*. This is equal to
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 让我们尝试计算这个均匀分布的熵。记住，熵的公式是 *–Σ[i]* *p[i]* log *p[i]*，其中我们对不同可能事件 *i* 进行求和。这等于
- en: '![](../../OEBPS/Images/06-06-Equations_ch-6-1.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/06-06-Equations_ch-6-1.png)'
- en: So, the amount of uncertainty we have in our prior distribution of *L* is roughly
    6.64.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们对 *L* 先验分布的不确定性大约为 6.64。
- en: 'Next, we tackle the same question: How should we search over this 100-element
    list to locate *z* as quickly as possible? We do this by following the entropy
    search procedure described in section 6.1.2, where we aim to minimize the entropy
    in the posterior distribution of the quantity that we care about—in this case,
    the location *L*.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们解决同样的问题：我们应该如何在这个 100 元素列表中搜索以尽快找到 *z*？我们通过遵循第 6.1.2 节描述的熵搜索过程来做到这一点，我们的目标是尽量减少我们关心的量的后验分布的熵，也就是说，在这种情况下是位置
    *L*。
- en: 'How do we compute the entropy of the posterior distribution of *L* after inspecting
    a given location? This calculation requires us to reason about what we can conclude
    about *L* upon inspecting a given location, which is quite easy to do. Say we
    decide to inspect the first location *x*[1]. According to our belief about *L*,
    there is a 1% chance that *L* is at this location and a 99% chance that *L* is
    in one of the remaining slots:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在检查了给定位置后，我们如何计算后验分布 *L* 的熵？这个计算要求我们推理在检查了给定位置后我们对 *L* 可以得出什么结论，这是相当容易做到的。假设我们决定检查第一个位置
    *x*[1]。根据我们对 *L* 的信念，*L* 在这个位置的可能性为 1%，而在其余位置的可能性为 99%：
- en: If *L* is, indeed, at this location, then our posterior entropy about *L* drops
    to 0, as there’s no more uncertainty about this quantity.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 *L* 确实在这个位置，那么关于 *L* 的后验熵将降为 0，因为对于这个量再也没有不确定性了。
- en: Otherwise, the distribution of *L* is updated to reflect this observation that
    *z* is not the first number of the list.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 否则，*L* 的分布将更新以反映出观察到 *z* 不是列表的第一个数字这一事实。
- en: Figure 6.7 shows this process as a diagram in which we need to update the distribution
    for *L* so that each of the 99 locations has a 1 ÷ 99, or roughly 1.01%, probability
    of containing *z*. Each of the 99 locations is still equally likely, but the probability
    of each location has gone up a bit since we have ruled out the first location
    in this hypothetical scenario.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7 将这个过程显示为一个图表，我们需要更新 *L* 的分布，以便每个位置都有 1 ÷ 99，或大约 1.01% 的概率包含 *z*。每个位置仍然同样可能，但每个位置的概率稍微增加了一些，因为在这种假设的情况下，我们已经排除了第一个位置。
- en: '![](../../OEBPS/Images/06-07.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/06-07.png)'
- en: Figure 6.7 Posterior distributions of the location of target *z* within the
    100-element list upon inspecting the first element. In each scenario, the probability
    that *z* is at a given location is updated accordingly.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7 给出了在检查第一个元素后目标 *z* 在 100 元素列表中位置的后验分布。在每种情景中，*z* 在给定位置的概率会相应更新。
- en: Note Again, we’re only considering the case where *z* exists in the list, so
    either the smallest element in the list *x*[1] is equal to *z*, or the former
    is less than the latter.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 再次说明，我们只考虑 *z* 存在于列表中的情况，所以要么列表中最小的元素 *x*[1] 等于 *z*，要么前者小于后者。
- en: Following the same calculation, we can obtain the entropy for this new distribution
    as
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 按照同样的计算，我们可以得到这个新分布的熵为
- en: '![](../../OEBPS/Images/06-07-Equations_ch-6-2.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/06-07-Equations_ch-6-2.png)'
- en: 'Again, this is the posterior entropy of *L* in the second case, where *z* is
    not present in the first location. The last step we need to take to compute the
    overall posterior entropy after inspecting the first location is to take the average
    of the two cases:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 再次说明，这是在第二种情况下 *z* 不在第一个位置的情况下 *L* 的后验熵。我们需要采取的最后一步来计算在检查第一个位置后的总后验熵是取这两种情况的平均值：
- en: If *z* is in the first slot, which is 1% likely, then the posterior entropy
    is 0.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 *z* 在第一个位置，这个可能性是 1%，那么后验熵为 0。
- en: If *z* is not in the first slot, which is 99% likely, then the posterior entropy
    is 6.63.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 *z* 不在第一个位置，这个可能性是 99%，那么后验熵为 6.63。
- en: Taking the average, we have 0.01 (0) + 0.99 (6.63) = 6.56\. So, on average,
    we expect to see a posterior entropy of 6.56 when we choose to look into the first
    element of the array. Now, to determine whether looking at the first element is
    the optimal decision or there’s a better location for obtaining more information
    about *L*, we need to repeat this procedure for the other locations in the list.
    Specifically, for a given location, we need to
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Iterate over each of the potential scenarios while inspecting the location
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the posterior entropy of the distribution of *L* for each scenario
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the average posterior entropy across the scenarios based on how likely
    each of them is
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s do this one more time for, say, the 10th location *x*[10]; the corresponding
    diagram is shown in figure 6.8\. While this scenario is slightly different from
    what we just went over, the underlying idea is still the same. First, there are
    various scenarios that can take place when we look at *x*[10]:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: The 10th element *x*[10] can be greater than *z*, in which case we can rule
    out the last 91 elements in the list and focus our search on the first 9 elements.
    Here, each of the 9 locations has an 11% chance of containing *z*, and the posterior
    entropy, by using the same formula, can be computed to be roughly 3.17.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The tenth element *x*[10] can be exactly equal to *z*, in which case our posterior
    entropy is once again zero.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The tenth element *x*[10] can be less than *z*, in which case we narrow our
    search to the last 90 elements. The posterior entropy in this case is around 6.49.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/06-08.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 Posterior distributions of the location of target *z* within the
    100-element list upon inspecting the 10th element. In each scenario, the probability
    that *z* is at a given location is updated accordingly.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Note Make sure you attempt the entropy computations yourself to understand how
    we are getting these numbers.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we take the weighted average of these entropies using the corresponding
    probabilities: 0.09 (3.17) + 0.01 (0) + 0.9 (6.49) = 6.13\. This number presents
    the expected posterior entropy—that is, the expected posterior uncertainty we
    have about *L*, the location of *z*, after inspecting the 10th element *x*[10].'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Compared to the same number for the first element *x*[1], 6.56, we conclude
    that looking at *x*[10] on average gives us more information about *L* than looking
    at *x*[1]. In other words, inspecting *x*[10] is the better decision in terms
    of information theory.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: But what is the *optimal* decision in terms of information theory—the one that
    gives us the most information about *L*? To determine this, we simply repeat the
    computation we just performed on *x*[1] and *x*[10] for the other locations in
    the list and pick out the one with the lowest expected posterior entropy.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.9 shows this quantity, the expected posterior entropy in the location
    of our target, as a function of the location we choose to inspect. The first thing
    we notice is the symmetry of the curve: looking at the last location gives us
    the same expected posterior entropy (uncertainty) as looking at the first; similarly,
    the 10th and the 90th locations give the same amount of information, and so on.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9展示了我们所寻找目标的预期后验熵随我们选择的检查位置而变化的情况。我们首先注意到曲线的对称性：只看最后一个位置与只看第一个位置的预期后验熵（不确定性）是相等的；同样地，第10个位置和第90个位置给我们提供的信息是一样的。
- en: '![](../../OEBPS/Images/06-09.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/06-09.png)'
- en: Figure 6.9 Expected posterior entropy in the location of the target as a function
    of the location within the list to inspect. The middle location is optimal, minimizing
    the expected entropy.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9展示了随着检查列表中位置的变化，目标的预期后验熵如何变化。检查中间位置是最优的，可以最小化预期熵。
- en: More importantly, we see that inspecting the locations in the middle, either
    the 50th or 51st number in the list, gives us the maximal amount of information.
    This is because once we do, we are guaranteed to rule out *half* of the list,
    regardless of whether our target number is greater or less than the number in
    the middle. This is not the case for other locations. As we saw earlier, we may
    be able to rule out 90 numbers in the list when looking at the 10th number, but
    this only happens with 0.1 probability. When looking at the first number, there’s
    a 99% probability that we will only be able to rule out one number.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 更为重要的是，我们可以看到只有检查中间位置，也就是列表的第50个或第51个位置，才能提供最大的信息量。这是因为，一旦我们这样做了，无论我们的目标数字大于还是小于中间数字，我们都能排除一半的列表。但对于其他位置并非如此。如前所述，当我们检查第10个数字时，可能能排除列表中的90个数字，但这只有0.1的概率。而当我们检查第一个数字时，99%的概率是只能排除一个数字。
- en: Note All in all, inspecting the numbers in the middle maximizes the amount of
    information we gain, on average.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在平均上，只检查中间的数字最大化我们获得的信息。
- en: 'The rest of the search for the target follows the same procedure: computing
    the expected posterior entropy that will result from each decision and then choosing
    the decision that minimizes that entropy. As the probability distribution we work
    with is always a uniform distribution after each update, the optimal number to
    inspect is always the one in the middle of the array that hasn’t been ruled out.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找目标的其余流程遵循同样的程序：计算每个决策可能导致的预期后验熵，然后选择使得后验熵最小的决策。由于每次更新后我们所处理的概率分布总是一个均匀分布，因此最优的检查位置总是还没有被排除的中间位置。
- en: This is exactly the strategy of binary search! From the perspective of information
    theory, binary search is the optimal solution to the problem of search for a number
    in a sorted list.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是二分查找的策略！从信息论角度来看，二分查找是在有序列表中查找数字的最优解决方案。
- en: Justifying binary search with information theory
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 用信息论证明二分查找
- en: When I first learned about this algorithm, I remember thinking that the strategy
    of searching in the middle of an array seemed, although reasonable, quite unique
    and “out of nowhere.” However, we have just learned to derive the same solution
    from an information-theoretic perspective, which concretely quantifies the idea
    of ruling out half of our search space in the service of gaining as much information
    as possible or reducing as much entropy as possible.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 当我第一次学习这个算法时，我记得曾经认为在数组中间搜索的策略似乎是独特的且来源神秘。然而，我们刚刚从信息理论的角度推导出了相同的解决方案，这明确量化了在服务于获得尽可能多的信息或减少尽可能多的熵的目的下，排除一半的搜索空间的想法。
- en: 'The application of information theory and entropy doesn’t stop at binary search.
    As we saw, the procedure we went through is generalizable to other decision-making
    problems: if we can model the problem of interest in terms of probability distributions
    for unknown quantities, actions we can take, and how the distributions can be
    updated when an action is taken, then we can once again choose the optimal action
    in terms of information theory, which is the action that reduces our uncertainty
    about the quantity of interest the most. For the remainder of this chapter, we
    learn to apply this idea to BayesOpt and implement the resulting entropy search
    policy with BoTorch.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Entropy search in BayesOpt
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Taking the same approach presented in the previous section, we obtain entropy
    search policies in BayesOpt. The main idea is to choose our actions, our experiments,
    so that we can reduce the most amount of entropy in the posterior distribution
    of what we care about. In this section, we first discuss how to do this on a high
    level and then move on to implementation in BoTorch.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.1 Searching for the optimum using information theory
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In our remote control example, we aim to search for the remote within an apartment
    and, thus, want to reduce the entropy of the distribution of the location of the
    remote. In binary search, the process is similar: our target is the location of
    a specific number we’d like to search for within a list, and we want to reduce
    the entropy of the distribution of that number. Now, to design an entropy search
    policy, we must determine what to use as our target in BayesOpt and how to use
    information theory to aid the search process, which we learn how to do here.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall our ultimate goal in using BayesOpt: to search within the domain *D*
    of a black box function at the location where the function is maximized. This
    means a natural search target for us is the location *x** that maximizes the objective
    value of the function *f*. That is, *f * = f(x*)* ≥ *f(x)*, for all *x* in *D*.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Definition The optimal location *x** is often called the *optimizer* of the
    objective function *f*.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a GP belief about the objective function *f*, there is a corresponding
    probabilistic belief about the optimizer *x**, which is treated as a random variable.
    Figure 6.10 shows an example of a trained GP and the distribution of the objective
    optimizer *x** induced from the GP. It’s important to keep a few interesting characteristics
    of this distribution in mind:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: The distribution is complicated and multimodal (having several local optima).
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The most likely location for the optimizer *x** is a bit to the left of zero.
    This is where the predictive mean of the GP is maximized.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is non-negligible probability that the optimizer *x** is at an end point
    –5 or 5.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The probability that *x** is around 2 is almost zero, corresponding to the fact
    that we already observed a higher objective value than *f*(2).
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/06-10.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 GP belief (top) and the distribution of the function optimizer *x**
    (bottom). The distribution of the optimizer is non-Gaussian and fairly complicated,
    posing a challenge for modeling and decision-making.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: These characteristics make modeling this distribution of *x** very challenging.
    The most simple method of gauging how this quantity *x** is distributed is to
    simply draw many samples from the GP and record the location at which each sample
    is maximized. This is, in fact, how figure 6.10 is generated.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Making matters worse is the fact that we need exponentially more samples to
    estimate the distribution of *x** when the dimensionality of the objective function
    (the length of the input vector *x* or the number of features each *x* has) increases.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Definition This is an instance of the *curse of dimensionality*, which, in ML,
    is often used to refer to the exponential cost of many procedures with respect
    to the dimensionality of an object of interest.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: To use entropy search in BayesOpt, we need to model our belief about the location
    of the optimum *x** using a probability distribution. However, we can’t exactly
    model the distribution of the optimizer *x**; instead, we must approximate it
    using samples drawn from a GP. Unfortunately, this process quickly becomes computationally
    expensive as the number of dimensions in the objective function (the length of
    *x*) increases.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Note There are, in fact, research papers in BayesOpt that seek to search for
    the location of the optimizer *x** using entropy. The resulting policies, however,
    often prove too computationally expensive to run and are not implemented in BoTorch.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: But that doesn’t mean we need to abandon the effort of using information theory
    in BayesOpt altogether. It just means we need to modify our search procedure to
    make it more amenable to computational methodologies. One easy way to achieve
    this is to target a quantity other than the optimizer *x** that is, on the one
    hand, connected to the search for *x** and, on the other, easier to reason about.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: A quantity of interest in optimization, other than the optimizer *x**, is the
    optimal value *f** = *f*(*x**), achieved at the optimizer, which is also a random
    variable, according to our GP belief about the objective function *f*. As one
    can imagine, learning about the optimal value *f** might tell us a lot about the
    optimizer *x**; that is, the two quantities are connected in terms of information
    theory. However, the optimal value *f** is much easier to work with than the optimizer
    *x**, as the former is just a real-valued number, while the latter is a vector
    of length equal to the number of dimensions of the objective function.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: An example of the distribution of the optimal value *f** induced by a GP is
    shown in figure 6.11, on the right panel. We see that that this distribution is
    roughly truncated around 1.6, which is exactly the value of the incumbent in our
    training dataset; this makes sense, as the optimal value *f** must be at least
    the incumbent value 1.6.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/06-11.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 The GP belief (top left), the distribution of the function optimizer
    *x** (bottom), and the distribution of the optimal value *f** (right). The distribution
    of the optimal value is always one-dimensional and, therefore, easier to work
    with than that of the optimizer.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Note The main advantage of focusing our effort on this distribution of *f**
    is the fact that the distribution is always one-dimensional, regardless of the
    dimensionality of the objective function.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we can draw samples from this one-dimensional distribution to
    approximate the expected posterior entropy upon making a query. From this point,
    we follow the same idea behind entropy search: choosing the query that (approximately)
    minimizes the expected posterior entropy or, in other words, maximizes the expected
    *reduction* in entropy.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Definition By using this expected-reduction-in-entropy quantity as the acquisition
    score, we obtain the *Max-value Entropy Search* (MES) policy. The term *max-value*
    denotes the fact that we are using information theory to search for the max value,
    or the optimal value *f**, of the objective function.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.12 shows the MES acquisition score in our running example in the bottom
    panel, where the point around –2 is where we should query next, according to this
    information-theory–based criterion. The MES policy prefers this location because
    it has both a relatively high mean and a high CI, thus balancing exploration and
    exploitation.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/06-12.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 The GP belief (top left), the distribution of the optimal value
    *f** (right), and the approximate expected reduction in entropy, which is used
    as the acquisition function score (bottom). The distribution of the optimal value
    is always one-dimensional and, therefore, easier to work with than that of the
    optimizer.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, the acquisition landscape here looks somewhat similar to the
    distribution of the optimizer *x** itself, which is shown in figure 6.11, where
    we see that the curve
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Reaches its zenith somewhere in the middle
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Achieves a non-negligible value at the end points
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bottoms out at 0 around 2, where we know for certain the location is not optimal
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is an indication that the optimal value *f** is intimately linked to the
    optimizer *x**, and while we lose some information by altering our objective,
    searching for *f** is a good proxy for searching for *x** and is much more computationally
    tractable.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.2 Implementing entropy search with BoTorch
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Having discussed the high-level idea behind MES, we are now ready to implement
    it using BoTorch and plug it into our optimization pipeline. The MES policy is
    implemented by the `qMaxValueEntropy` class in `botorch.acquisition.max_value_entropy_
    search` as a PyTorch module, similar to most BayesOpt policies we have seen. When
    initialized, this class takes in two arguments: a GPyTorch GP model and a set
    of points that will be used as samples in the approximation procedure described
    in the previous section.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'While there are many ways that these sample points can be generated, one particular
    way we have learned from section 5.3.2 is using a Sobol sequence, which does a
    better job of covering the targeted space. Overall, the MES policy is implemented
    as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Generates the samples between 0 and 1 using a Sobol sequence
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Rescales the samples to be inside the domain
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Declares the MES policy object
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Computes the acquisition scores
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Here, `num_candidates` is a tunable parameter that sets the number of samples
    you’d like to use in the MES computation. A larger value would mean an approximation
    with higher fidelity, but it would come at a higher computational cost.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now apply this code to our running problem of optimizing the one-dimensional
    Forrester function, as implemented in the CH06/01 - BayesOpt loop.ipynb notebook.
    We are already familiar with most of this code, so we don’t go into the details
    here.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.13 shows the progress made by MES for 10 queries, in which the policy
    quickly finds the global optimum of the Forrester after five queries. Interestingly,
    as optimization progresses, we become more and more certain that looking at other
    regions in the search space will not result in any substantial reduction in entropy,
    which helps us stay close to the optimal location.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/06-13.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13 Progress made by the MES policy. The policy quickly finds the global
    optimum after five queries.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: We have seen that information theory gives us a principled, mathematically elegant
    framework for decision-making that revolves around trying to learn as much as
    possible about a quantity of interest. This comes down to reducing the expected
    posterior entropy in the distribution that models the quantity we care about.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Within BayesOpt, we saw that a direct translation of this procedure poses computational
    challenges in modeling the location of the optimal value of the objective function,
    our primary search target. Instead, we shift our focus to the optimal value of
    the objective function itself, making the computation more tractable. Fortunately,
    all of this math is nicely abstracted away by BoTorch, leaving us with a convenient,
    modular interface we can plug into any optimization problem.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: This is also where the second part of the book on BayesOpt policies comes to
    an end. The last three chapters covered some of the most commonly used heuristics
    to decision-making in BayesOpt and the corresponding policies, ranging from seeking
    improvement from the incumbent to borrowing multi-armed bandit methods and, in
    this chapter, using information theory.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: The remainder of the book takes our discussion to the next level by introducing
    special optimization settings that differ from what we have seen so far, where
    we sequentially observe a single data point at each step of the optimization.
    These chapters show that the methods we have learned can be translated to practical
    settings in the real world to accelerate optimization.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Exercises
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two exercises in this chapter:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: The first exercise covers a variant of binary search in which prior information
    can be taken into account when making decisions.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The second walks us through the process of implementing MES in the hyperparameter
    tuning problem seen in previous chapters.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '6.3.1 Exercise 1: Incorporating prior knowledge into entropy search'
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We saw in section 6.1.3 that by placing a uniform prior distribution on the
    location of the target within the array, the optimal information-theoretical search
    decision is to cut the array in half. What happens if a uniform distribution doesn’t
    represent your prior belief faithfully and you’d like to use a different distribution?
    This exercise, implemented in the CH06/02 - Exercise 1.ipynb notebook, shows us
    an instance of this as well as how to derive the resulting optimal decision. Solving
    this exercise should help us further appreciate the elegance and flexibility of
    entropy search as a generic decision-making procedure under uncertainty.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine the following scenario: you work at a phone manufacturing company in
    the quality control department, and your current project is to stress test the
    robustness of the casing of the company’s newest product. Specifically, your team
    wants to find out from which floor of a 10-story building one can drop the phone
    to the ground and not break it. A few rules apply:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: The engineers who made the phone are sure it won’t break if dropped from the
    first floor.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the phone breaks when dropped from a given floor, then it will also break
    when dropped from a higher floor.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You are tasked with finding the highest floor from which one can drop the phone
    without it breaking—we denote this unknown floor as *X*—by conducting trials.
    That is, you must drop actual phones from specific floors to determine *X*. The
    question is the following: How should you choose which floors to drop the phones
    from to find *X*? Since the phones are expensive, you need to conduct as few trials
    as possible and would like to use information theory to aid the search:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Assume that by taking into account physics and the materials and construction
    of the phone, the engineers have an initial guess regarding which floors might
    be possible.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Specifically, the prior distribution of *X* is exponential in that the probability
    that *X* is equal to a number is inversely exponential with respect to that number:
    *Pr*(*X* = *n*) = 1 / 2^(*n*), for *n* = 1, 2, ..., 9; the probability corresponding
    to the highest (tenth) floor is *Pr*(*X* = 10) = 1 / 2⁹. So the probability that
    *X* = 1 is 50%, and this probability is cut in half as the number increases. This
    probability distribution is visualized in figure 6.14.'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/06-14.png)'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 6.14 The probability that *X* is equal to a number between 1 and 10 (that
    is, the probability that a floor is the highest floor that doesn’t cause the phone
    to break when dropped)
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Verify that this is a valid probability distribution by proving that the probabilities
    sum to one. That is, prove that *Pr*(*X* = 1) + *Pr*(*X* = 2) + ... + *Pr*(*X*
    = 10) = 1.
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Calculate the entropy of this prior distribution using the formula given at
    the end of section 6.1.1.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Given the prior distribution defined between 1 and 10, what is the probability
    that the phone will break when dropped from the second floor? What is this probability
    for the fifth floor? How about the first floor?
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assume that after observing the result of any trial, the posterior distribution
    for *X* is once again exponential and is defined between the lowest and highest
    possible floors.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For example, if you observe that the phone doesn’t break when dropped from the
    fifth floor, then we know that *X* is at least 5 and the posterior distribution
    of *X* is such that *Pr*(*X* = 5) = 1 / 2, *Pr*(*X* = 6) = 1 / 4, ..., *Pr*(*X*
    = 9) = 1 / 32, *Pr*(*X* = 10) = 1 / 32\. If, on the other hand, the phone breaks
    when dropped from the fifth floor, then we know that *X* is at most 4 and the
    posterior distribution is such that *Pr*(*X* = 1) = 1 / 2, *Pr*(*X* = 2) = 1 /
    4, *Pr*(*X* = 3) = 1 / 8, *Pr*(*X* = 4) = 1 / 8\. Figure 6.15 shows these two
    scenarios.
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/06-15.png)'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 6.15 The posterior probability distributions of *X* in two scenarios
    when the phone is dropped from the fifth floor. Each posterior distribution is
    still exponential.
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Compute the entropy of this fictitious posterior distribution in the two cases.
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Given the prior distribution, compute the expected posterior entropy after you
    conduct a trial on the fifth floor (that is, after you drop the phone from the
    fifth floor and observe whether it breaks).
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute this expected posterior entropy for other floors. Which floor gives
    the highest reduction in entropy? Is this still the same result as binary search?
    If not, what has changed?
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '6.3.2 Exercise 2: Bayesian optimization for hyperparameter tuning'
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This exercise, implemented in the CH06/03 - Exercise 2.ipynb notebook, applies
    BayesOpt to an objective function that simulates the accuracy surface of a support-vector
    machine model in a hyperparameter tuning task. The *x*-axis denotes the value
    of the penalty parameter *C*, while the *y*-axis denotes the value of the RBF
    kernel parameter *γ*. See the exercises in chapters 3 and 4 for more detail. Complete
    the exercise using the following steps:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Recreate the BayesOpt loop in the CH05/03 - Exercise 2.ipynb notebook, including
    the outer loop that implements repeated experiments.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the MES policy. Since our objective function is two-dimensional, we should
    increase the size of the Sobol sequence used by MES. For example, you can set
    it at 2,000\. Observe its aggregated performance.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeated experiments in BayesOpt
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Refer to step 9 of exercise 2 from chapter 4 to see how we can run repeated
    experiments in BayesOpt.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Information theory studies the representation, quantification, and transfer
    of information. One of the core concepts in this field is entropy, which quantifies
    our uncertainty about a random variable from the variable’s probability distribution.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An entropy search procedure considers the expected reduction in entropy (and
    therefore in uncertainty) of a quantity of interest by taking an action, choosing
    the action that maximizes this reduction. We can apply this generic procedure
    to many decision-making problems under uncertainty.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Binary search may be obtained as the result of entropy search applied to the
    problem of finding the location of a specific number in a sorted array.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The curse of dimensionality refers to the exponential cost of many procedures
    in ML with respect to the dimensionality of an object of interest. As the number
    of dimensions increases, exponentially more time is needed to finish the procedure.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In BayesOpt, while entropy search may be applied to the problem of finding the
    location of the function optimizer, it is computationally expensive due to the
    curse of dimensionality.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To overcome the curse of dimensionality, we modify our goal of finding the function’s
    optimized value, making it a one-dimensional search problem. The resulting BayesOpt
    policy is called Max-value Entropy Search (MES).
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to the complex behavior of the global optimum of a function modeled by a
    GP, it is infeasible to compute the acquisition scores of MES in closed form.
    However, we can draw samples from probability distributions to approximate the
    acquisition scores.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing MES in BoTorch follows the same procedure as implementing other
    BayesOpt policies. To facilitate the sampling procedure in the acquisition score
    approximation, we use a Sobol sequence when initializing the policy object.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
