- en: 6 Using information theory with entropy-based policies
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用基于熵的策略和信息论的知识
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容包括
- en: Entropy as an information-theoretic measure of uncertainty
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为衡量不确定性的信息论量度的熵
- en: Information gain as a method of reducing entropy
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过信息增益减少熵的方法
- en: BayesOpt policies that use information theory for their search
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用信息理论进行搜索的 BayesOpt 策略
- en: We saw in chapter 4 that by aiming to improve from the best value achieved so
    far, we can design improvement-based BayesOpt policies, such as Probability of
    Improvement (POI) and Expected Improvement (EI). In chapter 5, we used multi-armed
    bandit (MAB) policies to obtain Upper Confidence Bound (UCB) and Thompson sampling
    (TS), each of which uses a unique heuristic to balance exploration and exploitation
    in the search for the global optimum of the objective function.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第 4 章中看到，通过力争从迄今为止取得的最佳值改进，我们可以设计基于改进的 BayesOpt 策略，如改进概率 (POI) 和期望改进 (EI)。在第
    5 章中，我们使用多臂老虎机 (MAB) 策略获得了上限置信度 (UCB) 和汤普森抽样 (TS)，每种策略都使用独特的启发式方法来平衡在搜索目标函数全局最优解时的探索和开发。
- en: In this chapter, we learn about another heuristic to decision-making, this time
    using information theory to design BayesOpt policies we can use in our optimization
    pipeline. Unlike the heuristics we have seen (seeking improvement, optimism in
    the face of uncertainty, and random sampling), which might seem unique to optimization-related
    tasks, information theory is a major subfield of mathematics that has applications
    in a wide range of topics. As we discuss in this chapter, by appealing to information
    theory or, more specifically, *entropy*, a quantity that measures uncertainty
    in terms of information, we can design BayesOpt policies that seek to reduce our
    uncertainty about the objective function to be optimized in a principled and mathematically
    elegant manner.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了另一种启发式决策方法，这次是利用信息理论来设计我们可以在优化流程中使用的 BayesOpt 策略。与我们所见过的启发式方法（寻求改进、面对不确定性的乐观和随机抽样）不同，这些方法可能看起来独特于与优化相关的任务，信息理论是数学的一个主要子领域，其应用涵盖广泛的主题。正如我们在本章中讨论的，通过诉诸信息理论或更具体地说是*熵*，一种以信息量衡量不确定性的量，我们可以设计出以一种有原则和数学上优雅的方式来减少我们对待优化的目标函数不确定性的
    BayesOpt 策略。
- en: 'The idea behind entropy-based search is quite simple: we look at places where
    our information about a quantity of interest will most increase. As we cover later
    in the chapter, this is similar to looking for a lost remote control in the living
    room, where the TV is, as opposed to in the bathroom.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 基于熵的搜索背后的想法非常简单：我们看看我们关心的数量的信息将最大增加的地方。正如我们在本章后面所讨论的，这类似于在客厅寻找遥控器，而不是在浴室里。
- en: The first part of this chapter is a high-level exposition on information theory,
    entropy, and ways to maximize the amount of information we receive upon performing
    an action. This is done by reinterpreting the familiar example of binary search.
    Armed with the fundamentals of information theory, we then move on to discussing
    BayesOpt policies that maximize information about the global optimum of an objective
    function. These policies are the result of applying information theory to the
    task of BayesOpt. As always, we also learn how to implement these policies in
    Python.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的第一部分是对信息理论、熵以及在执行动作时最大化我们所接收的信息量的方法的高层级阐述。这是通过重新解释熟悉的二分查找示例来完成的。具备信息理论的基础知识后，我们继续讨论最大化关于目标函数全局最优解的信息的
    BayesOpt 策略。这些策略是将信息理论应用于 BayesOpt 任务的结果。与往常一样，我们还学习如何在 Python 中实现这些策略。
- en: By the end of the chapter, you will gain a working understanding of what information
    theory is, how entropy as a measure of uncertainty is quantified, and how entropy
    is translated to BayesOpt. This chapter adds another policy to our BayesOpt toolkit
    and concludes the second part of the book on BayesOpt policies.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '在本章结束时，你将对信息理论是什么、熵作为不确定性度量是如何量化的以及熵如何转化为 BayesOpt 有一个工作理解。本章为我们的 BayesOpt
    工具包增加了另一个策略，并结束了关于 BayesOpt 策略的本书第二部分。 '
- en: 6.1 Measuring knowledge with information theory
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用信息论测量知识
- en: '*Information theory* is a subfield of mathematics that studies how to best
    represent, quantify, and reason about information in a principled, mathematical
    manner. In this section, we introduce information theory on a high level and discuss
    how it is related to decision-making under uncertainty. We do this by reexamining
    the idea behind binary search, a popular algorithm in computer science, from the
    perspective of information theory. This discussion subsequently allows us to connect
    information theory to BayesOpt and motivates an information-theoretic policy for
    optimization.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*信息论*是数学的一个子领域，研究如何以原则性和数学性的方式最佳表示、量化和推理信息。在本节中，我们从信息论的角度重新审视了二分查找的思想，这是计算机科学中一个常用的算法。这次讨论随后允许我们将信息论与BayesOpt相连接，并为优化问题提出信息论策略。'
- en: 6.1.1 Measuring uncertainty with entropy
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.1 使用熵来度量不确定性
- en: Information theory is especially prevalent in computer science, where digital
    information is represented as bits (0s and 1s). You might remember calculating
    how many bits are required to represent a given number of integers—for example,
    a single bit is enough to represent two numbers, 0 and 1, while five bits are
    necessary to represent 32 (2 raised to the fifth power) different numbers. These
    calculations are examples of information theory in practice.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 信息论在计算机科学中特别常见，其中数字信息被表示为二进制（0和1）。你可能还记得计算表示给定数量所需的比特数的例子，例如，一个比特足以表示两个数字，0和1，而五个比特则需要表示32个（2的五次方）不同数字。这些计算是信息论在实践中的例子。
- en: '![](../../OEBPS/Images/06-00-unnumb-1.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/06-00-unnumb-1.png)'
- en: The information-theoretic concept of interest within the context of decision-making
    under uncertainty is *entropy*. Entropy measures the level of uncertainty we have
    in an unknown quantity. If this unknown quantity is modeled as a random variable,
    entropy measures the variability in the possible values of the random variable.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 决策不确定性下，信息论中的重要概念是*熵*。熵度量了我们对未知数量的不确定程度。如果将这个未知数量建模为随机变量，熵度量的是随机变量可能取值的变异性。
- en: Note This uncertainty measure, entropy, is similar to but not quite the same
    as what we’ve been calling *uncertainty* in the predictions made by a GP thus
    far, which is simply the standard deviation of the predictive distribution.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '注意: 这个不确定性度量，熵，与迄今为止我们所称的高斯过程预测中的不确定性有点相似，后者简单地是预测分布的标准差。'
- en: In this subsection, we learn more about entropy as a concept and how it is computed
    for a simple Bernoulli distribution for binary events. We show how entropy successfully
    quantifies uncertainty in an unknown quantity.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将进一步了解熵作为一个概念以及如何计算二元事件的伯努利分布的熵。我们展示熵如何成功地量化了对未知数量的不确定性。
- en: 'Let’s go back to the first example of any Introduction to Probability course:
    coin flipping. Say you are about to flip a biased coin that will land on heads
    with some probability *p* between 0 and 1, and you’d like to reason about the
    event that the coin will, indeed, land on heads. Denote *X* as the binary random
    variable that indicates whether the event happens (that is, *X* = 1 if the coin
    lands on heads and *X* = 0 otherwise). Then, we say that *X* follows a Bernoulli
    distribution with parameter *p*, and the probability that *X* = 1 is equal to
    *p*.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到第一个概率论课的例子：抛硬币。假设你准备抛一枚有偏差的硬币，硬币以概率*p*（介于0和1之间）正面朝上，你想要推理这个硬币正面朝上的事件。用二进制随机变量*X*表示这个事件是否发生（即，如果硬币正面朝上，*X*
    = 1，否则*X* = 0）。那么，我们说*X*符合参数为*p*的伯努利分布，并且*X* = 1的概率等于*p*。
- en: '![](../../OEBPS/Images/06-00-unnumb-2.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/06-00-unnumb-2.png)'
- en: 'Here, the entropy of *X* is defined as –*p* log *p* – (1 – *p*) log(1 – *p*),
    where *log* is the logarithmic function in base 2\. We see that this is a function
    of the probability of heads *p*. Figure 6.1 shows the function of entropy for
    *p* in (0, 1), from which we can gain some insights:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*X*的熵定义为–*p* log *p* – (1 – *p*) log(1 – *p*)，其中*log*是以2为底的对数函数。我们看到这是一个关于硬币正面概率*p*的函数。图6.1展示了*p*在(0,
    1)区间内熵函数的形状，从中我们可以得出一些见解：
- en: The entropy is always nonnegative.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熵始终为非负数。
- en: The entropy increases when *p* increases before 0.5, reaches its highest at
    *p* = 0.5, and then decreases afterwards.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当*p*小于0.5时，**熵**随*p*的增加而增加，在*p* = 0.5时达到最高点，然后逐渐下降。
- en: '![](../../OEBPS/Images/06-01.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/06-01.png)'
- en: Figure 6.1 Entropy of a Bernoulli random variable as a function of the success
    probability. The entropy is maximized (uncertainty is at its highest) when the
    success probability is at 0.5.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 伯努利随机变量的熵作为成功概率的函数。当成功概率为0.5时，熵最大化（不确定性达到最高）。
- en: 'Both insights are valuable when we study our uncertainty about whether or not
    the coin will land on heads. First, we shouldn’t have negative uncertainty about
    something, so it makes sense that the entropy is never negative. More importantly,
    entropy is maximized right at the middle, when *p* = 0.5\. This is quite reasonable:
    as *p* gets farther and farther away from 0.5, we become more certain about the
    outcome of the event—whether the coin will land on heads.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们研究我们对硬币是否落在正面的不确定性时，这两种见解都是宝贵的。首先，我们不应该对某事物有负面不确定性，因此熵永远不是负数。更重要的是，熵在中间处达到最大值，当
    *p* = 0.5 时。这是非常合理的：随着 *p* 离0.5越来越远，我们对事件结果的确定性越来越大—硬币是否落在正面。
- en: For instance, if *p* = 0.7, then we are more certain it will land on heads—our
    entropy is around 0.9 here. If *p* = 0.1, then we are even more certain about
    the outcome (this time that it will land on tails)—the entropy here is roughly
    0.5\. While entropy is not defined at the endpoints (due to the logarithmic function),
    entropy approaches zero as we get closer to either endpoint, indicating zero uncertainty.
    When *p* = 0.5, on the other hand, our uncertainty is at its maximum, as we are
    maximally unsure whether the coin will land on heads or tails. These calculations
    demonstrate that entropy is an appropriate measure of uncertainty.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果 *p* = 0.7，则我们更确定它会落在正面上—这里我们的熵约为0.9。如果 *p* = 0.1，则我们对结果更确定（这次是它会落在反面上）—这里的熵大约是0.5。虽然由于对数函数的原因，熵在端点处未定义，但当我们接近任一端点时，熵接近零，表示零不确定性。另一方面，当
    *p* = 0.5 时，我们的不确定性达到最大值，因为我们对硬币是落正面还是反面最不确定。这些计算表明熵是不确定性的合适度量。
- en: Entropy vs. standard deviation
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 熵 vs. 标准差
- en: When we used the term *uncertainty* in previous chapters, we were referring
    to the standard deviations of the predictive normal distributions produced by
    a GP. The *standard deviation* of a distribution, as the name suggests, measures
    how much the values within the distribution deviate from the mean and is, therefore,
    a valid measure of uncertainty.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在之前的章节中使用术语 *不确定性* 时，我们指的是由GP产生的预测正态分布的标准差。分布的 *标准差* 正如其名称所示，衡量了分布内的值与平均值偏离的程度，因此是一种有效的不确定性度量。
- en: Entropy, on the other hand, is motivated by concepts of information theory,
    and it is also a valid measure of uncertainty. In fact, it is a more elegant and
    general approach to quantify uncertainty and could more accurately model uncertainty
    in edge cases of many situations.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 熵，另一方面，是受到信息理论概念的启发，它也是一种有效的不确定性度量。事实上，它是一种更加优雅和通用的方法来量化不确定性，并且能更准确地模拟许多情况下的边缘情况中的不确定性。
- en: Definition For a given probability distribution, the entropy is defined to be
    –Σ*[i]* *p[i]* log *p[i]*, where we sum over the different possible events indexed
    by *i*.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 对于给定的概率分布，熵被定义为 –Σ*[i]* *p[i]* log *p[i]*，其中我们对不同可能的事件按 *i* 索引进行求和。
- en: We see that what we used for the preceding Bernoulli distribution is a special
    case of this formula. We also use this formula later in the chapter when we work
    with uniform distributions.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到我们用于前述伯努利分布的公式是这个公式的一个特殊情况。我们在本章后面处理均匀分布时也使用了这个公式。
- en: 6.1.2 Looking for a remote control using entropy
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.2 使用熵寻找遥控器
- en: As entropy measures how much uncertainty there is in our knowledge about a quantity
    or event of interest, it can inform our decisions, helping us most efficiently
    reduce our uncertainty about the quantity or event. We look at an example of this
    in this subsection, in which we decide where to best look for a lost remote control.
    Although simple, the example presents the information-theoretic reasoning we use
    in subsequent discussions, where entropy is used for more complex decision-making
    problems.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 由于熵衡量了我们对感兴趣的数量或事件的知识中有多少不确定性，因此它可以指导我们的决策，帮助我们最有效地减少我们对数量或事件的不确定性。我们在本小节中看一个例子，我们在其中决定在哪里最好地寻找丢失的遥控器。虽然简单，但这个例子呈现了我们在后续讨论中使用的信息论推理，在那里熵被用于更复杂的决策问题。
- en: Imagine that while trying to turn on the TV in your living room one day, you
    realize you can’t find the remote control on the table, which is where it usually
    is. You, therefore, decide to conduct a search for this remote. First, you reason
    that it should be in the living room somewhere, but you don’t have any idea about
    where the remote is within the living room, so all locations are equally likely.
    In the language of probabilistic inference that we've been using, you can say
    that the *distribution of the location of the remote* is uniform over the living
    room.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，有一天，当你试图在客厅里打开电视时，你意识到找不到通常放在桌子上的遥控器。因此，你决定对这个遥控器进行搜索。首先，你推理说它应该在客厅的某个地方，但你不知道遥控器在客厅的哪个位置，所以所有的位置都是同样可能的。用我们一直在使用的概率推断的语言来说，你可以说遥控器位置的*分布*在客厅内是均匀的。
- en: '![](../../OEBPS/Images/06-02.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/06-02.png)'
- en: Figure 6.2 A sample floor plan for the example of finding the remote control.
    The living room is uniformly shaded to indicate that the distribution of the location
    of the remote is uniform over the living room.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2是寻找遥控器示例的一个样本平面图。客厅均匀阴影表示遥控器位置在客厅内的分布是均匀的。
- en: 'Figure 6.2 visualizes this belief about the location of the remote control
    that you have, indicated by the shaded living room, which is where the remote
    is (according to your belief). Now, you might ask yourself this question: Where
    in this house should you look for the remote? It’s reasonable to think that you
    should look in the living room, as opposed to, say, the bathroom, because that’s
    where the TV is. But how does one quantifiably justify that choice?'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2可视化了你对遥控器位置的信念，用阴影标示的客厅表示遥控器所在的位置（根据你的信念）。现在，你可能会问自己这个问题：在这个房子里，你应该在哪里找遥控器？认为你应该在客厅里找到遥控器是合理的，而不是在浴室里，因为电视就在那里。但是，如何量化地证明这个选择是正确的呢？
- en: Information theory, specifically entropy, offers a way of doing that by allowing
    us to reason about how much entropy remains after a search for a remote in the
    living room versus in the bathroom. That is, it allows us to determine how much
    uncertainty about the location of the remote we have left after looking in the
    living room as opposed to after looking in the bathroom.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 信息理论，特别是熵，通过允许我们推理在客厅与浴室中寻找遥控器后还剩多少熵来提供了一种方法。也就是说，它允许我们确定在客厅里寻找遥控器后与在浴室里寻找遥控器后我们对遥控器位置的不确定性有多少。
- en: '![](../../OEBPS/Images/06-03.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/06-03.png)'
- en: Figure 6.3 Entropy of the location of the remote after a portion of the living
    room is searched. If the remote is found (upper right), then no uncertainty remains.
    Otherwise, entropy is still reduced (lower right), as the distribution of the
    location of the remote is now narrower.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3显示了在搜索了客厅的一部分后遥控器位置的熵。如果找到遥控器（右上角），则不再存在不确定性。否则，熵仍然会减少（右下角），因为遥控器位置的分布现在更窄了。
- en: 'Figure 6.3 shows how entropy of the location of the remote decreases once the
    upper portion of the living room is searched. We can reason the following:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3显示了一旦搜索了客厅的上部分，遥控器位置的熵如何减少。我们可以推理如下：
- en: If the remote is found within the searched region, then you will no longer have
    any uncertainty about its location. In other words, the entropy will be zero.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果在搜索的区域内找到遥控器，那么你将不再对其位置有任何不确定性。换句话说，熵将为零。
- en: If the remote is not found, then our posterior belief about the location of
    the remote is updated to the shaded region in the lower-right section. This distribution
    spans a smaller region than the one in figure 6.2, so there is less uncertainty
    (entropy).
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果没有找到遥控器，那么我们对遥控器位置的后验信念将被更新为右下角阴影区域。这个分布跨越的区域比图6.2中的区域要小，因此不确定性（熵）更小。
- en: 'Either way, the entropy is reduced by looking in the specified portion of the
    living room. What would happen, then, if you decided to search for the remote
    in the bathroom? Figure 6.4 shows the corresponding reasoning:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 无论哪种方式，查找客厅指定部分的内容都会减少熵。那么，如果你决定在浴室里寻找遥控器会发生什么呢？图6.4显示了相应的推理：
- en: If the remote is found in the bathroom, then entropy will still drop to zero.
    However, this is unlikely to happen, according to your belief about the location
    of the remote.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果在浴室里找到了遥控器，那么熵仍然会降到零。然而，根据你对遥控器位置的信念，这种情况不太可能发生。
- en: If the remote is not found in the bathroom, then your posterior belief about
    the location of the remote doesn’t change from figure 6.2, and the resulting entropy
    remains the same.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果遥控器在浴室中找不到，那么你对遥控器位置的后验信念不会从图6.2中改变，结果熵保持不变。
- en: '![](../../OEBPS/Images/06-04.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/06-04.png)'
- en: Figure 6.4 Entropy of the location of the remote after the bathroom is searched.
    As the remote cannot be found in the bathroom, the entropy in the posterior distribution
    of the location of the remote is unchanged.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 在搜索完浴室后，遥控器位置的熵。由于遥控器在浴室中找不到，遥控器位置的后验分布中的熵不变。
- en: Searching for and not finding the remote in the bathroom doesn’t reduce the
    entropy of the location of the remote. In other words, looking in the bathroom
    doesn’t provide any extra information about the location of the remote, so it
    is the suboptimal decision according to information theory.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在浴室里搜索而没有找到遥控器并不会减少遥控器位置的熵。换句话说，在浴室里寻找并不提供有关遥控器位置的任何额外信息，因此根据信息论，这是次优的决定。
- en: 'This comparison would not be as cut and dried if the prior distribution of
    the location of the remote (your initial guess about where it is) was over the
    entire house, not just the living room. After all, there is always a small probability
    that the remote got misplaced outside the living room. However, the procedure
    of determining where to look—that is, the portion of the house that will give
    you maximal information about the location of the remote—is still the same:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果遥控器位置的先验分布（关于它在哪里的你的初始猜测）涵盖整个房子而不仅仅是客厅，那么这种比较就不会那么明显了。毕竟，遥控器被误放在客厅外的概率总是很小的。然而，决定在哪里寻找的过程——即能够为你提供有关遥控器位置最大信息的房子部分——仍然是相同的：
- en: Consider the posterior distribution of the location of the remote if it is found,
    and compute the entropy of that distribution.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑如果找到了遥控器的话，遥控器位置的后验分布，并计算该分布的熵。
- en: Compute the same entropy if the remote is not found.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算如果遥控器没有找到时的熵。
- en: Compute the average entropy over the two cases.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算两种情况下的平均熵。
- en: Repeat this computation for all locations you are considering looking in, and
    pick the one that gives the lowest entropy.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为考虑寻找的所有位置重复此计算，并选择给出最低熵的位置。
- en: Entropy gives us a way to quantify our uncertainty about a quantity of interest,
    using its probabilistic distribution in an information-theoretic manner. This
    procedure uses entropy to identify the action that maximally reduces entropy.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 熵提供了一种用信息论方法量化我们对感兴趣的数量的不确定性的方法，利用其在概率分布中的信息。此过程使用熵来识别最大程度减少熵的行动。
- en: Note This is a mathematically elegant procedure applicable to many decision-making
    situations under uncertainty. We can think of this entropy-based search procedure
    as a kind of search for the truth in which we aim to take the action that takes
    us as close to the truth as possible by maximally reducing uncertainty.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 这是一个在许多不确定性下决策情况下适用的数学上优雅的过程。我们可以将这种基于熵的搜索过程看作是一种搜索真相的过程，我们的目标是通过最大程度地减少不确定性来尽可能地接近真相。
- en: 6.1.3 Binary search using entropy
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.3 利用熵进行二进制搜索
- en: 'To further understand entropy-based search, we now see how this procedure manifests
    itself in one of the classic algorithms in computer science: binary search. You
    are most likely already familiar with this algorithm, so we don’t go into much
    detail here. For an excellent and beginner-friendly explanation of binary search,
    I recommend chapter 1 of Aditya Bhargava’s *Grokking Algorithms* (Manning, 2016).
    On a high level, we employ binary search when we want to look for the position
    of a specific targeted number within a sorted list such that the elements in the
    list are increasing from the first to the last element.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步理解基于熵的搜索，我们现在看看这个过程如何在计算机科学中的经典算法之一：二分查找中体现。您很可能已经熟悉这个算法，所以我们在这里不会详细介绍。对于对二分查找有很好且适合初学者的解释，我推荐阅读
    Aditya Bhargava 的《Grokking Algorithms》（Manning，2016）的第1章。从高层次上来说，当我们想要在一个排序列表中查找特定目标数字的位置，使得列表中的元素从第一个到最后一个元素递增时，我们使用二分查找。
- en: Tip The idea behind binary search is to look at the middle element of the list
    and compare it to the target. If the target is smaller than the middle element,
    then we know to only look at the first half of the list; otherwise, we look at
    the second half. We repeat this process of halving the list until we find the
    target.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 提示 二分搜索的思想是查看列表的中间元素并将其与目标进行比较。 如果目标小于中间元素，则我们只查看列表的前一半；否则，我们查看后一半。 我们重复这个列表减半的过程，直到找到目标。
- en: Consider a concrete example in which we have a sorted list of 100 elements [*x*[1],
    *x*[2], ..., *x*[100]], and we’d like to find the location of a given target *z*,
    assuming *z* is, indeed, in the sorted list.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个具体的例子，我们有一个排序过的100个元素列表 [*x*[1]，*x*[2]，...，*x*[100]]，我们想要找到给定目标 *z* 的位置，假设
    *z* 确实在排序列表中。
- en: '![](../../OEBPS/Images/06-05.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/06-05.png)'
- en: Figure 6.5 An illustration of binary search on a 100-element list. At each iteration
    of the search, the target is compared against the middle element of the current
    list. Depending on the result of this comparison, we remove either the first or
    second half of the list from the search space.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 在100个元素列表上执行二分搜索的示例。 在搜索的每次迭代中，目标被与当前列表的中间元素进行比较。 根据这个比较的结果，我们将第一半或第二半列表从搜索空间中移除。
- en: 'As illustrated by figure 6.5, binary search works by dividing the list into
    two halves: the first 50 elements and the last 50 elements. Since we know the
    list is sorted, we know the following:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 正如图6.5所示，二分搜索通过将列表分为两半进行工作：前50个元素和后50个元素。 由于我们知道列表是排序过的，我们知道以下内容：
- en: If our target *z* is less than the 50th element *x*[50], then we only need to
    consider the first 50 elements, since the last 50 elements are all greater than
    target *z*.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们的目标 *z* 小于第50个元素 *x*[50]，那么我们只需要考虑前50个元素，因为最后50个元素都大于目标 *z*。
- en: If our target is greater than *x*[50], then we only need to look at the second
    half of the list.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们的目标大于 *x*[50]，那么我们只需要查看列表的后一半。
- en: Terminating the search
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 终止搜索
- en: For each comparison in figure 6.5, we omit the situation where *z* is equal
    to the number it’s being compared to, in which case we can simply terminate the
    search.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图6.5中的每次比较，我们忽略*z*等于被比较的数字的情况，这种情况下我们可以简单地终止搜索。
- en: On average, this procedure helps us find the location of *z* within the list
    much more quickly than sequentially searching through the list from one end to
    the other. Binary search is a realization of the goal of making the optimal decision
    based on information theory in this game of searching for the location of a number
    within a sorted list, if we were to tackle the problem from a probabilistic angle.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 平均而言，这个过程可以帮助我们更快地在列表中找到 *z* 的位置，比在列表中顺序搜索从一端到另一端要快得多。 如果我们从概率的角度来处理这个问题，二分搜索是在排序列表中的数字位置搜索游戏中基于信息理论作出最佳决策的实现目标的方法。
- en: Note Binary search strategy is the optimal solution for finding *z*, allowing
    us to locate it more quickly than any other strategy, on average.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 二分搜索策略是寻找 *z* 的最佳解决方案，使我们能够比其他任何策略更快地找到它，平均来看。
- en: First, let’s use the random variable *L* to denote the location of our target
    *z* within the sorted list. Here, we would like to use a distribution to describe
    our belief about the variable. Since from our perspective, any location within
    the list is equally likely to contain the value of *z*, we use a uniform distribution
    for modeling.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们用随机变量 *L* 表示我们目标 *z* 在排序列表中的位置。 这里，我们想要使用一个分布来描述我们对这个变量的信念。 由于从我们的角度来看，列表中的任何位置都同样可能包含
    *z* 的值，我们使用均匀分布进行建模。
- en: Figure 6.6 visualizes this distribution, which, again, represents our belief
    about the location of *z*. Since each location is equally as likely as any other,
    the probability that a given location contains *z* is uniformly 1 ÷ 100, or 1%.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6可视化了这种分布，再次表示我们对 *z* 位置的信念。 由于每个位置都与其他任何位置一样可能，特定位置包含 *z* 的概率是均匀的1 ÷ 100，即1%。
- en: '![](../../OEBPS/Images/06-06.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/06-06.png)'
- en: Figure 6.6 Prior distribution of the location of the target *z* within the 100-element
    list. Since each location is as equally likely as any other, the probability that
    a given location contains *z* is 1%.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 给出了100个元素列表中目标 *z* 位置的先验分布。 由于每个位置一样可能包含 *z*，特定位置包含 *z* 的概率为1%。
- en: Note Let’s try computing the entropy of this uniform distribution. Remember,
    the formula for the entropy is *–Σ[i]* *p[i]* log *p[i]*, where we sum over the
    different possible events indexed by *i*. This is equal to
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 让我们尝试计算这个均匀分布的熵。记住，熵的公式是 *–Σ[i]* *p[i]* log *p[i]*，其中我们对不同可能事件 *i* 进行求和。这等于
- en: '![](../../OEBPS/Images/06-06-Equations_ch-6-1.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/06-06-Equations_ch-6-1.png)'
- en: So, the amount of uncertainty we have in our prior distribution of *L* is roughly
    6.64.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们对 *L* 先验分布的不确定性大约为 6.64。
- en: 'Next, we tackle the same question: How should we search over this 100-element
    list to locate *z* as quickly as possible? We do this by following the entropy
    search procedure described in section 6.1.2, where we aim to minimize the entropy
    in the posterior distribution of the quantity that we care about—in this case,
    the location *L*.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们解决同样的问题：我们应该如何在这个 100 元素列表中搜索以尽快找到 *z*？我们通过遵循第 6.1.2 节描述的熵搜索过程来做到这一点，我们的目标是尽量减少我们关心的量的后验分布的熵，也就是说，在这种情况下是位置
    *L*。
- en: 'How do we compute the entropy of the posterior distribution of *L* after inspecting
    a given location? This calculation requires us to reason about what we can conclude
    about *L* upon inspecting a given location, which is quite easy to do. Say we
    decide to inspect the first location *x*[1]. According to our belief about *L*,
    there is a 1% chance that *L* is at this location and a 99% chance that *L* is
    in one of the remaining slots:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在检查了给定位置后，我们如何计算后验分布 *L* 的熵？这个计算要求我们推理在检查了给定位置后我们对 *L* 可以得出什么结论，这是相当容易做到的。假设我们决定检查第一个位置
    *x*[1]。根据我们对 *L* 的信念，*L* 在这个位置的可能性为 1%，而在其余位置的可能性为 99%：
- en: If *L* is, indeed, at this location, then our posterior entropy about *L* drops
    to 0, as there’s no more uncertainty about this quantity.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 *L* 确实在这个位置，那么关于 *L* 的后验熵将降为 0，因为对于这个量再也没有不确定性了。
- en: Otherwise, the distribution of *L* is updated to reflect this observation that
    *z* is not the first number of the list.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 否则，*L* 的分布将更新以反映出观察到 *z* 不是列表的第一个数字这一事实。
- en: Figure 6.7 shows this process as a diagram in which we need to update the distribution
    for *L* so that each of the 99 locations has a 1 ÷ 99, or roughly 1.01%, probability
    of containing *z*. Each of the 99 locations is still equally likely, but the probability
    of each location has gone up a bit since we have ruled out the first location
    in this hypothetical scenario.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7 将这个过程显示为一个图表，我们需要更新 *L* 的分布，以便每个位置都有 1 ÷ 99，或大约 1.01% 的概率包含 *z*。每个位置仍然同样可能，但每个位置的概率稍微增加了一些，因为在这种假设的情况下，我们已经排除了第一个位置。
- en: '![](../../OEBPS/Images/06-07.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/06-07.png)'
- en: Figure 6.7 Posterior distributions of the location of target *z* within the
    100-element list upon inspecting the first element. In each scenario, the probability
    that *z* is at a given location is updated accordingly.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7 给出了在检查第一个元素后目标 *z* 在 100 元素列表中位置的后验分布。在每种情景中，*z* 在给定位置的概率会相应更新。
- en: Note Again, we’re only considering the case where *z* exists in the list, so
    either the smallest element in the list *x*[1] is equal to *z*, or the former
    is less than the latter.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 再次说明，我们只考虑 *z* 存在于列表中的情况，所以要么列表中最小的元素 *x*[1] 等于 *z*，要么前者小于后者。
- en: Following the same calculation, we can obtain the entropy for this new distribution
    as
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 按照同样的计算，我们可以得到这个新分布的熵为
- en: '![](../../OEBPS/Images/06-07-Equations_ch-6-2.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/06-07-Equations_ch-6-2.png)'
- en: 'Again, this is the posterior entropy of *L* in the second case, where *z* is
    not present in the first location. The last step we need to take to compute the
    overall posterior entropy after inspecting the first location is to take the average
    of the two cases:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 再次说明，这是在第二种情况下 *z* 不在第一个位置的情况下 *L* 的后验熵。我们需要采取的最后一步来计算在检查第一个位置后的总后验熵是取这两种情况的平均值：
- en: If *z* is in the first slot, which is 1% likely, then the posterior entropy
    is 0.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 *z* 在第一个位置，这个可能性是 1%，那么后验熵为 0。
- en: If *z* is not in the first slot, which is 99% likely, then the posterior entropy
    is 6.63.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 *z* 不在第一个位置，这个可能性是 99%，那么后验熵为 6.63。
- en: Taking the average, we have 0.01 (0) + 0.99 (6.63) = 6.56\. So, on average,
    we expect to see a posterior entropy of 6.56 when we choose to look into the first
    element of the array. Now, to determine whether looking at the first element is
    the optimal decision or there’s a better location for obtaining more information
    about *L*, we need to repeat this procedure for the other locations in the list.
    Specifically, for a given location, we need to
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 取平均值，我们有0.01（0）+0.99（6.63）=6.56。因此，平均而言，当我们选择查看数组的第一个元素时，我们期望看到的后验熵为6.56。现在，为了确定是否查看第一个元素是最佳决定，或者是否有更好的位置可以获取更多关于*L*的信息，我们需要重复此过程以检查列表中的其他位置。具体而言，对于给定的位置，我们需要
- en: Iterate over each of the potential scenarios while inspecting the location
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在检查位置时迭代每个潜在的情况
- en: Compute the posterior entropy of the distribution of *L* for each scenario
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个场景中*L*的后验熵
- en: Compute the average posterior entropy across the scenarios based on how likely
    each of them is
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于每个场景的可能性来计算跨场景的平均后验熵
- en: 'Let’s do this one more time for, say, the 10th location *x*[10]; the corresponding
    diagram is shown in figure 6.8\. While this scenario is slightly different from
    what we just went over, the underlying idea is still the same. First, there are
    various scenarios that can take place when we look at *x*[10]:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次为第10个位置*x*[10]做一次；相应的图示在图6.8中显示。虽然这种情况与我们刚刚讨论的稍有不同，但基本思想仍然相同。首先，当我们查看*x*[10]时，可能会出现各种情况：
- en: The 10th element *x*[10] can be greater than *z*, in which case we can rule
    out the last 91 elements in the list and focus our search on the first 9 elements.
    Here, each of the 9 locations has an 11% chance of containing *z*, and the posterior
    entropy, by using the same formula, can be computed to be roughly 3.17.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第10个元素*x*[10]可能大于*z*，在这种情况下，我们可以排除列表中的最后91个元素，并将搜索集中在前9个元素上。在这里，每个位置都有11%的机会包含*z*，并且通过使用相同的公式，可以计算后验熵约为3.17。
- en: The tenth element *x*[10] can be exactly equal to *z*, in which case our posterior
    entropy is once again zero.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第十个元素*x*[10]可能正好等于*z*，在这种情况下，我们的后验熵再次为零。
- en: The tenth element *x*[10] can be less than *z*, in which case we narrow our
    search to the last 90 elements. The posterior entropy in this case is around 6.49.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第十个元素*x*[10]可能小于*z*，在这种情况下，我们将搜索范围缩小到最后90个元素。在这种情况下，后验熵约为6.49。
- en: '![](../../OEBPS/Images/06-08.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/06-08.png)'
- en: Figure 6.8 Posterior distributions of the location of target *z* within the
    100-element list upon inspecting the 10th element. In each scenario, the probability
    that *z* is at a given location is updated accordingly.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在检查第10个元素时，目标*z*在100个元素列表中的后验分布。在每种情况下，*z*在给定位置的概率会相应更新。
- en: Note Make sure you attempt the entropy computations yourself to understand how
    we are getting these numbers.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 注意确保自己尝试熵计算，以了解我们是如何得到这些数字的。
- en: 'Finally, we take the weighted average of these entropies using the corresponding
    probabilities: 0.09 (3.17) + 0.01 (0) + 0.9 (6.49) = 6.13\. This number presents
    the expected posterior entropy—that is, the expected posterior uncertainty we
    have about *L*, the location of *z*, after inspecting the 10th element *x*[10].'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用相应的概率对这些熵进行加权平均：0.09（3.17）+0.01（0）+0.9（6.49）=6.13。这个数字表示了预期的后验熵——即，在检查第10个元素*x*[10]后，我们对*L*的位置*z*的预期后验不确定性。
- en: Compared to the same number for the first element *x*[1], 6.56, we conclude
    that looking at *x*[10] on average gives us more information about *L* than looking
    at *x*[1]. In other words, inspecting *x*[10] is the better decision in terms
    of information theory.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 与第一个元素*x*[1]的相同数量，6.56，相比，我们得出结论：平均而言，查看*x*[10]比查看*x*[1]给我们更多关于*L*的信息。换句话说，从信息理论的角度来看，检查*x*[10]是更好的决定。
- en: But what is the *optimal* decision in terms of information theory—the one that
    gives us the most information about *L*? To determine this, we simply repeat the
    computation we just performed on *x*[1] and *x*[10] for the other locations in
    the list and pick out the one with the lowest expected posterior entropy.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 但从信息理论的角度来看，什么是最佳决定——哪个给我们*L*的最多信息？为了确定这一点，我们只需为列表中的其他位置重复刚刚对*x*[1]和*x*[10]执行的计算，并挑选出具有最低预期后验熵的位置。
- en: 'Figure 6.9 shows this quantity, the expected posterior entropy in the location
    of our target, as a function of the location we choose to inspect. The first thing
    we notice is the symmetry of the curve: looking at the last location gives us
    the same expected posterior entropy (uncertainty) as looking at the first; similarly,
    the 10th and the 90th locations give the same amount of information, and so on.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9展示了我们所寻找目标的预期后验熵随我们选择的检查位置而变化的情况。我们首先注意到曲线的对称性：只看最后一个位置与只看第一个位置的预期后验熵（不确定性）是相等的；同样地，第10个位置和第90个位置给我们提供的信息是一样的。
- en: '![](../../OEBPS/Images/06-09.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/06-09.png)'
- en: Figure 6.9 Expected posterior entropy in the location of the target as a function
    of the location within the list to inspect. The middle location is optimal, minimizing
    the expected entropy.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9展示了随着检查列表中位置的变化，目标的预期后验熵如何变化。检查中间位置是最优的，可以最小化预期熵。
- en: More importantly, we see that inspecting the locations in the middle, either
    the 50th or 51st number in the list, gives us the maximal amount of information.
    This is because once we do, we are guaranteed to rule out *half* of the list,
    regardless of whether our target number is greater or less than the number in
    the middle. This is not the case for other locations. As we saw earlier, we may
    be able to rule out 90 numbers in the list when looking at the 10th number, but
    this only happens with 0.1 probability. When looking at the first number, there’s
    a 99% probability that we will only be able to rule out one number.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 更为重要的是，我们可以看到只有检查中间位置，也就是列表的第50个或第51个位置，才能提供最大的信息量。这是因为，一旦我们这样做了，无论我们的目标数字大于还是小于中间数字，我们都能排除一半的列表。但对于其他位置并非如此。如前所述，当我们检查第10个数字时，可能能排除列表中的90个数字，但这只有0.1的概率。而当我们检查第一个数字时，99%的概率是只能排除一个数字。
- en: Note All in all, inspecting the numbers in the middle maximizes the amount of
    information we gain, on average.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在平均上，只检查中间的数字最大化我们获得的信息。
- en: 'The rest of the search for the target follows the same procedure: computing
    the expected posterior entropy that will result from each decision and then choosing
    the decision that minimizes that entropy. As the probability distribution we work
    with is always a uniform distribution after each update, the optimal number to
    inspect is always the one in the middle of the array that hasn’t been ruled out.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找目标的其余流程遵循同样的程序：计算每个决策可能导致的预期后验熵，然后选择使得后验熵最小的决策。由于每次更新后我们所处理的概率分布总是一个均匀分布，因此最优的检查位置总是还没有被排除的中间位置。
- en: This is exactly the strategy of binary search! From the perspective of information
    theory, binary search is the optimal solution to the problem of search for a number
    in a sorted list.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是二分查找的策略！从信息论角度来看，二分查找是在有序列表中查找数字的最优解决方案。
- en: Justifying binary search with information theory
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 用信息论证明二分查找
- en: When I first learned about this algorithm, I remember thinking that the strategy
    of searching in the middle of an array seemed, although reasonable, quite unique
    and “out of nowhere.” However, we have just learned to derive the same solution
    from an information-theoretic perspective, which concretely quantifies the idea
    of ruling out half of our search space in the service of gaining as much information
    as possible or reducing as much entropy as possible.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 当我第一次学习这个算法时，我记得曾经认为在数组中间搜索的策略似乎是独特的且来源神秘。然而，我们刚刚从信息理论的角度推导出了相同的解决方案，这明确量化了在服务于获得尽可能多的信息或减少尽可能多的熵的目的下，排除一半的搜索空间的想法。
- en: 'The application of information theory and entropy doesn’t stop at binary search.
    As we saw, the procedure we went through is generalizable to other decision-making
    problems: if we can model the problem of interest in terms of probability distributions
    for unknown quantities, actions we can take, and how the distributions can be
    updated when an action is taken, then we can once again choose the optimal action
    in terms of information theory, which is the action that reduces our uncertainty
    about the quantity of interest the most. For the remainder of this chapter, we
    learn to apply this idea to BayesOpt and implement the resulting entropy search
    policy with BoTorch.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 信息论和熵的应用不仅限于二分搜索。正如我们所见，我们经历的过程可以推广到其他决策问题：如果我们能够用概率分布来建模我们感兴趣的问题，包括未知数量、我们可以采取的行动以及在采取行动时如何更新分布，那么我们可以再次选择在信息理论的框架下的最优行动，这是能够最大程度减少我们对感兴趣的数量的不确定性的行动。在本章的剩余部分，我们将学习如何将这个想法应用到BayesOpt，并使用BoTorch实现由此产生的熵搜索策略。
- en: 6.2 Entropy search in BayesOpt
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 BayesOpt中的熵搜索
- en: Taking the same approach presented in the previous section, we obtain entropy
    search policies in BayesOpt. The main idea is to choose our actions, our experiments,
    so that we can reduce the most amount of entropy in the posterior distribution
    of what we care about. In this section, we first discuss how to do this on a high
    level and then move on to implementation in BoTorch.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 采用前一节提出的相同方法，我们在BayesOpt中获得了熵搜索策略。主要思想是选择我们的行动，我们的实验，以便我们可以在我们关心的后验分布中减少最大数量的熵。在本节中，我们首先讨论如何在高层次上做到这一点，然后转移到在BoTorch中实现。
- en: 6.2.1 Searching for the optimum using information theory
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.1 使用信息理论搜索最优解
- en: 'In our remote control example, we aim to search for the remote within an apartment
    and, thus, want to reduce the entropy of the distribution of the location of the
    remote. In binary search, the process is similar: our target is the location of
    a specific number we’d like to search for within a list, and we want to reduce
    the entropy of the distribution of that number. Now, to design an entropy search
    policy, we must determine what to use as our target in BayesOpt and how to use
    information theory to aid the search process, which we learn how to do here.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的遥控器示例中，我们的目标是在公寓内搜索遥控器，因此希望减少遥控器位置分布的熵。在二分搜索中，过程类似：我们的目标是在列表中搜索我们想要搜索的特定数字的位置，并且我们希望减少该数字的分布的熵。现在，要设计一个熵搜索策略，我们必须确定在BayesOpt中使用什么作为我们的目标以及如何使用信息理论来辅助搜索过程，这是我们在这里要学习的。
- en: 'Recall our ultimate goal in using BayesOpt: to search within the domain *D*
    of a black box function at the location where the function is maximized. This
    means a natural search target for us is the location *x** that maximizes the objective
    value of the function *f*. That is, *f * = f(x*)* ≥ *f(x)*, for all *x* in *D*.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下我们在使用BayesOpt时的最终目标：在黑盒函数的定义域*D*内搜索函数最大化的位置。这意味着我们的自然搜索目标是使函数的目标值最大化的位置*x*，即*f*
    = f(x*)* ≥ *f(x)*，对于*D*中的所有*x*。
- en: Definition The optimal location *x** is often called the *optimizer* of the
    objective function *f*.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 最优位置*x*通常称为目标函数*f*的*优化器*。
- en: 'Given a GP belief about the objective function *f*, there is a corresponding
    probabilistic belief about the optimizer *x**, which is treated as a random variable.
    Figure 6.10 shows an example of a trained GP and the distribution of the objective
    optimizer *x** induced from the GP. It’s important to keep a few interesting characteristics
    of this distribution in mind:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 给定对目标函数*f*的GP信念，存在对优化器*x*的相应概率信念，它被视为随机变量。图6.10显示了一个经过训练的GP的示例以及从GP中导出的目标优化器*x*的分布。需要牢记这个分布的一些有趣特征：
- en: The distribution is complicated and multimodal (having several local optima).
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布复杂且多峰（具有几个局部最优解）。
- en: The most likely location for the optimizer *x** is a bit to the left of zero.
    This is where the predictive mean of the GP is maximized.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化器*x*最有可能的位置在零的左边一点。这是GP的预测均值最大化的地方。
- en: There is non-negligible probability that the optimizer *x** is at an end point
    –5 or 5.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化器*x*位于端点-5或5的概率不可忽略。
- en: The probability that *x** is around 2 is almost zero, corresponding to the fact
    that we already observed a higher objective value than *f*(2).
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x*大约为2的概率几乎为零，这对应于我们已经观察到的高于*f*(2)的目标值。'
- en: '![](../../OEBPS/Images/06-10.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/06-10.png)'
- en: Figure 6.10 GP belief (top) and the distribution of the function optimizer *x**
    (bottom). The distribution of the optimizer is non-Gaussian and fairly complicated,
    posing a challenge for modeling and decision-making.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10 GP信念（顶部）和函数优化器*x*的分布（底部）。优化器的分布是非高斯的，相当复杂，这对建模和决策提出了挑战。
- en: These characteristics make modeling this distribution of *x** very challenging.
    The most simple method of gauging how this quantity *x** is distributed is to
    simply draw many samples from the GP and record the location at which each sample
    is maximized. This is, in fact, how figure 6.10 is generated.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特征使得建模优化器*x*的分布变得非常具有挑战性。衡量这个量*x*分布的最简单方法就是从GP中简单地抽取许多样本，并记录每个样本被最大化的位置。事实上，这就是图6.10生成的方法。
- en: Making matters worse is the fact that we need exponentially more samples to
    estimate the distribution of *x** when the dimensionality of the objective function
    (the length of the input vector *x* or the number of features each *x* has) increases.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 更糟糕的是，当目标函数的维度（输入向量*x*的长度或每个*x*具有的特征数量）增加时，我们需要指数级别的样本来估计*x*的分布。
- en: Definition This is an instance of the *curse of dimensionality*, which, in ML,
    is often used to refer to the exponential cost of many procedures with respect
    to the dimensionality of an object of interest.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 这是维度诅咒的一个例子，在机器学习中，它经常用来指代与感兴趣对象的维度相关的许多过程的指数成本。
- en: To use entropy search in BayesOpt, we need to model our belief about the location
    of the optimum *x** using a probability distribution. However, we can’t exactly
    model the distribution of the optimizer *x**; instead, we must approximate it
    using samples drawn from a GP. Unfortunately, this process quickly becomes computationally
    expensive as the number of dimensions in the objective function (the length of
    *x*) increases.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 要在BayesOpt中使用熵搜索，我们需要对最优解的位置*x*的信念建模，使用概率分布。然而，我们无法精确地建模优化器*x*的分布；相反，我们必须使用从GP中抽取的样本来近似它。不幸的是，随着目标函数中维度的增加（*x*的长度），这个过程很快就变得计算昂贵起来。
- en: Note There are, in fact, research papers in BayesOpt that seek to search for
    the location of the optimizer *x** using entropy. The resulting policies, however,
    often prove too computationally expensive to run and are not implemented in BoTorch.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 实际上，有研究论文在BayesOpt中寻求使用熵来搜索优化器*x*的位置。然而，由此产生的策略通常运行起来成本过高，且未在BoTorch中实现。
- en: But that doesn’t mean we need to abandon the effort of using information theory
    in BayesOpt altogether. It just means we need to modify our search procedure to
    make it more amenable to computational methodologies. One easy way to achieve
    this is to target a quantity other than the optimizer *x** that is, on the one
    hand, connected to the search for *x** and, on the other, easier to reason about.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 但这并不意味着我们需要完全放弃在BayesOpt中使用信息理论的努力。这只意味着我们需要修改我们的搜索过程，使其更适合计算方法。一个简单的方法是针对除了优化器*x*之外的其他量，一方面与寻找*x*相关联，另一方面更容易进行推理。
- en: A quantity of interest in optimization, other than the optimizer *x**, is the
    optimal value *f** = *f*(*x**), achieved at the optimizer, which is also a random
    variable, according to our GP belief about the objective function *f*. As one
    can imagine, learning about the optimal value *f** might tell us a lot about the
    optimizer *x**; that is, the two quantities are connected in terms of information
    theory. However, the optimal value *f** is much easier to work with than the optimizer
    *x**, as the former is just a real-valued number, while the latter is a vector
    of length equal to the number of dimensions of the objective function.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在优化中的一个感兴趣的量，除了优化器*x*之外，就是在优化器处实现的最优值*f* = *f*(*x*)，它也是一个随机变量，根据我们对目标函数*f*的GP信念而定。正如大家可以想象的那样，了解最优值*f*可能会告诉我们很多关于优化器*x*的信息；也就是说，这两个量在信息理论上是相关联的。然而，最优值*f*比优化器*x*更容易处理，因为前者只是一个实数值，而后者是一个长度等于目标函数维度的向量。
- en: An example of the distribution of the optimal value *f** induced by a GP is
    shown in figure 6.11, on the right panel. We see that that this distribution is
    roughly truncated around 1.6, which is exactly the value of the incumbent in our
    training dataset; this makes sense, as the optimal value *f** must be at least
    the incumbent value 1.6.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11右侧面板显示了由GP诱导的最优值*f**的分布示例。我们看到，这个分布大致截断在1.6左右，这恰好是我们训练数据集中的现任值；这是有道理的，因为最优值*f**必须至少是现任值1.6。
- en: '![](../../OEBPS/Images/06-11.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/06-11.png)'
- en: Figure 6.11 The GP belief (top left), the distribution of the function optimizer
    *x** (bottom), and the distribution of the optimal value *f** (right). The distribution
    of the optimal value is always one-dimensional and, therefore, easier to work
    with than that of the optimizer.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11 GP信念（左上）、函数优化器*x**的分布（底部）以及最优值*f**的分布（右）。最优值的分布始终是一维的，因此比优化器的分布更容易处理。
- en: Note The main advantage of focusing our effort on this distribution of *f**
    is the fact that the distribution is always one-dimensional, regardless of the
    dimensionality of the objective function.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 将我们的努力集中在*f**的分布上的主要优势在于，不管目标函数的维度如何，该分布始终是一维的。
- en: 'Specifically, we can draw samples from this one-dimensional distribution to
    approximate the expected posterior entropy upon making a query. From this point,
    we follow the same idea behind entropy search: choosing the query that (approximately)
    minimizes the expected posterior entropy or, in other words, maximizes the expected
    *reduction* in entropy.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们可以从这个一维分布中抽样来近似查询后验熵的期望。从这一点出发，我们遵循熵搜索背后的相同思想：选择（近似）最小化期望后验熵的查询，换句话说，最大化熵的*减少*。
- en: Definition By using this expected-reduction-in-entropy quantity as the acquisition
    score, we obtain the *Max-value Entropy Search* (MES) policy. The term *max-value*
    denotes the fact that we are using information theory to search for the max value,
    or the optimal value *f**, of the objective function.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 通过使用这个期望熵减少量作为获取得分，我们得到了*最大值熵搜索*（MES）策略。术语*最大值*表示我们正在使用信息理论来搜索最大值，或目标函数的最优值*f**。
- en: Figure 6.12 shows the MES acquisition score in our running example in the bottom
    panel, where the point around –2 is where we should query next, according to this
    information-theory–based criterion. The MES policy prefers this location because
    it has both a relatively high mean and a high CI, thus balancing exploration and
    exploitation.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12在底部面板显示了我们运行示例中的MES获取得分，根据这个信息理论基础的准则，我们应该在约为-2的位置查询下一个点。MES策略更喜欢这个位置，因为它既有相对较高的均值又有较高的CI，因此平衡了探索和利用。
- en: '![](../../OEBPS/Images/06-12.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/06-12.png)'
- en: Figure 6.12 The GP belief (top left), the distribution of the optimal value
    *f** (right), and the approximate expected reduction in entropy, which is used
    as the acquisition function score (bottom). The distribution of the optimal value
    is always one-dimensional and, therefore, easier to work with than that of the
    optimizer.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12 GP信念（左上）、最优值*f**的分布（右）以及用作获取函数得分的近似期望熵减少量。最优值的分布始终是一维的，因此比优化器的分布更容易处理。
- en: Interestingly, the acquisition landscape here looks somewhat similar to the
    distribution of the optimizer *x** itself, which is shown in figure 6.11, where
    we see that the curve
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，这里的收益情况看起来与优化器*x**的分布有些相似，如图6.11所示，我们看到曲线
- en: Reaches its zenith somewhere in the middle
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在中间某处达到顶峰
- en: Achieves a non-negligible value at the end points
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在端点处达到一个非零值
- en: Bottoms out at 0 around 2, where we know for certain the location is not optimal
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在2附近达到0，我们确切知道该位置不是最佳的
- en: This is an indication that the optimal value *f** is intimately linked to the
    optimizer *x**, and while we lose some information by altering our objective,
    searching for *f** is a good proxy for searching for *x** and is much more computationally
    tractable.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明最优值*f**与优化器*x**密切相关，虽然我们通过改变我们的目标失去了一些信息，但搜索*f**是搜索*x**的一个良好代理，并且计算上更容易处理。
- en: 6.2.2 Implementing entropy search with BoTorch
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.2 使用 BoTorch 实现熵搜索
- en: 'Having discussed the high-level idea behind MES, we are now ready to implement
    it using BoTorch and plug it into our optimization pipeline. The MES policy is
    implemented by the `qMaxValueEntropy` class in `botorch.acquisition.max_value_entropy_
    search` as a PyTorch module, similar to most BayesOpt policies we have seen. When
    initialized, this class takes in two arguments: a GPyTorch GP model and a set
    of points that will be used as samples in the approximation procedure described
    in the previous section.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论了 MES 背后的高层思想后，我们现在准备使用 BoTorch 实现它，并将其插入我们的优化管道中。MES 策略由 `botorch.acquisition.max_value_entropy_search`
    中的 `qMaxValueEntropy` 类实现为一个 PyTorch 模块，类似于我们之前见过的大多数贝叶斯优化策略。当初始化时，这个类接受两个参数：一个
    GPyTorch GP 模型和一组将用作前面部分描述的近似过程中样本的点。
- en: 'While there are many ways that these sample points can be generated, one particular
    way we have learned from section 5.3.2 is using a Sobol sequence, which does a
    better job of covering the targeted space. Overall, the MES policy is implemented
    as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有许多方法可以生成这些样本点，但我们从 5.3.2 节学到的一种特殊方式是使用 Sobol 序列，它更好地覆盖了目标空间。总的来说，MES 策略的实现如下所示：
- en: '[PRE0]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Generates the samples between 0 and 1 using a Sobol sequence
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用 Sobol 序列在 0 和 1 之间生成样本
- en: ❷ Rescales the samples to be inside the domain
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 重新缩放样本以使其位于域内
- en: ❸ Declares the MES policy object
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 声明 MES 策略对象
- en: ❹ Computes the acquisition scores
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 计算获取分数
- en: Here, `num_candidates` is a tunable parameter that sets the number of samples
    you’d like to use in the MES computation. A larger value would mean an approximation
    with higher fidelity, but it would come at a higher computational cost.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`num_candidates` 是一个可调参数，它设置了在 MES 计算中您想要使用的样本数量。较大的值意味着更高保真度的近似，但这将带来更高的计算成本。
- en: Let’s now apply this code to our running problem of optimizing the one-dimensional
    Forrester function, as implemented in the CH06/01 - BayesOpt loop.ipynb notebook.
    We are already familiar with most of this code, so we don’t go into the details
    here.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将此代码应用于我们正在解决的优化一维 Forrester 函数的问题中，该函数在 CH06/01 - BayesOpt loop.ipynb
    笔记本中实现。我们已经熟悉大部分代码，所以我们不在这里详细介绍。
- en: Figure 6.13 shows the progress made by MES for 10 queries, in which the policy
    quickly finds the global optimum of the Forrester after five queries. Interestingly,
    as optimization progresses, we become more and more certain that looking at other
    regions in the search space will not result in any substantial reduction in entropy,
    which helps us stay close to the optimal location.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.13 展示了 MES 对 10 个查询的进展，其中策略在五次查询后迅速找到了 Forrester 函数的全局最优解。有趣的是，随着优化的进行，我们对于查看搜索空间中的其他区域不会导致任何实质性熵减少越来越确信，这有助于我们保持接近最优位置。
- en: '![](../../OEBPS/Images/06-13.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/06-13.png)'
- en: Figure 6.13 Progress made by the MES policy. The policy quickly finds the global
    optimum after five queries.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.13 MES 策略的进展。策略在五次查询后迅速找到全局最优解。
- en: We have seen that information theory gives us a principled, mathematically elegant
    framework for decision-making that revolves around trying to learn as much as
    possible about a quantity of interest. This comes down to reducing the expected
    posterior entropy in the distribution that models the quantity we care about.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到信息理论为我们提供了一个基于数学的、优雅的决策框架，围绕着尽可能多地了解感兴趣的数量。这归结为减少模型我们关心的数量的预期后验熵。
- en: Within BayesOpt, we saw that a direct translation of this procedure poses computational
    challenges in modeling the location of the optimal value of the objective function,
    our primary search target. Instead, we shift our focus to the optimal value of
    the objective function itself, making the computation more tractable. Fortunately,
    all of this math is nicely abstracted away by BoTorch, leaving us with a convenient,
    modular interface we can plug into any optimization problem.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在贝叶斯优化中，我们看到直接将这个过程进行转换在模型化目标函数的最优值的位置方面存在计算挑战，这是我们的主要搜索目标。相反，我们将重点转移到目标函数本身的最优值，使计算更易处理。幸运的是，所有这些数学内容都被
    BoTorch 很好地抽象出来，留下了一个方便的、模块化的接口，我们可以将其插入任何优化问题中。
- en: This is also where the second part of the book on BayesOpt policies comes to
    an end. The last three chapters covered some of the most commonly used heuristics
    to decision-making in BayesOpt and the corresponding policies, ranging from seeking
    improvement from the incumbent to borrowing multi-armed bandit methods and, in
    this chapter, using information theory.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是关于BayesOpt策略的书的第二部分的结束。最后三章涵盖了一些最常用的启发式方法来进行BayesOpt中的决策以及相应的策略，从寻求改进到借用多臂老虎机方法，以及在本章中使用信息理论。
- en: The remainder of the book takes our discussion to the next level by introducing
    special optimization settings that differ from what we have seen so far, where
    we sequentially observe a single data point at each step of the optimization.
    These chapters show that the methods we have learned can be translated to practical
    settings in the real world to accelerate optimization.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的剩余部分将我们的讨论提升到一个新的水平，介绍了与我们迄今所见不同的特殊优化设置，在这些设置中，我们在优化的每一步都会顺序观察一个单独的数据点。这些章节表明了我们学到的方法可以被转化为现实世界中的实际设置，以加速优化。
- en: 6.3 Exercises
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 练习
- en: 'There are two exercises in this chapter:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 本章有两个练习：
- en: The first exercise covers a variant of binary search in which prior information
    can be taken into account when making decisions.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一个练习涵盖了一种二分搜索的变体，在这种搜索中可以考虑先前的信息来做决策。
- en: The second walks us through the process of implementing MES in the hyperparameter
    tuning problem seen in previous chapters.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二个练习讲解了在先前章节中出现的超参数调整问题中实施MES的过程。
- en: '6.3.1 Exercise 1: Incorporating prior knowledge into entropy search'
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.1 练习1：将先验知识纳入熵搜索
- en: We saw in section 6.1.3 that by placing a uniform prior distribution on the
    location of the target within the array, the optimal information-theoretical search
    decision is to cut the array in half. What happens if a uniform distribution doesn’t
    represent your prior belief faithfully and you’d like to use a different distribution?
    This exercise, implemented in the CH06/02 - Exercise 1.ipynb notebook, shows us
    an instance of this as well as how to derive the resulting optimal decision. Solving
    this exercise should help us further appreciate the elegance and flexibility of
    entropy search as a generic decision-making procedure under uncertainty.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第6.1.3节中看到，通过在数组中目标位置上放置均匀先验分布，最优的信息理论搜索决策是将数组切分一半。如果均匀分布不能忠实地表示你的先验信念，你想使用不同的分布会发生什么？这个练习在CH06/02
    - Exercise 1.ipynb笔记本中实现，向我们展示了这个例子以及如何推导出结果的最优决策。解决这个练习应该会帮助我们进一步欣赏熵搜索作为一种在不确定性下的通用决策过程的优雅和灵活性。
- en: 'Imagine the following scenario: you work at a phone manufacturing company in
    the quality control department, and your current project is to stress test the
    robustness of the casing of the company’s newest product. Specifically, your team
    wants to find out from which floor of a 10-story building one can drop the phone
    to the ground and not break it. A few rules apply:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下以下情景：你在一个电话制造公司的质量控制部门工作，你目前的项目是对公司最新产品的外壳的耐用性进行压力测试。具体来说，你的团队想要找出从一个10层楼的建筑的哪一层楼可以将手机扔到地上而不会摔坏。有一些规则适用：
- en: The engineers who made the phone are sure it won’t break if dropped from the
    first floor.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 制造手机的工程师确定如果从一楼扔下手机，它不会摔坏。
- en: If the phone breaks when dropped from a given floor, then it will also break
    when dropped from a higher floor.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果手机从某个楼层掉下来时摔坏了，那么它也会在更高的楼层掉下时摔坏。
- en: 'You are tasked with finding the highest floor from which one can drop the phone
    without it breaking—we denote this unknown floor as *X*—by conducting trials.
    That is, you must drop actual phones from specific floors to determine *X*. The
    question is the following: How should you choose which floors to drop the phones
    from to find *X*? Since the phones are expensive, you need to conduct as few trials
    as possible and would like to use information theory to aid the search:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 你的任务是找出可以从中扔手机而不会摔坏的最高楼层 — 我们将这个未知楼层称为 *X* — 通过进行试验。也就是说，你必须从特定楼层扔真正的手机来确定 *X*。问题是：你应该如何选择从哪些楼层扔手机以找到
    *X*？由于手机很昂贵，你需要尽量少进行试验，并且希望使用信息理论来辅助搜索：
- en: Assume that by taking into account physics and the materials and construction
    of the phone, the engineers have an initial guess regarding which floors might
    be possible.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设通过考虑物理学、手机的材料和构造，工程师们对可能的楼层有一个初始猜测。
- en: 'Specifically, the prior distribution of *X* is exponential in that the probability
    that *X* is equal to a number is inversely exponential with respect to that number:
    *Pr*(*X* = *n*) = 1 / 2^(*n*), for *n* = 1, 2, ..., 9; the probability corresponding
    to the highest (tenth) floor is *Pr*(*X* = 10) = 1 / 2⁹. So the probability that
    *X* = 1 is 50%, and this probability is cut in half as the number increases. This
    probability distribution is visualized in figure 6.14.'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 具体来说，*X* 的先验分布是指数型的，即*X* 等于一个数字的概率与该数字的倒数成指数关系：*Pr*(*X* = *n*) = 1 / 2^(*n*)，对于*n*
    = 1, 2, ..., 9；与最高（第十）层对应的概率是*Pr*(*X* = 10) = 1 / 2⁹。因此，*X* = 1 的概率为 50%，随着数字的增加，这个概率减半。这个概率分布在图
    6.14 中可视化。
- en: '![](../../OEBPS/Images/06-14.png)'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/06-14.png)'
- en: Figure 6.14 The probability that *X* is equal to a number between 1 and 10 (that
    is, the probability that a floor is the highest floor that doesn’t cause the phone
    to break when dropped)
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.14 *X* 等于 1 到 10 之间的数字的概率（即，当手机从高楼掉落时不会摔碎的最高楼层的概率）
- en: Verify that this is a valid probability distribution by proving that the probabilities
    sum to one. That is, prove that *Pr*(*X* = 1) + *Pr*(*X* = 2) + ... + *Pr*(*X*
    = 10) = 1.
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 确认这是一个有效的概率分布，方法是证明概率之和等于一。也就是说，证明*Pr*(*X* = 1) + *Pr*(*X* = 2) + ... + *Pr*(*X*
    = 10) = 1。
- en: Calculate the entropy of this prior distribution using the formula given at
    the end of section 6.1.1.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用第 6.1.1 节末尾给出的公式计算这个先验分布的熵。
- en: Given the prior distribution defined between 1 and 10, what is the probability
    that the phone will break when dropped from the second floor? What is this probability
    for the fifth floor? How about the first floor?
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定在 1 到 10 之间定义的先验分布，从第二层掉落时手机摔碎的概率是多少？第五层呢？第一层呢？
- en: Assume that after observing the result of any trial, the posterior distribution
    for *X* is once again exponential and is defined between the lowest and highest
    possible floors.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设在观察任何试验结果之后，*X* 的后验分布再次是指数型的，并且在最低和最高可能的楼层之间定义。
- en: For example, if you observe that the phone doesn’t break when dropped from the
    fifth floor, then we know that *X* is at least 5 and the posterior distribution
    of *X* is such that *Pr*(*X* = 5) = 1 / 2, *Pr*(*X* = 6) = 1 / 4, ..., *Pr*(*X*
    = 9) = 1 / 32, *Pr*(*X* = 10) = 1 / 32\. If, on the other hand, the phone breaks
    when dropped from the fifth floor, then we know that *X* is at most 4 and the
    posterior distribution is such that *Pr*(*X* = 1) = 1 / 2, *Pr*(*X* = 2) = 1 /
    4, *Pr*(*X* = 3) = 1 / 8, *Pr*(*X* = 4) = 1 / 8\. Figure 6.15 shows these two
    scenarios.
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如，如果观察到手机从五楼掉落时不会摔碎，那么我们知道*X*至少为 5，*X* 的后验分布是这样的，*Pr*(*X* = 5) = 1 / 2，*Pr*(*X*
    = 6) = 1 / 4，...，*Pr*(*X* = 9) = 1 / 32，*Pr*(*X* = 10) = 1 / 32。另一方面，如果手机从五楼掉落时摔碎了，那么我们知道*X*至多为
    4，后验分布是这样的，*Pr*(*X* = 1) = 1 / 2，*Pr*(*X* = 2) = 1 / 4，*Pr*(*X* = 3) = 1 / 8，*Pr*(*X*
    = 4) = 1 / 8。图 6.15 显示了这两种情况。
- en: '![](../../OEBPS/Images/06-15.png)'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/06-15.png)'
- en: Figure 6.15 The posterior probability distributions of *X* in two scenarios
    when the phone is dropped from the fifth floor. Each posterior distribution is
    still exponential.
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.15 当手机从五楼掉落时*X*的两种情况下的后验概率分布。每个后验分布仍然是指数型的。
- en: Compute the entropy of this fictitious posterior distribution in the two cases.
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算这个虚构的后验分布在这两种情况下的熵。
- en: Given the prior distribution, compute the expected posterior entropy after you
    conduct a trial on the fifth floor (that is, after you drop the phone from the
    fifth floor and observe whether it breaks).
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定先验分布，在你对第五层进行一次试验之后（即，你从第五层扔手机并观察是否摔碎），计算期望后验熵。
- en: Compute this expected posterior entropy for other floors. Which floor gives
    the highest reduction in entropy? Is this still the same result as binary search?
    If not, what has changed?
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算其他楼层的预期后验熵。哪一层楼的熵减少最多？这仍然与二分搜索的结果相同吗？如果不是，发生了什么变化？
- en: '6.3.2 Exercise 2: Bayesian optimization for hyperparameter tuning'
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.2 练习 2：贝叶斯优化用于超参数调整
- en: 'This exercise, implemented in the CH06/03 - Exercise 2.ipynb notebook, applies
    BayesOpt to an objective function that simulates the accuracy surface of a support-vector
    machine model in a hyperparameter tuning task. The *x*-axis denotes the value
    of the penalty parameter *C*, while the *y*-axis denotes the value of the RBF
    kernel parameter *γ*. See the exercises in chapters 3 and 4 for more detail. Complete
    the exercise using the following steps:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 此练习在CH06/03 - Exercise 2.ipynb笔记本中实现，将BayesOpt应用于模拟超参数调整任务中支持向量机模型的准确度表面的目标函数。
    *x*轴表示惩罚参数*C*的值，而*y*轴表示RBF核参数*γ*的值。有关更多详细信息，请参阅第3章和第4章的练习。完成以下步骤：
- en: Recreate the BayesOpt loop in the CH05/03 - Exercise 2.ipynb notebook, including
    the outer loop that implements repeated experiments.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在CH05/03 - Exercise 2.ipynb笔记本中重新创建BayesOpt循环，包括实现重复实验的外部循环。
- en: Run the MES policy. Since our objective function is two-dimensional, we should
    increase the size of the Sobol sequence used by MES. For example, you can set
    it at 2,000\. Observe its aggregated performance.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行MES策略。由于我们的目标函数是二维的，我们应该增加MES使用的Sobol序列的大小。例如，您可以将其设置为2,000。观察其聚合性能。
- en: Repeated experiments in BayesOpt
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: BayesOpt中的重复实验
- en: Refer to step 9 of exercise 2 from chapter 4 to see how we can run repeated
    experiments in BayesOpt.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 参考第4章练习2的第9步，看看我们如何在BayesOpt中运行重复实验。
- en: Summary
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Information theory studies the representation, quantification, and transfer
    of information. One of the core concepts in this field is entropy, which quantifies
    our uncertainty about a random variable from the variable’s probability distribution.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信息论研究信息的表示、量化和传输。该领域的核心概念之一是熵，它量化了我们对随机变量的不确定性，根据变量的概率分布。
- en: An entropy search procedure considers the expected reduction in entropy (and
    therefore in uncertainty) of a quantity of interest by taking an action, choosing
    the action that maximizes this reduction. We can apply this generic procedure
    to many decision-making problems under uncertainty.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熵搜索过程考虑通过采取行动减少量化的期望熵（因此减少不确定性）的感兴趣数量。我们可以将这个通用过程应用于许多不确定性下的决策问题。
- en: Binary search may be obtained as the result of entropy search applied to the
    problem of finding the location of a specific number in a sorted array.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二进制搜索可能是将熵搜索应用于在排序数组中找到特定数字位置的问题的结果。
- en: The curse of dimensionality refers to the exponential cost of many procedures
    in ML with respect to the dimensionality of an object of interest. As the number
    of dimensions increases, exponentially more time is needed to finish the procedure.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维度诅咒是指与所关注对象的维度相对应的许多ML过程的指数成本。随着维度的增加，完成该过程所需的时间呈指数增长。
- en: In BayesOpt, while entropy search may be applied to the problem of finding the
    location of the function optimizer, it is computationally expensive due to the
    curse of dimensionality.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在BayesOpt中，虽然熵搜索可以应用于寻找函数优化器位置的问题，但由于维度诅咒，它的计算成本很高。
- en: To overcome the curse of dimensionality, we modify our goal of finding the function’s
    optimized value, making it a one-dimensional search problem. The resulting BayesOpt
    policy is called Max-value Entropy Search (MES).
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了克服维度诅咒，我们修改了找到函数优化值的目标，将其变为一维搜索问题。由此产生的BayesOpt策略称为最大值熵搜索（MES）。
- en: Due to the complex behavior of the global optimum of a function modeled by a
    GP, it is infeasible to compute the acquisition scores of MES in closed form.
    However, we can draw samples from probability distributions to approximate the
    acquisition scores.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于由GP模型的全局最优的复杂行为，计算MES的收购分数的封闭形式是不可行的。但是，我们可以从概率分布中抽取样本来近似获得收购分数。
- en: Implementing MES in BoTorch follows the same procedure as implementing other
    BayesOpt policies. To facilitate the sampling procedure in the acquisition score
    approximation, we use a Sobol sequence when initializing the policy object.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在BoTorch中实现MES遵循与实现其他BayesOpt策略相同的过程。为了促进收购分数近似中的采样过程，我们在初始化策略对象时使用Sobol序列。
