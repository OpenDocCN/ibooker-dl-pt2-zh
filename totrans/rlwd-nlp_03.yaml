- en: 1 Introduction to natural language processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 自然语言处理入门
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容包括
- en: What natural language processing (NLP) is, what it is not, and why it’s such
    an interesting, yet challenging, field
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）是什么，它不是什么，为什么它是一个有趣而具有挑战性的领域
- en: How NLP relates to other fields, including artificial intelligence (AI) and
    machine learning (ML)
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言处理与其他领域的关系，包括人工智能（AI）和机器学习（ML）
- en: What typical NLP applications and tasks are
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 典型的自然语言处理应用程序和任务是什么
- en: How a typical NLP application is developed and structured
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 典型自然语言处理应用程序的开发和结构
- en: This is not an introductory book to machine learning or deep learning. You won’t
    learn how to write neural networks in mathematical terms or how to compute gradients,
    for example. But don’t worry, even if you don’t have any idea what they are. I’ll
    explain those concepts as needed, not mathematically but conceptually. In fact,
    this book contains no mathematical formulae—not a single one. Also, thanks to
    modern deep learning libraries, you don’t really need to understand the math to
    build practical NLP applications. If you are interested in learning the theories
    and the math behind machine learning and deep learning, you can find a number
    of great resources out there.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是一本机器学习或深度学习的入门书籍。你不会学到如何以数学术语编写神经网络，或者如何计算梯度，等等。但是不用担心，即使你对这些概念一无所知，我会在需要的时候进行解释，不会使用数学术语，而是从概念上解释。事实上，这本书不包含任何数学公式，一个都没有。此外，多亏了现代深度学习库，你真的不需要理解数学就能构建实用的自然语言处理应用程序。如果你有兴趣学习机器学习和深度学习背后的理论和数学，可以找到很多优秀的资源。
- en: But you do need to be at least comfortable enough to write in Python and know
    its ecosystems. However, you don’t need to be an expert in software engineering
    topics. In fact, this book’s purpose is to introduce software engineering best
    practices for developing NLP applications. You also don’t need to know NLP in
    advance. Again, this book is designed to be a gentle introduction to the field.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 但你至少需要对Python编程感到自在，并了解它的生态系统。然而，你不需要成为软件工程领域的专家。事实上，本书的目的是介绍开发自然语言处理应用程序的软件工程最佳实践。你也不需要事先了解自然语言处理。再次强调，本书旨在对这个领域进行初步
- en: You need Python version 3.6.1 or higher and AllenNLP 2.5.0 or higher to run
    the code examples in this book. Note that we do not support Python 2, mainly because
    AllenNLP ([https://allennlp.org/](https://allennlp.org/)), the deep natural language
    processing framework I’m going to heavily use in this book, supports only Python
    3\. If you haven’t done so, I strongly recommend upgrading to Python 3 and familiarizing
    yourself with the latest language features such as type hints and new string-formatting
    syntax. This will be helpful, even if you are developing non-NLP applications.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 运行本书中代码示例需要Python版本3.6.1或更高版本以及AllenNLP版本2.5.0或更高版本。请注意，我们不支持Python 2，主要是因为这本书中我要大量使用的深度自然语言处理框架AllenNLP（[https://allennlp.org/](https://allennlp.org/)）只支持Python
    3。如果还没有升级到Python 3，我强烈建议你进行升级，并熟悉最新的语言特性，如类型提示和新的字符串格式化语法。即使你正在开发非自然语言处理的应用程序，这也会很有帮助。
- en: Don’t worry if you don’t have a Python development environment ready. Most of
    the examples in this book can be run via the Google Colab platform ([https://colab.research.google.com](https://colab.research.google.com)).
    You need only a web browser to build and experiment with NLP models!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有准备好Python开发环境，不要担心。本书中的大多数示例可以通过Google Colab平台（[https://colab.research.google.com](https://colab.research.google.com)）运行。你只需要一个网页浏览器就可以构建和实验自然语言处理模型！
- en: This book will use PyTorch ([https://pytorch.org/](https://pytorch.org/)) as
    its main choice of deep learning framework. This was a difficult decision for
    me, because several deep learning frameworks are equally great choices for building
    NLP applications, namely, TensorFlow, Keras, and Chainer. A few factors make PyTorch
    stand out among those frameworks—it’s a flexible and dynamic framework that makes
    it easier to prototype and debug NLP models; it’s becoming increasingly popular
    within the research community, so it’s easy to find open source implementations
    of major models; and the deep NLP framework AllenNLP mentioned earlier is built
    on top of PyTorch.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本书将使用PyTorch（[https://pytorch.org/](https://pytorch.org/)）作为主要的深度学习框架。这对我来说是一个难以选择的决定，因为有几个深度学习框架同样适合构建自然语言处理应用程序，包括TensorFlow、Keras和Chainer。有几个因素使得PyTorch在这些框架中脱颖而出——它是一个灵活且动态的框架，使得原型设计和调试自然语言处理模型更容易；它在研究界越来越受欢迎，所以很容易找到一些主要模型的开源实现；而之前提到的深度自然语言处理框架AllenNLP是建立在PyTorch之上的。
- en: 1.1 What is natural language processing (NLP)?
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: NLP is a principled approach to processing human language. Formally, it is a
    subfield of artificial intelligence (AI) that refers to computational approaches
    to process, understand, and generate human language. The reason it is part of
    AI is because language processing is considered a huge part of human intelligence.
    The use of language is arguably the most salient skill that separates humans from
    other animals.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 1.1.1 What is NLP?
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: NLP includes a range of algorithms, tasks, and problems that take human-produced
    text as an input and produce some useful information, such as labels, semantic
    representations, and so on, as an output. Other tasks, such as translation, summarization,
    and text generation, directly produce text as output. In any case, the focus is
    on producing some output that is useful per se (e.g., a translation) or as input
    to other downstream tasks (e.g., parsing). I’ll touch upon some popular NLP applications
    and tasks in section 1.3.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'You might wonder why NLP explicitly has “natural” in its name. What does it
    mean for a language to be natural? Are there any *un*natural languages? Is English
    natural? Which is more natural: Spanish or French?'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: The word “natural” here is used to contrast natural languages with formal languages.
    In this sense, all the languages humans speak are natural. Many experts believe
    that language emerged naturally tens of thousands of years ago and has evolved
    organically ever since. Formal languages, on the other hand, are types of languages
    that are invented by humans and have strictly and explicitly defined syntax (i.e.,
    what is grammatical) and semantics (i.e., what it means).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Programming languages such as C and Python are good examples of formal languages.
    These languages are defined in such a strict way that it is always clear what
    is grammatical and ungrammatical. When you run a compiler or an interpreter on
    the code you write in those languages, you either get a syntax error or not. The
    compiler won’t say something like, “Hmm, this code is maybe 50% grammatical.”
    Also, the behavior of your program is always the same if it’s run on the same
    code, assuming external factors such as the random seed and the system states
    remain constant. Your interpreter won’t show one result 50% of the time and another
    the other 50% of the time.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'This is not the case for human languages. You can write a sentence that is
    *maybe* grammatical. For example, do you consider the phrase “The person I spoke
    to” ungrammatical? There are some grammar topics where even experts disagree with
    each other. This is what makes human languages interesting but challenging, and
    why the entire field of NLP even exists. Human languages are ambiguous, meaning
    that their interpretation is often not unique. Both structures (how sentences
    are formed) and semantics (what sentences mean) can have ambiguities in human
    language. As an example, let’s take a close look at the next sentence:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '*He saw a girl with a telescope.*'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: When you read this sentence, who do you think has a telescope? Is it the boy,
    who’s using a telescope to see a girl (from somewhere far), or the girl, who has
    a telescope and is seen by the boy? There seem to be at least two interpretations
    of this sentence as shown in figure 1.1.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '![CH01_F01_Hagiwara](../Images/CH01_F01_Hagiwara.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
- en: Figure 1.1 Two interpretations of “He saw a girl with a telescope.”
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: The reason you are confused upon reading this sentence is because you don’t
    know what the phrase “with a telescope” is about. More technically, you don’t
    know what this prepositional phrase (PP) modifies. This is called a *PP-attachment*
    problem and is a classic example of *syntactic ambiguity*. A syntactically ambiguous
    sentence has more than one interpretation of how the sentence is structured. You
    can interpret the sentence in multiple ways, depending on which structure of the
    sentence you believe.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'Another type of ambiguity that may arise in natural language is *semantic ambiguity*.
    This is when the meaning of a word or a sentence, not its structure, is ambiguous.
    For example, let’s look at the following sentence:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '*I saw a bat.*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 'There is no question how this sentence is structured. The subject of the sentence
    is “I” and the object is “a bat,” connected by the verb “saw.” In other words,
    there is no syntactical ambiguity in it. But how about its meaning? “Saw” has
    at least two meanings. One is the past tense of the verb “to see.” The other is
    to cut some object with a saw. Similarly, “a bat” can mean two very different
    things: is it a nocturnal flying mammal or a piece of wood used to hit a ball?
    All in all, does this sentence mean that I observed a nocturnal flying mammal
    or that I cut a baseball or cricket bat? Or even (cruelly) that I cut a nocturnal
    animal with a saw? You never know, at least from this sentence alone.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Ambiguity is what makes natural languages rich but also challenging to process.
    We can’t simply run a compiler or an interpreter on a piece of text and just “get
    it.” We need to face the complexities and subtleties of human languages. We need
    a scientific, principled approach to deal with them. That’s what NLP is all about.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Welcome to the beautiful world of natural languages.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 1.1.2 What is not NLP?
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now let’s consider the following scenario and think how you’d approach this
    problem: you are working as a junior developer at a midsized company that has
    a consumer-facing product line. It’s 3 p.m. on a Friday. The rest of the team
    is becoming more and more restless as the weekend approaches. That’s when your
    boss drops by at your cubicle.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: “Hey, got a minute? I’ve got something interesting to show you. I just sent
    it to you.”
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Your boss just sent you an email with a huge zip file attached to it.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: “OK, so this is a giant TSV file. It contains all the responses to the survey
    questions about our product. I just got this data from the marketing team.”
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, the marketing team has been collecting user opinions about one of
    the products through a series of survey questions online.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: “The survey questions include standard ones like ‘How did you know about our
    product?’ and ‘How do you like our product?’ There is also a free-response question,
    where our customers can write whatever they feel about our product. The thing
    is, the marketing team realized there was a bug in the online system and the answers
    to the second question were not recorded in the database at all.”
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: “Wait, so there’s no way to tell how the customers are feeling about our product?”
    This sounds weirdly familiar. This must be a copy-and-paste error. When you first
    created an online data-collection interface, you copied and pasted the backend
    code without modifying the ID parameters, resulting in a loss of some data fields.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: “So,” your boss continues. “I was wondering if we can recover the lost data
    somehow. The marketing team is a little desperate now because they need to report
    the results to the VP early next week.”
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: At this point, your bad feeling has been confirmed. Unless you come up with
    a way to get this done as quickly as possible, your weekend plans will be ruined.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: “Didn’t you say you were interested in some machine learning? I think this is
    a perfect project for you. Anyway, it’d be great if you can give it a stab and
    let me know what you find. Do you think you can have some results by Monday?”
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: “Well, I’ll give it a try.”
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: You know “no” is not an acceptable answer here. Satisfied with your answer,
    your boss smiles and walks off.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: You start by skimming the TSV file. To your relief, its structure is fairly
    standard—it has several fields such as timestamps and submission IDs. At the end
    of each line is a lengthy field for the free-response question. Here they are,
    you think. At least you know where to look for some clues.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'After a quick glance over the field, you find responses such as “A very good
    product!” and “Very bad. It crashes all the time!” Not too bad, you think. At
    least you can capture these simple cases. You start by writing the following method
    that captures those two cases:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Then you run this method on the responses in the file and log the results, along
    with the original input. As intended, this method seems to be able to capture
    a dozen or so of the responses that contains “good” or “bad.”
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 'But then you start to see something alarming, as shown next:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '“I can’t think of a single good reason to use this product”: positive'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '“It’s not bad.”: negative'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'Oops, you think. *Negation*. Yeah, of course. But this is pretty easy to deal
    with. You modify the method as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You run the script again. This time, it seems to be behaving as intended, until
    you see an even more complicated example:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '“The product is not only cheap but also very good!”: negative'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: 'Hmm, you think. This is probably not as straightforward as I initially thought
    after all. Maybe the negation has to be somewhere near “good” or “bad” for it
    to be effective. Wondering what steps you could take next, you scroll down to
    see more examples, which is when you see responses like these:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: “嗯，你想。这可能并不像我最初想的那么简单。也许否定词必须在‘好’或‘坏’附近才能有效果。想知道接下来可以采取什么步骤，你向下滚动以查看更多示例，这时你看到了这样的回答：
- en: '“I always wanted this feature badly!”: negative'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '“我一直很想要这个功能！”: negative'
- en: '“It’s very badly made.”: negative'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '“它做得很糟糕。”: negative'
- en: You silently curse to yourself. How could a single word in a language have two
    completely opposite meanings? At this point, your little hope for enjoying the
    weekend has already disappeared. You are already wondering what excuses you use
    with your boss next Monday.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 你默默地咒骂自己。一个语言中的一个单词怎么会有完全相反的两个意思？此时，你对周末愉快的小希望已经消失了。你已经在想下周一对老板使用什么借口了。
- en: As a reader of this book, you’ll know better. You’ll know that NLP is not about
    throwing a bunch of ifs and thens at natural language text. It is a more principled
    approach to processing natural language. In the following chapters, you’ll learn
    how you should approach this problem before writing a single line of code and
    how to build a custom-made NLP application just for your task at hand.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 作为本书的读者，你会更清楚。你会知道 NLP 不是简单地在自然语言文本中加入一堆 if 和 then。这是一个更有原则性的处理自然语言的方法。在接下来的章节中，你将学习在编写一行代码之前应该如何处理这个问题，以及如何为手头的任务构建一个定制的
    NLP 应用程序。
- en: 1.1.3 AI, ML, DL, and NLP
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1.3 AI、ML、DL 和 NLP
- en: Before delving into the details of NLP, it’d be useful to clarify how it relates
    to other, similar fields. Most of you have at least heard about artificial intelligence
    (AI) and machine learning (ML). You may also have heard of deep learning (DL),
    because it’s generating a lot of buzz in popular media these days. Figure 1.2
    illustrates how those different fields overlap with each other.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究 NLP 的细节之前，澄清它与其他类似领域的关系是有用的。你们大多数人至少听说过人工智能（AI）和机器学习（ML）。你们可能也听说过深度学习（DL），因为它在当今流行媒体中引起了很多关注。图
    1.2 显示了这些不同领域之间的重叠关系。
- en: '![CH01_F02_Hagiwara](../Images/CH01_F02_Hagiwara.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F02_Hagiwara](../Images/CH01_F02_Hagiwara.png)'
- en: 'Figure 1.2 The relationship among different fields: AI, ML, DL, and NLP'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2 不同领域之间的关系：AI、ML、DL 和 NLP
- en: Artificial intelligence (AI) is a broad umbrella field that is concerned with
    achieving human-like intelligence using machines. It encompasses a wide range
    of subfields, including machine learning, natural language processing, computer
    vision, and speech recognition. The field also includes subfields such as reasoning,
    planning, and search, which do not fall under either machine learning or natural
    language processing and are not in the scope of this book.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能（AI）是一个广泛的领域，致力于利用机器实现类似人类的智能。它涵盖了一系列子领域，包括机器学习、自然语言处理、计算机视觉和语音识别。该领域还包括推理、规划和搜索等子领域，这些子领域既不属于机器学习也不属于自然语言处理，也不在本书的范围内。
- en: Machine learning (ML) is usually considered a subfield of artificial intelligence
    that is about improving computer algorithms through experience and data. This
    includes learning a general function that maps inputs to outputs based on past
    experience (supervised learning), drawing hidden patterns and structures from
    data (unsupervised learning), and learning how to act in a dynamic environment
    based on indirect rewards (reinforcement learning). Throughout this book, we’ll
    make a heavy use of supervised machine learning, which is the main paradigm for
    training NLP models.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习（ML）通常被认为是人工智能的一个子领域，它通过经验和数据改进计算机算法。这包括学习一个基于过去经验将输入映射到输出的一般函数（监督学习）、从数据中提取隐藏的模式和结构（无监督学习），以及根据间接奖励学习如何在动态环境中行动（强化学习）。在本书中，我们将大量使用监督机器学习，这是训练
    NLP 模型的主要范式。
- en: Deep learning (DL) is a subfield of machine learning that usually uses deep
    neural networks. These neural network models are called “deep” because they consist
    of a number of layers. A *layer* is just a fancy word for a substructure of neural
    networks. By having many stacked layers, deep neural networks can learn complex
    representations of data and can capture highly complicated relationships between
    the input and the output.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）是机器学习的一个子领域，通常使用深度神经网络。这些神经网络模型之所以称为“深度”，是因为它们由许多层组成。*层*只是神经网络的一个子结构的花哨说法。通过具有许多堆叠层，深度神经网络可以学习数据的复杂表示，并可以捕捉输入和输出之间的高度复杂的关系。
- en: As the amount of available data and computational resources increases, modern
    NLP makes a heavier and heavier use of machine learning and deep learning. Modern
    NLP applications and tasks are usually built on top of machine learning pipelines
    and trained from data. But also notice in figure 1.2 that a part of NLP does not
    overlap with machine learning. Traditional methods such as counting words and
    measuring similarities between text are usually not considered to be machine learning
    techniques per se, although they can be important building blocks for ML-based
    models.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 随着可用数据量和计算资源的增加，现代NLP越来越多地使用机器学习和深度学习。现代NLP的应用和任务通常建立在机器学习管道之上，并从数据中进行训练。但请注意，在图1.2中，NLP的一部分与机器学习不重叠。诸如计数单词和衡量文本相似性之类的传统方法通常不被视为机器学习技术本身，尽管它们可以是ML模型的重要构建块。
- en: I’d also like to mention some other fields that are related to NLP. One such
    field is *computational linguistics* (CL). As its name suggests, computational
    linguistics is a subfield of linguistics that uses computational approaches to
    study human language. The main distinction between CL and NLP is that the former
    encompasses scientific approaches to study language, whereas the latter is concerned
    with engineering approaches for making computers perform something useful related
    to language. People often use those terms interchangeably, partly due to some
    historical reasons. For example, the most prestigious conference in the field
    of NLP is called ACL, which actually stands for “the Association for Computational
    Linguistics!”
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我还想提一下与自然语言处理（NLP）相关的其他领域。其中一个领域是*计算语言学*（CL）。顾名思义，计算语言学是语言学的一个子领域，它使用计算方法来研究人类语言。CL和NLP的主要区别在于前者涵盖了研究语言的科学方法，而后者关注的是使计算机执行与语言相关的有用任务的工程方法。人们经常将这些术语互换使用，部分原因是由于历史原因。例如，该领域中最负盛名的会议被称为ACL，实际上代表着“计算语言学协会！”
- en: Another related field is *text mining*. Text mining is a type of data mining
    targeted at textual data. Its focus is on drawing useful insights from unstructured
    textual data, which is a type of text data that is not formatted in a form that
    is easily interpretable by computers. Such data is usually collected from various
    sources, such as crawling the web and social media. Although its purpose is slightly
    different from that of NLP, these two fields are similar, and we can use the same
    tools and algorithms for both.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个相关领域是*文本挖掘*。文本挖掘是一种针对文本数据的数据挖掘类型。它的重点是从非结构化的文本数据中获取有用的见解，这种文本数据不易被计算机解释。这些数据通常来自各种来源，如网络爬虫和社交媒体。虽然其目的与NLP略有不同，但这两个领域相似，我们可以为两者使用相同的工具和算法。
- en: 1.1.4 Why NLP?
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么选择NLP？
- en: If you are reading this, you have at least some interest in NLP. Why is NLP
    exciting? Why is it worth learning more about NLP and, specifically, real-world
    NLP?
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在阅读这篇文章，你至少对NLP有一些兴趣。为什么NLP令人兴奋？为什么值得更深入了解NLP，尤其是现实中的NLP？
- en: The first reason is that NLP is booming. Even without the recent AI and ML boom,
    NLP is more important than ever. We are witnessing the advent of practical NLP
    applications in our daily lives, such as conversational agents (think Apple Siri,
    Amazon Alexa, and Google Assistant) and near human-level machine translation (think
    Google Translate). A number of NLP applications are already an integral part of
    our day-to-day activities, such as spam filtering, search engines, and spelling
    correction, as we’ll discuss later. The number of Stanford students enrolled in
    NLP classes grew fivefold from 2008 to 2018 ([http://realworldnlpbook.com/ch1.html#tweet1](http://realworldnlpbook.com/ch1.html#tweet1)).
    Similarly, the number of attendees for EMNLP (Empirical Methods in Natural Language
    Processing), one of the top NLP conferences, doubled within just one year ([http://realworldnlpbook
    .com/ch1.html#tweet2](http://realworldnlpbook.com/ch1.html#tweet2)). Other major
    NLP conferences are also experiencing similar increases in participants and paper
    submissions ([http://realworldnlpbook.com/ch1 .html#nivre17](http://realworldnlpbook.com/ch1.html#nivre17)).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: The second reason is that NLP is an evolving field. The field of NLP itself
    has a long history. The first experiment to build a machine translation system,
    called *The Georgetown-IBM Experiment*, was attempted back in 1954\. For more
    than 30 years since this experiment, most NLP systems relied on handwritten rules.
    Yes, it was not much different from what you just saw in section 1.1.1\. The first
    milestone, which came in the late 1980s, was the use of statistical methods and
    machine learning for NLP. Many NLP systems started leveraging statistical models
    trained from data. This led to some recent successes in NLP, including, most notably,
    IBM Watson. The second milestone was more drastic. Starting around the late 2000s,
    the use of so-called deep learning, that is, deep neural network models, took
    the field by storm. By the mid-2010s, deep neural network models became the new
    standard in the field.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: This second milestone was so drastic and fast that it’s worth noting here. New
    neural network-based NLP models are not only more effective but also a lot simpler.
    For example, it used to take a lot of expertise and effort to replicate even a
    simple, baseline machine translation model. One of the most popular open source
    software packages for statistical machine translation, called Moses ([http://www.statmt.org/moses/](http://www.statmt.org/moses/)),
    is a behemoth, consisting of 100,000s of lines of code and dozens of supporting
    modules and tools. Experts spent hours just installing the software and making
    it work. On the other hand, as of 2018, anyone with some prior programming experience
    could run a neural machine translation system more powerful than traditional statistical
    models with a fraction of the code size—less than a few thousand lines of code
    (e.g., see TensorFlow’s neural machine translation tutorial at [https://github.com/tensorflow/nmt](https://github.com/tensorflow/nmt)).
    Also, the new neural network models are trained “end-to-end,” which means that
    those big, monolithic networks take the input and directly produce the output.
    Entire models are trained to match the desired output. On the other hand, traditional
    machine learning models consist of (at least) several submodules. These submodules
    are trained separately using different machine learning algorithms. In this book,
    I’ll mainly discuss modern neural network-based approaches to NLP but also touch
    upon some traditional concepts as well.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这第二个里程碑变化如此巨大和迅速，以至于值得在这里注意。基于新的神经网络的自然语言处理模型不仅更有效，而且更简单。例如，以前复制甚至是一个简单的基准机器翻译模型都需要很多专业知识和努力。一个最流行的用于统计机器翻译的开源软件包，称为Moses（[http://www.statmt.org/moses/](http://www.statmt.org/moses/)），是一个庞然大物，包含数十万行代码和数十个支持模块和工具。专家们花了几个小时的时间来安装软件并使其正常工作。另一方面，截至2018年，只要有一些先前的编程经验，任何人都可以运行一个比传统的统计模型更强大的神经机器翻译系统，代码量只有几千行以下（例如，请参阅TensorFlow的神经机器翻译教程[https://github.com/tensorflow/nmt](https://github.com/tensorflow/nmt)）。此外，新的神经网络模型是“端到端”训练的，这意味着那些庞大的、整体的网络接收输入并直接产生输出。整个模型都是为了匹配所需的输出而进行训练的。另一方面，传统的机器学习模型由（至少）几个子模块组成。这些子模块是使用不同的机器学习算法分别训练的。在本书中，我将主要讨论基于现代神经网络的自然语言处理方法，但也会涉及一些传统概念。
- en: The third and final reason is that NLP is challenging. Understanding and producing
    language is the central problem of artificial intelligence, as we saw in the previous
    section. The accuracy and performance in major NLP tasks such as speech recognition
    and machine translation got drastically better in the past decade or so. But human-level
    understanding of language is far from being solved.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个也是最后一个原因是自然语言处理是具有挑战性的。理解和产生语言是人工智能的核心问题，正如我们在前一节中看到的。在过去的十年左右，主要的自然语言处理任务，如语音识别和机器翻译的准确性和性能都得到了显著提高。但是，人类水平的语言理解距离解决尚远。
- en: 'To verify this quickly, open up your favorite machine translation service (or
    simply Google Translate), and type this sentence: “I saw her duck.” Try to translate
    it to Spanish or some other language you understand. You should see words like
    “*pato*,” which means “a duck” in Spanish. But did you notice another interpretation
    of this sentence? See figure 1.3 for the two interpretations. The word “duck”
    here could be a verb meaning “to crouch down.” Try adding another sentence after
    this, such as “She tried to avoid a flying ball.” Did the machine translation
    change the first translation in any way? The answer is probably no. You should
    still see the same “*pato*” in the translation. As you can see, most (if not all)
    commercial machine translation systems that are available as of today do not understand
    the context outside of the sentence that is being translated. A lot of research
    effort is spent on this problem in academia, but this is still one of many problems
    in NLP that is considered unsolved.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 要快速验证这一点，打开你最喜欢的机器翻译服务（或简单地使用谷歌翻译），然后输入这句话：“I saw her duck.” 尝试将其翻译成西班牙语或其他你理解的语言。你应该看到像“*pato*”这样的词，在西班牙语中意思是“一只鸭子”。但是你是否注意到了这句话的另一种解释？请参见图1.3，其中包含了两种解释。这里的“duck”可能是一个动词，意思是“蹲下”。尝试在此之后添加另一句话，例如“她试图避开一只飞来的球。”
    机器翻译是否以任何方式改变了第一种翻译？答案很可能是否定的。你仍然应该在翻译中看到同样的“*pato*”。正如你所看到的，截至目前为止，大多数（如果不是全部）商业机器翻译系统都无法理解除正在翻译的句子之外的上下文。学术界在解决这个问题上花费了大量研究力量，但这仍然是自然语言处理中被认为尚未解决的问题之一。
- en: '![CH01_F03_Hagiwara](../Images/CH01_F03_Hagiwara.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F03_Hagiwara](../Images/CH01_F03_Hagiwara.png)'
- en: Figure 1.3 Two interpretations of “I saw her duck.”
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3 “我看见她的鸭子”的两种解释。
- en: Compared to other AI fields such as robotics and computer vision, language has
    its own quirks. Unlike images, utterances and sentences have variable length.
    You can say a very short sentence (“Hello.”) or a very long one (“A quick brown
    fox . . .”). Most machine learning algorithms are not good at dealing with something
    of variable length, and you need to come up with ways to represent languages with
    something more fixed. If you look back at the history of the field, NLP is largely
    concerned with the problem of how to *represent* language mathematically. Vector
    space models and word embeddings (discussed in chapter 3) are some examples of
    this.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 与机器人技术和计算机视觉等其他人工智能领域相比，语言有其自己的特点。与图像不同，话语和句子的长度是可变的。你可以说一个非常短的句子（“你好。”）或一个非常长的句子（“一个快速的棕色狐狸……”）。大多数机器学习算法不擅长处理可变长度的东西，你需要想办法用更固定的东西来表示语言。如果你回顾一下这个领域的历史，你会发现自然语言处理主要关注的问题是如何数学上*表示*语言。向量空间模型和词嵌入（在第3章中讨论）就是一些例子。
- en: Another characteristic of language is that it is *discrete*. What this means
    is that things in languages are separate as concepts. For example, if you take
    a word “rat” and change its first letter to the next one, you’ll have “sat.” In
    computer memory, the difference is just a single bit. However, there is no relationship
    between those two words except they both end with “at,” and maybe a rat can sit.
    There is no such thing as something that is in between “rat” and “sat.” These
    two are totally discrete, separate concepts that happen to have similar spelling.
    On the other hand, if you take an image of a car and change the value of a pixel
    by a single bit, you still have a car that is almost identical to the one before
    this change. Maybe it has a slightly different color. In other words, images and
    sounds are continuous, meaning that you can make small modifications without greatly
    affecting what they are. Many mathematical toolkits, such as vectors, matrices,
    and functions, are good at dealing with something continuous. The history of NLP
    is actually a history of challenging this discreteness of language, and only recently
    have we begun to see some successes on this front, for example, with word embeddings.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 语言的另一个特点是它是*离散*的。这意味着语言中的事物作为概念是分离的。例如，如果你拿一个词“rat”并将它的第一个字母改为下一个，你会得到“sat”。在计算机内存中，它们之间的差异仅仅是一个比特。然而，除了它们都以“at”结尾以外，这两个单词之间没有关系，也许老鼠可以坐着。不存在介于“rat”和“sat”之间的东西。这两者是完全离散的，独立的概念，只是拼写相似。另一方面，如果你拿一张汽车的图像并将一个像素的值改变一个比特，你仍然得到一辆几乎与改变前相同的汽车。也许颜色略有不同。换句话说，图像和声音是连续的，这意味着你可以做出小的修改而不会对它们的本质产生太大影响。许多数学工具包，如向量、矩阵和函数，都擅长处理连续性的事物。自然语言处理的历史实际上是挑战语言的这种离散性的历史，而且直到最近我们才开始在这个方面取得一些成功，例如，使用词嵌入。
- en: 1.2 How NLP is used
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 自然语言处理的应用
- en: As I mentioned previously, NLP is already an integral part of our daily life.
    In modern life, a larger and larger portion of our daily communication is done
    online, and our online communication is still largely conducted in natural language
    text. Think of your favorite social networking services, such as Facebook and
    Twitter. Although you can post photos and videos, a large portion of communication
    is still in text. As long as you are dealing with text, there is a need for NLP.
    For example, how do you know if a particular post is spam? How do you know which
    posts are the ones you are most likely to “like?” How do you know which ads you
    are most likely to click?
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我之前提到的，自然语言处理已经成为我们日常生活的一个组成部分。在现代生活中，我们日常通信的越来越大的部分是在线完成的，而我们的在线通信仍然主要是自然语言文本。想想你最喜欢的社交网络服务，如Facebook和Twitter。虽然你可以发布照片和视频，但是很大一部分的通信仍然是文本。只要你在处理文本，就需要自然语言处理。例如，你怎么知道某个帖子是垃圾邮件？你怎么知道哪些帖子是你最可能“喜欢”的？你怎么知道哪些广告是你最可能点击的？
- en: Because many large internet companies need to deal with text in one way or another,
    chances are many of them are already using NLP. You can also confirm this from
    their “careers” page—you’ll see that they are always hiring NLP engineers and
    data scientists. NLP is also used to a varying extent in many other industries
    and products including, but not limited to, customer service, e-commerce, education,
    entertainment, finance, and health care, which all involve text in some ways.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Many NLP systems and services can be classified into or built by combining some
    major types of NLP applications and tasks. In this section, I’m going to introduce
    some of the most popular applications of NLP as well as common NLP tasks.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.1 NLP applications
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An NLP application is a software application whose main purpose is to process
    natural language text and draw some useful information from it. Similar to general
    software applications, it can be implemented in various ways, such as an offline
    data-processing script, an offline standalone application, a backend service,
    or a full-stack service with a frontend, depending on its scope and use cases.
    It can be built for end users to use directly, for other backend services to consume
    its output, or for other businesses to use as a SaaS (software as a service).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: You can use many NLP applications out of the box, such as machine translation
    software and major SaaS products (e.g., Google Cloud API), if your requirement
    is generic and doesn’t require a high level of customization. You can also build
    your own NLP applications if you need customizations and/or you need to deal with
    a specific target domain. This is exactly what you’ll learn throughout this book!
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Machine translation
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Machine translation is probably one of the most popular and easy-to-understand
    NLP applications. Machine translation (MT) systems translate a given text from
    one language to another language. An MT system can be implemented as a full-stack
    service (e.g., Google Translate), as well as a pure backend service (e.g., NLP
    SaaS products). The language the input text is written in is called *the source
    language*, whereas the one for the output is called *the target language*. MT
    encompasses a wide range of NLP problems, including language understanding and
    generation, because MT systems need to understand the input and then generate
    the output. MT is one of the most well-studied areas in NLP, and it was one of
    the earliest applications of NLP as well.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: One challenge in MT is the tradeoff between *fluency* and *adequacy*. Translation
    needs to be fluent, meaning that the output has to sound natural in the target
    language. Translation also needs to be adequate, meaning that the output has to
    reflect the meaning expressed by the input as closely as possible. These two are
    often in conflict, especially when the source and the target languages are not
    very similar (e.g., English and Mandarin Chinese). You can write a sentence that
    is a precise, verbatim translation of the input, but doing so often leads to something
    that doesn’t sound natural in the target language. On the other hand, you can
    make up something that sounds natural but might not reflect the precise meaning.
    Good human translators address this tradeoff in a creative way. It’s their job
    to come up with translations that are natural in the target language while reflecting
    the meaning of the original.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Grammatical and spelling error correction
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Most major web browsers nowadays support spelling correction. Even if you forget
    how to spell “Mississippi,” you can do your best and type what you remember, and
    the browser highlights it with a correction. Some word-processing software applications,
    including recent versions of Microsoft Word, do more than just correct spelling.
    They point out grammatical errors such as uses of “it’s” instead of “its.” This
    is not an easy feat, because both words are, in a sense, “correct” (no mistakes
    in spelling), and the system needs to infer whether they are used correctly from
    the context. Some commercial products (most notably, Grammarly, [https://www.grammarly.com/](https://www.grammarly.com/))
    specialize in grammatical error correction. Some products go a long way and point
    out incorrect usage of punctuation and even writing styles. These products are
    popular among both native and non-native speakers of the language.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Research into grammatical error correction has been active due to the increasing
    number of non-native English speakers. Traditionally, grammatical error correction
    systems for non-native English speakers dealt with individual types of mistakes
    one by one. For example, you could think of a subcomponent of the system that
    detects and corrects only incorrect uses of articles (*a*, *an*, *the*, etc.),
    which is very common among non-native English speakers. More recent approaches
    to grammatical error correction are similar to the ones for machine translation.
    You can think of the (potentially incorrect) input as one language and the corrected
    output as another. Then your job is to “translate” between these two languages!
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Search engine
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Another application of NLP that is already an integral part of our daily lives
    is search engines. Few people would think of search engines as an NLP application,
    yet NLP plays such an important role in making search engines useful that they
    are worth mentioning here.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Page analysis is one area where NLP is heavily used for search engines. Ever
    wonder why you don’t see any “hot dog” pages when you search for “dogs?” If you
    have any experience building your own full-text search engines using open source
    software such as Solr and Elasticsearch, and if you simply used a word-based index,
    your search result pages would be littered with “hot dogs,” even when you want
    just “dogs.” Major commercial search engines solve this problem by running the
    page content being indexed through NLP pipelines that recognize that “hot dogs”
    are not a type of “dogs.” But the extent and types of NLP pipelines that go into
    page analysis is confidential information for search engines and is difficult
    to know.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Query analysis is another NLP application in search engines. If you have noticed
    Google showing a box with pictures and bios when you search for a celebrity or
    a box with the latest news stories when you search for certain current events,
    that’s query analysis in action. Query analysis identifies the intent (what the
    user wants) of the query and shows relevant information accordingly. A common
    way to implement query analysis is to make it a classification problem, where
    an NLP pipeline classifies queries into classes of intent (e.g., celebrity, news,
    weather, videos), although again, the details of how commercial search engines
    run query analysis are usually highly confidential.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Finally, search engines are not only about analyzing pages and classifying queries.
    They have many other functionalities that make your searches easier, one of which
    is query correction. This comes into play when you make a spelling or a grammatical
    mistake when formulating the query, and Google and other major search engines
    show corrections with labels such as “showing results for:” and “Did you mean.”
    How this works is somewhat similar to grammatical error correction that I mentioned
    earlier, except it is optimized for the types of mistakes and queries that search
    engine users use.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Dialog systems
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Dialog systems are machines that humans can have conversations with. The field
    of dialog systems has a long history. One of the earliest dialog systems, ELIZA,
    was developed in 1966.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: But it’s only recently that dialog systems have found their ways into our daily
    lives. We have seen an almost exponential increase in their popularity in recent
    years, mainly driven by the availability of consumer-facing “conversational AI”
    products such as Amazon Alexa and Google Assistant. In fact, according to a survey
    in 2018, 20% of US homes already own a smart speaker. You may also remember being
    mind-blown watching the keynote at Google IO in 2018, where Google’s conversational
    AI called Google Duplex was shown making a phone call to a hair salon and a restaurant,
    having natural conversations with the staff at the business, and making an appointment
    on behalf of its user.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: The two main types of dialog systems are task-oriented and chatbots. Task-oriented
    dialog systems are used to achieve specific goals (for example, reserving a plane
    ticket), obtaining some information, and, as we saw, making a reservation at a
    restaurant. Task-oriented dialog systems are usually built as an NLP pipeline
    consisting of several components, including speech recognition, language understanding,
    dialog management, response generation, and speech synthesis, which are usually
    trained separately. Similar to machine translation, though, there are new deep
    learning approaches where dialog systems (or their subsystems) are trained end-to-end.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: The other type of dialog system is chatbots, whose main purpose is to have conversations
    with humans. Traditional chatbots are usually managed by a set of handwritten
    rules (e.g., when the human says this, say that). Recently, the use of deep neural
    networks, particularly sequence-to-sequence models and reinforcement learning,
    has become increasingly popular. However, because the chatbots do not serve particular
    purposes, the evaluation of chatbots, that is, assessing how good a particular
    chatbot is, remains an open question.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.2 NLP tasks
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Behind the scenes, many NLP applications are built by combining multiple NLP
    components that solve different NLP problems. In this section, I introduce some
    notable NLP tasks that are commonly used in NLP applications.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Text classification
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Text classification is the process of classifying pieces of text into different
    categories. This NLP task is one of the simplest yet most widely used. You might
    not have heard of the term “text classification” before, but I bet most of you
    benefit from this NLP task every day. For example, spam filtering is one type
    of text classification. It classifies emails (or other types of text, such as
    web pages) into two categories—spam or not spam. This is why you get very few
    spam emails when you use Gmail and you see so few spammy (low-quality) web pages
    when you use Google.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Another type of text classification is called *sentiment analysis*, which is
    what we saw in section 1.1\. Sentiment analysis is used to automatically identify
    subjective information, such as opinions, emotions, and feelings, within text.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Part-of-speech tagging
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: A *part of speech* (POS) is a category of words that share the similar grammatical
    properties. In English, for example, nouns describe the names of things like objects,
    animals, people, and concepts, among many other things. A noun can be used as
    a subject of a verb, an object of a verb, and an object of a preposition. Verbs,
    in contrast, describe actions, states, and occurrences. Other English parts of
    speech include adjectives (*green*, *furious*), adverbs (*cheerfully*, *almost*),
    determiners (*a*, *the*, *this*, *that*), prepositions (*in*, *from*, *with*),
    conjunctions (*and*, *yet*, *because*), and many others. Almost all languages
    have nouns and verbs, but other parts of speech differ from language to language.
    For example, many languages, such as Hungarian, Turkish, and Japanese, have *postpositions*
    instead of prepositions, which are placed *after* words to add some extra meaning
    to them. A group of NLP researchers came up with a set of tags that cover frequent
    parts of speech that exist in most languages, called a *universal part-of-speech
    tagset* ([http://realworldnlpbook.com/ch1.html#universal-pos](http://realworldnlpbook.com/ch1.html#universal-pos)).
    This tagset is widely used for language-independent tasks.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Part-of-speech tagging is the process of tagging each word in a sentence with
    a corresponding part-of-speech tag. Some of you may have done this at school.
    As an example, let’s take the sentence “I saw a girl with a telescope.” The POS
    tags for this sentence are shown in figure 1.4.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '![CH01_F04_Hagiwara](../Images/CH01_F04_Hagiwara.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
- en: Figure 1.4 Part-of-speech (POS) tagging
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: These tags come from the Penn Treebank POS tagset, which is the most popular
    standard corpus for training and evaluating various NLP tasks such as POS tagging
    and parsing. Traditionally, POS tagging was solved by sequential labeling algorithms
    such as hidden Markov models (HMMs) and conditional random fields (CRFs). Recently,
    recurrent neural networks (RNNs) have become a popular and practical choice for
    training a POS tagger with high accuracy. The results of POS tagging are often
    used as the input to other downstream NLP tasks, such as machine translation and
    parsing. I’ll cover part-of-speech tagging in more detail in chapter 5.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Parsing
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Parsing is the task of analyzing the structure of a sentence. Broadly speaking,
    there are two main types of parsing, *constituency parsing* and *dependency parsing*,
    which we’ll discuss in detail next.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'Constituency parsing uses *context-free grammars* to represent natural language
    sentences. (See [http://mng.bz/GO5q](http://mng.bz/GO5q) for a brief introduction
    to context-free grammars). A context-free grammar is a way to specify how smaller
    building blocks of a language (e.g., words) are combined to form larger building
    blocks (e.g., phrases and clauses) and eventually sentences. To put it another
    way, it specifies how the largest unit (a sentence) is broken down to phrases
    and clauses and all the way down to words. The ways the linguistic units interact
    with each other are specified by a set of *production rules as follows*:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: A production rule describes a transformation from the symbol on the left-hand
    side (e.g., “S”) to the symbols on the right-hand side (e.g., “NP VP”). The first
    rule means that a sentence is a noun phrase (NP) followed by a verb phrase (VP).
    Some of the symbols (e.g., DT, NN, VBD) may look familiar to you—yes, they are
    the POS tags we just saw in the POS tagging section. In fact, you can consider
    POS tags as the smallest grammatical categories that behave in similar ways (because
    they are!).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Now the parser’s job is to figure out how to reach the final symbol (in this
    case, “S”) starting from the raw words in the sentence. You can think of those
    rules as transformation rules from the symbols on the right to the ones on the
    left by traversing the arrow backward. For example, using the rule “DT  a” and
    “NN  girl,” you can convert “a girl” to “DT NN.” Then, if you use “NP  DT NN,”
    you can reduce the entire phrase to “NP.” If you illustrate this process in a
    tree-like diagram, you get something like the one shown in figure 1.5.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '![CH01_F05_Hagiwara](../Images/CH01_F05_Hagiwara.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
- en: Figure 1.5 Subtree for “a girl”
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Tree structures that are created in the process of parsing are called *parse
    trees**,* or simply *parses*. The figure is a subtree because it doesn’t cover
    the entirety of the tree (i.e., it doesn’t show all the way from “S” to words).
    Using the sentence “I saw a girl with a telescope” that we discussed earlier and
    see if you can parse it by hand. If you keep breaking down the sentence using
    the production rules until you get the final “S” symbol, you get the tree-like
    structure shown in figure 1.6.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '![CH01_F06_Hagiwara](../Images/CH01_F06_Hagiwara.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
- en: Figure 1.6 Parse tree for “I saw a girl with a telescope.”
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Don’t worry if the tree in figure 1.6 is different from what you got. Actually,
    there’s another parse tree that is a valid parse of this sentence, shown in figure
    1.7.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '![CH01_F07_Hagiwara](../Images/CH01_F07_Hagiwara.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
- en: Figure 1.7 Another parse tree for “I saw a girl with a telescope.”
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: If you look at those two trees carefully, you’ll notice a difference where the
    “PP” (prepositional phrase) is located, or attached. In fact, these two parse
    trees correspond to the two different interpretations of this sentence we discussed
    in section 1.1\. The first tree (figure 1.6), where the PP attaches the verb “saw,”
    corresponds to the interpretation where the boy is using a telescope to see the
    girl. In the second tree (figure 1.7), where the PP attaches to the noun “a girl,”
    the boy saw the girl who has a telescope. Parsing is a great step forward to reveal
    the structure and the semantics of a sentence, but in cases like this one, parsing
    alone cannot uniquely decide what is the single most likely interpretation of
    a sentence.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: The other type of parsing is called *dependency parsing*. Dependency parsing
    uses dependency grammars to describe the structure of sentences, not in terms
    of phrases but in terms of words and the binary relations between them. For example,
    the result of dependency parsing of the earlier sentence is shown in figure 1.8.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '![CH01_F08_Hagiwara](../Images/CH01_F08_Hagiwara.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
- en: Figure 1.8 Dependency parse for “I saw a girl with a telescope.”
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Notice that each relation is directional and labeled. A relation specifies which
    word depends on which word and the type of relationship between the two. For example,
    the relation connecting “a” to “girl” is labeled “det,” meaning the first word
    is the determiner of the second. If you take the most central word, “saw,” and
    pull it upward, you’ll notice that these words and relations form a tree. Such
    trees are called *dependency trees*.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: One advantage of dependency grammars is that they are agnostic regarding some
    word-order changes, meaning that the order of certain words in the sentence will
    not change the dependency tree. For example, in English, there is some freedom
    as to where to put an adverb in a sentence, especially when the adverb describes
    the manner in which the action referred to by the verb is done. For example, “I
    carefully painted the house” and “I painted the house carefully” are both acceptable
    and mean the same thing. If you represent these sentences by a dependency grammar,
    the word “carefully” always modifies the verb “painted,” and the two sentences
    have completely identical dependency trees. Dependency grammars capture more than
    just phrasal structures of sentences—they capture something more fundamental about
    the relationship of the words. Therefore, dependency parsing is considered an
    important step toward semantic analysis of natural language. A group of researchers
    is working on a formal language-independent dependency grammar, called Universal
    Dependencies, that is linguistically motivated and applicable to many languages,
    similar to the universal POS tagset.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Text generation
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Text generation, also called *natural language generation* (NLG), is the process
    of generating natural language text from something else. In a broader sense, machine
    translation, which we discussed previously, involves a text-generation problem,
    because MT systems need to generate text in the target language. Similarly, summarization,
    text simplification, and grammatical error correction all produce natural language
    text as output and are instances of text-generation tasks. Because all of these
    tasks take natural language text as their input, they are called *text-to-text*
    generation.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Another class of text-generation task is called *data-to-text* generation. For
    those tasks, the input is data that is not text. For example, a dialog system
    needs to generate natural utterances based on the current state of the conversation.
    A publisher may wish to generate news text based on events such as sports game
    outcomes and weather. There is also a growing interest in generating natural language
    text that best describes a given image, called *image captioning*.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, a third class of text classification is unconditional text generation,
    where natural language text is generated randomly from a model. You can train
    models so that they can generate random academic papers, Linux source code, or
    even poems and play scripts. For example, Andrej Karpathy trained an RNN model
    from all of Shakespeare’s works and succeeded in generating pieces of text that
    look exactly like his work ([http://realworldnlpbook.com/ch1.html#karpathy15](http://realworldnlpbook.com/ch1.html#karpathy15)),
    as shown next:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Traditionally, text generation has been solved by handcrafted templates and
    rules for generating text from some information. You can think of this as the
    reverse of parsing, where rules are used to infer information about natural language
    text, as we discussed earlier. In recent years, neural network models are an increasingly
    popular choice for natural language generation, be it text-to-text generation
    (sequence-to-sequence models), data-to-text generation (encoder-decoder models),
    and unconditional text generation (neural language models and generative adversarial
    networks, or GANs). We’ll discuss text generation more in chapter 5.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 Building NLP applications
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, I’m going to show you how NLP applications are typically developed
    and structured. Although details may vary on a case-by-case basis, understanding
    the typical process helps you plan and budget before you start developing an application.
    It also goes a long way if you know best practices in developing NLP applications
    beforehand.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.1 Development of NLP applications
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The development of NLP applications is a highly iterative process, consisting
    of many phases of research, development, and operations (figure 1.9). Most learning
    materials such as books and online tutorials focus mainly on the training phase,
    although all the other phases of application development are equally important
    for real-world NLP applications. In this section, I briefly introduce what each
    stage involves. Note that no clear boundary exists between these phases. It is
    not uncommon that application developers (researchers, engineers, managers, and
    other stakeholders) go back and forth between some of these phases through trial
    and error.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '![CH01_F09_Hagiwara](../Images/CH01_F09_Hagiwara.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
- en: Figure 1.9 The development cycle of NLP applications
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Data Collection
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Most modern NLP applications are based on machine learning. Machine learning,
    by definition, requires data on which NLP models are trained (remember the definition
    of ML we talked about previously—it’s about improving algorithms through data).
    In this phase, NLP application developers discuss how to formulate the application
    as an NLP/ML problem and what kind of data should be collected. Data can be collected
    from humans (e.g., by hiring in-house annotators and having them go through a
    bunch of text instances), crowdsourcing (e.g., using platforms such as Amazon
    Mechanical Turk), or automated mechanisms (e.g., from application logs or clickstreams).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: You may choose not to use machine learning approaches for your NLP application
    at first, which could totally be the right choice depending on various factors,
    such as time, budgets, the complexity of the task, and the expected amount of
    data you might be able to collect. Even in that case, it may be a good idea to
    collect a small amount of data for validation purposes. I’ll talk more about training,
    validation, and testing of NLP applications in chapter 11.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Analysis and experimenting
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: 'After collecting the data, you move on to the next phase where you analyze
    and run some experiments. For analyses, you usually look for signals such as:
    What are the characteristics of the text instances? How are the training labels
    distributed? Can you come up with signals that are correlated with the training
    labels? Can you come up with some simple rules that can predict the training labels
    with reasonable accuracy? Should we even use ML? This list goes on and on. This
    analysis phase includes aspects of data science, where various statistical techniques
    may come in handy.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'You run experiments to try a number of prototypes quickly. The goal in this
    phase is to narrow down the possible set of approaches to a couple of promising
    ones, before you go all-in and start training a gigantic model. By running experiments,
    you wish to answer questions including: What types of NLP tasks and approaches
    are appropriate for this NLP application? Is this a classification, parsing, sequence
    labeling, regression, text generation, or some other problem? What is the performance
    of the baseline approach? What is the performance of the rule-based approach?
    Should we even use ML? What is the estimate of training and serving time for the
    promising approaches?'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: I call these first two phases the “research” phase. The existence of this phase
    is arguably the biggest difference between NLP applications and other generic
    software systems. Due to its nature, it is difficult to predict the performance
    and the behavior of a machine learning system, or an NLP system, for that matter.
    At this point, you might not have written a single line of production code, and
    that’s totally fine. The point of this research phase is to prevent you from wasting
    your effort writing production code that turns out to be useless at a later stage.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Training
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: At this point you have pretty clear ideas what the approaches will be for your
    NLP application. This is when you start adding more data and computational resources
    (e.g., GPUs) for training your model. It is not uncommon for modern NLP models
    to take days if not weeks to train, especially if they are based on neural network
    models. It is always a good practice to gradually ramp up the amount of the data
    and the size of the model you train. You don’t want to spend weeks training a
    gigantic neural network model only to find that a smaller and simpler model performs
    just as well, or even worse, that you introduced a bug in the model and that the
    model you spent weeks training is simply useless!
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: It is critical at this phase that you keep your training pipeline reproducible.
    Chances are, you will need to run this several times with different sets of hyperparameters,
    which are tuning values set before starting the model’s learning process. It is
    also likely that you will need to run this pipeline several months later, if not
    years. I’ll touch upon some best practices when training NLP/ML models in chapter
    10.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'When you have a model that is working with acceptable performance, you move
    on to the implementation phase. This is when you start making your application
    “production ready.” This process basically follows software engineering best practices,
    including: writing unit and integration tests for your NLP modules, refactoring
    your code, having your code reviewed by other developers, improving the performance
    of your NLP modules, and dockerizing your application. I’ll talk more about this
    process in chapter 11.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Deploying
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Your NLP application is finally ready to deploy. You can deploy your NLP application
    in many ways—it can be an online service, a recurring batch job, an offline application,
    or an offline one-off task. If this is an online service that needs to serve its
    predictions in real time, it is a good idea to make this a *microservice* to make
    it loosely coupled with other services. In any case, it is a good practice to
    use continuous integration (CI) for your application, where you run tests and
    verify that your code and model are working as intended every time you make changes
    to your application.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'An important final step for developing NLP applications is monitoring. This
    not only includes monitoring the infrastructure such as server CPU, memory, and
    request latency, but also higher-level ML statistics such as the distributions
    of the input and the predicted labels. Some of the important questions to ask
    at this stage are: What do the input instances look like? Are they what you expected
    when you built your model? What do the predicted labels look like? Does the predicted
    label distribution match the one in the training data? The purpose of the monitoring
    is to check that the model you built is behaving as intended. If the incoming
    text or data instances or the predicted labels do not match your expectation,
    you may have an out-of-domain problem, meaning that the domain of the natural
    language data you are receiving is different from the one in which your model
    is trained. Machine learning models are usually not good at dealing with out-of-domain
    data, and the prediction accuracy may suffer. If this issue becomes obvious, it
    may be a good idea to repeat the whole process again, starting from collecting
    more in-domain data.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.2 Structure of NLP applications
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The structures of modern, machine learning-based NLP applications are becoming
    surprisingly similar for two main reasons—one is that most modern NLP applications
    rely on machine learning to some degree, and they should follow best practices
    for machine learning applications. The other is that, due to the advent of neural
    network models, a number of NLP tasks, including text classification, machine
    translation, dialog systems, and speech recognition, can now be trained end-to-end,
    as I mentioned before. Some of these tasks used to be hairy, enormous monsters
    with dozens of components with complex plumbing. Now, however, some of these tasks
    can be solved by less than 1,000 lines of Python code, provided that there’s enough
    data to train the model end-to-end.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.10 illustrates the typical structure of a modern NLP application.
    There are two main infrastructures: the training and the serving infrastructure.
    The training infrastructure is usually offline and serves the purpose of training
    the machine learning model necessary for the application. It takes the training
    data, converts it to some data structure that can be handled by the pipeline,
    and further processes it by transforming the data and extracting the features.
    This part varies greatly from task to task. Finally, if the model is a neural
    network, data instances are batched and fed to the model, which is optimized to
    minimize the loss. Don’t worry if you don’t understand what I’m talking about
    in that last sentence—we’ll talk about those technical terms used with neural
    networks in chapter 2\. The trained model is usually serialized and stored to
    be passed to the serving infrastructure.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '![CH01_F10_Hagiwara](../Images/CH01_F10_Hagiwara.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
- en: Figure 1.10 Structure of typical NLP applications
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: The serving infrastructure’s job is to, given a new instance, produce the prediction,
    such as classes, tags, or translations. The first part of this infrastructure,
    which reads the instance and transforms it into some numbers, is similar to the
    one for training. In fact, you must keep the dataset reader and the transformer
    identical. Otherwise, discrepancies will arise in the way those two process the
    data, also known as *training-serving skew*. After the instance is processed,
    it’s fed to the pretrained model to produce the prediction. I’ll talk more about
    designing your NLP applications in chapter 11.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Natural language processing (NLP) is a subfield of artificial intelligence (AI)
    that refers to computational approaches to process, understand, and generate human
    language.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the challenges for NLP is ambiguity in natural languages. There is syntactic
    and semantic ambiguity.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where there is text, there is NLP. Many tech companies use NLP to draw information
    from a large amount of text. Typical NLP applications include machine translation,
    grammatical error correction, search engines, and dialog systems.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NLP applications are developed in an iterative way, with more emphasis on the
    research phase.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many modern NLP applications rely heavily on machine learning (ML) and are structurally
    similar to ML systems.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
