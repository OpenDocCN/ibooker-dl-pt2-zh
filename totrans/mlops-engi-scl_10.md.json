["```py\nimport torch as pt\nx = pt.tensor(3., requires_grad=True)     ❶\ny = x ** 2\nfor _ in range(5):\n  y.backward(retain_graph=True)           ❷\n  print(x.grad)\n```", "```py\ntensor(6.)\ntensor(12.)\ntensor(18.)\ntensor(24.)\ntensor(30.)\n```", "```py\npt.manual_seed(42)                                     ❶\nFEATURES = 4                                           ❷\nTRAINING_DATASET_SIZE = 1000                           ❸\n\nX_train = pt.distributions.multivariate_normal.\n➥   MultivariateNormal(                               ❹\n  pt.arange(FEATURES, dtype=pt.float32),               ❺\n  pt.eye(FEATURES)).sample((TRAINING_DATASET_SIZE,))   ❻\n\ny_train = X_train @ (pt.arange(FEATURES,\n                      dtype=pt.float32) + 1)           ❼\n```", "```py\nprint(X_train[0, :] @ pt.tensor([1, 2, 3, 4], dtype = pt.float32))\n```", "```py\ntensor(19.1816)\n```", "```py\nprint(X_train.shape, y_train.shape)\n```", "```py\n(torch.Size([1000, 4]), torch.Size([1000]))\n```", "```py\npt.manual_seed(42)\nw = pt.randn(FEATURES, requires_grad = True)     ❶\ndef forward(w, X):                               ❷\n  return X @ w\n\ndef mse(y_est, y):\n  err = y_est - y                                ❸\n  return (err ** 2).mean()                       ❹\n```", "```py\nEPOCHS = 500\nLEARNING_RATE = 0.01\nIN_MEMORY_SHARD_SIZE = 250\n\nfor epoch in range(EPOCHS):\n  for i in range(0, \\\n  TRAINING_DATASET_SIZE // IN_MEMORY_SHARD_SIZE):   ❶\n\n    start_idx = i * IN_MEMORY_SHARD_SIZE\n    end_idx = start_idx + IN_MEMORY_SHARD_SIZE\n    y_shard = y_train[start_idx : end_idx]\n    X_shard = X_train[start_idx : end_idx]          ❷\n\n    y_est = forward(w, X_shard)                     ❸\n    loss = \\                                        ❹\n      (IN_MEMORY_SHARD_SIZE / TRAINING_DATASET_SIZE) * mse(y_est, y_shard)\n\n    loss.backward()                                 ❺\n\n  #notice that the following is\n  #in scope of the outer for loop\n  w.data -= LEARNING_RATE * w.grad                  ❻\n  w.grad = None                                     ❼\n```", "```py\nprint(w)\n```", "```py\ntensor([1.0000, 2.0000, 3.0000, 4.0000], requires_grad=True)\n```", "```py\n(1 / IN_MEMORY_SHARD_SIZE) * ((y_est - y_shard) ** 2).sum()\n```", "```py\nNODES = \\\n  TRAINING_DATASET_SIZE // IN_MEMORY_SHARD_SIZE       ❶\n\nGRADIENTS = [5., 3., 2., 1.]                          ❷\n\nnode_to_gradients = \\\n  dict(zip(range(NODES), GRADIENTS))                  ❸\n\nfor iter in range(NODES - 1):                         ❹\n  node = (iter + 1) % NODES                           ❺\n  grad = node_to_gradients[node]                      ❻\n  next_node = (node + 1) % NODES                      ❼\n\n  # simulate \"sending\" of the gradient value\n  # over the network to the next node in the ring\n  node_to_gradients[next_node] += grad                ❽\n```", "```py\nprint(node_to_gradients)\n```", "```py\n{0: 11.0, 1: 3.0, 2: 5.0, 3: 6.0}\n```", "```py\npt.manual_seed(42)\nw_src = pt.randn((4,))\nW = [pt.tensor(w_src.detach().numpy(),\n                requires_grad=True) for _ in range(NODES)]\n```", "```py\nfrom torch.utils.data import TensorDataset, DataLoader\ntrain_dl = DataLoader(TensorDataset(y_train, X_train), \\\n                      batch_size = IN_MEMORY_SHARD_SIZE,\n                      shuffle = False)\n\nfor node, (y_shard, X_shard) in zip(range(NODES), train_dl):\n  y_est = forward(W[node], X_shard)\n  loss = \\\n    (IN_MEMORY_SHARD_SIZE / TRAINING_DATASET_SIZE) * mse(y_shard, y_est)\n  loss.backward()\n```", "```py\n[W[node].grad for node in range(NODES)]\n```", "```py\n[tensor([ -0.1776, -10.4762, -19.9037, -31.2003]),\n tensor([  0.0823, -10.3284, -20.6617, -30.2549]),\n tensor([ -0.1322, -10.9773, -20.4698, -30.2835]),\n tensor([  0.1597, -10.4902, -19.8841, -29.5041])]\n```", "```py\nfor iter in range(NODES - 1):\n  for node in range(NODES):\n    seg = (node - iter - 1) % NODES     ❶\n    grad = W[node].grad[seg]            ❷\n\n    next_node = (node + 1) % NODES\n    W[next_node].grad[seg] += grad      ❸\n```", "```py\nprint([f\"{W[node].grad}\" for node in range(NODES)])\n```", "```py\n['tensor([ -0.0679, -31.9437, -39.7879, -31.2003])',\n 'tensor([  0.0823, -42.2722, -60.4496, -61.4552])',\n 'tensor([-4.9943e-02, -1.0977e+01, -8.0919e+01, -9.1739e+01])',\n 'tensor([ 1.0978e-01, -2.1468e+01, -1.9884e+01, -1.2124e+02])'].\n```", "```py\nprint([f\"{W[node].grad[node]}\" for node in range(NODES)]),\n```", "```py\n['-0.06785149872303009', '-42.27215576171875', '-80.91938018798828', '-121.24281311035156']\n```", "```py\nfor iter in range(NODES - 1):\n  for node in range(NODES):\n    seg = (node - iter) % NODES       ❶\n    grad = W[node].grad[seg]\n\n    next_node = (node + 1) % NODES\n    W[next_node].grad[seg] = grad     ❷\n```", "```py\nprint([f\"{W[node].grad}\" for node in range(NODES)]),\n```", "```py\n['tensor([-6.7851e-02, -4.2272e+01, -8.0919e+01, -1.2124e+02])',\n 'tensor([-6.7851e-02, -4.2272e+01, -8.0919e+01, -1.2124e+02])',\n 'tensor([-6.7851e-02, -4.2272e+01, -8.0919e+01, -1.2124e+02])',\n 'tensor([-6.7851e-02, -4.2272e+01, -8.0919e+01, -1.2124e+02])']\n```", "```py\nimport torch as pt\nfrom torch.utils.data import TensorDataset, DataLoader\n\nIN_MEMORY_SHARD_SIZE = 250\nTRAINING_DATASET_SIZE = 1000\nNODES = TRAINING_DATASET_SIZE // IN_MEMORY_SHARD_SIZE\n\nFEATURES = 4\npt.manual_seed(42)\nw_src = pt.randn((FEATURES,))\nW = [pt.tensor(w_src.detach().numpy(),\n                requires_grad=True) for _ in range(NODES)]\n\ndef forward(w, X):\n  return X @ w\n\ndef mse(y_est, y):\n  err = y_est - y\n  return (err ** 2).mean()\n\nX_train = pt.distributions.multivariate_normal.MultivariateNormal(\n    pt.arange(FEATURES, dtype=pt.float32),\n    pt.eye(FEATURES)).sample((TRAINING_DATASET_SIZE,))\ny_train = X_train @ (pt.arange(FEATURES, dtype=pt.float32) + 1)\ntrain_dl = DataLoader(TensorDataset(y_train, X_train), \\\n                      batch_size = IN_MEMORY_SHARD_SIZE,\n                      shuffle = False)\n\nEPOCHS = 1000\nLEARNING_RATE = 0.01\nfor epoch in range(EPOCHS):\n\n  #compute per shard gradients on each node\n  for node, (y_shard, X_shard) in zip(range(NODES), train_dl):\n    y_est = forward(W[node], X_shard)\n    loss = \\\n      (IN_MEMORY_SHARD_SIZE / TRAINING_DATASET_SIZE) * mse(y_shard, y_est)\n    loss.backward()\n\n  #horovod phase 1: reduce-scatter\n  for iter in range(NODES - 1):\n    for node in range(NODES):\n      seg = (node - iter - 1) % NODES\n      grad = W[node].grad[seg]\n\n      next_node = (node + 1) % NODES\n      W[next_node].grad[seg] += grad\n\n  #horovod phase 2: all-gather\n  for iter in range(NODES - 1):\n    for node in range(NODES):\n      seg = (node - iter) % NODES\n      grad = W[node].grad[seg]\n\n      next_node = (node + 1) % NODES\n      W[next_node].grad[seg] = grad\n\n  #perform a step of gradient descent\n  for node in range(NODES):\n    W[node].data -= LEARNING_RATE * W[node].grad\n    W[node].grad = None\n\nprint([f\"{W[node].data}\" for node in range(NODES)])\n```", "```py\n['tensor([1.0000, 2.0000, 3.0000, 4.0000])',\n 'tensor([1.0000, 2.0000, 3.0000, 4.0000])',\n 'tensor([1.0000, 2.0000, 3.0000, 4.0000])',\n 'tensor([1.0000, 2.0000, 3.0000, 4.0000])']\n```"]