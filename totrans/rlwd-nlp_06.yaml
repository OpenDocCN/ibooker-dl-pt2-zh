- en: 4 Sentence classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 句子分类
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容包括
- en: Handling variable-length input with recurrent neural networks (RNN)
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用循环神经网络（RNN）处理长度可变的输入
- en: Working with RNNs and their variants (LSTMs and GRUs)
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理RNN及其变体（LSTM和GRU）
- en: Using common evaluation metrics for classification problems
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用常见的分类问题评估指标
- en: Developing and configuring a training pipeline using AllenNLP
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用AllenNLP开发和配置训练管道
- en: Building a language detector as a sentence classification task
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将语言识别器构建为句子分类任务
- en: In this chapter, we are going to study the task of sentence classification,
    where an NLP model receives a sentence and assigns some label to it. A spam filter
    is an application of sentence classification. It receives an email message and
    assigns whether or not it is spam. If you want to classify news articles into
    different topics (business, politics, sports, and so on), it’s also a sentence-classification
    task. Sentence classification is one of the simplest NLP tasks that has a wide
    range of applications, including document classification, spam filtering, and
    sentiment analysis. Specifically, we are going to revisit the sentiment classifier
    we introduced in chapter 2 and discuss its components in detail. At the end of
    this section, we are going to study another application of sentence classification—language
    detection.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将研究句子分类任务，其中NLP模型接收一个句子并为其分配一些标签。垃圾邮件过滤器是句子分类的一个应用。它接收一封电子邮件并指定它是否为垃圾邮件。如果你想将新闻文章分类到不同的主题（商业、政治、体育等），也是一个句子分类任务。句子分类是最简单的NLP任务之一，具有广泛的应用范围，包括文档分类、垃圾邮件过滤和情感分析。具体而言，我们将重新审视第2章中介绍的情感分类器，并详细讨论其组成部分。在本节结束时，我们将研究句子分类的另一个应用——语言检测。
- en: 4.1 Recurrent neural networks (RNNs)
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 循环神经网络（RNN）
- en: The first step in sentence classification is to represent variable-length sentences
    using neural networks (RNNs). In this section, I’m going to present the concept
    of recurrent neural networks, one of the most important concepts in deep NLP.
    Many modern NLP models use RNNs in some way. I’ll explain why they are important,
    what they do, and introduce their simplest variant.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 句子分类的第一步是使用神经网络（RNN）表示长度可变的句子。在本节中，我将介绍循环神经网络的概念，这是深度NLP中最重要的概念之一。许多现代NLP模型以某种方式使用RNN。我将解释它们为什么很重要，它们的作用是什么，并介绍它们的最简单变体。
- en: 4.1.1 Handling variable-length input
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.1 处理长度可变的输入
- en: The Skip-gram network structure shown in the previous chapter was simple. It
    takes a word vector of a fixed size, runs it through a linear layer, and obtains
    a distribution of scores over all the context words. The structure and the size
    of the input, output, and the network are all fixed throughout the training.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 上一章中展示的Skip-gram网络结构很简单。它接受一个固定大小的词向量，通过线性层运行它，得到所有上下文词之间的分数分布。网络的结构和大小以及输入输出都在训练期间固定。
- en: However, many, if not most, of what we deal with in NLP are sequences of variable
    lengths. For example, words, which are sequences of characters, can be short (“a,”
    “in”) or long (“internationalization”). Sentences (sequences of words) and documents
    (sequences of sentences) can be of any lengths. Even characters, if you look at
    them as sequences of strokes, can be simple (e.g., “O” and “L” in English) or
    more complex (e.g., “鬱” is a Chinese character meaning “depression” which, depressingly,
    has 29 strokes).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，自然语言处理（NLP）中面临的许多，如果不是大多数情况下，都是长度可变的序列。例如，单词是字符序列，可以短（“a”，“in”）也可以长（“internationalization”）。句子（单词序列）和文档（句子序列）可以是任何长度。即使是字符，如果将它们看作笔画序列，则可以是简单的（如英语中的“O”和“L”）或更复杂的（例如，“鬱”是一个包含29个笔画，并表示“抑郁”的中文汉字）。
- en: As we discussed in the previous chapter, neural networks can handle only numbers
    and arithmetic operations. That was why we needed to convert words and documents
    to numbers through embeddings. We used linear layers to convert a fixed-length
    vector into another. But to do something similar with variable-length inputs,
    we need to figure out how to structure the neural networks so that they can handle
    them.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一章中讨论的那样，神经网络只能处理数字和算术运算。这就是为什么我们需要通过嵌入将单词和文档转换为数字的原因。我们使用线性层将一个固定长度的向量转换为另一个向量。但是，为了处理长度可变的输入，我们需要找到一种处理方法，使得神经网络可以对其进行处理。
- en: 'One idea is to first convert the input (e.g., a sequence of words) to embeddings,
    that is, a sequence of vectors of floating-point numbers, then average them. Let’s
    assume the input sentence is sentence = ["john", "loves", "mary", "."], and you
    already know word embeddings for each word in the sentence v("john"), v("loves"),
    and so on. The average can be obtained with the following code and illustrated
    in figure 4.1:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一个想法是首先将输入（例如，一系列单词）转换为嵌入，即一系列浮点数向量，然后将它们平均。假设输入句子为sentence = ["john", "loves",
    "mary", "."]，并且你已经知道句子中每个单词的单词嵌入v("john")、v("loves")等。平均值可以用以下代码获得，并在图4.1中说明：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![CH04_F01_Hagiwara](../Images/CH04_F01_Hagiwara.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F01_Hagiwara](../Images/CH04_F01_Hagiwara.png)'
- en: Figure 4.1 Averaging embedding vectors
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1 平均嵌入向量
- en: This method is quite simple and is actually used in many NLP applications. However,
    it has one critical issue, which is that it cannot take word order into account.
    Because the order of input elements doesn’t affect the result of averaging, you’d
    get the same vector for both “Mary loves John” and “John loves Mary.” Although
    it’s up to the task in hand, it’s hard to imagine many NLP applications would
    want this kind of behavior.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法相当简单，实际上在许多自然语言处理应用中都有使用。然而，它有一个关键问题，即它无法考虑词序。因为输入元素的顺序不影响平均结果，你会得到“Mary
    loves John”和“John loves Mary”两者相同的向量。尽管它能胜任手头的任务，但很难想象有多少自然语言处理应用会希望这种行为。
- en: If we step back and reflect how we humans read language, this “averaging” is
    far from actuality. When we read a sentence, we don’t usually read individual
    words in isolation and remember them first, then move on to figuring out what
    the sentence means. We usually scan the sentence from the beginning, one word
    at a time, while holding what the “partial” sentence means up until the part you
    are reading in our short-term memory. In other words, you maintain some sort of
    mental representation of the sentence while you read it. When you reach the end
    of the sentence, the mental representation is its meaning.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们退后一步，思考一下我们人类如何阅读语言，这种“平均”与现实相去甚远。当我们阅读一句话时，我们通常不会孤立地阅读单个单词并首先记住它们，然后再弄清楚句子的含义。我们通常从头开始扫描句子，逐个单词地阅读，同时将“部分”句子在我们的短期记忆中的含义保持住直到你正在阅读的部分。换句话说，你在阅读时保持了一种对句子的心理表征。当你达到句子的末尾时，这种心理表征就是它的含义。
- en: Can we design a neural network structure that simulates this incremental reading
    of the input? The answer is a resounding yes. That structure is called *recurrent
    neural networks* (RNNs), which I’ll explain in detail next.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是否可以设计一个神经网络结构来模拟这种对输入的逐步阅读？答案是肯定的。这种结构被称为*循环神经网络*（RNNs），我将在接下来详细解释。
- en: 4.1.2 RNN abstraction
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.2 RNN抽象
- en: 'If you break down the reading process mentioned earlier, its core is the repetition
    of the following series of operations:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你分解前面提到的阅读过程，其核心是以下一系列操作的重复：
- en: Read a word.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 阅读一个词。
- en: Based on what has been read so far (your “mental state”), figure out what the
    word means.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据迄今为止所阅读的内容（你的“心理状态”），弄清楚这个词的含义。
- en: Update the mental state.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新心理状态。
- en: Move on to the next word.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续下一个词。
- en: 'Let’s see how this works using a concrete example. If the input sentence is
    sentence = ["john", "loves", "mary", "."], and each word is already represented
    as a word-embedding vector. Also, let’s denote your “mental state” as state, which
    is initialized by init_state(). Then, the reading process is represented by the
    following incremental operations:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个具体的例子来看看这是如何工作的。如果输入句子是sentence = ["john", "loves", "mary", "."]，并且每个单词已经表示为单词嵌入向量。另外，让我们将你的“心理状态”表示为state，它由init_state()初始化。然后，阅读过程由以下递增操作表示：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The final value of state becomes the representation of the entire sentence from
    this process. Notice that if you change the order in which these words are processed
    (e.g., by flipping “John” and “Mary”), the final value of state also changes,
    meaning that the state also encodes some information about the word order.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: state的最终值成为此过程中整个句子的表示。请注意，如果你改变这些单词处理的顺序（例如，交换“John”和“Mary”），state的最终值也会改变，这意味着state也编码了一些有关词序的信息。
- en: 'You can achieve something similar if you can design a network substructure
    that is applied to each element of the input while updating some internal states.
    RNNs are neural network structures that do exactly this. In a nutshell, an RNN
    is a neural network with a loop. At its core is an operation that is applied to
    every element in the input as they come in. If you wrote what RNNs do in Python
    pseudocode, it’d be like the following:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你可以设计一个网络子结构，可以在更新一些内部状态的同时应用于输入的每个元素，那么你可以实现类似的功能。RNNs 就是完全这样做的神经网络结构。简而言之，RNN
    是一个带有循环的神经网络。其核心是在输入中的每个元素上应用的操作。如果你用 Python 伪代码来表示 RNN 做了什么，就会像下面这样：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Notice that there is state that gets initialized first and passed around during
    the iteration. For every input word, state is updated based on the previous state
    and the input using the function update. The network substructure corresponding
    to this step (the code block inside the loop) is called a *cell*. This stops when
    the input is exhausted, and the final value of state becomes the result of this
    RNN. See figure 4.2 for the illustration.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这里有一个被初始化并在迭代过程中传递的状态。对于每个输入单词，状态会根据前一个状态和输入使用`update`函数进行更新。对应于这个步骤（循环内的代码块）的网络子结构被称为*单元*。当输入用尽时，这个过程停止，状态的最终值成为该
    RNN 的结果。见图 4.2 进行说明。
- en: '![CH04_F02_Hagiwara](../Images/CH04_F02_Hagiwara.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F02_Hagiwara](../Images/CH04_F02_Hagiwara.png)'
- en: Figure 4.2 RNN abstraction
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 RNN 抽象
- en: You can see the parallelism here. When you are reading a sentence (sequence
    of words), your internal mental representation of the sentence, state, is updated
    after reading each word. You can assume that the final state encodes the representation
    of the entire sentence.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里你可以看到并行性。当你阅读一个句子（一串单词）时，每读一个单词后你内部心理对句子的表示，即状态，会随之更新。可以假设最终状态编码了整个句子的表示。
- en: The only remaining work is to design two functions—init_state() and update().
    The state is usually initialized with zero (i.e., a vector filled with zeros),
    so you usually don’t have to worry about how to go about defining the former.
    It’s more important how you design update(), which determines the characteristics
    of the RNN.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一剩下的工作是设计两个函数——`init_state()` 和 `update()`。通常，状态初始化为零（即一个填满零的向量），所以你通常不用担心如何定义前者。更重要的是如何设计
    `update()`，它决定了 RNN 的特性。
- en: 4.1.3 Simple RNNs and nonlinearity
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.3 简单 RNNs 和非线性
- en: 'In section 3.4.3, I talked about how to implement a linear layer with an arbitrary
    number of inputs and outputs. Can we do something similar and implement update(),
    which is basically a function that takes two input variables and produces one
    output variable? After all, a cell is a neural network with its own input and
    output, right? The answer is yes, and it looks like this:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 3.4.3 节中，我解释了如何使用任意数量的输入和输出来实现一个线性层。我们是否可以做类似的事情，并实现`update()`，它基本上是一个接受两个输入变量并产生一个输出变量的函数呢？毕竟，一个单元是一个有自己输入和输出的神经网络，对吧？答案是肯定的，它看起来像这样：
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Notice that this is strikingly similar to the linear2() function in section
    3.4.3\. In fact, if you ignore the difference in variable names, it’s exactly
    the same except for the f() function. An RNN defined by this type of the update
    function is called a *simple RNN* or *Elman RNN*, which is, as its name suggests,
    one of the simplest RNN structures.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这与第 3.4.3 节中的 `linear2()` 函数非常相似。实际上，如果忽略变量名称的差异，除了 `f()` 函数之外，它们是完全一样的。由此类型的更新函数定义的
    RNN 被称为*简单 RNN*或*Elman RNN*，正如其名称所示，它是最简单的 RNN 结构之一。
- en: You may be wondering, then, what is this function f() doing here? What does
    it look like? Do we need it here at all? The function, called an *activation function*
    or *nonlinearity*, takes a single input (or a vector) and transforms it (or every
    element of a vector) in a nonlinear fashion. Many kinds of nonlinearities exist,
    and they play an indispensable role in making neural networks truly powerful.
    What they exactly do and why they are important requires some math to understand,
    which is outside the scope of this book, but I’ll attempt an intuitive explanation
    with a simple example next.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，这里的 `f()` 函数是做什么的？它是什么样的？我们是否需要它？这个函数被称为*激活函数*或*非线性函数*，它接受一个输入（或一个向量）并以非线性方式转换它（或转换向量的每个元素）。存在许多种非线性函数，它们在使神经网络真正强大方面起着不可或缺的作用。它们确切地做什么以及为什么它们重要需要一些数学知识来理解，这超出了本书的范围，但我将尝试用一个简单的例子进行直观解释。
- en: Imagine you are building an RNN that recognizes “grammatical” English sentences.
    Differentiating grammatical sentences from ungrammatical ones is itself a difficult
    NLP problem, which is, in fact, a well-established research area (see section
    1.2.1), but here, let’s simplify it and consider agreement only between the subject
    and the verb. Let’s further simplify it and assume that there are only four words
    in this “language”—“I,” “you,” “am,” and “are.” If the sentence is either “I am”
    or “you are,” it’s grammatically correct. The other two combinations, “I are”
    and “you am,” are incorrect. What you want to build is an RNN that outputs 1 for
    the correct sentences while producing 0 for the incorrect ones. How would you
    go about building such a neural network?
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下你正在构建一个识别“语法正确”的英语句子的 RNN。区分语法正确的句子和不正确的句子本身就是一个困难的自然语言处理问题，实际上是一个成熟的研究领域（参见第1.2.1节），但在这里，让我们简化它，并考虑主语和动词之间的一致性。让我们进一步简化，并假设这个“语言”中只有四个词——“I”，“you”，“am”
    和 “are”。如果句子是“I am” 或 “you are”，那么它就是语法正确的。另外两种组合，“I are” 和 “you am”，是不正确的。你想要构建的是一个
    RNN，对于正确的句子输出1，对于不正确的句子输出0。你会如何构建这样一个神经网络？
- en: The first step in almost every modern NLP model is to represent words with embeddings.
    As mentioned in the previous chapter, they are usually learned from a large dataset
    of natural language text, but here, we are simply going to give them some predefined
    values, as shown in figure 4.3.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎每个现代 NLP 模型的第一步都是用嵌入来表示单词。如前一章所述，它们通常是从大型自然语言文本数据集中学习到的，但在这里，我们只是给它们一些预定义的值，如图4.3所示。
- en: '![CH04_F03_Hagiwara](../Images/CH04_F03_Hagiwara.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F03_Hagiwara](../Images/CH04_F03_Hagiwara.png)'
- en: Figure 4.3 Recognizing grammatical English sentences using an RNN
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3 使用 RNN 识别语法正确的英语句子
- en: 'Now, let’s imagine there was no activation function. The previous update_ simple()
    function simplifies to the following:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们假设没有激活函数。前面的 update_simple() 函数简化为以下形式：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We will assume the initial value of state is simply [0, 0], because the specific
    initial values are not relevant to the discussion here. The RNN takes the first
    word embedding, x1, updates state, takes the second word embedding, x2, then produces
    the final state, which is a two-dimensional vector. Finally, the two elements
    in this vector are summed and converted to result. If result is close to 1, the
    sentence is grammatical. Otherwise, it is not. If you apply the update_simple_linear()
    function twice and simplify it a little bit, you get the following function, which
    is all this RNN does, after all:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将假设状态的初始值简单地为 [0, 0]，因为具体的初始值与此处的讨论无关。RNN 接受第一个单词嵌入 x1，更新状态，接受第二个单词嵌入 x2，然后生成最终状态，即一个二维向量。最后，将这个向量中的两个元素相加并转换为
    result。如果 result 接近于1，则句子是语法正确的。否则，不是。如果你应用 update_simple_linear() 函数两次并稍微简化一下，你会得到以下函数，这就是这个
    RNN 的全部功能：
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Remember, w1, w2, and b are parameters of the model (aka “magic constants”)
    that need to be trained (adjusted). Here, instead of adjusting these parameters
    using a training dataset, let’s assign some arbitrary values and see what happens.
    For example, when w1 = [1, 0], w2 = [0, 1], and b = [0, 0], the input and the
    output of this RNN will be as shown in figure 4.4.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，w1、w2 和 b 是模型的参数（也称为“魔法常数”），需要进行训练（调整）。在这里，我们不是使用训练数据集调整这些参数，而是将一些任意值赋给它们，然后看看会发生什么。例如，当
    w1 = [1, 0]，w2 = [0, 1]，b = [0, 0] 时，这个 RNN 的输入和输出如图4.4所示。
- en: '![CH04_F04_Hagiwara](../Images/CH04_F04_Hagiwara.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F04_Hagiwara](../Images/CH04_F04_Hagiwara.png)'
- en: Figure 4.4 Input and output when w1 = [1, 0], w2 = [0, 1], and b = [0, 0] without
    an activation function
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4 当 w1 = [1, 0]，w2 = [0, 1]，b = [0, 0] 且没有激活函数时的输入和输出
- en: If you look at the values of result, this RNN groups ungrammatical sentences
    (e.g., “I are”) with grammatical ones (e.g., “you are”), which is not the desired
    behavior. How about we try another set of values for the parameters? Let’s use
    w1 = [1, 0], w2 = [-1, 0], and b = [0, 0] and see what happens (see figure 4.5
    for the result).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看结果的值，这个 RNN 将不合语法的句子（例如，“I are”）与合语法的句子（例如，“you are”）分组在一起，这不是我们期望的行为。那么，我们尝试另一组参数值如何？让我们使用
    w1 = [1, 0]，w2 = [-1, 0]，b = [0, 0]，看看会发生什么（参见图4.5的结果）。
- en: '![CH04_F05_Hagiwara](../Images/CH04_F05_Hagiwara.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F05_Hagiwara](../Images/CH04_F05_Hagiwara.png)'
- en: Figure 4.5 Input and output when w1 = [1, 0], w2 = [-1, 0], and b = [0, 0] without
    an activation function
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5 当 w1 = [1, 0]，w2 = [-1, 0]，b = [0, 0] 且没有激活函数时的输入和输出
- en: This is much better, because the RNN is successful in grouping ungrammatical
    sentences by assigning 0 to both “I are” and “you am.” However, it also assigns
    completely opposite values (2 and -2) to grammatical sentences (“I am” and “you
    are”).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这好多了，因为 RNN 成功地通过将 “I are” 和 “you am” 都赋值为 0 来将不符合语法的句子分组。然而，它也给语法正确的句子（“I am”
    和 “you are”）赋予了完全相反的值（2 和 -2）。
- en: I’m going to stop here, but as it turns out, you cannot use this neural network
    to differentiate grammatical sentences from ungrammatical ones, no matter how
    hard you try. Despite the values you assign to the parameters, this RNN cannot
    produce results that are close enough to the desired values and, thus, are able
    to group sentences by their grammaticality.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我要在这里停下来，但事实证明，无论你如何努力，都不能使用这个神经网络区分语法正确的句子和不正确的句子。尽管你给参数分配了值，但这个 RNN 无法产生足够接近期望值的结果，因此无法根据它们的语法性将句子分组。
- en: Let’s step back and think why this is the case. If you look at the previous
    update function, all it does is multiply the input by some value and add them
    up. In more specific terms, it only transforms the input *in a linear fashion*.
    The result of this neural network always changes by some constant amount when
    you change the value of the input by some amount. But this is obviously not desirable—you
    want the result to be 1 only when the input variables are some specific values.
    In other words, you don’t want this RNN to be linear; you want it to be nonlinear.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们退一步思考为什么会出现这种情况。如果你看一下之前的更新函数，它所做的就是将输入乘以一些值然后相加。更具体地说，它只是*以线性方式*转换输入。当你改变输入的值时，这个神经网络的结果总是会以某个恒定的量变化。但显然这是不可取的——你希望结果只在输入变量是某些特定值时才为
    1。换句话说，你不希望这个 RNN 是线性的；你希望它是非线性的。
- en: To use an analogy, imagine you can use only assignment (“=”), addition (“+”),
    and multiplication (“*”) in your programming language. You can tweak the input
    values to some degree to come up with the result, but you can’t write more complex
    logic in such a restricted setting.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 用类比的方式来说，想象一下，假设你的编程语言只能使用赋值（“=”）、加法（“+”）和乘法（“*”）。你可以在这样受限制的环境中调整输入值以得到结果，但在这样的情况下，你无法编写更复杂的逻辑。
- en: 'Now let’s put the activation function f() back and see what happens. The specific
    activation function we are going to use is called *the hyperbolic tangent function*,
    or more commonly, *tanh*, which is one of the most commonly used activation functions
    in neural networks. The details of this function are not important in this discussion,
    but in a nutshell, it behaves as follows: tanh doesn’t do much to the input when
    it is close to zero, for example, 0.3 or -0.2\. In other words, the input passes
    through the function almost unchanged. When the input is far from zero, tanh tries
    to squeeze it between -1 and 1\. For example, when the input is large (say, 10.0),
    the output becomes very close to 1.0, whereas when it is small (say, -10.0), the
    output becomes almost -1.0\. This creates an effect similar to the OR logical
    gate (or an AND gate, depending on the weights) if two or more variables are fed
    into the activation function. The output of the gate becomes ON (~1) and OFF (~-1)
    depending on the input.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们把激活函数 f() 加回去，看看会发生什么。我们将使用的具体激活函数称为*双曲正切函数*，或者更常见的是*tanh*，它是神经网络中最常用的激活函数之一。在这个讨论中，这个函数的细节并不重要，但简而言之，它的行为如下：当输入接近零时，tanh
    对输入的影响不大，例如，0.3 或 -0.2。换句话说，输入几乎不经过函数而保持不变。当输入远离零时，tanh 试图将其压缩在 -1 和 1 之间。例如，当输入很大（比如，10.0）时，输出变得非常接近
    1.0，而当输入很小时（比如，-10.0）时，输出几乎为 -1.0。如果将两个或更多变量输入激活函数，这会产生类似于 OR 逻辑门（或 AND 门，取决于权重）的效果。门的输出根据输入变为开启（~1）和关闭（~-1）。
- en: When w1 = [-1, 2], w2 = [-1, 2], b = [0, 1], and the tanh activation function
    is used, the result of the RNN becomes a lot closer to what we desire (see figure
    4.6). If you round them to the closest integers, the RNN successfully groups sentences
    by their grammaticality.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 当 w1 = [-1, 2]，w2 = [-1, 2]，b = [0, 1]，并且使用 tanh 激活函数时，RNN 的结果更接近我们所期望的（见图 4.6）。如果将它们四舍五入为最接近的整数，RNN
    成功地通过它们的语法性将句子分组。
- en: '![CH04_F06_Hagiwara](../Images/CH04_F06_Hagiwara.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F06_Hagiwara](../Images/CH04_F06_Hagiwara.png)'
- en: Figure 4.6 Input and output when w1 = [-1, 2], w2 = [-1, 2], and b = [0, 1]
    with an activation function
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6 当 w1 = [-1, 2]，w2 = [-1, 2]，b = [0, 1] 且激活函数为时的输入和输出
- en: To use the same analogy, using activation functions in your neural networks
    is like using ANDs and ORs and IFs in your programming language, in addition to
    basic math operations like addition and multiplication. In this way, you can write
    complex logic and model complex interactions between input variables, like the
    example in this section.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 使用同样的类比，将激活函数应用于你的神经网络就像在你的编程语言中使用AND、OR和IF以及基本的数学运算，比如加法和乘法一样。通过这种方式，你可以编写复杂的逻辑并模拟输入变量之间的复杂交互，就像本节的例子一样。
- en: NOTE This example I use in this section is a slightly modified version of the
    popular XOR (or *exclusive-or*) example commonly seen in deep learning textbooks.
    This is the most basic and simplest example that can be solved by neural networks
    but not by other linear models.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本节中我使用的例子是流行的XOR（或*异或*）例子的一个略微修改版本，通常在深度学习教材中见到。这是神经网络可以解决但其他线性模型无法解决的最基本和最简单的例子。
- en: Some final notes on RNNs—they are trained just like any other neural networks.
    The final outcome is compared with the desired outcome using the loss function,
    then the difference between the two—the loss—is used for updating the “magic constants.”
    The magic constants are, in this case, w1, w2, and b in the update_simple() function.
    Note that the update function and its magic constants are identical across all
    the timesteps in the loop. This means that what RNNs are learning is a general
    form of updates that can be applied to any situation.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 关于RNN的一些最后说明——它们的训练方式与任何其他神经网络相同。最终的结果与期望结果使用损失函数进行比较，然后两者之间的差异——损失——用于更新“魔术常数”。在这种情况下，魔术常数是update_simple()函数中的w1、w2和b。请注意，更新函数及其魔术常数在循环中的所有时间步中都是相同的。这意味着RNN正在学习的是可以应用于任何情况的一般更新形式。
- en: 4.2 Long short-term memory units (LSTMs) and gated recurrent units (GRUs)
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 长短期记忆单元（LSTMs）和门控循环单元（GRUs）
- en: In fact, the simple RNNs that we discussed earlier are rarely used in real-world
    NLP applications due to one problem called the *vanishing gradients problem*.
    In this section, I’ll show the issue associated with simple RNNs and how more
    popular RNN architectures, namely LSTMs and GRUs, solve this particular problem.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们之前讨论过的简单RNN在真实世界的NLP应用中很少使用，因为存在一个称为*梯度消失问题*的问题。在本节中，我将展示与简单RNN相关的问题以及更流行的RNN架构，即LSTMs和GRUs，如何解决这个特定问题。
- en: 4.2.1 Vanishing gradients problem
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.1 梯度消失问题
- en: 'Just like any programming language, if you know the length of the input, you
    can rewrite a loop without using one. An RNN can also be rewritten without using
    a loop, which makes it look just like a regular neural network with many layers.
    For example, if you know that there are only six words in the input, the rnn()
    from earlier can be rewritten as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 就像任何编程语言一样，如果你知道输入的长度，你可以在不使用循环的情况下重写一个循环。RNN也可以在不使用循环的情况下重写，这使它看起来就像一个具有许多层的常规神经网络。例如，如果你知道输入中只有六个单词，那么之前的rnn()可以重写如下：
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Representing RNNs without loops is called *unrolling*. Now we know what update()
    looks like for a simple RNN (update_simple), so we can replace the function calls
    with their bodies, as shown here:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 不带循环的表示RNN被称为*展开*。现在我们知道简单RNN的update()是什么样子（update_simple），所以我们可以用其实体替换函数调用，如下所示：
- en: '[PRE7]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This is getting a bit ugly, but I just want you to notice the very deeply nested
    function calls and multiplications. Now, recall the task we wanted to accomplish
    in the previous section—classifying grammatical English sentences by recognizing
    subject-verb agreement. Let’s say the input is sentence = ["The", "books", "I",
    "read", "yesterday", "were"]. In this case, the innermost function call processes
    the first word “The,” the next one processes the second word “books,” and so on,
    all the way to the outermost function call, which processes “were.” If we rewrite
    the previous pseudocode slightly, as shown in the next code snippet, you can understand
    it more intuitively:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这变得有点丑陋，但我只是想让你注意到非常深度嵌套的函数调用和乘法。现在，回想一下我们在上一节中想要完成的任务——通过识别主谓一致来对语法正确的英语句子进行分类。假设输入是sentence
    = ["The", "books", "I", "read", "yesterday", "were"]。在这种情况下，最内层的函数调用处理第一个词“The”，下一个处理第二个词“books”，依此类推，一直到最外层的函数调用，处理“were”。如果我们稍微修改前面的伪代码，如下代码片段所示，你就可以更直观地理解它：
- en: '[PRE8]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: To recognize that the input is indeed a grammatical English sentence (or a prefix
    of a sentence), the RNN needs to retain the information about the subject (“the
    books”) in state until it sees the verb (“were”) without being distracted by anything
    in between (“I read yesterday”). In the previous pseudocode, the states are represented
    by the return values of function calls, so the information about the subject (return
    value of process_main_subject) needs to propagate all the way up in this chain
    until it reaches the outermost function (process_main_verb). This is starting
    to sound like a difficult task.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了识别输入确实是一句语法正确的英语句子（或一句句子的前缀），RNN需要保留有关主语（“书”）的信息在状态中，直到看到动词（“were”）而不会被中间的任何东西（“我昨天读了”）分散注意力。在先前的伪代码中，状态由函数调用的返回值表示，因此关于主题的信息（process_main_subject的返回值）需要在链中传播到达最外层函数（process_main_verb）。这开始听起来像是一项困难的任务。
- en: Things don’t look any better when it comes to training this RNN. RNNs, as well
    as any other neural networks, are trained using an algorithm called *backpropagation*.
    Backpropagation is a process where the components of a neural network communicate
    with previous components on how to adjust the parameters to minimize the loss.
    This is how it works for this particular case. First, you look at the outcome,
    that is, the return value of is_grammatical()and compare it with what you desire.
    The difference between these two is called the *loss*. The outermost function,
    is_grammatical(), basically has four ways to decrease the loss to make its output
    closer to what is desired—1) adjust w1 while fixing the return value of the nested
    function process_adverb(), 2) adjust w2, 3) adjust b, or 4) adjust the return
    value of process_adverb() while fixing the parameters. Adjusting the parameters
    (w1, w2, and b) is the easy part because the function knows the exact effect of
    adjusting each parameter to its return value. Adjusting the return value of the
    previous function, however, is not easy, because the caller has no idea about
    the inner workings of the function. Because of this, the caller tells the previous
    function (callee) to adjust its return values to minimize the loss. See figure
    4.7 for how the loss is propagated back to the parameters and previous functions.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及训练该RNN时，情况并不好。 RNN和其他任何神经网络都使用称为*反向传播*的算法进行训练。反向传播是一种过程，在该过程中，神经网络的组成部分与先前的组成部分通信，以便调整参数以最小化损失。对于这个特定的示例，它是如何工作的。首先，您查看结果，即is_grammatical的返回值()并将其与您期望的内容进行比较。这两者之间的差称为*损失*。最外层函数is_grammatical()基本上有四种方式来减少损失，使其输出更接近所需内容：1)调整w1，同时固定嵌套函数process_adverb()的返回值，2)调整w2，3)调整b，或4)调整process_adverb()的返回值，同时固定参数。调整参数（w1、w2和b）很容易，因为函数知道调整每个参数对其返回值的确切影响。然而，调整上一个函数的返回值是不容易的，因为调用者不知道函数内部的工作原理。因此，调用者告诉上一个函数（被调用方）调整其返回值以最小化损失。请参见图4.7，了解损失如何向后传播到参数和先前的函数。
- en: '![CH04_F07_Hagiwara](../Images/CH04_F07_Hagiwara.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F07_Hagiwara](../Images/CH04_F07_Hagiwara.png)'
- en: Figure 4.7 Backpropagation of loss
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7 损失的反向传播
- en: The nested function calls repeat this process and plays the Telephone game until
    the message reaches the innermost function. By that time, because the message
    needs to pass through many layers, it becomes so weak and obscure (or so strong
    and skewed because of some misunderstanding) that the inner functions have a difficult
    time figuring out what they did wrong.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌套的函数调用重复这个过程并玩转电话游戏，直到消息传递到最内层函数。到那个时候，因为消息需要经过许多层，它变得非常微弱和模糊（或者如果有误解则非常强大和扭曲），以至于内部函数很难弄清楚自己做错了什么。
- en: Technically, the deep learning literature calls this the *vanishing gradients
    problem*. A *gradient* is a mathematical term that corresponds to the message
    signal that each function receives from the next one that states how exactly they
    should improve their process (how to change their magic constants). The reverse
    Telephone game, where messages are passed backward from the final function (=
    loss function), is called backpropagation. I’m not going into the mathematical
    details of these terms, but it is useful to understand them at least conceptually.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 技术上讲，深度学习文献将此称为*梯度消失问题*。*梯度*是一个数学术语，对应于每个函数从下一个函数接收到的信息信号，该信号指示它们应该如何改进其过程（如何更改其魔法常数）。反向电话游戏，其中消息从最终函数（=损失函数）向后传递，称为反向传播。我不会涉及这些术语的数学细节，但至少在概念上理解它们是有用的。
- en: Because of the vanishing gradients problem, simple RNNs are difficult to train
    and rarely used in practice nowadays.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 由于梯度消失问题，简单的循环神经网络（Simple RNNs）难以训练，在实践中现在很少使用。
- en: 4.2.2 Long short-term memory (LSTM)
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.2 长短期记忆（LSTM）
- en: The way the nested functions mentioned earlier process information about grammar
    seems too inefficient. After all, why doesn’t the outermost function (is_grammatical)
    tell the particular function in charge (e.g., process_main_subject) what went
    wrong directly, instead of playing the Telephone game? It can’t, because the message
    can change its shape entirely after each function call because of w2 and f().
    The outermost function cannot tell which function was responsible for which part
    of the message from only the final output.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 之前提到的嵌套函数处理语法信息的方式似乎太低效了。毕竟，为什么外部函数（is_grammatical）不直接告诉负责的特定函数（例如，process_main_subject）出了什么问题，而不是玩电话游戏呢？它不能这样做，因为每次函数调用后消息都可以完全改变其形状，这是由于
    w2 和 f()。最外层函数无法仅从最终输出中告诉哪个函数负责消息的哪个部分。
- en: How could we address this inefficiency? Instead of passing the information through
    an activation function every time and changing its shape completely, how about
    adding and subtracting information relevant to the part of the sentence being
    processed at each step? For example, if process_main_subject() can directly add
    information about the subject to some type of “memory,” and the network can make
    sure the memory passes through the intermediate functions intact, is_grammatical()will
    have a much easier time telling the previous functions what to do to adjust its
    output.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何解决这个低效性呢？与其每次通过激活函数传递信息并完全改变其形状，不如在每一步中添加和减去与正在处理的句子部分相关的信息？例如，如果 process_main_subject()
    可以直接向某种“记忆”中添加有关主语的信息，并且网络可以确保记忆通过中间函数完整地传递，is_grammatical() 就会更容易告诉前面的函数如何调整其输出。
- en: '*Long short-term memory* units (LSTMs) are a type of RNN cell that is proposed
    based on this insight. Instead of passing around states, LSTM cells share a “memory”
    that each cell can remove old information from and/or add new information to,
    something like an assembly line in a manufacturing factory. Specifically, LSTM
    RNNs use the following function for updating states:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*长短期记忆*单元（LSTMs）是基于这一观点提出的一种 RNN 单元。LSTM 单元不是传递状态，而是共享“记忆”，每个单元都可以从中删除旧信息并/或添加新信息，有点像制造工厂中的装配线。具体来说，LSTM
    RNN 使用以下函数来更新状态：'
- en: '[PRE9]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![CH04_F08_Hagiwara](../Images/CH04_F08_Hagiwara.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F08_Hagiwara](../Images/CH04_F08_Hagiwara.png)'
- en: Figure 4.8 LSTM update function
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8 LSTM 更新函数
- en: 'Although this looks relatively complicated compared to its simple version,
    if you break it down to subcomponents, it’s not that difficult to understand what
    is going on here, as described next and shown in figure 4.8:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管与其简单版本相比，这看起来相对复杂，但是如果你将其分解为子组件，就不难理解这里正在发生的事情，如下所述并在图 4.8 中显示：
- en: The LSTM states comprise two halves—the cell state (the “memory” part) and the
    hidden state (the “mental representation” part).
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM 状态包括两个部分——细胞状态（“记忆”部分）和隐藏状态（“心理表征”部分）。
- en: The function forget() returns a value between 0 and 1, so multiplying by this
    number means erasing old memory from cell_state. How much to erase is determined
    from hidden_state and word (input). Controlling the flow of information by multiplying
    by a value between 0 and 1 is called *gating*. LSTMs are the first RNN architecture
    that uses this gating mechanism.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 函数 forget() 返回一个介于 0 和 1 之间的值，因此乘以这个数字意味着从 cell_state 中擦除旧的记忆。要擦除多少由 hidden_state
    和 word（输入）决定。通过乘以介于 0 和 1 之间的值来控制信息流动称为*门控*。LSTM 是第一个使用这种门控机制的 RNN 架构。
- en: The function add()returns a new value added to the memory. The value again is
    determined from hidden_state and word.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 函数 add() 返回添加到记忆中的新值。该值再次是由 hidden_state 和 word 决定的。
- en: Finally, hidden_state is updated using a function, whose value is computed from
    the previous hidden state, the updated memory, and the input word.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，使用一个函数更新 hidden_state，该函数的值是从前一个隐藏状态、更新后的记忆和输入单词计算得出的。
- en: I abstracted the update function by hiding some mathematical details in the
    functions forget(), add(), and update_hidden(), which are not important for the
    discussion here. If you are interested in understanding LSTMs more deeply, I refer
    you to a wonderful blog post Chris Olah wrote on this topic ([http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Because LSTMs have this cell state that stays constant across different timesteps
    unless explicitly modified, they are easier to train and relatively well behaved.
    Because you have a shared “memory” and functions are adding and removing information
    related to different parts of the input sentence, it is easier to pinpoint which
    function did what and what went wrong. The error signal from the outermost function
    can reach responsible functions more directly.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: A word on the terminology—LSTM refers to one particular type of architecture
    mentioned here, but people use “LSTMs” to mean RNNs with LSTM cells. Also, “RNN”
    is often used to mean “simple RNN,” introduced in section 4.1.3\. When you see
    “RNNs” in the literature, you need to be aware of which exact architectures they
    are using.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3 Gated recurrent units (GRUs)
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another RNN architecture, called *Gated Recurrent Units* (GRUs), uses the gating
    mechanism. The philosophy behind GRUs is similar to that of LSTMs, but GRUs use
    only one set of states instead of two halves. The update function for GRUs is
    shown next:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Instead of erasing or updating the memory, GRUs use a switching mechanism. The
    cell first computes the new state from the old state and the input. It then computes
    switch, a value between 0 and 1\. The state is chosen between the new state and
    the old one based on the value of switch. If it’s 0, the old state passes through
    intact. If it’s 1, it’s overwritten by the new state. If it’s somewhere in between,
    the state will be a mix of two. See figure 4.9 for an illustration of the GRU
    update function.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '![CH04_F09_Hagiwara](../Images/CH04_F09_Hagiwara.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 GRU update function
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the update function for GRUs is much simpler than that for the LSTMs.
    Indeed, it has fewer parameters (magic constants) that need to be trained compared
    to LSTMs. Because of this, GRUs are faster to train than LSTMs.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Finally, although we introduced two different types of RNN architecture, LSTM
    and GRU, there’s no consensus in the community on which type of architecture is
    the best for all applications. You often need to treat them as a hyperparameter
    and experiment with different configurations. Fortunately, it is easy to experiment
    with different types of RNN cells as long as you are using modern deep learning
    frameworks such as PyTorch and TensorFlow.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Accuracy, precision, recall, and F-measure
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In section 2.7, I briefly talked about some metrics that we use for evaluating
    the performance of a classification task. Before we move on to actually building
    a sentence classifier, I’d like to further discuss the evaluation metrics we are
    going to use—what they mean and what they actually measure.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1 Accuracy
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Accuracy is probably the simplest of all the evaluation metrics that we talk
    about here. In a classification setting, accuracy is the fraction of instances
    that your model got right. For example, if there are 10 emails and your spam-filtering
    model got 8 of them correct, the accuracy of your prediction is 0.8, or 80% (see
    figure 4.10).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '![CH04_F10_Hagiwara](../Images/CH04_F10_Hagiwara.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 Calculating accuracy
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Though simple, accuracy is not without its limitations. Specifically, accuracy
    can be misleading when the test set is imbalanced. An *imbalanced* dataset contains
    multiple class labels that greatly differ in their numbers. For example, if a
    spam-filtering dataset is imbalanced, it may contain 90% nonspam emails and 10%
    spams. In such a case, even a stupid classifier that labels everything as nonspam
    would be able to achieve an accuracy of 90%. As an example, if a “stupid” classifier
    classifies everything as “nonspam” in figure 4.10, it would still achieve an accuracy
    of 70% (7 out of 10 instances). If you look at this number in isolation, you might
    be fooled into thinking the performance of the classifier is actually great. When
    you are using accuracy as a metric, it is always a good idea to compare with the
    hypothetical, stupid classifier (*majority vote*) as a baseline.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2 Precision and recall
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The rest of the metrics—precision, recall, and F-measure—are used in a binary
    classification setting. The goal of a binary classification task is to identify
    one class (called a *positive class*) from the other (called a *negative class*).
    In the spam-filtering setting, the positive class is spam, whereas the negative
    class is nonspam.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'The Venn diagram in figure 4.11 contains four subregions: true positives, false
    positives, false negatives, and true negatives. True positives (TP) are instances
    that are predicted as positive (= spam) and are indeed in the positive class.
    False positives (FP) are instances that are predicted as positive (= spam) but
    are actually not in the positive class. These are noises in the prediction, that
    is, innocent nonspam emails that are mistakenly caught by the spam filter and
    end up in the spam folder of your email client.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, false negatives (FN) are instances that are predicted as
    negative but are actually in the positive class. These are spam emails that slip
    through the spam filter and end up in your inbox. Finally, true negatives (TN)
    are instances that are predicted as negative and are indeed in the negative class
    (nonspam emails in your inbox).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Precision is the fraction of instances that the model classifies as positive
    that are indeed correct. For example, if your spam filter identifies three emails
    as spam, and two of them are indeed spam, the precision will be 2/3, or about
    66%.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Recall is somewhat opposite of precision. It is the fraction of positive instances
    in your dataset that are identified as positive by your model. Again, using spam
    filtering as an example, if your dataset contains three spam emails and your model
    identifies two of them as spam successfully, the recall will be 2/3, or about
    66%.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.11 shows the relationship between predicted and true labels as well
    as recall and precision.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '![CH04_F11_Hagiwara](../Images/CH04_F11_Hagiwara.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 Precision and recall
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.3 F-measure
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You may have noticed a tradeoff between precision and recall. Imagine there’s
    a spam filter that is very, very careful in classifying emails. It outputs only
    one out of several thousand emails as spam, but when it does, it is always correct.
    This is not a difficult task, because some spam emails are pretty obvious—if they
    contain a word “v1@gra” in the text and it’s sent from someone in the spam blacklist,
    it should be pretty safe to mark it as a spam. What would the precision of this
    spam filter be? 100%. Similarly, there’s another spam filter that is very, very
    careless in classifying emails. It classifies every single email as spam, including
    the ones from your colleagues and friends. Its recall? 100%. Would any of these
    two spam filters be useful? Hardly!
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: As you’ve seen, improving precision or recall alone while ignoring the other
    is not a good practice, because of the tradeoff between them. It’s like you were
    looking only at your body weight when you are on a diet. You lost 10 pounds? Great!
    But what if you are seven feet tall? Not so much. You need to take into account
    both your height and weight—how much is too much depends on the other variable.
    That’s why there are measures like BMI (body mass index) that take both measures
    into account. Similarly, researchers came up with this metric called *F-measure*,
    which is an average (or, more precisely speaking, a harmonic mean) of precision
    and recall. Most often, a special case called *F1*-measure is used, which is the
    equally weighted version of F-measure. In a classification setting, it is a good
    practice to measure and try to maximize the F-measure.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Building AllenNLP training pipelines
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we are going to revisit the sentiment analyzer we built in
    chapter 2 and discuss how to build its training pipeline in more detail. Although
    I already showed the important steps for building an NLP application using AllenNLP,
    in this section we will dive deep into some important concepts and abstractions.
    Understanding these concepts is important not just in using AllenNLP but also
    in designing NLP applications in general, because NLP applications are usually
    built using these abstractions in some way or the other.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'To run the code in this section, you need to import the necessary classes and
    modules, as shown in the following code snippet (the code examples in this section
    can also be accessed via Google Colab, [http://www.realworldnlpbook.com/ch2.html#sst-nb](http://www.realworldnlpbook.com/ch2.html#sst-nb)):'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行本节中的代码，您需要导入必要的类和模块，如下面的代码片段所示（本节中的代码示例也可以通过 Google Colab 访问，[http://www.realworldnlpbook.com/ch2.html#sst-nb](http://www.realworldnlpbook.com/ch2.html#sst-nb)）。
- en: '[PRE11]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 4.4.1 Instances and fields
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.1 实例和字段
- en: As mentioned in section 2.2.1, an instance is the atomic unit for which a prediction
    is made by a machine learning algorithm. A dataset is a collection of instances
    of the same form. The first step in most NLP applications is to read in or receive
    some data (e.g., from a file or via network requests) and convert them to instances
    so that the NLP/ML algorithm can consume them.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如第 2.2.1 节所述，实例是机器学习算法进行预测的原子单位。数据集是同一形式实例的集合。大多数 NLP 应用的第一步是读取或接收一些数据（例如从文件或通过网络请求）并将其转换为实例，以便
    NLP/ML 算法可以使用它们。
- en: AllenNLP supports an abstraction called DatasetReader whose job is to read in
    some input (raw strings, CSV files, JSON data structures from network requests,
    and so on) and convert it to instances. AllenNLP already provides a wide range
    of dataset readers for major formats used in NLP, such as the CoNLL format (used
    in popular shared tasks for language analysis) and the Penn Treebank (a popular
    dataset for syntactic parsing). To read the Standard Sentiment Treebank, you can
    use the built-in StanfordSentimentTreeBankDatasetReader, which we used earlier
    in chapter 2\. You can also write your own dataset reader just by overriding some
    core methods from DatasetReader.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: AllenNLP 支持一个称为 DatasetReader 的抽象，它的工作是读取一些输入（原始字符串、CSV 文件、来自网络请求的 JSON 数据结构等）并将其转换为实例。AllenNLP
    已经为 NLP 中使用的主要格式提供了广泛的数据集读取器，例如 CoNLL 格式（在语言分析的流行共享任务中使用）和 Penn Treebank（一种流行的用于句法分析的数据集）。要读取
    Standard Sentiment Treebank，可以使用内置的 StanfordSentimentTreeBankDatasetReader，我们在第
    2 章中已经使用过了。您还可以通过覆盖 DatasetReader 的一些核心方法来编写自己的数据集阅读器。
- en: 'The AllenNLP class Instance represents a single instance. An instance can have
    one or more fields, which hold some type of data. For example, an instance for
    the sentiment analysis task has two fields—the text body and the label—which can
    be created by passing a dictionary of fields to its constructor as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: AllenNLP 类 Instance 表示一个单独的实例。一个实例可以有一个或多个字段，这些字段保存某种类型的数据。例如，情感分析任务的实例有两个字段——文本内容和标签——可以通过将字段字典传递给其构造函数来创建，如下所示：
- en: '[PRE12]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Here we assumed that you already created tokens, which is a list of tokens,
    and sentiment, a string label corresponding to the sentiment class, from reading
    the input file. AllenNLP supports other types of fields, depending on the task.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们假设您已经创建了 tokens（一个标记列表）和 sentiment（一个与情感类别对应的字符串标签），并从读取输入文件中获取了它们。根据任务，AllenNLP
    还支持其他类型的字段。
- en: 'The read()method of DatasetReader returns an iterator over instances, which
    enables you to enumerate the generated instances and visually check them, as shown
    in the following snippet:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: DatasetReader 的 read() 方法返回一个实例迭代器，使您能够枚举生成的实例并对其进行可视化检查，如下面的代码片段所示：
- en: '[PRE13]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In many cases, you access your dataset readers through data loaders. A data
    loader is an AllenNLP abstraction (which is really a thin wrapper around PyTorch’s
    data loaders) that handles the data and iterates over batched instances. You can
    specify how instances are sorted, grouped into batches, and fed to the training
    algorithm by supplying a batch sampler. Here, we are using a BucketBatchSampler,
    which does this by sorting instances by their length and grouping instances with
    similar lengths into a single batch, as shown next:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，您可以通过数据加载器访问数据集阅读器。数据加载器是 AllenNLP 的一个抽象（实际上是 PyTorch 数据加载器的一个薄包装），它处理数据并迭代批量实例。您可以通过提供批量样本器来指定如何对实例进行排序、分组为批次并提供给训练算法。在这里，我们使用了一个
    BucketBatchSampler，它通过根据实例的长度对其进行排序，并将长度相似的实例分组到一个批次中，如下所示：
- en: '[PRE14]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 4.4.2 Vocabulary and token indexers
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.2 词汇表和标记索引器
- en: The second step in many NLP applications is to build the vocabulary. In computer
    science, vocabulary is a theoretical concept that represents the set of *all*
    possible words in a language. In NLP, though, it often means just the set of all
    unique tokens that appeared in a dataset. It is simply impossible to know all
    the possible words in a language, nor is it necessary for an NLP application.
    What is stored in a vocabulary is called a *vocabulary item* (or just an *item*).
    A vocabulary item is usually a word, although depending on the task at hand, it
    can be any form of linguistic units, including characters, character n-grams,
    and labels for linguistic annotation.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: AllenNLP provides a class called Vocabulary. It not only takes care of storing
    vocabulary items that appeared in a dataset, but it also holds mappings between
    vocabulary items and their IDs. As mentioned earlier, neural networks and machine
    learning models in general can deal only with numbers, and there needs to be a
    way to map discrete items such as words to some numerical representations such
    as word IDs. The vocabulary is also used to map the results of an NLP model back
    to the original words and labels so that humans can actually read them.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'You can create a Vocabulary object from instances as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'A couple of things to note here: first, because we are dealing with iterators
    (returned by the data loaders’ iter_instances()method), we need to use the chain
    method from itertools to enumerate all the instances in both datasets.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Second, AllenNLP’s Vocabulary class supports *namespaces*, which are a system
    to separate different sets of items so that they don’t get mixed up. Here’s why
    they are useful—say you are building a machine translation system, and you just
    read a dataset that contains English and French translations. Without namespaces,
    you’d have just one set that contains all words in English and French. This is
    usually not a big issue because English words (“hi,” “thank you,” “language”)
    and French words (“bonjour,” “merci, “langue”) look quite different in most cases.
    However, a number of words look exactly the same in both languages. For example,
    “chat” means “talk” in English and “cat” in French, but it’s hard to imagine anybody
    wanting to mix those two words and assign the same ID (and embeddings). To avoid
    this conflict, Vocabulary implements namespaces and assigns separate sets of items
    of different types.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: 'You may have noticed the form_instances() function call has a min_count argument.
    For each namespace, this specifies the minimum number of occurrences in the dataset
    that is necessary for an item to be included in the vocabulary. All the items
    that appear less frequently than this threshold are treated as “unknown” items.
    Here’s why this is a good idea: in a typical language, a very small number of
    words appear a lot (in English: “the,” “a,” “of”) and a very large number of words
    appear very infrequently. This usually exhibits a long tail distribution of word
    frequencies. But it is not likely that these super infrequent words add anything
    useful to the model, and precisely because they appear infrequently, it is difficult
    to learn any useful patterns from them anyway. Also, because there are so many
    of them, they inflate the size of the vocabulary and the number of model parameters.
    In such a case, a common practice in NLP is to cut this long tail and collapse
    all the infrequent words to a single entity <UNK> (for “unknown” words).'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能注意到`form_instances()`函数调用有一个`min_count`参数。对于每个命名空间，它指定了数据集中必须出现的最小次数，以便将项目包含在词汇表中。所有出现频率低于此阈值的项目都被视为“未知”项目。这是一个好主意的原因是：在典型的语言中，很少有一些词汇会频繁出现（英语中的“the”，“a”，“of”），而有很多词汇出现的频率很低。这通常表现为词频的长尾分布。但这些频率极低的词汇不太可能对模型有任何有用的信息，并且正因为它们出现频率较低，从中学习有用的模式也很困难。此外，由于这些词汇有很多，它们会增加词汇表的大小和模型参数的数量。在这种情况下，自然语言处理中常见的做法是截去这长尾部分，并将所有出现频率较低的词汇合并为一个单一的实体<UNK>（表示“未知”词汇）。
- en: Finally, a *token indexer* is an AllenNLP abstraction that takes in a token
    and returns its index, or a list of indices that represent the token. In most
    cases, there’s a one-to-one mapping between unique tokens and their indices, but
    depending on your model, you may need more advanced ways to index the tokens (such
    as using character n-grams).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，*令牌索引器*是AllenNLP的一个抽象概念，它接收一个令牌并返回其索引，或者返回表示令牌的索引列表。在大多数情况下，独特令牌和其索引之间存在一对一的映射，但根据您的模型，您可能需要更高级的方式来对令牌进行索引（例如使用字符n-gram）。
- en: 'After you create a vocabulary, you can tell the data loaders to index the tokens
    with the specified vocabulary, as shown in the next code snippet. This means that
    the tokens that the data loaders read from the datasets are converted to integer
    IDs according to the vocabulary’s mappings:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 创建词汇表后，你可以告诉数据加载器使用指定的词汇表对令牌进行索引，如下代码片段所示。这意味着数据加载器从数据集中读取的令牌会根据词汇表的映射转换为整数ID：
- en: '[PRE16]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 4.4.3 Token embedders and RNNs
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.3 令牌嵌入和RNN
- en: 'After you index words using a vocabulary and token indexers, you need to convert
    them to embeddings. An AllenNLP abstraction called TokenEmbedder takes word indices
    as an input and produces word embedding vectors as an output. You can embed words
    using continuous vectors in many ways, but if all you want is to map unique tokens
    to embedding vectors one-to-one, you can use the Embedding class as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用词汇表和令牌索引器索引单词后，需要将它们转换为嵌入。一个名为TokenEmbedder的AllenNLP抽象来接收单词索引作为输入并将其转换为单词嵌入向量作为输出。你可以使用多种方式嵌入连续向量，但如果你只想将唯一的令牌映射到嵌入向量时，可以使用Embedding类，如下所示：
- en: '[PRE17]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This will create an Embedding instance that takes word IDs and converts them
    to fixed-length vectors in a one-to-one fashion. The number of unique words this
    instance can support is given by num_embeddings, which is equal to the size of
    the tokens vocabulary namespace. The dimensionality of embeddings (i.e., the length
    of embedded vectors) is given by embedding_dim.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个Embedding实例，它接收单词ID并以一对一的方式将其转换为定长矢量。该实例支持的唯一单词数量由num_embeddings给出，它等于令牌词汇的大小。嵌入的维度（即嵌入矢量的长度）由embedding_dim给出。
- en: 'Next, let’s define our RNN and convert a variable-length input (a list of embedded
    words) to a fixed-length vector representation of the input. As we discussed in
    section 4.1, you can think of an RNN as a neural network structure that consumes
    a sequence of things (words) and returns a fixed-length vector. AllenNLP abstracts
    such models into the Seq2VecEncoder class, and you can create an LSTM RNN by using
    PytorchSeq2VecWrapper as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们定义我们的RNN，并将变长输入（嵌入词的列表）转换为输入的定长矢量表示。正如我们在第4.1节中讨论的那样，你可以将RNN看作是一个神经网络结构，它消耗一个序列的事物（词汇）并返回一个定长的矢量。AllenNLP将这样的模型抽象化为Seq2VecEncoder类，你可以通过使用PytorchSeq2VecWrapper创建一个LSTM
    RNN，如下所示：
- en: '[PRE18]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: A lot is happening here, but essentially this wraps PyTorch’s LSTM implementation
    (torch.nn.LSTM) and makes it pluggable to the rest of the AllenNLP pipeline. The
    first argument to torch.nn.LSTM() is the dimensionality of the input vector, and
    the second one is that of LSTM’s internal state. The last one, batch_first, specifies
    the structure of the input/output tensors for batching, but you usually don’t
    have to worry about its details as long as you are using AllenNLP.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: NOTE In AllenNLP, everything is batch first, meaning that the first dimension
    of any tensor is always equal to the number of instances in a batch.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.4 Building your own model
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we defined all the subcomponents, we are ready to build the model that
    executes the prediction. Thanks to AllenNLP’s well-designed abstractions, you
    can easily build your model by inheriting AllenNLP’s Model class and overriding
    the forward() method. You don’t usually need to be aware of details such as the
    shapes and dimensions of tensors. The following listing defines the LSTM RNN used
    for classifying sentences.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.1 LSTM sentence classifier
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ AllenNLP models inherit Model.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Creates a linear layer to convert the RNN output to a vector of another length
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: ❸ F1Measure() requires the label ID for the positive class. '4' means “very
    positive.”
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Cross-entropy loss is used for classification tasks. CrossEntropyLoss directly
    takes logits (no softmax needed).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Instances are destructed to individual fields and passed to forward().
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Output of forward() is a dict, which contains a “loss” key.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Returns accuracy, precision, recall, and F1-measure as the metrics
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Every AllenNLP Model inherits from PyTorch’s Module class, meaning you can use
    PyTorch’s low-level operations if necessary. This gives you a lot of flexibility
    in defining your model while leveraging AllenNLP’s high-level abstractions.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.5 Putting it all together
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finally, we finish this section by implementing the entire pipeline to train
    the sentiment analyzer, as shown next.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.2 Training pipeline for the sentiment analyzer
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ Defines how to construct the data loaders
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Initializes the model
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Defines the optimizer
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Initializes the trainer
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: The training pipeline completes when the Trainer instance is created and invoked
    with train(). You pass all the ingredients that you need for training—the model,
    optimizer, data loaders, datasets, and a bunch of hyperparameters.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: An optimizer implements an algorithm for adjusting the parameters of the model
    to minimize the loss. Here, we are using one type of optimizer called *Adam*,
    which is a good “default” optimizer to use as your first option. However, as I
    mentioned in chapter 2, you often need to experiment with many different optimizers
    that work best for your model.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Configuring AllenNLP training pipelines
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may have noticed that very little of listing 4.2 is actually specific to
    the sentence-classification problem. Indeed, loading datasets, initializing a
    model, and plugging an iterator and an optimizer into the trainer are all common
    steps across almost every NLP training pipeline. What if you want to reuse the
    same training pipeline for many related tasks without writing the training script
    from scratch? Also, what if you want to experiment with different sets of configurations
    (e.g., different hyperparameters, neural network architectures) and save the exact
    configurations you tried?
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: For those problems, AllenNLP provides a convenient framework where you can write
    configuration files in the JSON format. The idea is that you write the specifics
    of your training pipeline—for example, which dataset reader to use, which models
    and their subcomponents to use, and what hyper-parameters to use for training—in
    a JSON-formatted file (more precisely, AllenNLP uses a format called *Jsonnet*,
    which is a superset of JSON). Instead of rewriting your model file or the training
    script, you feed the configuration file to the AllenNLP executable, and the framework
    takes care of running the training pipeline. If you want to try a different configuration
    for your model, you simply change the configuration file (or make a new one) and
    run the pipeline again, without changing the Python code. This is a great practice
    for making your experiment manageable and reproducible. You need to manage only
    the configuration files and their results—the same configuration always yields
    the same result.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical AllenNLP configuration file consists of three main parts—the dataset,
    your model, and the training pipeline. The first part, shown next, specifies which
    dataset files to use and how:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Three keys are in this part: dataset_reader, train_data_path, and validation_data_path.
    The first key, dataset_reader, specifies which DatasetReader to use to read the
    files. Dataset readers, models, and predictors, as well as many other types of
    modules in AllenNLP, can be registered using the decorator syntax and be referred
    to from configuration files. For example, if you peek at the following code where
    StanfordSentimentTreeBankDatasetReader is defined'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'you notice that it is decorated by @DatasetReader.register("sst_tokens"). This
    registers StanfordSentimentTreeBankDatasetReader under the name sst_tokens, which
    allows you to refer it by "type": "sst_tokens" from the configuration files.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 'In the second part of the configuration file, you specify the main model to
    be trained as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'As mentioned before, models in AllenNLP can be registered using the decorator
    syntax and be referred from the configuration files via the type key. For example,
    the LstmClassifier class referred here is defined as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Other keys in the model definition JSON dict correspond to the names of the
    parameters of the model constructor. In the previous definition, because LstmClassifier’s
    constructor takes two parameters, word_embeddings and encoder (in addition to
    vocab, which is passed by default and can be omitted, and positive_label, for
    which we are going to use the default value), the model definition has two corresponding
    keys, the values of which are also model definitions and follow the same convention.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'In the final part of the configuration file, the data loader and the trainer
    are specified. The convention here is similar to the model definition—you specify
    the type of the class along with other parameters passed to the constructor as
    follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'You can see the full JSON configuration file in the code repository ([http://realworldnlpbook.com/ch4.html#sst-json](http://realworldnlpbook.com/ch4.html#sst-json)).
    Once you define the JSON configuration file, you can simply feed it to the allennlp
    command as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The —serialization-dir specifies where the trained model (along with additional
    information such as serialized vocabulary data) is going to be stored. You also
    need to specify the module path to LstmClassifier using —include-package so that
    the configuration file can find the registered class.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'As we saw in chapter 2, when the training is finished, you can launch a simple
    web-based demo interface using the following command:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '4.6 Case study: Language detection'
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this final section of the chapter, we are going to discuss another scenario—language
    detection—which can also be formulated as a sentence-classification task. A language-detection
    system, given a piece of text, detects the language the text is written in. It
    has a wide range of uses in other NLP applications. For example, a web search
    engine may want to detect the language a web page is written in before processing
    and indexing it. Google Translate also switches the source language automatically
    based on what is typed in the input textbox.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see what this actually looks like. Can you tell the language of each of
    the following lines? These sentences are all taken from the Tatoeba project ([https://tatoeba.org/](https://tatoeba.org/)).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Contamos con tu ayuda.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Bitte überleg es dir.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Parti için planları tartıştılar.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Je ne sais pas si je peux le faire.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Você estava em casa ontem, não estava?
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Ĝi estas rapida kaj efika komunikilo.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Ha parlato per un'ora.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Szeretnék elmenni és meginni egy italt.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Ttwaliɣ nezmer ad nili d imeddukal.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 'The answer is: Spanish, German, Turkish, French, Portuguese, Esperanto, Italian,
    Hungarian, and Berber. I chose them from the top 10 most popular languages on
    Tatoeba that are written in the roman alphabet. You may not be familiar with some
    of the languages listed here. For those of you who are not, Esperanto is a constructed
    auxiliary language invented in the late 19th century. Berber is actually a group
    of related languages spoken in some parts of North Africa that are cousins of
    Semitic languages such as Arabic.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Maybe you were able to recognize some of these languages, even though you don’t
    actually speak them. I’d like you to step back and think *how* you did it. It’s
    quite interesting that people can do this without actually being able to speak
    the language, because these languages are all written in the roman alphabet and
    could look quite similar to each other. You may have recognized some unique diacritic
    marks (accents) for some of the languages—for example, “ü” for German and “ã”
    for Portuguese. These are a strong clue for these languages. Or you just knew
    some words—for example, “ayuda” for Spanish (meaning “help”) and “pas” in French
    (“ne . . . pas” is a French negation syntax). It appears that every language has
    its own characteristics—be it some unique characters or words—that makes it easy
    to tell it apart from others. This is starting to sound a lot like a kind of problem
    that machine learning is good at solving. Can we build an NLP system that can
    do this automatically? How should we go about building it?
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 4.6.1 Using characters as input
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A language detector can also be built in a similar way to the sentiment analyzer.
    You can use an RNN to read the input text and convert it to some internal representation
    (hidden states). You can then use a linear layer to convert them to a set of scores
    corresponding to how likely the text is written in each language. Finally, you
    can use cross-entropy loss to train the model.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: One major difference between the sentiment analyzer and the language detector
    is how you feed the input into an RNN. When building the sentiment analyzer, we
    used the Stanford Sentiment Treebank and were able to assume that the input text
    is always English and already tokenized. But this is not the case for language
    detection. In fact, you don’t even know whether the input text is written in a
    language that can be tokenized easily—what if the sentence is written in Chinese?
    Or in Finnish, which is infamous for its complex morphology? You could use a tokenizer
    that is specific to the language if you know what language it is, but we are building
    the language detector because we don’t know what language it is in the first place.
    This sounds like a typical chicken-and-egg problem.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: To address this issue, we are going to use characters instead of tokens as the
    input to an RNN. The idea is to break down the input into individual characters,
    even including whitespace and punctuation, and feed them to the RNN one at a time.
    Using characters is a common practice used when the input can be better represented
    as a sequence of characters (such as Chinese, or of an unknown origin), or when
    you’d like to make the best use of internal structures of words (such as the fastText
    model we mentioned in chapter 3). The RNN’s powerful representational power can
    still capture interactions between characters and some common words and n-grams
    mentioned earlier.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 4.6.2 Creating a dataset reader
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For this language-detection task, I created the train and the validation datasets
    from the Tatoeba project by taking the 10 most popular languages on Tatoeba that
    use the roman alphabet and by sampling 10,000 sentences for the train set and
    1,000 for the validation set. An excerpt of this dataset follows:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The first field is a three-letter language code that describes which language
    the text is written in. The second field is the text itself. The fields are delimited
    by a tab character. You can obtain the datasets from the code repository ([https://github.com/mhagiwara/realworldnlp/tree/master/data/tatoeba](https://github.com/mhagiwara/realworldnlp/tree/master/data/tatoeba)).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step in building a language detector is to prepare a dataset reader
    that can read datasets in this format. In the previous example (the sentiment
    analyzer), because AllenNLP already provides StanfordSentimentTreeBankDatasetReader,
    you just needed to import and use it. In this scenario, however, you need to write
    your own. Fortunately, writing a dataset reader that can read this particular
    format is not that difficult. To write a dataset reader, you just need to do the
    following three things:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Create your own dataset reader class by inheriting DatasetReader.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Override the text_to_instance() method that takes raw text and converts it to
    an instance object.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Override the_read() method that reads the content of a file and yields instances,
    by calling text_to_instance() above.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The complete dataset reader for the language detector is shown in listing 4.3\.
    We also assume that you already imported necessary modules and classes as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Listing 4.3 Dataset reader for the language detector
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: ❶ Every new dataset reader inherits DatasetReader.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Uses CharacterTokenizer() to tokenize text into characters
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Label will be None at test time.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: ❹ If file_path is an URL, returns the actual path to a cached file on disk
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Yields instances using text_to_instance(), defined earlier
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Note that the dataset reader in listing 4.3 uses CharacterTokenizer() to tokenize
    text into characters. Its tokenize() method returns a list of tokens, which are
    AllenNLP objects that represent tokens but actually contain characters in this
    scenario.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: 4.6.3 Building the training pipeline
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once you build the dataset reader, the rest of the training pipeline looks
    similar to that of the sentiment analyzer. In fact, we can reuse the LstmClassifier
    class we defined previously without any modification. The entire training pipeline
    is shown in listing 4.4\. You can access the Google Colab notebook of the entire
    code from here: [http://realworldnlpbook.com/ch4.html#langdetect](http://realworldnlpbook.com/ch4.html#langdetect).'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.4 Training pipeline for the language detector
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'When you run this training pipeline, you’ll get the metrics on the dev set
    that are in the ballpark of the following:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This is not bad at all! This means that the trained detector makes only one
    mistake out of about 20 sentences. Precision of 0.9481 means there’s only one
    false positive (non-English sentence) out of 20 instances that are classified
    as English. Recall of 0.9490 means there’s only one false negative (English sentence
    that was missed by the detector) out of 20 true English instances.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 4.6.4 Running the detector on unseen instances
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finally, let’s try running the detector we just trained on a set of unseen instances
    (instances that didn’t appear either in the train or the validation sets). It
    is always a good idea to try feeding a small number of instances to your model
    and observe how it behaves.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 'The recommended way for feeding instances into a trained AllenNLP model is
    to use a predictor, as we did in chapter 2\. But here I’d like to do something
    simpler and instead write a method that, given a piece of text and a model, runs
    the prediction pipeline. To run a model on arbitrary instances, you can call the
    model’s forward_ on_instances() method, as shown in the following snippet:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: This method first takes the input (text and model) and passes it through a tokenizer
    to create an instance object. Then it calls model’s forward_on_instance() method
    to retrieve the logits, the scores for target labels (languages). It gets the
    label ID that corresponds to the maximum logit value by calling np.argmax and
    then converts it to the label text by using the vocabulary object associated with
    the model.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: 'When I ran this method on some sentences that are not in the two datasets,
    I got the following results. Note that the result you get may be different from
    mine due to some randomness:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: These predictions are almost perfect, except the very first sentence—it is English,
    not French. It is surprising that the model makes a mistake on such a seemingly
    easy sentence while it predicts more difficult languages (such as Hungarian) perfectly.
    But remember, how difficult the language is for English speakers has nothing to
    do with how difficult it is for a computer to classify. In fact, some of the “difficult”
    languages such as Hungarian and Turkish here have very clear signals (accent marks
    and unique words) that make it easy to detect them. On the other hand, lack of
    clear signals in the first sentence may have made it more difficult to classify
    it from other languages.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 'As a next step, you could try a couple of things: for example, you can tweak
    some of the hyperparameters to see how the evaluation metrics and the final prediction
    results change. You can also try a larger number of test instances to see how
    exactly the mistakes are distributed (e.g., between which two languages). You
    can also zero in on some of the instances and see why the model made such mistakes.
    These are all important practices when you are working on real-world NLP applications.
    I’ll discuss these topics in detail in chapter 10.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A recurrent neural network (RNN) is a neural network with a loop. It can transform
    a variable-length input to a fixed-length vector.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nonlinearity is a crucial component that makes neural networks truly powerful.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LSTMs and GRUs are two variants of RNN cells and are easier to train than vanilla
    RNNs.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You use accuracy, precision, recall, and F-measure for classification problems.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AllenNLP provides useful NLP abstractions such as dataset readers, instances,
    and vocabulary. It also provides a way to configure the training pipeline in the
    JSON format.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can build a language detector as a sentence-classification application similar
    to the sentiment analyzer.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
