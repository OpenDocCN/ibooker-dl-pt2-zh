- en: '5 State-of-the-art in deep learning: Transformers'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 深度学习的最新技术：Transformer
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容包括：
- en: Representing text in numerical format for machine learning models
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为机器学习模型以数值形式表示文本
- en: Building a Transformer model using the Keras sub-classing API
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Keras sub-classing API 构建 Transformer 模型
- en: We have seen many different deep learning models so far, namely fully connected
    networks, convolutional neural networks, and recurrent neural networks. We used
    a fully connected network to reconstruct corrupted images, a convolutional neural
    network to classify vehicles from other images, and finally an RNN to predict
    future CO2 concentration values. In this chapter we are going to talk about a
    new type of model known as the Transformer.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了许多不同的深度学习模型，包括全连接网络、卷积神经网络和循环神经网络。我们使用全连接网络来重建受损图像，使用卷积神经网络来对车辆进行分类，最后使用
    RNN 来预测未来的 CO2 浓度值。在本章中，我们将讨论一种新型的模型，即 Transformer。
- en: Transformers are the latest generation of deep networks to emerge. Vaswani et
    al., in their paper “Attention Is All You Need” ([https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf)),
    popularized the idea. They coined the term *Transformer* and explained how it
    shows great promise for the future. In the years following, leading tech companies
    like Google, OpenAI, and Facebook implemented bigger and better Transformer models
    that have significantly outperformed other models in the NLP domain. Here, we
    will refer to the model introduced in their paper by Vaswani et al. to learn about
    it. Although Transformers do exist for other domains (e.g., computer vision),
    we will focus on how the Transformer is used in the NLP domain, particularly on
    a machine translation task (i.e., language translation using machine learning
    models). This discussion will leave out some of the details from the original
    Transformer paper to improve clarity, but these details will be covered in a later
    chapter.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 是最新一代的深度网络。瓦斯瓦尼等人在他们的论文《Attention Is All You Need》（[https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf)）中普及了这个想法。他们创造了“Transformer”这个术语，并解释了它在未来有很大潜力。在随后的几年里，谷歌、OpenAI
    和 Facebook 等领先的科技公司实施了更大更好的 Transformer 模型，这些模型在 NLP 领域显著优于其他模型。在这里，我们将参考瓦斯瓦尼等人在论文中介绍的模型来学习它。虽然
    Transformer 也存在于其他领域（例如计算机视觉），我们将重点介绍 Transformer 在 NLP 领域中的应用，特别是在机器翻译任务中（即使用机器学习模型进行语言翻译）。本章将省略原始
    Transformer 论文中的一些细节，以提高清晰度，但这些细节将在后面的章节中进行介绍。
- en: Knowing the inner workings of the Transformer model is a must for anyone who
    wants to excel at using deep learning models to solve real-world problems. As
    explained, the Transformer model has proliferated the machine learning field quite
    rapidly. This is mainly because of the performance it has demonstrated in solving
    complex machine learning problems.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 想要在使用深度学习模型解决实际问题时出类拔萃，了解 Transformer 模型的内部工作原理是必不可少的。如前所述，Transformer 模型在机器学习领域迅速普及。这主要是因为它在解决复杂机器学习问题方面展现出的性能。
- en: 5.1 Representing text as numbers
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 将文本表示为数字
- en: Say you are taking part in a game show. One challenge in the game is called
    Word Boxes. There is a matrix of transparent boxes (3 rows, 5 columns, 10 depths).
    You also have balls with 0 or 1 painted on them. You are given three sentences,
    and your task is to fill all the boxes with 1s and 0s to represent those sentences.
    Additionally, you can write a short message (within a minute) that helps someone
    decipher this later. Later, another team member looks at the boxes and writes
    down as many words in the original sentences you were initially given.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在参加一个游戏节目。游戏中的一个挑战叫做单词盒子。有一个由透明盒子组成的矩阵（3行，5列，10深度）。你也有一些上面涂有0或1的球。你被给予了三个句子，你的任务是用1和0填充所有的盒子来表示这些句子。此外，你可以在一分钟内写一条简短的信息，帮助其他人在以后破译这一信息。之后，另一个队员看着盒子，写下最初给你的原始句子中的尽可能多的单词。
- en: The challenge is essentially how you can transform text to numbers for machine
    translation models. This is also an important problem you work on before learning
    about any NLP model. The data we have seen so far has been numerical data structures.
    For example, an image can be represented as a 3D array (height, width, and channel
    dimensions), where each value represents a pixel intensity (i.e., a value between
    0 and 255). But what about text? How can we make a computer understand characters,
    words, or sentences? We will learn how to do this with Transformers in the context
    of natural language processing (NLP).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这个挑战本质上是如何将文本转换成数字，用于机器翻译模型。这也是在了解任何NLP模型之前需要解决的重要问题。到目前为止我们看到的数据都是数值型数据结构。例如，一张图像可以被表示为一个3D数组（高度，宽度和通道维度），其中每个值表示像素强度（即，取值范围在0至255之间）。但文本呢？我们怎么让计算机理解字符、单词或句子呢？我们将在自然语言处理（NLP）的情境中学习如何用Transformer完成这一点。
- en: 'You have the following set of sentences:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 您有以下一组句子：
- en: I went to the beach.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我去了海滩。
- en: It was cold.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 天气很冷。
- en: I came back to the house.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我回到了房子。
- en: 'The first thing you do is assign each word in your vocabulary an ID starting
    from 1\. We will reserve the number 0 for a special token we will see later. Say
    you assign the following IDs:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你要做的第一件事是给词汇表中的每个单词分配一个从1开始的ID。我们将保留数字0给我们稍后会看到的特殊标记。假设你分配了以下ID：
- en: I → 1
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: I → 1
- en: went → 2
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: went → 2
- en: to → 3
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: to → 3
- en: the → 4
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: the → 4
- en: beach → 5
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: beach → 5
- en: It → 6
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它 → 6
- en: was → 7
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: was → 7
- en: cold → 8
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: cold → 8
- en: came → 9
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: came → 9
- en: back → 10
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: back → 10
- en: house → 11
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: house → 11
- en: 'After mapping the words to the corresponding IDs, our sentences become the
    following:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 将单词映射到相应的ID后，我们的句子变为了下面这个样子：
- en: '[1, 2, 3, 4, 5]'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1, 2, 3, 4, 5]'
- en: '[6, 7, 8]'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6, 7, 8]'
- en: '[1, 9, 10, 3, 4, 11]'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1, 9, 10, 3, 4, 11]'
- en: Remember, you need to fill in all the boxes and have a maximum length of 5\.
    Note that our last sentence has six words. This means all the sentences need to
    be represented by a fixed length. Deep learning models face a similar problem.
    They process data in batches, and to process it efficiently, the sequence length
    needs to be fixed for that batch. Real-world sentences can vary significantly
    in terms of their length. Therefore, we need to
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，您需要填写所有方框，并且最多长度为5。请注意我们的最后一句有六个单词。这意味着所有句子都需要表示为固定长度。深度学习模型面临类似的问题。它们以批处理的方式处理数据，并且为了高效处理数据，批处理的序列长度需要是固定的。真实世界的句子在长度上可能差异很大。因此，我们需要
- en: Pad short sentences with a special token <PAD> (with ID 0)
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用特殊标记<PAD>（ID为0）填充短句
- en: Truncate long sentences
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 截断长句
- en: 'to make them the same length. If we pad the short sentences and truncate long
    sentences so that the length is 5, we get the following:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 使它们具有相同的长度。如果我们填充短句并截断长句，使长度为5，我们得到以下结果：
- en: '[1, 2, 3, 4, 5]'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1, 2, 3, 4, 5]'
- en: '[6, 7, 8, 0, 0]'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6, 7, 8, 0, 0]'
- en: '[1, 9, 10, 3, 4]'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1, 9, 10, 3, 4]'
- en: Here, we have a 2D matrix of size 3 × 5, which represents our batch of sentences.
    The final thing to do is represent each of these IDs as vectors. Because our balls
    have 1s and 0s, you can represent each word with 11 balls (we have 10 different
    words and the special token <PAD>), where the ball at the position indicated by
    the word ID is 1 and the rest are 0s. This method is known as one-hot encoding.
    For example,
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们有一个大小为3×5的2D矩阵，它表示我们的一批句子。最后要做的一件事是将这些ID表示为向量。因为我们的球有1和0，你可以用11个球（我们有10个不同的单词和特殊标记<PAD>）代表每个单词，其中由单词ID指示的位置上的球为1，其余为0。这种方法称为one-hot编码。例如，
- en: 0 → [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以下分别代表着各自的ID：
- en: 1 → [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 1 → [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
- en: . . .
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 。 。 。
- en: 10 → [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 10 → [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
- en: 11→ [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 11 → [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
- en: Now you can fill the boxes with 1s and 0s such that you get something like figure
    5.1\. This way, anyone who has the word for ID mapping (provided in a sheet of
    paper) can decipher most of the words (except for those truncated) that were initially
    provided.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以用1和0填写方框，使得你得到类似图5.1的结果。这样，任何一位有这些ID映射的人（提供在一张纸上）都可以解密最初所提供的大部分单词（除了被截断的单词）。
- en: '![05-01](../../OEBPS/Images/05-01.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![05-01](../../OEBPS/Images/05-01.png)'
- en: Figure 5.1 The boxes in the Word Boxes game. The shaded boxes represent a single
    word (i.e., the first word in the first sentence, “I,” which has an ID of 1).
    You can see it’s represented by a single ball of 1 and nine balls of 0.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 词盒游戏中的方框。阴影方框代表一个词（即第一个句子中的第一个词“I”，它的ID是1）。你可以看到它被一个1和九个0所表示。
- en: 'Again, this is a transformation done to words in NLP problems. You might ask,
    “Why not feed the word IDs directly?” There are two problems:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，这是在 NLP 问题中对单词进行的转换。你可能会问：“为什么不直接提供单词 ID？”存在两个问题：
- en: The value ranges the neural network sees are very large (0-100,000+) for a real-world
    problem. This will cause instabilities and make the training difficult.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络看到的值范围非常大（0-100,000+）对于一个现实世界的问题。这会导致不稳定性并使训练困难。
- en: Feeding in IDs would falsely indicate that words with similar IDs should be
    alike (e.g., word ID 4 and 5). This is never the case and would confuse the model
    and lead to poor performance.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入 ID 会错误地表明具有相似 ID 的单词应该是相似的（例如，单词 ID 4 和 5）。这从未发生过，会使模型混淆并导致性能下降。
- en: Therefore, it is important to bring words to some vector representation. There
    are many ways to turn words into vectors, such as one-hot encoding and word embeddings.
    You have already seen how one-hot encoding works, and we will discuss word embeddings
    in detail later. When we represent words as vectors, our 2D matrix becomes a 3D
    matrix. For example, if we set the vector length to 4, you will have a 3 × 6 ×
    4 3D tensor. Figure 5.2 depicts what the final matrix looks like.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，将单词转换为某种向量表示是很重要的。有许多将单词转换为向量的方法，例如独热编码和词嵌入。你已经看到了独热编码的工作原理，我们将稍后详细讨论词嵌入。当我们将单词表示为向量时，我们的
    2D 矩阵变为 3D 矩阵。例如，如果我们将向量长度设置为 4，你将得到一个 3 × 6 × 4 的 3D 张量。图 5.2 描述了最终矩阵的外观。
- en: '![05-02](../../OEBPS/Images/05-02.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![05-02](../../OEBPS/Images/05-02.png)'
- en: 'Figure 5.2 3D matrix representing a batch of a sequence of words, where each
    word is represented by a vector (i.e., the shaded block in the matrix). There
    are three dimensions: batch, sequence (time), and feature.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 表示一个单词序列批次的 3D 矩阵，其中每个单词由一个向量表示（即矩阵中的阴影块）。有三个维度：批次、序列（时间）和特征。
- en: Next we will discuss the various components of the popular Transformer model,
    which will give us a solid grounding in how these models perform internally.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论流行的 Transformer 模型的各个组成部分，这将为我们提供对这些模型内部执行的基础。
- en: 5.2 Understanding the Transformer model
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 理解 Transformer 模型
- en: You are currently working as a deep learning research scientist and were recently
    invited to conduct a workshop on Transformers at a local TensorFlow conference.
    Transformers are a new family of deep learning models that have surpassed their
    older counterparts in a plethora of tasks. You are planning to first explain the
    architecture of the Transformer network and then walk the participants through
    several exercises, where they will implement the basic computations found in Transformers
    as sub-classed Keras layers and finally use these to implement a basic small-scale
    Transformer using Keras.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 你目前是一名深度学习研究科学家，并最近受邀在当地 TensorFlow 大会上进行有关 Transformer 的研讨会。Transformer 是一类新型的深度学习模型，在众多任务中已经超越了它们的老对手。你计划首先解释
    Transformer 网络的架构，然后带领参与者完成几个练习，在这些练习中，他们将使用 Keras 的子类化层实现在 Transformers 中找到的基本计算，最后使用这些计算来实现一个基本的小规模
    Transformer。
- en: 5.2.1 The encoder-decoder view of the Transformer
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.1 Transformer 的编码器-解码器视图
- en: The Transformer network is based on an encoder-decoder architecture. The encoder-decoder
    pattern is common in deep learning for certain types of tasks (e.g., machine translation,
    question answering, unsupervised image reconstruction). The idea is that the encoder
    takes an input and maps it to some latent (or hidden) representation (typically
    smaller), and the decoder constructs a meaningful output using latent representation.
    For example, in machine translation, a sentence from language A is mapped to a
    latent vector, from which the decoder constructs the translation of that sentence
    in language B. You can think of the encoder and decoder as two separate machine
    learning models, where the decoder depends on the output of the encoder. This
    process is depicted in figure 5.3\. At a given time, both the encoder and the
    decoder consume a batch of a sequence of words (e.g., a batch of sentences). As
    machine learning models don’t understand text, every word in this batch is represented
    by a numerical vector. This is done by following a process such as one-hot encoding,
    similar to what we discussed in section 5.1.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '![05-03](../../OEBPS/Images/05-03.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 The encoder-decoder architecture for a machine translation task
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: The encoder-decoder pattern is common in real life as well. Say you are a tour
    guide in France and take a group of tourists to a restaurant. The waiter is explaining
    the menu in French, and you need to translate this to English for the group. Imagine
    how you would do that. When the waiter explains the dish in French, you process
    those words and create a mental image of what the dish is, and then you translate
    that mental image into a sequence of English words.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s dive more into the individual components and what they are made of.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2 Diving deeper
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Naturally, you might be asking yourself, “What do the encoder and the decoder
    consist of?” This is the topic of this section. Note that the encoder and decoder
    discussed here are quite different from the autoencoder model you saw in chapter
    3\. As said previously, the encoder and the decoder individually act like multilayered
    deep neural networks. They consist of several layers, where each layer comprises
    sublayers that encapsulate certain computations done on inputs to produce outputs.
    The output of the previous layer feeds as the input to the next layer. It is also
    important to note that inputs and outputs of the encoder and the decoder are sequences,
    such as sentences. Each layer within these models takes in a sequence of elements
    and outputs another sequence of elements. So, what constitutes a single layer
    in the encoder and the decoder?
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: 'Each encoder layer comprises two sublayers:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Self-attention layer
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fully connected layer
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The self-attention layer produces its final output similarly to a fully connected
    layer (i.e., using matrix multiplications and activation functions). A typical
    fully connected layer will take all elements in the input sequence, process them
    separately, and output an element in place of each input element. But the self-attention
    layer can select and combine different elements in the input sequence to output
    a given element. This makes the self-attention layer much more powerful than a
    typical fully connected layer (figure 5.4).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力层的最终输出与全连接层类似（即使用矩阵乘法和激活函数）。典型的全连接层将处理输入序列中的所有元素，并分别处理它们，然后输出一个元素以替换每个输入元素。但自注意力层可以选择和组合输入序列中的不同元素以输出给定元素。这使得自注意力层比典型的全连接层更加强大（见图5.4）。
- en: '![05-04](../../OEBPS/Images/05-04.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![05-04](../../OEBPS/Images/05-04.png)'
- en: Figure 5.4 The difference between the self-attention sublayer and the fully
    connected sublayer. The self-attention sublayer looks at all the inputs in the
    sequence, whereas the fully connected sublayer only looks at the input that is
    processed.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4 自注意力子层和全连接子层之间的区别。自注意力子层查看序列中的所有输入，而全连接子层只查看正在处理的输入。
- en: Why does it pay to select and combine different input elements this way? In
    an NLP context, the self-attention layer enables the model to look at other words
    while it processes a certain word. But what does that mean for the model? This
    means that while the encoder is processing the word “it” in the sentence “I kicked
    the *ball* and *it* disappeared,” the model can attend to the word “ball.” By
    seeing both words “ball” and “it” at the same time (learning dependencies), disambiguating
    words is easier. Such capabilities are of paramount importance for language understanding.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么以这种方式选择和组合不同的输入元素有好处？在自然语言处理的上下文中，自注意力层使模型在处理某个单词时能够查看其他单词。但这对模型意味着什么？这意味着在编码器处理句子“I
    kicked the *ball* and *it* disappeared”中的单词“it”时，模型可以关注单词“ball”。通过同时看到“ball”和“it”两个单词（学习依赖关系），消除歧义的单词变得更容易。这样的能力对语言理解至关重要。
- en: 'We can understand how self-attention helps us solve a task conveniently through
    a real-world example. Assume you are playing a game with two people: person A
    and person B. Person A holds a question written on a board, and you need to speak
    the answer. Say person A reveals just one word at a time, and after the last word
    of the question, it is revealed that you are answering the question. For long
    and complex questions, this is challenging, as you cannot physically see the complete
    question and have to heavily rely on memory when answering the question. This
    is what it feels like without self-attention. On the other hand, say person B
    reveals the full question on the board instead of word by word. Now it is much
    easier to answer the question, as you can see the whole question at once. If the
    question is complex and requires a complex answer, you can look at different parts
    of the question as you provide various sections of the full answer. This is what
    the self-attention layer enables.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过一个现实世界的例子了解自注意力如何方便地帮助我们解决任务。假设你正在和两个人玩游戏：A和B。A手持写有问题的板子，你需要说出答案。假设A一次只透露一个单词，直到问题的最后一个单词被揭示，才揭示你正在回答问题。对于长而复杂的问题，这是具有挑战性的，因为你不能物理上看到完整的问题，必须严重依赖记忆来回答问题。这就是没有自注意力时的感觉。另一方面，假设B将整个问题一次性展示在板上，而不是逐字逐句地展示。现在回答问题要容易得多，因为你可以一次看到整个问题。如果问题很复杂，需要复杂的答案，你可以在提供完整答案的各个部分时查看问题的不同部分。这就是自注意力层的作用。
- en: Next, the fully connected layer takes the output elements produced by the self-attention
    sublayer and produces a hidden representation for each output element in an element-wise
    fashion. This make the model deeper, allowing it to perform better.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，全连接层以逐元素的方式接受自注意力子层产生的输出元素，并为每个输出元素生成一个隐藏表示。这使得模型更加深入，从而表现更好。
- en: 'Let’s look in more detail at how data flows through the model in order to better
    understand the organization of layers and sublayers. Assume the task of translating
    the sentence “Dogs are great” (English) to “*Les chiens sont super*” (French).
    First, the encoder takes in the full sentence “Dogs are great” and produces an
    output for each word in the sentence. The self-attention layer selects the most
    important words for each position, computes an output, and sends that information
    to the fully connected layer to produce a deeper representation. The decoder produces
    output words iteratively, one after the other. To do that, the decoder looks at
    the final output sequence of the encoder and all the previous words predicted
    by the decoder. Assume the final prediction is <SOS> *les chiens sont super* <EOS>.
    Here, <SOS> marks the start of the sentence and <EOS> the end of the sentence.
    The first input it takes is a special tag that indicates the start of a sentence
    (<SOS>), along with the encoder outputs, and it produces the next word in the
    translation: “*les*.” The decoder then consumes <SOS> and “*les*” as inputs, produces
    the word “*chiens*,” and continues until the model reaches the end of the translation
    (marked by <EOS>). Figure 5.5 depicts this process.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: In the original Transformer paper, the encoder has six layers, and a single
    layer has a self-attention sublayer and a fully connected sublayer, in that order.
    First, the self-attention layer takes the English words as a time-series input.
    However, before feeding these words to the encoder, you need to create a numerical
    representation of each word, as discussed earlier. In the paper, word embeddings
    (with some additional encoding) are used to represent the words. Each of these
    embeddings is a 512-long vector. Then the self-attention layer computes a hidden
    representation for each word of the input sentence. If we ignore some of the implementation
    details, this hidden representation at time step *t* can be thought as a weighted
    sum of all the inputs (in a single sequence), where the weight for position *i*
    of the input is determined by how important it is to select (or attend to) the
    encoder word *ew*[i] in the input sequence while processing the word *ew*[t] in
    the encoder input. The encoder makes this decision for every position *t* in the
    input sequence. For example, while processing the word “it” in the sentence “I
    kicked the *ball* and *it* disappeared,” the encoder needs to pay more attention
    to the word “ball” than to the word “the.” The weights in the self-attention sublayer
    are trained to demonstrate such properties. This way, the self-attention layer
    produces a hidden representation for each encoder input. We call this the *attended
    representation/output*.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: The fully connected sublayer then takes over and is quite straightforward. It
    has two linear layers and a ReLU activation in between the layers. It takes the
    outputs of the self-attention layer and transforms to a hidden output using
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '*h*[1] = *ReLU*(*xW*[1] + *b*[1])'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '*h*[2] = *h*[1]*W*[2] + *b*[2]'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the second layer does not have a nonlinear activation. Next, the
    decoder has six layers as well, where each layer has three sublayers:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: A masked self-attention layer
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An encoder-decoder attention layer
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A fully connected layer
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The masked self-attention layer operates similarly to the self-attention layer.
    However, while processing the s^(th) word (i.e., *dw*[s]), it masks the words
    ahead of *dw*[s]. For example, while processing the word “*chiens*,” it can only
    attend to the words “<sos>” and “*les*.” This is important because the decoder
    must be able to predict the correct word, given only the previous words it predicted,
    so it makes sense to force the decoder to attend only to the words it has already
    seen.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Next, the encoder-decoder attention layer takes the encoder output and the outputs
    produced by the masked self-attention layer and produce a series of outputs. The
    purpose of this layer is to compute a hidden representation (i.e., an attended
    representation) at time *s* as a weighted sum of encoder inputs, where the weight
    for position *j* is determined by how important it is to attend to encoder input
    *e w*[j], while processing the decoder word *dw*[s].
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Finally, a fully connected layer identical to the fully connected layer from
    the encoder layer takes the output of the self-attention layer to produce the
    final output of the layer. Figure 5.5 depicts the layers and operations discussed
    in this section at a high level.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '![05-05](../../OEBPS/Images/05-05.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 Various layers in the encoder and the decoder and various connections
    formed within the encoder, within the decoder, and between the encoder and the
    decoder. The squares represent inputs and outputs of the models. The rectangular
    shaded boxes represent interim outputs of the sublayers.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss what the self-attention layer looks like.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.3 Self-attention layer
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have covered the purpose of the self-attention layer at an abstract level
    of understanding. It is to, while processing the word *w*[t] at time step *t*,
    determine how important it is to attend to the *i*^(th) word (i.e., *w*[i]) in
    the input sequence. In other words, the layer needs to determine the importance
    of all the other words (indexed by *i*) for every word (indexed by *t*). Let’s
    now understand the computations involved in this process at a more granular level.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'First, there are three different entities involved in the computation:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '*A query*—The query’s purpose is to represent the word currently being processed.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A key*—The key’s purpose is to represent the candidate words to be attended
    to while processing the current word.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A value*—The value’s purpose is to compute a weighted sum of all words in
    the sequence, where the weight for each word is based on how important it is for
    understanding the current word'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a given input sequence, query, key, and value need to be calculated for
    every position of the input. These are calculated by an associated weight matrix
    with each entity.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Note that this is an oversimplification of their relationship, and the actual
    relationship is somewhat complex and convoluted. But this understanding provides
    the motivation for why we need three different entities to compute self-attention
    outputs.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will understand how exactly a self-attention layer goes from an input
    sequence to a query, key, and value tensor and finally to the output sequence.
    The input word sequence is first converted to a numerical representation using
    word embedding lookup. Word embeddings are essentially a giant matrix, where there’s
    a vector of floats (i.e., an embedding vector) for each word in your vocabulary.
    Typically, these embeddings are several hundreds of elements long. For a given
    input sequence, we assume the input sequence is *n* elements long and each word
    vector is *d*[model] elements long. Then we have a *n* × *d*[model] matrix. In
    the original Transformer paper, word vectors are 512 elements long.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three weight matrices in the self-attention layer: query weights
    (*W*[q]), key weights (*W*[k]), and value weights (*W*[v]), respectively used
    to compute the query, key, and value vectors. *W*[q] is *d*[model] × *d*[q], *W*[k]
    is *d*[model] × *d*[k], and *W*[v] is *d*[model] × *d*[v]. Let’s define these
    elements in TensorFlow assuming a dimensionality of 512, as in the original Transformer
    paper. That is,'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '*d*[model] = *d*[q] = *d*[k] = *d*[v] = 512'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: We will first define our input x as a tf.constant, which has three dimensions
    (batch, time, feature). Wq, Wk, and Wv are declared as tf.Variable objects, as
    these are the parameters of the self-attention layer
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: which has shapes
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, *q*, *k*, and *v* are computed as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '*q* = *xW*[q]; shape transformation: *n* × *d*[model]. *d*[model] × *d*[q]
    = *n × d*[q]'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '*k* = *xW*[k]; shape transformation: *n* × *d*[model]. *d*[model] × *d*[k]
    = *n × d*[k]'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '*v* = *xW*[v]; shape transformation: *n* × *d*[model]. *d*[model] × *d*[v]
    = *n × d*[v]'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 'It is evident that computing *q*, *k*, and *v* is a simple matrix multiplication
    away. Remember that there is a batch dimension in front of all the inputs (i.e.,
    x) and output tensors (i.e,. q, k, and v) as we process batches of data. But to
    avoid clutter, we are going to ignore the batch dimension. Then we compute the
    final output of the self-attention layer as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '![05_05a](../../OEBPS/Images/05_05a.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
- en: Here, the component ![05_05b](../../OEBPS/Images/05_05b.png) (which will be
    referred to as *P*) is a probability matrix. This is all there is in the self-attention
    layer. Implementing self-attention with TensorFlow is very straightforward. As
    good data scientists, let’s create it as a reusable Keras layer, as shown in the
    next listing.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.1 The self-attention sublayer
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Defining the output dimensionality of the self-attention outputs
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Defining the variables for computing the query, key, and value entities
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Computing the query, key, and value tensors
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Computing the probability matrix
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Computing the final output
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a quick refresher:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: __init__(self, d)—Defines any hyperparameters of the layer
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: build(self, input_shape)—Creates the parameters of the layer as variables
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: call(self, v_x, k_x, q_x)—Defines the computations happening in the layer
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you look at the call(self, v_x, k_x, q_x) function, it takes in three inputs:
    one each for computing value, key, and query. In most cases these are the same
    input. However, there are instances where different inputs come into these computations
    (e.g., some computations in the decoder). Also, note that we return both h (i.e.,
    the final output) and p (i.e., the probability matrix). The probability matrix
    is an important visual aid, as it helps us understand when and where the model
    paid attention to words. If you want to get the output of the layer, you can do
    the following'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: which will return
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Exercise 1
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Given the following input
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: and assuming we need an output of size 512, write the code to create Wq, Wk,
    and Wv as tf.Variable objects. Use the np.random.normal() function to set the
    initial values.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.4 Understanding self-attention using scalars
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It is not yet very clear why the computations are designed the way they are.
    To understand and visualize what this layer is doing, we will assume a feature
    dimensionality of 1\. That is, a single word is represented by a single value
    (i.e., a scalar). Figure 5.6 visualizes the computations that happen in the self-attention
    layer if we assume a single-input sequence and the dimensionality of inputs (*d*[model]),
    query length (*d*[q]), key length (*d*[k]), and value length (*d*[v]) is 1\. As
    a concrete example, we start with an input sequence *x*, which has seven words
    (i.e., *n* × 1 matrix). Under the assumptions we’ve made, *W*[q], *W*[k], and
    *W*[v] will be scalars. The matrix multiplications used for computing *q*, *k*,
    and *v* essentially become scalar multiplications:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '*q* = (*q*[1], *q*[2],..., *q*[7]), where *q*[i] = *x*[i] *W*[q]'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '*k* = (*k*[1], *k*[2],..., *k*[7]), where *k*[i] = *x*[i] *W*[k]'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '*v* = (*v*[1], *v*[2],..., *v*[7]), where *v*[i] = *x*[i] *W*[v]'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Next, we need to compute the *P* = softmax ((*Q.K*^T) / √(*d*[k])) component.
    *Q.K*^T is essentially an *n* × *n* matrix that has an item representing every
    query and key combination (figure 5.6). The *i* ^(th) row and *j* ^(th) column
    of *Q.K*[(i,j)]^T are computed as
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '*Q.K*[(i,j)]^T =*q* [i] × *k* [j]'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Then, by applying the softmax, this matrix is converted to a row-wise probability
    distribution. You might have noted a constant √(*d*[k]) appearing within the softmax
    transformation. This is a normalization constant that helps prevent large gradient
    values and achieve stable gradients. In our example, you can ignore this as √(*d*[k])
    = 1.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we compute the final output *h* = (*h*[1],*h*[2],...,*h*[7]), where
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '*h*[i] = *P*[(i],[1)] *v*[1] + *P*[(i],[2)] *v*[2] +...+ *P*[(i],[7)] *v*[7]'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can more clearly see the relationship between *q*, *k*, and *v*. *q*
    and *k* are used to compute a soft-index mechanism for *v* when computing the
    final output. For example, when computing the fourth output (i.e., *h*[4]), we
    first hard-index the fourth row (following *q*[4]), and then mix various *v* values
    based on the soft index (i.e., probabilities) given by the columns (i.e., *k*
    values) of that row. Now it is more clear what purpose *q*, *k*, and *v* serve:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '*Query*—Helps build a probability matrix that is eventually used for indexing
    values (v). Query affects the rows of the matrix and represents the index of the
    current word that’s being processed.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Key*—Helps build a probability matrix that is eventually used for indexing
    values (v). Key affects the columns of the matrix and represents the candidate
    words that need to be mixed depending on the query word.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Value*—Hidden (i.e., attended) representation of the inputs used to compute
    the final output by indexing using the probability matrix created using query
    and key'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can easily take the big gray box in figure 5.6, place it over the self-attention
    sublayer, and still have the output shape (as shown in figure 5.5) being produced
    (figure 5.7).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '![05-06](../../OEBPS/Images/05-06.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 The computations in the self-attention layer. The self-attention
    layer starts with an input sequence and computes sequences of query, key, and
    value vectors. Then the queries and keys are converted to a probability matrix,
    which is used to compute the weighted sum of values.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '![05-07](../../OEBPS/Images/05-07.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 (top) and figure 5.6 (bottom). You can take the gray box from the
    bottom and plug it into a self-attention sublayer on the top and see that the
    same output sequence is being produced.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s scale up our self-attention layer and revisit the specific computations
    behind it and why they matter. Going back to our previous notation, we start with
    a sequence of words, which has *n* elements. Then, after the embedding lookup,
    which retrieves an embedding vector for each word, we have a matrix of size *n*
    × *d*[model]. Next, we have the weights and biases to compute each of the query,
    key, and value vectors:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '*q* = *xW*[q], where *x* ∈ ℝ^(n×dmodel). *W*[q] ∈ ℝ^(dmodel×)*dq* and *q* ∈
    ℝ^(n×d)*q*'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '*k* = *xW*[k], where *x* ∈ ℝ^(n×dmodel). *W*[k] ∈ ℝ^(dmodel×)*dk* and *k* ∈
    ℝ^(n×d)*k*'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '*v* = *xW*[v], where *x* ∈ ℝ^(n×dmodel). *W*[v] ∈ ℝ^(dmodel×)*dv* and *v* ∈
    ℝ^(n×d)*v*'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: For example, the query, or *q*, is a vector of size n × *d*[q], obtained by
    multiplying the input *x* of size *n* × *d*[model] with the weight matrix *W*[q]
    of size *d*[model] × *d*[q]. Also remember that, as in the original Transformer
    paper, we make sure that all of the input embedding of query, key, and value vectors
    are the same size. In other words,
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '*d*[model] = *d*[q] = *d*[k] = *d*[v] = 512'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we compute the probability matrix using the q and k values we obtained:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '![05_06a](../../OEBPS/Images/05_06a.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
- en: 'Finally, we multiply this probably matrix with our value matrix to obtain the
    final output of the self-attention layer:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '![05_06b](../../OEBPS/Images/05_06b.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
- en: The self-attention layer takes a batch of a sequence of words (e.g., a batch
    of sentences of fixed length), where each word is represented by a vector, and
    produces a batch of a sequence of hidden outputs, where each hidden output is
    a vector.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: How does self-attention compare to recurrent neural networks (RNNs)?
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Before Transformer models, RNNs governed the domain of NLP. RNNs were popular
    for NLP problems because most are inherently time-series problems. You can think
    of a sentence/phrase as a sequence of words (i.e., each represented by a feature
    vector) spread across time. The RNN goes through this sequence, consuming one
    word at a time (while maintaining a memory/state vector), and produces some output
    (or a series of outputs) at the end. But you will see that RNNs perform more and
    more poorly as the length of the sequence increases. This is because by the time
    the RNN gets to the end of the sequence, it has probably forgotten what it saw
    at the start.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: You can see that this problem is alleviated by the self-attention mechanism,
    which allows the model to look at the full sequence at a given time. This enables
    Transformer models to perform much better than RNN-based models.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.5 Self-attention as a cooking competition
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The concept of self-attention might still be a little bit elusive, making it
    difficult to understand what exactly is transpiring in the self-attention sublayer.
    The following analogy might alleviate the burden and make it easier. Say you are
    taking part in a cooking show with six other contestants (seven contestants in
    total). The game is as follows.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: You are at a supermarket and are given a T-shirt with a number on it (from 1-7)
    and a trolley. The supermarket has seven aisles. You have to sprint to the aisle
    with the number on your T-shirt, and there will be a name of some beverage (e.g.,
    apple juice, orange juice, lime juice) posted on the wall. You need to pick what’s
    necessary to make that beverage, sprint to your allocated table, and make that
    beverage.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Say that you are number 4 and got orange juice, so you’ll make your way to aisle
    4 and collect oranges, a bit of salt, a lime, sugar, and so on. Now say the opponent
    next to you (number 3), had to make lime juice; they will pick limes, sugar, and
    salt. As you can see, you are picking different items as well as different quantities
    of the same item. For example, your opponent hasn’t picked oranges, but you have,
    and you probably picked fewer limes compared to your opponent who is making lime
    juice.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: This is quite similar to what’s happening in the self-attention layer. You and
    your contestants are the inputs (at a single time step) to the model. The aisles
    are the queries, and the grocery items you have to pick are the keys. Just like
    indexing the probability matrix with query and keys to get the “mixing coefficients”
    (i.e., attention weights) for the values, you index the items you need by the
    aisle number allocated to you (i.e., query) and the quantity of each item in the
    aisle (i.e., key). Finally, the beverage you make is the value. Note that this
    analogy does not have 100% correspondence to the computations in the self-attention
    sublayer. However, you can draw significant similarities between the two processes
    at an abstract level. The similarities we discovered are shown in figure 5.8.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '![05-08](../../OEBPS/Images/05-08.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
- en: Figure 5.8 Self-attention depicted with the help of a cooking competition. The
    contestants are the queries, the keys are the grocery items you have to choose
    from, and the values are the final beverage you’re making.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Next we will discuss what is meant by a masked self-attention layer.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.6 Masked self-attention layers
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As you have already seen, the decoder has a special additional self-attention
    sublayer called *masked self-attention*. As we have already stated, the idea is
    to prevent the model from “cheating” by attending to the words it shouldn’t (i.e.,
    the words ahead of the position the model has predicted for). To understand this
    better, assume two people are teaching a student to translate from English to
    French. The first person gives an English sentence, asks the student to produce
    the translation word by word, and provides feedback up to the word translated
    so far. The second person gives an English sentence and asks the student to produce
    the translation but provides the full translation in advance. In the second instance,
    it is much easier for a student to cheat, providing a good quality translation,
    while having very little knowledge of the languages. Now let’s understand the
    looming danger of attending to the words it shouldn’t from a machine learning
    point of view.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Take the task of translating the sentence “dogs are great” to “*les chiens sont
    super*.” When processing the sentence “Dogs are great,” the model should be able
    to attend to any word in that sentence, as that’s an input fully available to
    the model at any given time. But, while processing the sentence “*Les chiens sont
    super*,” we need to be careful about what we show to the model and what we don’t.
    For example, while training the model, we typically feed the full output sequence
    at once, as opposed to iteratively feeding the words, to enhance computational
    efficiency. When feeding the full output sequence to the decoder, we must mask
    all words ahead of what is currently being processed because it is not fair for
    the model to predict the word “*chiens*” when it can see everything that comes
    after that word. It is imperative you do this. If you don’t, the code will run
    fine. But ultimately you will have very poor performance when you bring it to
    the real world. The way to force this is by making the probability matrix p a
    lower-triangular matrix. This will essentially give zero probability for mixing
    any input ahead of itself during the attention/output computation. The differences
    between standard self-attention and masked self-attention are shown in figure
    5.9.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '![05-09](../../OEBPS/Images/05-09.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 Standard self-attention versus masked self-attention methods. In
    the standard attention method, a given step can see an input from any other timestep,
    regardless of whether those inputs appear before or after the current time step.
    However, in the masked self-attention method, the current timestep can only see
    the current input and what came before that time step.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Let’s learn how we can do this in TensorFlow. We do a very simple change to
    the call() function by introducing a new argument, mask, which represents the
    items the model shouldn’t see with a 1 and the rest with a 0\. Then, to those
    elements the model shouldn’t see, we add a very large negative number (i.e., -
    10⁹) so that when softmax is applied they become zeros (listing 5.2).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.2 Masked self-attention sublayer
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ The call function takes an additional mask argument (i.e., a matrix of 0s
    and 1s).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Now, the SelfAttentionLayer supports both masked and unmasked inputs.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: ❸ If the mask is provided, add a large negative value to make the final probabilities
    zero for the words not to be seen.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Creating the mask is easy; you can use the tf.linalg.band_part() function to
    create triangular matrices
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: which gives
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We can easily verify if the masking worked by looking at the probability matrix
    p. It must be a lower triangular matrix
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: which gives
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now, when computing the value, the model cannot see or attend the words it hasn’t
    seen by the time it comes to the current word.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.7 Multi-head attention
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The original Transformer paper discusses something called multi-head attention,
    which is an extension of the self-attention layer. The idea is simple once you
    understand the self-attention mechanism. The multi-head attention creates multiple
    parallel self-attention heads. The motivation for this is that, practically, when
    the model is given the opportunity to learn multiple attention patterns (i.e.,
    multiple sets of weights) for an input sequence, it performs better.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Remember that in a single attention head we had all query, key, and value dimensionality
    set to 512\. In other words,
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '*d*[q] = *d*[k] = *d*[v] = 512'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: With multi-head attention, assuming we are using eight attention heads,
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '*d*[q] = *d*[k] = *d*[v] = 512/8 = 64'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Then the final outputs of all attention heads are concatenated to create the
    final output, which will have a dimensionality of 64 × 8 = 512
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '*H* = *Concat* (*h*¹, *h*², ... , *h*⁸)'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: where *h*^i is the output of the *i*^(th) attention head. Using the SelfAttentionLayer
    we just implemented, the code becomes
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: which gives
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As you can see, it still has the same shape as before (without multiple heads).
    However, this output is computed using multiple heads, which have smaller dimensionality
    than the original self-attention layer.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.8 Fully connected layer
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The fully connected layer is a piece of cake compared to what we just learned.
    So far, the self-attention layer has produced a *n* × *d*[v]-sized output (ignoring
    the batch dimension). The fully connected layer takes this input and performs
    the following transformation
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '*h*[1] = *ReLU*(*xW*[1] + *b*[1])'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: where *W*[1] is a *d*[v] × *d*[ff1] matrix and *b*[1] is a *d*[ff1]-sized vector.
    Therefore, this operation gives out a *n*×*d*[ff1]-sized tensor. The resulting
    output is passed onto another layer, which does the following computation
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '*h*[2] = *h*[1] *W*[2] + *b* [2]'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: where *W*[2] is a *d*[ff1] × *d*[ff2]-sized matrix and *b*[2] is a *d*[ff2]-sized
    vector. This operation gives a tensor of size *n* × *d*[ff2]. In TensorFlow parlance,
    we can again encapsulate these computations as a reusable Keras layer (see the
    next listing).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.3 The fully connected sublayer
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ The output dimensionality of the first fully connected computation
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: ❷ The output dimensionality of the second fully connected computation
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Defining W1, b1, W2, and b2 accordingly. We use glorot_uniform as the initializer.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Computing the first fully connected computation
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Computing the second fully connected computation
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Here, you could use the tensorflow.keras.layers.Dense() layer to implement this
    functionality. However, we will do it with raw TensorFlow operations as an exercise
    to familiarize ourselves with low-level TensorFlow. In this setup, we will change
    the FCLayer, as shown in the following listing.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.4 The fully connected layer implemented using Keras Dense layers
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Defining the first Dense layer in the __init__ function of the subclassed
    layer
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Defining the second Dense layer. Note how we are not specifying an activation
    function.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Calling the first dense layer to get the output
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Calling the second dense layer with the output of the first Dense layer to
    get the final output
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Now you know what computations take place in the Transformer architecture and
    how to implement them with TensorFlow. But keep in mind that there are various
    fine-grained details explained in the original Transformer paper, which we haven’t
    discussed. Most of these details will be discussed in a later chapter.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 2
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Say you have been asked to experiment with a new type of multi-head attention
    mechanism. Instead of concatenating outputs from smaller heads (of size 64), the
    outputs (of size 512) are summed. Write TensorFlow code using the SelfAttentionLayer
    to achieve this effect. You can use the tf.math.add_n() function to sum a list
    of tensors element-wise.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.9 Putting everything together
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s bring all these elements together to create a Transformer network. Let’s
    first create an encoder layer, which contains a set of SelfAttentionLayer objects
    (one for each head) and a FCLayer (see the next listing).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.5 The encoder layer
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Create multiple attention heads. Each attention head has d/n_heads-sized feature
    dimensionality.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Create the fully connected layer, where the intermediate layer has 2,048 nodes
    and the final sublayer has d nodes.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Create a function that computes the multi-head attention output given an input.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Compute multi-head attention using the defined function.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Get the final output of the layer.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 'The EncoderLayer takes in two parameters during initialization: d (dimensionality
    of the output) and n_heads (number of attention heads). Then, when calling the
    layer, a single input x is passed. First, the attended output of the attention
    heads (SelfAttentionLayer) is computed, followed by the output of the fully connected
    layer (FCLayer). This wraps the crux of an encoder layer. Next, we create a Decoder
    layer (see the next listing).'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.6 The DecoderLayer
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Create the attention heads that process the decoder input only.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Create the attention heads that process both the encoder output and decoder
    input.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: ❸ The final fully connected sublayer
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: ❹ The function that computes the multi-head attention. This function takes three
    inputs (decoder’s previous output, encoder output, and an optional mask).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Each head takes the first argument of the function as the query and key and
    the second argument of the function as the value.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Compute the first attended output. This only looks at the decoder inputs.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Compute the second attended output. This looks at both the previous decoder
    output and the encoder output.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Compute the final output of the layer by feeding the output through a fully
    connected sublayer.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: The decoder layer has several differences compared to the encoder layer. It
    contains two multi-head attention layers (one masked and one unmasked) and a fully
    connected layer. First, the output of the first multi-head attention layer (masked)
    is computed. Remember that we are masking any decoder input that is ahead of the
    current decoder input that’s been processed. We use the decoder inputs to compute
    the output of the first attention layer. However, the computations happening in
    the second layer are a bit tricky. Brace yourselves! The second attention layer
    takes the encoder network’s last attended output as query and key; then, to compute
    the value, the output of the first attention layer is used. Think of this layer
    as a mixer that mixes attended encoder outputs and attended decoder inputs.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: With that, we can create a simple Transformer model with two encoder layers
    and two decoder layers). We’ll use the Keras functional API (see the next listing).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.7 The full Transformer model
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ The hyperparameters of the Transformer model
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: ❷ The mask that will be used to mask decoder inputs
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: ❸ The encoder’s input layer. It accepts a batch of a sequence of word IDs.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: ❹ The embedding layer that will look up the word ID and return an embedding
    vector for that ID
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Compute the output of the first encoder layer.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: ❻ The decoder’s input layer. It accepts a batch of a sequence of word IDs.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: ❼ The decoder’s embedding layer
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Compute the output of the first decoder layer.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: ❾ The final prediction layer that predicts the correct output sequence
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: ❿ Defining the model. Note how we are providing a name for the model.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Before diving into the details, let’s refresh our memory with what the Transformer
    architecture looks like (figure 5.10).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '![05-10](../../OEBPS/Images/05-10.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
- en: Figure 5.10 The Transformer model architecture
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Since we have explored the underpinning elements quite intensively, the network
    should be very easy to follow. All we have to do is set up the encoder model,
    set up the decoder model, and combine these appropriately by creating a Model
    object. Initially we define several hyperparameters. Our model takes n_steps-long
    sentences. This means that if a given sentence is shorter than n_steps, we will
    pad a special token to make it n_steps long. If a given sentence is longer than
    n_steps, we will truncate the sentence up to n_steps words. The larger the n_steps
    value, the more information you retain in the sentences, but also the more memory
    your model will consume. Next, we have the vocabulary size of the encoder inputs
    (i.e., the number of unique words in the data set fed to the encoder) (n_en_vocab),
    the vocabulary size of the decoder inputs (n_de_vocab), the number of heads (n_heads),
    and the output dimensionality (d).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 'With that we have defined the encoder input layer, which takes a batch of n_steps-long
    sentences. In these sentences, each word will be represented by a unique ID. For
    example, the sentence “The cat sat on the mat” will be converted to [1, 2, 3,
    4, 1, 5]. Next, we have a special layer called Embedding, which provides a d elements-long
    representation for each word (i.e., word vectors). After this transformation,
    you have a (batch size, n_steps, d)-sized output, which is the format of the output
    that should go into the self-attention layer. We discussed this transformation
    briefly in chapter 3 (section 3.4.3). The Embedding layer is essentially a lookup
    table. Given a unique ID (each ID represents a word), it gives out a vector that
    is d elements long. In other words, this layer encapsulates a large matrix of
    size (vocabulary size, d). You can see that when defining the Embedding layer:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We need to provide the vocabulary size (the first argument) and the output dimensionality
    (the second argument), and finally, since we are processing an input sequence
    of length n_steps, we need to specify the input_length argument. With that, we
    can pass the output of the embedding layer (en_emb) to an Encoder layer. You can
    see that we have two encoder layers in our model.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, moving on to the decoder, everything at a high level looks identical
    to the encoder, except for two differences:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: The Decoder layer takes both the encoder output (en_out2) and the decoder input
    (de_emb or de_out1) as inputs.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Decoder layer also has a final Dense layer that produces the correct output
    sequence (e.g., in a machine translation task, these would be the translated word
    probabilities for each time step).
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can now define and compile the model as
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Note that we can provide a name for our model when defining it. We will name
    our model “MinTransformer.” As the final step, let’s look at the model summary,
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'which will provide the following output:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The workshop participants are going to walk out of this workshop a happy bunch.
    You have covered the essentials of Transformer networks while teaching the participants
    to implement their own. We first explained that the Transformer has an encoder-decoder
    architecture. We then looked at the composition of the encoder and the decoder,
    which are made of self-attention layers and fully connected layers. The self-attention
    layer allows the model to attend to other input words while processing a given
    input word, which is important when processing natural language. We also saw that,
    in practice, the model uses multiple attention heads in a single attention layer
    to improve performance. Next, the fully connected layer creates a nonlinear representation
    of the attended output. After understanding the basic elements, we implemented
    a basic small-scale Transformer network using reusable custom layers we created
    for the self-attention (SelfAttentionLayer) and fully connected layer (FCLayer).
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to train this model on an NLP data set (e.g., machine translation).
    However, training these models is a topic for a separate chapter. There’s a lot
    more to Transformers than what we have discussed. For example, there are pretrained
    transformer-based models that you can use readily to solve NLP tasks. We will
    revisit Transformers again in a later chapter.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Transformer networks have outperformed other models in almost all NLP tasks.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformers are an encoder-decoder-type neural network that is mainly used
    for learning NLP tasks.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With Transformers, the encoder and decoder are made of two computational sublayers:
    self-attention layers and fully connected layers.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The self-attention layer produces a weighted sum of inputs for a given time
    step, based on how important it is to attend to other positions in the sequence
    while processing the current position.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fully connected layer creates a nonlinear representation of the attended
    output produced by the self-attention layer.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The decoder uses masking in its self-attention layer to make sure that the decoder
    does not see any future predictions while producing the current prediction.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Answers to exercises
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Exercise 1**'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '**Exercise 2**'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
