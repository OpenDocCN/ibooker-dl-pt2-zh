- en: '5 State-of-the-art in deep learning: Transformers'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 深度学习的最新技术：Transformer
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容包括：
- en: Representing text in numerical format for machine learning models
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为机器学习模型以数值形式表示文本
- en: Building a Transformer model using the Keras sub-classing API
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Keras sub-classing API 构建 Transformer 模型
- en: We have seen many different deep learning models so far, namely fully connected
    networks, convolutional neural networks, and recurrent neural networks. We used
    a fully connected network to reconstruct corrupted images, a convolutional neural
    network to classify vehicles from other images, and finally an RNN to predict
    future CO2 concentration values. In this chapter we are going to talk about a
    new type of model known as the Transformer.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了许多不同的深度学习模型，包括全连接网络、卷积神经网络和循环神经网络。我们使用全连接网络来重建受损图像，使用卷积神经网络来对车辆进行分类，最后使用
    RNN 来预测未来的 CO2 浓度值。在本章中，我们将讨论一种新型的模型，即 Transformer。
- en: Transformers are the latest generation of deep networks to emerge. Vaswani et
    al., in their paper “Attention Is All You Need” ([https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf)),
    popularized the idea. They coined the term *Transformer* and explained how it
    shows great promise for the future. In the years following, leading tech companies
    like Google, OpenAI, and Facebook implemented bigger and better Transformer models
    that have significantly outperformed other models in the NLP domain. Here, we
    will refer to the model introduced in their paper by Vaswani et al. to learn about
    it. Although Transformers do exist for other domains (e.g., computer vision),
    we will focus on how the Transformer is used in the NLP domain, particularly on
    a machine translation task (i.e., language translation using machine learning
    models). This discussion will leave out some of the details from the original
    Transformer paper to improve clarity, but these details will be covered in a later
    chapter.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 是最新一代的深度网络。瓦斯瓦尼等人在他们的论文《Attention Is All You Need》（[https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf)）中普及了这个想法。他们创造了“Transformer”这个术语，并解释了它在未来有很大潜力。在随后的几年里，谷歌、OpenAI
    和 Facebook 等领先的科技公司实施了更大更好的 Transformer 模型，这些模型在 NLP 领域显著优于其他模型。在这里，我们将参考瓦斯瓦尼等人在论文中介绍的模型来学习它。虽然
    Transformer 也存在于其他领域（例如计算机视觉），我们将重点介绍 Transformer 在 NLP 领域中的应用，特别是在机器翻译任务中（即使用机器学习模型进行语言翻译）。本章将省略原始
    Transformer 论文中的一些细节，以提高清晰度，但这些细节将在后面的章节中进行介绍。
- en: Knowing the inner workings of the Transformer model is a must for anyone who
    wants to excel at using deep learning models to solve real-world problems. As
    explained, the Transformer model has proliferated the machine learning field quite
    rapidly. This is mainly because of the performance it has demonstrated in solving
    complex machine learning problems.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 想要在使用深度学习模型解决实际问题时出类拔萃，了解 Transformer 模型的内部工作原理是必不可少的。如前所述，Transformer 模型在机器学习领域迅速普及。这主要是因为它在解决复杂机器学习问题方面展现出的性能。
- en: 5.1 Representing text as numbers
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 将文本表示为数字
- en: Say you are taking part in a game show. One challenge in the game is called
    Word Boxes. There is a matrix of transparent boxes (3 rows, 5 columns, 10 depths).
    You also have balls with 0 or 1 painted on them. You are given three sentences,
    and your task is to fill all the boxes with 1s and 0s to represent those sentences.
    Additionally, you can write a short message (within a minute) that helps someone
    decipher this later. Later, another team member looks at the boxes and writes
    down as many words in the original sentences you were initially given.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在参加一个游戏节目。游戏中的一个挑战叫做单词盒子。有一个由透明盒子组成的矩阵（3行，5列，10深度）。你也有一些上面涂有0或1的球。你被给予了三个句子，你的任务是用1和0填充所有的盒子来表示这些句子。此外，你可以在一分钟内写一条简短的信息，帮助其他人在以后破译这一信息。之后，另一个队员看着盒子，写下最初给你的原始句子中的尽可能多的单词。
- en: The challenge is essentially how you can transform text to numbers for machine
    translation models. This is also an important problem you work on before learning
    about any NLP model. The data we have seen so far has been numerical data structures.
    For example, an image can be represented as a 3D array (height, width, and channel
    dimensions), where each value represents a pixel intensity (i.e., a value between
    0 and 255). But what about text? How can we make a computer understand characters,
    words, or sentences? We will learn how to do this with Transformers in the context
    of natural language processing (NLP).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这个挑战本质上是如何将文本转换成数字，用于机器翻译模型。这也是在了解任何NLP模型之前需要解决的重要问题。到目前为止我们看到的数据都是数值型数据结构。例如，一张图像可以被表示为一个3D数组（高度，宽度和通道维度），其中每个值表示像素强度（即，取值范围在0至255之间）。但文本呢？我们怎么让计算机理解字符、单词或句子呢？我们将在自然语言处理（NLP）的情境中学习如何用Transformer完成这一点。
- en: 'You have the following set of sentences:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 您有以下一组句子：
- en: I went to the beach.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我去了海滩。
- en: It was cold.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 天气很冷。
- en: I came back to the house.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我回到了房子。
- en: 'The first thing you do is assign each word in your vocabulary an ID starting
    from 1\. We will reserve the number 0 for a special token we will see later. Say
    you assign the following IDs:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你要做的第一件事是给词汇表中的每个单词分配一个从1开始的ID。我们将保留数字0给我们稍后会看到的特殊标记。假设你分配了以下ID：
- en: I → 1
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: I → 1
- en: went → 2
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: went → 2
- en: to → 3
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: to → 3
- en: the → 4
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: the → 4
- en: beach → 5
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: beach → 5
- en: It → 6
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它 → 6
- en: was → 7
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: was → 7
- en: cold → 8
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: cold → 8
- en: came → 9
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: came → 9
- en: back → 10
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: back → 10
- en: house → 11
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: house → 11
- en: 'After mapping the words to the corresponding IDs, our sentences become the
    following:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 将单词映射到相应的ID后，我们的句子变为了下面这个样子：
- en: '[1, 2, 3, 4, 5]'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1, 2, 3, 4, 5]'
- en: '[6, 7, 8]'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6, 7, 8]'
- en: '[1, 9, 10, 3, 4, 11]'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1, 9, 10, 3, 4, 11]'
- en: Remember, you need to fill in all the boxes and have a maximum length of 5\.
    Note that our last sentence has six words. This means all the sentences need to
    be represented by a fixed length. Deep learning models face a similar problem.
    They process data in batches, and to process it efficiently, the sequence length
    needs to be fixed for that batch. Real-world sentences can vary significantly
    in terms of their length. Therefore, we need to
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，您需要填写所有方框，并且最多长度为5。请注意我们的最后一句有六个单词。这意味着所有句子都需要表示为固定长度。深度学习模型面临类似的问题。它们以批处理的方式处理数据，并且为了高效处理数据，批处理的序列长度需要是固定的。真实世界的句子在长度上可能差异很大。因此，我们需要
- en: Pad short sentences with a special token <PAD> (with ID 0)
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用特殊标记<PAD>（ID为0）填充短句
- en: Truncate long sentences
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 截断长句
- en: 'to make them the same length. If we pad the short sentences and truncate long
    sentences so that the length is 5, we get the following:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 使它们具有相同的长度。如果我们填充短句并截断长句，使长度为5，我们得到以下结果：
- en: '[1, 2, 3, 4, 5]'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1, 2, 3, 4, 5]'
- en: '[6, 7, 8, 0, 0]'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6, 7, 8, 0, 0]'
- en: '[1, 9, 10, 3, 4]'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1, 9, 10, 3, 4]'
- en: Here, we have a 2D matrix of size 3 × 5, which represents our batch of sentences.
    The final thing to do is represent each of these IDs as vectors. Because our balls
    have 1s and 0s, you can represent each word with 11 balls (we have 10 different
    words and the special token <PAD>), where the ball at the position indicated by
    the word ID is 1 and the rest are 0s. This method is known as one-hot encoding.
    For example,
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们有一个大小为3×5的2D矩阵，它表示我们的一批句子。最后要做的一件事是将这些ID表示为向量。因为我们的球有1和0，你可以用11个球（我们有10个不同的单词和特殊标记<PAD>）代表每个单词，其中由单词ID指示的位置上的球为1，其余为0。这种方法称为one-hot编码。例如，
- en: 0 → [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以下分别代表着各自的ID：
- en: 1 → [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 1 → [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
- en: . . .
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 。 。 。
- en: 10 → [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 10 → [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
- en: 11→ [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 11 → [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
- en: Now you can fill the boxes with 1s and 0s such that you get something like figure
    5.1\. This way, anyone who has the word for ID mapping (provided in a sheet of
    paper) can decipher most of the words (except for those truncated) that were initially
    provided.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以用1和0填写方框，使得你得到类似图5.1的结果。这样，任何一位有这些ID映射的人（提供在一张纸上）都可以解密最初所提供的大部分单词（除了被截断的单词）。
- en: '![05-01](../../OEBPS/Images/05-01.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![05-01](../../OEBPS/Images/05-01.png)'
- en: Figure 5.1 The boxes in the Word Boxes game. The shaded boxes represent a single
    word (i.e., the first word in the first sentence, “I,” which has an ID of 1).
    You can see it’s represented by a single ball of 1 and nine balls of 0.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 词盒游戏中的方框。阴影方框代表一个词（即第一个句子中的第一个词“I”，它的ID是1）。你可以看到它被一个1和九个0所表示。
- en: 'Again, this is a transformation done to words in NLP problems. You might ask,
    “Why not feed the word IDs directly?” There are two problems:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，这是在 NLP 问题中对单词进行的转换。你可能会问：“为什么不直接提供单词 ID？”存在两个问题：
- en: The value ranges the neural network sees are very large (0-100,000+) for a real-world
    problem. This will cause instabilities and make the training difficult.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络看到的值范围非常大（0-100,000+）对于一个现实世界的问题。这会导致不稳定性并使训练困难。
- en: Feeding in IDs would falsely indicate that words with similar IDs should be
    alike (e.g., word ID 4 and 5). This is never the case and would confuse the model
    and lead to poor performance.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入 ID 会错误地表明具有相似 ID 的单词应该是相似的（例如，单词 ID 4 和 5）。这从未发生过，会使模型混淆并导致性能下降。
- en: Therefore, it is important to bring words to some vector representation. There
    are many ways to turn words into vectors, such as one-hot encoding and word embeddings.
    You have already seen how one-hot encoding works, and we will discuss word embeddings
    in detail later. When we represent words as vectors, our 2D matrix becomes a 3D
    matrix. For example, if we set the vector length to 4, you will have a 3 × 6 ×
    4 3D tensor. Figure 5.2 depicts what the final matrix looks like.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，将单词转换为某种向量表示是很重要的。有许多将单词转换为向量的方法，例如独热编码和词嵌入。你已经看到了独热编码的工作原理，我们将稍后详细讨论词嵌入。当我们将单词表示为向量时，我们的
    2D 矩阵变为 3D 矩阵。例如，如果我们将向量长度设置为 4，你将得到一个 3 × 6 × 4 的 3D 张量。图 5.2 描述了最终矩阵的外观。
- en: '![05-02](../../OEBPS/Images/05-02.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![05-02](../../OEBPS/Images/05-02.png)'
- en: 'Figure 5.2 3D matrix representing a batch of a sequence of words, where each
    word is represented by a vector (i.e., the shaded block in the matrix). There
    are three dimensions: batch, sequence (time), and feature.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 表示一个单词序列批次的 3D 矩阵，其中每个单词由一个向量表示（即矩阵中的阴影块）。有三个维度：批次、序列（时间）和特征。
- en: Next we will discuss the various components of the popular Transformer model,
    which will give us a solid grounding in how these models perform internally.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论流行的 Transformer 模型的各个组成部分，这将为我们提供对这些模型内部执行的基础。
- en: 5.2 Understanding the Transformer model
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 理解 Transformer 模型
- en: You are currently working as a deep learning research scientist and were recently
    invited to conduct a workshop on Transformers at a local TensorFlow conference.
    Transformers are a new family of deep learning models that have surpassed their
    older counterparts in a plethora of tasks. You are planning to first explain the
    architecture of the Transformer network and then walk the participants through
    several exercises, where they will implement the basic computations found in Transformers
    as sub-classed Keras layers and finally use these to implement a basic small-scale
    Transformer using Keras.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 你目前是一名深度学习研究科学家，并最近受邀在当地 TensorFlow 大会上进行有关 Transformer 的研讨会。Transformer 是一类新型的深度学习模型，在众多任务中已经超越了它们的老对手。你计划首先解释
    Transformer 网络的架构，然后带领参与者完成几个练习，在这些练习中，他们将使用 Keras 的子类化层实现在 Transformers 中找到的基本计算，最后使用这些计算来实现一个基本的小规模
    Transformer。
- en: 5.2.1 The encoder-decoder view of the Transformer
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.1 Transformer 的编码器-解码器视图
- en: The Transformer network is based on an encoder-decoder architecture. The encoder-decoder
    pattern is common in deep learning for certain types of tasks (e.g., machine translation,
    question answering, unsupervised image reconstruction). The idea is that the encoder
    takes an input and maps it to some latent (or hidden) representation (typically
    smaller), and the decoder constructs a meaningful output using latent representation.
    For example, in machine translation, a sentence from language A is mapped to a
    latent vector, from which the decoder constructs the translation of that sentence
    in language B. You can think of the encoder and decoder as two separate machine
    learning models, where the decoder depends on the output of the encoder. This
    process is depicted in figure 5.3\. At a given time, both the encoder and the
    decoder consume a batch of a sequence of words (e.g., a batch of sentences). As
    machine learning models don’t understand text, every word in this batch is represented
    by a numerical vector. This is done by following a process such as one-hot encoding,
    similar to what we discussed in section 5.1.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 网络基于编码器-解码器架构。编码器-解码器模式在某些类型的深度学习任务中很常见（例如，机器翻译、问答、无监督图像重建）。其思想是编码器将输入映射到某种潜在（或隐藏）表示（通常较小），而解码器使用潜在表示构建有意义的输出。例如，在机器翻译中，语言
    A 的句子被映射到一个潜在向量，解码器从中构建语言 B 中该句子的翻译。你可以将编码器和解码器视为两个独立的机器学习模型，其中解码器依赖于编码器的输出。这个过程如图
    5.3 所示。在给定的时间点，编码器和解码器同时处理一批词序列（例如，一批句子）。由于机器学习模型不理解文本，因此该批次中的每个单词都由一个数字向量表示。这是通过一种过程来实现的，例如独热编码，类似于我们在第
    5.1 节中讨论的内容。
- en: '![05-03](../../OEBPS/Images/05-03.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![05-03](../../OEBPS/Images/05-03.png)'
- en: Figure 5.3 The encoder-decoder architecture for a machine translation task
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 机器翻译任务的编码器-解码器架构
- en: The encoder-decoder pattern is common in real life as well. Say you are a tour
    guide in France and take a group of tourists to a restaurant. The waiter is explaining
    the menu in French, and you need to translate this to English for the group. Imagine
    how you would do that. When the waiter explains the dish in French, you process
    those words and create a mental image of what the dish is, and then you translate
    that mental image into a sequence of English words.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器-解码器模式在现实生活中也很常见。假设你是法国的导游，带着一群游客去一家餐厅。服务员用法语解释菜单，你需要为团队将其翻译成英语。想象一下你会如何做。当服务员用法语解释菜肴时，你处理这些词语并创建出菜肴的心理图像，然后将该心理图像翻译成一系列英语词语。
- en: Now let’s dive more into the individual components and what they are made of.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们更深入地了解各个组件及其构成。
- en: 5.2.2 Diving deeper
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.2 更深入地探讨
- en: Naturally, you might be asking yourself, “What do the encoder and the decoder
    consist of?” This is the topic of this section. Note that the encoder and decoder
    discussed here are quite different from the autoencoder model you saw in chapter
    3\. As said previously, the encoder and the decoder individually act like multilayered
    deep neural networks. They consist of several layers, where each layer comprises
    sublayers that encapsulate certain computations done on inputs to produce outputs.
    The output of the previous layer feeds as the input to the next layer. It is also
    important to note that inputs and outputs of the encoder and the decoder are sequences,
    such as sentences. Each layer within these models takes in a sequence of elements
    and outputs another sequence of elements. So, what constitutes a single layer
    in the encoder and the decoder?
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，你可能会问自己：“编码器和解码器由什么组成？”这是本节的主题。请注意，此处讨论的编码器和解码器与你在第 3 章中看到的自编码器模型有很大不同。正如之前所述，编码器和解码器分别像多层深度神经网络一样工作。它们由多个层组成，每个层包含子层，封装了对输入进行的某些计算以产生输出。前一层的输出作为下一层的输入。还需要注意的是，编码器和解码器的输入和输出是序列，例如句子。这些模型中的每个层都接收一个元素序列并输出另一个元素序列。那么，编码器和解码器中的单个层包含什么？
- en: 'Each encoder layer comprises two sublayers:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 每个编码器层包含两个子层：
- en: Self-attention layer
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自注意力层
- en: Fully connected layer
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全连接层
- en: The self-attention layer produces its final output similarly to a fully connected
    layer (i.e., using matrix multiplications and activation functions). A typical
    fully connected layer will take all elements in the input sequence, process them
    separately, and output an element in place of each input element. But the self-attention
    layer can select and combine different elements in the input sequence to output
    a given element. This makes the self-attention layer much more powerful than a
    typical fully connected layer (figure 5.4).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力层的最终输出与全连接层类似（即使用矩阵乘法和激活函数）。典型的全连接层将处理输入序列中的所有元素，并分别处理它们，然后输出一个元素以替换每个输入元素。但自注意力层可以选择和组合输入序列中的不同元素以输出给定元素。这使得自注意力层比典型的全连接层更加强大（见图5.4）。
- en: '![05-04](../../OEBPS/Images/05-04.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![05-04](../../OEBPS/Images/05-04.png)'
- en: Figure 5.4 The difference between the self-attention sublayer and the fully
    connected sublayer. The self-attention sublayer looks at all the inputs in the
    sequence, whereas the fully connected sublayer only looks at the input that is
    processed.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4 自注意力子层和全连接子层之间的区别。自注意力子层查看序列中的所有输入，而全连接子层只查看正在处理的输入。
- en: Why does it pay to select and combine different input elements this way? In
    an NLP context, the self-attention layer enables the model to look at other words
    while it processes a certain word. But what does that mean for the model? This
    means that while the encoder is processing the word “it” in the sentence “I kicked
    the *ball* and *it* disappeared,” the model can attend to the word “ball.” By
    seeing both words “ball” and “it” at the same time (learning dependencies), disambiguating
    words is easier. Such capabilities are of paramount importance for language understanding.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么以这种方式选择和组合不同的输入元素有好处？在自然语言处理的上下文中，自注意力层使模型在处理某个单词时能够查看其他单词。但这对模型意味着什么？这意味着在编码器处理句子“I
    kicked the *ball* and *it* disappeared”中的单词“it”时，模型可以关注单词“ball”。通过同时看到“ball”和“it”两个单词（学习依赖关系），消除歧义的单词变得更容易。这样的能力对语言理解至关重要。
- en: 'We can understand how self-attention helps us solve a task conveniently through
    a real-world example. Assume you are playing a game with two people: person A
    and person B. Person A holds a question written on a board, and you need to speak
    the answer. Say person A reveals just one word at a time, and after the last word
    of the question, it is revealed that you are answering the question. For long
    and complex questions, this is challenging, as you cannot physically see the complete
    question and have to heavily rely on memory when answering the question. This
    is what it feels like without self-attention. On the other hand, say person B
    reveals the full question on the board instead of word by word. Now it is much
    easier to answer the question, as you can see the whole question at once. If the
    question is complex and requires a complex answer, you can look at different parts
    of the question as you provide various sections of the full answer. This is what
    the self-attention layer enables.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过一个现实世界的例子了解自注意力如何方便地帮助我们解决任务。假设你正在和两个人玩游戏：A和B。A手持写有问题的板子，你需要说出答案。假设A一次只透露一个单词，直到问题的最后一个单词被揭示，才揭示你正在回答问题。对于长而复杂的问题，这是具有挑战性的，因为你不能物理上看到完整的问题，必须严重依赖记忆来回答问题。这就是没有自注意力时的感觉。另一方面，假设B将整个问题一次性展示在板上，而不是逐字逐句地展示。现在回答问题要容易得多，因为你可以一次看到整个问题。如果问题很复杂，需要复杂的答案，你可以在提供完整答案的各个部分时查看问题的不同部分。这就是自注意力层的作用。
- en: Next, the fully connected layer takes the output elements produced by the self-attention
    sublayer and produces a hidden representation for each output element in an element-wise
    fashion. This make the model deeper, allowing it to perform better.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，全连接层以逐元素的方式接受自注意力子层产生的输出元素，并为每个输出元素生成一个隐藏表示。这使得模型更加深入，从而表现更好。
- en: 'Let’s look in more detail at how data flows through the model in order to better
    understand the organization of layers and sublayers. Assume the task of translating
    the sentence “Dogs are great” (English) to “*Les chiens sont super*” (French).
    First, the encoder takes in the full sentence “Dogs are great” and produces an
    output for each word in the sentence. The self-attention layer selects the most
    important words for each position, computes an output, and sends that information
    to the fully connected layer to produce a deeper representation. The decoder produces
    output words iteratively, one after the other. To do that, the decoder looks at
    the final output sequence of the encoder and all the previous words predicted
    by the decoder. Assume the final prediction is <SOS> *les chiens sont super* <EOS>.
    Here, <SOS> marks the start of the sentence and <EOS> the end of the sentence.
    The first input it takes is a special tag that indicates the start of a sentence
    (<SOS>), along with the encoder outputs, and it produces the next word in the
    translation: “*les*.” The decoder then consumes <SOS> and “*les*” as inputs, produces
    the word “*chiens*,” and continues until the model reaches the end of the translation
    (marked by <EOS>). Figure 5.5 depicts this process.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看一下数据如何通过模型流动，以更好地理解层和子层的组织。假设要将句子“Dogs are great”（英语）翻译成“*Les chiens
    sont super*”（法语）。首先，编码器接收完整的句子“Dogs are great”，并为句子中的每个单词生成一个输出。自注意力层选择每个位置的最重要的单词，计算一个输出，并将该信息发送到全连接层以产生更深层的表示。解码器迭代地生成输出单词，一个接着一个。为此，解码器查看编码器的最终输出序列以及解码器预测的所有先前单词。假设最终预测是
    <SOS> *les chiens sont super* <EOS>。这里，<SOS> 标记了句子的开始，<EOS> 标记了句子的结束。它接收的第一个输入是一个特殊标记，表示句子的开始（<SOS>），以及编码器的输出，并产生翻译中的下一个单词：“*les*”。然后解码器消耗
    <SOS> 和 “*les*” 作为输入，生成单词“*chiens*”，并继续直到模型达到翻译的末尾（由 <EOS> 标记）。图 5.5 描述了这个过程。
- en: In the original Transformer paper, the encoder has six layers, and a single
    layer has a self-attention sublayer and a fully connected sublayer, in that order.
    First, the self-attention layer takes the English words as a time-series input.
    However, before feeding these words to the encoder, you need to create a numerical
    representation of each word, as discussed earlier. In the paper, word embeddings
    (with some additional encoding) are used to represent the words. Each of these
    embeddings is a 512-long vector. Then the self-attention layer computes a hidden
    representation for each word of the input sentence. If we ignore some of the implementation
    details, this hidden representation at time step *t* can be thought as a weighted
    sum of all the inputs (in a single sequence), where the weight for position *i*
    of the input is determined by how important it is to select (or attend to) the
    encoder word *ew*[i] in the input sequence while processing the word *ew*[t] in
    the encoder input. The encoder makes this decision for every position *t* in the
    input sequence. For example, while processing the word “it” in the sentence “I
    kicked the *ball* and *it* disappeared,” the encoder needs to pay more attention
    to the word “ball” than to the word “the.” The weights in the self-attention sublayer
    are trained to demonstrate such properties. This way, the self-attention layer
    produces a hidden representation for each encoder input. We call this the *attended
    representation/output*.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始的 Transformer 论文中，编码器有六个层，并且一个单层有一个自注意力子层和一个全连接子层，按顺序排列。首先，自注意力层将英文单词作为时间序列输入。然而，在将这些单词馈送到编码器之前，您需要为每个单词创建一个数值表示，如前面所讨论的。在论文中，词嵌入（附加一些编码）用于表示这些单词。每个嵌入都是一个
    512 长的向量。然后自注意力层计算输入句子中每个单词的隐藏表示。如果我们忽略一些实现细节，这个时间步长 *t* 的隐藏表示可以被看作是所有输入的加权和（在一个单一序列中），其中输入位置
    *i* 的权重由在处理编码器输入中的单词 *ew*[t] 时选择（或关注）编码器单词 *ew*[i] 在输入序列中的重要性来确定。编码器在输入序列中的每个位置
    *t* 上都做出这样的决定。例如，在处理句子“我踢了 *球* 并且 *它* 消失了”中的单词“它”时，编码器需要更多地关注单词“球”而不是单词“the”。自注意力子层中的权重被训练以展示这样的属性。这样，自注意力层为每个编码器输入生成了一个隐藏表示。我们称之为
    *关注表示/输出*。
- en: The fully connected sublayer then takes over and is quite straightforward. It
    has two linear layers and a ReLU activation in between the layers. It takes the
    outputs of the self-attention layer and transforms to a hidden output using
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 全连接子层然后接管并且非常直观。它有两个线性层，并且在这两个层之间有一个 ReLU 激活函数。它接收自注意力层的输出，并将其转换为隐藏输出使用。
- en: '*h*[1] = *ReLU*(*xW*[1] + *b*[1])'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*h*[1] = *ReLU*(*xW*[1] + *b*[1])'
- en: '*h*[2] = *h*[1]*W*[2] + *b*[2]'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*h*[2] = *h*[1]*W*[2] + *b*[2]'
- en: 'Note that the second layer does not have a nonlinear activation. Next, the
    decoder has six layers as well, where each layer has three sublayers:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，第二层没有非线性激活函数。接下来，解码器也有六个层，每个层都有三个子层：
- en: A masked self-attention layer
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个掩码自注意力层
- en: An encoder-decoder attention layer
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个编码器-解码器注意力层
- en: A fully connected layer
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个全连接层
- en: The masked self-attention layer operates similarly to the self-attention layer.
    However, while processing the s^(th) word (i.e., *dw*[s]), it masks the words
    ahead of *dw*[s]. For example, while processing the word “*chiens*,” it can only
    attend to the words “<sos>” and “*les*.” This is important because the decoder
    must be able to predict the correct word, given only the previous words it predicted,
    so it makes sense to force the decoder to attend only to the words it has already
    seen.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 掩码自注意力层的操作方式与自注意力层类似。然而，在处理第 *s* 个单词（即*dw*[s]）时，它会屏蔽 *dw*[s] 之前的单词。例如，在处理单词“*chiens*”时，它只能关注单词“<sos>”和“*les*”。这很重要，因为解码器必须能够预测正确的单词，只给出它先前预测的单词，所以强制解码器只关注它已经看到的单词是有意义的。
- en: Next, the encoder-decoder attention layer takes the encoder output and the outputs
    produced by the masked self-attention layer and produce a series of outputs. The
    purpose of this layer is to compute a hidden representation (i.e., an attended
    representation) at time *s* as a weighted sum of encoder inputs, where the weight
    for position *j* is determined by how important it is to attend to encoder input
    *e w*[j], while processing the decoder word *dw*[s].
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，编码器-解码器注意力层获取编码器输出和掩码自注意力层产生的输出，并产生一系列输出。该层的目的是计算时间 *s* 处的隐藏表示（即一个受关注的表示），作为编码器输入的加权和，其中位置
    *j* 的权重由处理解码器单词 *dw*[s] 时关注编码器输入 *e w*[j]的重要性确定。
- en: Finally, a fully connected layer identical to the fully connected layer from
    the encoder layer takes the output of the self-attention layer to produce the
    final output of the layer. Figure 5.5 depicts the layers and operations discussed
    in this section at a high level.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，与编码器层相同的全连接层接收自注意力层的输出以生成层的最终输出。图5.5以高层次描述了本节讨论的层和操作。
- en: '![05-05](../../OEBPS/Images/05-05.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![05-05](../../OEBPS/Images/05-05.png)'
- en: Figure 5.5 Various layers in the encoder and the decoder and various connections
    formed within the encoder, within the decoder, and between the encoder and the
    decoder. The squares represent inputs and outputs of the models. The rectangular
    shaded boxes represent interim outputs of the sublayers.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5 编码器和解码器中的各个层以及编码器内部、解码器内部和编码器与解码器之间形成的各种连接。方框表示模型的输入和输出。长方形阴影框表示子层的临时输出。
- en: In the next section, we will discuss what the self-attention layer looks like.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论自注意力层的外观。
- en: 5.2.3 Self-attention layer
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.3 自注意力层
- en: We have covered the purpose of the self-attention layer at an abstract level
    of understanding. It is to, while processing the word *w*[t] at time step *t*,
    determine how important it is to attend to the *i*^(th) word (i.e., *w*[i]) in
    the input sequence. In other words, the layer needs to determine the importance
    of all the other words (indexed by *i*) for every word (indexed by *t*). Let’s
    now understand the computations involved in this process at a more granular level.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在抽象级别上介绍了自注意力层的目的。在处理时间步 *t* 的单词*w*[t]时，其目的是确定关注输入序列中的第 *i* 个单词（即*w*[i]）对理解当前单词有多重要。换句话说，该层需要确定对于每个单词（由
    *t* 索引）所有其他单词（由 *i* 索引）的重要性。现在让我们以更细粒度的方式理解涉及此过程的计算。
- en: 'First, there are three different entities involved in the computation:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，计算涉及三个不同的实体：
- en: '*A query*—The query’s purpose is to represent the word currently being processed.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*查询* — 查询的目的是表示当前正在处理的单词。'
- en: '*A key*—The key’s purpose is to represent the candidate words to be attended
    to while processing the current word.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*键* — 键的目的是表示在处理当前单词时要关注的候选单词。'
- en: '*A value*—The value’s purpose is to compute a weighted sum of all words in
    the sequence, where the weight for each word is based on how important it is for
    understanding the current word'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*值* — 值的目的是计算序列中所有单词的加权和，其中每个单词的权重基于它对理解当前单词的重要性。'
- en: For a given input sequence, query, key, and value need to be calculated for
    every position of the input. These are calculated by an associated weight matrix
    with each entity.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的输入序列，需要为输入的每个位置计算查询、键和值。这些是由与每个实体相关联的权重矩阵计算的。
- en: Note that this is an oversimplification of their relationship, and the actual
    relationship is somewhat complex and convoluted. But this understanding provides
    the motivation for why we need three different entities to compute self-attention
    outputs.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这是它们关系的简化，实际关系有些复杂和混乱。但这种理解为什么我们需要三个不同的实体来计算自注意力输出提供了动机。
- en: Next, we will understand how exactly a self-attention layer goes from an input
    sequence to a query, key, and value tensor and finally to the output sequence.
    The input word sequence is first converted to a numerical representation using
    word embedding lookup. Word embeddings are essentially a giant matrix, where there’s
    a vector of floats (i.e., an embedding vector) for each word in your vocabulary.
    Typically, these embeddings are several hundreds of elements long. For a given
    input sequence, we assume the input sequence is *n* elements long and each word
    vector is *d*[model] elements long. Then we have a *n* × *d*[model] matrix. In
    the original Transformer paper, word vectors are 512 elements long.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将了解自注意力层如何从输入序列到查询、键和值张量，最终到输出序列。首先，将输入的单词序列使用单词嵌入查找转换为数值表示。单词嵌入本质上是一个巨大的矩阵，其中词汇表中的每个单词都有一个浮点向量（即嵌入向量）。通常，这些嵌入是几百个元素长。对于给定的输入序列，我们假设输入序列的长度为
    *n* 元素，并且每个单词向量的长度为 *d*[model] 元素。然后我们有一个 *n* × *d*[model] 矩阵。在原始 Transformer
    论文中，单词向量长度为 512 个元素。
- en: 'There are three weight matrices in the self-attention layer: query weights
    (*W*[q]), key weights (*W*[k]), and value weights (*W*[v]), respectively used
    to compute the query, key, and value vectors. *W*[q] is *d*[model] × *d*[q], *W*[k]
    is *d*[model] × *d*[k], and *W*[v] is *d*[model] × *d*[v]. Let’s define these
    elements in TensorFlow assuming a dimensionality of 512, as in the original Transformer
    paper. That is,'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力层中有三个权重矩阵：查询权重（*W*[q]）、键权重（*W*[k]）和值权重（*W*[v]），分别用于计算查询、键和值向量。*W*[q] 是 *d*[model]
    × *d*[q]，*W*[k] 是 *d*[model] × *d*[k]，*W*[v] 是 *d*[model] × *d*[v]。让我们假设这些元素在
    TensorFlow 中的维度为 512，就像原始 Transformer 论文中一样。即，
- en: '*d*[model] = *d*[q] = *d*[k] = *d*[v] = 512'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*d*[model] = *d*[q] = *d*[k] = *d*[v] = 512'
- en: We will first define our input x as a tf.constant, which has three dimensions
    (batch, time, feature). Wq, Wk, and Wv are declared as tf.Variable objects, as
    these are the parameters of the self-attention layer
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将我们的输入 x 定义为一个 tf.constant，它有三个维度（批量、时间、特征）。Wq、Wk 和 Wv 声明为 tf.Variable 对象，因为这些是自注意力层的参数。
- en: '[PRE0]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: which has shapes
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 其形状为
- en: '[PRE1]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, *q*, *k*, and *v* are computed as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，*q*、*k* 和 *v* 计算如下：
- en: '*q* = *xW*[q]; shape transformation: *n* × *d*[model]. *d*[model] × *d*[q]
    = *n × d*[q]'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '*q* = *xW*[q]；形状变换：*n* × *d*[model]。*d*[model] × *d*[q] = *n × d*[q]'
- en: '*k* = *xW*[k]; shape transformation: *n* × *d*[model]. *d*[model] × *d*[k]
    = *n × d*[k]'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '*k* = *xW*[k]；形状变换：*n* × *d*[model]。*d*[model] × *d*[k] = *n × d*[k]'
- en: '*v* = *xW*[v]; shape transformation: *n* × *d*[model]. *d*[model] × *d*[v]
    = *n × d*[v]'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*v* = *xW*[v]；形状变换：*n* × *d*[model]。*d*[model] × *d*[v] = *n × d*[v]'
- en: 'It is evident that computing *q*, *k*, and *v* is a simple matrix multiplication
    away. Remember that there is a batch dimension in front of all the inputs (i.e.,
    x) and output tensors (i.e,. q, k, and v) as we process batches of data. But to
    avoid clutter, we are going to ignore the batch dimension. Then we compute the
    final output of the self-attention layer as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，计算 *q*、*k* 和 *v* 只是一个简单的矩阵乘法。请记住，所有输入（即 x）和输出张量（即 q、k 和 v）前面都有一个批处理维度，因为我们处理数据批次。但为了避免混乱，我们将忽略批处理维度。然后我们按以下方式计算自注意力层的最终输出：
- en: '![05_05a](../../OEBPS/Images/05_05a.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![05_05a](../../OEBPS/Images/05_05a.png)'
- en: Here, the component ![05_05b](../../OEBPS/Images/05_05b.png) (which will be
    referred to as *P*) is a probability matrix. This is all there is in the self-attention
    layer. Implementing self-attention with TensorFlow is very straightforward. As
    good data scientists, let’s create it as a reusable Keras layer, as shown in the
    next listing.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，组件 ![05_05b](../../OEBPS/Images/05_05b.png)（将被称为 *P*）是一个概率矩阵。这就是自注意力层的全部内容。使用
    TensorFlow 实现自注意力非常简单。作为优秀的数据科学家，让我们将其创建为可重复使用的 Keras 层，如下所示。
- en: Listing 5.1 The self-attention sublayer
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.1 自注意力子层
- en: '[PRE2]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Defining the output dimensionality of the self-attention outputs
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义自注意力输出的输出维度
- en: ❷ Defining the variables for computing the query, key, and value entities
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义计算查询、键和值实体的变量
- en: ❸ Computing the query, key, and value tensors
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 计算查询、键和值张量
- en: ❹ Computing the probability matrix
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 计算概率矩阵
- en: ❺ Computing the final output
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 计算最终输出
- en: 'Here’s a quick refresher:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个快速的复习：
- en: __init__(self, d)—Defines any hyperparameters of the layer
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: __init__(self, d)—定义层的任何超参数
- en: build(self, input_shape)—Creates the parameters of the layer as variables
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: build(self, input_shape)—创建层的参数作为变量
- en: call(self, v_x, k_x, q_x)—Defines the computations happening in the layer
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: call(self, v_x, k_x, q_x)—定义层中发生的计算
- en: 'If you look at the call(self, v_x, k_x, q_x) function, it takes in three inputs:
    one each for computing value, key, and query. In most cases these are the same
    input. However, there are instances where different inputs come into these computations
    (e.g., some computations in the decoder). Also, note that we return both h (i.e.,
    the final output) and p (i.e., the probability matrix). The probability matrix
    is an important visual aid, as it helps us understand when and where the model
    paid attention to words. If you want to get the output of the layer, you can do
    the following'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看一下 call(self, v_x, k_x, q_x) 函数，它接受三个输入：分别用于计算值、键和查询。在大多数情况下，这些输入是相同的。然而，也有一些情况下，不同的输入被用于这些计算（例如，解码器中的一些计算）。此外，请注意我们同时返回
    h（即最终输出）和 p（即概率矩阵）。概率矩阵是一个重要的视觉辅助工具，它帮助我们理解模型何时以及在哪里关注了单词。如果你想获取层的输出，可以执行以下操作
- en: '[PRE3]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: which will return
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 将返回
- en: '[PRE4]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Exercise 1
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 练习1
- en: Given the following input
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 给定以下输入
- en: '[PRE5]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: and assuming we need an output of size 512, write the code to create Wq, Wk,
    and Wv as tf.Variable objects. Use the np.random.normal() function to set the
    initial values.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 并假设我们需要一个大小为 512 的输出，编写代码创建 Wq、Wk 和 Wv 作为 tf.Variable 对象。使用 np.random.normal()
    函数设置初始值。
- en: 5.2.4 Understanding self-attention using scalars
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.4 使用标量理解自注意力
- en: 'It is not yet very clear why the computations are designed the way they are.
    To understand and visualize what this layer is doing, we will assume a feature
    dimensionality of 1\. That is, a single word is represented by a single value
    (i.e., a scalar). Figure 5.6 visualizes the computations that happen in the self-attention
    layer if we assume a single-input sequence and the dimensionality of inputs (*d*[model]),
    query length (*d*[q]), key length (*d*[k]), and value length (*d*[v]) is 1\. As
    a concrete example, we start with an input sequence *x*, which has seven words
    (i.e., *n* × 1 matrix). Under the assumptions we’ve made, *W*[q], *W*[k], and
    *W*[v] will be scalars. The matrix multiplications used for computing *q*, *k*,
    and *v* essentially become scalar multiplications:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 目前还不太清楚为什么设计了这样的计算方式。为了理解和可视化这个层正在做什么，我们将假设特征维度为 1\. 也就是说，一个单词由一个值（即标量）表示。图
    5.6 可视化了如果我们假设单一输入序列和输入的维度（*d*[model]）、查询长度（*d*[q]）、键长度（*d*[k]）和值长度（*d*[v]）的维度都为
    1\. 在我们所做的假设下，*W*[q]、*W*[k] 和 *W*[v] 将是标量。用于计算 *q*、*k* 和 *v* 的矩阵乘法本质上变成了标量乘法：
- en: '*q* = (*q*[1], *q*[2],..., *q*[7]), where *q*[i] = *x*[i] *W*[q]'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '*q* = (*q*[1], *q*[2],..., *q*[7])，其中 *q*[i] = *x*[i] *W*[q]'
- en: '*k* = (*k*[1], *k*[2],..., *k*[7]), where *k*[i] = *x*[i] *W*[k]'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '*k* = (*k*[1], *k*[2],..., *k*[7])，其中 *k*[i] = *x*[i] *W*[k]'
- en: '*v* = (*v*[1], *v*[2],..., *v*[7]), where *v*[i] = *x*[i] *W*[v]'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '*v* = (*v*[1], *v*[2],..., *v*[7])，其中 *v*[i] = *x*[i] *W*[v]'
- en: Next, we need to compute the *P* = softmax ((*Q.K*^T) / √(*d*[k])) component.
    *Q.K*^T is essentially an *n* × *n* matrix that has an item representing every
    query and key combination (figure 5.6). The *i* ^(th) row and *j* ^(th) column
    of *Q.K*[(i,j)]^T are computed as
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要计算 *P* = softmax ((*Q.K*^T) / √(*d*[k])) 组件。*Q.K*^T 本质上是一个 *n* × *n*
    的矩阵，它代表了每个查询和键组合的项（图 5.6）。*Q.K*[(i,j)]^T 的第 *i* 行和第 *j* 列是按如下计算的
- en: '*Q.K*[(i,j)]^T =*q* [i] × *k* [j]'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '*Q.K*[(i,j)]^T =*q* [i] × *k* [j]'
- en: Then, by applying the softmax, this matrix is converted to a row-wise probability
    distribution. You might have noted a constant √(*d*[k]) appearing within the softmax
    transformation. This is a normalization constant that helps prevent large gradient
    values and achieve stable gradients. In our example, you can ignore this as √(*d*[k])
    = 1.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，通过应用 softmax 函数，该矩阵被转换为行向量的概率分布。你可能已经注意到 softmax 转换中出现了一个常数 √(*d*[k])。这是一个归一化常数，有助于防止梯度值过大并实现稳定的梯度。在我们的示例中，你可以忽略这个因为
    √(*d*[k]) = 1。
- en: Finally, we compute the final output *h* = (*h*[1],*h*[2],...,*h*[7]), where
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们计算最终输出 *h* = (*h*[1],*h*[2],...,*h*[7])，其中
- en: '*h*[i] = *P*[(i],[1)] *v*[1] + *P*[(i],[2)] *v*[2] +...+ *P*[(i],[7)] *v*[7]'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '*h*[i] = *P*[(i],[1)] *v*[1] + *P*[(i],[2)] *v*[2] +...+ *P*[(i],[7)] *v*[7]'
- en: 'Here, we can more clearly see the relationship between *q*, *k*, and *v*. *q*
    and *k* are used to compute a soft-index mechanism for *v* when computing the
    final output. For example, when computing the fourth output (i.e., *h*[4]), we
    first hard-index the fourth row (following *q*[4]), and then mix various *v* values
    based on the soft index (i.e., probabilities) given by the columns (i.e., *k*
    values) of that row. Now it is more clear what purpose *q*, *k*, and *v* serve:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以更清楚地看到 *q*、*k* 和 *v* 之间的关系。当计算最终输出时，*q* 和 *k* 被用于计算 *v* 的软索引机制。例如，当计算第四个输出（即
    *h*[4]）时，我们首先对第四行进行硬索引（跟随 *q*[4]），然后根据该行的列（即 *k* 值）给出的软索引（即概率），混合各种 *v* 值。现在更清楚
    *q*、*k* 和 *v* 的作用是什么了：
- en: '*Query*—Helps build a probability matrix that is eventually used for indexing
    values (v). Query affects the rows of the matrix and represents the index of the
    current word that’s being processed.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*查询*—帮助构建最终用于索引值（v）的概率矩阵。查询影响矩阵的行，并表示正在处理的当前单词的索引。'
- en: '*Key*—Helps build a probability matrix that is eventually used for indexing
    values (v). Key affects the columns of the matrix and represents the candidate
    words that need to be mixed depending on the query word.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*键*—帮助构建最终用于索引值（v）的概率矩阵。键影响矩阵的列，并表示根据查询单词需要混合的候选单词。'
- en: '*Value*—Hidden (i.e., attended) representation of the inputs used to compute
    the final output by indexing using the probability matrix created using query
    and key'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*值*—通过使用查询和键创建的概率矩阵进行索引，用于计算最终输出的隐藏（即关注）表示。'
- en: You can easily take the big gray box in figure 5.6, place it over the self-attention
    sublayer, and still have the output shape (as shown in figure 5.5) being produced
    (figure 5.7).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以轻松地将图 5.6 中的大灰色框放置在自注意子层上，并仍然产生输出形状（如图 5.5 中所示）（图 5.7）。
- en: '![05-06](../../OEBPS/Images/05-06.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![05-06](../../OEBPS/Images/05-06.png)'
- en: Figure 5.6 The computations in the self-attention layer. The self-attention
    layer starts with an input sequence and computes sequences of query, key, and
    value vectors. Then the queries and keys are converted to a probability matrix,
    which is used to compute the weighted sum of values.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6 自注意层中的计算。自注意层从输入序列开始，并计算查询、键和值向量的序列。然后将查询和键转换为概率矩阵，该矩阵用于计算值的加权和。
- en: '![05-07](../../OEBPS/Images/05-07.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![05-07](../../OEBPS/Images/05-07.png)'
- en: Figure 5.7 (top) and figure 5.6 (bottom). You can take the gray box from the
    bottom and plug it into a self-attention sublayer on the top and see that the
    same output sequence is being produced.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7（顶部）和图 5.6（底部）。您可以从底部获取灰色框，并将其插入到顶部的自注意子层中，然后可以看到产生相同的输出序列。
- en: 'Now let’s scale up our self-attention layer and revisit the specific computations
    behind it and why they matter. Going back to our previous notation, we start with
    a sequence of words, which has *n* elements. Then, after the embedding lookup,
    which retrieves an embedding vector for each word, we have a matrix of size *n*
    × *d*[model]. Next, we have the weights and biases to compute each of the query,
    key, and value vectors:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们扩展我们的自注意层，并重新审视其背后的具体计算及其重要性。回到我们先前的表示法，我们从一个具有 *n* 个元素的单词序列开始。然后，在嵌入查找之后，为每个单词检索一个嵌入向量，我们有一个大小为
    *n* × *d*[model] 的矩阵。接下来，我们有权重和偏差来计算每个查询、键和值向量：
- en: '*q* = *xW*[q], where *x* ∈ ℝ^(n×dmodel). *W*[q] ∈ ℝ^(dmodel×)*dq* and *q* ∈
    ℝ^(n×d)*q*'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '*q* = *xW*[q]，其中 *x* ∈ ℝ^(n×dmodel)。*W*[q] ∈ ℝ^(dmodel×*dq*)，而 *q* ∈ ℝ^(n×d)*q*'
- en: '*k* = *xW*[k], where *x* ∈ ℝ^(n×dmodel). *W*[k] ∈ ℝ^(dmodel×)*dk* and *k* ∈
    ℝ^(n×d)*k*'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '*k* = *xW*[k]，其中 *x* ∈ ℝ^(n×dmodel)。*W*[k] ∈ ℝ^(dmodel×*dk*)，而 *k* ∈ ℝ^(n×d)*k*'
- en: '*v* = *xW*[v], where *x* ∈ ℝ^(n×dmodel). *W*[v] ∈ ℝ^(dmodel×)*dv* and *v* ∈
    ℝ^(n×d)*v*'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '*v* = *xW*[v]，其中 *x* ∈ ℝ^(n×dmodel)。*W*[v] ∈ ℝ^(dmodel×*dv*)，而 *v* ∈ ℝ^(n×d)*v*'
- en: For example, the query, or *q*, is a vector of size n × *d*[q], obtained by
    multiplying the input *x* of size *n* × *d*[model] with the weight matrix *W*[q]
    of size *d*[model] × *d*[q]. Also remember that, as in the original Transformer
    paper, we make sure that all of the input embedding of query, key, and value vectors
    are the same size. In other words,
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，查询，或 *q*，是一个大小为 n × *d*[q] 的向量，通过将大小为 *n* × *d*[model] 的输入 *x* 与大小为 *d*[model]
    × *d*[q] 的权重矩阵 *W*[q] 相乘获得。还要记住，正如在原始 Transformer 论文中一样，我们确保查询、键和值向量的所有输入嵌入大小相同。换句话说，
- en: '*d*[model] = *d*[q] = *d*[k] = *d*[v] = 512'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '*d*[model] = *d*[q] = *d*[k] = *d*[v] = 512'
- en: 'Next, we compute the probability matrix using the q and k values we obtained:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用我们获得的q和k值计算概率矩阵：
- en: '![05_06a](../../OEBPS/Images/05_06a.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![05_06a](../../OEBPS/Images/05_06a.png)'
- en: 'Finally, we multiply this probably matrix with our value matrix to obtain the
    final output of the self-attention layer:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将这个概率矩阵与我们的值矩阵相乘，以获得自注意力层的最终输出：
- en: '![05_06b](../../OEBPS/Images/05_06b.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![05_06b](../../OEBPS/Images/05_06b.png)'
- en: The self-attention layer takes a batch of a sequence of words (e.g., a batch
    of sentences of fixed length), where each word is represented by a vector, and
    produces a batch of a sequence of hidden outputs, where each hidden output is
    a vector.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力层接受一批词序列（例如，一批具有固定长度的句子），其中每个词由一个向量表示，并产生一批隐藏输出序列，其中每个隐藏输出是一个向量。
- en: How does self-attention compare to recurrent neural networks (RNNs)?
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力与循环神经网络（RNNs）相比如何？
- en: Before Transformer models, RNNs governed the domain of NLP. RNNs were popular
    for NLP problems because most are inherently time-series problems. You can think
    of a sentence/phrase as a sequence of words (i.e., each represented by a feature
    vector) spread across time. The RNN goes through this sequence, consuming one
    word at a time (while maintaining a memory/state vector), and produces some output
    (or a series of outputs) at the end. But you will see that RNNs perform more and
    more poorly as the length of the sequence increases. This is because by the time
    the RNN gets to the end of the sequence, it has probably forgotten what it saw
    at the start.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在Transformer模型出现之前，RNNs主导了自然语言处理的领域。RNNs在NLP问题中很受欢迎，因为大多数问题本质上都是时间序列问题。你可以将句子/短语视为一系列单词（即每个单词由一个特征向量表示）在时间上的分布。RNN通过这个序列，一次消耗一个单词（同时保持一个记忆/状态向量），并在最后产生一些输出（或一系列输出）。但是你会发现，随着序列长度的增加，RNN的表现越来越差。这是因为当RNN到达序列末尾时，它可能已经忘记了开始时看到的内容。
- en: You can see that this problem is alleviated by the self-attention mechanism,
    which allows the model to look at the full sequence at a given time. This enables
    Transformer models to perform much better than RNN-based models.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，自注意力机制缓解了这个问题，它允许模型在给定时间内查看完整的序列。这使得Transformer模型比基于RNN的模型表现得好得多。
- en: 5.2.5 Self-attention as a cooking competition
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.5 自注意力作为烹饪比赛
- en: The concept of self-attention might still be a little bit elusive, making it
    difficult to understand what exactly is transpiring in the self-attention sublayer.
    The following analogy might alleviate the burden and make it easier. Say you are
    taking part in a cooking show with six other contestants (seven contestants in
    total). The game is as follows.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力的概念可能仍然有点难以捉摸，这使得理解自注意力子层中究竟发生了什么变得困难。以下类比可能会减轻负担并使其更容易理解。假设你参加了一个与其他六位选手（总共七位选手）一起的烹饪节目。游戏如下。
- en: You are at a supermarket and are given a T-shirt with a number on it (from 1-7)
    and a trolley. The supermarket has seven aisles. You have to sprint to the aisle
    with the number on your T-shirt, and there will be a name of some beverage (e.g.,
    apple juice, orange juice, lime juice) posted on the wall. You need to pick what’s
    necessary to make that beverage, sprint to your allocated table, and make that
    beverage.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 你在超市里拿到一件印有号码（从1到7）的T恤和一个手推车。超市有七个过道。你必须飞奔到印有T恤上号码的过道，墙上会贴着某种饮料的名称（例如，苹果汁，橙汁，酸橙汁）。你需要挑选制作该饮料所需的物品，然后飞奔到你分配的桌子上，制作那种饮料。
- en: Say that you are number 4 and got orange juice, so you’ll make your way to aisle
    4 and collect oranges, a bit of salt, a lime, sugar, and so on. Now say the opponent
    next to you (number 3), had to make lime juice; they will pick limes, sugar, and
    salt. As you can see, you are picking different items as well as different quantities
    of the same item. For example, your opponent hasn’t picked oranges, but you have,
    and you probably picked fewer limes compared to your opponent who is making lime
    juice.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你是号码4并且拿到了橙汁，所以你会前往4号过道并收集橙子，一点盐，一颗酸橙，糖等等。现在假设你旁边的对手（编号3）要制作酸橙汁；他们会挑选酸橙，糖和盐。正如你所看到的，你们选取了不同的物品以及相同物品的不同数量。例如，你的对手没有选择橙子，但你选择了，并且你可能选择了较少的酸橙，与你正在制作酸橙汁的对手相比。
- en: This is quite similar to what’s happening in the self-attention layer. You and
    your contestants are the inputs (at a single time step) to the model. The aisles
    are the queries, and the grocery items you have to pick are the keys. Just like
    indexing the probability matrix with query and keys to get the “mixing coefficients”
    (i.e., attention weights) for the values, you index the items you need by the
    aisle number allocated to you (i.e., query) and the quantity of each item in the
    aisle (i.e., key). Finally, the beverage you make is the value. Note that this
    analogy does not have 100% correspondence to the computations in the self-attention
    sublayer. However, you can draw significant similarities between the two processes
    at an abstract level. The similarities we discovered are shown in figure 5.8.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这与自注意力层中发生的情况非常相似。你和你的竞争对手是模型的输入（在一个时间步上）。通道是查询，你需要选择的货品是键。就像通过查询和键来索引概率矩阵以获得“混合系数”（即注意力权重）来获取值一样，你可以通过分配给你的通道号（即查询）和通道中每个货品的数量（即键）来索引你所需要的货品。最后，你制作的饮料就是值。请注意，这个类比并不完全对应于自注意力子层中的计算。然而，你可以在抽象层面上发现这两个过程之间的显著相似之处。我们发现的相似之处如图5.8所示。
- en: '![05-08](../../OEBPS/Images/05-08.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![05-08](../../OEBPS/Images/05-08.png)'
- en: Figure 5.8 Self-attention depicted with the help of a cooking competition. The
    contestants are the queries, the keys are the grocery items you have to choose
    from, and the values are the final beverage you’re making.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8 以烹饪比赛为背景描述的自注意力。选手是查询，货品是你需要选择的食材，值是你制作的最终饮料。
- en: Next we will discuss what is meant by a masked self-attention layer.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将讨论什么是蒙版自注意力层。
- en: 5.2.6 Masked self-attention layers
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.6 蒙版自注意力层
- en: As you have already seen, the decoder has a special additional self-attention
    sublayer called *masked self-attention*. As we have already stated, the idea is
    to prevent the model from “cheating” by attending to the words it shouldn’t (i.e.,
    the words ahead of the position the model has predicted for). To understand this
    better, assume two people are teaching a student to translate from English to
    French. The first person gives an English sentence, asks the student to produce
    the translation word by word, and provides feedback up to the word translated
    so far. The second person gives an English sentence and asks the student to produce
    the translation but provides the full translation in advance. In the second instance,
    it is much easier for a student to cheat, providing a good quality translation,
    while having very little knowledge of the languages. Now let’s understand the
    looming danger of attending to the words it shouldn’t from a machine learning
    point of view.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你已经看到的，解码器有一个特殊的额外的自注意子层，称为*蒙版自注意力*。正如我们已经提到的，这个想法是防止模型通过关注不应关注的单词（也就是模型预测位置之前的单词）来“作弊”。为了更好地理解这一点，假设有两个人在教一个学生从英语翻译成法语。第一个人给出一个英语句子，要求学生逐词翻译，同时给出到目前为止已经翻译的反馈。第二个人给出一个英语句子，要求学生翻译，但提前提供完整的翻译。在第二种情况下，学生很容易作弊，提供高质量的翻译，虽然对语言几乎一无所知。现在让我们从机器学习的角度来理解关注不应关注的单词的潜在危险。
- en: Take the task of translating the sentence “dogs are great” to “*les chiens sont
    super*.” When processing the sentence “Dogs are great,” the model should be able
    to attend to any word in that sentence, as that’s an input fully available to
    the model at any given time. But, while processing the sentence “*Les chiens sont
    super*,” we need to be careful about what we show to the model and what we don’t.
    For example, while training the model, we typically feed the full output sequence
    at once, as opposed to iteratively feeding the words, to enhance computational
    efficiency. When feeding the full output sequence to the decoder, we must mask
    all words ahead of what is currently being processed because it is not fair for
    the model to predict the word “*chiens*” when it can see everything that comes
    after that word. It is imperative you do this. If you don’t, the code will run
    fine. But ultimately you will have very poor performance when you bring it to
    the real world. The way to force this is by making the probability matrix p a
    lower-triangular matrix. This will essentially give zero probability for mixing
    any input ahead of itself during the attention/output computation. The differences
    between standard self-attention and masked self-attention are shown in figure
    5.9.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们要将句子 “dogs are great” 翻译为 “*les chiens sont super*。” 当处理句子 “Dogs are great”
    时，模型应该能够关注该句子中的任何单词，因为这是模型在任何给定时间完全可用的输入。但是，在处理句子 “*Les chiens sont super*” 时，我们需要注意向模型展示什么和不展示什么。例如，在训练模型时，我们通常一次性提供完整的输出序列，而不是逐字节地提供单词，以增强计算效率。在向解码器提供完整输出序列时，我们必须屏蔽当前正在处理的单词之前的所有单词，因为让模型在看到该单词之后的所有内容后预测单词
    “*chiens*” 是不公平的。这是必须做的。如果不这样做，代码会正常运行。但最终，当你将其带到现实世界时，性能会非常差。强制执行这一点的方法是将概率矩阵
    p 设为下三角矩阵。这将在注意力/输出计算期间基本上为混合输入的任何内容赋予零概率。标准自注意力和蒙版自注意力之间的差异如图 5.9 所示。
- en: '![05-09](../../OEBPS/Images/05-09.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![05-09](../../OEBPS/Images/05-09.png)'
- en: Figure 5.9 Standard self-attention versus masked self-attention methods. In
    the standard attention method, a given step can see an input from any other timestep,
    regardless of whether those inputs appear before or after the current time step.
    However, in the masked self-attention method, the current timestep can only see
    the current input and what came before that time step.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9 标准自注意力与蒙版自注意力方法。在标准注意力方法中，给定步骤可以看到来自当前时间步之前或之后的任何其他时间步的输入。然而，在蒙版自注意力方法中，当前时间步只能看到当前输入和之前的时间步。
- en: Let’s learn how we can do this in TensorFlow. We do a very simple change to
    the call() function by introducing a new argument, mask, which represents the
    items the model shouldn’t see with a 1 and the rest with a 0\. Then, to those
    elements the model shouldn’t see, we add a very large negative number (i.e., -
    10⁹) so that when softmax is applied they become zeros (listing 5.2).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们学习如何在 TensorFlow 中实现这一点。我们对 call() 函数进行了非常简单的更改，引入了一个新参数 mask，该参数表示模型不应该看到的项目，用
    1 表示，其余项目用 0 表示。然后，对于模型不应该看到的元素，我们添加一个非常大的负数（即 - 10⁹），以便在应用 softmax 时它们变成零（见清单
    5.2）。
- en: Listing 5.2 Masked self-attention sublayer
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 5.2 蒙版自注意力子层
- en: '[PRE6]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ The call function takes an additional mask argument (i.e., a matrix of 0s
    and 1s).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ call 函数接受额外的蒙版参数（即 0 和 1 的矩阵）。
- en: ❷ Now, the SelfAttentionLayer supports both masked and unmasked inputs.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 现在，SelfAttentionLayer 支持蒙版和非蒙版输入。
- en: ❸ If the mask is provided, add a large negative value to make the final probabilities
    zero for the words not to be seen.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 如果提供了蒙版，添加一个大的负值以使最终概率为零，以防止看到的单词。
- en: Creating the mask is easy; you can use the tf.linalg.band_part() function to
    create triangular matrices
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 创建蒙版很容易；您可以使用 tf.linalg.band_part() 函数创建三角矩阵
- en: '[PRE7]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: which gives
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 给出
- en: '[PRE8]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We can easily verify if the masking worked by looking at the probability matrix
    p. It must be a lower triangular matrix
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过查看概率矩阵 p 来轻松验证屏蔽是否起作用。它必须是一个下三角矩阵
- en: '[PRE9]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: which gives
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 给出
- en: '[PRE10]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now, when computing the value, the model cannot see or attend the words it hasn’t
    seen by the time it comes to the current word.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在计算值时，模型无法看到或关注到它在处理当前单词时尚未看到的单词。
- en: 5.2.7 Multi-head attention
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.7 多头注意力
- en: The original Transformer paper discusses something called multi-head attention,
    which is an extension of the self-attention layer. The idea is simple once you
    understand the self-attention mechanism. The multi-head attention creates multiple
    parallel self-attention heads. The motivation for this is that, practically, when
    the model is given the opportunity to learn multiple attention patterns (i.e.,
    multiple sets of weights) for an input sequence, it performs better.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 原始Transformer论文中讨论了一种称为多头注意力的方法，它是自注意力层的扩展。一旦理解了自注意机制，这个想法就很简单。多头注意力创建多个并行的自注意力头。这样做的动机是，当模型有机会为输入序列学习多个注意力模式（即多组权重）时，它的性能更好。
- en: Remember that in a single attention head we had all query, key, and value dimensionality
    set to 512\. In other words,
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在单个注意力头中，所有的查询、键和值的维度都设置为512。换句话说，
- en: '*d*[q] = *d*[k] = *d*[v] = 512'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '*d*[q] = *d*[k] = *d*[v] = 512'
- en: With multi-head attention, assuming we are using eight attention heads,
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多头注意力，假设我们使用八个注意力头，
- en: '*d*[q] = *d*[k] = *d*[v] = 512/8 = 64'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '*d*[q] = *d*[k] = *d*[v] = 512/8 = 64'
- en: Then the final outputs of all attention heads are concatenated to create the
    final output, which will have a dimensionality of 64 × 8 = 512
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将所有注意力头的最终输出连接起来，形成最终输出，它的维度将为64 × 8 = 512
- en: '*H* = *Concat* (*h*¹, *h*², ... , *h*⁸)'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '*H* = *Concat* (*h*¹, *h*², ... , *h*⁸)'
- en: where *h*^i is the output of the *i*^(th) attention head. Using the SelfAttentionLayer
    we just implemented, the code becomes
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*h*^i是第*i*个注意力头的输出。使用刚刚实现的SelfAttentionLayer，代码变为
- en: '[PRE11]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: which gives
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 得到
- en: '[PRE12]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As you can see, it still has the same shape as before (without multiple heads).
    However, this output is computed using multiple heads, which have smaller dimensionality
    than the original self-attention layer.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，它仍然具有之前的相同形状（没有多个头）。然而，此输出是使用多个头进行计算的，这些头的维度比原始的自注意层要小。
- en: 5.2.8 Fully connected layer
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.8 全连接层
- en: The fully connected layer is a piece of cake compared to what we just learned.
    So far, the self-attention layer has produced a *n* × *d*[v]-sized output (ignoring
    the batch dimension). The fully connected layer takes this input and performs
    the following transformation
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们刚刚学习的内容相比，全连接层更加简单。到目前为止，自注意力层产生了一个*n*×*d*[v]大小的输出（忽略批处理维度）。全连接层将输入数据进行以下转换
- en: '*h*[1] = *ReLU*(*xW*[1] + *b*[1])'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '*h*[1] = *ReLU*(*xW*[1] + *b*[1])'
- en: where *W*[1] is a *d*[v] × *d*[ff1] matrix and *b*[1] is a *d*[ff1]-sized vector.
    Therefore, this operation gives out a *n*×*d*[ff1]-sized tensor. The resulting
    output is passed onto another layer, which does the following computation
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*W*[1]是一个*d*[v] × *d*[ff1]的矩阵，*b*[1]是一个*d*[ff1]大小的向量。因此，这个操作产生一个*n*×*d*[ff1]大小的张量。结果输出传递到另一层，进行以下计算
- en: '*h*[2] = *h*[1] *W*[2] + *b* [2]'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '*h*[2] = *h*[1] *W*[2] + *b* [2]'
- en: where *W*[2] is a *d*[ff1] × *d*[ff2]-sized matrix and *b*[2] is a *d*[ff2]-sized
    vector. This operation gives a tensor of size *n* × *d*[ff2]. In TensorFlow parlance,
    we can again encapsulate these computations as a reusable Keras layer (see the
    next listing).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这里*W*[2]是一个*d*[ff1] × *d*[ff2]大小的矩阵，*b*[2]是一个*d*[ff2]大小的向量。该操作得到一个大小为*n*×*d*[ff2]的张量。在TensorFlow中，我们可以将这些计算再次封装成一个可重用的Keras层（见下一个列表）。
- en: Listing 5.3 The fully connected sublayer
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.3 全连接子层
- en: '[PRE13]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ The output dimensionality of the first fully connected computation
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 第一个全连接计算的输出维度
- en: ❷ The output dimensionality of the second fully connected computation
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 第二个全连接计算的输出维度
- en: ❸ Defining W1, b1, W2, and b2 accordingly. We use glorot_uniform as the initializer.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 分别定义W1、b1、W2和b2。我们使用glorot_uniform作为初始化器。
- en: ❹ Computing the first fully connected computation
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 计算第一个全连接计算
- en: ❺ Computing the second fully connected computation
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 计算第二个全连接计算
- en: Here, you could use the tensorflow.keras.layers.Dense() layer to implement this
    functionality. However, we will do it with raw TensorFlow operations as an exercise
    to familiarize ourselves with low-level TensorFlow. In this setup, we will change
    the FCLayer, as shown in the following listing.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以使用tensorflow.keras.layers.Dense()层来实现此功能。然而，我们将使用原始的TensorFlow操作进行练习，以熟悉低级TensorFlow。在这个设置中，我们将改变FCLayer，如下面的列表所示。
- en: Listing 5.4 The fully connected layer implemented using Keras Dense layers
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.4 使用Keras Dense层实现的全连接层
- en: '[PRE14]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Defining the first Dense layer in the __init__ function of the subclassed
    layer
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在子类化层的__init__函数中定义第一个全连接层
- en: ❷ Defining the second Dense layer. Note how we are not specifying an activation
    function.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Calling the first dense layer to get the output
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Calling the second dense layer with the output of the first Dense layer to
    get the final output
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Now you know what computations take place in the Transformer architecture and
    how to implement them with TensorFlow. But keep in mind that there are various
    fine-grained details explained in the original Transformer paper, which we haven’t
    discussed. Most of these details will be discussed in a later chapter.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 2
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Say you have been asked to experiment with a new type of multi-head attention
    mechanism. Instead of concatenating outputs from smaller heads (of size 64), the
    outputs (of size 512) are summed. Write TensorFlow code using the SelfAttentionLayer
    to achieve this effect. You can use the tf.math.add_n() function to sum a list
    of tensors element-wise.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.9 Putting everything together
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s bring all these elements together to create a Transformer network. Let’s
    first create an encoder layer, which contains a set of SelfAttentionLayer objects
    (one for each head) and a FCLayer (see the next listing).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.5 The encoder layer
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Create multiple attention heads. Each attention head has d/n_heads-sized feature
    dimensionality.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Create the fully connected layer, where the intermediate layer has 2,048 nodes
    and the final sublayer has d nodes.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Create a function that computes the multi-head attention output given an input.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Compute multi-head attention using the defined function.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Get the final output of the layer.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 'The EncoderLayer takes in two parameters during initialization: d (dimensionality
    of the output) and n_heads (number of attention heads). Then, when calling the
    layer, a single input x is passed. First, the attended output of the attention
    heads (SelfAttentionLayer) is computed, followed by the output of the fully connected
    layer (FCLayer). This wraps the crux of an encoder layer. Next, we create a Decoder
    layer (see the next listing).'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.6 The DecoderLayer
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Create the attention heads that process the decoder input only.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Create the attention heads that process both the encoder output and decoder
    input.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: ❸ The final fully connected sublayer
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: ❹ The function that computes the multi-head attention. This function takes three
    inputs (decoder’s previous output, encoder output, and an optional mask).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Each head takes the first argument of the function as the query and key and
    the second argument of the function as the value.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Compute the first attended output. This only looks at the decoder inputs.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Compute the second attended output. This looks at both the previous decoder
    output and the encoder output.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Compute the final output of the layer by feeding the output through a fully
    connected sublayer.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: The decoder layer has several differences compared to the encoder layer. It
    contains two multi-head attention layers (one masked and one unmasked) and a fully
    connected layer. First, the output of the first multi-head attention layer (masked)
    is computed. Remember that we are masking any decoder input that is ahead of the
    current decoder input that’s been processed. We use the decoder inputs to compute
    the output of the first attention layer. However, the computations happening in
    the second layer are a bit tricky. Brace yourselves! The second attention layer
    takes the encoder network’s last attended output as query and key; then, to compute
    the value, the output of the first attention layer is used. Think of this layer
    as a mixer that mixes attended encoder outputs and attended decoder inputs.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器层与编码器层相比有几个不同之处。它包含两个多头注意力层（一个被屏蔽，一个未被屏蔽）和一个全连接层。首先计算第一个多头注意力层（被屏蔽）的输出。请记住，我们会屏蔽任何超出当前已处理的解码器输入的解码器输入。我们使用解码器输入来计算第一个注意力层的输出。然而，第二层中发生的计算有点棘手。做好准备！第二个注意力层将编码器网络的最后一个被关注的输出作为查询和键；然后，为了计算值，使用第一个注意力层的输出。将这一层看作是一个混合器，它混合了被关注的编码器输出和被关注的解码器输入。
- en: With that, we can create a simple Transformer model with two encoder layers
    and two decoder layers). We’ll use the Keras functional API (see the next listing).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个，我们可以用两个编码器层和两个解码器层创建一个简单的 Transformer 模型）。我们将使用 Keras 函数式 API（见下一个列表）。
- en: Listing 5.7 The full Transformer model
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.7 完整的 Transformer 模型
- en: '[PRE17]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ The hyperparameters of the Transformer model
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ Transformer 模型的超参数
- en: ❷ The mask that will be used to mask decoder inputs
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 用于屏蔽解码器输入的掩码
- en: ❸ The encoder’s input layer. It accepts a batch of a sequence of word IDs.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 编码器的输入层。它接受一个批量的单词 ID 序列。
- en: ❹ The embedding layer that will look up the word ID and return an embedding
    vector for that ID
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 嵌入层将查找单词 ID 并返回该 ID 的嵌入向量。
- en: ❺ Compute the output of the first encoder layer.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 计算第一个编码器层的输出。
- en: ❻ The decoder’s input layer. It accepts a batch of a sequence of word IDs.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 解码器的输入层。它接受一个批量的单词 ID 序列。
- en: ❼ The decoder’s embedding layer
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 解码器的嵌入层
- en: ❽ Compute the output of the first decoder layer.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 计算第一个解码器层的输出。
- en: ❾ The final prediction layer that predicts the correct output sequence
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 预测正确输出序列的最终预测层
- en: ❿ Defining the model. Note how we are providing a name for the model.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 定义模型。注意我们为模型提供了一个名称。
- en: Before diving into the details, let’s refresh our memory with what the Transformer
    architecture looks like (figure 5.10).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入细节之前，让我们回顾一下 Transformer 架构的外观（图 5.10）。
- en: '![05-10](../../OEBPS/Images/05-10.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![05-10](../../OEBPS/Images/05-10.png)'
- en: Figure 5.10 The Transformer model architecture
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10 Transformer 模型架构
- en: Since we have explored the underpinning elements quite intensively, the network
    should be very easy to follow. All we have to do is set up the encoder model,
    set up the decoder model, and combine these appropriately by creating a Model
    object. Initially we define several hyperparameters. Our model takes n_steps-long
    sentences. This means that if a given sentence is shorter than n_steps, we will
    pad a special token to make it n_steps long. If a given sentence is longer than
    n_steps, we will truncate the sentence up to n_steps words. The larger the n_steps
    value, the more information you retain in the sentences, but also the more memory
    your model will consume. Next, we have the vocabulary size of the encoder inputs
    (i.e., the number of unique words in the data set fed to the encoder) (n_en_vocab),
    the vocabulary size of the decoder inputs (n_de_vocab), the number of heads (n_heads),
    and the output dimensionality (d).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经相当深入地探讨了底层元素，因此网络应该非常易于理解。我们所要做的就是设置编码器模型，设置解码器模型，并通过创建一个 Model 对象来适当地组合这些内容。最初我们定义了几个超参数。我们的模型接受长度为
    n_steps 的句子。这意味着如果给定句子的长度小于 n_steps，则我们将填充一个特殊的标记使其长度为 n_steps。如果给定句子的长度大于 n_steps，则我们将截断句子至
    n_steps 个词。n_steps 值越大，句子中保留的信息就越多，但模型消耗的内存也越多。接下来，我们有编码器输入的词汇表大小（即，馈送给编码器的数据集中唯一单词的数量）（n_en_vocab）、解码器输入的词汇表大小（n_de_vocab）、头数（n_heads）和输出维度（d）。
- en: 'With that we have defined the encoder input layer, which takes a batch of n_steps-long
    sentences. In these sentences, each word will be represented by a unique ID. For
    example, the sentence “The cat sat on the mat” will be converted to [1, 2, 3,
    4, 1, 5]. Next, we have a special layer called Embedding, which provides a d elements-long
    representation for each word (i.e., word vectors). After this transformation,
    you have a (batch size, n_steps, d)-sized output, which is the format of the output
    that should go into the self-attention layer. We discussed this transformation
    briefly in chapter 3 (section 3.4.3). The Embedding layer is essentially a lookup
    table. Given a unique ID (each ID represents a word), it gives out a vector that
    is d elements long. In other words, this layer encapsulates a large matrix of
    size (vocabulary size, d). You can see that when defining the Embedding layer:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We need to provide the vocabulary size (the first argument) and the output dimensionality
    (the second argument), and finally, since we are processing an input sequence
    of length n_steps, we need to specify the input_length argument. With that, we
    can pass the output of the embedding layer (en_emb) to an Encoder layer. You can
    see that we have two encoder layers in our model.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, moving on to the decoder, everything at a high level looks identical
    to the encoder, except for two differences:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: The Decoder layer takes both the encoder output (en_out2) and the decoder input
    (de_emb or de_out1) as inputs.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Decoder layer also has a final Dense layer that produces the correct output
    sequence (e.g., in a machine translation task, these would be the translated word
    probabilities for each time step).
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can now define and compile the model as
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Note that we can provide a name for our model when defining it. We will name
    our model “MinTransformer.” As the final step, let’s look at the model summary,
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'which will provide the following output:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The workshop participants are going to walk out of this workshop a happy bunch.
    You have covered the essentials of Transformer networks while teaching the participants
    to implement their own. We first explained that the Transformer has an encoder-decoder
    architecture. We then looked at the composition of the encoder and the decoder,
    which are made of self-attention layers and fully connected layers. The self-attention
    layer allows the model to attend to other input words while processing a given
    input word, which is important when processing natural language. We also saw that,
    in practice, the model uses multiple attention heads in a single attention layer
    to improve performance. Next, the fully connected layer creates a nonlinear representation
    of the attended output. After understanding the basic elements, we implemented
    a basic small-scale Transformer network using reusable custom layers we created
    for the self-attention (SelfAttentionLayer) and fully connected layer (FCLayer).
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to train this model on an NLP data set (e.g., machine translation).
    However, training these models is a topic for a separate chapter. There’s a lot
    more to Transformers than what we have discussed. For example, there are pretrained
    transformer-based models that you can use readily to solve NLP tasks. We will
    revisit Transformers again in a later chapter.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是在NLP数据集上训练这个模型（例如机器翻译）。然而，训练这些模型是一个单独章节的主题。 Transformers比我们讨论的还要复杂得多。例如，有预训练的基于Transformer的模型，你可以随时使用它们来解决NLP任务。我们将在后面的章节再次讨论Transformers。
- en: Summary
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: Transformer networks have outperformed other models in almost all NLP tasks.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer网络在几乎所有NLP任务中都表现优于其他模型。
- en: Transformers are an encoder-decoder-type neural network that is mainly used
    for learning NLP tasks.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer是一种主要用于学习NLP任务的编码器 - 解码器型神经网络。
- en: 'With Transformers, the encoder and decoder are made of two computational sublayers:
    self-attention layers and fully connected layers.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Transformer，编码器和解码器由两个计算子层组成：自我注意层和完全连接层。
- en: The self-attention layer produces a weighted sum of inputs for a given time
    step, based on how important it is to attend to other positions in the sequence
    while processing the current position.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自我注意层根据处理当前位置时，与序列中其他位置之间的相对重要性产生一个给定时间步长的输入的加权和。
- en: The fully connected layer creates a nonlinear representation of the attended
    output produced by the self-attention layer.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全连接层对自我注意层产生的注意输出进行了非线性表示。
- en: The decoder uses masking in its self-attention layer to make sure that the decoder
    does not see any future predictions while producing the current prediction.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器在其自我注意层中使用掩码，以确保在产生当前预测时，解码器不会看到任何未来的预测。
- en: Answers to exercises
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习答案
- en: '**Exercise 1**'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 1**'
- en: '[PRE22]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '**Exercise 2**'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 2**'
- en: '[PRE23]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
