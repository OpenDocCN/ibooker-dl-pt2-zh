["```py\n# (1) Start minio server\n./scripts/dm-001-start-minio.sh     \n\n# (2) start dataset management service, it will build \n➥ the dm image and run the container.\n./scripts/dm-002-start-server.sh\n```", "```py\nmc -q cp data-management/src/test/resources/datasets/test.csv  ❶\n➥ myminio/\"${MINIO_DM_BUCKET}\"/upload/001.csv\n\ngrpcurl -v -plaintext \\                                        ❷\n -d '{\"name\": \"dataset-1\", \\\n      \"dataset_type\": \"LANGUAGE_INTENT\", \\                     ❸\n      \"bucket\": \"mini-automl\", \\                               ❹\n      \"path\": \"{DATA_URL_IN_MINIO}\"}' \\                        ❹\n ${DM_SERVER}:${DM_PORT} \\\n data_management.DataManagementService/CreateDataset           ❺\n```", "```py\n{\n \"datasetId\": \"1\", \n  \"name\": \"dataset-1\",\n \"dataset_type\": \"TEXT_INTENT\",\n  \"last_updated_at\": \"2021-10-09T23:44:00\",\n  \"commits\": [                                ❶\n    {       \n      \"dataset_id\": \"1\",\n      \"commit_id\": \"1\",                       ❷\n      \"created_at\": \"2021-10-09T23:44\",\n \"commit_message\": \"Initial commit\",\n      \"tags\": [                               ❸\n        {\n          \"tag_key\": \"category\",\n          \"tag_value\": \"test set\"\n        }\n      ],\n      \"path\": \"dataset/1/commit/1\",\n      \"statistics\": {                         ❹\n        \"numExamples\": \"5500\",\n        \"numLabels\": \"151\"\n      }\n    }\n  ]\n}\n```", "```py\nmc -q cp data-management/src/test/resources/datasets/train.csv  myminio/\"${MINIO_DM_BUCKET}\"/upload/002.csv        ❶\n\ngrpcurl -v -plaintext \\                                 ❷\n -d '{\"dataset_id\": \"1\", \\                              ❸\n      \"commit_message\": \"More training data\", \\\n      \"bucket\": \"mini-automl\", \\                        ❹\n      \"path\": \"upload/002.csv\", \\                       ❹\n      \"tags\": [{ \\\n        \"tag_key\": \"category\", \\\n        \"tag_value\": \"training set\\\"}]}' \\ \n${DM_SERVER}:${DM_PORT} \\\ndata_management.DataManagementService/UpdateDataset     ❺\n```", "```py\n{\n  \"datasetId\": \"1\",\n  \"name\": \"dataset-1\",\n  \"dataset_type\": \"TEXT_INTENT\",\n  \"last_updated_at\": \"2021-10-09T23\",\n  \"commits\": [\n    {\n      \"commit_id\": \"1\",        ❶\n       .. .. ..\n    },\n    {\n      \"dataset_id\": \"1\",\n      \"commit_id\": \"2\",        ❷\n      \"created_at\": \"2021-10-09T23:59:17\",\n      \"commit_message\": \"More training data\",\n      \"tags\": [\n        {\n          \"tag_key\": \"category\",     \n          \"tag_value\": \"training set\" \n        }\n      ],\n      \"path\": \"dataset/1/commit/2\",\n      \"statistics\": {\n        \"numExamples\": \"7600\",\n        \"numLabels\": \"151\"\n      }\n    }\n  ]\n}\n```", "```py\ngrpcurl -v -plaintext \n  -d '{\"datasetId\": \"1\"}' \\     ❶\n${DM_SERVER}:${DM_PORT} \\\ndata_management.DataManagementService/GetDatasetSummary\n```", "```py\ngrpcurl -plaintext \\                   ❶\n -d “{“dataset_id”: “1”}” \\            ❶\n ${DM_SERVER}:${DM_PORT} \\             ❶\ndata_management.DataManagementService/PrepareTrainingDataset\n\ngrpcurl -plaintext \\                   ❷\n -d “{“dataset_id”: “1”, \\             ❷\n “Tags”:[ \\                            ❷\n   {“tag_key”:”category”, \\            ❸\n    “tag_value”:”training set”}]}” \\   ❸\n ${DM_SERVER}:${DM_PORT} data_management.DataManagementService/PrepareTrainingDataset\n```", "```py\n{\n  \"dataset_id\": \"1\",\n  \"name\": \"dataset-1\",\n  \"dataset_type\": \"TEXT_INTENT\",\n  \"last_updated_at\": \"2021-10-09T23:44:00\",\n  \"version_hash\": \"hashDg==\",     ❶\n  \"commits\": [\n    {                             ❷\n      \"commit_id\": \"1\",           ❷\n      .. .. ..                    ❷\n    },                            ❷\n    {                             ❷\n      \"commit_id\": \"2\",           ❷\n      .. .. ..                    ❷\n    }                             ❷\n  ]\n}\n```", "```py\ngrpcurl -plaintext \\\n -d \"{\"dataset_id\": \"1\", \\\n      \"version_hash\":          ❶\n      \"hashDg==\"}\" \\           ❶\n${DM_SERVER}:${DM_PORT} \ndata_management.DataManagementService/FetchTrainingDataset\n```", "```py\n{\n  \"dataset_id\": \"1\",\n  \"version_hash\": \"hashDg==\",\n  \"state\": \"READY\",                                          ❶\n  \"parts\": [\n    {                                                        ❷\n      \"name\": \"examples.csv\",                                ❷\n      \"bucket\": \"mini-automl-dm\",                            ❷\n      \"path\": \"versionedDatasets/1/hashDg==/examples.csv\"    ❷\n    },                                                       ❷\n    {                                                        ❷\n      \"name\": \"labels.csv\",                                  ❷\n      \"bucket\": \"mini-automl-dm\",                            ❷\n      \"path\": \"versionedDatasets/1/hashDg==/labels.csv\"      ❷\n    }                                                        ❷\n  ],                                                         ❷\n  \"statistics\": {\n    \"numExamples\": \"16200\",\n    \"numLabels\": \"151\"\n  }\n}\n```", "```py\n# create a new dataset and save data into it\nrpc CreateDataset (CreateDatasetRequest) returns (DatasetSummary);\n\n# add new data to an existing dataset \nrpc UpdateDataset (CreateCommitRequest) returns (DatasetSummary);\n\n# get summary and history of a given dataset\nrpc GetDatasetSummary (DatasetPointer) returns (DatasetSummary);\n\n# list all existing datasets’ summary\nrpc ListDatasets (ListQueryOptions) returns (stream DatasetSummary);\n\nmessage CreateDatasetRequest {\n string name = 1;\n string description = 2;\n DatasetType dataset_type = 3;   ❶\n string bucket = 4;              ❷\n string path = 5;                ❷\n repeated Tag tags = 6;          ❸\n}\n```", "```py\npublic void createDataset(CreateDatasetRequest request) {\n\n  int datasetId = store.datasetIdSeed.incrementAndGet();      ❶\n\n  Dataset dataset = new Dataset(                              ❷\n    datasetId, request.getName(),                             ❷\n    request.getDescription(),                                 ❷\n    request.getDatasetType());                                ❷\n  int commitId = dataset.getNextCommitId();                   ❷\n\n  CommitInfo.Builder builder = DatasetIngestion               ❸\n    .ingest(minioClient, datasetId, commitId,                 ❸\n    request.getDatasetType(), request.getBucket(),            ❸\n    request.getPath(), config.minioBucketName);               ❸\n\n  store.datasets.put(Integer.toString(datasetId), dataset);   ❹\n  dataset.commits.put(commitId, builder                       ❹\n    .setCommitMessage(\"Initial commit\")                       ❹\n    .addAllTags(request.getTagsList()).build());              ❹\n\n  responseObserver.onNext(dataset.toDatasetSummary());        ❺\n  responseObserver.onCompleted();                             ❺\n}\n```", "```py\npublic void updateDataset(CreateCommitRequest request) {\n\n  String datasetId = request.getDatasetId();   ❶\n\n  Dataset dataset = store.datasets             ❷\n    .get(datasetId);                           ❷\n  String commitId = Integer.toString(dataset   ❷\n    .getNextCommitId());                       ❷\n\n // the rest code are the same as listing 2.7\n  .. .. .. \n}\n```", "```py\nrpc PrepareTrainingDataset (DatasetQuery)     ❶\n  returns (DatasetVersionHash);               ❶\n\nrpc FetchTrainingDataset (VersionHashQuery)   ❷\n  returns (VersionHashDataset);               ❷\n\nmessage DatasetQuery {                        ❸\n string dataset_id = 1;                       ❹\n string commit_id = 2;                        ❺\n repeated Tag tags = 3;                       ❻\n}\n\nmessage VersionHashQuery {                    ❼\n string dataset_id = 1; \n string version_hash = 2;                     ❽\n}\n```", "```py\npublic void prepareTrainingDataset(DatasetQuery request) {\n # step 1, receive dataset preparation request\n  Dataset dataset = store.datasets.get(datasetId);\n  String commitId;\n  .. .. ..\n  # step 2, select data commits by checking tag filter\n  BitSet pickedCommits = new BitSet();\n  List<DatasetPart> parts = Lists.newArrayList();\n  List<CommitInfo> commitInfoList = Lists.newLinkedList();\n  for (int i = 1; i <= Integer.parseInt(commitId); i++) {\n    CommitInfo commit = dataset.commits.get(Integer.toString(i));\n    boolean matched = true;\n    for (Tag tag : request.getTagsList()) {\n      matched &= commit.getTagsList().stream().anyMatch(k -> k.equals(tag));\n    }\n    if (!matched) {\n      continue;\n    }\n    pickedCommits.set(i);\n    commitInfoList.add(commit);\n    .. .. ..\n  }\n\n # step 3, generate version hash from the selected commits list\n  String versionHash = String.format(\"hash%s\", \n    Base64.getEncoder().encodeToString(pickedCommits.toByteArray()));\n\n  if (!dataset.versionHashRegistry.containsKey(versionHash)) {\n    dataset.versionHashRegistry.put(versionHash,      ❶\n      VersionedSnapshot.newBuilder()                  ❶\n        .setDatasetId(datasetId)                      ❶\n        .setVersionHash(versionHash)                  ❶\n        .setState(SnapshotState.RUNNING).build());    ❶\n\n # step 5,6,7,8, start a background thread to aggregate data \n # from commits to the training dataset     \n    threadPool.submit(\n      new DatasetCompressor(minioClient, store, datasetId,\n        dataset.getDatasetType(), parts, versionHash, config.minioBucketName));\n   }\n\n # step 4, return hash version string to customer\n  responseObserver.onNext(responseBuilder.build());\n  responseObserver.onCompleted();\n}\n```", "```py\npublic void fetchTrainingDataset(VersionQuery request) {\n  String datasetId = request.getDatasetId(); \n  Dataset dataset = store.datasets.get(datasetId); \n\n  if (dataset.versionHashRegistry.containsKey(   ❶\n      request.getVersionHash())) {               ❶\n\n    responseObserver.onNext(\n\n      dataset.versionHashRegistry.get(           ❷\n request.getVersionHash()));                ❷\n    responseObserver.onCompleted();\n  } \n  .. .. .. \n}\n```", "```py\n>> TEXT_INTENT dataset ingestion data schema\n<text utterance>, <label>,<label>,<label>, ...\n```", "```py\n“I am still waiting on my credit card”, activate_my_card      ❶\n➥ ;card_arrival                                              ❶\n“I couldn’t purchase gas in Costco”, card_not_working\n```", "```py\nexamples.csv: <text utterance>, <label_id>,<label_id>, ...\n“I am still waiting on my credit card”, 0;1\n“I couldn’t purchase gas in Costco”, 2\n\nLabels.csv: <label_id>, <label_name>\n0, activate_my_card\n1, card_arrival\n2, card_not_working\n```", "```py\n>> examples.csv: <image filename>,<label id>\n“imageA.jpg”, 0\n“imageB.jpg”, 1\n“imageC.jpg”, 0\n\n>> labels.csv: <label id>,<label name>\n0, cat\n1, dog\n\n>> examples/ - folder\nimageA.jpg\nimageB.jpg\nimageC.jpg\n```", "```py\n├── cat\n│   ├── catA.jpg\n│   ├── catB.jpg\n│   └── catC.jpg\n└── dog\n    ├── dogA.jpg\n    ├── dogB.jpg\n    └── dogC.jpg\n```", "```py\npathToTable = \"/my/sample/text/intent/dataset/A\"\n\ndeltaTable = DeltaTable.forPath(spark, pathToTable)    ❶\nfullHistoryDF = deltaTable.history()                   ❷\nlastOperationDF = deltaTable.history(1)                ❸\n\ndf = spark.read.format(\"delta\")                        ❹\n       .option(\"timestampAsOf\", \"2021-07-01\")          ❹\n       .load(pathToTable)                              ❹\n\ndf = spark.read.format(\"delta\")                        ❺\n      .option(\"versionAsOf\", \"12\")                     ❺\n      .load(pathToTable)                               ❺\n```", "```py\n## Step 1: load all raw images files\npath_labeled_rawdata = “datacollablab/flower_photos/”\n\nimages = spark.read.format(\"binary\")                     ❶\n .option(\"recursiveFileLookup\", \"true\")                  ❶\n .option(\"pathGlobFilter\", \"*.jpg\")                      ❶\n .load(path_labeled_rawdata)                             ❶\n .repartition(4)                                         ❶\n\n## Step 2: define ETL extract functions\ndef extract_label(path_col):                             ❷\n \"\"\"Extract label from file path using built-in SQL functions.\"\"\"\n return regexp_extract(path_col, \"flower_photos/([^/]+)\", 1)\n\ndef extract_size(content):                               ❸\n \"\"\"Extract image size from its raw content.\"\"\"\n image = Image.open(io.BytesIO(content))\n return image.size \n\n@pandas_udf(\"width: int, height: int\")\ndef extract_size_udf(content_series):                    ❸\n sizes = content_series.apply(extract_size)\n return pd.DataFrame(list(sizes))\n## Step 3: construct and execute ETL to generate a data frame \n## contains label, image, image size and path for each image. \ndf = images.select(\n col(\"path\"),\n extract_size_udf(col(\"content\")).alias(\"size\"),\n extract_label(col(\"path\")).alias(\"label\"),\n col(\"content\"))\n\n## Step 4: save the image dataframe produced \n# by ETL to a Delta Lake table\ngold_table_training_dataset = “datacollablab.flower_train_binary”\nspark.conf.set(\"spark.sql.parquet.compression.codec\", \"uncompressed\")\ndf.write.format(“delta”).mode(“overwrite”)\n  .saveAsTable(gold_table_training_dataset)\n\n>>> \nColumnName: path: string                                ❹\nColumnName: label: string                               ❹\nColumnName: labelIndex: bigint                          ❹\nColumnName: size: struct<width:int, length:int>         ❹\nColumnName: content: binary                             ❹\n```", "```py\n## Step 1: Read dataframe from Delta Lake table\ndf = spark.read.format(\"delta\")\n  .load(gold_table_training_dataset) \n .select(col(\"content\"), col(\"label_index\")) \n .limit(100)\nnum_classes = df.select(\"label_index\").distinct().count()\n\ndf_train, df_val = df                                         ❶\n  .randomSplit([0.9, 0.1], seed=12345)                        ❶\n\n## (2) Load dataframes into Petastorm converter \nspark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF,  \n  \"file:///dbfs/tmp/petastorm/cache\")\nconverter_train = make_spark_converter(df_train)\nconverter_val = make_spark_converter(df_val)\n\n## (3) Read training data in PyTorch by using \n## Petastorm converter\ndef train_and_evaluate(lr=0.001):\n device = torch.device(\"cuda\")\n  model = get_model(lr=lr)\n    .. .. .. \n\n  with converter_train.make_torch_dataloader(                 ❷\n         transform_spec=get_transform_spec(is_train=True),\n         batch_size=BATCH_SIZE) as train_dataloader,\n converter_val.make_torch_dataloader(                   ❷\n         transform_spec=get_transform_spec(is_train=False),\n         batch_size=BATCH_SIZE) as val_dataloader:\n\n train_dataloader_iter = iter(train_dataloader)\n   steps_per_epoch = len(converter_train) // BATCH_SIZE\n\n val_dataloader_iter = iter(val_dataloader)\n   validation_steps = max(1, len(converter_val) // BATCH_SIZE)\n\n   for epoch in range(NUM_EPOCHS):\n     .. .. \n     train_loss, train_acc = train_one_epoch(\n        model, criterion, optimizer,  \n        exp_lr_scheduler,\n train_dataloader_iter,                                ❸\n        steps_per_epoch, epoch,device)\n\n     val_loss, val_acc = evaluate(\n        model, criterion, \n val_dataloader_iter,                                  ❸\n        validation_steps, device)\n return val_loss\n```", "```py\n{\n  \"pipeline\": {\n    \"name\": \"edges\"        ❶\n  },\n  \"description\": \"A pipeline that performs image \\\n     edge detection by using the OpenCV library.\",\n  \"transform\": {\n    \"cmd\": [ \"python3\", \"/edges.py\" ],\n    \"image\": \"pachyderm/opencv\"\n  },\n  \"input\": {\n    \"pfs\": {\n      \"repo\": \"images\",    ❷\n      \"glob\": \"/*\"\n    }\n  }\n}\n```", "```py\n$ pachctl list commit edges #A\nREPO  BRANCH COMMIT                           FINISHED\nedges master 0547b62a0b6643adb370e80dc5edde9f 3 minutes ago \nedges master eb58294a976347abaf06e35fe3b0da5b 3 minutes ago \nedges master f06bc52089714f7aa5421f8877f93b9c 7 minutes ago \n```", "```py\n“eb58294a976347abaf06e35fe3b0da5b”. \n$ pachctl inspect commit edges@eb58294a976347abaf06e35fe3b0da5b \\\n       --full-timestamps\n```", "```py\nCommit: edges@eb58294a976347abaf06e35fe3b0da5b\nOriginal Branch: master\nParent: f06bc52089714f7aa5421f8877f93b9c\nStarted: 2021-07-19T05:04:23.652132677Z\nFinished: 2021-07-19T05:04:26.867797209Z\nSize: 59.38KiB\nProvenance:  __spec__@91da2f82607b4c40911d48be99fd3031 (edges)  ❶\nimages@66f4ff89a017412090dc4a542d9b1142 (master)                ❶\n```"]