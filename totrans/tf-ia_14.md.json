["```py\nimport os\nimport requests\nimport zipfile\n\n# Make sure the zip file has been downloaded\nif not os.path.exists(os.path.join('data','deu-eng.zip')):\n    raise FileNotFoundError(\n        \"Uh oh! Did you download the deu-eng.zip from \n➥ http:/ /www.manythings.org/anki/deu-eng.zip manually and place it in the \n➥ Ch11/data folder?\"\n    )\n\nelse:\n    if not os.path.exists(os.path.join('data', 'deu.txt')):\n        with zipfile.ZipFile(os.path.join('data','deu-eng.zip'), 'r') as zip_ref:\n            zip_ref.extractall('data')\n    else:\n        print(\"The extracted data already exists\")\n```", "```py\nGo.    Geh.    CC-BY 2.0 (France) Attribution: tatoeba.org \n➥ #2877272 (CM) & #8597805 (Roujin)\nHi.    Hallo!    CC-BY 2.0 (France) Attribution: tatoeba.org \n➥ #538123 (CM) & #380701 (cburgmer)\nHi.    Grüß Gott!    CC-BY 2.0 (France) Attribution: \n➥ tatoeba.org #538123 (CM) & #659813 (Esperantostern)\n...\nIf someone who doesn't know your background says that you sound like \n➥ a native speaker, ... . In other words, you don't really sound like \n➥ a native speaker.    Wenn jemand, der nicht weiß, woher man \n➥ kommt, sagt, man erwecke doch den Eindruck, Muttersprachler zu sein, \n➥ ... - dass man diesen Eindruck mit anderen Worten eigentlich nicht \n➥ erweckt.    CC-BY 2.0 (France) Attribution: tatoeba.org #953936 \n➥  (CK) & #8836704 (Pfirsichbaeumchen)\nDoubtless there exists in this world precisely the right woman for \n➥ any given man to marry and vice versa; ..., that probably, since \n➥ the earth was created, the right man has never yet met the right \n➥ woman.    Ohne Zweifel findet sich auf dieser Welt zu jedem Mann \n➥ genau die richtige Ehefrau und umgekehrt; ..., dass seit Erschaffung \n➥ ebenderselben wohl noch nie der richtige Mann der richtigen Frau \n➥ begegnet ist.    CC-BY 2.0 (France) Attribution: tatoeba.org \n➥ #7697649 (RM) & #7729416 (Pfirsichbaeumchen)\n```", "```py\nimport pandas as pd\n\n# Read the csv file\ndf = pd.read_csv(\n    os.path.join('data', 'deu.txt'), delimiter='\\t', header=None\n)\n# Set column names\ndf.columns = [\"EN\", \"DE\", \"Attribution\"]\ndf = df[[\"EN\", \"DE\"]]\n```", "```py\nprint('df.shape = {}'.format(df.shape))\n```", "```py\ndf.shape = (227080, 2)\n```", "```py\nclean_inds = [i for i in range(len(df)) if b\"\\xc2\" not in df.iloc[i][\"DE\"].encode(\"utf-8\")]\n\ndf = df.iloc[clean_inds]\n```", "```py\nn_samples = 50000\ndf = df.sample(n=n_samples, random_state=random_seed)\n```", "```py\nstart_token = 'sos'\nend_token = 'eos'\ndf[\"DE\"] = start_token + ' ' + df[\"DE\"] + ' ' + end_token\n```", "```py\n# Randomly sample 10% examples from the total 50000 randomly\ntest_df = df.sample(n=n=int(n_samples/10), random_state=random_seed)\n# Randomly sample 10% examples from the remaining randomly\nvalid_df = df.loc[~df.index.isin(test_df.index)].sample(\n    n=n=int(n_samples/10), random_state=random_seed\n)\n# Assign the rest to training data\ntrain_df = df.loc[~(df.index.isin(test_df.index) | \n➥ df.index.isin(valid_df.index))]\n```", "```py\nfrom collections import Counter\n\nen_words = train_df[\"EN\"].str.split().sum()                    ❶\nde_words = train_df[\"DE\"].str.split().sum()                    ❷\n\nn=10                                                           ❸\n\ndef get_vocabulary_size_greater_than(words, n, verbose=True):\n\n    \"\"\" Get the vocabulary size above a certain threshold \"\"\"\n\n    counter = Counter(words)                                   ❹\n\n    freq_df = pd.Series(                                       ❺\n        list(counter.values()), \n        index=list(counter.keys())\n    ).sort_values(ascending=False)\n\n    if verbose:\n        print(freq_df.head(n=10))                              ❻\n\n    n_vocab = (freq_df>=n).sum()                               ❼\n\n    if verbose:\n        print(\"\\nVocabulary size (>={} frequent): {}\".format(n, n_vocab))\n\n    return n_vocab\n\nprint(\"English corpus\")\nprint('='*50)\nen_vocab = get_vocabulary_size_greater_than(en_words, n)\n\nprint(\"\\nGerman corpus\")\nprint('='*50)\nde_vocab = get_vocabulary_size_greater_than(de_words, n)\n```", "```py\nEnglish corpus\n==================================================\nTom    9427\nto     8673\nI      8436\nthe    6999\nyou    6125\na      5680\nis     4374\nin     2664\nof     2613\nwas    2298\ndtype: int64\n\nVocabulary size (>=10 frequent): 2238\nGerman corpus\n==================================================\nsos      40000\neos      40000\nTom       9928\nIch       7749\nist       4753\nnicht     4414\nzu        3583\nSie       3465\ndu        3112\ndas       2909\ndtype: int64\n\nVocabulary size (>=10 frequent): 2497\n```", "```py\ndef print_sequence_length(str_ser):\n\n    \"\"\" Print the summary stats of the sequence length \"\"\"\n\n    seq_length_ser = str_ser.str.split(' ').str.len()             ❶\n\n    print(\"\\nSome summary statistics\")                            ❷\n    print(\"Median length: {}\\n\".format(seq_length_ser.median()))  ❷\n    print(seq_length_ser.describe())                              ❷\n\n    print(\n        \"\\nComputing the statistics between the 1% and 99% quantiles (to \n➥ ignore outliers)\"\n    )\n    p_01 = seq_length_ser.quantile(0.01)                          ❸\n    p_99 = seq_length_ser.quantile(0.99)                          ❸\n\n    print(\n        seq_length_ser[\n            (seq_length_ser >= p_01) & (seq_length_ser < p_99)\n        ].describe()                                              ❹\n    )\n```", "```py\nprint(\"English corpus\")\nprint('='*50)\nprint_sequence_length(train_df[\"EN\"])\n\nprint(\"\\nGerman corpus\")\nprint('='*50)\nprint_sequence_length(train_df[\"DE\"])\n```", "```py\nEnglish corpus\n==================================================\nSome summary statistics\nMedian length: 6.0\n\ncount    40000.000000\nmean         6.360650\nstd          2.667726\nmin          1.000000\n25%          5.000000\n50%          6.000000\n75%          8.000000\nmax        101.000000\nName: EN, dtype: float64\n\nComputing the statistics between the 1% and 99% quantiles (to ignore outliers)\ncount    39504.000000\nmean         6.228002\nstd          2.328172\nmin          2.000000\n25%          5.000000\n50%          6.000000\n75%          8.000000\nmax         14.000000\nName: EN, dtype: float64\n\nGerman corpus\n==================================================\n\nSome summary statistics\nMedian length: 8.0\n\ncount    40000.000000\nmean         8.397875\nstd          2.652027\nmin          3.000000\n25%          7.000000\n50%          8.000000\n75%         10.000000\nmax         77.000000\nName: DE, dtype: float64\n\nComputing the statistics between the 1% and 99% quantiles (to ignore outliers)\ncount    39166.000000\nmean         8.299035\nstd          2.291474\nmin          5.000000\n25%          7.000000\n50%          8.000000\n75%         10.000000\nmax         16.000000\nName: DE, dtype: float64\n```", "```py\nprint(\"EN vocabulary size: {}\".format(en_vocab))\nprint(\"DE vocabulary size: {}\".format(de_vocab))\n\n# Define sequence lengths with some extra space for longer sequences\nen_seq_length = 19\nde_seq_length = 21\n\nprint(\"EN max sequence length: {}\".format(en_seq_length))\nprint(\"DE max sequence length: {}\".format(de_seq_length))\n```", "```py\nEN vocabulary size: 359\nDE vocabulary size: 336\nEN max sequence length: 19\nDE max sequence length: 21\n```", "```py\n0       [a, b, c]\n1          [d, e]\n2    [f, g, h, i]\n...\n\ndtype: object\n```", "```py\nfrom tensorflow.keras.layers.experimental.preprocessing import \n➥ TextVectorization\n```", "```py\nen_vectorize_layer = TextVectorization(\n    max_tokens=en_vocab,\n    output_mode='int',\n    output_sequence_length=None\n)\n```", "```py\nen_vectorize_layer.adapt(np.array(train_df[\"EN\"].tolist()).astype('str'))\n```", "```py\nprint(en_vectorize_layer.get_vocabulary()[:10])\n```", "```py\n['', '[UNK]', 'tom', 'to', 'you', 'the', 'i', 'a', 'is', 'that']\n```", "```py\nprint(len(en_vectorize_layer.get_vocabulary()))\n```", "```py\n2238\n```", "```py\ntoy_model = tf.keras.models.Sequential()\n```", "```py\ntoy_model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n```", "```py\ntoy_model.add(en_vectorize_layer)\n```", "```py\ninput_data = [[\"run\"], [\"how are you\"],[\"ectoplasmic residue\"]]\npred = toy_model.predict(input_data)\n```", "```py\nprint(\"Input data: \\n{}\\n\".format(input_data))\nprint(\"\\nToken IDs: \\n{}\".format(pred))\n```", "```py\nInput data: \n[['run'], ['how are you'], ['ectoplasmic residue']]\nToken IDs: \n[[427   0   0]\n [ 40  23   4]\n [  1   1   0]]\n```", "```py\ndef get_vectorizer(\n    corpus, n_vocab, max_length=None, return_vocabulary=True, name=None\n):\n\n    \"\"\" Return a text vectorization layer or a model \"\"\"\n\n    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='encoder_input')❶\n\n    vectorize_layer = \n➥ tf.keras.layers.experimental.preprocessing.TextVectorization(\n        max_tokens=n_vocab+2,                                              ❷\n        output_mode='int',\n        output_sequence_length=max_length,                \n    )\n\n    vectorize_layer.adapt(corpus)                                          ❸\n\n    vectorized_out = vectorize_layer(inp)                                  ❹\n\n    if not return_vocabulary: \n        return tf.keras.models.Model(\n            inputs=inp, outputs=vectorized_out, name=name\n        )                                                                  ❺\n    else:\n        return tf.keras.models.Model(\n            inputs=inp, outputs=vectorized_out, name=name                  ❻\n        ), vectorize_layer.get_vocabulary()        \n```", "```py\n# Get the English vectorizer/vocabulary\nen_vectorizer, en_vocabulary = get_vectorizer(\n    corpus=np.array(train_df[“EN”].tolist()), n_vocab=en_vocab, \n    max_length=en_seq_length, name=’en_vectorizer’\n)\n# Get the German vectorizer/vocabulary\nde_vectorizer, de_vocabulary = get_vectorizer(\n    corpus=np.array(train_df[“DE”].tolist()), n_vocab=de_vocab, \n    max_length=de_seq_length-1, name=’de_vectorizer’\n)\n```", "```py\n# The input is (None,1) shaped and accepts an array of strings\ninp = tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input')\n```", "```py\n# Vectorize the data (assign token IDs)\nvectorized_out = en_vectorizer(inp)\n```", "```py\n# Define an embedding layer to convert IDs to word vectors\nemb_layer = tf.keras.layers.Embedding(\n    input_dim=n_vocab+2, output_dim=128, mask_zero=True, name=’e_embedding’\n)\n# Get the embeddings of the token IDs\nemb_out = emb_layer(vectorized_out)\n```", "```py\ngru_layer = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128))\n```", "```py\ngru_out = gru_layer(emb_out)\n```", "```py\nencoder = tf.keras.models.Model(inputs=inp, outputs=gru_out)\n```", "```py\ndef get_encoder(n_vocab, vectorizer):\n    \"\"\" Define the encoder of the seq2seq model\"\"\"\n\n    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input')   ❶\n\n    vectorized_out = vectorizer(inp)                                    ❷\n\n    emb_layer = tf.keras.layers.Embedding(\n        n_vocab+2, 128, mask_zero=True, name='e_embedding'              ❸\n    )\n\n    emb_out = emb_layer(vectorized_out)                                 ❹\n\n    gru_layer = tf.keras.layers.Bidirectional(\n        tf.keras.layers.GRU(128, name='e_gru'),                         ❺\n        name='e_bidirectional_gru'\n    )\n\n    gru_out = gru_layer(emb_out)                                        ❻\n\n    encoder = tf.keras.models.Model(\n        inputs=inp, outputs=gru_out, name='encoder'\n    )                                                                   ❼\n\n    return encoder\n```", "```py\nencoder = get_encoder(en_vocab, en_vectorizer)\n```", "```py\ne_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input_final') \n```", "```py\nd_init_state = encoder(e_inp)\n```", "```py\nd_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='d_input')\n```", "```py\nvectorized_out = de_vectorizer(inp)\n```", "```py\nemb_layer = tf.keras.layers.Embedding(\n    input_dim=n_vocab+2, output_dim=128, mask_zero=True, name='d_embedding'\n)\nemb_out = emb_layer(vectorized_out)\n```", "```py\ngru_layer = tf.keras.layers.GRU(256, return_sequences=True)\n```", "```py\ngru_out = gru_layer(emb_out, initial_state=d_init_state)\n```", "```py\ndef get_final_seq2seq_model(n_vocab, encoder, vectorizer):\n    \"\"\" Define the final encoder-decoder model \"\"\"    \n    e_inp = tf.keras.Input(\n        shape=(1,), dtype=tf.string, name='e_input_final'\n    )                                                                    ❶\n\n    d_init_state = encoder(e_inp)                                        ❶\n\n    d_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='d_input')  ❷\n\n    d_vectorized_out = vectorizer(d_inp)                                 ❸\n\n    d_emb_layer = tf.keras.layers.Embedding(\n        n_vocab+2, 128, mask_zero=True, name='d_embedding'               ❹\n    )\n    d_emb_out = d_emb_layer(d_vectorized_out)\n\n    d_gru_layer = tf.keras.layers.GRU(\n        256, return_sequences=True, name='d_gru'\n    )                                                                    ❺\n\n    d_gru_out = d_gru_layer(d_emb_out, initial_state=d_init_state)       ❻\n\n    d_dense_layer_1 = tf.keras.layers.Dense(\n        512, activation='relu', name='d_dense_1'\n    )                                                                    ❼\n    d_dense1_out = d_dense_layer_1(d_gru_out)                            ❼\n\n    d_dense_layer_final = tf.keras.layers.Dense(\n        n_vocab+2, activation='softmax', name='d_dense_final'            ❽\n    )\n    d_final_out = d_dense_layer_final(d_dense1_out)                      ❽\n\n    seq2seq = tf.keras.models.Model(\n        inputs=[e_inp, d_inp], outputs=d_final_out, name='final_seq2seq' ❾\n    )\n\n    return seq2seq\n```", "```py\n# Get the English vectorizer/vocabulary\nen_vectorizer, en_vocabulary = get_vectorizer(\n    corpus=np.array(train_df[\"EN\"].tolist()), n_vocab=en_vocab, \n    max_length=en_seq_length, name='e_vectorizer'\n)\n# Get the German vectorizer/vocabulary\nde_vectorizer, de_vocabulary = get_vectorizer(\n    corpus=np.array(train_df[\"DE\"].tolist()), n_vocab=de_vocab,\n    max_length=de_seq_length-1, name='d_vectorizer'\n)\n\n# Define the final model\nencoder = get_encoder(n_vocab=en_vocab, vectorizer=en_vectorizer)\n   final_model = get_final_seq2seq_model(\n       n_vocab=de_vocab, encoder=encoder, vectorizer=de_vectorizer\n   )\n```", "```py\nfrom tensorflow.keras.metrics import SparseCategoricalAccuracy\n\nfinal_model.compile(\n    loss='sparse_categorical_crossentropy', \n    optimizer='adam', \n    metrics=['accuracy']\n)\n```", "```py\nfinal_model.summary()\n```", "```py\nModel: \"final_seq2seq\"\n___________________________________________________________________________\nLayer (type)                    Output Shape         Param #       \n➥ Connected to                     \n===========================================================================\nd_input (InputLayer)            [(None, 1)]          0                     \n___________________________________________________________________________\nd_vectorizer (Functional)       (None, 20)           0           \n➥ d_input[0][0]                    \n___________________________________________________________________________\ne_input_final (InputLayer)      [(None, 1)]          0                     \n___________________________________________________________________________\nd_embedding (Embedding)         (None, 20, 128)      319872      \n➥ d_vectorizer[0][0]               \n___________________________________________________________________________\nencoder (Functional)            (None, 256)          484864      \n➥ e_input_final[0][0]              \n___________________________________________________________________________\nd_gru (GRU)                     (None, 20, 256)      296448      \n➥ d_embedding[0][0]                \n➥ encoder[0][0]                    \n___________________________________________________________________________\nd_dense_1 (Dense)               (None, 20, 512)      131584      \n➥ d_gru[0][0]                      \n___________________________________________________________________________\nd_dense_final (Dense)           (None, 20, 2499)     1281987     \n➥ d_dense_1[0][0]                  \n===========================================================================\nTotal params: 2,514,755\nTrainable params: 2,514,755\nNon-trainable params: 0\n___________________________________________________________________________\n```", "```py\nen_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input')\nen_vectorized_out = en_vectorizer(inp)\nen_emb_layer = tf.keras.layers.Embedding(\n   en_vocab+2, 128, mask_zero=True, name='e_embedding'\n)\nen_emb_out = emb_layer(vectorized_out)\nen_gru_layer = tf.keras.layers.GRU(256, name='e_gru')\nen_gru_out = gru_layer(emb_out)\n```", "```py\ndef prepare_data(train_df, valid_df, test_df):\n    \"\"\" Create a data dictionary from the dataframes containing data \"\"\"\n\n    data_dict = {}                                                       ❶\n    for label, df in zip(\n        ['train', 'valid', 'test'], [train_df, valid_df, test_df]\n    ):                                                                   ❷\n        en_inputs = np.array(df[\"EN\"].tolist())                          ❸\n        de_inputs = np.array(\n            df[\"DE\"].str.rsplit(n=1, expand=True).iloc[:,0].tolist()     ❹\n        )\n        de_labels = np.array(\n            df[\"DE\"].str.split(n=1, expand=True).iloc[:,1].tolist()      ❺\n        )\n        data_dict[label] = {\n            'encoder_inputs': en_inputs,                                 ❻\n            'decoder_inputs': de_inputs, \n            'decoder_labels': de_labels\n        }\n\n    return data_dict\n```", "```py\ndef shuffle_data(en_inputs, de_inputs, de_labels, shuffle_indices=None): \n    \"\"\" Shuffle the data randomly (but all of inputs and labels at ones)\"\"\"\n\n    if shuffle_indices is None:        \n        shuffle_indices = np.random.permutation(np.arange(en_inputs.shape[0]))       ❶\n    else:        \n        shuffle_indices = np.random.permutation(shuffle_indices)                     ❷\n\n    return (\n        en_inputs[shuffle_indices], \n        de_inputs[shuffle_indices], \n        de_labels[shuffle_indices]                                                   ❸\n    ), shuffle_indices\n```", "```py\nclass BLEUMetric(object):\n\n    def __init__(self, vocabulary, name='perplexity', **kwargs):\n      \"\"\" Computes the BLEU score (Metric for machine translation) \"\"\"\n      super().__init__()\n      self.vocab = vocabulary                                                ❶\n      self.id_to_token_layer = StringLookup(\n          vocabulary=self.vocab, invert=True, \n          num_oov_indices=0\n      )                                                                      ❷\n\n    def calculate_bleu_from_predictions(self, real, pred):\n        \"\"\" Calculate the BLEU score for targets and predictions \"\"\"\n\n        pred_argmax = tf.argmax(pred, axis=-1)                               ❸\n\n        pred_tokens = self.id_to_token_layer(pred_argmax)                    ❹\n        real_tokens = self.id_to_token_layer(real)                           ❹\n\n        def clean_text(tokens):\n\n            \"\"\" Clean padding and [SOS]/[EOS] tokens to only keep meaningful words \"\"\"\n\n            t = tf.strings.strip(                                            ❺\n                        tf.strings.regex_replace(                            ❻\n                            tf.strings.join(                                 ❼\n                                tf.transpose(tokens), separator=' '\n                            ),\n                        \"eos.*\", \"\"),\n                   )\n\n            t = np.char.decode(t.numpy().astype(np.bytes_), encoding='utf-8')❽\n\n            t = [doc if len(doc)>0 else '[UNK]' for doc in t ]               ❾\n\n            t = np.char.split(t).tolist()                                    ❿\n\n            return t\n\n        pred_tokens = clean_text(pred_tokens)                                ⓫\n        real_tokens = [[r] for r in clean_text(real_tokens)]                 ⓬\n\n        bleu, precisions, bp, ratio, translation_length, reference_length = \n➥ compute_bleu(real_tokens, pred_tokens, smooth=False)                      ⓭\n\n        return bleu\n```", "```py\npred_argmax = tf.argmax(pred, axis=-1)  \n```", "```py\npred_tokens = self.id_to_token_layer(pred_argmax)\nreal_tokens = self.id_to_token_layer(real)\n```", "```py\nreal = [\n    [4,1,0],\n    [8,2,21]\n]\n\nvocabulary = ['', '[UNK]', 'sos', 'eos', 'tom', 'ich', 'nicht', 'ist', 'du', 'sie']\n```", "```py\nreal_tokens = tf.Tensor([\n    [b'tom' b'[UNK]' b'']\n    [b'du' b'sos' b'[UNK]']\n], shape=(2, 3), dtype=string)\n```", "```py\ndef clean_text(tokens):\n\n    \"\"\" Clean padding and [SOS]/[EOS] tokens to only keep meaningful words \"\"\"\n    # 3\\. Strip the string of any extra white spaces\n    translations_in_bytes = tf.strings.strip(\n        # 2\\. Replace everything after the eos token with blank\n        tf.strings.regex_replace(\n            # 1\\. Join all the tokens to one string in each sequence\n            tf.strings.join(tf.transpose(tokens), separator=' '),\n             \"eos.*\", \"\"\n        ),\n     )\n\n     # Decode the byte stream to a string\n     translations = np.char.decode(\n         translations_in_bytes.numpy().astype(np.bytes_), encoding='utf-8'\n     )\n\n     # If the string is empty, add a [UNK] token\n     # Otherwise get a Division by zero error\n     translations = [sent if len(sent)>0 else '[UNK]' for sent in translations ]\n\n     # Split the sequences to individual tokens \n     translations = np.char.split(translations).tolist()\n\n     return translations\n```", "```py\ntranslations_in_bytes = tf.strings.strip(\n        # 2\\. Replace everything after the eos token with blank\n        tf.strings.regex_replace(\n            # 1\\. Join all the tokens to one string in each sequence\n            tf.strings.join(tf.transpose(tokens), separator=' '),\n             \"eos.*\", \"\"\n        ),\n     )\n```", "```py\n[\n    ['a','b','c'],\n    ['d', 'e', 'f']\n]\n```", "```py\n['ad', 'be', 'cf']\n```", "```py\npred_tokens = clean_text(pred_tokens)\nreal_tokens = [[r] for r in clean_text(real_tokens)]\n\nbleu, precisions, bp, ratio, translation_length, reference_length = \n➥ compute_bleu(real_tokens, pred_tokens)\n```", "```py\ntranslation = [['[UNK]', '[UNK]', 'mÃssen', 'wir', 'in', 'erfahrung', \n➥ 'bringen', 'wo', 'sie', 'wohnen']]\nreference = [[['als', 'mÃssen', 'mÃssen', 'wir', 'in', 'erfahrung', \n➥ 'bringen', 'wo', 'sie', 'wohnen']]]\n\nbleu1, _, _, _, _, _ = compute_bleu(reference, translation)\n\ntranslation = [['[UNK]', 'einmal', 'mÃssen', '[UNK]', 'in', 'erfahrung', \n➥ 'bringen', 'wo', 'sie', 'wohnen']]\nreference = [[['als', 'mÃssen', 'mÃssen', 'wir', 'in', 'erfahrung', \n➥ 'bringen', 'wo', 'sie', 'wohnen']]]\n\nbleu2, _, _, _, _, _ = compute_bleu(reference, translation)\n\nprint(\"BLEU score with longer correctly predict phrases: {}\".format(bleu1))\nprint(\"BLEU score without longer correctly predict phrases: \n➥ {}\".format(bleu2))\n```", "```py\nBLEU score with longer correctly predict phrases: 0.7598356856515925\nBLEU score without longer correctly predict phrases: 0.537284965911771\n```", "```py\ndef evaluate_model(\n    model, vectorizer, en_inputs_raw, de_inputs_raw, de_labels_raw, batch_size\n):\n    \"\"\" Evaluate the model on various metrics such as loss, accuracy and BLEU \"\"\"\n\n    bleu_metric = BLEUMetric(de_vocabulary)                                   ❶\n\n    loss_log, accuracy_log, bleu_log = [], [], []\n\n    n_batches = en_inputs_raw.shape[0]//batch_size                            ❷\n    print(\" \", end='\\r')\n\n    for i in range(n_batches):                                                ❸\n\n        print(\"Evaluating batch {}/{}\".format(i+1, n_batches), end='\\r')      ❹\n        x = [\n            en_inputs_raw[i*batch_size:(i+1)*batch_size],                     ❺\n            de_inputs_raw[i*batch_size:(i+1)*batch_size]\n        ]\n        y = de_vectorizer(de_labels_raw[i*batch_size:(i+1)*batch_size])       ❺\n\n        loss, accuracy = model.evaluate(x, y, verbose=0)                      ❻\n        pred_y = model.predict(x)                                             ❼\n           bleu = bleu_metric.calculate_bleu_from_predictions(y, pred_y)      ❽\n        loss_log.append(loss)                                                 ❾\n        accuracy_log.append(accuracy)                                         ❾\n        bleu_log.append(bleu)                                                 ❾\n\n    return np.mean(loss_log), np.mean(accuracy_log), np.mean(bleu_log)\n```", "```py\ndef train_model(model, vectorizer, train_df, valid_df, test_df, epochs, batch_size):\n    \"\"\" Training the model and evaluating on validation/test sets \"\"\"\n\n    bleu_metric = BLEUMetric(de_vocabulary)                                ❶\n\n    data_dict = prepare_data(train_df, valid_df, test_df)                  ❷\n    shuffle_inds = None\n\n    for epoch in range(epochs):\n\n        bleu_log = []                                                      ❸\n        accuracy_log = []                                                  ❸\n        loss_log = []                                                      ❸\n\n        (en_inputs_raw,de_inputs_raw,de_labels_raw), shuffle_inds  = \n➥ shuffle_data(                                                           ❹\n            data_dict['train']['encoder_inputs'],\n            data_dict['train']['decoder_inputs'],\n            data_dict['train']['decoder_labels'],\n            shuffle_inds\n        )\n\n        n_train_batches = en_inputs_raw.shape[0]//batch_size               ❺\n\n        for i in range(n_train_batches):                                   ❻\n\n            print(\"Training batch {}/{}\".format(i+1, n_train_batches), \n➥ end='\\r')                                                               ❼\n\n            x = [                                                          ❽\n                en_inputs_raw[i*batch_size:(i+1)*batch_size],  \n                de_inputs_raw[i*batch_size:(i+1)*batch_size]\n            ]\n            y = vectorizer(de_labels_raw[i*batch_size:(i+1)*batch_size])   ❾\n\n            model.train_on_batch(x, y)                                     ❿\n            loss, accuracy = model.evaluate(x, y, verbose=0)               ⓫\n            pred_y = model.predict(x)                                      ⓬\n            bleu = bleu_metric.calculate_bleu_from_predictions(y, pred_y)  ⓭\n\n            loss_log.append(loss)                                          ⓮\n            accuracy_log.append(accuracy)                                  ⓮\n            bleu_log.append(bleu)                                          ⓮\n\n        val_en_inputs = data_dict['valid']['encoder_inputs']               ⓯\n        val_de_inputs = data_dict['valid']['decoder_inputs']               ⓯\n        val_de_labels = data_dict['valid']['decoder_labels']               ⓯\n\n        val_loss, val_accuracy, val_bleu = evaluate_model(                 ⓰\n            model, \n            vectorizer, \n            val_en_inputs, \n            val_de_inputs, \n            val_de_labels, \n            epochs, \n            batch_size\n        )\n\n        print(\"\\nEpoch {}/{}\".format(epoch+1, epochs))                     ⓱\n        print(\n            \"\\t(train) loss: {} - accuracy: {} - bleu: {}\".format(\n                np.mean(loss_log), np.mean(accuracy_log), np.mean(bleu_log)\n            )\n      )\n      print(\n          \"\\t(valid) loss: {} - accuracy: {} - bleu: {}\".format(\n              val_loss, val_accuracy, val_bleu\n          )\n      )\n\n    test_en_inputs = data_dict['test']['encoder_inputs']\n    test_de_inputs = data_dict['test']['decoder_inputs']\n    test_de_labels = data_dict['test']['decoder_labels']\n\n    test_loss, test_accuracy, test_bleu = evaluate_model(\n            model, \n            vectorizer, \n            test_en_inputs, \n            test_de_inputs, \n            test_de_labels, \n            epochs, \n            batch_size\n    )\n\n    print(\"\\n(test) loss: {} - accuracy: {} - bleu: {}\".format(\n        test_loss, test_accuracy, test_bleu)\n    )\n```", "```py\nepochs = 5\nbatch_size = 128\n\ntrain_model(final_model, de_vectorizer, train_df, valid_df, test_df, \n➥ epochs, batch_size)\n```", "```py\nEvaluating batch 39/39\nEpoch 1/5\n    (train) loss: 1.7741597780050375 - accuracy: 0.2443966139585544 - \n➥ bleu: 0.0014343267864378607\n    (valid) loss: 1.4453194752717629 - accuracy: 0.3318057709779495 - \n➥ bleu: 0.010740537197906803\nEvaluating batch 39/39\n...\n\nEpoch 5/5\n    (train) loss: 0.814081399104534 - accuracy: 0.5280381464041196 - \n➥ bleu: 0.1409178724874819\n    (valid) loss: 0.8876287539800009 - accuracy: 0.514901713683055 - \n➥ bleu: 0.1285171513954398\nEvaluating batch 39/39\n(test) loss: 0.9077589313189188 - accuracy: 0.5076315150811122 - bleu: \n➥ 0.12664703414801345\n```", "```py\n## Save the model\nos.makedirs('models', exist_ok=True)\ntf.keras.models.save_model(final_model, os.path.join('models', 'seq2seq'))\n\nimport json\nos.makedirs(os.path.join('models', 'seq2seq_vocab'), exist_ok=True)\n\n# Save the vocabulary files\nwith open(os.path.join('models', 'seq2seq_vocab', 'en_vocab.json'), 'w') as f:\n    json.dump(en_vocabulary, f)    \nwith open(os.path.join('models', 'seq2seq_vocab', 'de_vocab.json'), 'w') as f:\n    json.dump(de_vocabulary, f)\n```", "```py\nfor epoch in range(epochs):\n\n    bleu_log = []                            \n\n    n_train_batches = en_inputs_raw.shape[0]//batch_size       \n    for i in range(n_train_batches):                        \n\n        print(\"Training batch {}/{}\".format(i+1, n_train_batches), end='\\r')    \n\n        x = [                                                   \n            en_inputs_raw[i*batch_size:(i+1)*batch_size],  \n            de_inputs_raw[i*batch_size:(i+1)*batch_size]\n        ]\n        y = vectorizer(de_labels_raw[i*batch_size:(i+1)*batch_size])\n\n        model.train_on_batch(x, y)\n        pred_y = model.predict(x)\n\n        bleu_log.append(bleu_metric.calculate_bleu_from_predictions(y, pred_y)) \n\n    mean_bleu = np.mean(bleu_log)\n```", "```py\nmodel = tf.keras.models.load_model(save_path)\n```", "```py\nen_model = model.get_layer(\"encoder\")\n```", "```py\nd_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='d_infer_input')\nd_state_inp = tf.keras.Input(shape=(256,), name='d_infer_state')\n```", "```py\n# Generate the vectorized output of inp\nd_vectorizer = model.get_layer('d_vectorizer')    \nd_vectorized_out = d_vectorizer(d_inp)\n\n# Generate the embeddings from the vectorized input\nd_emb_out = model.get_layer('d_embedding')(d_vectorized_out)\n\n# Get the GRU layer\nd_gru_layer = model.get_layer(\"d_gru\")\n# Since we generate one word at a time, we will not need the return_sequences\nd_gru_layer.return_sequences = False\n# Get the GRU out while using d_state_inp from earlier, as the initial state\nd_gru_out = d_gru_layer(d_emb_out, initial_state=d_state_inp) \n\n# Get the dense output\nd_dense1_out = model.get_layer(\"d_dense_1\")(d_gru_out) \n\n# Get the final output\nd_final_out = model.get_layer(\"d_dense_final\")(d_dense1_out) \n```", "```py\nde_model = tf.keras.models.Model(\n    inputs=[d_inp, d_state_inp], outputs=[d_final_out, d_gru_out]\n)\n```", "```py\nimport tensorflow.keras.backend as K\nK.clear_session()\n\ndef get_inference_model(save_path):\n    \"\"\" Load the saved model and create an inference model from that \"\"\"\n\n    model = tf.keras.models.load_model(save_path)                        ❶\n\n    en_model = model.get_layer(\"encoder\")                                ❷\n\n    d_inp = tf.keras.Input(\n        shape=(1,), dtype=tf.string, name='d_infer_input'\n    )                                                                    ❸\n    d_state_inp = tf.keras.Input(shape=(256,), name='d_infer_state')     ❹\n\n    d_vectorizer = model.get_layer('d_vectorizer')                       ❺\n    d_vectorized_out = d_vectorizer(d_inp)                               ❺\n\n    d_emb_out = model.get_layer('d_embedding')(d_vectorized_out)         ❻\n\n    d_gru_layer = model.get_layer(\"d_gru\")                               ❼\n    d_gru_layer.return_sequences = False                                 ❽\n    d_gru_out = d_gru_layer(d_emb_out, initial_state=d_state_inp)        ❾\n\n    d_dense1_out = model.get_layer(\"d_dense_1\")(d_gru_out)               ❿\n\n    d_final_out = model.get_layer(\"d_dense_final\")(d_dense1_out)         ⓫\n\n    de_model = tf.keras.models.Model(\n        inputs=[d_inp, d_state_inp], outputs=[d_final_out, d_gru_out]    ⓬\n    )\n\n    return en_model, de_model\n```", "```py\ndef get_vocabularies(save_dir):\n    \"\"\" Load the vocabulary files from a given path\"\"\"\n\n    with open(os.path.join(save_dir, 'en_vocab.json'), 'r') as f:\n        en_vocabulary = json.load(f)\n\n    with open(os.path.join(save_dir, 'de_vocab.json'), 'r') as f:\n        de_vocabulary = json.load(f)\n\n    return en_vocabulary, de_vocabulary\n\nprint(\"Loading vocabularies\")\nen_vocabulary, de_vocabulary = get_vocabularies(\n    os.path.join('models', 'seq2seq_vocab')\n)\n\nprint(\"Loading weights and generating the inference model\")\nen_model, de_model = get_inference_model(os.path.join('models', 'seq2seq'))\nprint(\"\\tDone\")\n```", "```py\ndef generate_new_translation(en_model, de_model, de_vocabulary, sample_en_text):\n    \"\"\" Generate a new translation \"\"\"\n\n    start_token = 'sos'    \n\n    print(\"Input: {}\".format(sample_en_text))                              ❶\n\n    d_state = en_model.predict(np.array([sample_en_text]))                 ❷\n\n    de_word = start_token                                                  ❸\n\n    de_translation = []                                                    ❹\n\n    while de_word != end_token:                                            ❺\n\n        de_pred, d_state = de_model.predict([np.array([de_word]), d_state])❻\n        de_word = de_vocabulary[np.argmax(de_pred[0])]                     ❼\n        de_translation.append(de_word)                                     ❽\n\n    print(\"Translation: {}\\n\".format(' '.join(de_translation)))\n```", "```py\nfor i in range(5):\n    sample_en_text = test_df[\"EN\"].iloc[i]\n    generate_new_translation(en_model, de_model, de_vocabulary, sample_en_text)\n```", "```py\nInput: The pleasure's all mine.\nTranslation: die [UNK] [UNK] mir eos\n\nInput: Tom was asking for it.\nTranslation: tom sprach es zu tun eos\n\nInput: He denied having been involved in the affair.\nTranslation: er [UNK] sich auf das [UNK] [UNK] eos\n\nInput: Is there something in particular that you want to drink?\nTranslation: gibt es etwas [UNK] wenn du etwas [UNK] eos\n\nInput: Don't run. Walk slowly.\nTranslation: [UNK] nicht zu fuß eos\n```", "```py\nd_inp = tf.keras.Input(shape=(1,), dtype=tf.string)       \nd_state_inp = tf.keras.Input(shape=(256,))                \n\nd_vectorized_out = de_vectorizer(d_inp)\nd_emb_out = tf.keras.layers.Embedding(de_vocab+2, 128, mask_zero=True)(d_vectorized_out)\n\nd_gru_out = tf.keras.layers.GRU(256)(d_emb_out, initial_state=d_state_inp)\n\nd_final_out = tf.keras.layers.Dense(\n    de_vocab+2, activation='softmax'\n)(d_gru_out)                    \n\nde_model = tf.keras.models.Model(\n    inputs=[d_inp, d_state_inp], outputs=[d_final_out, d_gru_out]\n)\n```", "```py\nlstm_out, state_h, state_c = tf.keras.layers.LSTM(256, return_state=True)(x)\n```", "```py\ndef vocab_size(ser):\n\n    cnt = Counter(ser.sum())\n    return len(cnt)\n```", "```py\n# The decoder\nen_repeat_out = tf.keras.layers.RepeatVector(de_seq_length)(en_gru_out)\nd_gru_layer = tf.keras.layers.GRU(256, return_sequences=True, name='d_gru')\nd_gru_out = d_gru_layer(en_repeat_out, initial_state=gru_out)\nd_dense_layer_1 = tf.keras.layers.Dense(512, activation='relu', name='d_dense_1')\nd_dense1_out = d_dense_layer_1(d_gru_out)\nd_dense_layer_final = tf.keras.layers.Dense(\n    de_vocab+2, activation='softmax', name='d_dense_final'\n)\nd_final_out = d_dense_layer_final(d_dense1_out)\n\n# Define the full model\nmodel = tf.keras.models.Model(\n    inputs=inp, outputs=d_final_out, name='final_seq2seq'\n)\n```", "```py\nprev_bleu = None\n\nfor epoch in range(epochs):\n\n    bleu_log = []  \n\n    n_train_batches = en_inputs_raw.shape[0]//batch_size\n\n    for i in range(n_train_batches):\n\n        print(\"Training batch {}/{}\".format(i+1, n_train_batches), end='\\r')\n\n        x = [         \n            en_inputs_raw[i*batch_size:(i+1)*batch_size],  \n            de_inputs_raw[i*batch_size:(i+1)*batch_size]\n        ]\n        y = vectorizer(de_labels_raw[i*batch_size:(i+1)*batch_size])\n        model.train_on_batch(x, y)\n        pred_y = model.predict(x)\n\n        bleu_log.append(bleu_metric.calculate_bleu_from_predictions(y, pred_y)) \n\n    mean_bleu = np.mean(bleu_log)\n\n    # The termination criteria\n    if prev_bleu and prev_bleu > mean_bleu:\n        break\n\n    prev_bleu = mean_bleu\n```", "```py\nd_inp = tf.keras.Input(shape=(1,), dtype=tf.string)       \nd_state_h_inp = tf.keras.Input(shape=(256,))                \nd_state_c_inp = tf.keras.Input(shape=(256,))                \n\nd_vectorized_out = de_vectorizer(d_inp)                                         \n\nd_emb_out = tf.keras.layers.Embedding(\n    de_vocab+2, 128, mask_zero=True\n)(d_vectorized_out)                    \n\nd_lstm_out, d_state_h, d_state_c = tf.keras.layers.LSTM(\n 256, return_state=True\n)(d_emb_out, initial_state=[d_state_h_inp, d_state_c_inp])\n\nd_final_out = tf.keras.layers.Dense(\n de_vocab+2, activation='softmax'\n)(d_lstm_out) \n\nde_model = tf.keras.models.Model(\n    inputs=[d_inp, d_state_h_inp, d_state_c_inp], \n    outputs=[d_final_out, d_state_h, d_state_c]              \n)\nde_model.summary()\n```"]