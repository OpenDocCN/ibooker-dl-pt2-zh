- en: 4 Finding meaning in word counts (semantic analysis)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4. 在词频统计中找到含义（语义分析）
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章内容包括
- en: Analyzing semantics (meaning) to create topic vectors
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析语义（含义）以创建主题向量
- en: Semantic search using the semantic similarity between topic vectors
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用主题向量之间的语义相似性进行语义搜索
- en: Scalable semantic analysis and semantic search for large corpora
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可伸缩的语义分析和大型语料库的语义搜索
- en: Using semantic components (topics) as features in your NLP pipeline
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你的NLP管道中使用语义组件（主题）作为特征
- en: Navigating high-dimensional vector spaces
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导航高维向量空间
- en: You have learned quite a few natural language processing tricks. But now may
    be the first time you will be able to do a little bit of "magic." This is the
    first time we talk about a machine being able to understand the *meaning* of words.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经学会了很多自然语言处理的技巧。但现在可能是你第一次能够做一点"魔术"。这是我们第一次讨论机器能够理解单词*含义*的时候。
- en: The TF-IDF vectors (term frequency – inverse document frequency vectors) from
    chapter 3 helped you estimate the importance of words in a chunk of text. You
    used TF-IDF vectors and matrices to tell you how important each word is to the
    overall meaning of a bit of text in a document collection. These TF-IDF "importance"
    scores worked not only for words, but also for short sequences of words, *n*-grams.
    They are great for searching text if you know the exact words or *n*-grams you’re
    looking for. But they also have certain limitations. Often, you need a representation
    that takes not just counts of words, but also their *meaning*.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 第三章的TF-IDF向量（词频 - 逆文档频率向量）帮助你估计了文本块中单词的重要性。你使用TF-IDF向量和矩阵告诉你每个单词对文档集合中一小部分文本的整体含义的重要性。这些TF-IDF"重要性"分数不仅适用于单词，还适用于短序列的单词，*n*-grams。如果你知道确切的单词或*n*-grams，它们对于搜索文本非常有效。但它们也有一定的局限性。通常，你需要一种不仅仅考虑单词计数，还考虑它们*含义*的表示。
- en: Researchers have discovered several ways to represent the meaning of words using
    their co-occurrence with other words. You will learn about some of them, like
    *Latent Semantic Analysis*(LSA) and *Latent Dirichlet Allocation*, in this chapter.
    These methods create *semantic* or *topic* vectors to represent words and documents.
    ^([[1](#_footnotedef_1 "View footnote.")]) You will use your weighted frequency
    scores from TF-IDF vectors, or the bag-of-words (BOW) vectors that you learned
    to create in the previous chapter. These scores and the correlations between them,
    will help you compute the topic "scores" that make up the dimensions of your topic
    vectors.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员发现了几种使用词语与其他词语的共现来表示词语含义的方法。在本章中，你将了解其中一些方法，比如*潜在语义分析*(LSA)和*潜在狄利克雷分配*。这些方法创建了用于表示词语和文档的*语义*或*主题*向量。你将使用TF-IDF向量的加权频率分数，或者上一章学到的词袋（BOW）向量来创建它们。这些分数以及它们之间的相关性，将帮助你计算构成你的主题向量维度的主题"分数"。
- en: Topic vectors will help you do a lot of interesting things. They make it possible
    to search for documents based on their meaning — *semantic search*. Most of the
    time, semantic search returns search results that are much better than keyword
    search. Sometimes semantic search returns documents that are exactly what the
    user is searching for, even when they can’t think of the right words to put in
    the query.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 主题向量将帮助你完成许多有趣的事情。它们使得根据其含义进行文档搜索成为可能 —— *语义搜索*。大多数情况下，语义搜索返回的搜索结果要比关键词搜索好得多。有时，即使用户无法想到正确的查询词，语义搜索也会返回用户正要搜索的文档。
- en: Semantic vectors can also be used to identify the words and *n*-grams that best
    represent the subject (topic) of a statement, document, or corpus (collection
    of documents). And with this vector of words and their relative importance, you
    can provide someone with the most meaningful words for a document — a set of keywords
    that summarizes its meaning.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 语义向量还可以用于识别最能代表语句、文档或语料库（文档集合）主题的单词和*n*-grams。有了这些单词及其相对重要性的向量，你可以为文档提供最有意义的单词
    —— 一组总结其含义的关键词。
- en: And lastly, you will be able to compare any two statements or documents and
    tell how "close" they are in *meaning* to each other.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你将能够比较任意两个语句或文档，并判断它们在*含义*上有多"接近"。
- en: Tip
  id: totrans-13
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: The terms "topic", "semantic", and "meaning" have a similar meaning and are
    often used interchangeably when talking about NLP. In this chapter, you’re learning
    how to build an NLP pipeline that can figure out this kind of synonymy, all on
    its own. Your pipeline might even be able to find the similarity in meaning of
    the phrase "figure it out" and the word "compute". Machines can only "compute"
    meaning, not "figure out" meaning.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: “主题”、“语义”和“含义”这些术语在自然语言处理（NLP）中有着相似的意义，并且在讨论时通常可以互换使用。在本章中，您将学习如何构建一个 NLP 流水线，它可以自行找出这种同义词，甚至能够找出“搞明白”这个短语和“计算”这个词的含义相似之处。机器只能“计算”含义，而不能“搞明白”含义。
- en: You’ll soon see that the linear combinations of words that make up the dimensions
    of your topic vectors are pretty powerful representations of meaning.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 很快您将会发现，构成主题向量维度的单词的线性组合是相当强大的含义表示。
- en: 4.1 From word counts to topic scores
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 从单词计数到主题分数
- en: You know how to count the frequency of words, and to score the importance of
    words in a TF-IDF vector or matrix. But that’s not enough. Let’s look at what
    problems that might create, and how to approach representing the meaning of your
    text rather than just individual term frequencies.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 您知道如何计算单词的频率，并在 TF-IDF 向量或矩阵中评分单词的重要性。但这还不够。让我们来看看这可能会产生哪些问题，以及如何处理文本的含义，而不仅仅是单个术语频率。
- en: 4.1.1 The limitations of TF-IDF vectors and lemmatization
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.1 TF-IDF 向量和词形还原的局限性
- en: TF-IDF vectors count the terms according to their exact spelling in a document.
    So texts that restate the same meaning will have completely different TF-IDF vector
    representations if they spell things differently or use different words. This
    messes up search engines and document similarity comparisons that rely on counts
    of tokens.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF 向量根据文档中单词的确切拼写进行计数。因此，如果文本以不同的方式表达相同的含义，它们的 TF-IDF 向量表示将完全不同，即使它们的拼写不同或使用不同的词汇。这会混淆搜索引擎和依赖于标记计数的文档相似性比较。
- en: In chapter 2, you normalized word endings so that words that differed only in
    their last few characters were collected together under a single token. You used
    normalization approaches such as stemming and lemmatization to create small collections
    of words with similar spellings, and often similar meanings. You labeled each
    of theses small collections of words, with their lemma or stem, and then you processed
    these new tokens instead of the original words.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 2 章中，您对单词的词尾进行了归一化处理，以便将只在最后几个字符上有所不同的单词收集到一个单一的标记下。您使用了标准化方法，如词干提取和词形还原，来创建拼写相似、意思通常也相似的小型单词集合。您为这些单词的每一个小集合进行了标记，标记为它们的词元或词干，然后您处理了这些新标记，而不是原始单词。
- en: This lemmatization approach kept similarly *spelled* ^([[2](#_footnotedef_2
    "View footnote.")]) words together in your analysis, but not necessarily words
    with similar meanings. And it definitely failed to pair up most synonyms. Synonyms
    usually differ in more ways than just the word endings that lemmatization and
    stemming deal with. Even worse, lemmatization and stemming sometimes erroneously
    lump together antonyms, words with opposite meaning.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这种词形还原方法将拼写相似的单词放在了一起进行分析，但并不一定是含义相似的单词。而且，它确实未能将大多数同义词配对起来。同义词通常在很多方面都不同，不仅仅是词形还原和词干提取处理的词尾。更糟糕的是，词形还原和词干提取有时会错误地将反义词（含义相反的单词）归类在一起。
- en: The end result is that two chunks of text that talk about the same thing but
    use different words will not be "close" to each other in your lemmatized TF-IDF
    vector space model. And sometimes two lemmatized TF-IDF vectors that are close
    to each other aren’t similar in meaning at all. Even a state-of-the-art TF-IDF
    similarity score from chapter 3, such as Okapi BM25 or cosine similarity, would
    fail to connect these synonyms or push apart these antonyms. Synonyms with different
    spellings produce TF-IDF vectors that just aren’t close to each other in the vector
    space.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，两个讨论相同事物但使用不同词汇的文本片段在您的词形还原的 TF-IDF 向量空间模型中将不会“接近”彼此。有时，即使两个词形还原的 TF-IDF
    向量彼此接近，它们的含义也完全不相似。即使是第 3 章中的最新的 TF-IDF 相似度评分，如 Okapi BM25 或余弦相似度，也无法连接这些同义词或将这些反义词分开。拼写不同的同义词产生的
    TF-IDF 向量在向量空间中并不接近。
- en: For example, the TF-IDF vector for this chapter in *NLPIA*, the chapter that
    you’re reading right now, may not be at all close to similar-meaning passages
    in university textbooks about latent semantic indexing. But that’s exactly what
    this chapter is about, only we use modern and colloquial terms in this chapter.
    Professors and researchers use more consistent, rigorous language in their textbooks
    and lectures. Plus, the terminology that professors used a decade ago has likely
    evolved with the rapid advances of the past few years. For example, terms such
    "latent semantic *indexing*" were more popular than the term "latent semantic
    analysis" that researchers now use.^([[3](#_footnotedef_3 "View footnote.")])
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在*NLPIA*这一章的 TF-IDF 向量，也就是你现在正在阅读的这一章，可能与关于潜在语义索引的大学教科书中的意思相去甚远。但这正是这一章所讨论的，只是我们在这一章中使用现代和口语化的术语。教授和研究人员在他们的教科书和讲座中使用更一致，更严格的语言。另外，教授们十年前使用的术语可能随着过去几年的快速进展而发生了变化。例如，像"潜在语义
    *索引*"这样的术语比研究人员现在使用的"潜在语义分析"这个术语更受欢迎。^([[3](#_footnotedef_3 "查看脚注。")])
- en: So, different words with similar meaning pose a problem for TF-IDF. But so do
    words that look similar, but mean very different things. Even formal English text
    written by an English professor can’t avoid the fact that most English words have
    multiple meanings, a challenge for any new learner, including machine learners.
    This concept of words with multiple meanings is called *polysemy*.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，具有相似含义的不同单词对 TF-IDF 造成问题。但是，看起来相似但含义完全不同的词也是如此。即使是由英语教授撰写的正式英语文本也无法避免大多数英语单词具有多重含义的事实，这对包括机器学习者在内的任何新学习者来说都是一个挑战。这种具有多重含义的单词的概念称为*多义性*。
- en: Here are some ways in which polysemy can affect the semantics of a word or statement.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些多义词可能影响单词或语句语义的方式。
- en: 'Homonyms — Words with the same spelling and pronunciation, but different meanings
    (For example: *The band was playing old Beatles'' songs. Her hair band was very
    beautiful.* )'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同音异义词 — 拼写和发音相同，但含义不同的词（例如：*乐队正在演奏老披头士的歌曲。她的发带非常漂亮。*）
- en: 'Homographs — Words spelled the same, but with different pronunciations and
    meanings.(For example: *I object to this decision. I don’t recognize this object.*)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同形异义词 — 拼写相同但发音和含义不同的词。（例如：*我反对这个决定。我不认识这个物体。*）
- en: 'Zeugma — Use of two meanings of a word simultaneously in the same sentence
    (For example: *Mr. Pickwick took his hat and his leave.*)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双关语 — 在同一句子中同时使用一个词的两个含义（例如：*皮克威克先生拿起了他的帽子和他的离开。*）
- en: You can see how all of these phenomena will lower TF-IDF’s performance, by making
    the TF-IDF vectors of sentences with similar words but different meanings being
    more similar to each other than they should be. To deal with these challenges,
    we need a more powerful tool.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到所有这些现象会降低 TF-IDF 的性能，因为使具有相似但含义不同的单词的句子的 TF-IDF 向量更相似于彼此，而不应该是这样。为了解决这些挑战，我们需要更强大的工具。
- en: 4.1.2 Topic vectors
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 话题向量
- en: When you do math on TF-IDF vectors, such as addition and subtraction, these
    sums and differences only tell you about the frequency of word uses in the documents
    whose vectors you combined or differenced. That math doesn’t tell you much about
    the "meaning" behind those words. You can compute word-to-word TF-IDF vectors
    (word co-occurrence or correlation vectors) by just multiplying your TF-IDF matrix
    by itself. But "vector reasoning" with these sparse, high-dimensional vectors
    doesn’t work well. You when you add or subtract these vectors from each other,
    they don’t represent an existing concept or word or topic well.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当你对 TF-IDF 向量进行数学运算，比如加法和减法时，这些和差只告诉你组合或差异化的向量所代表的文档中单词使用的频率。这种数学并不告诉你这些词背后的"含义"。你可以通过将
    TF-IDF 矩阵乘以自身来计算单词与单词的 TF-IDF 向量（单词共现或相关向量）。但是用这些稀疏的，高维的向量进行"向量推理"并不奏效。当你将这些向量相加或相减时，它们并不能很好地代表一个现有的概念或单词或主题。
- en: So you need a way to extract some additional information, meaning, from word
    statistics. You need a better estimate of what the words in a document "signify."
    And you need to know what that combination of words **means** in a particular
    document. You’d like to represent that meaning with a vector that’s like a TF-IDF
    vector, only more compact and more meaningful.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你需要一种方法来从单词统计中提取一些额外的信息和意义。你需要更好地估计文档中单词的"意义"。你需要知道在特定文档中那组词的含义**是什么**。你希望用一个类似于
    TF-IDF 向量的向量来表示那个意义，只是更紧凑更有意义。
- en: Essentially, what you’ll be doing when creating these new vectors is defining
    a new space. When you represent words and documents by TF-IDF or bag-of-words
    vectors, you are operating in a space defined by the words, or terms occuring
    in your document. There is a dimension for each term - that’s why you easily reach
    several thousand dimensions. And every term is "orthogonal" to every other term
    - when you multiply the vector signifying one word with a vector representing
    another one, you always get a zero, even if these words are synonyms.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，创建这些新向量时，您将定义一个新的空间。当您用 TF-IDF 或词袋向量表示单词和文档时，您正在一个由文档中出现的单词或术语定义的空间中操作。每个术语都有一个维度
    - 这就是为什么您很容易达到数千个维度。每个术语与每个其他术语都是"正交"的 - 当您将表示一个单词的向量与表示另一个单词的向量相乘时，即使这些单词是同义词，您总是得到一个零。
- en: The process of topic modeling is finding a space with fewer dimensions, so that
    words that are close semantically are aligned to similar dimensions. We will call
    these dimensions *topics*, and the vectors in the new space *topic vectors*. You
    can have as many topics as you like. Your topic space can have just one dimension,
    or thousands of dimensions.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 主题建模的过程是找到一个维度较少的空间，使语义上相近的单词对齐到类似的维度。我们将这些维度称为*主题*，新空间中的向量称为*主题向量*。您可以拥有任意数量的主题。您的主题空间可以只有一个维度，也可以有数千个维度。
- en: You can add and subtract the topic vectors you’ll compute in this chapter just
    like any other vector. Only this time the sums and differences mean a lot more
    than they did with TF-IDF vectors. The distance or *similarity* between topic
    vectors is useful for things like finding documents about similar subjects,or
    for semantic search.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以像处理任何其他向量一样添加和减去您在本章中将计算的主题向量。只不过这一次，和差的含义比 TF-IDF 向量时更重要。主题向量之间的距离或*相似度*对于诸如查找与相似主题相关的文档或语义搜索等事情非常有用。
- en: When you’ll transform your vectors into the new space, you’ll have one document-topic
    vector for each document in your corpus. You’ll have one word-topic vector for
    each word in your lexicon (vocabulary). So you can compute the topic vector for
    any new document by just adding up all its word topic vectors.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当您将您的向量转换到新空间时，您将为语料库中的每个文档有一个文档-主题向量。您的词汇表中每个单词都将有一个词-主题向量。因此，您只需将其所有词-主题向量相加，就可以计算任何新文档的主题向量。
- en: Coming up with a numerical representation of the semantics (meaning) of words
    and sentences can be tricky. This is especially true for "fuzzy" languages like
    English, which has multiple dialects and many different interpretations of the
    same words.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 创造出单词和句子语义（含义）的数值表示可能会有些棘手。这对于"模糊"语言如英语来说尤其如此，因为英语有多种方言，对相同单词有许多不同的解释。
- en: Keeping these challenges in mind, can you imagine how you might squash a TF-IDF
    vector with one million dimensions (terms) down to a vector with 10 or 100 dimensions
    (topics)? This is like identifying the right mix of primary colors to try to reproduce
    the paint color in your apartment so you can cover over those nail holes in your
    wall.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些挑战，您能想象如何将具有一百万维度（术语）的 TF-IDF 向量压缩为具有 10 或 100 维度（主题）的向量吗？这就像确定正确的基本颜色混合以尝试复制您公寓中的油漆颜色，以便您可以覆盖那些墙上的钉孔。
- en: You’d need to find those word dimensions that "belong" together in a topic and
    add their TF-IDF values together to create a new number to represent the amount
    of that topic in a document. You might even weight them for how important they
    are to the topic, how much you’d like each word to contribute to the "mix." And
    you could have negative weights for words that reduce the likelihood that the
    text is about that topic.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要找到那些在一个主题中“属于”一起的单词维度，并将它们的 TF-IDF 值相加，以创建一个新的数字来表示文档中该主题的数量。您甚至可以根据它们对主题的重要性对它们进行加权，以及您希望每个单词对"混合"的贡献有多少。您甚至可以为减少文本与该主题相关的可能性的单词添加负权重。
- en: 4.1.3 Thought experiment
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.3 思想实验
- en: Let’s walk through a thought experiment. Let’s assume you have some TF-IDF vector
    for a particular document and you want to convert that to a topic vector. You
    can think about how much each word contributes to your topics.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进行一个思想实验。假设您有某个特定文档的一些 TF-IDF 向量，并且您希望将其转换为一个主题向量。您可以考虑每个单词对您的主题的贡献。
- en: 'Let’s say you’re processing some sentences about pets in Central Park in New
    York City (NYC). Let’s create three topics: one about pets, one about animals,
    and another about cities. Call these topics "petness", "animalness", and "cityness."
    So your "petness" topic about pets will score words like "cat" and "dog" significantly,
    but probably ignore words like "NYC" and "apple." The "cityness" topic will ignore
    words like "cat" and "dog" but might give a little weight to "apple", just because
    of the "Big Apple" association.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在处理有关纽约市中央公园的宠物的一些句子（NYC）。让我们创建三个主题：一个关于宠物，一个关于动物，另一个关于城市。将这些主题称为“petness”、“animalness”和“cityness”。因此，关于宠物的“petness”主题将显著评分像“猫”和“狗”这样的词汇，但可能忽略像“NYC”和“苹果”这样的词汇。关于城市的“cityness”主题将忽略像“猫”和“狗”这样的词汇，但可能会对“苹果”稍微加权，仅仅因为与“大苹果”有关联。
- en: If you "trained" your topic model like this, without using a computer, just
    your common sense, you might come up with some weights like those in Listing 4.1.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你像这样“训练”你的主题模型，而不使用计算机，只使用你的常识，你可能会得出类似于清单 4.1 中的一些权重。
- en: Listing 4.1 Sample weights for your topics
  id: totrans-44
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 4.1 你的主题的示例权重
- en: '[PRE0]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this thought experiment, we added up the word frequencies that might be indicators
    of each of your topics. We weighted the word frequencies (TF-IDF values) by how
    likely the word is associated with a topic. Note that these weights can be negative
    as well for words that might be talking about something that is in some sense
    the opposite of your topic.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个思想实验中，我们将可能是指示您的每个主题的单词频率相加起来。我们根据单词与主题相关的可能性加权单词频率（TF-IDF 值）。请注意，这些权重也可能是负值，因为某种意义上可能谈论与您的主题相反的内容的单词。
- en: Note this is not a real algorithm, or example implementation, just a thought
    experiment. You’re just trying to figure out how you can teach a machine to think
    like you do. You arbitrarily chose to decompose your words and documents into
    only three topics ("petness", "animalness", and "cityness"). And your vocabulary
    is limited, it has only six words in it.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这不是一个真正的算法或示例实现，只是一个思想实验。你只是试图弄清楚如何教机器像你一样思考。你任意选择将你的单词和文档分解为只有三个主题（“petness”、“animalness”和“cityness”）。并且你的词汇是有限的，只有六个单词。
- en: The next step is to think through how a human might decide mathematically which
    topics and words are connected, and what weights those connections should have.
    Once you decided on three topics to model, you then had to then decide how much
    to weight each word for those topics. You blended words in proportion to each
    other to make your topic "color mix." The topic modeling transformation (color
    mixing recipe) is a 3 x 6 matrix of proportions (weights) connecting three topics
    to six words. You multiplied that matrix by an imaginary 6 x 1 TF-IDF vector to
    get a 3 x 1 topic vector for that document.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是思考一个人可能如何在数学上决定哪些主题和单词是相关的，以及这些连接应该具有什么权重。一旦你决定了三个要建模的主题，你就必须确定为这些主题中的每个单词分配多少权重。你按比例混合单词以使你的主题“颜色混合”。主题建模转换（颜色混合配方）是一个
    3 x 6 的比例（权重）矩阵，将三个主题与六个单词相连。你将该矩阵乘以一个想象中的 6 x 1 的 TF-IDF 向量，以获得该文档的 3 x 1 主题向量。
- en: You made a judgment call that the terms "cat" and "dog" should have similar
    contributions to the "petness" topic (weight of .3). So the two values in the
    upper left of the matrix for your TF-IDF-to-topic transformation are both `.3`.
    Can you imagine ways you might "compute" these proportions with software? Remember,
    you have a bunch of documents your computer can read, tokenize, and count tokens
    for. You have TF-IDF vectors for as many documents as you like. Keep thinking
    about how you might use those counts to compute topic weights for a word as you
    read on.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 你做出了判断，认为术语“猫”和“狗”应该对“petness”主题具有类似的贡献（权重为 0.3）。因此，用于你的 TF-IDF 到主题转换的矩阵左上角的两个值都是
    0.3。你能想象出可能使用软件“计算”这些比例的方法吗？记住，你有一堆计算机可以阅读，标记和计算标记的文档。你可以为尽可能多的文档制作 TF-IDF 向量。继续思考在阅读时如何使用这些计数来计算单词的主题权重。
- en: You decided that the term "NYC" should have a negative weight for the "petness"
    topic. In some sense city names, and proper names in general, and abbreviations,
    and acronyms, share little in common with words about pets. Think about what "sharing
    in common" means for words. Is there something in a TF-IDF matrix that represents
    the meaning that words share in common?
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 你决定术语“NYC”在“petness”主题中应具有负权重。在某种意义上，城市名称，以及一般的专有名称，缩写和首字母缩写，与有关宠物的词汇几乎没有共同之处。思考一下单词“共同之处”在词汇中的含义。TF-IDF
    矩阵中是否有表示单词共同含义的内容？
- en: Notice the small amount of the word "apple" into the topic vector for "city."
    This could be because you’re doing this by hand and we humans know that "NYC"
    and "Big Apple" are often synonymous. Our semantic analysis algorithm will hopefully
    be able to calculate this synonymy between "apple" and "NYC" based on how often
    "apple" and "NYC" occur in the same documents.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 "city" 这个主题向量中有少量的 "apple" 一词。这可能是因为你是手动进行的，而我们人类知道 "NYC" 和 "Big Apple" 经常是同义词。我们的语义分析算法有望能够根据
    "apple" 和 "NYC" 在相同文档中出现的频率来计算出它们之间的同义关系。
- en: As you read the rest of the weighted sums in Listing 4.1, try to guess how we
    came up with these weights for these three topics and six words. You may have
    a different "corpus" in your head than the one we used in our heads. So you may
    have a different opinion about the "appropriate" weights for these words. How
    might you change them? What could you use as an objective measure of these proportions
    (weights)? We’ll answer that question in the next section.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读清单4.1中的加权和之后，试着猜猜我们是如何得出这三个主题和六个单词的权重的。你脑海中可能有一个不同的"语料库"，与我们在头脑中使用的不同。所以你可能对这些单词的"适当"权重有不同的看法。你会如何改变它们？你可以用什么客观的标准来衡量这些比例（权重）？我们将在下一节回答这个问题。
- en: Note
  id: totrans-53
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: We chose a signed weighting of words to produce the topic vectors. This allows
    you to use negative weights for words that are the "opposite" of a topic. And
    because you’re doing this manually by hand, we chose to normalize your topic vectors
    by the easy-to-compute L¹-norm (meaning the sum of absolute values of the vector
    dimensions equals 1). Nonetheless, the real LSA you’ll use later in this chapter
    normalizes topic vectors by the more useful L²-norm. We’ll cover the different
    norms and distances later in this chapter.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了一种有符号的词权重来生成主题向量。这样可以使用负权重来表示与主题相反的词。因为你是手工进行的，我们选择使用易于计算的 L¹-norm （即向量维度的绝对值之和等于1）来对你的主题向量进行归一化。不过，在本章稍后使用的真正的潜在语义分析（LSA）算法则通过更有用的
    L²-norm 对主题向量进行归一化。我们将在本章后面介绍不同的范数和距离。
- en: 'You might have realized in reading these vectors that the relationships between
    words and topics can be "flipped." The 3 x 6 matrix of three topic vectors can
    be transposed to produce topic weights for each word in your vocabulary. These
    vectors of weights would be your word vectors for your six words:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读这些向量时，你可能已经意识到单词和主题之间的关系是可以"翻转"的。一个 3x6 的三个主题向量矩阵可以通过转置来产生你的词汇表中每个单词的主题权重。这些权重向量将成为你六个单词的词向量：
- en: '[PRE1]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: These six word-topic vectors shown in Figure [4.1](#six-lovable-words), one
    for each word, represent the meanings of your six words as 3D vectors.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这六个单词主题向量在图[4.1](#六个可爱的词)中显示，每个单词对应一个向量，表示你的六个词的含义。
- en: Figure 4\. 1\. 3D vectors for a thought experiment about six words about pets
    and NYC
  id: totrans-58
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.1. 关于宠物和纽约市的六个单词的思想实验的3D向量
- en: '![cats and dogs petness 3D](images/cats_and_dogs_petness_3D.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![猫和狗可爱程度的3D图](images/cats_and_dogs_petness_3D.png)'
- en: Earlier, the vectors for each topic, with weights for each word, gave you 6-D
    vectors representing the linear combination of words in your three topics. Now,
    you hand-crafted a way to represent a document by its topics. If you just count
    up occurrences of these six words and multiply them by your weights, you get the
    3D topic vector for any document. And 3D vectors are fun because they’re easy
    for humans to visualize. You can plot them and share insights about your corpus
    or a particular document in graphical form.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，每个主题的向量都带有每个单词的权重，给出了表示三个主题中单词的线性组合的6-D向量。现在，你手工设计了一种通过主题来表示文档的方法。如果你只计算这些六个单词出现的次数，并将它们乘以相应的权重，就可以得到任何文档的3D主题向量。3D向量非常有趣，因为人们可以很容易地进行可视化。你可以将它们绘制出来，并以图形形式分享关于你的语料库或特定文档的见解。
- en: 3D vectors (or any low-dimensional vector space) are great for machine learning
    classification problems, too. An algorithm can slice through the vector space
    with a plane (or hyperplane) to divide up the space into classes.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 3D向量（或任何低维向量空间）对于机器学习分类问题也非常有用。算法可以通过平面（或超平面）在向量空间中划分不同的类别。
- en: The documents in your corpus might use many more words, but this particular
    topic vector model will only be influenced by the use of these six words. You
    could extend this approach to as many words as you had the patience (or an algorithm)
    for. As long as your model only needed to separate documents according to three
    different dimensions or topics, your vocabulary could keep growing as much as
    you like. In the thought experiment, you compressed six dimensions (TF-IDF normalized
    frequencies) into three dimensions (topics).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: This subjective, labor-intensive approach to semantic analysis relies on human
    intuition and common sense to break documents down into topics. Common sense is
    hard to code into an algorithm.^([[4](#_footnotedef_4 "View footnote.")]) And
    obviously this isn’t suitable for a machine learning pipeline. Plus it doesn’t
    scale well to more topics and words.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: So let’s automate this manual procedure. Let’s use an algorithm that doesn’t
    rely on common sense to select topic weights for us.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: If you think about it, each of these weighted sums is just a dot product. And
    three dot products (weighted sums) is just a matrix multiplication, or inner product.
    You multiply a 3 x *n* weight matrix with a TF-IDF vector (one value for each
    word in a document), where *n* is the number of terms in your vocabulary. The
    output of this multiplication is a new 3 x 1 topic vector for that document. What
    you’ve done is "transform" a vector from one vector space (TF-IDFs) to another
    lower-dimensional vector space (topic vectors). Your algorithm should create a
    matrix of *n* terms by *m* topics that you can multiply by a vector of the word
    frequencies in a document to get your new topic vector for that document.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.4 Algorithms for scoring topics
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You still need an algorithmic way to determine these topic vectors, or to derive
    them from vectors you already have - like TF-IDF or bag-of-words (BOW) vectors.
    A machine can’t tell which words belong together or what any of them signify,
    can it? J. R. Firth, a 20th century British linguist, studied the ways you can
    estimate what a word or morpheme ^([[5](#_footnotedef_5 "View footnote.")]) signifies.
    In 1957 he gave you a clue about how to compute the topics for words. Firth wrote:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: You shall know a word by the company it keeps.
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: — J. R. Firth
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '1957'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: So how do you tell the "company" of a word? Well, the most straightforward approach
    would be to count co-occurrences in the same document. And you have exactly what
    you need for that in your BOW and TF-IDF vectors from chapter 3\. This "counting
    co-occurrences" approach led to the development of several algorithms for creating
    vectors to represent the statistics of word usage within documents or sentences.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: In the next sections, you’ll see 2 algorithms for creating these topic vectors.
    The first one, *Latent Semantic Analysis* (LSA), is applied to your TF-IDF matrix
    to gather up words into topics. It works on bag-of-words vectors, too, but TF-IDF
    vectors give slightly better results. LSA optimizes these topics to maintain diversity
    in the topic dimensions; when you use these new topics instead of the original
    words, you still capture much of the meaning (semantics) of the documents. The
    number of topics you need for your model to capture the meaning of your documents
    is far less than the number of words in the vocabulary of your TF-IDF vectors.
    So LSA is often referred to as a dimension reduction technique. LSA reduces the
    number of dimensions you need to capture the meaning of your documents.^([[6](#_footnotedef_6
    "View footnote.")])
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，你将看到两种用于创建这些主题向量的算法。第一种，*潜在语义分析*（LSA），应用于你的TF-IDF矩阵以将单词聚合到主题中。它也适用于词袋向量，但TF-IDF向量的效果略好一些。LSA优化这些主题以保持主题维度的多样性；当你使用这些新主题而不是原始单词时，仍然能捕捉到文档的大部分含义（语义）。你的模型所需的主题数量远远少于TF-IDF向量词汇表中的单词数量，因此LSA通常被称为一种维度缩减技术。LSA减少了你需要捕捉文档含义的维度数量。^([[6](#_footnotedef_6
    "查看脚注。")])
- en: The other algorithm we’ll cover is called *Latent Dirichlet Allocation*, often
    shortened to LDA. Because we use LDA to signify Latent Discriminant Analysis classifier
    in this book, we will shorten Latent Dirichlet Allocation to LDiA instead.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要介绍的另一种算法被称为*潜在狄利克雷分配*，通常缩写为LDA。因为在本书中我们使用LDA来表示潜在判别分析分类器，所以我们将潜在狄利克雷分配简称为LDiA。
- en: LDiA takes the math of LSA in a different direction. It uses a nonlinear statistical
    algorithm to group words together. As a result, it generally takes much longer
    to train than linear approaches like LSA. Often this makes LDiA less practical
    for many real-world applications, and it should rarely be the first approach you
    try. Nonetheless, the statistics of the topics it creates sometimes more closely
    mirror human intuition about words and topics. So LDiA topics will often be easier
    for you to explain to your boss. It is also more useful for some single-document
    problems such as document summarization.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: LDiA将LSA的数学带入了不同的方向。它使用非线性统计算法将单词分组在一起。因此，通常比LSA之类的线性方法需要更长的训练时间。这使得LDiA在许多实际应用中不太实用，并且它很少是你尝试的第一种方法。尽管如此，它创建的主题的统计数据有时更接近人们对单词和主题的直觉。因此，LDiA的主题通常更容易向你的老板解释。它还更适用于一些单文档问题，如文档摘要。
- en: For most classification or regression problems, you’re usually better off using
    LSA. So we explain LSA and its underlying SVD linear algebra first.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数分类或回归问题，通常最好使用LSA。因此，我们首先解释LSA及其基础的SVD线性代数。
- en: '4.2 The challenge: detecting toxicity'
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 挑战：检测毒性
- en: 'To see the power of topic modeling, we’ll try to solve a real problem: recognizing
    toxicity in Wikipedia comments. This is a common NLP task that content and social
    media platforms face nowadays. Throughout this chapter, we’ll work on a dataset
    of Wikipedia discussion comments,^([[7](#_footnotedef_7 "View footnote.")]) which
    we’ll want to classify into two categories - toxic and non-toxic. First, let’s
    load our dataset and take a look at it:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看到主题建模的威力，我们将尝试解决一个真实问题：识别维基百科评论中的有毒性。这是当前内容和社交媒体平台面临的常见自然语言处理任务。在本章中，我们将处理一个维基百科讨论评论的数据集，^([[7](#_footnotedef_7
    "查看脚注。")])我们将希望将其分类为两个类别 - 有毒和无毒。首先，让我们加载数据集并查看一下：
- en: Listing 4.2 The toxic comment dataset
  id: totrans-78
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第4.2节 有毒评论数据集
- en: '[PRE2]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: So you have 5,000 comments, and 650 of them are labeled with the binary class
    label "toxic."
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你有5,000条评论，其中650条被标记为二进制类别标签“有毒”。
- en: Before you dive into all the fancy dimensionality reduction stuff, let’s try
    to solve our classification problem using vector representations for the messages
    that you are already familiar with - TF-IDF. But what *model* will you choose
    to classify the messages? To decide, let’s look at the TF-IDF vectors first.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在你深入了解所有复杂的降维技术之前，让我们尝试使用你已经熟悉的消息的向量表示来解决我们的分类问题 - TF-IDF。但是你会选择什么*模型*来对消息进行分类呢？为了决定，让我们首先看看TF-IDF向量。
- en: Listing 4.3 Creating TF-IDF vectors for the SMS dataset
  id: totrans-82
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第4.3节 为SMS数据集创建TF-IDF向量
- en: '[PRE3]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The spaCy tokenizer gave you 19,169 words in your vocabulary. You have almost
    4 times as many words as you have messages. And you have almost 30 times as many
    words as toxic comments. So your model will not have a lot of information about
    the words that will indicate whether a comment is toxic or not.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: You have already met at least one classifier in this book - Naive Bayes in chapter
    2\. Usually, a Naive Bayes classifier will not work well when your vocabulary
    is much larger than the number of labeled examples in your dataset. So we need
    something different this time.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 Latent Discriminant Analysis classifier
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, we’re going to introduce a classifier that is based on an algorithm
    called Latent Discriminant Analysis (LDA). LDA is one of the most straightforward
    and fast classification models you’ll find, and it requires fewer samples than
    the fancier algorithms.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: The input to LDA will be a labeled data - so we need not just the vectors representing
    the messages, but their class too. In this case, we have two classes - toxic comments
    and non-toxic comments. LDA algorithm uses some math that beyond the scope of
    this book, but in the case of two classes, its implementation is pretty intuitive.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'In essence, this is what LDA algorithm does when faced with a two-class problem:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: It finds a line, or axis, in your vector space, such that if you project all
    the vectors (data points) in the space on that axis, the two classes would be
    as separated as possible.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It projects all the vectors on that line.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It predicts the probability of each vector to belong to one of two classes,
    according to a *cutoff* point between the two classes.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Surprisingly, in the majority of cases, the line that maximizes class separation
    is very close to the line that connects the two *centroids* ^([[8](#_footnotedef_8
    "View footnote.")]) of the clusters representing each class.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Let’s perform manually this approximation of LDA, and see how it does on our
    dataset.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The toxicity score for a particular comment is the length of the shadow (projection)
    of that comment’s vector along the line between the nontoxic and nontoxic comments.
    You compute these projections just as you did for the cosine distance. It is the
    normalized dot product of the comment’s vector with the vector pointing from nontoxic
    comments towards toxic comments. You calculated the toxicity score by projecting
    each TF-IDF vector onto that line between the centroids using the dot product.
    And you did those 5,000 dot products all at once in a "vectorized" numpy operation
    using the `.dot()` method. This can speed things up 100 times compared to a Python
    `for` loop.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: 'You have just one step left in our classification. You need to transform our
    score into the actual class prediction. Ideally, you’d like your score to range
    between 0 and 1, like a probability. Once you have the scores normalized, you
    can deduce the classification from the score based on a cutoff - here, we went
    with a simple 0.5 You can use `sklearn` `MinMaxScaler` to perform the normalization:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: That looks pretty good. Almost all of the first six messages were classified
    correctly. Let’s see how it did on the rest of the training set.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Not bad! 89.5% of the messages were classified correctly with this simple "approximate"
    version of LDA. How will the "full" LDA do? Use SciKit Learn (`sklearn`) to get
    a state-of-the art LDA implementation.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 99.9%! Almost perfect accuracy. Does this mean you don’t need to use fancier
    topic modeling algorithms like Latent Dirichlet Allocation or deep learning? This
    is a trick question. You have probably already figured out the trap. The reason
    for this perfect 99.9% result is that we haven’t separated out a test set. This
    A+ score is on "questions" that the classifier has already "seen." This is like
    getting an exam in school with the exact same questions that you practiced on
    the day before. So this model probably wouldn’t do well in the real world of trolls
    and spammers.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-105
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Note the class methods you used in order to train and make predictions. Every
    model in `sklearn` has those same methods: `fit()` and `predict()`. And all classifier
    models will even have a `predict_proba()` method that gives you the probability
    scores for all the classes. That makes it easier to swap out different model algorithms
    as you try to find the best ones for solving your machine learning problems. That
    way you can save your brainpower for the creative work of an NLP engineer, tuning
    your model hyperparameters to work in the real world.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how our classifier does in a more realistic situation. You’ll split
    your comment dataset into 2 parts - training set and testing set. (As you can
    imagine, there is a function in `sklearn` just for that!) And you’ll see how the
    classifier performs on the messages it wasn’t trained on.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.4 LDA model performance with train-test split
  id: totrans-108
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The training set accuracy for your TF-IDF based model is almost perfect. But
    the test set accuracy is 0.55 - a bit better than flipping a coin. And test set
    accuracy is the only accuracy that counts. This is exactly what topic modeling
    will help you. It will allow you to generalize your models from a small training
    set so it still works well on messages using different combinations of words (but
    similar topics).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Note the `random_state` parameter for the `train_test_split` The algorithm for
    `train_test_split()` are stochastic. So each time you run it you will get different
    results and different accuracy values. If you want to make your pipeline repeatable,
    look for the `seed` argument for these models and dataset splitters. You can set
    the seed to the same value with each run to get reproducible results.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look a bit deeper at how our LDA model did, using a tool called *confusion
    matrix*. The confusion matrix will tell you the number of times the model made
    a mistake. There are two kinds of mistakes, *false positive* mistakes and *false
    negative* mistakes. Mistakes on on examples that were labeled toxic in the test
    set are called "false negatives" because they were falsely labeled as negative
    (nontoxic) and should have been labeled positive (toxic). Mistakes on the nontoxic
    labels in the test set are called "false positives" because they should have been
    labeled negative (nontoxic) but were falsely labeled toxic. Here’s how you do
    it with an `sklearn function`:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Hmmm. It’s not exactly clear what’s going on here. Fortunately, `sklearn` have
    taken into account that you might need a more visual way to present your confusion
    matrix to people, and included a function just for that. Let’s try it out:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: You can see the resulting `matplotlib` plot in Figure [4.2](#confusion-matrix)
    showing the number of incorrect and correct predictions for each of the two labels
    (toxic and non-toxic). Check out this plot to see if you can tell what is wrong
    with your model’s performance.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.2 Confusion matrix of TF-IDF based classifier
  id: totrans-118
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![confusion matrix drawio](images/confusion-matrix_drawio.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
- en: First of all, out of 326 comments in the test set that were actually toxic,
    the model was able to identify correctly only 125 - that’s 38.3%. This measure
    (how many of the instances of the class we’re interested in the model was able
    to identify), is called *recall*, or *sensitivity*. On the other hand, out of
    1038 comments the model labeled as toxic, only 125 are truly toxic comments. So
    the "positive" label is only correct in 12% of cases. This measure is called *precision*.^([[9](#_footnotedef_9
    "View footnote.")])
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: You can already see how precision and recall give us more information than model
    accuracy. For example, imagine that instead of using machine learning models,
    you decided to use a deterministic rule and just label all the comments as non-toxic.
    As about 13% of comments in our dataset are actually toxic, this model will have
    accuracy of 0.87 - much better than the last LDA model you trained! However, its
    recall is going to be 0 - it doesn’t help you at all in our task, which is to
    identify toxic messages.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: You might also realize that there is a tradeoff between these two measures.
    What if you went with another deterministic rule and labeled all the comments
    as toxic? In this case, your recall would be perfect, as you would correctly classify
    all the toxic comments. However, the precision will suffer, as most of the comments
    labeled as toxic will actually be perfectly OK.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Depending our your use case, you might decide to prioritize either precision
    or recall on top of the other. But in a lot of cases, you would want both of them
    to be reasonably good.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: In this case, you’re likely to use the *F[1] score* - a harmonic mean of precision
    and recall. Higher precision and higher recall both lead to a higher F[1] score,
    making it easier to benchmark your models with just one metric.^([[10](#_footnotedef_10
    "View footnote.")])
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: You can learn more about analyzing your classifier’s performance in Appendix
    D. For now, we will just note this model’s F[1] score before we continue on.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Going beyond linear
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LDA is going to serve you well in many circumstances. However, it still has
    some assumptions that will cause the classifier to underperform when these assumptions
    are not fulfilled. For example, LDA assumes that the feature covariance matrices
    for all of your classes are the same. That’s a pretty strong assumption! As a
    result of it, LDA can only learn linear boundaries between classes.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: If you need to relax this assumption, you can use a more general case of LDA
    called *Quadratic Discriminant Analysis*, or QDA. QDA allows different covariance
    matrices for different classes, and estimates each covariance matrix separately.
    That’s why it can learn quadratic, or curved, boundaries.^([[11](#_footnotedef_11
    "View footnote.")]) That makes it more flexible, and helps it to perform better
    in some cases.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Reducing dimensions
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we dive into LSA, let’s take a moment to understand what, conceptually,
    it does to our data. The idea behind LSA’s approach to topic modeling is *dimensionality
    reduction*. As its name suggests, dimensionality reduction is a process in which
    we find a lower-dimensional representation of data that retains as much information
    as possible.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Let’s examine this definition and understand what it means. To give you an intuition,
    let’s step away from NLP for a moment and switch to more visual examples. First,
    what’s a lower-dimension representation of data? Think about taking a 3-D object
    (like your sofa) and representing it in 2-D space. For example, if you shine a
    light behind your sofa in a dark room, its shadow on the wall is its two-dimensional
    representation.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Why would we want such a representation? There might be many reasons. Maybe
    we don’t have the capacity to store or transmit the full data as it is. Or maybe
    we want to visualize our data to understand it better. You already saw the power
    of visualizing your data points and clustering them when we talked about LDA.
    But our brain can’t really work with more than 2 or 3 dimensions - and when we’re
    dealing with real-world data, especially natural language data, our datasets might
    have hundreds or even thousands of dimensions. Dimensionality reduction tools
    like PCA are very useful when we want to simplify and visually map our dataset.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Another important reason is the curse of dimensionality we briefly mentioned
    in chapter 3\. Sparse, multidimensional data is harder to work with, and classifiers
    trained on it are more prone to overfitting. A rule of thumb that’s often used
    by data scientists is that there should be at least 5 records for every dimension.
    We’ve already seen that even for small text datasets, TF-IDF matrices can quickly
    push into 10 or 20 thousand dimensions. And that’s true for many other types of
    data, too.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: From the "sofa shadow" example, you can see that we can build infinitely many
    lower-dimensional representations of the same "original" dataset. But some representations
    are better than others. What does "better" mean in this case? When talking about
    visual data, you can intuitively understand that a representation that allows
    us to recognize the object is better than one that doesn’t. For example, let’s
    take a point cloud that was taken from a 3D scan of a real object, and project
    it onto a two dimensional plane.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: You can see the result in Figure 4.3\. Can you guess what the 3D object was
    from that representation?
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.3 Looking up from below the "belly" at the point cloud for a real object
  id: totrans-136
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![3d pointcloud bottom](images/3d-pointcloud-bottom.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
- en: To continue our "shadows" analogy, think about the midday sun shining above
    the heads of a group of people. Every person’s shadow would be a round patch.
    Would we be able to use those patches to tell who is tall and who is short, or
    which people have long hair? Probably not.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Now you understand that good dimensionality reduction has to do with being able
    to *distinguish* between different objects and data points in the new representation.
    And that not all features, or dimensions, of your data are equally important for
    that process of distinguishing. So there will be features which you can easily
    discard without losing much information. But for some features, losing them will
    significantly hurt your ability to understand your data. And because you are dealing
    with linear algebra here, you don’t only have the option of leaving out or including
    a dimension - you can also combine several dimensions into a smaller dimension
    set that will represent our data in a more concise way. Let’s see how we do that.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1 Enter Principal Component Analysis
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You now know that to find your data’s representation in fewer dimensions, you
    need to find a combination of dimensions that will preserve your ability to distinguish
    between data points. This will let you, for example, to separate them into meaningful
    clusters. To continue the shadow example, a good "shadow representation" allows
    you to see where is the head and where are the legs of your shadow. It does it
    by preserving the difference in height between these objects, rather than "squishing
    them" into one spot like the "midday sun representation" does. On the other hand,
    our body’s "thickness" is roughly uniform from top to bottom - so when you see
    our "flat" shadow representation, that discards that dimension, you don’t lose
    as much information as in the case of discarding our height.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在知道，为了在更少的维度中找到数据的表示，你需要找到一个维度的组合，能够保持你区分数据点的能力。这将使你能够，例如，将它们分成有意义的聚类。继续上面的阴影例子，一个好的“阴影表示”可以让你看到你的阴影的头在哪里，腿在哪里。它通过保持这些对象之间的高度差异来实现，而不是像“中午的太阳表示”那样“压扁”它们到一个点。另一方面，我们身体的“厚度”从顶部到底部大致是均匀的
    - 所以当你看到我们的“扁平”阴影表示时，丢弃了那个维度，你不会像丢弃我们的高度那样丢失太多信息。
- en: In mathematics, this difference is represented by *variance*. And when you think
    about it, it makes sense that features with *more* variance - wider and more frequent
    deviation from the mean - are more helpful for you to tell the difference between
    data points.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学中，这种差异被*方差*所代表。当你想一想的时候，更有*方差*的特征 - 与平均值的偏离更广泛和更频繁 - 对于你来区分数据点更有帮助是有意义的。
- en: 'But you can go beyond looking at each feature by itself. What matters also
    is how the features relate between each other. Here, the visual analogies may
    start to fail you, because the three dimensions we operate in are orthogonal to
    each other, and thus completely unrelated. But let’s think back about your topic
    vectors you saw in the previous part: "animalness", "petness", "cityness". If
    you examine every two features among this triad, it becomes obvious that some
    features are more strongly connected than others. Most words that have a "petness"
    quality to them, also have some "animalness" one. This property of a pair of features,
    or dimensions, is called *covariance*. It is strongly connected to *correlation*,
    which is just covariance normalized by the variance of each feature in the tandem.
    The higher the covariance between features, the more connected they are - and
    therefore, there is more redundancy between the two of them, as you can deduce
    one from the other. It also means that you can find a single dimension that preserves
    most of the variance contained in these two dimensions.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 但你可以超越单独观察每个特征。更重要的是特征之间的关系如何。在这里，视觉类比可能开始让你失望，因为我们操作的三个维度彼此正交，因此完全不相关。但让我们回想一下你在上一部分看到的主题向量：“动物性”，“宠物性”，“都市性”。如果你检查这三元组中的每两个特征，就会显而易见地发现一些特征之间的联系更紧密。大多数具有“宠物性”质量的词，也具有一些“动物性”的质量。一对特征或者维度的这种性质被称为*协方差*。它与*相关性*密切相关，后者仅仅是将每个特征的协方差归一化为这两个特征的差异。特征之间的协方差越高，它们之间的联系就越紧密
    - 因此，它们之间的冗余也更多，因为你可以从一个特征推断出另一个特征。这也意味着你可以找到一个单一的维度，能够保持这两个维度中包含的大部分方差。
- en: To summarize, to reduce the number of dimensions describing our data without
    losing information, you need to find a representation that *maximizes* the variance
    along each of its new axes, while reducing the dependence between the dimensions
    and getting rid of those with high covariance. This is exactly what *Principal
    Component Analysis*, or PCA, does. It finds a set of dimensions along which the
    variance is maximized. These dimensions are *orthonormal* (like *x,y* and *z*
    axes in the physical world) and are called *principal components* - hence the
    name of the method. PCA also allows you to see how much variance each dimension
    "is responsible for", so that you can choose the optimal number of principal components
    that preserve the "essence" of your data set. PCA then takes your data and projects
    it into a new set of coordinates.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Before we dive into how PCA does that, let’s see the magic in action. In the
    following listing, you will use the PCA method of Scikit-Learn to take the same
    3D point cloud you’ve seen on the last page, and find a set of two dimensions
    that will maximize the variance of this point cloud.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.5 PCA Magic
  id: totrans-146
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: When you reduce the dimensionality of 3-D points (vectors) to 2-D it’s like
    taking a picture of that 3-D cloud of points. The result may look like a picture
    on the right or the left of figure 4.4, but it will never tip or twist to a new
    angle. The x-axis (axis 0) will always be aligned along the longest axis of the
    point cloud points, where the points are spread out the most. That’s because PCA
    always finds the dimensions that will maximize the variance and arranges them
    in order of decreasing variance. The direction with the highest variance will
    become the first axis (x). The dimension with the second highest variance becomes
    the second dimension (y-axis) after the PCA transformation. However the *polarity*
    (sign) of these axes is arbitrary. The optimization is free to mirror (flip) the
    vectors (points) around the x or y axis, or both.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.4 Head-to-head horse point clouds upside down
  id: totrans-149
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![two horses](images/two-horses.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
- en: Now that we’ve seen PCA in the works,^([[12](#_footnotedef_12 "View footnote.")])
    let’s take a look at how it finds those principal components that allow us to
    work with our data in fewer dimensions without losing much information.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2 Singular Value Decomposition
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At the heart of PCA is a mathematical procedure called Singular Value Decomposition,
    or SVD.^([[13](#_footnotedef_13 "View footnote.")]) SVD is an algorithm for decomposing
    any matrix into three "factors", three matrices that can be multiplied together
    to recreate the original matrix. This is analogous to finding exactly three integer
    factors for a large integer. But your factors aren’t scalar integers, they are
    2D real matrices with special properties.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say we have our dataset, consisting of *m* n-dimensional points, represented
    by a matrix W. In its full version, this is what SVD of W would look like in math
    notation (assuming *m>n*):'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: W[m] [x] [n] = U[m] [x] [m] S[m] [x] [n] V[n] [x] [n]^T
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: The matrices U, S and V have special properties. U and V matrices are *orthogonal*,
    meaning that if you multiply them by their transposed versions, you’ll get a unit
    matrix. And S is *diagonal*, meaning that it has non-zero values only on its diagonal.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Note the equality sign in this formula. It means that if you multiply U, S and
    V, you’ll get *exactly* W, our original dataset. But you can see that the smallest
    dimension of our matrices is still *n*. Didn’t we want to reduce the number of
    dimensions? That’s why in this chapter, you’ll be using the version of SVD called
    *reduced*, or *truncated* SVD.^([[14](#_footnotedef_14 "View footnote.")]) That
    means that you’ll only looking for the top *p* dimensions that you’re interested
    in.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: At this point you could say "Wait, but couldn’t we do the full SVD and just
    take the dimensions that preserve maximum variance?" And you’ll be completely
    right, we could do it this way! However, there are other benefits to using truncated
    SVD. In particular, there are several algorithms that allow computing truncated
    SVD decomposition of the matrix pretty fast, especially when the matrice is sparse.
    *Sparse matrices* are matrices that have the same value (usually zero or NaN)
    in most of its cells. NLP bag-of-words and TF-IDF matrices are almost always sparse
    because most documents don’t contain many of the words in your vocabulary.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'This is what truncated SVD looks like:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: W[m] [x] [n] ~ U[m] [x] [p] S[p] [x] [p] V[p] [x] [n]^T In this formula, *m*
    and *n* are the number of rows and columns in the original matrix, while *p* is
    the number of dimensions you want to keep. For example, in the horse example,
    *p* would be equal to two if we want to display the horse in a two-dimensional
    space. In the next chapter, when you’ll use SVD for LSA, it will signify the number
    of topics you want to use while analyzing your documents. Of course, *p* needs
    to be lesser than both *m* and *n*.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Note the "approximately equal" sign in this case - because we’re losing dimensions,
    we can’t expect to get exactly the same matrix when we multiply our factors! There’s
    always some loss of information. What we’re gaining, though, is a new way to represent
    our data with fewer dimensions than the original representation. With our horse
    point cloud, we are now able to convey its "horsy" essence without needing to
    print voluminous 3-D plots. And when PCA is used in real life, it can simplify
    hundred- or thousand-dimensional data into short vectors that are easier to analyze,
    cluster and visualize.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: So, what are the matrices U,S and V useful for? For now, we’ll give you a simple
    intuition of their roles. In the next chapter, we’ll dive deeper into these matrices'
    application when we talk about LSA.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with *V^T* - or rather, with its transposed version *V*. *V* matrix’s
    columns are sometimes called *principal directions*, and sometimes *principal
    components*. As Scikit-Learn library, which you utilize in this chapter, uses
    the latter convention, we’re going to stick to it as well.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'You can think of *V* as a "transformer" tool, that is used to map your data
    from the "old" space (its representation in matrix W’s "world") to the new, lower
    dimensional one. Imagine our we added a few more points to our 3D horse point
    cloud and now want to understand where those new points would be in our 2D representation,
    without needing to recalculate the transformation for all the points. To map every
    new point *q* to its location on a 2D plot, all you need to do is to multiply
    it by V:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '`q̂ = q · V`'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: What is, then the meaning of *`U · S`*? With some algebra wizardry, you can
    see that it is actually your data mapped into the new space! Basically, it your
    data points in new, lesser-dimensional representation.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Latent Semantic Analysis
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, we can stop "horsing around" and get back to topic modeling! Let’s
    see how everything you’ve learned about dimensionality reduction, PCA and SVD
    will start making sense when we talk about finding topics and concepts in our
    text data.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with the dataset itself. You’ll use the same comment corpus you
    used for the LDA classifier in section 4.1, and transform it into a matrix using
    TF-IDF. You might remember that the result is called a term-document matrix. This
    name is useful because it gives you an intuition on what the rows and the columns
    of the matrix contain: the rows would be terms, your vocabulary words; and the
    columns will be documents.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s re-run listings 4.1 and 4.2 to get to our TF-IDF matrix again. Before
    diving into LSA, we examined the matrix shape:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: So what do you have here? A 19,169-dimensional dataset, whose "space" is defined
    by the terms in the corpus vocabulary. It’s quite a hassle to work with a single
    vector representation of comments in this space, because there are almost 20,000
    numbers to work with in each vector - longer than the message itself! It’s also
    hard to see if the messages, or sentences inside them, are similar conceptually
    - for example, expressions like "leave this page" and "go away" will have very
    low similarity score, despite their meanings being very close to each other. So
    it’s much harder to cluster and classify documents in the way it’s represented
    in TF-IDF matrix.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Also note that only 650 of your 5,000 messages (13%) are labeled as toxic. So
    you have an unbalanced training set with about 8:1 normal comments to toxic comments
    (personal attacks, obscenity, racial slurs, etc.). And you have a large vocabulary
    - the number of your vocabulary tokens (25172) is greater than the 4,837 messages
    (samples) you have to go on. So you have many more unique words in your vocabulary
    (or lexicon) than you have comments, and even more when you compare it to the
    number of toxic messages. That’s a recipe for overfitting.^([[15](#_footnotedef_15
    "View footnote.")]) Only a few unique words out of your large vocabulary will
    be labeled as "toxic" words in your dataset.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting means that you will "key" off of only a few words in your vocabulary.
    So your toxicity filter will be dependent on those toxic words being somewhere
    in the toxic messages it filters out. Trolls could easily get around your filter
    if they just used synonyms for those toxic words. If your vocabulary doesn’t include
    the new synonyms, then your filter will misclassify those cleverly constructed
    comments as non-toxic.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: And this overfitting problem is an inherent problem in NLP. It’s hard to find
    a labeled natural language dataset that includes all the ways that people might
    say something that should be labeled that way. We couldn’t find an "ideal" set
    of comments that included all the different ways people say toxic and nontoxic
    things. And only a few corporations have the resources to create such a dataset.
    So all the rest of us need to have "countermeasures" for overfitting. You have
    to use algorithms that "generalize" well on just a few examples.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: The primary countermeasure to overfitting is to map this data into a new, lower-dimensional
    space. What will define this new space are weighted combinations of words, or
    *topics*, that your corpus talks about in a variety of ways. Representing your
    messages using topics, rather than specific term frequency, will make your NLP
    pipeline more "general", and allow our spam filter to work on a wider range of
    messages. That’s exactly what LSA does - it finds the new topic "dimensions",
    along which variance is maximized, using SVD method we discovered in the previous
    section.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: These new topics will not necessarily correlate to what we humans think about
    as topics, like "pets" or "history". The machine doesn’t "understand" what combinations
    of words mean, just that they go together. When it sees words like "dog", "cat",
    and "love" together a lot, it puts them together in a topic. It doesn’t know that
    such a topic is likely about "pets." It might include a lot of words like "domesticated"
    and "feral" in that same topic, words that mean the opposite of each other. If
    they occur together a lot in the same documents, LSA will give them high scores
    for the same topics together. It’s up to us humans to look at what words have
    a high weight in each topic and give them a name.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: But you don’t have to give the topics a name to make use of them. Just as you
    didn’t analyze all the 1000s of dimensions in your stemmed bag-of-words vectors
    or TF-IDF vectors from previous chapters, you don’t have to know what all your
    topics "mean." You can still do vector math with these new topic vectors, just
    like you did with TF-IDF vectors. You can add and subtract them and estimate the
    similarity between documents based on their "topic representation", rather than
    "term frequency representation". And these similarity estimates will be more accurate,
    because your new representation actually takes into account the meaning of tokens
    and their co-occurence with other tokens.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.1 Diving into semantic analysis
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: But enough talking about LSA - let’s do some coding! This time, we’re going
    to use another Scikit-Learn tool named `TruncatedSVD` that performs - what a surprise
    - the truncated SVD method that we examined in the previous chapter. We could
    use the `PCA` model you saw in the previous section, but we’ll go with this more
    direct approach - it will allow us to understand better what’s happening "under
    the hood". In addition `TruncatedSVD` is meant to deal with sparse matrices, so
    it will perform better on most TF-IDF and BOW matrices.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: We will start with decreasing the number of dimensions from 9232 to 16 - we’ll
    explain later how we chose that number.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.6 LSA using TruncatedSVD
  id: totrans-182
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: What you have just produced using `fit-transform` method is your document vectors
    in the new representation. Instead of representing your comments with 19,169 frequency
    counts, you represented it with just 16\. This matrix is also called *document-topic*
    matrix. By looking at the columns, you can see how much every topic is "expressed"
    in every comment.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-185
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: How do the methods we use relate to the matrix decomposition process we described?
    You might have realized that what the `fit_transform` method returns is exactly
    \({U \cdot S}\) - your tf-idf vectors projected into the new space. And your V
    matrix is saved inside the `TruncatedSVD` object in the `components_` variable.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: If you want to explore your topics, you can find out how much of each word they
    "contain" by examining the weights of each word, or groups of words, across every
    topic.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: First let’s assign words to all the dimensions in your transformation. You need
    to get them in the right order because your `TFIDFVectorizer` stores the vocabulary
    as a dictionary that maps each term to an index number (column number).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Now you can create a nice Pandas DataFrame containing the weights, with labels
    for all the columns and rows in the right place. But it looks like our first few
    terms are just different combinations of newlines - that’s not very useful!
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Whoever gave you the dataset should have done a better job of cleaning them
    out. Let’s look at a few random terms from your vocabulary using the helpful Pandas
    method `DataFrame.sample()`
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: None of these words looks like "inherently toxic". Let’s look at some words
    that we would intuitively expect to appear in "toxic" comments, and see how much
    weight those words have in different topics.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Topics 2 and 4 appear to be more likely to contain toxic sentiment. And topic
    10 seems to be an "anti-toxic" topic. So words associated with toxicity can have
    a positive impact on some topics and a negative impact on others. There’s no single
    obvious toxic topic number.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: And what `transform` method does is just multiply whatever you pass to it with
    V matrix, which is saved in `components_`. You can check out the code of `TruncatedSVD`
    to see it with your own eyes! ^([[16](#_footnotedef_16 "View footnote.")]) link
    at the top left of the screen.]
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.2 TruncatedSVD or PCA?
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You might be asking yourself now - why did we use Scikit-Learn’s `PCA` class
    in the horse example, but `TruncatedSVD` for topic analysis for our comment dataset?
    Didn’t we say that PCA is based on the SVD algorithm?
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: And you will be right - if you look into the implementation of `PCA` and `TruncatedSVD`
    in `sklearn`, you’ll see that most of the code is similar between the two. They
    both use the same algorithms for SVD decomposition of matrices. However, there
    are several differences that might make each model preferrable for some use cases
    or others.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'The biggest difference is that `TruncatedSVD` does not center the matrix before
    the decomposition, while `PCA` does. What this means is that if you center your
    data before performing TruncatedSVD by subtracting columnwise mean from the matrix,
    like this:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: You’ll get the same results for both methods. Try this yourself by comparing
    the results of `TruncatedSVD` on centered data and of PCA, and see what you get!
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: The fact that the data is being centered is important for some properties of
    Principal Component Analysis,^([[17](#_footnotedef_17 "View footnote.")]) which,
    you might remember, has a lot of applications outside NLP. However, for TF-IDF
    matrices, that are mostly sparse, centering doesn’t always make sense. In most
    cases, centering makes a sparse matrix dense, which causes the model run slower
    and take much more memory. PCA is often used to deal with dense matrices and can
    compute a precise, full-matrix SVD for small matrices. In contrast, `TruncatedSVD`
    already assumes that the input matrix is sparse, and uses the faster approximated,
    randomized methods. So it deals with your TF-IDF data much more efficiently than
    PCA.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.3 How well LSA performs for toxicity detection?
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You’ve spent enough time peering into the topics - let’s see how our model
    performs with lower-dimensional representation of the comments! You’ll use the
    same code we ran in listing 4.3, but will apply it on the new 16-dimensional vectors.
    This time, the classification will go much faster:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Wow, what a difference! The classifier’s accuracy on the training set dropped
    from 99.9% for TF-IDF vectors to 88.1% But the test set accuracy jumped by 33%!
    That’s quite an improvement.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s check the F1 score:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We’ve almost doubled out F1 score, compared to TF-IDF vectors classification!
    Not bad.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Unless you have a perfect memory, by now you must be pretty annoyed by scrolling
    or paging back to the performance of the previous model. And when you’ll be doing
    real-life natural langugae processing, you’ll probably be trying much more models
    than in our toy example. That’s why data scientists record their model parameters
    and performance in a *hyperparameter table*.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Let’s make one of our own. First, recall the classification performance we got
    when we run an LDA classifier on TF-IDF vectors, and save it into our table.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Actually, because you’re going to extract these scores for a few models, it
    might make sense to create a function that does this:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.7 A function that creates a record in hyperparameter table.
  id: totrans-215
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'You can go even further and wrap most of your analysis in a nice function,
    so that you don’t have to copy-paste again:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 4.4.4 Other ways to reduce dimensions
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SVD is by far the most popular way to reduce dimensions of a dataset, making
    LSA your first choice when thinking about topic modeling. However, there are several
    other dimensionality reduction techniques you can also use to achieve the same
    goal. Not all of them are even used in NLP, but it’s good to be aware of them.
    We’ll mention two methods here - *random projection* and *non-negative matrix
    factorization* (NMF).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Random projection is a method to project a high-dimensional data on lower-dimensional
    space, so that the distances between data points are preserved. Its stochastic
    nature makes it easier to run it on parallel machines. It also allows the algorithm
    to use less memory as it doesn’t need to hold all the the data in the memory at
    the same time the way PCA does. And because its computational complexity lower,
    random projections can be occasionally used on datasets with very high dimensions,
    when decomposition speed is an important factor.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, NMF is another matrix factorization method that is similar to SVD,
    but assumes that the data points and the components are all non-negative. It’s
    more commonly used in image processing and computer vision, but can occasionally
    come handy in NLP and topic modeling too.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: In most cases, you’re better off sticking with LSA, which uses the tried and
    true SVD algorithm under the hood.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Latent Dirichlet allocation (LDiA)
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You’ve spent most of this chapter learning about latent semantic analysis and
    various ways to represent the underlying meaning of words and phrases as vectors
    using Scikit-Learn. LSA should be your first choice for most topic modeling, semantic
    search, or content-based recommendation engines.^([[18](#_footnotedef_18 "View
    footnote.")]) Its math is straightforward and efficient, and it produces a linear
    transformation that can be applied to new batches of natural language without
    training and with little loss in accuracy. Here you will learn about a more sophisticated
    algorithm, *Latent Dirichlet Allocation*, or "LDiA" to distinguish it from LDA,
    Linear Discriminant Analysis. LDiA will give you slightly better results in some
    situations.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: LDiA does a lot of the things you did to create your topic models with LSA (and
    SVD under the hood), but unlike LSA, LDiA assumes a Dirichlet distribution of
    word frequencies. It’s more precise about the statistics of allocating words to
    topics than the linear math of LSA.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: LDiA creates a semantic vector space model (like your topic vectors) using an
    approach similar to how your brain worked during the thought experiment earlier
    in the chapter. In your thought experiment, you manually allocated words to topics
    based on how often they occurred together in the same document. The topic mix
    for a document can then be determined by the word mixtures in each topic by which
    topic those words were assigned to. This makes an LDiA topic model much easier
    to understand, because the words assigned to topics and topics assigned to documents
    tend to make more sense than for LSA.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: LDiA assumes that each document is a mixture (linear combination) of some arbitrary
    number of topics that you select when you begin training the LDiA model. LDiA
    also assumes that each topic can be represented by a distribution of words (term
    frequencies). The probability or weight for each of these topics within a document,
    as well as the probability of a word being assigned to a topic, is assumed to
    start with a Dirichlet probability distribution (the *prior* if you remember your
    statistics). This is where the algorithm gets it name.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.1 The LDiA idea
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The LDiA approach was developed in 2000 by geneticists in the UK to help them
    "infer population structure" from sequences of genes.^([[19](#_footnotedef_19
    "View footnote.")]) Stanford Researchers (including Andrew Ng) popularized the
    approach for NLP in 2003.^([[20](#_footnotedef_20 "View footnote.")]) But don’t
    be intimidated by the big names that came up with this approach. We explain the
    key points of it in a few lines of Python shortly. You only need to understand
    it enough to get a feel for what it’s doing (an intuition), so you know what you
    can use it for in your pipeline.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Blei and Ng came up with the idea by flipping your thought experiment on its
    head. They imagined how a machine that could do nothing more than roll dice (generate
    random numbers) could write the documents in a corpus you want to analyze. And
    because you’re only working with bags of words, they cut out the part about sequencing
    those words together to make sense, to write a real document. They just modeled
    the statistics for the mix of words that would become a part of a particular the
    BOW for each document.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: They imagined a machine that only had two choices to make to get started generating
    the mix of words for a particular document. They imagined that the document generator
    chose those words randomly, with some probability distribution over the possible
    choices, like choosing the number of sides of the dice and the combination of
    dice you add together to create a D&D character sheet. Your document "character
    sheet" needs only two rolls of the dice. But the dice are large and there are
    several of them, with complicated rules about how they are combined to produce
    the desired probabilities for the different values you want. You want particular
    probability distributions for the number of words and number of topics so that
    it matches the distribution of these values in real documents analyzed by humans
    for their topics and words.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'The two rolls of the dice represent:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Number of words to generate for the document (Poisson distribution)
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Number of topics to mix together for the document (Dirichlet distribution)
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After it has these two numbers, the hard part begins, choosing the words for
    a document. The imaginary BOW generating machine iterates over those topics and
    randomly chooses words appropriate to that topic until it hits the number of words
    that it had decided the document should contain in step 1\. Deciding the probabilities
    of those words for topics — the appropriateness of words for each topic — is the
    hard part. But once that has been determined, your "bot" just looks up the probabilities
    for the words for each topic from a matrix of term-topic probabilities. If you
    don’t remember what that matrix looks like, glance back at the simple example
    earlier in this chapter.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: So all this machine needs is a single parameter for that Poisson distribution
    (in the dice roll from step 1) that tells it what the "average" document length
    should be, and a couple more parameters to define that Dirichlet distribution
    that sets up the number of topics. Then your document generation algorithm needs
    a term-topic matrix of all the words and topics it likes to use, its vocabulary.
    And it needs a mix of topics that it likes to "talk" about.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Let’s flip the document generation (writing) problem back around to your original
    problem of estimating the topics and words from an existing document. You need
    to measure, or compute, those parameters about words and topics for the first
    two steps. Then you need to compute the term-topic matrix from a collection of
    documents. That’s what LDiA does.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'Blei and Ng realized that they could determine the parameters for steps 1 and
    2 by analyzing the statistics of the documents in a corpus. For example, for step
    1, they could calculate the mean number of words (or *n*-grams) in all the bags
    of words for the documents in their corpus, something like this:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Or, using the sum function:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Keep in mind, you should calculate this statistic directly from your BOWs. You
    need to make sure you’re counting the tokenized and vectorized words in your documents.
    And make sure you’ve applied any stop word filtering, or other normalizations
    before you count up your unique terms. That way your count includes all the words
    in your BOW vector vocabulary (all the *n*-grams you’re counting), but only those
    words that your BOWs use (not stop words, for example). This LDiA algorithm relies
    on a bag-of-words vector space model, unlike LSA that took TF-IDF matrix as input.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: The second parameter you need to specify for an LDiA model, the number of topics,
    is a bit trickier. The number of topics in a particular set of documents can’t
    be measured directly until after you’ve assigned words to those topics. Like *k-means*
    and *KNN* and other clustering algorithms, you must tell it the *k* ahead of time.
    You can guess the number of topics (analogous to the *k* in k-means, the number
    of "clusters") and then check to see if that works for your set of documents.
    Once you’ve told LDiA how many topics to look for, it will find the mix of words
    to put in each topic to optimize its objective function.^([[21](#_footnotedef_21
    "View footnote.")])
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: You can optimize this "hyperparameter" (*k*, the number of topics)^([[22](#_footnotedef_22
    "View footnote.")]) by adjusting it until it works for your application. You can
    automate this optimization if you can measure something about the quality of your
    LDiA language model for representing the meaning of your documents. One "cost
    function" you could use for this optimization is how well (or poorly) that LDiA
    model performs in some classification or regression problem, like sentiment analysis,
    document keyword tagging, or topic analysis. You just need some labeled documents
    to test your topic model or classifier on.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.2 LDiA topic model for comments
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The topics produced by LDiA tend to be more understandable and "explainable"
    to humans. This is because words that frequently occur together are assigned the
    same topics, and humans expect that to be the case. Where LSA tries to keep things
    spread apart that were spread apart to start with, LDiA tries to keep things close
    together that started out close together.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: This may sound like it’s the same thing, but it’s not. The math optimizes for
    different things. Your optimizer has a different objective function so it will
    reach a different objective. To keep close high-dimensional vectors close together
    in the lower-dimensional space, LDiA has to twist and contort the space (and the
    vectors) in nonlinear ways. This is a hard thing to visualize until you do it
    on something 3D and take "projections" of the resultant vectors in 2D.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how that works for a dataset of a few thousand comments, labeled for
    spaminess. First, compute the TF-IDF vectors and then some topics vectors for
    each SMS message (document). We assume the use of only 16 topics (components)
    to classify the spaminess of messages, as before. Keeping the number of topics
    (dimensions) low can help reduce overfitting.^([[23](#_footnotedef_23 "View footnote.")])
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 'LDiA works with raw BOW count vectors rather than normalized TF-IDF vectors.
    You’ve already done this process in Chapter 3:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Let’s double-check that your counts make sense for that first comment labeled
    "comment0":'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We’ll apply Latent Dirichlet Allocation to the count vector matrix in the same
    way we applied LSA to TF-IDF matrix:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: So your model has allocated your 19,169 words (terms) to 16 topics (components).
    Let’s take a look at the first few words and how they’re allocated. Keep in mind
    that your counts and topics will be different from ours. LDiA is a stochastic
    algorithm that relies on the random number generator to make some of the statistical
    decisions it has to make about allocating words to topics. So each time you run
    `sklearn.LatentDirichletAllocation` (or any LDiA algorithm), you will get different
    results unless you set the random seed to a fixed value.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: It looks like the values in LDiA topic vectors have much higher spread than
    LSA topic vectors - there are a lot of near-zero values, but also some really
    big ones. Let’s do the same trick you did when performing topic modeling with
    LSA. We can look at typical "toxic" words and see how pronounced they are in every
    topic.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: That looks very different from the LSA representation of our toxic terms! Looks
    like some terms can have high topic-term weights in some topics, but not others.
    `topic0` and `topic1` seem pretty "indifferent" to toxic terms, while topic 2
    and topic 15 have quite large topic-terms weight for at least 4 or 5 of the toxic
    terms. And `topic14` has a very high weight for the term `hate`!
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see what other terms scored high in this topic. As you saw earlier, because
    we didn’t do any preprocessing to our dataset, a lot of terms are not very interesting.
    Let’s focus on terms that are words, and are longer than 3 letters - that would
    eliminate a lot of the stop words.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: It looks like a lot of the words in the topic have semantic relationship between
    them. Words like "killed" and "hate", or "wicked" and "witch", seem to belong
    in the "toxic" domain. You can see that the allocation of words to topics can
    be rationalized or reasoned about, even with this quick look.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Before you fit your classifier, you need to compute these LDiA topic vectors
    for all your documents (comments). And let’s see how they are different from the
    topic vectors produced by LSA for those same documents.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: You can see that these topics are more cleanly separated. There are a lot of
    zeros in your allocation of topics to messages. This is one of the things that
    makes LDiA topics easier to explain to coworkers when making business decisions
    based on your NLP pipeline results.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: So LDiA topics work well for humans, but what about machines? How will your
    LDA classifier fare with these topics?
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.3 Detecting toxicity with LDiA
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s see how good these LDiA topics are at predicting something useful, such
    as comment toxicity. You’ll use your LDiA topic vectors to train an LDA model
    again (like you did twice - with your TF-IDF vectors and LSA topic vectors). And
    because of the handy function you defined in listing 4.5, you only need a couple
    of lines of code to evaluate your model:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: It looks that the classification performance on 16-topic LDIA vectors is worse
    than on the raw TF-IDF vectors, without topic modeling. Does it mean the LDiA
    is useless in this case? Let’s not give up on it too soon and try to increase
    the number of topics.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '4.5.4 A fairer comparison: 32 LDiA topics'
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s try one more time with more dimensions, more topics. Perhaps LDiA isn’t
    as efficient as LSA so it needs more topics to allocate words to. Let’s try 32
    topics (components).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: That’s nice! Increasing the dimensions for LDIA almost doubled both the precision
    and the recall of the models, and our F1 score looks much better. The larger number
    of topics allows LDIA to be more precise about topics, and, at least for this
    dataset, produce topics that linearly separate better. But the performance of
    these vector representations still is not quite as good as that of LSA. So LSA
    is keeping your comment topic vectors spread out more efficiently, allowing for
    a wider gap between comments to cut with a hyperplane to separate classes.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to explore the source code for the Dirichlet allocation models available
    in both Scikit-Learn as well as `gensim`. They have an API similar to LSA (`sklearn.TruncatedSVD`
    and `gensim.LsiModel`). We’ll show you an example application when we talk about
    summarization in later chapters. Finding explainable topics, like those used for
    summarization, is what LDiA is good at. And it’s not too bad at creating topics
    useful for linear classification.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-277
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'You saw earlier how you can browse the source code of all ''sklearn'' from
    the documentation pages. But there is even a more straightforward method to do
    it from your Python console. You can find the source code path in the `__file__`
    attribute on any Python module, such as `sklearn.__file__`. And in `ipython` (`jupyter
    console`), you can view the source code for any function, class, or object with
    `??`, like `LDA??`:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This won’t work on functions and classes that are extensions, whose source code
    is hidden within a compiled C++ module.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: 4.6 Distance and similarity
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We need to revisit those similarity scores we talked about in chapters 2 and
    3 to make sure your new topic vector space works with them. Remember that you
    can use similarity scores (and distances) to tell how similar or far apart two
    documents are based on the similarity (or distance) of the vectors you used to
    represent them.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: You can use similarity scores (and distances) to see how well your LSA topic
    model agrees with the higher-dimensional TF-IDF model of chapter 3\. You’ll see
    how good your model is at retaining those distances after having eliminated a
    lot of the information contained in the much higher-dimensional bags of words.
    You can check how far away from each other the topic vectors are and whether that’s
    a good representation of the distance between the documents' subject matter. You
    want to check that documents that mean similar things are close to each other
    in your new topic vector space.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: LSA preserves large distances, but it does not always preserve close distances
    (the fine "structure" of the relationships between your documents). The underlying
    SVD algorithm is focused on maximizing the variance between all your documents
    in the new topic vector space.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 'Distances between feature vectors (word vectors, topic vectors, document context
    vectors, and so on) drive the performance of an NLP pipeline, or any machine learning
    pipeline. So what are your options for measuring distance in high-dimensional
    space? And which ones should you chose for a particular NLP problem? Some of these
    commonly used examples may be familiar from geometry class or linear algebra,
    but many others are probably new to you:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: 'Euclidean or Cartesian distance, or root mean square error (RMSE): 2-norm or
    L[2]'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Squared Euclidean distance, sum of squares distance (SSD): L[2]²'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cosine or angular or projected distance: normalized dot product'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Minkowski distance: p-norm or L[p]'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fractional distance, fractional norm: p-norm or L[p] for `0 < p < 1`'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'City block, Manhattan, or taxicab distance, sum of absolute distance (SAD):
    1-norm or L[1]'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jaccard distance, inverse set similarity
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mahalanobis distance
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Levenshtein or edit distance
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The variety of ways to calculate distance is a testament to how important it
    is. In addition to the pairwise distance implementations in Scikit-Learn, many
    others are used in mathematics specialties such as topology, statistics, and engineering.^([[24](#_footnotedef_24
    "View footnote.")]) For reference, here are all the ways you can compute distances
    in the `sklearn.metrics` module: ^([[25](#_footnotedef_25 "View footnote.")])'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.8 Pairwise distances available in `sklearn`
  id: totrans-296
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Distance measures are often computed from similarity measures (scores) and
    vice versa such that distances are inversely proportional to similarity scores.
    Similarity scores are designed to range between 0 and 1\. Typical conversion formulas
    look like this:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'But for distances and similarity scores that range between 0 and 1, like probabilities,
    it’s more common to use a formula like this:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'And cosine distances have their own convention for the range of values they
    use. The angular distance between two vectors is often computed as a fraction
    of the maximum possible angular separation between two vectors, which is 180 degrees
    or `pi` radians.^([[26](#_footnotedef_26 "View footnote.")]) As a result cosine
    similarity and distance are the reciprocal of each other:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Why do we spend so much time talking about distances? In the last section of
    this book, we’ll be talking about semantic search. The idea behind semantic search
    is to find documents that have the highest *semantic similarity* with your search
    query - or the lowest *semantic distance*. In our semantic search application,
    we’ll be using cosine similarity - but as you can see in the last two pages, there
    are multiple ways to measure how similar documents are.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: 4.7 Steering with feedback
  id: totrans-305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All the previous approaches to semantic analysis failed to take into account
    information about the similarity between documents. We created topics that were
    optimal for a generic set of rules. Our unsupervised learning of these models
    for feature (topic) extraction didn’t have any data about how "close" the topic
    vectors should be to each other. We didn’t allow any "feedback" about where the
    topic vectors ended up, or how they were related to each other.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: Steering or "learned distance metrics"^([[27](#_footnotedef_27 "View footnote.")])
    are the latest advancement in dimension reduction and feature extraction. By adjusting
    the distance scores reported to clustering and embedding algorithms, you can "steer"
    your vectors so that they minimize some cost function. In this way you can force
    your vectors to focus on some aspect of the information content that you’re interested
    in.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: In the previous sections about LSA, you ignored all the meta information about
    your documents. For example, with the comments you ignored the sender of the message.
    This is a good indication of topic similarity and could be used to inform your
    topic vector transformation (LSA).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: At Talentpair we experimented with matching resumes to job descriptions using
    the cosine distance between topic vectors for each document. This worked OK. But
    we learned pretty quickly that we got much better results when we started "steering"
    our topic vectors based on feedback from candidates and account managers responsible
    for helping them find a job. Vectors for "good pairings" were steered closer together
    than all the other pairings.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: One way to do this is to calculate the mean difference between your two centroids
    (like you did for LDA) and add some portion of this "bias" to all the resume or
    job description vectors. Doing so should take out the average topic vector difference
    between resumes and job descriptions. Topics such as beer on tap at lunch might
    appear in a job description but never in a resume. Similarly bizarre hobbies,
    such as underwater sculpture, might appear in some resumes but never a job description.
    Steering your topic vectors can help you focus them on the topics you’re interested
    in modeling.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: 4.8 Topic vector power
  id: totrans-311
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With topic vectors, you can do things like compare the meaning of words, documents,
    statements, and corpora. You can find "clusters" of similar documents and statements.
    You’re no longer comparing the distance between documents based merely on their
    word usage. You’re no longer limited to keyword search and relevance ranking based
    entirely on word choice or vocabulary. You can now find documents that are relevant
    to your query, not just a good match for the word statistics themselves.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: This is called "semantic search", not to be confused with the "semantic web."^([[28](#_footnotedef_28
    "View footnote.")]) Semantic search is what strong search engines do when they
    give you documents that don’t contain many of the words in your query, but are
    exactly what you were looking for. These advanced search engines use LSA topic
    vectors to tell the difference between a `Python` package in "The Cheese Shop"
    and a python in a Florida pet shop aquarium, while still recognizing its similarity
    to a "Ruby gem."^([[29](#_footnotedef_29 "View footnote.")])
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: Semantic search gives you a tool for finding and generating meaningful text.
    But our brains are not good at dealing with high-dimensional objects, vectors,
    hyperplanes, hyperspheres, and hypercubes. Our intuitions as developers and machine
    learning engineers breaks down above three dimensions.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: For example, to do a query on a 2D vector, like your lat/lon location on Google
    Maps, you can quickly find all the coffee shops nearby without much searching.
    You can just scan (with your eyes or with code) near your location and spiral
    outward with your search. Alternatively, you can create bigger and bigger bounding
    boxes with your code, checking for longitudes and latitudes within some range
    on each, that’s just for comparison operations and that should find you everything
    nearby.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: However, dividing up a high dimensional vector space (hyperspace) with hyperplanes
    and hypercubes as the boundaries for your search is impractical, and in many cases,
    impossible.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: As Geoffry Hinton says, "To deal with hyperplanes in a 14-dimensional space,
    visualize a 3D space and say 14 to yourself loudly." If you read Abbott’s 1884
    *Flatland* when you were young and impressionable, you might be able to do a little
    bit better than this hand waving. You might even be able to poke your head partway
    out of the window of your 3D world into hyperspace, enough to catch a glimpse
    of that 3D world from the outside. Like in *Flatland*, you used a lot of 2D visualizations
    in this chapter to help you explore the shadows that words in hyperspace leave
    in your 3D world. If you’re anxious to check them out, skip ahead to the section
    showing "scatter matrices" of word vectors. You might also want to glance back
    at the 3D bag-of-words vector in the previous chapter and try to imagine what
    those points would look like if you added just one more word to your vocabulary
    to create a 4-D world of language meaning.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: If you’re taking a moment to think deeply about four dimensions, keep in mind
    that the explosion in complexity you’re trying to wrap your head around is even
    greater than the complexity growth from 2D to 3D and exponentially greater than
    the growth in complexity from a 1D world of numbers to a 2D world of triangles,
    squares, and circles.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: 4.8.1 Semantic search
  id: totrans-319
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When you search for a document based on a word or partial word it contains,
    that’s called *full text search*. This is what search engines do. They break a
    document into chunks (usually words) that can be indexed with an *inverted index*
    like you’d find at the back of a textbook. It takes a lot of bookkeeping and guesswork
    to deal with spelling errors and typos, but it works pretty well.^([[30](#_footnotedef_30
    "View footnote.")])
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: Semantic search is full-text search that takes into account the meaning of the
    words in your query and the documents you’re searching. In this chapter, you’ve
    learned two ways —  LSA and LDiA — to compute topic vectors that capture the semantics
    (meaning) of words and documents in a vector. One of the reasons that latent semantic
    analysis was first called latent semantic *indexing* was because it promised to
    power semantic search with an index of numerical values, like BOW and TF-IDF tables.
    Semantic search was the next big thing in information retrieval.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: But unlike BOW and TF-IDF tables, tables of semantic vectors can’t be easily
    discretized and indexed using traditional inverted index techniques. Traditional
    indexing approaches work with binary word occurrence vectors, discrete vectors
    (BOW vectors), sparse continuous vectors (TF-IDF vectors), and low-dimensional
    continuous vectors (3D GIS data). But high-dimensional continuous vectors, such
    as topic vectors from LSA or LDiA, are a challenge.^([[31](#_footnotedef_31 "View
    footnote.")]) Inverted indexes work for discrete vectors or binary vectors, like
    tables of binary or integer word-document vectors, because the index only needs
    to maintain an entry for each nonzero discrete dimension. Either that value of
    that dimension is present or not present in the referenced vector or document.
    Because TF-IDF vectors are sparse, mostly zero, you don’t need an entry in your
    index for most dimensions for most documents.^([[32](#_footnotedef_32 "View footnote.")])
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: LSA (and LDiA) produce topic vectors that are high-dimensional, continuous,
    and dense (zeros are rare). And the semantic analysis algorithm does not produce
    an efficient index for scalable search. In fact, the curse of dimensionality that
    you talked about in the previous section makes an exact index impossible. The
    "indexing" part of latent semantic indexing was a hope, not a reality, so the
    LSI term is a misnomer. Perhaps that is why LSA has become the more popular way
    to describe semantic analysis algorithms that produce topic vectors.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: One solution to the challenge of high-dimensional vectors is to index them with
    a *locality-sensitive hash* (LSH). A locality-sensitive hash is like a zip code
    (postal code) that designates a region of hyperspace so that it can easily be
    found again later. And like a regular hash, it is discrete and depends only on
    the values in the vector. But even this doesn’t work perfectly once you exceed
    about 12 dimensions. In Figure 4.6, each row represents a topic vector size (dimensionality),
    starting with 2 dimensions and working up to 16 dimensions, like the vectors you
    used earlier for the SMS spam problem.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.5 Semantic search accuracy deteriorates at around 12-D
  id: totrans-325
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![semantic search lsh table](images/semantic-search-lsh-table.png)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
- en: The table shows how good your search results would be if you used locality sensitive
    hashing to index a large number of semantic vectors. Once your vector had more
    than 16 dimensions, you’d have a hard time returning 2 search results that were
    any good.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: So how can you do semantic search on 100-D vectors without an index? You now
    know how to convert the query string into a topic vector using LSA. And you know
    how to compare two vectors for similarity using the cosine similarity score (the
    scalar product, inner product, or dot product) to find the closest match. To find
    precise semantic matches, you need to find all the closest document topic vectors
    to a particular query (search) topic vector. (In the professional lingo, it’s
    called *exhaustive search*.) But if you have *n* documents, you have to do *n*
    comparisons with your query topic vector. That’s a lot of dot products.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: You can vectorize the operation in `numpy` using matrix multiplication, but
    that doesn’t reduce the number of operations, it only makes them 100 times faster.^([[33](#_footnotedef_33
    "View footnote.")]) Fundamentally, exact semantic search still requires *O*(*N*)
    multiplications and additions for each query. So it scales only linearly with
    the size of your corpus. That wouldn’t work for a large corpus, such as Google
    Search or even Wikipedia semantic search.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: The key is to settle for "good enough" rather than striving for a perfect index
    or LSH algorithm for our high-dimensional vectors. There are now several open
    source implementations of some efficient and accurate *approximate nearest neighbors*
    algorithms that use LSH to efficiently implement semantic search. We’ll talk more
    about them in chapter 10.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: Technically these indexing or hashing solutions cannot guarantee that you will
    find all the best matches for your semantic search query. But they can get you
    a good list of close matches almost as fast as with a conventional reverse index
    on a TF-IDF vector or bag-of-words vector, if you’re willing to give up a little
    precision.^([[34](#_footnotedef_34 "View footnote.")])
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: 4.9 Equipping your bot with semantic search
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s use your newly-acquired knowledge in topic modeling to improve the bot
    you started to build in the previous chapter. We’ll focus on the same task - question
    answering.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: Our code is actually going to be pretty similar to your code in chapter 3\.
    We will still use vector representations to find the most similar question in
    our dataset. But this time, our representations are going to be closer to representing
    the meaning of those questions.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s load the question and answer data just like we did in the last
    chapter
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The next step is to represent both the questions and our query as vectors. This
    is where we need to add just a few lines to make our representations semantic.
    Because our qestion dataset is small, we won’t need to apply LSH or any other
    indexing algorithm.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Let’s do a sanity check of our model and make sure it still can answer easy
    questions:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Now, let’s give our model a tougher nut to crack - like the question our previous
    model wasn’t good in dealing with. Can it do better?
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Wow! Looks like the new version of our bot was able to "realize" that 'decrease'
    and 'reduce' have similar meanings. Not only that, it was also able to "understand"
    that 'Logistic Regression' and "LogisticRegression" are very close - such a simple
    step was almost impossible for our TF-IDF model.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: Looks like we’re getting closer to building a truly robust question-answering
    system. We’ll see in the next chapter how we can do even better than topic modeling!
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: 4.10 What’s Next?
  id: totrans-345
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the next chapters, you’ll learn how to fine tune this concept of topic vectors
    so that the vectors associated with words are more precise and useful. To do this
    we first start learning about neural nets. This will improve your pipeline’s ability
    to extract meaning from short texts or even solitary words.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: 4.11 Test yourself
  id: totrans-347
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What preprocessing techniques would you use to prepare your text for more efficient
    topic modeling with LDiA? What about LSA?
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can you think of a dataset/problem where TF-IDF performs better than LSA? What
    about the opposite?
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We mentioned filtering stopwords as a prep process for LDiA. When would this
    filtering be beneficial?
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main challenge of semantic search is that the dense LSA topic vectors are
    not inverse-indexable. Can you explain why it’s so?
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4.12 Summary
  id: totrans-352
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can derive the meaning of your words and documents by analyzing the co-occurence
    of terms in your dataset.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SVD can be used for semantic analysis to decompose and transform TF-IDF and
    BOW vectors into topic vectors.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyperparameter table can be used to compare the performances of different pipelines
    and models.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use LDiA when you need to conduct an explainable topic analysis.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No matter how you create your topic vectors, they can be used for semantic search
    to find documents based on their meaning.
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[1]](#_footnoteref_1) We use the term "topic vector" in this chapter about
    topic analysis and we use the term "word vector" in chapter 6 about Word2vec.
    Formal NLP texts such as the NLP bible by Jurafsky and Martin ( [https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf#chapter.15:](slp3.html))
    use "topic vector." Others, like the authors of Semantic Vector Encoding and Similarity
    Search ( [https://arxiv.org/pdf/1706.00957.pdf:](pdf.html)), use the term "semantic
    vector."'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: '[[2]](#_footnoteref_2) Both stemming and lemmatization remove or alter the
    word endings and prefixes, the last few characters of a word. Edit-distance calculations
    are better for identifying similarly spelled (or misspelled) words'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: '[[3]](#_footnoteref_3) I love Google Ngram Viewer for visualizing trends like
    this one: ( [http://mng.bz/ZoyA](mng.bz.html)).'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: '[[4]](#_footnoteref_4) Doug Lenat at Stanford is trying to do just that, code
    common sense into an algorithm. See the Wired Magazine article "Doug Lenat’s Artificial
    Intelligence Common Sense Engine" ( [https://www.wired.com/2016/03/doug-lenat-artificial-intelligence-common-sense-engine](03.html)).'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: '[[5]](#_footnoteref_5) A *morpheme* is the smallest meaningful parts of a word.
    See Wikipedia article "Morpheme" ( [https://en.wikipedia.org/wiki/Morpheme](wiki.html)).'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: '[[6]](#_footnoteref_6) The wikipedia page for topic models has a video that
    shows the intuition behind LSA. [http://mng.bz/VRYW](mng.bz.html)'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: '[[7]](#_footnoteref_7) The larger version of this dataset was a basis for a
    Kaggle competition in 2017( [https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge](c.html)),
    and was released by Jigsaw under CC0 license.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: '[[8]](#_footnoteref_8) A centroid of a cluster is a point whose coordinates
    are the average of the coordinates of all the points in that cluster.'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: '[[9]](#_footnoteref_9) To gain some more intuition about precision and recall,
    Wikipedia’s article ( [https://en.wikipedia.org/wiki/Precision_and_recall](wiki.html))
    has some good visuals.'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: '[[10]](#_footnoteref_10) You can read more about the reasons *not* to use F
    [1] score in some cases, and about alternative metrics in the Wikipedia article:
    [https://en.wikipedia.org/wiki/F-score](wiki.html)'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '[[11]](#_footnoteref_11) You can see a visual example of the two estimator’s
    in Scikit-Learn’s documentation: [https://scikit-learn.org/dev/modules/lda_qda.html](modules.html)'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: '[[12]](#_footnoteref_12) To understand dimensionality reduction more in depth,
    check out this great 4-part post series by Hussein Abdullatif: [http://mng.bz/RlRv](mng.bz.html)'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: '[[13]](#_footnoteref_13) There are actually two main ways to perform PCA; you
    can dig into the Wikipedia article for PCA ( [https://en.wikipedia.org/wiki/Principal_component_analysis#Singular_value_decomposition](wiki.html))
    and see what the other method is and how the two basically yield an almost identical
    result.'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '[[14]](#_footnoteref_14) To learn more about *Full* SVD and its other applications,
    you can read the Wikipedia article: [https://en.wikipedia.org/wiki/Singular_value_decomposition](wiki.html)'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: '[[15]](#_footnoteref_15) See the web page titled "Overfitting - Wikipedia"
    ( [https://en.wikipedia.org/wiki/Overfitting](wiki.html)).'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: '[[16]](#_footnoteref_16) You can access the code of any Scikit-Learn function
    by clicking the [source'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: '[[17]](#_footnoteref_17) You can dig into the maths of PCA here: [https://en.wikipedia.org/wiki/Principal_component_analysis](wiki.html)'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: '[[18]](#_footnoteref_18) A 2015 comparison of content-based movie recommendation
    algorithms by Sonia Bergamaschi and Laura Po found LSA to be approximately twice
    as accurate as LDiA. See "Comparing LDA and LSA Topic Models for Content-Based
    Movie Recommendation Systems" by Sonia Bergamaschi and Laura Po ( [https://www.dbgroup.unimo.it/~po/pubs/LNBI_2015.pdf](pubs.html)).'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: '[[19]](#_footnoteref_19) "Jonathan K. Pritchard, Matthew Stephens, Peter Donnelly,
    Inference of Population Structure Using Multilocus Genotype Data" [http://www.genetics.org/content/155/2/945](2.html)'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: '[[20]](#_footnoteref_20) See the PDF titled "Latent Dirichlet Allocation" by
    David M. Blei, Andrew Y. Ng, and Michael I. Jordan ( [http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf](blei03a.html)).'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: '[[21]](#_footnoteref_21) You can learn more about the particulars of the LDiA
    objective function here in the original paper "Online Learning for Latent Dirichlet
    Allocation" by Matthew D. Hoffman, David M. Blei, and Francis Bach ( [https://www.di.ens.fr/%7Efbach/mdhnips2010.pdf](%7Efbach.html)).'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: '[[22]](#_footnoteref_22) The symbol used by Blei and Ng for this parameter
    was *theta* rather than *k*'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: '[[23]](#_footnoteref_23) See Appendix D if you want to learn more about why
    overfitting is a bad thing and how *generalization* can help.'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: '[[24]](#_footnoteref_24) See Math.NET Numerics for more distance metrics (
    [https://numerics.mathdotnet.com/Distance.html](numerics.mathdotnet.com.html)).'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: '[[25]](#_footnoteref_25) See the documentation for sklearn.metrics ( [https://scikit-learn.org/stable/modules/generated/sklearn.metrics.DistanceMetric.html](generated.html)).'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: '[[26]](#_footnoteref_26) See the web page titled "Cosine similarity - Wikipedia"
    ( [https://en.wikipedia.org/wiki/Cosine_similarity](wiki.html)).'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: '[[27]](#_footnoteref_27) See the web page titled "eccv spgraph" ( [http://users.cecs.anu.edu.au/~sgould/papers/eccv14-spgraph.pdf](papers.html)).'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: '[[28]](#_footnoteref_28) The semantic web is the practice of structuring natural
    language text with the use of tags in an HTML document so that the hierarchy of
    tags and their content provide information about the relationships (web of connections)
    between elements (text, images, videos) on a web page.'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: '[[29]](#_footnoteref_29) Ruby is a programming language whose packages are
    called `gems`.'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: '[[30]](#_footnoteref_30) A full-text index in a database like PostgreSQL is
    usually based on trigrams of characters, to deal with spelling errors and text
    that doesn’t parse into words.'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '[[31]](#_footnoteref_31) Clustering high-dimensional data is equivalent to
    discretizing or indexing high-dimensional data with bounding boxes and is described
    in the Wikipedia article "Clustering high dimensional data" ( [https://en.wikipedia.org/wiki/Clustering_high-dimensional_data](wiki.html)).'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: '[[32]](#_footnoteref_32) See the web page titled "Inverted index - Wikipedia"
    ( [https://en.wikipedia.org/wiki/Inverted_index](wiki.html)).'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: '[[33]](#_footnoteref_33) Vectorizing your Python code, especially doubly-nested
    `for` loops for pairwise distance calculations can speed your code by almost 100-fold.
    See Hacker Noon article "Vectorizing the Loops with Numpy" ( [https://hackernoon.com/speeding-up-your-code-2-vectorizing-the-loops-with-numpy-e380e939bed3](hackernoon.com.html)).'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: '[[34]](#_footnoteref_34) If you want to learn about faster ways to find a high-dimensional
    vector’s nearest neighbors, check out appendix F, or just use the Spotify `annoy`
    package to index your topic vectors.'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
