- en: 4 Finding meaning in word counts (semantic analysis)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4. 在词频统计中找到含义（语义分析）
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章内容包括
- en: Analyzing semantics (meaning) to create topic vectors
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析语义（含义）以创建主题向量
- en: Semantic search using the semantic similarity between topic vectors
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用主题向量之间的语义相似性进行语义搜索
- en: Scalable semantic analysis and semantic search for large corpora
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可伸缩的语义分析和大型语料库的语义搜索
- en: Using semantic components (topics) as features in your NLP pipeline
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你的NLP管道中使用语义组件（主题）作为特征
- en: Navigating high-dimensional vector spaces
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导航高维向量空间
- en: You have learned quite a few natural language processing tricks. But now may
    be the first time you will be able to do a little bit of "magic." This is the
    first time we talk about a machine being able to understand the *meaning* of words.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经学会了很多自然语言处理的技巧。但现在可能是你第一次能够做一点"魔术"。这是我们第一次讨论机器能够理解单词*含义*的时候。
- en: The TF-IDF vectors (term frequency – inverse document frequency vectors) from
    chapter 3 helped you estimate the importance of words in a chunk of text. You
    used TF-IDF vectors and matrices to tell you how important each word is to the
    overall meaning of a bit of text in a document collection. These TF-IDF "importance"
    scores worked not only for words, but also for short sequences of words, *n*-grams.
    They are great for searching text if you know the exact words or *n*-grams you’re
    looking for. But they also have certain limitations. Often, you need a representation
    that takes not just counts of words, but also their *meaning*.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 第三章的TF-IDF向量（词频 - 逆文档频率向量）帮助你估计了文本块中单词的重要性。你使用TF-IDF向量和矩阵告诉你每个单词对文档集合中一小部分文本的整体含义的重要性。这些TF-IDF"重要性"分数不仅适用于单词，还适用于短序列的单词，*n*-grams。如果你知道确切的单词或*n*-grams，它们对于搜索文本非常有效。但它们也有一定的局限性。通常，你需要一种不仅仅考虑单词计数，还考虑它们*含义*的表示。
- en: Researchers have discovered several ways to represent the meaning of words using
    their co-occurrence with other words. You will learn about some of them, like
    *Latent Semantic Analysis*(LSA) and *Latent Dirichlet Allocation*, in this chapter.
    These methods create *semantic* or *topic* vectors to represent words and documents.
    ^([[1](#_footnotedef_1 "View footnote.")]) You will use your weighted frequency
    scores from TF-IDF vectors, or the bag-of-words (BOW) vectors that you learned
    to create in the previous chapter. These scores and the correlations between them,
    will help you compute the topic "scores" that make up the dimensions of your topic
    vectors.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员发现了几种使用词语与其他词语的共现来表示词语含义的方法。在本章中，你将了解其中一些方法，比如*潜在语义分析*(LSA)和*潜在狄利克雷分配*。这些方法创建了用于表示词语和文档的*语义*或*主题*向量。你将使用TF-IDF向量的加权频率分数，或者上一章学到的词袋（BOW）向量来创建它们。这些分数以及它们之间的相关性，将帮助你计算构成你的主题向量维度的主题"分数"。
- en: Topic vectors will help you do a lot of interesting things. They make it possible
    to search for documents based on their meaning — *semantic search*. Most of the
    time, semantic search returns search results that are much better than keyword
    search. Sometimes semantic search returns documents that are exactly what the
    user is searching for, even when they can’t think of the right words to put in
    the query.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 主题向量将帮助你完成许多有趣的事情。它们使得根据其含义进行文档搜索成为可能 —— *语义搜索*。大多数情况下，语义搜索返回的搜索结果要比关键词搜索好得多。有时，即使用户无法想到正确的查询词，语义搜索也会返回用户正要搜索的文档。
- en: Semantic vectors can also be used to identify the words and *n*-grams that best
    represent the subject (topic) of a statement, document, or corpus (collection
    of documents). And with this vector of words and their relative importance, you
    can provide someone with the most meaningful words for a document — a set of keywords
    that summarizes its meaning.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 语义向量还可以用于识别最能代表语句、文档或语料库（文档集合）主题的单词和*n*-grams。有了这些单词及其相对重要性的向量，你可以为文档提供最有意义的单词
    —— 一组总结其含义的关键词。
- en: And lastly, you will be able to compare any two statements or documents and
    tell how "close" they are in *meaning* to each other.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你将能够比较任意两个语句或文档，并判断它们在*含义*上有多"接近"。
- en: Tip
  id: totrans-13
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: The terms "topic", "semantic", and "meaning" have a similar meaning and are
    often used interchangeably when talking about NLP. In this chapter, you’re learning
    how to build an NLP pipeline that can figure out this kind of synonymy, all on
    its own. Your pipeline might even be able to find the similarity in meaning of
    the phrase "figure it out" and the word "compute". Machines can only "compute"
    meaning, not "figure out" meaning.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: “主题”、“语义”和“含义”这些术语在自然语言处理（NLP）中有着相似的意义，并且在讨论时通常可以互换使用。在本章中，您将学习如何构建一个 NLP 流水线，它可以自行找出这种同义词，甚至能够找出“搞明白”这个短语和“计算”这个词的含义相似之处。机器只能“计算”含义，而不能“搞明白”含义。
- en: You’ll soon see that the linear combinations of words that make up the dimensions
    of your topic vectors are pretty powerful representations of meaning.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 很快您将会发现，构成主题向量维度的单词的线性组合是相当强大的含义表示。
- en: 4.1 From word counts to topic scores
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 从单词计数到主题分数
- en: You know how to count the frequency of words, and to score the importance of
    words in a TF-IDF vector or matrix. But that’s not enough. Let’s look at what
    problems that might create, and how to approach representing the meaning of your
    text rather than just individual term frequencies.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 您知道如何计算单词的频率，并在 TF-IDF 向量或矩阵中评分单词的重要性。但这还不够。让我们来看看这可能会产生哪些问题，以及如何处理文本的含义，而不仅仅是单个术语频率。
- en: 4.1.1 The limitations of TF-IDF vectors and lemmatization
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.1 TF-IDF 向量和词形还原的局限性
- en: TF-IDF vectors count the terms according to their exact spelling in a document.
    So texts that restate the same meaning will have completely different TF-IDF vector
    representations if they spell things differently or use different words. This
    messes up search engines and document similarity comparisons that rely on counts
    of tokens.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF 向量根据文档中单词的确切拼写进行计数。因此，如果文本以不同的方式表达相同的含义，它们的 TF-IDF 向量表示将完全不同，即使它们的拼写不同或使用不同的词汇。这会混淆搜索引擎和依赖于标记计数的文档相似性比较。
- en: In chapter 2, you normalized word endings so that words that differed only in
    their last few characters were collected together under a single token. You used
    normalization approaches such as stemming and lemmatization to create small collections
    of words with similar spellings, and often similar meanings. You labeled each
    of theses small collections of words, with their lemma or stem, and then you processed
    these new tokens instead of the original words.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 2 章中，您对单词的词尾进行了归一化处理，以便将只在最后几个字符上有所不同的单词收集到一个单一的标记下。您使用了标准化方法，如词干提取和词形还原，来创建拼写相似、意思通常也相似的小型单词集合。您为这些单词的每一个小集合进行了标记，标记为它们的词元或词干，然后您处理了这些新标记，而不是原始单词。
- en: This lemmatization approach kept similarly *spelled* ^([[2](#_footnotedef_2
    "View footnote.")]) words together in your analysis, but not necessarily words
    with similar meanings. And it definitely failed to pair up most synonyms. Synonyms
    usually differ in more ways than just the word endings that lemmatization and
    stemming deal with. Even worse, lemmatization and stemming sometimes erroneously
    lump together antonyms, words with opposite meaning.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这种词形还原方法将拼写相似的单词放在了一起进行分析，但并不一定是含义相似的单词。而且，它确实未能将大多数同义词配对起来。同义词通常在很多方面都不同，不仅仅是词形还原和词干提取处理的词尾。更糟糕的是，词形还原和词干提取有时会错误地将反义词（含义相反的单词）归类在一起。
- en: The end result is that two chunks of text that talk about the same thing but
    use different words will not be "close" to each other in your lemmatized TF-IDF
    vector space model. And sometimes two lemmatized TF-IDF vectors that are close
    to each other aren’t similar in meaning at all. Even a state-of-the-art TF-IDF
    similarity score from chapter 3, such as Okapi BM25 or cosine similarity, would
    fail to connect these synonyms or push apart these antonyms. Synonyms with different
    spellings produce TF-IDF vectors that just aren’t close to each other in the vector
    space.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，两个讨论相同事物但使用不同词汇的文本片段在您的词形还原的 TF-IDF 向量空间模型中将不会“接近”彼此。有时，即使两个词形还原的 TF-IDF
    向量彼此接近，它们的含义也完全不相似。即使是第 3 章中的最新的 TF-IDF 相似度评分，如 Okapi BM25 或余弦相似度，也无法连接这些同义词或将这些反义词分开。拼写不同的同义词产生的
    TF-IDF 向量在向量空间中并不接近。
- en: For example, the TF-IDF vector for this chapter in *NLPIA*, the chapter that
    you’re reading right now, may not be at all close to similar-meaning passages
    in university textbooks about latent semantic indexing. But that’s exactly what
    this chapter is about, only we use modern and colloquial terms in this chapter.
    Professors and researchers use more consistent, rigorous language in their textbooks
    and lectures. Plus, the terminology that professors used a decade ago has likely
    evolved with the rapid advances of the past few years. For example, terms such
    "latent semantic *indexing*" were more popular than the term "latent semantic
    analysis" that researchers now use.^([[3](#_footnotedef_3 "View footnote.")])
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在*NLPIA*这一章的 TF-IDF 向量，也就是你现在正在阅读的这一章，可能与关于潜在语义索引的大学教科书中的意思相去甚远。但这正是这一章所讨论的，只是我们在这一章中使用现代和口语化的术语。教授和研究人员在他们的教科书和讲座中使用更一致，更严格的语言。另外，教授们十年前使用的术语可能随着过去几年的快速进展而发生了变化。例如，像"潜在语义
    *索引*"这样的术语比研究人员现在使用的"潜在语义分析"这个术语更受欢迎。^([[3](#_footnotedef_3 "查看脚注。")])
- en: So, different words with similar meaning pose a problem for TF-IDF. But so do
    words that look similar, but mean very different things. Even formal English text
    written by an English professor can’t avoid the fact that most English words have
    multiple meanings, a challenge for any new learner, including machine learners.
    This concept of words with multiple meanings is called *polysemy*.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，具有相似含义的不同单词对 TF-IDF 造成问题。但是，看起来相似但含义完全不同的词也是如此。即使是由英语教授撰写的正式英语文本也无法避免大多数英语单词具有多重含义的事实，这对包括机器学习者在内的任何新学习者来说都是一个挑战。这种具有多重含义的单词的概念称为*多义性*。
- en: Here are some ways in which polysemy can affect the semantics of a word or statement.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些多义词可能影响单词或语句语义的方式。
- en: 'Homonyms — Words with the same spelling and pronunciation, but different meanings
    (For example: *The band was playing old Beatles'' songs. Her hair band was very
    beautiful.* )'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同音异义词 — 拼写和发音相同，但含义不同的词（例如：*乐队正在演奏老披头士的歌曲。她的发带非常漂亮。*）
- en: 'Homographs — Words spelled the same, but with different pronunciations and
    meanings.(For example: *I object to this decision. I don’t recognize this object.*)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同形异义词 — 拼写相同但发音和含义不同的词。（例如：*我反对这个决定。我不认识这个物体。*）
- en: 'Zeugma — Use of two meanings of a word simultaneously in the same sentence
    (For example: *Mr. Pickwick took his hat and his leave.*)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双关语 — 在同一句子中同时使用一个词的两个含义（例如：*皮克威克先生拿起了他的帽子和他的离开。*）
- en: You can see how all of these phenomena will lower TF-IDF’s performance, by making
    the TF-IDF vectors of sentences with similar words but different meanings being
    more similar to each other than they should be. To deal with these challenges,
    we need a more powerful tool.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到所有这些现象会降低 TF-IDF 的性能，因为使具有相似但含义不同的单词的句子的 TF-IDF 向量更相似于彼此，而不应该是这样。为了解决这些挑战，我们需要更强大的工具。
- en: 4.1.2 Topic vectors
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 话题向量
- en: When you do math on TF-IDF vectors, such as addition and subtraction, these
    sums and differences only tell you about the frequency of word uses in the documents
    whose vectors you combined or differenced. That math doesn’t tell you much about
    the "meaning" behind those words. You can compute word-to-word TF-IDF vectors
    (word co-occurrence or correlation vectors) by just multiplying your TF-IDF matrix
    by itself. But "vector reasoning" with these sparse, high-dimensional vectors
    doesn’t work well. You when you add or subtract these vectors from each other,
    they don’t represent an existing concept or word or topic well.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当你对 TF-IDF 向量进行数学运算，比如加法和减法时，这些和差只告诉你组合或差异化的向量所代表的文档中单词使用的频率。这种数学并不告诉你这些词背后的"含义"。你可以通过将
    TF-IDF 矩阵乘以自身来计算单词与单词的 TF-IDF 向量（单词共现或相关向量）。但是用这些稀疏的，高维的向量进行"向量推理"并不奏效。当你将这些向量相加或相减时，它们并不能很好地代表一个现有的概念或单词或主题。
- en: So you need a way to extract some additional information, meaning, from word
    statistics. You need a better estimate of what the words in a document "signify."
    And you need to know what that combination of words **means** in a particular
    document. You’d like to represent that meaning with a vector that’s like a TF-IDF
    vector, only more compact and more meaningful.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你需要一种方法来从单词统计中提取一些额外的信息和意义。你需要更好地估计文档中单词的"意义"。你需要知道在特定文档中那组词的含义**是什么**。你希望用一个类似于
    TF-IDF 向量的向量来表示那个意义，只是更紧凑更有意义。
- en: Essentially, what you’ll be doing when creating these new vectors is defining
    a new space. When you represent words and documents by TF-IDF or bag-of-words
    vectors, you are operating in a space defined by the words, or terms occuring
    in your document. There is a dimension for each term - that’s why you easily reach
    several thousand dimensions. And every term is "orthogonal" to every other term
    - when you multiply the vector signifying one word with a vector representing
    another one, you always get a zero, even if these words are synonyms.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，创建这些新向量时，您将定义一个新的空间。当您用 TF-IDF 或词袋向量表示单词和文档时，您正在一个由文档中出现的单词或术语定义的空间中操作。每个术语都有一个维度
    - 这就是为什么您很容易达到数千个维度。每个术语与每个其他术语都是"正交"的 - 当您将表示一个单词的向量与表示另一个单词的向量相乘时，即使这些单词是同义词，您总是得到一个零。
- en: The process of topic modeling is finding a space with fewer dimensions, so that
    words that are close semantically are aligned to similar dimensions. We will call
    these dimensions *topics*, and the vectors in the new space *topic vectors*. You
    can have as many topics as you like. Your topic space can have just one dimension,
    or thousands of dimensions.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 主题建模的过程是找到一个维度较少的空间，使语义上相近的单词对齐到类似的维度。我们将这些维度称为*主题*，新空间中的向量称为*主题向量*。您可以拥有任意数量的主题。您的主题空间可以只有一个维度，也可以有数千个维度。
- en: You can add and subtract the topic vectors you’ll compute in this chapter just
    like any other vector. Only this time the sums and differences mean a lot more
    than they did with TF-IDF vectors. The distance or *similarity* between topic
    vectors is useful for things like finding documents about similar subjects,or
    for semantic search.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以像处理任何其他向量一样添加和减去您在本章中将计算的主题向量。只不过这一次，和差的含义比 TF-IDF 向量时更重要。主题向量之间的距离或*相似度*对于诸如查找与相似主题相关的文档或语义搜索等事情非常有用。
- en: When you’ll transform your vectors into the new space, you’ll have one document-topic
    vector for each document in your corpus. You’ll have one word-topic vector for
    each word in your lexicon (vocabulary). So you can compute the topic vector for
    any new document by just adding up all its word topic vectors.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当您将您的向量转换到新空间时，您将为语料库中的每个文档有一个文档-主题向量。您的词汇表中每个单词都将有一个词-主题向量。因此，您只需将其所有词-主题向量相加，就可以计算任何新文档的主题向量。
- en: Coming up with a numerical representation of the semantics (meaning) of words
    and sentences can be tricky. This is especially true for "fuzzy" languages like
    English, which has multiple dialects and many different interpretations of the
    same words.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 创造出单词和句子语义（含义）的数值表示可能会有些棘手。这对于"模糊"语言如英语来说尤其如此，因为英语有多种方言，对相同单词有许多不同的解释。
- en: Keeping these challenges in mind, can you imagine how you might squash a TF-IDF
    vector with one million dimensions (terms) down to a vector with 10 or 100 dimensions
    (topics)? This is like identifying the right mix of primary colors to try to reproduce
    the paint color in your apartment so you can cover over those nail holes in your
    wall.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些挑战，您能想象如何将具有一百万维度（术语）的 TF-IDF 向量压缩为具有 10 或 100 维度（主题）的向量吗？这就像确定正确的基本颜色混合以尝试复制您公寓中的油漆颜色，以便您可以覆盖那些墙上的钉孔。
- en: You’d need to find those word dimensions that "belong" together in a topic and
    add their TF-IDF values together to create a new number to represent the amount
    of that topic in a document. You might even weight them for how important they
    are to the topic, how much you’d like each word to contribute to the "mix." And
    you could have negative weights for words that reduce the likelihood that the
    text is about that topic.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要找到那些在一个主题中“属于”一起的单词维度，并将它们的 TF-IDF 值相加，以创建一个新的数字来表示文档中该主题的数量。您甚至可以根据它们对主题的重要性对它们进行加权，以及您希望每个单词对"混合"的贡献有多少。您甚至可以为减少文本与该主题相关的可能性的单词添加负权重。
- en: 4.1.3 Thought experiment
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.3 思想实验
- en: Let’s walk through a thought experiment. Let’s assume you have some TF-IDF vector
    for a particular document and you want to convert that to a topic vector. You
    can think about how much each word contributes to your topics.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进行一个思想实验。假设您有某个特定文档的一些 TF-IDF 向量，并且您希望将其转换为一个主题向量。您可以考虑每个单词对您的主题的贡献。
- en: 'Let’s say you’re processing some sentences about pets in Central Park in New
    York City (NYC). Let’s create three topics: one about pets, one about animals,
    and another about cities. Call these topics "petness", "animalness", and "cityness."
    So your "petness" topic about pets will score words like "cat" and "dog" significantly,
    but probably ignore words like "NYC" and "apple." The "cityness" topic will ignore
    words like "cat" and "dog" but might give a little weight to "apple", just because
    of the "Big Apple" association.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在处理有关纽约市中央公园的宠物的一些句子（NYC）。让我们创建三个主题：一个关于宠物，一个关于动物，另一个关于城市。将这些主题称为“petness”、“animalness”和“cityness”。因此，关于宠物的“petness”主题将显著评分像“猫”和“狗”这样的词汇，但可能忽略像“NYC”和“苹果”这样的词汇。关于城市的“cityness”主题将忽略像“猫”和“狗”这样的词汇，但可能会对“苹果”稍微加权，仅仅因为与“大苹果”有关联。
- en: If you "trained" your topic model like this, without using a computer, just
    your common sense, you might come up with some weights like those in Listing 4.1.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你像这样“训练”你的主题模型，而不使用计算机，只使用你的常识，你可能会得出类似于清单 4.1 中的一些权重。
- en: Listing 4.1 Sample weights for your topics
  id: totrans-44
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 4.1 你的主题的示例权重
- en: '[PRE0]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this thought experiment, we added up the word frequencies that might be indicators
    of each of your topics. We weighted the word frequencies (TF-IDF values) by how
    likely the word is associated with a topic. Note that these weights can be negative
    as well for words that might be talking about something that is in some sense
    the opposite of your topic.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个思想实验中，我们将可能是指示您的每个主题的单词频率相加起来。我们根据单词与主题相关的可能性加权单词频率（TF-IDF 值）。请注意，这些权重也可能是负值，因为某种意义上可能谈论与您的主题相反的内容的单词。
- en: Note this is not a real algorithm, or example implementation, just a thought
    experiment. You’re just trying to figure out how you can teach a machine to think
    like you do. You arbitrarily chose to decompose your words and documents into
    only three topics ("petness", "animalness", and "cityness"). And your vocabulary
    is limited, it has only six words in it.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这不是一个真正的算法或示例实现，只是一个思想实验。你只是试图弄清楚如何教机器像你一样思考。你任意选择将你的单词和文档分解为只有三个主题（“petness”、“animalness”和“cityness”）。并且你的词汇是有限的，只有六个单词。
- en: The next step is to think through how a human might decide mathematically which
    topics and words are connected, and what weights those connections should have.
    Once you decided on three topics to model, you then had to then decide how much
    to weight each word for those topics. You blended words in proportion to each
    other to make your topic "color mix." The topic modeling transformation (color
    mixing recipe) is a 3 x 6 matrix of proportions (weights) connecting three topics
    to six words. You multiplied that matrix by an imaginary 6 x 1 TF-IDF vector to
    get a 3 x 1 topic vector for that document.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是思考一个人可能如何在数学上决定哪些主题和单词是相关的，以及这些连接应该具有什么权重。一旦你决定了三个要建模的主题，你就必须确定为这些主题中的每个单词分配多少权重。你按比例混合单词以使你的主题“颜色混合”。主题建模转换（颜色混合配方）是一个
    3 x 6 的比例（权重）矩阵，将三个主题与六个单词相连。你将该矩阵乘以一个想象中的 6 x 1 的 TF-IDF 向量，以获得该文档的 3 x 1 主题向量。
- en: You made a judgment call that the terms "cat" and "dog" should have similar
    contributions to the "petness" topic (weight of .3). So the two values in the
    upper left of the matrix for your TF-IDF-to-topic transformation are both `.3`.
    Can you imagine ways you might "compute" these proportions with software? Remember,
    you have a bunch of documents your computer can read, tokenize, and count tokens
    for. You have TF-IDF vectors for as many documents as you like. Keep thinking
    about how you might use those counts to compute topic weights for a word as you
    read on.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 你做出了判断，认为术语“猫”和“狗”应该对“petness”主题具有类似的贡献（权重为 0.3）。因此，用于你的 TF-IDF 到主题转换的矩阵左上角的两个值都是
    0.3。你能想象出可能使用软件“计算”这些比例的方法吗？记住，你有一堆计算机可以阅读，标记和计算标记的文档。你可以为尽可能多的文档制作 TF-IDF 向量。继续思考在阅读时如何使用这些计数来计算单词的主题权重。
- en: You decided that the term "NYC" should have a negative weight for the "petness"
    topic. In some sense city names, and proper names in general, and abbreviations,
    and acronyms, share little in common with words about pets. Think about what "sharing
    in common" means for words. Is there something in a TF-IDF matrix that represents
    the meaning that words share in common?
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 你决定术语“NYC”在“petness”主题中应具有负权重。在某种意义上，城市名称，以及一般的专有名称，缩写和首字母缩写，与有关宠物的词汇几乎没有共同之处。思考一下单词“共同之处”在词汇中的含义。TF-IDF
    矩阵中是否有表示单词共同含义的内容？
- en: Notice the small amount of the word "apple" into the topic vector for "city."
    This could be because you’re doing this by hand and we humans know that "NYC"
    and "Big Apple" are often synonymous. Our semantic analysis algorithm will hopefully
    be able to calculate this synonymy between "apple" and "NYC" based on how often
    "apple" and "NYC" occur in the same documents.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 "city" 这个主题向量中有少量的 "apple" 一词。这可能是因为你是手动进行的，而我们人类知道 "NYC" 和 "Big Apple" 经常是同义词。我们的语义分析算法有望能够根据
    "apple" 和 "NYC" 在相同文档中出现的频率来计算出它们之间的同义关系。
- en: As you read the rest of the weighted sums in Listing 4.1, try to guess how we
    came up with these weights for these three topics and six words. You may have
    a different "corpus" in your head than the one we used in our heads. So you may
    have a different opinion about the "appropriate" weights for these words. How
    might you change them? What could you use as an objective measure of these proportions
    (weights)? We’ll answer that question in the next section.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读清单4.1中的加权和之后，试着猜猜我们是如何得出这三个主题和六个单词的权重的。你脑海中可能有一个不同的"语料库"，与我们在头脑中使用的不同。所以你可能对这些单词的"适当"权重有不同的看法。你会如何改变它们？你可以用什么客观的标准来衡量这些比例（权重）？我们将在下一节回答这个问题。
- en: Note
  id: totrans-53
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: We chose a signed weighting of words to produce the topic vectors. This allows
    you to use negative weights for words that are the "opposite" of a topic. And
    because you’re doing this manually by hand, we chose to normalize your topic vectors
    by the easy-to-compute L¹-norm (meaning the sum of absolute values of the vector
    dimensions equals 1). Nonetheless, the real LSA you’ll use later in this chapter
    normalizes topic vectors by the more useful L²-norm. We’ll cover the different
    norms and distances later in this chapter.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了一种有符号的词权重来生成主题向量。这样可以使用负权重来表示与主题相反的词。因为你是手工进行的，我们选择使用易于计算的 L¹-norm （即向量维度的绝对值之和等于1）来对你的主题向量进行归一化。不过，在本章稍后使用的真正的潜在语义分析（LSA）算法则通过更有用的
    L²-norm 对主题向量进行归一化。我们将在本章后面介绍不同的范数和距离。
- en: 'You might have realized in reading these vectors that the relationships between
    words and topics can be "flipped." The 3 x 6 matrix of three topic vectors can
    be transposed to produce topic weights for each word in your vocabulary. These
    vectors of weights would be your word vectors for your six words:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读这些向量时，你可能已经意识到单词和主题之间的关系是可以"翻转"的。一个 3x6 的三个主题向量矩阵可以通过转置来产生你的词汇表中每个单词的主题权重。这些权重向量将成为你六个单词的词向量：
- en: '[PRE1]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: These six word-topic vectors shown in Figure [4.1](#six-lovable-words), one
    for each word, represent the meanings of your six words as 3D vectors.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这六个单词主题向量在图[4.1](#六个可爱的词)中显示，每个单词对应一个向量，表示你的六个词的含义。
- en: Figure 4\. 1\. 3D vectors for a thought experiment about six words about pets
    and NYC
  id: totrans-58
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.1. 关于宠物和纽约市的六个单词的思想实验的3D向量
- en: '![cats and dogs petness 3D](images/cats_and_dogs_petness_3D.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![猫和狗可爱程度的3D图](images/cats_and_dogs_petness_3D.png)'
- en: Earlier, the vectors for each topic, with weights for each word, gave you 6-D
    vectors representing the linear combination of words in your three topics. Now,
    you hand-crafted a way to represent a document by its topics. If you just count
    up occurrences of these six words and multiply them by your weights, you get the
    3D topic vector for any document. And 3D vectors are fun because they’re easy
    for humans to visualize. You can plot them and share insights about your corpus
    or a particular document in graphical form.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，每个主题的向量都带有每个单词的权重，给出了表示三个主题中单词的线性组合的6-D向量。现在，你手工设计了一种通过主题来表示文档的方法。如果你只计算这些六个单词出现的次数，并将它们乘以相应的权重，就可以得到任何文档的3D主题向量。3D向量非常有趣，因为人们可以很容易地进行可视化。你可以将它们绘制出来，并以图形形式分享关于你的语料库或特定文档的见解。
- en: 3D vectors (or any low-dimensional vector space) are great for machine learning
    classification problems, too. An algorithm can slice through the vector space
    with a plane (or hyperplane) to divide up the space into classes.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 3D向量（或任何低维向量空间）对于机器学习分类问题也非常有用。算法可以通过平面（或超平面）在向量空间中划分不同的类别。
- en: The documents in your corpus might use many more words, but this particular
    topic vector model will only be influenced by the use of these six words. You
    could extend this approach to as many words as you had the patience (or an algorithm)
    for. As long as your model only needed to separate documents according to three
    different dimensions or topics, your vocabulary could keep growing as much as
    you like. In the thought experiment, you compressed six dimensions (TF-IDF normalized
    frequencies) into three dimensions (topics).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: This subjective, labor-intensive approach to semantic analysis relies on human
    intuition and common sense to break documents down into topics. Common sense is
    hard to code into an algorithm.^([[4](#_footnotedef_4 "View footnote.")]) And
    obviously this isn’t suitable for a machine learning pipeline. Plus it doesn’t
    scale well to more topics and words.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: So let’s automate this manual procedure. Let’s use an algorithm that doesn’t
    rely on common sense to select topic weights for us.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: If you think about it, each of these weighted sums is just a dot product. And
    three dot products (weighted sums) is just a matrix multiplication, or inner product.
    You multiply a 3 x *n* weight matrix with a TF-IDF vector (one value for each
    word in a document), where *n* is the number of terms in your vocabulary. The
    output of this multiplication is a new 3 x 1 topic vector for that document. What
    you’ve done is "transform" a vector from one vector space (TF-IDFs) to another
    lower-dimensional vector space (topic vectors). Your algorithm should create a
    matrix of *n* terms by *m* topics that you can multiply by a vector of the word
    frequencies in a document to get your new topic vector for that document.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.4 Algorithms for scoring topics
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You still need an algorithmic way to determine these topic vectors, or to derive
    them from vectors you already have - like TF-IDF or bag-of-words (BOW) vectors.
    A machine can’t tell which words belong together or what any of them signify,
    can it? J. R. Firth, a 20th century British linguist, studied the ways you can
    estimate what a word or morpheme ^([[5](#_footnotedef_5 "View footnote.")]) signifies.
    In 1957 he gave you a clue about how to compute the topics for words. Firth wrote:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: You shall know a word by the company it keeps.
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: — J. R. Firth
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '1957'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: So how do you tell the "company" of a word? Well, the most straightforward approach
    would be to count co-occurrences in the same document. And you have exactly what
    you need for that in your BOW and TF-IDF vectors from chapter 3\. This "counting
    co-occurrences" approach led to the development of several algorithms for creating
    vectors to represent the statistics of word usage within documents or sentences.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: In the next sections, you’ll see 2 algorithms for creating these topic vectors.
    The first one, *Latent Semantic Analysis* (LSA), is applied to your TF-IDF matrix
    to gather up words into topics. It works on bag-of-words vectors, too, but TF-IDF
    vectors give slightly better results. LSA optimizes these topics to maintain diversity
    in the topic dimensions; when you use these new topics instead of the original
    words, you still capture much of the meaning (semantics) of the documents. The
    number of topics you need for your model to capture the meaning of your documents
    is far less than the number of words in the vocabulary of your TF-IDF vectors.
    So LSA is often referred to as a dimension reduction technique. LSA reduces the
    number of dimensions you need to capture the meaning of your documents.^([[6](#_footnotedef_6
    "View footnote.")])
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，你将看到两种用于创建这些主题向量的算法。第一种，*潜在语义分析*（LSA），应用于你的TF-IDF矩阵以将单词聚合到主题中。它也适用于词袋向量，但TF-IDF向量的效果略好一些。LSA优化这些主题以保持主题维度的多样性；当你使用这些新主题而不是原始单词时，仍然能捕捉到文档的大部分含义（语义）。你的模型所需的主题数量远远少于TF-IDF向量词汇表中的单词数量，因此LSA通常被称为一种维度缩减技术。LSA减少了你需要捕捉文档含义的维度数量。^([[6](#_footnotedef_6
    "查看脚注。")])
- en: The other algorithm we’ll cover is called *Latent Dirichlet Allocation*, often
    shortened to LDA. Because we use LDA to signify Latent Discriminant Analysis classifier
    in this book, we will shorten Latent Dirichlet Allocation to LDiA instead.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要介绍的另一种算法被称为*潜在狄利克雷分配*，通常缩写为LDA。因为在本书中我们使用LDA来表示潜在判别分析分类器，所以我们将潜在狄利克雷分配简称为LDiA。
- en: LDiA takes the math of LSA in a different direction. It uses a nonlinear statistical
    algorithm to group words together. As a result, it generally takes much longer
    to train than linear approaches like LSA. Often this makes LDiA less practical
    for many real-world applications, and it should rarely be the first approach you
    try. Nonetheless, the statistics of the topics it creates sometimes more closely
    mirror human intuition about words and topics. So LDiA topics will often be easier
    for you to explain to your boss. It is also more useful for some single-document
    problems such as document summarization.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: LDiA将LSA的数学带入了不同的方向。它使用非线性统计算法将单词分组在一起。因此，通常比LSA之类的线性方法需要更长的训练时间。这使得LDiA在许多实际应用中不太实用，并且它很少是你尝试的第一种方法。尽管如此，它创建的主题的统计数据有时更接近人们对单词和主题的直觉。因此，LDiA的主题通常更容易向你的老板解释。它还更适用于一些单文档问题，如文档摘要。
- en: For most classification or regression problems, you’re usually better off using
    LSA. So we explain LSA and its underlying SVD linear algebra first.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数分类或回归问题，通常最好使用LSA。因此，我们首先解释LSA及其基础的SVD线性代数。
- en: '4.2 The challenge: detecting toxicity'
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 挑战：检测毒性
- en: 'To see the power of topic modeling, we’ll try to solve a real problem: recognizing
    toxicity in Wikipedia comments. This is a common NLP task that content and social
    media platforms face nowadays. Throughout this chapter, we’ll work on a dataset
    of Wikipedia discussion comments,^([[7](#_footnotedef_7 "View footnote.")]) which
    we’ll want to classify into two categories - toxic and non-toxic. First, let’s
    load our dataset and take a look at it:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看到主题建模的威力，我们将尝试解决一个真实问题：识别维基百科评论中的有毒性。这是当前内容和社交媒体平台面临的常见自然语言处理任务。在本章中，我们将处理一个维基百科讨论评论的数据集，^([[7](#_footnotedef_7
    "查看脚注。")])我们将希望将其分类为两个类别 - 有毒和无毒。首先，让我们加载数据集并查看一下：
- en: Listing 4.2 The toxic comment dataset
  id: totrans-78
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第4.2节 有毒评论数据集
- en: '[PRE2]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: So you have 5,000 comments, and 650 of them are labeled with the binary class
    label "toxic."
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你有5,000条评论，其中650条被标记为二进制类别标签“有毒”。
- en: Before you dive into all the fancy dimensionality reduction stuff, let’s try
    to solve our classification problem using vector representations for the messages
    that you are already familiar with - TF-IDF. But what *model* will you choose
    to classify the messages? To decide, let’s look at the TF-IDF vectors first.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在你深入了解所有复杂的降维技术之前，让我们尝试使用你已经熟悉的消息的向量表示来解决我们的分类问题 - TF-IDF。但是你会选择什么*模型*来对消息进行分类呢？为了决定，让我们首先看看TF-IDF向量。
- en: Listing 4.3 Creating TF-IDF vectors for the SMS dataset
  id: totrans-82
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第4.3节 为SMS数据集创建TF-IDF向量
- en: '[PRE3]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The spaCy tokenizer gave you 19,169 words in your vocabulary. You have almost
    4 times as many words as you have messages. And you have almost 30 times as many
    words as toxic comments. So your model will not have a lot of information about
    the words that will indicate whether a comment is toxic or not.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy分词器为您的词汇表提供了19,169个单词。您的词汇表中几乎有4倍于您的消息的单词数量。而且您的单词数量几乎是有毒评论的30倍。因此，您的模型将不会有太多关于表明评论是否有毒的单词的信息。
- en: You have already met at least one classifier in this book - Naive Bayes in chapter
    2\. Usually, a Naive Bayes classifier will not work well when your vocabulary
    is much larger than the number of labeled examples in your dataset. So we need
    something different this time.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 你在本书中已经至少遇到了一个分类器 - 第2章中的朴素贝叶斯。通常，当您的词汇量远大于数据集中标记示例的数量时，朴素贝叶斯分类器的效果不会很好。所以这次我们需要点不同的东西。
- en: 4.2.1 Latent Discriminant Analysis classifier
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.1 潜在判别分析分类器
- en: In this chapter, we’re going to introduce a classifier that is based on an algorithm
    called Latent Discriminant Analysis (LDA). LDA is one of the most straightforward
    and fast classification models you’ll find, and it requires fewer samples than
    the fancier algorithms.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍一种基于称为潜在判别分析（LDA）的算法的分类器。LDA是您会找到的最简单和最快的分类模型之一，并且它需要的样本比较花哨的算法要少。
- en: The input to LDA will be a labeled data - so we need not just the vectors representing
    the messages, but their class too. In this case, we have two classes - toxic comments
    and non-toxic comments. LDA algorithm uses some math that beyond the scope of
    this book, but in the case of two classes, its implementation is pretty intuitive.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: LDA的输入将是一个带有标签的数据 - 因此我们不仅需要表示消息的向量，还需要它们的类别。在这种情况下，我们有两个类别 - 有毒评论和非有毒评论。LDA算法使用了一些超出本书范围的数学知识，但在两个类别的情况下，其实现非常直观。
- en: 'In essence, this is what LDA algorithm does when faced with a two-class problem:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，当面临两类问题时，这就是LDA算法的工作原理：
- en: It finds a line, or axis, in your vector space, such that if you project all
    the vectors (data points) in the space on that axis, the two classes would be
    as separated as possible.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它找到一个线，或者说轴，在您的向量空间中，如果您将空间中的所有向量（数据点）投影到该轴上，两个类别将尽可能地分离。
- en: It projects all the vectors on that line.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它将所有向量投影到那条线上。
- en: It predicts the probability of each vector to belong to one of two classes,
    according to a *cutoff* point between the two classes.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它预测每个向量属于两个类别之一的概率，根据两个类别之间的一个*cutoff*点。
- en: Surprisingly, in the majority of cases, the line that maximizes class separation
    is very close to the line that connects the two *centroids* ^([[8](#_footnotedef_8
    "View footnote.")]) of the clusters representing each class.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，在大多数情况下，最大化类别分离的线非常接近连接代表每个类别的聚类的两个*质心*的线。
- en: Let’s perform manually this approximation of LDA, and see how it does on our
    dataset.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们手动执行这个LDA的近似，并看看它在我们的数据集上的表现如何。
- en: '[PRE4]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The toxicity score for a particular comment is the length of the shadow (projection)
    of that comment’s vector along the line between the nontoxic and nontoxic comments.
    You compute these projections just as you did for the cosine distance. It is the
    normalized dot product of the comment’s vector with the vector pointing from nontoxic
    comments towards toxic comments. You calculated the toxicity score by projecting
    each TF-IDF vector onto that line between the centroids using the dot product.
    And you did those 5,000 dot products all at once in a "vectorized" numpy operation
    using the `.dot()` method. This can speed things up 100 times compared to a Python
    `for` loop.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 特定评论的毒性评分是该评论向量在非有毒评论和非有毒评论之间的线上的投影的长度。您计算这些投影的方法与您对余弦距离所做的计算相同。它是评论向量与从非有毒评论指向有毒评论的向量之间的向量的归一化点积。通过将每个TF-IDF向量投影到该线上并使用点积来计算毒性分数。您使用`dot()`方法一次性进行了这5000个点积的“向量化”numpy操作。与Python的`for`循环相比，这可以加速100倍。
- en: 'You have just one step left in our classification. You need to transform our
    score into the actual class prediction. Ideally, you’d like your score to range
    between 0 and 1, like a probability. Once you have the scores normalized, you
    can deduce the classification from the score based on a cutoff - here, we went
    with a simple 0.5 You can use `sklearn` `MinMaxScaler` to perform the normalization:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的分类中，你只剩下一步了。你需要将我们的分数转换为实际的类预测。理想情况下，你希望你的分数在 0 和 1 之间，就像概率一样。一旦你对分数进行了归一化，你就可以根据一个截止值推断出分类
    - 在这里，我们选择了一个简单的 0.5。你可以使用 `sklearn` 的 `MinMaxScaler` 来执行归一化：
- en: '[PRE6]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: That looks pretty good. Almost all of the first six messages were classified
    correctly. Let’s see how it did on the rest of the training set.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来不错。前六条消息几乎全部被正确分类了。让我们看看它在其余的训练集上的表现如何。
- en: '[PRE7]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Not bad! 89.5% of the messages were classified correctly with this simple "approximate"
    version of LDA. How will the "full" LDA do? Use SciKit Learn (`sklearn`) to get
    a state-of-the art LDA implementation.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 不错！这个简单的“近似”版本的 LDA 准确地分类了 89.5% 的消息。完整的 LDA 会表现如何？使用 SciKit Learn (`sklearn`)
    来获得最先进的 LDA 实现。
- en: '[PRE8]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 99.9%! Almost perfect accuracy. Does this mean you don’t need to use fancier
    topic modeling algorithms like Latent Dirichlet Allocation or deep learning? This
    is a trick question. You have probably already figured out the trap. The reason
    for this perfect 99.9% result is that we haven’t separated out a test set. This
    A+ score is on "questions" that the classifier has already "seen." This is like
    getting an exam in school with the exact same questions that you practiced on
    the day before. So this model probably wouldn’t do well in the real world of trolls
    and spammers.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 99.9%! 几乎完美的准确率。这意味着你不需要使用更复杂的主题建模算法，比如潜在狄利克雷分配或深度学习吗？这是一个陷阱问题。你可能已经发现了陷阱。这个完美的
    99.9% 的结果之所以如此完美，是因为我们没有分离出一个测试集。这个 A+ 分数是在分类器已经“见过”的“问题”上获得的。这就像在学校考试时拿到了和前一天练习的完全相同的问题一样。所以这个模型在恶意评论和垃圾邮件的真实世界中可能表现不佳。
- en: Tip
  id: totrans-105
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: 'Note the class methods you used in order to train and make predictions. Every
    model in `sklearn` has those same methods: `fit()` and `predict()`. And all classifier
    models will even have a `predict_proba()` method that gives you the probability
    scores for all the classes. That makes it easier to swap out different model algorithms
    as you try to find the best ones for solving your machine learning problems. That
    way you can save your brainpower for the creative work of an NLP engineer, tuning
    your model hyperparameters to work in the real world.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 注意你用来训练和进行预测的类方法。`sklearn` 中的每个模型都有相同的方法：`fit()` 和 `predict()`。而且所有的分类器模型甚至都会有一个
    `predict_proba()` 方法，用于给出所有类别的概率分数。这样，当你尝试找到解决机器学习问题的最佳模型算法时，更容易进行不同模型算法的替换。这样你就可以将你的脑力集中在
    NLP 工程师的创造性工作上，调整你的模型超参数以在实际世界中发挥作用。
- en: Let’s see how our classifier does in a more realistic situation. You’ll split
    your comment dataset into 2 parts - training set and testing set. (As you can
    imagine, there is a function in `sklearn` just for that!) And you’ll see how the
    classifier performs on the messages it wasn’t trained on.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们的分类器在一个更加现实的情况下的表现。你将把你的评论数据集分成两部分 - 训练集和测试集。（你可以想象，在 `sklearn` 中有一个专门的函数用于此！）然后你将看到分类器在它没有被训练的消息上的表现。
- en: Listing 4.4 LDA model performance with train-test split
  id: totrans-108
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.4 使用训练-测试拆分的 LDA 模型性能
- en: '[PRE9]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The training set accuracy for your TF-IDF based model is almost perfect. But
    the test set accuracy is 0.55 - a bit better than flipping a coin. And test set
    accuracy is the only accuracy that counts. This is exactly what topic modeling
    will help you. It will allow you to generalize your models from a small training
    set so it still works well on messages using different combinations of words (but
    similar topics).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 TF-IDF 的模型的训练集准确率几乎完美。但测试集准确率为 0.55 - 比抛硬币稍微好一点。而测试集准确率才是唯一重要的准确率。这正是主题建模将帮助你的地方。它将允许你从一个小训练集中推广你的模型，使其在使用不同词语组合（但是相似主题）的消息上仍然表现良好。
- en: Tip
  id: totrans-111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: Note the `random_state` parameter for the `train_test_split` The algorithm for
    `train_test_split()` are stochastic. So each time you run it you will get different
    results and different accuracy values. If you want to make your pipeline repeatable,
    look for the `seed` argument for these models and dataset splitters. You can set
    the seed to the same value with each run to get reproducible results.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 `train_test_split` 函数中的 `random_state` 参数。`train_test_split()` 函数是随机的。所以每次运行它都会得到不同的结果和不同的准确度值。如果你想要让你的流程可重复，可以查找这些模型和数据集拆分器的
    `seed` 参数。你可以将种子设置为相同的值来获得可再现的结果。
- en: 'Let’s look a bit deeper at how our LDA model did, using a tool called *confusion
    matrix*. The confusion matrix will tell you the number of times the model made
    a mistake. There are two kinds of mistakes, *false positive* mistakes and *false
    negative* mistakes. Mistakes on on examples that were labeled toxic in the test
    set are called "false negatives" because they were falsely labeled as negative
    (nontoxic) and should have been labeled positive (toxic). Mistakes on the nontoxic
    labels in the test set are called "false positives" because they should have been
    labeled negative (nontoxic) but were falsely labeled toxic. Here’s how you do
    it with an `sklearn function`:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地看一下我们的 LDA 模型的表现，使用一种称为 *混淆矩阵* 的工具。混淆矩阵将告诉你模型犯错的次数。有两种类型的错误，*假阳性* 错误和
    *假阴性* 错误。在测试集中标记为有毒的示例上出现的错误称为“假阴性”，因为它们被错误地标记为负面（无毒）并且应该被标记为正面（有毒）。在测试集中标记为非有毒标签上的错误称为“假阳性”，因为它们应该被标记为负面（无毒），但被错误地标记为有毒。下面是使用
    `sklearn 函数` 的方法：
- en: '[PRE10]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Hmmm. It’s not exactly clear what’s going on here. Fortunately, `sklearn` have
    taken into account that you might need a more visual way to present your confusion
    matrix to people, and included a function just for that. Let’s try it out:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯。这里的情况不太清楚。幸运的是，`sklearn` 考虑到你可能需要一种更直观的方式来向人们展示你的混淆矩阵，并包含了一个专门的函数。让我们试试：
- en: '[PRE11]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: You can see the resulting `matplotlib` plot in Figure [4.2](#confusion-matrix)
    showing the number of incorrect and correct predictions for each of the two labels
    (toxic and non-toxic). Check out this plot to see if you can tell what is wrong
    with your model’s performance.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在图 [4.2](#confusion-matrix) 中看到生成的 `matplotlib` 图，显示了两个标签（有毒和非有毒）的每个标签的不正确和正确的预测数量。检查这个图表，看看你能否发现你的模型性能有什么问题。
- en: Figure 4.2 Confusion matrix of TF-IDF based classifier
  id: totrans-118
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.2 基于 TF-IDF 的分类器的混淆矩阵
- en: '![confusion matrix drawio](images/confusion-matrix_drawio.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![混淆矩阵图](images/confusion-matrix_drawio.png)'
- en: First of all, out of 326 comments in the test set that were actually toxic,
    the model was able to identify correctly only 125 - that’s 38.3%. This measure
    (how many of the instances of the class we’re interested in the model was able
    to identify), is called *recall*, or *sensitivity*. On the other hand, out of
    1038 comments the model labeled as toxic, only 125 are truly toxic comments. So
    the "positive" label is only correct in 12% of cases. This measure is called *precision*.^([[9](#_footnotedef_9
    "View footnote.")])
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在实际上是有毒的测试集中的 326 条评论中，模型只能正确识别出 125 条 - 这是 38.3%。这个指标（我们感兴趣的类别中模型能够识别出多少个实例），称为
    *召回率*，或 *敏感度*。另一方面，模型标记为有毒的 1038 条评论中，只有 125 条是真正有毒的评论。所以“正面”标签在 12% 的情况下才是正确的。这个指标称为
    *精度*。^([[9](#_footnotedef_9 "View footnote.")])
- en: You can already see how precision and recall give us more information than model
    accuracy. For example, imagine that instead of using machine learning models,
    you decided to use a deterministic rule and just label all the comments as non-toxic.
    As about 13% of comments in our dataset are actually toxic, this model will have
    accuracy of 0.87 - much better than the last LDA model you trained! However, its
    recall is going to be 0 - it doesn’t help you at all in our task, which is to
    identify toxic messages.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经可以看到精度和召回率比模型准确度给我们更多的信息。例如，想象一下，如果你决定使用确定性规则而不是使用机器学习模型，并只将所有评论标记为非有毒。由于我们数据集中约有
    13% 的评论实际上是有毒的，所以这个模型的准确度将达到 0.87 - 比你上次训练的 LDA 模型要好得多！但是，它的召回率将为 0 - 在我们的任务中完全没有帮助，即识别有毒消息。
- en: You might also realize that there is a tradeoff between these two measures.
    What if you went with another deterministic rule and labeled all the comments
    as toxic? In this case, your recall would be perfect, as you would correctly classify
    all the toxic comments. However, the precision will suffer, as most of the comments
    labeled as toxic will actually be perfectly OK.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能也意识到这两个指标之间存在一种权衡。如果您采用另一种确定性规则，并将所有评论标记为有毒呢？在这种情况下，您的召回率将是完美的，因为您将正确分类所有有毒评论。但是，精确度将会下降，因为大多数被标记为有毒的评论实际上是完全正常的。
- en: Depending our your use case, you might decide to prioritize either precision
    or recall on top of the other. But in a lot of cases, you would want both of them
    to be reasonably good.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的用例，您可能会决定优先考虑另一方面的精确度或召回率。但在很多情况下，您希望它们两者都足够好。
- en: In this case, you’re likely to use the *F[1] score* - a harmonic mean of precision
    and recall. Higher precision and higher recall both lead to a higher F[1] score,
    making it easier to benchmark your models with just one metric.^([[10](#_footnotedef_10
    "View footnote.")])
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，您可能会使用*F[1]分数* - 精确度和召回率的调和平均值。较高的精确度和较高的召回率都会导致较高的F[1]分数，使得只使用一个指标来评估您的模型更容易。
- en: You can learn more about analyzing your classifier’s performance in Appendix
    D. For now, we will just note this model’s F[1] score before we continue on.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在附录 D 中了解有关分析分类器性能的更多信息。暂时，在我们继续之前，我们将只记录此模型的F[1]分数。
- en: 4.2.2 Going beyond linear
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 超越线性
- en: LDA is going to serve you well in many circumstances. However, it still has
    some assumptions that will cause the classifier to underperform when these assumptions
    are not fulfilled. For example, LDA assumes that the feature covariance matrices
    for all of your classes are the same. That’s a pretty strong assumption! As a
    result of it, LDA can only learn linear boundaries between classes.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: LDA在许多情况下都会为您服务。然而，当这些假设不被满足时，它仍然有一些假设将导致分类器性能不佳。例如，LDA假定所有类别的特征协方差矩阵都相同。这是一个相当强的假设！因此，由此造成的结果是，LDA只能在类别之间学习线性边界。
- en: If you need to relax this assumption, you can use a more general case of LDA
    called *Quadratic Discriminant Analysis*, or QDA. QDA allows different covariance
    matrices for different classes, and estimates each covariance matrix separately.
    That’s why it can learn quadratic, or curved, boundaries.^([[11](#_footnotedef_11
    "View footnote.")]) That makes it more flexible, and helps it to perform better
    in some cases.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要放松这个假设，您可以使用称为*二次判别分析*或QDA的更一般情况的LDA。QDA允许不同类别的不同协方差矩阵，并分别估计每个协方差矩阵。这就是为什么它可以学习二次或曲线边界的原因。这使得它更加灵活，并在某些情况下有助于其表现更好。
- en: 4.3 Reducing dimensions
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 减少维度
- en: Before we dive into LSA, let’s take a moment to understand what, conceptually,
    it does to our data. The idea behind LSA’s approach to topic modeling is *dimensionality
    reduction*. As its name suggests, dimensionality reduction is a process in which
    we find a lower-dimensional representation of data that retains as much information
    as possible.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入了解LSA之前，让我们花点时间了解一下它对我们的数据做了什么概念上的事情。LSA对主题建模的方法背后的想法是*降维*。顾名思义，降维是一个过程，在这个过程中，我们找到数据的一个低维表示，保留尽可能多的信息。
- en: Let’s examine this definition and understand what it means. To give you an intuition,
    let’s step away from NLP for a moment and switch to more visual examples. First,
    what’s a lower-dimension representation of data? Think about taking a 3-D object
    (like your sofa) and representing it in 2-D space. For example, if you shine a
    light behind your sofa in a dark room, its shadow on the wall is its two-dimensional
    representation.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们审视这个定义并理解它的含义。为了让您有直观的理解，让我们暂时远离自然语言处理，并切换到更直观的例子。首先，什么是数据的低维表示？想象一下将一个三维物体（比如你的沙发）表示为二维空间。例如，如果您在黑暗的房间里用光照射在沙发后面，它在墙上的阴影就是它的二维表示。
- en: Why would we want such a representation? There might be many reasons. Maybe
    we don’t have the capacity to store or transmit the full data as it is. Or maybe
    we want to visualize our data to understand it better. You already saw the power
    of visualizing your data points and clustering them when we talked about LDA.
    But our brain can’t really work with more than 2 or 3 dimensions - and when we’re
    dealing with real-world data, especially natural language data, our datasets might
    have hundreds or even thousands of dimensions. Dimensionality reduction tools
    like PCA are very useful when we want to simplify and visually map our dataset.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为什么需要这样的表示？可能有很多原因。也许我们没有能力存储或传输完整的数据。或者我们想要可视化我们的数据以更好地理解它。当我们谈论 LDA 时，你已经看到了可视化数据点并将它们聚类的强大能力。但我们的大脑实际上不能处理超过
    2 或 3 个维度 - 当我们处理现实世界的数据，特别是自然语言数据时，我们的数据集可能有数百甚至数千个维度。像 PCA 这样的降维工具在我们想要简化和可视化映射我们的数据时非常有用。
- en: Another important reason is the curse of dimensionality we briefly mentioned
    in chapter 3\. Sparse, multidimensional data is harder to work with, and classifiers
    trained on it are more prone to overfitting. A rule of thumb that’s often used
    by data scientists is that there should be at least 5 records for every dimension.
    We’ve already seen that even for small text datasets, TF-IDF matrices can quickly
    push into 10 or 20 thousand dimensions. And that’s true for many other types of
    data, too.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要原因是我们在第三章中简要提到的维度诅咒。稀疏、多维数据更难处理，而在其上训练的分类器更容易过拟合。数据科学家经常使用的一个经验法则是，每个维度至少应该有5条记录。我们已经看到，即使对于小型文本数据集，TF-IDF
    矩阵也可能迅速扩展到 10 或 20 万个维度。这也适用于许多其他类型的数据。
- en: From the "sofa shadow" example, you can see that we can build infinitely many
    lower-dimensional representations of the same "original" dataset. But some representations
    are better than others. What does "better" mean in this case? When talking about
    visual data, you can intuitively understand that a representation that allows
    us to recognize the object is better than one that doesn’t. For example, let’s
    take a point cloud that was taken from a 3D scan of a real object, and project
    it onto a two dimensional plane.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 从“沙发影子”示例中，你可以看到我们可以构建无限多个相同“原始”数据集的低维表示。但有些表示比其他表示更好。在这种情况下，“更好”是什么意思？当谈到视觉数据时，你可以直观地理解，一个可以让我们识别对象的表示比一个不能的表示更好。例如，让我们拿一个从真实对象的
    3D 扫描中获取的点云，并将其投影到一个二维平面上。
- en: You can see the result in Figure 4.3\. Can you guess what the 3D object was
    from that representation?
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在图 4.3 中看到结果。你能猜到那个 3D 对象是什么吗？
- en: Figure 4.3 Looking up from below the "belly" at the point cloud for a real object
  id: totrans-136
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.3 从下面看实际对象的点云
- en: '![3d pointcloud bottom](images/3d-pointcloud-bottom.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![3d 点云底部](images/3d-pointcloud-bottom.png)'
- en: To continue our "shadows" analogy, think about the midday sun shining above
    the heads of a group of people. Every person’s shadow would be a round patch.
    Would we be able to use those patches to tell who is tall and who is short, or
    which people have long hair? Probably not.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 继续我们的“影子”类比，想象一下正午的太阳照射在一群人的头顶上。每个人的影子都是一个圆形斑点。我们能用这些斑点来判断谁高谁矮，或者哪些人头发长吗？可能不行。
- en: Now you understand that good dimensionality reduction has to do with being able
    to *distinguish* between different objects and data points in the new representation.
    And that not all features, or dimensions, of your data are equally important for
    that process of distinguishing. So there will be features which you can easily
    discard without losing much information. But for some features, losing them will
    significantly hurt your ability to understand your data. And because you are dealing
    with linear algebra here, you don’t only have the option of leaving out or including
    a dimension - you can also combine several dimensions into a smaller dimension
    set that will represent our data in a more concise way. Let’s see how we do that.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你明白了良好的降维与能够在新表示中*区分*不同对象和数据点有关。并不是你数据的所有特征或维度对这个区分过程同样重要。因此，可能有一些特征你可以轻松舍弃而不会丢失太多信息。但对于某些特征，丢失它们将严重影响你理解数据的能力。并且因为你在这里处理的是线性代数，你不仅可以选择留下或包括一个维度
    - 你还可以将几个维度组合成一个更小的维度集，以更简洁的方式表示我们的数据。让我们看看我们是如何做到的。
- en: 4.3.1 Enter Principal Component Analysis
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.1 进入主成分分析
- en: You now know that to find your data’s representation in fewer dimensions, you
    need to find a combination of dimensions that will preserve your ability to distinguish
    between data points. This will let you, for example, to separate them into meaningful
    clusters. To continue the shadow example, a good "shadow representation" allows
    you to see where is the head and where are the legs of your shadow. It does it
    by preserving the difference in height between these objects, rather than "squishing
    them" into one spot like the "midday sun representation" does. On the other hand,
    our body’s "thickness" is roughly uniform from top to bottom - so when you see
    our "flat" shadow representation, that discards that dimension, you don’t lose
    as much information as in the case of discarding our height.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在知道，为了在更少的维度中找到数据的表示，你需要找到一个维度的组合，能够保持你区分数据点的能力。这将使你能够，例如，将它们分成有意义的聚类。继续上面的阴影例子，一个好的“阴影表示”可以让你看到你的阴影的头在哪里，腿在哪里。它通过保持这些对象之间的高度差异来实现，而不是像“中午的太阳表示”那样“压扁”它们到一个点。另一方面，我们身体的“厚度”从顶部到底部大致是均匀的
    - 所以当你看到我们的“扁平”阴影表示时，丢弃了那个维度，你不会像丢弃我们的高度那样丢失太多信息。
- en: In mathematics, this difference is represented by *variance*. And when you think
    about it, it makes sense that features with *more* variance - wider and more frequent
    deviation from the mean - are more helpful for you to tell the difference between
    data points.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学中，这种差异被*方差*所代表。当你想一想的时候，更有*方差*的特征 - 与平均值的偏离更广泛和更频繁 - 对于你来区分数据点更有帮助是有意义的。
- en: 'But you can go beyond looking at each feature by itself. What matters also
    is how the features relate between each other. Here, the visual analogies may
    start to fail you, because the three dimensions we operate in are orthogonal to
    each other, and thus completely unrelated. But let’s think back about your topic
    vectors you saw in the previous part: "animalness", "petness", "cityness". If
    you examine every two features among this triad, it becomes obvious that some
    features are more strongly connected than others. Most words that have a "petness"
    quality to them, also have some "animalness" one. This property of a pair of features,
    or dimensions, is called *covariance*. It is strongly connected to *correlation*,
    which is just covariance normalized by the variance of each feature in the tandem.
    The higher the covariance between features, the more connected they are - and
    therefore, there is more redundancy between the two of them, as you can deduce
    one from the other. It also means that you can find a single dimension that preserves
    most of the variance contained in these two dimensions.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 但你可以超越单独观察每个特征。更重要的是特征之间的关系如何。在这里，视觉类比可能开始让你失望，因为我们操作的三个维度彼此正交，因此完全不相关。但让我们回想一下你在上一部分看到的主题向量：“动物性”，“宠物性”，“都市性”。如果你检查这三元组中的每两个特征，就会显而易见地发现一些特征之间的联系更紧密。大多数具有“宠物性”质量的词，也具有一些“动物性”的质量。一对特征或者维度的这种性质被称为*协方差*。它与*相关性*密切相关，后者仅仅是将每个特征的协方差归一化为这两个特征的差异。特征之间的协方差越高，它们之间的联系就越紧密
    - 因此，它们之间的冗余也更多，因为你可以从一个特征推断出另一个特征。这也意味着你可以找到一个单一的维度，能够保持这两个维度中包含的大部分方差。
- en: To summarize, to reduce the number of dimensions describing our data without
    losing information, you need to find a representation that *maximizes* the variance
    along each of its new axes, while reducing the dependence between the dimensions
    and getting rid of those with high covariance. This is exactly what *Principal
    Component Analysis*, or PCA, does. It finds a set of dimensions along which the
    variance is maximized. These dimensions are *orthonormal* (like *x,y* and *z*
    axes in the physical world) and are called *principal components* - hence the
    name of the method. PCA also allows you to see how much variance each dimension
    "is responsible for", so that you can choose the optimal number of principal components
    that preserve the "essence" of your data set. PCA then takes your data and projects
    it into a new set of coordinates.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，为了减少描述我们的数据的维数而不丢失信息，您需要找到一种表示，*最大化*其新轴上的方差，同时减少维度之间的依赖性，并消除具有高协方差的维度。
    这正是*主成分分析*（PCA）所做的。 它找到一组最大化方差的维度。 这些维度是*正交*的（就像物理世界中的*x，y*和*z*轴），称为*主成分* - 因此得名该方法。
    PCA 还允许您查看每个维度“负责”的方差有多少，以便您可以选择保留数据集“本质”的最佳主要成分数量。 然后，PCA 将您的数据投影到一组新坐标中。
- en: Before we dive into how PCA does that, let’s see the magic in action. In the
    following listing, you will use the PCA method of Scikit-Learn to take the same
    3D point cloud you’ve seen on the last page, and find a set of two dimensions
    that will maximize the variance of this point cloud.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入研究 PCA 如何做到这一点之前，让我们看看魔术是如何发挥作用的。 在下面的清单中，您将使用 Scikit-Learn 的 PCA 方法获取上一页上看到的相同的
    3D 点云，并找到一组最大化此点云方差的两个维度。
- en: Listing 4.5 PCA Magic
  id: totrans-146
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 4.5 PCA 魔法
- en: '[PRE12]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: When you reduce the dimensionality of 3-D points (vectors) to 2-D it’s like
    taking a picture of that 3-D cloud of points. The result may look like a picture
    on the right or the left of figure 4.4, but it will never tip or twist to a new
    angle. The x-axis (axis 0) will always be aligned along the longest axis of the
    point cloud points, where the points are spread out the most. That’s because PCA
    always finds the dimensions that will maximize the variance and arranges them
    in order of decreasing variance. The direction with the highest variance will
    become the first axis (x). The dimension with the second highest variance becomes
    the second dimension (y-axis) after the PCA transformation. However the *polarity*
    (sign) of these axes is arbitrary. The optimization is free to mirror (flip) the
    vectors (points) around the x or y axis, or both.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将 3D 点（向量）的维数减少到 2D 时，就像是拍摄了那个 3D 点云的照片。 结果可能看起来像图 4.4 的右边或左边的照片，但它永远不会倾斜或扭曲到新的角度。
    x 轴（轴 0）将始终沿着点云点的最长轴对齐，在那里点的分布最广泛。 这是因为 PCA 始终找到将最大化方差的维度，并按照方差递减的顺序排列它们。 具有最高方差的方向将成为第一个轴（x）。
    第二高方差的维度在 PCA 变换后成为第二维度（y 轴）。 但是这些轴的*极性*（符号）是任意的。 优化可以自由地围绕 x 轴或 y 轴镜像（翻转）向量（点），或两者兼而有之。
- en: Figure 4.4 Head-to-head horse point clouds upside down
  id: totrans-149
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.4 马头对马头的点云颠倒过来
- en: '![two horses](images/two-horses.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![两匹马](images/two-horses.png)'
- en: Now that we’ve seen PCA in the works,^([[12](#_footnotedef_12 "View footnote.")])
    let’s take a look at how it finds those principal components that allow us to
    work with our data in fewer dimensions without losing much information.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了 PCA 是如何工作的^([[12](#_footnotedef_12 "查看注释。")])，让我们看看它是如何找到那些允许我们在较少维度中处理数据而不丢失太多信息的主要成分的。
- en: 4.3.2 Singular Value Decomposition
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.2 奇异值分解
- en: At the heart of PCA is a mathematical procedure called Singular Value Decomposition,
    or SVD.^([[13](#_footnotedef_13 "View footnote.")]) SVD is an algorithm for decomposing
    any matrix into three "factors", three matrices that can be multiplied together
    to recreate the original matrix. This is analogous to finding exactly three integer
    factors for a large integer. But your factors aren’t scalar integers, they are
    2D real matrices with special properties.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 的核心是一种称为奇异值分解（SVD）的数学过程^([[13](#_footnotedef_13 "查看注释。")])。 SVD 是一种将任何矩阵分解为三个“因子”的算法，可以将这三个矩阵相乘以重新创建原始矩阵。
    这类似于为大整数找到确切的三个整数因子。 但是您的因子不是标量整数，而是具有特殊属性的 2D 实数矩阵。
- en: 'Let’s say we have our dataset, consisting of *m* n-dimensional points, represented
    by a matrix W. In its full version, this is what SVD of W would look like in math
    notation (assuming *m>n*):'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个数据集，由 *m* 个 n维点组成，用矩阵 W 表示。在其完整版本中，这就是 W 的SVD在数学符号中的样子（假设 *m>n*）：
- en: W[m] [x] [n] = U[m] [x] [m] S[m] [x] [n] V[n] [x] [n]^T
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: W[m] [x] [n] = U[m] [x] [m] S[m] [x] [n] V[n] [x] [n]^T
- en: The matrices U, S and V have special properties. U and V matrices are *orthogonal*,
    meaning that if you multiply them by their transposed versions, you’ll get a unit
    matrix. And S is *diagonal*, meaning that it has non-zero values only on its diagonal.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵 U、S 和 V 具有特殊的性质。U 和 V 矩阵是*正交*的，这意味着如果你将它们乘以它们的转置版本，你将得到一个单位矩阵。而 S 是*对角*的，意味着它只在对角线上有非零值。
- en: Note the equality sign in this formula. It means that if you multiply U, S and
    V, you’ll get *exactly* W, our original dataset. But you can see that the smallest
    dimension of our matrices is still *n*. Didn’t we want to reduce the number of
    dimensions? That’s why in this chapter, you’ll be using the version of SVD called
    *reduced*, or *truncated* SVD.^([[14](#_footnotedef_14 "View footnote.")]) That
    means that you’ll only looking for the top *p* dimensions that you’re interested
    in.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这个公式中的等号。它意味着如果你乘以 U、S 和 V，你会得到 *完全相同的* W，我们的原始数据集。但是你可以看到我们的矩阵的最小维度仍然是 *n*。我们不是想要减少维度吗？这就是为什么在这一章中，你将使用SVD的版本称为*减少*，或*截断*
    SVD。这意味着你只需要找到你感兴趣的前 *p* 个维度。
- en: At this point you could say "Wait, but couldn’t we do the full SVD and just
    take the dimensions that preserve maximum variance?" And you’ll be completely
    right, we could do it this way! However, there are other benefits to using truncated
    SVD. In particular, there are several algorithms that allow computing truncated
    SVD decomposition of the matrix pretty fast, especially when the matrice is sparse.
    *Sparse matrices* are matrices that have the same value (usually zero or NaN)
    in most of its cells. NLP bag-of-words and TF-IDF matrices are almost always sparse
    because most documents don’t contain many of the words in your vocabulary.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，你可能会说“等等，但我们不能做完整的SVD然后只取保留最大方差的维度吗？” 你完全正确，我们可以这样做！然而，使用截断SVD还有其他好处。特别是，有几种算法可以快速计算矩阵的截断SVD分解，特别是当矩阵是稀疏的时候。*稀疏矩阵*
    是指在其大多数单元格中具有相同值（通常为零或NaN）的矩阵。NLP词袋和TF-IDF矩阵几乎总是稀疏的，因为大多数文档不包含词汇表中的许多单词。
- en: 'This is what truncated SVD looks like:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是截断SVD的样子：
- en: W[m] [x] [n] ~ U[m] [x] [p] S[p] [x] [p] V[p] [x] [n]^T In this formula, *m*
    and *n* are the number of rows and columns in the original matrix, while *p* is
    the number of dimensions you want to keep. For example, in the horse example,
    *p* would be equal to two if we want to display the horse in a two-dimensional
    space. In the next chapter, when you’ll use SVD for LSA, it will signify the number
    of topics you want to use while analyzing your documents. Of course, *p* needs
    to be lesser than both *m* and *n*.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: W[m] [x] [n] ~ U[m] [x] [p] S[p] [x] [p] V[p] [x] [n]^T 在这个公式中，*m* 和 *n* 是原始矩阵中的行数和列数，而
    *p* 是您想要保留的维数。例如，在马的例子中，如果我们想要在二维空间中显示马，则 *p* 将等于二。在下一章中，当您使用SVD进行LSA时，它将表示您在分析文档时想要使用的主题数。当然，*p*
    需要小于 *m* 和 *n*。
- en: Note the "approximately equal" sign in this case - because we’re losing dimensions,
    we can’t expect to get exactly the same matrix when we multiply our factors! There’s
    always some loss of information. What we’re gaining, though, is a new way to represent
    our data with fewer dimensions than the original representation. With our horse
    point cloud, we are now able to convey its "horsy" essence without needing to
    print voluminous 3-D plots. And when PCA is used in real life, it can simplify
    hundred- or thousand-dimensional data into short vectors that are easier to analyze,
    cluster and visualize.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这种情况下的“近似等于”符号 - 因为我们失去了维度，所以当我们乘以我们的因子时，不能期望得到完全相同的矩阵！总会有一些信息丢失。然而，我们所获得的是一种新的表示方式，用比原始表示更少的维度来表示我们的数据。通过我们的马点云，我们现在能够传达其“马”的本质，而无需打印庞大的3D图。当PCA在现实生活中使用时，它可以将百或千维数据简化为更容易分析、聚类和可视化的短向量。
- en: So, what are the matrices U,S and V useful for? For now, we’ll give you a simple
    intuition of their roles. In the next chapter, we’ll dive deeper into these matrices'
    application when we talk about LSA.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，矩阵 U、S 和 V 有什么用呢？现在，我们将简单地介绍一下它们的作用。在下一章中，我们将深入探讨这些矩阵在LSA中的应用。
- en: Let’s start with *V^T* - or rather, with its transposed version *V*. *V* matrix’s
    columns are sometimes called *principal directions*, and sometimes *principal
    components*. As Scikit-Learn library, which you utilize in this chapter, uses
    the latter convention, we’re going to stick to it as well.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从*V^T*开始 - 或者更确切地说，从它的转置版本*V*开始。*V*矩阵的列有时被称为*主方向*，有时被称为*主成分*。由于本章中使用的Scikit-Learn库采用了后一种惯例，我们也将坚持使用这种说法。
- en: 'You can think of *V* as a "transformer" tool, that is used to map your data
    from the "old" space (its representation in matrix W’s "world") to the new, lower
    dimensional one. Imagine our we added a few more points to our 3D horse point
    cloud and now want to understand where those new points would be in our 2D representation,
    without needing to recalculate the transformation for all the points. To map every
    new point *q* to its location on a 2D plot, all you need to do is to multiply
    it by V:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将*V*看作是一个“转换器”工具，用于将你的数据从“旧”空间（在矩阵W的“世界”中的表示）映射到新的、低维度的空间。想象一下，我们在我们的3D马点云中添加了几个新点，现在想要了解这些新点在我们的2D表示中的位置，而不需要重新计算所有点的变换。要将每个新点*q*映射到其在2D图中的位置，你所需要做的就是将其乘以V：
- en: '`q̂ = q · V`'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '`q̂ = q · V`'
- en: What is, then the meaning of *`U · S`*? With some algebra wizardry, you can
    see that it is actually your data mapped into the new space! Basically, it your
    data points in new, lesser-dimensional representation.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 那么*`U · S`*的含义是什么呢？通过一些代数技巧，你会发现它实际上是你的数据映射到新空间！基本上，它是你的数据点在新的、更低维度的表示中。
- en: 4.4 Latent Semantic Analysis
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.4 潜在语义分析
- en: Finally, we can stop "horsing around" and get back to topic modeling! Let’s
    see how everything you’ve learned about dimensionality reduction, PCA and SVD
    will start making sense when we talk about finding topics and concepts in our
    text data.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以停止“围绕”，回到主题建模！让我们看看当我们谈论如何在我们的文本数据中找到主题和概念时，你所学到的关于降维、PCA和SVD的一切将开始变得有意义。
- en: 'Let’s start with the dataset itself. You’ll use the same comment corpus you
    used for the LDA classifier in section 4.1, and transform it into a matrix using
    TF-IDF. You might remember that the result is called a term-document matrix. This
    name is useful because it gives you an intuition on what the rows and the columns
    of the matrix contain: the rows would be terms, your vocabulary words; and the
    columns will be documents.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从数据集本身开始。你将使用第4.1节中用于LDA分类器的相同评论语料库，并使用TF-IDF将其转换为矩阵。你可能还记得结果被称为术语 - 文档矩阵。这个名字很有用，因为它让你直观地理解了矩阵的行和列包含的内容：行是术语，即你的词汇词；列将是文档。
- en: 'Let’s re-run listings 4.1 and 4.2 to get to our TF-IDF matrix again. Before
    diving into LSA, we examined the matrix shape:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新运行列表 4.1 和 4.2 以再次得到我们的 TF-IDF 矩阵。在深入LSA之前，我们研究了矩阵的形状：
- en: '[PRE13]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: So what do you have here? A 19,169-dimensional dataset, whose "space" is defined
    by the terms in the corpus vocabulary. It’s quite a hassle to work with a single
    vector representation of comments in this space, because there are almost 20,000
    numbers to work with in each vector - longer than the message itself! It’s also
    hard to see if the messages, or sentences inside them, are similar conceptually
    - for example, expressions like "leave this page" and "go away" will have very
    low similarity score, despite their meanings being very close to each other. So
    it’s much harder to cluster and classify documents in the way it’s represented
    in TF-IDF matrix.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 那么这里有什么？一个19,169维的数据集，其“空间”由语料库词汇中的术语定义。在这个空间中使用单个向量表示评论是相当麻烦的，因为每个向量中有将近20,000个数字
    - 比消息本身还要长！而且很难看出消息或其中的句子在概念上是否相似 - 例如，“离开这个页面”和“走开”这样的表达将具有非常低的相似度分数，尽管它们的含义非常接近。因此，在TF-IDF矩阵中表示文档的聚类和分类要困难得多。
- en: Also note that only 650 of your 5,000 messages (13%) are labeled as toxic. So
    you have an unbalanced training set with about 8:1 normal comments to toxic comments
    (personal attacks, obscenity, racial slurs, etc.). And you have a large vocabulary
    - the number of your vocabulary tokens (25172) is greater than the 4,837 messages
    (samples) you have to go on. So you have many more unique words in your vocabulary
    (or lexicon) than you have comments, and even more when you compare it to the
    number of toxic messages. That’s a recipe for overfitting.^([[15](#_footnotedef_15
    "View footnote.")]) Only a few unique words out of your large vocabulary will
    be labeled as "toxic" words in your dataset.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要注意，你的 5000 条消息中只有 650 条（13%）被标记为有毒。所以你的训练集是不平衡的，约有 8:1 的正常评论和有毒评论（人身攻击、淫秽语言、种族歧视等）。而且你的词汇量很大
    - 你的词汇量标记（25172）比你要处理的 4837 条消息（样本）还要多。所以你的词汇表（或词汇）中有很多更多的唯一词，而你的评论数量要少得多，甚至在与有毒消息数量比较时更多。这是一种过拟合的情况。^([[15](#_footnotedef_15
    "查看脚注。")]) 在你的大词汇表中，只有很少的唯一词会被标记为“有毒”词汇在你的数据集中。
- en: Overfitting means that you will "key" off of only a few words in your vocabulary.
    So your toxicity filter will be dependent on those toxic words being somewhere
    in the toxic messages it filters out. Trolls could easily get around your filter
    if they just used synonyms for those toxic words. If your vocabulary doesn’t include
    the new synonyms, then your filter will misclassify those cleverly constructed
    comments as non-toxic.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合意味着你的词汇表中只会“关键”几个词。所以你的毒性过滤器将依赖于那些毒性词在过滤出来的毒性消息中的位置。如果恶意用户只是使用那些毒性词的同义词，那么他们很容易绕过你的过滤器。如果你的词汇表不包括新的同义词，那么你的过滤器就会误将那些构造巧妙的评论分类为非毒性。
- en: And this overfitting problem is an inherent problem in NLP. It’s hard to find
    a labeled natural language dataset that includes all the ways that people might
    say something that should be labeled that way. We couldn’t find an "ideal" set
    of comments that included all the different ways people say toxic and nontoxic
    things. And only a few corporations have the resources to create such a dataset.
    So all the rest of us need to have "countermeasures" for overfitting. You have
    to use algorithms that "generalize" well on just a few examples.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这种过拟合问题是自然语言处理中的固有问题。很难找到一个标记的自然语言数据集，其中包含所有人们可能表达的应该被标记的方式。我们找不到一个“理想”的评论集，其中包含人们说有毒和无毒话的所有不同方式。只有少数几家公司有能力创建这样的数据集。所以我们其他人都需要对过拟合采取“对策”。你必须使用算法，在只有少数几个示例的情况下就能“泛化”得很好。
- en: The primary countermeasure to overfitting is to map this data into a new, lower-dimensional
    space. What will define this new space are weighted combinations of words, or
    *topics*, that your corpus talks about in a variety of ways. Representing your
    messages using topics, rather than specific term frequency, will make your NLP
    pipeline more "general", and allow our spam filter to work on a wider range of
    messages. That’s exactly what LSA does - it finds the new topic "dimensions",
    along which variance is maximized, using SVD method we discovered in the previous
    section.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗过拟合的主要措施是将这些数据映射到一个新的、低维空间中。定义这个新空间的是你的语料库以各种方式讨论的加权词汇组合，或者*话题*。用话题来表示你的消息，而不是具体的词频，会使你的自然语言处理管道更“通用”，并允许我们的垃圾邮件过滤器处理更广泛的消息。这正是
    LSA 所做的 - 它找到新的“维度”话题，使方差最大化，使用我们在前一节中发现的 SVD 方法。
- en: These new topics will not necessarily correlate to what we humans think about
    as topics, like "pets" or "history". The machine doesn’t "understand" what combinations
    of words mean, just that they go together. When it sees words like "dog", "cat",
    and "love" together a lot, it puts them together in a topic. It doesn’t know that
    such a topic is likely about "pets." It might include a lot of words like "domesticated"
    and "feral" in that same topic, words that mean the opposite of each other. If
    they occur together a lot in the same documents, LSA will give them high scores
    for the same topics together. It’s up to us humans to look at what words have
    a high weight in each topic and give them a name.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这些新话题不一定与我们人类认为的话题相关，比如“宠物”或“历史”。机器不“理解”单词组合的含义，只知道它们在一起。当它经常看到“狗”、“猫”和“爱”这样的词一起出现时，它会把它们放在一个话题中。它不知道这样的话题可能是关于“宠物”的。它可能会在同一个话题中包含很多像“驯养”的词和“野生”的词，它们是彼此的反义词。如果它们在同一份文件中经常出现在一起，LSA
    将为它们同时获得高分数。我们人类要看一下哪些词在每个话题中具有较高的权重，并为它们取一个名字。
- en: But you don’t have to give the topics a name to make use of them. Just as you
    didn’t analyze all the 1000s of dimensions in your stemmed bag-of-words vectors
    or TF-IDF vectors from previous chapters, you don’t have to know what all your
    topics "mean." You can still do vector math with these new topic vectors, just
    like you did with TF-IDF vectors. You can add and subtract them and estimate the
    similarity between documents based on their "topic representation", rather than
    "term frequency representation". And these similarity estimates will be more accurate,
    because your new representation actually takes into account the meaning of tokens
    and their co-occurence with other tokens.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.1 Diving into semantic analysis
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: But enough talking about LSA - let’s do some coding! This time, we’re going
    to use another Scikit-Learn tool named `TruncatedSVD` that performs - what a surprise
    - the truncated SVD method that we examined in the previous chapter. We could
    use the `PCA` model you saw in the previous section, but we’ll go with this more
    direct approach - it will allow us to understand better what’s happening "under
    the hood". In addition `TruncatedSVD` is meant to deal with sparse matrices, so
    it will perform better on most TF-IDF and BOW matrices.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: We will start with decreasing the number of dimensions from 9232 to 16 - we’ll
    explain later how we chose that number.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.6 LSA using TruncatedSVD
  id: totrans-182
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: What you have just produced using `fit-transform` method is your document vectors
    in the new representation. Instead of representing your comments with 19,169 frequency
    counts, you represented it with just 16\. This matrix is also called *document-topic*
    matrix. By looking at the columns, you can see how much every topic is "expressed"
    in every comment.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-185
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: How do the methods we use relate to the matrix decomposition process we described?
    You might have realized that what the `fit_transform` method returns is exactly
    \({U \cdot S}\) - your tf-idf vectors projected into the new space. And your V
    matrix is saved inside the `TruncatedSVD` object in the `components_` variable.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: If you want to explore your topics, you can find out how much of each word they
    "contain" by examining the weights of each word, or groups of words, across every
    topic.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: First let’s assign words to all the dimensions in your transformation. You need
    to get them in the right order because your `TFIDFVectorizer` stores the vocabulary
    as a dictionary that maps each term to an index number (column number).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Now you can create a nice Pandas DataFrame containing the weights, with labels
    for all the columns and rows in the right place. But it looks like our first few
    terms are just different combinations of newlines - that’s not very useful!
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Whoever gave you the dataset should have done a better job of cleaning them
    out. Let’s look at a few random terms from your vocabulary using the helpful Pandas
    method `DataFrame.sample()`
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: None of these words looks like "inherently toxic". Let’s look at some words
    that we would intuitively expect to appear in "toxic" comments, and see how much
    weight those words have in different topics.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Topics 2 and 4 appear to be more likely to contain toxic sentiment. And topic
    10 seems to be an "anti-toxic" topic. So words associated with toxicity can have
    a positive impact on some topics and a negative impact on others. There’s no single
    obvious toxic topic number.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: And what `transform` method does is just multiply whatever you pass to it with
    V matrix, which is saved in `components_`. You can check out the code of `TruncatedSVD`
    to see it with your own eyes! ^([[16](#_footnotedef_16 "View footnote.")]) link
    at the top left of the screen.]
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.2 TruncatedSVD or PCA?
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You might be asking yourself now - why did we use Scikit-Learn’s `PCA` class
    in the horse example, but `TruncatedSVD` for topic analysis for our comment dataset?
    Didn’t we say that PCA is based on the SVD algorithm?
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: And you will be right - if you look into the implementation of `PCA` and `TruncatedSVD`
    in `sklearn`, you’ll see that most of the code is similar between the two. They
    both use the same algorithms for SVD decomposition of matrices. However, there
    are several differences that might make each model preferrable for some use cases
    or others.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'The biggest difference is that `TruncatedSVD` does not center the matrix before
    the decomposition, while `PCA` does. What this means is that if you center your
    data before performing TruncatedSVD by subtracting columnwise mean from the matrix,
    like this:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: You’ll get the same results for both methods. Try this yourself by comparing
    the results of `TruncatedSVD` on centered data and of PCA, and see what you get!
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: The fact that the data is being centered is important for some properties of
    Principal Component Analysis,^([[17](#_footnotedef_17 "View footnote.")]) which,
    you might remember, has a lot of applications outside NLP. However, for TF-IDF
    matrices, that are mostly sparse, centering doesn’t always make sense. In most
    cases, centering makes a sparse matrix dense, which causes the model run slower
    and take much more memory. PCA is often used to deal with dense matrices and can
    compute a precise, full-matrix SVD for small matrices. In contrast, `TruncatedSVD`
    already assumes that the input matrix is sparse, and uses the faster approximated,
    randomized methods. So it deals with your TF-IDF data much more efficiently than
    PCA.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.3 How well LSA performs for toxicity detection?
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You’ve spent enough time peering into the topics - let’s see how our model
    performs with lower-dimensional representation of the comments! You’ll use the
    same code we ran in listing 4.3, but will apply it on the new 16-dimensional vectors.
    This time, the classification will go much faster:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Wow, what a difference! The classifier’s accuracy on the training set dropped
    from 99.9% for TF-IDF vectors to 88.1% But the test set accuracy jumped by 33%!
    That’s quite an improvement.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 哇，差异如此之大！分类器对TF-IDF向量的训练集准确率从99.9%下降到了88.1%，但测试集准确率却提高了33%！这是相当大的进步。
- en: 'Let’s check the F1 score:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看F1分数：
- en: '[PRE20]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We’ve almost doubled out F1 score, compared to TF-IDF vectors classification!
    Not bad.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的F1分数几乎比TF-IDF向量分类时翻了一番！不错。
- en: Unless you have a perfect memory, by now you must be pretty annoyed by scrolling
    or paging back to the performance of the previous model. And when you’ll be doing
    real-life natural langugae processing, you’ll probably be trying much more models
    than in our toy example. That’s why data scientists record their model parameters
    and performance in a *hyperparameter table*.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 除非你有完美的记忆力，到现在你一定对滚动或翻页找到之前模型的性能感到很烦恼。当你进行现实的自然语言处理时，你可能会尝试比我们的玩具示例中更多的模型。这就是为什么数据科学家会在*超参数表*中记录他们的模型参数和性能。
- en: Let’s make one of our own. First, recall the classification performance we got
    when we run an LDA classifier on TF-IDF vectors, and save it into our table.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们制作自己的超参数表。首先，回想一下在TF-IDF向量上运行LDA分类器时我们得到的分类性能，并将其保存到我们的表中。
- en: '[PRE21]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Actually, because you’re going to extract these scores for a few models, it
    might make sense to create a function that does this:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，因为你要提取几个模型的这些分数，所以创建一个执行这项任务的函数是有道理的：
- en: Listing 4.7 A function that creates a record in hyperparameter table.
  id: totrans-215
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.7 创建超参数表中记录的函数。
- en: '[PRE22]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'You can go even further and wrap most of your analysis in a nice function,
    so that you don’t have to copy-paste again:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 你甚至可以进一步将大部分分析包装在一个很好的函数中，这样你就不必再次复制粘贴：
- en: '[PRE23]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 4.4.4 Other ways to reduce dimensions
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.4 降维的其他方法
- en: SVD is by far the most popular way to reduce dimensions of a dataset, making
    LSA your first choice when thinking about topic modeling. However, there are several
    other dimensionality reduction techniques you can also use to achieve the same
    goal. Not all of them are even used in NLP, but it’s good to be aware of them.
    We’ll mention two methods here - *random projection* and *non-negative matrix
    factorization* (NMF).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: SVD是迄今为止最流行的降维数据集的方法，使LSA成为你在考虑主题建模时的首选。然而，还有几种其他降维技术可以用来达到相同的目标。并非所有技术都用于自然语言处理，但了解它们也是很好的。我们在这里提到了两种方法-
    *随机投影*和*非负矩阵分解*（NMF）。
- en: Random projection is a method to project a high-dimensional data on lower-dimensional
    space, so that the distances between data points are preserved. Its stochastic
    nature makes it easier to run it on parallel machines. It also allows the algorithm
    to use less memory as it doesn’t need to hold all the the data in the memory at
    the same time the way PCA does. And because its computational complexity lower,
    random projections can be occasionally used on datasets with very high dimensions,
    when decomposition speed is an important factor.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 随机投影是将高维数据投影到低维空间的方法，以便保留数据点之间的距离。其随机性使得能够在并行计算机上更容易运行。它还允许算法使用更少的内存，因为它不需要像PCA那样同时在内存中保存所有数据。并且由于它的计算复杂度较低，随机投影在处理维度非常高的数据集时可以偶尔使用，尤其是在分解速度成为重要因素时。
- en: Similarly, NMF is another matrix factorization method that is similar to SVD,
    but assumes that the data points and the components are all non-negative. It’s
    more commonly used in image processing and computer vision, but can occasionally
    come handy in NLP and topic modeling too.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，NMF是另一种矩阵因式分解方法，类似于SVD，但假设数据点和成分都是非负的。它在图像处理和计算机视觉中更常见，但在自然语言处理和主题建模中偶尔也很有用。
- en: In most cases, you’re better off sticking with LSA, which uses the tried and
    true SVD algorithm under the hood.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，最好坚持使用LSA，它在内部使用经过试验的SVD算法。
- en: 4.5 Latent Dirichlet allocation (LDiA)
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.5 潜在狄利克雷分配（LDiA）
- en: You’ve spent most of this chapter learning about latent semantic analysis and
    various ways to represent the underlying meaning of words and phrases as vectors
    using Scikit-Learn. LSA should be your first choice for most topic modeling, semantic
    search, or content-based recommendation engines.^([[18](#_footnotedef_18 "View
    footnote.")]) Its math is straightforward and efficient, and it produces a linear
    transformation that can be applied to new batches of natural language without
    training and with little loss in accuracy. Here you will learn about a more sophisticated
    algorithm, *Latent Dirichlet Allocation*, or "LDiA" to distinguish it from LDA,
    Linear Discriminant Analysis. LDiA will give you slightly better results in some
    situations.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的大部分时间里，你已经学习了关于潜在语义分析以及使用 Scikit-Learn 将单词和短语的潜在含义表示为向量的各种方法。LSA 应该是大多数主题建模、语义搜索或基于内容的推荐引擎的首选^([[18](#_footnotedef_18
    "查看脚注。")])。它的数学是简单而高效的，并且它产生的线性转换可以应用到新的自然语言批次中，而无需训练，且准确度损失很小。在这里，你将了解一个更复杂的算法，*潜在狄利克雷分配*，或称为"LDiA"以区别于LDA，即线性判别分析。在某些情况下，LDiA
    将会给你略微更好的结果。
- en: LDiA does a lot of the things you did to create your topic models with LSA (and
    SVD under the hood), but unlike LSA, LDiA assumes a Dirichlet distribution of
    word frequencies. It’s more precise about the statistics of allocating words to
    topics than the linear math of LSA.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: LDiA 做了很多与使用 LSA（和底层的 SVD）创建主题模型相似的事情，但与 LSA 不同的是，LDiA 假设单词频率呈狄利克雷分布。它在将单词分配给主题的统计学方面比
    LSA 的线性数学更精确。
- en: LDiA creates a semantic vector space model (like your topic vectors) using an
    approach similar to how your brain worked during the thought experiment earlier
    in the chapter. In your thought experiment, you manually allocated words to topics
    based on how often they occurred together in the same document. The topic mix
    for a document can then be determined by the word mixtures in each topic by which
    topic those words were assigned to. This makes an LDiA topic model much easier
    to understand, because the words assigned to topics and topics assigned to documents
    tend to make more sense than for LSA.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: LDiA 创建了一个语义向量空间模型（类似于你的主题向量），使用的方法类似于本章早些时候的思维实验中你的大脑是如何工作的。在你的思维实验中，你根据它们在同一篇文档中出现的频率手动将单词分配给主题。然后，文档的主题混合可以通过每个主题中单词混合来确定，这些单词被分配到哪个主题。这使得
    LDiA 主题模型更容易理解，因为分配给主题的单词和分配给文档的主题倾向于比 LSA 更有意义。
- en: LDiA assumes that each document is a mixture (linear combination) of some arbitrary
    number of topics that you select when you begin training the LDiA model. LDiA
    also assumes that each topic can be represented by a distribution of words (term
    frequencies). The probability or weight for each of these topics within a document,
    as well as the probability of a word being assigned to a topic, is assumed to
    start with a Dirichlet probability distribution (the *prior* if you remember your
    statistics). This is where the algorithm gets it name.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: LDiA 假设每个文档都是一些任意数量的主题（线性组合）的混合，你在开始训练 LDiA 模型时选择这些主题。LDiA 还假设每个主题可以由单词（术语频率）的分布表示。每个文档中这些主题的概率或权重，以及单词被分配给主题的概率，都假设从一个狄利克雷概率分布开始（如果你记得你的统计学，这是*先验*）。这就是算法得到它名字的地方。
- en: 4.5.1 The LDiA idea
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5.1 LDiA 的概念
- en: The LDiA approach was developed in 2000 by geneticists in the UK to help them
    "infer population structure" from sequences of genes.^([[19](#_footnotedef_19
    "View footnote.")]) Stanford Researchers (including Andrew Ng) popularized the
    approach for NLP in 2003.^([[20](#_footnotedef_20 "View footnote.")]) But don’t
    be intimidated by the big names that came up with this approach. We explain the
    key points of it in a few lines of Python shortly. You only need to understand
    it enough to get a feel for what it’s doing (an intuition), so you know what you
    can use it for in your pipeline.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: LDiA 方法是在 2000 年由英国的遗传学家们开发的，以帮助他们从基因序列中“推断人群结构”^([[19](#_footnotedef_19 "查看脚注。")])。斯坦福大学的研究人员（包括安德鲁·吴）于
    2003 年将该方法推广应用于 NLP^([[20](#_footnotedef_20 "查看脚注。")])。但不要被提出这种方法的大人物吓到。我们很快就会用几行
    Python 解释它的要点。你只需要理解足够多，以便对它正在做的事情有所感觉（直觉），这样你就知道你可以在管道中使用它做什么。
- en: Blei and Ng came up with the idea by flipping your thought experiment on its
    head. They imagined how a machine that could do nothing more than roll dice (generate
    random numbers) could write the documents in a corpus you want to analyze. And
    because you’re only working with bags of words, they cut out the part about sequencing
    those words together to make sense, to write a real document. They just modeled
    the statistics for the mix of words that would become a part of a particular the
    BOW for each document.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: They imagined a machine that only had two choices to make to get started generating
    the mix of words for a particular document. They imagined that the document generator
    chose those words randomly, with some probability distribution over the possible
    choices, like choosing the number of sides of the dice and the combination of
    dice you add together to create a D&D character sheet. Your document "character
    sheet" needs only two rolls of the dice. But the dice are large and there are
    several of them, with complicated rules about how they are combined to produce
    the desired probabilities for the different values you want. You want particular
    probability distributions for the number of words and number of topics so that
    it matches the distribution of these values in real documents analyzed by humans
    for their topics and words.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'The two rolls of the dice represent:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Number of words to generate for the document (Poisson distribution)
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Number of topics to mix together for the document (Dirichlet distribution)
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After it has these two numbers, the hard part begins, choosing the words for
    a document. The imaginary BOW generating machine iterates over those topics and
    randomly chooses words appropriate to that topic until it hits the number of words
    that it had decided the document should contain in step 1\. Deciding the probabilities
    of those words for topics — the appropriateness of words for each topic — is the
    hard part. But once that has been determined, your "bot" just looks up the probabilities
    for the words for each topic from a matrix of term-topic probabilities. If you
    don’t remember what that matrix looks like, glance back at the simple example
    earlier in this chapter.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: So all this machine needs is a single parameter for that Poisson distribution
    (in the dice roll from step 1) that tells it what the "average" document length
    should be, and a couple more parameters to define that Dirichlet distribution
    that sets up the number of topics. Then your document generation algorithm needs
    a term-topic matrix of all the words and topics it likes to use, its vocabulary.
    And it needs a mix of topics that it likes to "talk" about.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Let’s flip the document generation (writing) problem back around to your original
    problem of estimating the topics and words from an existing document. You need
    to measure, or compute, those parameters about words and topics for the first
    two steps. Then you need to compute the term-topic matrix from a collection of
    documents. That’s what LDiA does.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将文档生成（写作）问题反过来，回到你最初的问题，即从现有文档中估计主题和单词。您需要测量或计算前两步的有关单词和主题的参数。然后，您需要从一组文档中计算出术语-主题矩阵。这就是
    LDiA 的作用。
- en: 'Blei and Ng realized that they could determine the parameters for steps 1 and
    2 by analyzing the statistics of the documents in a corpus. For example, for step
    1, they could calculate the mean number of words (or *n*-grams) in all the bags
    of words for the documents in their corpus, something like this:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: Blei 和 Ng 意识到，可以通过分析语料库中文档的统计数据来确定步骤1和步骤2的参数。例如，针对步骤1，他们可以计算他们语料库中所有文档的单词（或
    *n*-grams）袋子中的平均数量，类似于这样：
- en: '[PRE24]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Or, using the sum function:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，使用 sum 函数：
- en: '[PRE25]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Keep in mind, you should calculate this statistic directly from your BOWs. You
    need to make sure you’re counting the tokenized and vectorized words in your documents.
    And make sure you’ve applied any stop word filtering, or other normalizations
    before you count up your unique terms. That way your count includes all the words
    in your BOW vector vocabulary (all the *n*-grams you’re counting), but only those
    words that your BOWs use (not stop words, for example). This LDiA algorithm relies
    on a bag-of-words vector space model, unlike LSA that took TF-IDF matrix as input.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您应直接从 BOW 中计算此统计数据。您需要确保计算您文档中的标记化和向量化单词。在计算您的唯一术语之前，请确保您已经应用了任何停用词过滤或其他标准化。这样，您的计数将包括您的
    BOW 向量词汇表中的所有单词（您正在计算的全部 *n*-grams），但仅包括您的 BOWs 使用的单词（例如不包括停用词）。与将 TF-IDF 矩阵作为输入的
    LSA 不同，此 LDiA 算法依赖于词袋向量空间模型。
- en: The second parameter you need to specify for an LDiA model, the number of topics,
    is a bit trickier. The number of topics in a particular set of documents can’t
    be measured directly until after you’ve assigned words to those topics. Like *k-means*
    and *KNN* and other clustering algorithms, you must tell it the *k* ahead of time.
    You can guess the number of topics (analogous to the *k* in k-means, the number
    of "clusters") and then check to see if that works for your set of documents.
    Once you’ve told LDiA how many topics to look for, it will find the mix of words
    to put in each topic to optimize its objective function.^([[21](#_footnotedef_21
    "View footnote.")])
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 LDiA 模型的第二个需要指定的参数——主题数——有点棘手。在分配单词到这些主题之后，您才能直接测量特定文档集中的主题数。与 *k-means*
    和 *KNN* 等其他聚类算法一样，您必须事先告诉它 *k*。您可以猜测主题数（类似于 k-means 中的 *k*，即“簇”的数量），然后检查它是否适用于文档集。告诉
    LDiA 要查找多少个主题后，它将找到每个主题中要放入的单词组合，以优化其目标函数。
- en: You can optimize this "hyperparameter" (*k*, the number of topics)^([[22](#_footnotedef_22
    "View footnote.")]) by adjusting it until it works for your application. You can
    automate this optimization if you can measure something about the quality of your
    LDiA language model for representing the meaning of your documents. One "cost
    function" you could use for this optimization is how well (or poorly) that LDiA
    model performs in some classification or regression problem, like sentiment analysis,
    document keyword tagging, or topic analysis. You just need some labeled documents
    to test your topic model or classifier on.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过调整“超参数”（主题数 *k*）来优化此参数，直到适合您的应用程序为止。如果您可以衡量 LDiA 语言模型在表示文档含义方面的质量的某些方面，则可以自动化此优化。您可以使用一些分类或回归问题（如情感分析、文档关键字标记或主题分析）中
    LDiA 模型的执行情况作为此优化的 “成本函数”。您只需要一些标记过的文档来测试您的主题模型或分类器。
- en: 4.5.2 LDiA topic model for comments
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5.2 评论的 LDiA 主题模型
- en: The topics produced by LDiA tend to be more understandable and "explainable"
    to humans. This is because words that frequently occur together are assigned the
    same topics, and humans expect that to be the case. Where LSA tries to keep things
    spread apart that were spread apart to start with, LDiA tries to keep things close
    together that started out close together.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: LDiA 产生的主题更易于人理解和“解释”。这是因为经常一起出现的单词被分配到相同的主题，而人们期望是这种情况。LSA 尝试保持原本分开的事物的分散程度，而
    LDiA 则试图保持原本在一起的事物的接近程度。
- en: This may sound like it’s the same thing, but it’s not. The math optimizes for
    different things. Your optimizer has a different objective function so it will
    reach a different objective. To keep close high-dimensional vectors close together
    in the lower-dimensional space, LDiA has to twist and contort the space (and the
    vectors) in nonlinear ways. This is a hard thing to visualize until you do it
    on something 3D and take "projections" of the resultant vectors in 2D.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how that works for a dataset of a few thousand comments, labeled for
    spaminess. First, compute the TF-IDF vectors and then some topics vectors for
    each SMS message (document). We assume the use of only 16 topics (components)
    to classify the spaminess of messages, as before. Keeping the number of topics
    (dimensions) low can help reduce overfitting.^([[23](#_footnotedef_23 "View footnote.")])
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 'LDiA works with raw BOW count vectors rather than normalized TF-IDF vectors.
    You’ve already done this process in Chapter 3:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Let’s double-check that your counts make sense for that first comment labeled
    "comment0":'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We’ll apply Latent Dirichlet Allocation to the count vector matrix in the same
    way we applied LSA to TF-IDF matrix:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: So your model has allocated your 19,169 words (terms) to 16 topics (components).
    Let’s take a look at the first few words and how they’re allocated. Keep in mind
    that your counts and topics will be different from ours. LDiA is a stochastic
    algorithm that relies on the random number generator to make some of the statistical
    decisions it has to make about allocating words to topics. So each time you run
    `sklearn.LatentDirichletAllocation` (or any LDiA algorithm), you will get different
    results unless you set the random seed to a fixed value.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: It looks like the values in LDiA topic vectors have much higher spread than
    LSA topic vectors - there are a lot of near-zero values, but also some really
    big ones. Let’s do the same trick you did when performing topic modeling with
    LSA. We can look at typical "toxic" words and see how pronounced they are in every
    topic.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: That looks very different from the LSA representation of our toxic terms! Looks
    like some terms can have high topic-term weights in some topics, but not others.
    `topic0` and `topic1` seem pretty "indifferent" to toxic terms, while topic 2
    and topic 15 have quite large topic-terms weight for at least 4 or 5 of the toxic
    terms. And `topic14` has a very high weight for the term `hate`!
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see what other terms scored high in this topic. As you saw earlier, because
    we didn’t do any preprocessing to our dataset, a lot of terms are not very interesting.
    Let’s focus on terms that are words, and are longer than 3 letters - that would
    eliminate a lot of the stop words.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: It looks like a lot of the words in the topic have semantic relationship between
    them. Words like "killed" and "hate", or "wicked" and "witch", seem to belong
    in the "toxic" domain. You can see that the allocation of words to topics can
    be rationalized or reasoned about, even with this quick look.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Before you fit your classifier, you need to compute these LDiA topic vectors
    for all your documents (comments). And let’s see how they are different from the
    topic vectors produced by LSA for those same documents.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: You can see that these topics are more cleanly separated. There are a lot of
    zeros in your allocation of topics to messages. This is one of the things that
    makes LDiA topics easier to explain to coworkers when making business decisions
    based on your NLP pipeline results.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: So LDiA topics work well for humans, but what about machines? How will your
    LDA classifier fare with these topics?
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.3 Detecting toxicity with LDiA
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s see how good these LDiA topics are at predicting something useful, such
    as comment toxicity. You’ll use your LDiA topic vectors to train an LDA model
    again (like you did twice - with your TF-IDF vectors and LSA topic vectors). And
    because of the handy function you defined in listing 4.5, you only need a couple
    of lines of code to evaluate your model:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: It looks that the classification performance on 16-topic LDIA vectors is worse
    than on the raw TF-IDF vectors, without topic modeling. Does it mean the LDiA
    is useless in this case? Let’s not give up on it too soon and try to increase
    the number of topics.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '4.5.4 A fairer comparison: 32 LDiA topics'
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s try one more time with more dimensions, more topics. Perhaps LDiA isn’t
    as efficient as LSA so it needs more topics to allocate words to. Let’s try 32
    topics (components).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: That’s nice! Increasing the dimensions for LDIA almost doubled both the precision
    and the recall of the models, and our F1 score looks much better. The larger number
    of topics allows LDIA to be more precise about topics, and, at least for this
    dataset, produce topics that linearly separate better. But the performance of
    these vector representations still is not quite as good as that of LSA. So LSA
    is keeping your comment topic vectors spread out more efficiently, allowing for
    a wider gap between comments to cut with a hyperplane to separate classes.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to explore the source code for the Dirichlet allocation models available
    in both Scikit-Learn as well as `gensim`. They have an API similar to LSA (`sklearn.TruncatedSVD`
    and `gensim.LsiModel`). We’ll show you an example application when we talk about
    summarization in later chapters. Finding explainable topics, like those used for
    summarization, is what LDiA is good at. And it’s not too bad at creating topics
    useful for linear classification.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-277
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'You saw earlier how you can browse the source code of all ''sklearn'' from
    the documentation pages. But there is even a more straightforward method to do
    it from your Python console. You can find the source code path in the `__file__`
    attribute on any Python module, such as `sklearn.__file__`. And in `ipython` (`jupyter
    console`), you can view the source code for any function, class, or object with
    `??`, like `LDA??`:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This won’t work on functions and classes that are extensions, whose source code
    is hidden within a compiled C++ module.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: 4.6 Distance and similarity
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We need to revisit those similarity scores we talked about in chapters 2 and
    3 to make sure your new topic vector space works with them. Remember that you
    can use similarity scores (and distances) to tell how similar or far apart two
    documents are based on the similarity (or distance) of the vectors you used to
    represent them.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: You can use similarity scores (and distances) to see how well your LSA topic
    model agrees with the higher-dimensional TF-IDF model of chapter 3\. You’ll see
    how good your model is at retaining those distances after having eliminated a
    lot of the information contained in the much higher-dimensional bags of words.
    You can check how far away from each other the topic vectors are and whether that’s
    a good representation of the distance between the documents' subject matter. You
    want to check that documents that mean similar things are close to each other
    in your new topic vector space.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: LSA preserves large distances, but it does not always preserve close distances
    (the fine "structure" of the relationships between your documents). The underlying
    SVD algorithm is focused on maximizing the variance between all your documents
    in the new topic vector space.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 'Distances between feature vectors (word vectors, topic vectors, document context
    vectors, and so on) drive the performance of an NLP pipeline, or any machine learning
    pipeline. So what are your options for measuring distance in high-dimensional
    space? And which ones should you chose for a particular NLP problem? Some of these
    commonly used examples may be familiar from geometry class or linear algebra,
    but many others are probably new to you:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: 'Euclidean or Cartesian distance, or root mean square error (RMSE): 2-norm or
    L[2]'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Squared Euclidean distance, sum of squares distance (SSD): L[2]²'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cosine or angular or projected distance: normalized dot product'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Minkowski distance: p-norm or L[p]'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fractional distance, fractional norm: p-norm or L[p] for `0 < p < 1`'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'City block, Manhattan, or taxicab distance, sum of absolute distance (SAD):
    1-norm or L[1]'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jaccard distance, inverse set similarity
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mahalanobis distance
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Levenshtein or edit distance
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The variety of ways to calculate distance is a testament to how important it
    is. In addition to the pairwise distance implementations in Scikit-Learn, many
    others are used in mathematics specialties such as topology, statistics, and engineering.^([[24](#_footnotedef_24
    "View footnote.")]) For reference, here are all the ways you can compute distances
    in the `sklearn.metrics` module: ^([[25](#_footnotedef_25 "View footnote.")])'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.8 Pairwise distances available in `sklearn`
  id: totrans-296
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Distance measures are often computed from similarity measures (scores) and
    vice versa such that distances are inversely proportional to similarity scores.
    Similarity scores are designed to range between 0 and 1\. Typical conversion formulas
    look like this:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'But for distances and similarity scores that range between 0 and 1, like probabilities,
    it’s more common to use a formula like this:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'And cosine distances have their own convention for the range of values they
    use. The angular distance between two vectors is often computed as a fraction
    of the maximum possible angular separation between two vectors, which is 180 degrees
    or `pi` radians.^([[26](#_footnotedef_26 "View footnote.")]) As a result cosine
    similarity and distance are the reciprocal of each other:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Why do we spend so much time talking about distances? In the last section of
    this book, we’ll be talking about semantic search. The idea behind semantic search
    is to find documents that have the highest *semantic similarity* with your search
    query - or the lowest *semantic distance*. In our semantic search application,
    we’ll be using cosine similarity - but as you can see in the last two pages, there
    are multiple ways to measure how similar documents are.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: 4.7 Steering with feedback
  id: totrans-305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All the previous approaches to semantic analysis failed to take into account
    information about the similarity between documents. We created topics that were
    optimal for a generic set of rules. Our unsupervised learning of these models
    for feature (topic) extraction didn’t have any data about how "close" the topic
    vectors should be to each other. We didn’t allow any "feedback" about where the
    topic vectors ended up, or how they were related to each other.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: Steering or "learned distance metrics"^([[27](#_footnotedef_27 "View footnote.")])
    are the latest advancement in dimension reduction and feature extraction. By adjusting
    the distance scores reported to clustering and embedding algorithms, you can "steer"
    your vectors so that they minimize some cost function. In this way you can force
    your vectors to focus on some aspect of the information content that you’re interested
    in.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: In the previous sections about LSA, you ignored all the meta information about
    your documents. For example, with the comments you ignored the sender of the message.
    This is a good indication of topic similarity and could be used to inform your
    topic vector transformation (LSA).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: At Talentpair we experimented with matching resumes to job descriptions using
    the cosine distance between topic vectors for each document. This worked OK. But
    we learned pretty quickly that we got much better results when we started "steering"
    our topic vectors based on feedback from candidates and account managers responsible
    for helping them find a job. Vectors for "good pairings" were steered closer together
    than all the other pairings.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: One way to do this is to calculate the mean difference between your two centroids
    (like you did for LDA) and add some portion of this "bias" to all the resume or
    job description vectors. Doing so should take out the average topic vector difference
    between resumes and job descriptions. Topics such as beer on tap at lunch might
    appear in a job description but never in a resume. Similarly bizarre hobbies,
    such as underwater sculpture, might appear in some resumes but never a job description.
    Steering your topic vectors can help you focus them on the topics you’re interested
    in modeling.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: 4.8 Topic vector power
  id: totrans-311
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With topic vectors, you can do things like compare the meaning of words, documents,
    statements, and corpora. You can find "clusters" of similar documents and statements.
    You’re no longer comparing the distance between documents based merely on their
    word usage. You’re no longer limited to keyword search and relevance ranking based
    entirely on word choice or vocabulary. You can now find documents that are relevant
    to your query, not just a good match for the word statistics themselves.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: This is called "semantic search", not to be confused with the "semantic web."^([[28](#_footnotedef_28
    "View footnote.")]) Semantic search is what strong search engines do when they
    give you documents that don’t contain many of the words in your query, but are
    exactly what you were looking for. These advanced search engines use LSA topic
    vectors to tell the difference between a `Python` package in "The Cheese Shop"
    and a python in a Florida pet shop aquarium, while still recognizing its similarity
    to a "Ruby gem."^([[29](#_footnotedef_29 "View footnote.")])
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: Semantic search gives you a tool for finding and generating meaningful text.
    But our brains are not good at dealing with high-dimensional objects, vectors,
    hyperplanes, hyperspheres, and hypercubes. Our intuitions as developers and machine
    learning engineers breaks down above three dimensions.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: For example, to do a query on a 2D vector, like your lat/lon location on Google
    Maps, you can quickly find all the coffee shops nearby without much searching.
    You can just scan (with your eyes or with code) near your location and spiral
    outward with your search. Alternatively, you can create bigger and bigger bounding
    boxes with your code, checking for longitudes and latitudes within some range
    on each, that’s just for comparison operations and that should find you everything
    nearby.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: However, dividing up a high dimensional vector space (hyperspace) with hyperplanes
    and hypercubes as the boundaries for your search is impractical, and in many cases,
    impossible.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: As Geoffry Hinton says, "To deal with hyperplanes in a 14-dimensional space,
    visualize a 3D space and say 14 to yourself loudly." If you read Abbott’s 1884
    *Flatland* when you were young and impressionable, you might be able to do a little
    bit better than this hand waving. You might even be able to poke your head partway
    out of the window of your 3D world into hyperspace, enough to catch a glimpse
    of that 3D world from the outside. Like in *Flatland*, you used a lot of 2D visualizations
    in this chapter to help you explore the shadows that words in hyperspace leave
    in your 3D world. If you’re anxious to check them out, skip ahead to the section
    showing "scatter matrices" of word vectors. You might also want to glance back
    at the 3D bag-of-words vector in the previous chapter and try to imagine what
    those points would look like if you added just one more word to your vocabulary
    to create a 4-D world of language meaning.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: If you’re taking a moment to think deeply about four dimensions, keep in mind
    that the explosion in complexity you’re trying to wrap your head around is even
    greater than the complexity growth from 2D to 3D and exponentially greater than
    the growth in complexity from a 1D world of numbers to a 2D world of triangles,
    squares, and circles.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: 4.8.1 Semantic search
  id: totrans-319
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When you search for a document based on a word or partial word it contains,
    that’s called *full text search*. This is what search engines do. They break a
    document into chunks (usually words) that can be indexed with an *inverted index*
    like you’d find at the back of a textbook. It takes a lot of bookkeeping and guesswork
    to deal with spelling errors and typos, but it works pretty well.^([[30](#_footnotedef_30
    "View footnote.")])
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: Semantic search is full-text search that takes into account the meaning of the
    words in your query and the documents you’re searching. In this chapter, you’ve
    learned two ways —  LSA and LDiA — to compute topic vectors that capture the semantics
    (meaning) of words and documents in a vector. One of the reasons that latent semantic
    analysis was first called latent semantic *indexing* was because it promised to
    power semantic search with an index of numerical values, like BOW and TF-IDF tables.
    Semantic search was the next big thing in information retrieval.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: But unlike BOW and TF-IDF tables, tables of semantic vectors can’t be easily
    discretized and indexed using traditional inverted index techniques. Traditional
    indexing approaches work with binary word occurrence vectors, discrete vectors
    (BOW vectors), sparse continuous vectors (TF-IDF vectors), and low-dimensional
    continuous vectors (3D GIS data). But high-dimensional continuous vectors, such
    as topic vectors from LSA or LDiA, are a challenge.^([[31](#_footnotedef_31 "View
    footnote.")]) Inverted indexes work for discrete vectors or binary vectors, like
    tables of binary or integer word-document vectors, because the index only needs
    to maintain an entry for each nonzero discrete dimension. Either that value of
    that dimension is present or not present in the referenced vector or document.
    Because TF-IDF vectors are sparse, mostly zero, you don’t need an entry in your
    index for most dimensions for most documents.^([[32](#_footnotedef_32 "View footnote.")])
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: LSA (and LDiA) produce topic vectors that are high-dimensional, continuous,
    and dense (zeros are rare). And the semantic analysis algorithm does not produce
    an efficient index for scalable search. In fact, the curse of dimensionality that
    you talked about in the previous section makes an exact index impossible. The
    "indexing" part of latent semantic indexing was a hope, not a reality, so the
    LSI term is a misnomer. Perhaps that is why LSA has become the more popular way
    to describe semantic analysis algorithms that produce topic vectors.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: One solution to the challenge of high-dimensional vectors is to index them with
    a *locality-sensitive hash* (LSH). A locality-sensitive hash is like a zip code
    (postal code) that designates a region of hyperspace so that it can easily be
    found again later. And like a regular hash, it is discrete and depends only on
    the values in the vector. But even this doesn’t work perfectly once you exceed
    about 12 dimensions. In Figure 4.6, each row represents a topic vector size (dimensionality),
    starting with 2 dimensions and working up to 16 dimensions, like the vectors you
    used earlier for the SMS spam problem.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.5 Semantic search accuracy deteriorates at around 12-D
  id: totrans-325
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![semantic search lsh table](images/semantic-search-lsh-table.png)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
- en: The table shows how good your search results would be if you used locality sensitive
    hashing to index a large number of semantic vectors. Once your vector had more
    than 16 dimensions, you’d have a hard time returning 2 search results that were
    any good.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: So how can you do semantic search on 100-D vectors without an index? You now
    know how to convert the query string into a topic vector using LSA. And you know
    how to compare two vectors for similarity using the cosine similarity score (the
    scalar product, inner product, or dot product) to find the closest match. To find
    precise semantic matches, you need to find all the closest document topic vectors
    to a particular query (search) topic vector. (In the professional lingo, it’s
    called *exhaustive search*.) But if you have *n* documents, you have to do *n*
    comparisons with your query topic vector. That’s a lot of dot products.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: You can vectorize the operation in `numpy` using matrix multiplication, but
    that doesn’t reduce the number of operations, it only makes them 100 times faster.^([[33](#_footnotedef_33
    "View footnote.")]) Fundamentally, exact semantic search still requires *O*(*N*)
    multiplications and additions for each query. So it scales only linearly with
    the size of your corpus. That wouldn’t work for a large corpus, such as Google
    Search or even Wikipedia semantic search.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: The key is to settle for "good enough" rather than striving for a perfect index
    or LSH algorithm for our high-dimensional vectors. There are now several open
    source implementations of some efficient and accurate *approximate nearest neighbors*
    algorithms that use LSH to efficiently implement semantic search. We’ll talk more
    about them in chapter 10.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: Technically these indexing or hashing solutions cannot guarantee that you will
    find all the best matches for your semantic search query. But they can get you
    a good list of close matches almost as fast as with a conventional reverse index
    on a TF-IDF vector or bag-of-words vector, if you’re willing to give up a little
    precision.^([[34](#_footnotedef_34 "View footnote.")])
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: 4.9 Equipping your bot with semantic search
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s use your newly-acquired knowledge in topic modeling to improve the bot
    you started to build in the previous chapter. We’ll focus on the same task - question
    answering.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: Our code is actually going to be pretty similar to your code in chapter 3\.
    We will still use vector representations to find the most similar question in
    our dataset. But this time, our representations are going to be closer to representing
    the meaning of those questions.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s load the question and answer data just like we did in the last
    chapter
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The next step is to represent both the questions and our query as vectors. This
    is where we need to add just a few lines to make our representations semantic.
    Because our qestion dataset is small, we won’t need to apply LSH or any other
    indexing algorithm.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Let’s do a sanity check of our model and make sure it still can answer easy
    questions:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Now, let’s give our model a tougher nut to crack - like the question our previous
    model wasn’t good in dealing with. Can it do better?
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Wow! Looks like the new version of our bot was able to "realize" that 'decrease'
    and 'reduce' have similar meanings. Not only that, it was also able to "understand"
    that 'Logistic Regression' and "LogisticRegression" are very close - such a simple
    step was almost impossible for our TF-IDF model.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: Looks like we’re getting closer to building a truly robust question-answering
    system. We’ll see in the next chapter how we can do even better than topic modeling!
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: 4.10 What’s Next?
  id: totrans-345
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the next chapters, you’ll learn how to fine tune this concept of topic vectors
    so that the vectors associated with words are more precise and useful. To do this
    we first start learning about neural nets. This will improve your pipeline’s ability
    to extract meaning from short texts or even solitary words.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: 4.11 Test yourself
  id: totrans-347
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What preprocessing techniques would you use to prepare your text for more efficient
    topic modeling with LDiA? What about LSA?
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can you think of a dataset/problem where TF-IDF performs better than LSA? What
    about the opposite?
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We mentioned filtering stopwords as a prep process for LDiA. When would this
    filtering be beneficial?
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main challenge of semantic search is that the dense LSA topic vectors are
    not inverse-indexable. Can you explain why it’s so?
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4.12 Summary
  id: totrans-352
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can derive the meaning of your words and documents by analyzing the co-occurence
    of terms in your dataset.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SVD can be used for semantic analysis to decompose and transform TF-IDF and
    BOW vectors into topic vectors.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyperparameter table can be used to compare the performances of different pipelines
    and models.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use LDiA when you need to conduct an explainable topic analysis.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No matter how you create your topic vectors, they can be used for semantic search
    to find documents based on their meaning.
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[1]](#_footnoteref_1) We use the term "topic vector" in this chapter about
    topic analysis and we use the term "word vector" in chapter 6 about Word2vec.
    Formal NLP texts such as the NLP bible by Jurafsky and Martin ( [https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf#chapter.15:](slp3.html))
    use "topic vector." Others, like the authors of Semantic Vector Encoding and Similarity
    Search ( [https://arxiv.org/pdf/1706.00957.pdf:](pdf.html)), use the term "semantic
    vector."'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: '[[2]](#_footnoteref_2) Both stemming and lemmatization remove or alter the
    word endings and prefixes, the last few characters of a word. Edit-distance calculations
    are better for identifying similarly spelled (or misspelled) words'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: '[[3]](#_footnoteref_3) I love Google Ngram Viewer for visualizing trends like
    this one: ( [http://mng.bz/ZoyA](mng.bz.html)).'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: '[[4]](#_footnoteref_4) Doug Lenat at Stanford is trying to do just that, code
    common sense into an algorithm. See the Wired Magazine article "Doug Lenat’s Artificial
    Intelligence Common Sense Engine" ( [https://www.wired.com/2016/03/doug-lenat-artificial-intelligence-common-sense-engine](03.html)).'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: '[[5]](#_footnoteref_5) A *morpheme* is the smallest meaningful parts of a word.
    See Wikipedia article "Morpheme" ( [https://en.wikipedia.org/wiki/Morpheme](wiki.html)).'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: '[[6]](#_footnoteref_6) The wikipedia page for topic models has a video that
    shows the intuition behind LSA. [http://mng.bz/VRYW](mng.bz.html)'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: '[[7]](#_footnoteref_7) The larger version of this dataset was a basis for a
    Kaggle competition in 2017( [https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge](c.html)),
    and was released by Jigsaw under CC0 license.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: '[[8]](#_footnoteref_8) A centroid of a cluster is a point whose coordinates
    are the average of the coordinates of all the points in that cluster.'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: '[[9]](#_footnoteref_9) To gain some more intuition about precision and recall,
    Wikipedia’s article ( [https://en.wikipedia.org/wiki/Precision_and_recall](wiki.html))
    has some good visuals.'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: '[[10]](#_footnoteref_10) You can read more about the reasons *not* to use F
    [1] score in some cases, and about alternative metrics in the Wikipedia article:
    [https://en.wikipedia.org/wiki/F-score](wiki.html)'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '[[11]](#_footnoteref_11) You can see a visual example of the two estimator’s
    in Scikit-Learn’s documentation: [https://scikit-learn.org/dev/modules/lda_qda.html](modules.html)'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: '[[12]](#_footnoteref_12) To understand dimensionality reduction more in depth,
    check out this great 4-part post series by Hussein Abdullatif: [http://mng.bz/RlRv](mng.bz.html)'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: '[[13]](#_footnoteref_13) There are actually two main ways to perform PCA; you
    can dig into the Wikipedia article for PCA ( [https://en.wikipedia.org/wiki/Principal_component_analysis#Singular_value_decomposition](wiki.html))
    and see what the other method is and how the two basically yield an almost identical
    result.'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '[[14]](#_footnoteref_14) To learn more about *Full* SVD and its other applications,
    you can read the Wikipedia article: [https://en.wikipedia.org/wiki/Singular_value_decomposition](wiki.html)'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: '[[15]](#_footnoteref_15) See the web page titled "Overfitting - Wikipedia"
    ( [https://en.wikipedia.org/wiki/Overfitting](wiki.html)).'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: '[[16]](#_footnoteref_16) You can access the code of any Scikit-Learn function
    by clicking the [source'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: '[[17]](#_footnoteref_17) You can dig into the maths of PCA here: [https://en.wikipedia.org/wiki/Principal_component_analysis](wiki.html)'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: '[[18]](#_footnoteref_18) A 2015 comparison of content-based movie recommendation
    algorithms by Sonia Bergamaschi and Laura Po found LSA to be approximately twice
    as accurate as LDiA. See "Comparing LDA and LSA Topic Models for Content-Based
    Movie Recommendation Systems" by Sonia Bergamaschi and Laura Po ( [https://www.dbgroup.unimo.it/~po/pubs/LNBI_2015.pdf](pubs.html)).'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: '[[19]](#_footnoteref_19) "Jonathan K. Pritchard, Matthew Stephens, Peter Donnelly,
    Inference of Population Structure Using Multilocus Genotype Data" [http://www.genetics.org/content/155/2/945](2.html)'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: '[[20]](#_footnoteref_20) See the PDF titled "Latent Dirichlet Allocation" by
    David M. Blei, Andrew Y. Ng, and Michael I. Jordan ( [http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf](blei03a.html)).'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: '[[21]](#_footnoteref_21) You can learn more about the particulars of the LDiA
    objective function here in the original paper "Online Learning for Latent Dirichlet
    Allocation" by Matthew D. Hoffman, David M. Blei, and Francis Bach ( [https://www.di.ens.fr/%7Efbach/mdhnips2010.pdf](%7Efbach.html)).'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: '[[22]](#_footnoteref_22) The symbol used by Blei and Ng for this parameter
    was *theta* rather than *k*'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: '[[23]](#_footnoteref_23) See Appendix D if you want to learn more about why
    overfitting is a bad thing and how *generalization* can help.'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: '[[24]](#_footnoteref_24) See Math.NET Numerics for more distance metrics (
    [https://numerics.mathdotnet.com/Distance.html](numerics.mathdotnet.com.html)).'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: '[[25]](#_footnoteref_25) See the documentation for sklearn.metrics ( [https://scikit-learn.org/stable/modules/generated/sklearn.metrics.DistanceMetric.html](generated.html)).'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: '[[26]](#_footnoteref_26) See the web page titled "Cosine similarity - Wikipedia"
    ( [https://en.wikipedia.org/wiki/Cosine_similarity](wiki.html)).'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: '[[27]](#_footnoteref_27) See the web page titled "eccv spgraph" ( [http://users.cecs.anu.edu.au/~sgould/papers/eccv14-spgraph.pdf](papers.html)).'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: '[[28]](#_footnoteref_28) The semantic web is the practice of structuring natural
    language text with the use of tags in an HTML document so that the hierarchy of
    tags and their content provide information about the relationships (web of connections)
    between elements (text, images, videos) on a web page.'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: '[[29]](#_footnoteref_29) Ruby is a programming language whose packages are
    called `gems`.'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: '[[30]](#_footnoteref_30) A full-text index in a database like PostgreSQL is
    usually based on trigrams of characters, to deal with spelling errors and text
    that doesn’t parse into words.'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '[[31]](#_footnoteref_31) Clustering high-dimensional data is equivalent to
    discretizing or indexing high-dimensional data with bounding boxes and is described
    in the Wikipedia article "Clustering high dimensional data" ( [https://en.wikipedia.org/wiki/Clustering_high-dimensional_data](wiki.html)).'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: '[[32]](#_footnoteref_32) See the web page titled "Inverted index - Wikipedia"
    ( [https://en.wikipedia.org/wiki/Inverted_index](wiki.html)).'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: '[[33]](#_footnoteref_33) Vectorizing your Python code, especially doubly-nested
    `for` loops for pairwise distance calculations can speed your code by almost 100-fold.
    See Hacker Noon article "Vectorizing the Loops with Numpy" ( [https://hackernoon.com/speeding-up-your-code-2-vectorizing-the-loops-with-numpy-e380e939bed3](hackernoon.com.html)).'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: '[[34]](#_footnoteref_34) If you want to learn about faster ways to find a high-dimensional
    vector’s nearest neighbors, check out appendix F, or just use the Spotify `annoy`
    package to index your topic vectors.'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
