- en: Chapter 13\. Summary, conclusions, and beyond
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第13章。 总结、结论和展望
- en: '*This chapter covers*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章包括*'
- en: Looking back at the high-level concepts and ideas about AI and deep learning
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回顾人工智能和深度学习的高层次概念和想法
- en: A quick overview of the different types of deep-learning algorithms we’ve visited
    in this book, when they are useful, and how to implement them in TensorFlow.js
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在本书中访问的不同类型的深度学习算法的快速概述，它们何时有用，以及如何在 TensorFlow.js 中实现它们
- en: Pretrained models from the ecosystem of TensorFlow.js
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自 TensorFlow.js 生态系统的预训练模型
- en: Limitations of deep learning as it currently stands; and an educated prediction
    for trends in deep learning that we will see in the coming years
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习当前存在的局限性；以及我们将在未来几年看到的深度学习趋势的教育预测
- en: Guidance for how to further advance your deep-learning knowledge and stay up-to-date
    with the fast-moving field
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何进一步提升你的深度学习知识并跟上这个快速发展的领域的指导
- en: This is the final chapter of this book. Previous chapters have been a grand
    tour of the current landscape of deep learning, enabled by the vehicles of TensorFlow.js
    and your own hard work. Through this journey, you have hopefully gained quite
    a few new concepts and skills. It is time to step back and look at the big picture
    again, as well as get a refresher on some of the most important concepts you’ve
    learned. This last chapter will summarize and review core concepts while expanding
    your horizons beyond the relatively basic notions you’ve learned so far. We want
    to make sure you realize this and are properly equipped to take the next steps
    of the journey on your own.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这是本书的最后一章。之前的章节是对当前深度学习领域的总体概况，通过 TensorFlow.js 和你自己的努力实现。通过这段旅程，你可能已经学到了很多新的概念和技能。现在是时候再次退后一步，重新审视全局，并对你学到的一些最重要的概念进行复习。这最后一章将总结和审查核心概念，同时将你的视野扩展到迄今为止学到的相对基本的概念之外。我们希望确保你意识到这一点，并且准备好自己继续旅程的下一步。
- en: We’ll start with a bird’s-eye view of what you should take away from this book.
    This should refresh your memory regarding some of the concepts you’ve learned.
    Next, we’ll present an overview of some key limitations of deep learning. To use
    a tool properly, you should not only know what it *can* do but also what it *can’t*
    do. The chapter ends with a list of resources and strategies for furthering your
    knowledge and skills about deep learning and AI in the JavaScript ecosystem and
    staying up-to-date with new developments.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从鸟瞰视角开始，总结你应该从这本书中学到的东西。这应该让你记起你学到的一些概念。接下来，我们将概述深度学习的一些关键局限性。要正确使用工具，你不仅应该知道它
    *能* 做什么，还应该知道它 *不能* 做什么。本章以资源列表和进一步了解深度学习和 JavaScript 生态系统中人工智能知识和技能的策略结束，并保持与新发展的步伐同步。
- en: 13.1\. Key concepts in review
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.1\. 复习重点概念
- en: This section briefly synthesizes the key takeaways from this book. We will start
    from the overall landscape of the AI field and end with why bringing deep learning
    and JavaScript together introduces unique and exciting opportunities.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分简要总结了这本书的关键要点。我们将从人工智能领域的整体格局开始，以为什么将深度学习和 JavaScript 结合起来会带来独特而令人兴奋的机遇而结束。
- en: 13.1.1\. Various approaches to AI
  id: totrans-11
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 13.1.1\. 人工智能的各种方法
- en: First of all, deep learning is not synonymous with AI or even with machine learning.
    *Artificial intelligence* is a broad field with a long history. It can generally
    be defined as “all attempts to automate the cognitive process”—in other words,
    the automation of thought. This can range from very basic tasks, such as an Excel
    spreadsheet, to very advanced endeavors, such as a humanoid robot that can walk
    and talk.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，深度学习与人工智能甚至与机器学习并不是同义词。 *人工智能* 是一个历史悠久的广泛领域。它通常可以定义为“所有试图自动化认知过程”的尝试——换句话说，思维的自动化。这可以从非常基本的任务，如
    Excel 电子表格，到非常高级的努力，如一个可以行走和说话的类人机器人。
- en: '*Machine learning* is one of the many subfields of AI. It aims at automatically
    developing programs (called *models*) purely from exposure to training data. This
    process of turning data into a program (the model) is called *learning*. Although
    machine learning has been around for a long time (at least several decades), it
    only started to take off in practical applications in the 1990s.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*机器学习* 是人工智能的许多子领域之一。它旨在通过暴露给训练数据自动开发程序（称为 *模型*）。这个将数据转化为程序（模型）的过程被称为 *学习*。尽管机器学习已经存在了很长一段时间（至少有几十年了），但它直到1990年代才开始在实际应用中蓬勃发展。'
- en: '*Deep learning* is one of many forms of machine learning. In deep learning,
    models consist of many steps of representation transformation, applied one after
    another (hence the adjective “deep”). These operations are structured into modules
    called *layers*. Deep-learning models are typically stacks of many layers or,
    more generally, graphs of many layers. These layers are parameterized by *weights*,
    numeric values that help transform a layer’s input into its output and are updated
    during the training process. The “knowledge” learned by a model during training
    is embodied in its weights. The training process is primarily about finding a
    good set of values for these weights.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*深度学习* 是机器学习的众多形式之一。在深度学习中，模型由许多步骤的表示转换组成，依次应用（因此有形容词“深度”）。这些操作被结构化为称为 *层*
    的模块。深度学习模型通常是许多层的堆栈或更一般地说是许多层的图。这些层由 *权重* 参数化，数字值有助于将层的输入转换为其输出，并在训练过程中更新。模型在训练过程中学到的“知识”体现在其权重中。训练过程主要是为这些权重找到一个良好的值。'
- en: Even though deep learning is just one among many approaches to machine learning,
    it has proven to be a breakout success compared to other approaches. Let’s quickly
    review the reasons behind deep learning’s success.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习只是机器学习的众多方法之一，但与其他方法相比，它已被证明是一个突破性的成功。让我们快速回顾一下深度学习成功背后的原因。
- en: 13.1.2\. What makes deep learning stand out among the subfields of machine learning
  id: totrans-16
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 13.1.2\. 使深度学习在机器学习子领域中脱颖而出的原因
- en: In the span of only a few years, deep learning has achieved tremendous breakthroughs
    in multiple tasks that have been historically thought of as extremely difficult
    for computers, especially in the area of machine perception—namely, extracting
    useful information from images, audio, video, and similar modalities of perceptual
    data with a sufficiently high accuracy. Given sufficient training data (in particular,
    *labeled* training data), it is now possible to extract from perceptual data almost
    anything that humans can extract, sometimes with an accuracy that exceeds that
    of humans. Hence, it is sometimes said that deep learning has largely “solved
    perception,” although this is true only for a fairly narrow definition of perception
    (see [section 13.2.5](#ch13lev2sec10) for the limitations of deep learning).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅在几年的时间里，**深度学习**在多个历史上被认为对计算机极其困难的任务上取得了巨大突破，特别是在机器感知领域——即从图像、音频、视频和类似感知数据中提取有用信息的能力，具有足够高的准确性。如果有足够的训练数据（特别是*标记*的训练数据），现在可以从感知数据中提取几乎任何人类可以提取的东西，有时甚至准确度超过人类。因此，有时说深度学习在很大程度上“解决了感知”问题，尽管这只适用于对感知的一个相当狭义的定义（参见
    [第 13.2.5 节](#ch13lev2sec10) 以了解深度学习的局限性）。
- en: 'Due to its unprecedented technical success, deep learning has single-handedly
    brought about the third and by far the largest-so-far *AI summer*, also referred
    to as the *deep-learning revolution*, which is a period of intense interest, investment,
    and hype in the field of AI. Whether this period will end in the near future,
    and what happens to it afterward, are topics of speculation and debate. But one
    thing is certain: in stark contrast with previous AI summers, deep learning has
    provided enormous value to a large number of technology companies, enabling human-level
    image classification, object detection, speech recognition, smart assistants,
    natural language processing, machine translation, recommendation systems, self-driving
    cars, and more. The hype may recede (rightfully), but the sustained technological
    impact and economic value of deep learning will remain. In that sense, deep learning
    could be analogous to the internet: it may be overly hyped for a few years, causing
    unreasonable expectations and overinvestment, but in the long term, it will remain
    a major revolution that will impact many areas of technology and transform our
    lives.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其空前的技术成功，**深度学习**独自引发了第三次，迄今为止最大的 *AI 夏季*，也被称为 *深度学习革命*，这是人工智能领域的一个充满兴趣、投资和炒作的时期。这一时期是否会在不久的将来结束，以及之后会发生什么，是人们猜测和讨论的话题。但有一点是确定的：与以往的
    AI 夏季形成鲜明对比，深度学习为许多技术公司提供了巨大价值，实现了人类级别的图像分类、目标检测、语音识别、智能助手、自然语言处理、机器翻译、推荐系统、自动驾驶汽车等。炒作可能会减退（理所当然），但深度学习的持续技术影响和经济价值将会保持。从这个意义上说，深度学习可能类似于互联网：可能在几年内被过度炒作，导致不合理的期望和过度投资，但从长远来看，它将成为一个影响技术许多领域并改变我们生活的重大革命。
- en: We are particularly optimistic about deep learning because even if we were to
    make no further academic progress in it in the next decade, putting the existing
    deep-learning techniques to every applicable practical problem would still be
    a game changer for many industries (online advertisement, finance, industrial
    automation, and assistive technologies for people with disabilities, just to list
    a few). Deep learning is nothing short of a revolution, and progress is currently
    happening at an incredibly fast pace due to an exponential investment in resources
    and headcount. From where we stand, the future looks bright, although short-term
    expectations may be somewhat overly optimistic; deploying deep learning to the
    full extent of its potential will take well over a decade.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对深度学习特别乐观，因为即使在未来十年内我们在其中不再取得进一步的学术进展，将现有的深度学习技术应用于每一个适用的实际问题仍将改变许多行业的游戏规则（在线广告、金融、工业自动化和残疾人辅助技术，只是列举了一部分）。深度学习无疑是一场革命，目前的进展速度之快令人难以置信，这要归功于资源和人员的指数级投资。从我们的角度来看，未来看起来很光明，尽管短期内的预期可能有些过于乐观；充分发挥深度学习的潜力将需要超过十年的时间。
- en: 13.1.3\. How to think about deep learning at a high level
  id: totrans-20
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 13.1.3\. 怎样从高层次上思考深度学习
- en: One of the most surprising aspects of deep learning is how simple it is, given
    how well it works and how more complicated machine-learning techniques that came
    before it didn’t work quite as well. Ten years ago, nobody expected that we could
    achieve such amazing results on machine-perception problems by using only parametric
    models trained with gradient descent. Now it turns out that all you need is sufficiently
    large parametric models trained with gradient descent and sufficiently many labeled
    examples. As Richard Feynman once said about his understanding of the universe,
    “It’s not complicated, it’s just a lot of it.”^([[1](#ch13fn1)])
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习最令人惊讶的一个方面是它的简单性，考虑到它的工作效果以及之前的更复杂的机器学习技术的效果并不如人意。十年前，没有人预料到我们能够仅通过使用梯度下降训练的参数模型在机器感知问题上取得如此惊人的结果。现在事实证明，你只需要足够大的通过梯度下降训练的参数模型以及足够多的带标签示例。正如理查德·费曼曾经关于他对宇宙的理解所说：“这并不复杂，只是有很多。”^([[1](#ch13fn1)])
- en: ¹
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Richard Feynman, interview, “The World from Another Point of View,” Yorkshire
    Television, 1972.
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 理查德·费曼，采访，“另一种视角下的世界”，约克郡电视台，1972年。
- en: In deep learning, everything is represented as a series of numbers—in other
    words, a *vector*. A vector can be viewed as a *point* in a *geometric space*.
    Model inputs (tabular data, images, text, and so on) are first vectorized, or
    turned into a set of points in the input vector space. In a similar way, the targets
    (labels) are also vectorized and turned into their corresponding set of points
    in the target vector space. Then, each layer in a deep neural network performs
    one simple geometric transformation on the data that goes through it. Together,
    the chain of layers in the neural network forms a complex geometric transformation,
    made of a series of simple geometric transformations. This complex transformation
    attempts to map the points in the input vector space to those in the target vector
    space. This transformation is parameterized by the weights of the layers, which
    are iteratively updated based on how good the transformation currently is. A key
    characteristic of this geometric transformation is that it is *differentiable:*
    this is what makes gradient descent possible.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，一切都被表示为一系列数字——换句话说，是一个*向量*。一个向量可以被看作是*几何空间*中的一个*点*。模型输入（表格数据、图像、文本等）首先被向量化，或者转换为输入向量空间中的一组点。同样地，目标（标签）也被向量化，并转换为其相应的目标向量空间中的一组点。然后，深度神经网络中的每一层对通过它的数据执行一个简单的几何变换。在一起，神经网络中的层链形成了一个由一系列简单的几何变换组成的复杂几何变换。这个复杂的变换试图将输入向量空间中的点映射到目标向量空间中的点。这个变换由层的权重参数化，这些权重根据当前变换的好坏进行迭代更新。这个几何变换的一个关键特征是它是*可微的*：这就是梯度下降变得可能的原因。
- en: 13.1.4\. Key enabling technologies of deep learning
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 13.1.4\. 深度学习的关键技术
- en: 'The deep-learning revolution that’s currently unfolding didn’t start overnight.
    Instead, like any other revolution, it’s the product of an accumulation of several
    enabling factors—slowly at first, and then suddenly accelerating once critical
    mass is reached. In the case of deep learning, we can point out the following
    key factors:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 正在进行的深度学习革命并非一夜之间开始。相反，和其他任何革命一样，它是一系列因素的积累——一开始缓慢，然后一旦到达关键点就突然加速。就深度学习而言，我们可以指出以下关键因素：
- en: Incremental algorithmic innovations, first spread over two decades^([[2](#ch13fn2)])
    and then accelerating as more research effort was poured into deep learning after
    2012.^([[3](#ch13fn3)])
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 渐进式的算法创新，首先涉及两个十年，然后随着深度学习在 2012 年之后投入更多研究力量而加速发展。
- en: ²
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ²
- en: ''
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: Starting with the invention of backpropagation by Rumelhart, Hinton, and Williams,
    convolutional layers by LeCun and Bengio, and recurrent networks by Graves and
    Schmidthuber.
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 起始于 Rumelhart、Hinton 和 Williams 的反向传播算法，LeCun 和 Bengio 的卷积层，以及 Graves 和 Schmidthuber
    的循环网络。
- en: ³
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³
- en: ''
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: For example, improved weight initialization methods, new activation functions,
    dropout, batch normalization, residual connections.
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 例如，改进的权重初始化方法，新的激活函数，dropout，批量归一化，残差连接。
- en: The availability of large amounts of labeled data, spanning many data modalities,
    including perceptual (images, audio, and video), numeric, and text, which enables
    large models to be trained on sufficient amounts of data. This is a byproduct
    of the rise of the consumer internet, spurred by the popularization of mobile
    devices, as well as Moore’s law applied to storage media.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大量的标记数据可用，覆盖许多数据模式，包括知觉（图像、音频和视频）、数字和文本，这使大型模型可以在足够数量的数据上进行训练。这是消费互联网兴起的副产品，由流行的移动设备推动，以及存储介质中的摩尔定律。
- en: The availability of fast, highly parallelized computation hardware at a low
    cost, especially the GPUs produced by NVIDIA—first gaming GPUs repurposed for
    parallel computing and then chips designed ground-up for deep learning.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速、高度并行化的计算硬件以低成本提供，尤其是 NVIDIA 生产的 GPU——首先是为并行计算重新用途的游戏 GPU，然后是从头设计用于深度学习的芯片。
- en: 'A complex stack of open source software that makes this computational power
    available to many human developers and learners, while hiding the enormous amount
    of complexity underneath: the CUDA language, the web browser’s WebGL shader languages,
    and frameworks such as TensorFlow.js, TensorFlow, and Keras, which perform automatic
    differentiation and provide easy-to-use, high-level building blocks such as layers,
    loss functions, and optimizers. Deep learning is changing from the exclusive domain
    of specialists (researchers, graduate students in AI, and engineers with an academic
    background) into a tool for every programmer. TensorFlow.js is an exemplary framework
    in this front. It brings two rich and vibrant ecosystems together: the cross-platform
    ecosystem of JavaScript and the fast-advancing deep-learning ecosystem.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复杂的开源软件堆栈使许多人类开发者和学习者可以使用这种计算能力，同时隐藏了庞大的复杂性：CUDA 语言、WebGL 着色器语言以及框架，如 TensorFlow.js、TensorFlow
    和 Keras，其执行自动差分并提供易于使用的高级搭建块，如层、损失函数和优化器。深度学习正在从专家领域（研究人员、AI 研究生和具有学术背景的工程师）转变为每位程序员的工具。TensorFlow.js
    在这方面是一个典范性的框架。它将两个丰富活跃的生态系统结合在一起：JavaScript 跨平台生态系统和快速发展的深度学习生态系统。
- en: A manifestation of the wide and deep impact of the deep-learning revolution
    is its fusion with technological stacks different from the one in which it originated
    (the C++ and Python ecosystem and the numeric computation field). Its cross-pollination
    with the JavaScript ecosystem, the main theme of the book, is a prime example
    of that. In the next section, we will review the key reasons why bringing deep
    learning to the world of JavaScript unlocks exciting new opportunities and possibilities.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习革命的广泛和深远影响之一是它与其他不同于它的技术栈（如 C++ 和 Python 生态系统和数字计算领域）的融合。它与 JavaScript 生态系统的跨界融合是其中的一个典型例子。在接下来的部分，我们将回顾为什么将深度学习引入
    JavaScript 世界将开启令人兴奋的新机遇和可能性的关键原因。
- en: 13.1.5\. Applications and opportunities unlocked by deep learning in JavaScript
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 13.1.5 用 JavaScript 实现深度学习的应用与机遇
- en: The main purpose of training a deep-learning model is to make it available for
    users to use. For many types of input modalities, such as images from the webcam,
    sounds from the microphone, and text and gesture input by the user, the data is
    generated and directly available on the client. JavaScript is perhaps the most
    mature and ubiquitous language and ecosystem for client-side programming. The
    same code written in JavaScript can be deployed as web pages and UIs on a wide
    range of devices and platforms. The web browser’s WebGL API enables cross-platform
    parallel computation on a variety of GPUs, which is leveraged by TensorFlow.js.
    These factors make JavaScript an attractive option for the deployment of deep-learning
    models. TensorFlow.js offers a converter tool that allows you to convert models
    trained with popular Python frameworks such as TensorFlow and Keras into a web-friendly
    format and deploy them into web pages for inference and transfer learning.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 训练深度学习模型的主要目的是使其可供用户使用。对于许多类型的输入模态，例如来自网络摄像头的图像、来自麦克风的声音以及用户输入的文本和手势输入，数据是由客户端生成并直接可用的。JavaScript
    或许是最成熟和普及的客户端编程语言和生态系统。用 JavaScript 编写的相同代码可以部署为网页和 UI，在各种设备和平台上运行。Web 浏览器的 WebGL
    API 允许在各种 GPU 上进行跨平台并行计算，TensorFlow.js 利用了这一点。这些因素使 JavaScript 成为部署深度学习模型的一种吸引人的选择。TensorFlow.js
    提供了一个转换工具，允许您将使用流行的 Python 框架（如 TensorFlow 和 Keras）训练的模型转换为适合 Web 的格式，并将其部署到网页上进行推理和迁移学习。
- en: 'Apart from the ease of deployment, there are also a number of additional advantages
    to serving and fine-tuning deep-learning models using JavaScript:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 除了部署的便利性之外，使用 JavaScript 提供服务和微调深度学习模型还有许多其他优势：
- en: Compared to server-side inference, client-side inference foregoes the latency
    of two-way data transfer, benefiting availability and leading to a smoother user
    experience.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与服务器端推理相比，客户端推理放弃了双向数据传输的延迟，有利于可用性，并带来更流畅的用户体验。
- en: By performing computation at the edge using on-device GPU acceleration, client-side
    deep learning removes the need to manage server-side GPU resources, significantly
    reducing the complexity and maintenance costs of your technology stack.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用设备 GPU 加速在边缘执行计算，客户端深度学习消除了管理服务器端 GPU 资源的需要，显着降低了技术堆栈的复杂性和维护成本。
- en: By virtue of keeping the data and inference results on the client, the user’s
    data privacy is protected. This is important for domains such as healthcare and
    fashion.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过保持数据和推理结果在客户端，用户的数据隐私得到了保护。这对于医疗保健和时尚等领域至关重要。
- en: The visual and interactive nature of the browser and other JavaScript-based
    UI environments provides unique opportunities for visualization, aided understanding,
    and teaching of neural networks.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 浏览器和其他基于 JavaScript 的 UI 环境的视觉和交互性为神经网络的可视化、辅助理解和教学提供了独特的机会。
- en: TensorFlow.js supports not only inference but also training. This opens the
    door to client-side transfer learning and fine-tuning, which leads to better personalization
    of machine-learning models.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow.js 不仅支持推理，还支持训练。这为客户端迁移学习和微调打开了大门，从而实现了机器学习模型的更好个性化。
- en: In the web browser, JavaScript provides a platform-independent API for access
    to on-device sensors, such as webcams and microphones, which accelerates the development
    of cross-platform applications that use inputs from these sensors.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Web 浏览器中，JavaScript 提供了一个独立于平台的 API，用于访问设备传感器，如网络摄像头和麦克风，这加速了使用这些传感器输入的跨平台应用程序的开发。
- en: In addition to its client-side eminence, JavaScript extends its prowess to the
    server side. In particular, Node.js is a highly popular framework for server-side
    applications in JavaScript. Using the Node.js version of TensorFlow.js (tfjs-node),
    you can train and serve deep-learning models outside the web browser and hence
    without resource constraints. This taps into the vast ecosystem of Node.js and
    simplifies the tech stack for members of the community. All of this can be achieved
    by using essentially the same TensorFlow.js code that you write for the client
    side, which brings you closer to the vision of “write once, run everywhere,” as
    has been demonstrated by several examples throughout the book.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 除了其客户端的卓越性能，JavaScript 还将其能力扩展到服务器端。特别是，Node.js 是 JavaScript 中高度流行的用于服务器端应用的框架。使用
    Node.js 版本的 TensorFlow.js（tfjs-node），您可以在网页浏览器之外的环境中训练和提供深度学习模型，因此不受资源限制。这利用了
    Node.js 的庞大生态系统，并简化了社区成员的技术堆栈。所有这些都可以通过使用与您为客户端编写的基本相同的 TensorFlow.js 代码来实现，这使您更接近“一次编写，到处运行”的愿景，正如本书中的几个示例所示。
- en: 13.2\. Quick overview of the deep-learning workflow and algorithms in TensorFlow.js
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2\. TensorFlow.js 中深度学习工作流程和算法的快速概述
- en: With the historical overview out of the way, let’s now visit the technical aspects
    of TensorFlow.js. In this section, we will review the general workflow you should
    follow when approaching a machine-learning problem and highlight some of the most
    important considerations and common pitfalls. We will then go over the various
    neural network building blocks (layers) that we’ve covered in the book. In addition,
    we’ll survey the pretrained models in the TensorFlow.js ecosystem, which you can
    use to accelerate your development cycle. To wrap up this section, we will present
    the range of machine-learning problems you can potentially address by using these
    building blocks, stimulating you to imagine how deep neural networks written in
    TensorFlow.js can assist you in addressing your own machine-learning problems.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 通过历史概述，让我们现在来看看 TensorFlow.js 的技术方面。在本节中，我们将回顾当你面对一个机器学习问题时应该遵循的一般工作流程，并强调一些最重要的考虑因素和常见陷阱。然后，我们将过一下我们在书中涵盖的各种神经网络构建块（层）。此外，我们将调查
    TensorFlow.js 生态系统中的预训练模型，这些模型可以加速您的开发周期。为了结束本节，我们将介绍您可以通过使用这些构建块解决的一系列机器学习问题，激励您想象使用
    TensorFlow.js 编写的深度神经网络如何帮助您解决自己的机器学习问题。
- en: 13.2.1\. The universal workflow of supervised deep learning
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 13.2.1\. 监督深度学习的通用工作流程
- en: 'Deep learning is a powerful tool. But perhaps somewhat surprisingly, the most
    difficult and time-consuming part of the machine-learning workflow is often everything
    that comes before designing and training such models (and for models deployed
    to production, what comes after it, too). These difficult steps include understanding
    the problem domain well enough to be able to determine what sort of data is needed,
    what sort of predictions can be made with reasonable accuracy and generalization
    power, how the machine-learning model fits into the overall solution that addresses
    a practical problem, and how to measure the degree to which the model succeeds
    at doing its job. Although these are prerequisites for any successful application
    of machine learning, they aren’t something that a software library such as TensorFlow.js
    can automate for you. As a reminder, what follows is a quick summary of the typical
    supervised-learning workflow:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是一个强大的工具。但或许有些令人惊讶的是，机器学习工作流程中最困难和耗时的部分通常是设计和训练这些模型之前的一切（以及对于部署到生产环境的模型来说，还有之后的一切）。这些困难的步骤包括充分了解问题域，以便能够确定需要什么样的数据，以及可以以合理的准确性和泛化能力进行什么样的预测，机器学习模型如何适用于解决实际问题的整体解决方案中，以及如何度量模型在完成其工作时成功的程度。尽管这些是任何成功应用机器学习的先决条件，但它们不是像
    TensorFlow.js 这样的软件库可以自动化的内容。作为提醒，以下是典型监督学习工作流程的快速摘要：
- en: '*Determine if machine learning is the right approach*. First, consider if machine
    learning is the right approach to your problem, and proceed with the following
    steps only if the answer is yes. In some cases, a non-machine-learning approach
    will work equally well or perhaps even better, at a lower cost.'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*确定机器学习是否是正确的方法*。首先，考虑一下机器学习是否是解决您的问题的正确方法，只有在答案是肯定的情况下才继续以下步骤。在某些情况下，非机器学习方法同样有效，甚至可能更好，成本更低。'
- en: '*Define the machine-learning problem*. Determine what sort of data is available
    and what you are trying to predict using the data.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义机器学习问题。确定可用的数据类型以及您试图使用数据预测的内容。
- en: '*Check if your data is sufficient*. Determine if the amount of data you already
    have is sufficient for model training. You may need to collect more data or hire
    people to manually label an unlabeled dataset.'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查您的数据是否足够。确定您已经拥有的数据量是否足以进行模型训练。您可能需要收集更多数据或雇用人员手动标记一个未标记的数据集。
- en: '*Identify a way to reliably measure the success of a trained model on your
    goal*. For simple tasks, this may be just prediction accuracy, but in many cases,
    it will require more sophisticated, domain-specific metrics.'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定一种可靠地衡量训练模型成功的方法。对于简单任务，这可能只是预测准确率，但在许多情况下，它将需要更复杂、特定于领域的度量标准。
- en: '*Prepare the evaluation process*. Design the validation process that you’ll
    use to evaluate your models. In particular, you should split your data into three
    homogeneous yet nonoverlapping sets: a training set, a validation set, and a test
    set. The validation- and test-set labels ought not to leak into the training data.
    For instance, with temporal prediction, the validation and test data should come
    from time intervals after the training data. Your data-preprocessing code should
    be covered by tests to guard against bugs ([section 12.1](kindle_split_025.html#ch12lev1sec1)).'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备评估过程。设计用于评估模型的验证过程。特别是，您应该将数据分成三个同质但不重叠的集合：一个训练集、一个验证集和一个测试集。验证集和测试集的标签不应泄漏到训练数据中。例如，对于时间预测，验证和测试数据应来自训练数据之后的时间间隔。您的数据预处理代码应该经过测试，以防止错误（[第12.1节](kindle_split_025.html#ch12lev1sec1)）。
- en: '*Vectorize the data*. Turn your data into tensors, or *n*-dimensional arrays—the
    lingua franca of machine-learning models in frameworks such as TensorFlow.js and
    TensorFlow. You often need to preprocess the tensorized data in order to make
    it more amenable to your models (for example, through normalization).'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据向量化。将您的数据转换为张量，或者*n*维数组——TensorFlow.js和TensorFlow等框架中机器学习模型的通用语言。通常需要对张量化的数据进行预处理，以使其更适合您的模型（例如，通过归一化）。
- en: '*Beat the commonsense baseline*. Develop a model that beats a non-machine-learning
    baseline (such as predicting the population average for a regression problem or
    predicting the last datapoint in a time-series prediction problem), thereby demonstrating
    that machine learning can truly add value to your solution. This may not always
    be the case (see step 1).'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 超越常识基线。开发一个能够超越非机器学习基线的模型（例如，在回归问题中预测人口平均值或在时间序列预测问题中预测最后一个数据点），从而证明机器学习确实可以为您的解决方案增加价值。这并不总是可能的（见步骤1）。
- en: '*Develop a model with sufficient capacity*. Gradually refine your model architecture
    by tuning hyperparameters and adding regularization. Make changes based on the
    prediction accuracy on the validation set only, not the training set or the test
    set. Remember that you should get your model to overfit the problem (achieve a
    better prediction accuracy on the training set than on the validation set), thus
    identifying a model capacity that’s greater than what you need. Only then should
    you begin to use regularization and other approaches to reduce overfitting.'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开发具有足够容量的模型。通过调整超参数和添加正则化逐渐完善模型架构。仅基于验证集的预测准确率进行更改，而不是基于训练集或测试集。请记住，您应该使您的模型过度拟合问题（在训练集上达到比验证集更好的预测准确率），从而确定一个比您需要的容量更大的模型容量。只有在那之后，您才应该开始使用正则化和其他方法来减少过拟合。
- en: '*Tune hyperparameters*. Beware of validation-set overfitting when tuning hyperparameters.
    Because hyperparameters are determined based on the performance on the validation
    set, their values will be overspecialized for the validation set and therefore
    may not generalize well to other data. It is the purpose of the test set to obtain
    an unbiased estimate of the model’s accuracy after hyperparameter tuning. So,
    you shouldn’t use the test set when tuning the hyperparameters.'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整超参数。在调整超参数时要注意验证集的过拟合。因为超参数是基于验证集上的性能确定的，所以它们的值会过度专门化于验证集，因此可能不会很好地推广到其他数据。测试集的目的是在调整超参数后获得模型准确性的无偏估计。因此，在调整超参数时不应使用测试集。
- en: '*Validate and evaluate the trained model*. As we discussed in [section 12.1](kindle_split_025.html#ch12lev1sec1),
    test your model with an up-to-date evaluation dataset, and decide if the prediction
    accuracy meets a predetermined criterion for serving actual users. In addition,
    perform a deeper analysis of the model’s quality on different slices (subsets)
    of the data, aiming at detecting any unfair behaviors (such as vastly different
    accuracies on different slices of the data) or unwanted biases.^([[4](#ch13fn4)])
    Proceed to the final step only if the model passes these evaluation criteria.'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*验证和评估训练好的模型*。正如我们在[第12.1节](kindle_split_025.html#ch12lev1sec1)中讨论的，用最新的评估数据集测试您的模型，并决定预测精度是否达到为实际用户服务的预定标准。此外，对模型在不同数据切片（子集）上的质量进行更深入的分析，旨在检测任何不公平行为（例如在不同数据切片上的精度差异）或不希望的偏差。^([[4](#ch13fn4)])
    只有当模型通过这些评估标准时，才进行最终步骤。'
- en: ⁴
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁴
- en: ''
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: Fairness in machine learning is a nascent field of study; see the following
    link for more discussion [http://mng.bz/eD4Q](http://mng.bz/eD4Q).
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 机器学习中的公平性是一个新兴的研究领域；更多讨论请参见以下链接[http://mng.bz/eD4Q](http://mng.bz/eD4Q)。
- en: '*Optimize and deploy the model*. Perform model optimization in order to shrink
    its size and boost its inference speed. Then deploy the model into the serving
    environment, such as a web page, a mobile app, or an HTTP service endpoint ([section
    12.3](kindle_split_025.html#ch12lev1sec3)).'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*优化和部署模型*。对模型进行优化，以缩小其尺寸并提高其推理速度。然后将模型部署到服务环境中，如网页、移动应用程序或 HTTP 服务端点（[第12.3节](kindle_split_025.html#ch12lev1sec3)）。'
- en: This recipe is for supervised learning, which is encountered in many practical
    problems. Other types of machine-learning workflows covered in this book include
    (supervised) transfer learning, RL (reinforcement learning), and generative deep
    learning. Supervised transfer learning ([chapter 5](kindle_split_016.html#ch05))
    shares the same workflow as nontransfer supervised learning, with the slight difference
    that the model design and training steps build on a pretrained model and may require
    a smaller amount of training data than training a model from scratch. Generative
    deep learning has a different type of goal from supervised learning—that is, to
    create fake examples that look as real as possible. In practice, there are techniques
    that turn the training of generative models into supervised learning, as we saw
    in the VAE and GAN examples in [chapter 9](kindle_split_021.html#ch09). RL, on
    the other hand, involves a fundamentally different problem formulation and consequently
    a dramatically different workflow—one in which the primary players are the environment,
    the agent, the actions, the reward structure, and the algorithm or model types
    employed to solve the problem. [Chapter 11](kindle_split_023.html#ch11) provided
    a quick overview of the basic concepts and algorithms in RL.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这个教程是关于监督学习的，它在许多实际问题中都会遇到。本书涵盖的其他类型的机器学习工作流包括（监督）迁移学习、RL（强化学习）和生成式深度学习。监督迁移学习（[第5章](kindle_split_016.html#ch05)）与非迁移监督学习的工作流相同，唯一的区别是模型设计和训练步骤是基于预训练模型构建的，并且可能需要比从头开始训练模型更少的训练数据。生成式深度学习的目标与监督学习有所不同——即尽可能创建看起来像真实的假例。实际上，有一些技术将生成模型的训练转化为监督学习，就像我们在[第9章](kindle_split_021.html#ch09)中看到的
    VAE 和 GAN 示例一样。另一方面，RL 包含一个根本不同的问题形式化，并因此具有截然不同的工作流程——其中主要参与者是环境、代理、行动、奖励结构以及用于解决问题的算法或模型类型。[第11章](kindle_split_023.html#ch11)提供了
    RL 中基本概念和算法的快速概述。
- en: '13.2.2\. Reviewing model and layer types in TensorFlow.js: A quick reference'
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 13.2.2\. 回顾 TensorFlow.js 中的模型和层类型：快速参考
- en: 'All the numerous neural networks covered in this book can be divided into three
    families: densely connected networks (also referred to as MLPs, or multilayer
    perceptrons), convnets (convolutional networks), and recurrent networks. These
    are the three basic families of networks that every deep-learning practitioner
    should be familiar with. Each type of network is suitable for a specific type
    of input: a network architecture (MLP, convolutional, or recurrent) encodes assumptions
    about the structure of the input data—a hypothesis space within which the search
    for a good model via backpropagation and hyperparameter tuning occurs. Whether
    a given architecture will work for a given problem depends entirely on how well
    the structure in the data matches the assumption of the network architecture.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 本书涵盖的所有众多神经网络可以分为三类系列：密集连接网络（也称为 MLPs 或多层感知器）、卷积网络和循环网络。这三种基本系列的网络是每个深度学习实践者都应该熟悉的。每种类型的网络适用于特定类型的输入：网络架构（MLP、卷积或循环）对输入数据的结构进行假设—通过反向传播和超参数调整来搜索好模型的假设空间。给定问题是否适用于给定架构完全取决于数据中的结构与网络架构的假设匹配得有多好。
- en: 'These different network types can easily be combined in a LEGO-like fashion
    to form more complex and multimodal networks. In a way, deep-learning layers are
    LEGO bricks for differentiable information processing. We provide a quick overview
    of the mapping between the modality of input data and the appropriate network
    architecture:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这些不同类型的网络可以像积木一样轻松地组合成更复杂和多模态的网络。从某种意义上说，深度学习层是可微分信息处理的积木。我们快速概述输入数据的模态与适当网络架构之间的映射：
- en: Vector data (without temporal or serial order)—MLPs (dense `layers)`
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量数据（没有时间序列或串行顺序）—MLPs（密集 `layers`）
- en: Image data (black-and-white, grayscale, or color)—2D convnets
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像数据（黑白、灰度或彩色）—2D 卷积网络
- en: Audio data as spectrograms—2D convnets or RNNs
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 音频数据作为频谱图—2D 卷积网络或 RNNs
- en: Text data—1D convnets or RNNs
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本数据—1D 卷积网络或 RNNs
- en: Time-series data—1D convnets or RNNs
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间序列数据—1D 卷积网络或 RNNs
- en: Volumetric data (such as 3D medical images)—3D convnets
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 体积数据（例如 3D 医学图像）—3D 卷积网络
- en: Video data (sequence of images)—either 3D convnets (if you need to capture motion
    effects) or a combination of a frame-by-frame 2D convnet for feature extraction
    followed by either an RNN or a 1D convnet to process the resulting feature sequence
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视频数据（图像序列）—要么是 3D 卷积网络（如果你需要捕捉运动效果），要么是一个逐帧 2D 卷积网络用于特征提取，随后是一个 RNN 或 1D 卷积网络来处理生成的特征序列的组合
- en: Now let’s dive a little deeper into each of the three major architecture families,
    the tasks they are good at, and how to use them through TensorFlow.js.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们深入了解每个主要架构系列，它们擅长的任务以及如何通过 TensorFlow.js 使用它们。
- en: Densely connected networks and multilayer perceptrons
  id: totrans-79
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 密集连接网络和多层感知器
- en: The terms *densely connected networks* and *multilayer perceptron* are largely
    exchangeable, with the caveat that a densely connected network can contain as
    little as one layer, while an MLP must consist of at least a hidden layer and
    an output layer. We will use the term *MLP* to refer to all models built primarily
    with dense layers for the sake of succinctness. Such networks are specialized
    for unordered vector data (for example, the numeric features in the phishing-website-detection
    problem and the housing-price-prediction problem). Each dense layer attempts to
    model the relation between all possible pairs of input features and the layer’s
    output activations. This is achieved through a matrix multiplication between the
    dense layer’s kernel and the input vector (followed by addition with a bias vector
    and an activation function). The fact that every output activation is affected
    by every input feature is the reason such layers and the networks built on them
    are referred to as *densely connected* (or referred to as *fully connected* by
    some authors). This is in contrast to other types of architecture (convnets and
    RNNs) in which an output element depends only on a subset of the elements in the
    input data.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*密集连接网络*和*多层感知机*这两个术语在很大程度上可以互换使用，但要注意密集连接网络可以只包含一个层，而多层感知机必须至少包含一个隐藏层和一个输出层。为了简洁起见，我们将使用*MLP*这个术语来指代主要由密集层构建的所有模型。此类网络专门用于无序向量数据（例如，在钓鱼网站检测问题和房价预测问题中的数值特征）。每个密集层试图对输入特征的所有可能的对和该层的输出激活之间的关系进行建模。这通过密集层的核与输入向量之间的矩阵乘法（然后加上偏差向量和激活函数）来实现。每个输出激活受到每个输入特征的影响是这些层和建立在它们上面的网络被称为*密集连接*（或被一些作者称为*完全连接*）的原因。这与其他类型的架构（卷积网络和循环神经网络）形成对比，在这些架构中，一个输出元素仅依赖于输入数据中的一部分元素。'
- en: MLPs are most commonly used for categorical data (for example, where the input
    features are a list of attributes, such as in the phishing-website-detection problem).
    They are also often used as the final output stages of most neural networks for
    classification and regression, which may contain convolutional or recurrent layers
    as feature extractors that feed feature inputs to such MLPs. For instance, the
    2D convnets we covered in [chapters 4](kindle_split_015.html#ch04) and [5](kindle_split_016.html#ch05)
    all end with one or two dense layers, and so do the recurrent networks we visited
    in [chapter 9](kindle_split_021.html#ch09).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: MLP最常用于分类数据（例如，输入特征是属性列表，比如在钓鱼网站检测问题中）。它们也经常被用作大多数分类和回归神经网络的最终输出阶段，这些网络可以包含卷积或循环层作为特征提取器，将特征输入馈送到这样的MLP中。例如，我们在[第4章](kindle_split_015.html#ch04)和[第5章](kindle_split_016.html#ch05)中介绍的二维卷积网络都以一个或两个密集层结尾，我们在[第9章](kindle_split_021.html#ch09)中介绍的循环网络也是如此。
- en: Let’s briefly review how to select the activation of the output layer of an
    MLP for different types of tasks in supervised learning. To perform binary classification,
    the final dense layer of your MLP should have exactly one unit and use the sigmoid
    activation. The `binaryCrossentropy` loss function should be used as the loss
    function during the training of such a binary-classifier MLP. The examples in
    your training data should have binary labels (labels of 0 or 1). Specifically,
    the TensorFlow.js code looks like
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要回顾一下在监督学习中如何选择MLP的输出层激活函数以适应不同类型的任务。要进行二元分类，你的MLP的最后一个密集层应该正好有一个单元，并使用sigmoid激活函数。在训练这样一个二元分类器MLP时，应该使用`binaryCrossentropy`作为损失函数。你的训练数据中的例子应该有二元标签（0或1的标签）。具体来说，TensorFlow.js的代码如下：
- en: '[PRE0]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: To perform single-label multiclass classification (where each example has exactly
    one class among multiple candidate classes), end your stack of layers with a dense
    layer that contains a softmax activation and a number of units equal to the number
    of classes. If your targets are one-hot encoded, use `categoricalCrossentropy`
    as the loss function; if they are integer indices, use `sparseCategoricalCrossentropy`.
    For instance,
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行单标签多类别分类（其中每个例子在多个候选类别中只有一个类别），请在你的层堆叠的末尾加上一个包含softmax激活函数的密集层，单位数量等于类别的数量。如果你的目标是独热编码，请使用`categoricalCrossentropy`作为损失函数；如果它们是整数索引，请使用`sparseCategoricalCrossentropy`。例如：
- en: '[PRE1]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To perform multilabel multiclass classification (where each example can have
    several correct classes), end your stack of layers with a dense layer that contains
    a sigmoid activation and a number of units equal to the number of all candidate
    classes. Use `binaryCrossentropy` for the loss function. Your targets should be
    k-hot encoded:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行多标签多类别分类（每个示例可能具有多个正确类别），将你的层堆栈结束为一个包含sigmoid激活和单位数量等于所有候选类别数量的密集层。使用`binaryCrossentropy`作为损失函数。你的目标应该是k-hot编码：
- en: '[PRE2]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To perform regression toward a vector of continuous values, end your stack
    of layers with a dense layer with the number of units equal to the number of values
    you are trying to predict (often only one number, such as the price of housing
    or a temperature value) and the default linear activation. Several loss functions
    can be suitable for regression. The most commonly used ones are `meanSquaredError`
    and `meanAbsoluteError`:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对连续值向量执行回归，将你的层堆栈结束为一个具有单位数量等于你尝试预测的值数量（通常只有一个数字，比如房价或温度值）的密集层，并使用默认的线性激活函数。多个损失函数可用于回归。最常用的是`meanSquaredError`和`meanAbsoluteError`：
- en: '[PRE3]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Convolutional networks
  id: totrans-90
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 卷积网络
- en: 'Convolutional layers look at local spatial patterns by applying the same geometric
    transformation to different spatial locations (patches) in an input tensor. This
    results in representations that are translation-invariant, making convolutional
    layers highly data efficient and modular. This idea is applicable to spaces of
    any dimensionality: 1D (sequences), 2D (images or similar representation of nonimage
    quantities, such as sound spectrograms), 3D (volumes), and so forth. You can use
    the `tf.layers.conv1d` layer to process sequences, the conv2d layer to process
    images, and the conv3d layer to process volumes.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层通过将相同的几何变换应用于输入张量中的不同空间位置（补丁）来查看局部空间模式。这导致了是平移不变的表示，使卷积层高度数据有效和模块化。这个想法适用于任何维度的空间：1D（序列），2D（图像或类似于非图像数量的表示，如声音频谱图），3D（体积）等等。你可以使用`tf.layers.conv1d`层来处理序列，使用conv2d层来处理图像，使用conv3d层来处理体积。
- en: Convnets consist of stacks of convolutional and pooling layers. The pooling
    layers let you spatially downsample the data, which is required to keep feature
    maps to a reasonable size as the number of features grows, and to allow subsequent
    layers to “see” a greater spatial extent of the convnet’s input images. Convnets
    are often terminated with a flatten layer or a global pooling layer, turning spatial
    feature maps into vectors, which can in turn be processed by a stack of dense
    layers (an MLP) to achieve classification or regression outputs.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积网络由一系列卷积和池化层组成。池化层允许你对数据进行空间降采样，这对于保持特征图的合理大小是必需的，因为特征数量增加，同时也允许后续层“看到”卷积网络输入图像的更大空间范围。卷积网络通常以展平层或全局池化层结束，将空间特征图转换为向量，然后可以通过一系列密集层（MLP）处理以实现分类或回归输出。
- en: 'It is highly likely that regular convolution will soon be mostly (or completely)
    replaced by an equivalent but faster and more efficient alternative: depthwise
    separable convolution (`tf.layers.separableConv2d` layers). When you are building
    a network from scratch, using depthwise separable convolution is highly recommended.
    The separableConv2d layer can be used as a drop-in replacement for `tf.layers
    .conv2d`, resulting in a smaller and faster network that performs equally well
    or better on its task. Following is a typical image-classification network (single-label
    multiclass classification, in this case). Its topology contains repeating patterns
    of convolution-pooling layer groups:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 很可能常规卷积很快就会被一个等价但更快更高效的替代品所取代（或完全取代）：深度可分离卷积（`tf.layers.separableConv2d`层）。当你从头开始构建一个网络时，强烈推荐使用深度可分离卷积。可分离卷积层可以用作`tf.layers
    .conv2d`的即插即用替代品，结果是一个更小更快的网络，在其任务上表现同样好或更好。以下是一个典型的图像分类网络（单标签多类别分类，在本例中）。其拓扑结构包含重复的卷积-池化层组的模式：
- en: '[PRE4]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Recurrent networks
  id: totrans-95
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 循环网络
- en: RNNs work by processing sequences of inputs one timestamp at a time and maintaining
    a state throughout. A state is typically a vector or a set of vectors (a point
    in a geometric space). RNNs should be used preferentially over 1D convnets in
    the case of sequences in which the patterns of interest are not temporally invariant
    (for instance, time-series data in which the recent past is more important than
    the distant past).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: RNN通过一次处理一个时间戳的输入序列并始终保持状态来工作。状态通常是一个向量或一组向量（几何空间中的一个点）。在不是时间上不变的序列（例如，时间序列数据，其中最近的过去比遥远的过去更重要）的情况下，应优先使用RNN，而不是1D卷积网络。
- en: 'Three RNN layer types are available in TensorFlow.js: simpleRNN, GRU, and LSTM.
    For most practical purposes, you should use either GRU or LSTM. LSTM is the more
    powerful of the two, but it is also computationally more expensive. You can think
    of GRU as a simpler and cheaper alternative to LSTM.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow.js提供了三种RNN层类型：simpleRNN、GRU和LSTM。对于大多数实际目的，您应该使用GRU或LSTM。LSTM是这两者中更强大的，但也更消耗计算资源。您可以将GRU视为LSTM的简化和更便宜的替代品。
- en: In order to stack multiple RNN layers on top of each other, every layer except
    the last one needs to be configured to return the full sequence of its outputs
    (each input timestep will correspond to an output timestep). If no stacking of
    RNN layers is required, usually the RNN layer needs to return only the last output,
    which in itself contains information about the entire sequence.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将多个RNN层堆叠在一起，除了最后一层之外的每一层都需要配置为返回其输出的完整序列（每个输入时间步对应一个输出时间步）。如果不需要堆叠RNN层，通常RNN层只需返回最后一个输出，其中包含有关整个序列的信息。
- en: 'The following is an example of using a single RNN layer together with a dense
    layer to perform binary classification of a vector sequence:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用单个RNN层与密集层一起执行向量序列的二进制分类的示例：
- en: '[PRE5]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Next is a model with a stack of RNN layers for single-label multiclass classification
    of a vector sequence:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是一个带有RNN层堆叠的模型，用于向量序列的单标签多类别分类：
- en: '[PRE6]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Layers and regularizers that help mitigate overfitting and improve convergence
  id: totrans-103
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 用于减轻过拟合和提高收敛性的层和正则化器
- en: 'Apart from the aforementioned mainstay layer types, some other types of layers
    are applicable across a wide range of model and problem types and assist the training
    process. Without these layers, the state-of-the-art accuracies on many machine-learning
    tasks wouldn’t be as high as they are today. For instance, the dropout and batchNormalization
    layers are often inserted in MLPs, convnets, and RNNs to help the model converge
    faster during training and to reduce overfitting. The following example shows
    a regression MLP with dropout layers included:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述主要的基本层类型之外，还有一些其他类型的层适用于广泛的模型和问题类型，并协助训练过程。没有这些层，许多机器学习任务的最新准确性不会像今天这样高。例如，dropout和batchNormalization层经常插入到MLP、卷积网络和RNN中，以帮助模型在训练过程中更快地收敛并减少过拟合。以下示例显示了包含dropout层的回归MLP：
- en: '[PRE7]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 13.2.3\. Using pretrained models from TensorFlow.js
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 13.2.3. 从TensorFlow.js使用预训练模型
- en: When the machine-learning problem you aim to solve is specific to your application
    or dataset, building and training a model from scratch is the right way to go,
    and TensorFlow.js empowers you to do that. However, in some cases, the problem
    you face is a generic one for which there exist pretrained models that either
    match your requirement exactly or can satisfy your needs with only minor tweaking.
    A collection of pretrained models is available from TensorFlow.js and third-party
    developers who build on them. Such models provide clean and easy-to-use APIs.
    They are also packaged nicely as npm packages that you can conveniently depend
    on in your JavaScript applications (including web apps and Node.js projects).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 当您要解决的机器学习问题特定于您的应用程序或数据集时，从头开始构建和训练模型是正确的方法，而TensorFlow.js使您能够做到这一点。然而，在某些情况下，您面临的问题是通用的，存在预训练模型，这些模型要么完全符合您的要求，要么只需进行轻微调整即可满足您的需求。来自TensorFlow.js和第三方开发人员的预训练模型集合。这些模型提供了干净且易于使用的API。它们还作为npm包打包得很好，您可以方便地依赖它们在您的JavaScript应用程序（包括Web应用程序和Node.js项目）中。
- en: Using such pretrained models in appropriate use cases can substantially accelerate
    your development. Since it’s impossible to list all the TensorFlow.js-based pretrained
    models out there, we will survey only the most popular ones that we are aware
    of. The packages with the name prefix @tensorflow-models/ are first-party and
    maintained by the TensorFlow.js team, while the rest are the work of third-party
    developers.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在适当的使用案例中使用这些预训练模型可以大大加快您的开发速度。由于不可能列出所有基于TensorFlow.js的预训练模型，因此我们只会调查我们所知道的最流行的那些。以@tensorflow-models/为前缀的软件包是由TensorFlow.js团队维护的第一方软件包，而其余的是第三方开发者的工作。
- en: '@tensorflow-models/mobilenet is a lightweight image-classification model. It
    outputs the probability scores for the 1,000 ImageNet classes given an input image.
    It is useful for labeling images in web pages and for detecting specific contents
    from the webcam input stream, as well as for transfer-learning tasks involving
    image inputs. While @tensorflow-models/mobilenet is concerned with generic image
    classes, there are third-party packages for more domain-specific image classification.
    For instance, nsfwjs classifies images into those that contain pornographic and
    other inappropriate content versus safe content, which is useful for parental
    control, safe browsing, and similar applications.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '@tensorflow-models/mobilenet是一个轻量级图像分类模型。它能够根据输入图像输出1,000个ImageNet类别的概率分数。它适用于在网页中为图像标记、从网络摄像头输入流中检测特定内容，以及涉及图像输入的迁移学习任务。虽然@tensorflow-models/mobilenet关注通用的图像类别，但也有第三方软件包用于更具特定领域的图像分类。例如，nsfwjs将图像分类为包含色情和其他不当内容与安全内容，这对家长监控、安全浏览等应用非常有用。'
- en: As we discussed in [chapter 5](kindle_split_016.html#ch05), object detection
    differs from image classification in that it outputs not only *what* objects an
    image contains but also *where* they are in the coordinate system of the image.
    @tensorflow-models/coco-ssd is an object-detection model capable of detecting
    90 classes of objects. For each input image, it can detect multiple target objects
    with potentially overlapping bounding boxes, if they exist ([figure 13.1](#ch13fig01),
    panel A).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在[第5章](kindle_split_016.html#ch05)中讨论的那样，目标检测与图像分类不同之处在于，它不仅输出图像中包含的物体，还输出它们在图像坐标系中的*位置*。@tensorflow-models/coco-ssd是一个能够检测90种对象的目标检测模型。对于每个输入图像，它都能够检测出可能有重叠边界框的多个目标对象（[图13.1](#ch13fig01)，A面）。
- en: 'Figure 13.1\. Screenshots from several pretrained, npm-package models built
    with TensorFlow.js. Panel A: @tensorflow-models/coco-ssd is a multitarget object
    detector. Panel B: face-api.js is for real-time face and facial-key-point detection
    (reproduced from [https://github.com/justadudewhohacks/face-api.js](https://github.com/justadudewhohacks/face-api.js)
    with permission by Vincent Mühler). Panel C: handtrack.js tracks the location
    of one or both hands in real time (reproduced from [https://github.com/victordibia/handtrack.js/](https://github.com/victordibia/handtrack.js/)
    with permission by Victor Dibia). Panel D: @tensorflow-models/posenet detects
    skeletal key points of the human body using image input in real time. Panel E:
    @tensorflow-models/toxicity detects and labels seven types of inappropriate content
    in any English text input.'
  id: totrans-111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '图13.1。几个使用TensorFlow.js构建的预训练npm软件包模型的屏幕截图。A面：@tensorflow-models/coco-ssd是一个多目标对象检测器。B面：face-api.js用于实时人脸和面部特征点检测（通过Vincent
    Mühler的许可从[https://github.com/justadudewhohacks/face-api.js](https://github.com/justadudewhohacks/face-api.js)复制）。C面：handtrack.js实时跟踪一个或两只手的位置（通过Victor
    Dibia的许可从[https://github.com/victordibia/handtrack.js/](https://github.com/victordibia/handtrack.js/)复制）。D面：@tensorflow-models/posenet使用实时图像输入检测人体的骨骼关键点。E面：@tensorflow-models/toxicity检测并标记任何英文文本输入中的七种不当内容。 '
- en: '![](13fig01_alt.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](13fig01_alt.jpg)'
- en: For web applications, certain types of objects are of especially high interest
    due to their potential for enabling novel and fun computer-human interactions.
    These include the human face, the hands, and the whole body. For each of the three,
    there are specialized third-party models based on TensorFlow.js. For the face,
    face-api.js and handsfree both support real-time face tracking and detection of
    facial landmarks (such as the eyes or mouth; [figure 13.1](#ch13fig01), panel
    B). For the hands, handtrack.js can track the location of one or both hands in
    real time ([figure 13.1](#ch13fig01), panel C). For the whole body, @tensorflow-models/posenet
    enables high-precision, real-time detection of skeletal key points (such as shoulders,
    elbows, hips, and knees; [figure 13.1](#ch13fig01), panel D).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 对于网络应用程序，特定类型的对象特别受到关注，因为它们有可能实现新颖有趣的计算机人类交互。这些包括人脸、手和整个身体。针对每一种类型，都有基于TensorFlow.js的专门的第三方模型。对于人脸，face-api.js和handsfree都支持实时人脸跟踪和检测面部特征点（如眼睛或嘴巴；[图
    13.1](#ch13fig01)，面板 B）。对于手部，handtrack.js可以实时跟踪一个或两只手的位置（[图 13.1](#ch13fig01)，面板
    C）。对于整个身体，@tensorflow-models/posenet实现了高精度、实时的骨架关键点检测（如肩膀、肘部、臀部和膝盖；[图 13.1](#ch13fig01)，面板
    D）。
- en: For the audio input modality, @tensorflow-models/speech-commands offers a pretrained
    model that detects 18 English words in real time, directly utilizing the browser’s
    WebAudio API. Although this is not as powerful as large-vocabulary continuous
    speech recognition, it nonetheless enables a range of voice-based user interactions
    in the browser.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 对于音频输入模态，@tensorflow-models/speech-commands提供了一个预训练模型，可以实时检测浏览器的WebAudio API中的18个英文单词。虽然这不像大词汇连续语音识别那样强大，但它仍然可以在浏览器中实现一系列基于语音的用户交互。
- en: There are also pretrained models for text input. For instance, the model from
    @tensorflow-models/toxicity determines how toxic given English input texts are
    along several dimensions (for example, threatening, insulting, or obscene), which
    is useful for aided content moderation ([figure 13.1](#ch13fig01), panel E). The
    toxicity model is built on top of a more generic natural language processing model
    called @tensorflow-models/universal-sentence-encoder, which maps any given English
    sentence into a vector that can then be used for a wide range of natural language
    processing tasks, such as intent classification, topic classification, sentiment
    analysis, and question answering.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 对于文本输入，也有预训练模型可用。例如，来自@tensorflow-models/toxicity的模型确定给定的英文输入文本在多个维度上的毒性程度（例如，威胁、侮辱或淫秽），这对于辅助内容审核很有用（[图
    13.1](#ch13fig01)，面板 E）。毒性模型是建立在一个更通用的自然语言处理模型@tensorflow-models/universal-sentence-encoder之上的，该模型将任何给定的英文句子映射到一个向量，然后可以用于广泛的自然语言处理任务，如意图分类、主题分类、情感分析和问题回答。
- en: It needs to be emphasized that some of the models mentioned not only support
    simple inference but also can form the basis for transfer learning or downstream
    machine learning, which lets you apply the power of these pretrained models to
    your domain-specific data without a lengthy model-building or training process.
    This is partly due to the LEGO-like composability of layers and models. For example,
    the output of the universal sentence encoder is primarily intended to be used
    by a downstream model. The speech-commands model has built-in support for you
    to collect voice samples for new word classes and train a new classifier based
    on the samples, which is useful for voice-command applications that require custom
    vocabulary or user-specific voice adaptation. In addition, outputs from models
    such as PoseNet and face-api.js regarding the moment-by-moment location of the
    head, hands, or body posture can be fed into a downstream model that detects specific
    gestures or movement sequences, which is useful for many applications, such as
    alternative communication for accessibility use cases.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 需要强调的是，提到的一些模型不仅支持简单的推理，还可以为迁移学习或下游机器学习提供基础，使您能够将这些预训练模型的强大功能应用于您的领域特定数据，而无需进行冗长的模型构建或训练过程。这在一定程度上是由于层和模型的乐高式可组合性。例如，通用句子编码器的输出主要用于下游模型。语音命令模型内置支持您收集新单词类别的语音样本，并基于样本训练一个新的分类器，这对于需要自定义词汇或用户特定语音适应的语音命令应用非常有用。此外，来自PoseNet和face-api.js等模型的有关头部、手部或身体姿势的时时位置的输出可以输入到一个下游模型中，该模型检测特定的手势或动作序列，这对于许多应用程序非常有用，如辅助使用案例的替代通信。
- en: Apart from the input modality-oriented models mentioned previously, there are
    also TensorFlow.js-based third-party pretrained models oriented toward artistic
    creativity. For instance, ml5.js includes a model for fast style transfer between
    images and a model that can draw sketches automatically. @magenta/music features
    a model that can transcribe piano music (“audio-to-score”) and MusicRNN, a “language
    model for melodies” that can “write” melodies based on a few seeding notes, along
    with other intriguing pretrained models.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 除了之前提到的面向输入模态的模型之外，还有基于 TensorFlow.js 的第三方预训练模型，面向艺术创造性。例如，ml5.js 包括一个用于图像之间的快速风格转移的模型，以及一个可以自动绘制素描的模型。@magenta/music
    提供了一个可以将钢琴音乐转录成谱曲的模型（“音频转谱”），以及一个“旋律的语言模型”，可以根据几个种子音符“写”出旋律，还有其他有趣的预训练模型。
- en: The collection of pretrained models is large and continues to grow. The JavaScript
    community and the deep-learning community both have an open culture and sharing
    spirit. As you go further on your deep-learning journey, you may come across interesting
    new ideas that are potentially useful to other developers, at which point you
    are encouraged to train, package, and upload your models to npm in the form of
    the pretrained models we’ve mentioned, followed by interaction with users and
    making iterative improvements to your package. Then you’ll truly become a contributing
    member of the JavaScript deep-learning community.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练模型的收集庞大且不断增长。JavaScript 社区和深度学习社区都拥有开放的文化和分享精神。随着你在深度学习的旅程中不断前行，你可能会遇到一些有趣且对其他开发者有用的新想法，此时，我们鼓励你将这些模型以我们提到的预训练模型的形式训练、打包并上传到
    npm，然后与用户互动并对你的包进行迭代改进。那时，你将真正成为 JavaScript 深度学习社区的一员贡献者。
- en: 13.2.4\. The space of possibilities
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 13.2.4\. 可能性的空间
- en: 'With all these layers and pretrained modules as building blocks, what useful
    and fun models can you build? Remember, building deep-learning models is like
    playing with LEGO bricks: layers and modules can be plugged together to map essentially
    anything to anything, as long as the inputs and outputs are represented as tensors,
    and the layers have compatible input and output tensor shapes. The resulting stack
    of layers that is the model performs a differentiable geometric transformation,
    which can learn the mapping relation between the input and the output as long
    as the relation is not overly complex given the model’s capacity. In this paradigm,
    the space of possibilities is infinite. This section offers a few examples to
    inspire you to think beyond the basic classification and regression tasks that
    we’ve emphasized in this book.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 有了所有这些层和预训练模块作为构建模块，你可以构建出哪些有用且有趣的模型呢？记住，构建深度学习模型就像玩乐高积木一样：层和模块可以插在一起，将任何东西映射到任何东西，只要输入和输出表示为张量，并且层具有兼容的输入和输出张量形状。模型的结果层叠执行可微分几何变换，它可以学习输入和输出之间的映射关系，只要关系不过于复杂，以至于超出模型的容量。在这种范式中，可能性的空间是无限的。本节提供了一些示例，以激发你超越我们在本书中强调的基本分类和回归任务的思考。
- en: 'We have sorted the suggestions by input and output modalities. Note that quite
    a few of them stretch the limits of what is possible. Although a model could be
    trained on any of the tasks, given that a sufficient amount of training data is
    available, in some cases, such a model probably wouldn’t generalize well far from
    its training data:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已根据输入和输出模态对建议进行了排序。请注意，其中有不少都在可能性的边界上。虽然模型可以针对任何任务进行训练，只要有足够的训练数据可用，但在某些情况下，这样的模型可能无法很好地泛化远离其训练数据：
- en: Mapping vector to vector
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将向量映射到向量
- en: '*Predictive healthcare*—Mapping patient medical records to predicted treatment
    outcomes'
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预测性医疗保健*——将患者医疗记录映射到预测的治疗结果'
- en: '*Behavioral targeting*—Mapping a set of website attributes to a potential viewer’s
    behavior on the website (including page views, clicks, and other engagements)'
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*行为定位*——将一组网站属性映射到潜在观众在网站上的行为（包括页面浏览、点击和其他互动）'
- en: '*Product quality control*—Mapping a set of attributes related to a manufactured
    product to predictions about how well the product will perform on the market (sales
    and profits in different areas of the market)'
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*产品质量控制*——将与制造产品相关的一组属性映射到关于产品在市场上表现如何的预测（在市场的不同领域的销售和利润）'
- en: Mapping image to vector
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将图像映射到向量
- en: '*Medical image AI*—Mapping medical images (such as X-rays) to diagnostic results'
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*医学图像人工智能*——将医学图像（如X光片）映射到诊断结果'
- en: '*Automatic vehicle steering*—Mapping images from cameras to vehicle control
    signals, such as wheel-steering actions'
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自动车辆转向*—将来自摄像头的图像映射到车辆控制信号，如方向盘转向动作'
- en: '*Diet helper*—Mapping images of foods and dishes to predicted health effects
    (for example, calorie counts or allergy warnings)'
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*饮食助手*—将食物和菜肴的图像映射到预测的健康效应（例如，卡路里计数或过敏警告）'
- en: '*Cosmetic product recommendation*—Mapping selfie images to recommended cosmetic
    products'
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*化妆品推荐*—将自拍图像映射到推荐的化妆品'
- en: Mapping time-series data to vector
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将时间序列数据映射到向量
- en: '*Brain-computer interfaces*—Mapping electroencephalogram (EEG) signals to user
    intentions'
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*脑机接口*—将脑电图（EEG）信号映射到用户意图'
- en: '*Behavioral targeting*—Mapping past history of product purchases (such as movie
    or book purchases) to probabilities of purchasing other products in the future'
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*行为定向*—将产品购买的过去历史（例如电影或书籍购买）映射到未来购买其他产品的概率'
- en: '*Prediction of earthquakes and aftershocks*—Mapping seismic instrument data
    sequences to the predicted likelihoods of earthquakes and ensuing aftershocks'
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*地震和余震预测*—将地震仪器数据序列映射到地震和随后余震发生的预测概率'
- en: Mapping text to vector
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将文本映射到向量
- en: '*Email sorter*—Mapping email content to generic or user-defined labels (for
    example, work-related, family-related, and spam)'
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*电子邮件分类器*—将电子邮件内容映射到通用或用户定义的标签（例如，与工作相关的、与家庭相关的和垃圾邮件）'
- en: '*Grammar scorer*—Mapping student writing samples to writing-quality scores'
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*语法评分器*—将学生写作样本映射到写作质量评分'
- en: '*Speech-based medical triaging*—Mapping a patient’s description of illness
    to the medical department that the patient should be referred to'
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基于语音的医疗分诊*—将患者对疾病的描述映射到应该转诊给的医疗部门'
- en: Mapping text to text
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将文本映射到文本
- en: '*Reply-message suggestion*—Mapping emails to a set of possible response messages'
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*回复消息建议*—将电子邮件映射到一组可能的响应消息'
- en: '*Domain-specific question answering*—Mapping customer questions to automated
    reply texts'
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*领域特定问答*—将客户问题映射到自动回复文本'
- en: '*Summarization*—Mapping a long article to a short summary'
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*摘要*—将长文章映射到简短摘要'
- en: Mapping images to text
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将图像映射到文本
- en: '*Automated alt-text generation*—Given an image, generating a short snippet
    of text that captures the essence of the content'
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自动生成替代文本*—给定一幅图像，生成捕捉内容要点的短文本片段'
- en: '*Mobility aids for the visually impaired*—Mapping images of interior or exterior
    surroundings to spoken guidance and warnings about potential mobility hazards
    (for example, locations of exits and obstacles)'
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*盲人移动辅助*—将内部或外部环境的图像映射到口头指导和有关潜在移动障碍的警告（例如，出口和障碍物位置）'
- en: Mapping images to images
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将图像映射到图像
- en: '*Image super-resolution*—Mapping low-resolution images to higher-resolution
    ones'
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图像超分辨率*—将低分辨率图像映射到更高分辨率的图像'
- en: '*Image-based 3D reconstruction*—Mapping ordinary images to images of the same
    object but viewed from a different angle'
  id: totrans-148
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基于图像的三维重建*—将普通图像映射到同一物体的图像，但从不同角度观察'
- en: Mapping image and time-series data to vector
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将图像和时间序列数据映射到向量
- en: '*Doctor’s multimodal assistant*—Mapping a patient’s medical image (such as
    an MRI) and history of vital signs (blood pressure, heart rate, and so on) to
    predictions of treatment outcomes'
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*医生的多模式助手*—将患者的医学图像（例如MRI）和生命体征历史（血压、心率等）映射到治疗结果的预测'
- en: Mapping image and text to text
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将图像和文本映射到文本
- en: '*Image-based question answering*—Mapping an image and a question related to
    it (for instance, an image of a used car and a question about its make and year)
    to an answer'
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基于图像的问答*—将图像和与之相关的问题（例如，一辆二手车的图像和关于其品牌和年份的问题）映射到一个答案'
- en: Mapping image and vector to image
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将图像和向量映射到图像
- en: '*Virtual try-on for clothes and cosmetic products*—Mapping a user’s selfie
    and a vector representation of a cosmetic or garment to an image of the user wearing
    that product'
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*服装和化妆品虚拟试穿*—将用户的自拍和化妆品或服装的向量表示映射到用户穿着该产品的图像'
- en: Mapping time-series data and vector to time-series data
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将时间序列数据和向量映射到时间序列数据
- en: '*Musical style transfer*—Mapping a musical score (such as a classical piece
    represented as a timeseries of notes) and a description of the desired style (for
    example, jazz) to a new musical score in the desired style'
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*音乐风格转换*—将音乐谱（例如表示为音符时间序列的古典乐曲）和所需风格的描述（例如，爵士乐）映射到所需风格的新音乐谱'
- en: As you may have noticed, the last four categories in this list involve mixed
    modalities in input data. At this point in our technological history, where most
    things in life have been digitized and can hence be represented as tensors, what
    you can potentially achieve with deep learning is limited only by your own imagination
    and the availability of training data. Although almost any mapping is possible,
    not every mapping is. We’ll discuss in the next section what deep learning *cannot*
    do yet.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您可能已经注意到的，此列表中的最后四个类别涉及输入数据中的混合模态。在我们技术史上的这一时刻，生活中的大多数事物都已数字化，因此可以表示为张量，您可以通过深度学习潜在地实现的东西仅受限于您自己的想象力和训练数据的可用性。虽然几乎任何映射都是可能的，但并非每个映射都是。在下一节中，我们将讨论深度学习*尚不能*做到的事情。
- en: 13.2.5\. Limitations of deep learning
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 13.2.5。深度学习的限制
- en: The space of applications that can be implemented with deep learning is nearly
    infinite. As a result, it is easy to overestimate the power of deep neural networks
    and be overly optimistic about what problems they can solve. This section briefly
    talks about some of the limitations that they still have.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用深度学习实现的应用程序空间几乎是无限的。因此，很容易高估深度神经网络的力量，并对它们可以解决的问题过于乐观。本节简要讨论了它们仍然具有的一些限制。
- en: Neural networks do not see the world in the same way humans do
  id: totrans-160
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 神经网络并不以与人类相同的方式看待世界。
- en: A risk we face when trying to understand deep learning is *anthropomorphization*—that
    is, the tendency to misinterpret deep neural networks as mimicking perception
    and cognition in humans. Anthropomorphizing deep neural networks is demonstrably
    wrong in a few regards. First, when humans perceive a sensory stimulus (such as
    an image with a girl’s face in it or an image with a toothbrush), they not only
    perceive the brightness and color patterns of the input but also extract the deeper
    and more important concepts represented by those superficial patterns (for example,
    the face of a young, female individual or a dental hygiene product, and the relation
    between the two). Deep neural networks, on the other hand, don’t work this way.
    When you’ve trained an image-captioning model to map images to text output, it
    is wrong to believe that the model understands the image in a human sense. In
    some cases, even the slightest departure from the sort of images present in the
    training data can cause the model to generate absurd captions (as in [figure 13.2](#ch13fig02)).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在尝试理解深度学习时，我们面临的一个风险是*拟人化*，即倾向于误解深度神经网络仿效人类感知和认知。在几个方面，将深度神经网络拟人化是明显错误的。首先，当人类感知到感官刺激（例如带有女孩脸的图像或带有牙刷的图像）时，他们不仅感知到输入的亮度和颜色模式，还提取由这些表面模式表示的更深层次和更重要的概念（例如，年轻女性个体的面孔或口腔卫生产品，以及两者之间的关系）。另一方面，深度神经网络不是这样工作的。当你训练了一个图像标题模型来将图像映射到文本输出时，认为该模型以人类意义理解图像是错误的。在某些情况下，即使是与训练数据中出现的图像类型稍有不同，也可能导致模型生成荒谬的标题（如[图13.2](#ch13fig02)）。
- en: Figure 13.2\. Failure of an image-captioning model trained with deep learning
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图13.2。使用深度学习训练的图像标题模型失败
- en: '![](13fig02_alt.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](13fig02_alt.jpg)'
- en: In particular, the peculiar, nonhuman way in which deep neural networks process
    their inputs is highlighted by *adversarial examples*, which are samples purposefully
    designed to trick a machine-learning model into making classification mistakes.
    As we demonstrated by finding the maximally activating images for convnet filters
    in [section 7.2](kindle_split_019.html#ch07lev1sec2), it’s possible to do gradient
    ascent in the input space to maximize the activation of a convnet filter. The
    idea can be extended to output probabilities, so we can perform gradient ascent
    in the input space to maximize the model’s predicted probability for any given
    output class. By taking a picture of a panda and adding a “gibbon gradient” to
    it, we can cause a model to misclassify the image as a gibbon ([figure 13.3](#ch13fig03)).
    This is despite the fact that the gibbon gradient is noise-like and small in magnitude,
    so that the resulting adversarial image looks indistinguishable from the original
    panda image to humans.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，深度神经网络处理输入的非人类方式被*对抗样本*突显出来，这些样本是特意设计的，目的是欺骗机器学习模型使其产生分类错误。正如我们在[第 7.2 节](kindle_split_019.html#ch07lev1sec2)中通过寻找激活最大的图像来为卷积神经网络滤波器找到的那样，可以在输入空间中进行梯度上升以最大化卷积神经网络滤波器的激活。这个想法可以扩展到输出概率，因此我们可以在输入空间中进行梯度上升，以最大化模型对任何给定输出类别的预测概率。通过给熊猫拍照并添加“长臂猿梯度”，我们可以使模型将图像误分类为长臂猿（[图
    13.3](#ch13fig03)）。这尽管长臂猿梯度在噪声和幅度上都很小，因此导致的对抗性图像对人类来说看起来与原始熊猫图像无法区分。
- en: 'Figure 13.3\. Adversarial example: changes imperceptible to human eyes can
    throw off a deep convnet’s classification result. See more discussion on adversarial
    attacks of deep neural networks at [http://mng.bz/pyGz](http://mng.bz/pyGz).'
  id: totrans-165
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 13.3\. 对抗样本：对人眼来说几乎无法察觉的改变可能会影响深度卷积神经网络的分类结果。有关深度神经网络对抗攻击的更多讨论，请参见[http://mng.bz/pyGz](http://mng.bz/pyGz)。
- en: '![](13fig03_alt.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](13fig03_alt.jpg)'
- en: 'So, deep neural networks for computer vision don’t possess a real understanding
    of images—at least not in a human sense. Another area in which human learning
    stands in sharp contrast with deep learning is how the two types of learning generalize
    from a limited number of training examples. Deep neural networks can do what can
    be called *local generalization*. [Figure 13.4](#ch13fig04) illustrates a scenario
    in which a deep neural network and a human are tasked to learn the boundary of
    a single class in a 2D parametric space by using only a small number of (say,
    eight) training examples. The human realizes that the shape of the class boundary
    should be smooth and the region should be connected, and quickly draws a single
    closed curve as the “guesstimated” boundary. A neural network, on the other hand,
    suffers from a lack of abstraction and prior knowledge. Therefore, it may end
    up with an ad hoc irregular boundary severely overfit to the few training samples.
    The trained model will generalize very poorly beyond the training samples. Adding
    more samples can help the neural network but is not always practically possible.
    The main problem is that the neural network is created from scratch, just for
    this particular problem. Unlike a human individual, it doesn’t have any prior
    knowledge to rely on and hence doesn’t know what to “expect.”^([[5](#ch13fn5)])
    This is the fundamental reason behind a major limitation of current deep-learning
    algorithms: namely, a large number of human-labeled training data is usually required
    to train a deep neural network to decent generalization accuracy.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，用于计算机视觉的深度神经网络并没有真正理解图像，至少不是以人类的方式。人类学习与深度学习在如何从有限数量的训练样本中泛化这两种学习方式之间存在鲜明对比的另一个领域。深度神经网络可以做到所谓的*局部泛化*。[图
    13.4](#ch13fig04)展示了一个场景，在这个场景中，深度神经网络和人类被要求仅使用少量（比如，八个）训练样本来学习二维参数空间中单个类别的边界。人类意识到类别边界的形状应该是平滑的，区域应该是连通的，并迅速绘制出一个闭合的曲线作为“猜测的”边界。另一方面，神经网络缺乏抽象和先验知识。因此，它可能会得到一个专门的、不规则的边界，严重过拟合于少量训练样本。训练好的模型将在训练样本之外泛化得非常差。增加更多的样本可以帮助神经网络，但这并不总是可行的。主要问题是神经网络是从零开始创建的，只为了解决这个特定的问题。与人类个体不同，它没有任何可以依赖的先验知识，因此不知道要“期望”什么。^([[5](#ch13fn5)])这是当前深度学习算法主要局限性的根本原因：通常需要大量的人工标记的训练数据才能训练出一个泛化准确度良好的深度神经网络。
- en: ⁵
  id: totrans-168
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁵
- en: ''
  id: totrans-169
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: There are research efforts to train a single deep neural network on many different
    and seemingly unrelated tasks to facilitate cross-domain knowledge sharing (see,
    for example, Lukasz Kaiser et al., “One Model To Learn Them All,” submitted 16
    Jun. 2017, [https://arxiv.org/abs/1706.05137](https://arxiv.org/abs/1706.05137)).
    But such multitask models have not received wide adoption yet.
  id: totrans-170
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 有研究工作在训练单个深度神经网络上进行许多不同且看似无关的任务，以促进跨领域知识共享（参见，例如，Lukasz Kaiser 等人，“学习所有任务的一个模型”，2017年6月16日提交，[https://arxiv.org/abs/1706.05137](https://arxiv.org/abs/1706.05137)）。但是，这种多任务模型尚未被广泛采用。
- en: Figure 13.4\. Local generalization in deep-learning models vs. extreme generalization
    in human intelligence
  id: totrans-171
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图13.4\. 深度学习模型中的局部泛化与人类智能的极端泛化
- en: '![](13fig04_alt.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](13fig04_alt.jpg)'
- en: 13.3\. Trends in deep learning
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.3\. 深度学习的趋势
- en: 'As we’ve discussed, deep learning has made amazing progress in recent years,
    but it still suffers from some limitations. But the field is not static; it keeps
    advancing at a breathtaking velocity, so it’s likely that some of the limitations
    will be ameliorated in the near future. This section contains a set of educated
    guesses about what important breakthroughs in deep learning we’ll witness in the
    coming years:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们讨论过的，深度学习在近年取得了惊人的进展，但仍然存在一些局限性。但这个领域并不是静止的；它以惊人的速度不断前进，因此很可能在不久的将来一些局限性会得到改善。本节包含了我们预计在未来几年将见证的深度学习重要突破的一系列合理猜测：
- en: First, unsupervised or semisupervised learning could see significant advancements.
    This will have a profound impact on all forms of deep learning because even though
    labeled datasets are costly to construct and hard to come across, there is an
    abundance of unlabeled datasets in all sorts of business domains. If we can invent
    a way to use a small amount of labeled data to guide the learning from a vast
    amount of unlabeled data, it will unlock many new applications for deep learning.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，无监督或半监督学习可能会有重大进展。这将对所有形式的深度学习产生深远影响，因为尽管标记数据集的构建成本高昂且难以获得，但在各种业务领域都有大量的未标记数据集。如果我们能够发明一种方法，利用少量标记数据来引导从大量未标记数据中学习，它将为深度学习开启许多新的应用。
- en: Second, hardware for deep learning may continue to be improved, ushering in
    more and more powerful neural network accelerators (such as the future generations
    of the Tensor Processing Unit^([[6](#ch13fn6)])). This will allow researchers
    to train ever more powerful networks with ever larger datasets and thereby continue
    to push forward the state-of-the-art accuracy on many machine-learning tasks,
    such as computer vision, speech recognition, natural language processing, and
    generative models.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，深度学习的硬件可能会继续改进，引入越来越强大的神经网络加速器（例如张量处理单元的未来一代^([[6](#ch13fn6)])）。这将使研究人员能够使用越来越大的数据集训练更加强大的网络，并继续推动计算机视觉、语音识别、自然语言处理和生成模型等许多机器学习任务的最新技术准确性。
- en: ⁶
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁶
- en: ''
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: Norman P. Jouppi et al., “In-Datacenter Performance Analysis of a Tensor Processing
    Unit™,” 2017, [https://arxiv.org/pdf/1704.04760.pdf](https://arxiv.org/pdf/1704.04760.pdf).
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Norman P. Jouppi 等人，“数据中心张量处理单元™的性能分析”，2017年，[https://arxiv.org/pdf/1704.04760.pdf](https://arxiv.org/pdf/1704.04760.pdf)。
- en: Designing model architecture and tuning model hyperparameters will likely become
    more and more automated. We are already seeing a trend in this area, as exemplified
    by technologies such as AutoML^([[7](#ch13fn7)]) and Google Vizier.^([[8](#ch13fn8)])
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计模型架构和调整模型超参数可能会变得越来越自动化。我们已经在这个领域看到了一个趋势，如 AutoML^([[7](#ch13fn7)]) 和 Google
    Vizier^([[8](#ch13fn8)]) 等技术的示例所示。
- en: ⁷
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁷
- en: ''
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: Barret Zoph and Quoc V. Le, “Neural Architecture Search with Reinforcement Learning,”
    submitted 5 Nov. 2016, [https://arxiv.org/abs/1611.01578](https://arxiv.org/abs/1611.01578).
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Barret Zoph 和 Quoc V. Le，“利用强化学习进行神经架构搜索”，2016年11月5日提交，[https://arxiv.org/abs/1611.01578](https://arxiv.org/abs/1611.01578)。
- en: ⁸
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁸
- en: ''
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Daniel Golovin, “Google Vizier: A Service for Black-Box Optimization,” Proc.
    23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
    2017, pp. 1487–1495, [http://mng.bz/O9yE](http://mng.bz/O9yE).'
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Daniel Golovin，“Google Vizier：一种用于黑盒优化的服务”，2017年第23届ACM SIGKDD国际知识发现与数据挖掘会议论文集，第1487–1495页，[http://mng.bz/O9yE](http://mng.bz/O9yE)。
- en: The sharing and reuse of neural network components will likely continue to grow.
    Transfer learning based on pretrained models will gain further momentum. State-of-the-art
    deep-learning models are getting increasingly powerful and generic by the day.
    They are increasingly trained on larger and larger datasets, sometimes with huge
    amounts of computation power for the sake of automated architectural search and
    hyperparameter tuning (see the first and second predictions). As a consequence,
    it’s becoming more sensible and economical to reuse such pretrained models, for
    either direct inference or transfer learning, than to train them from scratch
    over and over again. In a way, this makes the field of deep learning more similar
    to traditional software engineering, in which high-quality libraries are depended
    on and reused regularly, to the benefit of standardization and the development
    velocity of the field as a whole.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络组件的共享和重用可能会继续增长。基于预训练模型的迁移学习将进一步发展。每天都有最先进的深度学习模型变得越来越强大和通用。它们越来越多地在更大更大的数据集上进行训练，有时候为了自动化架构搜索和超参数调整而需要大量的计算资源（请参阅第一和第二条预测）。因此，与其一次又一次地从头开始训练它们，不如对这些预训练模型进行直接推断或迁移学习，这样做更加明智和经济。在某种程度上，这使得深度学习领域更类似于传统的软件工程，高质量的库被定期依赖和重用，这有利于整个领域的标准化和发展速度。
- en: Deep learning may be deployed to new areas of application, improving many existing
    solutions and opening up new practical use cases. In our opinion, the potential
    areas of application are truly limitless. Fields including agriculture, finance,
    education, transportation, healthcare, fashion, sports, and entertainment present
    countless opportunities waiting to be explored for deep-learning practitioners.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习可能会部署到新的应用领域，改进许多现有解决方案，并开启新的实际应用案例。在我们看来，潜在的应用领域是真正无限的。农业、金融、教育、交通、医疗保健、时尚、体育和娱乐等领域提供了无数等待深度学习从业者探索的机会。
- en: As deep learning penetrates more application domains, there will likely be a
    growing emphasis on deep learning at the edge because edge devices are closest
    to where the users are. As a result, the field will likely invent smaller and
    more power-efficient neural network architectures that achieve the same prediction
    accuracy and speed as existing, larger models.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着深度学习渗透到更多的应用领域，对边缘深度学习的重视可能会日益增加，因为边缘设备最接近用户所在地。因此，该领域可能会发明更小、更节能的神经网络架构，实现与现有更大模型相同的预测准确性和速度。
- en: All these predictions will affect deep learning in JavaScript, but the last
    three predictions are especially relevant. Expect more powerful and efficient
    models to become available to TensorFlow.js in the future.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些预测都将影响 JavaScript 中的深度学习，但最后三个预测尤为重要。可以预期未来 TensorFlow.js 将有更强大、更高效的模型可用。
- en: 13.4\. Pointers for further exploration
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.4\. 进一步探索的指针
- en: As final parting words, we want to give you some pointers about how to keep
    learning and updating your knowledge and skills after you’ve turned the last page
    of this book. The field of modern deep learning as we know it today is only a
    few years old, despite a long, slow prehistory stretching back decades. With an
    exponential increase in financial resources and research headcount since 2013,
    the field as a whole is now moving at a frenetic pace. Many of the things you
    learned in this book won’t stay relevant for very long. It is the core ideas of
    deep learning (learning from data, reducing manual feature engineering, layer-by-layer
    transformation of representation) that will likely stick around for a longer time.
    More importantly, the foundation of knowledge you developed by reading this book
    will hopefully prepare you to learn about new developments and trends in the field
    of deep learning on your own. Fortunately, the field has an open culture in which
    most cutting-edge advances (including many datasets!) are published in the form
    of openly accessible and free preprints, accompanied by public blog posts and
    tweets. Here are a few top resources you should be familiar with.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们想给你一些关于在你翻阅完本书最后一页后如何继续学习和更新知识和技能的指导。尽管现代深度学习领域今天我们所知道的只有几年的历史，但它的漫长而缓慢的前史可以追溯到几十年前。自2013年以来，随着财政资源和研究人员数量的指数增长，整个领域现在正以疯狂的速度发展。你在本书中学到的很多东西不会保持很长时间的相关性。深度学习的核心思想（从数据中学习，减少手动特征工程，逐层转换表示）可能会更长时间地存在。更重要的是，通过阅读本书你所建立的知识基础将有望使你能够自己了解深度学习领域的新发展和趋势。幸运的是，这个领域有着开放的文化，其中大多数前沿进展（包括许多数据集！）都以公开可访问和免费的预印本的形式发布，附有公开的博客文章和推文。以下是您应该熟悉的一些顶级资源。
- en: 13.4.1\. Practice real-world machine-learning problems on Kaggle
  id: totrans-193
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 13.4.1\. 在Kaggle上练习真实世界的机器学习问题
- en: An effective way to acquire real-world experience in machine learning (and especially
    deep learning) is to try your hand at competitions on Kaggle ([https://kaggle.com](https://kaggle.com)).
    The only real way to learn machine learning is through actual coding, model building,
    and tuning. That’s the philosophy of the book, as reflected in its numerous code
    examples ready for you to study, tweak, and hack. But nothing is as effective
    in teaching you how to do machine learning as building your models and machine-learning
    systems in a ground-up fashion, using a library such as TensorFlow.js. On Kaggle,
    you can find an array of constantly renewed data-science competitions and datasets,
    many of which involve deep learning.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 获得机器学习（特别是深度学习）的真实世界经验的有效方法是在Kaggle竞赛中尝试手气（[https://kaggle.com](https://kaggle.com)）。学习机器学习的唯一真正方法是通过实际的编码、模型构建和调整。这正是本书的哲学，体现在其众多的代码示例中，供您学习、调整和修改。但没有什么比在地基础上使用TensorFlow.js等库从头开始构建你的模型和机器学习系统更有效的教你如何做机器学习了。在Kaggle上，你可以找到一系列不断更新的数据科学竞赛和数据集，其中许多涉及深度学习。
- en: Although most Kaggle users use Python tools (such as TensorFlow and Keras) to
    solve the competitions, most of the datasets on Kaggle are language-agnostic.
    So, it is entirely feasible to solve most Kaggle problems using a non-Python deep-learning
    framework like TensorFlow.js. By participating in a few competitions, maybe as
    a part of a team, you’ll become familiar with the practical side of some of the
    advanced best practices described in this book, especially hyperparameter tuning
    and avoiding validation-set overfitting.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然大多数Kaggle用户使用Python工具（如TensorFlow和Keras）来解决竞赛问题，但Kaggle上的大多数数据集都与语言无关。因此，完全有可能使用非Python深度学习框架（如TensorFlow.js）解决大多数Kaggle问题。通过参加一些竞赛，也许作为团队的一部分，你将熟悉本书中描述的一些高级最佳实践的实际应用，尤其是超参数调整和避免验证集过拟合。
- en: 13.4.2\. Read about the latest developments on arXiv
  id: totrans-196
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 13.4.2\. 阅读arXiv上的最新发展
- en: 'Deep-learning research, in contrast with some other academic fields, takes
    place almost completely in the open. Papers are made publicly and freely accessible
    as soon as they are finalized and pass review, and a lot of related software is
    open source. ArXiv ([https://arxiv.org](https://arxiv.org))—pronounced “archive”
    (the X stands for the Greek letter *chi*)—is an open-access preprint server for
    mathematics, physics, and computer science papers. It has become the de facto
    way to publish cutting-edge work in the field of machine learning and deep learning,
    and hence is also the de facto way to stay up-to-date with the field. This allows
    the field to move extremely fast: all new findings and inventions are instantly
    available for all to see, to critique, and to build on.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他学术领域相比，深度学习研究几乎完全是公开进行的。论文一经完成并经过审查便会公开并免费提供，许多相关软件也是开源的。ArXiv（[https://arxiv.org](https://arxiv.org)）—读作“archive”（X代表希腊字母*chi*）—是一家数学、物理和计算机科学论文的开放获取预印本服务器。它成为了发表机器学习和深度学习领域最尖端工作的实际方式，因此也成为了学习该领域最新进展的实际方式。这使得该领域能够以极快的速度前进：所有新的发现和发明都能立即供所有人查阅、评论和建立在其基础之上。
- en: An important downside of ArXiv is the sheer quantity of new papers posted every
    day, which makes it impossible to skim them all. The fact that many of the papers
    on ArXiv aren’t peer-reviewed makes it difficult to identify which ones are important
    and of high quality. The community has built tools to help with these challenges.
    For example, a website called ArXiv Sanity Preserver ([arxiv-sanity.com](http://arxiv-sanity.com))
    serves as a recommendation engine for new ArXiv papers and can help you keep track
    of new developments in specific vertical domains of deep learning (such as natural
    language processing or object detection). Additionally, you can use Google Scholar
    to keep track of publications in your areas of interest and by your favorite authors.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ArXiv的一个重要缺点是每天发布的新论文数量实在太多，以至于不可能都浏览一遍。ArXiv上的许多论文没有经过同行评审，这使得它很难识别哪些是重要且高质量的。社区已经建立了工具来应对这些挑战。例如，一个名为ArXiv
    Sanity Preserver（[arxiv-sanity.com](http://arxiv-sanity.com)）的网站作为ArXiv新论文的推荐引擎，可以帮助您跟踪深度学习特定垂直领域（如自然语言处理或目标检测）的新发展。此外，您还可以使用Google学术跟踪您感兴趣的领域和您喜欢的作者的出版物。
- en: 13.4.3\. Explore the TensorFlow.js Ecosystem
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 13.4.3\. 探索TensorFlow.js生态系统
- en: 'TensorFlow.js has a vibrant and growing ecosystem of documentation, guides,
    tutorials, blogosphere, and open source projects:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow.js拥有充满活力且不断发展的生态系统，包括文档、指南、教程、博客圈和开源项目：
- en: Your main reference for working with TensorFlow.js is the official online documentation
    at [www.tensorflow.org/js/](http://www.tensorflow.org/js/). The detailed, up-to-date
    API documentation is available at [https://js.tensorflow.org/api/latest/](https://js.tensorflow.org/api/latest/).
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您使用TensorFlow.js的主要参考资料是官方在线文档，网址为[www.tensorflow.org/js/](http://www.tensorflow.org/js/)。详细的最新API文档可在[https://js.tensorflow.org/api/latest/](https://js.tensorflow.org/api/latest/)找到。
- en: 'You can ask questions about TensorFlow.js on Stack Overflow using the tag “tensorflow.js”:
    [https://stackoverflow.com/questions/tagged/tensorflow.js](https://stackoverflow.com/questions/tagged/tensorflow.js).'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以在Stack Overflow上使用“tensorflow.js”标签提出有关TensorFlow.js的问题：[https://stackoverflow.com/questions/tagged/tensorflow.js](https://stackoverflow.com/questions/tagged/tensorflow.js)。
- en: 'For general discussion about the library, use the Google Group: [https://groups.google.com/a/tensorflow.org/forum/#!forum/tfjs](https://groups.google.com/a/tensorflow.org/forum/#!forum/tfjs).'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关该库的一般讨论，请使用Google Group：[https://groups.google.com/a/tensorflow.org/forum/#!forum/tfjs](https://groups.google.com/a/tensorflow.org/forum/#!forum/tfjs)。
- en: You can also follow members of the TensorFlow.js team who have an active presence
    on Twitter, including
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您还可以关注在Twitter上活跃的TensorFlow.js团队成员，包括
- en: '[https://twitter.com/sqcai](https://twitter.com/sqcai)'
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://twitter.com/sqcai](https://twitter.com/sqcai)'
- en: '[https://twitter.com/nsthorat](https://twitter.com/nsthorat)'
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://twitter.com/nsthorat](https://twitter.com/nsthorat)'
- en: '[https://twitter.com/dsmilkov](https://twitter.com/dsmilkov)'
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://twitter.com/dsmilkov](https://twitter.com/dsmilkov)'
- en: '[https://twitter.com/tensorflow](https://twitter.com/tensorflow)'
  id: totrans-208
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://twitter.com/tensorflow](https://twitter.com/tensorflow)'
- en: Final words
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最后的话
- en: This is the end of *Deep Learning with JavaScript*! We hope you’ve learned a
    thing or two about AI, deep learning, and how to perform some basic deep-learning
    tasks in JavaScript using TensorFlow.js. Like any interesting and useful topic,
    learning about AI and deep learning is a life-long journey. The same can be said
    for the application of AI and deep learning to practical problems. This is true
    for professionals and amateurs alike. For all the progress made in deep learning
    so far, most of the fundamental questions remain unanswered, and most of the practical
    potential of deep learning has barely been tapped. Please keep learning, questioning,
    researching, imaging, hacking, building, and sharing! We look forward to seeing
    what you build using deep learning and JavaScript!
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是*JavaScript深度学习*的结尾！希望你在AI、深度学习以及如何在JavaScript中使用TensorFlow.js执行一些基本的深度学习任务方面学到了一些东西。像任何有趣且有用的话题一样，学习AI和深度学习是一次终身的旅程。这同样适用于将AI和深度学习应用于实际问题。无论是专业人士还是业余爱好者都是如此。尽管在深度学习方面取得了许多进展，但大部分基本问题仍然没有得到答案，大部分深度学习的潜在威力也几乎没有得到发掘。请继续学习、质疑、研究、想象、探索、构建和分享！我们期待着看到你用深度学习和JavaScript构建的作品！
