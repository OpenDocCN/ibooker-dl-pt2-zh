- en: 7 Finding Kernels of Knowledge in Text with Convolutional Neural Networks (CNNs)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过卷积神经网络（CNNs）在文本中找到知识的核心
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章内容包括
- en: Understanding neural networks for NLP
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解自然语言处理的神经网络
- en: Finding patterns in sequences
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在序列中找到模式
- en: Building a CNN with PyTorch
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PyTorch构建CNN
- en: Training a CNN
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练一个CNN
- en: Training embeddings
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练嵌入
- en: Classifying text
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对文本进行分类
- en: In this chapter, you will unlock the misunderstood superpowers of convolution
    for Natural Language Processing. This will help your machine understand words
    by detecting patterns in sequences of words and how they are related to their
    neighbors.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将解锁卷积在自然语言处理中被误解的超能力。这将帮助你的机器通过检测单词序列中的模式以及它们与邻居的关系来理解单词。
- en: '*Convolutional Neural Networks* (CNNs) are all the rage for *computer vision*
    (image processing). But few businesses appreciate the power of CNNs for NLP. This
    creates an opportunity for you in your NLP learning and for entrepreneurs that
    understand what CNNs can do. For example, in 2022 Cole Howard and Hannes Hapke
    (coauthors on the first edition of this book) used their NLP CNN expertise to
    help their startup automate business and accounting decisions.^([[1](#_footnotedef_1
    "View footnote.")]) And deep learning deep thinkers in academia like Christopher
    Manning and Geoffrey Hinton use CNNs to crush the competition in NLP. You can
    too.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*卷积神经网络*（CNNs）在*计算机视觉*（图像处理）领域大行其道。但很少有企业认识到CNNs在自然语言处理中的威力。这为你在自然语言处理学习中以及理解CNNs能做什么的企业家创造了机会。例如，2022年，科尔·霍华德和汉内斯·哈普克（本书第一版的共同作者）利用他们的自然语言处理CNN专业知识帮助他们的初创公司自动化业务和会计决策。^([[1](#_footnotedef_1
    "View footnote.")])并且学术界的深度学习专家，如克里斯托弗·曼宁和杰弗里·辛顿，使用CNNs在自然语言处理领域击败竞争对手。你也可以。'
- en: So why haven’t CNNs caught on with the industry and big tech corporations? Because
    they are too good — too efficient. CNNs don’t need the massive amounts of data
    and compute resources that are central to Big Tech’s monopoly power in AI. They
    are interested in models that "scale" to huge datasets, like reading the entire
    Internet. Researchers with access to big data focus on problems and models that
    leverage their competitive advantage with data, "the new oil."^([[2](#_footnotedef_2
    "View footnote.")]) It’s hard to charge people much money for a model anyone can
    train and run on their own laptop.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 那么为什么CNNs在行业和大型科技公司中没有引起注意呢？因为它们太好了——太有效率了。CNNs不需要大量的数据和计算资源，这是大科技公司在人工智能领域的垄断力量的核心。他们对能够"扩展"到海量数据集的模型感兴趣，比如阅读整个互联网。拥有大数据访问权限的研究人员专注于利用他们在数据方面的竞争优势的问题和模型，即"新石油"。^([[2](#_footnotedef_2
    "View footnote.")])让人们为一个任何人都可以在自己的笔记本电脑上训练和运行的模型付钱是很困难的。
- en: Another more mundane reason CNNs are overlooked is that properly configured
    and tuned CNNs for NLP are hard to find. I wasn’t able to find a single reference
    implementation of CNNs for NLP in PyTorch, Keras, or TensorFlow. And the unofficial
    implementations seemed to transpose the CNN channels used for image processing
    to create convolutions in embedding dimensions rather than convolution in time.
    You’ll soon see why that is a bad idea. But don’t worry, you’ll soon see the mistakes
    that others have made and you’ll be building CNNs like a pro. Your CNNs will be
    more efficient and performant than anything coming out of the blogosphere.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个更加平凡的原因是CNNs被忽视的原因是，为自然语言处理正确配置和调整的CNNs很难找到。我找不到一个在PyTorch、Keras或TensorFlow中为自然语言处理实现的CNNs的单一参考实现。而非官方的实现似乎将用于图像处理的CNN通道转置为在嵌入维度上创建卷积，而不是在时间上进行卷积。很快你就会明白为什么这是一个糟糕的想法。但别担心，你很快就会看到别人犯的错误，你将像专业人士一样构建CNNs。你的CNNs将比博客圈中出现的任何东西更有效率，性能更高。
- en: Perhaps you’re asking yourself why should you learn about CNNs when the shiny
    new thing in NLP, *transformers*, are all the rage. You’ve probably heard of *GPT-J*,
    *GPT-Neo*, *PaLM* and others. After reading this chapter you’ll be able to build
    better, faster, cheaper NLP models based on CNNs while everyone else is wasting
    time and money on Giga-parameter transformers. You won’t need the unaffordable
    compute and training data that large transformers require.^([[3](#_footnotedef_3
    "View footnote.")]) ^([[4](#_footnotedef_4 "View footnote.")]) ^([[5](#_footnotedef_5
    "View footnote.")])
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 或许你正在问自己，为什么在自然语言处理领域新潮流是*transformers*时，你还应该学习卷积神经网络（CNNs）。你可能听说过*GPT-J*、*GPT-Neo*、*PaLM*等等。阅读完本章后，你将能够基于CNNs构建更好、更快、更便宜的自然语言处理模型，而其他人还在浪费时间和金钱在千亿参数的transformers上。你不需要大型transformers所需的昂贵计算资源和训练数据。^([[3](#_footnotedef_3
    "View footnote.")]) ^([[4](#_footnotedef_4 "View footnote.")]) ^([[5](#_footnotedef_5
    "View footnote.")])
- en: '**PaLM**: 540B parameters'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PaLM**：540B参数'
- en: '**GPT-3**: 175B parameters'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPT-3**：175B参数'
- en: '**T5-11B**: 11B parameters (FOSS, outperforms GPT-3)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**T5-11B**：11B参数（FOSS，胜过GPT-3）'
- en: '**GPT-J**: 6B parameters (FOSS, outperforms GPT-3)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPT-J**：6B参数（FOSS，胜过GPT-3）'
- en: '**CNNs** (in this Chapter): less than 200k parameters'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CNNs**（本章中）：少于200k参数'
- en: Yes, in this chapter you’re going to learn how to build CNN models that are
    a million times smaller and faster than the big transformers you read about in
    the news. And CNNs are often the best tool for the job.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，在本章中，你将学会如何构建比你在新闻中读到的大型Transformer小一百万倍且更快的CNN模型。而CNN通常是完成任务的最佳工具。
- en: 7.1 Patterns in sequences of words
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 单词序列中的模式
- en: Individual words worked well for you in the previous chapters. You can say a
    lot with individual words. You just need to choose the right words or find the
    keywords in a passage of text and that can usually capture the meaning. And the
    order doesn’t matter too much for the kinds of problems you solved in previous
    chapters. If you throw all the words from a job title such as "Junior Engineer"
    or "Data Scientist" into a bag of words (BOW) vector, the jumbled-up BOW contains
    most of the information content of the original title. That’s why all the previous
    examples in this book worked best on short phrases or individual words. That’s
    why keywords are usually enough to learn the most important facts about a job
    title or get the gist of a movie title.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在以前的章节中，单个单词对你来说效果很好。你可以用单个单词说很多话。你只需要选择正确的单词或在一段文字中找到关键词，那通常就能捕捉到意思。而且在你以前解决的那些问题中，顺序并不是很重要。如果你将“初级工程师”或“数据科学家”等工作标题的所有单词都放入词袋（BOW）向量中，那么搅在一起的BOW包含了原始标题的大部分信息内容。这就是为什么本书中以前的所有例子在短语或单个单词上效果最好的原因。这就是为什么关键词通常足以了解一个工作标题的最重要信息或理解一部电影的主要内容。
- en: And that’s why it’s so hard to choose just a few words to summarize a book or
    job with its title. For short phrases, the occurrence of words is all that matters.
    When you want to express a complete thought, more than just a title, you have
    to use longer sequences of words. And the order matters.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是为什么要选择几个词来总结一本书或一份带有标题的工作是如此困难。对于短语，单词的出现是唯一重要的。当你想要表达一个完整的思想，不仅仅是一个标题时，你必须使用更长的单词序列。而且顺序很重要。
- en: Before NLP, and even before computers, humans used a mathematical operation
    called *convolution* to detect patterns in sequences. For NLP, convolution is
    used to detect patterns that span multiple words and even multiple sentences.
    The original convolutions were handcrafted on paper with a quill pen or even a
    cuneiform on a clay tablet! Once computers were invented, researchers and mathematicians
    would handcraft the math to match what they wanted to achieve for each problem.
    Common hand-crafted kernels for image processing include Laplacian, Sobel, and
    Gaussian filters. And in digital signal processing similar to what is used in
    NLP, low pass and high pass convolution filters can be designed from first principles.
    If you’re a visual learner or are into computer vision it might help you grasp
    convolution if you check out heatmap plots of the kernels used for these convolutional
    filters on Wikipedia.^([[6](#_footnotedef_6 "View footnote.")]) ^([[7](#_footnotedef_7
    "View footnote.")]) ^([[8](#_footnotedef_8 "View footnote.")]) These filters might
    even give you ideas for initializations of your CNN filter weights to speed learning
    and create more explainable deep learning language models.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在NLP之前，甚至在计算机出现之前，人类使用一种叫做*卷积*的数学运算来检测序列中的模式。对于NLP，卷积被用来检测跨越多个单词甚至多个句子的模式。最初的卷积是用鹅毛笔甚至粘土板上的楔形文字手工制作的！一旦计算机被发明出来，研究人员和数学家就会手工制作数学公式来匹配他们想要解决的每个问题。用于图像处理的常见手工制作内核包括拉普拉斯、索贝尔和高斯滤波器。在数字信号处理中，类似于NLP中使用的低通和高通卷积滤波器可以根据第一原理进行设计。如果你是一个视觉学习者或对计算机视觉感兴趣，通过查看维基百科上用于这些卷积滤波器的热图绘制，你可能会更容易理解卷积。这些滤波器甚至可能给你关于初始化CNN滤波器权重以加快学习并创建更可解释的深度学习语言模型的想法。
- en: But that gets tedious after a while, and we no longer even consider handcrafted
    filters to be important in computer vision or NLP. Instead, we use statistics
    and neural networks to automatically *learn* what patterns to look for in images
    and text. Researchers started with linear fully connected networks (multi-layer
    perceptrons). But these had a real problem with over-generalization and couldn’t
    recognize when a pattern of words moved from the beginning to the end of the sentence.
    Fully-connect neural networks are not scale- and translation-invariant. But then
    David Rumelhart invented and Geoffrey Hinton popularized the backpropagation approach
    that helped CNNs and deep learning bring the world out of a long AI winter.^([[9](#_footnotedef_9
    "View footnote.")]) ^([[10](#_footnotedef_10 "View footnote.")]) This approach
    birthed the first practical CNNs for computer vision, time series forecasting
    and NLP.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 但是随着时间的推移，这变得乏味了，我们甚至不再认为手工制作的滤镜在计算机视觉或自然语言处理中很重要。相反，我们使用统计学和神经网络来自动*学习*在图像和文本中寻找的模式。研究人员开始使用线性全连接网络（多层感知器）。但是这些网络存在一个严重的问题，即泛化能力过强，无法识别当单词模式从句子开头移动到句子末尾时。全连接神经网络不具有尺度不变性和平移不变性。但是后来
    David Rumelhart 发明了，Geoffrey Hinton 推广了反向传播方法，帮助 CNN 和深度学习使世界摆脱了长时间的人工智能冬季。[[9]](#_footnotedef_9)
    [[10]](#_footnotedef_10) 这种方法孕育了第一个实用的计算机视觉、时间序列预测和自然语言处理的 CNN。
- en: Figuring out how to combine convolution with neural networks to create CNNs
    was just the boost neural networks needed. CNNs now dominate computer vision.
    And for NLP, CNNs are still the most efficient models for many advanced natural
    language processing problems. For example, spaCy switched to CNNs for version
    2.0\. CNNs work great for *named entity recognition* (NER) and other word tagging
    problems.^([[11](#_footnotedef_11 "View footnote.")]) And CNNs in your brain seem
    to be responsible for your ability to recognize language patterns that are too
    complex for other animals.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 想出如何将卷积与神经网络结合起来创建 CNN 只是神经网络所需要的提升。CNN 现在主导着计算机视觉。而对于自然语言处理，CNN 仍然是许多先进的自然语言处理问题中最有效的模型。例如，spaCy
    切换到 CNNs 版本 2.0。CNNs 对于命名实体识别（NER）和其他单词标记问题非常有效。[[11]](#_footnotedef_11) 而且你的大脑中的
    CNNs 似乎负责识别其他动物无法理解的语言模式。
- en: 7.1.1 Scale and Translation Invariance
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.1 尺度和平移不变性
- en: The main advantage of CNNs over previous NLP algorithms is that they can recognize
    patterns in text no matter where those patterns occur in the text (*translation
    invariance*) and how spread out they are (*scale invariance*). TF-IDF vectors
    don’t have any way of recognizing and generalizing from patterns in your text.
    And fully connected neural networks over-generalize from particular patterns at
    particular locations in the text.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs 相对于以前的 NLP 算法的主要优势是，它们可以识别文本中的模式，无论这些模式在文本中出现在哪里（*平移不变性*）以及它们有多分散（*尺度不变性*）。TF-IDF
    向量没有任何方法可以识别和从文本中的模式进行泛化。而全连接神经网络会从文本中特定位置的特定模式进行过度泛化。
- en: 'As far back as the 1990s famous researchers like Yann LeCun, Yoshua Bengio,
    and Geoffrey Hinton were using convolution for computer vision and OCR (optical
    character recognition).^([[12](#_footnotedef_12 "View footnote.")]) They got this
    idea from our brains. Neural networks are often referred to as "neuromorphic"
    computing because they mimic or simulate what happens in our brains. Neural networks
    simulate in software what brains (networks of biological neurons) do in wetware.
    And because CNNs are based on brains, they can be used for all kinds of "off-label"
    NLP applications: voice, audio, text, weather, and time series. NLP CNNs are useful
    for any series of symbols or numerical vectors (embeddings). This intuition empowers
    you to apply your NLP CNNs to a wide variety of problems that you will run into
    at your job - such as financial time series forecasting and weather forecasting.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 早在 1990 年代，像 Yann LeCun、Yoshua Bengio 和 Geoffrey Hinton 这样的著名研究人员就已经开始将卷积用于计算机视觉和
    OCR（光学字符识别）。[[12]](#_footnotedef_12) 他们从我们的大脑中得到了这个想法。神经网络通常被称为“类脑”计算，因为它们模仿或模拟了我们大脑中发生的事情。神经网络在软件中模拟了大脑（生物神经元网络）在生物硬件中的工作。而且因为
    CNNs 基于大脑，所以它们可以用于各种“非处方”NLP 应用：语音、音频、文本、天气和时间序列。NLP CNNs 对于任何一系列符号或数值向量（嵌入）都很有用。这种直觉使你能够将你的
    NLP CNNs 应用于各种各样的问题，你在工作中会遇到，比如金融时间序列预测和天气预测。
- en: The scale invariance of convolution means you can understand others even if
    they stretch out the patterns in their words over a long time by speaking slowly
    or adding a lot of filler words. And translation invariance means you can understand
    peoples' intent whether they lead with the good news or the bad news. You’ve probably
    gotten pretty good at handling feedback from your parents, teachers, and bosses
    whether it is authentic constructive criticism or even if the "meat" is hidden
    inside a "praise sandwich."^([[13](#_footnotedef_13 "View footnote.")]) Perhaps
    because of the subtle ways we use language and how import it is in culture and
    memory, convolution is built into our brains. We are the only species to have
    convolution networks built into our brains. And some people have as many as 3
    layers of convolutions happening within the part of the brain that processes voice,
    called "Heschl’s gyrus" (HG).^([[14](#_footnotedef_14 "View footnote.")])
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积的缩放不变性意味着即使别人将他们的单词模式拉长时间通过说话慢或添加大量废话，你仍可以理解他们。翻译的不变性意味着你可以理解人们的意图，无论他们先说好消息还是坏消息。你可能已经很善于处理来自父母、教师和老板的反馈，无论是真正的建设性批评还是即使“肉”隐藏在“表扬三明治”之内。也许是因为我们使用语言的微妙方式以及语言在文化和记忆中的重要性，卷积被建立在我们的大脑中。我们是唯一有卷积网络内置在大脑中的物种。有些人在处理声音的大脑区域——赫氏回旋部（HG）甚至有着高达三层的卷积层发生。^[[14](#_footnotedef_14
    "返回脚注")]
- en: 'You’ll soon see how to incorporate the power of translation and scale invariant
    convolutional filters into your own neural networks. You will use CNNs to classify
    questions and toots (Mastodon ^([[15](#_footnotedef_15 "View footnote.")]) posts)
    and even the beeps and boops of Morse code. Your machine will soon be able to
    tell whether a question is about a person, a thing, a historical date, or a general
    concept. You’ll even try to see if a question classifier can tell if someone is
    asking you out on a date. And you might be surprised to learn that CNNs can detect
    subtle differences between catastrophes you might read about online: catastrophic
    birdsite post vs a real-world disaster.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 你很快就会看到如何将平移和缩放不变的卷积滤波器的威力嵌入你自己的神经网络中。你将使用卷积神经网络对问题和“toots（Mastodon^[[15](#_footnotedef_15
    "返回脚注")]”帖子进行分类，甚至还可以识别莫尔斯电码中的嘟嗒声和哔哔声。你的机器很快就能判断一个问题是有关人、物、历史日期还是一个一般概念。你甚至可以尝试看看问题分类器是否可以判断别人是否在约你出去。你可能会惊讶地发现，CNN可以检测出你在网上阅读到的灾难性帖子之间的微妙差异：灾难性的
    “birdsite” 帖子与现实中的灾难之间的差异。
- en: 7.2 Convolution
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 卷积
- en: The concept of *convolution* is not as complicated as it sounds. The math is
    almost the same as for calculating the correlation coefficient. Correlation helps
    you measure the covariance or similarity between a pattern and a signal. In fact,
    its purpose is the same as for correlation - pattern recognition. Correlation
    allows you to detect the similarity between a series of numbers and some other
    series of numbers representing the pattern you’re looking to match.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*卷积*这个概念并不像听起来那么复杂。它的数学公式几乎和计算相关系数一样简单。相关系数帮助你测量模式和信号之间的协方差或相似性。事实上，它的目的和相关系数相同——模式识别。相关系数可以帮助你检测一系列数字和另一系列数字之间的相似性，这些数字代表你要匹配的模式。'
- en: 7.2.1 Stencils for natural language text
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.1 处理自然语言文本的模板
- en: Have you ever seen a lettering stencil? A lettering stencil is a piece of cardboard
    or plastic with the outline of printed letters cut out. When you want to paint
    words onto something, such as a storefront sign, or window display, you can use
    a stencil to make your sign come out looking just like printed text. You use a
    stencil like a movable masking tape to keep you from painting in the wrong places.
    But in this example, you’re going to use the stencil in reverse. Instead of painting
    words with your stencil, you’re going to detect patterns of letters and words
    with a stencil. Your NLP stencil is an array of weights (floating point numbers)
    called a *filter* or *kernel*.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你见过字母模板吗？字母模板是一个有印刷字母轮廓的纸板或塑料片。当你想给某物（例如店铺标志或橱窗展示）上字时，你可以使用模板，使你的标志看起来像印刷文字一样。你可以像使用可移动遮蔽胶带一样使用模板，以防止你涂绘到错误的位置。但在这个例子中，你要反向使用模板。而不是用模板画字，你要使用模板检测字母和单词的模式。你的
    NLP 模板是一个带权重（浮点数）的数组，称为*滤波器*或*内核*。
- en: So imagine you create a lettering stencil for the nine letters (and one *space*
    character) in the text "are sacred". And imagine it was exactly the size and shape
    of the text in this book that you are reading right now.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，想象一下，你为文本中的九个字母（以及一个*空格*字符）创建了一份字母模板"are sacred"。想象一下，它恰好是你正在阅读的书中文本的大小和形状。
- en: Figure 7.1 A real-life stencil
  id: totrans-35
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.1 一个真实的模板
- en: '![cnn stencil sliding over phrase words are sacred drawio](images/cnn-stencil-sliding-over-phrase-words-are-sacred_drawio.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![cnn stencil sliding over phrase words are sacred drawio](images/cnn-stencil-sliding-over-phrase-words-are-sacred_drawio.png)'
- en: Now, in your mind, set the stencil down on top of the book so that it covers
    the page and you can only see the words that "fit" into the stencil cutout. You
    have to slide that stencil across the page until the stencil lines up with this
    pair of words in the book. At that point, you’d be able to see the words spelled
    out clearly through the stencil or mask. The black lettering of the text would
    fill the holes in the stencil. And the amount of black that you see is a measure
    of how good the match is. If you used a white stencil, the words "are sacred"
    would shine through and would be the only words you could see.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在你的脑海中，将模板放在书的顶部，以覆盖页面，你只能看到符合模板切口的单词。你需要将该模板滑过页面，直到该模板与书中的这对单词对齐。在那时，你将能够通过模板或掩膜清晰地看到单词的拼写。文本的黑色字母会填补模板的空洞。而你看到的黑色数量是匹配程度的度量。如果你使用了白色模板，单词"are
    sacred"将闪耀出来，这将是你唯一能看到的单词。
- en: If you used a stencil this way, sliding it across the text to find the maximum
    match between your pattern and a piece of text, you’d be doing *convolution* with
    a stencil! When talking about deep learning and CNNs the stencil is called a *kernel*
    or *filter*. In CNNs, a *kernel* is an array of floating point numbers rather
    than a cardboard cutout. And the kernel is designed to match a general pattern
    in the text. Your text has also been converted to an array of numerical values.
    Convolution is the process of sliding that kernel across your numerical representation
    of text to see what pops out.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你这样使用模板，将其滑动到文本中，以找到模式和文本之间的最大匹配，你就在使用模板进行*卷积*！当谈论深度学习和CNN时，模板被称为*卷积核*或*过滤器*。在CNN中，*卷积核*是浮点数数组而不是纸板剪影。卷积核被设计成匹配文本中的一般模式。你的文本也被转换成数字值的数组。卷积是将卷积核滑动过你的文本数字表示，以查看其中的内容。
- en: Just a decade or so ago, before CNNs, you would have had to hand-craft your
    kernels to match whatever patterns you could dream up. But with CNNs you don’t
    have to program the kernels at all, except to decide how wide the kernels are
    - how many letters or words you think will capture the patterns you need. Your
    CNN optimizer will fill in the weights within your kernel. As you train a model,
    the optimizer will find the best array of weights that matches the patterns that
    are most predictive of the target variable in your NLP problem. The backpropagation
    algorithm will incrementally adjust the weights bit by bit until they match the
    right patterns for your data.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 十年前，在有了CNN之前，你不得不手工制作适合你想象的任何模式的卷积核。但是使用CNN时，除了决定卷积核的宽度 - 你认为需要多少个字母或单词来捕捉你需要的模式，你不需要编程卷积核。你的CNN优化器将填充卷积核中的权重。当你训练模型时，优化器会找到最能预测NLP问题目标变量的模式所匹配的最佳权重数组。反向传播算法会逐步调整权重，直到它们与你的数据的正确模式匹配。
- en: You need to add a few more steps to your mental model of stencils and kernels
    to give you a complete understanding of how CNNs work. A CNN needs to do 3 things
    with a kernel (stencil) to incorporate it into a natural language processing pipeline.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对CNN的工作原理有一个完整的理解，你需要在脑海中增加一些与模板和卷积核相关的步骤，将其融合到一个自然语言处理流程中。CNN需要执行三项任务来使用卷积核（模板）。
- en: Measure the amount of match or similarity between the kernel and the text
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测量卷积核和文本之间的匹配或相似度
- en: Find the maximum value of the kernel match as it slides across some text
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在文本中滑动卷积核寻找最大匹配值
- en: Convert the maximum value to a binary value or probability using an activation
    function
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用激活函数将最大值转换为二进制值或概率。
- en: You can think of the amount of blackness that pops through your stencil as a
    measure of the amount of match between your stencil and the text. So step 1 for
    a CNN, is to multiply the weights in your kernel by the numerical values for a
    piece of text and to add up all those products to create a total match score.
    This is just the dot product or correlation between the kernel and that particular
    window of text.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将印刷版的黑暗程度视为印版和文本之间匹配程度的一种度量。因此，卷积神经网络（CNN）的第一步是将核函数中的权重乘以文本中的数值，然后将所有乘积相加，得到总的匹配分数。这仅仅是核函数与该文本窗口之间的点积或相关性。
- en: Step 2 is to slide your window across the text and do the dot product of step
    1 again. This kernel window sliding, multiplying, and summing is called convolution.
    Convolutions turn one sequence of numbers into another sequence of numbers that’s
    about the same size as the original text sequence. Depending on the details of
    how you do this sliding and multiplying (convolution) you can end up with a slightly
    shorter or longer sequence of numbers. But either way, the convolution operation
    outputs a sequence of numerical values, one for every possible position of the
    kernel in your text.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步是在文本上滑动窗口，并再次进行步骤1的点积。这个卷积窗口滑动、乘法和求和被称为卷积。卷积将一个数字序列转换为与原始文本序列大小相同的另一个数字序列。根据滑动和乘法（卷积）的细节，您可能得到一个稍微较短或较长的数字序列。但无论如何，卷积操作输出一个数字序列，其中每个可能的核函数位置都有一个数值。
- en: Step 3 is to decide whether the text contains a good match somewhere within
    it. For this, your CNN converts the sequence of values output by convolution into
    a single value. The result is a single value representing the likelihood that
    the kernel’s pattern was somewhere in the text. Most CNNs are designed to take
    the maximum value of this sequence of numbers as a measure of a match. This approach
    is called *max pooling* because it collects or pools all of the values from the
    convolution into a single maximum value.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 第三步是判断文本中是否存在一个良好的匹配。为此，你的CNN将卷积输出的一系列值转换为一个单一的值。结果是一个表示核函数模式可能在文本中某处的概率的单一值。大多数CNN设计成将这一系列数值的最大值作为匹配的度量。这种方法被称为“最大池化”，因为它将卷积中的所有值集中到一个最大值中。
- en: Note
  id: totrans-47
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: If the patterns that you are looking for are spread out over multiple different
    locations within a passage of text, then you may want to try *mean pooling* for
    some of your kernels.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你要寻找的模式在文本的不同位置上分布开来，那么你可能想尝试一些“均值池化”来处理一些核函数。
- en: You can see how convolution enables your CNN to extract patterns that depend
    on the order of words. And this allows CNN kernels to recognize subtleties in
    the meaning of natural language text that are lost if you only use BOW (bag-of-words)
    representations of text.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，卷积使得你的CNN能够提取依赖于单词顺序的模式。这使得CNN的核函数能够识别自然语言文本意义上的微妙差别，而这些差别如果你只使用词袋（BOW）表示法的话就会丢失。
- en: Words are sacred. If you get the right ones in the right order you can nudge
    the world a little.
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 单词是神圣的。如果你以正确顺序使用正确的单词，你就能微调世界一点点。
- en: — Tom Stoppard
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: —— 汤姆·斯托帕德
- en: The Real Thing
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 真实的事物
- en: In the first few chapters, you treated words as sacred by learning how best
    to tokenize text into words and then compute vector representations of individual
    words. Now you can combine that skill with convolution to give you the power to
    "nudge the world a little" with your next chatbot on Mastodon.^([[16](#_footnotedef_16
    "View footnote.")])
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，你通过学习如何最好地将文本分词为单词，并计算每个单词的向量表示来将单词视为神圣的。现在，你可以将这个技巧与卷积相结合，以便通过你的下一个Mastodon聊天机器人“微调世界”。^([[16](#_footnotedef_16
    "View footnote.")])
- en: 7.2.2 A bit more stenciling
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.2 再多一点铅字
- en: Remember the lettering stencil analogy? Reverse lettering stencils would not
    be all that useful for NLP because cardboard cutouts can only match the "shape"
    of words. You want to match the meaning and grammar of how words are used in a
    sentence. So how can you upgrade your reverse stencil concept to make it more
    like what you need for NLP? Suppose you want your stencil to detect `(adjective,
    noun)` 2-grams, such as "right word" and "right order" in the quote by Tom Stoppard.
    Here’s how you can label the words in a portion of the quote with their parts
    of speech.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 还记得字母模板的类比吗？反向字母模板对 NLP 来说并不是那么有用，因为硬纸板切割只能匹配单词的“形状”。您想要匹配单词在句子中的使用方式的含义和语法。那么你如何升级你的反向模板概念，使其更像你需要的
    NLP？假设你想要你的模板检测`(形容词，名词)` 2-gram，例如 "right word" 和 "right order" 在汤姆·斯托帕德的引语中。以下是您如何用词性标记部分引用中的单词的方法。
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Just as you learned in Chapter 6, you want to create a vector representation
    of each word so that the text can be converted to numbers for use in the CNN.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 就像你在第 6 章中学到的一样，你希望为每个单词创建一个向量表示，以便文本可以转换为数字，用于 CNN 中。
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now your stencil or kernel will have to be expanded a bit to span two of the
    7-D one-hot vectors. You will create imaginary cutouts for the 1’s in the one-hot
    encoded vectors so that the pattern of holes matches up with the sequence of parts
    of speech you want to match. Your adjective-noun stencil has holes in the first
    row and the first column for the adjective at the beginning of a 2-gram. You will
    need a hole in the second row and the fifth column for the noun as the second
    word in the 2-gram. As you slide your imaginary stencil over each pair of words
    it will output a boolean `True` or `False` depending on whether the stencil matches
    the text or not.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你的模板或内核将必须扩展一点以跨越两个 7-D 单热矢量。你将为单热编码向量中的 1 创建想象中的切割，以使孔的模式与您想要匹配的词性序列相匹配。你的形容词-名词模板在第一行和第一列中有形容词在
    2-gram 开头的孔。你需要在第二行和第五列中为名词作为 2-gram 中的第二个单词的孔。当你将你的想象模板滑动到每一对词时，它将根据模板是否匹配文本输出布尔值`True`或`False`。
- en: 'The first pair of words will create a match:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 第一对单词将创建一个匹配：
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Moving the stencil to cover the second 2-gram, it will output False because
    the two gram starts with a noun and ends with a fails to beep
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 将模板移动以覆盖第二个 2 克拉姆，它将输出 False，因为两个克拉姆以名词开头，以失败的方式结束
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Continuing with the remaining words we end up with this 9-element map for the
    10-word phrase.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 继续使用剩余的单词，我们最终得到了这个 10 个词短语的 9 元素图。
- en: '| **Span** | **Pair** | **Is match?** |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| **跨度** | **对** | **匹配？** |'
- en: '| 0, 1 | (right, ones) | **True** (1) |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 0, 1 | (正确的，那些) | **True** (1) |'
- en: '| 1, 2 | (ones, in) | False (0) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 1, 2 | (那些，在) | False (0) |'
- en: '| 2, 3 | (in, the) | False (0) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 2, 3 | (在，那个) | False (0) |'
- en: '| 3, 4 | (the, right) | False (0) |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 3, 4 | (正确的，右) | False (0) |'
- en: '| 4, 5 | (right, order) | **True** (1) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 4, 5 | (正确的，秩序) | **True** (1) |'
- en: '| 5, 6 | (order, you) | False (0) |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 5, 6 | (秩序，你) | False (0) |'
- en: '| 6, 7 | (you, can) | False (0) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 6, 7 | (你，可以) | False (0) |'
- en: '| 7, 8 | (can, nudge) | False (0) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 7, 8 | (可以，推动) | False (0) |'
- en: '| 8, 9 | (nudge, the) | False (0) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 8, 9 | (推动，那个) | False (0) |'
- en: '| 9, 10 | (the, world) | False (0) |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 9, 10 | (这个，世界) | False (0) |'
- en: Congratulations. What you just did was convolution. You transformed smaller
    chunks of an input text, in this case 2-grams, to reveal where there was a match
    for the pattern you were looking for. It’s usually helpful to add padding to your
    token sequences. And to clip your text at a maximum length. This ensures that
    your output sequence is always the same length, no matter how long your text is
    your kernel.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜。你刚刚做的是卷积。你将输入文本的较小块，本例中为 2 克拉姆，转换为显示你正在寻找的模式的匹配位置。将填充添加到您的令牌序列通常是有帮助的。并将您的文本剪切到最大长度。这样可以确保您的输出序列始终具有相同的长度，无论您的文本有多长您的内核。
- en: '*Convolution*, then, is'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*卷积*，然后是'
- en: a transformation…
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种转换…
- en: of input that may have been padded…
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能已被填充的输入…
- en: to produce a map…
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成地图…
- en: of where in the input certain conditions existed (e.g. two consecutive adverbs)
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其中某些条件存在的输入的位置（例如，两个连续的副词）
- en: 'Later in the chapter, you will use the terms *kernel* and *stride* to talk
    about your stencil and how you slide it across the text. In this case, your *stride*
    was one and the kernel size was two. And for the part-of-speech vectors, your
    kernel was designed to handle 7-D embedding vectors. Had you used the same kernel
    size of two but stepped it across the text with a stride of two, then you would
    get the following output:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 章节后面，你将使用术语*核*和*步幅*来讨论你的模板以及如何将其滑动到文本上。在这种情况下，你的*步幅*为一，核大小为二。而对于词性向量，你的核被设计为处理7维嵌入向量。如果你使用相同大小的核，但将其以步幅为二滑动到文本上，那么你会得到以下输出：
- en: '| **Span** | **Pair** | **Is match?** |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| **跨度** | **配对** | **匹配？** |'
- en: '| 0, 1 | (right, ones) | **True** (1) |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 0, 1 | (right, ones) | **True** (1) |'
- en: '| 2, 3 | (in, the) | False (0) |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 2, 3 | (in, the) | False (0) |'
- en: '| 4, 5 | (right, order) | **True** (1) |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 4, 5 | (right, order) | **True** (1) |'
- en: '| 6, 7 | (you, can) | False (0) |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 6, 7 | (you, can) | False (0) |'
- en: '| 8, 9 | (nudge, the) | False (0) |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 8, 9 | (nudge, the) | False (0) |'
- en: In this case, you got lucky with your stride because the two adjective-noun
    pairs were an even number of words apart. So your kernel successfully detected
    both matches for your pattern. But you would only get luck 50% of the time with
    this configuration. So it is much more common to have a stride of one and kernel
    sizes of two or more.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，你的步幅运气很好，因为两个形容词-名词对之间的词数是偶数。所以你的核成功地检测到了模式的两个匹配项。但是在这种配置下，你只有50%的几率会如此幸运。因此，使用步幅为一和核大小为二或更大的情况更为常见。
- en: 7.2.3 Correlation vs. convolution
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.3 相关性与卷积
- en: In case you’ve forgotten, listing 7.1 should remind you what correlation looks
    like in Python. (You can also use `scipy.stats.pearsonr`).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你忘记了，清单7.1应该会提醒你Python中相关性是什么样子的。（你也可以使用`scipy.stats.pearsonr`）。
- en: Listing 7.1 Python implementation of correlation
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单7.1 相关性的Python实现
- en: '[PRE6]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: However, correlation only works when the series are the same length. And you
    definitely want to create some math that can work with patterns that are shorter
    than the sequence of numbers representing your text. That’s how mathematicians
    came up with the concept of convolution. They split the longer sequence into smaller
    ones that are the same length as the shorter one and then apply the correlation
    function to each of these pairs of sequences. That way convolution can work for
    any 2 sequences of numbers no matter how long or short they are. So in NLP, we
    can make our pattern (called a *kernel*) as short as we need to. And the series
    of tokens (text) can be as long as you like. You compute correlation over a sliding
    window of text to create a sequence of correlation coefficients that represent
    the meaning of the text.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，相关性只在系列长度相同时才有效。而且你肯定希望创建一些能够处理比表示文本的数字序列更短的模式的数学内容。这就是数学家提出卷积概念的方式。他们将较长的序列分成与较短序列相同长度的较小序列，然后对这些序列对的每一个应用相关函数。这样，卷积可以处理任何两个序列的数字，无论它们的长度有多长或多短。所以在自然语言处理中，我们可以将我们的模式（称为*核*）设计得尽可能短。而标记（文本）的序列可以任意长。你可以在文本的滑动窗口上计算相关性，从而创建代表文本含义的相关系数序列。
- en: 7.2.4 Convolution as a mapping function
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.4 卷积作为映射函数
- en: CNNs (in our brains and in machines) are the "mapping" in a map-reduce algorithm.
    It outputs a new sequence that is shorter than the original sequence, but not
    short enough. That will come later with the *reduce* part of the pipeline. Pay
    attention to the size of the outputs of each convolutional layer.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs（无论是在我们的大脑中还是在机器中）是map-reduce算法中的“映射”部分。它输出一个比原始序列短的新序列，但还不够短。这将在流水线的*减少*部分后面进行。注意每个卷积层的输出大小。
- en: The math of convolution allows you to detect patterns in text no matter where
    (or when) they occur in that text. We call an NLP algorithm "time-invariant" if
    it produces feature vectors that are the same no matter where (when) a particular
    pattern of words occurs. Convolution is a time-invariant operation, so it’s perfect
    for text classification and sentiment analysis and NLU. Time invariance is a big
    advantage of convolution over other approaches you’ve used so far. Your CNN output
    vector gives you a consistent representation of the thought expressed by a piece
    of text no matter where in the text that thought is expressed. Unlike word embedding
    representations, convolution will pay attention to the meaning of the order of
    the vectors and won’t smush them all together into a pointless average.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积的数学运算可以让你在文本中无论何处（或何时）都能检测到模式。如果一个自然语言处理算法生成的特征向量无论一个特定词语模式出现在何处（何时）都相同，我们称之为“时不变”。卷积是一个时不变的操作，因此非常适用于文本分类、情感分析和自然语言理解。与你目前使用的其他方法相比，时不变性是卷积的一个巨大优势。你的
    CNN 输出向量为你提供了一致的表达方式，表达了文本中的思想，无论该思想在文本中的哪个位置表达出来。与单词嵌入表示不同，卷积将注意力集中在向量的顺序意义上，并不会将它们全部混合成毫无意义的平均值。
- en: 'Another advantage of convolution is that it outputs a vector representation
    of your text that is the same size no matter how long your text is. Whether your
    text is a one-word name or a ten-thousand-word document, a convolution across
    that sequence of tokens would output the same size vector to represent the meaning
    of that text. Convolution creates embedding vectors that you can use to make all
    sorts of predictions, just like you did with word embeddings in Chapter 6\. But
    now these embeddings will work on sequences of words, not just individual words.
    Your embedding, your vector representation of meaning, will be the same size no
    matter whether the text you’re processing is the three words "I love you" or much
    longer: "I feel profound and compersive love for you." The feeling or sentiment
    of love will end up in the same place in both vectors despite the word love occurring
    at different locations in the text. And the meaning of the text is spread over
    the entire vector creating what is called a "dense" vector representation of meaning.
    When you use convolution, there are no gaps in your vector representation for
    text. Unlike the sparse TF-IDF vectors of earlier chapters, the dimensions of
    your convolution output vectors are all packed meaning for every single bit of
    text you process.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积的另一个优势是，它输出的文本向量表示大小始终相同，无论你的文本有多长。无论你的文本是一个词名还是一份长达一万字的文档，对该序列的卷积都会输出相同大小的向量来表示该文本的含义。卷积创建的嵌入向量可用于做各种预测，就像你在第六章中使用单词嵌入所做的一样。但现在，这些嵌入将作用于单词序列，而不仅仅是单个单词。你的嵌入，你的含义向量表示，无论你处理的文本是三个词“我爱你”还是更长的文本：“我对你感到深深的爱和欣慰。”爱的感觉或情感会在两个向量中相同的位置结束，尽管单词“爱”出现在文本的不同位置。文本的含义分布在整个向量上，形成所谓的“密集”向量表示。当你使用卷积时，文本向量表示中没有间隙。与之前章节中稀疏的
    TF-IDF 向量不同，你的卷积输出向量的维度都是填充的，对你处理的每一小段文本都有意义。
- en: 7.2.5 Python convolution example
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.5 Python 卷积示例
- en: You’re going to start with a pure Python implementation of convolution. This
    will give you a mental model of the math for convolution, and most importantly,
    of the shapes of the matrices and vectors for convolution. And it will help you
    appreciate the purpose of each layer in a convolutional neural network. For this
    first convolution, you will hard-code the weights in the convolution kernel to
    compute a 2-point moving average. This might be useful if you want to extract
    some machine learning features from daily cryptocurrency prices in Robinhood.
    Or perhaps it would be better to imagine you trying to solve a solvable problem
    like doing feature engineering of some 2-point averages on the reports of rainfall
    for a rainy city like Portland, Oregon. Or even better yet, imagine you are trying
    to build a detector that detects a dip in the part-of-speech tag for an adverb
    in natural language text. Because this is a hard-coded kernel, you won’t have
    to worry about training or fitting your convolution to data just yet.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 您将从一个纯 Python 实现的卷积开始。这将为您提供卷积的数学模型，更重要的是，为卷积的矩阵和向量形状提供心理模型。这将帮助您理解卷积神经网络中每一层的目的。对于这第一个卷积，您将在卷积核中硬编码权重以计算
    2 点移动平均值。如果您想要从 Robinhood 的日常加密货币价格中提取一些机器学习特征，这可能很有用。或者也许更好的想象一下，您正在尝试解决一个可解决的问题，比如对像波特兰（俄勒冈州）这样多雨城市的降雨报告进行一些
    2 点平均值的特征工程。或者更好的是，想象您正在尝试构建一个检测自然语言文本中副词部分的词性标签下降的检测器。因为这是一个硬编码的核，所以您现在不必担心训练或拟合您的卷积数据。
- en: You are going to hard-code this convolution to detect a pattern in a sequence
    of numbers just like you hard-coded a regular expression to recognize tokens in
    a sequence of characters in Chapter 2\. When you hard-code a convolutional filter,
    you have to know what patterns you’re looking for so you can put that pattern
    into the coefficients of your convolution. This works well for easy-to-spot patterns
    like dips in a value or brief spikes upward in a value. These are the kinds of
    patterns you’ll be looking for in Morse code "text" later in this chapter. In
    section 3 of this chapter, you will learn how to build on this skill to create
    a convolutional neural network in PyTorch that can *learn* on its own which patterns
    to look for in your text.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 您将硬编码此卷积以检测数字序列中的模式，就像您在第二章中硬编码正则表达式来识别字符序列中的标记一样。当您硬编码卷积滤波器时，您必须知道您要寻找的模式，以便将该模式放入您的卷积系数中。这对于易于识别的模式非常有效，比如数值下降或数值短暂上升。这些是本章后面将要寻找的摩尔斯电码“文本”的模式。在本章的第三节中，您将学习如何利用这一技能在
    PyTorch 中构建一个卷积神经网络，该网络可以自行学习在您的文本中寻找哪些模式。
- en: In computer vision and image processing you would need to use a 2-D convolutional
    filter so you can detect both vertical and horizontal patterns, and everything
    in-between. For natural language processing, you only need 1-dimensional convolutional
    filters. You’re only doing convolution in one dimension, the time dimension, the
    position in your sequence of tokens. You can store the components of your embedding
    vectors, or perhaps other parts of speech, in `channels` of a convolution. More
    on that later, once you’re done with the pure Python convolution. Here’s the Python
    for perhaps the simplest possible useful 1-D convolution.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉和图像处理中，您需要使用 2-D 卷积滤波器，这样您就可以检测垂直和水平模式，以及中间的所有内容。对于自然语言处理，您只需要 1 维卷积滤波器。您只需在一个维度上进行卷积，即时间维度，在您的标记序列中的位置。您可以将嵌入向量的组件，或者也许是其他词性，存储在卷积的`通道`中。稍后会详细讨论这一点，等您完成纯
    Python 卷积。以下是也许是最简单但有用的 1-D 卷积的 Python 代码。
- en: Listing 7.4 shows you how to create a 1-D convolution in pure Python for a hard-coded
    kernel (`[.5, .5]`) with only two weights of `.5` in it.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.4 显示了如何在纯 Python 中创建一个 1-D 卷积，用于一个硬编码的核（`[.5, .5]`），其中只有两个权重为 `.5` 的权重。
- en: This kernel is computing the rolling or moving average of two numbers in a sequence
    of numbers. For natural language processing, the numbers in the input sequence
    represent the occurrence (presence or absence) of a token in your vocabulary.
    And your token can be anything, like the part-of-speech tag that we used to mark
    the presence or absence (occurrence) of adverbs in listing. Or the input could
    be the fluctuating numerical values of a dimension in your word embeddings for
    each token.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这个核正在计算数字序列中两个数字的移动平均值。对于自然语言处理，输入序列中的数字表示词汇表中标记的出现（存在或不存在）。而且您的标记可以是任何东西，例如我们在示例中用于标记副词出现（存在性）的词性标签。或者输入可以是每个标记中词嵌入维度的波动数值。
- en: This moving average filter can detect the occurrence of two things in a row
    because `(.5 * 1 + .5 * 1)` is `1`. A `1` is how your code tells you it has found
    something. Convolution is great at detecting *patterns* like this that other NLP
    algorithms would miss. Rather than looking for two occurrences of a word, you
    are going to look for two aspects of meaning in a row. And you’ve just learned
    all about the different aspects of meaning in the last chapter, the dimensions
    of word vectors. For now, you’re just looking for a single aspect of words, their
    part of speech. You are looking for one particular part of speech, adverbs. You’re
    looking for two adverbs in a row.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这个移动平均滤波器可以检测到连续出现两个事物的情况，因为 `(.5 * 1 + .5 * 1)` 是 `1`。代码会以数字 `1` 来告诉您它找到了某些东西。卷积对于像这样的其他自然语言处理算法可能会错过的*模式*非常擅长。与寻找两个词的两个实例不同，您将寻找连续出现的两个意思。而且您刚刚在上一章中了解了不同的意思方面，即单词向量的维度。现在，您只寻找单词的一个方面，即它们的词性。你要找的是两个连续的副词。
- en: The right word may be effective, but no word was ever as effective as a rightly
    timed pause.
  id: totrans-108
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 合适的词可能具有影响力，但没有一个词能像合适的暂停一样有效。
- en: — Mark Twain
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: — 马克·吐温
- en: Can you spot the two adverbs in a row? I had to cheat and use SpaCy to find
    this example. Subtle patterns of meaning like this are very hard for a human to
    consciously notice. But measuring the *adverbiness* of text is just a matter of
    math for a convolutional filter. And convolution will work in parallel for all
    the other aspects of meaning that you might be looking for. In fact, once you’re
    done with this first example, you will run convolution on *all* of the dimensions
    of words. Convolution works best when you use the word embeddings from the previous
    chapter that keep track of all the dimensions of words in vectors.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 你能找出两个连续出现的副词吗？我不得不借助 SpaCy 来找到这个例子。类似这样的微妙意义模式对于人类来说很难有意识地注意到。但是对于卷积滤波器来说，测量文本的**副词特性**只是一门数学问题。卷积将并行处理您可能正在寻找的所有其他意义方面。实际上，一旦您完成了第一个例子，您将对单词的*所有*方面运行卷积。当您使用前一章节中跟踪单词所有维度的词嵌入时，卷积效果最佳。
- en: Not only will convolution look at all the dimensions of meaning in words but
    also all the *patterns* of meaning in all those dimensions of words. A convolutional
    neural network (CNN) looks at your desired output (target variable) to find all
    the patterns in all dimensions of word embeddings that influence your target variable.
    For this example, you’re defining an "adverby" sentence as one that contains two
    adverbs consecutively within a sentence. This is just to help you see the math
    for a very simple problem. Adverbiness is just one of many features you need to
    engineer from text in machine learning pipelines. A CNN will automate that engineering
    for you by learning just the right combination of adverbiness, nounness, stopwordness,
    and lots of other "nesses". For now, you’ll just do it all by hand for this one
    adverbiness feature. The goal is to understand the kinds of patterns a CNN can
    learn to recognize in your data.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积将查看单词意思的所有维度以及所有维度的单词意义的*模式*。卷积神经网络（CNN）会查看您的目标输出（目标变量），以查找影响目标变量的单词嵌入的所有维度中的所有模式。对于这个例子，您将定义一个“副词句”为在句子中连续包含两个副词的句子。这只是为了帮助您看到一个非常简单的问题的数学计算。副词特性只是您需要在机器学习流程中从文本中提取的众多特征之一。CNN将通过学习适当的副词特性、名词特性、停词特性和其他很多“特性”的组合来自动完成这种工程。现在，您只需手动完成这一个副词特性。目标是了解
    CNN 可以学习识别数据中的哪些模式。
- en: Listing 7.2 shows how to tag the quote with parts of speech tags using SpaCy
    and then create a binary series to represent the one aspect of the words you are
    searching for, adverbiness.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2展示了如何使用SpaCy对引用进行词性标注，然后创建一个二进制系列来表示你正在搜索的单词的一个方面，即副词性。
- en: Listing 7.2 Tag a quote with parts of speech
  id: totrans-113
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.2 用词性标记引用
- en: '[PRE7]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now you have your sequence of `ADV` ones and zeros so you can process it with
    convolution to match the pattern you’re looking for.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你有了你的一串`ADV`的零和一，所以你可以用卷积来处理它，以匹配你正在寻找的模式。
- en: Listing 7.3 Define your input sequence for convolution
  id: totrans-117
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.3 为卷积定义输入序列
- en: '[PRE9]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Wow, this cheating worked too well! We can clearly see there are two adverbs
    in a row somewhere in the sentence. Let’s use our convolution filter to find where
    exactly.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 哇，这种作弊效果太好了！我们清楚地看到在句子中有两个副词是连续的。让我们使用我们的卷积滤波器来找出确切的位置。
- en: Listing 7.4 Convolution in pure Python
  id: totrans-121
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.4 纯Python中的卷积
- en: '[PRE11]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You can see now why you had to stop the `for` loop 1 short of the end of the
    input sequence. Otherwise, our kernel with 2 weights in it would have overflowed
    off the end of the input sequence. You may have seen this kind of software pattern
    called "map-reduce" elsewhere. And you can see how you might use the Python built-in
    functions `map()` and `filter()` to implement the code in listing 7.4.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以明白为什么你必须在输入序列的末尾停止`for`循环了。否则，我们的内核中的2个权重将会溢出到输入序列的末尾。你可能在其他地方见过这种软件模式称为“map-reduce”。你可以看到如何使用Python内置函数`map()`和`filter()`来实现列表7.4中的代码。
- en: You can create a moving average convolution that computes the adverbiness of
    a text according to our 2-consecutive-adverb definition if you use the sum function
    as your *pooling* function. If you want it to compute an unweighted moving average
    you then just have to make sure your kernel values are all `1 / len(kernel)` so
    that they sum to 1 and are all equal.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你把和函数作为你的*池化*函数，你可以创建一个移动平均卷积，根据我们的2个连续副词的定义来计算文本的副词性。如果你想要计算一个无权重的移动平均，你只需要确保你的内核值都是`1
    / len(kernel)`，这样它们就会加起来为1，并且都是相等的。
- en: Listing 7.5 will create a line plot to help you visualize the convolution output
    and the original `is_adv` input on top of each other.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 将创建一条线图，帮助你可视化卷积输出和原始的`is_adv`输入重叠在一起。
- en: Listing 7.5 Line plot of input (is_adv) and output (adverbiness)
  id: totrans-127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.5 输入（is_adv）和输出（副词性）的折线图
- en: '[PRE13]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Did you notice how the output sequence for this convolution by a size 2 kernel
    produced output that was one shorter than the input sequence? Figure 7.2 shows
    a line plot of the input and output of this moving average convolution. When you
    multiply two numbers by `.5` and add them together, you get the average of those
    two numbers. So this particular kernel (`[.5, .5]`) is a very small (two-sample)
    moving average filter.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 你有没有注意到这个大小为2的内核的卷积的输出序列产生的输出比输入序列短一个？图7.2显示了这个移动平均卷积的输入和输出的线图。当你把两个数字分别乘以`.5`然后相加时，你得到这两个数字的平均值。所以这个特定的内核（`[.5,
    .5]`）是一个非常小的（两个样本）移动平均滤波器。
- en: Figure 7.2 Line plot of `is_adv` and `adverbiness` convolution
  id: totrans-130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.2 `is_adv`和`副词性`卷积的线图
- en: '![square wave pure python](images/square-wave-pure-python.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![纯Python的方波](images/square-wave-pure-python.png)'
- en: Looking at Figure 7.2 you might notice that it looks a bit like the moving average
    or smoothing filters for financial time series data or daily rainfall values.
    For a 7-day moving average of your GreenPill token prices, you would use a size
    7 convolution kernel with values of one-seventh (`0.142`) for each day of the
    week.^([[17](#_footnotedef_17 "View footnote.")]) A size 7 moving average convolution
    would just smooth your spikes in adverbiness even more, creating a much more curved
    signal in your line plots. But you’d never achieve a 1.0 adverbiness score on
    any organic quotes unless you carefully crafted a statement yourself that contained
    seven adverbs in a row.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 看着图7.2，你可能会注意到它看起来有点像金融时间序列数据或每日降雨量值的移动平均或平滑滤波器。对于你的GreenPill令牌价格的7天移动平均，你将使用一个大小为7的卷积内核，每周的每一天都为`0.142`。一个大小为7的移动平均卷积将会更加平滑你副词性中的尖峰，从而在你的线图中创建一个更加曲线的信号。但是除非你精心制作了一个包含七个连续副词的声明，否则你永远不会在任何有机引用中获得1.0的副词性分数。
- en: You can generalize your Python script in Listing 7.6 to create a convolution
    function that will work even when the size of the kernel changes. This way you
    can reuse it in later examples.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将列表7.6中的Python脚本泛化，以创建一个卷积函数，即使内核大小发生变化也能正常工作。这样你就可以在以后的例子中重复使用它。
- en: Listing 7.6 Generalized convolution function
  id: totrans-134
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 7.6 通用卷积函数
- en: '[PRE14]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The `convolve()` function you created here sums the input multiplied by the
    kernel weights. You could also use the Python `map()` function to create a convolution.
    And you used the Python `sum()` function to *reduce* the amount of data in your
    output. This combination makes the convolution algorithm a *map reduce* operation
    that you may have heard of in your computer science or data science courses.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 你在这里创建的 `convolve()` 函数将输入乘以核权重相加。你也可以使用 Python 的 `map()` 函数来创建卷积。你使用了 Python
    的 `sum()` 函数来 *减少* 输出中的数据量。这种组合使卷积算法成为一个你在计算机科学或数据科学课程中可能听说过的 *map reduce* 操作。
- en: Important
  id: totrans-137
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 重要
- en: Map-reduce operations such as convolution are highly parallelizable. Each of
    the kernel multiplications by a window of data could be done simultaneously in
    parallel. This parallelizability is what makes convolution such a powerful, efficient,
    and successful way to process natural language data.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 像卷积这样的 map-reduce 操作高度可并行化。数据窗口的每个核乘法可以同时并行进行。这种可并行性是使卷积成为处理自然语言数据的一种强大、高效和成功的方式的原因。
- en: 7.2.6 PyTorch 1-D CNN on 4-D embedding vectors
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.6 PyTorch 1-D CNN 在 4-D 嵌入向量上
- en: You can see how 1-D convolution is used to find simple patterns in a sequence
    of tokens. In previous chapters, you used regular expressions to find patterns
    in a 1-D sequence of characters. But what about more complex patterns in grammar
    that involve multiple different aspects of the meaning of words? For that, you
    will need to use word embeddings (from Chapter 6) combined with a *convolutional
    neural network*. You want to use PyTorch to take care of all the bookkeeping of
    all these linear algebra operations. You’ll keep it simple with this next example
    by using 4-D one-hot encoded vectors for the parts of speech of words. Later you’ll
    learn how to use 300-D GloVE vectors that keep track of the meaning of words in
    addition to their grammatical role.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到 1-D 卷积是如何用于在令牌序列中查找简单模式的。在之前的章节中，你使用正则表达式来查找字符序列中的模式。但是对于涉及单词意义的多个不同方面的语法的更复杂模式呢？为此，你需要使用单词嵌入（来自第
    6 章）结合 *卷积神经网络*。你想要使用 PyTorch 来处理所有这些线性代数操作的簿记。通过使用 4-D 独热编码向量来表示单词的词性，你将在下一个示例中简化它。稍后，你将学习如何使用
    300-D GloVE 向量，这些向量除了保留单词的语法角色外，还跟踪单词的含义。
- en: Because word embeddings or vectors capture all the different components of meaning
    in words, they include parts of speech. Just as in the adverby quote example earlier,
    you will match a grammatical pattern based on the parts of speech of words. But
    this time your words will have a 3-D part-of-speech vector representing the parts
    of speech noun, verb, and adverb. And your new CNN can detect a very specific
    pattern, an adverb followed by a verb then a noun. Your CNN is looking for the
    "rightly timed pause" in the Mark Twain quote. Refer back to Listing 7.2 if you
    need help creating a DataFrame containing the POS tags for the "rightly timed
    pause" quote.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 因为词嵌入或向量捕捉了单词中所有不同的意义组成部分，它们包括了词性。就像之前的广告引用示例一样，你将根据单词的词性匹配一个语法模式。但这次，你的单词将具有表示名词、动词和副词的
    3-D 词性向量。你的新 CNN 可以检测到一个非常特定的模式，即一个副词后跟一个动词，然后是一个名词。你的 CNN 正在寻找马克·吐温引用中的“正确的时机”。如果需要帮助创建一个包含“正确时机”的
    POS 标签的 DataFrame，请参考清单 7.2。
- en: '[PRE15]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Figure 7.3 Sentence tagged with parts of speech
  id: totrans-144
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 7.3 带有词性标记的句子
- en: '![conv1d pos rightly timed pause df](images/conv1d-pos-rightly-timed-pause_df.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![conv1d pos rightly timed pause df](images/conv1d-pos-rightly-timed-pause_df.png)'
- en: To keep things efficient, PyTorch does not accept arbitrary Pandas or numpy
    objects. Instead, you must convert all input data to `torch.Tensor` containers
    with `torch.float` or `torch.int` data type (`dtype`) objects inside.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持高效，PyTorch 不接受任意的 Pandas 或 numpy 对象。相反，你必须将所有输入数据转换为具有 `torch.float` 或 `torch.int`
    数据类型（`dtype`）对象的 `torch.Tensor` 容器。
- en: Listing 7.7 Convert a DataFrame to a tensor with the correct size
  id: totrans-147
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 7.7 将 DataFrame 转换为正确大小的张量
- en: '[PRE17]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now you construct that pattern that we want to search for in the text: adverb,
    verb, then noun. You will need to create a separate filter or kernel for each
    part of speech that you care about. Each kernel will be lined up with the others
    to find the pattern you’re looking for in all aspects of the meaning of the words
    simultaneously.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你构建了我们想在文本中搜索的模式：副词、动词，然后名词。你需要为你关心的每个词性创建一个单独的过滤器或核。每个核将与其他核对齐，以同时在单词意义的所有方面找到你正在寻找的模式。
- en: Before you had only one dimension to worry about, the adverb tag. Now you’ll
    need to work with all 4 dimensions of these word vectors to get the pattern right.
    And you need to coordinate four different "features" or channels of data. So for
    a 3-word, 4-channel kernel we need a 4x3 matrix. Each row represents a channel
    (part-of-speech tag), and each column represents a word in the sequence. The word
    vectors are 4-D column vectors.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之前，您只需要担心一个维度，即副词标签。现在，您需要处理这些单词向量的所有4个维度，以确保模式正确。您需要协调四个不同的“特征”或数据通道。因此，对于一个3个词、4个通道的核心，我们需要一个4x3矩阵。每一行代表一个通道（词性标签），每一列代表序列中的一个单词。单词向量是4维列向量。
- en: '[PRE18]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: You can see that this DataFrame is just an exact copy of the sequence of vectors
    you want to match in your text samples. Of course, you were only able to do this
    because you knew what you were looking for in this one toy example. In a real
    neural network, the deep learning optimizer will use backpropagation to *learn*
    the sequences of vectors that are most helpful in predicting your target variable
    (the label).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，这个DataFrame只是您想要在文本样本中匹配的向量序列的精确副本。当然，您之所以能够做到这一点，是因为您在这一个玩具示例中知道您在寻找什么。在真实的神经网络中，深度学习优化器将使用反向传播来*学习*最有助于预测您的目标变量（标签）的向量序列。
- en: How is it possible for a machine to match patterns? What is the math that causes
    a kernel to always match the pattern that it contains? In Figure 7.4 you can do
    the math yourself for a couple of strides of the filter across your data. This
    will help you see how all this works and why it’s so simple and yet so powerful.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 机器如何匹配模式是如何可能的？是什么数学导致核心始终匹配其包含的模式？在图7.4中，您可以自己进行一些数据的滤波器跨越几个步骤的数学计算。这将帮助您了解所有这些是如何工作的，以及为什么它既简单又强大。
- en: Figure 7.4 Check the convolution pattern matching yourself
  id: totrans-154
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.4 自己检查卷积模式匹配
- en: '![conv1d pos rightly timed pause squares drawio](images/conv1d-pos-rightly-timed-pause-squares_drawio.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![卷积1D定时暂停正方形drawio](images/conv1d-pos-rightly-timed-pause-squares_drawio.png)'
- en: Have you checked the math in Figure 7.4? Make sure you do this before you let
    PyTorch do the math, to embed this pattern of math in your neural network so you
    can do it in the future if you ever need to debug problems with your CNN.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在让PyTorch进行数学计算之前，您是否检查了图7.4中的数学计算？确保在让PyTorch进行数学计算之前进行此操作，以嵌入此数学模式到您的神经网络中，以便将来如果您需要调试CNN中的问题，您就可以进行数学计算。
- en: In PyTorch or any other deep learning framework designed to process multiple
    samples in parallel, you have to unsqueeze the kernel to add a dimension to hold
    additional samples. Your unsqueezed kernel (weight matrix) needs to be the same
    shape as your batch of input data. The first dimension is for the samples from
    your training or test datasets that are being input to the convolutional layer.
    Normally this would be the output of an embedding layer and would already be sized
    appropriately. But since you are hard-coding all the weights and input data to
    get to know how the Conv1d layer works, you will need to unsqueeze the 2-D tensor
    matrix to create a 3-D tensor cube. Since you only have the one quote you want
    to push forward through the convolution the dataset you only need a size of 1
    in the first dimension.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch或任何其他设计用于同时处理多个样本的深度学习框架中，您必须将核张量进行扩展，以添加一个维度来容纳额外的样本。您扩展的核（权重矩阵）需要与输入数据的批次具有相同的形状。第一个维度用于输入到卷积层的来自训练或测试数据集的样本。通常，这将是嵌入层的输出，并且已经具有适当的大小。但是，由于您正在硬编码所有权重和输入数据以了解Conv1d层的工作原理，因此您需要扩展2-D张量矩阵以创建3-D张量立方体。由于您只有一个引用要通过卷积推进数据集，因此您只需要在第一个维度上具有大小为1的尺寸。
- en: Listing 7.8 Load hard-coded weights into a Conv1d layer
  id: totrans-158
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.8 将硬编码的权重加载到Conv1d层中
- en: '[PRE19]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Finally you’re ready to see if your hand-crafted kernel can detect a sequence
    of adverb, verb, noun in this text.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您准备好看看您手工制作的核心是否可以检测到文本中的副词、动词、名词序列。
- en: Listing 7.9 Running a single example through a convolutional layer
  id: totrans-161
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.9 通过卷积层运行单个示例
- en: '[PRE20]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Figure 7.5 Conv1d output predicting rightly timed pause
  id: totrans-163
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.5 Conv1d输出正确预测定时暂停
- en: '![conv1d pos rightly timed pause y df](images/conv1d-pos-rightly-timed-pause-y_df.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![卷积1D定位恰当时机的暂停y_df](images/conv1d-pos-rightly-timed-pause-y_df.png)'
- en: The y value reaches a maximum value of 3 where all 3 values of 1 in the kernel
    line up perfectly with the three 1’s forming the same pattern within the part-of-speech
    tags for the sentence. Your kernel correctly detected the adverb, verb, noun sequence
    at the end of the sentence. The value of 3 for your convolution output rightly
    lines up with the word "rightly", the 16th word in the sequence. The is where
    the sequence of 3 words is located which match your pattern at positions 16, 17,
    and 18\. And it makes sense that the output would have a value of three, because
    each of the three matched parts of speech had a weight of one in your kernel,
    summing to a total of three matches.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: y 值达到最大值 3，其中内核中的所有 3 个值为 1 的部分与句子中的三个 1 完美匹配，形成了相同的词性标签模式。您的内核正确地检测到了句子末尾的副词、动词、名词序列。您卷积输出的值为
    3 是正确的，因为在序列中第 16 个单词“rightly” 的位置，存在 3 个与您的模式匹配的单词。这是匹配您的模式的 3 个单词序列的位置，分别位于位置
    16、17 和 18。而且，输出值为三是有意义的，因为每个匹配的词性都在您的内核中具有权重为一，总共有三个匹配。
- en: Don’t worry, you’ll never have to hand-craft a kernel for a convolutional neural
    network ever again…​ unless you want to remind yourself how the math is working
    so you can explain it to others.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 别担心，您永远不必再为卷积神经网络手工制作内核...除非您想提醒自己数学是如何工作的，以便向他人解释。
- en: 7.2.7 Natural examples
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.7 自然示例
- en: In the optical world of eyes and cameras, convolution is everywhere. When you
    look down at the surface of the ocean or a lake with polarizing sunglasses, the
    lenses do convolution on the light to filter out the noise. The lenses of polarized
    glasses help fishermen filter out the scattered light and see beneath the surface
    of the water to find fish.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在眼睛和相机的光学世界中，卷积无处不在。当您通过偏振太阳镜向下看海洋或湖泊表面时，镜片会对光进行卷积以滤除噪声。偏振眼镜的镜片有助于渔民滤除散射光，并看穿水面下找到鱼。
- en: And for a wilder example, consider a zebra standing behind a fence. The stripes
    on a zebra can be thought of as a visual natural language. A zebra’s stripes send
    out a signal to predators and potential mates about the health of that zebra.
    And the convolution that happens when a zebra is running among grass or bamboo
    or tree trunks can create a shimmering effect that makes Zebras difficult to catch.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 至于更疯狂的例子，想象一下一只斑马站在围栏后面。斑马的条纹可以被视为一种视觉自然语言。斑马的条纹向捕食者和潜在伴侣发送关于斑马健康状况的信号。当斑马在草地、竹林或树干之间奔跑时发生的卷积会产生一种闪烁效果，使斑马难以捕捉。
- en: In figure 7.6 you can think of the cartoon fence as a kernel of alternating
    numerical values. And the zebra in the background is like your data with alternating
    numerical values for the light and dark areas in its stripes. And convolution
    is symmetric because multiplication and addition are commutative operations. So
    if you prefer you can think of the zebra stripes as the filter and a long length
    of fence as the data.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 7.6 中，您可以将卡通围栏视为交替数值的内核。而背景中的斑马则像您的数据，其条纹中的光暗区域具有交替的数值。而且卷积是对称的，因为乘法和加法是可交换的操作。因此，如果您愿意，您可以将斑马的条纹视为滤波器，而一长段围栏视为数据。
- en: Figure 7.6 Zebra behind a fence ^([[18](#_footnotedef_18 "View footnote.")])
  id: totrans-171
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 7.6 斑马在围栏后面 ^([[18](#_footnotedef_18 "查看脚注。")])
- en: '![800x741px Zebra standing behind cartoon fence cropped](images/800x741px_Zebra_standing_behind_cartoon_fence_cropped.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![800x741像素斑马站在卡通围栏后面被裁剪](images/800x741px_Zebra_standing_behind_cartoon_fence_cropped.png)'
- en: Imagine the zebra in figure 7.6 walking behind the fence or the fence sliding
    in front of the zebra. As the zebra walks, the gaps in the fence will periodically
    line up with the zebra’s stripes. This will create a pattern of light and dark
    as we move the fence (kernel) or the zebra. It will become dark in places where
    the zebra’s black strips line up with the gaps in the brown fence. And the zebra
    will appear brighter where the white parts of its coat line up with the fence
    gaps so they can shine through. So if you want to recognize alternating values
    of black and white or alternating numerical values you can use alternating high
    (1) and low values (0) in your kernel.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下图 7.6 中的斑马走在围栏后面，或者围栏在斑马前面滑动。当斑马行走时，围栏中的缝隙将定期与斑马的条纹对齐。这将在我们移动围栏（内核）或斑马时创建光与暗的图案。当斑马的黑色条纹与棕色围栏的缝隙对齐时，这些地方将变暗。当斑马的白色部分与围栏的缝隙对齐时，它们就能透过，因此斑马会显得更亮。因此，如果您想要识别黑色和白色的交替值或交替的数值，您可以在您的内核中使用交替的高（1）和低值（0）。
- en: If you don’t see zebras walking behind fences very often, maybe this next analogy
    will be better. If you spend time at the beach you can imagine the surf as a natural
    mechanical convolution over the bottom of the ocean. As waves pass over the sea
    floor and approach the beach they rise or fall depending on what is hidden underneath
    the surface such as sandbars and large rocks or reefs. The sand bars and rocks
    are like components of word meaning that you are trying to detect with your convolutional
    neural network. This cresting of the waves over the sand bars is like the multiplication
    operation of convolution passing in waves over your data.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不经常看到斑马在栅栏后面走动，也许下一个类比会更好理解。如果你在海滩上待一段时间，你可以把浪潮想象成海底的一种自然机械卷积。当波浪经过海底并接近海滩时，它们会上升或下降，这取决于水面下隐藏的东西，比如沙洲、大石头或礁石。沙洲和石头就像你试图用卷积神经网络检测的单词意义的组成部分一样。波浪在沙洲上涨的过程就像卷积乘法操作一样，在你的数据上波浪潮过去。
- en: Now imagine that you’ve dug a hole in the sand near the edge of the water. As
    the surf climbs the shore, depending on the height of the waves, some of the surf
    will spill into your little pool. The pool or moat in front of your sand castle
    is like the reduce or sum operation in a convolution. In fact you will see later
    that we use an operation called "max pooling" which behaves very much like this
    in a convolutional neural network. Max pooling helps your convolution measure
    the "impact" of a particular pattern of words just as your hole in the sand accumulates
    the impact of the surf on the shore. If nothing else, this image of surf and sand
    castles will help you remember the technical term *max pooling* when you see it
    later in this chapter.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在想象一下，你在靠近水边挖了一个洞。当浪潮爬上岸时，取决于波浪的高度，一些浪潮会溢入你的小水池中。你沙堡前的水池或护城河就像卷积中的减少或求和操作一样。事实上，你会看到我们后来使用的一种操作叫做“最大池化”，它在卷积神经网络中的行为非常像这样。最大池化帮助你的卷积测量出特定单词模式的“影响”，就像你的沙堆在海岸上累积了浪潮的影响一样。即使没有别的，这张关于浪潮和沙堡的图像也会帮助你在本章后面看到时记住技术术语*最大池化*。
- en: 7.3 Morse code
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 莫尔斯电码
- en: 'Before ASCII text and computers, and even telephones, there was another way
    to communicate natural language: *Morse code*.^([[19](#_footnotedef_19 "View footnote.")])
    Morse code is a text encoding that substitutes dots and dashes for natural language
    letters and words. These dots and dashes become long and short beeping tones on
    a telegraph wire or over the radio. Morse code sounds like the beeping in a really
    really slow dial-up Internet connection. Play the audio file used in the Python
    example later in this section to hear it for yourself.^([[20](#_footnotedef_20
    "View footnote.")]) Amateur radio operators send messages around the world by
    tapping on a single key. Can you imagine typing text on a computer keyboard that
    has only one key like the Framework laptop spacebar in Figure 7.7?!'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ASCII 文本和计算机甚至电话出现之前，还有另一种交流自然语言的方式：*莫尔斯电码*。莫尔斯电码是一种将点和短划替代自然语言字母和单词的文本编码。这些点和短划在电报线上或无线电波上变成长音和短音的蜂鸣声。莫尔斯电码听起来就像一个非常缓慢的拨号上网连接中的蜂鸣声。在本节后面的
    Python 示例中播放音频文件，亲自听一下吧。业余无线电操作员通过敲击单个键向世界各地发送消息。你能想象在计算机键盘上输入文本，而键盘上只有一个键，就像图
    7.7 中的 Framework 笔记本的空格键一样吗？
- en: Figure 7.7 A single key laptop keyboard
  id: totrans-178
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 7.7 单个关键的笔记本键盘
- en: '![framework laptop spacebar](images/framework-laptop-spacebar.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![Framework 笔记本空格键](images/framework-laptop-spacebar.png)'
- en: Figure 7.8 shows what an actual Morse code key looks like. Just like the key
    on a computer keyboard or the fire button on a game controller, the Morse code
    key just closes an electrical contact whenever the button is pressed.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8 显示了一个实际的莫尔斯电码键的样子。就像计算机键盘上的键或游戏控制器上的开火按钮一样，莫尔斯电码键只在按下按钮时关闭电气接触。
- en: Figure 7.8 An antique Morse code key
  id: totrans-181
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 7.8 一把古董莫尔斯电码键
- en: '![wikipedia morse code key](images/wikipedia-morse-code-key.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![维基百科莫尔斯电码键](images/wikipedia-morse-code-key.png)'
- en: Morse code is a language designed to be tapped out on a single key like this.
    It was used a lot in the age of telegraph, before telephones made it possible
    to send voice and data over wires. To visualize Morse code on paper people draw
    dots and dashes to represent short and long taps the key. You press the key down
    briefly to send out a dot, and you press it down a bit longer to send out a dash.
    There’s nothing but silence when you aren’t pressing the key at all. So it’s a
    bit different than typing text. It’s more like using your keyboard as the fire
    button on game controller. You can imagine a Morse code key like a video game
    laser or anything that sends out energy only while the key is pressed. You might
    even find a way to send secret messages in multiplayer games using your weapon
    as a telegraph.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 莫尔斯电码是一种设计成只需按下一个键敲出的语言，就像这样。它在电报时代被广泛使用，在电话使得通过电线发送语音和数据成为可能之前。为了在纸上可视化莫尔斯电码，人们用点和线来代表按键的短敲和长敲。按下键时，你短暂地向外发出一个点，而稍微按住键则会发出一个破折号。当你根本不按下该键时则是完全的沉默。所以它和输入文本不太一样。更像是把你的键盘当作游戏手柄上的开火按钮。你可以把莫尔斯电码键想象成视频游戏激光或以按下键的时候才发送能量的任何东西。你甚至可以通过在多人游戏中将武器当作电报机来发送秘密信息。
- en: Communicating with a single key on a computer keyboard would be nearly impossible
    if it weren’t for Samuel Morse’s work to create a new natural language. Morse
    did such a good job designing the language of Morse code, even ham-fisted amateur
    radio operators like me can use it in a pinch.^([[21](#_footnotedef_21 "View footnote.")])
    You’re about to learn the 2 most important bits of the language so you can use
    it too in an emergency. Don’t worry, you’re only going to learn 2 letters of the
    language. That should be enough to give you a clearer understanding of convolution
    and how it works on natural languages.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 要在计算机键盘上仅使用一个键进行通信几乎是不可能的，如果不是萨缪尔·莫尔斯创造新的自然语言的工作，就不会有这种可能。莫尔斯在设计莫尔斯电码的语言方面做得非常好，即使像我这样拙笨的业余无线电操作员也可以在紧急情况下使用它。接下来，你将学习这种语言中最重要的两个字母，以便在紧急情况下也能使用它。不用担心，你只需要学习这个语言的两个字母就足够了。这应该足以让你更清楚地理解卷积以及它在自然语言上的工作原理。
- en: Figure 7.9 Morse code dictionary
  id: totrans-185
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.9 莫尔斯电码字典
- en: '![wikipedia morse code table svg](images/wikipedia-morse-code-table_svg.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![wikipedia morse code table svg](images/wikipedia-morse-code-table_svg.png)'
- en: Morse code is still used today in situations when the radio waves are too noisy
    for someone to understand your voice. It’s especially useful when you really,
    really, really need to get a message out. Sailors trapped in an air pocket within
    a sunken submarine or ship have used it to communicate with rescuers by banging
    out Morse code on the metal hull. And people buried under rubble after earthquakes
    or mining accidents will bang on metal pipes and girders to communicate with rescuers.
    If you know a bit of Morse code you might be able to have a two-way conversation
    with someone, just by banging out your words in Morse code.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 莫尔斯电码至今仍然在无线电波嘈杂的情况下使用，以便使别人能够理解你的语音。当你真的，真的，真的需要传达信息时，它尤其有用。被困在沉没的潜艇或船内的水下空腔的水手使用莫尔斯电码在金属船体上敲出来与营救者进行交流。在地震或矿井事故后被埋在瓦砾下的人们会用金属管道和钢梁敲击来与救援人员进行通信。如果你懂一点莫尔斯电码，你也许可以通过用莫尔斯电码敲出你的话与别人进行双向对话。
- en: Here’s the example audio data for a secret message being broadcast in Morse
    code. You will process it in the next section using using a hand-crafted convolution
    kernel. For now you probably just want to play the audio track so you can hear
    what Morse code sounds like.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个以莫尔斯电码进行广播的秘密消息的音频数据示例。在接下来的部分中，你将使用手工制作的卷积核处理这个数据。现在，你可能只想播放音频轨道，以便听到莫尔斯电码的声音是什么样子。
- en: Listing 7.10 Download secret message
  id: totrans-189
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 代码清单7.10 下载秘密
- en: '[PRE21]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Of course your `.nlpia2-data` directory will be located in your `$HOME` directory
    rather than mine. That’s where you’ll find all the data used in these examples.
    Now you can load the wav file to create an array of numerical values for the audio
    signal that you can process later with convolution.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你的`.nlpia2-data`目录将位于你的`$HOME`目录下，而不是我的。这里是这些示例中使用的所有数据。现在，你可以加载wav文件，以创建一个包含音频信号的数值数组，稍后可以用卷积进行处理。
- en: 7.3.1 Decoding Morse with convolution
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.1 使用卷积解码莫尔斯电码
- en: If you know a little Python you can build a machine that can interpret Morse
    code for you so you won’t have to memorize all those dots and dashes in the morse
    code dictionary of figure 7.9\. Could come in handy during the zombie apocalypse
    or "The Big One" (Earthquake in California). Just make sure you hang onto a computer
    or phone that can run Python.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您了解一点 Python，您可以构建一个能够为您解释摩尔斯电码的机器，这样您就不必记住图 7.9 摩尔斯电码字典中所有的点和划线了。在僵尸启示录或“大事件”（加州地震）期间可能会派上用场。只需确保保留能够运行
    Python 的计算机或手机。
- en: Listing 7.11 Load the secret Morse code wav file
  id: totrans-194
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第 7.11 列 加载秘密摩尔斯电码 WAV 文件
- en: '[PRE22]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The audio signal in this wav file oscillates between 255 and 0 (max and min
    `uint8` values) when there is a beep tone. So you need to rectify the signal using
    `abs()` and then normalize it so the signal will be 1 when a tone is playing and
    0 when there is no tone. You also want to convert the sample numbers to milliseconds
    and downsample the signal so it’s easier to examine individual values and see
    what’s going on. Listing 7.12 centers, normalizes, and downsamples the audio data
    and extracts the first two seconds of this audio data.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 WAV 文件中的音频信号在哔哔声时在 255 和 0 之间振荡（最大和最小的 `uint8` 值）。因此，您需要使用 `abs()` 对信号进行矫正，然后将其标准化，使信号在播放音调时为
    1，在没有音调时为 0。您还希望将采样数转换为毫秒，并对信号进行降采样，以便更容易地检查单个值并查看发生了什么。第 7.12 列 居中、标准化和降采样音频数据，并提取此音频数据的前两秒。
- en: Listing 7.12 Normalize and downsample the audio signal
  id: totrans-198
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第 7.12 列 标准化和降采样音频信号
- en: '[PRE24]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Now, you can plot your shiny new Morse code dots and dashes with `audio.plot()`.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以使用 `audio.plot()` 绘制闪亮的新摩尔斯电码点和划线。
- en: Figure 7.10 Square waves morse code secret message
  id: totrans-201
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第 7.10 图 方波摩尔斯电码秘密消息
- en: '![morse code wav plot preprocessed](images/morse-code-wav-plot-preprocessed.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![摩尔斯电码 wav 经预处理的图](images/morse-code-wav-plot-preprocessed.png)'
- en: Can you see where the dots are in figure 7.10? The dots are 60 milliseconds
    of silence (signal value of 0) followed by 60 milliseconds of tone (signal value
    of 1) and then 60 seconds of silence again (signal value of 0).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 您能在图 7.10 中看到点在哪里吗？点是 60 毫秒的静音（信号值为 0），然后是 60 毫秒的音调（信号值为 1），然后再次是 60 秒的静音（信号值为
    0）。
- en: To detect a dot with convolution you want to design a kernel that matches this
    pattern of low, high, low. The only difference is that for the low signal, you
    need to use a negative one rather than a zero, so the math adds up. You want the
    output of the convolution to be a value of one when a dot symbol is detected.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 要通过卷积检测点，您需要设计一个与低、高、低的模式匹配的核心。唯一的区别是对于低信号，您需要使用负一而不是零，这样数学就会加起来。您希望卷积的输出在检测到点符号时为
    1。
- en: Lising 7.12 shows how to build dot-detecting kernel.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 第 7.12 列 展示了如何构建点检测核心。
- en: Listing 7.13 Dot detecting kernel
  id: totrans-206
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第 7.13 列 点检测核
- en: '[PRE25]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Figure 7.11 Morse code dot detecting kernel
  id: totrans-208
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第 7.11 图 摩尔斯电码点检测核心
- en: '![dot detecting kernel](images/dot-detecting-kernel.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![点检测核](images/dot-detecting-kernel.png)'
- en: You can try out your hand-crafted kernel by convolving it with the audio signal
    to see if it is able to detect the dots. The goal is for the convolved signal
    to be high, close to one, near the occurrences of a dot symbol, the short blips
    in the audio. You also want your dot detecting convolution to return a low value
    (close to zero) for any dash symbols or silence that comes before or after the
    dots.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过将其与音频信号进行卷积来尝试您手工制作的核心，以查看它是否能够检测到点。目标是使卷积信号在点符号出现时高、接近于 1，在音频中的短脉冲。您还希望您的点检测卷积在点之前或之后的任何短划线或静音处返回低值（接近于零）。
- en: Listing 7.14 Dot detector convolved with the secret message
  id: totrans-211
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第 7.14 列 点检测器与秘密消息卷积
- en: '[PRE26]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Figure 7.12 Hand-crafted dot detecting convolution
  id: totrans-213
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第 7.12 图 手工制作的点检测卷积
- en: '![hand crafted dot detector convolution](images/hand-crafted-dot-detector-convolution.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![手工制作的点检测卷积](images/hand-crafted-dot-detector-convolution.png)'
- en: Looks like the hand-crafted kernel did all right! The convolution output is
    close to one only in the middle of the dot symbols.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来手工制作的核心做得不错！卷积输出仅在点符号的中间接近于 1。
- en: Now that you understand how convolution works, feel free to use the `np.convolve()`
    function. It works faster and gives you more options for the `mode` of handling
    the padding.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您了解了卷积的工作原理，可以随意使用 `np.convolve()` 函数。它运行更快，并为您提供了更多关于填充处理的 `mode` 选项。
- en: Listing 7.15 Numpy convolve
  id: totrans-217
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第 7.15 列 NumPy 卷积
- en: '[PRE27]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Figure 7.13 Numpy convolution
  id: totrans-219
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第 7.13 图 NumPy 卷积
- en: '![hand crafted dot detector numpy convolution](images/hand-crafted-dot-detector-numpy-convolution.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![手工制作点检测器Numpy卷积](images/hand-crafted-dot-detector-numpy-convolution.png)'
- en: 'Numpy convolution gives you three possible modes for doing the convolution,
    in order of increasing output length:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: Numpy卷积有三种可能的模式可用于进行卷积，按输出长度递增的顺序依次为：
- en: '**valid**: Only output `len(kernel) - 1` values for the convolution as our
    pure python `'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**valid**: 以纯Python为例，只输出`len(kernel)-1`个卷积值。'
- en: '**same**: Output a signal that is the same length as the input by extrapolating
    the signal beyond the beginning and end of the array.'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**same**: 通过在数组的开始和结尾之外推算信号，输出与输入长度相同的信号。'
- en: '**full**: Output signal will have more sample than the input signal.'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**full**: 输出信号将比输入信号更长。'
- en: The numpy convolution set to 'same' mode seems to work better on our Morse code
    audio signal. So you’ll want to check that your neural network library uses a
    similar mode when performing convolution within your neural network.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: Numpy卷积设置为“same”模式似乎在我们的莫尔斯电码音频信号中运作得更好。因此，当在神经网络中进行卷积时，你需要检查你的神经网络库是否使用类似的模式。
- en: That was a lot of hard work building a convolutional filter to detect a single
    symbol in a Morse code audio file. And it wasn’t even a single character of natural
    language text, just one third of the letter "S"! Fortunately all you laborious
    hand-crafting is over. It’s possible to use the power of back-propagation within
    neural networks to *learn* the right kernels to detect all the different signals
    important to your problem.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 建造一个卷积滤波器以在莫尔斯电码音频文件中检测一个单一符号真是一项艰苦的工程。而且这还不是一个自然语言文本的单个字符， 只是 `S` 字母的三分之一！幸运的是，你辛勤手工制作的日子已经结束了。你可以在神经网络的反向传播中使用它所拥有的强大力量来学习正确的内核以检测解决问题所需的所有不同信号。
- en: 7.4 Building a CNN with PyTorch
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.4 使用PyTorch构建CNN
- en: Figure 7.14 shows you how text flows into a CNN network and then outputs a embedding.
    Just as with previous NLP pipelines, you need to tokenize your text first. Then
    you identify the set of all the tokens used in your text. You ignore the tokens
    you don’t want to *count* and assign an integer index to each word in your vocabulary.
    The input sentence has 4 tokens so we start with a sequence of 4 integer indices,
    one for each token.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.14展示了您如何将文本流入CNN网络，然后输出嵌入。与以前的NLP流水线一样，需要首先对文本进行标记化。然后您会识别出文本中使用的所有令牌集。您将忽略不想计数的令牌，并为词汇表中的每个单词分配一个整数索引。输入语句有4个令牌，因此我们从一个由4个整数索引组成的序列开始，每个令牌对应一个索引。
- en: CNNs usually use word embeddings rather than one-hot encodings to represent
    each word. You initialize a matrix of word embeddings that has the same number
    of rows as words in your vocabulary and 300 columns if you want to use 300-D embeddings.
    You can set all your initial word embeddings to zero or some small random values.
    If you want to do knowledge transfer and use pretrained word embeddings, you then
    look up your tokens in GloVE, Word2vec, fastText or any word embeddings you like.
    And you insert these vectors into your matrix of embeddings at the matching row
    based on your vocabulary index.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: CNN通常使用单词嵌入来代替单热编码来表示每个单词。您将初始化一个单词嵌入矩阵，该矩阵的行数与词汇表中的单词数量相同，并且如果要使用300-D嵌入，则有300个列。可以将所有初始单词嵌入设置为零或某些小的随机值。如果要进行知识转移并使用预训练的单词嵌入，则可以在GloVE、Word2vec、fastText或任何喜欢的单词嵌入中查找您的令牌。并将这些向量插入到与词汇表索引匹配的行中的嵌入矩阵中。
- en: For this four-token sentence you then look up the appropriate word embedding
    get a sequence of 4 embedding vectors once you have looked up each embedding in
    your word embedding matrix. You also get additional padding token embeddings that
    are typically set to zeros so they don’t interfere with the convolution. If you
    used the smallest GloVe embeddings, your word embeddings are 50 dimensional, so
    you end up with a 50 x 4 matrix of numerical values for this single short sentence.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个四令牌句子，然后可以查找适当的单词嵌入，一旦在单词嵌入矩阵中查找每个嵌入，就会得到一个4个嵌入向量的序列。你也会得到额外的填充标记嵌入，它们通常被设置为零，所以它们不会干扰卷积。如果您使用最小的GloVe嵌入，那么您的单词嵌入是50维的，因此您会得到一个50
    x 4的数值矩阵，用于这个短句子。
- en: Your convolutional layer can process each of these 50 dimensions with a 1-D
    convolutional kernel to squeeze this matrix of information about your sentence
    a bit. If you used a kernel of size (length) of two, and a stride of two, you
    would end up with a matrix of size 50 x 3 to represent the sequence of four 50-D
    word vectors.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 你的卷积层可以使用1-D卷积内核处理这50个维度中的每一个，稍微挤压一下关于你的句子的这个矩阵的信息。如果你使用了长度为2的内核和步幅为2，你将得到一个大小为50
    x 3的矩阵来表示四个50-D单词向量的序列。
- en: A *pooling layer*, typically max pooling, is used to reduce the size of the
    output even further. A max pooling layer with 1-D kernel will compress your sequence
    of three 50-D vectors down to a single 50-D vector. As the name implies, max pooling
    will take the largest most impactful output for each channel (dimension) of meaning
    in your sequence of vectors. Max pooling is usually pretty effective because it
    allows your convolution to find the most important dimensions of meaning for each
    n-gram in your original text. With multiple kernels they can each specialize on
    a separate aspect of the text that is influencing your target variable.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 通常使用*池化层*，通常是最大池化，来进一步减小输出的大小。带有1-D内核的最大池化层将把你的三个50-D向量的序列压缩成一个单一的50-D向量。顾名思义，最大池化将为向量序列中每个通道（维度）的最大和最有影响的输出。最大池化通常相当有效，因为它允许你的卷积为原始文本中每个n-gram找到最重要的意义维度。通过多个内核，它们可以分别专门化文本的不同方面，这些方面会影响你的目标变量。
- en: Note
  id: totrans-233
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: You should call the output of a convolutional layer an "encoding" rather than
    an "embedding". Both words are used to describe high dimensional vectors, but
    the word "encoding" implies processing over time or in a sequence. The convolution
    math happens over time in your sequences of word vectors, whereas "embedding"
    vectors are the result of processing of a single unchanging token. Embeddings
    don’t encode any information about the order or sequence of words. Encodings are
    more complete representations of the meaning of text because they account for
    the order of words in the same way that your brain does.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该将卷积层的输出称为“编码”，而不是“嵌入”。这两个词都用来描述高维向量，但是“编码”一词暗示着在时间上或序列中的处理。卷积数学在你的单词向量序列中的时间内发生，而“嵌入”向量是单个不变令牌的处理结果。嵌入不编码任何有关单词顺序或序列的信息。编码是对文本含义的更完整的表示，因为它们考虑了单词顺序，就像你的大脑一样。
- en: The encoding vector output by a CNN layer is a vector with whatever size (length)
    you specify. The length (number of dimensions) of your encoding vector doesn’t
    depend in any way on the length of your input text.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 由CNN层输出的编码向量是一个具有你指定的任意大小（长度）的向量。你的编码向量的长度（维度数）与输入文本的长度无关。
- en: Figure 7.14 CNN processing layers ^([[22](#_footnotedef_22 "View footnote.")])
  id: totrans-236
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.14 CNN处理层 ^([[22](#_footnotedef_22 "查看脚注。")])
- en: '![cnn architecture flow diagram drawio](images/cnn_architecture_flow_diagram_drawio.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![cnn架构流程图绘图](images/cnn_architecture_flow_diagram_drawio.png)'
- en: You’re going to need all your skills from the previous chapters to get the text
    in order so it can be input into your neural network. The first few stages of
    your pipeline in figure 7.14 are the tokenization and case folding that you did
    in previous chapters. You will use your experience from the previous examples
    to decide which words to ignore, such as stopwords, punctuation, proper nouns,
    or really rare words.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 你将需要利用前几章的所有技能来整理文本，以便将其输入到你的神经网络中。图7.14中你的管道的前几个阶段是你在前几章中做的标记和大小写转换。你将利用前面示例中的经验来决定忽略哪些单词，比如停用词、标点符号、专有名词或非常罕见的单词。
- en: Filtering out and ignoring words based on an arbitrary list of stopwords that
    you handcraft is usually a bad idea, especially for neural nets such as CNNs.
    Lemmatizing and stemming is also usually not a good idea. The model will know
    much more about the statistics of your tokens than you could ever guess at with
    your own intuition. Most examples you see on Kaggle and DataCamp and other data
    science websites will encourage you to hand craft these parts of your pipeline.
    You know better now.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你手工制作的任意停用词列表过滤和忽略单词通常是一个不好的主意，特别是对于像CNN这样的神经网络。词形还原和词干提取通常也不是一个好主意。模型将比你用直觉猜测的更了解你的令牌的统计信息。你在Kaggle、DataCamp和其他数据科学网站上看到的大多数示例都会鼓励你手工制作管道的这些部分。你现在知道得更清楚了。
- en: You aren’t going to handcraft you convolution kernels either. You are going
    to let the magic of backpropagation take care of that for you. A neural network
    can learn most of the parameters of your model, such as which words to ignore
    and which words should be lumped together because they have similar meaning. In
    fact, in chapter 6 you learned to represent the meanings of words with embedding
    vectors that capture exactly how they are similar to other words. You no longer
    have to mess around with lemmatization and stemming, as long as you have enough
    data to create these embeddings.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 你也不会手工制作卷积内核。你会让反向传播的魔力来处理这些事情。神经网络可以学习模型的大部分参数，例如哪些词要忽略，哪些词应该被合并在一起，因为它们具有相似的含义。实际上，在第6章中，您已经学会了用嵌入向量来表示单词的含义，这些嵌入向量精确地捕捉了它们与其他单词的相似程度。只要有足够的数据来创建这些嵌入向量，您就不再需要处理词形还原和词干提取。
- en: 7.4.1 Clipping and Padding
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.1 裁剪和填充
- en: CNN models require a consistent length input text so that all the output values
    within the encoding are at consistent positions within that vector. This ensures
    that the encoding vector your CNN outputs always has the same number of dimensions
    no matter how long, or short your text is. Your goal is to create vector representations
    of both a single character string and a whole page of text. Unfortunately a CNN
    can’t work with variable length text, so many of the words and characters will
    have to be "clipped" off at the end of your string if your text is too long for
    your CNN. And you need to insert filler tokens, called *padding*, to fill in the
    gaps in strings that are too short for your CNN.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 模型需要一致长度的输入文本，以便编码中的所有输出值在向量中处于一致的位置。这确保了你的 CNN 输出的编码向量始终具有相同的维度，无论你的文本是多长或多短。你的目标是创建一个字符串和一个整页文本的向量表示。不幸的是，CNN
    不能处理可变长度的文本，所以如果你的文本对于 CNN 来说太长，就会将许多单词和字符在字符串末尾进行 "裁剪"。而且你需要插入填充令牌，称为 *padding*，来填补那些对于您的
    CNN 来说太短的字符串中的空白部分。
- en: Remember that the convolution operation reduces the length of the input sequence
    by the same amount no matter how long it is. Convolution will always reduces the
    length of the input sequence by one less than the size of your kernel. And any
    pooling operation, such as max pooling, will also consistently reduce the length
    of the input sequence. So if you didn’t do any padding or clipping, long sentences
    would produce longer encoding vectors than shorter sentences. And that won’t work
    for an encoding, which needs to be size-invariant. You want your encoding vectors
    to always be the same length no matter the size of your input.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，卷积操作始终会减少输入序列的长度，无论其长度多长。卷积操作始终会将输入序列的长度减少一个比内核大小少的数。而任何池化操作，如最大池化，也会一致地减少输入序列的长度。因此，如果您没有进行任何填充或裁剪，长句子会产生比短句子更长的编码向量。而这对于需要具有大小不变性的编码是不起作用的。无论输入的大小如何，你希望你的编码向量始终具有相同的长度。
- en: This is a fundamental properties of vectors, that they have the same number
    of dimensions for the entire *vector space* that you are working in. And you want
    your NLP pipeline to be able to find a particular bit of meaning at the same location,
    or vector dimension, no matter where that sentiment occurred in a piece of text.
    Padding and clipping ensures that your CNN is location (time) and size (duration)
    invariant. Basically your CNN can find patterns in the meaning of text no matter
    where those patterns are in the text, as long as those patterns are somewhere
    within the maximum length that your CNN can handle.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这是向量的基本属性，即它们在整个你正在处理的*向量空间*中具有相同数量的维度。你希望你的 NLP 流水线能够在相同的位置或向量维度上找到特定的含义，无论这种情感在文本的哪个位置发生。填充和裁剪可以确保你的
    CNN 在位置（时间）和大小（持续时间）上是不变的。基本上，只要这些模式在您的 CNN 可处理的最大长度范围内的任何位置，您的 CNN 就可以在文本的含义中找到这些模式，无论这些模式在文本中的位置如何。
- en: You can chose any symbol you like to represent the padding token. Many people
    use the token "<PAD>", because it doesn’t exist in any natural language dictionary.
    And most English speaking NLP engineers will be able to guess what "<PAD>" means.
    And your NLP pipeline will see that these tokens are repeated a lot at the end
    of many strings. This will help it create the appropriate "filler" sentiment within
    the embedding layer. If you’re curious about what filler sentiment looks like,
    load your embedding vectors and compare the your embedding for "<PAD>" to the
    embedding for "blah" as in "blah blah blah". You just have to make sure that you
    use a consistent token and tell your embedding layer what token you used for your
    padding token. It’s common to make this the first token in your `id2token` or
    `vocab` sequence so it has an index and id value of `0`.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以选择任何你喜欢的符号来表示填充标记。许多人使用标记 "<PAD>"，因为它在任何自然语言字典中都不存在。大多数说英语的自然语言处理工程师都能猜到
    "<PAD>" 的含义。而且你的自然语言处理管道会注意到这些标记在许多字符串的末尾重复出现。这将帮助它在嵌入层中创建适当的 "填充" 情感。如果你对填充情感的样子感到好奇，加载你的嵌入向量，比较
    "<PAD>" 的嵌入和 "blah"（如 "blah blah blah"）的嵌入。你只需要确保使用一致的标记，并告诉你的嵌入层你用于填充标记的令牌是什么。通常将其作为你的
    `id2token` 或 `vocab` 序列中的第一个标记，以便它具有索引和 id 值 `0`。
- en: Once you’ve let everybody know what your padding token is, you now need to actually
    decide on a consistent padding approach. Just as in computer vision, you can pad
    either side of your token sequence, the beginning or the end. And you can even
    split the padding and put half at the beggining and half at the beginning. Just
    don’t insert them between words. That would interfere with the convolution math.
    And make sure you add the total number of padding tokens required to create the
    correct length sequences for your CNN.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你告诉大家你的填充标记是什么，你现在需要决定一个一致的填充方法。就像在计算机视觉中一样，你可以在你的令牌序列的任意一侧填充，即开头或结尾。你甚至可以拆分填充，将一半放在开头，另一半放在结尾。只是不要把它们插在单词之间。那会干扰卷积计算。并确保你添加的填充标记的总数能够创建正确长度的序列用于你的
    CNN。
- en: In listing Listing 7.16 you will load "birdsite" (microblog) posts that have
    been labeled by Kaggle contributors with their news-worthiness. Later you’ll use
    use your CNN model to predict whether CNN (Cable News Network) would be likely
    to "pick up" on the news before it spreads on its own in the "miasma."
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在清单 7.16 中，您将加载由 Kaggle 贡献者标记了其新闻价值的 "birdsite"（微博）帖子。稍后您将使用您的 CNN 模型来预测 CNN（有线电视新闻网）是否会在
    "miasma." 中的新闻在自己传播之前 "采取"。
- en: Important
  id: totrans-248
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 重要提示
- en: We intentionally use words that nudge you towards prosocial, authentic, mindful
    behavior. The dark patterns that permeate the Internet have nudged creative powerhouses
    in the tech world to create an alternate, more authentic universe with it’s own
    vocabulary.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有意使用能引导您朝着亲社会、真实、注意力集中的行为的词语。弥漫在互联网上的黑暗模式已经引导了科技界的创意中坚力量创建了一个替代的、更真实的宇宙，拥有它自己的词汇。
- en: '"Birdsite": What "fedies" call Twitter'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '"Birdsite"："fedies" 称之为 Twitter'
- en: '"Fedies": Users of federated social media apps that protect your well-being
    and privacy'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '"Fedies"：使用保护您健康和隐私的联合社交媒体应用的用户'
- en: '"Fediverse" Alternate universe of federated social media apps (Mastodon, PeerTube)'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '"Fediverse" 联合社交媒体应用的替代宇宙（Mastodon，PeerTube）'
- en: '"Nitter" is a less manipulative frontend for Twitter'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '"Nitter" 是 Twitter 的一个不那么操纵的前端。'
- en: '"Miasma" is Neil Stephenson’s name for a dystopian Internet'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '"Miasma" 是尼尔·斯蒂芬森对一个爱情的互联网的称呼'
- en: Listing 7.16 Load news posts
  id: totrans-255
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 7.16 加载新闻帖子
- en: '[PRE28]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: You can see in the examples above that some microblog posts push right up against
    the character limit of birdsite. Others get the point across with fewer words.
    So you will need to pad, or fill, these shorter texts so all of the examples in
    your dataset have the same number of tokens. If you plan to filter out really
    frequent words or really rare words later in your pipeline, your padding function
    needs to fill in those gaps too. So listing 7.17 tokenizes these texts and filters
    out a few of the most common tokens that it finds.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在上面的例子中看到，一些微博帖子几乎达到了 birdsite 的字符限制。其他则通过较少的词语表达了观点。因此，您需要对这些较短的文本进行填充，以便数据集中的所有示例具有相同数量的令牌。如果您计划在管道的后期过滤掉非常频繁的词或非常罕见的词，您的填充函数也需要填补这些差距。因此，清单
    7.17 对这些文本进行了标记化，并过滤掉了其中的一些最常见的标记。
- en: Listing 7.17 Most common words for your vocabulary
  id: totrans-259
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 7.17 词汇表中最常见的单词
- en: '[PRE30]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: You can see that the token "t" occurs almost as many times (5199) as there are
    posts (7613). This looks like part of a url created by a url shortener often used
    to track microbloggers on this app. You should ignore the first three url-like
    tokens if you want your CNN to focus on just the meaning of the words in the content
    that a human would likely read. If your goal is to build a CNN that reads and
    understands language like a human, you would create a more sophisticated tokenizer
    and token filter to strip out any text that humans don’t pay attention to, such
    as URLs and geospatial coordinates.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，令牌 "t" 出现的次数几乎和帖子数（7613）一样多（5199）。这看起来像是由 url 缩短器创建的部分 url，通常用于跟踪这个应用程序上的微博主。如果你希望你的
    CNN 专注于人类可能会阅读的内容中的单词的含义，你应该忽略前三个类似 url 的令牌。如果你的目标是构建一个像人类一样阅读和理解语言的 CNN，那么你将创建一个更复杂的分词器和令牌过滤器，以去除人类不关注的任何文本，例如
    URL 和地理空间坐标。
- en: Once you have your vocabulary and tokenizer dialed in, you can build a padding
    function to reuse whenever you need it. If you make your `pad()` function general
    enough, as in listing 7.18, you can use it on both string tokens and integer indexes.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你调整好了你的词汇表和分词器，你就可以构建一个填充函数，以便在需要时重复使用。如果你的 `pad()` 函数足够通用，就像清单 7.18 中一样，你可以将它用于字符串令牌和整数索引。
- en: Listing 7.18 Multipurpose padding function
  id: totrans-264
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 7.18 多功能填充函数
- en: '[PRE32]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We have one last preprocessing step to do for CNNs to work well. You want to
    include your token embeddings that you learned about in chapter 6.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要为 CNN 的良好工作进行最后一个预处理步骤。你想要包含你在第 6 章学到的令牌嵌入。
- en: 7.4.2 Better representation with word embeddings
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.2 用单词嵌入进行更好的表示
- en: Imagine you are running a short bit of text through your pipeline. Figure 7.15
    shows what this would look like before you’ve turned your word sequence into numbers
    (or vectors, hint hint) for the convolution operation.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你正在将一小段文本通过你的管道运行。图 7.15 展示了在你将单词序列转换为数字（或向量，提示提示）进行卷积操作之前的样子。
- en: Figure 7.15 Convolution striding
  id: totrans-269
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 7.15 卷积步幅
- en: '![cnn stride text words are sacred transparent drawio](images/cnn-stride-text-words-are-sacred_transparent_drawio.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![cnn 步幅文本 词语是神圣的 透明 drawio](images/cnn-stride-text-words-are-sacred_transparent_drawio.png)'
- en: 'Now that you have assembled a sequence of tokens, you need to represent their
    meaning well for your convolution to be able to compress and encode all that meaning.
    For the fully-connected neural networks we used in chapter 5 and 6 you could use
    one-hot encoding. But one-hot encoding creates extremely large, sparse matrices
    and you can do better than that now. You learned a really powerful way to represent
    words in chapter 6: word embeddings. Embeddings are much more information-rich
    and dense vector representation of your words. A CNN, and almost any other deep
    learning or NLP model, will work better when you represent words with embeddings.
    Figure 7.11 shows you how to do that.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经组装了一个令牌序列，你需要很好地表示它们的含义，以便你的卷积能够压缩和编码所有这些含义。在第 5 和 6 章中我们使用的全连接神经网络中，你可以使用
    one-hot 编码。但是 one-hot 编码会创建极其庞大、稀疏的矩阵，而现在你可以做得更好。你在第 6 章学到了一种非常强大的单词表示方式：单词嵌入。嵌入是你的单词的更加信息丰富和密集的向量表示。当你用嵌入来表示单词时，CNN
    和几乎任何其他深度学习或 NLP 模型都会表现得更好。图 7.11 展示了如何做到这一点。
- en: Figure 7.16 Word embeddings for convolution
  id: totrans-272
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 7.16 用于卷积的单词嵌入
- en: '![cnn embeddings glove words are sacred drawio](images/cnn-embeddings-glove-words-are-sacred_drawio.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![cnn 嵌入 glove 单词是神圣的 drawio](images/cnn-embeddings-glove-words-are-sacred_drawio.png)'
- en: Figure 7.16 shows what the `nn.Embedding` layer in PyTorch is doing behind the
    scenes. To orient you on how the 1-D convolution slides over your data, the diagram
    shows 3 steps of a two-length kernel stepping through your data. But how can a
    1-D convolution work on a sequence of 300-D GloVe word embeddings? You just have
    to create a convolution kernel (filter) for each dimension you want to find the
    patterns in. This means that each dimension of your word vectors is a channel
    in the convolution layer.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.16 展示了 PyTorch 中 `nn.Embedding` 层在幕后执行的操作。为了让你了解 1-D 卷积如何在你的数据上滑动，该图显示了一个两个长度的核在你的数据上移动的
    3 个步骤。但是一个 1-D 卷积如何在一个 300-D GloVe 单词嵌入序列上工作呢？你只需要为你想要查找模式的每个维度创建一个卷积核（滤波器）。这意味着你的单词向量的每个维度都是卷积层中的一个通道。
- en: Unfortunately, many blog posts and tutorials may mislead you about the proper
    size for a convolutional layer. Many PyTorch beginners assume that the output
    of an Embedding layer can flow right into a convolution layer without any resizing.
    Unfortunately this would create a 1-D convolution along the dimensions of the
    word embeddings rather than the sequence of words. So you will need to transpose
    your Embedding layer outputs so that the channels (word embedding dimensions)
    line up with the convolutional channels.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，许多博客文章和教程可能会误导您关于卷积层的正确尺寸。 许多 PyTorch 初学者认为 Embedding 层的输出可以直接流入卷积层而不需要任何调整大小。
    不幸的是，这将创建一个沿着单词嵌入维度而不是单词序列的 1-D 卷积。 因此，您需要转置您的嵌入层输出，以使通道（单词嵌入维度）与卷积通道对齐。
- en: PyTorch has an `nn.Embedding` layer you can use within all your deep learning
    pipelines. If you want your model to learn the embeddings from scratch you only
    need to tell PyTorch the number of embeddings you need, which is the same as your
    vocabulary size. The embedding layer also needs you to tell it the number of dimension
    to allocate for each embedding vector. Optionally you can define the padding token
    index id number.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 有一个 `nn.Embedding` 层，您可以在所有深度学习流水线中使用。 如果您希望模型从头开始学习嵌入，您只需要告诉 PyTorch
    您需要多少嵌入，这与您的词汇量大小相同。 嵌入层还需要您告诉它为每个嵌入向量分配多少维度。 可选地，您可以定义填充令牌索引 id 号。
- en: Listing 7.19 Learn your embeddings from scratch
  id: totrans-277
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 代码清单 7.19 从头开始学习嵌入
- en: '[PRE33]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The embedding layer will be the first layer in your CNN. That will convert your
    token IDs into their own unique 64-D word vectors. And backpropagation during
    training will adjust the weights in each dimension for each word to match 64 different
    ways that words can be used to talk about news-worthy disasters. These embeddings
    won’t represent the complete meaning of words the way the FastText and GloVe vectors
    did in chapter 6\. These embeddings are good for only one thing, determining if
    a Tweet contains newsworthy disaster information or not.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入层将是您的 CNN 中的第一层。这将把您的令牌 ID 转换成它们自己独特的 64-D 单词向量。在训练期间的反向传播将调整每个单词在每个维度上的权重，以匹配单词可用于谈论新闻灾害的
    64 种不同方式。这些嵌入不会像第 6 章中的 FastText 和 GloVe 向量一样代表单词的完整含义。这些嵌入只有一个好处，那就是确定一条 Tweet
    是否包含新闻灾害信息。
- en: Finally you can train your CNN to see how well it will do on an extremely narrow
    dataset like the Kaggle disaster tweets dataset. Those hours of work crafting
    a CNN will pay off with super-fast training time and impressive accuracy.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您可以训练您的 CNN，看看它在像 Kaggle 灾难推文数据集这样的极窄数据集上的表现如何。 那些花费时间打造 CNN 的小时将以极快的训练时间和令人印象深刻的准确性得到回报。
- en: Listing 7.20 Learn your embeddings from scratch
  id: totrans-281
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 代码清单 7.20 从头开始学习嵌入
- en: '[PRE34]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: After only 7 passes through your training dataset you achieved 79% accuracy
    on your test set. And on modern laptop CPU this should take less than a minute.
    And you kept the overfitting to a minimum by minimizing the total parameters in
    your model. The CNN uses very few parameters compared to the embedding layer.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅经过 7 次通过训练数据集，您就在测试集上实现了 79% 的准确率。 在现代笔记本电脑 CPU 上，这应该不到一分钟。 并且通过最小化模型中的总参数，您将过拟合保持到最低。
    与嵌入层相比，CNN 使用的参数非常少。
- en: What happens if you continue the training for a bit longer?
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您继续训练一段时间会发生什么？
- en: Listing 7.21 Continue training
  id: totrans-286
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 代码清单 7.21 继续训练
- en: '[PRE36]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Oh my, that looks fishy. That’s a lot of overfitting - 94% on the training set
    and 78% on the test set. The training set accuracy kept climbing and eventually
    got well above 90%. By the 20th epoch the model achieved 94% accuracy on the training
    set. It’s better than even expert humans. Read through a few examples yourself
    without looking at the label. Can you get 94% of them correct? Here are the first
    four, after tokenization, ignoring out-of-vocabulary words, and adding padding.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 哦，这看起来很可疑。 过拟合太严重了 - 在训练集上达到了 94%，在测试集上达到了 78%。 训练集准确率不断上升，最终超过了 90%。 到了第 20
    个 epoch，模型在训练集上的准确率达到了 94%。 它甚至比专家人类还要好。 自己阅读几个示例，不看标签，你能得到其中的 94% 吗？ 这是前四个示例，经过令牌化后，忽略了词汇表外的词汇，并添加了填充。
- en: '[PRE38]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: If you answered ["disaster", "not", "not", disaster"] then you got all 4 of
    these right. But keep going. Can you get nineteen out of twenty correct? That’s
    what you’d have to do to beat the training set accuracy of this CNN. It’s no surprise
    this is a hard problem and your CNN is getting only 79% accuracy on the test set.
    After all, bots are filling Twitter with disaster-sounding tweets all the time.
    And sometimes even real humans get sarcastic or sensationalist about world events.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的答案是["disaster", "not", "not", "disaster"]，那你全部答对了。但继续努力吧。你能做到十九对二十吗？这就是你需要在训练集准确率上击败这个卷积神经网络所需要做到的。这不是什么意外，因为机器人一直在推特上发布听起来像是灾难的推文。有时甚至真实的人类也会对世界事件感到讽刺或煽动性。
- en: What could be causing this overfitting? Are there too many parameters? Too much
    "capacity" in the neural net? Here’s a good function for displaying the parameters
    in each layer of your PyTorch neural networks.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 是什么导致了这种过拟合？是参数太多了吗？神经网络的"容量"太大了吗？以下是一个好的函数，用于显示PyTorch神经网络每层的参数。
- en: '[PRE40]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: When you have overfitting you can use pretrained models in your pipeline to
    help it generalize a bit better.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 当你遇到过拟合问题时，你可以在管道中使用预训练模型来改善其泛化能力。
- en: 7.4.3 Transfer learning
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.3 迁移学习
- en: Another enhancement that can help your CNN models it to use pretrained word
    embeddings such as GloVe. And it’s not cheating, because these models have been
    trained in a self-supervised way, without any labels from your disaster tweets
    dataset. You can transfer all the learning these GloVe vectors contain from the
    training that Stanford gave them on all of Wikipedia and other larger corpora.
    This way your model can get a head start learning a vocabulary of words about
    disasters by using the more general meaning of words. You just need to size your
    embedding layer to make room for the size GloVe embeddings you want to initialize
    your CNN with.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可以帮助你的CNN模型的优化方法是使用预训练的词嵌入，如GloVe。这并不是作弊，因为这些模型是以无监督的方式训练的，没有使用你的灾难推文数据集的任何标签。你可以将这些GloVe向量中所包含的所有学习迁移到您训练模型所使用的与灾难相关的词汇集上，通过使用单词的更一般的含义。你只需要调整嵌入层的大小，以容纳你希望用来初始化CNN的GloVe嵌入的大小。
- en: Listing 7.22 Make room for GloVE embeddings
  id: totrans-299
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 代码清单 7.22：为GloVE嵌入腾出空间
- en: '[PRE42]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: That’s it. Once PyTorch knows the number of embeddings and their dimensions
    it can allocate RAM to hold the embedding matrix for `num_embedding` rows and
    `embedding_dim` columns. This would train your embeddings from scratch at the
    same time it is training the rest of your CNN. Your domain-specific vocabulary
    and embeddings would be customized for your corpus. But training your embeddings
    from scratch doesn’t take advantage of the fact that words share meaning across
    many domains.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。一旦PyTorch知道嵌入的数量和它们的维度，它就可以分配内存来保存嵌入矩阵，其中有 `num_embedding` 行和 `embedding_dim`
    列。这将同时训练你的嵌入和其余的CNN部分。你的领域特定的词汇和嵌入将根据你的语料库进行自定义。但是，从头开始训练你的嵌入没有利用到单词在许多领域中共享的含义。
- en: If you want your pipeline to be "cross-fit" you can use embedding trained in
    other domains. This "cross training" of word embeddings is called *transfer learning*.
    This gives your Embedding layer a head start on learning the meaning of words
    by using pretrained word embeddings trained on a much broader corpus of text.
    For that, you will need to filter out all the words used in other domains so that
    the vocabulary for your CNN pipeline is based only on the words in your dataset.
    Then you can load the embeddings for those words into your `nn.Embedding` layer.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望你的管道能"跨域适应"，你可以使用在其他领域训练的嵌入。这种词嵌入的"跨训练"被称为*迁移学习*。通过使用在更广泛的文本语料库上训练的预训练词嵌入，这为你的嵌入层提前了解了单词的含义。为此，你需要过滤掉其他领域中使用的所有单词，以便你的CNN管道的词汇仅基于你的数据集中的单词。然后，你可以将这些单词的嵌入加载到你的`nn.Embedding`层中。
- en: Listing 7.23 Load embeddings and align with your vocabulary
  id: totrans-303
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 代码清单 7.23：加载嵌入并与你的词汇对齐
- en: '[PRE43]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: You now have your vocabulary of four thousand tokens converted into a 4000 by
    5 matrix of embeddings. Each row in the `embed` array represents the meaning of
    that vocabulary token with a 50-dimensional vector. And if the GloVe embedding
    doesn’t exist for a token in your vocabulary it will have a vector of zeroes.
    That essentially makes that token useless for understanding the meaning of your
    documents, just like an OOV (out-of-vocabulary) token.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经将 4000 个标记的词汇表转换为一个 4000×5 的嵌入矩阵。`embed`数组中的每一行都表示一个具有 50 维向量的词汇表标记的含义。如果在你的词汇表中一个标记的
    GloVe 嵌入不存在，那么它将有一个全为零的向量。这本质上使得那个标记对于理解你的文档毫无用处，就像一个 OOV（词汇表外）标记一样。
- en: '[PRE44]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: You have taken the top 4,000 most frequent tokens from the tweets. Of those
    4,000 words, 3,834 are available in the smallest GloVE word embeddings vocabulary.
    So you filled in those missing 166 tokens with zero vectors for their unknown
    embeddings. Your model will learn what these words mean and compute their embeddings
    as you train the Embedding layer within your neural network.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经从推文中取出了最常见的 4000 个标记。在这 4000 个词中，最小的 GloVE 词嵌入词汇表中有 3834 个可用的。因此，你用零向量填充了那
    166 个缺失词的未知嵌入。当你在神经网络中训练嵌入层时，你的模型会学习这些词的意义并计算它们的嵌入。
- en: Now that you have a consistent way of identifying tokens with an integer, you
    can load a matrix of GloVe embeddings into your `nn.Embedding` layer.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你有了一种将标记转换为整数的一致方法，你可以将 GloVe 嵌入矩阵加载到你的 `nn.Embedding`层中。
- en: Listing 7.24 Initialize your embedding layer with GloVE vectors
  id: totrans-309
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 代码清单 7.24 初始化使用 GloVE 向量的嵌入层
- en: '[PRE45]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Detecting meaningful patterns
  id: totrans-311
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 检测有意义的模式
- en: How you say something, the order of the words, makes a big difference. You combine
    words to create patterns that mean something significant to you, so that you can
    convey that meaning to someone else.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 你说话的方式、单词的顺序，都很重要。你组合单词以创建对你来说具有重要意义的模式，以便将那个意义传达给其他人。
- en: If you want your machine to be a meaningful natural language processor, it will
    need to be able to detect more than just the presence or absence of particular
    tokens. You want your machine to detect meaningful patterns hidden within word
    sequences.^([[23](#_footnotedef_23 "View footnote.")])
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望你的机器成为一个有意义的自然语言处理器，它需要能够检测到更多不仅仅是特定标记的存在或不存在。你希望你的机器能够检测到隐藏在单词序列中的有意义的模式。^([[23](#_footnotedef_23
    "View footnote.")])
- en: Convolutions are the filters that bring out meaningful patterns from words.
    And the best part is, you don’t have no longer have to hard-code these patterns
    into the convolutional kernel. The training process will search for the best possible
    pattern-matching convolutions for your particular problem. Each time you propagate
    the error from your labeled dataset back through the network (backpropagation),
    the optimizer will adjust the weights in each of your filters so that they get
    better and better at detecting meaning and classifying your text examples.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积是过滤器，它可以从单词中提取有意义的模式。最好的部分是，你不再需要将这些模式硬编码到卷积核中。训练过程将搜索最佳的模式匹配卷积，以解决你遇到的问题。每次将标记数据的错误通过网络向后传递时（反向传播），优化器会调整每个过滤器中的权重，使它们在检测意义和分类文本示例方面变得越来越好。
- en: 7.4.4 Robustifying your CNN with dropout
  id: totrans-315
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.4 使用丢弃进行卷积神经网络的健壮性增强
- en: Most neural networks are susceptible to adversarial examples that trick them
    into outputting incorrect classifications or text. And sometimes neural networks
    are susceptible to changes as straight forward as synonym substitution, misspellings,
    or insertion of slang. Sometimes all it takes is a little "word salad" — nonsensical
    random words — to distract and confuse an NLP algorithm. Humans know how to ignore
    noise and filter out distractors, but machines sometimes have trouble with this.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数神经网络容易受到对抗样本的影响，这些样本会欺骗它们输出错误的分类或文本。有时，神经网络容易受到同义词替换、拼写错误或俚语插入等简单变化的影响。有时候只需要一点“语词沙拉”——无意义的随机词语——就能分散并困惑
    NLP 算法。人类知道如何忽略噪音和过滤干扰，但机器有时会遇到麻烦。
- en: '*Robust NLP* is the study of approaches and techniques for building machines
    that are smart enough to handle unusual text from diverse sources.^([[24](#_footnotedef_24
    "View footnote.")]) In fact, research into robust NLP may uncover paths toward
    artificial general intelligence. Humans are able to learn new words and concepts
    from just a few examples. And we generalize well, not too much and not too little.
    Machines need a little help. And if you can figure out the "secret sauce" that
    makes us humans good at this, then you can encode it into your NLP pipelines.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '*鲁棒化自然语言处理（NLP）*是研究处理来自不同来源的非常规文本的方法和技术。事实上，鲁棒化NLP的研究可能会揭示通向人工通用智能的路径。人类能够从极少的例子中学习新词和概念。而我们的泛化能力很好，既不过多也不过少。机器需要一点帮助。如果你能找出我们人类擅长之处的“秘密酱料”，然后将其编码到NLP流程中，那你就能够让机器具备类似的能力。'
- en: One popular technique for increasing the robustness of neural networks is *random
    dropout*. *Random dropout*, or just *dropout*, has become popular because of its
    ease and effectiveness. Your neural networks will almost always benefit from a
    dropout layer. A dropout layer randomly hides some of the neurons outputs from
    the neurons listening to them. This causes that pathway in your artificial brain
    to go quiet and forces the other neurons to learn from the particular examples
    that are in front of it during that dropout.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 增强神经网络鲁棒性的一种常见技术是*随机丢弃法*。由于其简便性和有效性，*随机丢弃法*或简称*丢弃法*已经变得非常流行。你的神经网络几乎总会从丢弃层中受益。丢弃层会随机隐藏部分神经元的输出，使其不被其他神经元接收。这会导致你人造脑中的某条路径变得静音，并迫使其他神经元在丢弃期间学习当前的特定示例。
- en: It’s counter-intuitive, but dropout helps your neural network to spread the
    learning around. Without a dropout layer, your network will focus on the words
    and patterns and convolutional filters that helped it achieve the greatest accuracy
    boost. But you need your neurons to diversify their patterns so that your network
    can be "robust" to common variations on natural language text.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 这似乎与直觉相悖，但丢弃法有助于使你的神经网络学习更广泛。如果没有丢弃层，你的网络将专注于帮助提高最大准确度的单词、模式和卷积滤波器。但你需要神经元们扩展他们的模式，以便你的网络能够对自然语言文本上的常见变化保持“健壮性”。
- en: The best place in your neural network to install a dropout layer is close to
    the end, just before you run the fully connected linear layer that computes the
    predictions on a batch of data. This vector of weights passing into your linear
    layer are the outputs from your CNN and pooling layers. Each one of these values
    represents a sequence of words, or patterns of meaning and syntax. By hiding some
    of these patterns from your prediction layer, it forces your prediction layer
    to diversify its "thinking." Though your software isn’t really thinking about
    anything, it’s OK to anthropomorphize it a bit, if it helps you develop intuitions
    about why techniques like random dropout can improve your model’s accuracy.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中安装丢弃层的最佳位置是靠近末尾，就在运行完全连接的线性层之前。这个向量通过线性层传递的权重是来自CNN和池化层的输出。每个值代表一系列单词或意义和语法模式。通过隐藏一些模式，迫使你的预测层扩展其“思考”。虽然你的软件并没有真正考虑什么，但如果将其拟人化一点能够帮助你对为什么随机丢弃等技术可以提高模型准确度产生直觉。
- en: 7.5 PyTorch CNN to process disaster toots
  id: totrans-321
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.5 使用 PyTorch CNN 处理灾难推文
- en: Now comes the fun part. You are going to build a real world CNN that can distinguish
    real world news from sensationalism. Your model can help you filter out Tweets
    abiout the culture wars so you can focus on news from real war zones.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 现在进入有趣的部分。你要构造一个真实世界的CNN，可以区分真实新闻和煽动性报道。你的模型可以帮助你过滤掉与文化战争有关的推文，让你专注于来自真实战区的新闻。
- en: First you will see where your new convolution layers fit into the pipeline.
    Then you’ll assemble all the pieces to train a CNN on a dataset of "disaster tweets."
    And if doom scrolling and disaster is not your thing, the CNN is easily adaptable
    to any labeled dataset of tweets. You can even pick a hashtag that you like and
    use that as you target label. Then you can find tweets that match that hashtag
    topic even when the tweeter doesn’t know how to use hashtags.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你将看到新的卷积层在管道中的位置。然后你将组装所有组件，来训练一个基于“灾难推文”数据集的CNN。如果负能量滚动和灾难不是你的菜的话，这个CNN也很容易适应任何带标签的推文数据集。你甚至可以选择一个你喜欢的话题作为目标标签，即使推文的作者不知道如何使用标签，你也可以找到与该标签话题相匹配的推文。
- en: 7.5.1 Network architecture
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.1 网络架构
- en: Here are the processing steps and the corresponding shapes of the tensors for
    each stage of a CNN NLP pipeline. It turns out one of the trickiest things about
    building a new CNN is keeping track of the shaps of your tensors. You need to
    ensure that the shape of the outputs of one layer match the shape of the inputs
    for the next layer will be the same for this example as for previous examples.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是CNN NLP管道的每个阶段的处理步骤和张量的相应形状。构建新CNN中最棘手的事情之一是跟踪您张量的形状。您需要确保一个层的输出形状与下一层的输入形状相匹配，对于此示例与以前的示例相同。
- en: Tokenization ⇒ `(N_, )`
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 令牌化⇒`(N_, )`
- en: Padding ⇒ `(N,)`
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 填充⇒`(N,)`
- en: Embedding ⇒ `(M, N)`
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 嵌入⇒`(M, N)`
- en: Convolution(s) ⇒ `(M, N - K)`
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 卷积(s)⇒`(M, N - K)`
- en: Activation(s) ⇒ `(M, N - K)`
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 激活⇒`(M, N - K)`
- en: Pooling(s) ⇒ `(M, N - K)`
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 池化⇒`(M, N - K)`
- en: Dropout (optional) ⇒ `(M, N - K)`
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 丢弃（可选）⇒`(M, N - K)`
- en: Linear combination ⇒ `(L, )`
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 线性组合⇒`(L, )`
- en: Argmax, softmax or thresholding ⇒ `(L, )`
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Argmax、softmax或阈值化⇒`(L, )`
- en: And
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '`N_` is the number of tokens in your input text.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`N_`是您输入文本中的标记数。'
- en: '`N` is the number of tokens in your padded sequences.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`N`是您填充序列中的标记数。'
- en: '`M` is the number of dimensions in your word embeddings.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`M`是您的单词嵌入中的维度数。'
- en: '`K` is the size of your kernel.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`K`是您的核大小。'
- en: '`L` is the number of class labels or values your want to predict.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`L`是您想要预测的类标签或值的数量。'
- en: Your PyTorch model for a CNN has a few more hyperparameters than you had in
    chapters 5 and 6\. However, just as before, it’s a good idea to set up your hyperparameters
    within the `*init*` constructor of your `CNNTextClassifier` model.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 您的CNN的PyTorch模型比第5和第6章中的要多一些超参数。然而，与以前一样，将超参数设置在`CNNTextClassifier`模型的`*init*`构造函数中是一个好主意。
- en: Listing 7.25 CNN hyperparameters
  id: totrans-342
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.25 CNN超参数
- en: '[PRE46]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Just as for your hand-crafted convolutions earlier in this chapter, the sequence
    length is reduced by each convolutional operation. And the amount of shortening
    depends on the size of the kernel and the stride. The PyTorch documentation for
    a `Conv1d` layer provides this formula and a detailed explanation of the terms.^([[25](#_footnotedef_25
    "View footnote.")])
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 就像本章前面手工制作的卷积一样，每个卷积操作都会减少序列长度。缩短的量取决于内核的大小和步幅。`Conv1d`层的PyTorch文档提供了这个公式和对术语的详细解释。^([[25](#_footnotedef_25
    "查看脚注。")])
- en: '[PRE47]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Your first CNN layer is an `nn.Embedding` layer that converts a sequence of
    word id integers into a sequence of embedding vectors. It has as many rows as
    you have unique tokens in your vocabulary (including the new padding token). And
    it has a column for each dimension of the embedding vectors. You can load these
    embedding vectors from GloVe or any other pretrained embeddings.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 您的第一个CNN层是一个`nn.Embedding`层，它将一系列单词ID整数转换为一系列嵌入向量。它的行数与词汇表中唯一标记的数量相同（包括新的填充标记）。它的每个嵌入向量的维度都有一列。您可以从GloVe或任何其他预训练的嵌入中加载这些嵌入向量。
- en: Listing 7.26 Initialize CNN embedding
  id: totrans-347
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.26 初始化CNN嵌入
- en: '[PRE48]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Next you want to build the convolution and pooling layers. The output size of
    each convolution layer can be used to define a pooling layer whose kernel takes
    up the entire convolutional layer output sequence. This is how you accomplish
    "global" max pooling in PyTorch to produce a single maximum value for each convolutional
    filter (kernel) output. This is what NLP experts like Christopher Manning and
    Yoon Kim do in the research papers of theirs that achieved state-of-the-art performance.^([[26](#_footnotedef_26
    "View footnote.")])^([[27](#_footnotedef_27 "View footnote.")])
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您想构建卷积和池化层。每个卷积层的输出大小可以用来定义一个池化层，其核占据整个卷积层输出序列。这就是您在PyTorch中完成“全局”最大池化的方法，以产生每个卷积滤波器（核）输出的单个最大值。这就是自然语言处理专家如克里斯托弗·曼宁和Yoon
    Kim在他们的研究论文中所做的，这些论文取得了最先进的性能。^([[26](#_footnotedef_26 "查看脚注。")])^([[27](#_footnotedef_27
    "查看脚注。")])
- en: Listing 7.27 Construct convolution and pooling layers
  id: totrans-350
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.27 构建卷积和池化层
- en: '[PRE49]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Unlike the previous examples, you’re going to now create multiple convolution
    and pooling layers. For this example we won’t layer them up as is often done in
    computer vision. Instead you will concatenate the convolution and pooling outputs
    together. This is effective because you’ve limited the dimensionality of your
    convolution and pooling output by performing global max pooling and keeping the
    number of output channels much smaller than the number of embedding dimensions.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 与以前的例子不同，你现在要创建多个卷积和池化层。对于这个例子，我们不会像在计算机视觉中经常做的那样将它们一层层叠加。相反，你将连接卷积和池化的输出。这是有效的，因为你通过执行全局最大池化限制了卷积和池化输出的维度，并保持了输出通道的数量远远小于嵌入维度的数量。
- en: 'You can use print statements to help debug mismatching matrix shapes for each
    layer of your CNN. And you want to make sure you don’t unintentionally create
    too many trainable parameters that cause more overfitting than you’d like: Your
    pooling outputs each contain a sequence length of 1, but they also contain 5 channels
    for the embedding dimensions combined together during convolution. So the concatenated
    and pooled convolution outout is a 5x5 tensor which produces a 25-D linear layer
    for the output tensor that encodes the meaning of each text.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用打印语句来帮助调试 CNN 每一层的矩阵形状不匹配的问题。并且你要确保不会无意间创建太多可训练参数，导致过度拟合超过你的预期：你的池化输出每个包含一个长度为
    1 的序列，但它们也包含了在卷积期间组合在一起的 5 个通道的嵌入维度。因此，连接和池化的卷积输出是一个 5x5 张量，产生了一个 25-D 线性层的输出张量，编码了每个文本的含义。
- en: Listing 7.28 CNN layer shapes
  id: totrans-354
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.28 CNN 层形状
- en: '[PRE50]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: And the end result is a rapidly overfitting language model and text classifier.
    Your model achieves a maximum test accuracy of 73% at epoch 55 and a maximum training
    set accuracy of 81% at the last epoch, epoch 75\. You can accomplish even more
    overfitting by increasing the number of channels for the convolutional layers.
    You usually want to ensure your first training runs accomplish overfitting to
    ensure all your layers are configured correctly and to set an upper bound on the
    accuracy that is achievable on a particular problem or dataset.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果是一个迅速过拟合的语言模型和文本分类器。你的模型在第 55 个时期达到了最大的测试准确率 73%，在最后一个时期，第 75 个时期达到了最大的训练集准确率
    81%。通过增加卷积层的通道数，你甚至可以实现更多的过拟合。通常，你希望确保你的第一次训练运行能够完成过拟合，以确保所有层都正确配置，并为特定问题或数据集设置一个可实现的准确率的上限。
- en: '[PRE51]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'By reducing the number of channels from 5 to 3 for each embedding you can reduce
    the total output dimensionality from 25 to 15\. This will limit the overfitting
    but reduce the convergence rate unless you increase the learning coefficient:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将每个嵌入的通道数从 5 减少到 3，你可以将总输出维度从 25 减少到 15。这将限制过度拟合，但会降低收敛速率，除非你增加学习系数：
- en: '[PRE52]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 7.5.2 Pooling
  id: totrans-360
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.2 池化
- en: Pooling aggregates the data from a large tensor to compress the information
    into fewer values. This is often called a "reduce" operation in the world of "Big
    Data" where the map-reduce software pattern is common. Convolution and pooling
    lend themselves well to the map-reduce software pattern and can be parallelized
    within a GPU automatically using PyTorch. You can even use multi-server HPC (high
    performance computing) systems to speed up your training. But CNNs are so efficient,
    you aren’t likely to need this kind of horsepower.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 池化将大张量中的数据聚合以将信息压缩为较少的值。在“大数据”领域，这通常被称为“减少”操作，其中 map-reduce 软件模式很常见。卷积和池化非常适合
    map-reduce 软件模式，并且可以在 GPU 中自动并行化使用 PyTorch。你甚至可以使用多服务器的 HPC（高性能计算）系统来加速训练。但是 CNN
    是如此高效，你可能不太需要这种计算能力。
- en: 'All the statistics you’re used to calculating on a matrix of data can be useful
    as pooling functions for CNNs:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 你习惯计算的所有矩阵数据上的统计量都可以作为 CNN 的池化函数有用：
- en: '`min`'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min`'
- en: '`max`'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max`'
- en: '`std`'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`std`'
- en: '`sum`'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sum`'
- en: '`mean`'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mean`'
- en: The most common and most successful aggregations
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见和最成功的聚合
- en: 7.5.3 Linear layer
  id: totrans-369
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.3 线性层
- en: The concatenated encodings approach gave you a lot of information about each
    microblog post. The encoding vector had 1856 values. The largest word vectors
    you worked with in chapter 6 were 300 dimensions. And all you really want for
    this particular pipeline is the binary answer to the question "is it news worthy
    or not?"
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 连接编码方法为你提供了关于每条微博的大量信息。编码向量有 1856 个值。你在第 6 章中使用的最大词向量是 300 维。而对于这个特定的流水线，你真正想要的只是对问题“是否新闻值得报道？”的二进制答案。
- en: Do you remember in chapter 6 how you had to when you were trying to get a neural
    network to predict "yes or no" questions about the occurrence or absence of particular
    words? Even though you didn’t really pay attention to the answer to all those
    thousands of questions (one for each word in your vocabulary), it was the same
    problem you have now. So you can use the same approach, a `torch.nn.Linear` layer
    will optimally combine all the pieces of information together from a high dimensional
    vector to answer whatever question you pose it.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 你还记得第6章中当你试图让神经网络预测关于特定单词出现或缺失的“是或否”问题时，你是如何做的吗？尽管你并没有真正关注这几千个问题的答案（词汇表中每个词一个问题），但现在你面临的问题是一样的。所以你可以采用相同的方法，一个`torch.nn.Linear`层将会最优地将来自高维向量的所有信息组合在一起，以回答你提出的任何问题。
- en: So you need to add a Linear layer with as many weights as you have encoding
    dimensions that are being output from your pooling layers.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你需要添加一个线性层，其中包含与从池化层输出的编码维度数量相同的权重。
- en: Listing 7.26 shows the code you can use to calculate the size of the linear
    layer.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 清单7.26 显示了计算线性层大小的代码。
- en: Listing 7.29 Compute the tensor size for the output of a 1D convolution
  id: totrans-374
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单7.29 计算1D卷积输出的张量大小
- en: '[PRE53]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 7.5.4 Getting fit
  id: totrans-376
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.4 得到拟合
- en: Before you can train your CNN you need to tell it how to adjust the weights
    (parameters) with each batch of training data. You need to compute two pieces,
    the slopes of the weights relative to the loss function (the gradient) and an
    estimate of how far to try to descend that slope (the learning rate). For the
    single-layer perceptrons and even the logistic regressions of the previous chapters
    you were able to get away with using some general purpose optimizers like "Adam."
    And you can often set the learning rate to a fixed value for CNNs And those will
    work well for CNNs too. However, if you want to speed up your training you can
    try to find an optimizer that’s a bit more clever about how it adjusts all those
    parameters of your model. Geoffrey Hinton called this approach "rmsprop" because
    he uses the root mean square (RMS) formula to compute the moving average of the
    recent gradients. RMSprop aggregates an exponentially decaying window of the weights
    for each batch of data to improve the estimate of the parameter gradient (slopes)
    and speed up learning.^([[28](#_footnotedef_28 "View footnote.")]) ^([[29](#_footnotedef_29
    "View footnote.")]) It is usually a good bet for backpropagation within a convolutional
    neural network for NLP.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 在你训练卷积神经网络之前，你需要告诉它如何根据每一批训练数据来调整权重（参数）。你需要计算两个部分，权重相对于损失函数（梯度）的斜率，以及尝试下降该斜率的距离（学习率）。在前面章节中的单层感知机甚至逻辑回归中，你可以使用一些通用的优化器如“Adam”来实现。你通常可以为卷积神经网络设置一个固定的学习率，并且这些方法对卷积神经网络也适用。然而，如果你想加快训练速度，可以尝试找到一个更聪明的优化器，它可以更好地调整模型的所有参数。Geoffrey
    Hinton称这种方法为“rmsprop”，因为他使用了均方根（RMS）公式来计算最近梯度的移动平均值。RMSprop对每一批数据聚合一个指数衰减的窗口来改善参数梯度（斜率）的估计并加快学习速度。它通常是卷积神经网络在自然语言处理中反向传播的一个不错选择。
- en: 7.5.5 Hyperparameter Tuning
  id: totrans-378
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.5 超参数调优
- en: Explore the hyperparameter space to see if you can beat my performance. Fernando
    Lopez and others have achieved 80% validation and test set accuracy on this dataset
    using 1-D convolution. There’s likely a lot of room to grow.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 探索超参数空间，看看是否可以超过我的性能。Fernando Lopez和其他人已经使用1-D卷积在这个数据集上实现了80%的验证和测试集准确率。可能还有很大的提升空间。
- en: The nlpia2 package contains a command line script that accepts arguments for
    many of the hyperparameters you might want to adjust. Give it a try and see if
    you can find a more fertile part of the hyperspace universe of possibilities.
    You can see my latest attempt in listing 7.27
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: nlpia2包含一个命令行脚本，它接受许多你可能想要调整的超参数的参数。试一试，看看是否可以找到超参数空间中更丰富的部分。你可以在清单7.27中看到我的最新尝试。
- en: Listing 7.30 Command line script for optimizing hyperparameters
  id: totrans-381
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单7.30 用于优化超参数的命令行脚本
- en: '[PRE54]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Did you notice the `win=True` flag in listing 7.27? That is an Easter Egg or
    cheat code I created for myself within my CNN pipeline. Whenever I discover a
    winning ticket in the "Lottery Ticket Hypothesis" game, I hard code it into my
    pipeline. In order for this to work, you have to keep track of the random seeds
    you use and the exact dataset and software you are using. If you can recreate
    all of these pieces, it’s usually possible to recreate a particularly lucky "draw"
    to build on and improve later as you think of new architecture or parameter tweaks.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 您注意到清单 7.27 中的 `win=True` 标志了吗？这是我在我的 CNN 流水线中为自己创建的一个彩蛋或秘籍代码。每当我在“彩票假设”游戏中发现一个中奖票时，我就会把它硬编码到我的流水线中。为了使其生效，您必须跟踪您使用的随机种子、精确的数据集和软件。如果您能重现所有这些组件，通常可以重新创建一个特别幸运的“抽签”，以便在后续思考新的架构或参数调整时进行改进。
- en: In fact, this winning random number sequence initialized the weights of the
    model so well that the test accuracy started off better than the training set
    accuracy. It took 8 epochs for the training accuracy to overtake the test set
    accuracy. After 16 passes through the dataset (epochs), the model is fit 5% better
    to the training set than the test set.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这个获胜的随机数序列初始化了模型的权重，以至于测试准确性开始时比训练集准确性更好。训练准确性超过测试集准确性需要 8 个时期。在通过数据集进行
    16 次传递（时期）后，模型对训练集的拟合程度比测试集提高了 5%。
- en: If you want to achieve higher test set accuracy and reduce the overfitting,
    you can try adding some regularization or increasing the amount of data ignored
    within the Dropout layer. For most neural networks, dropout ratios of 30% to 50%
    often work well to prevent overfitting without delaying the learning too long.
    A single-layer CNN doesn’t benefit much from dropout ratios above 20%.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想要获得更高的测试集准确性并减少过拟合，您可以尝试添加一些正则化或增加在 Dropout 层中忽略的数据量。对于大多数神经网络来说，30% 到 50%
    的丢弃比率通常可以很好地防止过拟合，而不会延迟学习太久。单层 CNN 并不会因为丢弃比率超过 20% 而受益太多。
- en: Listing 7.31 CNN hyperparameter tuning
  id: totrans-387
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 7.31 CNN 超参数调优
- en: '[PRE56]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Can you find a better combination of hyperparameters to improve this model’s
    accuracy? Don’t expect to achieve much better than 80% test set accuracy, because
    this is a hard problem. Even human readers can’t reliably tell if a tweet represents
    a factual news-worthy disaster or not. After all, other humans (and bots) are
    composing these tweets in an attempt to fool readers. This is an adversarial problem.
    Even a small 1-layer CNN does a decent job.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 您能找到更好的超参数组合来提高此模型的准确性吗？不要期望能够达到比 80% 更好的测试集准确性，因为这是一个困难的问题。即使是人类读者也无法可靠地判断一条推文是否代表了真实的新闻灾难。毕竟，其他人类（和机器人）正在撰写这些推文，试图欺骗读者。这是一个对抗性问题。即使是一个小的单层
    CNN 也能做出体面的工作。
- en: Figure 7.17 Learning curve for the best hyperparamters we found
  id: totrans-390
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 7.17 我们找到的最佳超参数的学习曲线
- en: '![learning curve 87 79](images/learning-curve-87-79.png)'
  id: totrans-391
  prefs: []
  type: TYPE_IMG
  zh: '![学习曲线 87 79](images/learning-curve-87-79.png)'
- en: The key to hyperparameter tuning is to conscientiously record each experiment
    and make thoughtful decisions about the hyperparameter adjustments you make for
    the next experiment. You can automate this decisionmaking with a Bayesian Optimizer.
    But in most cases you can develop your intuition and accomplish faster tuning
    of your hyperparameters if you use your biological neural network to accomplish
    the Bayesian optimization. And if you are curious about the affect of the transpose
    operation on the embedding layer, you can try it both ways to see which works
    best on your problem. But you probably want to follow the experts if you want
    to get state-of-the-art results on hard problems. Don’t believe everything you
    read on the Internet, especially when it comes to CNNs for NLP.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调优的关键是要认真记录每一个实验，并对下一个实验中进行的超参数调整做出深思熟虑的决定。您可以使用贝叶斯优化器自动化这个决策过程。但在大多数情况下，如果您使用生物神经网络来完成贝叶斯优化，您可以培养自己的直觉并更快地调整超参数。如果您对转置操作对嵌入层的影响感兴趣，您可以尝试两种方法，看看哪种在您的问题上效果最好。但是如果您想在困难问题上获得最先进的结果，您可能想听取专家的意见。不要相信互联网上的一切，特别是涉及
    NLP 的 CNN。
- en: 7.6 Test yourself
  id: totrans-393
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.6 自我测试
- en: For a length 3 kernel and an input array of length 8 what is the length of the
    output?
  id: totrans-394
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于长度为 3 的核和长度为 8 的输入数组，输出的长度是多少？
- en: What is the kernel for detecting an "S O S" distress signal (**S**ave **O**ur
    **S**ouls, or **S**ave **O**ur **S**hip) within the secret message audio file
    used in this chapter?
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本章中使用的秘密消息音频文件中，用于检测“SOS”求救信号（**S**ave **O**ur **S**ouls，或 **S**ave **O**ur
    **S**hip）的内核是什么？
- en: What is the best training set accuracy you can achieve after tuning the hyperparameters
    for the news-worthiness microblog post problem?
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整超参数后，你能够为新闻价值微博问题达到的最佳训练集准确率是多少？
- en: How would you extend the model to accommodate an additional class? The `news.csv`
    file, provided in the `nlpia2` package on gitlab contains famous quotes to give
    you another level of profundity to attempt to classify with your CNN.
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你如何扩展模型以容纳额外的类？在 gitlab 上提供的 `nlpia2` 包中的 `news.csv` 文件包含了一些著名的引语，可以让你尝试用你的卷积神经网络进行分类。
- en: 'Write 3 kernels, one each for detecting dots, dashes, and pauses. Write a pooling
    function that *counts* unique occurrences of these symbols. BONUS: Create a system
    of functions that *translates* the secret message audio file into the symbols
    `"."`, `"-"`, and `" "`.'
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写 3 个内核，分别用于检测点、短划线和停顿。编写一个 *计数* 这些符号的唯一出现次数的池化函数。奖励：创建一个将秘密消息音频文件转换成符号 `"."`、`"-"`
    和 `" "` 的函数系统。
- en: Find some hyperparameters (don’t forget about random seeds) that achieve better
    than 80% accuracy on the test set for the disaster tweets dataset.
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到一些超参数（不要忘记随机种子），以在灾难推文数据集的测试集上达到超过 80% 的准确率。
- en: Create a sarcasm detector using a word-based CNN using datasets and examples
    on Hugging Face (huggingface.co). Is is credible that several published papers
    claim 91% accuracy at detecting sarcasm from a single tweet, without context?
    ^([[30](#_footnotedef_30 "View footnote.")]) ^([[31](#_footnotedef_31 "View footnote.")]).
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Hugging Face 上的数据集和示例创建一个基于单词的 CNN 的讽刺检测器（huggingface.co）。有几篇发表的论文声称可以从单个推文中，不需要上下文，检测到
    91% 的讽刺准确率。^([[30](#_footnotedef_30 "查看注释。")]) ^([[31](#_footnotedef_31 "查看注释。")])
- en: 7.7 Summary
  id: totrans-401
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: A convolution is a windowed filter that slides over your sequence of words to
    compress it’s meaning into an encoding vector.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积是一个窗口滤波器，它在你的单词序列上滑动以将其含义压缩为编码向量。
- en: Hand-crafted convolutional filters work great on predictable signals such as
    Morse code, but you will need CNNs that learn their own filters for NLP.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手工制作的卷积滤波器对可预测的信号（如摩尔斯电码）效果很好，但是你需要为自然语言处理训练自己的卷积神经网络来学习它们自己的滤波器。
- en: Neural networks can extract patterns in a sequence of words that other NLP approaches
    would miss.
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络可以提取出一系列单词中的模式，而其他自然语言处理方法可能会错过。
- en: During training, if you sandbag your model a bit with a dropout layer you can
    keep it from overachieving (over fitting) on your training data.
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练过程中，如果你通过使用一个 dropout 层稍微阻碍你的模型，你可以防止它在训练数据上过度表现（过拟合）。
- en: Hyperparameter tuning for neural networks gives you more room to exercise your
    creativity than conventional machine learning models.
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络的超参数调整给你比传统的机器学习模型更多的发挥空间。
- en: You can outperform 90% of bloggers at NLP competitions if your CNNs align the
    embedding dimension with the convolutional channels.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你的卷积神经网络将嵌入维度与卷积通道对齐，你可以在 NLP 竞赛中超过 90% 的博主。
- en: Old-fashioned CNNs may surprise you with their efficiency at solving hard problems
    such as detecting newsworthy tweets.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传统的 CNN 可能会让你惊讶地发现它们在解决诸如检测新闻推文之类的难题时的效率。
- en: '[[1]](#_footnoteref_1) Digits technology description ( [https://digits.com/technology](digits.com.html))'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: Digits 技术描述（ [https://digits.com/technology](digits.com.html)）
- en: '[[2]](#_footnoteref_2) Wired Magazine popularized the concept of data as the
    new oil in a 2014 article by that title ( [https://www.wired.com/insights/2014/07/data-new-oil-digital-economy/](data-new-oil-digital-economy.html))'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 《连线》杂志在一篇 2014 年的文章中提出了数据作为新石油的概念（ [https://www.wired.com/insights/2014/07/data-new-oil-digital-economy/](data-new-oil-digital-economy.html)）
- en: '[[3]](#_footnoteref_3) Google AI blog post on Pathways Language Model, or PaLM,
    ( [https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html](04.html))'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌 AI 博客上的路径语言模型，或称为 PaLM，( [https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html](04.html))
- en: '[[4]](#_footnoteref_4) GPT-J requires at least 48GB of RAM ( [https://huggingface.co/docs/transformers/model_doc/gptj](model_doc.html))'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-J 至少需要 48GB 的 RAM（ [https://huggingface.co/docs/transformers/model_doc/gptj](model_doc.html)）
- en: '[[5]](#_footnoteref_5) "T5 - A Detailed Explanation" by Qiurui Chen ( [http://archive.today/M2EM6](archive.today.html))'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 由陈秋睿撰写的《T5 - 详细解释》（ [http://archive.today/M2EM6](archive.today.html)）
- en: '[[6]](#_footnoteref_6) "Digital image processing" on Wikipedia ( [https://en.wikipedia.org/wiki/Digital_image_processing#Filtering](wiki.html))'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '[维基百科上的数字图像处理](https://en.wikipedia.org/wiki/Digital_image_processing#Filtering)（
    [https://en.wikipedia.org/wiki/Digital_image_processing#Filtering](wiki.html)）'
- en: '[[7]](#_footnoteref_7) "Sobel filter" on Wikipedia ( [https://en.wikipedia.org/wiki/Sobel_operator](wiki.html))'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '[[7]](#_footnoteref_7) 维基百科上的"Sobel filter"（[https://en.wikipedia.org/wiki/Sobel_operator](wiki.html)）'
- en: '[[8]](#_footnoteref_8) "Gaussian filter" ( [https://en.wikipedia.org/wiki/Gaussian_filter](wiki.html))'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '[[8]](#_footnoteref_8) "高斯滤波器"（[https://en.wikipedia.org/wiki/Gaussian_filter](wiki.html)）'
- en: '[[9]](#_footnoteref_9) May 2015, *nature*, "Deep Learning" by Hinton, LeCunn,
    and Benjio ( [https://www.nature.com/articles/nature14539](articles.html))'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '[[9]](#_footnoteref_9) 2015 年 5 月，《自然》杂志，Hinton、LeCunn 和 Benjio 的"深度学习"（[https://www.nature.com/articles/nature14539](articles.html)）'
- en: '[[10]](#_footnoteref_10) "A Brief History of Neural Nets and Deep Learning"
    by Andrey Kurenkov ( [https://www.skynettoday.com/overviews/neural-net-history](overviews.html))'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '[[10]](#_footnoteref_10) Andrey Kurenkov 撰写的"神经网络和深度学习的简要历史"（[https://www.skynettoday.com/overviews/neural-net-history](overviews.html)）'
- en: '[[11]](#_footnoteref_11) SpaCy NER documentation ( [https://spacy.io/universe/project/video-spacys-ner-model](project.html))'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '[[11]](#_footnoteref_11) SpaCy NER 文档（[https://spacy.io/universe/project/video-spacys-ner-model](project.html)）'
- en: '[[12]](#_footnoteref_12) LeCun, Y and Bengio, Y "Convolutional Networks for
    Images, Speech, and Time-series" ( [https://www.iro.umontreal.ca/~lisa/pointeurs/handbook-convo.pdf](pointeurs.html))'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '[[12]](#_footnoteref_12) LeCun, Y 和 Bengio, Y 撰写的"图像、语音和时间序列的卷积网络"（[https://www.iro.umontreal.ca/~lisa/pointeurs/handbook-convo.pdf](pointeurs.html)）'
- en: '[[13]](#_footnoteref_13) Sometimes "feedback sandwich" or "sh-t sandwich."'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '[[13]](#_footnoteref_13) 有时称为"反馈三明治"或"sh-t 三明治"。'
- en: '[[14]](#_footnoteref_14) "An anatomical and functional topography of human
    auditory cortical areas" by Michelle Moerel et al ( [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4114190/](PMC4114190.html))'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '[[14]](#_footnoteref_14) Michelle Moerel 等人撰写的"人类听觉皮层区域的解剖和功能地形学"（[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4114190/](PMC4114190.html)）'
- en: '[[15]](#_footnoteref_15) Mastodon is a community-owned, ad-free social network:
    [https://joinmastodon.org/](joinmastodon.org.html)'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '[[15]](#_footnoteref_15) Mastodon 是一个由社区拥有的、无广告的社交网络：[https://joinmastodon.org/](joinmastodon.org.html)'
- en: '[[16]](#_footnoteref_16) Mastodon is a FOSS ad-free microblogging platform
    similar to Twitter with an open standard API for retrieving NLP datasets ( [https://mastodon.social](.html))'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '[[16]](#_footnoteref_16) Mastodon 是一个类似于 Twitter 的 FOSS 无广告微博平台，具有用于检索 NLP
    数据集的开放标准 API（[https://mastodon.social](.html)）'
- en: '[[17]](#_footnoteref_17) GreenPill is a regenerative economics initiative that
    encourages crypto investors to contribute to public goods ( [https://greenpill.party](.html)).'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '[[17]](#_footnoteref_17) GreenPill 是一个鼓励加密货币投资者为公共产品做出贡献的再生经济倡议（[https://greenpill.party](.html)）。'
- en: '[[18]](#_footnoteref_18) GDFL (GNU Free Documentation License) pt.wikipedia.org
    [https://pt.wikipedia.org/wiki/Zebra#/media/Ficheiro:Zebra_standing_alone_crop.jpg](media.html)'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '[[18]](#_footnoteref_18) GDFL（GNU 自由文档许可证）pt.wikipedia.org [https://pt.wikipedia.org/wiki/Zebra#/media/Ficheiro:Zebra_standing_alone_crop.jpg](media.html)'
- en: '[[19]](#_footnoteref_19) "Morse code" article on Wikipedia ( [https://en.wikipedia.org/wiki/Morse_code](wiki.html))'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '[[19]](#_footnoteref_19) 维基百科上的"Morse code"文章（[https://en.wikipedia.org/wiki/Morse_code](wiki.html)）'
- en: '[[20]](#_footnoteref_20) Wikipedia commons secret message wave file ( [https://upload.wikimedia.org/wikipedia/commons/7/78/1210secretmorzecode.wav](78.html))'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '[[20]](#_footnoteref_20) 维基共享资源中的秘密信息波形文件（[https://upload.wikimedia.org/wikipedia/commons/7/78/1210secretmorzecode.wav](78.html)）'
- en: '[[21]](#_footnoteref_21) "Ham" was originally a pejorative term for ham-fisted
    Morse code "typists" ( [https://en.wikipedia.org/wiki/Amateur_radio#Ham_radio](wiki.html))'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '[[21]](#_footnoteref_21) "Ham" 最初是对于笨拙的摩尔斯电码"打字员"的蔑称（[https://en.wikipedia.org/wiki/Amateur_radio#Ham_radio](wiki.html)）'
- en: '[[22]](#_footnoteref_22) "A Unified Architecture for Natural Language Processing"
    by Ronan Collobert and Jason Weston ( [https://thetalkingmachines.com/sites/default/files/2018-12/unified_nlp.pdf](2018-12.html))'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '[[22]](#_footnoteref_22) Ronan Collobert 和 Jason Weston 撰写的"自然语言处理的统一架构"（[https://thetalkingmachines.com/sites/default/files/2018-12/unified_nlp.pdf](2018-12.html)）'
- en: '[[23]](#_footnoteref_23) *International Association of Facilitators Handbook*,
    [http://mng.bz/xjEg](mng.bz.html)'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '[[23]](#_footnoteref_23) *国际促进者协会手册*，[http://mng.bz/xjEg](mng.bz.html)'
- en: '[[24]](#_footnoteref_24) Robin Jia’s thesis on Robust NLP ( [https://robinjia.github.io/assets/pdf/robinjia_thesis.pdf](pdf.html))
    and his presentation with Kai-Wei Chang, He He and Sameer Singh ( [https://robustnlp-tutorial.github.io](.html))'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '[[24]](#_footnoteref_24) Robin Jia 关于 Robust NLP 的论文（[https://robinjia.github.io/assets/pdf/robinjia_thesis.pdf](pdf.html)）以及他与
    Kai-Wei Chang、He He 和 Sameer Singh 的演讲（[https://robustnlp-tutorial.github.io](.html)）'
- en: '[[25]](#_footnoteref_25) ( [https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html](generated.html))'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '[[25]](#_footnoteref_25)（[https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html](generated.html)）'
- en: '[[26]](#_footnoteref_26) Conv Nets for NLP by Chistopher Manning ( [http://mng.bz/1Meq](mng.bz.html))'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '[[26]](#_footnoteref_26) "自然语言处理中的卷积神经网络" 由 Christopher Manning 撰写（[http://mng.bz/1Meq](mng.bz.html)）'
- en: '[[27]](#_footnoteref_27) "A Sensitivity Analysis of CNNs for Sentence Classification"
    by Ye Zhang and Brian Wallace ( [https://arxiv.org/pdf/1510.03820.pdf](pdf.html))'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '[[27]](#_footnoteref_27) "《CNNs用于句子分类的敏感性分析》" 由 Ye Zhang 和 Brian Wallace 撰写（[https://arxiv.org/pdf/1510.03820.pdf](pdf.html)）'
- en: '[[28]](#_footnoteref_28) Slide 14 "Four ways to speed up machine learning"
    from "Overview of mini‐batch gradient descent" by Hinton ( [https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf](slides.html))'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '[[28]](#_footnoteref_28) 来自 Hinton 的《梯度下降小批量概览》的幻灯片 14 "加速机器学习的四种方法"（[https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf](slides.html)）'
- en: '[[29]](#_footnoteref_29) Ph D thesis "Optimizing Neural Networks that Generate
    Images" by Tijmen Tieleman ( [https://www.cs.toronto.edu/~tijmen/tijmen_thesis.pdf](~tijmen.html))'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: '[[29]](#_footnoteref_29) Tijmen Tieleman 的博士论文《优化生成图像的神经网络》（[https://www.cs.toronto.edu/~tijmen/tijmen_thesis.pdf](~tijmen.html)）'
- en: '[[30]](#_footnoteref_30) 92% is claimed by Ivan Helin for their model on Hugging
    Face ( [https://huggingface.co/helinivan/english-sarcasm-detector](helinivan.html))'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: '[[30]](#_footnoteref_30) Ivan Helin 在 Hugging Face 上声称他们的模型达到了92%的准确率（[https://huggingface.co/helinivan/english-sarcasm-detector](helinivan.html)）'
- en: '[[31]](#_footnoteref_31) 91% is claimed in "A Deeper Look into Sarcastic Tweets
    Using a CNN" by Soujanya Poria et al. ( [https://arxiv.org/abs/1610.08815](abs.html)
    )'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: '[[31]](#_footnoteref_31) "通过CNN深入研究讽刺推文" 由 Soujanya Poria 等人撰写，声称达到了91%的准确率（[https://arxiv.org/abs/1610.08815](abs.html)）'
