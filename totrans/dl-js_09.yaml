- en: 'Chapter 2\. Getting started: Simple linear regression in TensorFlow.js'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 2 章\. 入门：TensorFlow.js 中的简单线性回归
- en: '*This chapter covers*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章内容*'
- en: A minimal example of a neural network for the simple machine-learning task of
    linear regression
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个简单的神经网络的最小示例，用于线性回归这一简单的机器学习任务
- en: Tensors and tensor operations
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张量和张量操作
- en: Basic neural network optimization
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本神经网络优化
- en: Nobody likes to wait, and it’s especially annoying to wait when we don’t know
    how long we’ll have to wait for. Any user experience designer will tell you that
    if you can’t hide the delay, then the next best thing is to give the user a reliable
    estimate of the wait time. Estimating expected delays is a prediction problem,
    and the TensorFlow.js library can be used to build an accurate download-time prediction,
    sensitive to the context and user, enabling us to build clear, reliable experiences
    that respect the user’s time and attention.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 没有人喜欢等待，特别是当我们不知道要等多久时，等待就会变得非常烦人。任何用户体验设计师都会告诉你，如果无法隐藏延迟，那么下一个最好的办法就是给用户一个可靠的等待时间估计。估计预期延迟是一个预测问题，而
    TensorFlow.js 库可以用于构建一个敏感于上下文和用户的准确下载时间预测，从而使我们能够构建清晰、可靠的体验，尊重用户的时间和注意力。
- en: In this chapter, using a simple download-time prediction problem as our motivating
    example, we will introduce the main components of a complete machine-learning
    model. We will cover tensors, modeling, and optimization from a practical point
    of view so that you can build intuitions about what they are, how they work, and
    how to use them appropriately.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，以一个简单的下载时间预测问题作为我们的示例，我们将介绍完整机器学习模型的主要组成部分。我们将从实际角度介绍张量、建模和优化，以便你能够建立对它们是什么、如何工作以及如何适当使用它们的直觉。
- en: A complete understanding of the internals of deep learning—the type a dedicated
    researcher would build over years of study—requires familiarity with many mathematical
    subjects. For the deep-learning practitioner, however, expertise with linear algebra,
    differential calculus, and the statistics of high-dimensional spaces is helpful
    but not necessary, even to build complex, high-performance systems. Our goal in
    this chapter, and throughout this book, is to introduce technical topics as necessary—using
    code, rather than mathematical notation, when possible. We aim to convey an intuitive
    understanding of the machinery and its purpose without requiring domain expertise.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习内部的完全理解——这是一个专注研究者通过多年学习构建的类型——需要熟悉许多数学学科。然而，对于深度学习从业者来说，熟练掌握线性代数、微分计算和高维空间的统计学是有帮助的，但并非必需，即使要构建复杂、高性能的系统也是如此。我们在本章和整本书中的目标是根据需要介绍技术主题——尽可能使用代码，而不是数学符号。我们的目标是传达对机器的直觉理解及其目的，而不需要领域专业知识。
- en: '2.1\. Example 1: Predicting the duration of a download using TensorFlow.js'
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 示例 1：使用 TensorFlow.js 预测下载持续时间
- en: Let’s jump right in! We will construct a bare-minimum neural network that uses
    the TensorFlow.js library (sometimes shortened to tfjs) to predict download times
    given the size of the download. Unless you already have experience with TensorFlow.js
    or similar libraries, you won’t understand everything about this first example
    right away, and that’s fine. Each subject introduced here will be covered in detail
    in the coming chapters, so don’t worry if some parts look arbitrary or magical
    to you! We’ve got to start somewhere. We will begin by writing a short program
    that accepts a file size as input and outputs a predicted time to download the
    file.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！我们将构建一个最小的神经网络，使用 TensorFlow.js 库（有时缩写为 tfjs）来预测给定下载大小的下载时间。除非你已经有 TensorFlow.js
    或类似库的经验，否则你不会立即理解这个第一个示例的所有内容，但没关系。这里介绍的每个主题都将在接下来的章节中详细介绍，所以如果有些部分对你来说看起来是随意的或神奇的，不要担心！我们必须要从某个地方开始。我们将从编写一个接受文件大小作为输入并输出预测的文件下载时间的简短程序开始。
- en: '2.1.1\. Project overview: Duration prediction'
  id: totrans-10
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1\. 项目概述：持续时间预测
- en: When studying a machine-learning system for the first time, you may be intimidated
    by the variety of new concepts and lingo. Therefore, it’s helpful to look at the
    entire workflow first. The general outline of this example is illustrated in [figure
    2.1](#ch02fig01), and it is a pattern that we will see repeated across our examples
    in this book.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当你第一次学习机器学习系统时，可能会因为各种新概念和术语而感到害怕。因此，先看一下整个工作流程是很有帮助的。这个示例的总体概述如 [图 2.1](#ch02fig01)
    所示，并且这是我们在本书中将会反复看到的一种模式。
- en: Figure 2.1\. Overview of the major steps involved in the download-time prediction
    system, our first example
  id: totrans-12
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.1\. 下载时间预测系统的主要步骤概述，我们的第一个例子
- en: '![](02fig01_alt.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](02fig01_alt.jpg)'
- en: 'First, we will access our training data. In machine learning, data can be read
    from disk, downloaded over the network, generated, or simply hard-coded. In this
    example, we take the last approach because it is convenient, and we are dealing
    with only a small amount of data. Second, we will convert the data into tensors,
    so they can be fed to our model. The next step is creating a model, which, as
    we saw in [chapter 1](kindle_split_011.html#ch01), is akin to designing an appropriate
    trainable function: a function mapping input data to things we are trying to predict.
    In this case, the input data and the prediction targets are both numbers. Once
    our model and data are available, we will then train the model, monitoring its
    reported metrics as it goes. Finally, we will use the trained model to make predictions
    on data we haven’t seen yet and measure the model’s accuracy.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将访问我们的训练数据。在机器学习中，数据可以从磁盘中读取，通过网络下载，生成，或者简单地硬编码。在本例中，我们采用了最后一种方法，因为它很方便，并且我们只处理了少量数据。其次，我们将把数据转换成张量，以便将其馈送到我们的模型中。下一步是创建一个模型，就像我们在[第一章](kindle_split_011.html#ch01)中看到的那样，这类似于设计一个适当的可训练函数：一个将输入数据映射到我们试图预测的事物的函数。在这种情况下，输入数据和预测目标都是数字。一旦我们的模型和数据可用，我们将训练模型，监视其报告的指标。最后，我们将使用训练好的模型对我们尚未见过的数据进行预测，并测量模型的准确性。
- en: We will proceed through each of these phases with copy-and-paste runnable code
    snippets and explanations of both the theory and the tools.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过每个阶段的可复制粘贴的可运行代码片段以及对理论和工具的解释来进行。
- en: 2.1.2\. A note on code listings and console interactions
  id: totrans-16
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2\. 有关代码清单和控制台交互的注意事项
- en: Code in this book will be presented in two formats. The first format, the code
    *listing*, presents structural code that you will find in the referenced code
    repositories. Each listing has a title and a number. For example, [listing 2.1](#ch02ex01)
    contains a very short HTML snippet that you could copy verbatim into a file—for
    example, /tmp/tmp.html—on your computer and then open in your web browser at file:///tmp/tmp.html,
    though it won’t do much by itself.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的代码将以两种格式呈现。第一种格式是*代码清单*，展示了您将在引用的代码仓库中找到的结构化代码。每个清单都有一个标题和一个编号。例如，[清单 2.1](#ch02ex01)
    包含了一个非常简短的 HTML 片段，您可以将其逐字复制到一个文件中，例如 /tmp/tmp.html，在您的计算机上然后在您的 Web 浏览器中打开文件:///tmp/tmp.html，尽管它本身不会做太多事情。
- en: 'The second format of code is the *console interaction*. These more informal
    blocks are intended to convey example interactions at a JavaScript REPL,^([[1](#ch02fn1)])
    such as the browser’s JavaScript console (Cmd-Opt-J, Ctrl+Shift+J, or F12 in Chrome,
    but your browser/OS may be different). Console interactions are indicated with
    a preceding greater-than sign, like what we see in Chrome or Firefox, and their
    outputs are presented on the next line, just as in the console. For example, the
    following interaction creates an array and prints the value. The output you see
    at your JavaScript console may be slightly different, but the gist should be the
    same:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种代码格式是*控制台交互*。这些更为非正式的代码块旨在传达在 JavaScript REPL（交互式解释器或 shell）^([[1](#ch02fn1)])
    中的示例交互，例如浏览器的 JavaScript 控制台（在 Chrome 中是 Cmd-Opt-J、Ctrl+Shift+J 或 F12，但您的浏览器/操作系统可能会有所不同）。控制台交互以前导的大于号开头，就像我们在
    Chrome 或 Firefox 中看到的那样，并且它们的输出与控制台中的一样，呈现在下一行。例如，以下交互创建一个数组并打印其值。您在 JavaScript
    控制台中看到的输出可能略有不同，但要点应该是相同的：
- en: ¹
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Read-eval-print-loop, also known as an interactive interpreter or shell. The
    REPL allows us to interact actively with our code to interrogate variables and
    test functions.
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Read-eval-print-loop，也称为交互式解释器或 shell。REPL 允许我们与我们的代码进行积极的交互，以查询变量和测试函数。
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The best way to test, run, and learn from the code listings in this book is
    to clone the referenced repositories and then play with them. During the development
    of this book, we made frequent use of CodePen as a simple, interactive, shareable
    repository ([http://codepen.io](http://codepen.io)). For example, [listing 2.1](#ch02ex01)
    is available for you to play with at codepen.io/tfjs-book/pen/VEVMbx. When you
    navigate to the CodePen, it should run automatically. You should be able to see
    output printed to the console. Click Console at bottom left to open the console.
    If the CodePen doesn’t run automatically, try making a small, inconsequential
    change, such as adding a space to the end, to kick-start it.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中测试、运行和学习代码列表的最佳方式是克隆引用的存储库，然后与其一起玩耍。在本书的开发过程中，我们经常使用 CodePen 作为一个简单、交互式、可共享的存储库（[http://codepen.io](http://codepen.io)）。例如，[列表
    2.1](#ch02ex01) 可供你在 [codepen.io/tfjs-book/pen/VEVMbx](codepen.io/tfjs-book/pen/VEVMbx)
    上玩耍。当你导航到 CodePen 时，它应该会自动运行。你应该能够看到输出打印到控制台。点击左下角的 Console 打开控制台。如果 CodePen 没有自动运行，请尝试进行一个小的、无关紧要的更改，例如在末尾添加一个空格，以启动它。
- en: 'The listings from this section are available in this CodePen collection: codepen
    .io/collection/Xzwavm/. CodePen works well where there is a single JavaScript
    file, but our larger and more structured examples are kept in GitHub repositories,
    which you will see in later examples. For this example, we recommend reading through
    this section and then playing with the associated CodePens in order.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的列表可在此 CodePen 集合中找到：[codepen.io/collection/Xzwavm/](codepen.io/collection/Xzwavm/)。在只有单个
    JavaScript 文件的情况下，CodePen 的效果很好，但我们更大更结构化的示例保存在 GitHub 存储库中，你将在后面的示例中看到。对于这个示例，我们建议你先阅读本节，然后再玩一玩相关的
    CodePen。
- en: 2.1.3\. Creating and formatting the data
  id: totrans-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.3\. 创建和格式化数据
- en: Let’s estimate how long it will take to download a file on a machine, given
    only its size in MB. We’ll first use a pre-created dataset, but, if you’re motivated,
    you can create a similar dataset, modeling your own system’s network statistics.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们估计一下在一台机器上下载一个文件需要多长时间，只给出其大小（以 MB 为单位）。我们首先使用一个预先创建的数据集，但如果你有动力的话，你可以创建一个类似的数据集，模拟你自己系统的网络统计信息。
- en: Listing 2.1\. Hard-coding the training and test data (from CodePen 2-a)
  id: totrans-27
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.1\. 硬编码训练和测试数据（来自 CodePen 2-a）
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the previous HTML code listing, we’ve chosen to explicitly include the `<script>`
    tags, illustrating how to load the most recent version of the TensorFlow.js library
    using the `@latest` suffix (at the time of writing, this code ran with tfjs 0.13.5).
    We will go into more detail later about different ways to import TensorFlow.js
    into your application, but going forward, the `<script>` tags will be assumed.
    The first script loads the TensorFlow package and defines the symbol `tf`, which
    provides a way to refer to names in TensorFlow. For example, `tf.add()` refers
    to the TensorFlow add operation, which adds two tensors. Going forward, we will
    assume that the `tf` symbol is loaded and available in the global namespace by,
    for example, sourcing the TensorFlow.js script as previously.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述 HTML 代码列表中，我们选择了显式包含 `<script>` 标签，演示了如何使用 `@latest` 后缀加载最新版本的 TensorFlow.js
    库（在撰写本文时，此代码与 tfjs 0.13.5 兼容）。我们将在后面详细介绍不同的方式将 TensorFlow.js 导入到你的应用程序中，但在以后的过程中，我们将假定
    `<script>` 标签已经包含在内。第一个脚本加载 TensorFlow 包并定义了符号 `tf`，它提供了一种引用 TensorFlow 中名称的方式。例如，`tf.add()`
    指的是 TensorFlow 加法操作，用于将两个张量相加。在以后的过程中，我们将假设 `tf` 符号已经加载并在全局命名空间中可用，例如，通过之前的方式引用
    TensorFlow.js 脚本。
- en: '[Listing 2.1](#ch02ex01) creates two constants, `trainData` and `testData`,
    each representing 20 samples of how long it took to download a file (`timeSec`)
    and the size of that file (`sizeMB`). The elements in `sizeMB` and those in `timeSec`
    have one-to-one correspondence. For example, the first element of `sizeMB` in
    `trainData` is 0.080 MB, and downloading that file took 0.135 seconds—that is,
    the first element of `timeSec`—and so forth. Our goal in this example will be
    to estimate `timeSec`, given just `sizeMB`. In this first example, we are creating
    the data directly by hard-coding it in our code. This approach is expedient for
    this simple example but will become unwieldy very quickly when the size of the
    dataset grows. Future examples will illustrate how to stream data from external
    storage or over the network.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表2.1](#ch02ex01)创建了两个常量，`trainData`和`testData`，分别表示下载文件所需的时间（`timeSec`）和文件大小（`sizeMB`）的20个样本。`sizeMB`中的元素与`timeSec`中的元素一一对应。例如，在`trainData`中，`sizeMB`的第一个元素为0.080
    MB，并且下载该文件所需时间为0.135秒，即 `timeSec`的第一个元素，依此类推。在这个示例中，我们通过在代码中直接编写数据来创建数据。这种方法在这个简单的示例中是可行的，但是当数据集的大小增长时，它很快就会变得难以管理。未来的示例将演示如何从外部存储或网络上的流数据。'
- en: Back to the data. From the plot in [figure 2.2](#ch02fig02), we can see that
    there is a very predictable, if imperfect, relationship between the size and download
    time. Data in real life is noisy, but it looks like we should be able to make
    a pretty good linear estimate of the duration given the file size. Judging by
    eye, the duration should be about 0.1 seconds when the file size is zero and then
    grow at about 0.07 seconds for each additional MB. Recall from [chapter 1](kindle_split_011.html#ch01)
    that each input-output pair is sometimes called an *example*. The output is often
    referred to as the *target*, while the elements of the input are often called
    the *features*. In our case here, each of our 40 examples has exactly one feature,
    `sizeMB`, and a numeric target, `timeSec`.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 回到数据上。根据[图2.2](#ch02fig02)中的绘图，我们可以看到文件大小和下载时间之间存在着可预测但并不完美的关系。现实生活中的数据是嘈杂的，但看起来我们应该能够对文件大小给出一个相当好的线性估计值。根据视觉判断，当文件大小为零时，持续时间应该约为0.1秒，然后每增加1MB，持续时间大约增加0.07秒。请回忆起[第1章](kindle_split_011.html#ch01)中提到的，每个输入-输出对有时被称为*样本*。输出通常被称为*目标*，而输入的元素通常被称为*特征*。在我们的例子中，我们的40个样本中每个样本恰好有一个特征`sizeMB`和一个数值目标`timeSec`。
- en: Figure 2.2\. Measured download duration versus file size. If you are interested,
    at this point, in how to create plots like this, the code is listed in CodePen
    [codepen.io/tfjs-book/pen/dgQVze](http://codepen.io/tfjs-book/pen/dgQVze).
  id: totrans-32
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.2\. 下载持续时间与文件大小的测量数据。如果您对如何创建类似的绘图感兴趣，可以参考CodePen上的代码[codepen.io/tfjs-book/pen/dgQVze](http://codepen.io/tfjs-book/pen/dgQVze)。
- en: '![](02fig02_alt.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](02fig02_alt.jpg)'
- en: In [listing 2.1](#ch02ex01), you might have noticed that the data is split into
    two subsets, namely `trainData` and `testData`. `trainData` is the training set.
    It contains the examples the model will be trained on. `testData` is the test
    set. We will use it to judge how well the model is trained after the training
    is complete. If we trained and evaluated using the exact same data, it would be
    like taking a test after having already seen the answers. In the most extreme
    case, the model could theoretically memorize the `timeSec` value for each `sizeMB`
    in the training data—not a very good learning algorithm. The result would not
    be a good judge of future performance because it is unlikely that the values of
    the future input features will all be exactly the same as the ones the model has
    been trained on.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表2.1](#ch02ex01)中，您可能已经注意到数据被分为两个子集，即`trainData`和`testData`。`trainData`是训练集，它包含了模型将会在上面进行训练的样本。`testData`是测试集，我们将使用它来判断模型在训练完成后的效果如何。如果我们使用完全相同的数据进行训练和评估，那就像是在已经看到答案之后进行考试。在最极端的情况下，模型可以从训练数据中理论上记住每个`sizeMB`对应的`timeSec`
    值，这不是一个很好的学习算法。结果可能不是对未来性能的很好评估，因为未来输入特征的值很可能与模型进行训练时的值完全相同。
- en: Therefore, the workflow will be as follows. First we’ll fit the neural network
    on the training data to make accurate predictions of `timeSec` given `sizeMB`.
    Then, we’ll ask the network to produce predictions for `sizeMB` using the testing
    data, and we’ll measure how close those predictions are to `timeSec`. But first,
    we’ll have to convert this data into a format that TensorFlow.js will understand,
    and this will be our first example usage of tensors. The code in [listing 2.2](#ch02ex02)
    shows the first usage of functions under the `tf.*` namespace you will see in
    this book. Here, we see methods for converting data stored in raw JavaScript data
    structures into tensors.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，工作流程如下。首先，我们将在训练数据上拟合神经网络，以便准确预测`timeSec`给定`sizeMB`。然后，我们将要求网络使用测试数据为`sizeMB`生成预测，并测量这些预测与`timeSec`的接近程度。但首先，我们必须将此数据转换为
    TensorFlow.js 能够理解的格式，这将是我们对张量的第一个示例用法。[代码清单2.2](#ch02ex02)中的代码展示了在本书中你将看到的`tf.*`命名空间下的函数的第一个用法。在这里，我们看到了将存储在原始
    JavaScript 数据结构中的数据转换为张量的方法。
- en: Although the usage is pretty straightforward, those readers who wish to gain
    a firmer grounding in these APIs should read [appendix B](kindle_split_030.html#app02),
    which covers not only tensor-creation functions such as `tf.tensor2d()`, but also
    functions that perform operations transforming and combining tensors, and patterns
    of how common real-world data types, such as images and videos, are conventionally
    packed into tensors. We do not dive deeply into the low-level API in the main
    text because the material is somewhat dry and not tied to specific example problems.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用方法非常简单明了，但那些希望在这些 API 中获得更牢固基础的读者应该阅读[附录B](kindle_split_030.html#app02)，其中不仅涵盖了诸如`tf.tensor2d()`之类的张量创建函数，还涉及执行操作转换和合并张量的函数，以及常见的真实世界数据类型（如图像和视频）如何被惯例地打包成张量的模式。我们在主要文本中没有深入研究底层
    API，因为这些材料有些枯燥，并且与具体的示例问题无关。
- en: Listing 2.2\. Converting data into tensors (from CodePen 2-b)
  id: totrans-37
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 代码清单2.2\. 将数据转换为张量（来自 CodePen 2-b）
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '***1*** The [20, 1] here is the tensor’s “shape.” More will be explained later,
    but here this shape means we want to interpret the list of numbers as 20 samples,
    where each sample is 1 number. If the shape is obvious from, for example, the
    structure of the data array, this argument can be left out.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 这里的[20, 1]是张量的“形状”。稍后会有更多解释，但在这里这个形状意味着我们希望将数字列表解释为20个样本，每个样本是1个数字。如果形状从数据数组的结构中明显，则可以省略此参数。'
- en: 'In general, all current machine-learning systems use tensors as their basic
    data structure. Tensors are fundamental to the field—so fundamental that TensorFlow
    and TensorFlow.js are named after them. A quick reminder from [chapter 1](kindle_split_011.html#ch01):
    at its core, a tensor is a container for data—almost always numerical data. So,
    it can be thought of as a container for numbers. You may already be familiar with
    vectors and matrices, which are 1D and 2D tensors, respectively. Tensors are a
    generalization of matrices to an arbitrary number of dimensions. The number of
    dimensions and size of each dimension is called the tensor’s *shape*. For instance,
    a 3 × 4 matrix is a tensor with shape `[3, 4]`. A vector of length 10 is a 1D
    tensor with shape `[10]`.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，所有当前的机器学习系统都使用张量作为它们的基本数据结构。张量对于该领域是至关重要的——以至于 TensorFlow 和 TensorFlow.js
    都以它们命名。从[第1章](kindle_split_011.html#ch01)快速提醒：在其核心，张量是数据的容器——几乎总是数字数据。因此，它可以被认为是数字的容器。你可能已经熟悉向量和矩阵，它们分别是1D和2D张量。张量是矩阵向任意维度的泛化。张量的维数和每个维度的大小称为张量的*形状*。例如，一个3
    × 4矩阵是一个形状为`[3, 4]`的张量。长度为10的向量是一个形状为`[10]`的1D张量。
- en: In the context of tensors, a dimension is often called an *axis*. In TensorFlow.js,
    tensors are the common representation that lets components communicate and work
    with each other, whether on CPU, GPU, or other hardware. We will have more to
    say about tensors and their common use cases as the need arises, but for now,
    let’s continue with our prediction project.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在张量的上下文中，维度通常被称为*轴*。在 TensorFlow.js 中，张量是让组件之间通信和协同工作的常见表示，无论是在 CPU、GPU 还是其他硬件上。随着需求的出现，我们将对张量及其常见用例有更多介绍，但现在，让我们继续进行我们的预测项目。
- en: 2.1.4\. Defining a simple model
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.4\. 定义一个简单的模型
- en: In the context of deep learning, the function from input features to targets
    is known as a *mode**l*. The model function takes features, runs a computation,
    and produces predictions. The model we are building here is a function that takes
    a file size as input and outputs durations (see [figure 2.2](#ch02fig02)). In
    deep-learning parlance, sometimes we use *network* as a synonym for model. Our
    first model will be an implementation of *linear regression*.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习的上下文中，从输入特征到目标的函数称为*模型*。模型函数接受特征，运行计算，并产生预测。我们正在构建的模型是一个接受文件大小作为输入并输出持续时间的函数（参见[图
    2.2](#ch02fig02)）。在深度学习术语中，有时我们将*网络*用作模型的同义词。我们的第一个模型将是*线性回归*的实现。
- en: '*Regression*, in the context of machine learning, means that the model will
    output real-valued numbers and attempt to match the training targets; this is
    opposed to classification, which outputs choices from a set of options. In a regression
    task, a model that outputs numbers closer to the target is better than a model
    that outputs numbers farther away. If our model predicts that a 1 MB file will
    take about 0.15 seconds, that’s better (as we can see from [figure 2.2](#ch02fig02))
    than if our model predicts that a 1 MB file will take about 600 seconds.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*回归*，在机器学习的上下文中，意味着模型将输出实值，并尝试匹配训练目标；这与分类相反，后者输出来自一组选项的选择。在回归任务中，模型输出的数字越接近目标越好。如果我们的模型预测一个1
    MB文件大约需要0.15秒，那就比预测一个1 MB文件需要约600秒要好（正如我们从[图 2.2](#ch02fig02)中看到的）。'
- en: Linear regression is a specific type of regression in which the output, as a
    function of the input, can be illustrated as a straight line (or, by analogy,
    a flat plane in a higher-dimensional space when there are multiple input features).
    An important property of models is that they are *tunable*. This means that the
    input-output computation can be adjusted. We use this property to tune the model
    to better “fit” the data. In the linear case, the model input-output relationship
    is always a straight line, but we can adjust the slope and y-intercept.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归是一种特定类型的回归，其中输出作为输入的函数可以被表示为一条直线（或者类比为在存在多个输入特征时的高维空间中的一个平面）。模型的一个重要特性是它们是*可调的*。这意味着输入-输出计算可以被调整。我们利用这个特性来调整模型以更好地“拟合”数据。在线性情况下，模型的输入-输出关系总是一条直线，但我们可以调整斜率和y截距。
- en: Let’s build our first network to get a feel for this.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建我们的第一个网络来感受一下。
- en: Listing 2.3\. Constructing a linear regression model (from CodePen 2-c)
  id: totrans-47
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 代码清单 2.3 构建线性回归模型（来自 CodePen 2-c）
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The core building block of neural networks is the *layer*, a data-processing
    module that you can think of as a tunable function from tensors to tensors. Here,
    our network consists of a single dense layer. This layer has a constraint on the
    shape of the input tensor, as defined by the parameter `inputShape: [1]`. Here,
    it means that the layer is expecting input in the form of a 1D tensor with exactly
    one value. The output from the dense layer is always a 1D tensor for each example,
    but the size of that dimension is controlled by the `units` configuration parameter.
    In this case, we want just one output number because we are trying to predict
    exactly one number, namely the `timeSec`.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '神经网络的核心构建模块是*层*，一个你可以将其视为从张量到张量的可调函数的数据处理模块。在这里，我们的网络由一个单一的密集层组成。该层对输入张量的形状有约束，由参数`inputShape:
    [1]`定义。在这里，它意味着该层期望以一维张量形式接收输入，其中恰好有一个值。来自密集层的输出始终是每个示例的一维张量，但该维度的大小由`units`配置参数控制。在这种情况下，我们只需要一个输出数字，因为我们试图预测的恰好是一个数字，即`timeSec`。'
- en: 'At its core, the dense layer is a tunable multiply-add between each input and
    each output. Since there is only one input and one output, this model is the simple
    `y = m * x + b` linear equation you may recall from high school math. The dense
    layer internally calls `m` the *kernel* and `b` the *bias*, as illustrated in
    [figure 2.3](#ch02fig03). In this case, we have constructed a linear model for
    the relation between the input (`sizeMB`) and the output (`timeSec`):'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 核心部分，密集层是每个输入与每个输出之间的可调整乘加。由于只有一个输入和一个输出，这个模型就是你可能从高中数学中记得的简单的`y = m * x + b`线性方程。密集层内部将`m`称为*核*，将`b`称为*偏置*，如[图
    2.3](#ch02fig03)所示。在这种情况下，我们构建了一个关于输入(`sizeMB`)和输出(`timeSec`)之间关系的线性模型：
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Figure 2.3\. An illustration of our simple linear-regression model. The model
    has exactly one layer. The model’s tunable parameters (or weights), the kernel
    and bias, are shown within the dense layer.
  id: totrans-52
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.3\. 我们简单线性回归模型的示意图。该模型只有一个层。模型的可调参数（或权重），即核函数和偏差，显示在密集层内部。
- en: '![](02fig03_alt.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](02fig03_alt.jpg)'
- en: 'There are four terms in this equation. Two of them are fixed as far as model
    training is concerned: the values of `sizeMB` and `timeSec` are determined by
    the training data (see [listing 2.1](#ch02ex01)). The other two terms, the kernel
    and bias, are the model’s parameters. Their values are randomly chosen when the
    model is created. Those random values will not give good predictions of download
    duration. In order for decent predictions to happen, we must search for good values
    of the kernel and bias by allowing the model to learn from data. This search is
    the *training process*.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中有四个项。就模型训练而言，其中两个是固定的：`sizeMB` 和 `timeSec` 的值由训练数据确定（见 [listing 2.1](#ch02ex01)）。另外两个项，即核函数和偏差，是模型的参数。它们的值在模型创建时是随机选择的。这些随机值不能很好地预测下载持续时间。为了进行良好的预测，我们必须通过允许模型从数据中学习来搜索核函数和偏差的良好值。这个搜索过程就是*训练过程*。
- en: 'To find a good setting for the kernel and bias (collectively, the *weights*)
    we need two things:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到核函数和偏差（统称为*权重*）的良好设置，我们需要两样东西：
- en: A measure that tells us how well we are doing at a given setting of the weights
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个告诉我们在给定权重设置下我们做得有多好的度量
- en: A method to update the weights’ values so that next time we will do better than
    we currently are doing, according to the measure previously mentioned
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种方法来更新权重的值，以便下次我们的表现比当前更好，根据先前提到的度量
- en: This brings us to the next step in solving the linear-regression problem. To
    make the network ready for training, we need to pick the measure and the update
    method, which correspond to the two required items listed previously. This is
    done as part of what TensorFlow.js calls the *model compilation* step, which takes
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这将引导我们解决线性回归问题的下一步。为了使网络准备好进行训练，我们需要选择度量和更新方法，这对应于前面列出的两个必需项。这是 TensorFlow.js
    称为*模型编译*步骤的一部分，它采取
- en: A *loss function*—An error measurement. This is how the network measures its
    performance on the training data and steers itself in the right direction. Lower
    loss is better. As we train, we should be able to plot the loss over time and
    see it going down. If our model trains for a long while, and the loss is not decreasing,
    it could mean that our model is not learning to fit the data. Over the course
    of this book, you will learn to debug problems like this.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个*损失函数*—一个错误度量。这是网络在训练数据上衡量自己性能并使自己朝着正确方向前进的方式。更低的损失更好。当我们训练时，我们应该能够绘制随时间变化的损失并看到它下降。如果我们的模型训练了很长时间，而损失并没有减少，这可能意味着我们的模型没有学会拟合数据。在本书的过程中，您将学会解决此类问题。
- en: An *optimizer*—The algorithm by which the network will update its weights (kernel
    and bias, in this case) based on the data and the loss function.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个*优化器*—根据数据和损失函数，网络将如何更新其权重（在本例中为核函数和偏差）的算法。
- en: The exact purpose of the loss function and the optimizer, and how to make good
    choices for them, will be explored thoroughly throughout the next couple of chapters.
    But for now, the following choices will do.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数和优化器的确切目的，以及如何为它们做出良好选择，将在接下来的几章中进行彻底探讨。但现在，以下选择就足够了。
- en: 'Listing 2.4\. Configuring training options: model compilation (from CodePen
    2-c)'
  id: totrans-62
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 代码清单 2.4\. 配置训练选项：模型编译（来自 CodePen 2-c）
- en: '[PRE5]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We call the `compile` method on our model, specifying `''sgd''` as our optimizer
    and `''meanAbsoluteError''` as our loss. `''meanAbsoluteError''` means that our
    loss function will calculate how far our predictions are from the targets, take
    their absolute values (making them all positive), and then return the average
    of those values:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在模型上调用`compile`方法，指定`'sgd'`作为我们的优化器，`'meanAbsoluteError'`作为我们的损失。`'meanAbsoluteError'`表示我们的损失函数将计算我们的预测与目标的距离，取其绝对值（使它们全部为正数），然后返回这些值的平均值：
- en: '[PRE6]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: For example, given
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，给定
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: then,
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: If our model makes very bad predictions that are very far from the targets,
    then the `meanAbsoluteError` will be very large. In contrast, the best we could
    possibly do is to get every prediction exactly right, in which case the difference
    between our model output and the targets would be zero, and therefore the loss
    (the `meanAbsoluteError`) would be zero.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的模型做出非常糟糕的预测，与目标差距很大，那么`meanAbsoluteError`将非常大。相反，我们可能做的最好的事情是准确预测每一个，这样我们的模型输出和目标之间的差异将为零，因此损失（`meanAbsoluteError`）将为零。
- en: The `sgd` in [listing 2.4](#ch02ex04) stands for *stochastic gradient descent*,
    which we will describe a bit more in [section 2.2](#ch02lev1sec2). Briefly, it
    means that we will use calculus to determine what adjustments we should make to
    the weights in order to reduce the loss; then we will make those adjustments and
    repeat the process.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在[list 2.4](#ch02ex04)中的`sgd`代表*随机梯度下降*，我们将在[section 2.2](#ch02lev1sec2)中稍作描述。简而言之，这意味着我们将使用微积分来确定应该对权重进行哪些调整以减少损失；然后我们将进行这些调整并重复该过程。
- en: Our model is now ready to be fit to our training data.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型现在已经准备好适应我们的训练数据了。
- en: 2.1.5\. Fitting the model to the training data
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.5\. 将模型拟合到训练数据
- en: Training a model in TensorFlow.js is done by calling the model’s `fit()` method.
    We fit the model to the training data. Here, we pass in the `sizeMB` tensor as
    our input and the `timeSec` tensor as our desired output. We also pass in a configuration
    object with an `epochs` field that specifies that we would like to go through
    our training data exactly 10 times. In deep learning, each iteration through the
    complete training set is called an *epoch*.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow.js中训练模型是通过调用模型的`fit()`方法来完成的。我们将模型与训练数据拟合。在这里，我们将`sizeMB`张量作为我们的输入，将`timeSec`张量作为我们期望的输出。我们还传入一个配置对象，其中包含一个`epochs`字段，该字段指定我们想要完全遍历我们的训练数据10次。在深度学习中，通过完整训练集的每次迭代称为*epoch*。
- en: Listing 2.5\. Fitting a linear regression model (from CodePen 2-c)
  id: totrans-75
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: list 2.5\. 拟合线性回归模型（来自CodePen 2-c）
- en: '[PRE9]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The `fit()` method can often be long-running, lasting for seconds or minutes.
    Therefore, we utilize the *async/await* feature of ES2017/ES8 so that this function
    can be used in a way that does not block the main UI thread when running in the
    browser. This is similar to other potentially long-running functions in JavaScript,
    such as `async fetch`. Here, we wait for the `fit()` call to finish before going
    on, using the Immediately Invoked Async Function Expression^([[2](#ch02fn2)])
    pattern, but future examples will train in the background while doing other work
    in the foreground thread.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`fit()`方法通常运行时间较长，持续几秒钟或几分钟。因此，我们利用ES2017/ES8的*async/await*特性，以便在浏览器中运行时该函数不会阻塞主UI线程。这与JavaScript中其他可能运行时间较长的函数类似，例如`async
    fetch`。在这里，我们等待`fit()`调用完成后再继续进行，使用立即调用的异步函数表达式^([[2](#ch02fn2)])模式，但未来的示例将在前台线程中进行其他工作的同时在后台线程中进行训练。'
- en: ²
  id: totrans-78
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ²
- en: ''
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For more on Immediately Invoked Function Expressions, see [http://mng.bz/RPOZ](http://mng.bz/RPOZ).
  id: totrans-80
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 有关立即调用的函数表达式的更多信息，请参见[http://mng.bz/RPOZ](http://mng.bz/RPOZ)。
- en: Once our model has completed fitting, we will want to see whether it worked.
    Crucially, we will evaluate the model on data that was not used during training.
    This theme of separating test data from training data (and hence avoiding training
    on the test data) is something that will come up over and over in this book. It
    is an important part of the machine-learning workflow that you should internalize.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们的模型完成拟合，我们就会想要看看它是否起作用。至关重要的是，我们将在训练期间未使用的数据上评估模型。在本书中，将反复出现将测试数据与训练数据分离（因此避免在测试数据上训练）的主题。这是机器学习工作流程的重要部分，你应该内化。
- en: 'The model’s `evaluate()` method calculates the loss function as applied to
    the provided example features and targets. It is similar to the `fit()` method
    in that it calculates the same loss, but `evaluate()` does not update the model’s
    weights. We use `evaluate()` to estimate the quality of the model on the test
    data, so as to get an idea about how the model would perform in the future application:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的`evaluate()`方法计算应用于提供的示例特征和目标的损失函数。它与`fit()`方法类似，因为它计算相同的损失，但`evaluate()`不会更新模型的权重。我们使用`evaluate()`来估计模型在测试数据上的质量，以便了解模型在将来应用中的表现：
- en: '[PRE10]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Here, we see that the loss, averaged across the test data, is about 0.318\.
    Given that, by default, models are trained from a random initial state, you will
    get a different value. Another way to say the same thing is that the mean absolute
    error (MAE) of this model is just over 0.3 seconds. Is this good? Is it better
    than just estimating a constant? One good constant we could choose is the average
    delay. Let’s see what kind of error that would get, using TensorFlow.js’s support
    for mathematical operations on tensors. First, we’ll compute the average download
    time, calculated over our training set:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到损失在测试数据上平均约为 0.318。考虑到，默认情况下，模型是从随机初始状态训练的，你会得到不同的值。另一种说法是，该模型的平均绝对误差（MAE）略高于
    0.3 秒。这个好吗？比只估算一个常量好吗？我们可以选择一个好的常量是平均延迟。让我们看看使用这个常量会得到什么样的误差，使用 TensorFlow.js
    对张量进行数学运算的支持。首先，我们将计算在训练集上计算的平均下载时间：
- en: '[PRE11]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, let’s calculate the `meanAbsoluteError` by hand. MAE is simply the average
    value of how far our prediction was from the actual value. We’ll use `tf.sub()`
    to calculate the difference between the test targets and our (constant) prediction
    and `tf.abs()` to take the absolute value (since sometimes we’ll be too low and
    other times too high), and then take the average with `tf.mean`:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们手动计算 `meanAbsoluteError`。MAE 简单地是我们的预测值与实际值之间的平均差值。我们将使用 `tf.sub()` 计算测试目标与我们（常量）预测之间的差值，并使用
    `tf.abs()` 取绝对值（因为有时我们会偏低，有时偏高），然后使用 `tf.mean` 求平均值：
- en: '[PRE12]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: See [info box 2.1](#ch02sb01) for how to perform the same computation using
    the concise chaining API.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 请参见[信息框 2.1](#ch02sb01)了解如何使用简洁的链式 API 执行相同的计算。
- en: '|  |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Tensor chaining API**'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**张量链式 API**'
- en: 'In addition to the standard API, in which tensor functions are available under
    the `tf` namespace, most tensor functions are also available from the tensor objects
    themselves, allowing you to write in a chaining style if you prefer. The following
    code is functionally identical to the `meanAbsoluteError` computation in the main
    text:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 除了标准 API 外，在 `tf` 命名空间下可用的张量函数之外，大多数张量函数也可以直接从张量对象本身获得，如果你喜欢，可以采用链式编程风格进行编写。下面的代码在功能上与主文中的
    `meanAbsoluteError` 计算完全相同：
- en: '[PRE13]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '|  |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'It seems that the average delay is about 0.295 seconds and that always guessing
    the average value gives a better estimate than our network does. This means our
    model’s accuracy is even worse than that of a commonsense, trivial approach! Can
    we do better? It’s possible that we haven’t trained for enough epochs. Remember
    that during training, the values of the kernel and bias are updated step-by-step.
    In this case, each epoch is a step. If the model is trained only for a small number
    of epochs (steps), the parameter values may not have a chance to get close to
    the optimum. Let’s train our model a few more cycles and evaluate again:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来平均延迟约为 0.295 秒，总是猜测平均值比我们的网络更好地估计。这意味着我们的模型准确性甚至比一个常识性的、平凡的方法还要差！我们能做得更好吗？可能是我们训练的周期不够。请记住，在训练期间，核心和偏置的值是逐步更新的。在这种情况下，每个周期都是一步。如果模型只训练了少数周期（步骤），参数值可能没有机会接近最优值。让我们再训练几个周期，然后重新评估：
- en: '[PRE14]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '***1*** Be sure to wait for the promise returned from model.fit to resolve
    before executing model.evaluate.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 确保在执行 model.evaluate 之前等待 model.fit 返回的 promise 解析。'
- en: Much better! It seems we were previously *underfitting*, meaning our model hadn’t
    been adapted enough to the training data. Now our estimates are within 0.05 seconds,
    on average. We are four times more accurate than naively guessing the mean. In
    this book, we will offer guidance about how to avoid underfitting, as well as
    the more insidious problem of *overfitting*, where the model is tuned *too much*
    to the training data and doesn’t generalize well to data it hasn’t seen!
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 好多了！看起来我们之前是*欠拟合*，意味着我们的模型还没有足够地适应训练数据。现在我们的估计平均在 0.05 秒之内。我们比简单地猜测均值要准确四倍。在本书中，我们将提供关于如何避免欠拟合的指导，以及更隐蔽的*过拟合*问题的解决方法，过拟合是指模型对训练数据调整*过多*，导致在未见过的数据上泛化能力较差！
- en: 2.1.6\. Using our trained model to make predictions
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.6 使用我们训练的模型进行预测
- en: 'OK, great! We now have a model that can make accurate predictions of download
    time given an input size, but how do we use it? The answer is the model’s `predict()`
    method:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，太棒了！现在我们有了一个能够根据输入大小准确预测下载时间的模型，但我们如何使用它呢？答案是模型的 `predict()` 方法：
- en: '[PRE15]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Here, we see that our model predicts that a 10,000 MB file download will take
    about 718 seconds. Note that we didn’t have any examples in our training data
    near this size. In general, extrapolating to values well outside the training
    data is very risky, but with a problem this simple, it may be accurate . . . so
    long as we don’t run into new complications with memory buffers, input-output
    connectivity, and so on. It would be better if we could collect more training
    data in this range.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到我们的模型预测一个 10,000 MB 的文件下载大约需要 718 秒。请注意，我们的训练数据中没有任何接近这个大小的例子。通常来说，对训练数据范围之外的值进行外推是非常危险的，但对于一个如此简单的问题，它可能是准确的......只要我们不遇到内存缓冲区、输入输出连接等新问题。如果我们能够收集更多在这个范围内的训练数据将会更好。
- en: 'We see also that we needed to wrap our input variables into an appropriately
    shaped tensor. In [listing 2.3](#ch02ex03), we defined the `inputShape` to be
    `[1]`, so the model expects each example to have that shape. Both `fit()` and
    `predict()` work with multiple examples at a time. To provide `n` samples, we
    stack them together into a single input tensor, which thus must have the shape
    `[n, 1]`. If we had forgotten, and instead provided a tensor with the wrong shape
    to the model, we would have gotten a shape error, like the following code:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还看到我们需要将输入变量包装到一个适当形状的张量中。在[listing 2.3](#ch02ex03)中，我们定义`inputShape`为`[1]`，所以模型期望每个例子具有这个形状。`fit()`和`predict()`都可以一次处理多个例子。为了提供`n`个样本，我们将它们堆叠成一个单个输入张量，因此必须具有形状`[n,
    1]`。如果我们忘记了，并且向模型提供了形状错误的张量，我们将得到一个形状错误的错误，如下所示：
- en: '[PRE16]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Watch out for this type of shape mismatch because it is a very common type of
    error!
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 注意此类形状不匹配的问题，因为这是一种非常常见的错误！
- en: 2.1.7\. Summary of our first example
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.7\. 我们第一个示例的总结
- en: For this small example, it’s possible to illustrate the model’s result. [Figure
    2.4](#ch02fig04) shows the model’s output (`timeSec`) as a function of the input
    (`sizeMB`) for the models at four points in the process, beginning with the underfit
    one at 10 epochs and the converged one. We see that the converged model closely
    fits the data. If you are interested, at this point, in exploring how to plot
    data like that in [figure 2.4](#ch02fig04), please visit the CodePen at [codepen.io/tfjs-book/pen/VEVMMd](http://codepen.io/tfjs-book/pen/VEVMMd).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个小例子来说，可以说明模型的结果。[图2.4](#ch02fig04)展示了模型在该过程中的四个点（从10个周期的欠拟合到收敛）。我们可以看到收敛的模型与数据非常匹配。如果你对如何绘制这种类似于[图2.4](#ch02fig04)的数据感兴趣，请访问[http://codepen.io/tfjs-book/pen/VEVMMd](http://codepen.io/tfjs-book/pen/VEVMMd)上的
    CodePen。
- en: Figure 2.4\. The linear model fit after training for 10, 20, 100, and 200 epochs
  id: totrans-107
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.4\. 训练10、20、100和200个周期后的线性模型拟合情况
- en: '![](02fig04_alt.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](02fig04_alt.jpg)'
- en: This concludes our first example. You just saw how you can build, train, and
    evaluate a TensorFlow.js model in very few lines of JavaScript code (see [listing
    2.6](#ch02ex06)). In the next section, we’ll go a bit deeper into what’s going
    on inside of `model.fit`.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的第一个示例的结束。你刚刚看到了如何在很少的 JavaScript 代码行中构建、训练和评估一个 TensorFlow.js 模型（参见 [listing
    2.6](#ch02ex06)）。在下一节中，我们将更深入地了解`model.fit`内部发生的情况。
- en: Listing 2.6\. Model definition, training, evaluation, and prediction
  id: totrans-110
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 2.6\. 模型定义、训练、评估和预测
- en: '[PRE17]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '2.2\. Inside Model.fit(): Dissecting gradient descent from example 1'
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '2.2\. Model.fit()内部: 对示例1中的梯度下降进行解剖'
- en: In the previous section, we built a simple model and fit it to some training
    data, showing that we could make reasonably accurate predictions of download time
    given the file size. It isn’t the most impressive neural network, but it works
    in precisely the same way as the larger, much more complicated systems we’ll be
    building. We saw that fitting it for 10 epochs wasn’t very good, but fitting it
    for 200 epochs produced a quality model.^([[3](#ch02fn3)]) Let’s go into a bit
    more detail to understand exactly what happens under the hood when the model is
    trained.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们构建了一个简单的模型并拟合了一些训练数据，展示了在给定文件大小的情况下，我们可以进行相当准确的下载时间预测。它可能不是最令人印象深刻的神经网络，但它的工作方式与我们将要构建的更大、更复杂的系统完全相同。我们看到将其拟合10个周期并不好，但将其拟合200个周期产生了一个质量较高的模型^([[3](#ch02fn3)])。让我们更详细地了解一下模型训练时发生的确切情况。
- en: ³
  id: totrans-114
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³
- en: ''
  id: totrans-115
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Note that for a simple linear model like this one, simple, efficient, closed-form
    solutions exist. However, this optimization method will continue to work even
    for the more complicated models we introduce later.
  id: totrans-116
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意，对于像这个简单的线性模型，存在着简单、高效、封闭形式的解。然而，这种优化方法在我们后面介绍的更复杂的模型中仍然适用。
- en: 2.2.1\. The intuitions behind gradient-descent optimization
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1\. 梯度下降优化背后的直觉
- en: Recall that our simple, one-layer model is fitting a linear function `f(input)`,
    defined as
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，我们的简单单层模型是在拟合一个线性函数`f(input)`，定义为
- en: '[PRE18]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: where the kernel and bias are tunable parameters (the weights) of the dense
    layer. These weights contain the information learned by the network from exposure
    to the training data.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的kernel和bias是稠密层（dense layer）中的可调参数（权重）。这些权重包含了网络从训练数据中学到的信息。
- en: Initially, these weights are filled with small random values (a step called
    *random initialization*). Of course, there’s no reason to expect that `kernel
    * input + bias` will yield anything useful when the kernel and bias are random.
    Using our imagination, we can picture how the value of the MAE will change across
    different choices of these parameters. We expect that the loss will be low when
    they approximate the slope and intercept of the line we perceive in [figure 2.4](#ch02fig04),
    and that the loss will get worse as the parameters describe very different lines.
    This concept—the loss as a function of all tunable parameters—is known as the
    *loss surface*.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，这些权重被随机初始化为小的随机值（一个称为*随机初始化*的步骤）。当kernel和bias都是随机值时，我们当然不会指望`kernel * input
    + bias`会产生有用的结果。通过想象力，我们可以想象在不同的参数选择下，MAE的值会如何变化。我们预期当参数近似于我们在[图2.4](#ch02fig04)中观察到的直线的斜率和截距时，损失会很低，并且当参数描述非常不同的直线时，损失会变得更糟。这个概念——损失作为所有可调参数的函数——被称为*损失面*。
- en: Since this is a tiny example, and we just have two tunable parameters and a
    single target, it’s possible to illustrate the loss surface as a 2D contour plot,
    as [figure 2.5](#ch02fig05) shows. This loss surface has a nice bowl shape, with
    a global minimum at the bottom of the bowl representing the best parameter settings.
    In general, however, the loss surface of a deep-learning model is much more complex
    than this one. It will have many more than two dimensions and could have many
    local minima—that is, points that are lower than anything nearby but not the lowest
    overall.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这只是个小例子，我们只有两个可调参数和一个目标，所以可以将损失面绘制为2D等高线图，就像[图2.5](#ch02fig05)展示的那样。这个损失面呈现出一个漂亮的碗状，碗底的全局最小值代表了最佳的参数设置。然而，一个深度学习模型的损失面比这个要复杂得多。它会有多于两个维度，并且可能有很多局部最小值——也就是比附近任何点都更低但不是全局最低点的点。
- en: 'Figure 2.5\. The loss surface illustrates loss, shown against the model’s tunable
    parameters, as a contour plot. With this birds-eye view, we see that a choice
    of `{bias: 0.08, kernel: 0.07}` (marked with a white X) would be a reasonable
    choice for low loss. Rarely do we have the luxury of being able to test *all*
    the parameter settings to build a map like this, but if we did, optimization would
    be very easy; just pick the parameters corresponding to the lowest loss!'
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '图2.5\. 损失面展示了损失以及模型可调参数的等高线图。通过这个俯视图，我们可以看到选择`{bias: 0.08, kernel: 0.07}`（用白色X标记）作为低损失程度的合理选择。我们很少能有能力测试*所有*的参数设置来构建这样的图，但如果我们能，优化将会非常容易；只需选择对应最低损失的参数！'
- en: '![](02fig05_alt.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](02fig05_alt.jpg)'
- en: 'We see that this loss surface is shaped like a bowl, with the best (lowest)
    value somewhere around `{bias: 0.08, kernel: 0.07}`. This fits the geometry of
    the line implied by our data, where the download time is about 0.10 seconds, even
    when the file size is near zero. Our model’s random initialization starts us at
    a random parameter setting, analogous to a random location on this map, from which
    we calculate our initial loss. Next, we gradually adjust the parameters based
    on a feedback signal. This gradual adjustment, also called *training*, is the
    “learning” in “machine learning.” This happens within a *training loop*—illustrated
    in [figure 2.6](#ch02fig06).'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '我们可以看到这个损失面的形状像个碗，最好（最低）的值在`{bias: 0.08, kernel: 0.07}`附近。这符合我们的数据所暗示的直线的几何形状，其中下载时间约为0.10秒，即使文件大小接近零。我们模型的随机初始化让我们从随机的参数设置开始，类似于地图上的随机位置，然后我们计算我们的初始损失。接下来，我们根据一个反馈信号逐渐调整参数。这个逐渐调整，也称为*训练*，是“机器学习”中的“学习”。这发生在一个*训练循环*中，如[图2.6](#ch02fig06)所示。'
- en: Figure 2.6\. A flowchart illustrating the training loop, which updates the model
    via gradient descent
  id: totrans-126
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.6\. 描述训练循环，通过梯度下降更新模型
- en: '![](02fig06_alt.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](02fig06_alt.jpg)'
- en: '[Figure 2.6](#ch02fig06) illustrates how the training loop iterates through
    these steps as long as necessary:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.6](#ch02fig06)展示了训练循环在需要的情况下如何迭代执行这些步骤：'
- en: Draw a *batch* of training samples `x` and corresponding targets `y_true`. A
    batch is simply a number of input examples put together as a tensor. The number
    of examples in a batch is called the *batch size*. In practical deep learning,
    it is often set to be a power of 2, such as 128 or 256\. Examples are batched
    together to take advantage of the GPU’s parallel processing power and to make
    the calculated values of the gradients more stable (see [section 2.2.2](#ch02lev2sec9)
    for details).
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制一批训练样本`x`和相应的目标`y_true`。 一批简单地将若干输入示例组合成张量。 一批中的示例数量称为*批量大小*。 在实际的深度学习中，通常设置为2的幂，例如128或256。
    示例被批量处理以利用GPU的并行处理能力，并使梯度的计算值更稳定（详情请参见[第2.2.2节](#ch02lev2sec9)）。
- en: Run the network on `x` (a step called the *forward pass*) to obtain predictions
    `y_pred`.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`x`上运行网络（称为*前向传递*）以获得预测`y_pred`。
- en: Compute the loss of the network on the batch, a measure of the mismatch between
    `y_true` and `y_pred`. Recall that the loss function is specified when `model.compile()`
    is called.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算网络在批量上的损失，这是`y_true`和`y_pred`之间不匹配的度量。 请回忆，当调用`model.compile()`时指定了损失函数。
- en: Update all the weights (parameters) in the network in a way that slightly reduces
    the loss on this batch. The detailed updates to the individual weights are managed
    by the optimizer, another option we specified during the `model.compile()` call.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以稍微减少此批次上的损失的方式更新网络中的所有权重（参数）。 单个权重的详细更新由优化器管理，这是我们在`model.compile()`调用中指定的另一个选项。
- en: If you can lower your loss at every step, you will eventually end up with a
    network with low loss on the training data. The network has “learned” to map its
    inputs to correct targets. From afar, it may look like magic, but when reduced
    to these elementary steps, it turns out to be simple.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您可以在每一步中降低损失，最终您将获得一个在训练数据上损失较低的网络。 网络已经“学会”将其输入映射到正确的目标。 从远处看，它可能看起来像魔术，但当简化为这些基本步骤时，事实证明它是简单的。
- en: 'The only difficult part is step 4: how can you determine which weights should
    be increased, which should be decreased, and by how much? We could simply guess
    and check, and only accept updates that actually reduce the loss. Such an algorithm
    might work for a simple problem like this one, but it would be very slow. For
    larger problems, when we are optimizing millions of weights, the likelihood of
    randomly selecting a good direction becomes vanishingly small. A much better approach
    is to take advantage of the fact that all operations used in the network are *differentiable*
    and to compute the *gradient* of the loss with regard to the network’s parameters.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一困难的部分是步骤4：如何确定应该增加哪些权重，应该减少哪些权重，以及数量是多少？ 我们可以简单地猜测和检查，只接受实际减少损失的更新。 对于像这样的简单问题，这样的算法可能有效，但速度会很慢。
    对于更大的问题，当我们正在优化数百万个权重时，随机选择良好方向的可能性变得微乎其微。 更好的方法是利用网络中使用的所有操作都是*可微分*的事实，并计算损失相对于网络参数的*梯度*。
- en: 'What is a gradient? Instead of defining it precisely (which requires some calculus),
    we can describe it intuitively as the following:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是梯度？ 不是精确定义它（需要一些微积分），我们可以直观地描述它如下：
- en: '*A direction such that if you move the weights by a tiny bit in that direction,
    you will increase the loss function the fastest, among all possible directions*'
  id: totrans-136
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*一个方向，如果你将权重沿着那个方向微小移动，你将在所有可能的方向中最快地增加损失函数*'
- en: 'Even though this definition is not overly technical, there is still a lot to
    unpack, so let’s try to break it down:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 即使这个定义并不过于技术性，仍然有很多要解释的，所以让我们试着把它分解一下：
- en: First, the gradient is a vector. It has the same number of elements as the weights
    do. It represents a direction in the space of all choices of the weight values.
    If the weights of your model consist of two numbers, as is the case in our simple
    linear-regression network, then the gradient is a 2D vector. Deep-learning models
    often have thousands or millions of dimensions, and the gradients of these models
    are vectors (directions) with thousands or millions of elements.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，梯度是一个向量。 它的元素数量与权重相同。 它代表了在所有权重值选择空间中的方向。 如果您的模型的权重由两个数字组成，就像在我们的简单线性回归网络中一样，那么梯度就是一个2D向量。
    深度学习模型通常具有数千或数百万个维度，这些模型的梯度是具有数千或数百万个元素的向量（方向）。
- en: Second, the gradient depends on current weight values. In other words, different
    weight values will yield different gradients. This is clear from [figure 2.5](#ch02fig05),
    in which the direction that descends most quickly depends on where you are on
    the loss surface. On the left edge, we must go right. Near the bottom, we must
    go up, and so on.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，梯度取决于当前的权重值。换句话说，不同的权重值会产生不同的梯度。从[图2.5](#ch02fig05)可以清楚地看出，最快下降的方向取决于您在损失曲面上的位置。在左边缘，我们必须向右走。接近底部，我们必须向上走，依此类推。
- en: Finally, the mathematical definition of a gradient specifies a direction along
    which the loss function *increases*. Of course, when training neural networks,
    we want the loss to *decrease*. This is why we must move the weights in the direction
    *opposite* the gradient.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，梯度的数学定义指定了一个使损失函数*增加*的方向。当然，训练神经网络时，我们希望损失*减少*。这就是为什么我们必须沿着梯度的*相反*方向移动权重的原因。
- en: Consider, by way of analogy, a hike in a mountain range. Imagine we wish to
    travel to a place with the lowest altitude. In this analogy, we can change our
    altitude by moving in any direction defined by the east-west and north-south axes.
    We should interpret the first bullet point as saying that the gradient of our
    altitude is the direction most steeply upward given the slope under our feet.
    The second bullet is somewhat obvious, stating that the direction most steeply
    upward depends on our current position. Finally, if we wish to go to a low altitude,
    we should take steps in the direction *opposite* the gradient.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 比喻一下，想象一下在山脉中徒步旅行。假设我们希望前往海拔最低的地方。在这个比喻中，我们可以通过沿着东西和南北轴定义的任意方向改变我们的海拔。我们应该将第一个要点解释为，我们的海拔梯度是指在我们脚下的坡度下最陡的方向。第二个要点有点显而易见，说明最陡的方向取决于我们当前的位置。最后，如果我们希望海拔低，我们应该朝着梯度的*相反*方向迈步。
- en: 'This training process is aptly named *gradient descent*. Remember in [listing
    2.4](#ch02ex04), when we specified our model optimizer with the configuration
    `optimizer: ''sgd''`? The gradient-descent portion of stochastic gradient descent
    should now be clear. The “stochastic” part just means we draw random samples from
    the training data during each gradient-descent step for efficiency, as opposed
    to using every training data sample at every step. Stochastic gradient descent
    is simply a modification of gradient descent for computational efficiency.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '这个训练过程恰如其分地被命名为*梯度下降*。还记得在[清单2.4](#ch02ex04)中，当我们用配置`optimizer: ''sgd''`指定我们的模型优化器时吗？随机梯度下降中的梯度下降部分现在应该清楚了。
    "随机"部分只是意味着我们在每个梯度下降步骤中从训练数据中抽取随机样本以提高效率，而不是在每个步骤中使用每个训练数据样本。随机梯度下降只是梯度下降的一个针对计算效率的修改。'
- en: We now have tools for a more complete explanation of how optimization works,
    and why 200 epochs were better than 10 for our download-time estimation model.
    [Figure 2.7](#ch02fig07) illustrates how the gradient-descent algorithm follows
    a path down our loss surface to find a weight setting that fits our training data
    nicely. The contour plot in panel A of [figure 2.7](#ch02fig07) shows the same
    loss surface as before, zoomed in a bit and now overlaid with the path followed
    by the gradient-descent algorithm. The path begins at the *random initialization*—a
    random place on the image. We have to pick somewhere random to start since we
    don’t know the optimum beforehand! Several other points of interest are called
    out along the path, illustrating the positions corresponding to the underfit and
    the well-fit models. Panel B of [figure 2.7](#ch02fig07) shows a plot of the model
    loss as a function of the step, highlighting the analogous points of interest.
    Panel C illustrates the models using the weights as snapshots at the steps highlighted
    in B.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了更完整的工具来解释优化是如何工作的，以及为什么我们的下载时间估算模型的200个周期比10个周期更好。[图2.7](#ch02fig07)说明了梯度下降算法如何沿着我们的损失曲面找到一个很好地适应我们的训练数据的权重设置的路径。[图2.7](#ch02fig07)面板A中的等高线图显示了与之前相同的损失曲面，略微放大，并现在叠加了梯度下降算法所遵循的路径。该路径始于*随机初始化*——图像上的一个随机位置。由于我们事先不知道最优值，所以我们必须选择一个随机的起点！路径沿途还标出了其他几个感兴趣的点，说明了对应于欠拟合和良好拟合模型的位置。[图2.7](#ch02fig07)面板B显示了模型损失作为步骤的函数的图，突出显示了类似的感兴趣点。面板C说明了使用权重作为在B中突出显示的步骤的快照的模型。
- en: 'Figure 2.7\. Panel A: taking 200 moderate steps using gradient descent guides
    the parameter settings to the local optimum. Annotations highlight the starting
    weights and values after 20, 100, and 200 epochs. Panel B: a plot of the loss
    as a function of the epoch, highlighting the loss at the same points. Panel C:
    the function from `sizeMB` to `timeSec`, embodied by the fitted model after 10,
    20, 100, and 200 epochs of training, repeated here for you to easily compare the
    loss surface position and model output. See [codepen.io/tfjs-book/pen/JmerMM](http://codepen.io/tfjs-book/pen/JmerMM)
    to play with this code.'
  id: totrans-144
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.7\. 面板 A：使用梯度下降进行 200 次中等步长引导参数设置到局部最优解。注释突出显示了起始权重以及 20、100 和 200 个周期后的值。面板
    B：损失作为周期函数的绘图，突出显示了相同点的损失。面板 C：从 `sizeMB` 到 `timeSec` 的函数，经过 10、20、100 和 200 个周期的训练得到的拟合模型所体现的，这里重复给出以便您轻松比较损失表面位置和模型输出。请访问
    [codepen.io/tfjs-book/pen/JmerMM](http://codepen.io/tfjs-book/pen/JmerMM) 以玩耍这段代码。
- en: '![](02fig07_alt.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](02fig07_alt.jpg)'
- en: 'Our simple linear-regression model is the only model in this book where we
    will have the luxury to visualize the gradient-descent process this vividly. But
    when we encounter more complex models later, keep in mind that the essence of
    gradient descent remains the same: it’s just iteratively stepping down the slope
    of a complicated, high-dimensional surface, hoping that we will end up at a place
    with very low loss.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们简单的线性回归模型是本书唯一一个我们能够如此生动地可视化梯度下降过程的模型。但是当我们后面遇到更复杂的模型时，请记住梯度下降的本质仍然相同：它只是在一个复杂的、高维度表面上迭代地向下走，希望最终能够在一个损失非常低的地方停下来。
- en: In our initial effort, we used the default step size (determined by the *default
    learning rate*), but in looping over our limited data only 10 times, there weren’t
    enough steps to reach the optimum; 200 steps were enough. In general, how do you
    know how to set the learning rate, or how to know when training is done? There
    are some helpful rules of thumb, which we will cover over the course of this book,
    but there is no hard-and-fast rule that will always keep you out of trouble. If
    we use too small a learning rate and end up with *too small* a step, we won’t
    reach the optimum parameters within a reasonable amount of time. Conversely, if
    we use too large a learning rate and therefore *too big* of a step, we will completely
    skip over the minimum and may even end up with higher loss than the place we left.
    This will cause our model’s parameters to oscillate wildly around the optimum
    instead of approaching it quickly in a straightforward way. [Figure 2.8](#ch02fig08)
    illustrates what happens when our gradient step is too large. In more extreme
    cases, large learning rates will cause the parameter values to diverge and go
    to infinity, which will in turn generate NaN (not-a-number) values in the weights,
    completely ruining your model.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的初始尝试中，我们使用了默认步长（由*默认学习率*确定），但是在有限数据上仅循环了 10 次时，步数不足以达到最优值；200 步足够了。一般来说，您如何知道如何设置学习率，或者如何知道训练何时完成？有一些有用的经验法则，我们将在本书的过程中介绍，但没有一条硬性规定能够永远避免麻烦。如果我们使用的学习率太小，导致步长*太小*，我们将无法在合理的时间内达到最优参数。相反，如果我们使用的学习率太大，因此步长*太大*，我们将完全跳过最小值，甚至可能比我们离开的地方的损失更高。这将导致我们模型的参数在逼近最优值时出现剧烈振荡，而不是以直接的方式快速逼近。图
    2.8 示例如何当我们的梯度步长过大时会发生什么。在更极端的情况下，大的学习率会导致参数值发散并趋向无穷大，这将进一步在权重中生成 NaN（非数字）值，彻底破坏您的模型。
- en: Figure 2.8\. When the learning rate is too high, the gradient step will be too
    large, and the new parameters may be worse than the old ones. This could lead
    to oscillating behavior or some other instability resulting in infinities or NaNs.
    You can try increasing the learning rate in the CodePen code to 0.5 or higher
    to see this behavior.
  id: totrans-148
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.8\. 当学习率过高时，梯度步长会过大，新参数可能比旧参数更差。这可能导致振荡行为或其他稳定性问题，导致出现无穷大或 NaN。您可以尝试将 CodePen
    代码中的学习率增加到 0.5 或更高以查看此行为。
- en: '![](02fig08_alt.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](02fig08_alt.jpg)'
- en: '2.2.2\. Backpropagation: Inside gradient descent'
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2\. 反向传播：梯度下降的内部
- en: In the previous section, we explained how the step size of weight updates affects
    the process of gradient descent. However, we haven’t discussed how the *directions*
    of the updates are computed. The directions are critical to the neural network’s
    learning process. They are determined by the gradients with respect to the weights,
    and the algorithm for computing the gradients is called *backpropagation*. Invented
    in the 1960s, backpropagation is one of the foundations of neural networks and
    deep learning. In this section, we will use a simple example to show how backpropagation
    works. Note that this section is for readers who wish to get an understanding
    of backpropagation. It is not necessary if you only wish to apply the algorithm
    using TensorFlow.js, because these mechanisms are all hidden nicely under the
    `tf.Model.fit()` API; you may skip this section and continue reading at [section
    2.3](#ch02lev1sec3).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们解释了权重更新的步长如何影响梯度下降过程。但是，我们还没有讨论如何计算更新的*方向*。这些方向对于神经网络的学习过程是至关重要的。它们由相对于权重的梯度决定，计算这些梯度的算法称为*反向传播*。反向传播在20世纪60年代被发明，它是神经网络和深度学习的基础之一。在本节中，我们将使用一个简单的例子来展示反向传播的工作原理。请注意，本节是面向希望理解反向传播的读者。如果您只希望使用TensorFlow.js应用算法，这部分内容不是必需的，因为这些机制都被很好地隐藏在`tf.Model.fit()`
    API下面；您可以跳过本节，继续阅读第2.3节。
- en: Consider the simple model linear model
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个简单的线性模型
- en: '[PRE19]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'where `x` is the input feature and `y’` is the predicted output, and `v` is
    the only weight parameter of the model to be updated during backpropagation. Suppose
    we are using the squared error as the loss function; we then have the following
    relation between `loss`, `v`, `x`, and `y` (the actual target value):'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 `x` 是输入特征，`y’` 是预测输出，`v` 是在反向传播期间要更新的模型唯一的权重参数。假设我们使用平方误差作为损失函数；则我们有以下关系式，描述`loss`、`v`、`x`和`y`（实际目标值）之间的关系:'
- en: '[PRE20]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let’s assume the following concrete values: the two inputs are `x = 2` and
    `y = 5`, and the weight value is `v = 0`. The loss can then be calculated as 25\.
    This is shown step-by-step in [figure 2.9](#ch02fig09). Each gray square in panel
    A represents an input (that is, the `x` and the `y`). Each white box is an operation.
    There are a total of three operations. The edges connecting the operations (and
    the one that connects the tunable weight `v` with the first operation) are labeled
    `e[1]`, `e[2]`, and `e[3]`.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设以下具体值：两个输入变量的值为 `x = 2` 和 `y = 5`，权重值为 `v = 0`。损失可以计算为 25。这在[图2.9](#ch02fig09)中逐步显示。图中A面板中的每个灰色正方形代表一个输入变量（即`x`和`y`），每个白色方框表示一个操作。总共有三个操作。连接操作的边（以及将可调权重`v`与第一个操作连接的边）标记为`e[1]`、`e[2]`和`e[3]`。
- en: 'Figure 2.9\. Illustrating the backpropagation algorithm through a simple linear
    model with only one updatable weight (`v`). Panel A: forward pass on the model—the
    loss value is calculated from the weight (`v`) and the inputs (`x` and `y`). Panel
    B: backward pass—the gradient of loss with respect to `v` is calculated step-by-step,
    from the loss to `v`.'
  id: totrans-157
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.9。通过一个只有一个可更新权重（`v`）的简单线性模型说明反向传播算法。A面板：对模型的前向传递（从权重（`v`）和输入（`x`和`y`）计算出损失值）。B面板：反向传递——从损失到`v`逐步计算损失相对于`v`的梯度。
- en: '![](02fig09_alt.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](02fig09_alt.jpg)'
- en: 'An important step of backpropagation is to determine the following quantity:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '反向传播的一个重要步骤是确定以下量： '
- en: '*Assuming everything else (*`x` *and* `y` *in this case) stays the same, how
    much change in the loss value will we get if* `v` *is increased by a unit amount?*'
  id: totrans-160
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*假设其他所有内容（*在这种情况下是 `x` 和 `y`）保持不变，如果* `v`*增加一个单位，我们将获得的损失值的变化有多大？*'
- en: 'This quantity is referred to as *the gradient of loss with respect to* `v`.
    Why do we need this gradient? Because once we have it, we can alter `v` in the
    direction *opposite* to it, so we can get a decrease in the loss value. Note that
    we do not need the gradient of loss with respect to `x` or `y`, because `x` and
    `y` don’t need to be updated: they are the input data and are fixed.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这个量被称作*相对于* `v` *的损失梯度*。为什么我们需要这个梯度呢？因为一旦我们拥有了它，我们就可以朝着相反的方向改变 `v`，这样就可以得到损失值的减少。请注意，我们不需要相对于
    `x` 或 `y` 的损失梯度，因为 `x` 和 `y` 不需要被更新：它们是输入数据，并且是固定的。
- en: 'This gradient is computed step-by-step, starting from the loss value and going
    back toward the variable `v`, as illustrated in panel B of [figure 2.9](#ch02fig09).
    The direction in which the computation is carried out is the reason why this algorithm
    is called “backpropagation.” Let’s walk through the steps. Each of the following
    steps corresponds to an arrow in the figure:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这个梯度是逐步计算的，从损失值开始向后退到变量`v`，如[图2.9](#ch02fig09) B 面所示。计算的方向是这个算法被称为“反向传播”的原因。让我们来看看具体步骤。以下每个步骤都对应着图中的一个箭头：
- en: At the edge labeled `loss`, we start from a gradient value of 1\. This is making
    the trivial point, “a unit increase in `loss` corresponds to a unit increase in
    `loss` itself.”
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在标记为`loss`的边缘，我们从梯度值为1开始。这是一个微不足道的观点，“`loss`的单位增加对应着`loss`本身的单位增加”。
- en: 'At the edge labeled `e[3]`, we calculate the gradient of loss with respect
    to unit change of the current value at `e[3]`. Because the intervening operation
    is a square, and from basic calculus we know that the derivative (gradient in
    the one-variable case) of `(e[3])²` with respect to `e[3]` is `2 * e[3]`, we get
    a gradient value of `2 * -5 = -10`. The value `-`10 is multiplied with the gradient
    from before (that is, 1) to obtain the gradient on edge `e[3]`: `-`10\. This is
    the amount of increase in loss we’ll get if `e[3]` is increased by 1\. As you
    may have observed, the rule that we use to go from the gradient of the loss with
    respect to one edge to the one with respect to the next edge is to multiply the
    previous gradient with the gradient calculated locally at the current node. This
    rule is sometimes referred to as the *chain rule*.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在标记为`e[3]`的边缘，我们计算损失相对于`e[3]`当前值的单位变化的梯度。因为中间操作是一个平方，并且从基本微积分我们知道`(e[3])²`相对于`e[3]`的导数(在一维情况下的梯度)是`2
    * e[3]`，我们得到一个梯度值为`2 * -5 = -10`。值`-10`与之前的梯度(即1)相乘，得到边缘`e[3]`上的梯度：`-10`。这是如果`e[3]`增加1损失将增加的量。正如你可能已经观察到的，我们用来从损失相对于一个边缘的梯度转移到相对于下一个边缘的梯度的规则是将先前的梯度与当前节点局部计算的梯度相乘。这个规则有时被称为*链式法则*。
- en: At edge `e[2]`, we calculate the gradient of `e[3]` with respect to `e[2]`.
    Because this is a simple `add` operation, the gradient is simply 1, regardless
    of the other input value (`-y`). Multiplying this 1 with the gradient on edge
    `e[3]`, we get the gradient on edge `e[2]`, that is, `-`10.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在边缘`e[2]`，我们计算`e[3]`相对于`e[2]`的梯度。因为这是一个简单的`add`操作，梯度是1，不管其他输入值是什么(`-y`)。将这个1与边缘`e[3]`上的梯度相乘，我们得到边缘`e[2]`上的梯度，即`-10`。
- en: 'At edge `e[1]`, we calculate the gradient of `e[2]` with respect to `e[1]`.
    The operation here is a multiplication between `x` and `v`, that is, `x * v`.
    So, the gradient of `e[2]` with respect to `e[1]` (that is, with respect to `v`)
    is `x`, or 2\. The value of 2 is multiplied with the gradient on edge `e[2]` to
    get the final gradient: `2 * -10 = -20`.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在边缘`e[1]`，我们计算`e[2]`相对于`e[1]`的梯度。这里的操作是`x`和`v`之间的乘法，即`x * v`。所以，`e[2]`相对于`e[1]`（即相对于`v`）的梯度是`x`，即2。值2与边缘`e[2]`上的梯度相乘，得到最终的梯度：`2
    * -10 = -20`。
- en: Up to this point, we have obtained the gradient of loss with respect to `v:`
    it is `-`20\. In order to apply gradient descent, we need to multiply the negative
    of this gradient with the learning rate. Suppose the learning rate is 0.01\. Then
    we get a gradient update of
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经得到了`v`相对于损失的梯度：它是`-20`。为了应用梯度下降，我们需要将这个梯度的负数与学习率相乘。假设学习率是0.01。然后我们得到一个梯度更新为
- en: '[PRE21]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This is the update we will apply to `v` in this step of training:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们在训练的这一步将应用于`v`的更新：
- en: '[PRE22]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: As you can see, because we have `x = 2` and `y = 5`, and the function to be
    fit is `y’ = v * x,` the optimal value of `v` is `5/2 = 2.5`. After one step of
    training, the value of `v` changes from 0 to 0.2\. In other words, the weight
    `v` gets a little closer to the desired value. It will get closer and closer in
    subsequent training steps (ignoring any noise in the training data), which will
    be based on the same backpropagation algorithm previously described.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，因为我们有`x = 2`和`y = 5`，并且要拟合的函数是`y’ = v * x`，`v`的最佳值是`5/2 = 2.5`。经过一步训练后，`v`的值从0变为0.2。换句话说，权重`v`更接近期望值。在后续的训练步骤中，它将变得越来越接近（忽略训练数据中的任何噪声），这将基于先前描述的相同的反向传播算法。
- en: 'The prior example is made intentionally simple so that it’s easy to follow.
    Even though the example captures the essence of backpropagation, the backpropagation
    that happens in actual neural network training is different from it in the following
    aspects:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的示例被故意简化，以便易于跟踪。尽管该示例捕获了反向传播的本质，但实际神经网络训练中发生的反向传播与之不同，具有以下方面：
- en: Instead of providing a simple training example (`x = 2` and `y = 5`, in our
    case), usually a batch of many input examples are provided simultaneously. The
    loss value used to derive the gradient is an arithmetic mean of the loss values
    for all the individual examples.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常，不是提供一个简单的训练示例（在我们的例子中是`x = 2`和`y = 5`），而是同时提供许多输入示例的批处理。用于导出梯度的损失值是所有单个示例的损失值的算术平均值。
- en: The variables being updated generally have many more elements. So, instead of
    doing a simple, one-variable derivative as we just did, matrix calculus is often
    involved.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 被更新的变量通常有更多的元素。因此，通常涉及矩阵微积分，而不是我们刚刚做的简单的单变量导数。
- en: 'Instead of having to calculate the gradient for only one variable, multiple
    variables are generally involved. [Figure 2.10](#ch02fig10) shows an example,
    which is a slightly more complex linear model with two variables to optimize.
    In addition to `k`, the model has a bias term: `y’ = k * x + b`. Here, there are
    two gradients to compute, one for `k` and one for `b`. Both paths of backpropagation
    start from the loss. They share some common edges and form a tree-like structure.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与仅计算一个变量的梯度不同，通常涉及多个变量。[图2.10](#ch02fig10)显示了一个示例，这是一个略微更复杂的具有两个要优化变量的线性模型。除了`k`之外，模型还有一个偏置项：`y’
    = k * x + b`。在这里，有两个梯度要计算，一个是为了`k`，另一个是为了`b`。反向传播的两条路径都从损失开始。它们共享一些共同的边，并形成类似树的结构。
- en: Figure 2.10\. Schematic drawing showing backpropagation from loss to two updatable
    weights (`k` and `b`).
  id: totrans-176
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.10\. 示意图显示从损失到两个可更新权重(`k`和`b`)的反向传播。
- en: '![](02fig10_alt.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](02fig10_alt.jpg)'
- en: Our treatment of backpropagation in this section is a casual and high-level
    one. If you wish to gain a deeper understanding of the math and algorithms of
    backpropagation, refer to the links in [info box 2.2](#ch02sb02).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们对反向传播的处理是轻松和高层次的。如果您希望深入了解反向传播的数学和算法，请参考[信息框2.2](#ch02sb02)中的链接。
- en: At this point, you should have a pretty good understanding of what happens when
    fitting a simple model to training data, so let’s put away our tiny download-time
    prediction problem and use TensorFlow.js to tackle something a bit more challenging.
    In the next section, we’ll build a model to accurately predict the price of real
    estate from multiple input features simultaneously.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，您应该对将简单模型拟合到训练数据时发生的情况有很好的理解，因此让我们将我们的小型下载时间预测问题放在一边，并使用TensorFlow.js来解决一些更具挑战性的问题。在下一节中，我们将构建一个模型，以同时准确预测多个输入特征的房地产价格。
- en: '|  |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Further reading on gradient descent and backpropagation**'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '**有关梯度下降和反向传播的进一步阅读**'
- en: 'The differential calculus behind optimizing neural networks is definitely interesting
    and gives insight into how these algorithms behave; but beyond the basics, it
    is definitely *not* a requirement for the machine-learning practitioner, in the
    same way that understanding the intricacies of the TCP/IP protocol is useful but
    not critical to understanding how to build a modern web application. We invite
    the curious reader to explore the excellent resources here to build a deeper understanding
    of the mathematics of gradient-based optimization in networks:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 优化神经网络背后的微积分绝对是有趣的，并且能够洞察到这些算法的行为；但是在基础知识之上，它绝对*不*是机器学习从业者的必需品，就像理解TCP/IP协议的复杂性对于理解如何构建现代Web应用程序有用但并不重要一样。我们邀请好奇的读者探索这里的优秀资源，以建立对网络中基于梯度的优化数学的更深入的理解：
- en: 'Backpropagation demo scrollytelling illustration: [http://mng.bz/2J4g](http://mng.bz/2J4g)'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反向传播演示滚动说明：[http://mng.bz/2J4g](http://mng.bz/2J4g)
- en: 'Stanford CS231 lecture 4 course notes on backpropagation: [http://cs231n.github.io/optimization-2/](http://cs231n.github.io/optimization-2/)'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 斯坦福CS231讲座4课程关于反向传播的课程笔记：[http://cs231n.github.io/optimization-2/](http://cs231n.github.io/optimization-2/)
- en: Andrej Karpathy’s “Hacker’s Guide to Neural Nets:” [http://karpathy.github.io/neuralnets/](http://karpathy.github.io/neuralnets/)
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andrej Karpathy的“神经网络黑客指南:” [http://karpathy.github.io/neuralnets/](http://karpathy.github.io/neuralnets/)
- en: '|  |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 2.3\. Linear regression with multiple input features
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. 具有多个输入特征的线性回归
- en: In our first example, we had just one input feature, `sizeMB`, with which to
    predict our target, `timeSec`. A much more common scenario is to have multiple
    input features, to not know exactly which ones are the most predictive and which
    are only loosely related to the target, and to use them all simultaneously and
    let the learning algorithm sort it out. In this section, we will tackle this more
    complicated problem.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的第一个示例中，我们只有一个输入特征`sizeMB`，用它来预测我们的目标`timeSec`。更常见的情况是具有多个输入特征，不确定哪些特征最具预测性，哪些只与目标松散相关，并同时使用它们，并让学习算法来处理。在本节中，我们将解决这个更复杂的问题。
- en: By the end of this section, you will
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 到本节结束时，您将
- en: Understand how to build a model that takes in and learns from multiple input
    features.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解如何构建一个模型，该模型接收并从多个输入特征中学习。
- en: Use Yarn, Git, and the standard JavaScript project packaging structure to build
    and run a web app with machine learning.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Yarn、Git 和标准 JavaScript 项目打包结构构建和运行带有机器学习的 Web 应用程序。
- en: Know how to normalize data to stabilize the learning process.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 知道如何对数据进行归一化以稳定学习过程。
- en: Get a feel for using `tf.Model.fit()` callbacks to update a web UI while training.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 体会如何在训练过程中使用`tf.Model.fit()`回调来更新 Web UI。
- en: 2.3.1\. The Boston Housing Prices dataset
  id: totrans-194
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.1\. 波士顿房价数据集
- en: The Boston Housing Prices dataset^([[4](#ch02fn4)]) is a collection of 500 simple
    real-estate records collected in and around Boston, Massachusetts, in the late
    1970s. It has been used as a standard dataset for introductory statistics and
    machine-learning problems for decades. Each independent record in the dataset
    includes numeric measurements of a Boston neighborhood, including, for example,
    the typical size of homes, how far the region is from the closest highway, whether
    the area has waterfront property, and so on. [Table 2.1](#ch02table01) provides
    the precise ordered list of features, along with the average value of each feature.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 波士顿房价数据集^([[4](#ch02fn4)])是 1970 年代末在马萨诸塞州波士顿及周边地区收集的 500 条简单的房地产记录的集合。几十年来，它一直被用作介绍性统计和机器学习问题的标准数据集。数据集中的每个独立记录都包括波士顿社区的数值测量，例如房屋的典型大小、该地区距离最近的高速公路有多远、该地区是否拥有水边物业等。[表
    2.1](#ch02table01) 提供了特征的精确排序列表，以及每个特征的平均值。
- en: ⁴
  id: totrans-196
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁴
- en: ''
  id: totrans-197
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: David Harrison and Daniel Rubinfeld, “Hedonic Housing Prices and the Demand
    for Clean Air,” *Journal of Environmental Economics and Management*, vol. 5, 1978,
    pp. 81–102, [http://mng.bz/1wvX](http://mng.bz/1wvX).
  id: totrans-198
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 大卫·哈里森（David Harrison）和丹尼尔·鲁宾菲尔德（Daniel Rubinfeld），“享乐主义住房价格与对清洁空气的需求”，《环境经济与管理杂志》，第
    5 卷，1978 年，第 81–102 页，[http://mng.bz/1wvX](http://mng.bz/1wvX)。
- en: Table 2.1\. Features of the Boston-housing dataset
  id: totrans-199
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 2.1\. 波士顿房屋数据集的特征
- en: '| Index | Feature short name | Feature description | Mean value | Range (max
    – min) |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 索引 | 特征简称 | 特征描述 | 平均值 | 范围（最大值-最小值） |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | CRIM | Crime rate | 3.62 | 88.9 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 0 | CRIM | 犯罪率 | 3.62 | 88.9 |'
- en: '| 1 | ZN | Proportion of residential land zoned for lots over 25,000 sq. ft.
    | 11.4 | 100 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 1 | ZN | 用于超过 25,000 平方英尺的住宅用地比例 | 11.4 | 100 |'
- en: '| 2 | INDUS | Proportion of nonretail business acres (industry) in town | 11.2
    | 27.3 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 2 | INDUS | 城镇中非零售业务用地（工业）比例 | 11.2 | 27.3 |'
- en: '| 3 | CHAS | Whether or not the area is next to the Charles River | 0.0694
    | 1 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 3 | CHAS | 区域是否靠近查尔斯河 | 0.0694 | 1 |'
- en: '| 4 | NOX | Nitric oxide concentration (parts per 10 million) | 0.555 | 0.49
    |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 4 | NOX | 一氧化氮浓度（百万分之一） | 0.555 | 0.49 |'
- en: '| 5 | RM | Average number of rooms per dwelling | 6.28 | 5.2 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 5 | RM | 每个住宅的平均房间数 | 6.28 | 5.2 |'
- en: '| 6 | AGE | Portion of owner-occupied units built before 1940 | 68.6 | 97.1
    |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 6 | AGE | 1940 年前建造的自有住房比例 | 68.6 | 97.1 |'
- en: '| 7 | DIS | Weighted distances to five Boston employment centers | 3.80 | 11.0
    |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 7 | DIS | 到五个波士顿就业中心的加权距离 | 3.80 | 11.0 |'
- en: '| 8 | RAD | Index of accessibility to radial highways | 9.55 | 23.0 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 8 | RAD | 径向公路可达性指数 | 9.55 | 23.0 |'
- en: '| 9 | TAX | Tax rate per US$10,000 | 408.0 | 524.0 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 9 | TAX | 每 1 万美元的税率 | 408.0 | 524.0 |'
- en: '| 10 | PTRATIO | Pupil-teacher ratio | 18.5 | 9.40 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 10 | PTRATIO | 学生-教师比例 | 18.5 | 9.40 |'
- en: '| 11 | LSTAT | Percentage of working males without a high school education
    | 12.7 | 36.2 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 11 | LSTAT | 无高中学历的工作男性比例 | 12.7 | 36.2 |'
- en: '| 12 | MEDV | Median value of owner-occupied homes in units of $1,000 | 22.5
    | 45 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 12 | MEDV | 单位为 $1,000 的自有住房的中位数价值 | 22.5 | 45 |'
- en: In this section, we will build, train, and evaluate a learning system to estimate
    the median value of the house prices in a neighborhood (MEDV) given all the input
    features from the neighborhood. You can imagine it as a system for estimating
    the price of real estate from measurable neighborhood properties.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将构建、训练和评估一个学习系统，以估计邻域房屋价格的中位数值（MEDV），并给出邻域的所有输入特征。你可以把它想象成一个从可测量的邻域属性估计房地产价格的系统。
- en: 2.3.2\. Getting and running the Boston-housing project from GitHub
  id: totrans-216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.2\. 从 GitHub 获取并运行波士顿房屋项目
- en: Because this problem is a bit larger than the download-time prediction example
    and has more moving pieces, we will begin by providing the solution in the form
    of a working code repository, and then guide you through it. If you are already
    an expert in the Git source-control workflow and npm/Yarn package management,
    you may want to just skim this subsection quickly. More about basic JavaScript
    project structure is provided in [info box 2.3](#ch02sb03).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个问题比下载时间预测示例要复杂一些，并且有更多的组成部分，我们将首先以一个工作代码仓库的形式提供解决方案，然后引导你完成。如果你已经是 Git 源代码控制工作流和
    npm/Yarn 包管理的专家，你可能只需快速浏览一下这一小节。有关基本的 JavaScript 项目结构的更多信息，请参阅 [信息框 2.3](#ch02sb03)。
- en: 'We will begin by cloning the project repository from its source on GitHub^([[5](#ch02fn5)])
    to get a copy of the HTML, JavaScript, and configuration files required for the
    project. Except the simplest ones, which are hosted on CodePen, all the examples
    in this book are collected within one of two Git repositories and then separated
    by directory within the repository. The two repositories are tensorflow/tfjs-examples
    and tensorflow/tfjs-models, both hosted at GitHub. The following commands will
    clone the repository we need for this example locally and change the working directory
    to the Boston-housing prediction project:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从 GitHub 上的源获取项目仓库的副本来开始。获取项目所需的 HTML、JavaScript 和配置文件。除了最简单的那些（这些都托管在 CodePen
    上），本书中的所有示例都在两个 Git 仓库之一中收集，然后在仓库中分目录存放。这两个仓库分别是 tensorflow/tfjs-examples 和 tensorflow/tfjs-models，都托管在
    GitHub 上。以下命令将克隆我们需要的仓库到本地，并将工作目录切换到波士顿房屋预测项目：
- en: ⁵
  id: totrans-219
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁵
- en: ''
  id: totrans-220
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The examples in this book are open source and are hosted at [github.com](http://github.com)
    and [codepen.io](http://codepen.io). If you would like a refresher on how to use
    Git source-control tooling, GitHhub has a well-made tutorial beginning at [https://help.github.com/articles/set-up-git](https://help.github.com/articles/set-up-git).
    If you see a mistake or would like to help by clarifying something, you are welcome
    to send in fixes via GitHub pull requests.
  id: totrans-221
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 本书示例是开源的，托管在 [github.com](http://github.com) 和 [codepen.io](http://codepen.io)
    上。如果你想要关于如何使用 Git 源代码控制工具的温习，GitHub 有一个很好的教程，从 [https://help.github.com/articles/set-up-git](https://help.github.com/articles/set-up-git)
    开始。如果你发现错误或想通过 GitHub 提交更正，请随时发送修复请求。
- en: '[PRE23]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '|  |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Basic JavaScript project structure of examples used in this book**'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '**本书中使用的基本 JavaScript 项目结构**'
- en: The standard project structure we will be using for the examples in this book
    includes three important types of files. The first is HTML. The HTML files we
    will be using will be bare bones and serve mostly as a basic structure to hold
    a few components. Typically, there will be just one HTML file, titled index.html,
    which will include a few `div` tags, perhaps a few UI elements, and a source tag
    to pull in the JavaScript code, such as index.js.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 本书示例中使用的标准项目结构包括三种重要类型的文件。第一种是 HTML。我们将使用的 HTML 文件将是基本的骨架，主要用于承载几个组件。通常只会有一个名为
    index.html 的 HTML 文件，其中包含几个 `div` 标签，可能还有几个 UI 元素，以及一个 source 标签来引入 JavaScript
    代码，如 index.js。
- en: The JavaScript code will usually be modularized into several files in order
    to promote good readability and style. In the case of this Boston-housing project,
    code dealing with updating visual elements resides in ui.js, and code for downloading
    the data is in data.js. Both are referenced via `import` statements from index.js.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: JavaScript 代码通常会模块化成多个文件，以促进良好的可读性和风格。在波士顿房屋项目中，负责更新可视元素的代码存放在 ui.js 中，而处理数据下载的代码则在
    data.js 中。两者均通过 `import` 语句从 index.js 中引用。
- en: The third important file type we will be working with is the metadata package
    .json file, a requirement from the npm package manager ([www.npmjs.com](http://www.npmjs.com)).
    If you haven’t worked with npm or Yarn before, we recommend skimming the npm “getting
    started” documentation at [https://docs.npmjs.com/about-npm](https://docs.npmjs.com/about-npm)
    and becoming familiar enough to be able to build and run the example code. We
    will be using Yarn as our package manager ([https://yarnpkg.com/en/](https://yarnpkg.com/en/)),
    but you should be able to substitute npm for Yarn if it better suits your needs.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的第三种重要文件类型是元数据包 .json 文件，这是 npm 包管理器（[www.npmjs.com](http://www.npmjs.com)）的要求。如果您之前没有使用过
    npm 或者 Yarn，请我们建议您浏览一下 npm 的“入门”文档（[https://docs.npmjs.com/about-npm](https://docs.npmjs.com/about-npm)），并且熟悉到足以构建和运行示例代码的程度。我们将使用
    Yarn 作为我们的包管理器（[https://yarnpkg.com/en/](https://yarnpkg.com/en/)），但是如果您更喜欢使用
    npm，可以将 npm 替换为 Yarn。
- en: 'Inside the repository, take note of the following important files:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在存储库内，注意以下重要文件：
- en: '*index.html*—The root HTML file, which provides the DOM root and calls to the
    JavaScript scripts'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*index.html*—根 HTML 文件，它提供 DOM 根，并调用 JavaScript 脚本'
- en: '*index.js*—The root JavaScript file, which loads the data, defines the model
    and the training loop, and specifies the UI elements'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*index.js*—根 JavaScript 文件，该文件加载数据，定义模型和训练循环，并指定 UI 元素'
- en: '*data.js*—Implementation of the structures necessary for downloading and accessing
    the Boston-housing dataset'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*data.js*—下载和访问波士顿房价数据集所需的结构的实现'
- en: '*ui.js*—Implementation of the UI hooks for connecting UI elements to actions;
    specification of the plot configuration'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ui.js*—实现将 UI 元素与操作连接的 UI 钩子的文件；绘图配置的规范'
- en: '*normalization.js*—Numeric routines for, for example, subtracting the mean
    from the data'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*normalization.js*—数值例程，例如从数据中减去均值'
- en: '*package.json*—Standard npm package definition describing which dependencies
    are necessary for building and running this demo (such as TensorFlow.js!)'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*package.json*—标准的 npm 包定义，描述了构建和运行此演示所需的依赖项（例如 TensorFlow.js！）'
- en: Note that we do not follow the standard practice of putting HTML files and JavaScript
    files in type-specific subdirectories. This pattern, while best practice for larger
    repositories, obscures more than it clarifies for smaller examples like we will
    be using for this book or those you can find at [github.com/tensorflow/tfjs-examples](http://github.com/tensorflow/tfjs-examples).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们不遵循将 HTML 文件和 JavaScript 文件放在特定类型的子目录中的标准做法。这种模式在更大的存储库中是最佳做法，但对于我们将在本书中使用的较小示例或您可以在
    [github.com/tensorflow/tfjs-examples](http://github.com/tensorflow/tfjs-examples)
    找到的示例，它更多地是混淆而不是澄清。
- en: '|  |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'To run the demo, use Yarn:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行此演示，请使用 Yarn：
- en: '[PRE24]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This should open a new tab in your browser pointing to a port on `localhost`,
    which will run the example. If your browser doesn’t automatically react, you can
    navigate to the URL output on the command line. Clicking the button labeled Train
    Linear Regressor will trigger the routine to build a linear model and fit it to
    the Boston-housing data, and then output an animated graph of the loss on the
    training and testing datasets after each epoch, as [figure 2.11](#ch02fig11) illustrates.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在您的浏览器中打开一个指向 `localhost` 上的端口的新标签，该端口将运行示例。如果您的浏览器没有自动反应，可以在命令行中导航到输出的 URL。点击标记为“Train
    Linear Regressor”的按钮将触发构建线性模型并将其拟合到波士顿房价数据的过程，然后在每个周期后输出训练和测试数据集的损失的动态图表，如[图2.11](#ch02fig11)所示。
- en: Figure 2.11\. The Boston-housing linear-regression example from tfjs-examples
  id: totrans-240
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.11。tfjs-examples中的波士顿房价线性回归示例
- en: '![](02fig11_alt.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](02fig11_alt.jpg)'
- en: The rest of this section will go through the important points in the construction
    of this Boston-housing linear-regression web app demo. We will first review how
    the data is collected and processed so as to work with TensorFlow.js. We will
    then focus on the construction, training, and evaluation of the model; and, finally,
    we will show how to use the model for live predictions on the web page.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的其余部分将介绍构建这个波士顿房价线性回归 Web 应用演示的重要要点。我们首先将回顾数据是如何收集和处理的，以便与 TensorFlow.js 一起使用。然后我们将重点关注模型的构建、训练和评估；最后，我们将展示如何在网页上使用模型进行实时预测。
- en: 2.3.3\. Accessing the Boston-housing data
  id: totrans-243
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.3\. 访问波士顿房价数据
- en: 'In our first project, in [listing 2.1](#ch02ex01), we hard-coded our data as
    JavaScript arrays and converted it into tensors using the `tf.tensor2d` function.
    Hard-coding is fine for a little demo but clearly doesn’t scale to larger applications.
    In general, JavaScript developers will find that their data is located in some
    serialized format at some URL (which may be local). For instance, the Boston-housing
    data is publicly and freely available in CSV format from the Google Cloud at the
    following URLs:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的第一个项目中，在[清单2.1](#ch02ex01)中，我们将数据硬编码为JavaScript数组，并使用`tf.tensor2d`函数将其转换为张量。硬编码对于小型演示来说没问题，但显然不适用于更大的应用程序。一般来说，JavaScript开发人员会发现他们的数据位于某个URL（可能是本地）的某种序列化格式中。例如，波士顿房屋数据以CSV格式公开且免费提供，可以从Google
    Cloud的以下URL中获取：
- en: '[https://storage.googleapis.com/tfjs-examples/multivariate-linear-regression/data/train-data.csv](https://storage.googleapis.com/tfjs-examples/multivariate-linear-regression/data/train-data.csv)'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://storage.googleapis.com/tfjs-examples/multivariate-linear-regression/data/train-data.csv](https://storage.googleapis.com/tfjs-examples/multivariate-linear-regression/data/train-data.csv)'
- en: '[https://storage.googleapis.com/tfjs-examples/multivariate-linear-regression/data/train-target.csv](https://storage.googleapis.com/tfjs-examples/multivariate-linear-regression/data/train-target.csv)'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://storage.googleapis.com/tfjs-examples/multivariate-linear-regression/data/train-target.csv](https://storage.googleapis.com/tfjs-examples/multivariate-linear-regression/data/train-target.csv)'
- en: '[https://storage.googleapis.com/tfjs-examples/multivariate-linear-regression/data/test-data.csv](https://storage.googleapis.com/tfjs-examples/multivariate-linear-regression/data/test-data.csv)'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://storage.googleapis.com/tfjs-examples/multivariate-linear-regression/data/test-data.csv](https://storage.googleapis.com/tfjs-examples/multivariate-linear-regression/data/test-data.csv)'
- en: '[https://storage.googleapis.com/tfjs-examples/multivariate-linear-regression/data/test-target.csv](https://storage.googleapis.com/tfjs-examples/multivariate-linear-regression/data/test-target.csv)'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://storage.googleapis.com/tfjs-examples/multivariate-linear-regression/data/test-target.csv](https://storage.googleapis.com/tfjs-examples/multivariate-linear-regression/data/test-target.csv)'
- en: The data has been presplit by randomly assigning samples into training and testing
    portions. About two-thirds of the samples are in the training split, and the remaining
    one-third are reserved for independently evaluating the trained model. Additionally,
    for each split, the target feature has been separated into a CSV file apart from
    the other features, resulting in the four file names listed in [table 2.2](#ch02table02).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 数据已经通过将样本随机分配到训练和测试部分而进行了预拆分。大约有三分之二的样本在训练拆分中，剩下的三分之一用于独立评估经过训练的模型。此外，对于每个拆分，目标特征已经与其他特征分开成为CSV文件，导致了表2.2中列出的四个文件名。
- en: Table 2.2\. File names by split and contents for the Boston-housing dataset
  id: totrans-250
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表2.2\. 波士顿房屋数据集的拆分和内容的文件名
- en: '|   |   | Features (12 numbers) | Target (1 number) |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '|   |   | 特征（12个数字） | 目标（1个数字） |'
- en: '| --- | --- | --- | --- |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Train-test split** | **Training** | train-data.csv | train-target.csv |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| **训练-测试拆分** | **训练** | train-data.csv | train-target.csv |'
- en: '|   | **Testing** | test-data.csv | test-target.csv |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '|   | **测试** | test-data.csv | test-target.csv |'
- en: In order to pull these into our application, we will need to be able to download
    this data and convert it into a tensor of the appropriate type and shape. The
    Boston-housing project defines a class `BostonHousingDataset` in data.js for this
    purpose. This class abstracts away the dataset streaming operation, providing
    an API to retrieve the raw data as numeric matrices. Internally, the class uses
    the public open source Papa Parse library ([www.papaparse.com](http://www.papaparse.com))
    to stream and parse the remote CSV files. Once the file has been loaded and parsed,
    the library returns an array of arrays of numbers. It is then converted into a
    tensor using the same API as in the first example, as per the following listing,
    a slightly trimmed-down sample from `index.js focused on the relevant bits.`
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将这些数据引入我们的应用程序，我们需要能够下载这些数据并将其转换为适当类型和形状的张量。波士顿房屋项目在data.js中定义了一个名为`BostonHousingDataset`的类，用于此目的。该类抽象了数据集流操作，提供了一个API来检索原始数据作为数字矩阵。在内部，该类使用了公共开源Papa
    Parse库（[www.papaparse.com](http://www.papaparse.com)）来流式传输和解析远程CSV文件。一旦文件已加载和解析，库就会返回一个数字数组的数组。然后，使用与第一个示例中相同的API将其转换为张量，如下清单所示，这是`index.js`中的一个略微简化的示例，重点放在相关部分上。
- en: Listing 2.7\. Converting the Boston-housing data to tensors in index.js
  id: totrans-256
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单2.7\. 在index.js中将波士顿房屋数据转换为张量
- en: '[PRE25]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 2.3.4\. Precisely defining the Boston-housing problem
  id: totrans-258
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.4\. 精确定义波士顿房屋问题
- en: Now that we have access to our data in the form we want it, it is a good time
    to clarify our task more precisely. We said we would like to predict the MEDV
    from the other fields, but how will we decide if we’re doing a good job? How can
    we distinguish a good model from an even better one?
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以以我们想要的形式访问我们的数据，现在是时候更准确地澄清我们的任务了。我们说我们想要从其他字段预测 MEDV，但是我们将如何确定我们的工作是否做得好呢？我们如何区分一个好模型和一个更好的模型呢？
- en: The metric we used in our first example, `meanAbsoluteError`, counts all mistakes
    equally. If there were only 10 samples, and we made predictions for all 10, and
    we were exactly correct on 9 of them but off by 30 on the 10th sample, the `meanAbsoluteError`
    would be 3 (because 30/10 is 3). If, instead, our predictions were off by 3 on
    each and every sample, the `meanAbsoluteError` would still be 3\. This “equality
    of mistakes” principle might seem like the only obviously correct choice, but
    there are good reasons for picking loss metrics other than `meanAbsoluteError`.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第一个例子中使用的度量标准`meanAbsoluteError`将所有错误都视为平等。如果只有 10 个样本，并且我们对所有 10 个样本进行预测，并且我们在其中的第
    10 个样本上完全正确，但在其他 9 个样本上偏差为 30，则`meanAbsoluteError`将为 3（因为 30/10 为 3）。如果我们的预测对每个样本都偏差为
    3，那么`meanAbsoluteError`仍然为 3。这个“错误的平等性”原则可能似乎是唯一显然正确的选择，但是选择除`meanAbsoluteError`之外的损失度量有很好的理由。
- en: Another option is to weight large errors more than small errors. We could, instead
    of taking the average value of the absolute error, take the average value of the
    *squared* error.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择是将大错误的权重赋予小错误。我们可以不是取绝对误差的平均值，而是取*平方*误差的平均值。
- en: Continuing the case study with the 10 samples, this mean squared error (MSE)
    approach sees a lower loss in being off by 3 on every example (10 × 3² = 90) than
    being off by 30 on just one example (1 × 30² = 900). Because of the sensitivity
    to large mistakes, squared error can be more sensitive to sample outliers than
    absolute error. An optimizer fitting models to minimize MSE will prefer models
    that systematically make small mistakes over models that occasionally give very
    bad estimates. Obviously, both error measures would prefer models that make no
    mistakes at all! However, if your application might be sensitive to very incorrect
    outliers, MSE could be a better choice than MAE. There are other technical reasons
    why you might select MSE or MAE, but they aren’t important at this moment. In
    this example, we will use MSE for variety, but MAE would also suffice.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行有关这 10 个样本的案例研究时，这种均方误差（MSE）方法看到了在每个示例上偏差为 3 时（10 × 3² = 90）比在一个示例上偏差为 30
    时（1 × 30² = 900）较低的损失。由于对大错误的敏感性，平方误差比绝对误差更敏感于样本异常值。将模型拟合以最小化 MSE 的优化器将更喜欢系统地犯小错误的模型，而不是偶尔给出非常糟糕估计的模型。显然，这两种错误度量都会更喜欢根本没有错误的模型！但是，如果您的应用可能对非常不正确的异常值敏感，那么
    MSE 可能比 MAE 更好。选择 MSE 或 MAE 的其他技术原因，但它们在此时并不重要。在本例中，我们将使用 MSE 来增加变化，但 MAE 也足够。
- en: Before we continue, we should find a baseline estimate of the loss. If we don’t
    know the error from a very simple estimate, then we are not equipped to evaluate
    it from a more complicated model. We will use the average real-estate price as
    a stand-in for our “best naive guess” and calculate what the error would be from
    always guessing that value.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，我们应该找到损失的基准估计。如果我们不知道从一个非常简单的估计中得出的误差，那么我们就没有能力从一个更复杂的模型中评估它。我们将使用平均房地产价格作为我们的“最佳天真猜测”，并计算总是猜测该值时的误差。
- en: Listing 2.8\. Calculating baseline loss of guessing the mean price
  id: totrans-264
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.8\. 计算猜测平均价格的基线损失
- en: '[PRE26]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '***1*** Calculates the average price'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 计算平均价格'
- en: '***2*** Calculates the mean squared error on the test data. The sub(), pow,
    and mean() calls are the steps of calculating the mean squared error.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 计算测试数据上的平均平方误差。sub()、pow 和 mean() 调用是计算平均平方误差的步骤。'
- en: '***3*** Prints out the value of the loss'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 打印出损失值'
- en: Because TensorFlow.js optimizes its computation by scheduling on the GPU, tensors
    might not always be accessible to the CPU. The calls to `dataSync` in [listing
    2.8](#ch02ex08) tell TensorFlow.js to finish computing the tensor and pull the
    value from the GPU into the CPU, so it can be printed out or otherwise shared
    with a non-TensorFlow operation.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 TensorFlow.js 通过在 GPU 上进行调度来优化其计算，所以张量可能并不总是可供 CPU 访问。在[列表 2.8](#ch02ex08)
    中对`dataSync`的调用告诉 TensorFlow.js 完成张量的计算，并将值从 GPU 拉到 CPU 中，以便可以打印出来或以其他方式与非 TensorFlow
    操作共享。
- en: 'When executed, the code in [listing 2.8](#ch02ex08) yields the following at
    the console:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 当执行时，[列表 2.8](#ch02ex08) 中的代码将在控制台中产生以下输出：
- en: '[PRE27]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This tells us that the naive error rate is approximately 85.58\. If we were
    to build a model that always output 22.77, this model would achieve an MSE of
    85.58 on the test data. Again, notice that we calculate the metric on the training
    data and evaluate it on the test data to avoid unfair bias.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们，天真的误差率大约为85.58。如果我们构建一个总是输出22.77的模型，该模型在测试数据上将达到85.58的MSE。再次注意，我们在训练数据上计算指标，并在测试数据上评估它，以避免不公平的偏见。
- en: The average *squared* error is 85.58, so we should take the square root to get
    the average error. The square root of 85.58 is about 9.25\. Thus, we can say that
    we expect our (constant) estimate to be off (above or below) by about 9.25 on
    average. Since the values, as per [table 2.1](#ch02table01), are in thousands
    of US dollars, estimating a constant means we will be off by about US$9,250\.
    If this were good enough for our application, we could stop here! The wise machine-learning
    practitioner knows when to avoid unnecessary complexity. Let’s assume that our
    price estimator application needs to be closer than this. We will proceed by fitting
    a linear model to our data to see if we can achieve a better MSE than 85.58.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 平均*平方*误差为85.58，所以我们应该取平方根得到平均误差。85.58的平方根大约是9.25。因此，我们可以说我们期望我们的（常量）估计平均偏离（上下）约9.25。根据[表2.1](#ch02table01)的数值，以千美元为单位，估计一个常量意味着我们会偏离约9,250美元。如果这对我们的应用程序足够好，我们可以停止！明智的机器学习从业者知道何时避免不必要的复杂性。让我们假设我们的价格估计应用程序需要比这更接近。我们将通过拟合我们的数据来查看是否可以获得比85.58更好的MSE的线性模型。
- en: 2.3.5\. A slight diversion into data normalization
  id: totrans-274
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.5。稍微偏离数据标准化
- en: Looking at the Boston-housing features, we see a broad range of values. NOX
    ranges between 0.4 and 0.9, while TAX goes from 180 to 711\. To fit a linear regression,
    the optimizer will be attempting to find a weight for each feature such that the
    sum of the features times the weights will approximately equal the housing price.
    Recall that to find these weights, the optimizer is hunting around, following
    a gradient in the weight space. If some features are scaled very differently from
    others, then certain weights will be much more sensitive than others. A very small
    move in one direction will change the output more than a very large move in a
    different direction. This can cause instability and makes it difficult to fit
    the model.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 查看波士顿房屋的特征，我们会看到各种值。NOX的范围在0.4到0.9之间，而TAX则从180到711。为了拟合线性回归，优化器将尝试找到每个特征的权重，使特征的累加乘以权重大约等于房屋价格。请记住，为了找到这些权值，优化器正在寻找，遵循权重空间中的梯度。如果某些特征与其他特征的比例相差很大，那么某些权重将比其他权重敏感得多。向一个方向的一个非常小的移动将比另一个方向的一个非常大的移动更改输出。这可能导致不稳定，并使得难以拟合模型。
- en: To counteract this, we will first *normalize* our data. This means that we will
    scale our features so that they have zero mean and unit standard deviation. This
    type of normalization is common and may also be referred to as *standard transformation*
    or *z-score normalization*. The algorithm for doing this is simple—we first calculate
    the mean of each feature and subtract it from the original value so that the feature
    has an average value of zero. We then calculate the feature’s standard deviation
    with the mean value subtracted and do a division by that. In pseudo-code,
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对抗这一点，我们将首先*标准化*我们的数据。这意味着我们将缩放我们的特征，使它们的平均值为零，标准差为单位。这种标准化方法很常见，也可以被称为*标准转换*或*z-score标准化*。做这种操作的算法很简单——我们首先计算每个特征的平均值，并从原始值中减去，使得该特征的平均值为零。然后我们计算特征的标准差与减去的平均值，并进行除法。在伪代码中，
- en: '[PRE28]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: For instance, when the feature is `[10, 20, 30, 40]`, the normalized version
    would be approximately `[-1.3, -0.4, 0.4, 1.3]`, which clearly has a mean of zero;
    by eye, the standard deviation is about one. In the Boston-housing example, the
    normalization code is factored out into a separate file, normalization.js, the
    contents of which are in [listing 2.9](#ch02ex09). Here, we see two functions,
    one to calculate the mean and standard deviation from a provided rank-2 tensor
    and the other to normalize a tensor given the provided precalculated mean and
    standard deviation.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当特征是`[10, 20, 30, 40]`时，标准化后的版本大约是`[-1.3, -0.4, 0.4, 1.3]`，很明显的平均值为零；肉眼看，标准差大约为一。在波士顿房屋的例子中，标准化代码被分解到一个单独的文件中，normalization.js，其内容在[列表2.9](#ch02ex09)中。在这里，我们看到两个函数，一个用于计算所提供的二维张量的平均值和标准差，另一个用于在提供预先计算的平均值和标准差的情况下标准化张量。
- en: 'Listing 2.9\. Data normalization: zero mean, unit standard deviation'
  id: totrans-279
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.9。数据规范化：零均值，单位标准差
- en: '[PRE29]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Let’s dig into these functions a little. The function `determineMeanAndStddev`
    takes as input `data`, which is a rank-2 tensor. By convention, the first dimension
    is the *samples* dimension: each index corresponds to an independent, unique sample.
    The second dimension is the *feature* dimension: its 12 elements corresponds to
    the 12 input features (like CRIM, ZN, INDUS, and so on). Since we want to calculate
    the mean of each feature independently, we call'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍微深入一下这些函数。函数`determineMeanAndStddev`将`data`作为输入，这是一个秩2张量。按照惯例，第一个维度是*样本*维度：每个索引对应一个独立，唯一的样本。第二个维度是*特征*维度：其12个元素对应于12个输入特征（如CRIM，ZN，INDUS等）。由于我们要独立计算每个特征的平均值，因此调用
- en: '[PRE30]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The `0` in this call means that the mean is to be taken over the 0th-index (first)
    dimension. Recall that `data` is a rank-2 tensor and thus has two dimensions (or
    axes). The first axis, the “batch” axis, is the sample dimension. As we move from
    the first to the second to the third element along that axis, we refer to different
    samples, or, in our case, different pieces of real estate. The second dimension
    is the feature dimension. As we move from the first to the second element in this
    dimension, we are referring to different features, such as CRIM, ZN, and INDUS,
    from [table 2.1](#ch02table01). When we take the mean along axis 0, we are taking
    the average over the sample direction. The result is a rank-1 tensor with only
    the features axis remaining. We have the mean of each feature. If, instead, we
    took the mean over axis 1, we would still get a rank-1 tensor, but the remaining
    axis would be the sample dimension. The values would correspond to the mean value
    of each piece of real estate, which wouldn’t make sense for our application. Be
    careful when working with your axes that you are making your calculations in the
    right direction, as this is a common source of errors.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 这个调用中的`0`表示平均值是在第0维度（第一维度）上计算的。记得`data`是一个二维张量，因此有两个维度（或轴）。第一个轴，即“批处理”轴，是样本维度。当我们沿着该轴从第一个到第二个到第三个元素移动时，我们引用不同的样本，或者在我们的情况下，不同的房地产部分。第二个维度是特征维度。当我们在该维度的第一个元素移动到第二个元素时，我们引用不同的特征，例如CRIM，ZN和INDUS，来自[表2.1](#ch02table01)。当我们沿轴0取平均值时，我们正在沿样本方向取平均值。结果是具有仅保留特征轴的秩1张量。我们拥有每个特征的平均值。如果我们改为沿轴1取平均值，我们仍会得到一个秩1张量，但剩余轴将是样本维度。这些值将对应于每个房地产部分的平均值，这在我们的应用程序中没有意义。在使用轴进行计算时，请注意在正确方向上进行计算，因为这是常见的错误来源。
- en: 'Sure enough, if we set a breakpoint^([[6](#ch02fn6)]) here, we can use the
    JavaScript console to explore the calculated mean values, and we see mean values
    pretty close to what we calculated for the entire dataset. This means that our
    training sample was representative:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 果然，如果我们在这里设置一个断点^([[6](#ch02fn6)])，我们可以使用JavaScript控制台来探索计算出的平均值，我们看到的平均值非常接近我们为整个数据集计算的值。这意味着我们的训练样本是代表性的：
- en: ⁶
  id: totrans-285
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁶
- en: ''
  id: totrans-286
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The instructions for setting a breakpoint in Chrome are here: [http://mng.bz/rPQJ](http://mng.bz/rPQJ).
    If you need instructions for breakpoints in Firefox, Edge, or another browser,
    you may simply search for “how to set a breakpoint” using your favorite search
    engine.'
  id: totrans-287
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在Chrome中设置断点的说明在这里：[http://mng.bz/rPQJ](http://mng.bz/rPQJ)。如果您需要Firefox，Edge或其他浏览器中断点设置说明，您可以使用您喜欢的搜索引擎搜索“如何设置断点”。
- en: '[PRE31]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'In the next line, we subtract (using `tf.sub`) the mean from our data to obtain
    a centered version of the data:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一行中，我们通过使用`tf.sub`从我们的数据中减去平均值，从而获得数据的中心版本：
- en: '[PRE32]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: If you weren’t paying 100% attention, this line might have hidden a delightful
    little piece of magic. You see, `data` is a rank-2 tensor with shape `[333, 12]`,
    while `dataMean` is a rank-1 tensor with shape `[12]`. In general, it is not possible
    to subtract two tensors with different shapes. However, in this case, TensorFlow
    uses broadcasting to expand the shape of the second tensor by, in effect, repeating
    it 333 times, doing exactly what the user intended without making them spell it
    out. This usability win comes in handy, but sometimes the rules for which shapes
    are compatible for broadcasting can be a little confusing. If you are interested
    in the details of broadcasting, dive right into [info box 2.4](#ch02sb04).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有100％的注意力，这一行可能会隐藏一个迷人的小魔术。您看，`data`是一个形状为`[333,12]`的秩2张量，而`dataMean`是一个形状为`[12]`的秩1张量。通常情况下，不可能减去具有不同形状的两个张量。但是，在这种情况下，TensorFlow使用广播将第二个张量的形状扩展为在效果上重复它333次，而不使其清楚地拼写出来。这种易用性使操作变得更加简单，但是有时广播兼容的形状规则可能有点令人困惑。如果您对广播的细节感兴趣，请直接阅读[信息框2.4](#ch02sb04)。
- en: 'The next few lines of the `determineMeanAndStddev` function hold no new surprises:
    `tf.square()` multiplies each element by itself, while `tf.sqrt()` takes the square
    root of the elements. The detailed API for each method is documented at the TensorFlow.js
    API reference, [https://js.tensorflow.org/api/latest/](https://js.tensorflow.org/api/latest/).
    The documentation page also has live, editable widgets that allow you to explore
    how the functions work with your own parameter values, as illustrated in [figure
    2.12](#ch02fig12).'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '`determineMeanAndStddev`函数的下几行没有新的惊喜：`tf.square()`将每个元素乘以自身，而`tf.sqrt()`获取元素的平方根。每种方法的详细API在TensorFlow.js
    API参考文档中都有记录，[https://js.tensorflow.org/api/latest/](https://js.tensorflow.org/api/latest/)。该文档页面还具有实时的可编辑小部件，可以让您探索如何将函数与自己的参数值一起使用，如[图2.12](#ch02fig12)所示。'
- en: Figure 2.12\. The TensorFlow.js API documentation at [js.tensorflow.org](http://js.tensorflow.org)
    allows you to explore and interact with the TensorFlow API right within the documentation.
    This makes it simple and fast to understand functional uses and tricky edge cases.
  id: totrans-293
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.12。[js.tensorflow.org](http://js.tensorflow.org)的TensorFlow.js API文档允许您在文档内直接探索和交互使用TensorFlow
    API。这使得理解函数用途和棘手的边界案例变得简单而快速。
- en: '![](02fig12_alt.jpg)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![](02fig12_alt.jpg)'
- en: 'In this example, we’ve written our code prioritizing the clarity of the exposition,
    but the `determineMeanAndStddev` function can be expressed much more concisely:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们编写了代码以优先考虑阐述的清晰度，但是`determineMeanAndStddev`函数可以更简洁地表达：
- en: '[PRE33]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: You should be able to see that TensorFlow allows us to express quite a lot of
    numerical computation without much boilerplate code.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该能够看到，TensorFlow允许我们在不使用很多样板代码的情况下表达相当多的数字计算。
- en: '|  |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Broadcasting**'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '**广播**'
- en: 'Consider a tensor operation like `C = tf.someOperation(A, B)`, where `A` and
    `B` are tensors. When possible, and if there’s no ambiguity, the smaller tensor
    will be broadcast to match the shape of the larger tensor. Broadcasting consists
    of two steps:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个张量运算，如`C = tf.someOperation（A，B）`，其中`A`和`B`是张量。如果可能且没有歧义，较小的张量将被扩展到与较大的张量匹配的形状。广播包括两个步骤：
- en: Axes (called *broadcast axes*) are added to the smaller tensor to match the
    rank of the larger tensor.
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 小张量添加轴（称为*广播轴*）以匹配大张量的秩。
- en: The smaller tensor is repeated alongside these new axes to match the full shape
    of the larger tensor.
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 较小的张量将沿着这些新轴重复以匹配大张量的完整形状。
- en: In terms of implementation, no new tensor is actually created because that would
    be terribly inefficient. The repetition operation is entirely virtual—it happens
    at the algorithmic level rather than the memory level. But thinking of the smaller
    tensor being repeated along the new axis is a helpful mental model.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现方面，实际上没有创建新的张量，因为那将非常低效。重复操作完全是虚拟的，在算法级别而不是在内存级别上发生。但是思考较小张量沿着新轴重复是有帮助的。
- en: 'With broadcasting, you can generally apply two-tensor, element-wise operations
    if one tensor has shape `(a, b, ..., n, n + 1, ... m)` and the other has shape
    `(n, n + 1, ... , m)`. The broadcasting will then automatically happen for axis
    `a` through `n - 1`. For instance, the following example applies the element-wise
    `maximum` operation on two random tensors of different shapes via broadcasting:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 通过广播，如果一个张量的形状为`(a, b, ..., n, n + 1, ... m)`，另一个张量的形状为`(n, n + 1, ... , m)`，通常可以对两个张量进行逐元素操作。广播将自动发生在轴`a`到`n
    - 1`。例如，以下示例通过广播在不同形状的两个随机张量上应用逐元素`maximum`操作：
- en: '[PRE34]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '***1*** x is a random tensor with shape [64, 3, 11, 9].'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** x 是一个形状为 [64, 3, 11, 9] 的随机张量。'
- en: '***2*** y is a random tensor with shape [11, 9].'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** y 是一个形状为 [11, 9] 的随机张量。'
- en: '***3*** The output z has shape [64, 3, 11, 9] like x.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 输出 z 的形状与 x 相同，为 [64, 3, 11, 9]。'
- en: '|  |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 2.3.6\. Linear regression on the Boston-housing data
  id: totrans-310
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.6\. 波士顿房屋数据的线性回归
- en: 'Our data is normalized, and we have done the due diligence data work to calculate
    a reasonable baseline—the next step is to build and fit a model to see if we can
    outperform the baseline. In [listing 2.10](#ch02ex10), we define a linear-regression
    model like we did in [section 2.1](#ch02lev1sec1) (from index.js). The code is
    remarkably similar; the only difference we see from the download-time prediction
    model is in the `inputShape` configuration, which now accepts vectors of length
    12 instead of 1\. The single dense layer still has `units: 1`, indicating that
    a single number is the output.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的数据已经归一化，并且我们已经完成了对数据的尽职调查工作，计算出了一个合理的基线——下一步是构建和拟合一个模型，看看我们是否能超越基线。在 [listing
    2.10](#ch02ex10) 中，我们定义了一个线性回归模型，就像我们在 [section 2.1](#ch02lev1sec1) 中所做的那样（来自
    index.js）。代码非常相似；我们从下载时间预测模型看到的唯一区别在于 `inputShape` 配置，它现在接受长度为 12 的向量，而不是 1。单个密集层仍然具有
    `units: 1`，表示输出为一个数字。'
- en: Listing 2.10\. Defining a linear-regression model for Boston-housing
  id: totrans-312
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Listing 2.10\. 为波士顿房屋定义线性回归模型
- en: '[PRE35]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Recall that after our model is defined, but before we begin training, we must
    specify the loss and optimizer via a call to `model.compile`. In [listing 2.11](#ch02ex11),
    we see that the `'meanSquaredError'` loss is specified and that the optimizer
    is using a customized learning rate. In our previous example, the optimizer parameter
    was set to the string `'sgd'`, but now it is `tf.train.sgd(LEARNING_RATE)`. This
    factory function will return an object representing the stochastic gradient descent
    optimization algorithm, but parameterized with our custom learning rate. This
    is a common pattern in TensorFlow.js, borrowed from Keras, and you will see it
    adopted for many configurable options. For standard, well-known default parameters,
    a string sentinel value can substitute for the required object type, and TensorFlow.js
    will substitute the string for the required object with good default parameters.
    In this case, `'sgd'` would be replaced with `tf.train.sgd(0.01)`. When additional
    customizations are necessary, the user can build the object via the factory function
    and provide the required custom value. This allows code to be concise in most
    cases but allows the power user to override default behaviors when needed.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的模型被定义之后，但在我们开始训练之前，我们必须通过调用`model.compile`来指定损失和优化器。在 [listing 2.11](#ch02ex11)
    中，我们看到指定了`'meanSquaredError'`损失，并且优化器使用了自定义的学习率。在我们之前的示例中，优化器参数被设置为字符串`'sgd'`，但现在是`tf.train.sgd(LEARNING_RATE)`。这个工厂函数将返回一个代表随机梯度下降优化算法的对象，但是参数化了我们自定义的学习率。这是
    TensorFlow.js 中的一个常见模式，借鉴自 Keras，并且你将看到它被用于许多可配置选项。对于标准、已知的默认参数，字符串标记值可以替代所需的对象类型，TensorFlow.js
    将使用良好的默认参数替换所需对象的字符串。在这种情况下，`'sgd'`将被替换为`tf.train.sgd(0.01)`。当需要额外的定制时，用户可以通过工厂函数构建对象并提供所需的定制值。这允许代码在大多数情况下简洁，但允许高级用户在需要时覆盖默认行为。
- en: Listing 2.11\. Model compilation for Boston-housing (from index.js)
  id: totrans-315
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Listing 2.11\. 为波士顿房屋模型编译（来自 index.js）
- en: '[PRE36]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Now we can train our model with the training dataset. In [listings 2.12](#ch02ex12)
    through [2.14](#ch02ex14), we’ll use some additional features of the `model.fit()`
    call, but essentially it’s doing the same thing as in [figure 2.6](#ch02fig06).
    At each step, it selects a number of new samples from the features (`tensors.trainFeatures`)
    and targets (`tensors.trainTarget`), calculates the loss, and then updates the
    internal weights to reduce that loss. The process will repeat for `NUM_EPOCHS`
    complete passes through the training data and will select `BATCH_SIZE` samples
    at each step.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用训练数据集训练我们的模型。在[列表 2.12](#ch02ex12)到[2.14](#ch02ex14)中，我们将使用`model.fit()`调用的一些附加功能，但本质上它与[图
    2.6](#ch02fig06)中的情况相同。在每一步中，它从特征（`tensors.trainFeatures`）和目标（`tensors.trainTarget`）中选择一定数量的新样本，计算损失，然后更新内部权重以减少该损失。该过程将在训练数据上进行`NUM_EPOCHS`次完整的遍历，并且在每一步中将选择`BATCH_SIZE`个样本。
- en: Listing 2.12\. Training our model on the Boston-housing data
  id: totrans-318
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.12. 在波士顿房屋数据上训练我们的模型
- en: '[PRE37]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: In the Boston-housing web app, we illustrate a graph of the training loss as
    the model trains. This requires using the `model.fit()` callback feature to update
    the UI. The `model.fit()` callback API allows the user to provide callback functions,
    which will be executed at specific events. The complete list of callback triggers,
    as of version 0.12.0, is `onTrainBegin`, `onTrainEnd`, `onEpochBegin`, `onEpochEnd`,
    `onBatchBegin`, and `onBatchEnd`.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在波士顿房价 Web 应用程序中，我们展示了一个图表，显示模型训练时的训练损失。这需要使用`model.fit()`回调功能来更新用户界面。`model.fit()`回调API允许用户提供回调函数，在特定事件发生时执行。截至版本0.12.0，回调触发器的完整列表包括`onTrainBegin`、`onTrainEnd`、`onEpochBegin`、`onEpochEnd`、`onBatchBegin`和`onBatchEnd`。
- en: Listing 2.13\. Callbacks in `model.fit()`
  id: totrans-321
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.13. `model.fit()`中的回调函数
- en: '[PRE38]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: One last new customization introduced here is to make use of validation data.
    Validation is a machine-learning concept worth a bit of explanation. In the earlier
    downloading-time example, we separated our training data from our testing data
    because we wanted an unbiased estimate of how our model will perform on new, unseen
    data. Typically what happens, though, is that there is another split called *validation
    data*. Validation data is separate from both the training data and the testing
    data. What is validation data used for? The machine-learning engineer will see
    the result on the validation data and use that result to change certain configurations
    of the model^([[7](#ch02fn7)]) in order to improve the accuracy on the validation
    data. This is all well and good. However, if this cycle is done enough times,
    then we are in effect tuning on the validation data. If we use the same validation
    data to evaluate the model’s final accuracy, the result of the final evaluation
    will no longer be generalizable, in the sense that the model has already seen
    the data, and the result of the evaluation isn’t guaranteed to reflect how the
    model will perform on unseen data in the future. This is the purpose of separating
    validation out from testing data. The idea is that we will fit our model on the
    training data and adjust its hyperparameters based on assessments on the validation
    data. When we are all done and satisfied with the process, we will evaluate the
    model just one time on the testing data for a final, generalizable estimate of
    performance.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 这里介绍的最后一个新的自定义是利用验证数据。验证是一个值得解释的机器学习概念。在早期的下载时间示例中，我们将训练数据与测试数据分开，因为我们想要一个对模型在新的、未见过的数据上的性能进行无偏估计。通常情况下，还有一个称为*验证数据*的拆分。验证数据与训练数据和测试数据都是分开的。验证数据用于什么？机器学习工程师将在验证数据上看到结果，并使用该结果来更改模型的某些配置^[[7](#ch02fn7)]，以提高验证数据上的准确性。这都很好。然而，如果这个周期足够多次，那么我们实际上是在验证数据上进行调优。如果我们使用相同的验证数据来评估模型的最终准确性，那么最终评估的结果将不再具有泛化性，因为模型已经看到了数据，并且评估结果不能保证反映模型在未来未见数据上的表现。这就是将验证数据与测试数据分开的目的。这个想法是我们将在训练数据上拟合我们的模型，并根据验证数据上的评估来调整其超参数。当我们完成并满意整个过程时，我们将在测试数据上仅对模型进行一次评估，以获得最终的、可推广的性能估计。
- en: ⁷
  id: totrans-324
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁷
- en: ''
  id: totrans-325
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Examples of those configurations include the number of layers in the model,
    the sizes of the layers, the type of optimizer and learning rate to use during
    training, and so forth. They are referred to as the model’s *hyperparameters*,
    which we will cover in greater detail in [section 3.1.2](kindle_split_014.html#ch03lev2sec2)
    of the next chapter.
  id: totrans-326
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这些配置的示例包括模型中的层数、层的大小、训练过程中使用的优化器类型和学习率等。它们被称为模型的*超参数*，我们将在下一章的 [section 3.1.2](kindle_split_014.html#ch03lev2sec2)
    中更详细地介绍。
- en: 'Let’s summarize what the training, validation, and testing sets are and how
    to use them in TensorFlow.js. Not all projects will make use of all three types
    of data. Frequently, quick explorations or research projects will use only training
    and validation data and not reserve a set of “pure” data for the test. While less
    rigorous, this is sometimes the best use of limited resources:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下训练、验证和测试集在 TensorFlow.js 中的作用以及如何使用它们。并非所有项目都会使用这三种类型的数据。经常，快速探索或研究项目只会使用训练和验证数据，而不会保留一组“纯”数据用于测试。虽然不太严谨，但这有时是对有限资源的最佳利用：
- en: '*Training data*—For fitting the model weights with gradient descent'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*训练数据*—用于梯度下降优化模型权重'
- en: '*Usage in TensorFlow.js*: Typically, training data is employed using the main
    arguments (`x` and `y`) for calls to `Model.fit(x, y, config)`.'
  id: totrans-329
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在 TensorFlow.js 中的用法*：通常，使用主要参数（`x`和`y`）对`Model.fit(x, y, config)`进行调用来使用训练数据。'
- en: '*Validation data*—For selecting the model structure and hyperparameters'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*验证数据*—用于选择模型结构和超参数'
- en: '*Usage in TensorFlow.js*: `Model.fit()` has two ways of specifying validation
    data, both as parameters to the `config` argument. If you, the user, have explicit
    data to use for validation, this may be specified as `config.validationData`.
    If, instead, you wish the framework to split some of the training data off and
    use it as validation data, specify the fraction to use in `config.validationSplit`.
    The framework will take care to not use the validation data to train the model,
    so there is no overlap.'
  id: totrans-331
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在 TensorFlow.js 中的用法*：`Model.fit()` 有两种指定验证数据的方式，都作为`config`参数的一部分。如果您作为用户具有明确的用于验证的数据，则可以指定为`config.validationData`。相反，如果您希望框架拆分一些训练数据并将其用作验证数据，则可以在`config.validationSplit`中指定要使用的比例。框架将确保不使用验证数据来训练模型，因此不会有重叠。'
- en: '*Testing data*—For a final, unbiased estimate of model performance'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*测试数据*—用于对模型性能进行最终、无偏的估计'
- en: '*Usage in TensorFlow.js*: Evaluation data is exposed to the system by passing
    it in as the `x` and `y` arguments to `Model.evaluate(x, y, config)`.'
  id: totrans-333
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在 TensorFlow.js 中的用法*：通过将其作为`x`和`y`参数传递给`Model.evaluate(x, y, config)`，可以向系统公开评估数据。'
- en: 'In [listing 2.14](#ch02ex14), validation loss is calculated along with training
    loss. The `validationSplit: 0.2` field instructs the `model.fit()` machinery to
    select the last 20% of the training data to use as validation data. This data
    will not be used for training (it does not affect gradient descent).'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '在 [列表 2.14](#ch02ex14) 中，验证损失与训练损失一起计算。`validationSplit: 0.2`字段指示`model.fit()`机制选择最后20%的训练数据用作验证数据。这些数据将不用于训练（不影响梯度下降）。'
- en: Listing 2.14\. Including validation data in `model.fit()`
  id: totrans-335
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.14\. 在 `model.fit()` 中包含验证数据
- en: '[PRE39]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Training this model to 200 epochs takes approximately 11 seconds in the browser
    on a modern laptop. We can now evaluate the model on our test set to see if it’s
    any better than the baseline. The next listing shows how to use `model.evaluate()`
    to collect the performance of the model on our reserved test data and then call
    into our custom UI routines to update the view.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在浏览器上将此模型训练到 200 个周期大约需要 11 秒。我们现在可以对我们的测试集上评估模型，以查看它是否比基准更好。下一个列表显示了如何使用`model.evaluate()`来收集模型在我们保留的测试数据上的性能，然后调用我们的自定义
    UI 例程来更新视图。
- en: Listing 2.15\. Evaluating our model on the test data and updating the UI (from
    index.js)
  id: totrans-338
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.15\. 在测试数据上评估我们的模型并更新 UI（来自 index.js）
- en: '[PRE40]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Here, `model.evaluate()` returns a scalar (recall, a rank-0 tensor) containing
    the loss computed over the test set.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`model.evaluate()`返回一个标量（记住，一个秩为0的张量），其中包含对测试集计算得出的损失。
- en: 'Because of the randomness involved in gradient descent, you might get different
    results, but the following results are typical:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 由于梯度下降中涉及随机性，您可能会得到不同的结果，但以下结果是典型的：
- en: 'Final train-set loss: 21.9864'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '最终的训练集损失: 21.9864'
- en: 'Final validation-set loss: 31.1396'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '最终的验证集损失: 31.1396'
- en: 'Test-set loss: 25.3206'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '测试集损失: 25.3206'
- en: 'Baseline loss: 85.58'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '基准损失: 85.58'
- en: We see from this that our final, unbiased estimate of our error is about 25.3,
    which is much better than our naive baseline of 85.6\. Recall that our error is
    being calculated using `meanSquaredError`. Taking the square root, we see that
    the baseline estimate was typically off by more than 9.2, while the linear model
    is off by only about 5.0\. Quite a large improvement! If we were the only ones
    in the world with access to this info, we could be the best Boston real-estate
    investors in 1978! Unless, somehow, someone were able to build an even more accurate
    estimate . . .
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从中看到，我们的最终无偏估计错误约为25.3，远远好于我们的天真基线85.6。回想一下，我们的错误是使用`meanSquaredError`计算的。取平方根，我们看到基线估计通常偏离了9.2以上，而线性模型仅偏离了约5.0。相当大的改进！如果我们是世界上唯一拥有这些信息的人，我们可能是1978年波士顿最好的房地产投资者！除非，以某种方式，有人能够建立一个更准确的估算……
- en: If you have let your curiosity get ahead of you and clicked Train Neural Network
    Regressor, you already know that *much* better estimates are possible. In the
    next chapter, we will introduce nonlinear deep models to show how such feats are
    possible.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你让好奇心超过了自己，并点击了训练神经网络回归器，你已经知道可以得到*更好*的估计。在下一章中，我们将介绍非线性深度模型，展示这样的成就是如何可能的。
- en: 2.4\. How to interpret your model
  id: totrans-348
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4\. 如何解释你的模型
- en: Now that we’ve trained our model, and it’s able to make reasonable predictions,
    it’s natural to wonder what it has learned. Is there any way to peek inside the
    model to see how it understands the data? When the model predicts a specific price
    for an input, is it possible for you to find an understandable explanation for
    why it comes up with that value? For the general case of large deep networks,
    model understanding—also known as model interpretability—is still an area of active
    research, filling many posters and talks at academic conferences. But for this
    simple linear-regression model, it is quite simple.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练了我们的模型，并且它能够做出合理的预测，自然而然地想知道它学到了什么。有没有办法窥视模型，看看它是如何理解数据的？当模型为输入预测了一个特定的价格时，你能否找到一个可以理解的解释来解释它为什么得出这个值？对于大型深度网络的一般情况，模型理解——也称为模型可解释性——仍然是一个活跃的研究领域，在学术会议上填满了许多海报和演讲。但对于这个简单的线性回归模型来说，情况相当简单。
- en: By the end of this section, you will
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 到本节结束时，你将
- en: Be able to extract the learned weights from a model.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够从模型中提取学到的权重。
- en: Be able to interpret those weights and weigh them against your intuitions for
    what the weights *should* be.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够解释这些权重，并将它们与你对权重*应该*是什么的直觉进行权衡。
- en: 2.4.1\. Extracting meaning from learned weights
  id: totrans-353
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.1\. 从学到的权重中提取含义
- en: 'The simple linear model we built in [section 2.3](#ch02lev1sec3) contains 13
    learned parameters, contained in a kernel and a bias, just like our first linear
    model in [section 2.1.3](#ch02lev2sec3):'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[section 2.3](#ch02lev1sec3)中构建的简单线性模型包含了13个学到的参数，包含在一个核和一个偏差中，就像我们在[section
    2.1.3](#ch02lev2sec3)中的第一个线性模型一样：
- en: '[PRE41]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The values of the kernel and bias are both learned while fitting the model.
    In contrast to the *scalar* linear function learned in [section 2.1.3](#ch02lev2sec3),
    here, the features and the kernel are both *vectors*, and the “`·`” sign indicates
    the *inner product*, a generalization of scalar multiplication to vectors. The
    inner product, also known as the *dot product*, is simply the sum of the products
    of the matching elements. The pseudo-code in [listing 2.16](#ch02ex16) defines
    the inner product more precisely.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 核和偏差的值都是在拟合模型时学到的。与[section 2.1.3](#ch02lev2sec3)中学到的*标量*线性函数相比，这里，特征和核都是*向量*，而“`·`”符号表示*内积*，是标量乘以向量的一般化。内积，也称为*点积*，简单地是匹配元素的乘积的和。[清单
    2.16](#ch02ex16)中的伪代码更精确地定义了内积。
- en: We should take from this that there is a relationship between the elements of
    the features and the elements of the kernel. For each individual feature element,
    such as “Crime rate” and “Nitric oxide concentration,” as listed in [table 2.1](#ch02table01),
    there is an associated learned number in the kernel. Each value tells us something
    about what the model has learned about this feature and how the feature influences
    the output.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该从中得出结论，特征的元素与核的元素之间存在关系。对于每个单独的特征元素，例如表[table 2.1](#ch02table01)中列出的“犯罪率”和“一氧化氮浓度”，核中都有一个关联的学到的数字。每个值告诉我们一些关于模型对这个特征学到了什么以及这个特征如何影响输出的信息。
- en: Listing 2.16\. Inner product pseudo-code
  id: totrans-358
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 2.16\. 内积伪代码
- en: '[PRE42]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: For instance, if the model learns that `kernel[i]` is positive, it means that
    the output will be larger if the `feature[i]` value is larger. Vice versa, if
    the model learns that `kernel[j]` is negative, then a larger value of `feature[j]`
    reduces the predicted output. A learned value that is very small in magnitude
    indicates that the model believes the associated feature has little impact on
    the prediction, whereas a learned value with a large magnitude indicates that
    the model places a heavy emphasis on the feature, and small changes in the feature
    value will have a comparatively large impact on the prediction.^([[8](#ch02fn8)])
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果模型学到了`kernel[i]`是正的，那么这意味着如果`feature[i]`的值较大，则输出将更大。反之，如果模型学到了`kernel[j]`是负的，那么较大的`feature[j]`值会减少预测的输出。学到的值在大小上非常小意味着模型认为相关特征对预测的影响很小，而具有大幅度的学习值则表明模型对该特征的重视程度很高，并且特征值的微小变化将对预测产生相对较大的影响。^([[8](#ch02fn8)])
- en: ⁸
  id: totrans-361
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁸
- en: ''
  id: totrans-362
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Note that comparing magnitudes in this way is only possible if the features
    have been normalized, as we have done for the Boston-housing dataset.
  id: totrans-363
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意，只有在特征已经被归一化的情况下，才能以这种方式比较其大小，就像我们为波士顿房屋数据集所做的那样。
- en: To make this concrete, the top five feature values, by absolute value, are printed
    in [figure 2.13](#ch02fig13) for one run in the output area of the Boston-housing
    example. Subsequent runs may learn different values due to the randomness of the
    initialization. We can see that the values are negative for features we would
    expect to reflect negatively on the price of real estate, such as the rate at
    which local residents drop out of school and the distance of the real estate to
    desirable working locations. Learned weights are positive for features we would
    expect to correlate directly with the price, such as the number of rooms in the
    property.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 为了具体化，根据绝对值排名，前五个特征值被打印在[图 2.13](#ch02fig13)中，以显示波士顿房屋示例的输出区域中的一个运行。由于初始化的随机性，后续运行可能会学到不同的值。我们可以看到对于我们期望对房地产价格产生负面影响的特征，例如当地居民辍学率和房地产距离理想工作地点的距离，其值是负的。对于我们期望与价格直接相关的特征，例如房产中的房间数量，学到的权重是正的。
- en: Figure 2.13\. Ranked by absolute value, these are the top five weights learned
    in one run of the linear model on the Boston-housing prediction problem. Note
    the negative values for features that you would expect to reflect negatively on
    the price of housing.
  id: totrans-365
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.13。根据绝对值排名，这是在波士顿房屋预测问题的线性模型的一个运行中学到的前五个权重。注意对那些你期望对房价产生负面影响的特征的负值。
- en: '![](02fig13.jpg)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
  zh: '![](02fig13.jpg)'
- en: 2.4.2\. Extracting internal weights from the model
  id: totrans-367
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.2。从模型中提取内部权重
- en: The modular structure of the learned model makes it easy to extract the relevant
    weights; we can access them directly, but there are a few API levels that need
    to be reached through in order to get the raw values. It’s important to keep in
    mind that, since the value may be on the GPU, and interdevice communication is
    costly, requesting such values is asynchronous. The boldface code in [listing
    2.17](#ch02ex17) is an addition to the `model.fit` callbacks, extending [listing
    2.14](#ch02ex14) to illustrate the learned weights after each epoch. We will walk
    through the API calls step-by-step.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 学到的模型的模块化结构使得提取相关权重变得容易；我们可以直接访问它们，但是有几个需要通过的 API 级别以获取原始值。重要的是要记住，由于值可能在 GPU
    上，而设备间通信是昂贵的，请求这些值是异步的。[列表 2.17](#ch02ex17) 中的粗体代码是对 `model.fit` 回调的补充，扩展了 [列表
    2.14](#ch02ex14) 以在每个 epoch 后说明学到的权重。我们将逐步讲解 API 调用。
- en: Given the model, we first wish to access the correct layer. This is easy because
    there is only one layer in this model, so we can get a handle to it at `model.layers[0]`.
    Now that we have the layer, we can access the internal weights with `getWeights()`,
    which returns an array of the weights. For the case of a dense layer, this will
    always contain two weights, the kernel and the bias, in that order. Thus, we can
    access the correct tensor at
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 给定模型，我们首先希望访问正确的层。这很容易，因为这个模型中只有一个层，所以我们可以在 `model.layers[0]` 处获得它的句柄。现在我们有了层，我们可以使用
    `getWeights()` 访问内部权重，它返回一个权重数组。对于密集层的情况，这将始终包含两个权重，即核和偏置，顺序是这样的。因此，我们可以在以下位置访问正确的张量：
- en: '[PRE43]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Now that we have the right tensor, we can access its contents with a call to
    its `data()` method. Due to the asynchronous nature of GPU ↔ CPU communication,
    `data()` is asynchronous and returns a promise of the tensor’s value, not the
    actual value. In [listing 2.17](#ch02ex17), a callback passed to the `then()`
    method of the promise binds the tensor values to a variable called `kernelAsArr`.
    If the `console.log()` statement is uncommented, statements like the following,
    listing the values of the kernel, are logged to the console once per each epoch:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了正确的张量，我们可以通过调用其 `data()` 方法来访问其内容。由于 GPU ↔ CPU 通信的异步性质，`data()` 是异步的，并返回张量值的一个承诺，而不是实际值。在
    [2.17 节](#ch02ex17) 中，通过将承诺的 `then()` 方法传递给回调函数，将张量值绑定到名为 `kernelAsArr` 的变量上。如果取消注释
    `console.log()` 语句，则像下面这样的语句，列出内核值，将在每个纪元结束时记录到控制台：
- en: '[PRE44]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Listing 2.17\. Accessing internal model values
  id: totrans-373
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 2.17\. 访问内部模型值
- en: '[PRE45]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 2.4.3\. Caveats on interpretability
  id: totrans-375
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.3\. 解释性的注意事项
- en: The weights in [figure 2.13](#ch02fig13) tell a story. As a human reader, you
    might look at this and say that the model has learned that the “Number of rooms
    per house” feature positively correlates with the price output or that the real-estate
    AGE feature, which is not listed due to its lower absolute magnitude, is of lower
    importance than those first five features. Because of the way our minds like to
    tell stories, it is common to take this too far and imagine these numbers say
    more than the evidence supports. For instance, one way that this sort of analysis
    can fail is if two input features are strongly correlated.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 2.13](#ch02fig13) 中的权重讲述了一个故事。作为人类读者，你可能会看到这个并说这个模型已经学会了“每栋房子的房间数”特征与价格输出呈正相关，或者房地产的
    AGE 特征，由于其较低的绝对大小而未列出，比这前五个特征的重要性要低。由于我们的大脑喜欢讲故事的方式，很容易就把这些数字说得比证据支持的要多。例如，如果两个输入特征强相关，这种分析的一种失败方式是。
- en: Consider a hypothetical example in which the same feature is included twice,
    perhaps by accident. Call them FEAT1 and FEAT2. Imagine the weights learned for
    the two features are 10 and –5\. You might be inclined to say that increasing
    FEAT1 leads to larger outputs, while FEAT2 does the opposite. However, since the
    features are equivalent, the model would output the exact same values if the weights
    were reversed.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个假想的例子，其中相同的特征被意外地包含了两次。称它们为 FEAT1 和 FEAT2。假设学习到的两个特征的权重分别为 10 和 -5。你可能会倾向于认为增加
    FEAT1 会导致输出增加，而 FEAT2 则相反。然而，由于这些特征是等价的，如果权重反转，模型将输出完全相同的值。
- en: Another caveat to be aware of is the difference between correlation and causality.
    Imagine a simple model in which we wish to predict how hard it is raining outside
    from how wet our roof is. If we had a measure of roof wetness, we could probably
    make a prediction of how much rain there had been in the past hour. We could not,
    however, splash water on the sensor to make it rain!
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个需要注意的地方是相关性与因果关系之间的区别。想象一个简单的模型，我们希望根据屋顶的湿度来预测外面下雨的程度。如果我们有一个屋顶湿度的测量值，我们可能可以预测过去一小时下了多少雨。但是，我们不能够向传感器泼水来制造雨！
- en: Exercises
  id: totrans-379
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习
- en: The hard-coded time estimation problem in [section 2.1](#ch02lev1sec1) was selected
    because the data is roughly linear. Other datasets will have different loss surfaces
    and dynamics during fitting. You may want to try substituting your own data here
    to explore how the model reacts. You may need to play with the learning rate,
    initialization, or normalization to get the model to converge to something interesting.
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 [2.1 节](#ch02lev1sec1) 中的硬编码时间估计问题之所以被选中，是因为数据大致上是线性的。其他数据集在拟合过程中将有不同的损失曲面和动态。您可能希望在这里尝试替换自己的数据，以探索模型的反应。您可能需要调整学习率、初始化或规范化来使模型收敛到一些有趣的东西。
- en: In [section 2.3.5](#ch02lev2sec14), we spent some time describing why normalization
    is important and how to normalize the input data to have zero mean and unit variance.
    You should be able to modify the example to remove the normalization and see that
    the model no longer trains. You should also be able to modify the normalization
    routine to have, for example, a mean of something other than 0 or a standard deviation
    that is lower, but not as low. Some normalizations will work, and some will lead
    to a model that never converges.
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 [2.3.5节](#ch02lev2sec14) 中，我们花了一些时间描述为什么归一化很重要以及如何将输入数据归一化为零均值和单位方差。你应该能够修改示例以去除归一化，并看到模型不再训练。你还应该能够修改归一化例程，例如，使均值不为0或标准偏差较低，但不是很低。有些归一化方法会奏效，有些会导致模型永远不收敛。
- en: 'It is well known that some features of the Boston Housing Prices dataset are
    more *predictive* of the target than others. Some of the features are merely noise
    in the sense that they don’t carry useful information for predicting housing prices.
    If we were to remove all but one feature, which feature should we keep? What if
    we were to keep two features: how can we select which ones? Play with the code
    in the Boston-housing example to explore this.'
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 众所周知，波士顿房价数据集的一些特征比其他特征更具有*预测性*。一些特征只是噪声，意味着它们不携带有用于预测房价的信息。如果我们只移除一个特征，我们应该保留哪个特征？如果我们要保留两个特征：我们该如何选择？尝试使用波士顿房价示例中的代码来探索这个问题。
- en: Describe how gradient descent allows for the optimization of a model by updating
    weights in a better-than-random way.
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 描述梯度下降如何通过以优于随机的方式更新权重来优化模型。
- en: The Boston-housing example prints out the top five weights by absolute magnitude.
    Try modifying the code to print out the features associated with small weights.
    Can you imagine why those weights are small? If someone were to ask you why those
    weights were what they were, what could you tell them? What sorts of cautions
    would you tell that person about how to interpret the values?
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 波士顿房价示例打印出了绝对值最大的五个权重。尝试修改代码以打印与小权重相关联的特征。你能想象为什么这些权重很小吗？如果有人问你这些权重为什么是什么，你可以告诉他们什么？你会告诉那个人如何解释这些值的时候要注意什么？
- en: Summary
  id: totrans-385
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: It is simple to build, train, and evaluate a simple machine-learning model in
    five lines of JavaScript using TensorFlow.js.
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow.js 在五行 JavaScript 中构建、训练和评估一个简单的机器学习模型非常简单。
- en: Gradient descent, the basic algorithm structure behind deep learning, is conceptually
    simple and really just means repeatedly updating model parameters in small steps
    in the calculated direction that would most improve the model fit.
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度下降，深度学习背后的基本算法结构，从概念上来说很简单，实际上只是指反复以小步骤更新模型参数，以使模型拟合最佳方向的计算方向。
- en: A model’s loss surface illustrates how well the model fits for a grid of parameter
    values. The loss surface is not generally calculable because of the high-dimensionality
    of the parameter space, but it’s illustrative to think about and gives intuition
    to how machine learning works.
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的损失曲面展示了模型在一系列参数值的拟合程度。损失曲面通常无法计算，因为参数空间的维数很高，但思考一下并对机器学习的工作方式有直观的理解是很有意义的。
- en: A single dense layer is sufficient to solve some simple problems and can achieve
    reasonable performance on a real-estate pricing problem.
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个单独的密集层足以解决一些简单的问题，并且在房地产定价问题上可以获得合理的性能。
