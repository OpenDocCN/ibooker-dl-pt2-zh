- en: 11 Deep learning for text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*This chapter covers*'
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing text data for machine learning applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bag-of-words approaches and sequence-modeling approaches for text processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Transformer architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequence-to-sequence learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '11.1 Natural language processing: The bird’s-eye view'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In computer science, we refer to human languages, like English or Mandarin,
    as “natural” languages, to distinguish them from languages that were designed
    for machines, like Assembly, LISP, or XML. Every machine language was *designed*:
    its starting point was a human engineer writing down a set of formal rules to
    describe what statements you could make in that language and what they meant.
    Rules came first, and people started using the language only once the rule set
    was complete. With human language, it’s the reverse: usage comes first, and rules
    arise later. Natural language was shaped by an evolution process, much like biological
    organisms— that’s what makes it “natural.” Its “rules,” like the grammar of English,
    were formalized after the fact and are often ignored or broken by its users. As
    a result, although'
  prefs: []
  type: TYPE_NORMAL
- en: machine-readable language is highly structured and rigorous, using precise syntactic
    rules to weave together exactly defined concepts from a fixed vocabulary, natural
    language is messy—ambiguous, chaotic, sprawling, and constantly in flux.
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating algorithms that can make sense of natural language is a big deal:
    language—and in particular, text—underpins most of our communications and our
    cultural production. The internet is mostly text. Language is how we store almost
    all of our knowledge. Our very thoughts are largely built upon language. However,
    the ability to understand natural language has long eluded machines. Some people
    once naively thought that you could simply write down the “rule set of English,”
    much like one can write down the rule set of LISP. Early attempts to build natural
    language processing (NLP) systems were thus made through the lens of “applied
    linguistics.” Engineers and linguists would handcraft complex sets of rules to
    perform basic machine translation or create simple chatbots, like the famous ELIZA
    program from the 1960s, which used pattern matching to sustain very basic conversation.
    But language is a rebellious thing: it’s not easily pliable to formalization.
    After several decades of effort, the capabilities of these systems remained disappointing.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Handcrafted rules held out as the dominant approach well into the 1990s. But
    starting in the late 1980s, faster computers and greater data availability started
    making a better alternative viable. When you find yourself building systems that
    are big piles of ad hoc rules, as a clever engineer, you’re likely to start asking:
    “Could I use a corpus of data to automate the process of finding these rules?
    Could I search for the rules within some kind of rule space, instead of having
    to come up with them myself?” And just like that, you’ve graduated to doing machine
    learning. And so, in the late 1980s, we started seeing machine learning approaches
    to natural language processing. The earliest ones were based on decision trees—the
    intent was literally to automate the development of the kind of if/then/else rules
    of previous systems. Then statistical approaches started gaining speed, starting
    with logistic regression. Over time, learned parametric models fully took over,
    and linguistics came to be seen as more of a hindrance than a useful tool. Frederick
    Jelinek, an early speech recognition researcher, joked in the 1990s: “Every time
    I fire a linguist, the performance of the speech recognizer goes up.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s what modern NLP is about: using machine learning and large datasets
    to give computers the ability not to *understand* language, which is a more lofty
    goal, but to ingest a piece of language as input and return something useful,
    like predicting the following:'
  prefs: []
  type: TYPE_NORMAL
- en: “What’s the topic of this text?” (text classification)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Does this text contain abuse?” (content filtering)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Does this text sound positive or negative?” (sentiment analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “What should be the next word in this incomplete sentence?” (language modeling)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “How would you say this in German?” (translation)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “How would you summarize this article in one paragraph?” (summarization)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, keep in mind throughout this chapter that the text-processing models
    you will train won’t possess a human-like understanding of language; rather, they
    simply look for statistical regularities in their input data, which turns out
    to be sufficient to perform well on many simple tasks. In much the same way that
    computer vision is pattern recognition applied to pixels, NLP is pattern recognition
    applied to words, sentences, and paragraphs.
  prefs: []
  type: TYPE_NORMAL
- en: The toolset of NLP—decision trees and logistic regression—saw only slow evolution
    from the 1990s to the early 2010s. Most of the research focus was on feature engineering.
    When I (François) won my first NLP competition on Kaggle in 2013, my model was,
    you guessed it, based on decision trees and logistic regression. However, around
    2014–2015, things started changing at last. Multiple researchers began to investigate
    the language-understanding capabilities of recurrent neural networks, in particular
    LSTM—a sequence-processing algorithm from the late 1990s that had stayed under
    the radar until then.
  prefs: []
  type: TYPE_NORMAL
- en: In early 2015, Keras made available the first open source, easy-to-use implementation
    of LSTM, just at the start of a massive wave of renewed interest in recurrent
    neural networks. Until then, there had only been “research code” that couldn’t
    be readily reused. Then from 2015 to 2017, recurrent neural networks dominated
    the booming NLP scene. Bidirectional LSTM models, in particular, set the state
    of the art on many important tasks, from summarization to question-answering to
    machine translation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, around 2017–2018, a new architecture rose to replace RNNs: the Transformer,
    which you will learn about in the second half of this chapter. Transformers unlocked
    considerable progress across the field in a short period of time, and today most
    NLP systems are based on them.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dive into the details. This chapter will take you from the very basics
    to doing machine translation with a Transformer.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2 Preparing text data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Deep learning models, being differentiable functions, can process only numeric
    tensors: they can’t take raw text as input. *Vectorizing* text is the process
    of transforming text into numeric tensors. Text vectorization processes come in
    many shapes and forms, but they all follow the same template (see [figure 11.1](#fig11-1)):'
  prefs: []
  type: TYPE_NORMAL
- en: First, you *standardize* the text to make it easier to process, such as by converting
    it to lowercase or removing punctuation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You split the text into units (called *tokens*), such as characters, words,
    or groups of words. This is called *tokenization*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You convert each such token into a numerical vector. This will usually involve
    first *indexing* all tokens present in the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s review each of these steps.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0337-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 11.1 From raw text to vectors**'
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.1 Text standardization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Consider these two sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: “sunset came. i was staring at the Mexico sky. Isnt nature splendid??”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Sunset came; I stared at the México sky. Isn’t nature splendid?”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They’re very similar—in fact, they’re almost identical. Yet, if you were to
    convert them to byte strings, they would end up with very different representations,
    because “i” and “I” are two different characters, “Mexico” and “México” are two
    different words, “isnt” isn’t “isn’t,” and so on. A machine learning model doesn’t
    know a priori that “i” and “I” are the same letter, that “é” is an “e” with an
    accent, or that “staring” and “stared” are two forms of the same verb.
  prefs: []
  type: TYPE_NORMAL
- en: Text standardization is a basic form of feature engineering that aims to erase
    encoding differences that you don’t want your model to have to deal with. It’s
    not exclusive to machine learning, either—you’d have to do the same thing if you
    were building a search engine.
  prefs: []
  type: TYPE_NORMAL
- en: One of the simplest and most widespread standardization schemes is “convert
    to lowercase and remove punctuation characters.” Our two sentences would become
  prefs: []
  type: TYPE_NORMAL
- en: “sunset came i was staring at the mexico sky isnt nature splendid”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “sunset came i stared at the méxico sky isnt nature splendid”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Much closer already. Another common transformation is to convert special characters
    to a standard form, such as replacing “é” with “e,” “æ” with “ae,” and so on.
    Our token “méxico” would then become “mexico”.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, a much more advanced standardization pattern that is more rarely used
    in a machine learning context is *stemming*: converting variations of a term (such
    as different conjugated forms of a verb) into a single shared representation,
    like turning “caught” and “been catching” into “[catch]” or “cats” into “[cat]”.
    With stemming, “was staring” and “stared” would become something like “[stare]”,
    and our two similar sentences would finally end up with an identical encoding:'
  prefs: []
  type: TYPE_NORMAL
- en: “sunset came i [stare] at the mexico sky isnt nature splendid”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With these standardization techniques, your model will require less training
    data and will generalize better—it won’t need abundant examples of both “Sunset”
    and “sunset” to learn that they mean the same thing, and it will be able to make
    sense of “México”, even if it has only seen “mexico” in its training set. Of course,
    standardization may also erase some amount of information, so always keep the
    context in mind: for instance, if you’re writing a model that extracts questions
    from interview articles, it should definitely treat “?” as a separate token instead
    of dropping it, because it’s a useful signal for this specific task.'
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.2 Text splitting (tokenization)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once your text is standardized, you need to break it up into units to be vectorized
    (tokens), a step called *tokenization*. You could do this in three different ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Word-level tokenization*—Where tokens are space-separated (or punctuation-separated)
    substrings. A variant of this is to further split words into subwords when applicable,
    for instance, treating “staring” as “star+ing” or “called” as “call+ed.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*N-gram tokenization*—Where tokens are groups of *N* consecutive words. For
    instance, “the cat” or “he was” would be 2-gram tokens (also called bigrams).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Character-level tokenization*—Where each character is its own token. In practice,
    this scheme is rarely used, and you only really see it in specialized contexts,
    like text generation or speech recognition.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In general, you’ll always use either word-level or *N*-gram tokenization. There
    are two kinds of text-processing models: those that care about word order, called
    *sequence models*, and those that treat input words as a set, discarding their
    original order, called *bag-of-words models*. If you’re building a sequence model,
    you’ll use word-level tokenization, and if you’re building a bag-of-words model,
    you’ll use *N*-gram tokenization. *N*-grams are a way to artificially inject a
    small amount of local word-order information into the model. Throughout this chapter,
    you’ll learn more about each type of model and when to use them.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Understanding *N*-grams and bag-of-words**'
  prefs: []
  type: TYPE_NORMAL
- en: Word *N*-grams are groups of *N* (or fewer) consecutive words that you can extract
    from a sentence. The same concept may also be applied to characters instead of
    words.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a simple example. Consider the sentence “the cat sat on the mat.” It
    may be decomposed into the following set of 2-grams:'
  prefs: []
  type: TYPE_NORMAL
- en: c("the", "the cat", "cat", "cat sat", "sat",
  prefs: []
  type: TYPE_NORMAL
- en: '"sat on", "on", "on the", "the mat", "mat")'
  prefs: []
  type: TYPE_NORMAL
- en: 'It may also be decomposed into the following set of 3-grams:'
  prefs: []
  type: TYPE_NORMAL
- en: c("the", "the cat", "cat", "cat sat", "the cat sat",
  prefs: []
  type: TYPE_NORMAL
- en: '"sat", "sat on", "on", "cat sat on", "on the",'
  prefs: []
  type: TYPE_NORMAL
- en: '"sat on the", "the mat", "mat", "on the mat")'
  prefs: []
  type: TYPE_NORMAL
- en: 'Such a set is called a *bag-of-2-grams or bag-of-3-grams*, respectively. The
    term “bag” here refers to the fact that you’re dealing with a set of tokens rather
    than a list or sequence: the tokens have no specific order. This family of tokenization
    methods is called *bag-of-words* (or *bag-of-N-grams*).'
  prefs: []
  type: TYPE_NORMAL
- en: Because bag-of-words isn’t an order-preserving tokenization method (the tokens
    generated are understood as a set, not a sequence, and the general structure of
    the sentences is lost), it tends to be used in shallow language-processing models
    rather than in deep learning models. Extracting *N*-grams is a form of feature
    engineering, and deep learning sequence models do away with this manual approach,
    replacing it with hierarchical feature learning. One-dimensional convnets, recurrent
    neural networks, and Transformers are capable of learning representations for
    groups of words and characters without being explicitly told about the existence
    of such groups, by looking at continuous word or character sequences.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.3 Vocabulary indexing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once your text is split into tokens, you need to encode each token into a numerical
    representation. You could potentially do this in a stateless way, such as by hashing
    each token into a fixed binary vector, but in practice, the way you’d go about
    it is to build an index of all terms found in the training data (the “vocabulary”),
    and assign a unique integer to each entry in the vocabulary, something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: vocabulary <- character()
  prefs: []
  type: TYPE_NORMAL
- en: for (string in text_dataset) {
  prefs: []
  type: TYPE_NORMAL
- en: tokens <- string %>%
  prefs: []
  type: TYPE_NORMAL
- en: standardize() %>%
  prefs: []
  type: TYPE_NORMAL
- en: tokenize()
  prefs: []
  type: TYPE_NORMAL
- en: vocabulary <- unique(c(vocabulary, tokens))
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can then convert the integer index position into a vector encoding that
    can be processed by a neural network, like a one-hot vector:'
  prefs: []
  type: TYPE_NORMAL
- en: one_hot_encode_token <- function(token) {
  prefs: []
  type: TYPE_NORMAL
- en: vector <- array(0, dim = length(vocabulary))
  prefs: []
  type: TYPE_NORMAL
- en: token_index <- match(token, vocabulary)
  prefs: []
  type: TYPE_NORMAL
- en: vector[token_index] <- 1
  prefs: []
  type: TYPE_NORMAL
- en: vector
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: Note that at this step it’s common to restrict the vocabulary to only the top
    20,000 or 30,000 most common words found in the training data. Any text dataset
    tends to feature an extremely large number of unique terms, most of which show
    up only once or twice. Indexing those rare terms would result in an excessively
    large feature space, where most features would have almost no information content.
  prefs: []
  type: TYPE_NORMAL
- en: Remember when you were training your first deep learning models on the IMDB
    dataset in chapters 4 and 5? The data you were using from dataset_imdb() was already
    preprocessed into sequences of integers, where each integer stood for a given
    word. Back then, we used the setting num_words = 10000, to restrict our vocabulary
    to the top 10,000 most common words found in the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, there’s an important detail here that we shouldn’t overlook: when we look
    up a new token in our vocabulary index, it may not necessarily exist. Your training
    data may not have contained any instance of the word “cherimoya” (or maybe you
    excluded it from your index because it was too rare), so doing token_index = match(“cherimoya”,
    vocabulary) may return NA. To handle this, you should use an “out of vocabulary”
    index (abbreviated as *OOV index*)—a catch-all for any token that wasn’t in the
    index. It’s usually index 1: you’re actually doing token_index = match(“cherimoya”,
    vocabulary, nomatch = 1). When decoding a sequence of integers back into words,
    you’ll replace 1 with something like “[UNK]” (which you’d call an “OOV token”).'
  prefs: []
  type: TYPE_NORMAL
- en: '“Why use 1 and not 0?” you may ask. That’s because 0 is already taken. There
    are two special tokens that you will commonly use: the OOV token (index 1), and
    the *mask token* (index 0). Although the OOV token means “here was a word we did
    not recognize,” the mask token tells us “ignore me, I’m not a word.” You’d use
    it in particular to pad sequence data: because data batches need to be contiguous,
    all sequences in a batch of sequence data must have the same length, so shorter
    sequences should be padded to the length of the longest sequence. If you want
    to make a batch of data with the sequences c(5, 7, 124, 4, 89) and c(8, 34, 21),
    it would have to look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: rbind(c(5,  7, 124, 4, 89),
  prefs: []
  type: TYPE_NORMAL
- en: c(8, 34,  21, 0,  0))
  prefs: []
  type: TYPE_NORMAL
- en: The batches of integer sequences for the IMDB dataset that you worked with in
    chapters 4 and 5 were padded with zeros in this way.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.4 Using layer_text_vectorization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Every step I’ve introduced so far would be very easy to implement in pure R.
    Maybe you could write something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: new_vectorizer <- function() {
  prefs: []
  type: TYPE_NORMAL
- en: self <- new.env(parent = emptyenv())
  prefs: []
  type: TYPE_NORMAL
- en: attr(self, "class") <- "Vectorizer"
  prefs: []
  type: TYPE_NORMAL
- en: self$vocabulary <- c("[UNK]")
  prefs: []
  type: TYPE_NORMAL
- en: self$standardize <- function(text) {
  prefs: []
  type: TYPE_NORMAL
- en: text <- tolower(text)
  prefs: []
  type: TYPE_NORMAL
- en: gsub("[[:punct:]]", "", text)➊
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: self$tokenize <- function(text) {
  prefs: []
  type: TYPE_NORMAL
- en: unlist(strsplit(text, "[[:space:]]+"))➋
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: self$make_vocabulary <- function(text_dataset) {➌
  prefs: []
  type: TYPE_NORMAL
- en: tokens <- text_dataset %>%
  prefs: []
  type: TYPE_NORMAL
- en: self$standardize() %>%
  prefs: []
  type: TYPE_NORMAL
- en: self$tokenize()
  prefs: []
  type: TYPE_NORMAL
- en: self$vocabulary <- unique(c(self$vocabulary, tokens))
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: self$encode <- function(text) {
  prefs: []
  type: TYPE_NORMAL
- en: tokens <- text %>%
  prefs: []
  type: TYPE_NORMAL
- en: self$standardize() %>%
  prefs: []
  type: TYPE_NORMAL
- en: self$tokenize()
  prefs: []
  type: TYPE_NORMAL
- en: match(tokens, table = self$vocabulary, nomatch = 1)➍
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: self$decode <- function(int_sequence) {
  prefs: []
  type: TYPE_NORMAL
- en: vocab_w_mask_token <- c("", self$vocabulary)
  prefs: []
  type: TYPE_NORMAL
- en: vocab_w_mask_token[int_sequence + 1]➎
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: self
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: vectorizer <- new_vectorizer()
  prefs: []
  type: TYPE_NORMAL
- en: dataset <- c("I write, erase, rewrite",➏
  prefs: []
  type: TYPE_NORMAL
- en: '"Erase again, and then",'
  prefs: []
  type: TYPE_NORMAL
- en: '"A poppy blooms.")'
  prefs: []
  type: TYPE_NORMAL
- en: vectorizer$make_vocabulary(dataset)
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Remove punctuation.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Split on whitespace and return a flattened character vector.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **text_dataset will be a vector of strings, that is, an R character vector.**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **nomatch matches to "[UNK]".**
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **The mask token is typically encoded as a 0 integer, and decoded as an empty
    string:"".**
  prefs: []
  type: TYPE_NORMAL
- en: ➏ **Haiku by poet Hokushi**
  prefs: []
  type: TYPE_NORMAL
- en: 'It does the job:'
  prefs: []
  type: TYPE_NORMAL
- en: test_sentence <- "I write, rewrite, and still rewrite again"
  prefs: []
  type: TYPE_NORMAL
- en: encoded_sentence <- vectorizer$encode(test_sentence)
  prefs: []
  type: TYPE_NORMAL
- en: print(encoded_sentence)
  prefs: []
  type: TYPE_NORMAL
- en: '[1] 2 3 5 7 1 5 6'
  prefs: []
  type: TYPE_NORMAL
- en: decoded_sentence <- vectorizer$decode(encoded_sentence)
  prefs: []
  type: TYPE_NORMAL
- en: print(decoded_sentence)
  prefs: []
  type: TYPE_NORMAL
- en: '[1] "i"      "write"   "rewrite" "and"      "[UNK]"   "rewrite" "again"'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, using something like this wouldn’t be very performant. In practice,
    you’ll work with the Keras layer_text_vectorization(), which is fast and efficient
    and can be dropped directly into a TF Dataset pipeline or a Keras model. This
    is what layer_ text_vectorization() looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: text_vectorization <
  prefs: []
  type: TYPE_NORMAL
- en: layer_text_vectorization(output_mode = "int")➊
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Configure the layer to return sequences of words encoded as integer indices.
    There are several other output modes available, which you will see in action in
    a bit.**
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, layer_text_vectorization() will use the setting “convert to lowercase
    and remove punctuation” for text standardization, and “split on whitespace” for
    tokenization. But importantly, you can provide custom functions for standardization
    and tokenization, which means the layer is flexible enough to handle any use case.
    Note that such custom functions should operate on tf.string dtype tensors, not
    regular R character vectors! For instance, the default layer behavior is equivalent
    to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: library(tensorflow)
  prefs: []
  type: TYPE_NORMAL
- en: custom_standardization_fn <- function(string_tensor) {
  prefs: []
  type: TYPE_NORMAL
- en: string_tensor %>%
  prefs: []
  type: TYPE_NORMAL
- en: tf$strings$lower() %>% ➊
  prefs: []
  type: TYPE_NORMAL
- en: tf$strings$regex_replace("[[:punct:]]", "")➋
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: custom_split_fn <- function(string_tensor) {
  prefs: []
  type: TYPE_NORMAL
- en: tf$strings$split(string_tensor)➌
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: text_vectorization <- layer_text_vectorization(
  prefs: []
  type: TYPE_NORMAL
- en: output_mode = "int",
  prefs: []
  type: TYPE_NORMAL
- en: standardize = custom_standardization_fn,
  prefs: []
  type: TYPE_NORMAL
- en: split = custom_split_fn
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Convert strings to lowercase.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Replace punctuation characters with the empty string.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Split strings on whitespace.**
  prefs: []
  type: TYPE_NORMAL
- en: 'To index the vocabulary of a text corpus, just call the adapt() method of the
    layer with a TF Dataset object that yields strings, or just with an R character
    vector:'
  prefs: []
  type: TYPE_NORMAL
- en: dataset <- c("I write, erase, rewrite",
  prefs: []
  type: TYPE_NORMAL
- en: '"Erase again, and then",'
  prefs: []
  type: TYPE_NORMAL
- en: '"A poppy blooms.")'
  prefs: []
  type: TYPE_NORMAL
- en: adapt(text_vectorization, dataset)
  prefs: []
  type: TYPE_NORMAL
- en: Note that you can retrieve the computed vocabulary via get_vocabulary(). This
    can be useful if you need to convert text encoded as integer sequences back into
    words. The first two entries in the vocabulary are the mask token (index 0) and
    the OOV token (index 1). Entries in the vocabulary list are sorted by frequency,
    so with a real-world dataset, very common words like “the” or “a” would come first.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.1 Displaying the vocabulary**'
  prefs: []
  type: TYPE_NORMAL
- en: get_vocabulary(text_vectorization)
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0343-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For a demonstration, let’s try to encode and then decode an example sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: vocabulary <- text_vectorization %>% get_vocabulary()
  prefs: []
  type: TYPE_NORMAL
- en: test_sentence <- "I write, rewrite, and still rewrite again"
  prefs: []
  type: TYPE_NORMAL
- en: encoded_sentence <- text_vectorization(test_sentence)
  prefs: []
  type: TYPE_NORMAL
- en: decoded_sentence <- paste(vocabulary[as.integer(encoded_sentence) + 1],
  prefs: []
  type: TYPE_NORMAL
- en: collapse = " ")
  prefs: []
  type: TYPE_NORMAL
- en: encoded_sentence
  prefs: []
  type: TYPE_NORMAL
- en: tf.Tensor([ 7  3  5  9  1  5  10], shape=(7), dtype=int64)
  prefs: []
  type: TYPE_NORMAL
- en: decoded_sentence
  prefs: []
  type: TYPE_NORMAL
- en: '[1] "i write rewrite and [UNK] rewrite again"'
  prefs: []
  type: TYPE_NORMAL
- en: '**Using layer_text_vectorization() in a TF Dataset pipeline or as part of a
    model**'
  prefs: []
  type: TYPE_NORMAL
- en: Because layer_text_vectorization() is mostly a dictionary lookup operation that
    converts tokens to integers, it can’t be executed on a GPU (or TPU)—only on a
    CPU. So, if you’re training your model on a GPU, your layer_text_vectorization()
    will run on the CPU before sending its output to the GPU. This has important performance
    implications.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two ways we could use our layer_text_vectorization(). The first option
    is to put it in the TF Dataset pipeline, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: int_sequence_dataset <- string_dataset %>%➊
  prefs: []
  type: TYPE_NORMAL
- en: dataset_map(text_vectorization,
  prefs: []
  type: TYPE_NORMAL
- en: num_parallel_calls = 4)➋
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **string_dataset would be a TF Dataset that yields string tensors.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **The num_parallel_calls argument is used to parallelize the dataset_map()
    call across multiple CPU cores.**
  prefs: []
  type: TYPE_NORMAL
- en: 'The second option is to make it part of the model (after all, it’s a Keras
    layer), like this (in pseudocode):'
  prefs: []
  type: TYPE_NORMAL
- en: text_input <- layer_input(shape = shape(), dtype = "string")➊
  prefs: []
  type: TYPE_NORMAL
- en: vectorized_text <- text_vectorization(text_input)➋
  prefs: []
  type: TYPE_NORMAL
- en: embedded_input <- vectorized_text %>% layer_embedding(…)
  prefs: []
  type: TYPE_NORMAL
- en: output <- embedded_input %>% …➌
  prefs: []
  type: TYPE_NORMAL
- en: model <- keras_model(text_input, output)
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Create a symbolic input that expects strings.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Apply the text vectorization layer to it.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **You can keep chaining new layers on top— just your regular Functional API
    model.**
  prefs: []
  type: TYPE_NORMAL
- en: 'There’s an important difference between the two: if the vectorization step
    is part of the model, it will happen synchronously with the rest of the model.
    This means that at each training step, the rest of the model (placed on the GPU)
    will have to wait for the output of the layer_text_vectorization() (placed on
    the CPU) to be ready before it can get to work. Meanwhile, putting the layer in
    the TF Dataset pipeline enables you to do asynchronous preprocessing of your data
    on CPU: while the GPU runs the model on one batch of vectorized data, the CPU
    stays busy by vectorizing the next batch of raw strings.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re training the model on GPU or TPU, you’ll probably want to go with
    the first option to get the best performance. This is what we will do in all practical
    examples throughout this chapter. When training on a CPU, though, synchronous
    processing is fine: you will get 100% utilization of your cores, regardless of
    which option you go with.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, if you were to export our model to a production environment, you would
    want to ship a model that accepts raw strings as input, like in the code snippet
    for the second option above; otherwise, you would have to reimplement text standardization
    and tokenization in your production environment (maybe in JavaScript?), and you
    would face the risk of introducing small preprocessing discrepancies that would
    hurt the model’s accuracy. Thankfully, the layer_text_vectorization() enables
    you to include text preprocessing right into your model, making it easier to deploy,
    even if you were originally using the layer as part of a TF Dataset pipeline.
    In the sidebar box later in the chapter, “Exporting a model that processes raw
    strings,” you’ll learn how to export an inference-only trained model that incorporates
    text preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: You’ve now learned everything you need to know about text preprocessing. Let’s
    move on to the modeling stage.
  prefs: []
  type: TYPE_NORMAL
- en: '11.3 Two approaches for representing groups of words: Sets and sequences'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'How a machine learning model should represent *individual words* is a relatively
    uncontroversial question: they’re categorical features (values from a predefined
    set), and we know how to handle those. They should be encoded as dimensions in
    a feature space, or as category vectors (word vectors in this case). A much more
    problematic question, however, is how to encode *the way words are woven into
    sentences*: word order.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The problem of order in natural language is an interesting one: unlike the
    steps of a time series, words in a sentence don’t have a natural, canonical order.
    Different languages order similar words in very different ways. For instance,
    the sentence structure of English is quite different from that of Japanese. Even
    within a given language, you can typically say the same thing in different ways
    by reshuffling the words a bit. Even further, if you fully randomize the words
    in a short sentence, you can still largely figure out what it was saying, though
    in many cases, significant ambiguity seems to arise. Order is clearly important,
    but its relationship to meaning isn’t straightforward.'
  prefs: []
  type: TYPE_NORMAL
- en: 'How to represent word order is the pivotal question from which different kinds
    of NLP architectures spring. The simplest thing you could do is just discard order
    and  treat text as an unordered set of words—this gives you bag-of-words models.
    You could also decide that words should be processed strictly in the order in
    which they appear, one at a time, like steps in a time series—you could then leverage
    the recurrent models from the last chapter. Finally, a hybrid approach is also
    possible: the Transformer architecture is technically order agnostic, yet it injects
    word-position information into the representations it processes, which enables
    it to simultaneously look at different parts of a sentence (unlike RNNs) while
    still being order aware. Because they take into account word order, both RNNs
    and Transformers are called *sequence models*.'
  prefs: []
  type: TYPE_NORMAL
- en: Historically, most early applications of machine learning to NLP just involved
    bagof-words models. Interest in sequence models started rising only in 2015, with
    the rebirth of recurrent neural networks. Today, both approaches remain relevant.
    Let’s see how they work and when to leverage which.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll demonstrate each approach on a well-known text classification benchmark:
    the IMDB movie review sentiment-classification dataset. In chapters 4 and 5, you
    worked with a prevectorized version of the IMDB dataset; now let’s process the
    raw IMDB text data, just like you would do when approaching a new text-classification
    problem in the real world.'
  prefs: []
  type: TYPE_NORMAL
- en: 11.3.1 Preparing the IMDB movie reviews data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s start by downloading the dataset from the Stanford page of Andrew Maas
    and uncompressing it:'
  prefs: []
  type: TYPE_NORMAL
- en: url <- "https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
  prefs: []
  type: TYPE_NORMAL
- en: filename <- basename(url)
  prefs: []
  type: TYPE_NORMAL
- en: options(timeout = 60 * 10)➊
  prefs: []
  type: TYPE_NORMAL
- en: download.file(url, destfile = filename)
  prefs: []
  type: TYPE_NORMAL
- en: untar(filename)
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **10-minute timeout**
  prefs: []
  type: TYPE_NORMAL
- en: 'You’re left with a directory named aclImdb, with the following structure:'
  prefs: []
  type: TYPE_NORMAL
- en: fs::dir_tree("aclImdb", recurse = 1, type = "directory")
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0345-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For instance, the train/pos/ directory contains a set of 12,500 text files,
    each of which contains the text body of a positive-sentiment movie review to be
    used as training data. The negative-sentiment reviews live in the “neg” directories.
    In total, there are 25,000 text files for training and another 25,000 for testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'There’s also a train/unsup subdirectory in there, which we don’t need. Let’s
    delete it:'
  prefs: []
  type: TYPE_NORMAL
- en: fs::dir_delete("aclImdb/train/unsup/")
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the content of a few of these text files. Whether you’re working
    with text data or image data, remember to always inspect what your data looks
    like before you dive into modeling it. It will ground your intuition about what
    your model is actually doing:'
  prefs: []
  type: TYPE_NORMAL
- en: writeLines(readLines("aclImdb/train/pos/4077_10.txt", warn = FALSE))
  prefs: []
  type: TYPE_NORMAL
- en: I first saw this back in the early 90s on UK TV, i did like it then but i missed
    the chance to tape it, many years passed but the film always stuck with me and
    i lost hope of seeing it TV again, the main thing that stuck with me was the end,
    the hole castle part really touched me, its easy to watch, has a great story,
    great music, the list goes on and on, its OK me saying how good it is but everyone
    will take there own best bits away with them once they have seen it, yes the animation
    is top notch and beautiful to watch, it does show its age in a very few parts
    but that has now become part of it beauty, i am so glad it has came out on DVD
    as it is one of my top 10 films of all time. Buy it or rent it just see it, best
    viewing is at night alone with drink and food in reach so you don’t have to stop
    the film.<br /><br />Enjoy
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s prepare a validation set by setting apart 20% of the training text
    files in a new directory, aclImdb/val. As before, we’ll use the fs R package:'
  prefs: []
  type: TYPE_NORMAL
- en: library(fs)
  prefs: []
  type: TYPE_NORMAL
- en: set.seed(1337)➊
  prefs: []
  type: TYPE_NORMAL
- en: base_dir <- path("aclImdb")
  prefs: []
  type: TYPE_NORMAL
- en: for (category in c("neg", "pos")) {
  prefs: []
  type: TYPE_NORMAL
- en: filepaths <- dir_ls(base_dir / "train" / category)
  prefs: []
  type: TYPE_NORMAL
- en: num_val_samples <- round(0.2 * length(filepaths))➋
  prefs: []
  type: TYPE_NORMAL
- en: val_files <- sample(filepaths, num_val_samples)
  prefs: []
  type: TYPE_NORMAL
- en: dir_create(base_dir / "val" / category)
  prefs: []
  type: TYPE_NORMAL
- en: file_move(val_files, ➌
  prefs: []
  type: TYPE_NORMAL
- en: base_dir / "val" / category)
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Set a seed, to ensure we get the same validation set from the sample() call
    every time we run the code.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Take 20% of the training files to use for validation.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Move the files to aclImdb/val/neg and aclImdb/val/pos.**
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember how, in chapter 8, we used the image_dataset_from_directory() utility
    to create a batched TF Dataset of images and their labels for a directory structure?
    You can do the exact same thing for text files using the text_dataset_from_directory()
    utility. Let’s create three TF Dataset objects for training, validation, and testing:'
  prefs: []
  type: TYPE_NORMAL
- en: library(keras)
  prefs: []
  type: TYPE_NORMAL
- en: library(tfdatasets)
  prefs: []
  type: TYPE_NORMAL
- en: train_ds <- text_dataset_from_directory("aclImdb/train")➊
  prefs: []
  type: TYPE_NORMAL
- en: val_ds <- text_dataset_from_directory("aclImdb/val")
  prefs: []
  type: TYPE_NORMAL
- en: test_ds <- text_dataset_from_directory("aclImdb/test")➋
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Running this line should output "Found 20000 files belonging to 2 classes";
    if you see "Found 70000 files belonging to 3 classes," it means you forgot to
    delete the aclImdb/train/unsup directory.**
  prefs: []
  type: TYPE_NORMAL
- en: '➋ **The default batch_size is 32\. If you encounter out-of-memory errors when
    training models on your machine, you can try a smaller batch_size: text_dataset_from_directory("aclImdb/train",
    batch_size = 8).**'
  prefs: []
  type: TYPE_NORMAL
- en: These datasets yield inputs that are TensorFlow tf.string tensors and targets
    that are int32 tensors encoding the value “0” or “1.”
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.2 Displaying the shapes and dtypes of the first batch**'
  prefs: []
  type: TYPE_NORMAL
- en: c(inputs, targets) %<-% iter_next(as_iterator(train_ds))
  prefs: []
  type: TYPE_NORMAL
- en: str(inputs)
  prefs: []
  type: TYPE_NORMAL
- en: 'tf.Tensor: shape=(32), dtype=string, numpy=…>'
  prefs: []
  type: TYPE_NORMAL
- en: str(targets)
  prefs: []
  type: TYPE_NORMAL
- en: 'tf.Tensor: shape=(32), dtype=int32, numpy=…>'
  prefs: []
  type: TYPE_NORMAL
- en: inputs[1]
  prefs: []
  type: TYPE_NORMAL
- en: tf.Tensor(b’Let me start by saying that I\’d read a number of reviews before
    renting this film and kind of knew what to expect. Still, I was surprised by just
    how bad it was. <br /><br />I am a big werewolf fan, and have grown …
  prefs: []
  type: TYPE_NORMAL
- en: Otherwise, give this one a miss.’, shape=(), dtype=string)
  prefs: []
  type: TYPE_NORMAL
- en: targets[1]
  prefs: []
  type: TYPE_NORMAL
- en: tf.Tensor(0, shape=(), dtype=int32)
  prefs: []
  type: TYPE_NORMAL
- en: All set. Now let’s try learning something from this data.
  prefs: []
  type: TYPE_NORMAL
- en: '11.3.2 Processing words as a set: The bag-of-words approach'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The simplest way to encode a piece of text for processing by a machine learning
    model is to discard order and treat it as a set (a “bag”) of tokens. You could
    either look at individual words (unigrams) or try to recover some local order
    information by looking at groups of consecutive tokens (*N*-grams).
  prefs: []
  type: TYPE_NORMAL
- en: SINGLE WORDS (UNIGRAMS) WITH BINARY ENCODING
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you use a bag of single words, the sentence “the cat sat on the mat” becomes
    a character vector where we ignore order:'
  prefs: []
  type: TYPE_NORMAL
- en: c("cat", "mat", "on", "sat", "the")
  prefs: []
  type: TYPE_NORMAL
- en: The main advantage of this encoding is that you can represent an entire text
    as a single vector, where each entry is a presence indicator for a given word.
    For instance, using binary encoding (multi-hot), you’d encode a text as a vector
    with as many dimensions as there are words in your vocabulary, with 0s almost
    everywhere and some 1s for dimensions that encode words present in the text. This
    is what we did when we worked with text data in chapters 4 and 5\. Let’s try this
    on our task.
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s process our raw text datasets with a layer_text_vectorization()
    layer so that they yield multi-hot-encoded binary word vectors. Our layer will
    look only at single words (that is to say, *unigrams*).
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.3 Preprocessing our datasets with layer_text_vectorization()**'
  prefs: []
  type: TYPE_NORMAL
- en: text_vectorization <-
  prefs: []
  type: TYPE_NORMAL
- en: layer_text_vectorization(max_tokens = 20000,➊
  prefs: []
  type: TYPE_NORMAL
- en: output_mode = "multi_hot")➋
  prefs: []
  type: TYPE_NORMAL
- en: text_only_train_ds <- train_ds %>%➌
  prefs: []
  type: TYPE_NORMAL
- en: dataset_map(function(x, y) x)
  prefs: []
  type: TYPE_NORMAL
- en: adapt(text_vectorization, text_only_train_ds)➍
  prefs: []
  type: TYPE_NORMAL
- en: binary_1gram_train_ds <- train_ds %>%➎
  prefs: []
  type: TYPE_NORMAL
- en: dataset_map( ~ list(text_vectorization(.x), .y),
  prefs: []
  type: TYPE_NORMAL
- en: num_parallel_calls = 4)
  prefs: []
  type: TYPE_NORMAL
- en: binary_1gram_val_ds <- val_ds %>%
  prefs: []
  type: TYPE_NORMAL
- en: dataset_map( ~ list(text_vectorization(.x), .y),
  prefs: []
  type: TYPE_NORMAL
- en: num_parallel_calls = 4)
  prefs: []
  type: TYPE_NORMAL
- en: binary_1gram_test_ds <- test_ds %>%
  prefs: []
  type: TYPE_NORMAL
- en: dataset_map( ~ list(text_vectorization(.x), .y),
  prefs: []
  type: TYPE_NORMAL
- en: num_parallel_calls = 4)
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Limit the vocabulary to the 20,000 most frequent words. Otherwise we'd be
    indexing every word in the training data—potentially tens of thousands of terms
    that occur only once or twice and thus aren't informative. In general, 20,000
    is the right vocabulary size for text classification.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Encode the output tokens as multi-hot binary vectors.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Prepare a dataset that yields only raw text inputs (no labels).**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Use that dataset to index the dataset vocabulary via the adapt() method.**
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **Prepare processed versions of our training, validation, and test dataset.
    Make sure to specify num_parallel_calls to leverage multiple CPU cores.**
  prefs: []
  type: TYPE_NORMAL
- en: '**~ formula function definition**'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the map_func argument to dataset_map(), we passed a formula defined witĥ,
    not a function. If the map_func argument is a formula, e.g. ~ .x + 2, it is converted
    to a function. There are three ways to refer to the arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: For a single argument function, use .x.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a two argument function, use .x and .y.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more arguments, use ..1, ..2, ..3 and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This syntax allows you to create very compact anonymous functions. For more
    details and examples, see the ?purrr::map() help page in R.
  prefs: []
  type: TYPE_NORMAL
- en: You can try to inspect the output of one of these datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.4 Inspecting the output of our binary unigram dataset**'
  prefs: []
  type: TYPE_NORMAL
- en: c(inputs, targets) %<-% iter_next(as_iterator(binary_1gram_train_ds))
  prefs: []
  type: TYPE_NORMAL
- en: str(inputs)
  prefs: []
  type: TYPE_NORMAL
- en: '<tf.Tensor: shape=(32, 20000), dtype=float32, numpy=…>'
  prefs: []
  type: TYPE_NORMAL
- en: str(targets)
  prefs: []
  type: TYPE_NORMAL
- en: '<tf.Tensor: shape=(32), dtype=int32, numpy=…>'
  prefs: []
  type: TYPE_NORMAL
- en: inputs[1, ]➊
  prefs: []
  type: TYPE_NORMAL
- en: tf.Tensor([1\. 1\. 1\. … 0\. 0\. 0.], shape=(20000), dtype=float32)
  prefs: []
  type: TYPE_NORMAL
- en: targets[1]
  prefs: []
  type: TYPE_NORMAL
- en: tf.Tensor(1, shape=(), dtype=int32)
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Inputs are batches of 20,000-dimensional vectors. These vectors consist
    entirely of ones and zeros.**
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s write a reusable model-building function that we’ll use in all of
    our experiments in this section.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.5 Our model-building utility**'
  prefs: []
  type: TYPE_NORMAL
- en: get_model <- function(max_tokens = 20000, hidden_dim = 16) {
  prefs: []
  type: TYPE_NORMAL
- en: inputs <- layer_input(shape = c(max_tokens))
  prefs: []
  type: TYPE_NORMAL
- en: outputs <- inputs %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(hidden_dim, activation = "relu") %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dropout(0.5) %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(1, activation = "sigmoid")
  prefs: []
  type: TYPE_NORMAL
- en: model <- keras_model(inputs, outputs)
  prefs: []
  type: TYPE_NORMAL
- en: model %>% compile(optimizer = "rmsprop",
  prefs: []
  type: TYPE_NORMAL
- en: loss = "binary_crossentropy",
  prefs: []
  type: TYPE_NORMAL
- en: metrics = "accuracy")
  prefs: []
  type: TYPE_NORMAL
- en: model
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, let’s train and test our model.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.6 Training and testing the binary unigram model**'
  prefs: []
  type: TYPE_NORMAL
- en: model <- get_model()
  prefs: []
  type: TYPE_NORMAL
- en: model
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0349-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: callbacks <- list(
  prefs: []
  type: TYPE_NORMAL
- en: callback_model_checkpoint("binary_1gram.keras", save_best_only = TRUE)
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: model %>% fit(
  prefs: []
  type: TYPE_NORMAL
- en: dataset_cache(binary_1gram_train_ds),
  prefs: []
  type: TYPE_NORMAL
- en: validation_data = dataset_cache(binary_1gram_val_ds),➊
  prefs: []
  type: TYPE_NORMAL
- en: epochs = 10,
  prefs: []
  type: TYPE_NORMAL
- en: callbacks = callbacks
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: model <- load_model_tf("binary_1gram.keras")
  prefs: []
  type: TYPE_NORMAL
- en: cat(sprintf(
  prefs: []
  type: TYPE_NORMAL
- en: '"Test acc: %.3f\n", evaluate(model, binary_1gram_test_ds)["accuracy"]))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Test acc: 0.887'
  prefs: []
  type: TYPE_NORMAL
- en: '➊ **We call dataset_cache() on the datasets to cache them in memory: this way,
    we will do the preprocessing only once, during the first epoch, and we''ll reuse
    the preprocessed texts for the following epochs. This can only be done if the
    data is small enough to fit in memory.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'This gets us to a test accuracy of 88.7%: not bad! Note that in this case,
    because the dataset is a balanced two-class classification dataset (there are
    as many positive samples as negative samples), the “naive baseline” we could reach
    without training an actual model would only be 50%. Meanwhile, the best score
    that can be achieved on this dataset without leveraging external data is around
    95% test accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: BIGRAMS WITH BINARY ENCODING
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Of course, discarding word order is very reductive, because even atomic concepts
    can be expressed via multiple words: the term “United States” conveys a concept
    that is quite distinct from the meaning of the words “states” and “united” taken
    separately. For this reason, you will usually end up re-injecting local order
    information into your bag-of-words representation by looking at *N*-grams rather
    than single words (most commonly, bigrams).'
  prefs: []
  type: TYPE_NORMAL
- en: 'With bigrams, our sentence becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: c("the", "the cat", "cat", "cat sat", "sat",
  prefs: []
  type: TYPE_NORMAL
- en: '"sat on", "on", "on the", "the mat", "mat")'
  prefs: []
  type: TYPE_NORMAL
- en: 'The layer_text_vectorization() layer can be configured to return arbitrary
    *N-*grams: bigrams, trigrams, and so on. Just pass an ngrams = N argument as in
    the following listing.'
  prefs: []
  type: TYPE_NORMAL
- en: '****Listing 11.7 Configuring layer_text_vectorization() to return bigrams****'
  prefs: []
  type: TYPE_NORMAL
- en: text_vectorization <
  prefs: []
  type: TYPE_NORMAL
- en: layer_text_vectorization(ngrams = 2,
  prefs: []
  type: TYPE_NORMAL
- en: max_tokens = 20000,
  prefs: []
  type: TYPE_NORMAL
- en: output_mode = "multi_hot")
  prefs: []
  type: TYPE_NORMAL
- en: Let’s test how our model performs when trained on such binary-encoded bags of
    bigrams (listing 11.8).
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.8 Training and testing the binary bigram model**'
  prefs: []
  type: TYPE_NORMAL
- en: adapt(text_vectorization, text_only_train_ds)
  prefs: []
  type: TYPE_NORMAL
- en: dataset_vectorize <- function(dataset) {➊
  prefs: []
  type: TYPE_NORMAL
- en: dataset %>%
  prefs: []
  type: TYPE_NORMAL
- en: dataset_map(~ list(text_vectorization(.x), .y),
  prefs: []
  type: TYPE_NORMAL
- en: num_parallel_calls = 4)
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: binary_2gram_train_ds <- train_ds %>% dataset_vectorize()
  prefs: []
  type: TYPE_NORMAL
- en: binary_2gram_val_ds <- val_ds %>% dataset_vectorize()
  prefs: []
  type: TYPE_NORMAL
- en: binary_2gram_test_ds <- test_ds %>% dataset_vectorize()
  prefs: []
  type: TYPE_NORMAL
- en: model <- get_model()
  prefs: []
  type: TYPE_NORMAL
- en: model
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0351-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: callbacks <- list(callback_model_checkpoint("binary_2gram.keras",
  prefs: []
  type: TYPE_NORMAL
- en: save_best_only = TRUE))
  prefs: []
  type: TYPE_NORMAL
- en: model %>% fit(
  prefs: []
  type: TYPE_NORMAL
- en: dataset_cache(binary_2gram_train_ds),
  prefs: []
  type: TYPE_NORMAL
- en: validation_data = dataset_cache(binary_2gram_val_ds),
  prefs: []
  type: TYPE_NORMAL
- en: epochs = 10,
  prefs: []
  type: TYPE_NORMAL
- en: callbacks = callbacks
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: model <- load_model_tf("binary_2gram.keras")
  prefs: []
  type: TYPE_NORMAL
- en: evaluate(model, binary_2gram_test_ds)["accuracy"] %>%
  prefs: []
  type: TYPE_NORMAL
- en: 'sprintf("Test acc: %.3f\n", .) %>% cat()'
  prefs: []
  type: TYPE_NORMAL
- en: 'Test acc: 0.895'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Define a helper function for applying the text_vectorization layer to a
    text TF Dataset because we'll be doing this multiple times (with different text_vectorization
    layers) throughout the chapter.**
  prefs: []
  type: TYPE_NORMAL
- en: We’re now getting 89.5% test accuracy, a marked improvement! Turns out local
    order is pretty important.
  prefs: []
  type: TYPE_NORMAL
- en: BIGRAMS WITH TF-IDF ENCODING
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can also add a bit more information to this representation by counting
    how many times each word or *N*-gram occurs, that is to say, by taking the histogram
    of the words over the text:'
  prefs: []
  type: TYPE_NORMAL
- en: c("the" = 2, "the cat" = 1, "cat" = 1, "cat sat" = 1, "sat" = 1,
  prefs: []
  type: TYPE_NORMAL
- en: '"sat on" = 1, "on" = 1, "on the" = 1, "the mat" = 1, "mat" = 1)'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re doing text classification, knowing how many times a word occurs in
    a sample is critical: any sufficiently long movie review may contain the word
    “terrible” regardless of sentiment, but a review that contains many instances
    of the word “terrible” is likely a negative one. Here’s how you’d count bigram
    occurrences with layer_text_ vectorization():'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.9 Configuring layer_text_vectorization() to return token counts**'
  prefs: []
  type: TYPE_NORMAL
- en: text_vectorization <
  prefs: []
  type: TYPE_NORMAL
- en: layer_text_vectorization(ngrams = 2,
  prefs: []
  type: TYPE_NORMAL
- en: max_tokens = 20000,
  prefs: []
  type: TYPE_NORMAL
- en: output_mode = "count")
  prefs: []
  type: TYPE_NORMAL
- en: Now, of course, some words are bound to occur more often than others no matter
    what the text is about. The words “the,” “a,” “is,” and “are” will always dominate
    your word count histograms, drowning out other words, despite being pretty much
    useless features in a classification context. How could we address this?
  prefs: []
  type: TYPE_NORMAL
- en: 'You already guessed it: via normalization. We could just normalize word counts
    by subtracting the mean and dividing by the variance (computed across the entire
    training dataset). That would make sense. Except most vectorized sentences consist
    almost entirely of zeros (our previous example features 12 nonzero entries and
    19,988 zero entries), a property called “sparsity.” That’s a great property to
    have, because it dramatically reduces compute load and reduces the risk of overfitting.
    If we subtracted the mean from each feature, we’d wreck sparsity. Thus, whatever
    normalization scheme we use should be divide-only. What, then, should we use as
    the denominator? The best practice is to go with something called *TF-IDF normalization*—TF-IDF
    stands for “term frequency, inverse document frequency.”'
  prefs: []
  type: TYPE_NORMAL
- en: '**Understanding TF-IDF normalization**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The more a given term appears in a document, the more important that term is
    for understanding what the document is about. At the same time, the frequency
    at which the term appears across all documents in your dataset matters, too: terms
    that appear in almost every document (like “the” or “a”) aren’t particularly informative,
    while terms that appear only in a small subset of all texts (like “Herzog”) are
    very distinctive and, thus, important. TF-IDF is a metric that fuses these two
    ideas. It weights a given term by taking “term frequency,” how many times the
    term appears in the current document, and dividing it by a measure of “document
    frequency,” which estimates how often the term comes up across the dataset. You’d
    compute it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: tf_idf <- function(term, document, dataset) {
  prefs: []
  type: TYPE_NORMAL
- en: term_freq <- sum(document == term)➊
  prefs: []
  type: TYPE_NORMAL
- en: doc_freqs <- sapply(dataset, function(doc) sum(doc == term))➋
  prefs: []
  type: TYPE_NORMAL
- en: doc_freq <- log(1 + sum(doc_freqs))
  prefs: []
  type: TYPE_NORMAL
- en: term_freq / doc_freq
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Count the number times 'term' appears in the document.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Count the number times 'term' appears across the full dataset.**
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF is so common that it’s built into layer_text_vectorization(). All you
    need to do to start using it is to switch the output_mode argument to “tf_idf”.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.10 Configuring layer_text_vectorization to return TF-IDF outputs**'
  prefs: []
  type: TYPE_NORMAL
- en: text_vectorization <
  prefs: []
  type: TYPE_NORMAL
- en: layer_text_vectorization(ngrams = 2,
  prefs: []
  type: TYPE_NORMAL
- en: max_tokens = 20000,
  prefs: []
  type: TYPE_NORMAL
- en: output_mode = "tf_idf")
  prefs: []
  type: TYPE_NORMAL
- en: Let’s train a new model with this scheme.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.11 Training and testing the TF-IDF bigram model**'
  prefs: []
  type: TYPE_NORMAL
- en: '[with(tf$device("CPU"), {➊'
  prefs: []
  type: TYPE_NORMAL
- en: adapt(text_vectorization, text_only_train_ds) ➋
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  prefs: []
  type: TYPE_NORMAL
- en: tfidf_2gram_train_ds <- train_ds %>% dataset_vectorize()
  prefs: []
  type: TYPE_NORMAL
- en: tfidf_2gram_val_ds <- val_ds %>% dataset_vectorize()
  prefs: []
  type: TYPE_NORMAL
- en: tfidf_2gram_test_ds <- test_ds %>% dataset_vectorize()
  prefs: []
  type: TYPE_NORMAL
- en: model <- get_model()
  prefs: []
  type: TYPE_NORMAL
- en: model
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0353-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: callbacks <- list(callback_model_checkpoint("tfidf_2gram.keras",
  prefs: []
  type: TYPE_NORMAL
- en: save_best_only = TRUE))
  prefs: []
  type: TYPE_NORMAL
- en: model %>% fit(
  prefs: []
  type: TYPE_NORMAL
- en: dataset_cache(tfidf_2gram_train_ds),
  prefs: []
  type: TYPE_NORMAL
- en: validation_data = dataset_cache(tfidf_2gram_val_ds),
  prefs: []
  type: TYPE_NORMAL
- en: epochs = 10,
  prefs: []
  type: TYPE_NORMAL
- en: callbacks = callbacks
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: model <- load_model_tf("tfidf_2gram.keras")
  prefs: []
  type: TYPE_NORMAL
- en: evaluate(model, tfidf_2gram_test_ds)["accuracy"] %>%
  prefs: []
  type: TYPE_NORMAL
- en: 'sprintf("Test acc: %.3f", .) %>% cat("\n")'
  prefs: []
  type: TYPE_NORMAL
- en: 'Test acc: 0.896'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **We pin this operation to a CPU only, because it uses operations that a GPU
    device doesn't support yet.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **The adapt() call will learn the TF-IDF weights in addition to the vocabulary.**
  prefs: []
  type: TYPE_NORMAL
- en: 'This gets us an 89.6% test accuracy on the IMDB classification task: it doesn’t
    seem to be particularly helpful in this case. However, for many text-classification
    datasets, it would be typical to see a one-percentage-point increase when using
    TF-IDF compared to plain binary encoding.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exporting a model that processes raw strings**'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding examples, we did our text standardization, splitting, and indexing
    as part of the TF Dataset pipeline. But if we want to export a standalone model
    independent of this pipeline, we should make sure that it incorporates its own
    text preprocessing (otherwise, you’d have to reimplement in the production environment,
    which can be challenging or can lead to subtle discrepancies between the training
    data and the production data). Thankfully, this is easy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just create a new model that reuses your text_vectorization layer and adds
    to it the model you just trained:'
  prefs: []
  type: TYPE_NORMAL
- en: inputs <- layer_input(shape = c(1), dtype = "string") ➊
  prefs: []
  type: TYPE_NORMAL
- en: outputs <- inputs %>%
  prefs: []
  type: TYPE_NORMAL
- en: text_vectorization() %>%➋
  prefs: []
  type: TYPE_NORMAL
- en: model() ➌
  prefs: []
  type: TYPE_NORMAL
- en: inference_model <- keras_model(inputs, outputs)➍
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **One input sample would be one string.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Apply text preprocessing.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Apply the previously trained model.**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Instantiate the end-to-end model.**
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting model can process batches of raw strings:'
  prefs: []
  type: TYPE_NORMAL
- en: raw_text_data <- "That was an excellent movie, I loved it." %>%
  prefs: []
  type: TYPE_NORMAL
- en: as_tensor(shape = c(-1, 1))➊
  prefs: []
  type: TYPE_NORMAL
- en: predictions <- inference_model(raw_text_data)
  prefs: []
  type: TYPE_NORMAL
- en: str(predictions)
  prefs: []
  type: TYPE_NORMAL
- en: '<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.93249124]],'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common01bg.jpg) dtype=float32)>'
  prefs: []
  type: TYPE_NORMAL
- en: cat(sprintf("%.2f percent positive\n",
  prefs: []
  type: TYPE_NORMAL
- en: as.numeric(predictions) * 100))
  prefs: []
  type: TYPE_NORMAL
- en: 93.25 percent positive
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **The model expects inputs to be a batch of samples, that is, a one-column
    matrix.**
  prefs: []
  type: TYPE_NORMAL
- en: '11.3.3 Processing words as a sequence: The sequence model approach'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'These past few examples clearly show that word order matters: manual engineering
    of order-based features, such as bigrams, yields a nice accuracy boost. Now remember:
    the history of deep learning is that of a move away from manual feature engineering,
    toward letting models learn their own features from exposure to data alone. What
    if, instead of manually crafting order-based features, we exposed the model to
    raw word sequences and let it figure out such features on its own? This is what
    *sequence models* are about.'
  prefs: []
  type: TYPE_NORMAL
- en: To implement a sequence model, you’d start by representing your input samples
    as sequences of integer indices (one integer standing for one word). Then, you’d
    map each integer to a vector to obtain vector sequences. Finally, you’d feed these
    sequences of vectors into a stack of layers that could cross-correlate features
    from adjacent vectors, such as a 1D convnet, an RNN, or a Transformer.
  prefs: []
  type: TYPE_NORMAL
- en: For some time, around 2016–2017, bidirectional RNNs (in particular, bidirectional
    LSTMs) were considered to be the state of the art for sequence modeling. Because
    you’re already familiar with this architecture, this is what we’ll use in our
    first sequence model examples. However, nowadays sequence modeling is almost universally
    done with Transformers, which we will cover shortly. Oddly, one-dimensional convnets
    were never very popular in NLP, even though, in my own experience, a residual
    stack of depthwise-separable 1D convolutions can often achieve comparable performance
    to a bidirectional LSTM, at a greatly reduced computational cost.
  prefs: []
  type: TYPE_NORMAL
- en: A FIRST PRACTICAL EXAMPLE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s try out a first sequence model in practice. First, let’s prepare datasets
    that return integer sequences.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.12 Preparing integer sequence datasets**'
  prefs: []
  type: TYPE_NORMAL
- en: max_length <— 600 ➊
  prefs: []
  type: TYPE_NORMAL
- en: max_tokens <— 20000
  prefs: []
  type: TYPE_NORMAL
- en: text_vectorization <- layer_text_vectorization(
  prefs: []
  type: TYPE_NORMAL
- en: max_tokens = max_tokens,
  prefs: []
  type: TYPE_NORMAL
- en: output_mode = "int",
  prefs: []
  type: TYPE_NORMAL
- en: output_sequence_length = max_length
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: adapt(text_vectorization, text_only_train_ds)
  prefs: []
  type: TYPE_NORMAL
- en: int_train_ds <- train_ds %>% dataset_vectorize()
  prefs: []
  type: TYPE_NORMAL
- en: int_val_ds <- val_ds %>% dataset_vectorize()
  prefs: []
  type: TYPE_NORMAL
- en: int_test_ds <- test_ds %>% dataset_vectorize()
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **To keep a manageable input size, we'll truncate the inputs after the first
    600 words. This is a reasonable choice, because the average review length is 233
    words, and only 5% of reviews are longer than 600 words.**
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s make a model. The simplest way to convert our integer sequences
    to vector sequences is to one-hot-encode the integers (each dimension would represent
    one possible term in the vocabulary). On top of these one-hot vectors, we’ll add
    a simple bidirectional LSTM.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.13 A sequence model built on one-hot-encoded vector sequences**'
  prefs: []
  type: TYPE_NORMAL
- en: inputs <- layer_input(shape(NULL), dtype = "int64")➊
  prefs: []
  type: TYPE_NORMAL
- en: embedded <- inputs %>%
  prefs: []
  type: TYPE_NORMAL
- en: tf$one_hot(depth = as.integer(max_tokens))➋
  prefs: []
  type: TYPE_NORMAL
- en: outputs <- embedded %>%
  prefs: []
  type: TYPE_NORMAL
- en: bidirectional(layer_lstm(units = 32)) %>%➌
  prefs: []
  type: TYPE_NORMAL
- en: layer_dropout(.5) %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(1, activation = "sigmoid")➍
  prefs: []
  type: TYPE_NORMAL
- en: model <- keras_model(inputs, outputs)
  prefs: []
  type: TYPE_NORMAL
- en: model %>% compile(optimizer = "rmsprop",
  prefs: []
  type: TYPE_NORMAL
- en: loss = "binary_crossentropy",
  prefs: []
  type: TYPE_NORMAL
- en: metrics = "accuracy")
  prefs: []
  type: TYPE_NORMAL
- en: model
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0356-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: ➊ **One input is a sequence of integers.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Encode the integers into binary 20,000-dimensional vectors.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Add a bidirectional LSTM.**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Finally, add a classification layer.**
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s train our model.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.14 Training a first basic sequence model**'
  prefs: []
  type: TYPE_NORMAL
- en: callbacks <- list(
  prefs: []
  type: TYPE_NORMAL
- en: callback_model_checkpoint("one_hot_bidir_lstm.keras",
  prefs: []
  type: TYPE_NORMAL
- en: save_best_only = TRUE))
  prefs: []
  type: TYPE_NORMAL
- en: 'A first observation: this model trains very slowly, especially compared to
    the lightweight model of the previous section. This is because our inputs are
    quite large: each input sample is encoded as a matrix of size (600, 20000) (600
    words per sample, 20,000 possible words). That’s 12,000,000 floats for a single
    movie review. Our bidirectional LSTM has a lot of work to do. Second, the model
    gets only to 87% test accuracy— it doesn’t perform nearly as well as our (very
    fast) binary unigram model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Clearly, using one-hot encoding to turn words into vectors, which was the simplest
    thing we could do, wasn’t a great idea. There’s a better way: *word embeddings*.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reduce batch_size to avoid out-of-memory errors**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on your machine and the available RAM your GPU has, you may encounter
    out-of-memory errors trying to train this larger, bidirectional model. If that
    happens, try training with a smaller batch size. You can pass a smaller batch_size
    argument to text_dataset_from_directory(batch_size = ), or you can rebatch an
    existing TF Dataset like this:'
  prefs: []
  type: TYPE_NORMAL
- en: int_train_ds_smaller <- int_train_ds %>%
  prefs: []
  type: TYPE_NORMAL
- en: dataset_unbatch() %>%
  prefs: []
  type: TYPE_NORMAL
- en: dataset_batch(16)
  prefs: []
  type: TYPE_NORMAL
- en: model %>% fit(int_train_ds_smaller, validation_data = int_val_ds,
  prefs: []
  type: TYPE_NORMAL
- en: epochs = 10, callbacks = callbacks)
  prefs: []
  type: TYPE_NORMAL
- en: model <- load_model_tf("one_hot_bidir_lstm.keras")
  prefs: []
  type: TYPE_NORMAL
- en: 'sprintf("Test acc: %.3f", evaluate(model, int_test_ds)["accuracy"])'
  prefs: []
  type: TYPE_NORMAL
- en: '[1] "Test acc: 0.873"'
  prefs: []
  type: TYPE_NORMAL
- en: UNDERSTANDING WORD EMBEDDINGS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Crucially, when you encode something via one-hot encoding, you’re making a
    feature-engineering decision. You’re injecting into your model a fundamental assumption
    about the structure of your feature space. That assumption is that *the different
    tokens you’re encoding are all independent from each other*: indeed, one-hot vectors
    are all orthogonal to one another. And in the case of words, that assumption is
    clearly wrong. Words form a structured space: they share information with each
    other. The words “movie” and “film” are interchangeable in most sentences, so
    the vector that represents “movie” should not be orthogonal to the vector that
    represents “film”—they should be the same vector, or close enough.'
  prefs: []
  type: TYPE_NORMAL
- en: To get a bit more abstract, the *geometric relationship* between two word vectors
    should reflect the *semantic relationship* between these words. For instance,
    in a reasonable word vector space, you would expect synonyms to be embedded into
    similar word vectors, and in general, you would expect the geometric distance
    (such as the cosine distance or L2 distance) between any two word vectors to relate
    to the “semantic distance” between the associated words. Words that mean different
    things should lie far away from each other, whereas related words should be closer.
  prefs: []
  type: TYPE_NORMAL
- en: '*Word embeddings* are vector representations of words that achieve exactly
    this: they map human language into a structured geometric space. Whereas the vectors
    obtained through one-hot encoding are binary, sparse (mostly made of zeros), and
    very high-dimensional (the same dimensionality as the number of words in the vocabulary),
    word embeddings are low-dimensional floating-point vectors (i.e., dense vectors,
    as opposed to sparse vectors); see [figure 11.2](#fig11-2). It’s common to see
    word embeddings that are 256-dimensional, 512-dimensional, or 1,024-dimensional
    when dealing with very large vocabularies. On the other hand, one-hot-encoding
    words generally leads to vectors that are 20,000-dimensional or greater (capturing
    a vocabulary of 20,000 tokens, in this case). So, word embeddings pack more information
    into far fewer dimensions.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0358-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 11.2 Word representations obtained from one-hot encoding or hashing
    are sparse, high-dimensional, and hardcoded. Word embeddings are dense, relatively
    low-dimensional, and learned from data.**'
  prefs: []
  type: TYPE_NORMAL
- en: Besides being *dense* representations, word embeddings are also *structured*
    representations, and their structure is learned from data. Similar words are embedded
    in close locations, and further, specific *directions* in the embedding space
    are meaningful. To make this clearer, let’s look at a concrete example.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [figure 11.3](#fig11-3), four words are embedded on a 2D plane: *cat, dog,
    wolf*, and *tiger*. With the vector representations we chose here, some semantic
    relationships between these words can be encoded as geometric transformations.
    For instance, the same vector allows us to go from *cat* to *tiger* and from *dog*
    to *wolf*: this vector could be interpreted as the “from pet to wild animal” vector.
    Similarly, another vector lets us go from *dog* to *cat* and from *wolf* to *tiger*,
    which could be interpreted as a “from canine to feline” vector.'
  prefs: []
  type: TYPE_NORMAL
- en: In real-world word-embedding spaces, common examples of meaningful geometric
    transformations are “gender” vectors and “plural” vectors. For instance, by adding
    a “female” vector to the vector “king,” we obtain the vector “queen.” By adding
    a “plural” vector, we obtain “kings.” Word-embedding spaces typically feature
    thousands of such interpretable and potentially useful vectors.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0358-02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 11.3 A toy example of a word-embedding space**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at how to use such an embedding space in practice. There are two
    ways to obtain word embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: Learn word embeddings jointly with the main task you care about (such as document
    classification or sentiment prediction). In this setup, you start with random
    word vectors and then learn word vectors in the same way you learn the weights
    of a neural network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load into your model word embeddings that were precomputed using a different
    machine learning task than the one you’re trying to solve. These are called *pretrained
    word embeddings*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s review each of these approaches.
  prefs: []
  type: TYPE_NORMAL
- en: LEARNING WORD EMBEDDINGS WITH THE EMBEDDING LAYER
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Is there some ideal word-embedding space that would perfectly map human language
    and could be used for any natural language processing task? Possibly, but we have
    yet to compute anything of the sort. Also, there is no such a thing as *human
    language*— there are many different languages, and they aren’t isomorphic to one
    another, because a language is the reflection of a specific culture and a specific
    context. But more pragmatically, what makes a good word-embedding space depends
    heavily on your task: the perfect word-embedding space for an English-language
    movie-review sentiment-analysis model may look different from the perfect embedding
    space for an English-language legal-document classification model, because the
    importance of certain semantic relationships varies from task to task.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s thus reasonable to *learn* a new embedding space with every new task.
    Fortunately, backpropagation makes this easy, and Keras makes it even easier.
    It’s about learning the weights of a layer: layer_embedding().'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.15 Instantiating a layer_embedding**'
  prefs: []
  type: TYPE_NORMAL
- en: embedding_layer <- layer_embedding(input_dim = max_tokens,
  prefs: []
  type: TYPE_NORMAL
- en: output_dim = 256)➊
  prefs: []
  type: TYPE_NORMAL
- en: '➊layer_embedding() takes at least two arguments: the number of possible tokens
    and the dimensionality of the embeddings (here, 256).'
  prefs: []
  type: TYPE_NORMAL
- en: layer_embedding() is best understood as a dictionary that maps integer indices
    (which stand for specific words) to dense vectors. It takes integers as input,
    looks up these integers in an internal dictionary, and returns the associated
    vectors. It’s effectively a dictionary lookup (see [figure 11.4](#fig11-4)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0359-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 11.4 The Embedding layer**'
  prefs: []
  type: TYPE_NORMAL
- en: The embedding layer takes as input a rank 2 tensor of integers, of shape (batch_size,
    sequence_length), where each entry is a sequence of integers. The layer then returns
    a 3D floating-point tensor of shape (batch_size, sequence_length, embedding_ dimensionality).
  prefs: []
  type: TYPE_NORMAL
- en: When you instantiate a layer_embedding(), its weights (its internal dictionary
    of token vectors) are initially random, just as with any other layer. During training,
    these word vectors are gradually adjusted via backpropagation, structuring the
    space into something the downstream model can exploit. Once fully trained, the
    embedding space will show a lot of structure—a kind of structure specialized for
    the specific problem for which you’re training your model.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s build a model that includes a layer_embedding() and benchmark it on our
    task.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.16 Model that uses a layer_embedding trained from scratch**'
  prefs: []
  type: TYPE_NORMAL
- en: inputs <- layer_input(shape(NA), dtype = "int64")
  prefs: []
  type: TYPE_NORMAL
- en: embedded <- inputs %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_embedding(input_dim = max_tokens, output_dim = 256)
  prefs: []
  type: TYPE_NORMAL
- en: outputs <- embedded %>%
  prefs: []
  type: TYPE_NORMAL
- en: bidirectional(layer_lstm(units = 32)) %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dropout(0.5) %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(1, activation = "sigmoid")
  prefs: []
  type: TYPE_NORMAL
- en: model <- keras_model(inputs, outputs)
  prefs: []
  type: TYPE_NORMAL
- en: model %>%
  prefs: []
  type: TYPE_NORMAL
- en: compile(optimizer = "rmsprop",
  prefs: []
  type: TYPE_NORMAL
- en: loss = "binary_crossentropy",
  prefs: []
  type: TYPE_NORMAL
- en: metrics = "accuracy")
  prefs: []
  type: TYPE_NORMAL
- en: model
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0360-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: callbacks <- list(callback_model_checkpoint("embeddings_bidir_lstm.keras",
  prefs: []
  type: TYPE_NORMAL
- en: save_best_only = TRUE))
  prefs: []
  type: TYPE_NORMAL
- en: model %>%
  prefs: []
  type: TYPE_NORMAL
- en: fit(int_train_ds,
  prefs: []
  type: TYPE_NORMAL
- en: validation_data = int_val_ds,
  prefs: []
  type: TYPE_NORMAL
- en: epochs = 10,
  prefs: []
  type: TYPE_NORMAL
- en: callbacks = callbacks)
  prefs: []
  type: TYPE_NORMAL
- en: model <- load_model_tf("embeddings_bidir_lstm.keras")
  prefs: []
  type: TYPE_NORMAL
- en: evaluate(model, int_test_ds)["accuracy"] %>%
  prefs: []
  type: TYPE_NORMAL
- en: 'sprintf("Test acc: %.3f\n", .) %>% cat()'
  prefs: []
  type: TYPE_NORMAL
- en: 'Test acc: 0.842'
  prefs: []
  type: TYPE_NORMAL
- en: 'It trains much faster than the one-hot model (because the LSTM has to process
    only 256-dimensional vectors instead of 20,000-dimensional ones), and its test
    accuracy is comparable (84%). However, we’re still some way off from the results
    of our basic bigram model. Part of the reason is simply that the model is looking
    at slightly less data: the bigram model processed full reviews, whereas our sequence
    model truncates sequences after 600 words.'
  prefs: []
  type: TYPE_NORMAL
- en: UNDERSTANDING PADDING AND MASKING
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One thing that’s slightly hurting model performance here is that our input
    sequences are full of zeros. This comes from our use of the output_sequence_length
    = max_ length option in layer_text_vectorization() (with max_length equal to 600):
    sentences longer than 600 tokens are truncated to a length of 600 tokens, and
    sentences shorter than 600 tokens are padded with zeros at the end so that they
    can be concatenated with other sequences to form contiguous batches.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re using a bidirectional RNN: two RNN layers running in parallel, with one
    processing the tokens in their natural order, and the other processing the same
    tokens in reverse. The RNN that looks at the tokens in their natural order will
    spend its last iterations seeing only vectors that encode padding—possibly for
    several hundreds of iterations if the original sentence was short. The information
    stored in the internal state of the RNN will gradually fade out as it is exposed
    to these meaningless inputs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We need some way to tell the RNN that it should skip these iterations. There’s
    an API for that: *masking*. layer_embedding() is capable of generating a “mask”
    that corresponds to its input data. This mask is a tensor of ones and zeros (or
    TRUE/FALSE Booleans), of shape (batch_size, sequence_length), where the entry
    mask[i, t] indicates whether time step t of sample i should be skipped or not
    (the time step will be skipped if mask[i, t] is 0 or FALSE, and processed otherwise).'
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, this option isn’t active—you can turn it on by passing mask_zero
    = TRUE to your layer_embedding(). You can retrieve the mask with the compute_mask()
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: embedding_layer <- layer_embedding(input_dim = 10, output_dim = 256,
  prefs: []
  type: TYPE_NORMAL
- en: mask_zero = TRUE)
  prefs: []
  type: TYPE_NORMAL
- en: some_input <- rbind(c(4, 3, 2, 1, 0, 0, 0),
  prefs: []
  type: TYPE_NORMAL
- en: c(5, 4, 3, 2, 1, 0, 0),
  prefs: []
  type: TYPE_NORMAL
- en: c(2, 1, 0, 0, 0, 0, 0))
  prefs: []
  type: TYPE_NORMAL
- en: mask <- embedding_layer$compute_mask(some_input)
  prefs: []
  type: TYPE_NORMAL
- en: mask
  prefs: []
  type: TYPE_NORMAL
- en: tf.Tensor(
  prefs: []
  type: TYPE_NORMAL
- en: '[[ True  True  True  True False False False]'
  prefs: []
  type: TYPE_NORMAL
- en: '[ True  True  True  True  True False False]'
  prefs: []
  type: TYPE_NORMAL
- en: '[ True  True False False False False False]], shape=(3, 7), dtype=bool)'
  prefs: []
  type: TYPE_NORMAL
- en: In practice, you will almost never have to manage masks by hand. Instead, Keras
    will automatically pass on the mask to every layer that is able to process it
    (as a piece of metadata attached to the sequence it represents). This mask will
    be used by RNN layers to skip masked steps. If your model returns an entire sequence,
    the mask will also be used by the loss function to skip masked steps in the output
    sequence. Let’s try retraining our model with masking enabled.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.17 Using an embedding layer with masking enabled**'
  prefs: []
  type: TYPE_NORMAL
- en: inputs <- layer_input(c(NA), dtype = "int64")
  prefs: []
  type: TYPE_NORMAL
- en: embedded <- inputs %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_embedding(input_dim = max_tokens,
  prefs: []
  type: TYPE_NORMAL
- en: output_dim = 256,
  prefs: []
  type: TYPE_NORMAL
- en: mask_zero = TRUE)
  prefs: []
  type: TYPE_NORMAL
- en: outputs <- embedded %>%
  prefs: []
  type: TYPE_NORMAL
- en: bidirectional(layer_lstm(units = 32)) %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dropout(0.5) %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(1, activation = "sigmoid")
  prefs: []
  type: TYPE_NORMAL
- en: model <- keras_model(inputs, outputs)
  prefs: []
  type: TYPE_NORMAL
- en: model %>% compile(optimizer = "rmsprop",
  prefs: []
  type: TYPE_NORMAL
- en: loss = "binary_crossentropy",
  prefs: []
  type: TYPE_NORMAL
- en: metrics = "accuracy")
  prefs: []
  type: TYPE_NORMAL
- en: model
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0362-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: callbacks <- list(
  prefs: []
  type: TYPE_NORMAL
- en: callback_model_checkpoint("embeddings_bidir_lstm_with_masking.keras",
  prefs: []
  type: TYPE_NORMAL
- en: save_best_only = TRUE)
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: model %>%fit(
  prefs: []
  type: TYPE_NORMAL
- en: int_train_ds,
  prefs: []
  type: TYPE_NORMAL
- en: validation_data = int_val_ds,
  prefs: []
  type: TYPE_NORMAL
- en: epochs = 10,
  prefs: []
  type: TYPE_NORMAL
- en: callbacks = callbacks
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: model <- load_model_tf("embeddings_bidir_lstm_with_masking.keras")
  prefs: []
  type: TYPE_NORMAL
- en: 'cat(sprintf("Test acc: %.3f\n",'
  prefs: []
  type: TYPE_NORMAL
- en: evaluate(model, int_test_ds)["accuracy"]))
  prefs: []
  type: TYPE_NORMAL
- en: 'Test acc: 0.880'
  prefs: []
  type: TYPE_NORMAL
- en: This time we get to 88% test accuracy—a small but noticeable improvement.
  prefs: []
  type: TYPE_NORMAL
- en: USING PRETRAINED WORD EMBEDDINGS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Sometimes you have so little training data available that you can’t use your
    data alone to learn an appropriate task-specific embedding of your vocabulary.
    In such cases, instead of learning word embeddings jointly with the problem you
    want to solve, you can load embedding vectors from a precomputed embedding space
    that you know is highly structured and exhibits useful properties—one that captures
    generic aspects of language structure. The rationale behind using pretrained word
    embeddings in natural language processing is much the same as for using pretrained
    convnets in image classification: you don’t have enough data available to learn
    truly powerful features on your own, but you expect that the features you need
    are fairly generic—that is, common visual features or semantic features. In this
    case, it makes sense to reuse features learned on a different problem.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Such word embeddings are generally computed using word-occurrence statistics
    (observations about what words co-occur in sentences or documents), using a variety
    of techniques, some involving neural networks, others not. The idea of a dense,
    low-dimensional embedding space for words, computed in an unsupervised way, was
    initially explored by Bengio et al. in the early 2000s,^([1](#Rendnote1)) but
    it started to take off in research and industry applications only after the release
    of one of the most famous and successful word-embedding schemes: the Word2Vec
    algorithm ([https://code.google.com/archive/p/word2vec](https://code.google.com/archive/p/word2vec)),
    developed by Tomas Mikolov at Google in 2013\. Word2Vec dimensions capture specific
    semantic properties, such as gender.'
  prefs: []
  type: TYPE_NORMAL
- en: You can download various precomputed databases of word embeddings and use them
    in a Keras layer_embedding(). Word2Vec is one of them. Another popular one is
    called Global Vectors for Word Representation (GloVe, [https://nlp.stanford.edu/projects/glove](https://nlp.stanford.edu/projects/glove)),
    which was developed by Stanford researchers in 2014\. This embedding technique
    is based on factorizing a matrix of word co-occurrence statistics. Its developers
    have made available precomputed embeddings for millions of English tokens, obtained
    from Wikipedia and Common Crawl data.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at how you can get started using GloVe embeddings in a Keras model.
    The same method is valid for Word2Vec embeddings or any other word-embedding database.
    We’ll start by downloading the GloVe files and parsing them. We’ll then load the
    word vectors into a Keras layer_embedding() layer, which we’ll use to build a
    new model.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s download the GloVe word embeddings precomputed on the 2014 English
    Wikipedia dataset. It’s an 822 MB zip file containing 100-dimensional embedding
    vectors for 400,000 words (or nonword tokens):'
  prefs: []
  type: TYPE_NORMAL
- en: download.file("http://nlp.stanford.edu/data/glove.6B.zip",
  prefs: []
  type: TYPE_NORMAL
- en: destfile = "glove.6B.zip")
  prefs: []
  type: TYPE_NORMAL
- en: zip::unzip("glove.6B.zip")
  prefs: []
  type: TYPE_NORMAL
- en: Let’s parse the unzipped file (a .txt file) to build an index that maps words
    (as strings) to their vector representation. Because the file structure is essentially
    a numeric matrix with row names, that’s what we’ll make in R.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.18 Parsing the GloVe word-embeddings file**'
  prefs: []
  type: TYPE_NORMAL
- en: path_to_glove_file <- "glove.6B.100d.txt"
  prefs: []
  type: TYPE_NORMAL
- en: embedding_dim <- 100
  prefs: []
  type: TYPE_NORMAL
- en: df <- readr::read_table(
  prefs: []
  type: TYPE_NORMAL
- en: path_to_glove_file,
  prefs: []
  type: TYPE_NORMAL
- en: col_names = FALSE,➊
  prefs: []
  type: TYPE_NORMAL
- en: col_types = paste0("c", strrep("n", 100))➋
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: embeddings_index <- as.matrix(df[, -1])➌
  prefs: []
  type: TYPE_NORMAL
- en: rownames(embeddings_index) <- df[[1]]
  prefs: []
  type: TYPE_NORMAL
- en: colnames(embeddings_index) <- NULL ➍
  prefs: []
  type: TYPE_NORMAL
- en: rm(df)➎
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **read_table() returns a data.frame. col_names = FALSE tells read_table()
    the text file does not have a header line, and the data itself starts at the first
    line.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Passing col_types is not necessary, but is a best practice and a good guard
    against surprises (e.g., if you're reading a corrupted file, or the wrong file!).
    Here we tell read_table() the first column is of type 'character', and then the
    next 100 are of type 'numeric'.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **The first column is the word, and the remaining 100 columns are the numeric
    embeddings.**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Discard the column names that read_table() automatically created (R data.frames
    must have column names).**
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **Clear the temporary data.frame from memory.**
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is what embedding_matrix looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: str(embeddings_index)
  prefs: []
  type: TYPE_NORMAL
- en: num [1:400000, 1:100] -0.0382 -0.1077 -0.3398 -0.1529 -0.1897 …
  prefs: []
  type: TYPE_NORMAL
- en: '- attr(*, "dimnames")=List of 2'
  prefs: []
  type: TYPE_NORMAL
- en: '..$ : chr [1:400000] "the" "," "." "of" …'
  prefs: []
  type: TYPE_NORMAL
- en: '..$ : NULL'
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s build an embedding matrix that you can load into a layer_embedding(),
    It must be a matrix of shape (max_words, embedding_dim), where each entry *i*
    contains the embedding_dim-dimensional vector for the word of index *i* in the
    reference word index (built during tokenization).
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.19 Preparing the GloVe word-embeddings matrix**'
  prefs: []
  type: TYPE_NORMAL
- en: vocabulary <- text_vectorization %>% get_vocabulary() ➊
  prefs: []
  type: TYPE_NORMAL
- en: str(vocabulary)
  prefs: []
  type: TYPE_NORMAL
- en: chr [1:20000] "" "[UNK]" "the" "a" "and" "of" "to" "is" "in" "it" "i" …
  prefs: []
  type: TYPE_NORMAL
- en: tokens <- head(vocabulary[-1], max_tokens)➋
  prefs: []
  type: TYPE_NORMAL
- en: i <- match(vocabulary, rownames(embeddings_index),➌
  prefs: []
  type: TYPE_NORMAL
- en: nomatch = 0)
  prefs: []
  type: TYPE_NORMAL
- en: embedding_matrix <- array(0, dim = c(max_tokens, embedding_dim))➍
  prefs: []
  type: TYPE_NORMAL
- en: embedding_matrix[i != 0, ] <- embeddings_index[i, ]➎➏
  prefs: []
  type: TYPE_NORMAL
- en: str(embedding_matrix)
  prefs: []
  type: TYPE_NORMAL
- en: num [1:20000, 1:100] 0 0 -0.0382 -0.2709 -0.072 …
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Retrieve the vocabulary indexed by our previous text_vectorization layer.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **[-1] to remove the mask token "" in the first position. head(, max_tokens)
    is just a sanity check—we passed the same max_tokens to text_vectorization earlier.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **i is an integer vector of the row-number in embeddings_index that matched
    to each corresponding word in vocabulary, and 0 if there was no matching word.**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Prepare a matrix of all zeros that we'll fill with the GloVe vectors.**
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **Fill entries in the matrix with the corresponding word vector. Row numbers
    of rows embedding_matrix corresponds to index positions of words in vocabulary.
    Words not found in the embedding index will be all zeros.**
  prefs: []
  type: TYPE_NORMAL
- en: '➏ **0s in indexes passed to [ for R arrays are ignored. For example: (1:10)[c(1,0,2,0,3)]
    returns c(1, 2, 3).**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we use a initializer_constant() to load the pretrained embeddings
    in a layer_embedding(). So as not to disrupt the pretrained representations during
    training, we freeze the layer via trainable = FALSE:'
  prefs: []
  type: TYPE_NORMAL
- en: embedding_layer <- layer_embedding(
  prefs: []
  type: TYPE_NORMAL
- en: input_dim = max_tokens,
  prefs: []
  type: TYPE_NORMAL
- en: output_dim = embedding_dim,
  prefs: []
  type: TYPE_NORMAL
- en: embeddings_initializer = initializer_constant(embedding_matrix),
  prefs: []
  type: TYPE_NORMAL
- en: trainable = FALSE,
  prefs: []
  type: TYPE_NORMAL
- en: mask_zero = TRUE
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: We’re now ready to train a new model—identical to our previous model, but leveraging
    the 100-dimensional pretrained GloVe embeddings instead of 128-dimensional learned
    embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.20 Model that uses a pretrained embedding layer**'
  prefs: []
  type: TYPE_NORMAL
- en: inputs <- layer_input(shape(NA), dtype = "int64")
  prefs: []
  type: TYPE_NORMAL
- en: embedded <- embedding_layer(inputs)
  prefs: []
  type: TYPE_NORMAL
- en: outputs <- embedded %>%
  prefs: []
  type: TYPE_NORMAL
- en: bidirectional(layer_lstm(units = 32)) %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dropout(0.5) %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(1, activation = "sigmoid")
  prefs: []
  type: TYPE_NORMAL
- en: model <- keras_model(inputs, outputs)
  prefs: []
  type: TYPE_NORMAL
- en: model %>% compile(optimizer = "rmsprop",
  prefs: []
  type: TYPE_NORMAL
- en: loss = "binary_crossentropy",
  prefs: []
  type: TYPE_NORMAL
- en: metrics = "accuracy")
  prefs: []
  type: TYPE_NORMAL
- en: model
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0365-01.jpg) ![Image](../images/f0366-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: callbacks <- list(
  prefs: []
  type: TYPE_NORMAL
- en: callback_model_checkpoint("glove_embeddings_sequence_model.keras",
  prefs: []
  type: TYPE_NORMAL
- en: save_best_only = TRUE)
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: model %>%
  prefs: []
  type: TYPE_NORMAL
- en: fit(int_train_ds, validation_data = int_val_ds,
  prefs: []
  type: TYPE_NORMAL
- en: epochs = 10, callbacks = callbacks)
  prefs: []
  type: TYPE_NORMAL
- en: model <- load_model_tf("glove_embeddings_sequence_model.keras")
  prefs: []
  type: TYPE_NORMAL
- en: cat(sprintf(
  prefs: []
  type: TYPE_NORMAL
- en: '"Test acc: %.3f\n", evaluate(model, int_test_ds)["accuracy"]))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Test acc: 0.877'
  prefs: []
  type: TYPE_NORMAL
- en: You’ll find that on this particular task, pretrained embeddings aren’t very
    helpful, because the dataset contains enough samples that it is possible to learn
    a specialized enough embedding space from scratch. However, leveraging pretrained
    embeddings can be very helpful when you’re working with a smaller dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 11.4 The Transformer architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Starting in 2017, a new model architecture started overtaking recurrent neural
    networks across most natural language processing tasks: the Transformer. Transformers
    were introduced in the seminal paper “Attention Is All You Need” by Vaswani et
    al.^([2](#Rendnote2)) The gist of the paper is right there in the title: as it
    turned out, a simple mechanism called “neural attention” could be used to build
    powerful sequence models that didn’t feature any recurrent layers or convolution
    layers.'
  prefs: []
  type: TYPE_NORMAL
- en: This finding unleashed nothing short of a revolution in natural language processing
    and beyond. Neural attention has fast become one of the most influential ideas
    in deep learning. In this section, you’ll get an in-depth explanation of how it
    works and why it has proven so effective for sequence data. We’ll then leverage
    self-attention to create a Transformer encoder, one of the basic components of
    the Transformer architecture, and we’ll apply it to the IMDB movie review classification
    task.
  prefs: []
  type: TYPE_NORMAL
- en: 11.4.1 Understanding self-attention
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As you’re going through this book, you may be skimming some parts and attentively
    reading others, depending on what your goals or interests are. What if your models
    did the same? It’s a simple yet powerful idea: not all input information seen
    by a model is equally important to the task at hand, so models should “pay more
    attention” to some features and “pay less attention” to other features. Does that
    sound familiar? You’ve already encountered a similar concept twice in this book:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Max pooling in convnets looks at a pool of features in a spatial region and
    selects just one feature to keep. That’s an “all or nothing” form of attention:
    keep the most important feature and discard the rest.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TF-IDF normalization assigns importance scores to tokens based on how much information
    different tokens are likely to carry. Important tokens are boosted while irrelevant
    tokens are faded out. That’s a continuous form of attention.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many different forms of attention you could imagine, but they all
    start by computing importance scores for a set of features, with higher scores
    for more relevant features and lower scores for less relevant ones (see [figure
    11.5](#fig11-5)). How these scores should be computed, and what you should do
    with them, will vary from approach to approach.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0367-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 11.5 The general concept of “attention” in deep learning: Input features
    are assigned “attention scores,” which can be used to inform the next representation
    of the input.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Crucially, this kind of attention mechanism can be used for more than just
    highlighting or erasing certain features. It can be used to make features *context
    aware*. You’ve just learned about word embeddings: vector spaces that capture
    the “shape” of the semantic relationships between different words. In an embedding
    space, a single word has a fixed position—a fixed set of relationships with every
    other word in the space. But that’s not quite how language works: the meaning
    of a word is usually context specific. When you mark the date, you’re not talking
    about the same “date” as when you go on a date, nor is it the kind of date you’d
    buy at the market. When you say, “I’ll see you soon,” the meaning of the word
    “see” is subtly different from the “see” in “I’ll see this project to its end”
    or “I see what you mean.” And, of course, the meaning of pronouns like “he,” “it,”
    “you,” and so on is entirely sentence specific and can even change multiple times
    within a single sentence.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Clearly, a smart embedding space would provide a different vector representation
    for a word depending on the other words surrounding it. That’s where *self-attention*
    comes in. The purpose of self-attention is to modulate the representation of a
    token by using the representations of related tokens in the sequence. This produces
    context-aware token representations. Consider an example sentence: “The train
    left the station on time.” Now, consider one word in the sentence: station. What
    kind of station are we talking about? Could it be a radio station? Maybe the International
    Space Station? Let’s figure it out algorithmically via self-attention (see [figure
    11.6](#fig11-6)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0368-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 11.6 Self-attention: Attention scores are computed between “station”
    and every other word in the sequence, and they are then used to weight a sum of
    word vectors that becomes the new “station” vector.**'
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 is to compute relevancy scores between the vector for “station” and every
    other word in the sentence. These are our “attention scores.” We’re simply going
    to use the dot product between two word vectors as a measure of the strength of
    their relationship. It’s a very computationally efficient distance function, and
    it was already the standard way to relate two word embeddings to each other long
    before Transformers. In practice, these scores will also go through a scaling
    function and a softmax, but for now, that’s just an implementation detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2 is to compute the sum of all word vectors in the sentence, weighted
    by our relevancy scores. Words closely related to “station” will contribute more
    to the sum (including the word “station” itself), whereas irrelevant words will
    contribute almost nothing. The resulting vector is our new representation for
    “station”: a representation that incorporates the surrounding context. In particular,
    it includes part of the “train” vector, clarifying that it is, in fact, a “train
    station.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'You’d repeat this process for every word in the sentence, producing a new sequence
    of vectors encoding the sentence. Let’s see it in R-like pseudocode:'
  prefs: []
  type: TYPE_NORMAL
- en: self_attention <- function(input_sequence) {
  prefs: []
  type: TYPE_NORMAL
- en: c(sequence_len, embedding_size) %<-% dim(input_sequence)
  prefs: []
  type: TYPE_NORMAL
- en: output <- array(0, dim(input_sequence))
  prefs: []
  type: TYPE_NORMAL
- en: for (i in 1:sequence_len) {➊
  prefs: []
  type: TYPE_NORMAL
- en: pivot_vector <- input_sequence[i, ]
  prefs: []
  type: TYPE_NORMAL
- en: scores <- sapply(1:sequence_len, function(j) ➋
  prefs: []
  type: TYPE_NORMAL
- en: pivot_vector %*% input_sequence[j, ])➌
  prefs: []
  type: TYPE_NORMAL
- en: scores <- softmax(scores / sqrt(embedding_size))➍
  prefs: []
  type: TYPE_NORMAL
- en: broadcast_scores <
  prefs: []
  type: TYPE_NORMAL
- en: as.matrix(scores)[, rep(1, embedding_size)]➎
  prefs: []
  type: TYPE_NORMAL
- en: new_pivot_representation <
  prefs: []
  type: TYPE_NORMAL
- en: colSums(input_sequence * broadcast_scores)➏
  prefs: []
  type: TYPE_NORMAL
- en: output[i, ] <- new_pivot_representation
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: output
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: softmax <- function(x) {
  prefs: []
  type: TYPE_NORMAL
- en: e <- exp(x - max(x))
  prefs: []
  type: TYPE_NORMAL
- en: e / sum(e)
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Iterate over each token in the input sequence.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Compute the dot product (attention score) between the token and every other
    token.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **%*% with two 1D vectors returns a scalar, the dot product. scores has shape
    (sequence_len).**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Scale by a normalization factor, and apply a softmax.**
  prefs: []
  type: TYPE_NORMAL
- en: '➎ **Broadcast the scores vector (shape: (sequence_len)) into a matrix of shape
    (sequence_len, embedding_size), the shape of input_sequence.**'
  prefs: []
  type: TYPE_NORMAL
- en: ➏ **Sum the score-adjusted input sequences to make a new embedding vector.**
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, in practice you’d use a vectorized implementation. Keras has a built-in
    layer to handle it: layer_multi_head_attention(). Here’s how you would use it:'
  prefs: []
  type: TYPE_NORMAL
- en: num_heads <- 4
  prefs: []
  type: TYPE_NORMAL
- en: embed_dim <- 256
  prefs: []
  type: TYPE_NORMAL
- en: mha_layer <- layer_multi_head_attention(num_heads = num_heads,
  prefs: []
  type: TYPE_NORMAL
- en: key_dim = embed_dim)
  prefs: []
  type: TYPE_NORMAL
- en: outputs <- mha_layer(inputs, inputs, inputs)➊
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **inputs has shape (batch_size, sequence_length, embed_dim).**
  prefs: []
  type: TYPE_NORMAL
- en: 'Reading this, you’re probably wondering:'
  prefs: []
  type: TYPE_NORMAL
- en: Why are we passing the inputs to the layer *three* times? That seems redundant.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are these “multiple heads” we’re referring to? That sounds intimidating—
    do they also grow back if you cut them?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both of these questions have simple answers. Let’s take a look.
  prefs: []
  type: TYPE_NORMAL
- en: 'GENERALIZED SELF-ATTENTION: THE QUERY-KEY-VALUE MODEL'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'So far, we have considered only one input sequence. However, the Transformer
    architecture was originally developed for machine translation, where you have
    to deal with two input sequences: the source sequence you’re currently translating
    (such as “How’s the weather today?”), and the target sequence you’re converting
    it to (such as “¿Qué tiempo hace hoy?”). A Transformer is a *sequence-to-sequence*
    model: it was designed to convert one sequence into another. You’ll learn about
    sequence-to-sequence models in depth later in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s take a step back. The self-attention mechanism as we’ve introduced
    it performs the following, schematically:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0370-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This means “for each token in inputs (A), compute how much the token is related
    to every token in inputs (B), and use these scores to weight a sum of tokens from
    inputs (C).” Crucially, there’s nothing that requires A, B, and C to refer to
    the same input sequence. In the general case, you could be doing this with three
    different sequences. We’ll call them “query,” “keys,” and “values.” The operation
    becomes “for each element in the query, compute how much the element is related
    to every key, and use these scores to weight a sum of values”:'
  prefs: []
  type: TYPE_NORMAL
- en: outputs <- sum( **values** * pairwise_scores( **query**, **keys** ))
  prefs: []
  type: TYPE_NORMAL
- en: This terminology comes from search engines and recommender systems (see [figure
    11.7](#fig11-7)). Imagine that you’re typing up a query to retrieve a photo from
    your collection, “dogs on the beach.” Internally, each of your pictures in the
    database is described by a set of keywords—“cat,” “dog,” “party,” and so forth.
    We’ll call those “keys.” The search engine will start by comparing your query
    to the keys in the database. “Dog” yields a match of 1, and “cat” yields a match
    of 0\. It will then rank those keys by strength of match—relevance—and it will
    return the pictures associated with the top *N* matches, in order of relevance.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0371-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 11.7 Retrieving images from a database: The “query” is compared to
    a set of “keys,” and the match scores are used to rank “values” (images).**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Conceptually, this is what Transformer-style attention is doing. You’ve got
    a reference sequence that describes something you’re looking for: the query. You’ve
    got a body of knowledge that you’re trying to extract information from: the values.
    Each value is assigned a key that describes the value in a format that can be
    readily compared to a query. You simply match the query to the keys. Then you
    return a weighted sum of values.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, the keys and the values are often the same sequence. In machine
    translation, for instance, the query would be the target sequence, and the source
    sequence would play the roles of both keys and values: for each element of the
    target (like “tiempo”), you want to go back to the source (“How’s the weather
    today?”) and identify the different bits that are related to it (“tiempo” and
    “weather” should have a strong match). And naturally, if you’re just doing sequence
    classification, then query, keys, and values are all the same: you’re comparing
    a sequence to itself, to enrich each token with context from the whole sequence.'
  prefs: []
  type: TYPE_NORMAL
- en: That explains why we needed to pass inputs three times to our layer_multi_ head_attention()
    layer. But why “multi-head” attention?
  prefs: []
  type: TYPE_NORMAL
- en: 11.4.2 Multi-head attention
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '“Multi-head attention” is an extra tweak to the self-attention mechanism, introduced
    in “Attention Is All You Need.” The “multi-head” moniker refers to the fact that
    the output space of the self-attention layer is factored into a set of independent
    subspaces, learned separately: the initial query, key, and value are sent through
    three independent sets of dense projections, resulting in three separate vectors.
    Each vector is processed via neural attention, and the three outputs are concatenated
    back into a single output sequence. Each such subspace is called a “head.” The
    full picture is shown in [figure 11.8](#fig11-8).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0372-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 11.8 The MultiHeadAttention layer**'
  prefs: []
  type: TYPE_NORMAL
- en: The presence of the learnable dense projections enables the layer to actually
    learn something, as opposed to being a purely stateless transformation that would
    require additional layers before or after it to be useful. In addition, having
    independent heads helps the layer learn different groups of features for each
    token, where features within one group are correlated with each other but are
    mostly independent from features in a different group.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is similar in principle to what makes depthwise-separable convolutions
    work: in a depthwise-separable convolution, the output space of the convolution
    is factored into many subspaces (one per input channel) that are learned independently.
    The “Attention Is All You Need” paper was written at a time when the idea of factoring
    feature spaces into independent subspaces had been shown to provide great benefits
    for computer vision models, both in the case of depthwise-separable convolutions
    and in the case of a closely related approach, *grouped convolutions*. Multi-head
    attention is simply the application of the same idea to self-attention.'
  prefs: []
  type: TYPE_NORMAL
- en: 11.4.3 The Transformer encoder
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If adding extra dense projections is so useful, why don’t we also apply one
    or two to the output of the attention mechanism? Actually, that’s a great idea—let’s
    do that. And our model is starting to do a lot, so we might want to add residual
    connections to make sure we don’t destroy any valuable information along the way;
    you learned in chapter 9 that they’re a must for any sufficiently deep architecture.
    And there’s another thing you learned in chapter 9: normalization layers are supposed
    to help gradients flow better during backpropagation. Let’s add those, too.'
  prefs: []
  type: TYPE_NORMAL
- en: That’s roughly the thought process that I imagine unfolded in the minds of the
    inventors of the Transformer architecture at the time. Factoring outputs into
    multiple independent spaces, adding residual connections, adding normalization
    layers—all of these are standard architecture patterns that one would be wise
    to leverage in any complex model. Together, these bells and whistles form the
    Transformer encoder—one of two critical parts that make up the Transformer architecture
    (see [figure 11.9](#fig11-9)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The original Transformer architecture consists of two parts: a *Transformer
    encoder* that processes the source sequence, and a *Transformer decoder* that
    uses the source sequence to generate a translated version. You’ll learn about
    the decoder part in a minute.'
  prefs: []
  type: TYPE_NORMAL
- en: Crucially, the encoder part can be used for text classification. It’s a very
    generic module that ingests a sequence and learns to turn it into a more useful
    representation. Let’s implement a Transformer encoder and try it on the movie
    review sentiment classification task.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0373-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 11.9 The Transformer-Encoder chains a layer_multi_ head_attention()
    with a dense projection and adds normalization as well as residual connections.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.21 Transformer encoder implemented as a subclassed Layer**'
  prefs: []
  type: TYPE_NORMAL
- en: layer_transformer_encoder <- new_layer_class(
  prefs: []
  type: TYPE_NORMAL
- en: classname = "TransformerEncoder",
  prefs: []
  type: TYPE_NORMAL
- en: initialize = function(embed_dim, dense_dim, num_heads, …) {
  prefs: []
  type: TYPE_NORMAL
- en: super$initialize(…)
  prefs: []
  type: TYPE_NORMAL
- en: self$embed_dim <- embed_dim ➊
  prefs: []
  type: TYPE_NORMAL
- en: self$dense_dim <- dense_dim ➋
  prefs: []
  type: TYPE_NORMAL
- en: self$num_heads <- num_heads ➌
  prefs: []
  type: TYPE_NORMAL
- en: self$attention <
  prefs: []
  type: TYPE_NORMAL
- en: layer_multi_head_attention(num_heads = num_heads,
  prefs: []
  type: TYPE_NORMAL
- en: key_dim = embed_dim)
  prefs: []
  type: TYPE_NORMAL
- en: self$dense_proj <- keras_model_sequential() %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(dense_dim, activation = "relu") %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(embed_dim)
  prefs: []
  type: TYPE_NORMAL
- en: self$layernorm_1 <- layer_layer_normalization()
  prefs: []
  type: TYPE_NORMAL
- en: self$layernorm_2 <- layer_layer_normalization()
  prefs: []
  type: TYPE_NORMAL
- en: '},'
  prefs: []
  type: TYPE_NORMAL
- en: call = function(inputs, mask = NULL) {➍
  prefs: []
  type: TYPE_NORMAL
- en: if (!is.null(mask))➎
  prefs: []
  type: TYPE_NORMAL
- en: mask <- mask[, tf$newaxis, ]➎
  prefs: []
  type: TYPE_NORMAL
- en: inputs %>%
  prefs: []
  type: TYPE_NORMAL
- en: '{ self$attention(., ., attention_mask = mask) + . } %>% ➏'
  prefs: []
  type: TYPE_NORMAL
- en: self$layernorm_1() %>%
  prefs: []
  type: TYPE_NORMAL
- en: '{ self$dense_proj(.) + . } %>% ➐'
  prefs: []
  type: TYPE_NORMAL
- en: self$layernorm_2()
  prefs: []
  type: TYPE_NORMAL
- en: '},'
  prefs: []
  type: TYPE_NORMAL
- en: get_config = function() { ➑
  prefs: []
  type: TYPE_NORMAL
- en: config <- super$get_config()
  prefs: []
  type: TYPE_NORMAL
- en: for(name in c("embed_dim", "num_heads", "dense_dim"))
  prefs: []
  type: TYPE_NORMAL
- en: config[[name]] <- self[[name]]
  prefs: []
  type: TYPE_NORMAL
- en: config
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Size of the input token vectors**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Size of the inner dense layer**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Number of attention heads**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Computation goes in call()**
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **The mask that will be generated by the embedding layer will be 2D, but the
    attention layer expects it to be 3D or 4D, so we expand its rank.**
  prefs: []
  type: TYPE_NORMAL
- en: ➏ **Add residual connection to output of attention layer.**
  prefs: []
  type: TYPE_NORMAL
- en: ➐ **Add residual connection to output of the dense_proj() layer.**
  prefs: []
  type: TYPE_NORMAL
- en: ➑ **Implement serialization so we can save the model.**
  prefs: []
  type: TYPE_NORMAL
- en: '**%>% and { }**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the above example, we pipe with %>% into an expression that is wrapped with
    { }. This is an advanced feature of %>%, which allows you to pipe into complex
    or compound expressions. %>% will place in the piped argument to each location
    we request with the . symbol. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: x %>% { fn(., .) + . }
  prefs: []
  type: TYPE_NORMAL
- en: 'is equivalent to:'
  prefs: []
  type: TYPE_NORMAL
- en: fn(x, x) + x
  prefs: []
  type: TYPE_NORMAL
- en: 'If we were to write the call() method of layer_transformer_encoder() without
    %>%, it would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: call = function(inputs, mask = NULL) {
  prefs: []
  type: TYPE_NORMAL
- en: if (!is.null(mask))
  prefs: []
  type: TYPE_NORMAL
- en: mask <- mask[, tf$newaxis, ]
  prefs: []
  type: TYPE_NORMAL
- en: attention_output <- self$attention(inputs, inputs,
  prefs: []
  type: TYPE_NORMAL
- en: attention_mask = mask)
  prefs: []
  type: TYPE_NORMAL
- en: proj_input <- self$layernorm_1(inputs + attention_output)
  prefs: []
  type: TYPE_NORMAL
- en: proj_output <- self$dense_proj(proj_input)
  prefs: []
  type: TYPE_NORMAL
- en: self$layernorm_2(proj_input + proj_output)
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '**Saving custom layers**'
  prefs: []
  type: TYPE_NORMAL
- en: 'When you write custom layers, make sure to implement the get_config() method:
    this enables the layer to be reinstantiated from its config, which is useful during
    model saving and loading. The method should return a named R list that contains
    the values of the constructor arguments used to create the layer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'All Keras layers can be serialized and deserialized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: config <- layer$get_config()
  prefs: []
  type: TYPE_NORMAL
- en: new_layer <- do.call(layer_<type>, config)
  prefs: []
  type: TYPE_NORMAL
- en: 'where layer_<type> is the original layer constructor. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: layer <- layer_dense(units = 10)
  prefs: []
  type: TYPE_NORMAL
- en: config <- layer$get_config() ➊
  prefs: []
  type: TYPE_NORMAL
- en: new_layer <- do.call(layer_dense, config)➋
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **config is a regular named R list. You can safely save it to disk as an rds,
    then load it in a new R session.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **The config does not contain weight values, so all weights in the layer are
    initialized from scratch.**
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also access the unwrapped original layer constructor from any existing
    layer directly via the special symbol __class__ (though you rarely need to do
    so):'
  prefs: []
  type: TYPE_NORMAL
- en: layer$`__class__`
  prefs: []
  type: TYPE_NORMAL
- en: <class ‘keras.layers.core.dense.Dense’>
  prefs: []
  type: TYPE_NORMAL
- en: new_layer <- layer$`__class__`$from_config(config)
  prefs: []
  type: TYPE_NORMAL
- en: 'Defining the get_config() method in custom layer classes enables the same work-flow.
    For instance:'
  prefs: []
  type: TYPE_NORMAL
- en: layer <- layer_transformer_encoder(embed_dim = 256, dense_dim = 32,
  prefs: []
  type: TYPE_NORMAL
- en: num_heads = 2)
  prefs: []
  type: TYPE_NORMAL
- en: config <- layer$get_config()
  prefs: []
  type: TYPE_NORMAL
- en: new_layer <- do.call(layer_transformer_encoder, config)
  prefs: []
  type: TYPE_NORMAL
- en: -- or --
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: new_layer <- layer$`__class__`$from_config(config)
  prefs: []
  type: TYPE_NORMAL
- en: 'When saving a model that contains custom layers, the saved file will contain
    these configs. When loading the model from the file, you should provide the custom
    layer classes to the loading process, so that it can make sense of the config
    objects:'
  prefs: []
  type: TYPE_NORMAL
- en: model <- save_model_tf(model, filename)
  prefs: []
  type: TYPE_NORMAL
- en: model <- load_model_tf(filename,
  prefs: []
  type: TYPE_NORMAL
- en: custom_objects = list(layer_transformer_encoder))
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that if the list supplied to custom_objects is named, then names are matched
    to the classname argument that was provided when the custom object was constructed:'
  prefs: []
  type: TYPE_NORMAL
- en: model <- load_model_tf(
  prefs: []
  type: TYPE_NORMAL
- en: filename,
  prefs: []
  type: TYPE_NORMAL
- en: custom_objects = list(TransformerEncoder = layer_transformer_encoder))
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ll note that the normalization layers we’re using here aren’t layer_batch_
    normalization() like those we’ve used before in image models. That’s because layer_
    batch_normalization() doesn’t work well for sequence data. Instead, we’re using
    the layer_layer_normalization(), which normalizes each sequence independently
    from other sequences in the batch. Like this, in R pseudocode:'
  prefs: []
  type: TYPE_NORMAL
- en: layer_normalization <- function(batch_of_sequences) {
  prefs: []
  type: TYPE_NORMAL
- en: c(batch_size, sequence_length, embedding_dim) %<-%
  prefs: []
  type: TYPE_NORMAL
- en: dim(batch_of_sequences)➊
  prefs: []
  type: TYPE_NORMAL
- en: means <- variances <-
  prefs: []
  type: TYPE_NORMAL
- en: array(0, dim = dim(batch_of_sequences))
  prefs: []
  type: TYPE_NORMAL
- en: for (b in seq(batch_size))
  prefs: []
  type: TYPE_NORMAL
- en: for (s in seq(sequence_length)) {
  prefs: []
  type: TYPE_NORMAL
- en: embedding <- batch_of_sequences[b, s, ]➋
  prefs: []
  type: TYPE_NORMAL
- en: means[b, s, ] <- mean(embedding)
  prefs: []
  type: TYPE_NORMAL
- en: variances[b, s, ] <- var(embedding)
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: (batch_of_sequences - means) / variances
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '➊ **Input shape: (batch_size, sequence_length, embedding_dim)**'
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **To compute mean and variance, we pool data only over the last axis (axis
    -1, the embedding axis).**
  prefs: []
  type: TYPE_NORMAL
- en: 'Compare to layer_batch_normalization() (during training):'
  prefs: []
  type: TYPE_NORMAL
- en: batch_normalization <- function(batch_of_images) {
  prefs: []
  type: TYPE_NORMAL
- en: c(batch_size, height, width, channels) %<-%
  prefs: []
  type: TYPE_NORMAL
- en: dim(batch_of_images) ➊
  prefs: []
  type: TYPE_NORMAL
- en: means <- variances <-
  prefs: []
  type: TYPE_NORMAL
- en: array(0, dim = dim(batch_of_images))
  prefs: []
  type: TYPE_NORMAL
- en: for (ch in seq(channels)) {
  prefs: []
  type: TYPE_NORMAL
- en: channel <- batch_of_images[, , , ch]➋
  prefs: []
  type: TYPE_NORMAL
- en: means[, , , ch] <- mean(channel)
  prefs: []
  type: TYPE_NORMAL
- en: variances[, , , ch] <- var(channel)
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: (batch_of_images - means) / variances
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '➊ **Input shape: (batch_size, height, width, channels)**'
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Pool the data over the batch axis (the first axis), which creates interactions
    between samples in a batch.**
  prefs: []
  type: TYPE_NORMAL
- en: Although batch_normalization() collects information from many samples to obtain
    accurate statistics for the feature means and variances, layer_normalization()
    pools data within each sequence separately, which is more appropriate for sequence
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve implemented our TransformerEncoder, we can use it to assemble
    a text-classification model similar to the LSTM-based one you’ve seen previously.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.22 Using the Transformer Encoder for text classification**'
  prefs: []
  type: TYPE_NORMAL
- en: vocab_size <- 20000
  prefs: []
  type: TYPE_NORMAL
- en: embed_dim <- 256
  prefs: []
  type: TYPE_NORMAL
- en: num_heads <- 2
  prefs: []
  type: TYPE_NORMAL
- en: dense_dim <- 32
  prefs: []
  type: TYPE_NORMAL
- en: inputs <- layer_input(shape(NA), dtype = "int64")
  prefs: []
  type: TYPE_NORMAL
- en: outputs <- inputs %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_embedding(vocab_size, embed_dim) %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_transformer_encoder(embed_dim, dense_dim, num_heads) %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_global_average_pooling_1d() %>%➊
  prefs: []
  type: TYPE_NORMAL
- en: layer_dropout(0.5) %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(1, activation = "sigmoid")
  prefs: []
  type: TYPE_NORMAL
- en: model <- keras_model(inputs, outputs)
  prefs: []
  type: TYPE_NORMAL
- en: model %>% compile(optimizer = "rmsprop",
  prefs: []
  type: TYPE_NORMAL
- en: loss = "binary_crossentropy",
  prefs: []
  type: TYPE_NORMAL
- en: metrics = "accuracy")
  prefs: []
  type: TYPE_NORMAL
- en: model
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0377-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: ➊ **Because TransformerEncoder returns full sequences, we need to reduce each
    sequence to a single vector for classification via a global pooling layer.**
  prefs: []
  type: TYPE_NORMAL
- en: Let’s train it. It gets to 88.5% test accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.23 Training and evaluating the Transformer encoder–based model**'
  prefs: []
  type: TYPE_NORMAL
- en: callbacks = list(callback_model_checkpoint("transformer_encoder.keras",
  prefs: []
  type: TYPE_NORMAL
- en: save_best_only = TRUE))
  prefs: []
  type: TYPE_NORMAL
- en: model %>% fit(
  prefs: []
  type: TYPE_NORMAL
- en: int_train_ds,
  prefs: []
  type: TYPE_NORMAL
- en: validation_data = int_val_ds,
  prefs: []
  type: TYPE_NORMAL
- en: epochs = 20,
  prefs: []
  type: TYPE_NORMAL
- en: callbacks = callbacks
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: model <- load_model_tf(
  prefs: []
  type: TYPE_NORMAL
- en: '"transformer_encoder.keras",'
  prefs: []
  type: TYPE_NORMAL
- en: custom_objects = layer_transformer_encoder)➊
  prefs: []
  type: TYPE_NORMAL
- en: 'sprintf("Test acc: %.3f", evaluate(model, int_test_ds)["accuracy"])'
  prefs: []
  type: TYPE_NORMAL
- en: '[1] "Test acc: 0.885"'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Provide the custom TransformerEncoder class to the model-loading process.**
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you should start to feel a bit uneasy. Something’s off here.
    Can you tell what it is?
  prefs: []
  type: TYPE_NORMAL
- en: This section is ostensibly about “sequence models.” I started off by highlighting
    the importance of word order. I said that Transformer was a sequence-processing
    architecture, originally developed for machine translation. And yet… the Transformer
    encoder you just saw in action wasn’t a sequence model at all. Did you notice?
    It’s composed of dense layers that process sequence tokens independently from
    each other, and an attention layer that looks at the tokens *as a set*. You could
    change the order of the tokens in a sequence, and you’d get the exact same pairwise
    attention scores and the exact same context-aware representations. If you were
    to completely scramble the words in every movie review, the model wouldn’t notice,
    and you’d still get the exact same accuracy. Self-attention is a set-processing
    mechanism, focused on the relationships between pairs of sequence elements (see
    [figure 11.10](#fig11-10))—it’s blind to whether these elements occur at the beginning,
    at the end, or in the middle of a sequence. So why do we say that Transformer
    is a sequence model? And how could it possibly be good for machine translation
    if it doesn’t look at word order?
  prefs: []
  type: TYPE_NORMAL
- en: 'I hinted at the solution earlier in the chapter: I mentioned in passing that
    Transformer was a hybrid approach that is technically order agnostic but that
    manually injects order information in the representations it processes. This is
    the missing ingredient! It’s called *positional encoding*. Let’s take a look.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0378-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 11.10 Features of different types of NLP models**'
  prefs: []
  type: TYPE_NORMAL
- en: USING POSITIONAL ENCODING TO REINJECT ORDER INFORMATION
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The idea behind positional encoding is very simple: to give the model access
    to word-order information, we’re going to add the word’s position in the sentence
    to each word embedding. Our input word embeddings will have two components: the
    usual word vector, which represents the word independently of any specific context,
    and a position vector, which represents the position of the word in the current
    sentence. Hopefully, the model will then figure out how to best leverage this
    additional information.'
  prefs: []
  type: TYPE_NORMAL
- en: The simplest scheme you could come up with would be to concatenate the word’s
    position to its embedding vector. You’d add a “position” axis to the vector and
    fill it with 0 for the first word in the sequence, 1 for the second, and so on.
    That may not be ideal, however, because the positions can potentially be very
    large integers, which will disrupt the range of values in the embedding vector.
    As you know, neural networks don’t like very large input values, or discrete input
    distributions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The original “Attention Is All You Need” paper used an interesting trick to
    encode word positions: it added to the word embeddings a vector containing values
    in the range [-1, 1] that varied cyclically depending on the position (it used
    cosine functions to achieve this). This trick offers a way to uniquely characterize
    any integer in a large range via a vector of small values. It’s clever, but it’s
    not what we’re going to use in our case. We’ll do something simpler and more effective:
    we’ll learn position-embedding vectors the same way we learn to embed word indices.
    We’ll then proceed to add our position embeddings to the corresponding word embeddings,
    to obtain a position-aware word embedding. This technique is called “positional
    embedding.” Let’s implement it.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.24 Implementing positional embedding as a subclassed layer**'
  prefs: []
  type: TYPE_NORMAL
- en: layer_positional_embedding <- new_layer_class(
  prefs: []
  type: TYPE_NORMAL
- en: classname = "PositionalEmbedding",
  prefs: []
  type: TYPE_NORMAL
- en: initialize = function(sequence_length, ➊
  prefs: []
  type: TYPE_NORMAL
- en: input_dim, output_dim, …) {
  prefs: []
  type: TYPE_NORMAL
- en: super$initialize(…)
  prefs: []
  type: TYPE_NORMAL
- en: self$token_embeddings <-➋
  prefs: []
  type: TYPE_NORMAL
- en: layer_embedding(input_dim = input_dim,
  prefs: []
  type: TYPE_NORMAL
- en: output_dim = output_dim)
  prefs: []
  type: TYPE_NORMAL
- en: self$position_embeddings <-➌
  prefs: []
  type: TYPE_NORMAL
- en: layer_embedding(input_dim = sequence_length,
  prefs: []
  type: TYPE_NORMAL
- en: output_dim = output_dim)
  prefs: []
  type: TYPE_NORMAL
- en: self$sequence_length <- sequence_length
  prefs: []
  type: TYPE_NORMAL
- en: self$input_dim <- input_dim
  prefs: []
  type: TYPE_NORMAL
- en: self$output_dim <- output_dim
  prefs: []
  type: TYPE_NORMAL
- en: '},'
  prefs: []
  type: TYPE_NORMAL
- en: call = function(inputs) {
  prefs: []
  type: TYPE_NORMAL
- en: len <- tf$shape(inputs)[-1]➍
  prefs: []
  type: TYPE_NORMAL
- en: positions <-
  prefs: []
  type: TYPE_NORMAL
- en: tf$range(start = 0L, limit = len, delta = 1L)➎
  prefs: []
  type: TYPE_NORMAL
- en: embedded_tokens <- self$token_embeddings(inputs)
  prefs: []
  type: TYPE_NORMAL
- en: embedded_positions <- self$position_embeddings(positions)
  prefs: []
  type: TYPE_NORMAL
- en: embedded_tokens + embedded_positions➏
  prefs: []
  type: TYPE_NORMAL
- en: '},'
  prefs: []
  type: TYPE_NORMAL
- en: compute_mask = function(inputs, mask = NULL) {➐
  prefs: []
  type: TYPE_NORMAL
- en: inputs != 0
  prefs: []
  type: TYPE_NORMAL
- en: '},'
  prefs: []
  type: TYPE_NORMAL
- en: get_config = function() {➑
  prefs: []
  type: TYPE_NORMAL
- en: config <- super$get_config()
  prefs: []
  type: TYPE_NORMAL
- en: for(name in c("output_dim", "sequence_length", "input_dim"))
  prefs: []
  type: TYPE_NORMAL
- en: config[[name]] <- self[[name]]
  prefs: []
  type: TYPE_NORMAL
- en: config
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **A downside of position embeddings is that the sequence length needs to be
    known in advance.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Prepare a layer_embedding() for the token indices.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Prepare another one for the token positions.**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **tf$shape(inputs)[-1] slices out the last element of the shape, the size
    of the embedding dimension. (tf$shape() returns the shape as a tensor.)**
  prefs: []
  type: TYPE_NORMAL
- en: '➎ **tf$range() is similar to seq() in R, makes a integer sequence: [0, 1, 2,
    …, limit - 1].**'
  prefs: []
  type: TYPE_NORMAL
- en: ➏ **Add both embedding vectors together.**
  prefs: []
  type: TYPE_NORMAL
- en: ➐ **Like layer_embedding(), this layer should be able to generate a mask so
    we can ignore padding 0s in the inputs. The compute_mask() method will called
    automatically by the framework, and the mask will be propagated to the next layer.**
  prefs: []
  type: TYPE_NORMAL
- en: ➑ **Implement serialization so we can save the model.**
  prefs: []
  type: TYPE_NORMAL
- en: You would use this layer_positional_embedding() just like a regular layer_ embedding().
    Let’s see it in action!
  prefs: []
  type: TYPE_NORMAL
- en: 'PUTTING IT ALL TOGETHER: A TEXT-CLASSIFICATION TRANSFORMER'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All you have to do to start taking word order into account is swap the old layer_
    embedding() with our position-aware version.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.25 Combining the Transformer encoder with positional embedding
  prefs: []
  type: TYPE_NORMAL
- en: vocab_size <- 20000
  prefs: []
  type: TYPE_NORMAL
- en: sequence_length <- 600
  prefs: []
  type: TYPE_NORMAL
- en: embed_dim <- 256
  prefs: []
  type: TYPE_NORMAL
- en: num_heads <- 2
  prefs: []
  type: TYPE_NORMAL
- en: dense_dim <- 32
  prefs: []
  type: TYPE_NORMAL
- en: inputs <- layer_input(shape(NULL), dtype = "int64")
  prefs: []
  type: TYPE_NORMAL
- en: outputs <- inputs %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_positional_embedding(sequence_length, vocab_size, embed_dim) %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_transformer_encoder(embed_dim, dense_dim, num_heads) %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_global_average_pooling_1d() %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dropout(0.5) %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(1, activation = "sigmoid")
  prefs: []
  type: TYPE_NORMAL
- en: model <-
  prefs: []
  type: TYPE_NORMAL
- en: keras_model(inputs, outputs) %>%
  prefs: []
  type: TYPE_NORMAL
- en: compile(optimizer = "rmsprop",
  prefs: []
  type: TYPE_NORMAL
- en: loss = "binary_crossentropy",
  prefs: []
  type: TYPE_NORMAL
- en: metrics = "accuracy")
  prefs: []
  type: TYPE_NORMAL
- en: model
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0380-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: callbacks <- list(
  prefs: []
  type: TYPE_NORMAL
- en: callback_model_checkpoint("full_transformer_encoder.keras",
  prefs: []
  type: TYPE_NORMAL
- en: save_best_only = TRUE)
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: model %>% fit(
  prefs: []
  type: TYPE_NORMAL
- en: int_train_ds,
  prefs: []
  type: TYPE_NORMAL
- en: validation_data = int_val_ds,
  prefs: []
  type: TYPE_NORMAL
- en: epochs = 20,
  prefs: []
  type: TYPE_NORMAL
- en: callbacks = callbacks
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: model <- load_model_tf(
  prefs: []
  type: TYPE_NORMAL
- en: '"full_transformer_encoder.keras",'
  prefs: []
  type: TYPE_NORMAL
- en: custom_objects = list(layer_transformer_encoder,
  prefs: []
  type: TYPE_NORMAL
- en: layer_positional_embedding))
  prefs: []
  type: TYPE_NORMAL
- en: cat(sprintf(
  prefs: []
  type: TYPE_NORMAL
- en: '"Test acc: %.3f\n", evaluate(model, int_test_ds)["accuracy"]))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Test acc: 0.886'
  prefs: []
  type: TYPE_NORMAL
- en: Look here! We get to 88.6% test accuracy—an improvement that demonstrates the
    value of word-order information for text classification. This is our best sequence
    model so far! However, it’s still one notch below the bag-of-words approach.
  prefs: []
  type: TYPE_NORMAL
- en: 11.4.4 When to use sequence models over bag-of-words models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You may sometimes hear that bag-of-words methods are outdated and that Transformer-based
    sequence models are the way to go, no matter what task or dataset you’re looking
    at. This is definitely not the case: a small stack of dense layers on top of a
    bag-of-bigrams remains a perfectly valid and relevant approach in many cases.
    In fact, among the various techniques that we’ve tried on the IMDB dataset throughout
    this chapter, the best performing so far was the bag-of-bigrams! So, when should
    you prefer one approach over the other?'
  prefs: []
  type: TYPE_NORMAL
- en: In 2017, my team and I ran a systematic analysis of the performance of various
    text-classification techniques across many different types of text datasets, and
    we discovered a remarkable and surprising rule of thumb for deciding whether to
    go with a bag-of-words model or a sequence model ([http://mng.bz/AOzK](http://mng.bz/AOzK))—a
    golden constant of sorts. It turns out that when approaching a new text-classification
    task, you should pay close attention to the ratio between the number of samples
    in your training data and the mean number of words per sample (see [figure 11.11](#fig11-11)).
    If that ratio is small— less than 1,500—then the bag-of-bigrams model will perform
    better (and as a bonus, it will be much faster to train and to iterate on, too).
    If that ratio is higher than 1,500, then you should go with a sequence model.
    In other words, sequence models work best when lots of training data is available
    and when each sample is relatively short.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0381-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 11.11 A simple heuristic for selecting a text-classification model:
    The ratio between the number of training samples and the mean number of words
    per sample**'
  prefs: []
  type: TYPE_NORMAL
- en: So, if you’re classifying 1,000-word long documents, and you have 100,000 of
    them (a ratio of 100), you should go with a bigram model. If you’re classifying
    tweets that are 40 words long on average, and you have 50,000 of them (a ratio
    of 1,250), you should also go with a bigram model. But if you increase your dataset
    size to 500,000 tweets (a ratio of 12,500), go with a Transformer encoder. What
    about the IMDB movie-review classification task? We had 20,000 training samples
    and an average word count of 233, so our rule of thumb points toward a bigram
    model, which confirms what we found in practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'This intuitively makes sense: the input of a sequence model represents a richer
    and more complex space, and thus it takes more data to map out that space; meanwhile,
    a plain set of terms is a space so simple that you can train a logistic regression
    on top using just a few hundreds or thousands of samples. In addition, the shorter
    a sample is, the less the model can afford to discard any of the information it
    contains— in particular, word order becomes more important, and discarding it
    can create ambiguity. The sentences “this movie is the bomb” and “this movie was
    a bomb” have very close unigram representations, which could confuse a bag-of-words
    model, but a sequence model could tell which one is negative and which one is
    positive. With a longer sample, word statistics would become more reliable and
    the topic or sentiment would be more apparent from the word histogram alone.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, keep in mind that this heuristic rule was developed specifically for text
    classification. It may not necessarily hold for other NLP tasks—when it comes
    to machine translation, for instance, Transformer shines especially for very long
    sequences, compared to RNNs. Our heuristic is also just a rule of thumb, rather
    than a scientific law, so expect it to work most of the time, but not necessarily
    every time.
  prefs: []
  type: TYPE_NORMAL
- en: '11.5 Beyond text classification: Sequence-to-sequence learning'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You now possess all of the tools you will need to tackle most natural language
    processing tasks. However, you’ve seen these tools in action on only a single
    problem: text classification. This is an extremely popular use case, but there’s
    a lot more to NLP than classification. In this section, you’ll deepen your expertise
    by learning about *sequence-to-sequence models*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A sequence-to-sequence model takes a sequence as input (often a sentence or
    paragraph) and translates it into a different sequence. This is the task at the
    heart of many of the most successful applications of NLP:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Machine translation*—Convert a paragraph in a source language to its equivalent
    in a target language.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Text summarization*—Convert a long document to a shorter version that retains
    the most important information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Question answering*—Convert an input question into its answer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Chatbots*—Convert a dialogue prompt into a reply to this prompt, or convert
    the history of a conversation into the next reply in the conversation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Text generation*—Convert a text prompt into a paragraph that completes the
    prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And so forth.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The general template behind sequence-to-sequence models is described in [figure
    11.12](#fig11-12). During training:'
  prefs: []
  type: TYPE_NORMAL
- en: An *encoder* model turns the source sequence into an intermediate representation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A *decoder* is trained to predict the next token i in the target sequence by
    looking at both previous tokens (1 to i - 1) and the encoded source sequence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Image](../images/f0383-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 11.12 Sequence-to-sequence learning: The source sequence is processed
    by the encoder and is then sent to the decoder. The decoder looks at the target
    sequence so far and predicts the target sequence offset by one step in the future.
    During inference, we generate one target token at a time and feed it back into
    the decoder.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'During inference, we don’t have access to the target sequence—we’re trying
    to predict it from scratch. We’ll have to generate it one token at a time:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1** We obtain the encoded source sequence from the encoder.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**2** The decoder starts by looking at the encoded source sequence as well
    as an initial “seed” token (such as the string “[start]”), and uses that to predict
    the first real token in the sequence.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**3** The predicted sequence so far is fed back into the decoder, which generates
    the next token, and so on, until it generates a stop token (such as the string
    “[end]”).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Everything you’ve learned so far can be repurposed to build this new kind of
    model. Let’s dive in.
  prefs: []
  type: TYPE_NORMAL
- en: 11.5.1 A machine translation example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’ll demonstrate sequence-to-sequence modeling on a machine translation task.
    Machine translation is precisely what Transformer was developed for! We’ll start
    with a recurrent sequence model, and we’ll follow up with the full Transformer
    architecture. We’ll be working with an English-to-Spanish translation dataset
    available at [http://www.manythings.org/anki/.](http://www.manythings.org/anki/)
    Let’s download it:'
  prefs: []
  type: TYPE_NORMAL
- en: download.file(
  prefs: []
  type: TYPE_NORMAL
- en: '"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip",'
  prefs: []
  type: TYPE_NORMAL
- en: destfile = "spa-eng.zip")
  prefs: []
  type: TYPE_NORMAL
- en: zip::unzip("spa-eng.zip")
  prefs: []
  type: TYPE_NORMAL
- en: 'The text file contains one example per line: an English sentence, followed
    by a tab character, followed by the corresponding Spanish sentence. Let’s use
    readr::read_tsv() since we have tab-separated values:'
  prefs: []
  type: TYPE_NORMAL
- en: text_file <- "spa-eng/spa.txt"
  prefs: []
  type: TYPE_NORMAL
- en: text_pairs <- text_file %>%➊
  prefs: []
  type: TYPE_NORMAL
- en: readr::read_tsv(col_names = c("english", "spanish"),➋
  prefs: []
  type: TYPE_NORMAL
- en: col_types = c("cc")) %>%➌
  prefs: []
  type: TYPE_NORMAL
- en: within(spanish %<>% paste("[start]", ., "[end]"))➍
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Read the file using read_tsv() (tab-separated values).**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Each line contains an English phrase and its Spanish translation, tab separated.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Two-character columns**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **We prepend "[start]" and append "[end]" to the spanish sentence, to match
    the template from [figure 11.12](#fig11-12).**
  prefs: []
  type: TYPE_NORMAL
- en: 'Our text_pairs look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: str(text_pairs[sample(nrow(text_pairs), 1), ])
  prefs: []
  type: TYPE_NORMAL
- en: 'tibble [1 × 2] (S3: tbl_df/tbl/data.frame)'
  prefs: []
  type: TYPE_NORMAL
- en: '$ english: chr "I’m staying in Italy."'
  prefs: []
  type: TYPE_NORMAL
- en: '$ spanish: chr "[start] Me estoy quedando en Italia. [end]"'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s shuffle them and split them into the usual training, validation, and
    test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: num_test_samples <- num_val_samples <-
  prefs: []
  type: TYPE_NORMAL
- en: round(0.15 * nrow(text_pairs))
  prefs: []
  type: TYPE_NORMAL
- en: num_train_samples <- nrow(text_pairs) - num_val_samples - num_test_samples
  prefs: []
  type: TYPE_NORMAL
- en: pair_group <- sample(c(
  prefs: []
  type: TYPE_NORMAL
- en: rep("train", num_train_samples),
  prefs: []
  type: TYPE_NORMAL
- en: rep("test", num_test_samples),
  prefs: []
  type: TYPE_NORMAL
- en: rep("val", num_val_samples)
  prefs: []
  type: TYPE_NORMAL
- en: ))
  prefs: []
  type: TYPE_NORMAL
- en: train_pairs <- text_pairs[pair_group == "train", ]
  prefs: []
  type: TYPE_NORMAL
- en: test_pairs <- text_pairs[pair_group == "test", ]
  prefs: []
  type: TYPE_NORMAL
- en: val_pairs <- text_pairs[pair_group == "val", ]
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s prepare two separate TextVectorization layers: one for English
    and one for Spanish. We’re going to need to customize the way strings are preprocessed:'
  prefs: []
  type: TYPE_NORMAL
- en: We need to preserve the “[start]” and “[end]” tokens that we’ve inserted. By
    default, the characters [ and ] would be stripped, but we want to keep them so
    we can tell apart the word “start” and the start token “[start]”.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Punctuation is different from language to language! In the Spanish Text-Vectorization
    layer, if we’re going to strip punctuation characters, we need to also strip the
    character ¿.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that for a non-toy translation model, we would treat punctuation characters
    as separate tokens rather than stripping them, because we would want to be able
    to generate correctly punctuated sentences. In our case, for simplicity, we’ll
    get rid of all punctuation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We prepare a custom string standardization function for the Spanish TextVectorization
    layer: it preserves [ and ] but strips ¿, ¡, and all other characters from the
    [:punct:] class. (The double negation of the [:punct:] class cancels out, as if
    it was not negated at all. However, having the outer negated regex grouping lets
    us specifically exclude [ and ] from the [:punct:] regex class. We use | to add
    other special characters that are not in the [:punct:] character class, like ¡
    and ¿.)'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.26 Vectorizing the English and Spanish text pairs
  prefs: []
  type: TYPE_NORMAL
- en: punctuation_regex <- "[^[:^punct:][\\]]|[¡¿]"➊
  prefs: []
  type: TYPE_NORMAL
- en: library(tensorflow)
  prefs: []
  type: TYPE_NORMAL
- en: custom_standardization <- function(input_string) {➋
  prefs: []
  type: TYPE_NORMAL
- en: input_string %>%
  prefs: []
  type: TYPE_NORMAL
- en: tf$strings$lower() %>%
  prefs: []
  type: TYPE_NORMAL
- en: tf$strings$regex_replace(punctuation_regex, "")
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: input_string <- as_tensor("[start] ¡corre! [end]")
  prefs: []
  type: TYPE_NORMAL
- en: custom_standardization(input_string)
  prefs: []
  type: TYPE_NORMAL
- en: tf.Tensor(b’[start] corre [end]’, shape=(), dtype=string)➌
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Essentially, [[:punct:]], except it omits "[" and "]" and adds "¿" and "¡".**
  prefs: []
  type: TYPE_NORMAL
- en: '➋ **Note: this time we''re using tensor operations. This allows the function
    to be traced into a TensorFlow graph.**'
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Preserved the [] of [start] and [end], and stripped out ¡ and !.**
  prefs: []
  type: TYPE_NORMAL
- en: '**WARNING** The TensorFlow regex has minor differences from the R regex engine.
    Consult the source documentation if you need advanced regular expressions: [https://github.com/google/re2/wiki/Syntax](https://www.github.com/google/re2/wiki/Syntax).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: vocab_size <- 15000➊
  prefs: []
  type: TYPE_NORMAL
- en: sequence_length <- 20
  prefs: []
  type: TYPE_NORMAL
- en: source_vectorization <- layer_text_vectorization(➋
  prefs: []
  type: TYPE_NORMAL
- en: max_tokens = vocab_size,
  prefs: []
  type: TYPE_NORMAL
- en: output_mode = "int",
  prefs: []
  type: TYPE_NORMAL
- en: output_sequence_length = sequence_length
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: target_vectorization <- layer_text_vectorization(➌
  prefs: []
  type: TYPE_NORMAL
- en: max_tokens = vocab_size,
  prefs: []
  type: TYPE_NORMAL
- en: output_mode = "int", output_sequence_length = sequence_length + 1,➍
  prefs: []
  type: TYPE_NORMAL
- en: standardize = custom_standardization
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: adapt(source_vectorization, train_pairs$english)➎
  prefs: []
  type: TYPE_NORMAL
- en: adapt(target_vectorization, train_pairs$spanish)
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **To keep things simple, we'll look at only the top 15,000 words in each language,
    and we'll restrict sentences to 20 words.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **The English layer**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **The Spanish layer**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Generate Spanish sentences that have one extra token, because we'll need
    to offset the sentence by one step during training.**
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **Learn the vocabulary of each language.**
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we can turn our data into a TF Dataset pipeline. We want it to return
    a pair (inputs, target) where inputs is a named list with two entries, the english
    sentence (the encoder input), and the spanish sentence (the decoder input), and
    target is the Spanish sentence offset by one step ahead.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.27 Preparing datasets for the translation task
  prefs: []
  type: TYPE_NORMAL
- en: format_pair <- function(pair) {
  prefs: []
  type: TYPE_NORMAL
- en: eng <- source_vectorization(pair$english)➊
  prefs: []
  type: TYPE_NORMAL
- en: spa <- target_vectorization(pair$spanish)
  prefs: []
  type: TYPE_NORMAL
- en: inputs <- list(english = eng,
  prefs: []
  type: TYPE_NORMAL
- en: spanish = spa[NA:-2])➋
  prefs: []
  type: TYPE_NORMAL
- en: targets <- spa[2:NA]➌
  prefs: []
  type: TYPE_NORMAL
- en: list(inputs, targets)➍
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: batch_size <- 64
  prefs: []
  type: TYPE_NORMAL
- en: library(tfdatasets)
  prefs: []
  type: TYPE_NORMAL
- en: make_dataset <- function(pairs) {
  prefs: []
  type: TYPE_NORMAL
- en: tensor_slices_dataset(pairs) %>%
  prefs: []
  type: TYPE_NORMAL
- en: dataset_map(format_pair, num_parallel_calls = 4) %>%
  prefs: []
  type: TYPE_NORMAL
- en: dataset_cache() %>%➎
  prefs: []
  type: TYPE_NORMAL
- en: dataset_shuffle(2048) %>%
  prefs: []
  type: TYPE_NORMAL
- en: dataset_batch(batch_size) %>%
  prefs: []
  type: TYPE_NORMAL
- en: dataset_prefetch(16)
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: train_ds <- make_dataset(train_pairs)
  prefs: []
  type: TYPE_NORMAL
- en: val_ds <- make_dataset(val_pairs)
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **The vectorization layer can be called with either batched or unbatched data.
    Here, we apply the vectorization before batching the data.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Omit the last token from the Spanish sentence, so inputs and targets are
    the same length. [NA:-2] drops the last element of a tensor.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **[2:NA] drops the first element of a tensor.**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **The target Spanish sentence is one step ahead. Both are still the same length
    (20 words).**
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **Use in-memory caching to speed up preprocessing.**
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s what our dataset outputs look like:'
  prefs: []
  type: TYPE_NORMAL
- en: c(inputs, targets) %<-% iter_next(as_iterator(train_ds))
  prefs: []
  type: TYPE_NORMAL
- en: str(inputs)
  prefs: []
  type: TYPE_NORMAL
- en: List of 2
  prefs: []
  type: TYPE_NORMAL
- en: '$ english:<tf.Tensor: shape=(64, 20), dtype=int64, numpy=…>'
  prefs: []
  type: TYPE_NORMAL
- en: '$ spanish:<tf.Tensor: shape=(64, 20), dtype=int64, numpy=…>'
  prefs: []
  type: TYPE_NORMAL
- en: str(targets)
  prefs: []
  type: TYPE_NORMAL
- en: '<tf.Tensor: shape=(64, 20), dtype=int64, numpy=…>'
  prefs: []
  type: TYPE_NORMAL
- en: The data is now ready—time to build some models. We’ll start with a recurrent
    sequence-to-sequence model before moving on to a Transformer.
  prefs: []
  type: TYPE_NORMAL
- en: 11.5.2 Sequence-to-sequence learning with RNNs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recurrent neural networks dominated sequence-to-sequence learning from 2015–
    2017 before being overtaken by Transformer. They were the basis for many real-world
    machine translation systems, as mentioned in chapter 10\. Google Translate circa
    2017 was powered by a stack of seven large LSTM layers. It’s still worth learning
    about this approach today, because it provides an easy entry point to understanding
    sequence-to-sequence models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest, naive way to use RNNs to turn a sequence into another sequence
    is to keep the output of the RNN at each time step. In Keras, it would look like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: inputs <- layer_input(shape = c(sequence_length), dtype = "int64")
  prefs: []
  type: TYPE_NORMAL
- en: outputs <- inputs %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_embedding(input_dim = vocab_size, output_dim = 128) %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_lstm(32, return_sequences = TRUE) %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(vocab_size, activation = "softmax")
  prefs: []
  type: TYPE_NORMAL
- en: model <- keras_model(inputs, outputs)
  prefs: []
  type: TYPE_NORMAL
- en: 'However, this approach has two major issues:'
  prefs: []
  type: TYPE_NORMAL
- en: The target sequence must always be the same length as the source sequence. In
    practice, this is rarely the case. Technically, this isn’t critical, because you
    could always pad either the source sequence or the target sequence to make their
    lengths match.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to the step-by-step nature of RNNs, the model will be looking only at tokens
    1…*N* in the source sequence to predict token *N* in the target sequence. This
    constraint makes this setup unsuitable for most tasks, particularly translation.
    Consider translating “The weather is nice today” to French—that would be “Il fait
    beau aujourd’hui.” You’d need to be able to predict “Il” from just “The,” “Il
    fait” from just “The weather,” and so on, which is simply impossible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you’re a human translator, you’d start by reading the entire source sentence
    before starting to translate it. This is especially important if you’re dealing
    with languages that have wildly different word ordering, like English and Japanese.
    And that’s exactly what standard sequence-to-sequence models do.
  prefs: []
  type: TYPE_NORMAL
- en: In a proper sequence-to-sequence setup (see [figure 11.13](#fig11-13)), you
    would first use an RNN (the encoder) to turn the entire source sequence into a
    single vector (or set of vectors). This could be the last output of the RNN, or
    alternatively, its final internal state vectors. Then you would use this vector
    (or vectors) as the *initial state* of another RNN (the decoder), which would
    look at elements 1…*N* in the target sequence, and try to predict step *N*+1 in
    the target sequence.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0388-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 11.13 A sequence-to-sequence RNN: an RNN encoder is used to produce
    a vector that encodes the entire source sequence, which is used as the initial
    state for an RNN decoder.**'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s implement this in Keras with GRU-based encoders and decoders. The choice
    of GRU rather than LSTM makes things a bit simpler, because GRU has only a single
    state vector, whereas LSTM has multiple. Let’s start with the encoder.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.28 GRU-based encoder
  prefs: []
  type: TYPE_NORMAL
- en: embed_dim <- 256
  prefs: []
  type: TYPE_NORMAL
- en: latent_dim <- 1024
  prefs: []
  type: TYPE_NORMAL
- en: source <- layer_input(c(NA), dtype = "int64",
  prefs: []
  type: TYPE_NORMAL
- en: name = "english")➊
  prefs: []
  type: TYPE_NORMAL
- en: encoded_source <- source %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_embedding(vocab_size, embed_dim,
  prefs: []
  type: TYPE_NORMAL
- en: mask_zero = TRUE) %>%➋
  prefs: []
  type: TYPE_NORMAL
- en: bidirectional(layer_gru(units = latent_dim),
  prefs: []
  type: TYPE_NORMAL
- en: merge_mode = "sum")➌
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **The English source sentence goes here. Specifying the name of the input
    enables us to fit() the model with a named list of inputs.**
  prefs: []
  type: TYPE_NORMAL
- en: '➋ **Don''t forget masking: it''s critical in this setup.**'
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Our encoded source sentence is the last output of a bidirectional GRU.**
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s add the decoder—a simple GRU layer that takes as its initial state
    the encoded source sentence. On top of it, we add a layer_dense() that produces
    for each output step a probability distribution over the Spanish vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.29 GRU-based decoder and the end-to-end model
  prefs: []
  type: TYPE_NORMAL
- en: decoder_gru <- layer_gru(units = latent_dim, return_sequences = TRUE)
  prefs: []
  type: TYPE_NORMAL
- en: past_target <- layer_input(shape = c(NA), dtype = "int64", name = "spanish")➊
  prefs: []
  type: TYPE_NORMAL
- en: target_next_step <- past_target %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_embedding(vocab_size, embed_dim,
  prefs: []
  type: TYPE_NORMAL
- en: mask_zero = TRUE) %>%➋
  prefs: []
  type: TYPE_NORMAL
- en: decoder_gru(initial_state = encoded_source) %>%➌
  prefs: []
  type: TYPE_NORMAL
- en: layer_dropout(0.5) %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(vocab_size, activation = "softmax")➍
  prefs: []
  type: TYPE_NORMAL
- en: seq2seq_rnn <-
  prefs: []
  type: TYPE_NORMAL
- en: keras_model(inputs = list(source, past_target),➎
  prefs: []
  type: TYPE_NORMAL
- en: outputs = target_next_step)
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **The Spanish target sentence goes here.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Don't forget masking.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **The encoded source sentence serves as the initial state of the decoder GRU.**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Predict the next token.**
  prefs: []
  type: TYPE_NORMAL
- en: '➎ **End-to-end model: map the source sentence and the target sentence to the
    target sentence one step in the future**'
  prefs: []
  type: TYPE_NORMAL
- en: During training, the decoder takes as input the entire target sequence, but
    thanks to the step-by-step nature of RNNs, it looks only at tokens 1…*N* in the
    input to predict token *N* in the output (which corresponds to the next token
    in the sequence, because the output is intended to be offset by one step). This
    means we only use information from the past to predict the future, as we should;
    otherwise, we’d be cheating, and our model would not work at inference time.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start training.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.30 Training our recurrent sequence-to-sequence model
  prefs: []
  type: TYPE_NORMAL
- en: seq2seq_rnn %>% compile(optimizer = "rmsprop",
  prefs: []
  type: TYPE_NORMAL
- en: loss = "sparse_categorical_crossentropy",
  prefs: []
  type: TYPE_NORMAL
- en: metrics = "accuracy")
  prefs: []
  type: TYPE_NORMAL
- en: seq2seq_rnn %>% fit(train_ds, epochs = 15, validation_data = val_ds)
  prefs: []
  type: TYPE_NORMAL
- en: 'We picked accuracy as a crude way to monitor validation-set performance during
    training. We get to 64% accuracy: on average, the model predicts the next word
    in the Spanish sentence correctly 64% of the time. However, in practice, next-token
    accuracy isn’t a great metric for machine translation models, in particular because
    it makes the assumption that the correct target tokens from 0 to *N* are already
    known when predicting token *N* +1\. In reality, during inference, you’re generating
    the target sentence from scratch, and you can’t rely on previously generated tokens
    being 100% correct. If you work on a real-world machine translation system, you
    will likely use “BLEU scores” to evaluate your models—a metric that looks at entire
    generated sequences and that seems to correlate well with human perception of
    translation quality.'
  prefs: []
  type: TYPE_NORMAL
- en: At last, let’s use our model for inference. We’ll pick a few sentences in the
    test set and check how our model translates them. We’ll start from the seed token,
    “[start]”, and feed it into the decoder model, together with the encoded English
    source sentence. We’ll retrieve a next-token prediction, and we’ll reinject it
    into the decoder repeatedly, sampling one new target token at each iteration,
    until we get to “[end]” or reach the maximum sentence length.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.31 Translating new sentences with our RNN encoder and decoder
  prefs: []
  type: TYPE_NORMAL
- en: spa_vocab <- get_vocabulary(target_vectorization)➊
  prefs: []
  type: TYPE_NORMAL
- en: max_decoded_sentence_length <- 20
  prefs: []
  type: TYPE_NORMAL
- en: decode_sequence <- function(input_sentence) {
  prefs: []
  type: TYPE_NORMAL
- en: tokenized_input_sentence <
  prefs: []
  type: TYPE_NORMAL
- en: source_vectorization(array(input_sentence, dim = c(1, 1)))
  prefs: []
  type: TYPE_NORMAL
- en: decoded_sentence <- "[start]"➋
  prefs: []
  type: TYPE_NORMAL
- en: for (i in seq(max_decoded_sentence_length)) {
  prefs: []
  type: TYPE_NORMAL
- en: tokenized_target_sentence <-
  prefs: []
  type: TYPE_NORMAL
- en: target_vectorization(array(decoded_sentence, dim = c(1, 1)))
  prefs: []
  type: TYPE_NORMAL
- en: next_token_predictions <- seq2seq_rnn %>%
  prefs: []
  type: TYPE_NORMAL
- en: predict(list(tokenized_input_sentence,➌
  prefs: []
  type: TYPE_NORMAL
- en: tokenized_target_sentence))
  prefs: []
  type: TYPE_NORMAL
- en: sampled_token_index <- which.max(next_token_predictions[1, i, ])
  prefs: []
  type: TYPE_NORMAL
- en: sampled_token <- spa_vocab[sampled_token_index]➍
  prefs: []
  type: TYPE_NORMAL
- en: decoded_sentence <- paste(decoded_sentence, sampled_token)
  prefs: []
  type: TYPE_NORMAL
- en: if (sampled_token == "[end]")➎
  prefs: []
  type: TYPE_NORMAL
- en: break
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: decoded_sentence
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: for (i in seq(20)) {
  prefs: []
  type: TYPE_NORMAL
- en: input_sentence <- sample(test_pairs$english, 1)
  prefs: []
  type: TYPE_NORMAL
- en: print(input_sentence)
  prefs: []
  type: TYPE_NORMAL
- en: print(decode_sequence(input_sentence))
  prefs: []
  type: TYPE_NORMAL
- en: print("-")
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '[1] "Does this dress look OK on me?"'
  prefs: []
  type: TYPE_NORMAL
- en: '[1] "[start] este vestido me parece bien [UNK] [end]"'
  prefs: []
  type: TYPE_NORMAL
- en: '[1] "-"'
  prefs: []
  type: TYPE_NORMAL
- en: …
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Prepare vocabulary to convert token index predictions to string tokens.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Seed token**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Sample the next token.**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Convert the next token prediction to a string, and append it to the generated
    sentence.**
  prefs: []
  type: TYPE_NORMAL
- en: '➎ **Exit condition: either hit max length or sample a stop token.**'
  prefs: []
  type: TYPE_NORMAL
- en: decode_sequence() is working nicely, though perhaps a little slower than we
    would like. One easy way to speed up eager code like this is to use tf_function(),
    which we first saw in chapter 7\. Let’s rewrite decode_sentence() to be compiled
    by tf_function(). This means that instead of using eager R functions like seq(),
    predict(), and which.max(), we will use TensorFlow equivalents, like tf$range(),
    calling model() directly, and tf$argmax().
  prefs: []
  type: TYPE_NORMAL
- en: 'Because tf$range() and tf$argmax() return a 0-based value, we’ll set a function
    local option: option(tensorflow.extract.style = “python”). This changes the behavior
    of [ for tensors to be 0-based as well.'
  prefs: []
  type: TYPE_NORMAL
- en: tf_decode_sequence <- tf_function(function(input_sentence) {
  prefs: []
  type: TYPE_NORMAL
- en: withr::local_options(
  prefs: []
  type: TYPE_NORMAL
- en: tensorflow.extract.style = "python")➊
  prefs: []
  type: TYPE_NORMAL
- en: tokenized_input_sentence <- input_sentence %>%
  prefs: []
  type: TYPE_NORMAL
- en: as_tensor(shape = c(1, 1)) %>%
  prefs: []
  type: TYPE_NORMAL
- en: source_vectorization()
  prefs: []
  type: TYPE_NORMAL
- en: spa_vocab <- as_tensor(spa_vocab)
  prefs: []
  type: TYPE_NORMAL
- en: decoded_sentence <- as_tensor("[start]", shape = c(1, 1))
  prefs: []
  type: TYPE_NORMAL
- en: for (i in tf$range(as.integer(max_decoded_sentence_length))) {
  prefs: []
  type: TYPE_NORMAL
- en: tokenized_target_sentence <- decoded_sentence %>%
  prefs: []
  type: TYPE_NORMAL
- en: target_vectorization()
  prefs: []
  type: TYPE_NORMAL
- en: next_token_predictions <-
  prefs: []
  type: TYPE_NORMAL
- en: seq2seq_rnn(list(tokenized_input_sentence,
  prefs: []
  type: TYPE_NORMAL
- en: tokenized_target_sentence))
  prefs: []
  type: TYPE_NORMAL
- en: sampled_token_index <-
  prefs: []
  type: TYPE_NORMAL
- en: tf$argmax(next_token_predictions[0, i, ])➋
  prefs: []
  type: TYPE_NORMAL
- en: sampled_token <- spa_vocab[sampled_token_index]➌
  prefs: []
  type: TYPE_NORMAL
- en: decoded_sentence <-
  prefs: []
  type: TYPE_NORMAL
- en: tf$strings$join(c(decoded_sentence, sampled_token),
  prefs: []
  type: TYPE_NORMAL
- en: separator = " ")
  prefs: []
  type: TYPE_NORMAL
- en: if (sampled_token == "[end]")
  prefs: []
  type: TYPE_NORMAL
- en: break
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: decoded_sentence
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  prefs: []
  type: TYPE_NORMAL
- en: for (i in seq(20)) {
  prefs: []
  type: TYPE_NORMAL
- en: input_sentence <- sample(test_pairs$english, 1)
  prefs: []
  type: TYPE_NORMAL
- en: cat(input_sentence, "\n")
  prefs: []
  type: TYPE_NORMAL
- en: cat(input_sentence %>% as_tensor() %>%➍
  prefs: []
  type: TYPE_NORMAL
- en: tf_decode_sequence() %>% as.character(), "\n")
  prefs: []
  type: TYPE_NORMAL
- en: cat("-\n")
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Now all Tensor subsetting with [ will be 0-based until this function exits.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **i from tf$range() is 0-based.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **tf$argmax() returns a 0-based index.**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Convert to a tensor before calling tf_decode_sequence(), then convert output
    back to an R character.**
  prefs: []
  type: TYPE_NORMAL
- en: Our tf_decode_sentence() is about 10× faster than the eager version. Not bad!
  prefs: []
  type: TYPE_NORMAL
- en: Note that this inference setup, although very simple, is rather inefficient,
    because we reprocess the entire source sentence and the entire generated target
    sentence every time we sample a new word. In a practical application, you’d factor
    the encoder and the decoder as two separate models, and your decoder would run
    only a single step at each token-sampling iteration, reusing its previous internal
    state.
  prefs: []
  type: TYPE_NORMAL
- en: Here are our translation results. Our model works decently well for a toy model,
    though it still makes many basic mistakes.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.32 Some sample results from the recurrent translation model
  prefs: []
  type: TYPE_NORMAL
- en: Who is in this room?
  prefs: []
  type: TYPE_NORMAL
- en: '[start] quién está en esta habitación [end]'
  prefs: []
  type: TYPE_NORMAL
- en: '-'
  prefs: []
  type: TYPE_NORMAL
- en: That doesn’t sound too dangerous.
  prefs: []
  type: TYPE_NORMAL
- en: '[start] eso no es muy difícil [end]'
  prefs: []
  type: TYPE_NORMAL
- en: '-'
  prefs: []
  type: TYPE_NORMAL
- en: No one will stop me.
  prefs: []
  type: TYPE_NORMAL
- en: '[start] nadie me va a hacer [end]'
  prefs: []
  type: TYPE_NORMAL
- en: '-'
  prefs: []
  type: TYPE_NORMAL
- en: Tom is friendly.
  prefs: []
  type: TYPE_NORMAL
- en: '[start] tom es un buen [UNK] [end]'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many ways this toy model could be improved: we could use a deep stack
    of recurrent layers for both the encoder and the decoder (note that for the decoder,
    this makes state management a bit more involved). We could use an LSTM instead
    of a GRU. And so on. Beyond such tweaks, however, the RNN approach to sequence-to-sequence
    learning has a few fundamental limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: The source sequence representation has to be held entirely in the encoder state
    vector(s), which puts significant limitations on the size and complexity of the
    sentences you can translate. It’s a bit as if a human were translating a sentence
    entirely from memory, without looking twice at the source sentence while producing
    the translation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RNNs have trouble dealing with very long sequences, because they tend to progressively
    forget about the past—by the time you’ve reached the 100th token in either sequence,
    little information remains about the start of the sequence. That means RNN-based
    models can’t hold onto long-term context, which can be essential for translating
    long documents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These limitations are what has led the machine learning community to embrace
    the Transformer architecture for sequence-to-sequence problems. Let’s take a look.
  prefs: []
  type: TYPE_NORMAL
- en: 11.5.3 Sequence-to-sequence learning with Transformer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sequence-to-sequence learning is the task where Transformer really shines. Neural
    attention enables Transformer models to successfully process sequences that are
    considerably longer and more complex than those RNNs can handle.
  prefs: []
  type: TYPE_NORMAL
- en: As a human translating English to Spanish, you’re not going to read the English
    sentence one word at a time, keep its meaning in memory, and then generate the
    Spanish sentence one word at a time. That may work for a five-word sentence, but
    it’s unlikely to work for an entire paragraph. Instead, you’ll probably want to
    go back and forth between the source sentence and your translation in progress
    and pay attention to different words in the source as you’re writing down different
    parts of your translation.
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s exactly what you can achieve with neural attention and Transformers.
    You’re already familiar with the Transformer encoder, which uses self-attention
    to produce context-aware representations of each token in an input sequence. In
    a sequence-to-sequence Transformer, the Transformer encoder would naturally play
    the role of the encoder, which reads the source sequence and produces an encoded
    representation of it. Unlike our previous RNN encoder, though, the Transformer
    encoder keeps the encoded representation in a sequence format: it’s a sequence
    of context-aware embedding vectors.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second half of the model is the *Transformer decoder*. Just like the RNN
    decoder, it reads tokens 1…*N* in the target sequence and tries to predict token
    *N* + 1\. Crucially, while doing this, it uses neural attention to identify which
    tokens in the encoded source sentence are most closely related to the target token
    it’s currently trying to predict— perhaps not unlike what a human translator would
    do. Recall the query-key-value model: in a Transformer decoder, the target sequence
    serves as an attention “query” that is used to pay closer attention to different
    parts of the source sequence (the source sequence plays the roles of both keys
    and values).'
  prefs: []
  type: TYPE_NORMAL
- en: THE TRANSFORMER DECODER
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Figure 11.14](#fig11-14) shows the full sequence-to-sequence Transformer.
    Look at the decoder internals: you’ll recognize that it looks very similar to
    the Transformer encoder, except that an extra attention block is inserted between
    the self-attention block applied to the target sequence and the dense layers of
    the exit block.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0393-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 11.14 The TransformerDecoder is similar to the TransformerEncoder,
    except it features an additional attention block where the keys and values are
    the source sequence encoded by the TransformerEncoder. Together, the encoder and
    the decoder form an end-to-end Transformer.**'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s implement it. Like for the TransformerEncoder, we’ll be creating a new
    layer class. Before we focus on the call(), method, where the action happens,
    let’s start by defining the class constructor, containing the layers we’re going
    to need.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.33 The TransformerDecoder
  prefs: []
  type: TYPE_NORMAL
- en: layer_transformer_decoder <- new_layer_class(
  prefs: []
  type: TYPE_NORMAL
- en: classname = "TransformerDecoder",
  prefs: []
  type: TYPE_NORMAL
- en: initialize = function(embed_dim, dense_dim, num_heads, …) {
  prefs: []
  type: TYPE_NORMAL
- en: super$initialize(…)
  prefs: []
  type: TYPE_NORMAL
- en: self$embed_dim <- embed_dim
  prefs: []
  type: TYPE_NORMAL
- en: self$dense_dim <- dense_dim
  prefs: []
  type: TYPE_NORMAL
- en: self$num_heads <- num_heads
  prefs: []
  type: TYPE_NORMAL
- en: self$attention_1 <- layer_multi_head_attention(num_heads = num_heads,
  prefs: []
  type: TYPE_NORMAL
- en: key_dim = embed_dim)
  prefs: []
  type: TYPE_NORMAL
- en: self$attention_2 <- layer_multi_head_attention(num_heads = num_heads,
  prefs: []
  type: TYPE_NORMAL
- en: key_dim = embed_dim)
  prefs: []
  type: TYPE_NORMAL
- en: self$dense_proj <- keras_model_sequential() %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(dense_dim, activation = "relu") %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(embed_dim)
  prefs: []
  type: TYPE_NORMAL
- en: self$layernorm_1 <- layer_layer_normalization()
  prefs: []
  type: TYPE_NORMAL
- en: self$layernorm_2 <- layer_layer_normalization()
  prefs: []
  type: TYPE_NORMAL
- en: self$layernorm_3 <- layer_layer_normalization()
  prefs: []
  type: TYPE_NORMAL
- en: self$supports_masking <- TRUE➊
  prefs: []
  type: TYPE_NORMAL
- en: '},'
  prefs: []
  type: TYPE_NORMAL
- en: get_config = function() {
  prefs: []
  type: TYPE_NORMAL
- en: config <- super$get_config()
  prefs: []
  type: TYPE_NORMAL
- en: for (name in c("embed_dim", "num_heads", "dense_dim"))
  prefs: []
  type: TYPE_NORMAL
- en: config[[name]] <- self[[name]]
  prefs: []
  type: TYPE_NORMAL
- en: config
  prefs: []
  type: TYPE_NORMAL
- en: '},'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **This attribute ensures that the layer will propagate its input mask to its
    outputs; masking in Keras is explicitly opt-in. If you pass a mask to a layer
    that doesn't implement compute_mask() and that doesn't expose this supports_masking
    attribute, that's an error.**
  prefs: []
  type: TYPE_NORMAL
- en: 'The call() method is almost a straightforward rendering of the connectivity
    diagram from [figure 11.14](#fig11-14). But there’s an additional detail we need
    to take into account: *causal padding*. Causal padding is absolutely critical
    to successfully training a sequence-to-sequence Transformer. Unlike an RNN, which
    looks at its input one step at a time, and thus will only have access to steps
    1…*N* to generate output step *N* (which is token *N*+1 in the target sequence),
    the TransformerDecoder is order agnostic: it looks at the entire target sequence
    at once. If it were allowed to use its entire input, it would simply learn to
    copy input step *N*+1 to location *N* in the output. The model would thus achieve
    perfect training accuracy, but of course, when running inference, it would be
    completely useless, because input steps beyond *N* aren’t available.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The fix is simple: we’ll mask the upper half of the pairwise attention matrix
    to prevent the model from paying any attention to information from the future—only
    information from tokens 1…*N* in the target sequence should be used when generating
    target token *N*+1\. To do this, we’ll add a get_causal_attention_mask(inputs)
    method to our TransformerDecoder to retrieve an attention mask that we can pass
    to our MultiHeadAttention layers.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.34 TransformerDecoder method that generates a causal mask**'
  prefs: []
  type: TYPE_NORMAL
- en: get_causal_attention_mask = function(inputs) {
  prefs: []
  type: TYPE_NORMAL
- en: c(batch_size, sequence_length, .) %<-%➊
  prefs: []
  type: TYPE_NORMAL
- en: tf$unstack(tf$shape(inputs))
  prefs: []
  type: TYPE_NORMAL
- en: x <- tf$range(sequence_length)➋
  prefs: []
  type: TYPE_NORMAL
- en: i <- x[, tf$newaxis]
  prefs: []
  type: TYPE_NORMAL
- en: j <- x[tf$newaxis, ]
  prefs: []
  type: TYPE_NORMAL
- en: mask <- tf$cast(i >= j, "int32")➌ ➍
  prefs: []
  type: TYPE_NORMAL
- en: tf$tile(mask[tf$newaxis, , ],
  prefs: []
  type: TYPE_NORMAL
- en: tf$stack(c(batch_size, 1L, 1L)))➎
  prefs: []
  type: TYPE_NORMAL
- en: '},'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **The third axis is encoding_length; we do not use it here.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Integer sequence [0, 1, 2, … sequence_length-1]**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Use Tensor broadcasting in our >= operation. Cast dtype bool to int32.**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **mask is a square matrix with shape (sequence_length, sequence_length), with
    1s in the lower triangle and 0s everywhere else. For example, if sequence_length
    is 4, mask is:**
  prefs: []
  type: TYPE_NORMAL
- en: tf.Tensor([[1 0 0 0]
  prefs: []
  type: TYPE_NORMAL
- en: '[1 1 0 0]'
  prefs: []
  type: TYPE_NORMAL
- en: '[1 1 1 0]'
  prefs: []
  type: TYPE_NORMAL
- en: '[1 1 1 1]], shape=(4, 4), dtype=int32)'
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **Add a batch dimension to mask, then tile (rep()) along the batch dim for
    batch_size times. The returned tensor has shape (batch_size, sequence_length,
    sequence_length).**
  prefs: []
  type: TYPE_NORMAL
- en: Now we can write down the full call() method implementing the forward pass of
    the decoder.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.35 The forward pass of the TransformerDecoder
  prefs: []
  type: TYPE_NORMAL
- en: call = function(inputs, encoder_outputs, mask = NULL) {
  prefs: []
  type: TYPE_NORMAL
- en: causal_mask <- self$get_causal_attention_mask(inputs)➊
  prefs: []
  type: TYPE_NORMAL
- en: if (is.null(mask))➋
  prefs: []
  type: TYPE_NORMAL
- en: mask <- causal_mask
  prefs: []
  type: TYPE_NORMAL
- en: else
  prefs: []
  type: TYPE_NORMAL
- en: mask %<>% { tf$minimum(tf$cast(.[, tf$newaxis, ], "int32"),
  prefs: []
  type: TYPE_NORMAL
- en: causal_mask) }➌
  prefs: []
  type: TYPE_NORMAL
- en: inputs %>%
  prefs: []
  type: TYPE_NORMAL
- en: '{ self$attention_1(query = ., value = ., key = .,'
  prefs: []
  type: TYPE_NORMAL
- en: attention_mask = causal_mask) + . } %>%➍
  prefs: []
  type: TYPE_NORMAL
- en: self$layernorm_1() %>%➎
  prefs: []
  type: TYPE_NORMAL
- en: '{ self$attention_2(query = .,'
  prefs: []
  type: TYPE_NORMAL
- en: value = encoder_outputs,➏
  prefs: []
  type: TYPE_NORMAL
- en: key = encoder_outputs,➏
  prefs: []
  type: TYPE_NORMAL
- en: attention_mask = mask) + . } %>%➐
  prefs: []
  type: TYPE_NORMAL
- en: self$layernorm_2() %>%➑
  prefs: []
  type: TYPE_NORMAL
- en: '{ self$dense_proj(.) + . } %>%➒'
  prefs: []
  type: TYPE_NORMAL
- en: self$layernorm_3()
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Retrieve the causal mask.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **The mask supplied in the call is the padding mask (it describes padding
    locations in the target sequence).**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Combine the padding mask with the causal mask.**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Pass the causal mask to the first attention layer, which performs self-attention
    over the target sequence.**
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **The output of attention_1() with residual added is passed to layernorm_1().**
  prefs: []
  type: TYPE_NORMAL
- en: ➏ **Use encoder_outputs supplied in the call as the value and key to attention_2().**
  prefs: []
  type: TYPE_NORMAL
- en: ➐ **Pass the combined mask to the second attention layer, which relates the
    source sequence to the target sequence.**
  prefs: []
  type: TYPE_NORMAL
- en: ➑ **The output of attention_2() with residual added is passed to layernorm_2().**
  prefs: []
  type: TYPE_NORMAL
- en: ➒ **The output of dense_proj() with residual is added and passed to layernorm_3().**
  prefs: []
  type: TYPE_NORMAL
- en: 'PUTTING IT ALL TOGETHER: A TRANSFORMER FOR MACHINE TRANSLATION'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The end-to-end Transformer is the model we’ll be training. It maps the source
    sequence and the target sequence to the target sequence one step in the future.
    It straightforwardly combines the pieces we’ve built so far: PositionalEmbedding
    layers, the TransformerEncoder, and the TransformerDecoder. Note that both the
    Transformer-Encoder and the TransformerDecoder are shape invariant, so you could
    be stacking many of them to create a more powerful encoder or decoder. In our
    example, we’ll stick to a single instance of each.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.36 End-to-end Transformer**'
  prefs: []
  type: TYPE_NORMAL
- en: embed_dim <- 256
  prefs: []
  type: TYPE_NORMAL
- en: dense_dim <- 2048
  prefs: []
  type: TYPE_NORMAL
- en: num_heads <- 8
  prefs: []
  type: TYPE_NORMAL
- en: encoder_inputs <- layer_input(shape(NA), dtype = "int64", name = "english")
  prefs: []
  type: TYPE_NORMAL
- en: encoder_outputs <- encoder_inputs %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_positional_embedding(sequence_length, vocab_size, embed_dim) %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_transformer_encoder(embed_dim, dense_dim, num_heads)➊
  prefs: []
  type: TYPE_NORMAL
- en: transformer_decoder <-
  prefs: []
  type: TYPE_NORMAL
- en: layer_transformer_decoder(NULL, embed_dim, dense_dim, num_heads)➋
  prefs: []
  type: TYPE_NORMAL
- en: decoder_inputs <- layer_input(shape(NA), dtype = "int64", name = "spanish")
  prefs: []
  type: TYPE_NORMAL
- en: decoder_outputs <- decoder_inputs %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_positional_embedding(sequence_length, vocab_size, embed_dim) %>%
  prefs: []
  type: TYPE_NORMAL
- en: transformer_decoder(., encoder_outputs) %>%➌
  prefs: []
  type: TYPE_NORMAL
- en: layer_dropout(0.5) %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(vocab_size, activation = "softmax")➍
  prefs: []
  type: TYPE_NORMAL
- en: transformer <- keras_model(list(encoder_inputs, decoder_inputs),
  prefs: []
  type: TYPE_NORMAL
- en: decoder_outputs)
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Encode the source sentence.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Pass NULL for the first argument, so a layer instance is created and returned
    directly and not composed with anything yet.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Encode the target sentence and combine it with the encoded source sentence.**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Predict a word for each output position.**
  prefs: []
  type: TYPE_NORMAL
- en: We’re now ready to train our model—we get to 67% accuracy, a good deal above
    the GRU-based model.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.37 Training the sequence-to-sequence Transformer**'
  prefs: []
  type: TYPE_NORMAL
- en: transformer %>%
  prefs: []
  type: TYPE_NORMAL
- en: compile(optimizer = "rmsprop",
  prefs: []
  type: TYPE_NORMAL
- en: loss = "sparse_categorical_crossentropy",
  prefs: []
  type: TYPE_NORMAL
- en: metrics = "accuracy")
  prefs: []
  type: TYPE_NORMAL
- en: transformer %>%
  prefs: []
  type: TYPE_NORMAL
- en: fit(train_ds, epochs = 30, validation_data = val_ds)
  prefs: []
  type: TYPE_NORMAL
- en: Finally, let’s try using our model to translate never-seen-before English sentences
    from the test set. The setup is identical to what we used for the sequence-to-sequence
    RNN model; all that’s changed is we’re replaced seq2seq_rnn with transformer,
    and we drop the extra token that we configured our target_vectorization() layer
    to add.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.38 Translating new sentences with our Transformer model**'
  prefs: []
  type: TYPE_NORMAL
- en: tf_decode_sequence <- tf_function(function(input_sentence) {
  prefs: []
  type: TYPE_NORMAL
- en: withr::local_options(tensorflow.extract.style = "python")
  prefs: []
  type: TYPE_NORMAL
- en: tokenized_input_sentence <- input_sentence %>%
  prefs: []
  type: TYPE_NORMAL
- en: as_tensor(shape = c(1, 1)) %>%
  prefs: []
  type: TYPE_NORMAL
- en: source_vectorization()
  prefs: []
  type: TYPE_NORMAL
- en: spa_vocab <- as_tensor(spa_vocab)
  prefs: []
  type: TYPE_NORMAL
- en: decoded_sentence <- as_tensor("[start]", shape = c(1, 1))
  prefs: []
  type: TYPE_NORMAL
- en: for (i in tf$range(as.integer(max_decoded_sentence_length))) {
  prefs: []
  type: TYPE_NORMAL
- en: tokenized_target_sentence <-
  prefs: []
  type: TYPE_NORMAL
- en: target_vectorization(decoded_sentence)[, NA:-1]➊
  prefs: []
  type: TYPE_NORMAL
- en: next_token_predictions <-➋
  prefs: []
  type: TYPE_NORMAL
- en: transformer(list(tokenized_input_sentence,
  prefs: []
  type: TYPE_NORMAL
- en: tokenized_target_sentence))
  prefs: []
  type: TYPE_NORMAL
- en: sampled_token_index <- tf$argmax(next_token_predictions[0, i, ])
  prefs: []
  type: TYPE_NORMAL
- en: sampled_token <- spa_vocab[sampled_token_index]➌
  prefs: []
  type: TYPE_NORMAL
- en: decoded_sentence <-
  prefs: []
  type: TYPE_NORMAL
- en: tf$strings$join(c(decoded_sentence, sampled_token),
  prefs: []
  type: TYPE_NORMAL
- en: separator = " ")
  prefs: []
  type: TYPE_NORMAL
- en: if (sampled_token == "[end]")➍
  prefs: []
  type: TYPE_NORMAL
- en: break
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: decoded_sentence
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  prefs: []
  type: TYPE_NORMAL
- en: for (i in sample.int(nrow(test_pairs), 20)) {
  prefs: []
  type: TYPE_NORMAL
- en: c(input_sentence, correct_translation) %<-% test_pairs[i, ]
  prefs: []
  type: TYPE_NORMAL
- en: cat(input_sentence, "\n")
  prefs: []
  type: TYPE_NORMAL
- en: cat(input_sentence %>% as_tensor() %>%
  prefs: []
  type: TYPE_NORMAL
- en: tf_decode_sequence() %>% as.character(), "\n")
  prefs: []
  type: TYPE_NORMAL
- en: cat("-\n")
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Drop the last token; "python" style is not inclusive of a slice end.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Sample the next token.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Convert the next token prediction to a string, and append it to the generated
    sentence.**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Exit condition**
  prefs: []
  type: TYPE_NORMAL
- en: Subjectively, the Transformer seems to perform significantly better than the
    GRU-based translation model. It’s still a toy model, but it’s a better toy model.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.39 Some sample results from the Transformer translation model
  prefs: []
  type: TYPE_NORMAL
- en: This is a song I learned when I was a kid.
  prefs: []
  type: TYPE_NORMAL
- en: '[start] esta es una canción que aprendí cuando'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common01.jpg) era chico [end]➊'
  prefs: []
  type: TYPE_NORMAL
- en: '-'
  prefs: []
  type: TYPE_NORMAL
- en: She can play the piano.
  prefs: []
  type: TYPE_NORMAL
- en: '[start] ella puede tocar piano [end]'
  prefs: []
  type: TYPE_NORMAL
- en: '-'
  prefs: []
  type: TYPE_NORMAL
- en: I’m not who you think I am.
  prefs: []
  type: TYPE_NORMAL
- en: '[start] no soy la persona que tú creo que soy [end]'
  prefs: []
  type: TYPE_NORMAL
- en: '-'
  prefs: []
  type: TYPE_NORMAL
- en: It may have rained a little last night.
  prefs: []
  type: TYPE_NORMAL
- en: '[start] puede que llueve un poco el pasado [end]'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Although the source sentence wasn't gendered, this translation assumes a
    male speaker. Keep in mind that translation models will often make unwarranted
    assumptions about their input data, which leads to algorithmic bias. In the worst
    cases, a model might hallucinate memorized information that has nothing to do
    with the data it's currently processing.**
  prefs: []
  type: TYPE_NORMAL
- en: That concludes this chapter on natural language processing—you just went from
    the very basics to a fully fledged Transformer that can translate from English
    to Spanish. Teaching machines to make sense of language is the latest superpower
    you can add to your collection.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two kinds of NLP models: *bag-of-words models* that process sets
    of words or *N*-grams without taking into account their order, and *sequence models*
    that process word order. A bag-of-words model is made of Dense layers, whereas
    a sequence model could be an RNN, a 1D convnet, or a Transformer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When it comes to text classification, the ratio between the number of samples
    in your training data and the mean number of words per sample can help you determine
    whether you should use a bag-of-words model or a sequence model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Word embeddings* are vector spaces where semantic relationships between words
    are modeled as distance relationships between vectors that represent those words.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Sequence-to-sequence learning* is a generic, powerful learning framework that
    can be applied to solve many NLP problems, including machine translation. A sequence-to-sequence
    model is made of an encoder, which processes a source sequence, and a decoder,
    which tries to predict future tokens in target sequence by looking at past tokens,
    with the help of the encoder-processed source sequence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Neural attention* is a way to create context-aware word representations. It’s
    the basis for the Transformer architecture.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Transformer* architecture, which consists of a TransformerEncoder and a
    TransformerDecoder, yields excellent results on sequence-to-sequence tasks. The
    first half, the TransformerEncoder, can also be used for text classification or
    any sort of single-input NLP task.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ^([1](#endnote1)) Yoshua Bengio et al., “A Neural Probabilistic Language Model,”
    *Journal of Machine Learning Research* (2003).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ^([2](#endnote2)) Ashish Vaswani et al., “Attention Is All You Need” (2017),
    [https://arxiv.org/abs/1706.03762.](https://www.arxiv.org/abs/1706.03762)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
