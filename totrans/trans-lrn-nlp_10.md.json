["```py\n!pip install tensor2tensor\n!git clone https:/ /github.com/jessevig/bertviz.git\n```", "```py\n!pip install transformers\n```", "```py\nfrom transformers import BertTokenizer, BertModel                                         ❶\nmodel = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)            ❷\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)        ❸\n```", "```py\nsentence = \"He didnt want to talk about cells on the cell phone because he considered it boring\"\ninputs = tokenizer.encode(sentence, return_tensors='tf', add_special_tokens=True)                            ❶\nprint(inputs)\n```", "```py\ntf.Tensor(\n[[  101  2002  2134  2102  2215  2000  2831  2055  4442  2006  1996  3526\n   3042  2138  2002  2641  2009 11771   102]], shape=(1, 19), dtype=int32)\n```", "```py\ntokens = tokenizer.convert_ids_to_tokens(list(inputs[0]))     ❶\nprint(tokens)\n```", "```py\n['[CLS]', 'he', 'didn', '##t', 'want', 'to', 'talk', 'about', 'cells', 'on', 'the', 'cell', 'phone', 'because', 'he', 'considered', 'it', 'boring', '[SEP]']\n```", "```py\nfrom bertviz.bertviz import head_view                           ❶\n\ndef show_head_view(model, tokenizer, sentence):                 ❷\n    input_ids = tokenizer.encode(sentence, return_tensors='pt', add_special_tokens=True)                                   ❸\n    attention = model(input_ids)[-1]                            ❹\n    tokens = tokenizer.convert_ids_to_tokens(list(input_ids[0]))    \n    head_view(attention, tokens)                                ❺\n\nshow_head_view(model, tokenizer, sentence)                      ❻\n```", "```py\nfrom transformers import MarianMTModel, MarianTokenizer\n\nmodel = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-tw\")\ntokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-tw\")\n```", "```py\ntext = \"My name is Paul\"                                      ❶\ninputs = tokenizer.encode(text, return_tensors=\"pt\")          ❷\noutputs = model.generate(inputs)                              ❸\ndecoded_output = [tokenizer.convert_ids_to_tokens(int(outputs[0][i])) for i in range(len(outputs[0]))]                               ❹\nprint(\"Translation:\")                                         ❺\nprint(decoded_output)\n```", "```py\nTranslation:\n['<pad>', '▁Me', '▁din', '▁de', '▁Paul']\n```", "```py\nfrom transformers import pipeline\ngpt = pipeline('text-generation',model='gpt2')\n```", "```py\ngpt(\"Somewhere over the rainbow\", max_length=100)\n```", "```py\n[{'generated_text': \"Somewhere over the rainbow people live! I wonder how they get to know each other... They just have a wonderful community out there - but when they see each other as two of the best in school they never even realize them, just love, family, friends, and friends. I'm really proud of their talent and dedication to life. I've seen a lot of people that were raised by their mother and grandma in the Midwest and didn't understand there was such an opportunity and I truly cannot\"}]\n```", "```py\ngpt(\"Transfer learning is a field of study\", max_length=100)\n```", "```py\n[{'generated_text': \"Transfer learning is a field of study that has been around for centuries, and one that requires a thorough grounding in mathematics in order to understand the complexities of these systems. If you go to the library for your high school physics course, you know you're on the right track. The only problem with this position is that people don't ask questions. The only thing they really do ask is: how do we figure out how to apply these processes to the rest of physics and other sciences?\\n\\nIn\"}]\n```", "```py\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer      ❶\nimport torch                                                 ❷\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\nmodel = GPT2LMHeadModel.from_pretrained(\"microsoft/DialoGPT-medium\")\n```", "```py\nconversation_length = 5                                                                                           ❶\nfor step in range(conversation_length):\n    new_user_inputs_ids = tokenizer.encode(input(\"User: \") + tokenizer.eos_token, return_tensors='pt')            ❷\n    bot_input_ids = torch.cat([chat_history_ids, new_user_inputs_ids], dim=1)                                     ❸\n     if step > 0 else new_user_inputs_ids   \n    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)        ❹\n    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))               ❺\n```", "```py\nUser: Hi there.\nDialogGPT: Hi there.\nUser: How are you today?\nDialogGPT: I'm good, how are you?\nUser: Good! How much money do you have?\nDialogGPT: I have about 100k.\nUser: What will you spend it on?\nDialogGPT: I'm not sure, I'm not sure what I want to spend it on.\nUser: Make a decision, life is short.\nDialogGPT: I'm going to go with a lot of things.\n```", "```py\nUser: Hi robot.\nDialogGPT: Hello, human.\nUser: Huh?\nDialogGPT: I'm a bot.\nUser: OK, what is your name?\nDialogGPT: Robot. I'm a robot.\nUser: All right then.\nDialogGPT: Robot. I'm a robot.\nUser: Say something else.\nDialogGPT: Robot. I'm a robot.\n```"]