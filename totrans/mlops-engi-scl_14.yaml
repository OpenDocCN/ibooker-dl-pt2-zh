- en: 11 Hyperparameter optimization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11 超参数优化
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖内容
- en: Understanding machine learning with hyperparameter optimization
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过超参数优化理解机器学习
- en: Introducing hyperparameter optimization for the DC taxi model
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入超参数优化到DC taxi模型
- en: Visualizing the outcomes of hyperparameter optimization trials
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化超参数优化实验结果
- en: In chapter 10, you integrated your DC taxi model with the PyTorch Lightning
    framework, factoring out boilerplate engineering code and paving the way to hyperparameter
    optimization support. In this chapter, you are going to adopt Optuna, a hyperparameter
    optimization framework, to progress beyond a trial-and-error approach to selection
    of your machine learning hyperparameter values. You will train a collection of
    DC taxi model instances based on the hyperparameter values selected using Optuna’s
    Tree-Structured Parzen Estimator(TPE) that fits a Gaussian mixture model (GMM)
    to the hyperparameters in your machine learning system. The performance of these
    model instances is compared using various Optuna visualization plots.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在第10章，您将PyTorch Lightning框架与您的DC taxi模型集成，提取出模板化工程代码，并为其提供超参数优化支持的途径。在本章中，您将采用一种超参数优化框架Optuna，以进一步超越试错法来选择您的机器学习超参数值。您将训练一系列基于Optuna的Tree-Structured
    Parzen Estimator(TPE)选择的超参数值的DC taxi模型实例，该模型适配了您机器学习系统中的超参数的高斯混合模型（GMM）。使用各种Optuna可视化图比较这些模型实例的性能。
- en: 11.1 Hyperparameter optimization with Optuna
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.1 使用Optuna进行超参数优化
- en: This section teaches about Optuna for hyperparameter optimization (HPO) and
    how to add support for HPO to the DC taxi fare estimation model.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本小节介绍了Optuna用于超参数优化（HPO）以及如何为DC taxi车费估算模型添加HPO支持。
- en: Optuna is one of many open source HPO frameworks available for PyTorch. Like
    other HPO frameworks, Optuna includes a collection of gradient-free[¹](#pgfId-1011837)
    optimization algorithms that range from random search, and Bayesian optimization,
    to TPE. Optuna uses the concept of a *trial* to describe an instance of a process
    to compute the value of a loss function, for example an instance of an experiment
    on the DcTaxiModel model to compute its test loss based on a set of hyperparameter
    values.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Optuna是适用于PyTorch的众多开源HPO框架之一。与其他HPO框架一样，Optuna包括了一系列无梯度的优化算法，从随机搜索、贝叶斯优化到TPE不等。Optuna使用*trial*的概念来描述计算损失函数值的过程的实例，例如基于一组超参数值计算DcTaxiModel模型的测试损失的实验实例。
- en: In Optuna, a trial must produce a value for the loss function that you seek
    to minimize (or maximize). The implementation of the process to compute the loss
    value is usually captured in an objective function. Note that the implementation
    is intentionally incomplete ❸ to explain just the parts relevant to understanding
    the Optuna API.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在Optuna中，一个试验必须生成一个你希望最小化（或最大化）的损失函数的值。计算损失值的过程的实现通常在一个目标函数中进行捕捉。请注意，为了仅解释与理解Optuna
    API相关的部分，这个实现是有意不完整的 ❸。
- en: Listing 11.1 A starter for DC taxi HPO with objective function
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 代码清单11.1 DC taxi HPO的起始点与目标函数
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ An objective function is a standard interface to the Optuna trial.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 目标函数是Optuna试验的标准接口。
- en: ❷ suggest_int returns integers chosen by Optuna to optimize the objective function.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ suggest_int返回Optuna选择的整数，以优化目标函数。
- en: ❸ hparams’s implementation is completed later in this chapter.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ hparams的实现在本章后面完成。
- en: ❹ Optuna suggests hyperparameters for each trial to reduce the value of train_val_rmse.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ Optuna根据每次实验的结果建议调整超参数，以减小train_val_rmse的值。
- en: 'Notice that, in the hparams dictionary, there is a single int hyperparameter
    value requested from Optuna for the seed. The suggest_int method is one of several
    methods available from the Optuna trial API to obtain a value for a hyperparameter.
    (Other methods available from the trial interface are described here: [http://mng.bz/v4B7](http://mng.bz/v4B7).)
    In the example, the suggest_int(''seed'', 0, pt.iinfo(pt.int32).max - 1) method
    call specifies that Optuna should recommend values for the pseudorandom number
    seed generator from 0 up to and excluding the maximum positive 32-bit integer.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在hparams字典中，有一个从Optuna请求的整数超参数值seed。suggest_int方法是Optuna trial API中的几种方法之一，用于获取超参数的值。（试验接口中的其他可用方法在此处描述：[http://mng.bz/v4B7](http://mng.bz/v4B7)。）在本例中，suggest_int('seed',
    0, pt.iinfo(pt.int32).max - 1)方法调用指定Optuna应该为伪随机数生成器推荐从0到32位整数最大正值之前的值。
- en: Recall that the implementation of the DcTaxiModel depends on additional hyperparameter
    values, including, optimizer, bins, lr (the learning rate), max_batches, and potentially
    more. To enable support for these hyperparameters in the implementation of the
    DcTaxiModel, you need to expand the hparams dictionary with Optuna specification
    for the other hyperparameter values. Since the strategies for sampling these hyperparameters
    are more complex than suggest_int, the next section explains some foundational
    concepts.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，DcTaxiModel的实现取决于其他超参数值，包括optimizer、bins、lr（学习率）、max_batches等。为了在DcTaxiModel的实现中支持这些超参数，需要扩展hparams字典以符合其他超参数值的Optuna规范。由于这些超参数的取样策略比suggest_int更复杂，下一节将解释一些基本概念。
- en: 11.1.1 Understanding loguniform hyperparameters
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1.1 理解loguniform超参数
- en: This section puts forward the rationale for using log-uniform hyperameters during
    training and proposes the hyperparameters that should be set as log-uniform for
    the DC taxi model.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提出了在训练过程中使用对数均匀超参数的合理性，并提出了应将哪些超参数设置为DC出租车模型的对数均匀形式。
- en: 'For many continuous valued hyperparameters such as the learning rate, it is
    convenient to use the suggest_loguniform method of the Optuna trial API, which
    is invoked using the upper and lower boundary values for the hyperparameter. Since
    in the Optuna trial API there are several options for continuous hyperparameter
    values, it is useful to clarify why the learning rate should use Optuna suggest_loguniform
    as opposed to suggest_uniform. In general, loguniform is preferred to search over
    a range where the upper bound is more than one order of magnitude larger than
    the lower bound. The rationale has to do with how the base-10 numeric system works
    and is illustrated in the following example:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多连续的超参数，比如学习率，使用Optuna试验API的suggest_loguniform方法非常方便，该方法通过超参数的上限和下限值来调用。由于在Optuna试验API中，有几个连续超参数值的选项，因此有必要澄清为什么学习率应该使用Optuna
    suggest_loguniform而不是suggest_uniform。一般来说，loguniform更适合于搜索上限比下限大一个数量级以上的范围。这种合理性与十进制数系统的工作原理有关，下面的例子进行了解释：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: which outputs
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果为
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Since x is assigned to contain 300 floating point values from 0 to 1,000 (three
    orders of magnitude), the print statement outputs the count of values that appears
    within a range for each of the orders of magnitude (i.e., 0 to 10, 10 to 100,
    and 100 to 1,000). In this example, there is approximately a 100-times difference
    in the number of values of x that fall in the 0 to 10 range versus the number
    of the values in the 100 to 1,000 range, or more precisely 3 versus 269.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 由于x包含了从0到1,000（三个数量级）的300个浮点值，因此print语句输出了在每个数量级范围内出现的值的计数（即，从0到10，从10到100，从100到1,000）。在这个例子中，在0到10的范围内的x值的数量与在100到1,000范围内的值的数量之间大约相差100倍，或者更确切地说是3与269。
- en: 'In general, when sampling with uniform likelihood from a linear range that
    is 10*N* larger, you can expect to obtain on average 10*N* times more samples
    from the larger range. Application of the log function eliminates this unintended
    skew due to the base-10 numeric system, as evidenced by the following example:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，从一个线性范围中以均匀概率取样，可以预期从较大范围中获得的样本平均要多10*N*倍。log函数的应用消除了由于十进制数系统导致的这种无意的偏差，下面的例子证明了这一点：
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: which outputs
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果为
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: illustrating a roughly equal distribution of the 300 samples across the entire
    range from 1 to 1,000 due to the use of the log-base 10 scale as opposed to the
    regular unit scale.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 由于使用了以对数10为底的比例尺，示例中展示了约300个样本在整个从1到1,000的范围内大致相等地分布。
- en: The concept of using the sampling hyperparameter values over log-scaled ranges
    applies to discrete integer values as much as to the continuous values. For example,
    the max_batches hyperparameter can be initialized using log = True in the Optuna
    Trial API function call suggest_int('max_batches', 40, 4000, log = True) since
    the lower and upper boundary values in the call span a range greater than an order
    of magnitude.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在对离散整数值进行对数尺度范围上的超参数值取样的概念，适用于连续值一样。例如，max_batches超参数可以使用Optuna Trial API函数调用suggest_int('max_batches',
    40, 4000, log = True)进行初始化，因为调用中的下限和上限值跨越了一个数量级以上的范围。
- en: 11.1.2 Using categorical and log-uniform hyperparameters
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1.2 使用分类和对数均匀超参数
- en: This section builds on what you learned about log-uniform hyperparameters to
    explain how the optimizer learning rate can be initialized using a hyperparameter
    sampled from a log-uniform scale by Optuna.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 本节基于你学到的关于对数均匀超参数的知识，解释了如何通过 Optuna 从对数均匀尺度中采样的超参数来初始化优化器学习率。
- en: 'The optimizer and the corresponding learning rate are some of the other hyperparameters
    you may want to include in the HPO trials. As the optimizer learning rate is best
    represented with a continuous value over a range that spans several orders of
    magnitude, it should be optimized in the trial using the suggest_loguniform method.
    This can be implemented as follows, for values of candidate learning rates in
    the range [0.001, 0.1):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器和相应的学习率是你可能想要包含在 HPO 试验中的其他超参数之一。由于优化器学习率最好用连续值表示，跨越几个数量级的范围，因此应该在试验中使用 `suggest_loguniform`
    方法进行优化。这可以如下实现，对于候选学习率在范围 `[0.001, 0.1)` 中的值：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Use an optimizer learning rate from the log-uniform range [0.001, 0.1).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用来自对数均匀范围 `[0.001, 0.1)` 的优化器学习率。
- en: 'Since the existing implementation of the configure_optimizers method of the
    DcTaxiModel already includes support for stochastic gradient descent and Adam,
    you can have Optuna suggest the choice of these values (SGD or Adam) in the implementation
    of the objective method. This requires the use of the suggest_categorical method
    of the trial object as shown:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 DcTaxiModel 的 `configure_optimizers` 方法已经包含了对随机梯度下降和 Adam 的支持，因此你可以让 Optuna
    在 `objective` 方法的实现中建议这些值的选择（SGD 或 Adam）。这需要在试验对象的 `suggest_categorical` 方法中使用，如下所示：
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Use Adam and SGD as optimizer options in each HPO trial.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在每个 HPO 试验中，使用 Adam 和 SGD 作为优化器选项。
- en: 'The arguments for the trial API are computed at program runtime, which means
    that you can use standard Python features for a more expressive specification
    of the hyperparameter values. For example, the batch_size hyperparameter can be
    specified using a list of integers, where the integers are generated dynamically,
    resolving to powers of two from 2^(16) up to and including 2^(21)—in other words,
    the values [65536, 131072, 262144, 524288, 1048576, 2097152]:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 试验 API 的参数是在程序运行时计算的，这意味着你可以使用标准的 Python 特性来更加表达超参数值的规范。例如，`batch_size` 超参数可以使用一个整数列表来指定，其中整数是动态生成的，解析为从
    2^(16) 到 2^(21) 的二次幂，换句话说，值为 `[65536, 131072, 262144, 524288, 1048576, 2097152]`：
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Pre-compute a Python list of hyperparameter values before using suggest_categorical.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在使用 `suggest_categorical` 方法之前，预先计算一个 Python 列表的超参数值。
- en: A more interesting application of suggest_categorical is shown, with implementation
    of the specification for the num_hidden_neurons hyperparameter.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 展示了一个更有趣的 `suggest_categorical` 应用，实现了 `num_hidden_neurons` 超参数的规范化。
- en: Listing 11.2 Optuna trials to discover the neural net architecture
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.2 Optuna 试验以发现神经网络架构
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Specify the network architecture as a hyperparameter (for example, [5, 11,
    7]).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将网络架构指定为一个超参数（例如，[5, 11, 7]）。
- en: The DcTaxiModel can use a string representation of the number of neurons in
    hidden layers to build its model layers. For example, a string representation
    [3, 5, 7, 8] can represent four hidden torch.nn.Linear layers with three neurons
    in the first layer, five in the second, and so on.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: DcTaxiModel 可以使用隐藏层中神经元数量的字符串表示来构建其模型层。例如，一个字符串表示 `[3, 5, 7, 8]` 可以表示四个隐藏层，其中第一层有三个神经元，第二层有五个，依此类推。
- en: This type of a specification can be implemented as an Optuna hyperparameter
    using a combination of suggest_categorical calls. First, the hyperparameter for
    the total number of hidden layers (num_layers) is assigned a value by Optuna based
    on a set of possible hidden layer quantities from the list in listing 11.2 with
    the integer values [11, 13, 17, 19]. Next, depending on the value selected by
    Optuna for the number of hidden layers (num_layers), the next suggest_categorical
    call is invoked multiple times by the for operator, once for each hidden layer.
    Each of the invocations changes the assignment to the layer variable and instantiates
    a new hyperparameter, for example num_hidden_layer_0_neurons for the first layer
    in the architecture, num_hidden_layer_1_neurons for the second, and so on, depending
    on the value of the number of hyperparameter layers (num_layers). The value for
    each of these hyperparameters (each describing the number of neurons per hidden
    layer) is assigned from a different suggest_categorical list, specified as [7,
    11, 13, 19, 23]. Ultimately, num_hidden_neurons resolves to a Python list with
    an Optuna-proposed configuration of hidden layers and neurons.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的规范可以通过一系列的 suggest_categorical 调用来实现为 Optuna 超参数。首先，Optuna 为隐藏层的总数（num_layers）分配一个值，该值基于列表中的可能隐藏层数量
    [11, 13, 17, 19]，然后，根据 Optuna 为隐藏层数量（num_layers）选择的值，通过 for 运算符多次调用下一个 suggest_categorical
    调用，每个隐藏层调用一次。每个调用都会更改对 layer 变量的赋值，并实例化一个新的超参数，例如架构中的第一层的 num_hidden_layer_0_neurons，第二层的
    num_hidden_layer_1_neurons，依此类推，具体取决于超参数层数（num_layers）的值。为每个超参数（描述每个隐藏层的神经元数量）分配的值来自不同的
    suggest_categorical 列表，指定为 [7, 11, 13, 19, 23]。最终，num_hidden_neurons 解析为一个 Python
    列表，其中包含 Optuna 提出的隐藏层和神经元的配置。
- en: 'Combining these hyperparameters with the an entire implementation of the objective
    function results in the following:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些超参数与目标函数的整个实现结合起来，结果如下：
- en: '[PRE9]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 11.2 Neural network layers configuration as a hyperparameter
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.2 神经网络层配置作为超参数
- en: This section teaches how to extend the DC taxi model to enable support for deep
    learning models, with an arbitrary number of hidden layers and neurons per layer.
    The section also describes how a hyperparameter value can be used to specify this
    configuration.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍如何扩展 DC 出租车模型以支持深度学习模型，每层隐藏层和每层神经元数都是任意的。该部分还描述了如何使用超参数值来指定此配置。
- en: 'Instead of arbitrarily choosing the configuration of the neural network parameters
    (e.g., the number of layers or the number of the neurons per layer), it is valuable
    to treat the configuration as a hyperparameter to be optimized. Although neither
    PyTorch nor PyTorch Lightning has a straightforward technique for optimizing over
    your neural net configuration, you can easily implement a utility method to convert
    a string representation of a network’s hidden layers into a collection of nn.Module
    instances that mirror the string. For example, a string [3, 5, 8] can represent
    the three hidden layers of a neural network with three neurons in the first layer,
    five in the second layer, and eight in the third layer. The build_hidden_layers
    utility method shown in the following code snippet implements this conversion
    from a string to the torch.nn.Linear instances along with an arbitrary activation
    function:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 不是随意选择神经网络参数的配置（例如，层数或每层的神经元数），而是将配置视为要优化的超参数是有价值的。尽管 PyTorch 和 PyTorch Lightning
    都没有直接的技术来优化神经网络配置，但是您可以轻松实现一个实用方法来将网络的隐藏层的字符串表示转换为与字符串镜像的 nn.Module 实例集合。例如，字符串
    [3, 5, 8] 可以表示具有三个隐藏层的神经网络，第一层有三个神经元，第二层有五个神经元，第三层有八个神经元。以下代码片段中显示的 build_hidden_layers
    实用方法实现了从字符串到 torch.nn.Linear 实例的转换，以及任意激活函数：
- en: '[PRE10]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Create a Python list of the linear (feedforward) layers . . .
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个线性（前馈）层的 Python 列表...
- en: ❷ . . . and create a list of matching length consisting of activation classes.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ . . . 并创建一个匹配长度的激活类列表。
- en: ❸ Convert activation classes to activation instances.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将激活类转换为激活实例。
- en: ❹ Zip the linear layers with the activation function instances.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将线性层与激活函数实例进行压缩。
- en: ❺ Return the result as a flat Python list.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将结果作为平面 Python 列表返回。
- en: 'With the build_hidden_layers utility method added to the implementation of
    DcTaxiModel, you are ready to modify the __init__ method to use the build_hidden_
    layers as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在DcTaxiModel的实现中，添加了build_hidden_layers实用程序方法，现在您可以修改__init__方法以使用build_hidden_layers如下：
- en: '[PRE11]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Create a list of hidden layer neurons (for example, [3, 5, 8]).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个隐藏层神经元的列表（例如[3, 5, 8]）。
- en: ❷ Use sequences of feed-forward hidden layers for the model.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 对模型使用前馈隐藏层的序列。
- en: ❸ Specify the hidden layers using a string format.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用字符串格式指定隐藏层。
- en: In the example, the json.loads method is used to convert the hidden layer string
    representation of a list, for example [3, 5, 8], to a Python list of integer values.
    Also, while the self.layers reference to the neural network model still has four
    input features and one output value, the hidden values are specified by unrolling
    the list of the torch.nn.Linear instances from the build_hidden_layers method
    as individual objects passed to the torch.nn.Sequential initializer.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在示例中，使用json.loads方法将隐藏层字符串表示（例如[3, 5, 8]）转换为Python整数值列表。此外，虽然self.layers参考神经网络模型仍然具有四个输入特征和一个输出值，但隐藏值是通过将build_hidden_layers方法中的torch.nn.Linear实例的列表展开为作为torch.nn.Sequential初始化程序传递的单个对象来指定的。
- en: 11.3 Experimenting with the batch normalization hyperparameter
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.3 使用批量归一化超参数进行实验
- en: Batch normalization is a widely used technique to increase the rate of gradient
    descent convergence.[²](#pgfId-1014577) Despite the widespread use of batch normalization,
    it is useful to collect data that demonstrates it can help create more effective
    DC taxi models.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化是一种广泛使用的技术，可以增加梯度下降收敛率。[²](#pgfId-1014577)尽管批量归一化被广泛使用，但收集证明它可以帮助创建更有效的DC出租车模型的数据仍然很有用。
- en: 'Since the batch normalization feature should be enabled or disabled depending
    on a Boolean HPO flag, it is useful to introduce an automatic way of rewiring
    the layers of the DC taxi model to take advantage of the feature. The following
    batch_ norm_linear method implements an automatic insert of PyTorch torch.nn.BatchNorm1d
    class instances before each torch.nn.Linear layer in the model. The following
    implementation also correctly configures each of the BatchNorm1d instances to
    have the right number of inputs, matching the number of inputs in the corresponding
    Linear layer, which should follow the BatchNorm1d:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 由于批量归一化特性应根据布尔型HPO标志启用或禁用，因此引入一种自动重连DC出租车模型的图层以利用此特性的方法是很有用的。以下batch_norm_linear方法在模型中的每个torch.nn.Linear层之前自动插入PyTorch
    torch.nn.BatchNorm1d类实例。以下实现还正确配置每个BatchNorm1d实例，使其具有正确数量的输入，与相应的Linear层的输入数相匹配，应遵循BatchNorm1d：
- en: '[PRE12]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Build a list with the positional index of the Linear classes in the model.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个列表，其中包含模型中线性类的位置索引。
- en: ❷ Use the maximum int value as the last element of the list to represent an
    infinite value.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用最大整数值作为列表的最后一个元素，表示无限值。
- en: ❸ Create sublists with s as an index of each Linear and e as an index before
    each Linear.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 创建带有s作为每个Linear的索引和e作为每个Linear之前的索引的子列表。
- en: ❹ Instantiate BatchNorm1d with inputs matching the inputs of the corresponding
    Linear.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用与对应的Linear输入匹配的输入实例化BatchNorm1d。
- en: ❺ Insert the BatchNorm1d instances before the corresponding Linear.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 在对应的Linear之前插入BatchNorm1d实例。
- en: ❻ Package the entire sequence of BatchNorm1d and Linear layers into Sequential.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将所有的BatchNorm1d和Linear层序列打包成Sequential。
- en: Once the batch_norm_linear method is added to the DcTaxiModel class, the __init__
    method of the class should be modified (listing 11.3 ❶) to apply batch normalization
    depending on the value of the batch_norm_linear_layers hyperparameter.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦将batch_norm_linear方法添加到DcTaxiModel类中，就应该修改该类的__init__方法（列表11.3 ❶），以根据batch_norm_linear_layers超参数的值应用批量归一化。
- en: Listing 11.3 Using optional batch normalization
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.3 使用可选批量归一化
- en: '[PRE13]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Batch normalize Linear layers if batch_norm_linear_layers is True.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 如果batch_norm_linear_layers为True，则对Linear层进行批量归一化。
- en: With the batch normalization in place, DcTaxiModel and the matching build method
    are ready for HPO.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用批量归一化后，DcTaxiModel和匹配的构建方法已准备好用于HPO。
- en: Listing 11.4 DcTaxiModel implementation with HPO support
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.4 DcTaxiModel实现支持HPO
- en: '[PRE14]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 11.3.1 Using Optuna study for hyperparameter optimization
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3.1 使用Optuna研究进行超参数优化
- en: This section introduces the concept of an Optuna study, describes its relationship
    to the Optuna trials, and helps you use a study instance to run and analyze a
    collection of trials in your HPO implementation.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了 Optuna 研究的概念，描述了它与 Optuna 试验的关系，并帮助您使用研究实例来运行和分析 HPO 实现中的一组试验。
- en: The objective function in Optuna is responsible for the execution of a single
    trial, including the steps to retrieve the hyperparameter values from Optuna,
    train a model, and then evaluate the trained model in terms of the validation
    loss. Hence, each trial returns just a single evaluation measure to the Optuna
    HPO algorithm in order to make the decision about the next set of hyperparameter
    values to suggest for the next trial. A typical HPO process involves tens or hundreds
    of trials; therefore, it is important to have the capability to organize the trials,
    compare their outcomes, and analyze the hyperparameters involved in the trials.
    In Optuna, a study serves the role of a container for related trials and provides
    tabular as well as visual data about the trial outcomes and associated hyperparameters.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Optuna 中，目标函数负责执行单个试验，包括从 Optuna 中检索超参数值、训练模型，然后根据验证损失评估训练模型的步骤。因此，每个试验仅向
    Optuna HPO 算法返回单个评估指标，以便决定下一组建议的超参数值用于下一个试验。典型的 HPO 过程涉及几十甚至几百次试验；因此，有能力组织试验、比较其结果并分析试验中涉及的超参数是很重要的。在
    Optuna 中，研究扮演着相关试验的容器角色，并提供有关试验结果和相关超参数的表格数据以及可视化数据。
- en: As shown in listing 11.5, with an objective function in place, a study is defined
    by the direction of optimization (e.g., to minimize or maximize a function) and
    by a sampler, which is instantiated based on one of many HPO algorithms and frameworks
    supported by Optuna. The seed (listing 11.5 ❷) used to initialize a study is different
    from the seed values used to initialize PyTorch and NumPy. When using HPO, this
    seed can be used to create the seed values for downstream random number generators,
    including Python’s own random number generator. Although the HPO seed value is
    useless from the standpoint of the optimization of the DcTaxiModel machine learning
    performance, it serves the important purpose of ensuring the reproducibility of
    the HPO trials.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 11.5 所示，通过一个目标函数，一个研究由优化方向（例如，最小化或最大化函数）和采样器来定义，采样器基于 Optuna 支持的许多 HPO 算法和框架之一实例化。用于初始化研究的种子（图
    11.5 ❷）与用于初始化 PyTorch 和 NumPy 的种子值不同。在使用 HPO 时，此种子可用于创建下游随机数生成器的种子值，包括 Python
    自己的随机数生成器。尽管 HPO 种子值从优化 DcTaxiModel 机器学习性能的角度来看毫无用处，但它确实具有确保 HPO 试验的可重现性的重要目的。
- en: The entire HPO implementation for the DcTaxiModel from listing 11.4 is shown.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.4 中显示了 DcTaxiModel 的整个 HPO 实现。
- en: Listing 11.5 An Optuna study used to execute HPO
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.5 用于执行 HPO 的 Optuna 研究
- en: '[PRE15]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Configure the study to minimize MSE loss of the DcTaxiModel.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 配置研究以最小化 DcTaxiModel 的 MSE 损失。
- en: ❷ Use the TPE algorithm for HPO.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用 TPE 算法进行 HPO。
- en: ❸ Start the HPO using 100 trials.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用 100 次试验开始 HPO。
- en: After executing the code in listing 11.5, the study.optimize method completes
    100 trials of HPO. The details of the individual trials are made available using
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行列表 11.5 中的代码后，study.optimize 方法完成了 100 次 HPO 试验。使用以下方式可获得各个试验的详细信息
- en: '[PRE16]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'which should return a pandas data frame with the values resembling the following:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 应返回一个类似以下值的 pandas 数据帧：
- en: '| number | value | params_seed |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| number | value | params_seed |'
- en: '| 96 | 2.390541 | 1372300804 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 96 | 2.390541 | 1372300804 |'
- en: '| 56 | 7.403345 | 1017301131 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 56 | 7.403345 | 1017301131 |'
- en: '| 71 | 9.006614 | 939699871 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 71 | 9.006614 | 939699871 |'
- en: '| 74 | 9.139935 | 973536326 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 74 | 9.139935 | 973536326 |'
- en: '| 94 | 9.817746 | 1075268021 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 94 | 9.817746 | 1075268021 |'
- en: where the number column specifies the index of the trial suggested by Optuna,
    the value is the corresponding value of the loss function returned by trainer.callback_
    metrics['train_val_rmse'].item() in the objective method, and the params_seed
    is the seed value used to initialize the model parameters (weights) of the DcTaxiModel.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，数字列指定由 Optuna 建议的试验的索引，值是由目标方法中的 trainer.callback_ metrics['train_val_rmse'].item()
    返回的损失函数的相应值，params_seed 是用于初始化 DcTaxiModel 的模型参数（权重）的种子值。
- en: 11.3.2 Visualizing an HPO study in Optuna
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3.2 在 Optuna 中可视化 HPO 研究
- en: This section illustrates the HPO study executed in this chapter using three
    different Optuna visualizations and compares the visualizations in terms of their
    relevance to HPO.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了本章中使用三种不同 Optuna 可视化执行的 HPO 研究，并比较了这些可视化图在 HPO 方面的相关性。
- en: A completed study instance can also be visualized using the Optuna visualization
    package. While a comprehensive overview of the various visualizations in Optuna
    is outside the scope of this book, I found myself reusing just three visualizations
    consistently across a range of machine learning models. These visualizations are
    explained in the rest of this section in the order of declining importance.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 完成的研究实例也可以使用 Optuna 可视化包进行可视化。虽然全面概述 Optuna 中各种可视化的范围超出了本书的范围，但我发现自己在一系列机器学习模型中一直重复使用三个可视化图。这些可视化图将在本节的其余部分按重要性下降的顺序进行解释。
- en: The hyperparameter importance plot reveals surprising information about the
    relative influence of the study of hyperparameters on the objective function.
    It is particularly useful to have the seed hyperparameter included in the list
    to assess whether some hyperparameters had more or less importance than just a
    random variable used for model initialization. The hyperparameters that are more
    important than the random seed deserve further study, while the hyperparameters
    that are less important than a random variable should be de-prioritized.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数重要性图揭示了关于超参数对目标函数相对影响的惊人信息。在列表中包含种子超参数特别有用，以评估某些超参数是否比仅用于模型初始化的随机变量具有更多或更少的重要性。比随机种子更重要的超参数值得进一步研究，而比随机变量不重要的超参数应该降低优先级。
- en: To create an importance plot you can use
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建重要性图，您可以使用
- en: '[PRE17]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: which should render a bar graph resembling the one in figure 11.1.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会生成类似于图 11.1 的条形图。
- en: '![11-01](Images/11-01.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![11-01](Images/11-01.png)'
- en: Figure 11.1 The importances plot assists in guiding later HPO iterations.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.1 重要性图有助于指导后续 HPO 迭代。
- en: Once you have identified a subset of hyperparameters that you wish to explore
    in more detail, the next step is to plot them on a parallel coordinates chart.
    You can instantiate this plot using
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您确定了要更详细探讨的一组超参数，下一步就是在平行坐标图上绘制它们。您可以使用以下命令实例化此图。
- en: '[PRE18]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: which plots the relationship between the lr (learning rate), batch_size, and
    num_hidden_layer_0_neurons hyperparameters. Notice that in figure 11.2, the lines
    represent the individual trial configurations, with those of the darker shade
    corresponding to the trials with a lower value for the objective function. Hence,
    a collection of the darker lines that pass though a certain interval for a hyperparameter
    indicate that the hyperparameter interval deserves closer inspection and may warrant
    another iteration of HPO.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这会绘制学习率（lr）、批量大小和 num_hidden_layer_0_neurons 超参数之间的关系。请注意，在图 11.2 中，线条代表各个试验配置，颜色较深的线条对应于目标函数值较低的试验。因此，通过某个超参数的一系列较深线条穿过一定区间，表明该超参数区间值得更仔细检查，可能需要进行另一次
    HPO 迭代。
- en: '![11-02](Images/11-02.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![11-02](Images/11-02.png)'
- en: Figure 11.2 The parallel coordinate plot is useful in pinpointing the impactful
    intervals for hyperparameter values.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2 平行坐标图有助于确定超参数值的影响区间。
- en: Out of the plots described so far, the contour plot comes in last in terms of
    its ability to generate insights about the outcomes in a study. Since the contour
    plot is limited to visualization of pairs of hyperparameter values, you will find
    yourself generating multiple contour plots, often based on hyperparameters selected
    from either the importance or the parallel coordinate plots. As an example, to
    contour plot the relationship among batch_size, lr, and the objective function,
    you can run
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止描述的图中，轮廓图在生成有关研究结果的见解方面排在最后。由于轮廓图仅限于可视化超参数值对的图形，您会发现自己生成多个轮廓图，通常基于从重要性图或平行坐标图中选择的超参数。例如，要绘制批量大小、学习率和目标函数之间的关系，您可以运行
- en: '[PRE19]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: which should produce a plot resembling the one in figure 11.3.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该生成类似于图 11.3 的图形。
- en: '![11-03](Images/11-03.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![11-03](Images/11-03.png)'
- en: Figure 11.3 The contour plot helps in the analysis of hyperparameter pairs relative
    to the loss function.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.3 轮廓图有助于分析与损失函数相关的超参数对。
- en: Summary
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Optuna is a hyperparameter optimization framework with the native Python integration
    to support nontrivial hyperparameter configurations for your machine learning
    model experiments.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Optuna 是一个超参数优化框架，具有与本机 Python 集成，可支持机器学习模型实验中的复杂超参数配置。
- en: When using HPO for hyperparameters with ranges that span orders of magnitude,
    it is useful to adopt log-uniform sampling to ensure even (not skewed) distribution
    of samples over the range.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当使用 HPO 用于跨量级的超参数范围时，采用对数均匀采样是有用的，以确保样本在范围内均匀分布而不是倾斜分布。
- en: After the execution of HPO trials, Optuna visualization features help with the
    analysis of the trial outcomes and the associated hyperparameters.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在执行 HPO 试验之后，Optuna 的可视化功能有助于分析试验结果和相关的超参数。
- en: ^(1.)Gradient-free algorithms do not require computation of the gradient of
    a loss (or any objective) function to optimize the function parameters. In other
    words, gradient-free hyperparameter optimization can optimize an objective function
    even when the function does not have computable gradients.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ^(1.)无梯度算法不需要计算损失（或任何目标）函数的梯度来优化函数参数。换句话说，无梯度的超参数优化可以在目标函数没有可计算梯度的情况下进行优化。
- en: '^(2.)The original, widely cited paper “Batch Normalization: Accelerating Deep
    Network Training by Reducing Internal Covariate Shift” that introduced batch normalization
    is available from [https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167).'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '^(2.)最初广泛引用的论文《Batch Normalization: Accelerating Deep Network Training by Reducing
    Internal Covariate Shift》介绍了批归一化，并可从[https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167)获取。'
