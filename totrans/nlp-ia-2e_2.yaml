- en: 2 Tokens of thought (natural language words)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 个思维的标记（自然语言单词）
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Parsing your text into words and *n*-grams (tokens)
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将你的文本解析成单词和*n*-grams（标记）
- en: Tokenizing punctuation, emoticons, and even Chinese characters
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对标点符号，表情符号，甚至中文字符进行分词标记
- en: Consolidating your vocabulary with stemming, lemmatization, and case folding
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用词干提取、词形还原和大小写折叠巩固你的词汇表
- en: Building a structured numerical representation of natural language text
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建自然语言文本的结构化数字表示
- en: Scoring text for sentiment and prosocial intent
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为情感和亲社会意图对文本进行评分
- en: Using character frequency analysis to optimize your token vocabulary
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用字符频率分析来优化你的标记词汇表
- en: Dealing with variable length sequences of words and tokens
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理变长的单词和标记序列
- en: So you want to help save the world with the power of natural language processing
    (NLP)? First your NLP pipeline will need to compute something about text, and
    for that you’ll need a way to represent text in a numerical data structure. The
    part of an NLP pipeline that breaks up your text to create this structured numerical
    data is called a *parser*. For many NLP applications, you only need to convert
    your text to a sequence of words, and that can be enough for searching and classifying
    text.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你想要用自然语言处理（NLP）的力量来拯救世界？首先，你的NLP流水线需要计算关于文本的一些内容，为此你需要一种方式来用数字数据结构表示文本。NLP管道中将你的文本分解以创建这种结构化数字数据的部分被称为*解析器*。对于许多NLP应用程序来说，你只需要将文本转换为一系列单词，这对于搜索和分类文本可能已经足够了。
- en: You will now learn how to split a document, any string, into discrete tokens
    of meaning. You will be able to parse text documents as small as a single word
    and as large as an entire Encyclopedia. And they will all produce a consistent
    representation that you can use to compare them. For this chapter your tokens
    will be words, punctuation marks, and even pictograms such as Chinese characters,
    emojis and emoticons.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在将学习如何将一个文档，任何字符串，分割成有意义的离散标记。你将能够解析小到单个词，大到整部百科全书的文本文件。而且它们都会产生一致的表示，你可以用来比较它们。在本章中，你的标记将是单词、标点符号，甚至表情符号，如中文字符、表情符号。
- en: 'Later in the book you will see that you can use these same techniques to find
    packets of meaning in any discrete sequence. For example, your tokens could be
    the ASCII characters represented by a sequence of bytes, perhaps with ASCII emoticons.
    Or they could be Unicode emojis, mathematical symbols, Egyption, hieroglyphics,
    pictographs from languages like Kanji and Cantonese. You could even define the
    tokens for DNA and RNA sequences with letters for each of the five base nucleotides:
    adenine (A), guanine (G), cytosine (C), thymine (T), and uracil (U). Natural language
    sequences of tokens are all around you …​ and even inside you.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 书中稍后你将看到，你可以使用这些相同的技术在任何离散序列中找到意义的片段。例如，你的标记可以是由一系列字节表示的ASCII字符，也许还有ASCII表情符号。或者它们可以是Unicode表情符号、数学符号、埃及象形文字、类似汉字和粤语的象形文字。你甚至可以为DNA和RNA序列定义标记，每个字母代表五个碱基：腺嘌呤（A）、鸟嘌呤（G）、胞嘧啶（C）、胸腺嘧啶（T）和尿嘧啶（U）。自然语言的标记序列就在你身边…
    甚至在你体内。
- en: Is there something you can do with tokens that doesn’t require a lot of complicated
    deep learning? If you have a good tokenizer you can use it to identify statistics
    about the occurrence of tokens in a set of documents, such as your blog posts
    or a business website. Then you can build a search engine in pure Python with
    just a dictionary to represent to record links to the set of documents where those
    words occur. That Python dictionary that maps words to document links or pages
    is called a reverse index. It’s just like the index at the back of this book.
    This is called *information retrieval* — a really powerful tool in your NLP toolbox.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 有没有一些你可以用标记做的事情，不需要大量复杂的深度学习？如果你有一个好的分词器，你可以用它来识别关于一组文档中标记出现次数的统计信息，比如你的博客文章或企业网站。然后你可以用纯Python编写一个只需一个字典来记录这些词出现的文档链接的搜索引擎。将单词映射到文档链接或页面的Python字典称为逆向索引。就像这本书后面的索引一样。这被称为*信息检索*—这是你NLP工具箱中的一个非常强大的工具。
- en: Statistics about tokens are often all you need for keyword detection, full text
    search, and information retrieval. You can even build customer support chatbots
    using text search to find answers to customers' questions in your documentation
    or FAQ (frequently asked question) lists. A chatbot can’t answer your questions
    until it knows where to look for the answer. Search is the foundation of many
    state of the art applications such as conversational AI and open domain question
    answering. A tokenizer forms the foundation for almost all NLP pipelines.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅统计标记通常就足够进行关键词检测、全文搜索和信息检索。你甚至可以使用文本搜索构建客户支持聊天机器人，以在你的文档或FAQ（常见问题）列表中查找客户问题的答案。直到聊天机器人知道从哪里寻找答案，它才能回答你的问题。搜索是许多最先进的应用程序的基础，如会话型AI和开放域问答。分词器是几乎所有NLP流程的基础。
- en: 2.1 Tokens of emotion
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 情感标记
- en: Another practical use for your tokenizer is called *sentiment analysis*, or
    analysis of text to estimate emotion. You’ll see an example of a sentiment analysis
    pipeline later in this chapter. For now you just need to know how to build a tokenizer.
    And your tokenizer will almost certainly need to handle the tokens of emotion
    called *emoticons* and *emojis*.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个你的分词器的实际用途被称为*情感分析*，或者分析文本以估计情感。你将在本章后面看到一个情感分析流程的示例。现在你只需要知道如何构建一个分词器。而且你的分词器几乎肯定需要处理被称为*表情符号*和*表情符号*的情感标记。
- en: '*Emoticons* are a textual representations of a writer’s mood or facial expression,
    such as the *smiley* emoticon: `:-)`. They are kind-of like a modern hieroglyph
    or picture-word for computer users that only have access to an ASCII terminal
    for communication. *Emojis* are the graphical representation of these characters.
    For example, the smilie emoji has a small yellow circle with two black dots for
    eyes and a U shaped curve for a mouth. The smiley emoji is a graphical representation
    of the `:-)` smiley emoticon.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*表情符号*是作者情绪或面部表情的文本表示，例如*微笑*表情符号：`:-)`。它们类似于计算机用户的现代象形文字或图片词，只能通过ASCII终端进行通信。*表情符号*是这些字符的图形表示。例如，微笑表情符号具有一个小的黄色圆圈，两个黑点表示眼睛，U形曲线表示嘴巴。微笑表情符号是`:-)`微笑表情符号的图形表示。'
- en: Both emojis and emoticons have evolved into their own language. There are hundreds
    of popular emojis. People have created emojis for everything from company logos
    to memes and innuendo. Noncommercial social media networks such Mastodon even
    allow you to create your own custom emojis.^([[1](#_footnotedef_1 "View footnote.")])
    ^([[2](#_footnotedef_2 "View footnote.")])
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 表情符号和表情语言都有自己的语言演化。有数百种受欢迎的表情符号。人们为从公司标志到模因和暗示等各种事物创建了表情符号。甚至非商业社交媒体网络如Mastodon都允许你创建自定义表情符号。^([[1](#_footnotedef_1
    "查看脚注。")]) ^([[2](#_footnotedef_2 "查看脚注。")])
- en: Emojis and Emoticons
  id: totrans-18
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表情符号和表情符号
- en: '*Emoticons* were first typed into an ASCII text message in 1972 when Carnegie
    Mellon researchers mistakenly understood a text message about a mercury spill
    to be a joke. The professor, Dr. Scott E. Fahlman, suggested that `:-)` should
    be appended to messages that were jokes, and `:-(` emoticons should be used for
    serious warning messages. Gosh, how far we’ve come.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*表情符号*是1972年第一次输入到ASCII文本消息中的，当时卡内基梅隆大学的研究人员错误地把一个关于汞泄漏的短信误认为是个笑话。教授斯科特·E·福尔曼博士建议对于笑话，消息末尾应加上`:-)`，对于严肃的警告消息，则用`:-(`表情符号。我的天，我们走了多远。'
- en: The plural of "emoji" is either "emoji" (like "sushi") or "emojis" (like "Tsunamis"),
    however the the Atlantic and NY Times style editors prefer "emojis" to avoid ambiguity.
    Your NLP pipeline will learn what you mean no matter how you type it.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '"Emoji"的复数可以是"emoji"（像"sushi"）或"emojis"（像"Tsunamis"），但大西洋和纽约时报的风格编辑更喜欢"emojis"以避免歧义。无论你如何输入，你的NLP流程都将了解你的意思。'
- en: '![wikipedia smiley icon](images/wikipedia-smiley-icon.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![维基百科笑脸图标](images/wikipedia-smiley-icon.png)'
- en: 2.2 What is a token?
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 什么是标记？
- en: A token can be almost any chunk of text that you want to treat as a packet of
    thought and emotion. So you need to break your text into chunks that capture individual
    thoughts. You may be thinking that *words* are the obvious choice for tokens.
    So that’s what you will start with here. You’ll also learn how to include punctuation
    marks, emojis, numbers, and other word-like things in your vocabulary of words.
    Later you’ll see that you can use these same techniques to find packets of meaning
    in any discrete sequence. And later you will learn some even more powerful ways
    to split discrete sequences into meaningful packets. Your tokenizers will be soon
    able to analyze and structure any text document or string, from a single word,
    to a sentence, to an entire book.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一个令牌可以是几乎任何你想要视为一组思想和情感的文本块。所以你需要将文本分成捕捉单个思想的块。你可能会认为*单词*是令牌的明显选择。所以这就是你将从这里开始的内容。你还将学习如何将标点符号、表情符号、数字和其他类似单词的东西包括在你的单词词汇中。后来你会发现你可以使用这些相同的技巧来找到任何离散序列中的意义包。后来你将学习一些更强大的方法来将离散序列分割成有意义的包。你的标记器很快就能分析和结构化任何文本文档或字符串，从一个单词到一个句子，再到整本书。
- en: Think about a collection of documents, called a *corpus*, that you want to process
    with NLP. Think about the *vocabulary* that would be important to your NLP algorithm — the
    set of tokens you will need to keep track of. For example your tokens could be
    the characters for ASCII emoticons, if this is what is important in your NLP pipeline
    for a particular corpus. Or your tokens could be Unicode emojis, mathematical
    symbols, hieroglyphics, even pictographs like Kanji and Cantonese characters.
    Your tokenizer and your NLP pipeline would even be useful for the nucleotide sequences
    of DNA and RNA where your tokens might be A, C, T, G, U, and so on. And neuroscientists
    sometimes create sequences of discrete symbols to represent neurons firing in
    your brain when you read text like this sentence. Natural language sequences of
    tokens are inside you, all around you, and flowing through you. Soon you’ll be
    flowing streams of tokens through your machine learning NLP pipeline.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下一个文档集合，称为*语料库*，你想要使用NLP进行处理。想想对你的NLP算法重要的*词汇*，你将需要跟踪的令牌集合。例如，如果这是你NLP流水线中特定语料库的重要内容，那么你的令牌可能是ASCII表情符号的字符。或者你的令牌可以是Unicode表情符号、数学符号、象形文字，甚至象形文字像汉字和粤语字符。你的标记器和你的NLP流水线甚至对DNA和RNA的核苷酸序列也会有用，其中你的令牌可能是A、C、T、G、U等等。当你阅读像这句话这样的文本时，神经科学家有时会创建离散符号序列来表示你大脑中的神经元发射。自然语言的令牌序列在你内部，在你周围，在你身体里流动。很快你将通过你的机器学习NLP流水线流动一系列令牌。
- en: Retrieving tokens from a document will require some string manipulation beyond
    just the `str.split()` method employed in chapter 1\. You’ll probably want to
    split contractions like "you’ll" into the words that were combined to form them,
    perhaps "you" and "'ll", or perhaps "you" and "will." You’ll want to separate
    punctuation from words, like quotes at the beginning and end of quoted statements
    or words, such as those in the previous sentence. And you need to treat some punctuation
    such as dashes ("-") as part of singly-hyphenated compound words such as "singly-hyphenated."
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 从文档中检索令牌将需要一些字符串操作，不仅仅是第1章中使用的`str.split()`方法。你可能想要将像"you’ll"这样的缩写分割成形成它们的单词，也许是"you"和"'ll"，或者是"you"和"will"。你需要将标点符号与单词分开，比如引号在引用语句或单词的开头和结尾，比如前一句中的引号。并且你需要将一些标点符号，比如破折号("-")视为一部分，例如"单破折号化"中的单破折号化复合词。
- en: Once you have identified the tokens in a document that you would like to include
    in your vocabulary, you will return to the regular expression toolbox to build
    a tokenizer. And you can use regular expressions combine different forms of a
    word into a single token in your vocabulary — a process called *stemming*. Then
    you will assemble a vector representation of your documents called a *bag of words*.
    Finally, you will try to use this bag of words vector to see if it can help you
    improve upon the basic greeting recognizer at the end of chapter 1.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你确定了文档中想要包含在你的词汇表中的令牌，你将返回到正则表达式工具箱中构建一个标记器。你可以使用正则表达式将词的不同形式组合成你词汇中的一个单一令牌，这个过程称为*词干提取*。然后你将组装一个称为*词袋*的文档的向量表示。最后，你将尝试使用这个词袋向量来看看它是否能帮助你改进第1章末尾的基本问候识别器。
- en: 2.2.1 Alternative tokens
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.1 替代令牌
- en: Words aren’t the only packets of meaning we could use for our tokens. Think
    for a moment about what a word or token represents to you. Does it represent a
    single concept, or some blurry cloud of concepts? Could you always be sure to
    recognize where a word begins and ends? Are natural language words like programming
    language keywords that have precise spellings, definitions and grammatical rules
    for how to use them? Could you write software that reliably recognizes a word?
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 单词不是我们可以用于标记的唯一含义包。想一想单词或标记对你来说代表着什么。它代表一个单一概念，还是一团模糊的概念云？你能肯定地辨认出一个单词的起始和结束吗？自然语言单词像编程语言关键字一样，具有精确的拼写、定义和语法规则吗？你能编写可靠识别单词的软件吗？
- en: Do you think of "ice cream" as one word or two? Or maybe even three? Aren’t
    there at least two entries in your mental dictionary for "ice" and "cream" that
    are separate from your entry for the compound word "ice cream"? What about the
    contraction "don’t"? Should that string of characters be split into one, or two,
    or even three packets of meaning?
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 你认为“冰淇淋”是一个单词还是两个单词？甚至可能是三个？在你的心理词典中，“冰”和“淇淋”至少有两个单独的条目，与复合词“冰淇淋”的条目不同？那么缩略词“don’t”呢？这个字符串应该分成一个、两个，甚至三个含义包吗？
- en: You might even want to divide words into even smaller meaningful parts. Word
    pieces such as the prefix "pre", the suffix "fix", or the interior syllable "la"
    all have meaning. You can use these word pieces to transfer what you learn about
    the meaning of one word to another similar word in your vocabulary. Your NLU pipeline
    can even use these pieces to understand new words. And your NLG pipeline can use
    the pieces to create new words that succinctly capture ideas or memes circulating
    in the collective consciousness.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 甚至可能希望将单词划分为更小的有意义的部分。诸如前缀“pre”、后缀“fix”或内部音节“la”等词素都具有意义。你可以利用这些词素，将你对一个单词含义的理解转移到你词汇表中的另一个类似单词上。你的
    NLU 流水线甚至可以利用这些词素来理解新单词。而你的 NLG 流水线可以利用这些词素来创建简洁捕捉集体意识中流传的想法或模因的新单词。
- en: Your pipeline could break words into even smaller pieces. Letters, characters,
    or graphemes ^([[3](#_footnotedef_3 "View footnote.")]) carry sentiment and meaning
    too!^([[4](#_footnotedef_4 "View footnote.")]) We haven’t yet found the perfect
    encoding for packets of thought. And machines compute differently than brains.
    We explain language and concepts to each other in terms of words or terms. But
    machines can often see patterns in the use of characters that we miss. And for
    machines to be able to squeeze huge vocabularies into their limited RAM there
    are more efficient encodings for natural language.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你的流水线甚至可以将单词分解成更小的部分。字母、字符或图形也具有情感和意义！我们还没有找到完美的思维包编码。而且机器计算与大脑不同。我们用单词或术语解释语言和概念。但机器往往能够看到我们忽略的字符使用模式。为了让机器能够将庞大的词汇表压缩到有限的
    RAM 中，自然语言需要更高效的编码方式。
- en: The optimal tokens for efficient computation are different from the packets
    of thought (words) that we humans use. Byte Pair Encoding (BPE), Word Piece Encoding,
    and Sentence Piece Encoding, each can help machines use natural language more
    efficiently. BPE finds the optimal groupings of characters (bytes) for your particular
    set of documents and strings. If you want an **explainable** encoding, use the
    word tokenizers of the previous sections. If you want more flexible and accurate
    predictions and generation of text, then BPE, WPE, or SPE may be better for your
    application. Like the bias variance trade-off, there’s often a explainability/accuracy
    trade-off in NLP.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 用于高效计算的最佳标记与我们人类使用的思维包（单词）不同。字节对编码（BPE）、词元编码和句元编码都可以帮助机器更有效地使用自然语言。BPE 为您的特定文档和字符串找到最佳字符（字节）分组。如果你想要一个**可解释的**编码，请使用前面部分的词标记器。如果你想要更灵活和准确地预测和生成文本，那么
    BPE、WPE 或 SPE 可能更适合你的应用。就像偏差方差权衡一样，NLP 中经常存在着可解释性/准确性的权衡。
- en: What about invisible or implied words? Can you think of additional words that
    are implied by the single-word command "Don’t!"? If you can force yourself to
    think like a machine and then switch back to thinking like a human, you might
    realize that there are three invisible words in that command. The single statement
    "Don’t!" means "Don’t you do that!" or "You, do not do that!" That’s at least
    three hidden packets of meaning for a total of five tokens you’d like your machine
    to know about.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 有关隐形或隐含的单词呢？你能想到通过单词“不要！”这个命令隐含的其他单词吗？如果你能强迫自己像一台机器一样思考，然后再切换回人类的思维，你可能会意识到这个命令中有三个看不见的单词。单一的声明“不要！”意味着“你，不要这样做！”或者“你，不要做那个！”至少有三个隐藏的意义包袱，总共有五个你希望你的机器了解的标记。
- en: But don’t worry about invisible words for now. All you need for this chapter
    is a tokenizer that can recognize words that are spelled out. You will worry about
    implied words and connotation and even meaning itself in chapter 4 and beyond.^([[5](#_footnotedef_5
    "View footnote.")])
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 但现在不要担心看不见的单词。对于本章，你只需要一个能够识别拼写出的单词的分词器。在第四章及以后，你将担心隐含的单词、内涵甚至是意义本身。
- en: 'Your NLP pipeline can start with one of these five options as your tokens:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 你的自然语言处理流程可以从这五个选项中的任意一个开始作为你的标记：
- en: '**Bytes** - ASCII characters'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**字节** - ASCII 字符'
- en: '**Characters** - multi-byte Unicode characters'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**字符** - 多字节 Unicode 字符'
- en: '**Subwords** (Word pieces) - syllables and common character clusters'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**子词**（词片） - 音节和常见字符群'
- en: '**Words** - dictionary words or their roots (stems, lemmas)'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**单词** - 词典单词或它们的词根（词干，引文）'
- en: '**Sentence pieces** - short, common word and multi-word pieces'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**句子片段** - 短的、常见的词和多个词片段'
- en: As you work your way down this list your vocabulary size increases and your
    NLP pipeline will need more and more data to train. Character-based NLP pipelines
    are often used in translation problems or NLG tasks that need to generalize from
    a modest number of examples. The number of possible words that your pipeline can
    deal with is called its *vocabulary*. A character-based NLP pipeline typically
    needs fewer than 200 possible tokens to process many Latin-based languages. That
    small vocabulary ensures that byte- and character-based NLP pipelines can handle
    new unseen test examples without too many meaningless OOV (out of vocabulary)
    tokens.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当你逐渐往下看这个列表时，你的词汇量会增加，你的自然语言处理流程将需要越来越多的数据来训练。基于字符的自然语言处理流程通常用于翻译问题或需要从一些例子中推广的自然语言生成任务。你的流程可以处理的可能单词数量称为其*词汇量*。一个基于字符的自然语言处理流程通常只需要处理不到
    200 个可能的标记，就可以处理许多以拉丁语为基础的语言。这个小词汇量确保了字节和字符为基础的自然语言处理流程能够处理新的未见过的测试示例，而不会产生太多无意义的
    OOV（词汇外）标记。
- en: For word-based NLP pipelines your pipeline will need to start paying attention
    to how often tokens are used before deciding whether to "count it." You don’t
    want you pipeline to do anything meaningful with junk words such `asdf` - the
    But even if you make sure your pipeline on pays attention to words that occur
    a lot, you could end up with a vocabulary that’s as large as a typical dictionary
    - 20 to 50 thousand words.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于单词的自然语言处理流程，你的流程将需要开始关注标记的使用频率，然后决定是否“计数”。你不希望你的流程对诸如 `asdf` 之类的垃圾单词做任何有意义的事情，但即使你确保你的流程只关注出现频率很高的单词，你最终可能会得到一个与典型词典一样大的词汇量——20
    到 50 万个单词。
- en: Subwords are the optimal token to use for most Deep Learning NLP pipelines.
    Subword (Word piece) tokenizers are built into many state of the art transformer
    pipelines. Words are the token of choice for any linguistics project or academic
    research where your results need to be interpretable and explainable.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 子词是大多数深度学习自然语言处理流程的最佳标记。子词（词片）分词器内置于许多最先进的变换器流程中。对于任何需要结果可解释和可解释的语言学项目或学术研究，单词是首选的标记。
- en: Sentence pieces take the subword algorithm to the extreme. The sentence piece
    tokenizer allows your algorithm to combine multiple word pieces together into
    a single token that can sometimes span multiple words. The only hard limit on
    sentence pieces is that they do not extend past the end of a sentence. This ensures
    that the meaning of a token is associated with only a single coherent thought
    and is useful on single sentences as well as longer documents.W
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 句子片段将子词算法推到了极致。句子片段分词器允许您的算法将多个词片段组合成一个单一标记，有时可以跨越多个词。句子片段的唯一硬性限制是它们不会延伸到句子的末尾。这确保了标记的含义仅与单个连贯的思想相关，并且在单个句子以及更长的文档中都很有用。
- en: '*N*-grams'
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*N*-gram'
- en: No matter which kind of token you use for your pipeline, you will likely extract
    pairs, triplets, quadruplets, and even quintuplets of tokens. These are called
    *n*-grams_.^([[6](#_footnotedef_6 "View footnote.")]) Using *n*-grams enables
    your machine to know about the token "ice cream" as well as the individual tokens
    "ice" and "cream" that make it up. Another 2-gram that you’d like to keep together
    is "Mr. Smith". Your tokens and your vector representation of a document will
    likely want to have a place for "Mr. Smith" along with "Mr." and "Smith."
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你在流水线中使用哪种类型的标记，你都可能会提取出一对、三元组、四元组，甚至五元组的标记。这些被称为*n*-grams^([[6](#_footnotedef_6
    "查看注释。")])。使用*n*-grams让你的机器了解到"ice cream"这个标记，以及构成它的个别标记"ice"和"cream"。另一个你想要保留在一起的二元组是"Mr.
    Smith"。你的标记和你对文档的向量表示可能都希望有一个位置来放置"Mr. Smith"以及"Mr."和"Smith"。
- en: You will start with a short list of keywords as your vocabulary. This helps
    to keep your data structures small and understandable and can make it easier to
    explain your results. Explainable models create insights that you can use to help
    your stakeholders, hopefully the users themselves (rather than investors), accomplish
    their goals.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 你将从一个关键词的简短列表作为你的词汇表开始。这有助于保持你的数据结构小而易懂，并且可以更容易地解释你的结果。可解释的模型创造了你可以用来帮助你的利益相关者，希望是用户自己（而不是投资者），实现他们的目标的见解。
- en: For now, you can just keep track of all the short *n*-grams of words in your
    vocabulary. But in chapter 3, you will learn how to estimate the importance of
    words based on their document frequency, or how often they occur. That way you
    can filter out pairs and triplets of words that rarely occur together. You will
    find that the approaches we show are not perfect. Feature extraction can rarely
    retain all the information content of the input data in any machine learning pipeline.
    That is part of the art of NLP, learning when your tokenizer needs to be adjusted
    to extract more or different information from your text for your particular applications.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，你可以跟踪你词汇表中单词的所有短*n*-grams。但是在第3章，你将学习如何根据单词的文档频率或出现频率来估计单词的重要性。这样，你就可以过滤掉很少一起出现的单词对和三元组。你会发现我们展示的方法并不完美。特征提取很少能保留任何机器学习流水线中输入数据的所有信息内容。这就是自然语言处理的一部分艺术，学会在你的分词器需要调整以从文本中提取更多或不同信息以适用于你特定的应用程序时调整。
- en: In natural language processing, composing a numerical vector from text is a
    particularly "lossy" feature extraction process. Nonetheless the bag-of-words
    (BOW) vectors retain enough of the information content of the text to produce
    useful and interesting machine learning models. The techniques for sentiment analyzers
    at the end of this chapter are the exact same techniques Google used to save email
    technology from a flood of spam that almost made it useless.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理中，从文本中组成数值向量是一个特别"丢失"的特征提取过程。尽管如此，词袋（BOW）向量保留了足够的文本信息内容，以产生有用和有趣的机器学习模型。本章末尾的情感分析器技术与Google用来拯救电子邮件技术免受几乎使其无用的大量垃圾邮件的确切技术相同。
- en: 2.3 Challenges (a preview of stemming)
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 挑战（词干提取的预览）
- en: As an example of why feature extraction from text is hard, consider *stemming* — grouping
    the various inflections of a word into the same "bucket" or cluster. Very smart
    people spent their careers developing algorithms for grouping inflected forms
    of words together based only on their spelling. Imagine how difficult that is.
    Imagine trying to remove verb endings like "ing" from "ending" so you would have
    a stem called "end" to represent both words. And you would like to stem the word
    "running" to "run," so those two words are treated the same. And that is tricky
    because you have removed not only the "ing" but also the extra "n." But you want
    the word "sing" to stay whole. You would not want to remove the "ing" ending from
    "sing" or you would end up with a single-letter "s."
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 作为说明文本特征提取难度的例子，请考虑*词干提取* —— 将一个词的各种屈折形式分组到同一个"桶"或簇中。一些非常聪明的人花费了他们的职业生涯来开发基于拼写的算法，将单词的屈折形式分组在一起。想象一下这有多难。想象一下尝试从"ending"中移除动词后缀"ing"，这样你就会得到一个名为"end"的词干来代表这两个单词。而且你想将单词"running"变成"run"，这样这两个单词就被视为相同。这很棘手，因为你不仅移除了"ing"，还多了一个额外的"n"。但是你希望单词"sing"保持完整。你不希望从"sing"中移除"ing"后缀，否则你最终会得到一个单个字母"s"。
- en: Or imagine trying to discriminate between a pluralizing "s" at the end of a
    word like "words" and a normal "s" at the end of words like "bus" and "lens."
    Do isolated individual letters in a word or parts of a word provide any information
    at all about that word’s meaning? Can the letters be misleading? Yes and yes.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 或者想象一下，试图区分像“words”这样的单词末尾的复数化的“s”和像“bus”和“lens”这样的单词末尾的普通“s”。单词中孤立的个别字母或单词的部分是否提供任何关于该单词含义的信息？字母可能会误导吗？是的，都是。
- en: In this chapter we show you how to make your NLP pipeline a bit smarter by dealing
    with these word spelling challenges using conventional stemming approaches. Later,
    in chapter 5, we show you statistical clustering approaches that only require
    you to amass a collection of natural language text containing the words you are
    interested in. From that collection of text, the statistics of word usage will
    reveal "semantic stems" (actually, more useful clusters of words like lemmas or
    synonyms), without any hand-crafted regular expressions or stemming rules.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将向您展示如何通过使用常规的词干处理方法来使您的自然语言处理流水线更智能化以应对这些单词拼写挑战。稍后，在第5章中，我们将向您展示统计聚类方法，它只需要您收集一些包含您感兴趣的单词的自然语言文本。从该文本集合中，单词使用的统计信息将揭示“语义词干”（实际上，更有用的词族或同义词的集群），而不需要手工制作的正则表达式或词干规则。
- en: 2.3.1 Tokenization
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.1 标记化
- en: In NLP, *tokenization* is a particular kind of document *segmentation*. Segmentation
    breaks up text into smaller chunks or segments. The segments of text have less
    information than the whole. Documents can be segmented into paragraphs, paragraphs
    into sentences, sentences into phrases, and phrases into tokens (usually words
    and punctuation). In this chapter, we focus on segmenting text into *tokens* with
    a *tokenizer*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理中，*标记化*是一种特殊的文档*分割*。分割将文本分解为更小的块或片段。文本段比整体包含的信息少。文档可以被分割成段落，段落可以分割成句子，句子可以分割成短语，短语可以分割成标记（通常是单词和标点符号）。在本章中，我们重点研究了如何使用*标记器*将文本分割成*标记*。
- en: You may have heard of tokenizers before. If you took a computer science class
    you likely learned about how programming language compilers work. A tokenizer
    that is used to compile computer languages is called a *scanner* or *lexer*. In
    some cases your computer language parser can work directly on the computer code
    and doesn’t need a tokenizer at all. And for natural language processing, the
    only parser typically outputs a vector representation, //putting these sentances
    together might need some work// rather than if the tokenizer functionality is
    not separated from the compiler, the parser is often called a scannerless *parser*.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能以前听说过标记器。如果你学过计算机科学课程，你可能学过编程语言编译器的工作原理。用于编译计算机语言的标记器称为*扫描器*或*词法分析器*。在某些情况下，你的计算机语言解析器可以直接处理计算机代码，根本不需要标记器。对于自然语言处理，唯一的解析器通常输出一个向量表示，//组合这些句子可能需要一些工作//而不是如果标记器功能与编译器分开，解析器通常称为无扫描器的*解析器*。
- en: The set of valid tokens for a particular computer language is called the *vocabulary*
    for that language, or more formally its *lexicon*. Linguistics and NLP researchers
    use the term "lexicon" to refer to a set of natural language tokens. The term
    "vocabulary" is the more natural way to refer to a set of natural language words
    or tokens. So that’s what you will use here.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机语言的有效标记集称为该语言的*词汇*，或者更正式地称为*词汇表*。语言学和自然语言处理研究人员使用术语“词汇表”来指代一组自然语言标记。术语“词汇”更自然地用来指代一组自然语言单词或标记。所以这里你将使用它。
- en: The natural language equivalent of a computer language compiler is a natural
    language parser. A natural language tokenizer is called a *scanner*, or *lexer*,
    or *lexical analyzer* in the computer language world. Modern computer language
    compilers combine the *lexer* and *parser* into a single lexer-parser algorithm.
    The vocabulary of a computer language is usually called a *lexicon*. And computer
    language compilers sometimes refer to tokens as *symbols*.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机语言编译器的自然语言等效物是自然语言解析器。自然语言标记器在计算机语言世界中称为*扫描器*、*词法分析器*或*词法分析器*。现代计算机语言编译器将*词法分析器*和*解析器*合并为单一的词法解析器算法。计算机语言的词汇表通常称为*词汇表*。计算机语言编译器有时将标记称为*符号*。
- en: 'Here are five important NLP terms. Along side them are some roughly equivalent
    terms used in computer science when talking about programming language compilers:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有五个重要的NLP术语。以及一些在谈论编程语言编译器时在计算机科学中使用的大致等效术语：
- en: '*tokenizer* — scanner, lexer, lexical analyzer'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*标记器* — 扫描器，词法分析器，词法分析器'
- en: '*vocabulary* — lexicon'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*词汇表* — 词汇'
- en: '*parser* — compiler'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*解析器* — 编译器'
- en: '*token*, *term*, *word*, or *n-gram* — token or symbol'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*标记*，*术语*，*单词*，或 *n-gram* — 标记或符号'
- en: '*statement* — statement or expression'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*语句* — 语句或表达式'
- en: Tokenization is the first step in an NLP pipeline, so it can have a big impact
    on the rest of your pipeline. A tokenizer breaks unstructured data, natural language
    text, into chunks of information which can be counted as discrete elements. These
    counts of token occurrences in a document can be used directly as a vector representing
    that document. This immediately turns an unstructured string (text document) into
    a numerical data structure suitable for machine learning. These counts can be
    used directly by a computer to trigger useful actions and responses. Or they might
    also be used in a machine learning pipeline as features that trigger more complex
    decisions or behavior. The most common use for bag-of-words vectors created this
    way is for document retrieval, or search.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 分词是NLP管道中的第一步，因此它对管道的其余部分有很大的影响。分词器将非结构化数据，自然语言文本，分解成可以被视为离散元素的信息块。这些在文档中标记出现的计数可以直接用作表示该文档的向量。这立即将一个非结构化字符串（文本文档）转换为适合于机器学习的数值数据结构。这些计数可以被计算机直接用来触发有用的动作和响应。或者它们也可能被用于机器学习管道中，作为触发更复杂的决策或行为的特征。通过这种方式创建的词袋向量的最常见用途是文档检索或搜索。
- en: 2.4 Your tokenizer toolbox
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 你的分词器工具箱
- en: So each application you encounter you will want to think about which kind of
    tokenizer is appropriate for your application. And once you decide which kinds
    of tokens you want to try, you’ll need to configure a python package for accomplishing
    that goal.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你遇到的每个应用程序都需要考虑哪种类型的分词器适合你的应用程序。一旦你决定要尝试哪些类型的标记，你就需要配置一个Python包来实现这个目标。
- en: 'You can chose from several tokenizer implementations: ^([[7](#_footnotedef_7
    "View footnote.")])'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以选择几种分词器实现：^([[7](#_footnotedef_7 "查看脚注。")])
- en: 'Python: `str.split`, `re.split`'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Python：`str.split`，`re.split`
- en: 'NLTK: `PennTreebankTokenizer`, `TweetTokenizer`'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: NLTK：`PennTreebankTokenizer`，`TweetTokenizer`
- en: 'spaCy: state of the art tokenization is its reason for being'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: spaCy：最先进的分词是其存在的原因
- en: 'Stanford CoreNLP: linguistically accurate, requires Java interpreter'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Stanford CoreNLP：语言学准确，需要Java解释器
- en: 'Huggingface: `BertTokenizer`, a `WordPiece` tokenizer'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Huggingface：`BertTokenizer`，一个 `WordPiece` 分词器
- en: 2.4.1 The simplest tokenizer
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.1 最简单的分词器
- en: The simplest way to tokenize a sentence is to use whitespace within a string
    as the "delimiter" of words. In Python, this can be accomplished with the standard
    library method `split`, which is available on all `str` object instances as well
    as on the `str` built-in class itself.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 分词句子的最简单方法是使用字符串中的空格作为单词的“分隔符”。在Python中，这可以通过标准库方法 `split` 来实现，该方法可用于所有 `str`
    对象实例，以及 `str` 内置类本身。
- en: Let’s say your NLP pipeline needs to parse quotes from WikiQuote.org, and it’s
    having trouble with one titled *The Book Thief*.^([[8](#_footnotedef_8 "View footnote.")])
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你的NLP管道需要解析WikiQuote.org的引用，并且它在一个标题为《偷书贼》的引用上遇到了麻烦。^([[8](#_footnotedef_8
    "查看脚注。")])
- en: Listing 2.1 Example quote from *The Book Thief* split into tokens
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.1 从《偷书贼》中拆分为标记的示例引用
- en: '[PRE0]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Figure 2.1 Tokenized phrase
  id: totrans-79
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.1 分词短语
- en: '![book thief split](images/book-thief-split.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![偷书贼拆分](images/book-thief-split.png)'
- en: 'As you can see, this built-in Python method does an OK job of tokenizing this
    sentence. Its only "mistake" is to include commas within the tokens. This would
    prevent your keyword detector from detecting quite a few important tokens: `[''me'',
    ''though'', ''way'', ''arrived'', ''clouds'', ''out'', "rain"]`. Those words "clouds"
    and "rain" are pretty important to the meaning of this text. So you’ll need to
    do a bit better with your tokenizer to ensure you can catch all the important
    words and "hold" them like Liesel.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，这个内置的Python方法对这个句子的分词工作做得还可以。它唯一的“错误”是在标记内包括了逗号。这将阻止你的关键字检测器检测到相当多重要的标记：`['我',
    '虽然', '走', '的', '途中', '云', '外', '有', '雨']`。这些词“云”和“雨”对于这段文字的意义非常重要。所以你需要做得更好一点，确保你的分词器能够捕捉到所有重要的单词，并像莉泽尔一样“保持”它们。
- en: 2.4.2 Rule-based tokenization
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.2 基于规则的分词
- en: It turns out there is a simple fix to the challenge of splitting punctuation
    from words. You can use a regular expression tokenizer to create rules to deal
    with common punctuation patterns. Here’s just one particular regular expression
    you could use to deal with punctuation "hanger-ons." And while we’re at it, this
    regular expression will be smart about words that have internal punctuation, such
    as possessive words and contractions that contain apostrophes.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，解决将标点符号与单词分开的挑战有一个简单的方法。您可以使用正则表达式标记器来创建处理常见标点模式的规则。这里只是一个特定的正则表达式，您可以用来处理标点符号
    "hanger-ons"。而且当我们在这里处理时，这个正则表达式将对具有内部标点符号的单词（例如带有撇号的所有格词和缩写）进行智能处理。
- en: You’ll use a regular expression to tokenize some text from the book *Blindsight*
    by Peter Watts. The text describes how the most *adequate* humans tend to survive
    natural selection (and alien invasions).^([[9](#_footnotedef_9 "View footnote.")])
    The same goes for your tokenizer. You want to find an *adequate* tokenizer that
    solves your problem, not the perfect tokenizer. You probably can’t even guess
    what the *right* or *fittest* token is. You will need an accuracy number to evaluate
    your NLP pipeline with and that will tell you which tokenizer should survive your
    selection process. The example here should help you start to develop your intuition
    about applications for regular expression tokenizers.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用正则表达式对来自 Peter Watts 的书 *Blindsight* 中的一些文本进行标记化。这段文本描述了最 *足够* 的人类如何在自然选择（以及外星人入侵）中幸存。[[9](#_footnotedef_9
    "查看脚注。")] 对于您的标记器也是一样的。您需要找到一个 *足够* 的标记器来解决您的问题，而不是完美的标记器。您可能甚至无法猜出什么是 *正确* 或
    *最适合* 的标记。您将需要一个准确度数字来评估您的 NLP 流水线，并告诉您哪个标记器应该在您的选择过程中幸存下来。这里的示例应该帮助您开始培养对正则表达式标记器应用的直觉。
- en: '[PRE1]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Much better. Now the tokenizer separates punctuation from the end of a word,
    but doesn’t break up words that contain internal punctuation such as the apostrophe
    within the token "There’s." So all of these words were tokenized the way you wanted:
    "There’s", "fittest", "maybe". And this regular expression tokenizer will work
    fine on contractions even if they have more than one letter after the apostrophe
    such as "can’t", "she’ll", "what’ve". It will work even typos such as ''can"t''
    and "she,ll", and "what`ve". But this liberal matching of internal punctuation
    probably isn’t what you want if your text contains rare double contractions such
    as "couldn’t’ve", "ya’ll’ll", and "y’ain’t"'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 好多了。现在标记器将标点符号与单词的末尾分开，但不会将包含内部标点符号的单词分割开，比如单词 "There’s" 中的撇号。所以所有这些单词都被标记了：
    "There’s"， "fittest"， "maybe"。这个正则表达式标记器甚至可以处理具有撇号之后超过一个字母的缩写，比如 "can’t"， "she’ll"，
    "what’ve"。它甚至可以处理错别字，比如 'can"t' 和 "she,ll"，以及 "what`ve"。但是，即使有更多的例子，例如 "couldn’t’ve"，
    "ya’ll’ll" 和 "y’ain’t"，这种宽松匹配内部标点符号的方式可能不是您想要的。
- en: Tip
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'Pro tip: You can accommodate double-contractions with the regular expression
    `r''\w+(?:\''\w+){0,2}|[^\w\s]''`'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 'Pro tip: 你可以用正则表达式 `r''\w+(?:\''\w+){0,2}|[^\w\s]''` 来处理双重缩写。'
- en: This is the main idea to keep in mind. No matter how carefully you craft your
    tokenizer, it will likely destroy some amount of information in your raw text.
    As you are cutting up text, you just want to make sure the information you leave
    on the cutting room floor isn’t necessary for your pipeline to do a good job.
    Also, it helps to think about your downstream NLP algorithms. Later you may configure
    a case folding, stemming, lemmatizing, synonym substitution, or count vectorizing
    algorithm. When you do, you’ll have to think about what your tokenizer is doing,
    so your whole pipeline works together to accomplish your desired output.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这是要记住的主要思想。无论您如何精心制作您的标记器，它都很可能会破坏一些原始文本中的信息。当您切割文本时，您只需确保您留在地板上的信息对于您的管道来说并非必需即可。此外，思考您的下游
    NLP 算法也是有帮助的。稍后，您可能会配置大小写折叠、词干提取、词形还原、同义词替换或计数向量化算法。当您这样做时，您将不得不考虑您的标记器正在做什么，这样您的整个管道就可以共同完成您期望的输出。
- en: 'Take a look at the first few tokens in your lexographically sorted vocabulary
    for this short text:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下按字典顺序排序的词汇表中的前几个标记，针对这段简短文本：
- en: '[PRE2]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You can see how you may want to consider lowercasing all your tokens so that
    "Survival" is recognized as the same word as "survival". And you may want to have
    a synonym substitution algorithm to replace "There’s" with "There is" for similar
    reasons. However, this would only work if your tokenizer kept contraction and
    possessive apostrophes attached to their parent token.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，你可能希望考虑将所有标记转换为小写，以便“Survival”被识别为与“survival”相同的单词。而且你可能希望有一个同义词替换算法，将“There’s”替换为“There
    is”出于类似的原因。但是，只有当你的分词器保留缩写和所有格撇号附加到其父标记时，这才有效。
- en: Tip
  id: totrans-93
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: Make sure you take a look at your vocabulary whenever it seems your pipeline
    isn’t working well for a particular text. You may need to revise your tokenizer
    to make sure it can "see" all the tokens it needs to do well for your NLP task.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 确保在似乎你的管道在某个特定文本上表现不佳时查看你的词汇表。你可能需要修改你的分词器，以确保它可以“看到”所有它需要为你的NLP任务做得好的标记。
- en: 2.4.3 SpaCy
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.3 SpaCy
- en: Maybe you don’t want your regular expression tokenizer to keep contractions
    together. Perhaps you’d like to recognize the word "isn’t" as two separate words,
    "is" and "n’t". That way you could consolidate the synonyms "n’t" and "not" into
    a single token. This way your NLP pipeline would understand "the ice cream isn’t
    bad" to mean the same thing as "the ice cream is not bad". For some applications,
    such as full text search, intent recognition, and sentiment analysis, you want
    to be able to **uncontract** or expand contractions like this. By splitting contractions,
    you can use synonym substitution or contraction expansion to improve the recall
    of your search engine and the accuracy of your sentiment analysis.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 也许你不希望你的正则表达式分词器将缩写保持在一起。也许你想要将单词“isn’t”识别为两个单独的单词，“is”和“n’t”。这样，你就可以将“n’t”和“not”的同义词合并为一个标记。这样，你的NLP管道就可以理解“the
    ice cream isn’t bad”与“the ice cream is not bad”表示相同的意思。对于一些应用，比如全文搜索、意图识别和情感分析，你希望能够像这样**解开**或扩展缩写。通过分割缩写，你可以使用同义词替换或缩写扩展来提高搜索引擎的召回率和情感分析的准确性。
- en: Important
  id: totrans-97
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 重要提示
- en: We’ll discuss case folding, stemming, lemmatization, and synonym substitution
    later in this chapter. Be careful about using these techniques for applications
    such as authorship attribution, style transfer, or text fingerprinting. You want
    your authorship attribution or style-transfer pipeline to stay true to the author’s
    writing style and the exact spelling of words that they use.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章后面讨论大小写折叠、词干提取、词形还原和同义词替换。对于诸如作者归因、风格转移或文本指纹等应用，要谨慎使用这些技术。你希望你的作者归因或风格转移管道保持忠实于作者的写作风格和他们使用的确切拼写。
- en: SpaCy integrates a tokenizer directly into its state-of-the-art NLU pipeline.
    In fact the name "spaCy" is based on the word "space", as in the separator used
    in Western languages to separate words. And spaCy adds a lot of additional *tags*
    to tokens at the same time that it is applying rules to split tokens apart. So
    spaCy is often the first and last tokenizer you’ll ever need to use.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: SpaCy将分词器直接集成到其先进的NLU管道中。实际上，“spaCy”的名称是基于单词“space”，即用于西方语言中分隔单词的分隔符。在应用规则将标记分隔开的同时，spaCy还向标记添加了许多附加的*标签*。因此，spaCy通常是你需要使用的第一个和最后一个分词器。
- en: 'Let’s see how spaCy handles our collection of deep thinker quotes:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看spaCy如何处理我们的一系列深思家名言：
- en: '[PRE3]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: That tokenization may be more useful to you if you’re comparing your results
    to academic papers or colleagues at work. Spacy is doing a lot more under the
    hood. That small language model you downloaded is also identifying sentence breaks
    with some **sentence boundary detection** rules. A language model is a collection
    of regular expressions and finite state automata (rules). These rules are a lot
    like the grammar and spelling rules you learned in English class. They are used
    in the algorithms that tokenize and label your words with useful things like their
    part of speech and their position in a syntax tree of relationships between words.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将结果与学术论文或工作中的同事进行比较，那么该标记化对你可能更有用。Spacy在幕后做了更多工作。你下载的那个小语言模型还使用一些**句子边界检测**规则来识别句子中的断点。语言模型是一组正则表达式和有限状态自动机（规则）。这些规则很像你在英语课上学到的语法和拼写规则。它们用于分词和标记你的单词，以便为它们标记有用的东西，比如它们的词性和它们在单词之间关系语法树中的位置。
- en: '[PRE4]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'There are three ways you can create and view the sentence diagram from `displacy`:
    your web a dynamic HTML+SVG file in your web browser, a static SVG file on your
    hard drive, or an inline HTML object within a jupyter notebook. If you browse
    to the `sentence_diagram.svg` file on your local hard drive or the `localhost:5000`
    server you should see a sentence diagram that may be even better than what you
    could produce in school.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 有三种方式可以从`displacy`创建和查看句子图：在您的 web 浏览器中，您的网页中的一个动态 HTML+SVG 文件，在您的硬盘驱动器上的一个静态
    SVG 文件，或者在 jupyter 笔记本中的一个内联 HTML 对象中。如果您浏览到您本地硬盘上的`sentence_diagram.svg`文件或`localhost:5000`服务器，您应该可以看到一个句子图，可能比您在学校中可以制作的还要好。
- en: '![there such thing](images/there-such-thing.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![不存在这样的事情](images/there-such-thing.png)'
- en: You can see that spaCy does a lot more than simply separate text into tokens.
    It identifies sentence boundaries to automatically segment your text into sentences.
    And it tags tokens with various attributes like their part of speech (PoS) and
    even their role within the syntax of a sentence. You can see the lemmas displayed
    by `displacy` beneath the literal text for each token.^([[11](#_footnotedef_11
    "View footnote.")]) Later in the chapter we’ll explain how lemmatization and case
    folding and other vocabulary **compression** approaches can be helpful for some
    applications.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到 spaCy 不仅仅是将文本分隔成标记。它可以识别句子边界，自动将您的文本分割成句子。它还会标记各种属性的标记，比如它们在句子的语法中的词性（PoS）甚至是角色。您可以在`displacy`下看到每个标记的词形。^([[11](#_footnotedef_11
    "查看脚注。")]) 本章后面我们会解释词形化和大小写折叠以及其他词汇**压缩**方法对某些应用的帮助。
- en: So spaCy seems pretty great in terms of accuracy and some "batteries included"
    features, such as all those token tags for lemmas and dependencies. What about
    speed?
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，spaCy 在准确性和一些“内置”功能方面似乎相当出色，比如所有那些标记标记的词形和依赖关系。那速度呢？
- en: 2.4.4 Tokenizer race
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.4 标记器竞赛
- en: 'SpaCy can parse the AsciiDoc text for a chapter in this book in about 5 seconds.
    First download the AsciiDoc text file for this chapter:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: SpaCy 可以在约 5 秒钟内解析本书一章的 AsciiDoc 文本。首先下载本章的 AsciiDoc 文本文件：
- en: '[PRE5]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: There were about 160 thousand ASCII characters in this AsciiDoc file where I
    wrote this sentence that you are reading right now. What does that mean in terms
    of words-per-second, the standard benchmark for tokenizer speed?
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在我写下您正在阅读的这句话的 AsciiDoc 文件中大约有 160,000 个 ASCII 字符。以每秒字词数为标准的标记速度基准是什么意思？
- en: '[PRE6]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: That’s nearly 5 seconds for about 150,000 characters or 34,000 words of English
    and Python text or about 7000 words per second.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 对于约 150,000 个字符或 34,000 个英文和 Python 文本字词，或约每秒 7000 个字词，大约需要近 5 秒钟。
- en: That may seem fast enough for you on your personal projects. But on a medical
    records summarization project we needed to process thousands of large documents
    with a comparable amount of text as you find in this entire book. And the latency
    in our medical record summarization pipeline was a critical metric for the project.
    So this, full-featured spaCy pipeline would require at least 5 days to process
    10,000 books such as NLPIA or typical medical records for 10,000 patients.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于你的个人项目来说可能足够快了。但在一个医疗记录摘要项目中，我们需要处理数千个大型文档，其中包含与您在整本书中找到的相当数量的文本相当的文本量。我们医疗记录摘要管道中的延迟是该项目的关键指标。因此，这个功能齐全的
    spaCy 管道至少需要 5 天的时间来处理 10,000 本书，比如 NLPIA 或典型的 10,000 名患者的医疗记录。
- en: If that’s not fast enough for your application you can disable any of the tagging
    features of the spaCy pipeline that you do not need.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果对于您的应用程序来说速度还不够快，您可以禁用不需要的 spaCy 管道的任何标记特性。
- en: '[PRE7]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You can disable the pipeline elements you don’t need to speed up the tokenizer:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以禁用您不需要的管道元素以加速标记器：
- en: '`tok2vec`: word embeddings'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tok2vec`：单词嵌入'
- en: '`tagger`: part-of-speech (`.pos` and `.pos_`)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tagger`：词性（`.pos`和`.pos_`）'
- en: '`parser`: syntax tree role'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`parser`：语法树角色'
- en: '`attribute_ruler`: fine-grained POS and other tags'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attribute_ruler`：细粒度的 POS 和其他标记'
- en: '`lemmatizer`: lemma tagger'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lemmatizer`：词形标记'
- en: '`ner`: named entity recognition tagger'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ner`：命名实体识别标记'
- en: 'NLTK’s `word_tokenize` method is often used as the pace setter in tokenizer
    benchmark speed comparisons:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK 的 `word_tokenize` 方法通常用作标记器速度比较的标尺：
- en: '[PRE8]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Could it be that you found a winner for the tokenizer race? Not so fast. Your
    regular expression tokenizer has some pretty simple rules, so it should run pretty
    fast as well:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能觉得你找到了标记器竞赛的赢家吗？不要那么快。您的正则表达式标记器有一些非常简单的规则，因此它应该运行得相当快：
- en: '[PRE9]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now that’s not surprising. Regular expressions can be compiled and run very
    efficiently within low level C routines in Python.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不奇怪。正则表达式可以在 Python 的低级 C 例程内被编译和高效运行。
- en: Tip
  id: totrans-129
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: Use a regular expression tokenizer when speed is more import than accuracy.
    If you do not need the additional linguistic tags that spaCy and other pipelines
    provide your tokenizer doesn’t need to waste time trying to figure out those tags.^([[12](#_footnotedef_12
    "View footnote.")]) And each time you use a regular expression in the `re` or
    `regex` packages, a compiled and optimized version of it is cached in RAM. So
    there’s usually no need to *precompile* (using `re.compile()`) your regexes.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 当速度比准确性更重要时，请使用正则表达式分词器。如果您不需要像 spaCy 和其他管道提供的额外语言标签，您的分词器就不需要浪费时间去尝试识别这些标签。^([[12](#_footnotedef_12
    "查看注释。")]) 并且每次使用`re`或`regex`包中的正则表达式时，优化编译版本会缓存在内存中。因此通常不需要*预编译*（使用`re.compile()`）您的正则表达式。
- en: 2.5 Wordpiece tokenizers
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 Wordpiece 分词器
- en: It probably felt natural to think of words as indivisible atomic chunks of meaning
    and thought. However, you did find some words that didn’t clearly split on spaces
    or punctuation. And many compound words or named entities that you’d like to keep
    together have spaces within them. So it can help to dig a little deeper and think
    about the statistics of what makes a word. Think about how we can build up words
    from neighboring characters instead of cleaving text at separators such as spaces
    and punctuation.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 或许我们会自然地把词视为不可分割的，具有独立意义和思考的基本单元。然而，你可能会发现有些词在空格或标点符号上并不分明，以及很多复合词或命名实体内部有空格。因此，需要深入挖掘，并考虑什么使一个词成为一个词的统计规律。可以考虑如何从相邻字符构建词，而不是在分隔符（如空格和标点符号）处将文本分开。
- en: 2.5.1 Clumping characters into sentence pieces
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.1 聚合字符成句部
- en: Instead of thinking about breaking strings up into tokens, your tokenizer can
    look for characters that are used a lot right next to each other, such as "i"
    before "e". You can pair up characters and sequences of characters that belong
    together.^([[13](#_footnotedef_13 "View footnote.")]) These clumps of characters
    can become your tokens. An NLP pipeline only pays attention to the statistics
    of tokens. And hopefully these statistics will line up with our expectations for
    what a word is.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 您的分词器可以寻找紧密相邻使用的字符，例如在“i”之前的“e”，而不是考虑将字符串分割成标记。您可以将这些字符和字符序列组合在一起。^([[13](#_footnotedef_13
    "查看注释。")]) 这些字符团体可以成为您的标记。NLP 管道只关注标记的统计数据。希望这些统计数据与我们对“词”是什么的期望相吻合。
- en: Many of these character sequences will be whole words, or even compound words,
    but many will be pieces of words. In fact, all *subword tokenizers* maintain a
    token within the vocabulary for every individual character in your vocabulary.
    This means it never needs to use an OOV (Out-of-Vocabulary) token, as long as
    any new text doesn’t contain any new characters it hasn’t seen before. Subword
    tokenizers attempt to optimally clump characters together to create tokens. Using
    the statistics of character n-gram counts it’s possible for these algorithms to
    identify wordpieces and even sentence pieces that make good tokens.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 许多这些字符序列将是整个单词，甚至是复合词的一部分，但其中很多将是词的部分。事实上，所有*子词分词器*都在词汇表中为每个字符维护一个标记。这意味着只要新文本不包含它以前没有见过的字符，它就永远不需要使用
    OOV（词汇外）标记。子词分词器尝试将字符最优地聚集在一起以创建标记。利用字符 n-gram 计数的统计数据，这些算法可以识别出构成良好标记的词部甚至句部。
- en: It may seem odd to identify words by clumping characters. But to a machine,
    the only obvious, consistent division between elements of meaning in a text is
    the boundary between bytes or characters. And the frequency with which characters
    are used together can help the machine identify the meaning associated with subword
    tokens such as individual syllables or parts of compound words.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 通过聚合字符来确定词似乎有点奇怪。但对于机器来说，在文本中识别意义元素之间唯一明显、一致的分割就是字节或字符间的边界。并且字符一起使用的频率可以帮助机器确定与子词标记相关的意义，例如单个音节或复合词的部分。
- en: In English, even individual letters have subtle emotion (sentiment) and meaning
    (semantics) associated with them. However, there are only 26 unique letters in
    the English language. That doesn’t leave room for individual letters to *specialize*
    on any one topic or emotion. Nonetheless savvy marketers know that some letters
    are cooler than others. Brands will try to portray themselves as technologically
    advanced by choosing names with exotic letters like "Q" and "X" or "Z". This also
    helps with SEO (Search Engine Optimization) because rarer letters are more easily
    found among the sea of possible company and product names. Your NLP pipeline will
    pick up all these hints of meaning, connotation, and intent. Your token counters
    will provide the machine with the statistics it needs to infer the meaning of
    clumps of letters that are used together often.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在英语中，即使是单个字母也与微妙的情感（情绪）和含义（语义）相关联。然而，英语中只有 26 个唯一的字母。这并不留下单个字母在任何一个主题或情感上*专攻*的余地。尽管如此，精明的营销人员知道有些字母比其他字母更酷。品牌将尝试通过选择具有像
    "Q"、"X" 或 "Z" 这样的奇异字母的名称来展示自己技术先进。这也有助于 SEO（搜索引擎优化），因为更罕见的字母更容易在可能的公司和产品名称中被找到。你的
    NLP 流水线将捕捉到所有这些意义、内涵和意图的线索。你的标记计数器将为机器提供它需要推断经常一起使用的字母簇的含义的统计数据。
- en: The only disadvantage for subword tokenizers is the fact that they must pass
    through your corpus of text many times before converging on an optimal vocabulary
    and tokenizer. A subword tokenizer has to be trained or fit to your text just
    like a CountVectorizer. In fact you’ll use a CountVectorizer in the next section
    to see how subword tokenizers work.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 子词分词器唯一的劣势是它们必须在收敛于最佳词汇表和分词器之前多次通过你的文本语料库。与 CountVectorizer 类似，子词分词器必须像 CountVectorizer
    一样被训练或适应于你的文本。事实上，在下一节中，你将使用 CountVectorizer 来了解子词分词器的工作原理。
- en: 'There are two main approaches to subword tokenization: BPE (Byte-Pair Encoding)
    and Wordpiece tokenization.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 子词分词有两种主要方法：BPE（字节对编码）和 Wordpiece 分词。
- en: BPE
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: BPE
- en: In the previous edition of the book we insisted that words were the smallest
    unit of meaning in English that you need consider. With the rise of Transformers
    and other deep learning models that use BPE and similar techniques, we’ve changed
    our minds.^([[14](#_footnotedef_14 "View footnote.")]) Character-based subword
    tokenizers have proven to be more versatile and robust for most NLP problems.
    By building up a vocabulary from building blocks of Unicode multi-byte characters
    you can construct a vocabulary that can handle every possible natural language
    string you’ll ever see, all with a vocabulary of as few as 50,000 tokens.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的上一版中，我们坚持认为单词是英语中你需要考虑的最小意义单位。随着使用 BPE 和类似技术的 Transformer 和其他深度学习模型的兴起，我们改变了主意。基于字符的子词分词器已被证明对于大多数
    NLP 问题更具多功能性和鲁棒性。通过从 Unicode 多字节字符的构建块构建词汇表，你可以构建一个能处理你将要见到的每个可能的自然语言字符串的词汇表，所有这些只需
    50,000 个令牌的词汇表即可。
- en: You may think that Unicode characters are the smallest packet of meaning in
    natural language text. To a human, maybe, but to a machine, no way. Just as the
    BPE name suggests, characters don’t have to be your fundamental atom of meaning
    for your *base vocabulary*. You can split characters into 8-bit bytes. GPT-2 uses
    a byte-level BPE tokenizer to naturally compose all the unicode characters you
    need from the bytes that make them up. Though some special rules are required
    to handle unicode punctuation within a byte-based vocabulary, no other adjustment
    to the character-based BPE algorithm is required. A byte-level BPE tokenizer allows
    you to represent all possible texts with a base (minimum) vocabulary size of 256
    tokens. The GPT-2 model can achieve state-of-the-art performance with it’s default
    BPE vocabulary of only 50,000 multibyte *merge tokens* plus 256 individual byte
    tokens.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会认为 Unicode 字符是自然语言文本中含义的最小单元。对于人类来说，也许是这样，但对于机器来说却不是。正如 BPE 的名字所暗示的，字符不必是你*基本词汇*的基本含义原子。你可以将字符分割成
    8 位字节。GPT-2 使用字节级 BPE 分词器来自然地组成你需要的所有 Unicode 字符，从构成它们的字节中。尽管在基于字节的词汇表中处理 Unicode
    标点符号需要一些特殊规则，但对于基于字符的 BPE 算法不需要其他调整。字节级 BPE 分词器允许你用最少的 256 个令牌的基本（最小）词汇量来表示所有可能的文本。GPT-2
    模型可以使用仅有 50,000 个多字节*合并令牌*加上 256 个单独字节令牌的默认 BPE 词汇表实现最先进的性能。
- en: You can think of the BPE (Byte Pair Encoding) tokenizer algorithm as a matchmaker
    in a social network of friends. BPE pairs up characters that appear next to each
    other a lot and appear to be "friends." It then creates a new token for these
    character combinations. BPE can then pair up the multi-character tokens whenever
    those token pairings are common in your text. And it keeps doing this until it
    has a many frequently used character sequences as you’ve allowed in your vocabulary
    size limit.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: BPE（字节对编码）分词算法可以想象成一个社交网络中的红娘。BPE会把经常相邻出现且看起来“友好”的字符配对起来，然后为这些字符组合创建一个新的标记。BPE可以在您的文本中经常出现的地方组合多字符标记。并且它会一直这样做，直到达到您在词汇量限制中允许的常用字符序列数量为止。
- en: BPE is transforming the way we think about natural language tokens. NLP engineers
    are finally letting the data do the talking. Statistical thinking is better than
    human intuition when building an NLP pipeline. A machine can see how *most* people
    use language. You are only familiar with what *you* mean when you use particular
    words or syllables. Transformers have now surpassed human readers and writers
    at some natural language understanding and generation tasks, including finding
    meaning in subword tokens.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: BPE正在改变我们对自然语言标记的看法。自然语言处理工程师终于让数据说话了。在构建自然语言处理流水线时，统计思维比人类直觉更好。机器可以看到*大多数*人如何使用语言。而您只熟悉您在使用特定单词或音节时的意思。变换器现在已经在某些自然语言理解和生成任务中超越了人类读者和作者，包括在子词标记中找到含义。
- en: One complication you have not yet encountered is the dilemma of what to do when
    you encounter a new word. In the previous examples, we just keep adding new words
    to our vocabulary. But in the real world your pipeline will have been trained
    on an initial corpus of documents that may or may not represent all the kinds
    of tokens it will ever see. If your initial corpus is missing some of the words
    that you encounter later on, you will not have a slot in your vocabulary to put
    your counts of that new word. So when you train you initial pipeline, you will
    always reserve a slot (dimension) to hold the counts of your *out-of-vocabulary*
    (OOV) tokens. So if your original set of documents did not contain the girl’s
    name "Aphra", all counts of the name Aphra would be lumped into the OOV dimension
    as counts of Amandine and other rare words.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 您尚未遇到的一个复杂情况是当您遇到一个新单词时该怎么办的困境。在前面的例子中，我们只是不断向我们的词汇表中添加新单词。但在现实世界中，您的流水线将在一个初始文档语料库上进行训练，该语料库可能代表或可能不代表它将来可能见到的所有种类的标记。如果您的初始语料库缺少后来遇到的某些单词，您将没有一个词汇表位置来放置该新单词的计数。因此，在训练初始流水线时，您将始终保留一个位置（维度）来保存您的*超出词汇量*（OOV）标记的计数。因此，如果您的原始文档集中不包含女孩的名字“阿芙拉”，则所有阿芙拉的计数都将作为阿曼丁和其他稀有单词的计数被合并到OOV维度中。
- en: 'To give Aphra equal representation in your vector space, you can use BPE. BPE
    breaks down rare words into smaller pieces to create a *periodic table* of the
    elements for natural language in your corpus. So, because "aphr" is a common english
    prefix, your BPE tokenizer would probably give Aphra **two** slots for her counts
    in your vocabulary: one for "aphr" and one for "a". Actually, you might actually
    discover that the vobcabulary slots are for " aphr" and "a ", because BPE keeps
    track of spaces no differently than any other character in your alphabet.^([[15](#_footnotedef_15
    "View footnote.")])'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 要在您的向量空间中给予阿芙拉平等的表示，您可以使用BPE。BPE会将稀有单词拆分成较小的片段，为您语料库中的自然语言创建一个*元素周期表*。所以，因为“aphr”是一个常见的英语前缀，您的BPE分词器可能会为阿芙拉在您的词汇表中**留下两个**位置来计数：一个是“aphr”，另一个是“a”。实际上，您可能会发现词汇表中的位置是“
    aphr”和“a”，因为BPE对空格的处理方式与字母表中的其他字符没有区别。^([[15](#_footnotedef_15 "查看脚注。")])
- en: BPE gives you multilingual flexibility to deal with Hebrew names like Aphra.
    And it give your pipeline robustness against common misspellings and typos, such
    as "aphradesiac." Every word, including minority 2-grams such as "African American",
    have representation in the voting system of BPE.^([[16](#_footnotedef_16 "View
    footnote.")]) Gone are the days of using the kluge of OOV (Out-of-Vocabulary)
    tokens to handle the rare quirks of human communication. Because of this, state
    of the art deep learning NLP pipelines such as transformers all use word piece
    tokenization similar to BPE.^([[17](#_footnotedef_17 "View footnote.")])
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: BPE 让您可以处理希伯来语名字（如 Aphra）的多语言灵活性。它还可以使您的流程对常见拼写错误和打字错误具有健壮性，例如"aphradesiac"。每个单词，包括少数
    2-grams（例如"African American"），在 BPE 的投票系统中都有表示。^([[16](#_footnotedef_16 "查看脚注。")])
    过去使用处理人类交流的稀有怪癖的 OOV (Out-of-Vocabulary) 标记的方法已经过时了。因此，最先进的深度学习 NLP 流水线（如 transformers）都使用类似于
    BPE 的词片标记方法。^([[17](#_footnotedef_17 "查看脚注。")])
- en: BPE preserves some of the meaning of new words by using character tokens and
    word-piece tokens to spell out any unknown words or parts of words. For example,
    if "syzygy" is not in our vocabulary, we could represent it as the six tokens
    "s", "y", "z", "y", "g", and "y". Perhaps "smartz" could be represented as the
    two tokens "smart" and "z".
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: BPE 通过使用字符标记和词片标记来拼写任何未知单词或单词部分，从而保留了一些新单词的含义。例如，如果我们的词汇表中没有"syzygy"，我们可以将其表示为六个标记："s"，"y"，"z"，"y"，"g"和"y"。也许"smartz"可以表示为两个标记："smart"和"z"。
- en: 'That sounds smart. Let’s see how it works on our text corpus:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 听起来很聪明。让我们看看它如何在我们的文本语料库中工作：
- en: '[PRE10]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: You’ve created a `CountVectorizer` class that will tokenize the text into characters
    instead of words. And it will count token pairs (character 2-grams) in addition
    to single character tokens. These are the byte pairs in BPE encoding. Now you
    can examine your vocabulary to see what they look like.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 您创建了一个`CountVectorizer`类，它将文本分词为字符而不是单词。它将计算标记对（字符 2-grams）以及单个字符标记。这些是 BPE
    编码中的字节对。现在您可以检查词汇表以查看它们的样子。
- en: '[PRE11]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We configured the `CountVectorizer` to split the text into all the possible
    character 1-grams and 2-grams found in the texts. And `CountVectorizer` organizes
    the vocabulary in lexical order, so n-grams that start with a space character
    (`' '`) come first. Once the vectorizer knows what tokens it needs to be able
    to count, it can transform text strings into vectors, with one dimension for every
    token in your character n-gram vocabulary.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`CountVectorizer`配置为将文本分割成所有可能的字符 1-grams 和 2-grams。`CountVectorizer`将词汇按照词法顺序进行组织，因此以空格字符(`'
    '`)开头的 n-grams 位于前面。一旦向量化器知道需要计数的标记，它就可以将文本字符串转换为向量，其中每个字符 n-gram 词汇表中的标记都有一个维度。
- en: '[PRE12]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The DataFrame contains a column for each sentence and a row for each character
    2-gram. Check out the top four rows where the byte pair (character 2-gram) of
    " a" is seen to occur five times in these two sentences. So even spaces count
    as "characters" when you’re building a BPE tokenizer. This is one of the advantages
    of BPE, it will figure out what your token delimiters are, so it will work even
    in languages where there is no whitespace between words. And BPE will work on
    substitution cypher text like ROT13, a toy cypher that rotates the alphabet 13
    characters forward.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 数据框中每个句子都包含一列，每个字符 2-gram 都包含一行。看看前四行，其中字节对（字符 2-gram）" a" 在这两个句子中出现了五次。因此，即使在构建
    BPE 分词器时，空格也会计入"字符"。这是 BPE 的优点之一，它将找出您的标记分隔符是什么，所以它甚至可以在没有单词之间有空格的语言中工作。而且 BPE
    可以在 ROT13 这样的代换密码文本上起作用，ROT13 是一种玩具密码，将字母向前旋转 13 个字符。
- en: '[PRE13]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: A BPE tokenizer then finds the most frequent 2-grams and adds them to the permanent
    vocabulary. Over time it deletes the less frequent character pairs as it gets
    less and less likely that they won’t come up a lot more later in your text.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，BPE 分词器会找到最常见的 2-grams 并将其添加到永久词汇表中。随着时间的推移，它会删除较不常见的字符对，因为它们不太可能再次在文本中频繁出现。
- en: '[PRE14]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: So the next round of preprocessing in the BPE tokenizer would retain the character
    2-grams "en" and "an" and even " t" and "e ". Then the BPE algorithm would make
    another pass through the text with this smaller character bigram vocabulary. It
    would look for frequent pairings of these character bigrams with each other and
    individual characters. This process would continue until the maximum number of
    tokens is reached and the longest possible character sequences have been incorporated
    into the vocabulary.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 所以 BPE 分词器中的下一轮预处理将保留字符 2-grams "en" 和 "an"，甚至 " t" 和 "e"。然后，BPE 算法将使用这个较小的字符
    bigram 词汇再次通过文本。它会寻找这些字符 bigrams 与彼此和单个字符的频繁配对。这个过程将继续，直到达到最大标记数，并且最长的可能字符序列已经被纳入词汇表。
- en: Note
  id: totrans-160
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: You may see mention of *wordpiece* tokenizers which are used within some advanced
    language models such as `BERT` and its derivatives.^([[18](#_footnotedef_18 "View
    footnote.")]) It works the same as BPE, but it actually uses the underlying language
    model to predict the neighboring characters in string. It eliminates the characters
    from its vocabulary that hurt the accuracy of this language model the least. The
    math is subtly different and it produces subtly different token vocabularies,
    but you don’t need to select this tokenizer intentionally. The models that use
    it will come with it built into their pipelines.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会看到关于*wordpiece*分词器的提及，它被用在一些高级语言模型中，如 `BERT` 及其派生版本。它的工作方式与 BPE 相同，但它实际上使用底层语言模型来预测字符串中的相邻字符。它会从词汇表中消除对这个语言模型准确性影响最小的字符。数学上有些微的差异，产生了略有不同的标记词汇表，但您不需要故意选择这个分词器。使用它的模型将在其流水线中内置它。
- en: One big challenge of BPE-based tokenizers is that they must be trained on your
    individual corpus. So BPE tokenizers are usually only used for Transformers and
    Large Language Models (LLMs) which you will learn about in chapter 9.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 BPE 的分词器的一个重大挑战是它们必须在您的个人语料库上进行训练。因此，BPE 分词器通常仅用于您将在第 9 章学习的变形金刚和大型语言模型（LLM）。
- en: Another challenge of BPE tokenizers is all the book keeping you need to do to
    keep track of which trained tokenizer goes with each of your trained models. This
    was one of the big innovations of Huggingface. They made it easy to store and
    share all the preprocessing data, such as the tokenizer vocabulary, along side
    the language model. This makes it easier to reuse and share BPE tokenizers. If
    you want to become an NLP expert, you may want to imitate what they’ve done at
    HuggingFace with your own NLP preprocessing pipelines.^([[19](#_footnotedef_19
    "View footnote.")])
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: BPE 分词器的另一个挑战是您需要进行的所有簿记，以跟踪每个训练过的分词器与您训练过的每个模型对应。这是 Huggingface 的一项重大创新之一。他们简化了存储和共享所有预处理数据的过程，例如分词器词汇表，与语言模型一起。这使得重复使用和共享
    BPE 分词器变得更容易。如果你想成为一个 NLP 专家，你可能想模仿 HuggingFace 在你自己的 NLP 预处理流水线中所做的事情。
- en: 2.6 Vectors of tokens
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.6 标记的向量
- en: Now that you have broken your text into tokens of meaning, what do you do with
    them? How can you convert them to numbers that will be meaningful to the machine?
    The simplest most basic thing to do would be to detect whether a particular token
    you are interested in was present or not. You could hard-code the logic to check
    for important tokens, called a *keywords*.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经把你的文本分解成了有意义的标记，你会怎么处理它们呢？你怎样才能把它们转换成对机器有意义的数字？最简单、最基本的事情是检测你感兴趣的特定标记是否存在。你可以硬编码逻辑来检查重要标记，称为*关键词*。
- en: 'This might work well for your greeting intent recognizer in chapter 1\. Our
    greeting intent recognizer at the end of chapter 1 looked for words like "Hi"
    and "Hello" at the beginning of a text string. Your new tokenized text would help
    you detect the presence or absence of words such as "Hi" and "Hello" without getting
    confused by words like "Hiking" and "Hell." With your new tokenizer in place,
    your NLP pipeline wouldn’t misinterpret the word "Hiking" as the greeting "Hi
    king":'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于你的问候意图识别器可能效果很好。我们在第 1 章末尾的问候意图识别器寻找文本字符串开头的词语，如 "Hi" 和 "Hello"。你的新标记化文本将帮助你检测诸如
    "Hi" 和 "Hello" 等词语的存在或缺失，而不会因为 "Hiking" 和 "Hell" 等词语而混淆。有了你的新分词器，你的 NLP 流水线将不会把单词
    "Hiking" 错误地解释为问候 "Hi king"。
- en: '[PRE15]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: So tokenization can help you reduce the number of false positives in your simple
    intent recognition pipeline that looks for the presence of greeting words. This
    is often called keyword detection, because your vocabulary of words is limited
    to a set of words you think are important. However, it’s quite cumbersome to have
    to think of all the words that might appear in a greeting in order to recognize
    them all, including slang, misspellngs and typoos. And creating a for loop to
    iterate through them all would be inefficient. We can use the math of linear algebra
    and the vectorized operations of `numpy` to speed this process up.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，标记化可以帮助您减少简单意图识别管道中的假阳性数量，该管道寻找问候词的存在。这通常被称为关键词检测，因为您的词汇表限于您认为重要的一组词汇。但是，想到可能出现在问候语中的所有单词，包括俚语、拼写错误和错别字，这相当繁琐。创建一个for循环来遍历它们将是低效的。我们可以使用线性代数的数学和`numpy`的向量化操作来加速此过程。
- en: 'In order to detect tokens efficiently you will want to use three new tricks:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 为了高效地检测标记，您将需要使用三种新技巧：
- en: matrix and vector representations of documents
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文档的矩阵和向量表示
- en: vectorized operations in numpy
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: numpy中的向量化操作
- en: indexing of discrete vectors
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 离散向量的索引
- en: You’ll first learn the most basic, direct, raw and lossless way to represent
    words as a matrix, one-hot encoding.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 您将首先学习将单词表示为矩阵的最基本、直接、原始和无损的方法，即单独独热编码。
- en: 2.6.1 One-hot Vectors
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.6.1 单独独热向量
- en: Now that you’ve successfully split your document into the kinds of words you
    want, you’re ready to create vectors out of them. Vectors of numbers are what
    we need to do the math or processing of NL*P* on natural language text.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已成功将文档分割成所需的单词类型，您可以准备将它们转换成向量。数字向量是我们在自然语言文本上进行数学或处理所需要的。
- en: '[PRE16]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In this representation of this two-sentence quote, each row is a vector representation
    of a single word from the text. The table has the 15 columns because this is the
    number of unique words in your vocabulary. The table has 18 rows, one for each
    word in the document. A "1" in a column indicates a vocabulary word that was present
    at that position in the document.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个表示两句引用的表格中，每一行都是文本中单个词的向量表示。该表格有15列，因为这是您的词汇表中唯一单词的数量。该表格有18行，每个单词在文档中占据一行。列中的“1”表示该位置在文档中存在一个词汇单词。
- en: You can "read" a one-hot encoded (vectorized) text from top to bottom. You can
    tell that the first word in the text was the word "There’s", because the `1` on
    the first row is positioned under the column label "There’s". The next three rows
    (row indexes 1, 2, and 3) are blank, because we’ve truncated the table on the
    right to help it fit on the page. The fifth row of the text, with the 0-offset
    index number of `4` shows us that the fifth word in the text was the word "adequate",
    because there’s a `1` in that column.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从上到下“阅读”一个独热编码（向量化）的文本。您可以看到文本中的第一个词是“有”，因为第一行下面的`1`位于“有”的列标签下。接下来的三行（行索引1、2和3）为空白，因为我们在右边截断了表格以便它适合在页面上显示。文本的第五行，带有偏移索引号`4`，向我们展示了文本中的第五个词是“足够”，因为在该列中有一个`1`。
- en: One-hot vectors are super-sparse, containing only one nonzero value in each
    row vector. For display, this code replaces the `0’s with empty strings (’'`),
    to make it easier to read. But the code did not actually alter the `DataFrame`
    of data you are processing in your NLP pipeline. The Python code above was just
    to to make it easier to read, so you can see that it looks a bit like a player
    piano paper roll, or maybe a music box drum.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 单独独热向量非常稀疏，每个行向量中只包含一个非零值。为了显示，此代码将`0`替换为空字符串（’'`），以便更容易阅读。但是该代码实际上并没有改变您在NLP管道中处理的数据的`DataFrame`。上面的Python代码只是为了让阅读更轻松，这样您就可以看到它看起来有点像播放器钢琴卷轴，或者可能是音乐盒鼓。
- en: The Pandas `DataFrame` made this output a little easier to read and interpret.
    The `DataFrame.columns` keep track of labels for each column. This allows you
    to label each column in your table with a string, such as the token or word it
    represents. A `DataFrame` can also keep track of labels for each row in an the
    `DataFrame.index`, for speedy lookup.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas的`DataFrame`使得这个输出稍微容易阅读和解释。`DataFrame.columns`跟踪每列的标签。这使您可以使用字符串为表格中的每一列标记，例如其代表的标记或单词。`DataFrame`还可以在`DataFrame.index`中跟踪每行的标签，以便快速查找。
- en: Important
  id: totrans-181
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 重要提示
- en: Don’t add strings to any `DataFrame` you intend to use in your machine learning
    pipeline. The purpose of a tokenizer and vectorizer, like this one-hot vectorizer,
    is to create a numerical array that your NLP pipeline can do math on. You can’t
    do math on strings.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 不要向任何你打算在你的机器学习流水线中使用的`DataFrame`添加字符串。标记器和向量化器的目的，比如这个一热向量化器，是创建一个你的NLP流水线可以对其进行数学运算的数值数组。你不能对字符串进行数学运算。
- en: 'Each row of the table is a binary row vector, and you can see why it’s also
    called a one-hot vector: all but one of the positions (columns) in a row are `0`
    or blank. Only one column, or position in the vector is "hot" ("1"). A one (`1`)
    means on, or hot. A zero (`0`) mean off, or absent.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 表的每一行都是一个二进制行向量，你可以看到为什么它也被称为一热向量：除了一行中的一个位置（列）之外，其他位置都是`0`或空白。只有一个列，或者向量中的一个位置是“热”的（“1”）。一个`1`表示打开，或热。一个`0`表示关闭，或缺失。
- en: One nice feature of this vector representation of words and tabular representation
    of documents is that no information is lost. The exact sequence of tokens is encoded
    in the order of the one-hot vectors in the table representing a document. As long
    as you keep track of which words are indicated by which column, you can reconstruct
    the original sequence of tokens from this table of one-hot vectors perfectly.
    And this reconstruction process is 100% accurate even though your tokenizer was
    only 90% accurate at generating the tokens you thought would be useful. As a result,
    one-hot word vectors like this are typically used in neural nets, sequence-to-sequence
    language models, and generative language models. They are a good choice for any
    model or NLP pipeline that needs to retain all the meaning inherent in the original
    text.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种词向量表示和文档表格表示的一个很好的特点是没有信息丢失。令人满意的是，令牌的确切序列以表示文档的一热向量的顺序编码在表中。只要你记住哪些词由哪列表示，你就可以完美地从这个一热向量表中重构出原始的令牌序列。即使你的标记器只有90%的准确率生成你认为有用的令牌，这个重构过程也是100%准确的。因此，像这样的一热词向量通常用于神经网络、序列到序列语言模型和生成语言模型。它们是任何需要保留原始文本中所有含义的模型或NLP流水线的良好选择。
- en: Tip
  id: totrans-185
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: The one-hot encoder (vectorizer) did not discard any information from the text,
    but our tokenizer did. Our regular expression tokenizer discarded the whitespace
    characters (`\s`) that sometimes occur between words. So you could not perfectly
    reconstruct the original text with a *detokenizer*. Tokenizers like spaCy, however,
    keep track of these whitespace characters and can in fact detokenize a sequence
    of tokens perfectly. SpaCy was named for this feature of accurately accounting
    for white-**space** efficiently and accurately.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 一热编码器（向量化器）没有从文本中丢弃任何信息，但我们的标记器丢弃了。我们的正则表达式标记器丢弃了有时出现在单词之间的空白字符（`\s`）。因此，你不能用一个*解标记器*完美地重构原始文本。然而，像spaCy这样的标记器跟踪这些空格字符，并且实际上可以完美地解标记一个令牌序列。spaCy是因为准确高效地和准确地记录空格的这一特性而命名的。
- en: This sequence of one-hot vectors is like a digital recording of the original
    text. If you squint hard enough you might be able to imagine that the matrix of
    ones and zeros above is a player piano roll.^([[20](#_footnotedef_20 "View footnote.")]).
    Or maybe it’s the bumps on the metal drum of a music box.^([[21](#_footnotedef_21
    "View footnote.")]) The vocabulary key at the top tells the machine which "note"
    or word to play for each row in the sequence of words or piano music like in figure
    [2.2](#figure-player-piano-roll).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这一系列一热向量就像原始文本的数字录音。如果你看得够仔细，你可能会想象上面的一和零的矩阵是一个玩家钢琴卷。[^20] 或者它可能是音乐盒金属鼓上的凹痕。[^21]
    顶部的词汇表告诉机器在词序列或钢琴音乐中的每一行中播放哪个“音符”或单词，就像图[2.2](#figure-player-piano-roll)中一样。
- en: Figure 2.2 Player piano roll
  id: totrans-188
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.2 玩家钢琴卷
- en: '![piano roll](images/piano_roll.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![钢琴卷](images/piano_roll.png)'
- en: Unlike a player-piano or a music box, your mechanical word recorder and player
    is only allowed to use one "finger" at a time. It can only play one "note" or
    word at a time. It’s one-hot. And there is no variation in the spacing of the
    words.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 与钢琴卷或音乐盒不同，你的机械词记录器和播放器只允许一次使用一个“手指”。它只能一次播放一个“音符”或一个词。它是一热的。而且单词之间的间距没有变化。
- en: The important thing is that you’ve turned a sentence of natural language words
    into a sequence of numbers, or vectors. Now you can have the computer read and
    do math on the vectors just like any other vector or list of numbers. This allows
    your vectors to be input into any natural language processing pipeline that requires
    this kind of vector. The Deep Learning pipelines of chapter 5 through 10 typically
    require this representation, because they can be designed to extract "features"
    of meaning from these raw representations of text. And Deep Learning pipelines
    can generate text from numerical representations of meaning. So the stream of
    words emanating from your NLG pipelines in later chapters will often be represented
    by streams of one-hot encoded vectors, just like a player piano might play a song
    for a less artificial audience in West World.^([[22](#_footnotedef_22 "View footnote.")])
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，你已经将自然语言单词的句子转换为一系列数字，或向量。现在，你可以让计算机像处理任何其他向量或数字列表一样阅读和对这些向量进行数学运算。这使得你的向量可以被输入到任何需要这种向量的自然语言处理流水线中。第
    5 至 10 章的深度学习流水线通常需要这种表示形式，因为它们可以被设计为从这些原始文本表示中提取意义的“特征”。而且深度学习流水线可以从意义的数值表示生成文本。因此，从后面章节的你的自然语言生成流水线中流出的单词流通常将被表示为一系列
    one-hot 编码的向量，就像自动钢琴可能为西部世界中不那么人工的观众演奏一首歌一样。
- en: Now all you need to do is figure out how to build a "player piano" that can
    *understand* and combine those word vectors in new ways. Ultimately, you’d like
    your chatbot or NLP pipeline to play us a song, or say something, you haven’t
    heard before. You’ll get to do that in chapters 9 and 10 when you learn about
    recurrent neural networks that are effective for sequences of one-hot encoded
    tokens like this.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你所需要做的就是想出如何构建一个能够*理解*并以新的方式组合这些词向量的“自动钢琴”。最终，你希望你的聊天机器人或自然语言处理流水线能为我们演奏一首歌，或者说出一些你之前没听过的话。在第
    9 章和第 10 章，当你学习到适用于像这样的 one-hot 编码令牌序列的递归神经网络时，你将有机会做到这一点。
- en: 'This representation of a sentence in one-hot word vectors retains all the detail,
    grammar, and order of the original sentence. And you have successfully turned
    words into numbers that a computer can "understand." They are also a particular
    kind of number that computers like a lot: binary numbers. But this is a big table
    for a short sentence. If you think about it, you have expanded the file size that
    would be required to store your document. For a long document this might not be
    practical.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这种用 one-hot 词向量表示的句子保留了原始句子的所有细节、语法和顺序。而且你已经成功地将单词转换为计算机能“理解”的数字。它们也是一种计算机非常喜欢的特殊类型的数字：二进制数字。但对于一个简短的句子来说，这是一个很大的表格。如果你仔细想一想，你已经扩展了存储文档所需的文件大小。对于一个长文档来说，这可能不实用。
- en: How big is this **lossless** numerical representation of your collection of
    documents? Your vocabulary size (the length of the vectors) would get huge. The
    English language contains at least 20,000 common words, millions if you include
    names and other proper nouns. And your one-hot vector representation requires
    a new table (matrix) for every document you want to process. This is almost like
    a raw "image" of your document. If you have done any image processing, you know
    that you need to do dimension reduction if you want to extract useful information
    from the data.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 你的文档集的这种**无损**数值表示有多大？你的词汇量（向量长度）会变得非常庞大。英语至少包含 20,000 个常见单词，如果包括名称和其他专有名词，则有数百万个。而且你的
    one-hot 向量表示需要为你想要处理的每个文档创建一个新的表格（矩阵）。这几乎就像你的文档的原始“图像”。如果你做过任何图像处理，你就知道如果想从数据中提取有用信息，你需要进行维度缩减。
- en: Let’s run through the math to give you an appreciation for just how big and
    unwieldy these "piano rolls" are. In most cases, the vocabulary of tokens you’ll
    use in an NLP pipeline will be much more than 10,000 or 20,000 tokens. Sometimes
    it can be hundreds of thousands or even millions of tokens. Let’s assume you have
    a million tokens in your NLP pipeline vocabulary. And let’s say you have a meager
    3000 books with 3500 sentences each and 15 words per sentence — reasonable averages
    for short books. That’s a whole lot of big tables (matrices), one for each book.
    That would use 157.5 terabytes. You probably couldn’t even store that on disk.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一些数学来让你了解一下这些“钢琴卷”有多么庞大和难以控制。在大多数情况下，你在自然语言处理流水线中使用的标记词汇表将远远超过10,000或20,000个标记。有时甚至可以是数十万甚至数百万个标记。假设你的自然语言处理流水线词汇表中有一百万个标记。然后假设你有3000本书，每本书有3500个句子，每个句子有15个单词——这些都是短书的合理平均值。那就是很多很大的表（矩阵），每本书一个。那将使用157.5TB。你可能甚至无法把它存储到磁盘上。
- en: That is more than a million million bytes, even if you are super-efficient and
    use only one byte for each number in your matrix. At one byte per cell, you would
    need nearly 20 terabytes of storage for a small bookshelf of books processed this
    way. Fortunately you do not ever use this data structure for storing documents.
    You only use it temporarily, in RAM, while you are processing documents one word
    at a time.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 哪怕你非常高效，并且在矩阵中每个数字只使用一个字节，那也是超过一百万亿字节。以每个单元一个字节的计算，你需要将近20TB的存储空间来存放用这种方式处理的一小本书架上的书籍。幸运的是，你不会永久使用这种数据结构来存储文档。你只是在处理文档时临时使用它，存储在内存中，一次处理一个单词。
- en: So storing all those zeros, and recording the order of the words in all your
    documents does not make much sense. It is not practical. And it’s not very useful.
    Your data structure hasn’t abstracted or generalized from the natural language
    text. An NLP pipeline like this doesn’t yet do any real feature extraction or
    dimension reduction to help your machine learning work well in the real world.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，存储所有这些零，并记录所有文档中单词的顺序并没有太多意义。这不实际。也不是很有用。你的数据结构没有从自然语言文本中抽象或泛化出来。这样的自然语言处理流水线在现实世界中并没有做任何真正的特征提取或维度缩减，以帮助你的机器学习在实际情况下运行良好。
- en: What you really want to do is compress the meaning of a document down to its
    essence. You would like to compress your document down to a single vector rather
    than a big table. And you are willing to give up perfect "recall." You just want
    to capture most of the meaning (information) in a document, not all of it.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 你真正想做的是将文档的含义压缩到其本质中。你想将文档压缩成一个单一的向量，而不是一个大表。而且你愿意放弃完美的“回忆”。你只是想捕捉文档中的大部分含义（信息），而不是全部。
- en: Your regular expression tokenizer and one-hot vectors are great for creating
    *reverse indexes*. Just like the index at the end of a textbook, your matrix of
    one-hot vectors can be used to quickly find all the strings or documents where
    a particular word was used at least once. So the tools you’ve learned so far can
    be used as a foundation for a personalized search engine. However, you saw that
    search and information retrieval was only one of many many applications of NLP.
    To solve the more advanced problems you will need a more advanced tokenizer and
    more sophisticated vector representations of text. The Python package `spaCy`
    is that state-of-the-art tokenizer you are looking for.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 你的正则表达式分词器和独热向量对于创建*反向索引*非常有效。就像教科书末尾的索引一样，你的独热向量矩阵可以用来快速找到所有至少使用过一次特定单词的字符串或文档。因此，到目前为止你学到的工具可以作为个性化搜索引擎的基础。然而，你看到了搜索和信息检索只是自然语言处理的许多许多应用之一。要解决更高级的问题，你将需要更高级的分词器和更复杂的文本向量表示。Python包`spaCy`就是你在寻找的最先进的分词器。
- en: 2.6.2 SpaCy
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.6.2 SpaCy
- en: Maybe you don’t want your regular expression tokenizer to keep contractions
    together. Perhaps you’d like to recognize the word "isn’t" as two separate words,
    "is" and "n’t". That way you could consolidate the synonyms "n’t" and "not" into
    a single token. This way your NLP pipeline would understand "the ice cream isn’t
    bad" to mean the same thing as "the ice cream is not bad". For some applications,
    such as full text search, intent recognition, and sentiment analysis, you want
    to be able to **uncontract** or expand contractions like this. By splitting contractions,
    you can use synonym substitution or contraction expansion to improve the recall
    of your search engine and the accuracy of your sentiment analysis.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 也许您不希望您的正则表达式标记器将缩写词保持在一起。也许您想要将单词"isn’t"识别为两个单独的单词，"is"和"n’t"。这样，您可以将"n’t"和"not"这两个同义词合并为一个标记。这样，您的NLP管道就能理解"the
    ice cream isn’t bad"与"the ice cream is not bad"的含义相同。对于一些应用程序，例如全文搜索、意图识别和情感分析，您希望能够**展开**或扩展这样的缩写词。通过拆分缩写词，您可以使用同义词替换或扩展缩写词来提高搜索引擎的召回率和情感分析的准确性。
- en: Important
  id: totrans-202
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 重要提示
- en: We’ll discuss case folding, stemming, lemmatization, and synonym substitution
    later in this chapter. Be careful about using these techniques for applications
    such as authorship attribution, style transfer, or text fingerprinting. You want
    your authorship attribution or style-transfer pipeline to stay true to the author’s
    writing style and the exact spelling of words that they use.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将在本章后面讨论大小写折叠、词干提取、词形还原和同义词替换。在应用程序（如作者归因、风格转移或文本指纹）中使用这些技术时要小心。您希望您的作者归因或风格转移管道保持忠于作者的写作风格和他们使用的确切拼写。 '
- en: SpaCy integrates a tokenizer directly into its state-of-the-art NLU pipeline.
    In fact the name "spaCy" is based on the word "space", as in the separator used
    in Western languages to separate words. And spaCy adds a lot of additional *tags*
    to tokens at the same time that it is applying rules to split tokens apart. So
    spaCy is often the first and last tokenizer you’ll ever need to use.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: SpaCy将标记器直接集成到其最先进的NLU管道中。实际上，"spaCy"这个名字是基于单词"space"，就像西方语言中用于分隔单词的分隔符一样。而且，spaCy在应用规则拆分标记的同时，还向标记添加了许多其他*标签*。因此，spaCy通常是您唯一需要使用的第一个和最后一个标记器。
- en: 'Let’s see how spaCy handles our collection of deep thinker quotes. First you’re
    going to use a thin wrapper for the spacy.load function so that your NLP pipeline
    is *idempotent*. An idempotent pipeline can be run multiple times and achieve
    the same results each time:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看SpaCy如何处理我们的一系列深思者引用。首先，您将使用一个对spacy.load函数的薄包装器，以便您的NLP管道是*幂等*的。幂等的管道可以多次运行，并每次都达到相同的结果：
- en: '[PRE17]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Now that you’ve downloaded the small SpaCy language model and loaded it into
    memory (RAM), you can use it to tokenize and tag any text string. This will create
    a new SpaCy `Doc` object that contains whatever SpaCy was able to understand about
    the text using that language model.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经下载了小型的SpaCy语言模型并将其加载到内存（RAM）中，您可以使用它来标记和标记任何文本字符串。这将创建一个新的SpaCy `Doc`对象，其中包含SpaCy使用该语言模型理解文本的内容。
- en: '[PRE18]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'SpaCy has read and parsed your text to split it into tokens. The `Doc` object
    contains a sequence of `Token` objects which should each be a small packet of
    thought or meaning (typically words). See if these tokens are what you expected:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: SpaCy已经阅读并解析了您的文本，将其分割为标记。`Doc`对象包含一系列`Token`对象，每个对象应该是一个小的思想或含义包（通常是单词）。看看这些标记是否是您预期的：
- en: '[PRE19]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Spacy is doing a lot more under the hood than just splitting your text into
    tokens. That small language model you downloaded is also identifying sentence
    breaks with some **sentence boundary detection** rules. A language model is a
    collection of regular expressions and finite state automata (rules). These rules
    are a lot like the grammar and spelling rules you learned in English class. They
    are used in the algorithms that tokenize and label your words with useful things
    like their part of speech and their position in a syntax tree of relationships
    between words. Take another look at that sendence diagram
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: Spacy在幕后做的远不止将您的文本分割成标记。您下载的那个小语言模型还通过一些**句子边界检测**规则来识别句子分隔符。语言模型是一组正则表达式和有限状态自动机（规则）。这些规则很像你在英语课上学到的语法和拼写规则。它们用于将单词标记为有用的东西，如它们的词性和它们在单词之间的语法树关系中的位置。再仔细看看那个句子图表
- en: '[PRE20]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The diagram should appear *inline* in a jupyter notebook or in a separate window
    if you ran this code in `ipython` (`jupyter-console`). If you launched the displacy
    web server you can see the diagram by browsing to `localhost` (`127.0.0.1`) on
    port 5000 (`[http://127.0.0.1:5000](.html)`). You should see a sentence diagram
    that might be more correct than what you could produce in school:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图表应该*内联*显示在jupyter笔记本中，或者如果在`ipython`（`jupyter-console`）中运行此代码，则应在单独的窗口中显示。如果启动了displacy
    web服务器，您可以通过在端口5000上浏览到localhost（`127.0.0.1`）（`[http://127.0.0.1:5000](.html)`）来查看图表。您应该看到一个句子图表，这可能比您在学校能做的更正确：
- en: '![there such thing](images/there-such-thing.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![there such thing](images/there-such-thing.png)'
- en: You can see that spaCy does a lot more than simply separate text into tokens.
    It identifies sentence boundaries to automatically segment your text into sentences.
    And it tags tokens with various attributes like their part of speech (PoS) and
    even their role within the syntax of a sentence. You can see the lemmas displayed
    by `displacy` beneath the literal text for each token.^([[23](#_footnotedef_23
    "View footnote.")]) Later in the chapter you’ll learn how lemmatization and case
    folding and other vocabulary **compression** approaches can be helpful for some
    applications.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，spaCy做的远不止将文本分割成令牌那么简单。它会识别句子边界，自动将文本分段为句子。它还使用各种属性标记令牌，例如它们的词性（PoS）甚至它们在句子语法中的角色。你可以在`displacy`下看到显示的词元。^([[23](#_footnotedef_23
    "查看脚注。")]) 本章后面你将了解词形还原、大小写折叠和其他词汇**压缩**方法对某些应用有何帮助。
- en: So spaCy seems pretty great in terms of accuracy and some "batteries included"
    features, such as all those token tags for lemmas and dependencies. What about
    speed?
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，从准确性和一些“内置电池”功能（例如词元和依赖项的所有令牌标记）来看，spaCy似乎相当不错。速度如何？
- en: 2.6.3 Tokenizer race
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.6.3 令牌化竞赛
- en: 'SpaCy can parse the AsciiDoc text for a chapter in this book in about 5 seconds.
    First download the AsciiDoc text file for this chapter:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: SpaCy可以在大约5秒钟内解析本书一章的AsciiDoc文本。首先下载本章的AsciiDoc文本文件：
- en: '[PRE21]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: There were about 170 thousand unicode characters in this AsciiDoc text file
    where I wrote this sentence that you are reading right now. What does that mean
    in terms of words-per-second, the standard benchmark for tokenizer speed?
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在我写下您正在阅读的这个句子的AsciiDoc文本文件中，大约有170,000个Unicode字符。从词数每秒来看，这意味着什么，这是令牌化速度的标准基准？
- en: '[PRE22]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: That’s nearly 5 seconds for about 150,000 characters or 34,000 words of English
    and Python text or about 7000 words per second.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大约15万个字符或英语和Python文本中的约34,000个单词，或每秒约7,000个单词的标准，这几乎需要5秒钟。
- en: That may seem fast enough for you on your personal projects. But on a typical
    medical records summarization project for a commercial business you might need
    to process hundreds of large documents per minute. That’s a couple documents per
    second. And if each document contains the amount of text that you find in this
    book (nearly half a million tokens), that’s almost a million tokens per second.
    Latency on a medical record summarization pipeline can be a critical metric for
    the project. For example, on one project it took 5 days to process 10,000 patient
    medical records using SpaCy as the tokenizer.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 对于您的个人项目，这可能已经足够快了。但是在商业业务的典型医疗记录摘要项目中，您可能需要每分钟处理数百个大型文档。这是每秒处理几个文档。如果每个文档包含本书中的文本量（近一百万个令牌），那么每秒几乎可以达到一百万个令牌。医疗记录摘要管道的延迟可以是项目的关键指标。例如，在一个项目中，使用SpaCy作为分词器处理10,000份患者医疗记录花费了5天的时间。
- en: 'If you need to speed up your tokenzer, one option is to disable the tagging
    features of the spaCy pipeline that you do not need for your application:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要加速您的令牌化器，一种选择是禁用spaCy管道中您的应用程序不需要的标记功能：
- en: '[PRE23]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'You can disable the pipeline elements you don’t need to speed up the tokenizer:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以禁用不需要的管道元素以加速分词器：
- en: '`tok2vec`: word embeddings'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tok2vec`: 词嵌入'
- en: '`tagger`: part-of-speech (`.pos` and `.pos_`)'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tagger`: 词性（`.pos`和`.pos_`）'
- en: '`parser`: syntax tree role'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`parser`: 语法树角色'
- en: '`attribute_ruler`: fine-grained POS and other tags'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attribute_ruler`: 精细的词性和其他标记'
- en: '`lemmatizer`: lemma tagger'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lemmatizer`: 词元标记器'
- en: '`ner`: named entity recognition tagger'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ner`: 命名实体识别标记器'
- en: 'NLTK’s `word_tokenize` method is often used as the pace setter in tokenizer
    benchmark speed comparisons:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在令牌化器速度比较中，NLTK的`word_tokenize`方法通常用作速度的基准：
- en: '[PRE24]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Could it be that you found a winner for the tokenizer race? Not so fast. Your
    regular expression tokenizer has some pretty simple rules, so it should run pretty
    fast as well:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 你难道发现了分词器竞赛的赢家吗？不要太快。你的正则表达式分词器有一些非常简单的规则，因此也应该运行得非常快：
- en: '[PRE25]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Now that’s not too surprising. Python compiles and runs regular expressions
    very efficiently within low level C routines. In addition to speed, regular expressions
    and the NLTK tokenizer are often used in academic papers. That help others like
    you reproduce their results exactly. So if you’re just trying to reproduce someone
    else’s work, make sure you use their tokenizer, whether it’s NLTK, regular expressions,
    or a particular version of spaCy. In this book you are just trying to learn how
    things work, so we haven’t bothered to keep track of the particular versions of
    spaCy and NLTK that we used. But you may need to do this for yourself if you are
    sharing your results with others doing NLP research.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这不太令人惊讶。Python在低级别的C例程中编译和运行正则表达式非常高效。除了速度外，正则表达式和NLTK令牌化器经常用于学术论文。这帮助像你这样的人精确复现他们的结果。因此，如果你尝试复制别人的工作，请确保使用他们的分词器，无论是NLTK、正则表达式还是spaCy的特定版本。在本书中，你只是尝试学习事物的工作原理，所以我们没有费心追踪我们使用的spaCy和NLTK的特定版本。但是如果你正在与其他进行NLP研究的人分享你的结果，你可能需要为自己做这些事情。
- en: Tip
  id: totrans-238
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: Use a regular expression tokenizer when speed is more import than accuracy or
    when others will try to reproduce your results. ^([[24](#_footnotedef_24 "View
    footnote.")]) If you do not need the additional tags that spaCy provides, your
    tokenizer doesn’t need to waste time trying to process the grammar and meaning
    of the words to create those tags. And each time you use a regular expression
    in the `re` or `regex` packages, a compiled and optimized version of it is cached
    in RAM. So there’s usually no need to *precompile* (using `re.compile()`) your
    regexes.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 当速度比准确度更重要或其他人将尝试复制你的结果时，请使用正则表达式分词器。如果你不需要spaCy提供的额外标记，你的分词器不需要浪费时间来处理单词的语法和含义来创建这些标记。每次使用`re`或`regex`包中的正则表达式时，它的已编译和优化版本都会缓存在RAM中。因此，通常没有必要使用`re.compile()`预编译你的正则表达式。
- en: 2.7 Wordpiece tokenizers
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.7 单词片段分词器
- en: It probably felt natural to think of words as indivisible atomic chunks of meaning
    and thought. However, you did find some words that didn’t clearly split on spaces
    or punctuation. And many compound words or named entities that you’d like to keep
    together have spaces within them. So it can help to dig a little deeper and think
    about the statistics of what makes a word. Think about how we can build up words
    from neighboring characters instead of cleaving text at separators such as spaces
    and punctuation.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单词来说，将其视为不可分割的原子意义和思想是很自然的。然而，你可能会发现一些单词并不在空格或标点符号上清晰划分。而且，许多复合词或专有名词内部有空格，如果想要它们保持在一起，需要更深入地研究单词的统计特征。考虑如何通过邻近的字符来构建单词，而不是在分隔符，如空格和标点符号处切割文本。
- en: 2.7.1 Clumping characters into sentence pieces
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.7.1 按块组合字符成句子片段
- en: Instead of thinking about breaking strings up into tokens, your tokenizer can
    look for characters that are used a lot right next to each other, such as "i"
    before "e". You can pair up characters and sequences of characters that belong
    together.^([[25](#_footnotedef_25 "View footnote.")]) These clumps of characters
    can become your tokens. An NLP pipeline only pays attention to the statistics
    of tokens. And hopefully these statistics will line up with our expectations for
    what a word is.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 与其考虑将字符串分解为标记，不如让你的分词器寻找紧密相邻使用的字符，比如"i"在"e"的前面。你可以组合起来属于一起的字符和字符序列。这些字符簇可以成为你的标记。NLP管道只关注标记的统计信息。希望这些统计信息能符合我们对单词的期望。
- en: Many of these character sequences will be whole words, or even compound words,
    but many will be pieces of words. In fact, all *subword tokenizers* maintain a
    token within the vocabulary for every individual character in your vocabulary.
    This means it never needs to use an OOV (Out-of-Vocabulary) token, as long as
    any new text doesn’t contain any new characters it hasn’t seen before. Subword
    tokenizers attempt to optimally clump characters together to create tokens. Using
    the statistics of character n-gram counts it’s possible for these algorithms to
    identify wordpieces and even sentence pieces that make good tokens.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这些字符序列中的许多将是完整的单词，甚至是复合词，但许多将是单词的部分。事实上，所有*子词标记器*都在词汇表中为您的每个单词的每个字符保留一个标记。这意味着只要新的文本不包含它之前没有见过的新字符，它就永远不需要使用一个OOV（词汇外）标记。子词标记器尝试将字符最佳地聚集在一起以创建令牌。利用字符n-gram计数的统计数据，这些算法可以识别出可作为令牌的单词片段，甚至是句子片段。
- en: It may seem odd to identify words by clumping characters. But to a machine,
    the only obvious, consistent division between elements of meaning in a text is
    the boundary between bytes or characters. And the frequency with which characters
    are used together can help the machine identify the meaning associated with subword
    tokens such as individual syllables or parts of compound words.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将字符聚集在一起来识别单词可能看起来很奇怪。但对于机器来说，文本中意义元素之间唯一明显、一致的分界线就是字节或字符之间的边界。字符频繁一起使用的频率可以帮助机器识别与子词标记（例如单个音节或复合词的部分）相关联的含义。
- en: In English, even individual letters have subtle emotion (sentiment) and meaning
    (semantics) associated with them. However, there are only 26 unique letters in
    the English language. That doesn’t leave room for individual letters to *specialize*
    on any one topic or emotion. Nonetheless savvy marketers know that some letters
    are cooler than others. Brands will try to portray themselves as technologically
    advanced by choosing names with exotic letters like "Q" and "X" or "Z". This also
    helps with SEO (Search Engine Optimization) because rarer letters are more easily
    found among the sea of possible company and product names. Your NLP pipeline will
    pick up all these hints of meaning, connotation, and intent. Your token counters
    will provide the machine with the statistics it needs to infer the meaning of
    clumps of letters that are used together often.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在英语中，即使是单个字母也带有微妙的情感（情绪）和含义（语义）。然而，在英语中只有26个独特的字母。这并不给单个字母在任何一个主题或情感上*专门化*留下空间。尽管如此，精明的营销人员知道，有些字母比其他字母更酷。品牌将尝试通过选择带有像“Q”和“X”或“Z”这样的异国字母的名称来展示自己技术先进。这也有助于SEO（搜索引擎优化），因为较少见的字母在可能的公司和产品名称中更容易被发现。你的NLP管道将捕捉到所有这些含义、内涵和意图的暗示。你的标记计数器将为机器提供其需要推断经常一起使用的字母簇的含义的统计数据。
- en: The only disadvantage for subword tokenizers is the fact that they must pass
    through your corpus of text many times before converging on an optimal vocabulary
    and tokenizer. A subword tokenizer has to be trained or fit to your text just
    like a CountVectorizer. In fact you’ll use a CountVectorizer in the next section
    to see how subword tokenizers work.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 子词标记器唯一的劣势是它们必须多次通过您的文本语料库，才能收敛到最佳词汇表和分词器。子词标记器必须像CountVectorizer一样被训练或适合您的文本。事实上，在下一节中，您将使用CountVectorizer来了解子词标记器的工作原理。
- en: 'There are two main approaches to subword tokenization: BPE (Byte-Pair Encoding)
    and Wordpiece tokenization.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 子词分词有两种主要方法：BPE（字节对编码）和Wordpiece分词。
- en: BPE
  id: totrans-249
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: BPE
- en: In the previous edition of the book we insisted that words were the smallest
    unit of meaning in English that you need consider. With the rise of Transformers
    and other deep learning models that use BPE and similar techniques, we’ve changed
    our minds.^([[26](#_footnotedef_26 "View footnote.")]) Character-based subword
    tokenizers have proven to be more versatile and robust for most NLP problems.
    By building up a vocabulary from building blocks of Unicode multi-byte characters
    you can construct a vocabulary that can handle every possible natural language
    string you’ll ever see, all with a vocabulary of as few as 50,000 tokens.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的上一版中，我们坚持认为单词是您需要考虑的英语中最小的含义单位。随着Transformer和其他使用BPE和类似技术的深度学习模型的兴起，我们改变了主意。基于字符的子词标记器已被证明对于大多数NLP问题更加灵活和强大。通过从Unicode多字节字符的构建块中构建词汇表，您可以构建一个可以处理您将要看到的每一个自然语言字符串的词汇表，词汇表中的令牌数量只需为50,000个即可。
- en: You may think that Unicode characters are the smallest packet of meaning in
    natural language text. To a human, maybe, but to a machine, no way. Just as the
    BPE name suggests, characters don’t have to be your fundamental atom of meaning
    for your *base vocabulary*. You can split characters into 8-bit bytes. GPT-2 uses
    a byte-level BPE tokenizer to naturally compose all the unicode characters you
    need from the bytes that make them up. Though some special rules are required
    to handle unicode punctuation within a byte-based vocabulary, no other adjustment
    to the character-based BPE algorithm is required. A byte-level BPE tokenizer allows
    you to represent all possible texts with a base (minimum) vocabulary size of 256
    tokens. The GPT-2 model can achieve state-of-the-art performance with it’s default
    BPE vocabulary of only 50,000 multibyte *merge tokens* plus 256 individual byte
    tokens.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能认为Unicode字符是自然语言文本中包含意义的最小单位。对于人类来说可能是这样，但对于机器来说绝对不是。正如BPE的名称所暗示的，字符不必是你的
    *基本词汇* 的基本意义单位。你可以将字符分割成8位字节。GPT-2 使用字节级BPE分词器从组成它们的字节中自然组合出所有你需要的Unicode字符。虽然在基于字节的词汇表中处理Unicode标点需要一些特殊规则，但不需要对基于字符的BPE算法进行其他调整。字节级BPE分词器允许你用256个最小令牌的基础词汇大小来表示所有可能的文本。GPT-2
    模型可以通过其默认的由50,000个多字节 *合并令牌* 加上256个个别字节令牌组成的BPE词汇表实现最先进的性能。
- en: You can think of the BPE (Byte Pair Encoding) tokenizer algorithm as a matchmaker
    in a social network of friends. BPE pairs up characters that appear next to each
    other a lot and appear to be "friends." It then creates a new token for these
    character combinations. BPE can then pair up the multi-character tokens whenever
    those token pairings are common in your text. And it keeps doing this until it
    has a many frequently used character sequences as you’ve allowed in your vocabulary
    size limit.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将BPE (Byte Pair Encoding) 分词器算法看作是社交网络中的媒人。BPE将经常出现在一起且看起来是“朋友”的字符配对在一起。然后为这些字符组合创建一个新的标记。BPE可以在文本中经常使用这些标记对时，将多字符标记配对。并且会一直这样做，直到在您的词汇限制大小中允许的频繁使用的字符序列有很多。
- en: BPE is transforming the way we think about natural language tokens. NLP engineers
    are finally letting the data do the talking. Statistical thinking is better than
    human intuition when building an NLP pipeline. A machine can see how *most* people
    use language. You are only familiar with what *you* mean when you use particular
    words or syllables. Transformers have now surpassed human readers and writers
    at some natural language understanding and generation tasks, including finding
    meaning in subword tokens.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: BPE正在改变我们对自然语言标记的看法。自然语言处理工程师终于让数据说话。在构建自然语言处理流程时，统计思维比人类直觉要好。机器可以看到 *大多数* 人如何使用语言。你只对你使用特定单词或音节时的含义熟悉。Transformers现在已经在某些自然语言理解和生成任务上超越了人类读者和作者，包括在子词标记中找到含义。
- en: One complication you have not yet encountered is the dilemma of what to do when
    you encounter a new word. In the previous examples, we just keep adding new words
    to our vocabulary. But in the real world your pipeline will have been trained
    on an initial corpus of documents that may or may not represent all the kinds
    of tokens it will ever see. If your initial corpus is missing some of the words
    that you encounter later on, you will not have a slot in your vocabulary to put
    your counts of that new word. So when you train you initial pipeline, you will
    always reserve a slot (dimension) to hold the counts of your *out-of-vocabulary*
    (OOV) tokens. So if your original set of documents did not contain the girl’s
    name "Aphra", all counts of the name Aphra would be lumped into the OOV dimension
    as counts of Amandine and other rare words.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 你尚未遇到的一个复杂情况是在遇到新词时该做何选择的困境。在先前的例子中，我们只是不断地将新词加入到我们的词汇表中。但在现实世界中，你的流程将会在一个初始文档语料库上进行训练，这个语料库可能或可能不代表它将来可能看到的所有类型的标记。如果你的初始语料库缺少后来遇到的一些单词，那么你将没有一个插槽来放置那个新单词的计数。因此，在训练初始流程时，你将始终保留一个插槽
    (维度) 来存放你的 *词汇外* (OOV) 标记的计数。因此，如果你最初的文档集不包含女孩名为"Aphra"，那么对名为Aphra的所有计数将被汇总到OOV维度中，作为Amandine和其他罕见单词的计数。
- en: 'To give Aphra equal representation in your vector space, you can use BPE. BPE
    breaks down rare words into smaller pieces to create a *periodic table* of the
    elements for natural language in your corpus. So, because "aphr" is a common english
    prefix, your BPE tokenizer would probably give Aphra **two** slots for her counts
    in your vocabulary: one for "aphr" and one for "a". Actually, you might actually
    discover that the vobcabulary slots are for " aphr" and "a ", because BPE keeps
    track of spaces no differently than any other character in your alphabet.^([[27](#_footnotedef_27
    "View footnote.")])'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 要在您的向量空间中给予Aphra平等的表示，您可以使用BPE。BPE将罕见的单词分解为更小的片段，以在语料库中为自然语言创建一个元素的*周期表*。所以，因为“aphr”是一个常见的英语前缀，您的BPE分词器可能会为她的词汇中的计数提供**两个**插槽：一个用于“aphr”和一个用于“a”。实际上，您可能会发现词汇槽位是“
    aphr”和“a”，因为BPE对待空格与字母表中的任何其他字符没有任何区别。
- en: BPE gives you multilingual flexibility to deal with Hebrew names like Aphra.
    And it give your pipeline robustness against common misspellings and typos, such
    as "aphradesiac." Every word, including minority 2-grams such as "African American",
    have representation in the voting system of BPE.^([[28](#_footnotedef_28 "View
    footnote.")]) Gone are the days of using the kluge of OOV (Out-of-Vocabulary)
    tokens to handle the rare quirks of human communication. Because of this, state
    of the art deep learning NLP pipelines such as transformers all use word piece
    tokenization similar to BPE.^([[29](#_footnotedef_29 "View footnote.")])
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: BPE为您提供了处理希伯来语名称（如Aphra）的多语言灵活性。它还使您的管道对常见的拼写错误和打字错误具有健壮性，例如“aphradesiac”。每个词，包括诸如“African
    American”之类的少数2-gram，在BPE的投票系统中都有代表。过去使用OOV（词汇外）令牌来处理人类沟通的罕见怪癖的日子一去不复返。由于这个原因，像transformers这样的最先进的深度学习NLP管道都使用类似于BPE的词片分词技术。
- en: BPE preserves some of the meaning of new words by using character tokens and
    word-piece tokens to spell out any unknown words or parts of words. For example,
    if "syzygy" is not in our vocabulary, we could represent it as the six tokens
    "s", "y", "z", "y", "g", and "y". Perhaps "smartz" could be represented as the
    two tokens "smart" and "z".
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: BPE通过使用字符令牌和词片令牌来拼写任何未知单词或单词部分，从而保留了一些新单词的含义。例如，如果“syzygy”不在我们的词汇表中，我们可以将其表示为六个令牌“s”，“y”，“z”，“y”，“g”和“y”。也许“smartz”可以表示为两个令牌“smart”和“z”。
- en: 'That sounds smart. Let’s see how it works on our text corpus:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 那听起来很聪明。让我们看看它在我们的文本语料库中是如何工作的：
- en: '[PRE26]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: You’ve created a `CountVectorizer` class that will tokenize the text into characters
    instead of words. And it will count token pairs (character 2-grams) in addition
    to single character tokens. These are the byte pairs in BPE encoding. Now you
    can examine your vocabulary to see what they look like.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 您创建了一个`CountVectorizer`类，它将文本令牌化为字符而不是单词。它还将计数令牌对（字符2-gram）以及单个字符令牌。这些是BPE编码中的字节对。现在您可以检查您的词汇表，看看它们是什么样子的。
- en: '[PRE27]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We configured the `CountVectorizer` to split the text into all the possible
    character 1-grams and 2-grams found in the texts. And `CountVectorizer` organizes
    the vocabulary in lexical order, so n-grams that start with a space character
    (`' '`) come first. Once the vectorizer knows what tokens it needs to be able
    to count, it can transform text strings into vectors, with one dimension for every
    token in your character n-gram vocabulary.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们配置了`CountVectorizer`以将文本拆分为文本中找到的所有可能的字符1-gram和2-gram。而且`CountVectorizer`按词法顺序组织词汇表，因此以空格字符（`'
    '`)开头的n-gram首先出现。一旦向量化器知道它需要能够计数的令牌是什么，它就可以将文本字符串转换为向量，每个字符n-gram词汇表中的每个令牌都有一个维度。
- en: '[PRE28]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The DataFrame contains a column for each sentence and a row for each character
    2-gram. Check out the top four rows where the byte pair (character 2-gram) of
    " a" is seen to occur five times in these two sentences. So even spaces count
    as "characters" when you’re building a BPE tokenizer. This is one of the advantages
    of BPE, it will figure out what your token delimiters are, so it will work even
    in languages where there is no whitespace between words. And BPE will work on
    substitution cypher text like ROT13, a toy cypher that rotates the alphabet 13
    characters forward.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame包含每个句子的一列和每个字符2-gram的一行。查看前四行，其中字节对（字符2-gram）“ a”的出现次数在这两个句子中出现了五次。所以即使在构建BPE分词器时，空格也算作“字符”。这是BPE的优点之一，它会弄清楚您的令牌分隔符是什么，因此即使在没有单词之间有空格的语言中，它也会起作用。而且BPE将适用于替换密码文本，如ROT13，这是一个将字母表向前旋转13个字符的玩具密码。
- en: '[PRE29]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: A BPE tokenizer then finds the most frequent 2-grams and adds them to the permanent
    vocabulary. Over time it deletes the less frequent character pairs, because the
    further it reads into your text, the less less likely that those rare character
    pairs will come up before the end of the text. For those familiar with statistics,
    it models your text with a Baysean model, continuously updating prior predictions
    of token frequencies.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，BPE分词器会找到最常见的2元组并将它们添加到永久词汇表中。随着时间的推移，它会删除较不常见的字符对，因为它读取文本越深入，那些稀有的字符对在文本末尾之前出现的可能性就越小。对于熟悉统计学的人来说，它使用贝叶斯模型对您的文本进行建模，不断更新对标记频率的先验预测。
- en: '[PRE30]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: So the next round of preprocessing in the BPE tokenizer would retain the character
    2-grams "en" and "an" and even " c" and "e ". Then the BPE algorithm would make
    another pass through the text with this smaller character bigram vocabulary. It
    would look for frequent pairings of these character bigrams with each other and
    individual characters. This process would continue until the maximum number of
    tokens is reached and the longest possible character sequences have been incorporated
    into the vocabulary.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在BPE分词器的下一轮预处理中，将保留字符2元组"en"和"an"甚至是" c"和"e"。然后，BPE算法将使用这个较小的字符二元组词汇再次遍历文本。它将寻找这些字符二元组彼此之间以及单个字符的频繁配对。这个过程将持续进行，直到达到最大标记数，并且最长的可能字符序列已经被纳入词汇表。
- en: Note
  id: totrans-269
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: You may see mention of *wordpiece* tokenizers which are used within some advanced
    language models such as `BERT` and its derivatives.^([[30](#_footnotedef_30 "View
    footnote.")]) It works the same as BPE, but it actually uses the underlying language
    model to predict the neighboring characters in string. It eliminates the characters
    from its vocabulary that hurt the accuracy of this language model the least. The
    math is subtly different and it produces subtly different token vocabularies,
    but you don’t need to select this tokenizer intentionally. The models that use
    it will come with it built into their pipelines.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会看到关于*wordpiece*分词器的提及，它在某些高级语言模型中使用，例如`BERT`及其衍生产品。它的工作方式与BPE相同，但实际上使用底层语言模型来预测字符串中的相邻字符。它会消除对语言模型精度影响最小的字符。数学上有微妙的差异，产生微妙不同的标记词汇表，但您无需刻意选择此分词器。使用它的模型将在其管道中内置。
- en: One big challenge of BPE-based tokenizers is that they must be trained on your
    individual corpus. So BPE tokenizers are usually only used for Transformers and
    Large Language Models (LLMs) which you will learn about in chapter 9.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: BPE分词器的一个重大挑战是必须针对您的个体语料库进行训练。因此，BPE分词器通常仅用于Transformer和大型语言模型（LLM），您将在第9章学习到这些内容。
- en: Another challenge of BPE tokenizers is all the book keeping you need to do to
    keep track of which trained tokenizer goes with each of your trained models. This
    was one of the big innovations of Huggingface. They made it easy to store and
    share all the preprocessing data, such as the tokenizer vocabulary, along side
    the language model. This makes it easier to reuse and share BPE tokenizers. If
    you want to become an NLP expert, you may want to imitate what they’ve done at
    HuggingFace with your own NLP preprocessing pipelines.^([[31](#_footnotedef_31
    "View footnote.")])
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个BPE分词器的挑战是您需要进行大量的簿记工作，以跟踪每个已训练的分词器与您已训练的模型之间的对应关系。这是Huggingface的一项重大创新之一。他们简化了存储和共享所有预处理数据的过程，例如分词器词汇表，以及语言模型。这使得重用和共享BPE分词器变得更加容易。如果您想成为自然语言处理专家，您可能希望模仿HuggingFace的做法，使用自己的NLP预处理流程。
- en: 2.8 Vectors of tokens
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.8 标记的向量
- en: Now that you have broken your text into tokens of meaning, what do you do with
    them? How can you convert them to numbers that will be meaningful to the machine?
    The simplest most basic thing to do would be to detect whether a particular token
    you are interested in was present or not. You could hard-code the logic to check
    for important tokens, called a *keywords*.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您已经将文本分解为有意义的标记，接下来该怎么处理呢？如何将它们转换为机器可理解的数字？最简单最基本的做法是检测您感兴趣的特定标记是否存在。您可以硬编码逻辑来检查重要的标记，称为*关键词*。
- en: 'This might work well for your greeting intent recognizer in chapter 1\. Our
    greeting intent recognizer at the end of chapter 1 looked for words like "Hi"
    and "Hello" at the beginning of a text string. Your new tokenized text would help
    you detect the presence or absence of words such as "Hi" and "Hello" without getting
    confused by words like "Hiking" and "Hell." With your new tokenizer in place,
    your NLP pipeline wouldn’t misinterpret the word "Hiking" as the greeting "Hi
    king":'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能对第1章中的问候意图识别器很有效。我们在第1章末尾的问候意图识别器寻找了像“Hi”和“Hello”这样的词在文本字符串开头的情况。你的新标记化文本将帮助你检测诸如“Hi”和“Hello”之类的词的存在或不存在，而不会被“Hiking”和“Hell”这样的词所混淆。有了你的新分词器，你的NLP管道不会将单词“Hiking”误解为问候语“Hi
    king”：
- en: '[PRE31]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: So tokenization can help you reduce the number of false positives in your simple
    intent recognition pipeline that looks for the presence of greeting words. This
    is often called keyword detection, because your vocabulary of words is limited
    to a set of words you think are important. However, it’s quite cumbersome to have
    to think of all the words that might appear in a greeting in order to recognize
    them all, including slang, misspellngs and typoos. And creating a for loop to
    iterate through them all would be inefficient. We can use the math of linear algebra
    and the vectorized operations of `numpy` to speed this process up.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，标记化可以帮助你减少在简单意图识别管道中的假阳性数量，该管道用于寻找问候词的存在。这通常被称为关键词检测，因为你的词汇表限于你认为重要的一组词。然而，必须考虑到所有可能出现在问候语中的单词，包括俚语、拼写错误和打字错误，这是相当麻烦的。创建一个循环来迭代所有这些单词将是低效的。我们可以利用线性代数的数学和`numpy`的向量化操作来加速这个过程。
- en: 'In order to detect tokens efficiently you will want to use three new tricks:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效地检测标记，您将想要使用三个新技巧：
- en: matrix and vector representations of documents
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文档的矩阵和向量表示
- en: vectorized operations in numpy
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: numpy中的向量化操作
- en: indexing of discrete vectors
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 离散向量的索引
- en: You’ll first learn the most basic, direct, raw and lossless way to represent
    words as a matrix, one-hot encoding.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 你将首先学习最基本、直接、原始和无损的表示单词的矩阵方式，即一热编码。
- en: 2.8.1 BOW (Bag-of-Words) Vectors
  id: totrans-283
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.8.1 词袋（Bag-of-Words）向量
- en: Is there any way to squeeze all those *player piano music rolls* into a single
    vector? Vectors are a great way to represent any object. With vectors we could
    compare documents to each other just by checking the Euclidian distance between
    them. Vectors allow us to use all your linear algebra tools on natural language.
    And that’s really the goal of NLP, doing math on text.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 是否有办法将所有那些*自动钢琴乐谱*挤入一个单一的向量中？向量是表示任何对象的好方法。通过向量，我们可以通过检查它们之间的欧几里得距离来比较文档。向量允许我们在自然语言上使用所有的线性代数工具。这确实是NLP的目标，对文本进行数学处理。
- en: Let us assume you can ignore the order of the words in our texts. For this first
    cut at a vector representation of text you can just jumble them all up together
    into a "bag," one bag for each sentence or short document. It turns out just knowing
    what words are present in a document can give your NLU pipeline a lot of information
    about what’s in it. This is in fact the representation that power big Internet
    search engine companies. Even for documents several pages long, a bag-of-words
    vector is useful for summarizing the essence of a document.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设您可以忽略我们文本中单词的顺序。对于文本的向量表示的第一次尝试，您可以将它们全部混合在一起形成一个“袋子”，每个句子或短文档一个袋子。事实证明，只知道一个文档中存在哪些单词就能给你的NLU管道提供很多关于文档内容的信息。事实上，这是大型互联网搜索引擎公司使用的表示方法。即使对于几页长的文档，词袋向量也有助于概括文档的本质。
- en: 'Let’s see what happens when we jumble and count the words in our text from
    *The Book Thief*:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看当我们混淆和计算《偷书贼》中文本中的单词时会发生什么：
- en: '[PRE32]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Even with this jumbled up bag of words, you can get a general sense that this
    sentence is about: "Trust", "words", "clouds", "rain", and someone named "Liesel".
    One thing you might notice is that Python’s `sorted()` puts punctuation before
    characters, and capitalized words before lowercase words. This is the ordering
    of characters in the ASCII and Unicode character sets. However, the order of your
    vocabulary is unimportant. As long as you are consistent across all the documents
    you tokenize this way, a machine learning pipeline will work equally well with
    any vocabulary order.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 即使使用这种杂乱的词袋，你也可以大致感受到这个句子是关于：“信任”，“词语”，“云”，“雨”，和一个名叫“丽莎尔”的人。你可能会注意到一件事，那就是Python的`sorted()`将标点符号放在字符之前，将大写字母放在小写字母之前。这是ASCII和Unicode字符集中字符的顺序。然而，你的词汇表的顺序并不重要。只要你在所有这样标记化的文档中保持一致，机器学习管道将可以同样有效地使用任何词汇顺序。
- en: You can use this new bag-of-words vector approach to compress the information
    content for each document into a data structure that is easier to work with. For
    keyword search, you could **OR** your one-hot word vectors from the player piano
    roll representation into a binary bag-of-words vector. In the play piano analogy
    this is like playing several notes of a melody all at once, to create a "chord".
    Rather than "replaying" them one at a time in your NLU pipeline, you would create
    a single bag-of-words vector for each document.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用这种新的词袋向量方法，将每个文档的信息内容压缩到一个更易于处理的数据结构中。对于关键词搜索，你可以将你的独热词向量从播放钢琴卷轴表示中*OR*成一个二进制词袋向量。在播放钢琴的类比中，这就像同时演奏几个旋律音符，以创建一个“和弦”。与在你的NLU管道中逐个“重播”它们不同，你会为每个文档创建一个单一的词袋向量。
- en: You could use this single vector to represent the whole document in a single
    vector. Because vectors all need to be the same length, your BOW vector would
    need to be as long your vocabulary size which is the number of unique tokens in
    your documents. And you could ignore a lot of words that would not be interesting
    as search terms or keywords. This is why stop words are often ignored when doing
    BOW tokenization. This is an extremely efficient representation for a search engine
    index or the first filter for an information retrieval system. Search indexes
    only need to know the presence or absence of each word in each document to help
    you find those documents later.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用这个单一向量来表示整个文档。因为向量都需要是相同长度的，你的词袋向量需要和你的词汇量大小一样长，即你文档中唯一标记的数量。你可以忽略很多不作为搜索词或关键词的词。这就是为什么在进行词袋标记化时通常忽略停用词。这对于搜索引擎索引或信息检索系统的第一过滤器来说是一个极其高效的表示。搜索索引只需要知道每个单词在每个文档中的存在与否，以帮助你以后找到这些文档。
- en: This approach turns out to be critical to helping a machine "understand" a collection
    of words as a single mathematical object. And if you limit your tokens to the
    10,000 most important words, you can compress your numerical representation of
    your imaginary 3500 sentence book down to 10 kilobytes, or about 30 megabytes
    for your imaginary 3000-book corpus. One-hot vector sequences for such a modest-sized
    corpus would require hundreds of gigabytes.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法原来对帮助机器“理解”一组单词作为一个单一的数学对象是至关重要的。如果你将你的标记限制为最重要的1万个单词，你可以将你对虚构的3500句子书的数字表示压缩到10千字节，或者对于你的虚构的3000本书的语料库，大约是30兆字节。对于这样一个规模适中的语料库，独热向量序列将需要数百吉字节。
- en: Another advantage of the BOW representation of text is that it allows you to
    find similar documents in your corpus in constant time (`O(1)`). You can’t get
    any faster than this. BOW vectors are the precursor to a reverse index which is
    what makes this speed possible. In computer science and software engineering,
    you are always on the lookout for data structures that enable this kind of speed.
    All major full text search tools use BOW vectors to find what you’re looking for
    fast. You can see this numerical representation of natural language in EllasticSearch,
    Solr,^([[32](#_footnotedef_32 "View footnote.")]) PostgreSQL, and even state of
    the art web search engines such as Qwant,^([[33](#_footnotedef_33 "View footnote.")]),
    SearX,^([[34](#_footnotedef_34 "View footnote.")]), and Wolfram Alpha ^([[35](#_footnotedef_35
    "View footnote.")]).
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 文本的BOW表示的另一个优点是它允许你在常数时间（`O(1)`）内在你的语料库中找到相似的文档。你找不到比这更快的方法了。BOW向量是实现这种速度的反向索引的前身。在计算机科学和软件工程中，你总是在寻找能够实现这种速度的数据结构。所有主要的全文搜索工具都使用BOW向量来快速找到你需要的内容。你可以在EllasticSearch、Solr，[[32](#_footnotedef_32
    "View footnote.")] PostgreSQL以及最先进的网络搜索引擎（例如Qwant，[[33](#_footnotedef_33 "View
    footnote.")]]，SearX，[[34](#_footnotedef_34 "View footnote.")]，以及Wolfram Alpha[[35](#_footnotedef_35
    "View footnote.")]])中看到自然语言的这种数值表示。
- en: Fortunately, the words in your vocabulary are sparsely utilized in any given
    text. And for most bag-of-words applications, we keep the documents short, sometimes
    just a sentence will do. So rather than hitting all the notes on a piano at once,
    your bag-of-words vector is more like a broad and pleasant piano chord, a combination
    of notes (words) that work well together and contain meaning. Your NLG pipeline
    or chatbot can handle these chords even if there is a lot of "dissonance" from
    words in the same statement that are not normally used together. Even dissonance
    (odd word usage) is useful information about a statement that a machine learning
    pipeline can make use of.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，在给定的任何一段文本中，词汇表中的单词很少被使用。对于大多数词袋（bag-of-words）应用程序，我们保持文档简短，有时只需要一句话就足够。因此，与一次性击打钢琴上的所有音符不同，你的词袋向量更像是一个广泛而愉悦的钢琴和弦，是一组能很好地合作并带有意义的音符（单词）的组合。即使在同一语句中有一些不常用在一起的词（“不和谐”，即奇怪的用词），你的自然语言生成流水线或聊天机器人也可以处理这些和弦。甚至“不和谐”（奇怪的用词）也是关于一种陈述的有用信息，可以被机器学习流水线利用起来。
- en: Here is how you can put the tokens into a binary vector indicating the presence
    or absence of a particular word in a particular sentence. This vector representation
    of a set of sentences could be "indexed" to indicate which words were used in
    which document. This index is equivalent to the index you find at the end of many
    textbooks, except that instead of keeping track of which page a word occurs on,
    you can keep track of the sentence (or the associated vector) where it occurred.
    Whereas a textbook index generally only cares about important words relevant to
    the subject of the book, you keep track of every single word (at least for now).
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何将标记放入二进制向量中，指示特定句子中是否存在某个单词。这组句子的向量表示可以被“索引”，以指示哪些词语在哪个文档中被使用。这个索引类似于你在许多教科书末尾找到的索引，只不过它不是跟踪单词出现在哪一页上，而是跟踪它出现在哪个句子（或相关向量）中。然而教科书索引通常只关心与书的主题相关的重要单词，而你却跟踪每一个单词（至少现在是这样）。
- en: Sparse representations
  id: totrans-295
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 稀疏表示
- en: You might be thinking that if you process a huge corpus you’ll probably end
    up with thousands or even millions of unique tokens in your vocabulary. This would
    mean you would have to store a lot of zeros in your vector representation of our
    20-token sentence about Liesel. A `dict` would use much less memory than a vector.
    Any paired mapping of words to their 0/1 values would be more efficient than a
    vector. But you can’t do math on `dict’s. So this is why CountVectorizer uses
    a sparse numpy array to hold the counts of words in a word fequency vector. Using
    a dictionary or sparse array for your vector ensures that it only has to store
    a 1 when any one of the millions of possible words in your dictionary appear in
    a particular document.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 或许你会想，如果你处理一个庞大的语料库，你可能最终会得到成千上万个甚至是数百万个在你的词汇表中的唯一标记。这意味着你需要在表示我们关于Liesel的20个标记的句子的向量表达中存储许多零。与向量相比，`dict`使用的内存要少得多。将单词与它们的0/1值配对的任何映射都比向量更有效。但是你不能对
    `dict` 进行数学运算。这就是为什么CountVectorizer使用稀疏的numpy数组来保存词在词频向量中的计数的原因。使用字典或稀疏数组作为向量可以确保只在词典中的数百万个可能单词之一出现在特定文档中时存储一个1。
- en: But if you want to look at an individual vector to make sure everything is working
    correctly, a Pandas `Series` is the way to go. And you will wrap that up in a
    Pandas DataFrame so you can add more sentences to your binary vector "corpus"
    of quotes.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 但是如果你想查看一个单独的向量以确保一切工作正常，那么 Pandas 的 `Series` 是最好的选择。然后你会将它包装在一个 Pandas DataFrame
    中，这样你就可以向你的二进制向量“语料库”中添加更多的句子引用。
- en: 2.8.2 Dot product
  id: totrans-298
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.8.2 点积
- en: You’ll use the dot product a lot in NLP, so make sure you understand what it
    is. Skip this section if you can already do dot products in your head.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理中你会经常用到点积，所以确保你理解它是什么。如果你已经能够在头脑中进行点积，请跳过本节。
- en: The dot product is also called the *inner product* because the "inner" dimension
    of the two vectors (the number of elements in each vector) or matrices (the rows
    of the first matrix and the columns of the second matrix) must be the same because
    that is where the products happen. This is analogous to an "inner join" on two
    relational database tables.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 点积也被称为*内积*，因为两个向量（每个向量中的元素数）或矩阵（第一个矩阵的行和第二个矩阵的列）的“内部”维度必须相同，因为这是产品发生的地方。这类似于两个关系数据库表的“内连接”。
- en: The dot product is also called the *scalar product* because it produces a single
    scalar value as its output. This helps distinguish it from the *cross product*,
    which produces a vector as its output. Obviously, these names reflect the shape
    of the symbols used to indicate the dot product (\(\cdot\)) and cross product
    (\(\times\)) in formal mathematical notation. The scalar value output by the scalar
    product can be calculated by multiplying all the elements of one vector by all
    the elements of a second vector and then adding up those normal multiplication
    products.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 点积也称为*标量积*，因为它产生一个标量值作为其输出。这有助于将其与*叉积*区分开来，后者产生一个向量作为其输出。显然，这些名称反映了正式数学符号中用于表示点积（\(\cdot\)）和叉积（\(\times\)）的形状。标量积输出的标量值可以通过将一个向量的所有元素与第二个向量的所有元素相乘，然后将这些普通乘积相加来计算。
- en: 'Here is a Python snippet you can run in your Pythonic head to make sure you
    understand what a dot product is:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个你可以在你的 Pythonic 头脑中运行的 Python 片段，以确保你理解什么是点积：
- en: Listing 2.2 Example dot product calculation
  id: totrans-303
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.2 示例点积计算
- en: '[PRE33]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Tip
  id: totrans-305
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: 'The dot product is equivalent to the *matrix product*, which can be accomplished
    in NumPy with the `np.matmul()` function or the `@` operator. Since all vectors
    can be turned into Nx1 or 1xN matrices, you can use this shorthand operator on
    two column vectors (Nx1) by transposing the first one so their inner dimensions
    line up, like this: `v1.reshape-1, 1.T @ v2.reshape-1, 1`, which outputs your
    scalar product within a 1x1 matrix: `array([[20]])`'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 点积等同于*矩阵乘积*，可以在 NumPy 中用`np.matmul()`函数或`@`运算符完成。由于所有向量都可以转换为 Nx1 或 1xN 矩阵，所以你可以通过转置第一个向量，使它们的内部维度对齐，像这样使用这个简写运算符在两个列向量（Nx1）上：`v1.reshape-1,
    1.T @ v2.reshape-1, 1`，这样就输出了你的标量积在一个 1x1 矩阵中：`array([[20]])`
- en: 'This is your first vector space model of natural language documents (sentences).
    Not only are dot products possible, but other vector operations are defined for
    these bag-of-word vectors: addition, subtraction, OR, AND, and so on. You can
    even compute things such as Euclidean distance or the angle between these vectors.
    This representation of a document as a binary vector has a lot of power. It was
    a mainstay for document retrieval and search for many years. All modern CPUs have
    hardwired memory addressing instructions that can efficiently hash, index, and
    search a large set of binary vectors like this. Though these instructions were
    built for another purpose (indexing memory locations to retrieve data from RAM),
    they are equally efficient at binary vector operations for search and retrieval
    of text.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 这是自然语言文档（句子）的第一个向量空间模型。不仅可以进行点积，还可以对这些词袋向量进行其他向量操作：加法、减法、或运算、与运算等。甚至可以计算诸如欧几里得距离或这些向量之间的角度之类的东西。将文档表示为二进制向量的这种方式具有很强的功能。这在许多年里一直是文档检索和搜索的支柱。所有现代
    CPU 都有硬编址内存指令，可以高效地哈希、索引和搜索这样的大型二进制向量集合。尽管这些指令是为另一个目的而构建的（索引内存位置以从 RAM 检索数据），但它们同样有效地用于搜索和检索文本的二进制向量操作。
- en: 'NLTK and Stanford CoreNLP have been around the longest and are the most widely
    used for comparison of NLP algorithms in academic papers. Even though the Stanford
    CoreNLP has a Python API, it relies on the Java 8 CoreNLP backend, which must
    be installed and configured separately. So if you want to publish the results
    of your work in an academic paper and compare it to what other researchers are
    doing, you may need to use NLTK. The most common tokenizer used in academia is
    the PennTreebank tokenizer:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK和Stanford CoreNLP存在时间最长，并且在学术论文中用于NLP算法比较的最广泛使用。尽管Stanford CoreNLP有一个Python
    API，但它依赖于Java 8 CoreNLP后端，必须单独安装和配置。因此，如果你想在学术论文中发布你的工作结果，并将其与其他研究人员的工作进行比较，你可能需要使用NLTK。学术界最常用的分词器是PennTreebank分词器：
- en: '[PRE34]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The spaCy Python library contains a natural language processing pipeline that
    includes a tokenizer. In fact, the name of the package comes from the words "space"
    and "Cython". SpaCy was built using the Cython package to speed the tokenization
    of text, often using the **space** character (" ") as the delimmiter. SpaCy has
    become the **multitool** of NLP, because of its versatility and the elegance of
    its API. To use spaCy, you can start by creating an callable parser object, typically
    named `nlp`. You can customize your NLP pipeline by modifying the Pipeline elements
    within that parser object.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy Python库包含一个自然语言处理流水线，其中包括一个标记器。事实上，这个包的名称来自于"space"和"Cython"这两个词。SpaCy使用Cython包构建，以加速文本的标记，通常使用**space**字符("
    ")作为分隔符。SpaCy已经成为NLP的**多功能工具**，因为它的多功能性和API的优雅性。要使用spaCy，你可以通过修改解析器对象内的管道元素来自定义你的NLP流水线，通常命名为`nlp`。
- en: And spaCy has "batteries included." So even with the default smallest spaCy
    language model loaded, you can do tokenization and sentence segementation, plus
    **part-of-speech** and **abstract-syntax-tree** tagging — all with a single function
    call. When you call `nlp()` on a string, spaCy tokenizes the text and returns
    a `Doc` (document) object. A `Doc` object is a container for the sequence of sentences
    and tokens that it found in the text.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 而且spaCy已经“内置电池”。因此，即使加载了默认最小的spaCy语言模型，你也可以进行标记和句子分割，以及**词性**和**抽象语法树**标记 -
    所有这些都可以通过一个函数调用完成。当你在一个字符串上调用`nlp()`时，spaCy会对文本进行标记化，并返回一个`Doc`（文档）对象。一个`Doc`对象是一个包含在文本中找到的句子和标记序列的容器。
- en: The spaCy package tags each token with their linguistic function to provide
    you with information about the text’s grammatical structure. Each token object
    within a `Doc` object has attributes that provide these tags.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy包为每个标记标注了它们的语言功能，以提供有关文本的语法结构的信息。`Doc`对象中的每个标记对象都有提供这些标签的属性。
- en: 'For example: * `token.text` the original text of the word * `token.pos_` grammatical
    part of speech tag as a human-readable string * `token.pos` integer for the grammar
    part of speech tag * `token.dep_` indicates the tokens role in the syntactic dependency
    tree * `token.dep` integer corresponding to the syntactic dependency tree location'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：* `token.text` 单词的原始文本 * `token.pos_` 作为人类可读字符串的语法部分标签 * `token.pos` 表示语法部分标签的整数
    * `token.dep_` 表示标记在句法依赖树中的作用 * `token.dep` 对应于句法依赖树位置的整数
- en: The `.text` attribute provides the original text for the token. This is what
    is provided when you request the *str* representation of a token. A spaCy `Doc`
    object is allowing you to detokenize a document object to recreate the entire
    input text. i.e., the relation between tokens You can use these functions to examine
    the text in more depth.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '`.text`属性提供了标记的原始文本。当你请求标记的*str*表示时，就会提供这个。一个spaCy `Doc`对象允许你对一个文档对象进行去标记化，以重新创建整个输入文本。也就是说，标记之间的关系。你可以使用这些函数来更深入地检查文本。'
- en: '[PRE35]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 2.9 Challenging tokens
  id: totrans-316
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.9 挑战性的标记
- en: Chinese, Japanese, and other pictograph languages aren’t limited to a small
    small number letters in alphabets used to compose tokens or words. Characters
    in these traditional languages look more like drawings and are called "pictographs."
    There are many thousands of unique characters in the Chinese and Japanese languages.
    And these characters are used much like we use words in alphabet-based languages
    such as English. But each Chinese character is usually not a complete word on
    its own. A character’s meaning depends on the characters to either side. And words
    are not delimited with spaces. This makes it challenging to tokenize Chinese text
    into words or other packets of thought and meaning.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 汉语、日语和其他象形文字语言并不受限于用于构成标记或单词的字母数量。这些传统语言中的字符更像是绘画，被称为“象形文字”。中文和日文语言中有成千上万个独特的字符。而这些字符的使用方式与我们在英语等字母语言中使用单词的方式相似。但每个汉字通常不是一个完整的单词。一个字符的含义取决于两边的字符。而且单词之间没有用空格分隔。这使得将中文文本分词成单词或其他意思的分组成为一项具有挑战性的任务。
- en: 'The `jieba` package is a Python package you can use to segment traditional
    Chinese text into words. It supports three segmentation modes: 1) "full mode"
    for retrieving all possible words from a sentence, 2) "accurate mode" for cutting
    the sentence into the most accurate segments, 3) "search engine mode" for splitting
    long words into shorter ones, sort-of like splitting compound words or finding
    the roots of words in English. In the example below, the Chinese sentence "西安是一座举世闻名的文化古城"
    translates into "Xi’an is a city famous world-wide for it’s ancient culture."
    Or, a more compact and literal translation might be "Xi’an is a world-famous city
    for her ancient culture."'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '`jieba` 包是一个可以用来将繁体中文文本分词的 Python 包。它支持三种分词模式：1）“全模式”用于从句子中检索所有可能的词语，2）“精确模式”用于将句子切分为最精确的片段，3）“搜索引擎模式”用于将长词分割成更短的词语，有点像拆分复合词或找到英语中单词的根源。在下面的例子中，中文句子“西安是一座举世闻名的文化古城”翻译成“Xi’an
    is a city famous world-wide for its ancient culture.” 或者，更简洁直接的翻译可能是“Xi’an is
    a world-famous city for her ancient culture.”。'
- en: 'From a grammatical perspective, you can split the sentence into: 西安 (Xi’an),
    是 (is), 一座 (a), 举世闻名 (world-famous), 的 (adjective suffix), 文化 (culture), 古城 (ancient
    city). The character "座" is the quantifier meaning "ancient" that is normally
    used to modify the word "city." The `accurate mode` in `jieba` causes it to segment
    the sentence this way so that you can correctly extract a precise interpretation
    of the text.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 从语法的角度来看，你可以将这个句子分成：西安 (Xi’an), 是 (is), 一座 (a), 举世闻名 (world-famous), 的 (adjective
    suffix), 文化 (culture), 古城 (ancient city)。字`座`是修饰`城`的量词，表示“古老”。`jieba` 的`accurate
    mode`模式会以这种方式分割句子，以便你能正确提取文本的精确解释。
- en: Listing 2.3 Jieba in accurate mode
  id: totrans-320
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第 2.3 节 Jieba 的精确模式
- en: '[PRE36]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Jieba’s accurate mode minimizes the total number of tokens or words. This gave
    you 7 tokens for this short Jieba attempts to keep as many possible characters
    together. This will reduce the false positive rate or type 1 errors for detecting
    boundaries between words.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: Jieba 的 accurate 模式可以最小化标记或单词的总数。这为这个短句提供了 7 个标记。Jieba 试图尽可能保持更多的字符在一起。这将降低检测单词边界的误报率或类型
    1 错误。
- en: In full mode, jieba will attempt to split the text into smaller words, and more
    of them.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在全模式下，jieba 将尝试将文本分割为更小的单词，数量也更多。
- en: Listing 2.4 Jieba in full mode
  id: totrans-324
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第 2.4 节 Jieba 的全模式
- en: '[PRE37]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now you can try search engine mode to see if it’s possible to break up these
    tokens even further:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以尝试搜索引擎模式，看看是否可能进一步分解这些标记：
- en: Listing 2.5 Jieba in search engine mode
  id: totrans-327
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第 2.5 节 Jieba 的搜索引擎模式
- en: '[PRE38]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Unfortunately later versions of Python (3.5+) aren’t supported by Jieba’s part-of-speech
    tagging model.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，Jieba 的词性标注模型不支持后续版本的 Python (3.5+)。
- en: '[PRE39]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: You can find more information about jieba at ([https://github.com/fxsjy/jieba](fxsjy.html)
    ). SpaCy also contains Chinese language models that do a decent job of segmenting
    and tagging Chinese text.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 ([https://github.com/fxsjy/jieba](fxsjy.html) ) 找到更多有关 jieba 的信息。SpaCy
    还包含了一些中文语言模型，可以对中文文本进行分词和标记，做得相当不错。
- en: '[PRE40]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: As you may notice, spaCy provides slightly different tokenization and tagging,
    which is more attached to the original meaning of each word rather than the context
    of this sentence.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，spaCy 提供了稍微不同的标记和词性标注，更贴近每个词的原始含义，而不是这个句子的上下文。
- en: 2.9.1 A complicated picture
  id: totrans-334
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.9.1 一个复杂的图画
- en: 'Unlike English, there is no concept of stemming or lemmatization in pictographic
    languages such as Chinese and Japanese (Kanji). However, there’s a related concept.
    The most essential building blocks of Chinese characters are called *radicals*.
    To better understand *radicals*, you must first see how Chinese characters are
    constructed. There are six types of Chinese characters: 1) pictographs, 2) pictophonetic
    characters, 3) associative compounds, 4) self-explanatory characters, 5) phonetic
    loan characters, and 6) mutually explanatory characters. The top four categories
    are the most important and encompass most Chinese characters.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 与英文不同，中文和日文（汉字）等象形文字中没有词干或词形还原的概念。然而，有一个相关的概念。汉字最基本的组成部分叫做**部首**。要更好地理解**部首**，首先必须了解汉字是如何构成的。汉字有六种类型：1）象形字，2）形声字，3）会意字，4）指事字，5）借音字，以及6）假借字。前四类是最重要的，也包括了大部分的汉字。
- en: Pictographs (象形字)
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 象形字（Pictographs）
- en: Pictophonetic characters (形声字)
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 形声字（Pictophonetic characters）
- en: Associative compounds (会意字)
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 会意字（Associative compounds）
- en: 2 Pictographs (象形字)
  id: totrans-339
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2个象形字（Pictographs）
- en: '*Pictographs* were created from images of real objects, such as the characters
    for 口 (mouth) and 门 (door).'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '*象形字*是由真实物体的图像创造而成，比如口和门的汉字。'
- en: 2 Pictophonetic characters (形声字)
  id: totrans-341
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2个形声字（Pictophonetic characters）
- en: '*Pictophonetic characters* were created from a radical and a single Chinese
    character. One part represents its meaning and the other indicates its pronunciation.
    For example, 妈 (mā, mother) = 女 (female) + 马 (mǎ, horse). Squeezing 女 into 马 gives
    妈. The character 女 is the semantic radical that indicates the meaning of the character
    (female). 马 is a single character that has a similar pronunciation (mǎ). You can
    see that the character for mother (妈) is a combination of the characters for female
    an This is comparable to the English concept of homophones — words that sound
    alike but mean completely different things. But in Chinese use additional characters
    to disambiguate homophones. The character for female'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '*形声字*是由一个部首和一个单独的汉字合并而成。其中一部分代表其意义，另一部分表示其发音。例如，妈（mā，妈妈）= 女（女性）+ 马（mǎ，马）。将女插入到马中得到妈。女是语义部首，表示汉字的意义（女性）。马是一个有着类似发音（mǎ）的单独汉字。你可以看到，母亲（妈）这个汉字是女性和马两个汉字的组合。这与英文的“同音词”概念相似-发音相似但意思截然不同的词语。但是中文使用额外的汉字来消除同音词的歧义。女性的汉字'
- en: 3 Associative compounds (会意字)
  id: totrans-343
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3个会意字（Associative compounds）
- en: 'Associative compounds can be divided into two parts: one symbolizes the image,
    the other indicates the meaning.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 会意字可以分为两部分：一个表示图像，另一个表示意义。
- en: For example, 旦 (dawn), the upper part (日) is the sun and the lower part (一)
    is like the horizon line.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，旦（黎明），上部分（日）像太阳，下部分（一）类似地平线。
- en: Self-explanatory characters (指事字)
  id: totrans-346
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 指事字（Self-explanatory characters）
- en: Self-explanatory characters cannot be easily represented by an image, so they
    are shown by a single abstract symbol. For example, 上 (up), 下 (down).
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 指事字由于不能用图像来表示，所以用单个抽象符号来表示。例如，上（上升）、下（下降）。
- en: As you can see, procedures like stemming and lemmatization are harder or impossible
    for many Chinese characters. Separating the parts of a character may radically
    ;) change its meaning. And there’s not prescribed order or rule for combining
    radicals to create Chinese characters.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，像词干和词形还原这样的过程对于许多汉字来说更难或者不可能。分开一个汉字的部分可能会完全改变其意义。而且，组合部首以创建汉字没有规定的顺序或规则。
- en: Nonetheless, some kinds of stemming are harder in English than they are in Chinese
    For example, automatically removing the pluralization from words like "we", "us",
    "they" and "them" is hard in English but straightforward in Chinese. Chinese uses
    inflection to construct the plural form of characters, similar to adding s to
    the end of English words. In Chinese the pluralization suffix character is 们.
    The character 朋友 (friend) becomes 朋友们 (friends).
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，有些英语中的词干变化比中文更难。例如，自动去除像“我们”、“他们”等词的复数形式在英语中很难，但在中文中很简单。中文通过词缀来构造字符的复数形式，类似于在英语单词结尾加s。中文的复数化后缀字符是们。朋友（friend）一词变为朋友们（friends）。
- en: 'Even the characters for "we/us", "they/them", and "y’all" use the same pluralization
    suffix: 我们 (we/us), 他们 (they/them), 你们 (you). But in in English, you can remove
    the ''ing'' or ''ed'' from many verbs to get the root word. However, in Chinese,
    verb conjugation uses an additional character in the front or the end to indicate
    tense. There’s no prescribed rule for verb conjugation. For example, examine the
    character 学 (learn), 在学 (learning), and 学过 (learned). In Chinese, you can also
    use a suffix 学 to denote an academic discipline, such as 心理学 (psychology) or 社会学
    (sociology). In most cases, you want to keep the integrated Chinese character
    together rather than reducing it to its components.'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是“我们”，“他们 / 他们”和“y'all”的字符也使用相同的复数后缀：我们（we / us），他们（they / them），你们（you）。但是，在英语中，您可以从许多动词中删除“ing”或“ed”以获得根词。但是，在中文中，动词变位在前面或末尾使用一个额外的字符来指示时态。没有规定动词变位的规则。例如，检查字符“学”（学习），“在学”（学习）和“学过”（学过）。在中文中，您还可以使用后缀“学”来表示学术学科，例如“心理学”或“社会学”。在大多数情况下，您希望保持集成的中文字符而不是将其缩小到其组件。
- en: It turns out this is a good rule of thumb for all languages. Let the data do
    the talking. Do not stem or lemmatize unless the statistics indicate that it will
    help your NLP pipeline perform better. Is there not a small amount of meaning
    that is lost when "smarter" and "smartest" reduce to "smart"? Make sure stemming
    does not leave your NLP pipeline dumb.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 结果证明，这是所有语言的一个好习惯。让数据说话。除非统计数据表明它有助于您的NLP管道运行得更好，否则不要进行词干提取或词形还原。当“smarter”和“smartest”减小为“smart”时，不会丢失多少意义。确保词干提取不会使您的NLP管道变得愚蠢。
- en: Let the statistics of how of how characters and words are used together help
    you decide how, or if, to decompose any particular word or n-gram. In the next
    chapter we’ll show you some tools like Scikit-Learn’s `TfidfVectorizer` that handle
    all the tedious account required to get this right.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 让字符和单词如何使用的统计数据帮助你决定如何，或者是否要分解任何特定的单词或n-gram。在下一章中，我们将向您展示一些工具，如Scikit-Learn的`TfidfVectorizer`，它处理所有需要正确处理所需的繁琐帐户。
- en: Contractions
  id: totrans-353
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 缩略词
- en: You might be wondering why you would want to split the contraction `wasn’t`
    into `was` and `n’t`. For some applications, like grammar-based NLP models that
    use syntax trees, it is important to separate the words `was` and `not` to allow
    the syntax tree parser to have a consistent, predictable set of tokens with known
    grammar rules as its input. There are a variety of standard and nonstandard ways
    to contract words, by reducing contractions to their constituent words, a dependency
    tree parser or syntax parser only need to be programmed to anticipate the various
    spellings of individual words rather than all possible contractions.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能想知道为什么要将缩写`wasn't`拆分为`was`和`n't`。对于某些应用程序，例如使用语法树的基于语法的NLP模型，重要的是将单词`was`和`not`分开，以使语法树解析器具有一组已知语法规则的一致，可预测的标记作为其输入。有各种标准和非标准的缩写单词的方法，通过将缩写减小为其组成单词，依赖树解析器或语法分析器只需要编程来预测单个单词的各种拼写，而不是所有可能的缩写。
- en: Tokenize informal text from social networks such as Twitter and Facebook
  id: totrans-355
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 从社交网站如Twitter和Facebook中对非正式文本进行标记化
- en: 'The NLTK library includes a rule-based tokenizer to deal with short, informal,
    emoji-laced texts from social networks: `casual_tokenize`'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK库包括一种基于规则的分词器，用于处理来自社交网络的短，非正式，有表情的文本：`casual_tokenize`
- en: It handles emojis, emoticons, and usernames. The `reduce_len` option deletes
    less meaningful character repetitions. The `reduce_len` algorithm retains three
    repetitions, to approximate the intent and sentiment of the original text.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 它处理表情符号，表情符号和用户名。 `reduce_len`选项删除不太有意义的字符重复。 `reduce_len`算法保留三个重复项，以近似原始文本的意图和情感。
- en: '[PRE41]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 2.9.2 Extending your vocabulary with n-grams
  id: totrans-359
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.9.2 使用n-gram扩展词汇表
- en: Let’s revisit that "ice cream" problem from the beginning of the chapter. Remember
    we talked about trying to keep "ice" and "cream" together.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新审视本章开始时遇到的“冰淇淋”问题。记得我们谈论过试图让“ice”和“cream”在一起。
- en: I scream, you scream, we all scream for ice cream.
  id: totrans-361
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我尖叫，你尖叫，我们都为冰淇淋尖叫。
- en: But I do not know many people that scream for "cream". And nobody screams for
    "ice", unless they’re about to slip and fall on it. So you need a way for your
    word-vectors to keep "ice" and "cream" together.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我不知道有多少人为“cream”而尖叫。除非他们即将滑倒。因此，您需要一种方法来使您的单词向量保持“ice”和“cream”在一起。
- en: We all gram for *n*-grams
  id: totrans-363
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 我们都会gram *n* -gram
- en: An *n*-gram is a sequence containing up to *n* elements that have been extracted
    from a sequence of those elements, usually a string. In general the "elements"
    of an *n*-gram can be characters, syllables, words, or even symbols like "A",
    "D", and "G" used to represent the chemical amino acid markers in a DNA or RNA
    sequence.^([[38](#_footnotedef_38 "View footnote.")])
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 *n*-gram 是一个包含多达 *n* 个元素的序列，这些元素是从这些元素的序列中提取出来的，通常是一个字符串。通常 *n*-gram 的 "元素"
    可以是字符、音节、单词，甚至是用来表示 DNA 或 RNA 序列中化学氨基酸标记的符号 "A"、"D" 和 "G"。^([[38](#_footnotedef_38
    "View footnote.")])
- en: In this book, we’re only interested in *n*-grams of words, not characters.^([[39](#_footnotedef_39
    "View footnote.")]) So in this book, when we say 2-gram, we mean a pair of words,
    like "ice cream". When we say 3-gram, we mean a triplet of words like "beyond
    the pale" or "Johann Sebastian Bach" or "riddle me this". *n*-grams do not have
    to mean something special together, like compound words. They have to be frequent
    enough together to catch the attention of your token counters.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们只关心单词的*n*-gram，而不是字符。^([[39](#_footnotedef_39 "View footnote.")]) 所以在本书中，当我们说
    2-gram 时，我们指的是一对单词，比如 "冰淇淋"。当我们说 3-gram 时，我们指的是一组三个单词，比如 "超出常规" 或 "约翰·塞巴斯蒂安·巴赫"
    或 "给我个谜语"。*n*-grams 不一定要在一起有特殊意义，比如复合词。它们只需在一起频繁出现，以引起你的标记计数器的注意。
- en: Why bother with *n*-grams? As you saw earlier, when a sequence of tokens is
    vectorized into a bag-of-words vector, it loses a lot of the meaning inherent
    in the order of those words. By extending your concept of a token to include multiword
    tokens, *n*-grams, your NLP pipeline can retain much of the meaning inherent in
    the order of words in your statements. For example, the meaning-inverting word
    "not" will remain attached to its neighboring words, where it belongs. Without
    *n*-gram tokenization, it would be free floating. Its meaning would be associated
    with the entire sentence or document rather than its neighboring words. The 2-gram
    "was not" retains much more of the meaning of the individual words "not" and "was"
    than those 1-grams alone in a bag-of-words vector. A bit of the context of a word
    is retained when you tie it to its neighbor(s) in your pipeline.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么要使用 *n*-grams？正如你之前看到的，当一个 token 序列被向量化为词袋向量时，它会失去这些词序中固有的许多含义。通过将你的 token
    概念扩展到包括多词 token，*n*-grams，你的 NLP 流水线可以保留语句中单词顺序中固有的大部分含义。例如，意义颠倒的词 "not" 将保持与其相邻单词的联系，这是它应该的。没有
    *n*-gram 分词，它会自由漂浮。它的含义将与整个句子或文档相关联，而不是与其相邻单词相关联。2-gram "was not" 保留了比单独的 1-gram
    中更多的 "not" 和 "was" 单词的含义。当你将一个单词与其在流水线中的邻居联系起来时，会保留一些单词的上下文。
- en: In the next chapter, we show you how to recognize which of these *n*-grams contain
    the most information relative to the others, which you can use to reduce the number
    of tokens (*n*-grams) your NLP pipeline has to keep track of. Otherwise it would
    have to store and maintain a list of every single word sequence it came across.
    This prioritization of *n*-grams will help it recognize "Three Body Problem" and
    "ice cream", without paying particular attention to "three bodies" or "ice shattered".
    In chapter 4, we associate word pairs, and even longer sequences, with their actual
    meaning, independent of the meaning of their individual words. But for now, you
    need your tokenizer to generate these sequences, these *n*-grams.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将向你展示如何识别这些 *n*-grams 中包含的相对于其他 *n*-grams 的信息量，你可以用它来减少你的 NLP 流水线需要跟踪的标记（*n*-grams）数量。否则，它将不得不存储和维护每个单词序列的列表。对
    *n*-grams 的优先处理将帮助它识别 "三体问题" 和 "冰淇淋"，而不特别关注 "三个身体" 或 "碎冰"。在第四章中，我们将词对甚至更长的序列与它们的实际含义联系起来，而不是与它们各自单词的含义联系起来。但是现在，你需要你的分词器生成这些序列，这些
    *n*-grams。
- en: Stop words
  id: totrans-368
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 停用词
- en: Stop words are common words in any language that occur with a high frequency
    but carry much less substantive information about the meaning of a phrase. Examples
    of some common stop words include ^([[40](#_footnotedef_40 "View footnote.")])
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 停用词是任何语言中频繁出现但携带的实质信息较少的常见词。一些常见停用词的例子包括 ^([[40](#_footnotedef_40 "View footnote.")])
- en: a, an
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: a, an
- en: the, this
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: the, this
- en: and, or
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: and, or
- en: of, on
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: of, on
- en: 'Historically stop words have been excluded from NLP pipelines in order to reduce
    the computational effort to extract information from a text. Even though the words
    themselves carry little information, the stop words can provide important relational
    information as part of an *n*-gram. Consider these two examples:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 历史上，为了减少从文本中提取信息的计算工作量，停用词已被排除在 NLP 流水线之外。尽管这些词本身携带的信息很少，但停用词可以作为 *n*-gram 的一部分提供重要的关联信息。考虑以下两个例子：
- en: '`Mark reported to the CEO`'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Mark 向 CEO 汇报`'
- en: '`Suzanne reported as the CEO to the board`'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Suzanne 向董事会报告作为 CEO`'
- en: 'In your NLP pipeline, you might create 4-grams such as `reported to the CEO`
    and `reported as the CEO`. If you remove the stop words from the 4-grams, both
    examples would be reduced to `reported CEO`, and you would lack the information
    about the professional hierarchy. In the first example, Mark could have been an
    assistant to the CEO, whereas in the second example Suzanne was the CEO reporting
    to the board. Unfortunately, retaining the stop words within your pipeline creates
    another problem: It increases the length of the *n*-grams required to make use
    of these connections formed by the otherwise meaningless stop words. This issue
    forces us to retain at least 4-grams if you want to avoid the ambiguity of the
    human resources example.'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的 NLP 流水线中，你可能会创建诸如 `reported to the CEO` 和 `reported as the CEO` 这样的 4-gram。如果从这些
    4-gram 中删除停用词，两个例子都将被简化为 `reported CEO`，你将丧失关于专业层级的信息。在第一个例子中，Mark 可能是 CEO 的助手，而在第二个例子中，Suzanne
    是 CEO 向董事会汇报的 CEO。不幸的是，在你的流水线中保留停用词会产生另一个问题：它增加了 *n*-grams 所需的长度，以利用停用词形成的这些否则毫无意义的连接。这个问题迫使我们至少保留
    4-gram，如果你想避免人力资源示例的歧义。
- en: Designing a filter for stop words depends on your particular application. Vocabulary
    size will drive the computational complexity and memory requirements of all subsequent
    steps in the NLP pipeline. But stop words are only a small portion of your total
    vocabulary size. A typical stop word list has only 100 or so frequent and unimportant
    words listed in it. But a vocabulary size of 20,000 words would be required to
    keep track of 95% of the words seen in a large corpus of tweets, blog posts, and
    news articles.^([[41](#_footnotedef_41 "View footnote.")]) And that is just for
    1-grams or single-word tokens. A 2-gram vocabulary designed to catch 95% of the
    2-grams in a large English corpus will generally have more than 1 million unique
    2-gram tokens in it.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 设计停用词过滤器取决于你的特定应用。词汇量将决定 NLP 流水线中所有后续步骤的计算复杂性和内存需求。但是停用词只是你总词汇量的一小部分。一个典型的停用词列表只包含
    100 个左右频繁而不重要的单词。但是一个包含 20,000 个单词的词汇量将需要跟踪推特、博客文章和新闻文章等大型语料库中 95% 的单词。而且这仅仅是针对
    1-gram 或单词令牌的情况。一个旨在捕捉大型英语语料库中 95% 的 2-grams 的 2-gram 词汇量通常会有 100 万个以上的唯一 2-gram
    令牌。
- en: You may be worried that vocabulary size drives the required size of any training
    set you must acquire to avoid overfitting to any particular word or combination
    of words. And you know that the size of your training set drives the amount of
    processing required to process it all. However, getting rid of 100 stop words
    out of 20,000 is not going to significantly speed up your work. And for a 2-gram
    vocabulary, the savings you would achieve by removing stop words is minuscule.
    In addition, for 2-grams you lose a lot more information when you get rid of stop
    words arbitrarily, without checking for the frequency of the 2-grams that use
    those stop words in your text. For example, you might miss mentions of "The Shining"
    as a unique title and instead treat texts about that violent, disturbing movie
    the same as you treat documents that mention "Shining Light" or "shoe shining".
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能担心词汇量会影响你必须获取的训练集的大小，以避免过度拟合任何特定单词或单词组合。而且你知道，训练集的大小决定了需要处理的所有内容的处理量。但是，从
    20,000 个单词中去掉 100 个停用词并不会显著加快你的工作速度。而且对于 2-gram 词汇，通过去掉停用词而获得的节省微乎其微。此外，对于 2-gram
    词汇，当你随意去除停用词而不检查文本中使用这些停用词的 2-gram 的频率时，你会丢失更多的信息。例如，你可能会错过关于 "The Shining" 作为一个独特标题的提及，而将关于那部暴力、令人不安的电影的文本视为与提及
    "Shining Light" 或 "shoe shining" 的文档相同。
- en: So if you have sufficient memory and processing bandwidth to run all the NLP
    steps in your pipeline on the larger vocabulary, you probably do not want to worry
    about ignoring a few unimportant words here and there. And if you are worried
    about overfitting a small training set with a large vocabulary, there are better
    ways to select your vocabulary or reduce your dimensionality than ignoring stop
    words. Including stop words in your vocabulary allows the document frequency filters
    (discussed in chapter 3) to more accurately identify and ignore the words and
    *n*-grams with the least information content within your particular domain.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果你有足够的内存和处理带宽来在更大的词汇表上运行流水线中的所有 NLP 步骤，你可能不想担心偶尔忽略一些不重要的词语。如果你担心用大词汇表过度拟合一个小训练集，有更好的方法来选择你的词汇表或减少你的维度比忽略停用词更好。在你的词汇表中包括停用词允许文档频率过滤器（在第三章中讨论）更准确地识别和忽略你特定领域中信息量最少的词语和
    *n*-grams。
- en: The spaCy and NLTK packages include a variety of predefined sets of stop words
    for various use cases. ^([[42](#_footnotedef_42 "View footnote.")]) You probably
    won’t need a broad list of stopwords like the one in listing [2.6](#listing-broad-stop-words),
    but if you do you’ll want to check out both the spaCy and NLTK stopwords lists.
    And if you need an even broader set of stopwords you can `SearX` ^([[43](#_footnotedef_43
    "View footnote.")]) ^([[44](#_footnotedef_44 "View footnote.")]) for SEO companies
    that maintain lists of stopwords in many languages.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 和 NLTK 包含各种预定义的停用词集，适用于各种用例。^([[42](#_footnotedef_42 "查看脚注。")]) 你可能不需要像列表
    [2.6](#listing-broad-stop-words) 中那样广泛的停用词列表，但如果你需要，你应该查看一下 spaCy 和 NLTK 的停用词列表。如果你需要更广泛的停用词集，你可以
    `SearX` ^([[43](#_footnotedef_43 "查看脚注。")]) ^([[44](#_footnotedef_44 "查看脚注。")])
    搜索维护着多种语言停用词列表的 SEO 公司。
- en: If your NLP pipeline relies on a fine-tuned list of stop words to achieve high
    accuracy, it can be a significant maintenance headache. Humans and machines (search
    engines) are constantly changing which words they ignore. ^([[45](#_footnotedef_45
    "View footnote.")]) ^([[46](#_footnotedef_46 "View footnote.")]) If you can find
    the list of stopwords used by advertisers you can use them to detect manipulative
    web pages and SEO (Search Engine Optimization) content. If a web page or article
    doesn’t use stop words very often, it may have been "optimized" to deceive you.
    Listing [2.6](#listing-broad-stop-words) uses an exhaustive list of stopwords
    created from several of these sources. By filtering out this broad set of words
    from example text, you can see the amount of meaning "lost in translation." In
    most cases, you’ll find that ignoring stop words does not improve your NLP pipeline
    accuracy.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的 NLP 流水线依赖于一个经过精细调整的停用词列表来实现高准确度，那么它可能是一个重大的维护头痛。人类和机器（搜索引擎）不断变化着忽略哪些词语。^([[45](#_footnotedef_45
    "查看脚注。")]) ^([[46](#_footnotedef_46 "查看脚注。")]) 如果你能找到广告商使用的停用词列表，你可以用它们来检测欺骗性网页和
    SEO（搜索引擎优化）内容。如果一个网页或文章很少使用停用词，那么它可能被“优化”来欺骗你。列表 [2.6](#listing-broad-stop-words)
    使用了从这些来源中创建的详尽的停用词列表。通过从示例文本中过滤掉这个广泛的词汇集，你可以看到“翻译中丢失的意义”量。在大多数情况下，忽略停用词并不能提高你的
    NLP 流水线的准确性。
- en: Listing 2.6 Broad list of stop words
  id: totrans-383
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.6 广泛的停用词列表
- en: '[PRE42]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: This is a meaningful sentence from a short story by Ted Chiang about machines
    helping us remember our statements so we don’t have to rely on flawed memories.^([[47](#_footnotedef_47
    "View footnote.")]) In this phrase you lost two thirds of the words and only retained
    some of the meaning sentence’s meaning. However you can see that an important
    token "words" was discarded by using this particularly exhaustive set of stop
    words. You can sometimes get your point across without articles, prepositions,
    or even forms of the verb "to be." But it will reduce the precision and accuracy
    of your NLP pipeline, but at least some small amount of meaning will be lost.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一句有意义的句子，出自 Ted Chiang 的一篇短篇小说，讲述了机器帮助我们记住我们的陈述，这样我们就不必依赖有缺陷的记忆。^([[47](#_footnotedef_47
    "查看脚注。")]) 在这个短语中，你失去了三分之二的词语，只保留了一些意义的含义。但是你可以看到，通过使用这个特别详尽的停用词集，一个重要的标记“words”被丢弃了。有时候，你可以在不使用冠词、介词甚至动词“to
    be”的情况下表达你的观点。但这会降低你的 NLP 流水线的精度和准确性，但至少会丢失一些意义。
- en: You can see that some words carry more meaning than others. Imagine someone
    doing sign language or in a hurry to write a note to themselves. Which words would
    they choose to skip when they are in a hurry? That is the way linguists decide
    on lists of stop words. But if you’re in a hurry, and your NLP isn’t rushed for
    time like you are, you probably don’t want to waste your time creating and maintaining
    lists of stop words.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，有些单词比其他单词更有意义。想象一下做手语或赶着给自己写一张便条的人。当他们赶时间时会选择跳过哪些单词？这就是语言学家确定停用词列表的方法。但是，如果你赶时间，而你的NLP并不像你一样赶时间，那么你可能不想浪费时间创建和维护停用词列表。
- en: 'Here’s another common stop words list that isn’t quite as exhaustive:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有另一个不那么详尽的常见停用词列表：
- en: Listing 2.7 NLTK list of stop words
  id: totrans-388
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 代码清单2.7 NLTK停用词列表
- en: '[PRE43]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: A document that dwells on the first person is pretty boring, and more importantly
    for you, has low information content. The NLTK package includes pronouns (not
    just first person ones) in its list of stop words. And these one-letter stop words
    are even more curious, but they make sense if you have used the NLTK tokenizer
    and Porter stemmer a lot. These single-letter tokens pop up a lot when contractions
    are split and stemmed using NLTK tokenizers and stemmers.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 第一人称为主题的文档非常无聊，对你来说还更重要的是，它的信息量很低。NLTK包在其停用词列表中包括代词（不仅仅是第一人称代词）。这些单个字母的停用词更加好奇，但如果你经常使用NLTK分词器和波特词干剪裁器，它们就是有意义的。当使用NLTK分词器和剪裁器分裂缩略词时，这些单个字母标记会经常出现。
- en: Warning
  id: totrans-391
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 警告
- en: The set of English stop words in `sklearn`, `spacy`, `nltk`, and SEO tools are
    very different, and they are constantly evolving. At the time of this writing,
    `sklearn` has 318 stop words, NLTK has 179 stop words, spaCy has 326, and our
    'exhaustive' SEO list includes 667 stop words.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 在`sklearn`、`spacy`、`nltk`和SEO工具中的英文停用词集合非常不同，并且它们在不断发展。在撰写本文时，`sklearn`有318个停用词，NLTK有179个停用词，spaCy有326个停用词，我们的“详尽”SEO列表包括667个停用词。
- en: This is a good reason to consider **not** filtering stop words. If you do, others
    may not be able to reproduce your results.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 这是考虑**不要**过滤停用词的一个很好的理由。如果你这样做，其他人可能无法重现你的结果。
- en: 'Depending on how much natural language information you want to discard ;),
    you can take the union or the intersection of multiple stop word lists for your
    pipeline. Here are some stop_words lists we found, though we rarely use any of
    them in production:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 取决于您想要丢弃多少自然语言信息；）你可以取多个停用词列表的并集或交集，用于你的流程。这里有一些我们发现的停用词列表，但我们很少在生产中使用任何一个停用词列表：
- en: Listing 2.8 Collection of stop words lists
  id: totrans-395
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 代码清单2.8 停用词列表的集合
- en: '[PRE44]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 2.9.3 Normalizing your vocabulary
  id: totrans-397
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.9.3 规范化你的词汇表
- en: So you have seen how important vocabulary size is to the performance of an NLP
    pipeline. Another vocabulary reduction technique is to normalize your vocabulary
    so that tokens that mean similar things are combined into a single, normalized
    form. Doing so reduces the number of tokens you need to retain in your vocabulary
    and also improves the association of meaning across those different "spellings"
    of a token or *n*-gram in your corpus. And as we mentioned before, reducing your
    vocabulary can reduce the likelihood of overfitting.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经看到了词汇表大小对NLP流程性能的重要性。另一种减少词汇表的技术是规范化您的词汇表，以便将意思类似的标记组合成单个规范化的形式。这样做会减少您需要在词汇表中保留的标记数量，同时还提高语料库中该标记或*n*-gram的不同“拼写”之间的含义关联。正如我们之前提到的，减少词汇表可以减少过度拟合的
- en: Case folding
  id: totrans-399
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 折叠大小写
- en: Case folding is when you consolidate multiple "spellings" of a word that differ
    only in their capitalization. So why would we use case folding at all? Words can
    become case "denormalized" when they are capitalized because of their presence
    at the beginning of a sentence, or when they’re written in `ALL CAPS` for emphasis.
    Undoing this denormalization is called *case normalization*, or more commonly,
    *case folding*. Normalizing word and character capitalization is one way to reduce
    your vocabulary size and generalize your NLP pipeline. It helps you consolidate
    words that are intended to mean (and be spelled) the same thing under a single
    token.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 折叠大小写是指合并只有区分大小写的拼写不同的单词。那为什么我们要使用折叠大小写呢？当单词由于出现在句子开头或以`ALL CAPS`加粗书写时而被大写字母表示时，它们可能会变得不规范化。撤消这种不规范化称为*大小写规范化*，或更普遍地，*折叠大小写*。规范化单词和字符大小写是减小词汇表大小和泛化NLP流程的方法之一。它帮助您将旨在表示（并拼写）相同意思的单词合并为单个标记。
- en: However, some information is often communicated by capitalization of a word — for
    example, 'doctor' and 'Doctor' often have different meanings. Often capitalization
    is used to indicate that a word is a proper noun, the name of a person, place,
    or thing. You will want to be able to recognize proper nouns as distinct from
    other words, if named entity recognition is important to your pipeline. However,
    if tokens are not case normalized, your vocabulary will be approximately twice
    as large, consume twice as much memory and processing time, and might increase
    the amount of training data you need to have labeled for your machine learning
    pipeline to converge to an accurate, general solution. Just as in any other machine
    learning pipeline, your labeled dataset used for training must be "representative"
    of the space of all possible feature vectors your model must deal with, including
    variations in capitalization. For 100000-D bag-of-words vectors, you usually must
    have 100000 labeled examples, and sometimes even more than that, to train a supervised
    machine learning pipeline without overfitting. In some situations, cutting your
    vocabulary size by half can sometimes be worth the loss of information content.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有些信息经常通过单词的大写来传达——例如，“doctor”和“Doctor”通常具有不同的含义。通常，大写用于指示单词是专有名词，即人、地点或事物的名称。如果命名实体识别对您的流水线很重要，您将希望能够将专有名词与其他单词区分开来。然而，如果标记未进行大小写规范化，您的词汇量将大约增加一倍，消耗的内存和处理时间也会增加一倍，可能需要增加您需要为机器学习流水线标记的训练数据的数量，以便该流水线能够收敛到准确、普遍的解决方案。就像在任何其他机器学习流水线中一样，用于训练的标记数据集必须“代表”模型必须处理的所有可能特征向量的空间，包括大小写的变化。对于100000维词袋向量，通常必须有100000个带标签的示例，有时甚至更多，才能训练一个无过拟合的监督机器学习流水线。在某些情况下，将词汇量减少一半有时可能值得信息内容的损失。
- en: In Python, you can easily normalize the capitalization of your tokens with a
    list comprehension.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，您可以使用列表推导式轻松规范化标记的大小写。
- en: '[PRE45]'
  id: totrans-403
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: And if you are certain that you want to normalize the case for an entire document,
    you can `lower()` the text string in one operation, before tokenization. But this
    will prevent advanced tokenizers that can split *camel case* words like "WordPerfect",
    "FedEx", or "stringVariableName."^([[48](#_footnotedef_48 "View footnote.")])]
    Maybe you want WordPerfect to be its own unique thing (token), or maybe you want
    to reminisce about a more perfect word processing era. It is up to you to decide
    when and how to apply case folding.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您确信要对整个文档进行大小写规范化，可以在标记化之前使用`lower()`函数一次性将文本字符串转换为小写字母。但是，这将阻止能够分割*驼峰大小写*单词（如“WordPerfect”、“FedEx”或“stringVariableName”）的高级标记器。^([[48](#_footnotedef_48
    "查看脚注。")]) 或许您希望WordPerfect成为自己独特的东西（标记），或者您希望怀念更完美的文字处理时代。您可以决定何时以及如何应用大小写折叠。
- en: With case normalization, you are attempting to return these tokens to their
    "normal" state before grammar rules and their position in a sentence affected
    their capitalization. The simplest and most common way to normalize the case of
    a text string is to lowercase all the characters with a function like Python’s
    built-in `str.lower()`.^([[49](#_footnotedef_49 "View footnote.")]) Unfortunately
    this approach will also "normalize" away a lot of meaningful capitalization in
    addition to the less meaningful first-word-in-sentence capitalization you intended
    to normalize away. A better approach for case normalization is to lowercase only
    the first word of a sentence and allow all other words to retain their capitalization.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 通过大小写规范化，您试图将这些标记返回到它们在语法规则和它们在句子中的位置影响其大写之前的“正常”状态。将文本字符串的大小写规范化为最简单和最常见的方法是使用诸如Python内置的`str.lower()`之类的函数将所有字符都转换为小写字母。^([[49](#_footnotedef_49
    "查看脚注。")]) 不幸的是，这种方法也会“规范化”掉许多有意义的大写字母，除了您打算规范化掉的不那么有意义的句子中的第一个单词的大写字母之外。对于大小写规范化，更好的方法是仅将句子的第一个单词转换为小写字母，然后让所有其他单词保留其大写字母。
- en: Lowercasing on the first word in a sentence preserves the meaning of a proper
    nouns in the middle of a sentence, like "Joe" and "Smith" in "Joe Smith". And
    it properly groups words together that belong together, because they are only
    capitalized when they are at the beginning of a sentence, since they are not proper
    nouns. This prevents "Joe" from being confused with "coffee" ("joe")^([[50](#_footnotedef_50
    "View footnote.")]) during tokenization. And this approach prevents the blacksmith
    connotation of "smith" being confused the the proper name "Smith" in a sentence
    like "A word smith had a cup of joe." Even with this careful approach to case
    normalization, where you lowercase words only at the start of a sentence, you
    will still need to introduce capitalization errors for the rare proper nouns that
    start a sentence. "Joe Smith, the word smith, with a cup of joe." will produce
    a different set of tokens than "Smith the word with a cup of joe, Joe Smith."
    And you may not want that. In addition, case normalization is useless for languages
    that do not have a concept of capitalization, like Arabic or Hindi.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 在句子的第一个单词上进行小写处理可以保留句子中的专有名词的含义，例如在“Joe Smith”中的“Joe”和“Smith”。并且它可以正确地将属于一起的单词分组在一起，因为它们只有在句子开头才会大写，因为它们不是专有名词。这可以防止在标记化期间将“Joe”与“coffee”（“joe”）混淆。这种方法可以防止“smith”的铁匠含义与句子中的专有名词“Smith”混淆，例如在“A
    word smith had a cup of joe.”中。即使采用了对句子开头的单词进行小写处理的小心方法，您仍然需要为偶尔在句子开头的专有名词引入大小写错误。“Joe
    Smith, the word smith, with a cup of joe.”将产生与“Smith the word with a cup of joe,
    Joe Smith.”不同的标记集合。您可能不希望出现这种情况。此外，对于没有大写概念的语言（如阿拉伯语或印地语），大小写规范化是无用的。
- en: To avoid this potential loss of information, many NLP pipelines do not normalize
    for case at all. For many applications, the efficiency gain (in storage and processing)
    for reducing one’s vocabulary size by about half is outweighed by the loss of
    information for proper nouns. But some information may be "lost" even without
    case normalization. If you do not identify the word "The" at the start of a sentence
    as a stop word, that can be a problem for some applications. Really sophisticated
    pipelines will detect proper nouns before selectively normalizing the case for
    words at the beginning of sentences that are clearly not proper nouns. You should
    implement whatever case normalization approach makes sense for your application.
    If you do not have a lot of "Smith"s and "word smiths" in your corpus, and you
    do not care if they get assigned to the the same tokens, you can just lowercase
    everything. The best way to find out what works is to try several different approaches,
    and see which approach gives you the best performance for the objectives of your
    NLP project.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这种潜在的信息丢失，许多自然语言处理流水线根本不对大小写进行规范化。对于许多应用程序，将词汇量减半的效率收益（在存储和处理方面）被由于专有名词信息的丢失而抵消了。但是即使不进行大小写规范化，也可能会“丢失”一些信息。如果您没有将句子开头的单词“The”识别为停止词，对于某些应用程序来说这可能是个问题。真正复杂的流水线将在有明确不是专有名词的句子开头的词的情况下先检测专有名词，然后选择性地对这些词进行规范化。您应该实施对您的应用程序有意义的任何大小写规范化方法。如果您的语料库中没有很多“Smith”和“word
    smiths”，并且您不在乎它们是否被分配给相同的标记，您可以将所有内容都转换为小写。找出有效方法的最佳方式是尝试几种不同的方法，看看哪种方法对您自然语言处理项目的目标性能最佳。
- en: By generalizing your model to work with text that has odd capitalization, case
    normalization can reduce overfitting for your machine learning pipeline. Case
    normalization is particularly useful for a search engine. For search, normalization
    increases the number of matches found for a particular query. This is often called
    the "recall" performance metric for a search engine (or any other classification
    model).^([[51](#_footnotedef_51 "View footnote.")])
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型推广到处理具有奇怪大写形式的文本时，大小写规范化可以减少机器学习流水线的过拟合。大小写规范化对于搜索引擎特别有用。对于搜索来说，规范化可以增加对特定查询找到的匹配项数量。这通常被称为搜索引擎（或任何其他分类模型）的“召回率”性能指标。^([[51](#_footnotedef_51
    "查看脚注。")])
- en: For a search engine without normalization if you searched for "Age" you will
    get a different set of documents than if you searched for "age." "Age" would likely
    occur in phrases like "New Age" or "Age of Reason". In contrast, "age" would be
    more likely to occur in phrases like "at the age of" in your sentence about Thomas
    Jefferson. By normalizing the vocabulary in your search index (as well as the
    query), you can ensure that both kinds of documents about "age" are returned regardless
    of the capitalization in the query from the user.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个没有规范化的搜索引擎，如果你搜索“Age”，你将得到与如果你搜索“age”时不同的文档集合。 “Age”很可能出现在诸如“New Age”或“Age
    of Reason”之类的短语中。相比之下，“age”更可能出现在你有关托马斯·杰斐逊的句子中的“在...岁时”的短语中。通过在搜索索引（以及查询）中对词汇进行规范化，您可以确保无论用户查询的大小写如何，都会返回关于“age”的这两种类型的文档。
- en: 'However, this additional recall accuracy comes at the cost of precision, returning
    many documents that the user may not be interested in. Because of this issue,
    modern search engines allow users to turn off normalization with each query, typically
    by quoting those words for which they want only exact matches returned. If you
    are building such a search engine pipeline, in order to accommodate both types
    of queries you will have to build two indexes for your documents: one with case-normalized
    *n*-grams, and another with the original capitalization.'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种额外的召回准确性是以精度的代价为代价的，会返回许多用户不感兴趣的文档。由于这个问题，现代搜索引擎允许用户关闭每个查询的规范化，通常通过引用那些他们只希望返回精确匹配的单词来实现。如果您正在构建这样的搜索引擎管道，为了适应两种类型的查询，您将需要为您的文档构建两个索引：一个具有规范化的*n*-grams，另一个具有原始大小写。
- en: Stemming
  id: totrans-411
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 词干处理
- en: Another common vocabulary normalization technique is to eliminate the small
    meaning differences of pluralization or possessive endings of words, or even various
    verb forms. This normalization, identifying a common stem among various forms
    of a word, is called stemming. For example, the words `housing` and `houses` share
    the same stem, `house`. Stemming removes suffixes from words in an attempt to
    combine words with similar meanings together under their common stem. A stem is
    not required to be a properly spelled word, but merely a token, or label, representing
    several possible spellings of a word.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的词汇规范化技术是消除词汇的复数或所有格结尾甚至各种动词形式的细微含义差异，即识别单词的各种形式之间的共同词干，称为词干处理。例如，单词“housing”和“houses”共享相同的词干“house”。词干处理尝试从单词中去除后缀，以便将具有相似含义的单词组合在其共同的词干下。词干不需要是一个正确拼写的单词，而仅仅是一个标记或标签，代表单词的几种可能拼写。
- en: A human can easily see that "house" and "houses" are the singular and plural
    forms of the same noun. However, you need some way to provide this information
    to the machine. One of its main benefits is in the compression of the number of
    words whose meaning your software or language model needs to keep track of. It
    reduces the size of your vocabulary while limiting the loss of information and
    meaning, as much as possible. In machine learning this is referred to as dimension
    reduction. It helps generalize your language model, enabling the model to behave
    identically for all the words included in a stem. So, as long as your application
    does not require your machine to distinguish between "house" and "houses", this
    stem will reduce your programming or dataset size by half or even more, depending
    on the aggressiveness of the stemmer you chose.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 人类可以很容易地看出“house”和“houses”是同一名词的单数和复数形式。然而，您需要一些方法将此信息提供给机器。其中一个主要好处是压缩您的软件或语言模型需要跟踪意义的单词数量。它在尽可能限制信息和含义损失的同时缩小了您的词汇量。在机器学习中，这被称为降维。它有助于概括您的语言模型，使该模型能够对包括在词干中的所有单词保持相同的行为。因此，只要您的应用程序不要求机器区分“house”和“houses”，此词干将减少您的编程或数据集大小一半或更多，具体取决于您选择的词干处理器的强度。
- en: Stemming is important for keyword search or information retrieval. It allows
    you to search for "developing houses in Portland" and get web pages or documents
    that use both the word "house" and "houses" and even the word "housing" because
    these words are all stemmed to the "hous" token. Likewise you might receive pages
    with the words "developer" and "development" rather than "developing" because
    all these words typically reduce to the stem "develop". As you can see, this is
    a "broadening" of your search, ensuring that you are less likely to miss a relevant
    document or web page. This broadening of your search results would be a big improvement
    in the "recall" score for how well your search engine is doing its job at returning
    all the relevant documents.^([[52](#_footnotedef_52 "View footnote.")])
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 词干提取对于关键词搜索或信息检索非常重要。它允许你搜索“在波特兰开发房屋”，并获得同时使用“house”和“houses”甚至“housing”这些词的网页或文档，因为这些词都被词干提取为“hous”标记。同样，你可能会收到包含“developer”和“development”而不是“developing”的页面，因为所有这些词通常缩减到“develop”这个词干。正如你所见，这是对你搜索的“扩展”，确保你不太可能错过相关文档或网页。这种搜索结果的扩展将是搜索引擎“召回”分数的重大改进，表明你的搜索引擎在返回所有相关文档方面的工作效果如何。^([[52](#_footnotedef_52
    "View footnote.")])
- en: But stemming could greatly reduce the "precision" score for your search engine
    because it might return many more irrelevant documents along with the relevant
    ones. In some applications this "false-positive rate" (proportion of the pages
    returned that you do not find useful) can be a problem. So most search engines
    allow you to turn off stemming and even case normalization by putting quotes around
    a word or phrase. Quoting indicates that you only want pages containing the exact
    spelling of a phrase such as "'Portland Housing Development software'." That would
    return a different sort of document than one that talks about a "'a Portland software
    developer’s house'." And there are times when you want to search for "Dr. House’s
    calls" and not "dr house call", which might be the effective query if you used
    a stemmer on that query.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，词干提取可能会大大降低搜索引擎的“精确度”分数，因为它可能会返回许多与相关文档一起的无关文档。在某些应用中，这种“误报率”（返回的页面中你不认为有用的比例）可能是一个问题。因此，大多数搜索引擎允许你通过在单词或短语周围加上引号来关闭词干提取甚至大小写规范化。引号表示你只想要包含完全拼写相同的短语的页面，比如“'波特兰房地产开发软件'”。这将返回一种不同类型的文档，而不是那些谈论“'波特兰软件开发者的房子'”的文档。有时你想搜索“Dr.
    House's calls”，而不是“dr house call”，如果你在该查询上使用了一个词干提取器，后者可能是有效的查询。
- en: Here’s a simple stemmer implementation in pure Python that can handle trailing
    S’s.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个在纯 Python 中实现的简单词干提取器，可以处理末尾的 `s`。
- en: '[PRE46]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The preceding stemmer function follows a few simple rules within that one short
    regular expression:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的词干提取器函数遵循了一个简单的规则，就在那一个简短的正则表达式中：
- en: If a word ends with more than one `s`, the stem is the word and the suffix is
    a blank string.
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个单词以多于一个`s`结尾，那么词干就是该词，后缀为空字符串。
- en: If a word ends with a single `s`, the stem is the word without the `s` and the
    suffix is the `s`.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个单词以单个 `s` 结尾，那么词干就是不带 `s` 的该词，后缀是 `s`。
- en: If a word does not end on an `s`, the stem is the word and no suffix is returned.
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个单词不以 `s` 结尾，那么词干就是该词，不返回后缀。
- en: The strip method ensures that some possessive words can be stemmed along with
    plurals.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: strip 方法确保一些所有格词能够与复数形式一起被词干提取。
- en: This function works well for regular cases, but is unable to address more complex
    cases. For example, the rules would fail with words like `dishes` or `heroes`.
    For more complex cases like these, the NLTK package provides other stemmers.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数对于常规情况效果很好，但无法解决更复杂的情况。例如，这些规则在诸如 `dishes` 或 `heroes` 之类的单词上会失败。对于这种更复杂的情况，NLTK
    包提供了其他的词干提取器。
- en: It also does not handle the "housing" example from your "Portland Housing" search.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 它也不能处理你“波特兰房地产”搜索中的“housing”示例。
- en: Two of the most popular stemming algorithms are the Porter and Snowball stemmers.
    The Porter stemmer is named for the computer scientist Martin Porter who spent
    most of the 80’s and 90’s fine tuning this hard-coded algorithm.^([[53](#_footnotedef_53
    "View footnote.")]) Porter is also also responsible for enhancing the Porter stemmer
    to create the Snowball stemmer.^([[54](#_footnotedef_54 "View footnote.")]) Porter
    dedicated much of his lengthy career to documenting and improving stemmers, due
    to their value in information retrieval (keyword search). These stemmers implement
    more complex rules than our simple regular expression. This enables the stemmer
    to handle the complexities of English spelling and word ending rules.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 最流行的两种词干提取算法是波特和Snowball词干提取器。波特词干提取器以计算机科学家马丁·波特的名字命名，他在80年代和90年代大部分时间里不断调整这个硬编码算法。^([[53](#_footnotedef_53
    "查看脚注。")]) 波特还负责改进波特词干提取器以创建Snowball词干提取器。^([[54](#_footnotedef_54 "查看脚注。")])
    波特在他漫长的职业生涯中致力于记录和改进词干提取器，因为它们在信息检索（关键词搜索）中很有价值。这些词干提取器实现了比我们简单的正则表达式更复杂的规则。这使得词干提取器能够处理英语拼写和词尾规则的复杂性。
- en: '[PRE47]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Notice that the Porter stemmer, like the regular expression stemmer, retains
    the trailing apostrophe (unless you explicitly strip it), which ensures that possessive
    words will be distinguishable from nonpossessive words. Possessive words are often
    proper nouns, so this feature can be important for applications where you want
    to treat names differently than other nouns.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，波特词干提取器与正则表达式词干提取器一样，保留了尾随的撇号（除非你明确删除它），这确保了所有格词与非所有格词之间的区分。所有格词通常是专有名词，因此这个特性在你想要将名称与其他名词区分对待的应用程序中可能很重要。
- en: More on the Porter stemmer
  id: totrans-428
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 更多关于波特词干提取器的内容
- en: Julia Menchavez has graciously shared her translation of Porter’s original stemmer
    algorithm into pure python ([https://github.com/jedijulia/porter-stemmer/blob/master/stemmer.py](master.html)).
    If you are ever tempted to develop your own stemmer, consider these 300 lines
    of code and the lifetime of refinement that Porter put into them.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: Julia Menchavez 慷慨地分享了她对波特原始词干提取器算法的纯 Python 翻译（[https://github.com/jedijulia/porter-stemmer/blob/master/stemmer.py](master.html)）。如果你曾经想过开发自己的词干提取器，请考虑这
    300 行代码以及波特花在它们身上的精益求精的一生。
- en: 'There are eight steps to the Porter stemmer algorithm: 1a, 1b, 1c, 2, 3, 4,
    5a, and 5b. Step 1a is a bit like your regular expression for dealing with trailing
    "S"es:^([[55](#_footnotedef_55 "View footnote.")])'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 波特词干提取算法有八个步骤：1a、1b、1c、2、3、4、5a 和 5b。步骤 1a 有点像处理尾随“s”的正则表达式:^([[55](#_footnotedef_55
    "查看脚注。")])
- en: '[PRE48]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The remainining seven steps are much more complicated because they have to
    deal with the complicated English spelling rules for the following:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的七个步骤要复杂得多，因为它们必须处理以下复杂的英语拼写规则：
- en: '**Step 1a**: "s" and "es" endings'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤 1a**：以“s”和“es”结尾'
- en: '**Step 1b**: "ed", "ing", and "at" endings'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤 1b**：以“ed”，“ing”和“at”结尾'
- en: '**Step 1c**: "y" endings'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤 1c**：以“y”结尾'
- en: '**Step 2**: "nounifying" endings such as "ational", "tional", "ence", and "able"'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤 2**：使名词化的结尾，比如“ational”，“tional”，“ence”和“able”'
- en: '**Step 3**: adjective endings such as "icate",^([[56](#_footnotedef_56 "View
    footnote.")]), "ful", and "alize"'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤 3**：形容词结尾，比如“icate”，^([[56](#_footnotedef_56 "查看脚注。")])，“ful”和“alize”'
- en: '**Step 4**: adjective and noun endings such as "ive", "ible", "ent", and "ism"'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤 4**：形容词和名词结尾，比如“ive”，“ible”，“ent”和“ism”'
- en: '**Step 5a**: stubborn "e" endings, still hanging around'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤 5a**：顽固的“e”结尾，仍然存在'
- en: '**Step 5b**: trailing double-consonants for which the stem will end in a single
    "l"'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤 5b**：尾随的双辅音，词干将以单个“l”结尾'
- en: Snowball stemmer is more aggressive than the Porter stemmer. Notice that it
    stems 'fairly' to 'fair', which is more accurate than the Porter stemmer.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: Snowball词干提取器比波特词干提取器更具侵略性。注意它将“fairly”词干提取为“fair”，这比波特词干提取器更准确。
- en: '[PRE49]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Lemmatization
  id: totrans-443
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 词形归一化
- en: If you have access to information about connections between the meanings of
    various words, you might be able to associate several words together even if their
    spelling is quite different. This more extensive normalization down to the semantic
    root of a word — its lemma — is called lemmatization.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有关于各种单词含义之间的联系的信息，你可能能够将几个单词关联起来，即使它们的拼写非常不同。这种更广泛的归一化到一个单词的语义根 - 它的词元 -
    被称为词形归一化。
- en: In chapter 12, we show how you can use lemmatization to reduce the complexity
    of the logic required to respond to a statement with a chatbot. Any NLP pipeline
    that wants to "react" the same for multiple different spellings of the same basic
    root word can benefit from a lemmatizer. It reduces the number of words you have
    to respond to, the dimensionality of your language model. Using it can make your
    model more general, but it can also make your model less precise, because it will
    treat all spelling variations of a given root word the same. For example "chat",
    "chatter", "chatty", "chatting", and perhaps even "chatbot" would all be treated
    the same in an NLP pipeline with lemmatization, even though they have different
    meanings. Likewise "bank", "banked", and "banking" would be treated the same by
    a stemming pipeline despite the river meaning of "bank", the motorcycle meaning
    of "banked" and the finance meaning of "banking."
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 在第12章中，我们展示了如何使用词形归一化来减少响应聊天机器人声明所需的逻辑复杂性。任何想要对相同基本根词的多种不同拼写“反应”相同的NLP流水线都可以从词形归一化器中受益。它减少了您必须回应的单词数量，您的语言模型的维度。使用它可以使您的模型更通用，但也可能使您的模型不太精确，因为它将给定根词的所有拼写变体都视为相同。例如，在具有词形归一化的NLP流水线中，“chat”、“chatter”、“chatty”、“chatting”甚至可能是“chatbot”都将被视为相同，即使它们有不同的含义。同样，“bank”、“banked”和“banking”在词干提取流水线中也将被视为相同，尽管“bank”的含义是“河岸”，“banked”的含义是“机车”，“banking”的含义是“金融”。
- en: As you work through this section, think about words where lemmatization would
    drastically alter the meaning of a word, perhaps even inverting its meaning and
    producing the opposite of the intended response from your pipeline. This scenario
    is called *spoofing* — when you try to elicit the wrong response from a machine
    learning pipeline by cleverly constructing a difficult input.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 当您通过本节时，请考虑在词形归一化会大大改变单词含义的情况下，甚至颠倒其含义并从您的流水线产生相反的预期响应。这种情况被称为*欺骗* - 当您试图通过巧妙构造一个困难的输入来引诱机器学习管道产生错误的响应时。
- en: Sometimes lemmatization will be a better way to normalize the words in your
    vocabulary. You may find that for your application stemming and case folding create
    stems and tokens that do not take into account a word’s meaning. A lemmatizer
    uses a knowledge base of word synonyms and word endings to ensure that only words
    that mean similar things are consolidated into a single token.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，词形归一化可能是规范化词汇的更好方式。您可能会发现，对于您的应用程序来说，词干提取和大小写转换会创建不考虑单词含义的词干和标记。词形归一化器使用单词同义词和词尾的知识库，以确保只有意思相似的单词被合并成一个单一的标记。
- en: Some lemmatizers use the word’s part of speech (POS) tag in addition to its
    spelling to help improve accuracy. The POS tag for a word indicates its role in
    the grammar of a phrase or sentence. For example, the noun POS is for words that
    refer to "people, places, or things" within a phrase. An adjective POS is for
    a word that modifies or describes a noun. A verb refers to an action. The POS
    of a word in isolation cannot be determined. The context of a word must be known
    for its POS to be identified. So some advanced lemmatizers cannot be run on words
    in isolation.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 一些词形归一化器除了拼写外，还使用单词的词性（POS）标记来帮助提高准确性。单词的词性标记表示其在短语或句子语法中的作用。例如，名词POS是指短语中指代“人、地方或物品”的单词。形容词POS是用于修饰或描述名词的单词。动词指的是一种动作。无法确定单词在孤立状态下的POS。必须了解单词的上下文才能确定其POS。因此，一些高级词形归一化器不能在孤立的单词上运行。
- en: Can you think of ways you can use the part of speech to identify a better "root"
    of a word than stemming could? Consider the word `better`. Stemmers would strip
    the "er" ending from "better" and return the stem "bett" or "bet". However, this
    would lump the word "better" with words like "betting", "bets", and "Bet’s", rather
    than more similar words like "betterment", "best", or even "good" and "goods".
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 你能想到利用词性来识别比词干提取更好的单词“根”吗？考虑单词`better`。词干提取器会从“better”中剥离“er”结尾，并返回词干“bett”或“bet”。然而，这会将“better”与“betting”、“bets”和“Bet’s”等单词一起，而不是与更相似的单词如“betterment”、“best”或甚至“good”和“goods”一起。
- en: So lemmatizers are better than stemmers for most applications. Stemmers are
    only really used in large scale information retrieval applications (keyword search).
    And if you really want the dimension reduction and recall improvement of a stemmer
    in your information retrieval pipeline, you should probably also use a lemmatizer
    right before the stemmer. Because the lemma of a word is a valid English word,
    stemmers work well on the output of a lemmatizer. This trick will reduce your
    dimensionality and increase your information retrieval recall even more than a
    stemmer alone.^([[57](#_footnotedef_57 "View footnote.")])
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于大多数应用程序来说，词形还原器比词干提取器更好。词干提取器仅在大规模信息检索应用程序（关键字搜索）中真正有用。如果你真的想要在信息检索管道中获得词干提取器的降维和召回率提升效果，你应该在词干提取器之前使用词形还原器。因为单词的词形是一个有效的英文单词，所以词干提取器在词形还原器的输出上表现良好。这个技巧将比仅使用词干提取器进一步降低你的维度，并增加你的信息检索召回率。^([[57](#_footnotedef_57
    "View footnote.")])
- en: 'How can you identify word lemmas in Python? The NLTK package provides functions
    for this. Notice that you must tell the WordNetLemmatizer which part of speech
    you are interested in, if you want to find the most accurate lemma:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 如何在 Python 中识别单词的词形还原？NLTK 包提供了相应的功能。请注意，如果想找到最准确的词形还原，你必须告诉 WordNetLemmatizer
    你感兴趣的词性：
- en: '[PRE50]'
  id: totrans-452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: You might be surprised that the first attempt to lemmatize the word "better"
    did not change it at all. This is because the part of speech of a word can have
    a big effect on its meaning. If a POS is not specified for a word, then the NLTK
    lemmatizer assumes it is a noun. Once you specify the correct POS, 'a' for adjective,
    the lemmatizer returns the correct lemma. Unfortunately, the NLTK lemmatizer is
    restricted to the connections within the Princeton WordNet graph of word meanings.
    So the word "best" does not lemmatize to the same root as "better". This graph
    is also missing the connection between "goodness" and "good". A Porter stemmer,
    on the other hand, would make this connection by blindly stripping off the "ness"
    ending of all words.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会感到惊讶，第一次尝试对单词"better"进行词形还原时并没有改变它。这是因为单词的词性对其含义有很大影响。如果没有为单词指定词性，则 NLTK
    词形还原器会假定它是一个名词。一旦指定了正确的词性，比如用 'a' 表示形容词，词形还原器就会返回正确的词形。不幸的是，NLTK 词形还原器受限于普林斯顿
    WordNet 单词含义图中的连接。因此，单词"best"并不会被还原为与"better"相同的词根。这个图表还缺少"goodness"和"good"之间的连接。另一方面，波特词干提取器会通过盲目地剥离所有单词的"ness"结尾来建立这种连接。
- en: '[PRE51]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'You can easily implement lemmatization in spaCy by the following:'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过以下方式轻松地在 spaCy 中实现词形还原：
- en: '[PRE52]'
  id: totrans-456
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Unlike NLTK, spaCy lemmatizes "better" to "well" by assuming it is an adverb
    and returns the correct lemma for "best" ("good").
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 与 NLTK 不同，spaCy 通过假设"better"是一个副词将其词形还原为"well"，并为"best"（"good"）返回了正确的词形。
- en: Synonym substitution
  id: totrans-458
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 同义词替换
- en: There are five kinds of "synonyms" that are sometimes helpful in creating a
    consistent smaller vocabulary to help your NLP pipeline generalize well.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 有五种类型的 "同义词"，有时有助于创建一个一致的更小词汇表，以帮助你的自然语言处理管道良好地概括。
- en: Typo correction
  id: totrans-460
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打字错误纠正
- en: Spelling correction
  id: totrans-461
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拼写纠正
- en: Synonym substitution
  id: totrans-462
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同义词替换
- en: Contraction expansion
  id: totrans-463
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 缩写扩展
- en: Emoji expansion
  id: totrans-464
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 表情符号扩展
- en: Each of these synonym substitution algorithms can be designed to be more or
    less agressive. And you will want to think about the language used by your users
    in your domain. For example, in the legal, technical, or medical fields, it’s
    rarely a good idea to substitute synonyms. A doctor wouldn’t want a chatbot telling
    his patient their "heart is broken" because of some synonym substitutions on the
    heart emoticon ("<3").
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 这些同义词替换算法中的每一个都可以设计得更具侵略性或更加温和。你需要考虑你领域用户使用的语言。例如，在法律、技术或医学领域，很少有替换同义词的好主意。医生不会希望一个聊天机器人告诉他的病人，他们的"heart
    is broken"是因为一些对心脏表情符号 ("<3") 的同义词替换。
- en: Nonetheless, the use cases for lemmatization and stemming apply to synonym substitution.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，词形还原和词干提取的使用案例适用于同义词替换。
- en: Use cases
  id: totrans-467
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用案例
- en: When should you use a lemmatizer, stemmer, or synonym substitution? Stemmers
    are generally faster to compute and require less-complex code and datasets. But
    stemmers will make more errors and stem a far greater number of words, reducing
    the information content or meaning of your text much more than a lemmatizer would.
    Both stemmers and lemmatizers will reduce your vocabulary size and increase the
    ambiguity of the text. But lemmatizers do a better job retaining as much of the
    information content as possible based on how the word was used within the text
    and its intended meaning. As a result, some state of the art NLP packages, such
    as spaCy, do not provide stemming functions and only offer lemmatization methods.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 何时使用词形还原器、词干处理器或同义词替换？词干处理器通常计算速度更快，需要的代码和数据集更简单。但是，词干处理器会产生更多错误并对更多单词进行词干处理，比词形还原器更多地降低了文本的信息内容或含义。词干处理器和词形还原器都会减少您的词汇量并增加文本的歧义性。但是，词形还原器会更好地根据单词在文本中的使用方式和其预期含义保留尽可能多的信息内容。因此，一些最先进的自然语言处理包，如spaCy，不提供词干处理功能，而只提供词形还原方法。
- en: If your application involves search, stemming and lemmatization will improve
    the recall of your searches by associating more documents with the same query
    words. However, stemming, lemmatization, and even case folding will usually reduce
    the precision and accuracy of your search results. These vocabulary compression
    approaches may cause your information retrieval system (search engine) to return
    many documents not relevant to the words' original meanings. These are called
    "false positives", a incorrect matches to your search query. Sometimes "false
    positives" are less important than false negatives. A false negative for a search
    engine is when it fails to list the document you are looking for at all.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的应用涉及搜索，词干处理和词形还原将通过将更多文档与相同的查询词相关联来提高搜索的召回率。但是，词干处理、词形还原甚至大小写折叠通常会降低搜索结果的精度和准确性。这些词汇压缩方法可能导致您的信息检索系统（搜索引擎）返回许多与原始含义不相关的文档。这些被称为“假阳性”，是对您的搜索查询的错误匹配。有时“假阳性”比假阴性更不重要。对于搜索引擎来说，假阴性是指它根本没有列出您要查找的文档。
- en: Because search results can be ranked according to relevance, search engines
    and document indexes typically use lemmatization when they process your query
    and index your documents. Because search results can be ranked according to relevance,
    search engines and document indexes typically use lemmatization in their NLP pipeline.
    This means a search engine will use lemmatization when they tokenize your search
    text as well as when they index their collection of documents, such as the web
    pages they crawl.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 因为搜索结果可以根据相关性进行排名，搜索引擎和文档索引通常在处理您的查询和索引文档时使用词形还原。因为搜索结果可以根据相关性进行排名，搜索引擎和文档索引通常在其自然语言处理管道中使用词形还原。这意味着搜索引擎在对您的搜索文本进行分词和对其抓取的网页等文档集进行索引时都会使用词形还原。
- en: But they combine search results for unstemmed versions of words to rank the
    search results that they present to you.^([[58](#_footnotedef_58 "View footnote.")])
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，它们会合并单词的未经词干处理的版本的搜索结果，以对向您呈现的搜索结果进行排名。
- en: For a search-based chatbot, precision is usually more important than recall.
    A false positive match can cause your chatbot says something inappropriate. False
    negatives just cause your chatbot to have to humbly admit that it cannot find
    anything appropriate to say. Your chatbot will sound better if your NLP pipeline
    first searches for matches to your user’s questions using unstemmed, unnormalized
    words. Your search algorithm can fall back to normalized token matches if it cannot
    find anything else to say. And you can rank these **fallback** matches for normalized
    tokens lower than the unnormalized token matches. You can even give your bot humility
    and transparency by introducing lower ranked responses with a caveat, such as
    "I haven’t heard a phrase like that before, but using my stemmer I found…​" In
    a modern world crowded with blowhard chatbots, your humbler chatbot can make a
    name for itself and win out!^([[59](#_footnotedef_59 "View footnote.")])
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于搜索的聊天机器人，精确度通常比召回率更重要。一个错误的正向匹配可能导致您的聊天机器人说出不恰当的话。假阴性只会导致您的聊天机器人不得不谦虚地承认它找不到合适的话可说。如果您的自然语言处理流水线首先使用未词干化、未归一化的词搜索用户问题的匹配，您的聊天机器人会听起来更好。如果找不到其他内容，您的搜索算法可以退回到标准化的令牌匹配。您甚至可以通过引入一些警告来降低标准化令牌匹配的排名，从而使您的机器人谦卑透明，比如“我以前没听过这样的话，但使用我的词干器，我找到了……”。在一个充斥着吹牛的聊天机器人的现代世界里，您谦虚的聊天机器人可以立足并获胜！
- en: There are 4 situations when synonym substitution of some sort may make sense.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 有4种情况适合进行某种形式的同义词替换。
- en: Search engines
  id: totrans-474
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 搜索引擎
- en: Data augmentation
  id: totrans-475
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据增强
- en: Scoring the robustness of your NLP
  id: totrans-476
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估您的自然语言处理的健壮性
- en: Adversarial NLP
  id: totrans-477
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对抗性自然语言处理
- en: Search engines can improve their recall for rare terms by using synonym substitution.
    When you have limited labeled data, you can often expand your dataset 10 fold
    (10x) with synonym substitution alone. If you want to find a lower bound on the
    accuracy of your model you can aggressively substitute synonyms in your test set
    to see how robust your model is to these changes. And if you are searching for
    ways to poison or evade detection by an NLP algorithm, synonyms can give you a
    large number of probing texts to try. You can imagine that substituting the "currency"
    for the word "cash", "dollars", or "" might help evade a spam detector.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索引擎可以通过使用同义词替换来提高对罕见术语的召回率。当您有限的标记数据时，您通常可以仅通过同义词替换将数据集扩展10倍（10x）。如果您想找到模型准确度的下限，您可以在测试集中大量地使用同义词替换，以查看您的模型对这些变化的健壮性。如果您正在寻找毒害或规避自然语言处理算法的方法，同义词可以为您提供大量的探测文本尝试。您可以想象，将“货币”替换为“现金”、“美元”或“”可能有助于逃避垃圾邮件检测器。
- en: Important
  id: totrans-479
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 重要
- en: Bottom line, try to avoid stemming, lemmatization, case folding, or synonym
    substitution, unless you have a limited amount of text with contains usages and
    capitalizations of the words you are interested in. And with the explosion of
    NLP datasets, this is rarely the case for English documents, unless your documents
    use a lot of jargon or are from a very small subfield of science, technology,
    or literature. Nonetheless, for languages other than English, you may still find
    uses for lemmatization. The Stanford information retrieval course dismisses stemming
    and lemmatization entirely, due to the negligible recall accuracy improvement
    and the significant reduction in precision.^([[60](#_footnotedef_60 "View footnote.")])
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是，尽量避免使用词干提取、词形还原、大小写转换或同义词替换，除非您有一定数量的文本，其中包含您感兴趣的单词的用法和大写。随着自然语言处理数据集的爆炸式增长，这在英语文档中很少见，除非您的文档使用了大量行话或来自科学、技术或文学的非常小的子领域。尽管如此，对于英语以外的语言，您可能仍然会发现词形还原的用途。斯坦福信息检索课程完全忽略了词干提取和词形还原，因为召回率的几乎不可感知的提高和精确度的显著降低。
- en: 2.10 Sentiment
  id: totrans-481
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 情感
- en: Whether you use raw single-word tokens, *n*-grams, stems, or lemmas in your
    NLP pipeline, each of those tokens contains some information. An important part
    of this information is the word’s sentiment — the overall feeling or emotion that
    word invokes. This *sentiment analysis* — measuring the sentiment of phrases or
    chunks of text — is a common application of NLP. In many companies it is the main
    thing an NLP engineer is asked to do.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您在自然语言处理流水线中使用原始的单词标记、*n*-gram、词干还是词形还原，每个标记都包含一些信息。这些信息中的重要部分是单词的情感 - 单词所引起的整体感觉或情绪。这种*情感分析*
    - 测量短语或文本块的情感 - 是自然语言处理的常见应用。在许多公司中，这是自然语言处理工程师被要求做的主要事情。
- en: Companies like to know what users think of their products. So they often will
    provide some way for you to give feedback. A star rating on Amazon or Rotten Tomatoes
    is one way to get quantitative data about how people feel about products they’ve
    purchased. But a more natural way is to use natural language comments. Giving
    your user a blank slate (an empty text box) to fill up with comments about your
    product can produce more detailed feedback.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 公司喜欢知道用户对他们的产品有什么看法。所以他们经常会提供某种方式让你提供反馈。亚马逊或烂番茄上的星级评价是获取有关人们对他们购买的产品感觉的定量数据的一种方式。但更自然的方式是使用自然语言评论。给你的用户一个空白的画布（一个空的文本框）来填写关于你的产品的评论可以产生更详细的反馈。
- en: In the past you would have to read all that feedback. Only a human can understand
    something like emotion and sentiment in natural language text, right? However,
    if you had to read thousands of reviews you would see how tedious and error-prone
    a human reader can be. Humans are remarkably bad at reading feedback, especially
    criticism or negative feedback. And customers are not generally very good at communicating
    feedback in a way that can get past your natural human triggers and filters.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去，你必须阅读所有这些反馈。只有人类才能理解自然语言文本中的情感和情绪，对吧？然而，如果你不得不阅读数千条评论，你会看到人类读者可以是多么单调和容易出错。人类在阅读反馈时表现得非常糟糕，尤其是批评或负面反馈。而且顾客通常并不擅长以一种可以突破你的自然人类触发器和过滤器的方式传达反馈。
- en: But machines do not have those biases and emotional triggers. And humans are
    not the only things that can process natural language text and extract information,
    even meaning from it. An NLP pipeline can process a large quantity of user feedback
    quickly and objectively, with less chance for bias. And an NLP pipeline can output
    a numerical rating of the positivity or negativity or any other emotional quality
    of the text.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 但是机器没有这些偏见和情感触发器。而且人类并不是唯一能够处理自然语言文本并从中提取信息，甚至理解意义的事物。一个自然语言处理（NLP）管道可以快速客观地处理大量用户反馈，减少偏见的机会。而且一个NLP管道可以输出文本的积极性或消极性或任何其他情感质量的数字评分。
- en: 'Another common application of sentiment analysis is junk mail and troll message
    filtering. You would like your chatbot to be able to measure the sentiment in
    the chat messages it processes so it can respond appropriately. And even more
    importantly, you want your chatbot to measure its own sentiment of the statements
    it is about to send out, which you can use to steer your bot to be kind and pro-social
    with the statements it makes. The simplest way to do this might be to do what
    Moms told us to do: If you cannot say something nice, do not say anything at all.
    So you need your bot to measure the niceness of everything you are about to say
    and use that to decide whether to respond.'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析的另一个常见应用是垃圾邮件和恶意留言过滤。你希望你的聊天机器人能够测量它处理的聊天消息中的情感，以便能够适当地回应。更重要的是，你希望你的聊天机器人能够测量它即将发送的陈述的情感，你可以用来引导你的机器人以友善和亲社会的方式进行陈述。做到这一点的最简单方法可能是做妈妈告诉我们要做的事情：如果你说不出什么好的话，就什么都不要说。所以你需要你的机器人测量你即将说的每件事情的好坏，并使用它来决定是否回应。
- en: What kind of pipeline would you create to measure the sentiment of a block of
    text and produce this sentiment positivity number? Say you just want to measure
    the positivity or favorability of a text — how much someone likes a product or
    service that they are writing about. Say you want your NLP pipeline and sentiment
    analysis algorithm to output a single floating point number between -1 and +1\.
    Your algorithm would output +1 for text with positive sentiment like "Absolutely
    perfect! Love it! :-) :-) :-)". And your algorithm should output -1 for text with
    negative sentiment like "Horrible! Completely useless. :(". Your NLP pipeline
    could use values near 0, like say +0.1, for a statement like "It was OK. Some
    good and some bad things".
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 你会创建什么样的管道来测量一段文本的情感并产生这个情感积极性数字？比如说你只想测量一段文本的积极性或对一个他们所写的产品或服务的喜爱程度。比如说你希望你的NLP管道和情感分析算法输出一个-1到+1之间的单个浮点数。你的算法将为像“绝对完美！喜欢！:-)
    :-) :-)”这样具有积极情感的文本输出+1。而且你的算法应该为像“可怕！完全没用。:("这样具有负面情感的文本输出-1。你的NLP管道可以使用接近0的值，比如说+0.1，对于像“还行吧。有些好的和一些坏的事情”这样的陈述。
- en: 'There are two approaches to sentiment analysis:'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析有两种方法：
- en: A rule-based algorithm composed by a human
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由人类组成的基于规则的算法
- en: A *machine learning* model learned from data by a machine
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过机器学习从数据中学习的*机器学习*模型
- en: The first approach to sentiment analysis uses human-designed rules, sometimes
    called heuristics, to measure sentiment. A common rule-based approach to sentiment
    analysis is to find keywords in the text and map each one to numerical scores
    or weights in a dictionary or "mapping" — a Python `dict`, for example. Now that
    you know how to do tokenization, you can use stems, lemmas, or *n*-gram tokens
    in your dictionary, rather than just words. The "rule" in your algorithm would
    be to add up these scores for each keyword in a document that you can find in
    your dictionary of sentiment scores. Of course you need to hand-compose this dictionary
    of keywords and their sentiment scores before you can run this algorithm on a
    body of text. We show you how to do this using the VADER algorithm (in `sklearn`)
    in the upcoming listing.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种情感分析方法使用人为设计的规则，有时称为启发式方法，来衡量情感。情感分析的一种常见基于规则的方法是在文本中查找关键词，并将每个关键词映射到字典或“映射”中的数字分数或权重——例如一个
    Python `dict`。现在您知道如何进行标记化，您可以在字典中使用词干、词形还原或 *n*-gram 标记，而不仅仅是单词。算法中的“规则”是将这些分数相加，以找到在情感分数字典中可以找到的每个关键词在文档中的分数。当然，在您可以对一篇文本运行此算法之前，您需要手动组合这个关键词和它们的情感分数的字典。我们将在即将到来的列表中向您展示如何使用
    VADER 算法（在 `sklearn` 中）来完成这一点。
- en: The second approach, machine learning, relies on a labeled set of statements
    or documents to train a machine learning model to create those rules. A machine
    learning sentiment model is trained to process input text and output a numerical
    value for the sentiment you are trying to measure, like positivity or spamminess
    or trolliness. For the machine learning approach, you need a lot of data, text
    labeled with the "right" sentiment score. Twitter feeds are often used for this
    approach because the hash tags, such as `\#awesome` or `\#happy` or `\#sarcasm`,
    can often be used to create a "self-labeled" dataset. Your company may have product
    reviews with five-star ratings that you could associate with reviewer comments.
    You can use the star ratings as a numerical score for the positivity of each text.
    We show you shortly how to process a dataset like this and train a token-based
    machine learning algorithm called *Naive Bayes* to measure the positivity of the
    sentiment in a set of reviews after you are done with VADER.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法是机器学习，它依赖于一组带标签的陈述或文档来训练机器学习模型来创建这些规则。机器学习情感模型经过训练，以处理输入文本并输出您尝试测量的情感的数值，比如积极性、垃圾信息或恶意信息。对于机器学习方法，您需要大量的数据，带有“正确”情感分数的文本。Twitter
    feeds 通常用于此方法，因为标签，比如 `\#awesome` 或 `\#happy` 或 `\#sarcasm`，通常可用于创建“自标记”数据集。您的公司可能有与评论者评论相关联的五星评价的产品评论。您可以使用星级评分作为每个文本积极性的数值分数。我们将很快向您展示如何处理此类数据集，并在完成
    VADER 后训练一种基于令牌的机器学习算法，称为 *Naive Bayes*，以测量一组评论中情感的积极性。
- en: 2.10.1 VADER — A rule-based sentiment analyzer
  id: totrans-493
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.10.1 VADER — 一种基于规则的情感分析器
- en: Hutto and Gilbert at GA Tech came up with one of the first successful rule-based
    sentiment analysis algorithms. They called their algorithm VADER, for **V**alence
    **A**ware **D**ictionary for s**E**ntiment **R**easoning.^([[61](#_footnotedef_61
    "View footnote.")]) Many NLP packages implement some form of this algorithm. The
    NLTK package has an implementation of the VADER algorithm in `nltk.sentiment.vader`.
    Hutto himself maintains the Python package `vaderSentiment`. You will go straight
    to the source and use `vaderSentiment` here.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GA Tech，Hutto 和 Gilbert 提出了最早成功的基于规则的情感分析算法之一。他们将他们的算法称为 VADER，代表 **V**alence
    **A**ware **D**ictionary for s**E**ntiment **R**easoning。^([[61](#_footnotedef_61
    "查看脚注。")]) 许多 NLP 软件包实现了此算法的某种形式。NLTK 软件包中有一个 VADER 算法的实现，位于 `nltk.sentiment.vader`
    中。Hutto 本人维护着 Python 软件包 `vaderSentiment`。您将直接使用 `vaderSentiment`。
- en: You will need to `pip install vaderSentiment` to run the following example.^([[62](#_footnotedef_62
    "View footnote.")]) You have not included it in the `nlpia` package.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要执行 `pip install vaderSentiment` 来运行下面的示例。^([[62](#_footnotedef_62 "查看脚注。")])
    您没有将其包含在 `nlpia` 包中。
- en: '[PRE53]'
  id: totrans-496
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Let us see how well this rule-based approach does for the example statements
    we mentioned earlier.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这种基于规则的方法在前面提到的示例陈述中的表现如何。
- en: '[PRE54]'
  id: totrans-498
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: This looks a lot like what you wanted. So the only drawback is that VADER does
    not look at all the words in a document. VADER only "knows" about the 7,500 words
    or so that were hard-coded into its algorithm. What if you want all the words
    to help add to the sentiment score? And what if you do not want to have to code
    your own understanding of the words in a dictionary of thousands of words or add
    a bunch of custom words to the dictionary in `SentimentIntensityAnalyzer.lexicon`?
    The rule-based approach might be impossible if you do not understand the language
    because you would not know what scores to put in the dictionary (lexicon)!
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
- en: That is what machine learning sentiment analyzers are for.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
- en: 2.10.2 Closeness of vectors
  id: totrans-501
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Why do we use bags of words rather than bags of characters to represent natural
    language text? For a cryptographer trying to decrypt an unknown message, frequency
    analysis of the characters in the text would be a good way to go. But for natural
    language text in your native language, words turn out to be a better representation.
    You can see this if you think about what we are using these BOW vectors for.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
- en: If you think about it, you have a lot of different ways to measure the closeness
    of things. You probably have a good feel for what a close family relative would
    be. Or the closeness of the cafes where you can meet your friend to collaborate
    on writing a book about AI. For cafes your brain probably uses Euclidean distance
    on the 2D position of the cafes you know about. Or maybe Manhattan or taxi-cab
    distance.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
- en: But do you know how to measure the closeness of two pieces of text? In chapter
    4 you’ll learn about edit distances that check the similarity of two strings of
    characters. But that doesn’t really capture the essence of what you care about.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
- en: How close are these sentences to each other, in your mind?
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: I am now coming over to see you.
  id: totrans-506
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I am not coming over to see you.
  id: totrans-507
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Do you see the difference? Which one would you prefer to receive an email from
    your friend. The words "now" and "not" are very far apart in meaning. But they
    are very close in spelling. This is an example about how a single character can
    change the meaning of an entire sentence.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
- en: If you just counted up the characters that were different you’d get a distance
    of 1\. And then you could divide by the length of the longest sentence to make
    sure your distance value is between 0 and 1\. So your character difference or
    distance calculation would be 1 divided by 32 which gives 0.03125, or about 3%.
    Then, to turn a distance into a closeness you just subtract it from 1\. So do
    you think these two sentences are 0.96875, or about 97% the same? They mean the
    opposite. So we’d like a better measure than that.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
- en: What if you compared words instead of characters? In that case you would have
    one word out of seven that was changed. That is a little better than one character
    out of 32\. The sentences would now have a closeness score of six divided by seven
    or about 85%. That’s a little lower, which is what we want.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
- en: For natural language you don’t want your closeness or distance measure to rely
    only on a count of the differences in individual characters. This is one reason
    why you want to use words as your tokens of meaning when processing natural language
    text.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
- en: What about these two sentences?
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
- en: She and I will come over to your place at 3:00.
  id: totrans-513
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: At 3:00, she and I will stop by your apartment.
  id: totrans-514
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Are these two sentences close to each other in meaning? They have the exact
    same length in characters. And they use some of the same words, or at least synonyms.
    But those words and characters are not in the same order. So we need to make sure
    that our representation of the sentences does not rely on the precise position
    of words in a sentence.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
- en: Bag of words vectors accomplish this by creating a position or slot in a vector
    for every word you’ve seen in your vocabulary. You may have learned of a few measures
    of closeness in geometry and linear algebra.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
- en: As an example of why feature extraction from text is hard, consider *stemming* — grouping
    the various inflections of a word into the same "bucket" or cluster. Very smart
    people spent their careers developing algorithms for grouping inflected forms
    of words together based only on their spelling. Imagine how difficult that is.
    Imagine trying to remove verb endings like "ing" from "ending" so you’d have a
    stem called "end" to represent both words. And you’d like to stem the word "running"
    to "run," so those two words are treated the same. And that’s tricky, because
    you have remove not only the "ing" but also the extra "n". But you want the word
    "sing" to stay whole. You wouldn’t want to remove the "ing" ending from "sing"
    or you’d end up with a single-letter "s".
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
- en: Or imagine trying to discriminate between a pluralizing "s" at the end of a
    word like "words" and a normal "s" at the end of words like "bus" and "lens".
    Do isolated individual letters in a word or parts of a word provide any information
    at all about that word’s meaning? Can the letters be misleading? Yes and yes.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we show you how to make your NLP pipeline a bit smarter by dealing
    with these word spelling challenges using conventional stemming approaches. Later,
    in chapter 5, we show you statistical clustering approaches that only require
    you to amass a collection of natural language text containing the words you’re
    interested in. From that collection of text, the statistics of word usage will
    reveal "semantic stems" (actually, more useful clusters of words like lemmas or
    synonyms), without any hand-crafted regular expressions or stemming rules.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
- en: 2.10.3 Count vectorizing
  id: totrans-520
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous sections you’ve only been concerned with keyword detection.
    Your vectors indicated the presence or absence of words. In order to handle longer
    documents and improve the accuracy of your NLP pipeline, you’re going to start
    counting the occurrences of words in your documents.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
- en: You can put these counts into a sort-of histogram. Just like before you will
    create a vector for each document in your pipeline. Only instead of 0’s and 1’s
    in your vectors there will be counts. This will improve the accuracy of all the
    similarity and distance calculations you are doing with these counts. And just
    like normalizing histograms can improve your ability to compare two histograms,
    normalizing your word counts is also a good idea. Otherwise a really short wikipedia
    article that use Barak Obama’s name only once along side all the other presidents
    might get as much "Barack Obama" credit as a much longer page about Barack Obama
    that uses his name many times. Users and Question Answering bots like `qary` trying
    to answer questions about Obama might get distracted by pages listing all the
    presidents and might miss the main Barack Obama page entirely. So it’s a good
    idea to normalize your count vectors by dividing the counts by the total length
    of the document. This more fairly represents the distribution of tokens in the
    document and will create better similarity scores with other documents, including
    the text from a search query from `qary`.^([[63](#_footnotedef_63 "View footnote.")])
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
- en: Each position in your vector represents the count for one of your keywords.
    And having a small vocabulary keeps this vector small, low-dimensional, and easy
    to reason about. And you can use this *count vectorizing* approach even for large
    vocabularies.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
- en: And you can organize these counts of those keywords into You need to organize
    the counts into a vector. This opens up a whole range of powerful tools for doing
    vector algebra.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
- en: In natural language processing, composing a numerical vector from text is a
    particularly "lossy" feature extraction process. Nonetheless the bag-of-words
    (BOW) vectors retain enough of the information content of the text to produce
    useful and interesting machine learning models. The techniques for sentiment analyzers
    at the end of this chapter are the exact same techniques Google used to save email
    from a flood of spam that almost made it useless.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
- en: 2.10.4 Naive Bayes
  id: totrans-526
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A Naive Bayes model tries to find keywords in a set of documents that are predictive
    of your target (output) variable. When your target variable is the sentiment you
    are trying to predict, the model will find words that predict that sentiment.
    The nice thing about a Naive Bayes model is that the internal coefficients will
    map words or tokens to scores just like VADER does. Only this time you will not
    have to be limited to just what an individual human decided those scores should
    be. The machine will find the "best" scores for any problem.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
- en: For any machine learning algorithm, you first need to find a dataset. You need
    a bunch of text documents that have labels for their positive emotional content
    (positivity sentiment). Hutto compiled four different sentiment datasets for us
    when he and his collaborators built VADER. You will load them from the `nlpia`
    package.^([[64](#_footnotedef_64 "View footnote.")])
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-529
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'It looks like the movie reviews have been *centered*: normalized by subtracting
    the mean so that the new mean will be zero and they aren’t biased to one side
    or the other. And it seems the range of movie ratings allowed was -4 to +4.'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
- en: Now you can tokenize all those movie review texts to create a bag of words for
    each one. If you put them all into a Pandas DataFrame that will make them easier
    to work with.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-532
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: When you do not use case normalization, stop word filters, stemming, or lemmatization
    your vocabulary can be quite huge because you are keeping track of every little
    difference in spelling or capitalization of words. Try inserting some dimension
    reduction steps into your pipeline to see how they affect your pipeline’s accuracy
    and the amount of memory required to store all these BOWs.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
- en: Now you have all the data that a Naive Bayes model needs to find the keywords
    that predict sentiment from natural language text.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-535
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: To create a binary classification label you can use the fact that the centered
    movie ratings (sentiment labels) are positive (greater than zero) when the sentiment
    of the review is positive.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-537
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: This is a pretty good start at building a sentiment analyzer with only a few
    lines of code (and a lot of data). You did not have to guess at the sentiment
    associated with a list of 7500 words and hard code them into an algorithm such
    as VADER. Instead, you told the machine the sentiment ratings for whole text snippets.
    And then the machine did all the work to figure out the sentiment associated with
    each word in those texts. That is the power of machine learning and NLP!
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
- en: How well do you think this model will generalize to a completely different set
    of text examples such as product reviews? Do people use the same words to describe
    things they like in movie and product reviews such as electronics and household
    goods? Probably not. But it’s a good idea to check the robustness of your language
    models by running it against challenging text from a different domain. And by
    testing your model on new domains, you can get ideas for more examples and datasets
    to use in your training and test sets.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
- en: First, you need to load the product reviews. And take a look at the content’s
    of the file you loaded to make sure you understand what is in the dataset.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-541
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Next, we’ll you need to load the product reviews.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-543
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: What happens when you combine one dataframe of BOW vectors with another?
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-545
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: The combined DataFrame of bags of words has tokens that were used in product
    reviews but not in the movie reviews. You now have 23,302 unique tokens for both
    movie reviews and products in your vocabulary. Movie reviews only contained 20,756
    unique tokens. So there must be 23,302 - 20,756 or 2,546 new tokens about products
    that you didn’t have in your vocabulary before.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
- en: In order to use your Naive Bayes model to make predictions about product reviews,
    you need to make sure your new product bags of words have the same columns (tokens)
    in the exact same order as the original movie reviews used to train the model.
    After all, the model has no experience with these new tokens so it wouldn’t know
    which weights were appropriate for them. And you don’t want it to mix up the weights
    and apply them to the wrong tokens in the product reviews.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-548
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Now both of your sets of vectors (DataFrames) have 20,756 columns or unique
    tokens they keep track of. Now you need to convert the labels for the product
    review to mimic the binary movie review classification labels that you trained
    the original Naive Bayes model on.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-550
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: So your Naive Bayes model does a poor job predicting whether the sentiment of
    a product review is positive (thumbs up) or negative (thumbs down), only a little
    better than a coin flip. One reason for this subpar performance is that your vocabulary
    from the `casual_tokenize` product texts has 2546 tokens that were not in the
    movie reviews. That is about 10% of the tokens in your original movie review tokenization,
    which means that all those words will not have any weights or scores in your Naive
    Bayes model. Also, the Naive Bayes model does not deal with negation as well as
    VADER does. You would need to incorporate *n*-grams into your tokenizer to connect
    negation words (such as "not" or "never") to the positive words they might be
    used to qualify.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
- en: We leave it to you to continue the NLP action by improving on this machine learning
    model. And you can check your progress relative to VADER at each step of the way
    to see if you think machine learning is a better approach than hard-coding algorithms
    for NLP.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
- en: 2.11 Test yourself
  id: totrans-553
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What is the difference between a lemmatizer and a stemmer? Which one is better
    (in most cases)?
  id: totrans-554
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does a lemmatizer increase the likelihood that a search engine (such as
    You.com) returns search results that contain what you are looking for?
  id: totrans-555
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Will case folding, lemmatizing or stopword removal improve the accuracy of your
    typical NLP pipeline? What about for a problem like detecting misleading news
    article titles (clickbait)?^([[65](#_footnotedef_65 "View footnote.")])
  id: totrans-556
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Are there statistics in your token counts that you can use to decide what `n`
    to use in NLP pipeline?
  id: totrans-557
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is there a website where you can download the token frequencies for most of
    the words and n-grams ever published?^([[66](#_footnotedef_66 "View footnote.")])
  id: totrans-558
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the risks and possible benefits of pair coding AI assistants built
    with NLP? What sort of organizations and algorithms do you trust with your mind
    and your code?
  id: totrans-559
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 2.12 Summary
  id: totrans-560
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You implemented tokenization and configured a tokenizer for your application.
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*n*-gram tokenization helps retain some of the "word order" information in
    a document.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalization and stemming consolidate words into groups that improve the "recall"
    for search engines but reduce precision.
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lemmatization and customized tokenizers like `casual_tokenize()` can improve
    precision and reduce information loss.
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stop words can contain useful information, and discarding them is not always
    helpful.
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[1]](#_footnoteref_1) Mastodon servers you can join ( [https://proai.org/mastoserv](proai.org.html))'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
- en: '[[2]](#_footnoteref_2) Mastodon custom emoji documentation ( [https://docs.joinmastodon.org/methods/custom_emojis/](custom_emojis.html))'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
- en: '[[3]](#_footnoteref_3) ( [https://en.wikipedia.org/wiki/Grapheme](wiki.html))'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
- en: '[[4]](#_footnoteref_4) Suzi Park and Hyopil Shin *Grapheme-level Awareness
    in Word Embeddings for Morphologically Rich Languages* ( [https://www.aclweb.org/anthology/L18-1471.pdf](anthology.html))'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
- en: '[[5]](#_footnoteref_5) If you want to learn more about exactly what a "word"
    really is, check out the introduction to *The Morphology of Chinese* by Jerome
    Packard where he discusses the concept of a "word" in detail. The concept of a
    "word" did not exist at all in the Chinese language until the 20th century when
    it was translated from English grammar into Chinese.'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
- en: '[[6]](#_footnoteref_6) Pairs of adjacent words are called 2-grams or bigrams.
    Three words in sequency are called 3-grams or trigrams. Four words in a row are
    called 4-grams. 5-grams are probably the longest *n*-grams you’ll find in an NLP
    pipeline. Google counts all the 1 to 5-grams in nearly all the books ever written
    ( [https://books.google.com/ngrams](books.google.com.html)).'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
- en: '[[7]](#_footnoteref_7) Lysandre explains the various tokenizer options in the
    Huggingface documentation ( [https://huggingface.co/transformers/tokenizer_summary.html](transformers.html))'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
- en: '[[8]](#_footnoteref_8) Markus Zusak, *The Book Thief*, p. 85 ( [https://en.wikiquote.org/wiki/The_Book_Thief](wiki.html))'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
- en: '[[9]](#_footnoteref_9) Peter Watts, Blindsight, ( [https://rifters.com/real/Blindsight.htm](real.html))'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
- en: '[[10]](#_footnoteref_10) Thank you Wiktor Stribiżew ( [https://stackoverflow.com/a/43094210/623735](43094210.html)).'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
- en: '[[11]](#_footnoteref_11) nlpia2 source code for chapter 2 ( [https://proai.org/nlpia2-ch2](proai.org.html)
    ) has additional spaCy and displacy options and examples.'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
- en: '[[12]](#_footnoteref_12) Andrew Long, "Benchmarking Python NLP Tokenizers"
    ( [https://towardsdatascience.com/benchmarking-python-nlp-tokenizers-3ac4735100c5](towardsdatascience.com.html)
    )'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
- en: '[[13]](#_footnoteref_13) In many applications the term " *n*-gram" refers to
    character *n*-grams rather than word n-grams. For example the leading relational
    database PostgreSQL has a Trigram index which tokenizes your text into character
    3-grams not word 3-grams. In this book, we use " *n*-gram" to refer to sequences
    of word grams and "character *n*-grams" when talking about sequences of characters.'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
- en: '[[14]](#_footnoteref_14) Hannes and Cole are probably screaming "We told you
    so!" as they read this.'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
- en: '[[15]](#_footnoteref_15) Actually, the string representation of tokens used
    for BPE and Wordpiece tokenizer place marker characters at the beginning or end
    of the token string indicate the absence of a word boundary (typically a space
    or punctuation). So you may see the "aphr##" token in your BPE vocabulary for
    the prefix "aphr" in aphrodesiac ( [https://stackoverflow.com/a/55416944/623735](55416944.html)
    )'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
- en: '[[16]](#_footnoteref_16) Discriminatory voting restriction laws have recently
    been passed in US: ( [https://proai.org/apnews-wisconsin-restricts-blacks](proai.org.html)
    )'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
- en: '[[17]](#_footnoteref_17) See chapter 12 for information about another similar
    tokenizer — sentence piece tokenizer'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
- en: '[[18]](#_footnoteref_18) Lysandre Debut explains all the variations on subword
    tokenizers in the Hugging Face transformers documentation ( [https://huggingface.co/transformers/tokenizer_summary.html](transformers.html)
    )'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
- en: '[[19]](#_footnoteref_19) Huggingface documentation on tokenizers ( [https://huggingface.co/docs/transformers/tokenizer_summary](transformers.html)
    )'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
- en: '[[20]](#_footnoteref_20) See the "Player piano" article on Wikipedia ( [https://en.wikipedia.org/wiki/Player_piano](wiki.html)
    ).'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
- en: '[[21]](#_footnoteref_21) See the web page titled "Music box - Wikipedia" (
    [https://en.wikipedia.org/wiki/Music_box](wiki.html) ).'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
- en: '[[22]](#_footnoteref_22) West World is a television series about particularly
    malevolent humans and human-like robots, including one that plays a piano in the
    main bar.'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
- en: '[[23]](#_footnoteref_23) nlpia2 source code for chapter 2 ( [https://proai.org/nlpia2-ch2](proai.org.html)
    ) has additional spaCy and displacy options and examples.'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
- en: '[[24]](#_footnoteref_24) "Don’t use NLTK’s wordtokenize use NLTK’s regexptokenize"
    (blog with code) by Michael Bryan "Benchmarking Python NLP Tokenizers" ( [https://morioh.com/p/e2cfb73c8e86](p.html)
    )'
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
- en: '[[25]](#_footnoteref_25) In many applications the term " *n*-gram" refers to
    character *n*-grams rather than word n-grams. For example the leading relational
    database PostgreSQL has a Trigram index which tokenizes your text into character
    3-grams not word 3-grams. In this book, we use " *n*-gram" to refer to sequences
    of word grams and "character *n*-grams" when talking about sequences of characters.'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
- en: '[[26]](#_footnoteref_26) Hannes and Cole are probably screaming "We told you
    so!" as they read this.'
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
- en: '[[27]](#_footnoteref_27) Actually, the string representation of tokens used
    for BPE and Wordpiece tokenizer place marker characters at the beginning or end
    of the token string indicate the absence of a word boundary (typically a space
    or punctuation). So you may see the "aphr##" token in your BPE vocabulary for
    the prefix "aphr" in aphrodesiac ( [https://stackoverflow.com/a/55416944/623735](55416944.html)
    )'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
- en: '[[28]](#_footnoteref_28) Discriminatory voting restriction laws have recently
    been passed in US: ( [https://proai.org/apnews-wisconsin-restricts-blacks](proai.org.html)
    )'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
- en: '[[29]](#_footnoteref_29) See chapter 12 for information about another similar
    tokenizer — sentence piece tokenizer'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
- en: '[[30]](#_footnoteref_30) Lysandre Debut explains all the variations on subword
    tokenizers in the Hugging Face transformers documentation ( [https://huggingface.co/transformers/tokenizer_summary.html](transformers.html)
    )'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
- en: '[[31]](#_footnoteref_31) Huggingface documentation on tokenizers ( [https://huggingface.co/docs/transformers/tokenizer_summary](transformers.html)
    )'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
- en: '[[32]](#_footnoteref_32) Apache Solr home page and Java source code ( [https://solr.apache.org/](solr.apache.org.html))'
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
- en: '[[33]](#_footnoteref_33) Qwant web search engine based in Europe ( [https://www.qwant.com/](www.qwant.com.html))'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
- en: '[[34]](#_footnoteref_34) SearX git repository ( [https://github.com/searx/searx](searx.html))
    and web search ( [https://searx.thegpm.org/](searx.thegpm.org.html))'
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
- en: '[[35]](#_footnoteref_35) ( [https://www.wolframalpha.com/](www.wolframalpha.com.html))'
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
- en: '[[36]](#_footnoteref_36) excerpt from Martin A. Nowak and Roger Highfield in
    *SuperCooperators*: Altruism, Evolution, and Why We Need Each Other to Succeed.
    New York: Free Press, 2011.'
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
- en: '[[37]](#_footnoteref_37) excerpt from Martin A. Nowak and Roger Highfield SuperCooperators:
    Altruism, Evolution, and Why We Need Each Other to Succeed. New York: Free Press,
    2011.'
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
- en: '[[38]](#_footnoteref_38) Linguistic and NLP techniques are often used to glean
    information from DNA and RNA, this site provides a list of amino acid symbols
    that can help you translate amino acid language into a human-readable language:
    "Amino Acid - Wikipedia" ( [https://en.wikipedia.org/wiki/Amino_acid#Table_of_standard_amino_acid_abbreviations_and_properties](wiki.html)
    ).'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
- en: '[[39]](#_footnoteref_39) You may have learned about trigram indexes in your
    database class or the documentation for PostgreSQL ( `postgres`). But these are
    triplets of characters. They help you quickly retrieve fuzzy matches for strings
    in a massive database of strings using the `%` and `~*` SQL full text search queries.'
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
- en: '[[40]](#_footnoteref_40) A more comprehensive list of stop words for various
    languages can be found in NLTK’s corpora ( [https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/stopwords.zip](corpora.html)).'
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
- en: '[[41]](#_footnoteref_41) See the web page titled "Analysis of text data and
    Natural Language Processing" ( [http://rstudio-pubs-static.s3.amazonaws.com/41251_4c55dff8747c4850a7fb26fb9a969c8f.html](rstudio-pubs-static.s3.amazonaws.com.html)
    ).'
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
- en: '[[42]](#_footnoteref_42) The spaCy package contains a list of stop words that
    you can customize using this Stack Overflow answer ( [https://stackoverflow.com/a/51627002/623735](51627002.html)
    )'
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
- en: '[[43]](#_footnoteref_43) If you want to help others find SearX you can get
    in the habbit of saying "SearX" (pronounced "see Ricks") when talking or writing
    about doing a web search. You can shift the meaning of words in your world to
    make it a better place!'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
- en: '[[44]](#_footnoteref_44) The NLTK package ( [https://pypi.org/project/nltk](project.html)
    ) contains the list of stop words you’ll see in many online tutorials.'
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
- en: '[[45]](#_footnoteref_45) For marketing and SEO Damien Doyle maintains a list
    of search engine stopwords ranked by popularity here ( [https://www.ranks.nl/stopwords](www.ranks.nl.html)).'
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
- en: '[[46]](#_footnoteref_46) Vadi Kumar maintained some stop words lists here (
    [https://github.com/Vadi/stop-words](Vadi.html) ).'
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
- en: '[[47]](#_footnoteref_47) from Ted Chiang, *Exhalation*, "Truth of Fact, Truth
    of Fiction"'
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
- en: '[[48]](#_footnoteref_48) See the web page titled "Camel case case - Wikipedia"
    ( [https://en.wikipedia.org/wiki/Camel_case_case](wiki.html) ).'
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
- en: '[[49]](#_footnoteref_49) We’re assuming the behavior of str.lower() in Python
    3\. In Python 2, bytes (strings) could be lowercased by just shifting all alpha
    characters in the ASCII number ( `ord`) space, but in Python 3 `str.lower` properly
    translates characters so it can handle embellished English characters (like the
    "acute accent" diactric mark over the e in resumé) as well as the particulars
    of capitalization in non-English languages.'
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
- en: '[[50]](#_footnoteref_50) The trigram "cup of joe" ( [https://en.wiktionary.org/wiki/cup_of_joe](wiki.html)
    ) is slang for "cup of coffee."'
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
- en: '[[51]](#_footnoteref_51) Check our Appendix D to learn more about *precision*
    and *recall*. Here’s a comparison of the recall of various search engines on the
    Webology site ( [http://www.webology.org/2005/v2n2/a12.html](v2n2.html) ).'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
- en: '[[52]](#_footnoteref_52) Review Appendix D if you have forgotten how to measure
    recall or visit the wikipedia page to learn more ( [https://en.wikipedia.org/wiki/Precision_and_recall](wiki.html)
    ).'
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
- en: '[[53]](#_footnoteref_53) See "An algorithm for suffix stripping", 1980 ( [http://www.cs.toronto.edu/~frank/csc2501/Readings/R2_Porter/Porter-1980.pdf](R2_Porter.html)
    ) by M.F. Porter.'
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
- en: '[[54]](#_footnoteref_54) See the web page titled "Snowball: A language for
    stemming algorithms" ( [http://snowball.tartarus.org/texts/introduction.html](texts.html)
    ).'
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
- en: '[[55]](#_footnoteref_55) This is a trivially abbreviated version of Julia Menchavez’s
    implementation `porter-stemmer` on GitHub ( [https://github.com/jedijulia/porter-stemmer/blob/master/stemmer.py](master.html)
    ).'
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
- en: '[[56]](#_footnoteref_56) Sorry Chick, Porter doesn’t like your `obsfucate`
    username ;)'
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
- en: '[[57]](#_footnoteref_57) Thank you Kyle Gorman for pointing this out'
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
- en: '[[58]](#_footnoteref_58) Additional metadata is also used to adjust the ranking
    of search results. Duck Duck Go and other popular web search engines combine more
    than 400 independent algorithms (including user-contributed algorithms) to rank
    your search results ( [https://duck.co/help/results/sources](results.html) ).'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
- en: '[[59]](#_footnoteref_59) "Nice guys finish first!" — M.A. Nowak author of *SuperCooperators*"'
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
- en: '[[60]](#_footnoteref_60) See the Stanford NLP Information Retrieval (IR) book
    section titled "Stemming and lemmatization" ( [https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html](htmledition.html)
    ).'
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
- en: '[[61]](#_footnoteref_61) "VADER: A Parsimonious Rule-based Model for Sentiment
    Analysis of Social Media Text" by Hutto and Gilbert ( [http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf](papers.html)
    ).'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
- en: '[[62]](#_footnoteref_62) You can find more detailed installation instructions
    with the package source code on github ( [https://github.com/cjhutto/vaderSentiment](cjhutto.html)
    ).'
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
- en: '[[63]](#_footnoteref_63) Qary is an open source virtual assistant that actually
    assists you instead of manipulating and misinforming you ( [https://docs.qary.ai](.html)
    ).'
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
- en: '[[64]](#_footnoteref_64) If you have not already installed `nlpia`, check out
    the installation instructions at ( [http://gitlab.com/tangibleai/nlpia2](tangibleai.html)
    ).'
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
- en: '[[65]](#_footnoteref_65) Hint: When in doubt do an experiment. This is called
    *hyperparameter tuning*. Here’s a fake news dataset you can experiment with: (
    [https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset/download](fake-and-real-news-dataset.html)
    )'
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
- en: '[[66]](#_footnoteref_66) Hint: A company that aspired to "do no evil", but
    now does, created this massive NLP corpus.'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
