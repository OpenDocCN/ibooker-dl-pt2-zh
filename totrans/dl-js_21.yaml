- en: Chapter 12\. Testing, optimizing, and deploying models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*—WITH CONTRIBUTIONS FROM YANNICK ASSOGBA, PING YU, AND NICK* *KREEGER*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*This chapter covers*'
  prefs: []
  type: TYPE_NORMAL
- en: The importance of and practical guidelines for testing and monitoring machine-learning
    code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to optimize models trained in TensorFlow.js or converted to TensorFlow.js
    for faster loading and inference
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to deploy TensorFlow.js models to various platforms and environments, ranging
    from browser extensions to mobile apps, and from desktop apps to single-board
    computers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we mentioned in [chapter 1](kindle_split_011.html#ch01), machine learning
    differs from traditional software engineering in that it automates the discovery
    of rules and heuristics. The previous chapters of the book should have given you
    a solid understanding of this uniqueness of machine learning. However, machine-learning
    models and the code surrounding them are still code; they run as a part of your
    overall software system. In order to make sure that machine-learning models run
    reliably and efficiently, practitioners need to take similar precautions as they
    do when managing non-machine-learning code.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter is devoted to the practical aspects of using TensorFlow.js for
    machine learning as a part of your software stack. The first section explores
    the all-important but oft-neglected topic of testing and monitoring machine-learning
    code and models. The second section presents tools and tricks that help you reduce
    the size and computation footprint of your trained models, accelerating downloading
    and execution, which is a critical consideration for both client- and server-side
    model deployment. In the final section, we will give you a tour of the various
    environments in which models created with TensorFlow.js can be deployed. In doing
    so, we will discuss the unique benefits, constraints, and strategies that each
    of the deployment options involves.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be familiar with the best practices surrounding
    the testing, optimization, and deployment of deep-learning models in TensorFlow.js.
  prefs: []
  type: TYPE_NORMAL
- en: 12.1\. Testing TensorFlow.js models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we’ve talked about how to design, build, and train machine-learning
    models. Now we’re going to dive into some of the topics that arise when you deploy
    your trained models, starting with testing—of both the machine-learning code and
    the related non-machine-learning code. Some of the key challenges you face when
    you’re seeking to surround your model and its training process with tests are
    the size of the model, the time required to train, and nondeterministic behavior
    that happens during training (such as randomness in the initialization of weights
    and certain neural network operations such as dropout). As we expand from an individual
    model to a complete application, you’ll also run across various types of skew
    or drift between training and inference code paths, model versioning issues, and
    population changes in your data. You’ll see that testing needs to be complemented
    by a robust monitoring solution in order to achieve the reliability and confidence
    that you want in your entire machine-learning system.
  prefs: []
  type: TYPE_NORMAL
- en: One key consideration is, “How is your model version controlled?” In most cases,
    the model is tuned and trained until a satisfactory evaluation accuracy is reached,
    and then the model needs no further tweaking. The model is not rebuilt or retrained
    as part of the normal build process. Instead, the model topology and trained weights
    should be checked into your version-control system, more similar to a binary large
    object (BLOB) than a text/code artifact. Changing the surrounding code should
    not cause an update of your model version number. Likewise, retraining a model
    and checking it in shouldn’t require changing the non-model source code.
  prefs: []
  type: TYPE_NORMAL
- en: What aspect of a machine-learning system should be covered by tests? In our
    opinion, the answer is “every part.” [Figure 12.1](#ch12fig01) explains this answer.
    A typical system that goes from raw input data to a trained model ready for deployment
    consists of multiple key components. Some of them look similar to non-machine-learning
    code and are amenable to coverage by traditional unit testing, while others show
    more machine-learning-specific characteristics and hence require specially tailored
    testing or monitoring treatments. But the important take-home message here is
    never to ignore or underestimate the importance of testing just because you are
    dealing with a machine-learning system. Instead, we’d argue that unit testing
    is all the more important for machine-learning code, perhaps even more so than
    testing is for traditional software development, because machine-learning algorithms
    are typically more opaque and harder to understand than non-machine-learning ones.
    They can fail silently in the face of bad inputs, leading to issues that are hard
    to notice and debug, and the defense against such issues is testing and monitoring.
    In the following subsections, we will expand on various parts of [figure 12.1](#ch12fig01).
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 12.1\. The coverage of a production-ready machine-learning system by
    testing and monitoring. The top half of the diagram includes the key components
    of a typical pipeline for machine-learning model creation and training. The bottom
    half shows the testing practice that can be applied to each of the components.
    Some of the components are amenable to traditional unit testing practice: the
    code that creates and trains the code, and the code that performs pre- and postprocessing
    of the model’s input data and output results. Other components require more machine-learning-specific
    testing and monitoring practice. These include the example validation for the
    quality of data, the monitoring of the byte size and inference speed of the trained
    model, and the fine-grained validation and evaluation of the predictions made
    by the trained model.'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](12fig01a_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 12.1.1\. Traditional unit testing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Just as with non-machine-learning projects, reliable and lightweight unit tests
    should form the foundation of your test suites. However, special considerations
    are required to set up unit tests around machine-learning models. As you’ve seen
    in previous chapters, metrics such as accuracy on an evaluation dataset are often
    used to quantify the final quality of the model after successful hyperparameter
    tuning and training. Such evaluation metrics are important for monitoring by human
    engineers but are not suitable for automated testing. It is tempting to add a
    test that asserts that a certain evaluation metric is better than a certain threshold
    (for example, AUC for a binary-classification task is greater than 0.95, or MSE
    for a regression task is less than 0.2). However, these types of threshold-based
    assertions should be used with caution, if not completely avoided, because they
    tend to be fragile. The model’s training process contains multiple sources of
    randomness, including the initialization of weights and the shuffling of training
    examples. This leads to the fact that the result of model training varies slightly
    from run to run. If your datasets change (for instance, due to new data being
    added regularly), this will form an additional source of variability. As such,
    picking the threshold is a difficult task. Too lenient a threshold wouldn’t catch
    real problems when they occur. Too stringent a threshold would lead to a flaky
    test—that is, one that fails frequently without a genuine underlying issue.
  prefs: []
  type: TYPE_NORMAL
- en: 'The randomness in a TensorFlow.js program can usually be disabled by calling
    the `Math.seedrandom()` function prior to creating and running the model. For
    example, the following line will seed the random state of weight initializers,
    data shuffler, and dropout layers with a determined seed so that subsequent model
    training will yield deterministic results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '***1*** 42 is just an arbitrarily-selected, fixed random seed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is a useful trick in case you need to write tests that make assertions
    about the loss or metric values.
  prefs: []
  type: TYPE_NORMAL
- en: However, even with deterministic seeding, testing only `model.fit()` or similar
    calls is not sufficient for good coverage of your machine-learning code. Like
    other hard-to-unit-test sections of code, you should aim to fully unit test the
    surrounding code that is easy to unit test and explore alternative solutions for
    the model portion. All your code for data loading, data preprocessing, postprocessing
    of model outputs, and other utility methods should be amenable to normal testing
    practices. Additionally, some nonstringent tests on the model itself—its input
    and output shapes, for instance—along with an “ensure model does not throw an
    exception when trained one step” style test can provide the bare minimum of a
    test harness around the model that allows confidence during refactoring. (As you
    might have noticed when playing with the example code from the previous chapters,
    we use the Jasmine testing framework for testing in tfjs-examples, but you should
    feel free to use whatever unit test framework and runner you and your team prefer.)
  prefs: []
  type: TYPE_NORMAL
- en: For an example of this in practice, we can look at the tests for the sentiment-analysis
    examples we explored in [chapter 9](kindle_split_021.html#ch09). As you look through
    the code, you should see data_test.js, embedding_test.js, sequence_utils_test.js,
    and train_test.js. The first three of these files are covering the non-model code,
    and they look just like normal unit tests. Their presence gives us heightened
    confidence that the data that goes into the model during training and inference
    is in the expected source format, and our manipulations on it are valid.
  prefs: []
  type: TYPE_NORMAL
- en: The final file in that list concerns the machine-learning model and deserves
    a bit more of our attention. The following listing is an excerpt from it.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.1\. Unit tests of a model’s API—its input-output shapes and trainability
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '***1*** Ensures that the input and output of the model have the expected shape'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***2*** Trains the model very briefly; this should be fast, but it won’t be
    accurate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***3*** Checks that training is reporting metrics for each training step as
    a signal that training occurred'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***4*** Runs a prediction through the model focused on verifying the API is
    as expected'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***5*** Makes sure the prediction is in the range of possible answers; we don’t
    want to check for the actual value, as the training was exceptionally brief and
    might be unstable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This test is covering a lot of ground, so let’s break it down a little bit.
    We first build a model using a helper function. For this test, we don’t care about
    the structure of the model and will treat it like a black box. We then make assertions
    on the shape of the inputs and outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'These tests can catch problems in terms of misidentifying the batch dimension—regression
    versus classification, output shape, and so on. Next, we compile and train the
    model on a very small number of steps. Our goal is simply to ensure that the model
    is trainable—we’re not worried about accuracy, stability, or convergence at this
    point:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This snippet also checks that training reported the required metrics for analysis:
    if we trained for real, would we be able to inspect the progress of the training
    and the accuracy of the resulting model? Finally, we try a simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We’re not checking for any particular prediction result, as that might change
    based on the random initialization of weight values or possible future revisions
    to the model architecture. What we do check is that we get a prediction and that
    the prediction is in the expected range, in this case, between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: The most important lesson here is noticing that no matter how we change the
    inside of the model’s architecture, as long we don’t change its input or its output
    API, this test should always pass. If the test is failing, we have a problem in
    our model. These remain lightweight and fast tests that provide a strong degree
    of API correctness, and they are suitable for inclusion in whatever commonly run
    test hooks you use.
  prefs: []
  type: TYPE_NORMAL
- en: 12.1.2\. Testing with golden values
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the previous section, we talked about the unit testing we can do without
    asserting on a threshold metric value or requiring a stable or convergent training.
    Now let’s explore the types of testing people often want to run with a fully trained
    model, starting with checking predictions of particular data points. Perhaps there
    are some “obvious” examples that you want to test. For instance, for an object
    detector, an input image with a nice big cat in it should be labeled as such;
    for a sentiment analyzer, a text snippet that’s clearly a negative customer review
    should be classified as such. These correct answers for given model inputs are
    what we refer to as *golden values*. If you follow the mindset of traditional
    unit testing blindly, it is easy to fall into the trap of testing trained machine-learning
    models with golden values. After all, we want a well-trained object detector to
    always label the cat in an image with a cat in it, right? Not quite. Golden-value-based
    testing can be problematic in a machine-learning setting because we’re usurping
    our training, validation, and evaluation data split.
  prefs: []
  type: TYPE_NORMAL
- en: Assuming you had a representative sample for your validation and test datasets,
    and you set an appropriate target metric (accuracy, recall, and so on), why is
    any one example required to be right more than another? The training of a machine-learning
    model is concerned with accuracy on the entire validation and test sets. The predictions
    for individual examples may vary with the selection of hyperparameters and initial
    weight values. If there are some examples that must be classified correctly and
    are easy to identify, why not detect them before asking the machine-learning model
    to classify them and instead use a non-machine-learning code to handle them? Such
    examples are used occasionally in natural language processing systems, where a
    subset of query inputs (such as frequently encountered and easily identifiable
    ones) are automatically routed to a non-machine-learning module for handling,
    while the remaining queries are handled by a machine-learning model. You’ll save
    on compute time, and that portion of the code is easier to test with traditional
    unit testing. While adding a business-logic layer before (or after) the machine-learning
    predictor might seem like extra work, it gives you the hooks to control overrides
    of predictions. It’s also a place where you can add monitoring or logging, which
    you’ll probably want as your tool becomes more widely used. With that preamble,
    let’s explore the three common desires for golden values separately.
  prefs: []
  type: TYPE_NORMAL
- en: One common motivation of this type of golden-value test is in service to a full
    end-to-end test—given an unprocessed user input, what does the system output?
    The machine-learning system is trained, and a prediction is requested through
    the normal end-user code flow, with an answer being returned to the user. This
    is similar to our unit test in [listing 12.1](#ch12ex01), but the machine-learning
    system is in context with the rest of the application. We could write a test similar
    to [listing 12.1](#ch12ex01) that doesn’t care about the actual value of the prediction,
    and, in fact, that would be a more stable test. However, it’s very tempting to
    combine it with an example/prediction pair that makes sense and is easily understood
    when developers revisit the test.
  prefs: []
  type: TYPE_NORMAL
- en: This is when the trouble enters—we need an example whose prediction is known
    and guaranteed to be correct or else the end-to-end test fails. So, we add a smaller-scale
    test that tests that prediction through a subset of the pipeline covered by the
    end-to-end test. Now if the end-to-end test fails, and the smaller test passes,
    we’ve isolated the error to interactions between the core machine-learning model
    and other parts of the pipeline (such as data ingestion or postprocessing). If
    both fail in unison, we know our example/prediction invariant is broken. In this
    case, it’s more of a diagnostic tool, but the likely result of the paired failure
    is picking a new example to encode, not retraining the model entirely.
  prefs: []
  type: TYPE_NORMAL
- en: The next most common source is some form of business requirement. Some identifiable
    set of examples must be more accurate than the rest. As mentioned previously,
    this is the perfect setting for adding a pre- or post-model business-logic layer
    to handle these predictions. However, you can experiment with *example weighting*,
    in which some examples count for more than others when calculating the overall
    quality metrics. It won’t guarantee correctness, but it will bias the model toward
    getting those correct. If a business-logic layer is difficult because you can’t
    easily pre-identify the properties of the input that trigger the special case,
    you might need to explore a second model—one that is purely used to determine
    if override is needed. In this case, you’re using an ensemble of models, and your
    business logic is combining the predictions from two layers to do the correct
    action.
  prefs: []
  type: TYPE_NORMAL
- en: The last case here is when you have a bug report with a user-provided example
    that gave the wrong result. If it’s wrong for business reasons, we’re back in
    the immediately preceding case. If it’s wrong just because it falls into the failing
    percent of the model’s performance curve, there’s not a lot that we should do.
    It’s within the accepted performance of the trained algorithm; all models are
    expected to make some mistakes. You can add the example/correct prediction pair
    to your train/test/eval sets as appropriate to hopefully generate a better model
    in the future, but it’s not appropriate to use the golden values for unit testing.
  prefs: []
  type: TYPE_NORMAL
- en: An exception to this is if you’re keeping the model constant—you have the model
    weights and architecture checked into version control and are not regenerating
    them in the tests. Then it can be appropriate to use golden values to test the
    outputs of an inference system that uses the model as its core, as neither the
    model nor the examples are subject to change. Such an inference system contains
    parts other than the model, such as parts that preprocess the input data before
    feeding it to the model and ones that take the model’s outputs and transform them
    into forms more suitable for use by downstream systems. Such unit tests ensure
    the correctness of such pre- and postprocessing logic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another legitimate use of golden values is outside unit testing: the monitoring
    of the quality of a model (but not as unit testing) as it evolves. We will expand
    on this when we discuss the model validator and evaluator in the next section.'
  prefs: []
  type: TYPE_NORMAL
- en: 12.1.3\. Considerations around continuous training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In many machine-learning systems, you get new training data at fairly regular
    intervals (every week or every day). Perhaps you’re able to use your logs for
    the previous day to generate new, more timely training data. In such systems,
    the model needs to be retrained frequently, using the latest data available. In
    these cases, there is a belief that the age or staleness of the model affects
    its power. As time goes on, the inputs to the model drift to a different distribution
    than it was trained on, so the quality characteristics will get worse. As an example,
    you might have a clothing-recommendation tool that was trained in the winter but
    is making predictions in the summer.
  prefs: []
  type: TYPE_NORMAL
- en: Given the basic idea, as you begin to explore systems that require continuous
    training, you’ll have a wide variety of extra components that create your pipeline.
    A full discussion of these is outside the scope of this book, but TensorFlow Extended
    (TFX)^([[1](#ch12fn1)]) is an infrastructure to look at for more ideas. The pipeline
    components it lists that have the most relevance in a testing arena are the *example
    validator*, *model validator*, and *model evaluator*. The diagram in [figure 12.1](#ch12fig01)
    contains boxes that correspond to these components.
  prefs: []
  type: TYPE_NORMAL
- en: ¹
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Denis Baylor et al., “TFX: A TensorFlow-Based Production-Scale Machine Learning
    Platform,” KDD 2017, [www.kdd.org/kdd2017/papers/view/tfx-a-tensorflow-based-production-scale-machine-learning-platform](http://www.kdd.org/kdd2017/papers/view/tfx-a-tensorflow-based-production-scale-machine-learning-platform).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The example validator is about testing the data, an easy-to-overlook aspect
    of testing a machine-learning system. There is a famous saying among machine-learning
    practitioners: “garbage in, garbage out.” The quality of a trained machine-learning
    model is limited by the quality of the data that goes into it. Examples with invalid
    feature values or incorrect labels will likely hurt the accuracy of the trained
    model when deployed for use (that is, if the model-training job doesn’t fail because
    of the bad examples first!). The example validator is used to ensure that properties
    of the data that go into model training and evaluation always meet certain requirements:
    that you have enough data, that its distribution appears valid, and that you don’t
    have any odd outliers. For instance, if you have a set of medical data, the body
    height (in centimeters) should be a positive number no larger than 280; the patient
    age should be a positive number between 0 and 130; the oral temperature (in degrees
    Celsius) should be a positive number between roughly 30 and 45, and so forth.
    If certain data examples contain features that fall outside such ranges or have
    placeholder values such as “None” or NaN, we know something is wrong with those
    examples, and they should be treated accordingly—in most cases, excluded from
    the training and evaluation. Typically, errors here indicate either a failure
    of the data-collection process or that the “world has changed” in ways incompatible
    with the assumptions you held when building the system. Normally, this is more
    analogous to monitoring and alerting than integration testing.'
  prefs: []
  type: TYPE_NORMAL
- en: A component like an example validator is also useful for detecting *training-serving
    skew*, a particularly nasty type of bug that can arise in machine-learning systems.
    The two main causes are 1) training and serving data that belongs to different
    distributions and 2) data preprocessing involving code paths that behave differently
    during training and serving. An example validator deployed to both the training
    and serving environments has the potential to catch bugs introduced via either
    path.
  prefs: []
  type: TYPE_NORMAL
- en: The model validator plays the role of the person building the model in deciding
    if the model is “good enough” to use in serving. You configure it with the quality
    metrics you care about, and then it either “blesses” the model or rejects it.
    Again, like the example validator, this is more of a monitor-and-alert-style interaction.
    You’ll also typically want to log and chart your quality metrics over time (accuracy
    and so on) in order to see if you’re having small-scale, systematic degradations
    that might not trigger an alert by themselves but might still be useful for diagnosing
    long-term trends and isolating their causes.
  prefs: []
  type: TYPE_NORMAL
- en: The model evaluator is a sort of deeper dive into the quality statistics of
    the model, slicing and dicing the quality along a user-defined axis. Often, this
    is used to probe if the model is behaving fairly for different user populations—age
    bands, education bands, geographic, and so on. A simple example would be looking
    at the iris-flower examples we used in [section 3.3](kindle_split_014.html#ch03lev1sec3)
    and checking if our classification accuracy is roughly similar among the three
    iris species. If our test or evaluation sets are unusually biased toward one of
    the populations, it is possible we are always wrong on the smallest population
    without it showing up as a top-level accuracy problem. As with the model validator,
    the trends over time are often as useful as the individual point-in-time measurement.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2\. Model optimization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once you have painstakingly created, trained, and tested your model, it is time
    to put it to use. This process, called *model deployment*, is no less important
    than the previous steps of model development. Whether the model is to be shipped
    to the client side for inference or executed at the backend for serving, we always
    want the model to be fast and efficient. Specifically, we want the model to
  prefs: []
  type: TYPE_NORMAL
- en: Be small in size and hence fast to load over the web or from disk
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consume as little time, compute, and memory as possible when its `predict()`
    method is called
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This section describes techniques available in TensorFlow.js for optimizing
    the size and inference speed of trained models before they are released for deployment.
  prefs: []
  type: TYPE_NORMAL
- en: The meaning of the word *optimization* is overloaded. In the context of this
    section, *optimization* refers to improvements including model-size reduction
    and computation acceleration. This is not to be confused with weight-parameter
    optimization techniques such as gradient descent in the context of model training
    and optimizers. This distinction is sometimes referred to as model *quality* versus
    model *performance*. Performance refers to how much time and resources the model
    consumes to do its task. Quality refers to how close the results are to an ideal.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.1\. Model-size optimization through post-training weight quantization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The need to have small files that are swift to load over the internet should
    be abundantly clear to web developers. It is especially important if your website
    targets a very large user base or users with slow internet connections.^([[2](#ch12fn2)])
    In addition, if your model is stored on a mobile device (see [section 12.3.4](#ch12lev2sec9)
    for a discussion of mobile deployment with TensorFlow.js), the size of the model
    is often constrained by limited storage space. As a challenge for model deployment,
    neural networks are large and still getting larger. The capacity (that is, predictive
    power) of deep neural networks often comes at the cost of increased layer count
    and larger layer sizes. At the time of this writing, state-of-the-art image-recognition,^([[3](#ch12fn3)])
    speech-recognition,^([[4](#ch12fn4)]) natural language processing,^([[5](#ch12fn5)])
    and generative models^([[6](#ch12fn6)]) often exceed 1 GB in the size of their
    weights. Due to the tension between the need for models to be both small and powerful,
    a highly active area of research in deep learning is model-size optimization,
    or how to design a neural network with a size as small as possible that can still
    perform its tasks with an accuracy close to that of a larger neural network. Two
    general approaches are available. In the first approach, researchers design a
    neural network with the aim of minimizing model size from the outset. Second,
    there are techniques through which existing neural networks can be shrunk to a
    smaller size.
  prefs: []
  type: TYPE_NORMAL
- en: ²
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In March 2019, Google launched a Doodle featuring a neural network that can
    compose music in Johann Sebastian Bach’s style ([http://mng.bz/MOQW](http://mng.bz/MOQW)).
    The neural network runs in the browser, powered by TensorFlow.js. The model is
    quantized as 8-bit integers with the method described in this section, which cuts
    the model’s over-the-wire size by several times, down to about 380 KB. Without
    this quantization, it would be impossible to serve the model to an audience as
    wide as that of Google’s homepage (where Google Doodles appear).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ³
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Kaiming He et al., “Deep Residual Learning for Image Recognition,” submitted
    10 Dec. 2015, [https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ⁴
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Johan Schalkwyk, “An All-Neural On-Device Speech Recognizer,” Google AI Blog,
    12 Mar. 2019, [http://mng.bz/ad67](http://mng.bz/ad67).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ⁵
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Jacob Devlin et al., “BERT: Pre-training of Deep Bidirectional Transformers
    for Language Understanding,” submitted 11 Oct. 2018, [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ⁶
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Tero Karras, Samuli Laine, and Timo Aila, ”A Style-Based Generator Architecture
    for Generative Adversarial Networks,” submitted 12 Dec. 2018, [https://arxiv.org/abs/1812.04948](https://arxiv.org/abs/1812.04948).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: MobileNetV2, which we visited in the chapters on convnets, is produced by the
    first line of research.^([[7](#ch12fn7)]) It is a small, lightweight image model
    suitable for deployment on resource-restricted environments such as web browsers
    and mobile devices. The accuracy of MobileNetV2 is slightly worse compared to
    that of a larger image trained on the same tasks, such as ResNet50\. But its size
    (14 MB) is a few times smaller in comparison (ResNet50 is about 100 MB in size),
    which makes the slight reduction in accuracy a worthy trade-off.
  prefs: []
  type: TYPE_NORMAL
- en: ⁷
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Mark Sandler et al., “MobileNetV2: Inverted Residuals and Linear Bottlenecks,”
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 4510–4520,
    [http://mng.bz/NeP7](http://mng.bz/NeP7).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Even with its built-in size-squeezing, MobileNetV2 is still a little too large
    for most JavaScript applications. Consider the fact that its size (14 MB) is about
    eight times the size of an average web page.^([[8](#ch12fn8)]) MobileNetV2 offers
    a width parameter, which, if set to a value smaller than 1, reduces the size of
    all convolutional layers and hence provides further shrinkage in the size (and
    further loss in accuracy). For example, the version of MobileNetV2 with its width
    set to 0.25 is approximately a quarter of the size of the full model (3.5 MB).
    But even that may be unacceptable to high-traffic websites that are sensitive
    to increases in page weight and load time.
  prefs: []
  type: TYPE_NORMAL
- en: ⁸
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'According to HTTP Archive, the average page weight (total transfer size of
    HTML, CSS, JavaScript, images, and other static files) is about 1,828 KB for desktop
    and 1,682 KB for mobile as of May 2019: [https://httparchive.org/reports/page-weight](https://httparchive.org/reports/page-weight).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Is there a way to further reduce the size of such models? Luckily, the answer
    is yes. This brings us to the second approach mentioned, model-independent size
    optimization. The techniques in this category are more generic in that they do
    not require changes to the model architecture itself and hence should be applicable
    to a wide variety of existing deep neural networks.The technique we will specifically
    focus on here is called *post-training weight quantization*. The idea is simple:
    after a model is trained, store its weight parameters at a lower numeric precision.
    [Info box 12.1](#ch12sb01) describes how this is done for readers who are interested
    in the underlying mathematics.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '**The mathematics behind post-training weight quantization**'
  prefs: []
  type: TYPE_NORMAL
- en: The weight parameters of a neural network are represented as 32-bit floating-point
    (float32) numbers during training. This is true not only in TensorFlow.js but
    also in other deep-learning frameworks such as TensorFlow and PyTorch. This relatively
    expensive representation is usually okay because model training typically happens
    in environments with unrestricted resources (for example, the backend environment
    of a workstation equipped with ample memory, fast CPUs, and CUDA GPUs). However,
    empirical findings indicate that for many inference use cases, we can lower the
    precision of weights without causing a substantial decrease in accuracy. To reduce
    the representation precision, we map each float32 value onto an 8-bit or 16-bit
    integer value that represents the discretized location of the value within the
    range of all values in the same weight. This process is what we call *quantization*.
  prefs: []
  type: TYPE_NORMAL
- en: In TensorFlow.js, weight quantization is performed on a weight-by-weight basis.
    For example, if a neural network consists of four weight variables (such as the
    weights and biases of two dense layers), each of the weights will undergo quantization
    as a whole. The equation that governs quantization of a weight is
  prefs: []
  type: TYPE_NORMAL
- en: equation 12.1\.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](12fig01_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this equation, *B* is the number of bits that the quantization result will
    be stored in. It can be either 8 or 16, as currently supported by TensorFlow.js.
    *w*[Min] is the minimum value of the parameters of the weight. *w*Scale is the
    range of the parameters (the difference between the minimum and the maximum).
    The equation is valid, of course, only when *w*Scale is nonzero. In the special
    cases where *w*Scale is zero—that is, when all parameters of the weight have the
    same value—quantize(*w*) will return 0 for all *w*’s.
  prefs: []
  type: TYPE_NORMAL
- en: 'The two auxiliary values *w*[Min] and *w*Scale are saved together with the
    quantized weight values to support recovery of the weights (a process we refer
    to as *dequantization*) during model loading. The equation that governs dequantization
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: equation 12.2\.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](12eqa01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This equation is valid whether or not *w*Scale is zero.
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: 'Post-training quantization provides considerable reduction in model size: 16-bit
    quantization cuts the model size by approximately 50%, 8-bit quantization by 75%.
    These percentages are approximate for two reasons. First, a fraction of the model’s
    size is devoted to the model’s topology, as encoded in the JSON file. Second,
    as stated in the info box, quantization requires the storage of two additional
    floating-number values (*w*[Min] and *w*[Scale]), along with a new integer value
    (the bits of quantization). However, these are usually minor compared to the reduction
    in the number of bits used to represent the weight parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: Quantization is a lossy transformation. Some information in the original weight
    values is lost as a result of the decreased precision. It is analogous to reducing
    the bit depth of a 24-bit color image to an 8-bit one (the kind you may have seen
    on Nintendo’s game consoles from the 1980s), the effect of which is easily visible
    to human eyes. [Figure 12.2](#ch12fig02) provides intuitive comparisons of the
    degree of discretization that 16-bit and 8-bit quantization lead to. As you might
    expect, 8-bit quantization leads to a more coarse-grained representation of the
    original weights. Under 8-bit quantization, there are only 256 possible values
    over the entire range of a weight’s parameters, as compared with 65,536 possible
    values under 16-bit quantization. Both are dramatic reductions in precision compared
    to the 32-bit float representation.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.2\. Examples of 16-bit and 8-bit weight quantization. An original
    identity function (y = x, panel A) is reduced in size with 16-bit and 8-bit quantization;
    the results are shown in panels B and C, respectively. In order to make the quantization
    effects visible on the page, we zoom in on a small section of the identity function
    in the vicinity of *x* = 0.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](12eqa02_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Practically, does the loss of precision in weight parameters really matter?
    When it comes to the deployment of a neural network, what matters is its accuracy
    on test data. To answer this question, we compiled a number of models covering
    different types of tasks in the quantization example of tfjs-examples. You can
    run the quantization experiments there and see the effects for yourself. To check
    out the example, use
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The example contains four scenarios, each showcasing a unique combination of
    a dataset and the model applied on the dataset. The first scenario involves predicting
    average housing prices in geographic regions of California by using numeric features
    such as median age of the properties, total number of rooms, and so forth. The
    model is a five-layer network that includes dropout layers for the mitigation
    of overfitting. To train and save the original (nonquantized model), use this
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command performs 16- and 8-bit quantization on the saved model
    and evaluates how the two levels of quantization affect the model’s accuracy on
    a test set (a subset of the data unseen during the model’s training):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This command wraps a lot of actions inside for ease of use. However, the key
    step that actually quantizes the model can be seen in the shell script at quantization/quantize_
    evaluate.sh. In the script, you can see the following shell command that quantizes
    a model at the path `MODEL_JSON_PATH` with 16-bit quantization. You can follow
    the example of this command to quantize your own TensorFlow.js-saved models. If
    the option flag `--quantization_bytes` is set to `1` instead, 8-bit quantization
    will be performed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The previous command shows how to perform weight quantization on a model trained
    in JavaScript. `tensorflowjs_converter` also supports weight quantization when
    converting models from Python to JavaScript, the details of which are shown in
    [info box 12.2](#ch12sb02).
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '**Weight quantization and models from Python**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In [chapter 5](kindle_split_016.html#ch05), we showed how models from Keras
    (Python) can be converted to a format that can be loaded and used by TensorFlow.js.
    During such Python-to-JavaScript conversion, you can apply weight quantization.
    To do that, use the same `--quantization_ bytes` flag as described in the main
    text. For example, to convert a model in the HDF5 (.h5) format saved by Keras
    with 16-bit quantization, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In this command, `KERAS_MODEL_H5_PATH` is the path to the model exported by
    Keras, while `TFJS_MODEL_PATH` is the path to which the converted and weight-quantized
    model will be generated.
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: 'The detailed accuracy values you get will vary slightly from run to run due
    to the random initialization of weights and the random shuffling of data batches
    during training. However, the general conclusion should always hold: as shown
    by the first row of [table 12.1](#ch12table01), 16-bit quantization on weights
    leads to miniscule changes in the MAE of the housing-price prediction, while 8-bit
    quantization leads to a relatively larger (but still tiny in absolute terms) increase
    in the MAE.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 12.1\. Evaluation accuracies for four different models with post-training
    weight quantization
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Dataset and model | Evaluation loss and accuracy under no-quantization and
    different levels of quantization |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 32-bit full precision (no quantization) | 16-bit quantization | 8-bit quantization
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| California housing: MLP regressor | MAE^([[a](#ch12table01tn1)]) = 0.311984
    | MAE = 0.311983 | MAE = 0.312780 |'
  prefs: []
  type: TYPE_TB
- en: '| MNIST: convnet | Accuracy = 0.9952 | Accuracy = 0.9952 | Accuracy = 0.9952
    |'
  prefs: []
  type: TYPE_TB
- en: '| Fashion-MNIST: convnet | Accuracy = 0.922 | Accuracy = 0.922 | Accuracy =
    0.9211 |'
  prefs: []
  type: TYPE_TB
- en: '| ImageNet subset of 1,000: MobileNetV2 | Top-1 accuracy = 0.618 Top-5 accuracy
    = 0.788 | Top-1 accuracy = 0.624 Top-5 accuracy = 0.789 | Top-1 accuracy = 0.280
    Top-5 accuracy = 0.490 |'
  prefs: []
  type: TYPE_TB
- en: ^a
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The MAE loss function is used on the California-housing model. Lower is better
    for MAE, unlike accuracy.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The second scenario in the quantization example is based on the familiar MNIST
    dataset and deep convnet architecture. Similar to the housing experiment, you
    can train the original model and perform evaluation on quantized versions of it
    by using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As the second row of [table 12.1](#ch12table01) shows, neither the 16-bit nor
    8-bit quantization leads to any observable change in the model’s test accuracy.
    This reflects the fact that the convnet is a multiclass classifier, so small deviations
    in its layer output values may not alter the final classification result, which
    is obtained with an `argMax()` operation.
  prefs: []
  type: TYPE_NORMAL
- en: Is this finding representative of image-oriented multiclass classifiers? Keep
    in mind that MNIST is a relatively easy classification problem. Even a simple
    convnet like the one used in this example achieves near-perfect accuracy. How
    does quantization affect accuracies when we are faced with a harder image-classification
    problem? To answer this question, look at the two other scenarios in the quantization
    example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fashion-MNIST, which you encountered in the section on variational autoencoders
    in [chapter 10](kindle_split_022.html#ch10), is a harder problem that MNIST. By
    using the following commands, you can train a model on the Fashion-MNIST dataset
    and examine how 16- and 8-bit quantization affects its test accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The result, which is shown in the third row of [table 12.1](#ch12table01), indicates
    that there is a small decrease in the test accuracy (from 92.2% to 92.1%) caused
    by 8-bit quantization of the weights, although 16-bit quantization still leads
    to no observable change.
  prefs: []
  type: TYPE_NORMAL
- en: An even harder image-classification problem is the ImageNet classification problem,
    which involves 1,000 output classes. In this case, we download a pretrained MobileNetV2
    instead of training one from scratch, like we do in the other three scenarios
    in this example. The pretrained model is evaluated on a sample of 1,000 images
    from the ImageNet dataset, in its nonquantized and quantized forms. We opted not
    to evaluate the entire ImageNet dataset because the dataset itself is huge (with
    millions of images), and the conclusion we’d draw from that wouldn’t be much different.
  prefs: []
  type: TYPE_NORMAL
- en: To evaluate the model’s accuracy on the ImageNet problem in a more comprehensive
    fashion, we calculate both the top-1 and top-5 accuracies. Top-1 accuracy is the
    ratio of correct predictions when only the highest single logit output of the
    model is considered, while top-5 accuracy counts a prediction as right if any
    of the highest five logits includes the correct label. This is a standard approach
    in evaluating model accuracies on ImageNet because—due to the large number of
    class labels, some of which are very close to each other—models often show the
    correct label not in the top logit, but in one of the top-5 logits. To see the
    MobileNetV2 + ImageNet experiment in action, use
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Unlike the previous three scenarios, this experiment shows a substantial impact
    of 8-bit on the test accuracy (see the fourth row of [table 12.1](#ch12table01)).
    Both the top-1 and top-5 accuracies of the 8-bit quantized MobileNet are way below
    the original model, making 8-bit quantization an unacceptable size-optimization
    option for MobileNet. However, 16-bit quantized MobileNet still shows accuracies
    comparable to the nonquantized model.^([[9](#ch12fn9)]) We can see that the effect
    of quantization on accuracy depends on the model and the data. For some models
    and tasks (such as our MNIST convnet), neither 16-bit nor 8-bit quantization leads
    to any observable reduction in test accuracy. In these cases, we should by all
    means use the 8-bit quantized model during deployment to enjoy the reduced download
    time. For some models, such as our Fashion-MNIST convnet and our housing-price
    regression model, 16-bit quantization leads to no observed deterioration in accuracy,
    but 8-bit quantization does lead to a slight worsening of accuracy. In such cases,
    use your judgment as to whether the additional 25% reduction in model size outweighs
    the decrease in accuracy. Finally, for some types of models and tasks (such as
    our MobileNetV2 classification of ImageNet images), 8-bit quantization causes
    a large decrease in accuracy, which is probably unacceptable in most cases. For
    such problems, you need to stick with the original model or the 16-bit quantized
    version of it.
  prefs: []
  type: TYPE_NORMAL
- en: ⁹
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In fact, we can see small *increases* in accuracy, which are attributable to
    the random fluctuation on the relatively small test set that consists of only
    1,000 examples.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The cases in the quantization example are stock problems that may be somewhat
    simplistic. The problem you have at hand may be more complex and very different
    from those cases. The take-home message is that whether to quantize your model
    before deploying it and to what bit depth you should quantize it are empirical
    questions and can be answered only on a case-by-case basis. You need to try out
    the quantization and test the resulting models on real test data before making
    a decision. Exercise 1 at the end of this chapter lets you try your hand on the
    MNIST ACGAN we trained in [chapter 10](kindle_split_022.html#ch10) and decide
    whether 16-bit or 8-bit quantization is the right decision for such a generative
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Weight quantization and gzip compression
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: An additional benefit of 8-bit quantization that should be taken into account
    is the additional over-the-wire model-size reduction it provides under data-compression
    techniques such as gzip. gzip is widely used to deliver large files over the web.
    You should always enable gzip when serving TensorFlow.js model files over the
    web. The nonquantized float32 weights of a neural network are usually not very
    amenable to such compression due to the noise-like variation in the parameter
    values, which contains few repeating patterns. It is our observation that gzip
    typically can’t get more than 10–20% size reduction out of nonquantized weights
    for models. The same is true for models with 16-bit weight quantization. However,
    once a model’s weights undergo 8-bit quantization, there is often a considerable
    jump in the ratio of compression (up to 30–40% for small models and about 20–30%
    for larger ones; see [table 12.2](#ch12table02)).
  prefs: []
  type: TYPE_NORMAL
- en: Table 12.2\. The gzip compression ratios of model artifacts under different
    levels of quantization
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Dataset and model | gzip compression ratio^([[a](#ch12table02tn01)]) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 32-bit full precision (no quantization) | 16-bit quantization | 8-bit quantization
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| California-housing: MLP regressor | 1.121 | 1.161 | 1.388 |'
  prefs: []
  type: TYPE_TB
- en: '| MNIST: convnet | 1.082 | 1.037 | 1.184 |'
  prefs: []
  type: TYPE_TB
- en: '| Fashion-MNIST: convnet | 1.078 | 1.048 | 1.229 |'
  prefs: []
  type: TYPE_TB
- en: '| ImageNet subset of 1,000: MobileNetV2 | 1.085 | 1.063 | 1.271 |'
  prefs: []
  type: TYPE_TB
- en: ^a
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: (total size of the model.json and weight file)/(size of gzipped tar ball)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is due to the small number of bins available under the drastically reduced
    precision (only 256), which causes many values (such as the ones around 0) to
    fall into the same bin, and hence leads to more repeating patterns in the weight’s
    binary representation. This is an additional reason to favor 8-bit quantization
    in cases where it doesn’t lead to unacceptable deterioration in test accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, with post-training weight quantization, we can substantially reduce
    the size of the TensorFlow.js models transferred over the wire and stored on disk,
    especially with help from data-compression techniques such as gzip. This benefit
    of improved compression ratios requires no code change on the part of the developer,
    as the browser performs the unzipping transparently for you when it downloads
    the model files. However, it doesn’t change the amount of computation involved
    in executing the model’s inference calls. Neither does it change the amount of
    CPU or GPU memory consumption for such calls. This is because the weights are
    dequantized after they are loaded (see [equation 12.2](#ch12equ02) in [info box
    12.1](#ch12sb01)). As regards the operations that are run and the data types and
    shapes of the tensors output by the operations, there is no difference between
    a nonquantized model and a quantized model. However, for model deployment, an
    equally important concern is how to make a model that runs as fast as possible,
    as well as make it consume as little memory as possible when it’s running, because
    that improves user experience and reduces power consumption. Are there ways to
    make an existing TensorFlow.js model run faster when deployed, without loss of
    prediction accuracy and on top of model-size optimization? Luckily, the answer
    is yes. In the next section, we will focus on inference-speed optimization techniques
    that TensorFlow.js provides.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.2\. Inference-speed optimization using GraphModel conversion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This section is organized as follows. We will first present the steps involved
    in optimizing the inference speed of a TensorFlow.js model using the `GraphModel`
    conversion. We will then list detailed performance measurements that quantify
    the speed gain provided by this approach. Finally, we will explain how the `GraphModel`
    conversion approach works under the hood.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose you have a TensorFlow.js model saved at the path my/layers-model; you
    can use the following command to convert it to a `tf.GraphModel`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This command creates a model.json file under the output directory my/graph-model
    (the directory will be created if it doesn’t exist), along with a number of binary
    weight files. Superficially, this set of files may appear to be identical in format
    to the files in the input directory that contains the serialized `tf.LayersModel`.
    However, the output files encode a different kind of model called `tf.GraphModel`
    (the namesake of this optimization method). In order to load the converted model
    in the browser or Node.js, use the TensorFlow.js method `tf.loadGraphModel()`
    instead of the familiar `tf.loadLayersModel()`. Once the `tf.GraphModel` object
    is loaded, you can perform inference in exactly the same way as a `tf.LayersModel`
    by invoking the object’s `predict()` method. For example,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '***1*** Or use an http:// or https:// URL if loading the model in the browser.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***2*** Perform inference using input data ''xs''.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The enhanced inference speed comes with two limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: At the time of this writing, the latest version of TensorFlow.js (1.1.2) does
    not support recurrent layers such as `tf.layers.simpleRNN()`, `tf.layers.gru()`,
    and `tf.layers.lstm()` (see [chapter 9](kindle_split_021.html#ch09)) for `GraphModel`
    conversion.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The loaded `tf.GraphModel` object doesn’t have a `fit()` method and hence does
    not support further training (for example, transfer learning).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Table 12.3](#ch12table03) compares the inference speed of the two types of
    models with and without `GraphModel` conversion. Since `GraphModel` conversion
    does not support recurrent layers yet, only the results from an MLP and a convnet
    (MobileNetV2) are presented. To cover different deployment environments, the table
    presents results from both the web browser and tfjs-node running in the backend
    environment. From this table, we can see that `GraphModel` conversion invariably
    speeds up inference. However, the ratio of the speedup depends on model type and
    deployment environment. For the browser (WebGL) deployment environment, `GraphModel`
    conversion leads to a 20–30% speedup, while the speedup is more dramatic (70–90%)
    if the deployment environment is Node.js. Next, we will discuss why `GraphModel`
    conversion speeds up inference, as well as the reason why it speeds up the inference
    more for Node.js than for the browser environment.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 12.3\. Comparing the inference speed of two model types (an MLP and MobileNetV2)
    with and without `GraphModel` conversion optimization, and in different deployment
    environments^([[a](#ch12table03tn01)])
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: ^a
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The code with which these results were obtained is available at [https://github.com/tensorflow/tfjs/tree/master/tfjs/integration_tests/](https://github.com/tensorflow/tfjs/tree/master/tfjs/integration_tests/).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '| Model name and topology | predict() time (ms; lower is better) (Average over
    30 predict() calls preceded by 20 warm-up calls) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Browser WebGL | tfjs-node (CPU only) | tfjs-node-gpu |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| LayersModel | GraphModel | LayersModel | GraphModel | LayersModel | GraphModel
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| MLP^([[b](#ch12table03tn02)]) | 13 | 10 (1.3x) | 18 | 10 (1.8x) | 3 | 1.6
    (1.9x) |'
  prefs: []
  type: TYPE_TB
- en: '| MobileNetV2 (width = 1.0) | 68 | 57 (1.2x) | 187 | 111 (1.7x) | 66 | 39 (1.7x)
    |'
  prefs: []
  type: TYPE_TB
- en: ^b
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The MLP consists of dense layers with unit counts: 4,000, 1,000, 5,000, and
    1\. The first three layers have relu activation; the last has linear activation.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How GraphModel conversion speeds up model inference
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: How does `GraphModel` conversion boost TensorFlow.js models’ inference speed?
    It’s achieved by leveraging TensorFlow (Python)’s ahead-of-time analysis of the
    model’s computation graph at a fine granularity. The computation-graph analysis
    is followed by modifications to the graph that reduce the amount of computation
    while preserving the numeric correctness of the graph’s output result. Don’t be
    intimidated by terms such as *ahead-of-time analysis* and *fine granularity*.
    We will explain them in a bit.
  prefs: []
  type: TYPE_NORMAL
- en: 'To give a concrete example of the sort of graph modification we are talking
    about, let’s consider how a BatchNormalization layer works in a `tf.LayersModel`
    and a `tf.GraphModel`. Recall that BatchNormalization is a type of layer that
    improves convergence and reduces overfitting during training. It is available
    in the TensorFlow.js API as `tf.layers.batchNormalization()` and is used by popular
    pretrained models such as MobileNetV2\. When a BatchNormalization layer runs as
    a part of a `tf.LayersModel`, the computation follows the mathematical definition
    of batch normalization closely:'
  prefs: []
  type: TYPE_NORMAL
- en: equation 12.3\.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](12fig02_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Six operations (or ops) are needed in order to generate the output from the
    input (`x`), in the rough order of
  prefs: []
  type: TYPE_NORMAL
- en: '`sqrt`, with `var` as input'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`add`, with `epsilon` and the result of step 1 as inputs'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`sub`, with `x` and means as inputs'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`div`, with the results of steps 2 and 3 as inputs'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`mul`, with `gamma` and the result of step 4 as inputs'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`add`, with `beta` and the result of step 5 as inputs'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Based on simple arithmetic rules, it can be seen that [equation 12.3](#ch12equ03)
    can be simplified significantly, as long as the values of `mean`, `var`, `epsilon`,
    `gamma,` and `beta` are constant (do not change with the input or with how many
    times the layer has been invoked). After a model comprising a BatchNormalization
    layer is trained, all these variables indeed become constant. This is exactly
    what `GraphModel` conversion does: it “folds” the constants and simplifies the
    arithmetic, which leads to the following mathematically equivalent equation:'
  prefs: []
  type: TYPE_NORMAL
- en: equation 12.4\.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](12eqa03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The values of *k* and *b* are calculated during `GraphModel` conversion, not
    during inference:'
  prefs: []
  type: TYPE_NORMAL
- en: equation 12.5\.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](12eqa04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: equation 12.6\.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](12eqa05_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Therefore, [equations 12.5](#ch12equ05) and [12.6](#ch12equ06) do *not* factor
    into the amount of computation during inference; only [equation 12.4](#ch12equ04)
    does. Contrasting [equations 12.3](#ch12equ03) and [12.4](#ch12equ04), you can
    see that the constant folding and arithmetic simplification cut the number of
    operations from six to two (a `mul` op between *x* and *k* and an `add` op between
    *b* and the result of that `mul` operation), which leads to considerable speedup
    of this layer’s execution. But why does `tf.LayersModel` not perform this optimization?
    It’s because it needs to support training of the BatchNormalization layer, during
    which the values of `mean`, `var`, `gamma`, and `beta` are updated at every step
    of the training. `GraphModel` conversion takes advantage of the fact that these
    updated values are no longer required once the model training is complete.
  prefs: []
  type: TYPE_NORMAL
- en: The type of optimization seen in the BatchNormalization example is only possible
    if two requirements are met. First, the computation must be represented at a sufficiently
    *fine granularity*—that is, at the level of basic mathematical operations such
    as `add` and `mul`, instead of the coarser, layer-by-layer granularity at which
    the Layers API of TensorFlow.js resides. Second, all the computation is known
    ahead of time, before the calls to the model’s `predict()` method are executed.
    `GraphModel` conversion goes through TensorFlow (Python), which has access to
    a graph representation of the model that meets both criteria.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from the constant-folding and arithmetic optimization discussed previously,
    `GraphModel` conversion is capable of performing another type of optimization
    called *op fusion*. Take the frequently used dense layer type (`tf.layers.dense()`),
    for example. A dense layer involves three operations: a matrix multiplication
    (`matMul`) between the input *x* and the kernel *W*, a broadcasting addition between
    the result of the `matMul` and the bias (*b*), and the element-wise relu activation
    function ([figure 12.3](#ch12fig03), panel A). The op fusion optimization replaces
    the three separate operations with a single operation that carries out all the
    equivalent steps ([figure 12.3](#ch12fig03), panel B). This replacement may seem
    trivial, but it leads to faster computation due to 1) the reduced overhead of
    launching ops (yes, launching an op always involves a certain amount of overhead,
    regardless of the compute backend), and 2) more opportunity to perform smart tricks
    for speed optimization within the implementation of the fused op itself.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.3\. Schematic illustration of the internal operations in a dense layer,
    with (panel A) and without (panel B) op fusion
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](12fig03_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: How is op fusion optimization different from the constant folding and arithmetic
    simplification we just saw? Op fusion requires that the special fused op (`Fused`
    `matMul+relu`, in this case) be defined and available for the compute backend
    being used, while constant folding doesn’t. These special fused ops may be available
    only for certain compute backends and deployment environments. This is the reason
    why we saw a greater amount of inference speedup in the Node.js environment than
    in the browser (see [table 12.3](#ch12table03)). The Node.js compute backend,
    which uses libtensorflow written in C++ and CUDA, is equipped with a richer set
    of ops than TensorFlow.js’s WebGL backend in the browser.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from constant folding, arithmetic simplification, and op fusion, TensorFlow
    (Python)’s graph-optimization system Grappler is capable of a number of other
    kinds of optimizations, some of which may be relevant to how TensorFlow.js models
    are optimized through `GraphModel` conversion. However, we won’t cover those due
    to space limits. If you are interested in finding out more about this topic, you
    can read the informative slides by Rasmus Larsen and Tatiana Shpeisman listed
    at the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, `GraphModel` conversion is a technique provided by `tensorflowjs_
    converter`. It utilizes TensorFlow (Python)’s ahead-of-time graph-optimization
    capability to simplify computation graphs and reduce the amount of computation
    required for model inference. Although the detailed amount of inference speedup
    varies with model type and compute backend, it usually provides a speedup ratio
    of 20% or more, and hence is an advisable step to perform on your TensorFlow.js
    models before their deployment.
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '**How to properly measure a TensorFlow.js model’s inference time**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Both `tf.LayersModel` and `tf.GraphModel` provide the unified `predict()` method
    to support inference. This method takes one or more tensors as input and returns
    one or more tensors as the inference result. However, it is important to note
    that in the context of WebGL-based inference in the web browser, the `predict()`
    method only *schedules* operations to be executed on the GPU; it does not await
    the completion of their execution. As a result, if you naively time a `predict()`
    call in the following fashion, the result of the timing measurement will be wrong:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '***1*** Incorrect way of measuring inference time!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When `predict()` returns, the scheduled operations may not have finished executing.
    Therefore, the prior example will lead to a time measurement shorter than the
    actual time it takes to complete the inference. To ensure that the operations
    are completed before `console.timeEnd()` is called, you need to call one of the
    following methods of the returned tensor object: `array()` or `data()`. Both methods
    download the texture values that hold the elements of the output tensor from GPU
    to CPU. In order to do so, they must wait for the output tensor’s computation
    to finish. So, the correct way to measure the timing looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '***1*** The array() call won’t return until the scheduled computation of outputTensor
    has completed, hence ensuring the correctness of the inference-time measurement.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another important thing to bear in mind is that like all other JavaScript programs,
    the execution time of a TensorFlow.js model’s inference is variable. In order
    to obtain a reliable estimate of the inference time, the code in the previous
    snippet should be put in a `for` loop so that the measurement can be performed
    multiple times (for example, 50 times), and the average time can be calculated
    based on the accumulated individual measurements. The first few executions are
    usually slower than the subsequent ones due to the need to compile new WebGL shader
    programs and set up initial states. So, performance-measuring code often omits
    the first few (such as the first five) runs, which are referred to as *burn-in*
    or *warm-up* runs.
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in a deeper understanding of these performance-benchmarking
    techniques, work through exercise 3 at the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: 12.3\. Deploying TensorFlow.js models on various platforms and environments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You’ve optimized your model, it’s fast and lightweight, and all your tests are
    green. You’re good to go! Hooray! But before you pop that champagne, there’s a
    bit more work to do.
  prefs: []
  type: TYPE_NORMAL
- en: It’s time to put your model into your application and get it out in front of
    your user base. In this section, we will cover a few deployment platforms. Deploying
    to the web and deploying to a Node.js service are well-known paths, but we’ll
    also cover a few more exotic deployment scenarios, like deploying to a browser
    extension or a single-board embedded hardware application. We will point to simple
    examples and discuss special considerations important for the platforms.
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.1\. Additional considerations when deploying to the web
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let’s begin by revisiting the most common deployment scenario for TensorFlow.js
    models, deploying to the web as part of a web page. In this scenario, our trained,
    and possibly optimized, model is loaded via JavaScript from some hosting location,
    and then the model makes predictions using the JavaScript engine within the user’s
    browser. A good example of this pattern is the MobileNet image-classification
    example from [chapter 5](kindle_split_016.html#ch05). The example is also available
    to download from tfjs-examples/ mobilenet. As a reminder, the relevant code for
    loading a model and making a prediction can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This model is hosted from a Google Cloud Platform (GCP) bucket. For low-traffic,
    static applications like this one, it is easy to host the model statically alongside
    the rest of the site content. Larger, higher-traffic applications may choose to
    host the model through a content delivery network (CDN) alongside the other heavy
    assets. One common development mistake is to forget to account for Cross-Origin
    Resource Sharing (CORS) when setting up a bucket in GCP, Amazon S3, or other cloud
    services. If CORS is set incorrectly, the model will fail to load, and you should
    get a CORS-related error message delivered to the console. This is something to
    watch out for if your web application works fine locally but fails when pushed
    to your distribution platform.
  prefs: []
  type: TYPE_NORMAL
- en: After the user’s browser loads the HTML and JavaScript, the JavaScript interpreter
    will issue the call to load our model. The process of loading a small model takes
    a few hundred milliseconds on a modern browser with a good internet connection,
    but after the initial load, the model can be loaded much faster from the browser
    cache. The serialization format ensures that the model is sharded into small enough
    pieces to support the standard browser cache limit.
  prefs: []
  type: TYPE_NORMAL
- en: One nice property of web deployment is that prediction happens directly within
    the browser. Any data passed to the model is never sent over the wire, which is
    good for latency and great for privacy. Imagine a text-input prediction scenario
    where the model is predicting the next word for assistive typing, something that
    we see all the time in, for example, Gmail. If we need to send the typed text
    to servers in the cloud and wait for a response from those remote servers, then
    prediction will be delayed, and the input predictions will be much less useful.
    Furthermore, some users might consider sending their incomplete keystrokes to
    a remote computer an invasion of their privacy. Making predictions locally in
    their own browser is much more secure and privacy sensitive.
  prefs: []
  type: TYPE_NORMAL
- en: A downside of making predictions within the browser is model security. Sending
    the model to the user makes it easy for the user to keep the model and use it
    for other purposes. TensorFlow.js currently (as of 2019) does not have a solution
    for model security in the browser. Some other deployment scenarios make it harder
    for the user to use the model for purposes the developer didn’t intend. The distribution
    path with the greatest model security is to keep the model on servers you control
    and serve prediction requests from there. Of course, this comes at the cost of
    latency and data privacy. Balancing these concerns is a product decision.
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.2\. Deployment to cloud serving
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Many existing production systems provide machine-learning-trained prediction
    as a service, such as Google Cloud Vision AI ([https://cloud.google.com/vision](https://cloud.google.com/vision))
    or Microsoft Cognitive Services ([https://azure.microsoft.com/en-us/services/cognitive-services](https://azure.microsoft.com/en-us/services/cognitive-services)).
    The end user of such a service makes HTTP requests containing the input values
    to the prediction, such as an image for an object-detection task, and the response
    encodes the output of the prediction, such as the labels and positions of objects
    in the image.
  prefs: []
  type: TYPE_NORMAL
- en: As of 2019, there are two routes to serving a TensorFlow.js model from a server.
    The first route has the server running Node.js and performing the prediction using
    the native JavaScript runtime. Because TensorFlow.js is so new, we are not aware
    of production use cases that have chosen this approach, but proofs of concept
    are simple to build.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second route is to convert the model from TensorFlow.js into a format that
    can be served from a known existing server technology, such as the standard TensorFlow
    Serving system. From the documentation at [www.tensorflow.org/tfx/guide/serving](http://www.tensorflow.org/tfx/guide/serving):'
  prefs: []
  type: TYPE_NORMAL
- en: '*TensorFlow Serving is a flexible, high-performance serving system for machine-learning
    models, designed for production environments. TensorFlow Serving makes it easy
    to deploy new algorithms and experiments, while keeping the same server architecture
    and APIs. TensorFlow Serving provides out-of-the-box integration with TensorFlow
    models, but can be easily extended to serve other types of models and data.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The TensorFlow.js models we have serialized so far have been stored in a JavaScript-specific
    format. TensorFlow Serving expects models to be packaged in the TensorFlow standard
    SavedModel format. Fortunately, the tfjs-converter project makes it easy to convert
    to the necessary format.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [chapter 5](kindle_split_016.html#ch05) (transfer learning) we showed how
    SavedModels built with the Python implementation of TensorFlow could be used in
    TensorFlow.js. To do the reverse, first install the tensorflowjs pip package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, you must run the converter binary, specifying the input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This will create a new saved-model directory, which will contain the required
    topology and weights in a format that TensorFlow Serving understands. You should
    then be able to follow the instructions for building the TensorFlow Serving server
    and make gRPC prediction requests against the running model. Managed solutions
    also exist. For instance, Google Cloud Machine Learning Engine provides a path
    for you to upload your saved model to Cloud Storage and then set up serving as
    a service, without needing to maintain the server or the machine. You can learn
    more from the documentation at [https://cloud.google.com/ml-engine/docs/tensorflow/deploying-models](https://cloud.google.com/ml-engine/docs/tensorflow/deploying-models).
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of serving your model from the cloud is that you are in complete
    control of the model. It is easy to perform telemetry on what sorts of queries
    are being performed and to quickly detect problems. If it is discovered that there
    is some unforeseen problem with a model, it can be quickly removed or upgraded,
    and there is little risk of other copies on machines outside of your control.
    The downside is the additional latency and data privacy concerns, as mentioned.
    There is also the additional cost—both in monetary outlay and maintenance costs—in
    operating a cloud service, as you are in control of the system configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.3\. Deploying to a browser extension, like Chrome Extension
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Some client-side applications may require your application to be able to work
    across many different websites. Browser extension frameworks are available for
    all the major desktop browsers, including Chrome, Safari, and FireFox, among others.
    These frameworks enable developers to create experiences that modify or enhance
    the browsing experience itself by adding new JavaScript and manipulating the DOM
    of websites.
  prefs: []
  type: TYPE_NORMAL
- en: Since the extension is operating on top of JavaScript and HTML within the browser’s
    execution engine, what you can do with TensorFlow.js in a browser extension is
    similar to what is possible in a standard web page deployment. The model security
    story and data privacy story are identical to the web page deployment. By performing
    prediction directly within the browser, the users’ data is relatively secure.
    The model security story is also similar to that of web deployment.
  prefs: []
  type: TYPE_NORMAL
- en: As an example of what is possible using a browser extension, see the chrome-extension
    example within tfjs-examples. This extension loads a MobileNetV2 model and applies
    it to images on the web, selected by the user. Installing and using the extension
    is a little different from the other examples we have seen, since it is an extension,
    not a hosted website. This example requires the Chrome browser.^([[10](#ch12fn10)])
  prefs: []
  type: TYPE_NORMAL
- en: ^(10)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Newer versions of Microsoft Edge also offer some support for cross-browser extension
    loading.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'First, you must download and build the extension, similar to how you might
    build one of the other examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: After the extension has finished building, it is possible to load the unpacked
    extension in Chrome. To do so, you must navigate to chrome://extensions, enable
    developer mode, and then click Load Unpacked, as shown in [figure 12.4](#ch12fig04).
    This will bring up a file-selection dialog, where you must select the dist directory
    created under the chrome-extension directory. That’s the directory containing
    manifest.json.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.4\. Loading the TensorFlow.js MobileNet Chrome extension in developer
    mode
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](12fig04_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Once the extension is installed, you should be able to classify images in the
    browser. To do so, navigate to some site with images, such as the Google image
    search page for the term *tiger* used here. Then right-click the image you wish
    to classify. You should see a menu option for `Classify Image with TensorFlow.js.
    Clicking that menu option should cause the extension to execute the MobileNet
    model on the image and then add some text over the image, indicating the prediction
    (see [figure 12.5](#ch12fig05).)`
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.5\. The TensorFlow.js MobileNet Chrome extension helps classify images
    in a web page.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](12fig05_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: To remove the extension, click Remove on the Extensions page (see [figure 12.4](#ch12fig04)),
    or use the Remove from Chrome menu option when right-clicking the extension icon
    at top-right.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the model running in the browser extension has access to the same
    hardware acceleration as the model running in the web page and, indeed, uses much
    of the same code. The model is loaded with a call to `tf.loadGraphModel(...)`
    using a suitable URL, and predictions are made using the same `model.predict(...)`
    API we’ve seen. Migrating technology or a proof of concept from a web page deployment
    into a browser extension is relatively easy.
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.4\. Deploying TensorFlow.js models in JavaScript-based mobile applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For many products, the desktop browser does not provide enough reach, and the
    mobile browser does not provide the smoothly animated customized product experience
    that customers have come to expect. Teams working on these sorts of projects are
    often faced with the dilemma of how to manage the codebase for their web app alongside
    repositories for (typically) both Android (Java or Kotlin) and iOS (Objective
    C or Swift) native apps. While very large groups can support such an outlay, many
    developers are increasingly choosing to reuse much of their code across these
    deployments by leveraging hybrid cross-platform development frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-platform app frameworks, like React Native, Ionic, Flutter, and Progressive
    Web-Apps, enable you to write the bulk of an application once in a common language
    and then compile that core functionality to create native experiences with the
    look, feel, and performance that users expect. The cross-platform language/runtime
    handles much of the business logic and layout, and connects to native platform
    bindings for the standardized affordance visuals and feel. How to select the right
    hybrid app development framework is the topic of countless blogs and videos on
    the web, so we will not revisit that discussion here, but will rather focus on
    just one popular framework, React Native. [Figure 12.6](#ch12fig06) illustrates
    a minimal React Native app running a MobileNet model. Notice the lack of any browser
    top bar. Though this simple app doesn’t have UI elements, if it did, you would
    see that they match the native Android look and feel. The same app built for iOS
    would match *those* elements.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.6\. A screenshot from a sample native Android app built with React
    Native. Here, we are running a TensorFlow.js MobileNet model within the native
    app.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](12fig06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Happily, the JavaScript runtime within React Native supports TensorFlow.js
    natively without any special work. The tfjs-react-native package is still in alpha
    release (as of December 2019) but provides GPU support with WebGL via expo-gl.
    The user code looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The package also provides a special API for assisting with loading and saving
    model assets within the mobile app.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.2\. Loading and saving a model within a mobile app built with React-Native
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '***1*** Saves the model to AsyncStorage—a simple key-value storage system global
    to the app'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***2*** Loads the model from AsyncStorage'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While native app development through React Native still requires learning a
    few new tools, such as Android Studio for Android and XCode for iOS, the learning
    curve is shallower than diving straight into native development. That these hybrid
    app development frameworks support TensorFlow.js means that the machine-learning
    logic can live in a single codebase rather than requiring us to develop, maintain,
    and test a separate version for each hardware surface—a clear win for developers
    who wish to support the native app experience! But what about the native desktop
    experience?
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.5\. Deploying TensorFlow.js models in JavaScript-based cross-platform-
    m desktop applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: JavaScript frameworks such as Electron.js allow desktop applications to be written
    in a cross-platform manner reminiscent of cross-platform mobile applications written
    in React Native. With such frameworks, you need to write your code only once,
    and it can be deployed and run on mainstream desktop operating systems, including
    macOS, Windows, and major distributions of Linux. This greatly simplifies the
    traditional development workflow of maintaining separate codebases for largely
    incompatible desktop operating systems. Take Electron.js, the leading framework
    in this category, for example. It uses Node.js as the virtual machine that undergirds
    the application’s main process; for the GUI portion of the app, it uses Chromium,
    a full-blown and yet lightweight web browser that shares much of its code with
    Google Chrome.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow.js is compatible with Electron.js, as is demonstrated by the simple
    example in the tfjs-examples repository. This example, found in the electron directory,
    illustrates how to deploy a TensorFlow.js model for inference in an Electron.js-based
    desktop app. The app allows users to search the filesystem for image files that
    visually match one or more keywords (see the screenshot in [figure 12.7](#ch12fig07)).
    This search process involves applying a TensorFlow.js MobileNet model for inference
    on a directory of images.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.7\. A screenshot from the example Electron.js-based desktop application
    that utilizes a TensorFlow.js model, from tfjs-examples/electron
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](12fig07_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Despite its simplicity, this example app illustrates an important consideration
    in deploying TensorFlow.js models to Electron.js: the choice of the compute backend.
    An Electron.js application runs on a Node.js-based backend process as well as
    a Chromium-based frontend process. TensorFlow.js can run in either of those environments.
    As a result, the same model can run in either the application’s node-like backend
    process or the browser-like frontend process. In the case of backend deployment,
    the @tensorflow/tfjs-node package is used, while the @tensorflow/tfjs package
    is used for the frontend case ([figure 12.8](#ch12fig08)). A check box in the
    example application’s GUI allows you to switch between the backend and frontend
    inference modes ([figure 12.7](#ch12fig07)), although in an actual application
    powered by Electron.js and TensorFlow.js, you would normally decide on one environment
    for your model beforehand. We will next briefly discuss the pros and cons of the
    options.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.8\. The architecture of an Electron.js-based desktop application that
    utilizes TensorFlow.js for accelerated deep learning. Different compute backends
    of TensorFlow.js can be invoked, from either the main backend process or the in-browser
    renderer process. Different compute backends cause models to be run on different
    underlying hardware. Regardless of the choice of compute backend, the code that
    loads, defines, and runs deep-learning models in TensorFlow.js is largely the
    same. The arrowheads in this diagram indicate invocation of library functions
    and other callable routines.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](12fig08_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As [figure 12.8](#ch12fig08) shows, different choices of the compute backend
    cause the deep-learning computation to happen on different computation hardware.
    Backend deployment based on @tensorflow/tfjs-node assigns the workload to the
    CPU, leveraging the multithreaded and SIMD-enabled libtensorflow library. This
    Node.js-based model-deployment option is usually faster than the frontend option
    and can accommodate larger models due to the fact that the backend environment
    is free of resource restrictions. However, their major downside is the large package
    size, which is a result of the large size of libtensorflow (for tfjs-node, approximately
    50 MB with compression).
  prefs: []
  type: TYPE_NORMAL
- en: The frontend deployment dispatches deep-learning workloads to WebGL. For small-to-medium-sized
    models, and in cases where the latency of inference is not of major concern, this
    is an acceptable option. This option leads to a smaller package size, and it works
    out of the box for a wide range of GPUs, thanks to the wide support for WebGL.
  prefs: []
  type: TYPE_NORMAL
- en: As [figure 12.8](#ch12fig08) also illustrates, the choice of compute backend
    is a largely separate concern from the JavaScript code that loads and runs your
    model. The same API works for all three options. This is clearly demonstrated
    in the example app, where the same module (`ImageClassifier` in electron/image_classifier.js)
    subserves the inference task in both the backend and frontend environments. We
    should also point out that although the tfjs-examples/electron example shows only
    inference, you can certainly use TensorFlow.js for other deep-learning workflows,
    such as model creation and training (for example, transfer learning) in Electron.js
    apps equally well.
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.6\. Deploying TensorFlow.js models on WeChat and other JavaScript-bas-
    sed mobile app plugin systems
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are some places where the main mobile-app-distribution platform is neither
    Android’s Play Store nor Apple’s App Store, but rather a small number of “super
    mobile apps” that allow for third-party extensions within their own first-party
    curated experience.
  prefs: []
  type: TYPE_NORMAL
- en: A few of these super mobile apps come from Chinese tech giants, notably Tencent’s
    WeChat, Alibaba’s Alipay, and Baidu. These use JavaScript as their main technology
    to enable the creation of third-party extensions, making TensorFlow.js a natural
    fit for deploying machine learning on their platform. The set of APIs available
    within these mobile app plugin systems is not the same as the set available in
    native JavaScript, however, so some additional knowledge and work is required
    to deploy there.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use WeChat as an example. WeChat is the most widely used social media
    app in China, with over 1 billion monthly active users. In 2017, WeChat launched
    Mini Program, a platform for application developers to create JavaScript mini-programs
    within the WeChat system. Users can share and install these mini-programs inside
    the WeChat app on-the-fly, and it’s been a tremendous success. By Q2 2018, WeChat
    had more than 1 million mini-programs and over 600 million daily active mini-program
    users. There are also more than 1.5 million developers who are developing applications
    on this platform, partly because of the popularity of JavaScript.
  prefs: []
  type: TYPE_NORMAL
- en: WeChat mini-program APIs are designed to provide developers easy access to mobile
    device sensors (the camera, microphone, accelerometer, gyroscope, GPS, and so
    on). However, the native API provides very limited machine-learning functionality
    built into the platform. TensorFlow.js brings several advantages as a machine-learning
    solution for mini-programs. Previously, if developers wanted to embed machine
    learning in their applications, they needed to work outside the mini-program development
    environment with a server-side or cloud-based machine-learning stack. Doing so
    makes the barrier high for the large number of mini-program developers to build
    and use machine learning. Standing up an external serving infrastructure is outside
    of the scope of possibilities for most mini-program developers. With TensorFlow.js,
    machine-learning development happens right within the native environment. Furthermore,
    since it is a client-side solution, it helps reduce network traffic and improves
    latency, and it takes advantage of GPU acceleration using WebGL.
  prefs: []
  type: TYPE_NORMAL
- en: The team behind TensorFlow.js has created a WeChat mini-program you can use
    to enable TensorFlow.js for your mini-program (see [https://github.com/tensorflow/tfjs-wechat](https://github.com/tensorflow/tfjs-wechat)).
    The repository also contains an example mini-program that uses PoseNet to annotate
    the positions and postures of people sensed by the mobile device’s camera. It
    uses TensorFlow.js accelerated by a newly added WebGL API from WeChat. Without
    access to the GPU, the model would run too slowly to be useful for most applications.
    With this plugin, a WeChat mini-program can have the same model execution performance
    as a JavaScript app running inside mobile browsers. In fact, we have observed
    that the WeChat sensor API typically *outperforms* the counterpart in the browser.
  prefs: []
  type: TYPE_NORMAL
- en: As of late 2019, developing machine-learning experiences for super app plugins
    is still very new territory. Getting high performance may require some help from
    the platform maintainers. Still, it is the best way to deploy your app in front
    of the hundreds of millions of people for whom the super mobile app *is* the internet.
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.7\. Deploying TensorFlow.js models on single-board computers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For many web developers, deploying to a headless single-board computer sounds
    very technical and foreign. However, thanks to the success of the Raspberry Pi,
    developing and building simple hardware devices has never been easier. Single-board
    computers provide a platform to inexpensively deploy intelligence without depending
    on network connections to cloud servers or bulky, costly computers. Single-board
    computers can be used to back security applications, moderate internet traffic,
    control irrigation—the sky’s the limit.
  prefs: []
  type: TYPE_NORMAL
- en: Many of these single-board computers provide general-purpose input-output (GPIO)
    pins to make it easy to connect to physical control systems, and include a full
    Linux install to allow educators, developers, and hackers to develop a wide range
    of interactive devices. JavaScript has quickly become a popular language for building
    on these types of devices. Developers can use node libraries such as rpi-gpio
    to interact electronically at the lowest level, all in JavaScript.
  prefs: []
  type: TYPE_NORMAL
- en: 'To help support these users, TensorFlow.js currently has two runtimes on these
    embedded ARM devices: `tfjs-node (CPU^([[11](#ch12fn11)])) and tfjs-headless-nodegl
    (GPU). The entire TensorFlow.js library runs on these devices through those two
    backends. Developers can run inference using off-the-shelf models or train their
    own, all on the device hardware!`'
  prefs: []
  type: TYPE_NORMAL
- en: ^(11)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you are looking to utilize the CPU with ARM NEON acceleration, you should
    use the tfjs-node package on these devices. This package ships support for both
    ARM32 and ARM64 architectures.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The release of recent devices such as the NVIDIA Jetson Nano and Raspberry Pi
    4 brings a system-on-chip (SoC) with a modern graphics stack. The GPU on these
    devices can be leveraged by the underlying WebGL code used in core TensorFlow.js.
    The headless WebGL package `(`tfjs-backend-nodegl) allows users to run TensorFlow.js
    on Node.js purely accelerated by the GPU on these devices (see [figure 12.9](#ch12fig09)).
    By delegating the execution of TensorFlow.js to the GPU, developers can continue
    to utilize the CPU for controlling other parts of their devices.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.9\. TensorFLow.js executing MobileNet using headless WebGL on a Raspberry
    Pi 4
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](12fig09_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Model security and data security are very strong for the single-board computer
    deployment. Computation and actuation are handled directly on the device, meaning
    data does not need to go to a device outside of the owner’s control. Encryption
    can be used to guard the model even if the physical device is compromised.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment to single-board computers is still a very new area for JavaScript
    in general, and TensorFlow.js in particular, but it unlocks a wide range of applications
    that other deployment areas are unsuitable for.
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.8\. Summary of deployments
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this section, we’ve covered several different ways to get your TensorFlow.js
    machine-learning system out in front of the user base ([table 12.4](#ch12table04)
    summarizes them). We hope we’ve kindled your imagination and helped you dream
    about radical applications of the technology! The JavaScript ecosystem is vast
    and wide, and in the future, machine-learning-enabled systems will be running
    in areas we couldn’t even dream of today.
  prefs: []
  type: TYPE_NORMAL
- en: Table 12.4\. Target environments to which TensorFlow.js models can be deployed,
    and the hardware accelerator each environment can use
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Deployment | Hardware accelerator support |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Browser | WebGL |'
  prefs: []
  type: TYPE_TB
- en: '| Node.js server | CPU with multithreading and SIMD support; CUDA-enabled GPU
    |'
  prefs: []
  type: TYPE_TB
- en: '| Browser plugin | WebGL |'
  prefs: []
  type: TYPE_TB
- en: '| Cross-platform desktop app (such as Electron) | WebGL, CPU with multithreading
    and SIMD support, or CUDA-enabled GPU |'
  prefs: []
  type: TYPE_TB
- en: '| Cross-platform mobile app (such as React Native) | WebGL |'
  prefs: []
  type: TYPE_TB
- en: '| Mobile-app plugin (such as WeChat) | Mobile WebGL |'
  prefs: []
  type: TYPE_TB
- en: '| Single-board computer (such as Raspberry Pi) | GPU or ARM NEON |'
  prefs: []
  type: TYPE_TB
- en: Materials for further reading
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Denis Baylor et al., “TFX: A TensorFlow-Based Production-Scale Machine Learning
    Platform,” KDD 2017, [www.kdd.org/kdd2017/papers/view/tfx-a-tensorflow-based-production-scale-machine-learning-platform](http://www.kdd.org/kdd2017/papers/view/tfx-a-tensorflow-based-production-scale-machine-learning-platform).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Raghuraman Krishnamoorthi, “Quantizing Deep Convolutional Networks for Efficient
    Inference: A Whitepaper,” June 2018, [https://arxiv.org/pdf/1806.08342.pdf](https://arxiv.org/pdf/1806.08342.pdf).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rasmus Munk Larsen and Tatiana Shpeisman, “TensorFlow Graph Optimization,” [https://ai.google/research/pubs/pub48051](https://ai.google/research/pubs/pub48051).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Back in [chapter 10](kindle_split_022.html#ch10), we trained an Auxiliary Class
    GAN (ACGAN) on the MNIST dataset to generate fake MNIST digit images by class.
    Specifically, the example we used is in the mnist-acgan directory of the tfjs-examples
    repository. The generator part of the trained model has a total size of about
    10 MB, most of which is occupied by the weights stored as 32-bit floats. It’s
    tempting to perform post-training weight quantization on this model to speed up
    the page loading. However, before doing so, we need to make sure that no significant
    deterioration in the quality of the generated images results from such quantization.
    Test 16- and 8-bit quantization and determine whether either or both of them is
    an acceptable option. Use the `tensorflowjs_converter` workflow described in [section
    12.2.1](#ch12lev2sec4). What criteria will you use to evaluate the quality of
    the generated MNIST images in this case?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tensorflow models that run as Chrome extensions have the advantage of being
    able to control Chrome itself. In the speech-commands example in [chapter 4](kindle_split_015.html#ch04),
    we showed how to use a convolutional model to recognize spoken words. The Chrome
    extension API gives you the ability to query and change tabs. Try embedding the
    speech-commands model into an extension, and tune it to recognize the phrases
    “next tab” and “previous tab.” Use the results of the classifier to control the
    browser tab focus.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Info box 12.3](#ch12sb03) describes the correct way to measure the time that
    a TensorFlow.js model’s `predict()` call (inference call) takes and the cautionary
    points it involves. In this exercise, load a MobileNetV2 model in TensorFlow.js
    (see the simple-object-detection example in [section 5.2](kindle_split_016.html#ch05lev1sec2)
    if you need a reminder of how to do that) and time its `predict()` call:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As the first step, generate a randomly valued image tensor of shape `[1, 224,
    224, 3]` and the model’s inference on it by following the steps laid out in [info
    box 12.3](#ch12sb03). Compare the timing result with and without the `array()`
    or `data()` call on the output tensor. Which one is shorter? Which one is the
    correct time measurement?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: When the correct measurement is done 50 times in a loop, plot the individual
    timing numbers using the tfjs-vis line chart ([chapter 7](kindle_split_019.html#ch07))
    and get an intuitive appreciation of the variability. Can you see clearly that
    the first few measurements are significantly different from the rest? Given this
    observation, discuss the importance of performing burn-in or warm-up runs during
    performance benchmarking.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Unlike tasks a and b, replace the randomly generated input tensor with a real
    image tensor (such as one obtained from an `img` element using `tf.browser.fromPixels()`),
    and then repeat the measurements in step b. Does the content of the input tensor
    affect the timing measurements in any significant way?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Instead of running inference on a single example (batch size = 1), try increasing
    the batch size to 2, 3, 4, and so forth until you reach a relatively large number,
    such as 32\. Is the relation between the average inference time and batch size
    a monotonically increasing one? A linear one?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Good engineering discipline around testing is as important to your machine-learning
    code as it is to your non-machine-learning code. However, avoid the temptation
    to focus strongly on “special” examples or make assertions on “golden” model predictions.
    Instead, rely on testing the fundamental properties of your model, such as its
    input and output specifications. Furthermore, remember that all the data-preprocessing
    code before your machine-learning system is just “normal” code and should be tested
    accordingly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing the speed of downloading and inference is an important factor to
    the success of client-side deployment of TensorFlow.js models. Using the post-training
    weight quantization feature of the `tensorflowjs_converter` binary, you can reduce
    the total size of a model, in some cases without observable loss of inference
    accuracy. The graph-model conversion feature of `tensorflowjs_converter` helps
    to speed up model inference through graph transformations such as op fusion. You
    are highly encouraged to test and employ both model-optimization techniques when
    deploying your TensorFlow.js models to production.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A trained, optimized model is not the end of the story for your machine-learning
    application. You must find some way to integrate it with an actual product. The
    most common way for TensorFlow.js applications to be deployed is within web pages,
    but this is just one of a wide variety of deployment scenarios, each with its
    own strengths. TensorFlow.js models can run as browser extensions, within native
    mobile apps, as native desktop applications, and even on single-board hardware
    like the Raspberry Pi.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
