- en: Chapter 3\. Programming with GPT-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Almost all of GPT-3’s NLP capabilities are created in the Python programming
    language. But to enable wider accessibility, the API comes with built-in support
    for all the major programming languages so users can build GPT-3-powered applications
    using the programming language of their choice.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section we will illustrate how this works by replicating an example
    with three common programming languages: Python, Go, and Java.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Just a heads-up: In each language-specific section, we assume you have a basic
    understanding of the programming language being discussed. If you don’t, you can
    safely skip the section.'
  prefs: []
  type: TYPE_NORMAL
- en: Using the OpenAI API with Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Python is the most popular language for data science and machine learning tasks.
    Compared to conventional data science programming languages like R and Stata,
    Python shines because it’s scalable and integrates well with databases. It is
    widely used and has a flourishing community of developers keeping its ecosystem
    up to date. Python is easy to learn and comes with useful data science libraries
    like NumPy and pandas.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can pair GPT-3 with Python using a library called [Chronology](https://oreil.ly/9eULI)
    that provides a simple, intuitive interface. Chronology can mitigate the monotonous
    work of writing all of your code from scratch every time. Its features include
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: It calls the OpenAI API asynchronously, allowing you to generate multiple prompt
    completions at the same time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can create and modify training prompts easily; for example, modifying a
    training prompt used by a different example is fairly straightforward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It allows you to chain prompts together by plugging the output of one prompt
    into another.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chronology is hosted on PyPI and supports Python 3.6 and above. To install
    the library, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: After you install the Python library via PyPI, let’s look at an example of how
    to prime GPT-3 to summarize a given text document at a second-grade reading level.
    We’ll show you how to call the API, send the training prompt as a request, and
    get the summarized completion as an output. We’ve posted the code for you in a
    [GitHub repository](https://oreil.ly/nVaC9).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we will use the following training prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'First, import the following dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can create a function that reads the training prompt and provides the
    completion output. We have made this function asynchronous, which allows us to
    carry out parallel function calls. We will use the following configuration for
    the API parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: Maximum tokens=100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Execution Engine=“Davinci”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Temperature=0.5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Top-p=1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frequency Penalty=0.2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stop Sequence=[“\n\n”]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can create an asynchronous workflow, invoke that workflow using the
    `main` function provided by the library, and print the output in the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Save it as a Python script with the name *text_summarization.py* and run it
    from the terminal to generate the output. You can run the following command from
    your root folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you execute the script, your console should print the following summary
    of the prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: If you are not well-versed in Python and want to chain different prompts without
    writing code, you can use the [no-code interface](https://oreil.ly/L2TUK) built
    on top of the [Chronology library](https://oreil.ly/f02Cx) to create the prompt
    workflow using drag-and-drop. See our GitHub [repository](https://oreil.ly/AfTQM)
    for more examples of how you can use Python programming to interact with GPT-3.
  prefs: []
  type: TYPE_NORMAL
- en: Using the OpenAI API with Go
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Go is an open source programming language that combines the best features of
    other programming languages, blending the ease of programming of an interpreted,
    dynamically typed language with the efficiency and safety of a statically typed,
    compiled language. Developers often call it “C for the 21st century.”
  prefs: []
  type: TYPE_NORMAL
- en: 'Go is the language of preference for building projects that require high security,
    high speed, and high modularity. This makes it an attractive option for many projects
    in the fintech industry. Key features of Go are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Ease of use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State-of-the-art productivity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High-efficiency Static typing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced performance for networking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Full use of multicore power
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are completely new to Go and want to give it a try, you can [follow the
    documentation](https://oreil.ly/3ZjiB) to get started.
  prefs: []
  type: TYPE_NORMAL
- en: Once you are done with the installation and understand the basics of Go programming,
    you can follow these steps to use the [Go API wrapper for GPT-3](https://oreil.ly/j6lUY).
    To learn more about creating Go modules, see [this tutorial](https://oreil.ly/w1334).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you’ll create a module to track and import code dependencies. Create
    and initialize the `gogpt` module using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'After creating the `gogpt` module, let’s point it to [this GitHub repository](https://oreil.ly/6o2Hj)
    to download the necessary dependencies and packages for working with the API.
    Use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We’ll use the same text summarization example as in the previous section. (You
    can find all the code in the following [repository](https://oreil.ly/r5HhV).)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the necessary dependencies and packages for starters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Go programming organizes source files into system directories called *packages*,
    which makes it easier to reuse code across Go applications. In the first line
    of the code we call the package `main`, which tells the Go compiler that the package
    should compile as an executable program instead of a shared library.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In Go, whenever you build reusable pieces of code, you will develop a package
    as a shared library. But when you develop executable programs, you will use the
    package `main` to make the package an executable program. Because we are calling
    this package the main function in the package, `main` will be set up as the entry
    point of our executable program.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you’ll create a `main` function that will host the entire logic of reading
    the training prompt and providing the completion output. Use the following configuration
    for the API parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: Maximum tokens=100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Execution Engine=“Davinci”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Temperature=0.5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Top-p=1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frequency Penalty=0.2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stop Sequence=[“\n\n”]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This code performs the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Sets up a new API client by providing it with the API token and then leaves
    it to run in the background.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reads the prompt " " in the form of a text file from the *prompts* folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creates a completion request by providing the training prompt and specifying
    the values of API parameters (like temperature, Top P, stop sequence, and so forth).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calls the `create completion` function and provides it with the API client,
    completion request, and execution engine.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generates a response in the form of a completion, which prints toward the end
    in the console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can then save the code file as *text_summarization.go* and run it from
    the terminal to generate the output. Use the following command to run the file
    from your root folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you execute the file, your console will print the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: For more examples of how you can use Go programming to interact with GPT-3,
    please visit our GitHub [repository](https://oreil.ly/r5HhV).
  prefs: []
  type: TYPE_NORMAL
- en: Using the OpenAI API with Java
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Java is one of the oldest and most popular programming languages for developing
    conventional software systems; it is also a platform that comes with a runtime
    environment. It was developed by Sun Microsystems (now a subsidiary of Oracle)
    in 1995, and as of today more than three billion devices run on it. It is a general-purpose,
    class-based, object-oriented programming language designed to have fewer implementation
    dependencies. Its syntax is similar to that of C and C++. Two-thirds of the software
    industry still uses Java as its core programming language.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use our olive oil text summarization example once more. As we did with
    Python and Go, we’ll show you how to call the API, send the training prompt as
    a request, and get the summarized completion as an output using Java.
  prefs: []
  type: TYPE_NORMAL
- en: For a step-by-step code walk-through on your local machine, clone our GitHub
    [repository](https://oreil.ly/gpt3-repo). In the cloned repository go to the *Programming_with_GPT-3*
    folder and open the *GPT-3_Java* folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import all the relevant dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you’ll create a class named `OpenAiApiExample`. All of your code will be
    a part of it. Under that class, first create an `OpenAiService` object using the
    API token:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The connection to the OpenAI API is now established in the form of a *service
    object*. Read the training prompt from the *prompts* folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Then you can create a completion request with the following configuration for
    the API parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: Maximum tokens=100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Execution Engine=“Davinci”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Temperature=0.5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Top-p=1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frequency Penalty=0.2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stop Sequence=[“\n\n”]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Save the code file as *text_summarization.java* and run it from the terminal
    to generate the output. You can use the following command to run the file from
    your root folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Your console should print the same summary as it did with the previous examples.
    For more examples of how you can use Java to interact with GPT-3, see our GitHub
    [repository](https://oreil.ly/amtjY).
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3 Sandbox Powered by Streamlit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section we will walk you through the GPT-3 Sandbox, an open source tool
    we’ve created that provides boilerplate code to help you turn your ideas into
    reality with just a few lines of Python code. We’ll show you how to use it and
    how to customize it for your specific application.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of our sandbox is to empower you to create cool web applications, no
    matter what your technical background. It is built on top of the Streamlit framework.
  prefs: []
  type: TYPE_NORMAL
- en: To accompany this book, we have also created a [video series](https://oreil.ly/jQrlG)
    with step-by-step instructions for creating and deploying your GPT-3 application,
    which you can access by scanning the QR code in [Figure 3-1](#qr_code_for_gpt_three_sandbox_video_ser).
    Please follow it as you read this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gpt3_0301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-1\. QR code for GPT-3 Sandbox video series
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We use Visual Studio Code as the IDE for our examples, but feel free to use
    any IDE. You’ll need to install the IDE before you start. Please also make sure
    you are running Python version 3.7 or higher. You can confirm which version you
    have installed by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Clone the code from this [repository](https://oreil.ly/gpt3-repo) by opening
    a new terminal in your IDE and using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: After you clone the repository, the code structure in your IDE should look like
    [Figure 3-2](#sandbox_file_directory_structure).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gpt3_0302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-2\. Sandbox file directory structure
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Everything you need to create and deploy a web application is already present
    in the code. You just need to tweak a few files to customize the sandbox for your
    specific use case.
  prefs: []
  type: TYPE_NORMAL
- en: Create a [Python virtual environment](https://oreil.ly/iUWv4), which you’ll
    name *env*. Then you can install the required dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to the *email_generation* folder. Your path should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'From there, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Now you can start customizing the sandbox code. The first file that you need
    to look at is *training_data.py*. Open that file and replace the default prompt
    with the training prompt you want to use. You can use the GPT-3 Playground to
    experiment with different training prompts (see [Chapter 2](ch02.xhtml#using_the_openai_api)
    and our [video](https://oreil.ly/nCIgG) for more on customizing the sandbox).
  prefs: []
  type: TYPE_NORMAL
- en: You’re now ready to tweak the API parameters (maximum tokens, execution engine,
    temperature, Top P, frequency penalty, stop sequence) as per the requirements
    of your application use case. We recommend experimenting with different parameter
    values for a given training prompt in the Playground to determine what values
    will work best for your use case. Once you get satisfactory results, then you
    can alter the values in the *model_training_service.py* file.
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s it! Your GPT-3-based web application is now ready. You can run it locally
    using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Check to make sure it works, and then you can deploy the application to the
    internet using Streamlit sharing to showcase it to a wider audience. Our [video](https://oreil.ly/h5uTe)
    offers a full deployment walk-through.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This application follows a simple workflow, where the training prompt receives
    a single input from the UI and comes up with the response. If your application
    requires a more complex workflow, where the training prompt takes in multiple
    inputs, customize the UI elements by going through the scripts *app1.py*, *app2.py*,
    and *gpt_app.py*. For details, refer to the [Streamlit documentation](https://docs.streamlit.io).
  prefs: []
  type: TYPE_NORMAL
- en: In the next few chapters, we will explore different applications of GPT-3 and
    how successful businesses are built on top of it.
  prefs: []
  type: TYPE_NORMAL
- en: Going Live with GPT-3-Powered Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Are you ready to put your GPT-3-powered application into production?
  prefs: []
  type: TYPE_NORMAL
- en: Before you do, let’s discuss some risk mitigation measures. In [Chapter 6](ch06.xhtml#challengescomma_controversiescomma_and),
    you will learn some of the ways GPT-3 can be used to do harm. To safeguard against
    those malicious practices, OpenAI has created guidelines and procedures that you
    must follow before going live with your application. Currently, you can serve
    the API out to five people without pre-approval, but for more, you’ll need to
    apply for a pre-launch production review. We highly recommend reading the [Usage
    Guidelines](https://oreil.ly/oGBGJ) before you apply.
  prefs: []
  type: TYPE_NORMAL
- en: When you submit the [Pre-Launch Review Request](https://oreil.ly/SO0VZ), the
    OpenAI team looks into your use case in detail and flags any potential violations
    of the API [Safety Best Practices](https://oreil.ly/gXRhC). If your request is
    approved, OpenAI will grant you a maximum spend limit, which will increase over
    time as you build a track record. As your user base grows, you can submit a [Quota
    Increase Request](https://oreil.ly/b25mG). This gives you freedom to build and
    deploy your application while OpenAI monitors its potential impact on the platform.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to use the OpenAI API with the programming languages
    Python, Go, and Java. We also walked through a low-code sandbox environment created
    using Streamlit that will help you to quickly turn your idea into an application.
    Lastly, we looked at the key requirements to go live with a GPT-3 application.
    This chapter provided you with the programming outlook of the API; going forward
    we’ll dive deeper into the burgeoning ecosystem empowered by GPT-3.
  prefs: []
  type: TYPE_NORMAL
