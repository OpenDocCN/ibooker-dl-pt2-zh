["```py\nfrom nltk.corpus.reader import PlaintextCorpusReader\nfrom nltk.util import everygrams\nfrom nltk.lm.preprocessing import (\n    pad_both_ends,\n    flatten,\n    padded_everygram_pipeline,\n)\nfrom nltk.lm import MLE\n\n# Create a corpus from any number of plain .txt files\nmy_corpus = PlaintextCorpusReader(\"./\", \".*\\.txt\")\n\nfor sent in my_corpus.sents(fileids=\"hamlet.txt\"):\n    print(sent)\n\n# Pad each side of every line in the corpus with <s> and </s> to indicate the start and end of utterances\npadded_trigrams = list(\n    pad_both_ends(my_corpus.sents(fileids=\"hamlet.txt\")[1104], n=2)\n)\nlist(everygrams(padded_trigrams, max_len=3))\n\nlist(\n    flatten(\n        pad_both_ends(sent, n=2)\n        for sent in my_corpus.sents(fileids=\"hamlet.txt\")\n    )\n)\n\n# Allow everygrams to create a training set and a vocab object from the data\ntrain, vocab = padded_everygram_pipeline(\n    3, my_corpus.sents(fileids=\"hamlet.txt\")\n)\n\n# Instantiate and train the model we’ll use for N-Grams, a Maximum Likelihood Estimator (MLE)\n# This model will take the everygrams vocabulary, including the <UNK> token used for out-of-vocabulary\nlm = MLE(3)\nlen(lm.vocab)\n\nlm.fit(train, vocab)\nprint(lm.vocab)\nlen(lm.vocab)\n\n# And finally, language can be generated with this model and conditioned with n-1 tokens preceding\nlm.generate(6, [\"to\", \"be\"])\n```", "```py\n# Any set of tokens up to length=n can be counted easily to determine frequency\nprint(lm.counts)\nlm.counts[[\"to\"]][\"be\"]\n\n# Any token can be given a probability of occurrence, and can be augmented with up to n-1 tokens to precede it\nprint(lm.score(\"be\"))\nprint(lm.score(\"be\", [\"to\"]))\nprint(lm.score(\"be\", [\"not\", \"to\"]))\n\n# This can be done as a log score as well to avoid very big and very small numbers\nprint(lm.logscore(\"be\"))\nprint(lm.logscore(\"be\", [\"to\"]))\nprint(lm.logscore(\"be\", [\"not\", \"to\"]))\n\n# Sets of tokens can be tested for entropy and perplexity as well\ntest = [(\"to\", \"be\"), (\"or\", \"not\"), (\"to\", \"be\")]\nprint(lm.entropy(test))\nprint(lm.perplexity(test))\n```", "```py\nfrom utils import process_utt, lookup\nfrom nltk.corpus.reader import PlaintextCorpusReader\nimport numpy as np\n\nmy_corpus = PlaintextCorpusReader(\"./\", \".*\\.txt\")\n\nsents = my_corpus.sents(fileids=\"hamlet.txt\")\n\ndef count_utts(result, utts, ys):\n    \"\"\"\n    Input:\n        result: a dictionary that is used to map each pair to its frequency\n        utts: a list of utts\n        ys: a list of the sentiment of each utt (either 0 or 1)\n    Output:\n        result: a dictionary mapping each pair to its frequency\n    \"\"\"\n\n    for y, utt in zip(ys, utts):\n        for word in process_utt(utt):\n            # define the key, which is the word and label tuple\n            pair = (word, y)\n\n            # if the key exists in the dictionary, increment the count\n            if pair in result:\n                result[pair] += 1\n\n            # if the key is new, add it to the dict and set the count to 1\n            else:\n                result[pair] = 1\n\n    return result\n\nresult = {}\nutts = [\" \".join(sent) for sent in sents]\nys = [sent.count(\"be\") > 0 for sent in sents]\ncount_utts(result, utts, ys)\n\nfreqs = count_utts({}, utts, ys)\nlookup(freqs, \"be\", True)\nfor k, v in freqs.items():\n    if \"be\" in k:\n        print(f\"{k}:{v}\")\n\ndef train_naive_bayes(freqs, train_x, train_y):\n    \"\"\"\n    Input:\n        freqs: dictionary from (word, label) to how often the word appears\n        train_x: a list of utts\n        train_y: a list of labels correponding to the utts (0,1)\n    Output:\n        logprior: the log prior.\n        loglikelihood: the log likelihood of you Naive bayes equation.\n    \"\"\"\n    loglikelihood = {}\n    logprior = 0\n\n    # calculate V, the number of unique words in the vocabulary\n    vocab = set([pair[0] for pair in freqs.keys()])\n    V = len(vocab)\n\n    # calculate N_pos and N_neg\n    N_pos = N_neg = 0\n    for pair in freqs.keys():\n        # if the label is positive (greater than zero)\n        if pair[1] > 0:\n            # Increment the number of positive words (word, label)\n            N_pos += lookup(freqs, pair[0], True)\n\n        # else, the label is negative\n        else:\n            # increment the number of negative words (word,label)\n            N_neg += lookup(freqs, pair[0], False)\n\n    # Calculate D, the number of documents\n    D = len(train_y)\n\n    # Calculate the number of positive documents\n    D_pos = sum(train_y)\n\n    # Calculate the number of negative documents\n    D_neg = D - D_pos\n\n    # Calculate logprior\n    logprior = np.log(D_pos) - np.log(D_neg)\n\n    # For each word in the vocabulary...\n    for word in vocab:\n        # get the positive and negative frequency of the word\n        freq_pos = lookup(freqs, word, 1)\n        freq_neg = lookup(freqs, word, 0)\n\n        # calculate the probability that each word is positive, and negative\n        p_w_pos = (freq_pos + 1) / (N_pos + V)\n        p_w_neg = (freq_neg + 1) / (N_neg + V)\n\n        # calculate the log likelihood of the word\n        loglikelihood[word] = np.log(p_w_pos / p_w_neg)\n\n    return logprior, loglikelihood\n\ndef naive_bayes_predict(utt, logprior, loglikelihood):\n    \"\"\"\n    Input:\n        utt: a string\n        logprior: a number\n        loglikelihood: a dictionary of words mapping to numbers\n    Output:\n        p: the sum of all the logliklihoods + logprior\n    \"\"\"\n    # process the utt to get a list of words\n    word_l = process_utt(utt)\n\n    # initialize probability to zero\n    p = 0\n\n    # add the logprior\n    p += logprior\n\n    for word in word_l:\n        # check if the word exists in the loglikelihood dictionary\n        if word in loglikelihood:\n            # add the log likelihood of that word to the probability\n            p += loglikelihood[word]\n\n    return p\n\ndef test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n    \"\"\"\n    Input:\n        test_x: A list of utts\n        test_y: the corresponding labels for the list of utts\n        logprior: the logprior\n        loglikelihood: a dictionary with the loglikelihoods for each word\n    Output:\n        accuracy: (# of utts classified correctly)/(total # of utts)\n    \"\"\"\n    accuracy = 0  # return this properly\n\n    y_hats = []\n    for utt in test_x:\n        # if the prediction is > 0\n        if naive_bayes_predict(utt, logprior, loglikelihood) > 0:\n            # the predicted class is 1\n            y_hat_i = 1\n        else:\n            # otherwise the predicted class is 0\n            y_hat_i = 0\n\n        # append the predicted class to the list y_hats\n        y_hats.append(y_hat_i)\n\n    # error = avg of the abs vals of the diffs between y_hats and test_y\n    error = sum(\n        [abs(y_hat - test) for y_hat, test in zip(y_hats, test_y)]\n    ) / len(y_hats)\n\n    # Accuracy is 1 minus the error\n    accuracy = 1 - error\n\n    return accuracy\n\nif __name__ == \"__main__\":\n    logprior, loglikelihood = train_naive_bayes(freqs, utts, ys)\n    print(logprior)\n    print(len(loglikelihood))\n\n    my_utt = \"To be or not to be, that is the question.\"\n    p = naive_bayes_predict(my_utt, logprior, loglikelihood)\n    print(\"The expected output is\", p)\n\n    print(\n        \"Naive Bayes accuracy = %0.4f\"\n        % (test_naive_bayes(utts, ys, logprior, loglikelihood))\n    )\n```", "```py\nimport re\nimport random\nfrom nltk.tokenize import word_tokenize\nfrom collections import defaultdict, deque\n\nclass MarkovChain:\n    def __init__(self):\n        self.lookup_dict = defaultdict(list)\n        self._seeded = False\n        self.__seed_me()\n\n    def __seed_me(self, rand_seed=None):\n        if self._seeded is not True:\n            try:\n                if rand_seed is not None:\n                    random.seed(rand_seed)\n                else:\n                    random.seed()\n                self._seeded = True\n            except NotImplementedError:\n                self._seeded = False\n\n    def add_document(self, str):\n        preprocessed_list = self._preprocess(str)\n        pairs = self.__generate_tuple_keys(preprocessed_list)\n        for pair in pairs:\n            self.lookup_dict[pair[0]].append(pair[1])\n\n    def _preprocess(self, str):\n        cleaned = re.sub(r\"\\W+\", \" \", str).lower()\n        tokenized = word_tokenize(cleaned)\n        return tokenized\n\n    def __generate_tuple_keys(self, data):\n        if len(data) < 1:\n            return\n\n        for i in range(len(data) - 1):\n            yield [data[i], data[i + 1]]\n\n    def generate_text(self, max_length=50):\n        context = deque()\n        output = []\n        if len(self.lookup_dict) > 0:\n            self.__seed_me(rand_seed=len(self.lookup_dict))\n            chain_head = [list(self.lookup_dict)[0]]\n            context.extend(chain_head)\n\n            while len(output) < (max_length - 1):\n                next_choices = self.lookup_dict[context[-1]]\n                if len(next_choices) > 0:\n                    next_word = random.choice(next_choices)\n                    context.append(next_word)\n                    output.append(context.popleft())\n                else:\n                    break\n            output.extend(list(context))\n        return \" \".join(output)\n\nif __name__ == \"__main__\":\n    with open(\"hamlet.txt\", \"r\", encoding=\"utf-8\") as f:\n        text = f.read()\n    HMM = MarkovChain()\n    HMM.add_document(text)\n\n    print(HMM.generate_text(max_length=25))\n```", "```py\nimport nltk\nimport numpy as np\nfrom utils import get_batches, compute_pca, get_dict\nimport re\nfrom matplotlib import pyplot\n\n# Create our corpus for training\nwith open(\"hamlet.txt\", \"r\", encoding=\"utf-8\") as f:\n    data = f.read()\n\n# Slightly clean the data by removing punctuation, tokenizing by word, and converting to lowercase alpha characters\ndata = re.sub(r\"[,!?;-]\", \".\", data)\ndata = nltk.word_tokenize(data)\ndata = [ch.lower() for ch in data if ch.isalpha() or ch == \".\"]\nprint(\"Number of tokens:\", len(data), \"\\n\", data[500:515])\n\n# Get our Bag of Words, along with a distribution\nfdist = nltk.FreqDist(word for word in data)\nprint(\"Size of vocabulary:\", len(fdist))\nprint(\"Most Frequent Tokens:\", fdist.most_common(20))\n\n# Create 2 dictionaries to speed up time-to-convert and keep track of vocabulary\nword2Ind, Ind2word = get_dict(data)\nV = len(word2Ind)\nprint(\"Size of vocabulary:\", V)\n\nprint(\"Index of the word 'king':\", word2Ind[\"king\"])\nprint(\"Word which has index 2743:\", Ind2word[2743])\n\n# Here we create our Neural network with 1 layer and 2 parameters\ndef initialize_model(N, V, random_seed=1):\n    \"\"\"\n    Inputs:\n        N: dimension of hidden vector\n        V: dimension of vocabulary\n        random_seed: seed for consistent results in tests\n    Outputs:\n        W1, W2, b1, b2: initialized weights and biases\n    \"\"\"\n    np.random.seed(random_seed)\n\n    W1 = np.random.rand(N, V)\n    W2 = np.random.rand(V, N)\n    b1 = np.random.rand(N, 1)\n    b2 = np.random.rand(V, 1)\n\n    return W1, W2, b1, b2\n\n# Create our final classification layer, which makes all possibilities add up to 1\ndef softmax(z):\n    \"\"\"\n    Inputs:\n        z: output scores from the hidden layer\n    Outputs:\n        yhat: prediction (estimate of y)\n    \"\"\"\n    yhat = np.exp(z) / np.sum(np.exp(z), axis=0)\n    return yhat\n\n# Define the behavior for moving forward through our model, along with an activation function\ndef forward_prop(x, W1, W2, b1, b2):\n    \"\"\"\n    Inputs:\n        x: average one-hot vector for the context\n        W1,W2,b1,b2: weights and biases to be learned\n    Outputs:\n        z: output score vector\n    \"\"\"\n    h = W1 @ x + b1\n    h = np.maximum(0, h)\n    z = W2 @ h + b2\n    return z, h\n\n# Define how we determine the distance between ground truth and model predictions\ndef compute_cost(y, yhat, batch_size):\n    logprobs = np.multiply(np.log(yhat), y) + np.multiply(\n        np.log(1 - yhat), 1 - y\n    )\n    cost = -1 / batch_size * np.sum(logprobs)\n    cost = np.squeeze(cost)\n    return cost\n\n# Define how we move backward through the model and collect gradients\ndef back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size):\n    \"\"\"\n    Inputs:\n        x:  average one hot vector for the context\n        yhat: prediction (estimate of y)\n        y:  target vector\n        h:  hidden vector (see eq. 1)\n        W1, W2, b1, b2:  weights and biases\n        batch_size: batch size\n     Outputs:\n        grad_W1, grad_W2, grad_b1, grad_b2:  gradients of weights and biases\n    \"\"\"\n    l1 = np.dot(W2.T, yhat - y)\n    l1 = np.maximum(0, l1)\n    grad_W1 = np.dot(l1, x.T) / batch_size\n    grad_W2 = np.dot(yhat - y, h.T) / batch_size\n    grad_b1 = np.sum(l1, axis=1, keepdims=True) / batch_size\n    grad_b2 = np.sum(yhat - y, axis=1, keepdims=True) / batch_size\n\n    return grad_W1, grad_W2, grad_b1, grad_b2\n\n# Put it all together and train\ndef gradient_descent(data, word2Ind, N, V, num_iters, alpha=0.03):\n    \"\"\"\n    This is the gradient_descent function\n\n      Inputs:\n        data:      text\n        word2Ind:  words to Indices\n        N:         dimension of hidden vector\n        V:         dimension of vocabulary\n        num_iters: number of iterations\n     Outputs:\n        W1, W2, b1, b2:  updated matrices and biases\n\n    \"\"\"\n    W1, W2, b1, b2 = initialize_model(N, V, random_seed=8855)\n    batch_size = 128\n    iters = 0\n    C = 2\n    for x, y in get_batches(data, word2Ind, V, C, batch_size):\n        z, h = forward_prop(x, W1, W2, b1, b2)\n        yhat = softmax(z)\n        cost = compute_cost(y, yhat, batch_size)\n        if (iters + 1) % 10 == 0:\n            print(f\"iters: {iters+1} cost: {cost:.6f}\")\n        grad_W1, grad_W2, grad_b1, grad_b2 = back_prop(\n            x, yhat, y, h, W1, W2, b1, b2, batch_size\n        )\n        W1 = W1 - alpha * grad_W1\n        W2 = W2 - alpha * grad_W2\n        b1 = b1 - alpha * grad_b1\n        b2 = b2 - alpha * grad_b2\n        iters += 1\n        if iters == num_iters:\n            break\n        if iters % 100 == 0:\n            alpha *= 0.66\n\n    return W1, W2, b1, b2\n\n# Train the model\nC = 2\nN = 50\nword2Ind, Ind2word = get_dict(data)\nV = len(word2Ind)\nnum_iters = 150\nprint(\"Call gradient_descent\")\nW1, W2, b1, b2 = gradient_descent(data, word2Ind, N, V, num_iters)\nCall gradient descent\nIters: 10 loss: 0.525015\nIters: 20 loss: 0.092373\nIters: 30 loss: 0.050474\nIters: 40 loss: 0.034724\nIters: 50 loss: 0.026468\nIters: 60 loss: 0.021385\nIters: 70 loss: 0.017941\nIters: 80 loss: 0.015453\nIters: 90 loss: 0.012099\nIters: 100 loss: 0.012099\nIters: 110 loss: 0.011253\nIters: 120 loss: 0.010551\nIters: 130 loss: 0.009932\nIters: 140 loss: 0.009382\nIters: 150 loss: 0.008889\n```", "```py\n# After listing 2.4 is done and gradient descent has been executed\nwords = [\n    \"King\",\n    \"Queen\",\n    \"Lord\",\n    \"Man\",\n    \"Woman\",\n    \"Prince\",\n    \"Ophelia\",\n    \"Rich\",\n    \"Happy\",\n]\nembs = (W1.T + W2) / 2.0\nidx = [word2Ind[word] for word in words]\nX = embs[idx, :]\nprint(X.shape, idx)\n\nresult = compute_pca(X, 2)\npyplot.scatter(result[:, 0], result[:, 1])\nfor i, word in enumerate(words):\n    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\npyplot.show()\n```", "```py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass MultiLayerPerceptron(nn.Module):\n    def __init__(\n        self,\n        input_size,\n        hidden_size=2,\n        output_size=3,\n        num_hidden_layers=1,\n        hidden_activation=nn.Sigmoid,\n    ):\n        \"\"\"Initialize weights.\n        Args:\n            input_size (int): size of the input\n            hidden_size (int): size of the hidden layers\n            output_size (int): size of the output\n            num_hidden_layers (int): number of hidden layers\n            hidden_activation (torch.nn.*): the activation class\n        \"\"\"\n        super(MultiLayerPerceptron, self).__init__()\n        self.module_list = nn.ModuleList()\n        interim_input_size = input_size\n        interim_output_size = hidden_size\n        torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n        for _ in range(num_hidden_layers):\n            self.module_list.append(\n                nn.Linear(interim_input_size, interim_output_size)\n            )\n            self.module_list.append(hidden_activation())\n            interim_input_size = interim_output_size\n\n        self.fc_final = nn.Linear(interim_input_size, output_size)\n\n        self.last_forward_cache = []\n\n    def forward(self, x, apply_softmax=False):\n        \"\"\"The forward pass of the MLP\n\n        Args:\n            x_in (torch.Tensor): an input data tensor.\n                x_in.shape should be (batch, input_dim)\n            apply_softmax (bool): a flag for the softmax activation\n                should be false if used with the Cross Entropy losses\n        Returns:\n            the resulting tensor. tensor.shape should be (batch, output_dim)\n        \"\"\"\n        for module in self.module_list:\n            x = module(x)\n\n        output = self.fc_final(x)\n\n        if apply_softmax:\n            output = F.softmax(output, dim=1)\n\n        return output\n```", "```py\nimport torch\nfrom gensim.models import Word2Vec\nfrom sklearn.model_selection import train_test_split\n\n# Create our corpus for training\nwith open(\"./chapters/chapter_2/hamlet.txt\", \"r\", encoding=\"utf-8\") as f:\n    data = f.readlines()\n\n# Embeddings are needed to give semantic value to the inputs of an LSTM\n# embedding_weights = torch.Tensor(word_vectors.vectors)\n\nEMBEDDING_DIM = 100\nmodel = Word2Vec(data, vector_size=EMBEDDING_DIM, window=3, min_count=3, workers=4)\nword_vectors = model.wv\nprint(f\"Vocabulary Length: {len(model.wv)}\")\ndel model\n\npadding_value = len(word_vectors.index_to_key)\nembedding_weights = torch.Tensor(word_vectors.vectors)\n\nclass RNN(torch.nn.Module):\n    def __init__(\n        self,\n        input_dim,\n        embedding_dim,\n        hidden_dim,\n        output_dim,\n        embedding_weights,\n    ):\n        super().__init__()\n        self.embedding = torch.nn.Embedding.from_pretrained(\n            embedding_weights\n        )\n        self.rnn = torch.nn.RNN(embedding_dim, hidden_dim)\n        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x, text_lengths):\n        embedded = self.embedding(x)\n        packed_embedded = torch.nn.utils.rnn.pack_padded_sequence(\n            embedded, text_lengths\n        )\n        packed_output, hidden = self.rnn(packed_embedded)\n        output, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(\n            packed_output\n        )\n        return self.fc(hidden.squeeze(0))\n\nINPUT_DIM = 4764\nEMBEDDING_DIM = 100\nHIDDEN_DIM = 256\nOUTPUT_DIM = 1\n\nmodel = RNN(\n    INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, embedding_weights\n)\n\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\ncriterion = torch.nn.BCEWithLogitsLoss()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclass LSTM(torch.nn.Module):\n    def __init__(\n        self,\n        input_dim,\n        embedding_dim,\n        hidden_dim,\n        output_dim,\n        n_layers,\n        bidirectional,\n        dropout,\n        embedding_weights,\n    ):\n        super().__init__()\n        self.embedding = torch.nn.Embedding.from_pretrained(\n            embedding_weights\n        )\n        self.rnn = torch.nn.LSTM(\n            embedding_dim,\n            hidden_dim,\n            num_layers=n_layers,\n            bidirectional=bidirectional,\n            dropout=dropout,\n        )\n        self.fc = torch.nn.Linear(hidden_dim * 2, output_dim)\n        self.dropout = torch.nn.Dropout(dropout)\n\n    def forward(self, x, text_lengths):\n        embedded = self.embedding(x)\n        packed_embedded = torch.nn.utils.rnn.pack_padded_sequence(\n            embedded, text_lengths\n        )\n        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n        hidden = self.dropout(\n            torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n        )\n        return self.fc(hidden.squeeze(0))\n\nINPUT_DIM = padding_value\nEMBEDDING_DIM = 100\nHIDDEN_DIM = 256\nOUTPUT_DIM = 1\nN_LAYERS = 2\nBIDIRECTIONAL = True\nDROPOUT = 0.5\n\nmodel = LSTM(\n    INPUT_DIM,\n    EMBEDDING_DIM,\n    HIDDEN_DIM,\n    OUTPUT_DIM,\n    N_LAYERS,\n    BIDIRECTIONAL,\n    DROPOUT,\n    embedding_weights,\n)\n\noptimizer = torch.optim.Adam(model.parameters())\ncriterion = torch.nn.BCEWithLogitsLoss()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef binary_accuracy(preds, y):\n  rounded_preds = torch.round(torch.sigmoid(preds))\n  correct = (rounded_preds == y).float()\n  acc = correct.sum()/len(correct)\n  return acc\n\ndef train(model, iterator, optimizer, criterion):\n    epoch_loss = 0\n    epoch_acc = 0\n    model.train()\n    for batch in iterator:\n        optimizer.zero_grad()\n        predictions = model(batch[\"text\"], batch[\"length\"]).squeeze(1)\n        loss = criterion(predictions, batch[\"label\"])\n        acc = binary_accuracy(predictions, batch[\"label\"])\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n\n    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n\ndef evaluate(model, iterator, criterion):\n    epoch_loss = 0\n    epoch_acc = 0\n    model.eval()\n    with torch.no_grad():\n        for batch in iterator:\n            predictions = model(batch[\"text\"], batch[\"length\"]).squeeze(1)\n            loss = criterion(predictions, batch[\"label\"])\n            acc = binary_accuracy(predictions, batch[\"label\"])\n\n            epoch_loss += loss.item()\n            epoch_acc += acc.item()\n\n    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n\nbatch_size = 128 # Usually should be a power of 2 because it's the easiest for computer memory.\n\ndef iterator(X, y):\n  size = len(X)\n  permutation = np.random.permutation(size)\n  iterate = []\n  for i in range(0, size, batch_size):\n    indices = permutation[i:i+batch_size]\n    batch = {}\n    batch['text'] = [X[i] for i in indices]\n    batch['label'] = [y[i] for i in indices]\n\n    batch['text'], batch['label'] = zip(*sorted(zip(batch['text'], batch['label']), key = lambda x: len(x[0]), reverse = True))\n    batch['length'] = [len(utt) for utt in batch['text']]\n    batch['length'] = torch.IntTensor(batch['length'])\n    batch['text'] = torch.nn.utils.rnn.pad_sequence(batch['text'], batch_first = True).t()\n    batch['label'] = torch.Tensor(batch['label'])\n\n    batch['label'] = batch['label'].to(device)\n    batch['length'] = batch['length'].to(device)\n    batch['text'] = batch['text'].to(device)\n\n    iterate.append(batch)\n\n  return iterate\n\nindex_utt = word_vectors.key_to_index\n\n#You've got to determine some labels for whatever you're training on.\nX_train, X_test, y_train, y_test = train_test_split(index_utt, labels, test_size = 0.2)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2)\n\ntrain_iterator = iterator(X_train, y_train)\nvalidate_iterator = iterator(X_val, y_val)\ntest_iterator = iterator(X_test, y_test)\n\nprint(len(train_iterator), len(validate_iterator), len(test_iterator))\n\nN_EPOCHS = 25\n\nfor epoch in range(N_EPOCHS):\n    train_loss, train_acc = train(\n        model, train_iterator, optimizer, criterion\n    )\n    valid_loss, valid_acc = evaluate(model, validate_iterator, criterion)\n\n    print(\n        f\"| Epoch: {epoch+1:02} | Train Loss: {train_loss: .3f} | Train Acc: {train_acc*100: .2f}% | Validation Loss: {valid_loss: .3f} | Validation Acc: {valid_acc*100: .2f}% |\"\n    )\n#Training on our dataset\n| Epoch: 01 | Train Loss:  0.560 | Train Acc:  70.63% | Validation Loss:  0.574 | Validation Acc:  70.88% |\n| Epoch: 05 | Train Loss:  0.391 | Train Acc:  82.81% | Validation Loss:  0.368 | Validation Acc:  83.08% |\n| Epoch: 10 | Train Loss:  0.270 | Train Acc:  89.11% | Validation Loss:  0.315 | Validation Acc:  86.22% |\n| Epoch: 15 | Train Loss:  0.186 | Train Acc:  92.95% | Validation Loss:  0.381 | Validation Acc:  87.49% |\n| Epoch: 20 | Train Loss:  0.121 | Train Acc:  95.93% | Validation Loss:  0.444 | Validation Acc:  86.29% |\n| Epoch: 25 | Train Loss:  0.100 | Train Acc:  96.28% | Validation Loss:  0.451 | Validation Acc:  86.83% |\n```", "```py\nimport numpy as np\nfrom scipy.special import softmax\n\n# Step 1: Input: 3 inputs, d_model=4\nx = np.array([[1.0, 0.0, 1.0, 0.0],\n        [0.0, 2.0, 0.0, 2.0],\n        [1.0, 1.0, 1.0, 1.0]])\n\n# Step 2: weights 3 dimensions x d_model=4\nw_query = np.array([1,0,1],\n            [1,0,0],\n            [0,0,1],\n            [0,1,1]])\nw_key = np.array([[0,0,1],\n           [1,1,0],\n           [0,1,0],\n           [1,1,0]])\nw_value = np.array([[0,2,0],\n             [0,3,0],\n             [1,0,3],\n             [1,1,0]])\n\n# Step 3: Matrix Multiplication to obtain Q,K,V\n## Query: x * w_query\nQ = np.matmul(x,w_query)\n## Key: x * w_key\nK = np.matmul(x,w_key)\n## Value: x * w_value\nV = np.matmul(x,w_value)\n\n# Step 4: Scaled Attention Scores\n## Square root of the dimensions\nk_d = 1\nattention_scores = (Q @ K.transpose())/k_d\n\n# Step 5: Scaled softmax attention scores for each vector\nattention_scores[0] = softmax(attention_scores[0])\nattention_scores[1] = softmax(attention_scores[1])\nattention_scores[2] = softmax(attention_scores[2])\n\n# Step 6: attention value obtained by score1/k_d * V\nattention1 = attention_scores[0].reshape(-1,1)\nattention1 = attention_scores[0][0]*V[0]\nattention2 = attention_scores[0][1]*V[1]\nattention3 = attention_scores[0][2]*V[2]\n\n# Step 7: summed the results to create the first line of the output matrix\nattention_input1 = attention1 + attention2 + attention3\n\n# Step 8: Step 1 to 7 for inputs 1 to 3\n## Because this is just a demo, we’ll do a random matrix of the right dimensions\nattention_head1 = np.random.random((3,64))\n\n# Step 9: We train all 8 heads of the attention sub-layer using steps 1 through 7\n## Again, it’s a demo\nz0h1 = np.random.random((3,64))\nz1h2 = np.random.random((3,64))\nz2h3 = np.random.random((3,64))\nz3h4 = np.random.random((3,64))\nz4h5 = np.random.random((3,64))\nz5h6 = np.random.random((3,64))\nz6h7 = np.random.random((3,64))\nz7h8 = np.random.random((3,64))\n\n# Step 10: Concatenate heads 1 through 8 to get the original 8x64 output dimension of the model\nOutput_attention = np.hstack((z0h1,z1h2,z2h3,z3h4,z4h5,z5h6,z6h7,z7h8))\n\n# Here’s a function that performs all of these steps:\ndef dot_product_attention(query, key, value, mask, scale=True):\n    assert query.shape[-1] == key.shape[-1] == value.shape[-1], “q,k,v have different dimensions!”\n    if scale:\n        depth = query.shape[-1]\n    else:\n        depth = 1\n    dots = np.matmul(query, np.swapaxes(key, -1, -2)) / np.sqrt(depth)\n    if mask is not None:\n        dots = np.where(mask, dots, np.full_like(dots, -1e9))\n    logsumexp = scipy.special.logsumexp(dots, axis=-1, keepdims=True)\n    dots = np.exp(dots - logsumexp)\n    attention = np.matmul(dots, value)\n    return attention\n\n# Here’s a function that performs the previous steps but adds causality in masking\ndef masked_dot_product_self_attention(q,k,v,scale=True):\n    mask_size = q.shape[-2]\n    mask = np.tril(np.ones((1, mask_size, mask_size), dtype=np.bool_), k=0)\n    return DotProductAttention(q,k,v,mask,scale=scale)\n```", "```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nMODEL_NAME = \"bigscience/bloom\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n\nprompt = \"Hello world! This is my first time running an LLM!\"\n\ninput_tokens = tokenizer.encode(prompt, return_tensors=\"pt\", padding=True)\ngenerated_tokens = model.generate(input_tokens, max_new_tokens=20)\ngenerated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\nprint(generated_text)\n```"]