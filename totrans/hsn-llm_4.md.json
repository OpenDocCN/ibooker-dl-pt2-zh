["```py\nfrom urllib.request import urlopen\nfrom PIL import Image\n\n# Load an AI-generated image of a puppy playing in the snow\nimage = Image.open(urlopen(\"https://i.imgur.com/iQ5OtWi.png\"))\ncaption = \"a puppy playing in the snow\"\n```", "```py\nfrom transformers import CLIPTokenizerFast, CLIPProcessor, CLIPModel\n\nmodel_id = \"openai/clip-vit-base-patch32\"\n\n# Load a tokenizer to preprocess the text\ntokenizer = CLIPTokenizerFast.from_pretrained(model_id)\n\n# Load a processor to preprocess the images\nprocessor = CLIPProcessor.from_pretrained(model_id)\n\n# Main model for generating text and image embeddings\nmodel = CLIPModel.from_pretrained(model_id)\n```", "```py\n>>> # Tokenize our input\n>>> inputs = tokenizer(caption, return_tensors=\"pt\"); inputs\n\n{'input_ids': tensor([[49406,   320,  6829,  1629,   530,   518,  2583, 49407]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n```", "```py\n>>> tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n\n['<|startoftext|>',\n 'a</w>',\n 'puppy</w>',\n 'playing</w>',\n 'in</w>',\n 'the</w>',\n 'snow</w>',\n '<|endoftext|>']\n```", "```py\n>>> # Create a text embedding\n>>> text_embedding = model.get_text_features(**inputs)\n>>> text_embedding.shape\n\ntorch.Size([1, 512])\n```", "```py\n>>> # Preprocess image\n>>> processed_image = processor(text=None, images=image, return_tensors='pt')['pixel_values']\n>>> processed_image.shape\n\ntorch.Size([1, 3, 224, 224])\n```", "```py\nimport numpy as np\n\n# Prepare image for visualization\nimg = processed_image.squeeze(0).T\nimg = np.einsum('ijk->jik', img)\n\n# Visualize preprocessed image\nplt.imshow(a)\nplt.axis('off')\n```", "```py\n>>> # Create the image embedding\n>>> image_embedding = model.get_image_features(processed_image)\n>>> image_embedding.shape\n\ntorch.Size([1, 512])\n```", "```py\n>>> # Calculate the probability of the text belonging to the image\n>>> text_probs = (100.0 * image_embedding @ text_embedding.T).softmax(dim=-1)\n>>> text_probs\n\ntensor([[1.]], grad_fn=<SoftmaxBackward0>)\n```", "```py\n>>> # Normalize the embeddings\n>>> text_embedding /= text_embedding.norm(dim=-1, keepdim=True)\n>>> image_embedding /= image_embedding.norm(dim=-1, keepdim=True)\n>>> \n>>> # Calculate their similarity\n>>> text_embedding = text_embedding.detach().cpu().numpy()\n>>> image_embedding = image_embedding.detach().cpu().numpy()\n>>> score = np.dot(text_embedding, image_embedding.T)\n>>> score\n\narray([[0.33149636]], dtype=float32)\n```", "```py\nfrom sentence_transformers import SentenceTransformer, util\n\n# Load SBERT-compatible CLIP model\nmodel = SentenceTransformer('clip-ViT-B-32')\n\n# Encode the images\nimage_embeddings = model.encode(images)\n\n# Encode the captions\ntext_embeddings = model.encode(captions)\n\n#Compute cosine similarities \nsim_matrix = util.cos_sim(image_embeddings, text_embeddings)\nprint(sim_matrix)\n```", "```py\nfrom transformers import AutoProcessor, Blip2ForConditionalGeneration\nimport torch\n\n# Load processor and main model\nprocessor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\nmodel = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16)\n\n# Send the model to GPU to speed up inference\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\n```", "```py\nfrom urllib.request import urlopen\nfrom PIL import Image\n\n# Load a wide image\nlink = \"https://images.unsplash.com/photo-1524602010760-6eb67b3c63a0?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=2631&q=80\"\nimage = Image.open(urlopen(link)).convert(\"RGB\")\nimage\n```", "```py\n>>> np.array(image).shape\n\n(520, 492, 3)\n```", "```py\n>>> inputs = processor(image, return_tensors=\"pt\").to(device, torch.float16)\n>>> inputs[\"pixel_values\"].shape\n\ntorch.Size([1, 3, 224, 224])\n```", "```py\n>>> processor.tokenizer\n\nGPT2TokenizerFast(name_or_path='Salesforce/blip2-opt-2.7b', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True)}, clean_up_tokenization_spaces=True)\n```", "```py\n# Preprocess the text\ntext = \"Her vocalization was remarkably melodic\"\ntoken_ids = processor(image, text=text, return_tensors=\"pt\").to(device, torch.float16)[\"input_ids\"][0]\n\n# Convert input ids back to tokens\ntokens = processor.tokenizer.convert_ids_to_tokens(token_ids)\n```", "```py\n>>> tokens = [token.replace(\"Ä \", \"_\") for token in tokens]\n>>> tokens\n\n['</s>', 'Her', '_vocal', 'ization', '_was', '_remarkably', '_mel', 'odic']\n```", "```py\nfrom urllib.request import urlopen\nfrom PIL import Image\n\n# Load an AI-generated image of a supercar\nimage = Image.open(urlopen(\"https://i.imgur.com/zehSvAe.png\")).convert(\"RGB\")\n\n# Convert an image into inputs and preprocess it\ninputs = processor(image, return_tensors=\"pt\").to(device, torch.float16)\nimage\n```", "```py\n# Generate token ids using the full BLIP-2 model\ngenerated_ids = model.generate(**inputs, max_new_tokens=20)\n\n# Convert the token ids to text\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n```", "```py\n>>> print(generated_text)\n\nan orange supercar driving on the road at sunset\n```", "```py\n# Load rorschach image\nurl = \"https://upload.wikimedia.org/wikipedia/commons/7/70/Rorschach_blot_01.jpg\"\nimage = Image.open(urlopen(url)).convert(\"RGB\")\n\n# Generate caption\ninputs = processor(image, return_tensors=\"pt\").to(device, torch.float16)\ngenerated_ids = model.generate(**inputs, max_new_tokens=20)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n```", "```py\n>>> print(generated_text)\n\n\"a black and white ink drawing of a bat\"\n```", "```py\n# Load an AI-generated image of a supercar and process it\nimage = Image.open(urlopen(\"https://i.imgur.com/zehSvAe.png\")).convert(\"RGB\")\ninputs = processor(image, return_tensors=\"pt\").to(device, torch.float16)\n```", "```py\n# Visual Question Answering\nprompt = \"Question: Write down what you see in this picture. Answer:\"\n\n# Process both the image and the prompt\ninputs = processor(image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n\n# Generate text\ngenerated_ids = model.generate(**inputs, max_new_tokens=30)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n```", "```py\n>>> print(generated_text)\n\nA sports car driving on the road at sunset\n```", "```py\n>>> # Chat-like prompting\n>>> prompt = \"Question: Write down what you see in this picture. Answer: A sports car driving on the road >>> at sunset. Question: What would it cost me to drive that car? Answer:\"\n>>> \n>>> # Generate output\n>>> inputs = processor(image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n>>> generated_ids = model.generate(**inputs, max_new_tokens=30)\n>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n>>> print(generated_text)\n\n$1,000,000\n```", "```py\nfrom IPython.display import HTML, display\nimport ipywidgets as widgets\n\ndef text_eventhandler(*args):\n  question = args[0][\"new\"]\n  if question:\n    args[0][\"owner\"].value = \"\"\n\n    # Create prompt\n    if not memory:\n      prompt = \" Question: \" + question + \" Answer:\"\n    else:\n      template = \"Question: {} Answer: {}.\"\n      prompt = \" \".join([template.format(memory[i][0], memory[i][1]) for i in range(len(memory))]) + \" Question: \" + question + \" Answer:\"\n\n    # Generate text\n    inputs = processor(image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n    generated_ids = model.generate(**inputs, max_new_tokens=100)\n    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip().split(\"Question\")[0]\n\n    # Update memory\n    memory.append((question, generated_text))\n\n    # Assign to output\n    output.append_display_data(HTML(\"<b>USER:</b> \" + question))\n    output.append_display_data(HTML(\"<b>BLIP-2:</b> \" + generated_text))\n    output.append_display_data(HTML(\"<br>\"))\n\n# Prepare widgets\nin_text = widgets.Text()\nin_text.continuous_update = False\nin_text.observe(text_eventhandler, \"value\")\noutput = widgets.Output()\nmemory = []\n\n# Display chat box\ndisplay(\n    widgets.VBox(\n        children=[output, in_text],\n        layout=widgets.Layout(display=\"inline-flex\", flex_flow=\"column-reverse\"),\n    )\n)\n```"]