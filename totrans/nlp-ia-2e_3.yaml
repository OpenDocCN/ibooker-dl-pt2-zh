- en: 3 Math with words (TF-IDF vectors)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用单词做数学运算（TF-IDF向量）
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章包括
- en: Counting words, *n*-grams and *term frequencies* to analyze meaning
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算单词、*n*-grams和*词频*以分析含义
- en: Predicting word occurrence probabilities with *Zipf’s Law*
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用*Zipf定律*预测单词出现概率
- en: Representing natural language texts as vectors
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将自然语言文本表示为向量
- en: Finding relevant documents in a collection of text using *document frequencies*
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用*文档频率*在文本集合中找到相关文档
- en: Estimating the similarity of pairs of documents with *cosine similarity*
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用*余弦相似度*估计文档对的相似性
- en: Having collected and counted words (tokens), and bucketed them into stems or
    lemmas, it’s time to do something interesting with them. Detecting words is useful
    for simple tasks, like getting statistics about word usage or doing keyword search.
    But you would like to know which words are more important to a particular document
    and across the corpus as a whole. So you can use that "importance" value to find
    relevant documents in a corpus based on keyword importance within each document.
    That will make a spam detector a little less likely to get tripped up by a single
    curse word or a few slightly spammy words within an email. You would like to measure
    how positive and prosocial a Mastodon message is, especially when you have a variety
    of words with different degrees of "positivity" scores or labels. If you have
    an idea about the frequency with which those words appear in a document *in relation
    to* the rest of the documents, you can use that to further refine the "positivity"
    of the document. In this chapter, you’ll learn about a more nuanced, less binary
    measure of words and their usage within a document. This approach has been the
    mainstay for generating features from natural language for commercial search engines
    and spam filters for decades.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 收集并计数了单词（标记），并将它们分桶为词干或词元后，就可以对它们做一些有趣的事情了。检测单词对于简单任务非常有用，比如获取关于单词使用情况的统计信息或进行关键词搜索。但是你想知道哪些单词对特定文档和整个语料库更重要。因此，你可以使用该“重要性”值来根据每个文档内关键字的重要性来查找语料库中的相关文档。这样一来，垃圾邮件检测器就不太可能因为单个脏话或少量带有些许垃圾邮件特征的词而被触发。你想要测量一条Mastodon消息有多积极和亲社会，尤其是当你有各种单词，这些单词具有不同程度的“积极性”分数或标签时。如果你对这些单词相对于其他文档出现的频率有一个想法，那么你可以用它来进一步确定文档的“积极性”。在本章中，你将了解有关单词及其在文档中使用的更为细致、不那么二元的度量方法。这种方法是商业搜索引擎和垃圾邮件过滤器从自然语言生成特征的主要方法已经有几十年了。
- en: The next step in your adventure is to turn the words of Chapter 2 into continuous
    numbers rather than just integers representing word counts or binary "bit vectors"
    that detect the presence or absence of particular words. With representations
    of words in a continuous space, you can operate on their representation with more
    exciting math. Your goal is to find numerical representations of words that somehow
    capture the importance or information content of the words they represent. You’ll
    have to wait until chapter 4 to see how to turn this information content into
    numbers that represent the **meaning** of words.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你冒险的下一步是将第二章的文字转化为连续数字，而不仅仅是表示单词计数或二进制“位向量”的整数。通过在连续空间中表示单词，你可以用更加有趣的数学来操作它们的表达。你的目标是找到单词的数值表示，以某种方式捕捉它们所代表的重要性或信息内容。你得等到第四章才能看到如何将这些信息内容转化为代表单词**含义**的数字。
- en: 'In this chapter, we look at three increasingly powerful ways to represent words
    and their importance in a document:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将研究三种越来越强大的方式来表示单词及其在文档中的重要性：
- en: '*Bags of words* — Vectors of word counts or frequencies'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*词袋* — 单词计数或频率的向量'
- en: '*Bags of n-grams* — Counts of word pairs (bigrams), triplets (trigrams), and
    so on'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*n-gram包* — 单词对（bigrams）、三元组（trigrams）等的计数'
- en: '*TF-IDF vectors* — Word scores that better represent their importance'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*TF-IDF向量* — 更好地表示单词的重要性的单词分数'
- en: Important
  id: totrans-13
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 重要
- en: TF-IDF stands for ***t**erm **f**requency times **i**nverse **d**ocument **f**requency*.
    Term frequencies are the counts of each word in a document, which you learned
    about in previous chapters. Inverse document frequency means that you’ll divide
    each of those word counts by the number of documents in which the word occurs.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF代表***词频乘以逆文档频率*。词频是文档中每个单词的计数，这是你在前几章学到的。逆文档频率意味着你将每个单词的计数除以该单词出现的文档数。
- en: Each of these techniques can be applied separately or as part of an NLP pipeline.
    These are all statistical models in that they are *frequency* based. Later in
    the book, you’ll see various ways to peer even deeper into word relationships
    and their patterns and non-linearities.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术可以分别应用，也可以作为 NLP 流水线的一部分应用。这些都是统计模型，因为它们基于*频率*。在本书的后面，您将看到各种方法，更深入地了解单词之间的关系、模式和非线性。
- en: But these "shallow" NLP machines are powerful and useful for many practical
    applications such as search, spam filtering, sentiment analysis, and even chatbots.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 但是这些“浅层”自然语言处理（NLP）机器是强大且实用的，用于许多实际应用，如搜索、垃圾邮件过滤、情感分析，甚至聊天机器人。
- en: 3.1 Bag of words
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 词袋模型
- en: In the previous chapter, you created your first vector space model of a text.
    You used one-hot encoding of each word and then combined all those vectors with
    a binary OR (or clipped `sum`) to create a vector representation of a text. And
    this binary bag-of-words vector makes a great index for document retrieval when
    loaded into a data structure such as a Pandas DataFrame.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，您创建了文本的第一个向量空间模型。您对每个单词进行了独热编码，然后将所有这些向量与二进制 OR（或裁剪的 `sum`）结合起来，以创建文本的向量表示。这个二进制词袋向量在加载到诸如
    Pandas DataFrame 等数据结构时，可以用作文档检索的优秀索引。
- en: You then looked at an even more useful vector representation that counts the
    number of occurrences, or frequency, of each word in the given text. As a first
    approximation, you assume that the more times a word occurs, the more meaning
    it must contribute to that document. A document that refers to "wings" and "rudder"
    frequently may be more relevant to a problem involving jet airplanes or air travel,
    than say a document that refers frequently to "cats" and "gravity". Or if you
    have classified some words as expressing positive emotions — words like "good",
    "best", "joy", and "fantastic" — the more a document that contains those words,
    the more likely it is to have positive "sentiment". You can imagine though how
    an algorithm that relied on these simple rules might be mistaken or led astray.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 接着你看了一个更有用的向量表示，它计算了给定文本中每个单词出现的次数或频率。作为第一个近似，你假设一个单词出现的次数越多，它在文档中的贡献就越大。一个频繁提到“机翼”和“方向舵”的文档，可能与涉及喷气飞机或航空旅行的问题更相关，而不是频繁提到“猫”和“重力”的文档。或者，如果你已经将一些词分类为表达积极情感的词——像“好”、“最好”、“喜悦”和“棒极了”——那么包含这些词的文档越多，它就越有可能具有积极的“情感”。不过，你可以想象到一个依赖这些简单规则的算法可能会犯错或走上错误的道路。
- en: 'Let’s look at an example where counting occurrences of words is useful:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个统计单词出现次数有用的例子：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You only need to download a SpaCy language model once, and it can take quite
    a lot of Internet bandwidth. So only run `cli.download()` if this is the first
    time you’ve run this code in your Python virtual environment. The SpaCy language
    model tokenizes natural language text and returns a document object (`Doc` class)
    containing a sequence of all the tokens in the input text. It also segments the
    document to give you a sequence of sentences in the `.sents` attribute. With the
    Python `set()` type you could convert this sequence of tokens into the set of
    all the unique words in the text.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 您只需要下载 SpaCy 语言模型一次，它可能会消耗大量的互联网带宽。所以只有在您的 Python 虚拟环境中首次运行此代码时才运行 `cli.download()`。SpaCy
    语言模型将自然语言文本进行标记化，并返回一个包含输入文本中所有标记序列的文档对象（`Doc` 类）。它还会将文档分段，以便您在 `.sents` 属性中获得一个句子序列。借助
    Python 的 `set()` 类型，您可以将这个标记序列转换为文本中所有唯一单词的集合。
- en: The list of all the unique words in a document or corpus is called its *vocabulary*
    or *lexicon*. Creating your vocabulary is the most important step in your NLP
    Pipeline. If you don’t identify a particular token and give it a *place* to be
    stored, your pipeline will completely ignore it. In most NLP pipelines you will
    define a single token named `<OOV>` (Out of Vocabulary) where you will store information
    about all the tokens that your pipeline is ignoring, such as a count of their
    occurrences. So all the unusual or made-up "supercalifragilistic" words that you
    do not include in your vocabulary will be lumped together into a single generic
    token and your NLP pipeline will have no way of computing what it means.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 文档或语料库中所有唯一单词的列表称为其*词汇*或*词典*。创建您的词汇是您的 NLP 流水线中最重要的步骤。如果您不识别特定的标记并为其分配一个*位置*来存储它，您的流水线将完全忽略它。在大多数
    NLP 流水线中，您将定义一个名为`<OOV>`（超出词汇）的单个标记，您将在其中存储有关您的流水线正在忽略的所有标记的信息，例如它们的出现次数。因此，您没有包含在词汇中的所有不寻常或虚构的“超级长的”单词将被合并到一个单一的通用标记中，而您的
    NLP 流水线将无法计算其含义。
- en: The Python `Counter` class is an efficient way to count up occurrences of anything,
    including tokens, in a sequence or array. In Chapter 2, you learned that a `Counter`
    is a special kind of dictionary where the keys are all the unique objects in your
    array, and the dictionary values are the counts of each of those objects.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Python的`Counter`类是一种高效计算任何东西（包括标记）在序列或数组中出现次数的方法。在第 2 章中，您了解到`Counter`是一种特殊类型的字典，其中键是数组中的所有唯一对象，而字典的值是每个对象的计数。
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: A `collections.Counter` object is a `dict` under the hood. And that means that
    the keys are technically stored in an unordered collection or `set`, also sometimes
    called a "bag." It may look like this dictionary has maintained the order of the
    words in your sentence, but that’s just an illusion. You got lucky because your
    sentence didn’t contain many repeated tokens. And the latest versions of Python
    (3.6 and above) maintain the order of the keys based on when you insert new keys
    into a dictionary.^([[1](#_footnotedef_1 "View footnote.")]) But you are about
    to create vectors out of these dictionaries of tokens and their counts. You need
    vectors to do linear algebra and machine learning on a collection of documents
    (sentences in this case). Your bag-of-words vectors will keep track of each unique
    token with a consistent index number for a position within your vectors. This
    way the counts for tokens like "and" or the comma add up across all the vectors
    for your documents — the sentences in the Wikipedia article titled "Algorithmic
    Bias."
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '`collections.Counter` 对象实际上是一个 `dict`。这意味着键技术上存储在无序集合或`set`中，有时也称为“bag”。它可能看起来像这个字典已经保持了您的句子中单词的顺序，但这只是一种错觉。您有幸因为您的句子中没有包含许多重复的标记。而
    Python 的最新版本（3.6 及以上）基于您在字典中插入新键的时间来维护键的顺序。但是您即将从这些标记和它们的计数的字典中创建向量。您需要向量来对一系列文档（在这种情况下是句子）进行线性代数和机器学习。您的词袋向量将使用一致的索引号来跟踪每个唯一标记在向量中的位置。这样，诸如“and”或逗号之类的标记的计数将在您的文档的所有向量中累加——维基百科文章标题为“算法偏见”的句子中。'
- en: Important
  id: totrans-27
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 重要提示
- en: For NLP the order of keys in your dictionary won’t matter, because you will
    maintain a consistent ordering in vectors such as a Pandas Series. Just as in
    Chapter 2, the Counter dictionary orders your vocabulary (`dict` keys) according
    to when you processed each of the documents of your corpus. And sometimes you
    may want to alphabetize your vocabulary to make it easier to analyze. Once you
    assign each token to a dimension in your vectors of counts, be sure to record
    that order for the future, so you can reuse your pipeline without retraining it
    by reprocessing all your documents. And if you are trying to reproduce someone
    else’s NLP pipeline you’ll want to reuse their exact vocabulary (token list),
    in the exact same order. Otherwise, you will need to process their training dataset
    in the same order, using the exact same software, that they did.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于NLP，字典中键的顺序并不重要，因为您将在向量中保持一致的排序，例如Pandas Series。正如在第 2 章中一样，Counter 字典根据您处理语料库的每个文档的时间顺序对您的词汇（`dict`键）进行排序。有时您可能希望对您的词汇进行按字母顺序排列以便于分析。一旦您为计数向量的每个标记分配了一个维度，务必记录下该顺序以备将来使用，这样您就可以在不重新处理所有文档的情况下重新使用您的流水线而无需重新训练它。如果您试图复制他人的
    NLP 流水线，您将想要重用其确切的词汇表（标记列表），并按照完全相同的顺序重复使用。否则，您将需要按照与他们相同的顺序，使用完全相同的软件来处理他们的训练数据集。
- en: For short documents like this one, the jumbled bag of words still contains a
    lot of information about the original intent of the sentence. The information
    in a bag of words is sufficient to do some powerful things such as detect spam,
    compute sentiment (positivity or other emotions), and even detect subtle intent,
    such as sarcasm. It may be a bag, but it’s full of meaning and information. To
    make these words easier to think about and ensure your pipeline is consistent,
    you want to sort them in some consistent order. To rank the tokens by their count,
    the Counter object has a handy method, `most_common`.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像这样的短文档，打乱的词袋仍然包含了关于句子原始意图的大量信息。词袋中的信息足以执行一些强大的任务，比如检测垃圾邮件、计算情感（积极性或其他情绪），甚至检测微妙的意图，比如讽刺。它可能是一个袋子，但它充满了意义和信息。为了更容易地思考这些词，并确保你的管道是一致的，你想要以某种一致的顺序对它们进行排序。要按计数对标记进行排名，Counter对象有一个方便的方法，即`most_common`。
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: That’s handy! The `Counter.most_common` method will give your a ranked list
    of however many tokens you want, paired with their counts in 2-tuples. But this
    isn’t quite what you want. You need a vector representation to be able to easily
    do math on your token counts.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这很方便！`Counter.most_common`方法将给出一个排名列表，其中包含你想要的任意数量的标记，并与其计数配对为2元组。但这还不是你想要的。你需要一个向量表示来轻松地对你的标记计数进行数学运算。
- en: A Pandas `Series` is an efficient data structure for storing the token counts,
    including the 2-tuples from the `most_common` method. The nice thing about a Pandas
    `Series` is that it behaves like a vector (numpy array) whenever you use a math
    operator such as plus (`+`) or (`\*`) or even `.dot()`. And you can still access
    each named (labeled) dimension associated with each token using the normal square
    bracket (`['token']`) syntax.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas `Series`是一种高效的数据结构，用于存储标记计数，包括来自`most_common`方法的2元组。Pandas `Series`的好处是，每当你使用像加号（`+`）或（`*`）甚至`.dot()`这样的数学运算符时，它都表现得像一个向量（numpy数组）。而且你仍然可以使用正常的方括号（`['token']`）语法访问与每个标记关联的每个命名（标记）维度。
- en: Tip
  id: totrans-33
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: The Python expression `x[y]` is the same as `x.*getitem*(y)`. Square brackets
    (`[` and `]`) are syntactic sugar (shorthand) for the hidden `.*getitem*()` methods
    on dictionaries, lists, Pandas Series and many other container objects. If you
    ever want to use this operator on a container class of your own, you only need
    to define a `.*getitem*(index_value)` method that retrieves the appropriate element
    from your container.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Python表达式`x[y]`与`x.*getitem*(y)`是相同的。方括号（`[`和`]`）是字典、列表、Pandas Series和许多其他容器对象上隐藏的`.*getitem*()`方法的语法糖（简写）。如果你想在自己的容器类上使用这个运算符，你只需要定义一个`.*getitem*(index_value)`方法，从你的容器中检索适当的元素即可。
- en: You can coerce any list of 2-tuples into a dictionary using the built-in `dict`
    type constructor. And you can coerce any dictionary into a Pandas `Series` using
    the `Series` constructor.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用内置的`dict`类型构造函数将任意的2元组列表转换为字典。而且你可以使用`Series`构造函数将任意字典转换为Pandas `Series`。
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The Pandas Series displays nicely when you print it to the screen, which can
    be handy while you are trying to understand what a token count vector contains.
    Now that you’ve created a count vector you can do main on it like any other Pandas
    Series.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将Pandas Series打印到屏幕上时，它会显示得很好，这在你试图理解一个标记计数向量包含的内容时可能会很方便。现在你已经创建了一个计数向量，你可以像对待任何其他Pandas
    Series一样对待它。
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You can see that there are 23 tokens in the sentence, but only 18 unique tokens
    in your vocabulary for that sentence. So each of your document vectors will need
    to have at least 18 values, even if the other documents do not use those same
    18 words. This allows each token to have its own dimension (slot) in your count
    vectors. Each token is assigned a "slot" in your vectors corresponding to its
    position in your lexicon. Some of those token counts in the vector will be zeros,
    which is what you want.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到这个句子中有23个标记，但是你的词汇表中只有18个唯一的标记。因此，即使其他文档没有使用这些相同的18个词，你的每个文档向量也需要至少有18个值。这样可以让每个标记在你的计数向量中拥有自己的维度（槽）。每个标记在你的向量中被分配一个与其在词汇表中位置相对应的“槽”。向量中的某些标记计数将为零，这正是你想要的。
- en: It makes sense that the comma (",") token and the word "and" are at the top
    of your list of `most_common` tokens. Commas were used five times, the word "and"
    was used twice, and all the other words were only used once each in this sentence.
    Your top two terms or tokens in this particular sentence are ",", and "and." This
    is a pretty common problem with natural language text — the most common words
    are often the least meaningful. Stopwords such as these don’t tell you much about
    the meaning of this document, so you might be tempted to ignore them completely.
    A better approach is to scale your token counts using the statistics of words
    in *your* documents rather than someone else’s arbitrary list of stop words from
    their documents.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 逗号（","）标记和单词"and"位于你的`most_common`词项列表的顶部是有道理的。逗号使用了五次，单词"and"使用了两次，而在这个句子中所有其他单词只使用了一次。在这个特定句子中，你的前两个词项或标记是","和"and"。这是自然语言文本的一个相当普遍的问题——最常见的词往往是最无意义的。这些停用词并不能告诉你关于这个文档意义的很多，所以你可能会完全忽略它们。一个更好的方法是使用*你的*文档中词的统计数据来衡量你的词项计数，而不是别人从他们的文档中列出的停用词的任意列表。
- en: The number of times a word occurs in a given document is called the *term frequency*,
    commonly abbreviated "TF." One of the first things you will probably want to do
    is to normalize (divide) your token counts by the number of terms in the document.
    This will give you the relative frequency (percentage or fraction) of the document
    that contains a token, regardless of the length of the document. Check out the
    relative frequency of the word "justice" to see if this approach does justice
    to the importance of this word in this text.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 一个词在给定文档中出现的次数被称为*词频*，通常缩写为"TF"。你可能想要做的第一件事情之一就是通过文档中的词项数进行归一化（除以）。这将为你提供文档中包含词项的相对频率（百分比或分数），而不考虑文档的长度。查看一下单词"justice"的相对频率，看看这种方法是否能恰当地体现这个词在这段文本中的重要性。
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The *normalized term frequency* of the term "justice" in the opening sentence
    of this Wikipedia article is about 4%. And you probably won’t expect this percentage
    to go up as you process more sentences in the article. If the sentence and the
    article were both talking about "justice" about the same amount, this *normalized
    term frequency* score would remain roughly the same throughout the document.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇维基百科文章开头句子中，单词"justice"的*标准化词频*约为4%。而且你可能不会期望随着你处理文章中更多的句子，这个百分比会上升。如果句子和文章都在谈论"justice"大致相同的数量，那么这个*标准化词频*分数在整个文档中将保持大致不变。
- en: According to this term frequency, the word "justice" represents about 4% of
    the meaning of this sentence. That’s not a lot, considering how vital this word
    is to the meaning of the sentence. So you need to do one more normalization step
    to give this word a boost relative to the other words in the sentence.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这个词频，单词"justice"在这个句子中代表了约4%的意义。考虑到这个单词对句子的意义有多重要，这并不多。所以你需要再做一步归一化，以使这个词相对于句子中的其他词得到提升。
- en: To give the word "justice" a score for significance or importance you will need
    some statistics about it from more than just this one sentence. You need to find
    out how much "justice" is used elsewhere. Fortunately for budding NLP engineers,
    Wikipedia is full of high quality accurate natural language text in many languages.
    You can use this text to "teach" your machine about the importance of the word
    "justice" across many documents. To demonstrate the power of this approach, all
    you need is a few paragraphs from the Wikipedia article on algorithmic bias. Here
    are some more sentences from the Wikipedia article "Algorithmic Bias."
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 要给单词"justice"一个重要性或重要性评分，你需要一些关于它的统计数据，不仅仅是这一个句子。你需要找出"justice"在其他地方的使用情况。幸运的是对于初学的NLP工程师来说，维基百科充满了许多语言的高质量准确的自然语言文本。你可以使用这些文本来"教"你的机器关于单词"justice"在许多文档中的重要性。为了展示这种方法的威力，你只需要从维基百科上的算法偏见文章中选取几段。这里有一些来自维基百科文章"Algorithmic
    Bias"的句子。
- en: Algorithmic bias describes systematic and repeatable errors in a computer system
    that create unfair outcomes, such as privileging one arbitrary group of users
    over others. Bias can emerge due to many factors, including but not limited to
    the design of the algorithm or the unintended or unanticipated use or decisions
    relating to the way data is coded, collected, selected or used to train the algorithm.
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 算法偏见描述了计算机系统中的系统性和可重复的错误，这些错误会导致不公平的结果，例如偏袒某个任意的用户群体而不是其他人。偏见可能由许多因素引起，包括但不限于算法的设计或数据编码、收集、选择或用于训练算法的方式的意外或未预料到的使用或决策。
- en: ''
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: …​
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: …​
- en: ''
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Algorithmic bias has been cited in cases ranging from election outcomes to the
    spread of online hate speech. It has also arisen in criminal justice, healthcare,
    and hiring, compounding existing racial, economic, and gender biases.
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 算法偏见已在各种情况下被引用，从选举结果到网络仇恨言论的传播。它还出现在刑事司法、医疗保健和招聘中，加剧了现有的种族、经济和性别偏见。
- en: ''
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: …​
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: …​
- en: ''
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Problems in understanding, researching, and discovering algorithmic bias persist
    due to the proprietary nature of algorithms, which are typically treated as trade
    secrets.
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 由于算法的专有性质，即通常被视为商业机密，导致了对理解、研究和发现算法偏见的问题仍然存在。
- en: — Wikipedia
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: — 维基百科
- en: Algorithmic Bias ([https://en.wikipedia.org/wiki/Algorithmic_bias](wiki.html))
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 算法偏见 ([https://en.wikipedia.org/wiki/Algorithmic_bias](wiki.html))
- en: Look at these sentences to see if you can find keywords that are key to your
    understanding of the text. Your algorithm will need to make sure it includes these
    words and computes statistics about them. If you were trying to detect these important
    words with Python automatically (programatically) how would you compute an importance
    score? See if you can figure out how you could use the `Counter` dictionary to
    help your algorithm understand something about algorithmic bias.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 看看这些句子，看看是否能找到对您理解文本至关重要的关键字。您的算法需要确保包含这些词，并计算有关它们的统计数据。如果您尝试使用 Python 自动（程序化地）检测这些重要单词，您将如何计算重要性得分？看看您是否能想出如何使用
    `Counter` 字典来帮助您的算法理解算法偏见的某些方面。
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Looks like this sentence doesn’t reuse any words at all. The key to frequency
    analysis and term frequency vectors is the statistics of word usage *relative
    to other words*. So we need to input the other sentences and to create useful
    word counts that are normalized based on how words are used elsewhere. To grok
    (understand) "Algorithmic Bias" you could take the time to read and type all of
    the Wikipedia article into a Python string. Or you could download the text from
    Wikipedia using the `nlpia2_wikipedia` Python package and then process it with
    NLP to find the key concepts you need to review. To retrieve the latest "Algorithmic
    bias" text directly from Wikipedia, use `nlpia2.wikipedia` rather than the official
    (but abandoned) Wikipedia package.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来这句话根本没有重复使用任何词。频率分析和词频向量的关键在于单词使用的统计数据*相对于其他单词*。因此，我们需要输入其他句子，并创建基于单词如何在其他地方使用的归一化的有用单词计数。要理解“算法偏见”，您可以花时间阅读并将维基百科文章的所有内容输入到
    Python 字符串中。或者，您可以使用 `nlpia2_wikipedia` Python 包从维基百科下载文本，然后使用自然语言处理找到您需要复习的关键概念。要直接从维基百科检索最新的“算法偏见”文本，请使用
    `nlpia2.wikipedia` 而不是官方（但已废弃）的维基百科包。
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: You can also download a text file containing the first 3 paragraphs of the Wikipedia
    article from the `nlpia2` package on GitLab. If you have cloned the `nlpia2` package
    you will see the `src/nlpia2/ch03/bias_intro.txt` on your local hard drive. If
    you haven’t installed `nlpia2` from the source code you can use the code snippet
    here to retrieve the file using the `requests` package. The `requests` package
    is an handy tool for scraping and downloading natural language text from the web.
    All of the text that made ChatGPT, Bard, and YouChat seem so smart was scraped
    from web pages using tools similar to `requests`.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以从 GitLab 上的 `nlpia2` 包中下载包含维基百科文章的前 3 段的文本文件。如果您已经克隆了 `nlpia2` 包，您将在本地硬盘上看到
    `src/nlpia2/ch03/bias_intro.txt`。如果您尚未从源代码安装 `nlpia2`，您可以使用此处的代码片段使用 `requests`
    包检索文件。`requests` 包是一个方便的工具，用于从网络上抓取和下载自然语言文本。ChatGPT、Bard 和 YouChat 显得如此聪明的所有文本都是使用类似
    `requests` 的工具从网页上抓取的。
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `requests` package returns an HTTP response object containing the headers
    (in `.headers`) and body (`.text`) of an HTTP response. The `bias_intro.txt` file
    from the `nlpia2` package data is a 2023 snapshot of the first three paragraphs
    of the Wikipedia article.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`requests` 包返回一个包含 HTTP 响应的对象，其中包含报头（在`.headers`中）和正文（`.text`）的内容。`nlpia2`
    包数据中的 `bias_intro.txt` 文件是维基百科文章的前三段的2023快照。'
- en: '[PRE9]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: For a plain text document you can use the `response.content` attribute which
    contains the `bytes` of the raw HTML page. If you want to retrieve a string you
    can use the `response.text` property to automatically decode the text bytes to
    create a unicode `str`.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 对于纯文本文档，您可以使用`response.content`属性，该属性包含原始 HTML 页面的`bytes`。如果要获取一个字符串，可以使用`response.text`属性将文本字节自动解码为
    unicode `str`。
- en: 'The `Counter` class from the Python standard library in the `collections` module
    is great for efficiently counting any sequence of objects. That’s perfect for
    NLP when you want to count up occurrences of unique words and punctuation in a
    list of tokens:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 来自`collections`模块的 Python 标准库中的`Counter`类非常适合高效计数任何对象的序列。这对于 NLP 来说非常完美，当您希望计算一组令牌中唯一单词和标点的出现次数时：
- en: '[PRE10]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Okay, those counts are a bit more statistically significant. But there are still
    many meaningless words and punctuation marks that seem to have high counts. It’s
    not likely that this Wikipedia article is really about tokens such as "of", "to",
    commas, or periods. Perhaps paying attention to the least common tokens will be
    more useful than the most common ones.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，这些计数的统计意义更大一些。但是仍然有许多无意义的词汇和标点符号，它们的计数似乎很高。这篇维基百科文章可能并不真的是关于“of”、“to”、“commas”或“periods”等标记。也许关注最不常见的标记会比关注最常见的标记更有用。
- en: '[PRE11]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Well that didn’t work out so well. You were probably hoping to find terms such
    as "bias", "algorithmic", and "data." So you’re going to have to use a formula
    that balances the counts to come up with the "Goldilocks" score for the ones that
    are "just right." The way you can do that is to come up with another useful count — the
    number of documents that a word occurs in, called the "document frequency." This
    is when things get really interesting.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法不那么成功。您可能希望找到类似“偏见”、“算法”和“数据”之类的术语。因此，您需要使用一个平衡计数的公式，以得到“刚刚好”的术语的“Goldilocks”得分。你可以这样做，通过得到另一个有用的计数——一个单词出现在多少篇文章中的计数，称为“文档频率”。这就是事情变得真正有趣的时候。
- en: If you had a large corpus of many many documents you could normalize (divide)
    the counts within a document based on how often a token is used across all your
    documents. As you are just getting started with token count vectors it will be
    better to just create some small documents by splitting up a Wikipedia article
    summary, into smaller documents (sentences or paragraphs). This way you can at
    least see all the documents on one page and figure out where all the counts came
    from by running the code in your head. In the next section that is what you will
    do, split the "Algorithm Bias" article text into sentences and play around with
    different ways of normalizing and structuring the count dictionaries to make them
    more useful for NLP.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有一个大型语料库，您可以基于一个令牌在所有文档中使用的频率来归一化（除以）文档中的计数。由于您刚开始使用令牌计数向量，最好将维基百科文章摘要拆分成更小的文档（句子或段落），以创建一些小文档。这样，您至少可以在一页上看到所有文档，并通过脑海中运行代码来确定所有计数的来源。在接下来的部分中，您将拆分“Algorithm
    Bias”文章文本为句子，并尝试不同的归一化和结构化计数字典的方法，以使它们在自然语言处理中更有用。
- en: 3.1.1 Vectorizing text
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.1 文本向量化
- en: '`Counter` dictionaries are great for counting up tokens in text. But vectors
    are where it’s really at. And it turns out that dictionaries can be coerced into
    a `DataFrame` or `Series` just by calling the `DataFrame` constructor on a list
    of dictionaries. Pandas will take care of all the bookkeeping so that each unique
    token or dictionary key has its own column. Pandas will create NaNs whenever the
    `Counter` dictionary for a document is missing a particular key because the document
    doesn’t contain that word or symbol.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`Counter` 字典非常适合计数文本中的标记。但是向量才是真正需要的东西。原来字典可以通过在字典列表上调用 `DataFrame` 构造函数来强制转换为
    `DataFrame` 或 `Series`。Pandas 将负责所有簿记工作，以便每个唯一的令牌或字典键都有自己的列。当文档的 `Counter` 字典缺少特定的键时，Pandas
    会创建 NaN 值，因为该文档不包含该单词或符号。'
- en: Once you split the "Algorithmic Bias" article into lines you will see the power
    of vector representations. You will soon see why a Pandas Series is a much more
    useful data structure for working with Token counts compared to a standard Python
    `dict.`
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您将“算法偏差”文章拆分为行，您将看到向量表示的威力。您很快就会明白为什么Pandas Series与标准Python `dict.`相比，对于处理标记计数来说是一个更有用的数据结构。
- en: '[PRE12]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: When the dimensions of your vectors are used to hold scores for tokens or strings,
    that’s when you want to use a Pandas `DataFrame` or `Series` to store your vectors.
    That way you can see what each dimension is for. Check out that sentence that
    we started this chapter with. It happens to be the eleventh sentence in the Wikipedia
    article.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 当您的向量维度用于保存标记或字符串的分数时，这就是您想要使用Pandas `DataFrame`或`Series`来存储您的向量的时候。这样，您就可以看到每个维度的用途。检查一下我们在本章开头提到的那个句子。它碰巧是维基百科文章中的第11个句子。
- en: '[PRE13]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now this Pandas `Series` is a *vector*. That’s something you can do math on.
    And when you do that math, Pandas will keep track of which word is where so that
    "bias" and "justice" aren’t accidentally added together. Your row vectors in this
    DataFrame have a "dimension" for each word in your vocabulary. In fact, the `df.columns`
    attribute contains your vocabulary.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这个Pandas `Series`是一个*向量*。这是您可以进行数学计算的东西。当您进行数学计算时，Pandas将跟踪哪个单词在哪里，以便“偏见”和“正义”不会被意外加在一起。这个DataFrame中的行向量每个词汇中的一个“维度”。事实上，`df.columns`属性包含您的词汇表。
- en: But wait, there are more than 30,000 words in a standard English dictionary.
    If you start processing a lot of Wikipedia articles instead of just a few sentences,
    that’ll be a lot of dimensions to deal with. You are probably used to 2D and 3D
    vectors because they are easy to visualize. But do concepts like distance and
    length even work with 30,000 dimensions? It turns out they do, and you’ll learn
    how to improve on these high-dimensional vectors later in the book. For now just
    know that each element of a vector is used to represent the count, weight or importance
    of a word in the document you want the vector to represent.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 等等，标准英语词典中有超过30,000个单词。如果您开始处理大量维基百科文章而不仅仅是几句话，那么将有很多维度要处理。您可能习惯于2D和3D向量，因为它们易于可视化。但是30,000个维度的概念，例如距离和长度，甚至是否有效？事实证明它们确实有效，您将在本书后面学习如何改进这些高维向量。现在只需知道向量的每个元素用于表示您希望向量表示的文档中单词的计数，权重或重要性。
- en: You’ll find every unique word in each document and then find all the unique
    words in all of your documents. In math, this is the union of all the sets of
    words in each document. This master set of words for your documents is called
    the *vocabulary* of your pipeline. And if you decide to keep track of additional
    linguistic information about each word, such as spelling variations or parts of
    speech, you might call it a *lexicon*. And you might find academics that use the
    term *corpus* to describe a collection of documents will likely also use the word
    "lexicon," just because it is a more precise technical term than "vocabulary."
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 您将在每个文档中找到每个独特的单词，然后找到所有文档中的所有独特单词。在数学中，这就是每个文档中所有单词集合的并集。这些文档的主要单词集合称为您的管道的*词汇*。如果您决定跟踪有关每个单词的其他语言信息，例如拼写变体或词性，您可以称之为*词典*。您可能会发现使用术语*语料库*来描述一组文档的学者也可能会使用单词“词典”，只是因为它是比“词汇”更精确的技术术语。
- en: So take a look at the vocabulary or lexicon for this tiny corpus of three paragraphs.
    First, you will do *case folding* (lowercasing) to ignore the difference between
    capitalized words (such as proper nouns) and group them together into a single
    vocabulary token. This will reduce the number of unique words in your vocabulary
    in the later stages of your pipeline, which can make it easier to see what’s going
    on.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，看一看这三段话的词汇或词典。首先，您将进行*大小写转换*（小写化），以忽略大写字母（例如专有名词）之间的差异，并将它们组合到一个词汇标记中。这将减少您管道后续阶段中词汇表中独特单词的数量，这可以使您更容易看到发生的事情。
- en: '[PRE14]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Now that you have tokenized all 28 of these documents (sentences), you can concatenate
    all these token lists together to create one big list of all the tokens, including
    repetitions. The only difference between this list of tokens and the original
    document is that it has been segmented into sentences and tokenized into words.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经将这28篇文档（句子）全部标记化了，您可以将所有这些标记列表连接在一起，以创建一个包含所有标记的大列表，包括重复。此标记列表与原始文档唯一的区别在于，它已经被分割成句子并标记化为单词。
- en: '[PRE15]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Create a vocabulary (lexicon) from the sequence of tokens for the entire paragraph.
    Your lexicon or vocabulary is a list of all the unique tokens in your corpus.
    Just like a dictionary of words at the library, a lexicon doesn’t contain any
    duplicates. Which Python data types do you know of that remove duplicates (besides
    the `dict` type)?
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 从整个段落的标记序列中创建词汇表（词典）。你的词汇表是你语料库中所有唯一标记的列表。就像图书馆中的词典一样，词汇表不包含任何重复项。除了 `dict`
    类型，你还知道哪些 Python 数据类型可以去除重复项？
- en: '[PRE16]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Using the `set` data type ensures that no tokens are counted twice. After lowercasing
    (folding) all the tokens, there are only 248 uniquely spelled tokens in your short
    corpus of 498 words. This means that, on average, each token is used almost exactly
    twice (`498 / 248`).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `set` 数据类型确保没有标记被计数两次。在转换所有标记为小写之后，你的短语料库中只有 248 个拼写独特的标记。这意味着，平均而言，每个标记几乎被使用了两次（`498
    / 248`）。
- en: '[PRE17]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: It is usually best to run through your entire corpus to build up your vocabulary
    before you go back through the documents counting up tokens and putting them in
    the right slot in your vocab. If you do it this way, it allows you to alphabetize
    your vocabulary, making it easier to keep track of approximately where each token
    count should be in the vectors. And you can filter our really frequent or really
    rare tokens so you can ignore them and keep the dimensions low. This is especially
    important when you want to count n-grams longer than 1-grams.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 通常最好在回到文档中计算标记并将它们放入词汇表的正确位置之前，遍历整个语料库以建立起你的词汇表。如果你这样做，可以按字母顺序排列你的词汇表，这样更容易跟踪每个标记计数应该在向量中的大致位置。你还可以过滤掉非常频繁或非常稀有的标记，这样你就可以忽略它们并保持维度较低。当你想要计算比
    1-gram 更长的 n-grams 时，这一点尤为重要。
- en: Assuming you want to keep count of all 248 tokens in this all-lowercase 1-gram
    vocabulary, you can reassemble your count vector matrix.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想要统计这个全部小写的 1-gram 词汇中的所有 248 个标记的计数，你可以重新组装你的计数向量矩阵。
- en: '[PRE18]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Look through a few of those count vectors and see if you can find the sentence
    they correspond to in the Wikipedia article on "Algorithmic Bias." Can you see
    how you can get a feel for what each sentence is saying by only looking at the
    vector? A count vector puts the gist of a document into a numerical vector. And
    for a machine that knows nothing about the meaning of words, it is helpful to
    normalize these counts by how frequent a token is overall. For that you will use
    the Scikit-Learn package.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 浏览几个这些计数向量，看看你能否在“算法偏见”维基百科文章中找到它们对应的句子。你能否通过仅查看向量来感受到每个句子在说什么？一个计数向量将文档的要点放入一个数值向量中。对于一个对单词含义一无所知的机器来说，将这些计数归一化为标记的总体频率是有帮助的。为此，你将使用
    Scikit-Learn 包。
- en: 3.1.2 Faster, better, easier token counting
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.2 更快、更好、更容易的标记计数
- en: Now that you’ve manually created your count vectors, you might wonder if someone
    has built a library for all this token counting and accounting. You can usually
    count on the Scikit-Learn (`sklearn`) package to satisfy all your NLP needs.^([[2](#_footnotedef_2
    "View footnote.")]) If you have already installed the `nlpia2` package you will
    already have Scikit-Learn (`sklearn`) installed. If you would rather install it
    manually, here is one way.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经手动创建了你的计数向量，你可能想知道是否有人为所有这些标记计数和记账构建了一个库。你通常可以依靠 Scikit-Learn (`sklearn`)
    包来满足你所有的自然语言处理需求。如果你已经安装了 `nlpia2` 包，你已经安装了 Scikit-Learn (`sklearn`)。如果你更愿意手动安装它，这是一种方式。
- en: '[PRE19]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In an `ipython` console or `jupyter notebook` you can run bash commands using
    the exclamation point at the start of a line.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `ipython` 控制台或 `jupyter notebook` 中，你可以使用感叹号在行首运行 bash 命令。
- en: '[PRE20]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Once you have set up your environment and installed Scikit-Learn here is how
    you would create the term frequency vector. The `CountVectorizer` class is similar
    to that list of `Counter` classes that you used earlier. It is a standard *transformer*
    class with `.fit()` and `.transform()` methods that comply with the sklearn API
    for all machine learning models.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你设置好了你的环境并安装了 Scikit-Learn，你就可以创建术语频率向量了。`CountVectorizer` 类似于你之前使用过的 `Counter`
    类的列表。它是一个标准的*转换器*类，具有符合 sklearn API 的`.fit()` 和 `.transform()` 方法，适用于所有机器学习模型。
- en: Listing 3.1 Using `sklearn` to compute word count vectors
  id: totrans-99
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列出 3.1 使用 `sklearn` 计算单词计数向量
- en: '[PRE21]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Now you have a matrix (practically a list of lists in Python) that represents
    the three documents (the three rows of the matrix) and the count of each term,
    token, or word in your lexicon makes up the columns of the matrix. That was fast!
    With just 1 line of code, `vectorize.fit_transform(corpus)`, we have gotten to
    the same result as with dozens of lines you needed to manually tokenize, create
    a lexicon and count the terms. Note that these vectors have a length of 16, rather
    than 18 like the vectors you created manually. That’s because Scikit-Learn tokenizes
    the sentences slightly differently (it only considers words of 2 letters or more
    as tokens) and drops the punctuation.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你有一个矩阵（在 Python 中实际上是一个列表的列表），代表了三个文档（矩阵的三行）和词汇表中每个词的计数组成了矩阵的列。这很快！只需一行代码`vectorize.fit_transform(corpus)`，我们就达到了与你需要手动进行分词、创建词汇表和计数术语的几十行代码相同的结果。请注意，这些向量的长度为
    16，而不是像你手动创建的向量一样的 18。这是因为 Scikit-Learn 对句子进行了稍微不同的分词（它只考虑两个或更多字母的单词作为标记）并且去掉了标点符号。
- en: So, you have three vectors, one for each document. Now what? What can you do
    with them? Your document word-count vectors can do all the cool stuff any vector
    can do, so let’s learn a bit more about vectors and vector spaces first.^([[3](#_footnotedef_3
    "View footnote.")])
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，你有三个向量，每个文档一个。现在呢？你能做什么？你的文档词数向量可以做任何向量可以做的很酷的事情，所以让我们先学习更多关于向量和向量空间的知识。
- en: 3.1.3 Vectorize your code
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.3 将你的代码向量化
- en: If you read about "vectorizing code" on the internet means something entirely
    different than "vectorizing text." Vectorizing text is about converting text into
    a meaningful vector representation of that text. Vectorizing code is about speeding
    up your code by taking advantage of powerful compiled libraries like `numpy` and
    using Python to do math as little as possible. The reason it’s called "vectorizing"
    is because you can use vector algebra notation to eliminate the for loops in your
    code, the slowest part of many NLP pipelines. Instead of `for` loops iterating
    through all the elements of a vector or matrix to do math you just use numpy to
    do the for loop for you in compiled C code. And Pandas uses `numpy` under the
    hood for all its vector algebra, so you can mix and match a DataFrame with a numpy
    arrary or a Python float and it will all run really fast.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在网上读到关于“向量化代码”的内容，意味着与“向量化文本”完全不同。向量化文本是将文本转换为该文本的有意义的向量表示。向量化代码是通过利用强大的编译库（如`numpy`）加速代码，并尽可能少地使用
    Python 进行数学运算。之所以称其为“向量化”，是因为你可以使用向量代数表示法来消除代码中的`for`循环，这是许多 NLP 管道中最慢的部分。而不是使用`for`循环遍历向量或矩阵中的所有元素进行数学运算，你只需使用
    numpy 来在编译的 C 代码中为你执行`for`循环。Pandas 在其向量代数中使用了`numpy`，所以你可以混合和匹配 DataFrame 和 numpy
    数组或 Python 浮点数，所有这些都将运行得非常快。
- en: '[PRE22]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Python’s dynamic typing design makes all this magic possible. When you multiply
    a `float` by an `array` or `DataFrame`, instead of raising an error because you’re
    doing math on two different types, the interpreter will figure out what you’re
    trying to do and "make is so," just like Sulu. And it will compute what you’re
    looking for in the fastest possible way, using compiled C code rather than a Python
    `for` loop.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Python 的动态类型设计使得所有这些魔法成为可能。当你将一个`float`乘以一个`array`或`DataFrame`时，不会因为你在两种不同类型上进行数学运算而引发错误，解释器会弄清楚你想要做什么，就像苏鲁一样。“让它成为”并且它将以最快的方式计算你所寻找的东西，使用编译的
    C 代码而不是 Python 的`for`循环。
- en: Tip
  id: totrans-107
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: If you use vectorization to eleminate some of the `for` loops in your code,
    you can speed up your NLP pipeline by a 100x or more. This is 100x more models
    that you can try. The Berlin Social Science Center (WZB) has a great tutorial
    on vectorization.^([[4](#_footnotedef_4 "View footnote.")]). And if you poke around
    elsewhere on the site you’ll find perhaps the only trustworthy source of statistics
    and data on the effect NLP and AI are having on society.^([[5](#_footnotedef_5
    "View footnote.")])
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在代码中使用向量化来消除一些`for`循环，你可以将你的 NLP 管道加速 100 倍甚至更多。这意味着你可以尝试 100 倍以上的模型。柏林社会科学中心（WZB）有一个关于向量化的很棒的教程。而且如果你在网站的其他地方搜索，你会发现这可能是唯一一个对
    NLP 和 AI 对社会影响的统计数据和数据有信任的来源。
- en: 3.1.4 Vector spaces
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.4 向量空间
- en: Vectors are the primary building blocks of linear algebra, or vector algebra.
    They are an ordered list of numbers, or coordinates, in a vector space. They describe
    a location or position in that space. Or they can be used to identify a particular
    direction and magnitude or distance in that space. A *vector space* is the collection
    of all possible vectors that could appear in that space. So a vector with two
    values would lie in a 2D vector space, a vector with three values in 3D vector
    space, and so on.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 向量是线性代数或向量代数的主要构建块。它们是向量空间中的一组有序数字或坐标。它们描述了该空间中的位置或位置。或者它们可以用来标识该空间中的特定方向和大小或距离。*向量空间*是该空间中可能出现的所有可能向量的集合。因此，具有两个值的向量将位于二维向量空间中，具有三个值的向量将位于三维向量空间中，依此类推。
- en: A piece of graph paper, or a grid of pixels in an image, are both nice 2D vector
    spaces. You can see how the order of these coordinates matter. If you reverse
    the x and y coordinates for locations on your graph paper, without reversing all
    your vector calculations, all your answers for linear algebra problems would be
    flipped. Graph paper and images are examples of rectilinear, or Euclidean, spaces
    because the x and y coordinates are perpendicular to each other. The vectors you
    talk about in this chapter are all rectilinear, Euclidean spaces.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 一张纸上的一小块图，或者图像中的像素网格，都是不错的二维向量空间。你可以看到这些坐标的顺序很重要。如果你颠倒了图纸上位置的 x 和 y 坐标，而没有颠倒所有的向量计算，那么你所有的线性代数问题的答案都会被颠倒。图纸和图像是矩形的，或者欧几里得的空间的例子，因为
    x 和 y 坐标是彼此垂直的。本章中讨论的向量都是矩形的，欧几里得的空间。
- en: 'What about latitude and longitude on a map or globe? That geographic coordinate
    space is definitely two-dimensional because it’s an ordered list of two numbers:
    latitude and longitude. But each of the latitude-longitude pairs describes a point
    on an approximately spherical surface — the Earth’s surface. The latitude-longitude
    vector space is not rectilinear, and Euclidean geometry doesn’t exactly work in
    it. That means you have to be careful when you calculate things like distance
    or closeness between two points represented by a pair of 2D geographic coordinates,
    or points in any non-Euclidean space. Think about how you would calculate the
    distance between the latitude and longitude coordinates of Portland, OR and New
    York, NY.^([[6](#_footnotedef_6 "View footnote.")])'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 地图或地球上的经度和纬度呢？那地理坐标空间肯定是二维的，因为它是一组有序的两个数字：纬度和经度。但每个纬度-经度对描述的是一个近似球面的点——地球表面。纬度-经度向量空间不是直线的，并且欧几里得几何在其中不完全适用。这意味着在计算由一对二维地理坐标或任何非欧几里得空间中的点表示的距离或接近度时，你必须小心。想想如何计算波特兰和纽约的纬度和经度坐标之间的距离。^([[6](#_footnotedef_6
    "查看注释。")])
- en: Figure [3.1](#figure-2d-vectors) shows one way to visualize the three 2D vectors
    `(5, 5)`, `(3, 2)`, and `(-1, 1)`. The head of a vector (represented by the pointy
    tip of an arrow) is used to identify a location in a vector space. So the vector
    heads in this diagram will be at those three pairs of coordinates. The tail of
    a position vector (represented by the "rear" of the arrow) is always at the origin,
    or `(0, 0)`.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图[3.1](#figure-2d-vectors)展示了一种可视化三个二维向量`(5, 5)`、`(3, 2)`和`(-1, 1)`的方法。向量的头部（由箭头尖端表示）用于标识向量空间中的位置。因此，该图中的向量头将位于这三个坐标对处。位置向量的尾部（由箭头的“后部”表示）始终位于原点，或`(0,
    0)`。
- en: Figure 3\. 1\. 2D vectors
  id: totrans-114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3. 1. 二维向量
- en: '![vecs](images/vecs.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![vecs](images/vecs.png)'
- en: What about 3D vector spaces? Positions and velocities in the 3D physical world
    you live in can be represented by x, y, and z coordinates in a 3D vector. But
    you aren’t limited to normal 3D space. You can have 5 dimensions, 10 dimensions,
    5,000, whatever. The linear algebra all works out the same. You might need more
    computing power as the dimensionality grows. And you’ll run into some "curse-of-dimensionality"
    issues, but you can wait to deal with that until chapter 10.^([[7](#_footnotedef_7
    "View footnote.")])
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 三维向量空间呢？你生活的三维物理世界中的位置和速度可以用三维向量中的 x、y 和 z 坐标来表示。但你并不限于正常的三维空间。你可以有 5 个维度、10
    个维度、5000 个维度，等等。线性代数都能得到相同的结果。随着维度的增加，你可能需要更多的计算能力。你会遇到一些“维度灾难”问题，但你可以等到第 10 章再处理它们。^([[7](#_footnotedef_7
    "查看注释。")])
- en: For a natural language document vector space, the dimensionality of your vector
    space is the count of the number of distinct words that appear in the entire corpus.
    For TF (and TF-IDF to come), we call this dimensionality capital letter "K". This
    number of distinct words is also the vocabulary size of your corpus, so in an
    academic paper it’ll usually be called "|V|" You can then describe each document,
    within this K-dimensional vector space by a K-dimensional vector. K = 18 in your
    three-document corpus about Harry and Jill (or 16, if your tokenizer drops the
    punctuation). Because humans can’t easily visualize spaces of more than three
    dimensions, let’s set aside most of those dimensions and look at two for a moment,
    so you can have a visual representation of the vectors on this flat page you’re
    reading. So in figure [3.2](#figure-2d-term-frequency-vectors), K is reduced to
    two for a two-dimensional view of the 18-dimensional Harry and Jill vector space.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 对于自然语言文档向量空间，您的向量空间的维度是整个语料库中出现的不同单词数量的计数。对于 TF（和即将出现的 TF-IDF），我们将此维度称为大写字母“K”。这个不同单词的数量也是您语料库的词汇量大小，所以在学术论文中它通常被称为“|V|”。然后，您可以用一个
    K 维向量描述这个 K 维向量空间中的每个文档。在关于哈利和吉尔的三个文档语料库中，K = 18（或者如果您的分词器去除了标点符号，则为 16）。因为人类不能轻易地可视化超过三维的空间，所以让我们暂时搁置大部分维度，看一看其中的两个，这样你就可以在这张平面上的页面上对这些向量进行可视化表示了。因此，在图
    [3.2](#figure-2d-term-frequency-vectors) 中，K 被缩减为两个，以便二维查看 18 维哈利和吉尔向量空间。
- en: Figure 3\. 2\. 2D term frequency vectors
  id: totrans-118
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3. 2. 2D 项频率向量
- en: '![harry faster vecs](images/harry_faster_vecs.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![哈利更快的向量](images/harry_faster_vecs.png)'
- en: K-dimensional vectors work the same way, just in ways you can’t easily visualize.
    Now that you have a representation of each document and know they share a common
    space, you have a path to compare them. You could measure the Euclidean distance
    between the vectors by subtracting them and computing the length of that distance
    between them, which is called the 2-norm distance. It’s the distance a "crow"
    would have to fly (in a straight line) to get from a location identified by the
    tip (head) of one vector and the location of the tip of the other vector. Check
    out appendix C on linear algebra to see why this is a bad idea for word count
    (term frequency) vectors.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: K 维向量的工作方式相同，只是您不能轻易地可视化它们。现在您已经有了每个文档的表示，并且知道它们共享一个共同的空间，您可以比较它们了。您可以通过减去它们并计算它们之间的距离的长度来测量向量之间的欧几里德距离，这称为
    2-范数距离。它是一只“乌鸦”飞行（直线）从一个向量的尖端（头部）到另一个向量的尖端的距离。查看线性代数附录 C，了解为什么这对于单词计数（项频率）向量是个糟糕的主意。
- en: Two vectors are "similar" if they share similar direction. They might have similar
    magnitude (length), which would mean that the word count (term frequency) vectors
    are for documents of about the same length. But do you care about document length
    in your similarity estimate for vector representations of words in documents?
    Probably not. You’d like your estimate of document similarity to find use of the
    same words about the same number of times in similar proportions. This accurate
    estimate would give you confidence that the documents they represent are probably
    talking about similar things.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两个向量具有相似的方向，则它们是“相似的”。它们可能具有相似的大小（长度），这意味着单词计数（项频率）向量的长度大致相同。但是您是否在词汇量空间中对文档长度感兴趣？可能不。您希望您对文档相似性的估计发现相同单词的使用大致相同的次数和相似的比例。这样准确的估计会让您相信它们所代表的文档可能在讨论相似的内容。
- en: Figure 3\. 3\. 2D vectors and the angles between them
  id: totrans-122
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3. 3. 2D 向量及其之间的角度
- en: '![vecs cosine](images/vecs_cosine.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![向量余弦](images/vecs_cosine.png)'
- en: '*Cosine similarity*, is the cosine of the angle between two vectors (theta).
    Figure [3.3](#figure-vecs-cosine) shows how you can compute the cosine similarity
    dot product using equation [3.1](#equation_3_3). Cosine similarity is a popular
    among NLP engineers because:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '*余弦相似度*，是两个向量之间的夹角（θ）的余弦值。图 [3.3](#figure-vecs-cosine) 显示了如何使用方程 [3.1](#equation_3_3)
    计算余弦相似度点积。余弦相似度在 NLP 工程师中很受欢迎，因为：'
- en: Fast to compute even for high dimensional vectors
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使对于高维向量也能快速计算
- en: Sensitive to changes in a single dimension
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对单个维度的变化敏感
- en: Work well for high-dimensional vectors
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对高维向量表现良好
- en: Has a value between -1 and 1
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其值介于 -1 和 1 之间
- en: You can use cosine similarity without bogging down your NLP pipeline because
    you only need to compute the dot product. And you may be surprised to learn that
    you do not need to compute the cosine function to get the cosine similarity. You
    can use the linear algebra dot product, which does not require any trigonometric
    function evaluation. This makes it very efficient (fast) to calculate. And cosine
    similarity considers each dimension independently and their effect on the direction
    of the vector adds up, even for high dimensional vectors. TF-IDF can have thousands
    or even millions of dimensions, so you need to use a metric that doesn’t degrade
    in usefulness as the number of dimensions increases (called the curse of dimensionality).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用余弦相似度而不拖慢你的 NLP 管道，因为你只需要计算点积。你可能会惊讶地发现，你不需要计算余弦函数就能得到余弦相似度。你可以使用线性代数点积，它不需要进行任何三角函数计算。这使得计算非常高效（快速）。余弦相似度独立地考虑每个维度及其对向量方向的影响，即使对于高维向量也是如此。TF-IDF
    可能有数千甚至数百万个维度，因此你需要使用一个在维度数量增加时不会降低有用性的度量（称为维度灾难）。
- en: 'Another big advantage of cosine similarity is that it outputs a value between
    -1 and +1:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度的另一个重要优势是它输出一个介于 -1 和 +1 之间的值：
- en: -1 means the vectors point in exactly opposite directions - this can only happen
    for vectors that have negative values (TF-IDF vectors do not)
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: -1 表示向量指向完全相反的方向 - 这只会发生在具有负值的向量上（TF-IDF 向量除外）
- en: 0 means the vectors are perpendicular or orthogonal - this happens whenever
    your two TF-IDF vectors do not share any of the same words (dimensions)
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示向量是垂直或正交的 - 这会在你的两个 TF-IDF 向量不共享任何相同单词（维度）时发生
- en: +1 means the two vectors are perfectly aligned - this can happen whenever your
    two documents use the same words with the same relative frequency
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: +1 表示两个向量完全对齐 - 这会在你的两个文档使用相同单词且相对频率相同的情况下发生
- en: 'This makes it easier to guess at good thresholds to use in conditional expression
    within your pipeline. Here’s what the normalized dot product looks like in your
    linear algebra textbook:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这样更容易猜测在管道内的条件表达式中使用的好阈值。以下是在你的线性代数教科书中归一化点积的样子：
- en: Equation 3.1
  id: totrans-135
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 3.1
- en: \[\begin{equation} \boldsymbol{A} \cdot \boldsymbol{B} = |\boldsymbol{A}| |\boldsymbol{B}|
    * cos(\theta) \end{equation}\]
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation} \boldsymbol{A} \cdot \boldsymbol{B} = |\boldsymbol{A}| |\boldsymbol{B}|
    * cos(\theta) \end{equation}\]
- en: 'In Python you might use code like this to compute cosine similarity:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，你可能会使用类似以下的代码来计算余弦相似度：
- en: '[PRE23]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'If you solve this equation for `np.cos(angle_between_A_and_B)` (called "cosine
    similarity between vectors A and B") you can derive code to computer the cosine
    similarity:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你解出这个方程得到 `np.cos(angle_between_A_and_B)`（称为“向量 A 和 B 之间的余弦相似度”），你可以导出计算余弦相似度的代码：
- en: Listing 3.2 Cosine similarity formula in Python
  id: totrans-140
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.2 Python 中的余弦相似度公式
- en: '[PRE24]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In linear algebra notation this becomes equation [3.2](#equation_3_4):'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 用线性代数表示，这变成了方程式 [3.2](#equation_3_4)：
- en: Equation 3.2 cosine similarity between two vectors
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 3.2 两个向量之间的余弦相似度
- en: \[\begin{equation} cos(\theta) = \frac{\boldsymbol{A} \cdot \boldsymbol{B}}{|\boldsymbol{A}||\boldsymbol{B}|}
    \end{equation}\]
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation} cos(\theta) = \frac{\boldsymbol{A} \cdot \boldsymbol{B}}{|\boldsymbol{A}||\boldsymbol{B}|}
    \end{equation}\]
- en: 'Or in pure Python without `numpy`:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 或者在纯 Python 中，不使用 `numpy`：
- en: Listing 3.3 Compute cosine similarity in python
  id: totrans-146
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.3 在 Python 中计算余弦相似度
- en: '[PRE25]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: So you need to take the dot product of two of your vectors in question — multiply
    the elements of each vector pairwise — and then sum those products up. You then
    divide by the norm (magnitude or length) of each vector. The vector norm is the
    same as its Euclidean distance from the head to the tail of the vector — the square
    root of the sum of the squares of its elements. This *normalized dot product*,
    like the output of the cosine function, will be a value between -1 and 1\. It
    is the cosine of the angle between these two vectors. It gives you a value for
    how much the vectors point in the same direction.^([[8](#_footnotedef_8 "View
    footnote.")])
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你需要计算你感兴趣的两个向量的点积 - 将每个向量的元素成对相乘 - 然后将这些乘积相加。然后你除以每个向量的范数（大小或长度）。向量范数与其从头到尾的欧几里德距离相同
    - 其元素平方和的平方根。这个*归一化点积*，就像余弦函数的输出一样，将是介于 -1 和 1 之间的值。它是这两个向量之间夹角的余弦。它给出了这两个向量指向相同方向的程度的值。[[8](#_footnotedef_8
    "View footnote.")]
- en: A cosine similarity of **1** represents identical normalized vectors that point
    in exactly the same direction along all dimensions. The vectors may have different
    lengths or magnitudes, but they point in the same direction. Remember you divided
    the dot product by the norm of each vector. So the closer a cosine similarity
    value is to 1, the closer the two vectors are in angle. For NLP document vectors
    that have a cosine similarity close to 1, you know that the documents are using
    similar words in similar proportion. So the documents whose document vectors are
    close to each other are likely talking about the same thing.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '**1** 的余弦相似度代表指向所有维度上完全相同方向的标准化向量。这些向量可能具有不同的长度或大小，但它们指向相同的方向。请记住，你将点积除以每个向量的范数。因此，余弦相似度值越接近
    1，两个向量在角度上越接近。对于 NLP 文档向量，如果余弦相似度接近 1，你就知道这些文档使用相似的词汇以相似的比例。因此，文档向量彼此接近的文档很可能在谈论相同的事情。'
- en: A cosine similarity of **0** represents two vectors that share no components.
    They are orthogonal, perpendicular in all dimensions. For NLP TF vectors, this
    situation occurs only if the two documents share no words in common. This doesn’t
    necessarily mean they have different meanings or topics, just that they use completely
    different words.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '**0** 的余弦相似度代表两个向量没有共享成分。它们在所有维度上都是正交的，即在所有维度上都是垂直的。对于 NLP 的 TF 向量来说，只有当两个文档没有共同的词时才会出现这种情况。这并不一定意味着它们具有不同的含义或主题，只是它们使用完全不同的词语。'
- en: A cosine similarity of **-1** represents two vectors that are anti-similar,
    completely opposite. They point in opposite directions. This can never happen
    for simple word count (term frequency) vectors or even normalized TF vectors (which
    we talk about later). Counts of words can never be negative. So word count (term
    frequency) vectors will always be in the same "quadrant" of the vector space.
    None of the term frequency vectors can sneak around into one of the quadrants
    in the vector space. None of your term frequency vectors can have components (word
    frequencies) that are the negative of another term frequency vector, because term
    frequencies just can’t be negative.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**-1** 的余弦相似度代表两个完全相反的向量，完全相反。它们指向相反的方向。对于简单的词频（词项频率）向量甚至是标准化的 TF 向量（稍后我们会讨论），这种情况永远不会发生。单词的计数永远不会是负数。因此，词频（词项频率）向量始终位于向量空间的同一“象限”中。你的任何词频向量都不可能在向量空间的一个象限中悄悄溜走。你的任何词频向量都不可能有与另一个词频向量相反的分量（词频），因为词频就是不能是负数。'
- en: You won’t see any negative cosine similarity values for pairs of vectors for
    natural language documents in this chapter. But in the next chapter, we develop
    a concept of words and topics that are "opposite" to each other. And this will
    show up as documents, words, and topics that have cosine similarities of less
    than zero, or even **-1**.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章节中，你不会看到任何自然语言文档向量对的负余弦相似度值。但在下一章中，我们将发展出一种概念，即相互“相反”的单词和主题。这将显示为余弦相似度小于零，甚至是
    **-1** 的文档、单词和主题。
- en: 'If you want to compute cosine similarity for regular `numpy` vectors, such
    as those returned by `CountVectorizer`, you can use Scikit-Learn’s built-in tools.
    Here is how you can calculate the cosine similarity between word vectors 1 and
    2 that we computed in [3.4](#listing-cosine-similarity):'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要计算常规 `numpy` 向量的余弦相似度，比如由 `CountVectorizer` 返回的向量，你可以使用 Scikit-Learn 内置的工具。这是如何计算我们在
    [3.4](#listing-cosine-similarity) 中计算的词向量 1 和 2 之间的余弦相似度的方法：
- en: Listing 3.4 Cosine similarity
  id: totrans-154
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第3.4节 余弦相似度
- en: '[PRE26]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: That slicing of the term frequency (`tf`) DataFrame probably looks like an odd
    way to retrieve a vector. This is because the SciKit-Learn function for computing
    cosine similarity has been optimized to work efficiently on large arrays of vectors
    (2-D matrices). This code slices off the first and second row of the DataFrame
    as a 1xN array containing the counts of the words in the first sentences of the
    text. This count vector for the first sentence from the "Algorithmic Bias" article
    is only 11.7% similar (cosine similarity of 0.117) to the second sentence of the
    article. It seems that the second sentence shares very few words with the first
    sentence.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 对词频（`tf`）DataFrame 进行切片的操作可能看起来是检索向量的奇怪方式。这是因为 SciKit-Learn 用于计算余弦相似度的函数已经被优化为在大型向量数组（2-D
    矩阵）上高效工作。这段代码将 DataFrame 的第一行和第二行切片为包含文本第一句中单词计数的 1xN 数组。这个第一句话来自于“算法偏见”文章的计数向量与该文章第二句话只有
    11.7% 的相似度（余弦相似度为 0.117）。看起来第二句话与第一句话共享的单词非常少。
- en: To dig in deeper and understand cosine distance you can check that the code
    in [3.3](#listing-compute-cosine-similarity-in-python) will give you the same
    answer for Counter dictionaries as the `sklearn` cosine similarity function gives
    you for equivalent numpy arrays. And while you are at it, use *active learning*
    to guess the cosine similarity for each pair of sentences before seeing what your
    function outputs. Whenever you try to predict the output of an NLP algorithm,
    and then correct yourself based on what really happens, it improves your intuition
    about how NLP works.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更深入地了解余弦距离，你可以检查代码 [3.3](#listing-compute-cosine-similarity-in-python)，它会给你与`sklearn`余弦相似度函数在等效的numpy数组中给出的Counter字典相同的答案。当你尝试预测一个NLP算法的输出，然后根据实际情况进行修正时，它会提高你对NLP工作原理的直觉。
- en: 3.2 Counting n-grams
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 计数 n-grams
- en: You have already seen in the last chapter how to create *n*-grams from the tokens
    in your corpus. Now, it’s time to use them to create a better representation of
    documents. Fortunately for you, you can use the same tools you are already familiar
    with, just tweak the parameters slightly.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中你已经学到如何从语料库的标记中创建 *n*-gram。现在，是时候将它们用于创建更好的文档表示了。对你来说幸运的是，你可以使用你已经熟悉的相同工具，只需稍微调整参数即可。
- en: First, let’s add another sentence to our corpus, which will illustrate why ngram
    vectors can sometimes be more useful than count vectors.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们在我们的语料库中添加另一句话，这将说明为什么 n-gram 向量有时比计数向量更有用。
- en: '[PRE27]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'If you compute the vector of word counts for this new sentence (question),
    using the same vectorizer we trained in Listing 3.2, you will see that it is exactly
    equal to the representation of the second sentence:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用我们在 3.2 小节训练的相同的向量化器计算这个新句子（问题）的词频向量，你会发现它与第二个句子的表示完全相等：
- en: '[PRE28]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Sparse matrices are an efficient way to store token counts, but to build your
    intuition about what’s going on, or debug your code, you will want to *densify*
    a vector. You can convert a sparse vector (row of a sparse matrix) to a numpy
    array or Pandas series using the `.toarray()` method.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏矩阵是存储标记计数的高效方法，但为了增强对正在发生的情况的直观理解，或者调试代码，你会希望将向量*稠密化*。你可以使用`.toarray()`方法将稀疏向量（稀疏矩阵的行）转换为numpy数组或Pandas系列。
- en: '[PRE29]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: You can probably guess which word in the question that is showing up at the
    8th position (dimension) in the count vector. Remember that this is the 8th word
    in the vocabulary computed by the `CountVectorizer`, and it lexically sorts its
    vocabulary when you run `.fit()`. You can pair up the count vector with your vocabulary
    in a Pandas `Series` to see what is going on inside your count vectors.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能猜到了问题中显示在计数向量的第8个位置（维度）上的单词是哪个。记住，这是由`CountVectorizer`计算的词汇表中的第8个词，并且在运行`.fit()`时它会按字典序对其词汇表进行排序。你可以将计数向量与Pandas`Series`一起配对，以查看计数向量中的内容。
- en: '[PRE30]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Now, calculate the cosine similarity between the question vector and all the
    other vectors in your "knowledge base" of sentence vectors. This is what a search
    engine or database full text search will do to find answers to your queries.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，计算问题向量与你的句子向量"知识库"中所有其他向量之间的余弦相似度。这就是搜索引擎或数据库全文搜索用来查找问题答案的方法。
- en: '[PRE31]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The closest (most similar) is for the fourth sentence in the corpus. It has
    a cosine similarity of .433 with the `question_vector`. Check out the fourth sentence
    in your knowledge base of sentences to see if it might be a good match for this
    question.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 最相似的是语料库中的第四个句子。它与`question_vector`的余弦相似度为0.433。检查一下你的句子知识库中的第四个句子，看看它是否能很好地匹配这个问题。
- en: '[PRE32]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Not bad! That sentence would be a good start. However, the first sentence of
    the Wikipedia article is probably a better definition of algorithmic bias for
    this question. Think about how you could improve the vectorization pipeline so
    that your search would return the first sentence rather than the 4th.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 不错！那个句子可能是一个不错的开头。然而，维基百科文章的第一句可能更适合这个问题的算法偏见的定义。想一想如何改进向量化流水线，使得你的搜索返回第一句而不是第四句。
- en: To find out whether 2-grams might help, do the same vectorization process you
    did a few pages ago with `CountVectorizer`, but instead set the `n-gram` *hyperparameter*
    to count 2-grams instead of individual tokens (1-grams). A hyperparameter is just
    a function name or argument value or anything you may want to adjust to improve
    your NLP pipeline in some way. Finding the best hyperparmeters is called hyperparameter
    tuning. So start tuning up the `ngram_range` parameter to see if it helps.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 要找出 2-grams 是否有帮助，请执行与几页前使用 `CountVectorizer` 进行的相同向量化过程，但是将 `n-gram` *超参数*
    设置为计算 2-grams 而不是单个令牌（1-grams）。超参数只是一个函数名称、参数值或任何你可能想要调整以改善 NLP 流水线的东西。找到最佳超参数称为超参数调整。因此开始调整
    `ngram_range` 参数，看看是否有帮助。
- en: '[PRE33]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Looking at the dimensionality of the new count vectors you probably noticed
    that these vectors are significantly longer. There are always more unique 2-grams
    (pairs of words) than unique tokens. Check out the ngram-counts for that "algorithmic
    bias" 2-gram that is so important for your question.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 查看新计数向量的维数，你可能注意到这些向量要长得多。唯一的 2-grams（单词对）总是比唯一的令牌多。检查一下对你的问题非常重要的“算法偏差”2-gram
    的 ngram-计数。
- en: '[PRE34]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Now that first sentence might be a better match for your query. It is worth
    noting that bag-of-*n*-grams approach has its own challenges. With large texts
    and corpora, the amount of *n*-grams increases exponentially, causing "curse-of-dimensionality"
    issues we mentioned before. However, as you saw in this section, there might be
    cases where you will want to use it instead of single token counting.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，第一句话可能更符合你的查询。值得注意的是，词袋-*n*-gram 方法也有自己的挑战。在大型文本和语料库中，*n*-gram 的数量呈指数增长，导致了我们之前提到的“维度灾难”问题。然而，正如你在本节中看到的，可能会有一些情况，你会选择使用它而不是单个令牌计数。
- en: 3.2.1 Analyzing this
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.1 分析这个
- en: Even though until now we only dealt with *n*-grams of word token, *n*-gram of
    characters can be useful too. For example, they can be used for language detection,
    or authorship attribution (deciding who among the set of authors wrote the document
    analyzed). Let’s solve a puzzle using character *n*-grams and the `CountVectorizer`
    class you just learned how to use.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 即使到目前为止我们只处理了词令牌的 *n*-grams，字符的 *n*-grams 也是有用的。例如，它们可以用于语言检测或作者归属（确定在分析的文档集中谁是作者）。让我们使用字符
    *n*-grams 和你刚学会如何使用的 `CountVectorizer` 类来解决一个谜题。
- en: 'We’ll start by importing a small and interesting Python package called `this`,
    and examining some of its constants:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从导入一个名为 `this` 的小而有趣的 Python 包开始，并检查其中一些常量：
- en: '[PRE35]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: What are these strange words? In what language are they written? H.P. Lovecraft
    fans may think of the ancient language used to summon the dead deity Cthulhu.^([[9](#_footnotedef_9
    "View footnote.")]) But even to them, this message will be incomprehensible.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这些奇怪的词是什么？用什么语言写的？H.P. Lovecraft 的粉丝可能会想到用来召唤死神克苏鲁的古老语言。^([[9](#_footnotedef_9
    "查看脚注。")]) 但即使对他们来说，这个消息也将是难以理解的。
- en: To figure out the meaning of this cryptic piece of text, you’ll use the method
    you just learned — frequency analysis (counting tokens). Only this time, a little
    bird is telling you it might be worth to start with character tokens rather than
    word tokens! Luckily, `CountVectorizer` can serve you here as well. You can see
    the results of listing [3.5](#listing-countvectorizer-histogram) in figure 3.4a
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 为了弄清楚这段神秘文字的意思，你将使用你刚学到的方法 - 频率分析（计数令牌）。但这一次，一只小鸟告诉你，也许从字符令牌而不是单词令牌开始可能会更有价值！幸运的是，`CountVectorizer`
    在这里也能为你提供帮助。你可以在图 3.4a 中看到列出的结果 [3.5](#listing-countvectorizer-histogram) 。
- en: Listing 3.5 CountVectorizer histogram
  id: totrans-184
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.5 CountVectorizer 直方图
- en: '[PRE36]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Hmmm. Not quite sure what you can do with these frequency counts. But then
    again, you haven’t even seen the frequency counts for any other text yet. Let’s
    choose some big document - for example, the Wikipedia article for Machine Learning,^([[10](#_footnotedef_10
    "View footnote.")]) and try to do the same analysis (check out the results in
    Figure 3.4b):'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯。 不太确定你可以用这些频率计数做什么。 但再说一遍，你甚至还没有看到其他文本的频率计数。 让我们选择一些大型文档 - 例如，机器学习的维基百科文章，^([[10](#_footnotedef_10
    "查看脚注。")]) 并尝试进行相同的分析（查看图 3.4b 中的结果）：
- en: '[PRE37]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Now that looks interesting! If you look closely at the two frequency histograms,
    you might notice a pattern. The peaks and valleys of the histograms seem to be
    arranged in the same order. If you’ve worked with frequency spectra before, this
    may make sense. The pattern of character frequency peaks and valleys is similar,
    but shifted.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在看起来很有趣！如果你仔细观察两个频率直方图，你可能会注意到一个模式。直方图的峰值和谷值似乎以相同的顺序排列。如果你之前曾经处理过频率谱，这可能会有意义。字符频率峰值和谷值的模式是相似的，但是偏移了。
- en: To determine whether your eyes are seeing a real pattern, you need to check
    to see if the shift in the peaks and valleys is consistent. This signal processing
    approach is called *spectral analysis*. You can compute the relative position
    of the peaks by subtracting the positions of the highest points of each signal
    from each other.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 要确定你的眼睛是否看到了一个真实的模式，你需要检查峰值和谷值的变化是否一致。这种信号处理方法被称为*频谱分析*。你可以通过将每个信号的最高点的位置相互减去来计算峰值的相对位置。
- en: You can use a couple of built-in python functions, `ord()` and `chr()`, to convert
    back and forth between integers and characters. Fortunately these integers and
    character mappings are in alphabetical order "ABC…​".
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用几个内置的 Python 函数，`ord()` 和 `chr()`，来在整数和字符之间进行转换。幸运的是，这些整数和字符的映射是按字母顺序排列的，"ABC…​"。
- en: '[PRE38]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'So if you want to decode the letter "R" in this secret message you should probably
    subtract 13 from its *ordinal* (`ord`) value to get the letter "E" - the most
    frequently used letter in English. Likewise to decode the letter "V" you would
    replace it with "I" - the second most frequently use letter. The three most frequent
    letters have been shifted by the same `peak_distance` (13) to create the encoded
    message. And that distance is preserved between the least frequent letters, too:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，如果你想解码这个秘密信息中的字母"R"，你应该从它的*ordinal*（`ord`）值中减去13，以得到字母"E"——英语中最常用的字母。同样，要解码字母"V"，你可以将它替换为"I"——第二个最常用的字母。前三个最常用的字母已经被同样的`peak_distance`（13）移动，以创建编码消息。并且这个距离在最不常用的字母之间也被保持：
- en: '[PRE39]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'By this point, you have probably MetaGered (searched the web) for information
    about this puzzle.^([[11](#_footnotedef_11 "View footnote.")]) Maybe you discovered
    that this secret message is probably encoded using a ROT13 cipher (encoding).^([[12](#_footnotedef_12
    "View footnote.")]) The ROT13 algorithm rotates each letter in a string 13 positions
    forward in the alphabet. To decode a supposedly secret message that has been encoded
    with ROT13 you would only need to apply the inverse algorithm and rotate your
    alphabet backwards 13 positions. You can probably create the encoder and decoder
    functions yourself in a single line of code. Or you can use python’s builtin `codecs`
    package to reveal what `this` is all about:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 到这个点为止，你可能已经通过 MetaGered（搜索网络）查找了有关这个谜题的信息。^([[11](#_footnotedef_11 "View footnote.")])也许你发现了这个秘密信息很可能是使用
    ROT13 密码（编码）进行编码的。^([[12](#_footnotedef_12 "View footnote.")]) ROT13 算法将字符串中的每个字母向字母表的前面旋转13个位置。要解码一个据说是用
    ROT13 编码的秘密信息，你只需要应用逆算法，将你的字母表向后旋转13个位置。你可能可以在一行代码中创建编码器和解码器函数。或者你可以使用 Python
    的内置`codecs`包来揭示这一切是关于什么的：
- en: '[PRE40]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Now you know The Zen of Python! These words of wisdom were written by one of
    the Python tribe elders, Tim Peters, back in 1999\. Since the poem has been placed
    in the public domain, put to music,^([[13](#_footnotedef_13 "View footnote.")])
    and even parodied.^([[14](#_footnotedef_14 "View footnote.")]) The Zen of Python
    has helps the authors of this book write cleaner, more readable and reusable code.
    And thanks to a character-based `CountVectorizer`, you were able to decode these
    words of wisdom.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你知道了 Python 的禅意！这些智慧之言是由 Python 族长之一的 Tim Peters 在 1999 年写的。由于这首诗已经放入了公共领域，被谱曲，^([[13](#_footnotedef_13
    "View footnote.")])甚至被拙劣模仿。^([[14](#_footnotedef_14 "View footnote.")])Python
    的禅意已经帮助本书作者编写了更干净、更易读和可重用的代码。由于基于字符的`CountVectorizer`，你能够解码这些智慧之言。
- en: 3.3 Zipf’s Law
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 锡普夫定律
- en: Now on to our main topic — Sociology. Okay, not, but you’ll make a quick detour
    into the world of counting people and words, and you’ll learn a seemingly universal
    rule that governs the counting of most things. It turns out, that in language,
    like most things involving living organisms, patterns abound.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来到了我们的主题——社会学。好吧，不是，但你将会快速地进入人数和字词计数的世界，你将会学到一个看似普遍适用的规则来统计大多数事物。事实证明，在语言中，像大多数涉及到生物的事物一样，模式是丰富多彩的。
- en: In the early twentieth century, the French stenographer Jean-Baptiste Estoup
    noticed a pattern in the frequencies of words that he painstakingly counted by
    hand across many documents (thank goodness for computers and `Python`). In the
    1930s, the American linguist George Kingsley Zipf sought to formalize Estoup’s
    observation, and this relationship eventually came to bear Zipf’s name.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在20世纪初，法国速记员让-巴蒂斯特·埃斯图普（Jean-Baptiste Estoup）注意到他费力手动计算的许多文件中单词频率的模式（感谢计算机和`Python`）。20世纪30年代，美国语言学家乔治·金斯利·齐普夫试图正式化埃斯图普的观察，并最终这种关系以齐普夫的名字命名。
- en: Zipf’s law states that given some corpus of natural language utterances, the
    frequency of any word is inversely proportional to its rank in the frequency table.
  id: totrans-200
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Zipf定律指出，在给定自然语言话语语料库的情况下，任何单词的频率与其在频率表中的排名成反比。
- en: — Wikipedia
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: —— 维基百科
- en: Zipf`s Law [https://en.wikipedia.org/wiki/Zipfs_law](wiki.html)
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: Zipf定律 [https://en.wikipedia.org/wiki/Zipfs_law](wiki.html)
- en: Specifically, *inverse proportionality* refers to a situation where an item
    in a ranked list will appear with a frequency tied explicitly to its rank in the
    list. The first item in the ranked list will appear twice as often as the second,
    and three times as often as the third, for example. One of the quick things you
    can do with any corpus or document is plot the frequencies of word usages relative
    to their rank (in frequency). If you see any outliers that don’t fall along a
    straight line in a log-log plot, it may be worth investigating.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，*反比例* 指的是在排名列表中，项目将以与其在列表中的排名直接相关的频率出现。例如，排名列表中的第一项将出现两次，比第二项多三倍，依此类推。您可以对任何语料库或文档进行的一个快速处理是绘制单词使用频率相对于其（频率）排名的图表。如果在对数-对数图中看到不符合直线的任何异常值，可能值得调查。
- en: 'As an example of how far Zipf’s Law stretches beyond the world of words, figure
    3.6 charts the relationship between the population of US cities and the rank of
    that population. It turns out that Zipf’s Law applies to counts of lots of things.
    Nature is full of systems that experience exponential growth and "network effects"
    like population dynamics, economic output, and resource distribution.^([[15](#_footnotedef_15
    "View footnote.")]) It’s interesting that something as simple as Zipf’s Law could
    hold true across a wide range of natural and manmade phenomena. Nobel Laureate
    Paul Krugman, speaking about economic models and Zipf’s Law, put it succinctly:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 作为Zipf定律延伸至词语以外领域的例子，图3.6描绘了美国城市人口与排名之间的关系。事实证明，Zipf定律适用于许多事物的计数。自然界充满了经历指数增长和"网络效应"的系统，如人口动态、经济产出和资源分配^([[15](#_footnotedef_15
    "查看脚注。")])。有趣的是，像Zipf定律这样简单的东西能够在广泛的自然和人造现象中成立。诺贝尔奖得主保罗·克鲁格曼在谈论经济模型和Zipf定律时，简洁地表达了这一点：
- en: '*The usual complaint about economic theory is that our models are oversimplified — that
    they offer excessively neat views of complex, messy reality. [With Zipf’s law]
    the reverse is true: You have complex, messy models, yet reality is startlingly
    neat and simple.*'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '*关于经济理论的常见抱怨是，我们的模型过于简化 — 它们提供了对复杂混乱现实过度整洁的观点。 [使用Zipf定律] 反之亦然：你有复杂混乱的模型，然而现实却惊人地整洁和简单。*'
- en: Here is an updated version of Krugman’s city population plot:^([[16](#_footnotedef_16
    "View footnote.")])
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这是克鲁格曼城市人口图的更新版本:^([[16](#_footnotedef_16 "查看脚注。")])
- en: Figure 3.4 City population distribution
  id: totrans-207
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.4 城市人口分布
- en: '![log pop from wikipedia](images/log_pop_from_wikipedia.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![来自维基百科的对数人口](images/log_pop_from_wikipedia.png)'
- en: As with cities and social networks, so with words. Let’s first download the
    Brown Corpus from NLTK.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 与城市和社交网络一样，单词也是如此。让我们首先从NLTK下载布朗语料库。
- en: The Brown Corpus was the first million-word electronic corpus of English, created
    in 1961 at Brown University. This corpus contains text from 500 sources, and the
    sources have been categorized by genre, such as news, editorial, and so on.^([[17](#_footnotedef_17
    "View footnote.")])
  id: totrans-210
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 布朗语料库是1961年在布朗大学创建的第一个百万字英文电子语料库。该语料库包含来自500个来源的文本，这些来源已按体裁分类，如新闻、社论等^([[17](#_footnotedef_17
    "查看脚注。")])。
- en: — NLTK Documentation
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: —— NLTK 文档
- en: '[PRE41]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: So with over 1 million tokens, you have something meaty to look at.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，拥有超过100万个标记，您有一些值得关注的东西。
- en: '[PRE42]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: A quick glance shows that the word frequencies in the Brown corpus follow the
    logarithmic relationship Zipf predicted. "The" (rank 1 in term frequency) occurs
    roughly twice as often as "of" (rank 2 in term frequency), and roughly three times
    as often as "and" (rank 3 in term frequency). If you don’t believe us, use the
    example code ([https://gitlab.com/tangibleai/nlpia2/-/blob/main/src/nlpia2/ch03/ch03_zipf.py](ch03.html))
    in the `nlpia` package to see this yourself.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 快速浏览显示，Brown 语料库中的词频遵循了 Zipf 预测的对数关系。 "The"（在词频中排名第1）出现的次数大约是 "of"（在词频中排名第2）的两倍，大约是
    "and"（在词频中排名第3）的三倍。如果你不相信我们，可以使用示例代码（[ch03.html](https://gitlab.com/tangibleai/nlpia2/-/blob/main/src/nlpia2/ch03/ch03_zipf.py)）中的代码来亲自验证这一点。
- en: In short, if you rank the words of a corpus by the number of occurrences and
    list them in descending order, you’ll find that, for a sufficiently large sample,
    the first word in that ranked list is twice as likely to occur in the corpus as
    the second word in the list. And it is four times as likely to appear as the fourth
    word on the list. So given a large corpus, you can use this breakdown to say statistically
    how likely a given word is to appear in any given document of that corpus.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，如果你按照语料库中单词的出现次数对它们进行排名，并按降序列出它们，你会发现，对于足够大的样本，排名列表中的第一个单词在语料库中出现的可能性是第二个单词的两倍。它在列表中出现的可能性是第四个单词的四倍。因此，给定一个大语料库，你可以使用这个分解来统计地说出一个给定单词在该语料库的任何给定文档中出现的可能性有多大。
- en: 3.4 Inverse Document Frequency (IDF)
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 逆文档频率（IDF）
- en: Now back to your document vectors. Word counts and *n*-gram counts are useful,
    but pure word count, even when normalized by the length of the document, doesn’t
    tell you much about the importance of that word in that document *relative* to
    the rest of the documents in the corpus. If you could suss out that information,
    you could start to describe documents within the corpus. Say you have a corpus
    of every book about artificial intelligence (AI) ever written. "Intelligence"
    would almost surely occur many times in every book (document) you counted, but
    that doesn’t provide any new information, it doesn’t help distinguish between
    those documents. Whereas something like "neural network" or "conversational engine"
    might not be so prevalent across the entire corpus, but for the documents where
    it frequently occurred, you would know more about their nature. For this, you
    need another tool.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在回到你的文档向量。单词计数和*n*-gram 计数很有用，但纯单词计数，即使将其归一化为文档的长度，也不能告诉你有关该单词在该文档中*相对于*语料库中其他文档的重要性的多少。如果你能搞清楚这些信息，你就可以开始描述语料库中的文档了。假设你有一个关于人工智能（AI）的每本书的语料库。"Intelligence"
    几乎肯定会在你计算的每一本书（文档）中出现多次，但这并没有提供任何新信息，它并不能帮助区分这些文档。而像 "neural network" 或 "conversational
    engine" 这样的东西可能在整个语料库中并不那么普遍，但对于频繁出现的文档，你会更多地了解它们的性质。为此，你需要另一种工具。
- en: '*Inverse document frequency*, or IDF, is your window through Zipf in topic
    analysis. Let’s take your term frequency counter from earlier and expand on it.
    You can count tokens and bin them up two ways: per document and across the entire
    corpus. You’re going to be counting just by document.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '*逆文档频率*，或 IDF，是你通过 Zipf 进行主题分析的窗口。让我们拿你之前的词频计数器来扩展一下。你可以计数令牌并将它们分成两种方式：按文档和整个语料库。你将只按文档计数。'
- en: Let’s return to the Algorithmic Bias example from Wikipedia and grab another
    section (that deals with algorithmic racial and ethnic discrimination) and say
    it is the second document in your Bias corpus.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们返回维基百科中的算法偏见示例，并抓取另一个部分（涉及算法种族和民族歧视），假设它是你偏见语料库中的第二个文档。
- en: Algorithms have been criticized as a method for obscuring racial prejudices
    in decision-making. Because of how certain races and ethnic groups were treated
    in the past, data can often contain hidden biases. For example, black people are
    likely to receive longer sentences than white people who committed the same crime.
    This could potentially mean that a system amplifies the original biases in the
    data.
  id: totrans-221
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 算法被批评为是一种掩盖决策中种族偏见的方法。由于过去某些种族和民族群体的对待方式，数据往往会包含隐藏的偏见。例如，黑人可能会比犯同样罪行的白人接受更长的刑期。这可能意味着系统放大了数据中原有的偏见。
- en: ''
  id: totrans-222
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: …​
  id: totrans-223
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: …​
- en: ''
  id: totrans-224
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A study conducted by researchers at UC Berkeley in November 2019 revealed that
    mortgage algorithms have been discriminatory towards Latino and African Americans
    which discriminated against minorities based on "creditworthiness" which is rooted
    in the U.S. fair-lending law which allows lenders to use measures of identification
    to determine if an individual is worthy of receiving loans. These particular algorithms
    were present in FinTech companies and were shown to discriminate against minorities.
  id: totrans-225
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 2019年11月，加州大学伯克利分校的研究人员进行的一项研究揭示，抵押贷款算法在对待拉丁裔和非洲裔美国人方面存在歧视，这种歧视是基于“信用价值”的，这是美国公平借贷法的根源，该法允许贷方使用身份识别措施来确定一个人是否值得获得贷款。这些特定的算法存在于金融科技公司中，并被证明对少数族裔进行了歧视。
- en: — Wikipedia
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: — 维基百科
- en: 'Algorithmic Bias: Racial and ethnic discrimination ([https://en.wikipedia.org/wiki/Algorithmic_bias#Racial_and_ethnic_discrimination](wiki.html)'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 算法偏见：种族和族裔歧视 ([https://en.wikipedia.org/wiki/Algorithmic_bias#Racial_and_ethnic_discrimination](wiki.html)
- en: 'First let’s get the total word count for each document in your corpus:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们得到语料库中每个文档的总词数：
- en: '[PRE43]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Now with a couple of tokenized documents about bias in hand, let’s look at the
    term frequency of the term "bias" in each document. You will store the TFs you
    find in two dictionaries, one for each document.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，拿到几个关于偏见的tokenized文档，让我们看看每个文档中术语“偏见”的频率。您将把找到的TF存储在两个字典中，每个文档一个。
- en: '[PRE44]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Okay, you have a number eight times as large as the other. Is the intro section
    eight times as much about bias? No, not really. So you need to dig a little deeper.
    First, check out how those numbers compare to the scores for some other word,
    say the word "and".
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，你得到了一个比另一个大八倍的数字。那么“介绍”部分关于偏见多八倍？实际上不是。所以你需要深入挖掘一下。首先，看看这些数字与其他一些词的得分比较，比如词"和"。
- en: '[PRE45]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Great! You know both of these documents are about "and" just as much as they
    are about "bias" - actually, the discrimination chapter is more about "and" than
    about "bias"! Oh, wait.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！你知道这两个文档关于“和”和“偏见”一样多——实际上，歧视章节更多地涉及“和”。哦，等等。
- en: 'A good way to think of a term’s inverse document frequency is this: How surprising
    is it that this token is in this document? The concept of measuring the surprise
    in a token might not sound like a very mathematical idea. However, in statistics,
    physics and information theory, the surprise of a symbol is used to measure its
    *entropy* or information content. And that is exactly what you need to gage the
    importance of a particular word. If a term appears in one document a lot of times,
    but occurs rarely in the rest of the corpus, it is a word that distinguishes that
    document’s meaning from the other documents. This'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 一个衡量术语的逆文档频率的好方法是：这个标记在这个文档中是多么令人惊讶？在统计学、物理学和信息论中，衡量标记的惊讶程度用来衡量其*熵*或信息内容。这正是你需要衡量特定词的重要性的方式。如果一个术语在一个文档中出现了很多次，但在整个语料库中很少出现，那么它是将该文档的含义与其他文档区分开的词。这
- en: 'A term’s IDF is merely the ratio of the total number of documents to the number
    of documents the term appears in. In the case of "and" and "bias" in your current
    example, the answer is the same for both:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 一个术语的IDF只是文档总数与术语出现的文档数之比。在当前示例中，对于“和”和“偏见”，答案是相同的：
- en: '[PRE46]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Not very interesting. So let’s look at another word "black".
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 不是很有趣。所以我们来看另一个单词“黑色”。
- en: '[PRE47]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Okay, that’s something different. Let’s use this "rarity" measure to weight
    the term frequencies.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，这是另一回事了。让我们使用这个“稀有性”度量来加权词频。
- en: '[PRE48]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'And let’s grab the TF of "black" in the two documents:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 然后让我们获取两个文档中“黑色”的词频：
- en: '[PRE49]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'And finally, the IDF for all three. You’ll store the IDFs in dictionaries per
    document like you did with TF:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，三者的IDF。你将像之前的TF一样将IDF存储在每个文档的字典中：
- en: '[PRE50]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'And then for the intro document you find:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在引言文档中找到：
- en: '[PRE51]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'And then for the history document:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 然后看历史文件：
- en: '[PRE52]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 3.4.1 Return of Zipf
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.1 兹普夫的回归
- en: 'You’re almost there. Let’s say, though, you have a corpus of 1 million documents
    (maybe you’re baby-Google), and someone searches for the word "cat", and in your
    1 million documents you have exactly 1 document that contains the word "cat".
    The raw IDF of this is:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 差一点了。假设你有一个包含100万个文件的语料库（也许你是baby-Google），有人搜索词“猫”，而在你的100万个文件中只有1个包含词“猫”的文件。这个的原始IDF是：
- en: 1,000,000 / 1 = 1,000,000
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 1,000,000 / 1 = 1,000,000
- en: 'Let’s imagine you have 10 documents with the word "dog" in them. Your IDF for
    "dog" is:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设您有 10 个文档中都含有单词"狗"。您的"狗"的逆文档频率(idf)为：
- en: 1,000,000 / 10 = 100,000
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 1,000,000 / 10 = 100,000
- en: That’s a big difference. Your friend Zipf would say that’s **too** big because
    it’s likely to happen a lot. Zipf’s Law showed that when you compare the frequencies
    of two words, like "cat" and "dog", even if they occur a similar number of times
    the more frequent word will have an exponentially higher frequency than the less
    frequent one. So Zipf’s Law suggests that you scale all your word frequencies
    (and document frequencies) with the `log()` function, the inverse of `exp()`.
    This ensures that words with similar counts, such as "cat" and "dog", aren’t vastly
    different in frequency. And this distribution of word frequencies will ensure
    that your TF-IDF scores are more uniformly distributed. So you should redefine
    IDF to be the log of the original probability of that word occurring in one of
    your documents. You’ll also want to take the log of the term frequency as well.^([[18](#_footnotedef_18
    "View footnote.")])
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个很大的区别。您的朋友齐普夫可能会说这太大了，因为它可能经常发生。齐普夫定律表明，当您比较两个单词的频率时，例如"猫"和"狗"，即使它们出现的次数相似，频率更高的单词的频率也将比频率较低的单词高得多。因此，齐普夫定律建议您使用`log()`函数的逆函数`exp()`来缩放所有单词频率（和文档频率）。这确保了具有相似计数的单词，例如"猫"和"狗"，在频率上不会有很大差异。这种单词频率的分布将确保您的TF-IDF分数更加均匀分布。因此，您应该重新定义IDF为该单词在您的文档中出现的原始概率的对数。您还需要对术语频率取对数。
- en: 'The base of log function is not important, since you just want to make the
    frequency distribution uniform, not to scale it within a particular numerical
    range.^([[19](#_footnotedef_19 "View footnote.")]) If you use a base 10 log function,
    you’ll get:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 对数函数的底数并不重要，因为您只是想使频率分布均匀，而不是在特定数值范围内缩放它。如果使用底数为10的对数函数，您将获得：
- en: 'search: cat'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索：猫
- en: Equation 3.3
  id: totrans-258
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 3.3
- en: \[\begin{equation} \text{idf} = \log \left(\text{1,000,000} / 1 \right) = 6
    \end{equation}\]
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation} \text{idf} = \log \left(\text{1,000,000} / 1 \right) = 6
    \end{equation}\]
- en: 'search: dog'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索：狗
- en: Equation 3.4
  id: totrans-261
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 3.4
- en: \[\begin{equation} \text{idf} = \log \left(\text{1,000,000} / 10 \right) = 5
    \end{equation}\]
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation} \text{idf} = \log \left(\text{1,000,000} / 10 \right) = 5
    \end{equation}\]
- en: So now you’re weighting the TF results of each more appropriately to their occurrences
    in language, in general.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 所以现在您更适当地加权了每个TF的结果，以符合它们在语言中的出现次数。
- en: 'And then finally, for a given term, *t*, in a given document, *d*, in a corpus,
    *D*, you get:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，对于语料库*D*中给定文档*d*中的给定术语*t*，您得到：
- en: Equation 3.5
  id: totrans-265
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 3.5
- en: \[\begin{equation} \text{tf}\left(t, d\right) = \frac{\text{count}(t)}{\text{count}(d)}
    \end{equation}\]
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation} \text{tf}\left(t, d\right) = \frac{\text{count}(t)}{\text{count}(d)}
    \end{equation}\]
- en: Equation 3.6
  id: totrans-267
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 3.6
- en: \[\begin{equation} \text{idf}\left(t,D\right) = \log \left(\frac{\text{number
    of documents}}{\text{number of documents containing t}}\right) \end{equation}\]
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation} \text{idf}\left(t,D\right) = \log \left(\frac{\text{文档数量}}{\text{包含术语t的文档数量}}\right)
    \end{equation}\]
- en: Equation 3.7
  id: totrans-269
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 3.7
- en: \[\begin{equation} \text{tfidf}\left(t,d,D\right) = \text{tf}(t,d) \ast \text{idf}(t,D)
    \end{equation}\]
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation} \text{tfidf}\left(t,d,D\right) = \text{tf}(t,d) \ast \text{idf}(t,D)
    \end{equation}\]
- en: The more times a word appears in the document, the TF (and hence the TF-IDF)
    will go up. At the same time, as the number of documents that contain that word
    goes up, the IDF (and hence the TF-IDF) for that word will go down. So now, you
    have a number. Something your computer can chew on. But what is it exactly? It
    relates a specific word or token to a specific document in a specific corpus,
    and then it assigns a numeric value to the importance of that word in the given
    document, given its usage across the entire corpus.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 单词在文档中出现的次数越多，TF（因此 TF-IDF）就会增加。同时，随着包含该单词的文档数的增加，该单词的IDF（因此 TF-IDF）就会降低。所以现在，你有了一个数字。这是你的计算机可以处理的东西。但它到底是什么呢？它将特定单词或令牌与特定语料库中的特定文档相关联，然后将数值分配给该单词在给定文档中的重要性，考虑到其在整个语料库中的使用情况。
- en: 'In some classes, all the calculations will be done in log space so that multiplications
    become additions and division becomes subtraction:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些课程中，所有的计算都将在对数空间中进行，以便乘法变为加法，除法变为减法：
- en: '[PRE53]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: This single number, the TF-IDF score, is the humble foundation of all search
    engines. Now that you’ve been able to transform words and documents into numbers
    and vectors, it is time for some Python to put all those numbers to work. You
    won’t likely ever have to implement the TF-IDF formulas from scratch, because
    these algorithms are already implemented for you in many software libraries. You
    don’t need to be an expert at linear algebra to understand NLP, but it sure can
    boost your confidence if you have a mental model of the math that goes into a
    number like a TF-IDF score. If you understand the math, you can confidently tweak
    it for your application and perhaps even help an open-source project improve its
    NLP algorithms.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 这个单一的数字，即 TF-IDF 分数，是所有搜索引擎的谦逊基础。现在你已经能够将单词和文档转换为数字和向量，是时候用一些 Python 来让所有这些数字发挥作用了。你可能永远不会需要从头实现
    TF-IDF 公式，因为这些算法已经在许多软件库中为你实现了。你不需要成为线性代数的专家来理解自然语言处理，但如果你对生成像 TF-IDF 分数这样的数字的数学有一个心理模型，那肯定能提高你的信心。如果你理解了数学，你可以自信地为你的应用调整它，甚至可以帮助一个开源项目改进它的自然语言处理算法。
- en: 3.4.2 Relevance ranking
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.2 相关性排名
- en: As you saw earlier, you can easily compare two vectors and get their similarity,
    but you have since learned that merely counting words isn’t as effective as using
    their TF-IDF values. So in each document vector you want to replace each word’s
    count with its TF-IDF value (score). Now your vectors will more thoroughly reflect
    the meaning, or topic, of the document.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你之前看到的，你可以轻松比较两个向量并获得它们的相似性，但是你已经学到了，仅仅计算单词不如使用它们的 TF-IDF 值有效。因此，在每个文档向量中，你希望用单词的
    TF-IDF 值（分数）替换每个单词的计数。现在你的向量将更全面地反映文档的意思或主题。
- en: When you use a search engine such as MetaGer.org, Duck.com or You.com, the list
    of 10 or so search results is carefully crafted from TF-IDF vectors for each of
    those pages. If you think about it, it is quite amazing that an algorithm is able
    to give you 10 pages that almost always contain an important piece of information
    you are looking for. After all, there are billions of web pages for the search
    engine to choose from. How is that possible? Under the hood, all search engines
    start by computing the similarity between the TF-IDF vector for a query with the
    TF-IDF vector for the billions of web pages in their database. This similarity
    to your query is often called *relevance*. Here’s how you can rank any documents
    by relevance.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用像 MetaGer.org、Duck.com 或 You.com 这样的搜索引擎时，10 多个搜索结果列表是从这些页面的 TF-IDF 向量中精心制作出来的。如果你想一想，一个算法能够给你几乎总是包含你正在寻找的重要信息的
    10 个页面，这是相当了不起的。毕竟，搜索引擎可以从数十亿个网页中选择。这是怎么可能的？在幕后，所有搜索引擎都是通过计算查询的 TF-IDF 向量与其数据库中数十亿个网页的
    TF-IDF 向量之间的相似度来开始的。这种与你的查询的相似度通常被称为*相关性*。以下是你如何通过相关性对任何文档进行排名。
- en: '[PRE54]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: With this setup, you have K-dimensional vector representation of each document
    in the corpus. And now on to the hunt! Or search, in your case. From the previous
    section, you might remember how we defined similarity between vectors. Two vectors
    are considered similar if their cosine similarity is high, so you can find two
    similar vectors near each other if they maximize the cosine similarity.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个设置，你在语料库中的每个文档都有一个 K 维向量表示。现在开始猎杀吧！或者说搜索，在你的情况下。从前一节中，你可能还记得我们如何定义向量之间的相似性。如果两个向量的余弦相似性高，则认为它们相似，因此如果它们最大化余弦相似性，你可以找到两个相似的向量靠近彼此。
- en: Now you have all you need to do a basic TF-IDF-based search. You can treat the
    search query itself as a document, and therefore get a TF-IDF-based vector representation
    of it. The last step is then to find the documents whose vectors have the highest
    cosine similarities to the query and return those as the search results.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经有了进行基于 TF-IDF 的基本搜索所需的一切。你可以将搜索查询本身视为一个文档，并因此获得其基于 TF-IDF 的向量表示。然后，最后一步是找到与查询具有最高余弦相似度的文档，并将它们作为搜索结果返回。
- en: 'If you take your three documents about Harry, and make the query "How long
    does it take to get to the store?":'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你拿出关于哈利的三个文档，并提出查询“到商店需要多长时间？”：
- en: '[PRE55]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: You can safely say document 0 has the most relevance for your query! And with
    this, you can find relevant documents amidst any corpus, be it articles in Wikipedia,
    books from Project Gutenberg, or toots on ActivityPub (Mastodon). Google look
    out!
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以放心地说文档 0 对你的查询最具相关性！有了这个，你可以在任何语料库中找到相关的文档，无论是维基百科的文章、古登堡计划的书籍，还是 ActivityPub（Mastodon）上的
    toots。谷歌小心了！
- en: Actually, Google’s search engine is safe from competition from us. You have
    to do an "index scan" of your TF-IDF vectors with each query. That’s an \(O(N)\)
    algorithm. Most search engines can respond in constant time (\(O(1)\)) because
    they use an *inverted index*.^([[20](#_footnotedef_20 "View footnote.")]) You
    aren’t going to implement an index that can find these matches in constant time
    here, but if you’re interested you might like exploring the state-of-the-art Python
    implementation in the `Whoosh` ^([[21](#_footnotedef_21 "View footnote.")]) package
    and its source code.^([[22](#_footnotedef_22 "View footnote.")])
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，谷歌的搜索引擎不会受到我们的竞争威胁。你必须对每个查询的 TF-IDF 向量进行“索引扫描”。这是一个 \(O(N)\) 算法。大多数搜索引擎可以在常数时间
    (\(O(1)\)) 内响应，因为它们使用了一个*倒排索引*。^([[20](#_footnotedef_20 "查看脚注。")]) 你不会在这里实现一个能够在常数时间内找到这些匹配项的索引，但如果你感兴趣，你可能会喜欢探索
    `Whoosh` ^([[21](#_footnotedef_21 "查看脚注。")]) 包中的最先进的 Python 实现及其源代码。^([[22](#_footnotedef_22
    "查看脚注。")])
- en: Tip
  id: totrans-285
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: In the preceding code, you dropped the keys that were not found in your pipeline’s
    lexicon (vocabulary) to avoid a divide-by-zero error. But a better approach is
    to +1 the denominator of every IDF calculation, which ensures no denominators
    are zero. In fact this approach is so common it has a name, *additive smoothing*
    or "Laplace smoothing" ^([[23](#_footnotedef_23 "View footnote.")]) — will usually
    improve the search results for TF-IDF keyword-based searches.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述代码中，你删除了在管道词汇表中找不到的键，以避免除零错误。但更好的方法是对每个 IDF 计算的分母加 1，以确保没有分母为零。实际上，这种方法非常常见，有一个名字叫做*加法平滑*或"Laplace
    平滑"^([[23](#_footnotedef_23 "查看脚注。")]) — 通常会改善基于 TF-IDF 关键词搜索的搜索结果。
- en: 3.4.3 Another vectorizer
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.3 另一种向量化器
- en: Now that was a lot of code for things that have long since been automated. The
    `sklearn` package you used at the beginning of this chapter has a tool for TF-IDF
    too. Just as `CountVectorizer` you saw previously, it does tokenization, omits
    punctuation, and computes the tf-idf scores all in one.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这是很多代码，但这些早已自动化。你在本章开头使用的 `sklearn` 包也有一个用于 TF-IDF 的工具。就像你之前看到的 `CountVectorizer`
    一样，它进行标记化，省略标点，并一次性计算 tf-idf 分数。
- en: Here’s how you can use `sklearn` to build a TF-IDF matrix. The syntax is almost
    exactly the same as for `CountVectorizer`.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何使用 `sklearn` 构建 TF-IDF 矩阵的方法。语法几乎与 `CountVectorizer` 完全相同。
- en: Listing 3.6 Computing TF-IDF matrix using Scikit-Learn
  id: totrans-290
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.6 使用 Scikit-Learn 计算 TF-IDF 矩阵
- en: '[PRE56]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: With Scikit-Learn, in four lines of code, you created a matrix of your three
    documents and the inverse document frequency for each term in the lexicon. It’s
    very similar to the matrix you got from `CountVectorizer`, only this time it contains
    TF-IDF of each term, token, or word in your lexicon make up the columns of the
    matrix. On large texts this or some other pre-optimized TF-IDF model will save
    you scads of work.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Scikit-Learn，只需四行代码，你就创建了一个矩阵，其中包含你的三个文档和词汇表中每个词的逆文档频率。它与之前从 `CountVectorizer`
    得到的矩阵非常相似，只是这次它包含了词汇表中每个术语、标记或单词的 TF-IDF，构成了矩阵的列。在大型文本中，这种或其他一些预优化的 TF-IDF 模型将为你节省大量工作。
- en: 3.4.4 Alternatives
  id: totrans-293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.4 替代方案
- en: TF-IDF matrices (term-document matrices) have been the mainstay of information
    retrieval (search) for decades. As a result, researchers and corporations have
    spent a lot of time trying to optimize that IDF part to try to improve the relevance
    of search results. [3.1](#alternative_tfidf_normalizations_table) lists some of
    the ways you can normalize and smooth your term frequency weights.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 几十年来，TF-IDF 矩阵（术语-文档矩阵）一直是信息检索（搜索）的主要方法。因此，研究人员和公司花费了大量时间来优化 IDF 部分，以尝试改善搜索结果的相关性。[3.1](#alternative_tfidf_normalizations_table)
    列出了一些你可以规范化和平滑化术语频率权重的方法。
- en: Table 3.1 Alternative TF-IDF normalization approaches (Molino 2017) ^([[24](#_footnotedef_24
    "View footnote.")])
  id: totrans-295
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 3.1 替代 TF-IDF 规范化方法（Molino 2017）^([[24](#_footnotedef_24 "查看脚注。")])
- en: '| Scheme | Definition |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 方案 | 定义 |'
- en: '| None | ![table equation 1](images/table_equation_1.png) |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 无 | ![table equation 1](images/table_equation_1.png) |'
- en: '| TD-IDF | ![table equation 2](images/table_equation_2.png) |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| TD-IDF | ![table equation 2](images/table_equation_2.png) |'
- en: '| TF-ICF | ![table equation 3](images/table_equation_3.png) |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| TF-ICF | ![table equation 3](images/table_equation_3.png) |'
- en: '| Okapi BM25 | ![table equation 4](images/table_equation_4.png) |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| Okapi BM25 | ![table equation 4](images/table_equation_4.png) |'
- en: '| ATC | ![table equation 5](images/table_equation_5.png) |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| ATC | ![table equation 5](images/table_equation_5.png) |'
- en: '| LTU | ![table equation 6](images/table_equation_6.png) |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| LTU | ![table equation 6](images/table_equation_6.png) |'
- en: '| MI | ![table equation 7](images/table_equation_7.png) |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| MI | ![table equation 7](images/table_equation_7.png) |'
- en: '| PosMI | ![table equation 8](images/table_equation_8.png) |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| PosMI | ![表方程式8](images/table_equation_8.png) |'
- en: '| T-Test | ![table equation 9](images/table_equation_9.png) |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| T-Test | ![表方程式9](images/table_equation_9.png) |'
- en: '| chi² | See section 4.3.5 of *From Distributional to Semantic Similarity*
    ([https://www.era.lib.ed.ac.uk/bitstream/handle/1842/563/IP030023.pdf#subsection.4.3.5](563.html))
    by James Richard Curran |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 卡方检验 | 见 *从分布到语义相似性* ([https://www.era.lib.ed.ac.uk/bitstream/handle/1842/563/IP030023.pdf#subsection.4.3.5](563.html))，作者詹姆斯·理查德·柯兰
    |'
- en: '| Lin98a | ![table equation 10](images/table_equation_10.png) |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| Lin98a | ![表方程式10](images/table_equation_10.png) |'
- en: '| Lin98b | ![table equation 11](images/table_equation_11.png) |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| Lin98b | ![表方程式11](images/table_equation_11.png) |'
- en: '| Gref94 | ![table equation 12](images/table_equation_12.png) |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| Gref94 | ![表方程式12](images/table_equation_12.png) |'
- en: Search engines (information retrieval systems) match keywords (term) between
    queries and documents in a corpus. If you’re building a search engine and want
    to provide documents that are likely to match what your users are looking for,
    you should spend some time investigating the alternatives described by Piero Molino
    in figure 3.7.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索引擎（信息检索系统）在语料库中匹配查询和文档之间的关键词（术语）。如果您正在构建一个搜索引擎，并希望提供可能与用户所寻找内容匹配的文档，您应该花一些时间研究皮耶罗·莫利诺在图3.7中描述的替代方案。
- en: One such alternative to using straight TF-IDF cosine distance to rank query
    results is Okapi BM25, or its most recent variant, BM25F.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 用于对查询结果进行排名的另一种替代方法是Okapi BM25，或者其最新的变体BM25F。
- en: 3.4.5 Okapi BM25
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.5 Okapi BM25
- en: The smart people at London’s City University came up with a better way to rank
    search results. Rather than merely computing the TF-IDF cosine similarity, they
    normalize and smooth the similarity. They also ignore duplicate terms in the query
    document, effectively clipping the term frequencies for the query vector at 1\.
    And the dot product for the cosine similarity is not normalized by the TF-IDF
    vector norms (number of terms in the document and the query), but rather by a
    nonlinear function of the document length itself.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 伦敦城市大学的聪明人提出了一种更好的方法来对搜索结果进行排名。他们不仅仅计算TF-IDF余弦相似度，还对相似度进行归一化和平滑处理。他们还忽略了查询文档中的重复术语，有效地将查询向量的术语频率剪切为1。余弦相似度的点积不是由TF-IDF向量的规范化（文档和查询中的术语数）来规范化的，而是由文档长度本身的非线性函数来规范化的。
- en: '[PRE57]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: You can optimize your pipeline by choosing the weighting scheme that gives your
    users the most relevant results. But if your corpus isn’t too large, you might
    consider forging ahead with us into even more useful and accurate representations
    of the meaning of words and documents.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 通过选择使用户获得最相关结果的加权方案，您可以优化您的管道。但是，如果您的语料库不太大，您可能会考虑进一步前进，以获得更有用和准确的单词和文档含义的表示。
- en: 3.5 Using TF-IDF for your bot
  id: totrans-316
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.5 使用TF-IDF为您的机器人
- en: In this chapter, you learned how TF-IDF can be used to represent natural language
    documents with vectors, find similarities between them, and perform keyword search.
    But if you want to build a chatbot, how can you use those capabilities to make
    your first intelligent assistant?
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您学习了如何使用TF-IDF来表示自然语言文档，并找到它们之间的相似性，并执行关键字搜索。但是，如果您想构建一个聊天机器人，您该如何利用这些功能来制作您的第一个智能助手？
- en: Actually, many chatbots rely heavily on a search engine. And some chatbots use
    their search engine as their only algorithm for generating responses. You just
    need to take one additional step to turn your simple search index (TF-IDF) into
    a chatbot. To make this book as practical as possible, every chapter will show
    you how to make your bot smarter using the skills you picked up in that chapter.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，许多聊天机器人严重依赖搜索引擎。一些聊天机器人使用他们的搜索引擎作为生成响应的唯一算法。您只需要额外采取一步，将您的简单搜索索引（TF-IDF）转换为聊天机器人即可。为了使本书尽可能实用，每一章都将向您展示如何使用您在该章中掌握的技能使您的机器人更智能。
- en: 'In this chapter, you’re going to make your chatbot answer data science questions.
    The trick is simple: you’re store your training data in pairs of questions and
    appropriate responses. Then you can use TF-IDF to search for a question most similar
    to the user input text. Instead of returning the most similar statement in your
    database, you return the response associated with that statement. And with that,
    you’re chatting!'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将使您的聊天机器人回答数据科学问题。诀窍很简单：您将您的训练数据存储为问题和相应响应的对。然后，您可以使用TF-IDF搜索与用户输入文本最相似的问题。而不是返回数据库中最相似的语句，您返回与该语句相关联的响应。然后，您就可以聊天了！
- en: 'Let’s do it step by step. First, let’s load our data. You’ll use the corpus
    of data science questions that Hobson was asked by his mentees in the last few
    years. They are located in the `qary` repository:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步步来。首先，让我们加载我们的数据。你将使用 Hobson 的学生在过去几年中问他的数据科学问题的语料库。它们位于 `qary` 存储库中：
- en: '[PRE58]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Next, let’s create TF-IDF vectors for the questions in our dataset. You’ll use
    the Scikit-Learn TfidfVectorizer class you’ve seen in the previous section.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们为数据集中的问题创建 TF-IDF 向量。你将使用前一节中看到的 Scikit-Learn TfidfVectorizer 类。
- en: '[PRE59]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: We’re now ready to implement the question-answering itself. Your bot will reply
    to the user’s question by using the same vectorizer you trained on the dataset,
    and finding the most similar questions.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备实现问答功能本身。你的机器人将使用你在数据集上训练的相同向量化器来回答用户的问题，并找到最相似的问题。
- en: '[PRE60]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'And your first question-answering chatbot is ready! Let’s ask it its first
    question:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 你的第一个问答聊天机器人已经准备好了！让我们问它第一个问题：
- en: '[PRE61]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Try to play with it and ask it a couple more questions, such as: - What is
    a Gaussian distribution? - Who came up with the perceptron algorithm?'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试与它玩耍，问它更多的问题，比如：- 什么是高斯分布？- 谁提出了感知器算法？
- en: You’ll realize quickly, however, that your chatbot fails quite often - and not
    just because the dataset you trained it upon is small.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 但你会很快意识到，你的聊天机器人经常失败 - 不仅仅是因为你训练它的数据集很小。
- en: 'For example, let’s try the following question:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们尝试以下问题：
- en: '[PRE62]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: If you looked closely at the dataset, you might have seen it actually has an
    answer about decreasing overfitting for boosting models. However, our vectorizer
    is just a little bit too literal - and when it saw the word "decrease" in the
    wrong question, that caused the dot product to be higher for the wrong question.
    In the next chapter, we’ll see how we can overcome this challenge by looking at
    *meaning* rather than particular words.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细看了数据集，你可能会发现它实际上有一个关于减少提升模型过拟合的答案。然而，我们的向量化器只是有点太字面了 - 当它在错误的问题中看到“减少”一词时，这导致了对错误问题的点积更高。在下一章中，我们将看到如何通过查看*含义*而不是特定单词来克服这一挑战。
- en: 3.6 What’s next
  id: totrans-333
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.6 接下来要做什么
- en: Now that you can convert natural language text to numbers, you can begin to
    manipulate them and compute with them. Numbers firmly in hand, in the next chapter
    you’ll refine those numbers to try to represent the **meaning** or **topic** of
    natural language text instead of just its words. In subsequent chapters, we show
    you how to implement a semantic search engine that finds documents that "mean"
    something similar to the words in your query rather than just documents that use
    those exact words from your query. Semantic search is much better than anything
    TF-IDF weighting and stemming and lemmatization can ever hope to achieve. State-of-the-art
    search engines combine both TF-IDF vectors and semantic embedding vectors to achieve
    both higher accuracy than conventional search.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以将自然语言文本转换为数字了，你可以开始操作它们并计算它们。拿着牢牢的数字，在下一章中，你将对这些数字进行细化，试图代表自然语言文本的*含义*或*主题*，而不仅仅是它的词语。在随后的章节中，我们将向你展示如何实现一个语义搜索引擎，该引擎找到与你查询中的单词“意思”相似的文档，而不仅仅是使用你查询中的这些确切单词的文档。语义搜索比
    TF-IDF 加权和词干提取和词形还原能够实现的任何东西都要好得多。最先进的搜索引擎结合了 TF-IDF 向量和语义嵌入向量，以实现比传统搜索更高的准确性。
- en: The well-funded OpenSearch project, an ElasticSearch fork, is now leading the
    way in search innovation.^([[25](#_footnotedef_25 "View footnote.")]) ElasticSearch
    started walling off their technology garden in 2021\. The only reason Google and
    Bing and other web search engines don’t use the semantic search approach is that
    their corpus is too large. Semantic word and topic vectors don’t scale to billions
    of documents, but millions of documents are no problem. And some scrappy startups
    such as You.com are learning how to use open source to enable semantic search
    and conversational search (chat) on a web scale.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 资金充裕的 OpenSearch 项目，一个 ElasticSearch 的分支，现在正在引领搜索创新之路。[[25](#_footnotedef_25
    "查看脚注。")] ElasticSearch 在 2021 年开始封锁他们的技术花园。Google、Bing 和其他网络搜索引擎之所以不使用语义搜索方法，是因为它们的语料库太大了。语义词和主题向量无法扩展到数十亿个文档，但数百万个文档却没有问题。一些创业公司，比如
    You.com，正在学习如何使用开源技术实现语义搜索和网络规模的对话式搜索（聊天）。
- en: So you only need the most basic TF-IDF vectors to feed into your pipeline to
    get state-of-the-art performance for semantic search, document classification,
    dialog systems, and most of the other applications we mentioned in Chapter 1\.
    TF-IDFs are just the first stage in your pipeline, a basic set of features you’ll
    extract from text. In the next chapter, you will compute topic vectors from your
    TF-IDF vectors. Topic vectors are an even better representation of the meaning
    of a document than these carefully normalized and smoothed TF-IDF vectors. And
    things only get better from there as we move on to Word2vec word vectors in chapter
    6 and deep learning embeddings of the meaning of words and documents in later
    chapters.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: 3.7 Test yourself
  id: totrans-337
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What are the differences between the count vectors that `CountVectorizer.transform()`
    creates and a list of python `collections.Counter` objects? Can you convert them
    to identical `DataFrame` objects?
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you use `TFIDFVectorizer` on a large corpus (more than 1M documents) with
    a huge vocabulary (more than 1M tokens)? What problems do you expect to encounter?
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Think of an example of corpus or task where term frequency (TF) will perform
    better than TF-IDF.
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We mentioned that bag of character n-grams can be used for language recognition
    tasks. How would an algorithm that uses character n-grams to distinguish one language
    from another work?
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the limitations or disadvantages of TF-IDF you have seen throughout
    this chapter? Can you come up with additional ones that weren’t mentioned?
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How would you use TF-IDF as a base to improve how most search engines today
    work?
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 3.8 Summary
  id: totrans-344
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Any web-scale search engine with millisecond response times has the power of
    a TF-IDF term document matrix hidden under the hood.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zipf’s law can help you predict the frequencies of all sorts of things including
    words, characters, and people.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Term frequencies must be weighted by their inverse document frequency to ensure
    the most important, most meaningful words are given the heft they deserve.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bag-of-words / Bag-of-ngrams and TF-IDF are the most basic algorithms to represent
    natural language documents with a vector of real numbers.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Euclidean distance and similarity between pairs of high dimensional vectors
    don’t adequately represent their similarity for most NLP applications.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cosine distance, the amount of "overlap" between vectors, can be calculated
    efficiently just by multiplying the elements of normalized vectors together and
    summing up those products.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cosine distance is the go-to similarity score for most natural language vector
    representations.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[1]](#_footnoteref_1) StackOverflow discussion of whether to rely on this
    feature ( [https://stackoverflow.com/questions/39980323/are-dictionaries-ordered-in-python-3-6/39980744#39980744](are-dictionaries-ordered-in-python-3-6.html))'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: '[[2]](#_footnoteref_2) Scikit-Learn documentation ( [http://scikit-learn.org/](scikit-learn.org.html)).'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '[[3]](#_footnoteref_3) If you would like more details about linear algebra
    and vectors take a look at Appendix C.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '[[3]](#_footnoteref_3) 如果你想要更多关于线性代数和向量的细节，请查看附录 C。'
- en: '[[4]](#_footnoteref_4) "Vectorization and Parallelization" by WZB.eu ( [https://datascience.blog.wzb.eu/2018/02/02/vectorization-and-parallelization-in-python-with-numpy-and-pandas/](vectorization-and-parallelization-in-python-with-numpy-and-pandas.html)).'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '[[4]](#_footnoteref_4) "向量化和并行化" by WZB.eu ([https://datascience.blog.wzb.eu/2018/02/02/vectorization-and-parallelization-in-python-with-numpy-and-pandas/](vectorization-and-parallelization-in-python-with-numpy-and-pandas.html))'
- en: '[[5]](#_footnoteref_5) "Knowledge and Society in Times of Upheaval" ( [https://wzb.eu/en/node/60041](node.html))'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '[[5]](#_footnoteref_5) "动荡时代的知识与社会" ([https://wzb.eu/en/node/60041](node.html))'
- en: '[[6]](#_footnoteref_6) You’d need to use a package like GeoPy (geopy.readthedocs.io)
    to get the math right.'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '[[6]](#_footnoteref_6) 你需要使用类似 GeoPy（geopy.readthedocs.io）的包来确保数学正确。'
- en: '[[7]](#_footnoteref_7) The curse of dimensionality is that vectors will get
    exponentially farther and farther away from one another, in Euclidean distance,
    as the dimensionality increases. A lot of simple operations become impractical
    above 10 or 20 dimensions, like sorting a large list of vectors based on their
    distance from a "query" or "reference" vector (approximate nearest neighbor search).
    To dig deeper, check out Wikipedia’s "Curse of Dimensionality" article ( [https://en.wikipedia.org/wiki/Curse_of_dimensionality](wiki.html)).'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '[[7]](#_footnoteref_7) 维度诅咒是，随着维度的增加，向量在欧几里得距离上会以指数方式远离彼此。许多简单操作在超过10或20个维度时变得不切实际，比如基于它们与“查询”或“参考”向量的距离对大量向量列表进行排序（近似最近邻搜索）。要深入了解，请查看维基百科的“维度诅咒”文章
    ([https://en.wikipedia.org/wiki/Curse_of_dimensionality](wiki.html))。'
- en: '[[8]](#_footnoteref_8) These videos show how to create vectors for words and
    then compute their cosine similarity to each other using SpaCy and numpy ( [https://www.dropbox.com/sh/3p2tt55pqsisy7l/AAB4vwH4hV3S9pUO0n4kTZfGa?dl=0](3p2tt55pqsisy7l.html))'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '[[8]](#_footnoteref_8) 这些视频展示了如何使用 SpaCy 和 numpy 为单词创建向量，然后计算它们之间的余弦相似度 ([https://www.dropbox.com/sh/3p2tt55pqsisy7l/AAB4vwH4hV3S9pUO0n4kTZfGa?dl=0](3p2tt55pqsisy7l.html))'
- en: '[[9]](#_footnoteref_9) If the reference is unfamiliar to you, check out the
    story *Call of Cthulhu* by H.P. Lovecraft: [https://www.hplovecraft.com/writings/texts/fiction/cc.aspx](fiction.html)'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '[[9]](#_footnoteref_9) 如果这个参考对你来说陌生，请查看H.P.洛夫克拉夫特的故事*克苏鲁的呼唤*：[https://www.hplovecraft.com/writings/texts/fiction/cc.aspx](fiction.html)'
- en: '[[10]](#_footnoteref_10) Retrieved on July 9th 2021 from here: [https://en.wikipedia.org/wiki/Machine_learning](wiki.html)'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '[[10]](#_footnoteref_10) 于2021年7月9日从这里检索：[https://en.wikipedia.org/wiki/Machine_learning](wiki.html)'
- en: '[[11]](#_footnoteref_11) The nonprofit MetaGer search engine takes privacy,
    honesty, and ethics seriously unlike the top search engines you’re already familiar
    with ( [https://metager.org/](metager.org.html))'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '[[11]](#_footnoteref_11) 非营利性搜索引擎 MetaGer 严肃对待隐私、诚实和道德，不像你已经熟悉的顶级搜索引擎 ([https://metager.org/](metager.org.html))'
- en: '[[12]](#_footnoteref_12) Wikipedia ROT13 article ( [https://en.wikipedia.org/wiki/ROT13](wiki.html))'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '[[12]](#_footnoteref_12) 维基百科 ROT13 文章 ([https://en.wikipedia.org/wiki/ROT13](wiki.html))'
- en: '[[13]](#_footnoteref_13) Zbwedicon’s YouTube video about the Zen of Python
    ( [https://www.youtube.com/watch?v=i6G6dmVJy74](www.youtube.com.html))'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '[[13]](#_footnoteref_13) Zbwedicon 关于 Python 之禅的 YouTube 视频 ([https://www.youtube.com/watch?v=i6G6dmVJy74](www.youtube.com.html))'
- en: '[[14]](#_footnoteref_14) You can install and import PyDanny’s `that` package
    to have a laugh about Python antipatterns ( [https://pypi.org/project/that](project.html))'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '[[14]](#_footnoteref_14) 你可以安装和导入 PyDanny 的 `that` 包，以便笑笑 Python 的反模式 ([https://pypi.org/project/that](project.html))'
- en: '[[15]](#_footnoteref_15) See the web page titled "There is More than a Power
    Law in Zipf" ( [https://www.nature.com/articles/srep00812](articles.html)).'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '[[15]](#_footnoteref_15) 查看标题为"Zipf 定律之外还有更多"的网页 ([https://www.nature.com/articles/srep00812](articles.html))'
- en: '[[16]](#_footnoteref_16) Population data downloaded from Wikipedia using Pandas.
    See the ``nlpia.book.examples` code on GitHub ( [https://gitlab.com/tangibleai/nlpia2/-/blob/main/src/nlpia2/ch03/ch03_zipf.py](ch03.html))'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '[[16]](#_footnoteref_16) 使用 Pandas 从维基百科下载的人口数据。查看 GitHub 上的 `nlpia.book.examples`
    代码（[https://gitlab.com/tangibleai/nlpia2/-/blob/main/src/nlpia2/ch03/ch03_zipf.py](ch03.html)）'
- en: '[[17]](#_footnoteref_17) For a complete list, see [http://icame.uib.no/brown/bcm-los.html](brown.html)'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '[[17]](#_footnoteref_17) 完整列表请参见 [http://icame.uib.no/brown/bcm-los.html](brown.html)'
- en: '[[18]](#_footnoteref_18) Gerard Salton and Chris Buckley first demonstrated
    the usefulness of log scaling for information retrieval in their paper Term Weighting
    Approaches in Automatic Text Retrieval ( [https://ecommons.cornell.edu/bitstream/handle/1813/6721/87-881.pdf](6721.html)).'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '[[18]](#_footnoteref_18) Gerard Salton 和 Chris Buckley 首次在他们的论文《信息检索中的术语加权方法》中展示了对于信息检索的对数缩放的有用性（[https://ecommons.cornell.edu/bitstream/handle/1813/6721/87-881.pdf](6721.html)）。'
- en: '[[19]](#_footnoteref_19) Later we show you how to normalize the TF-IDF vectors
    after all the TF-IDF values have been calculated using this log scaling.'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '[[19]](#_footnoteref_19) 后面我们会向您展示如何在计算所有 TF-IDF 值后使用此对数缩放来归一化 TF-IDF 向量。'
- en: '[[20]](#_footnoteref_20) See the web page titled "Inverted index - Wikipedia"
    ( [https://en.wikipedia.org/wiki/Inverted_index](wiki.html)).'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '[[20]](#_footnoteref_20) 参见名为“倒排索引 - 维基百科”的网页（[https://en.wikipedia.org/wiki/Inverted_index](wiki.html)）。'
- en: '[[21]](#_footnoteref_21) See the web page titled "Whoosh : PyPI" ( [https://pypi.python.org/pypi/Whoosh](pypi.html)).'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '[[21]](#_footnoteref_21) 参见名为“Whoosh : PyPI”的网页（[https://pypi.python.org/pypi/Whoosh](pypi.html)）。'
- en: '[[22]](#_footnoteref_22) See the web page titled "GitHub - Mplsbeb/whoosh:
    A fast pure-Python search engine" ( [https://github.com/Mplsbeb/whoosh](Mplsbeb.html)).'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '[[22]](#_footnoteref_22) 参见名为“GitHub - Mplsbeb/whoosh: A fast pure-Python search
    engine”的网页（[https://github.com/Mplsbeb/whoosh](Mplsbeb.html)）。'
- en: '[[23]](#_footnoteref_23) See the web page titled "Additive smoothing - Wikipedia"
    ( [https://en.wikipedia.org/wiki/Additive_smoothing](wiki.html)).'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '[[23]](#_footnoteref_23) 参见名为“加法平滑 - 维基百科”的网页（[https://en.wikipedia.org/wiki/Additive_smoothing](wiki.html)）。'
- en: '[[24]](#_footnoteref_24) *Word Embeddings Past, Present and Future* by Piero
    Molino at AI with the Best 2017'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '[[24]](#_footnoteref_24) *Word Embeddings Past, Present and Future*，Piero Molino，于
    AI with the Best 2017'
- en: '[[25]](#_footnoteref_25) "The ABCs of semantic search in OpenSearch" by Milind
    Shyani ( [https://opensearch.org/blog/semantic-science-benchmarks/](semantic-science-benchmarks.html))'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '[[25]](#_footnoteref_25) “OpenSearch 中语义搜索的 ABC” ，Milind Shyani，（[https://opensearch.org/blog/semantic-science-benchmarks/](semantic-science-benchmarks.html)）'
