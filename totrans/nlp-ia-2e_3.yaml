- en: 3 Math with words (TF-IDF vectors)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Counting words, *n*-grams and *term frequencies* to analyze meaning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting word occurrence probabilities with *Zipf’s Law*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Representing natural language texts as vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding relevant documents in a collection of text using *document frequencies*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimating the similarity of pairs of documents with *cosine similarity*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having collected and counted words (tokens), and bucketed them into stems or
    lemmas, it’s time to do something interesting with them. Detecting words is useful
    for simple tasks, like getting statistics about word usage or doing keyword search.
    But you would like to know which words are more important to a particular document
    and across the corpus as a whole. So you can use that "importance" value to find
    relevant documents in a corpus based on keyword importance within each document.
    That will make a spam detector a little less likely to get tripped up by a single
    curse word or a few slightly spammy words within an email. You would like to measure
    how positive and prosocial a Mastodon message is, especially when you have a variety
    of words with different degrees of "positivity" scores or labels. If you have
    an idea about the frequency with which those words appear in a document *in relation
    to* the rest of the documents, you can use that to further refine the "positivity"
    of the document. In this chapter, you’ll learn about a more nuanced, less binary
    measure of words and their usage within a document. This approach has been the
    mainstay for generating features from natural language for commercial search engines
    and spam filters for decades.
  prefs: []
  type: TYPE_NORMAL
- en: The next step in your adventure is to turn the words of Chapter 2 into continuous
    numbers rather than just integers representing word counts or binary "bit vectors"
    that detect the presence or absence of particular words. With representations
    of words in a continuous space, you can operate on their representation with more
    exciting math. Your goal is to find numerical representations of words that somehow
    capture the importance or information content of the words they represent. You’ll
    have to wait until chapter 4 to see how to turn this information content into
    numbers that represent the **meaning** of words.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we look at three increasingly powerful ways to represent words
    and their importance in a document:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Bags of words* — Vectors of word counts or frequencies'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Bags of n-grams* — Counts of word pairs (bigrams), triplets (trigrams), and
    so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*TF-IDF vectors* — Word scores that better represent their importance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: TF-IDF stands for ***t**erm **f**requency times **i**nverse **d**ocument **f**requency*.
    Term frequencies are the counts of each word in a document, which you learned
    about in previous chapters. Inverse document frequency means that you’ll divide
    each of those word counts by the number of documents in which the word occurs.
  prefs: []
  type: TYPE_NORMAL
- en: Each of these techniques can be applied separately or as part of an NLP pipeline.
    These are all statistical models in that they are *frequency* based. Later in
    the book, you’ll see various ways to peer even deeper into word relationships
    and their patterns and non-linearities.
  prefs: []
  type: TYPE_NORMAL
- en: But these "shallow" NLP machines are powerful and useful for many practical
    applications such as search, spam filtering, sentiment analysis, and even chatbots.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Bag of words
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous chapter, you created your first vector space model of a text.
    You used one-hot encoding of each word and then combined all those vectors with
    a binary OR (or clipped `sum`) to create a vector representation of a text. And
    this binary bag-of-words vector makes a great index for document retrieval when
    loaded into a data structure such as a Pandas DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: You then looked at an even more useful vector representation that counts the
    number of occurrences, or frequency, of each word in the given text. As a first
    approximation, you assume that the more times a word occurs, the more meaning
    it must contribute to that document. A document that refers to "wings" and "rudder"
    frequently may be more relevant to a problem involving jet airplanes or air travel,
    than say a document that refers frequently to "cats" and "gravity". Or if you
    have classified some words as expressing positive emotions — words like "good",
    "best", "joy", and "fantastic" — the more a document that contains those words,
    the more likely it is to have positive "sentiment". You can imagine though how
    an algorithm that relied on these simple rules might be mistaken or led astray.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at an example where counting occurrences of words is useful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You only need to download a SpaCy language model once, and it can take quite
    a lot of Internet bandwidth. So only run `cli.download()` if this is the first
    time you’ve run this code in your Python virtual environment. The SpaCy language
    model tokenizes natural language text and returns a document object (`Doc` class)
    containing a sequence of all the tokens in the input text. It also segments the
    document to give you a sequence of sentences in the `.sents` attribute. With the
    Python `set()` type you could convert this sequence of tokens into the set of
    all the unique words in the text.
  prefs: []
  type: TYPE_NORMAL
- en: The list of all the unique words in a document or corpus is called its *vocabulary*
    or *lexicon*. Creating your vocabulary is the most important step in your NLP
    Pipeline. If you don’t identify a particular token and give it a *place* to be
    stored, your pipeline will completely ignore it. In most NLP pipelines you will
    define a single token named `<OOV>` (Out of Vocabulary) where you will store information
    about all the tokens that your pipeline is ignoring, such as a count of their
    occurrences. So all the unusual or made-up "supercalifragilistic" words that you
    do not include in your vocabulary will be lumped together into a single generic
    token and your NLP pipeline will have no way of computing what it means.
  prefs: []
  type: TYPE_NORMAL
- en: The Python `Counter` class is an efficient way to count up occurrences of anything,
    including tokens, in a sequence or array. In Chapter 2, you learned that a `Counter`
    is a special kind of dictionary where the keys are all the unique objects in your
    array, and the dictionary values are the counts of each of those objects.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: A `collections.Counter` object is a `dict` under the hood. And that means that
    the keys are technically stored in an unordered collection or `set`, also sometimes
    called a "bag." It may look like this dictionary has maintained the order of the
    words in your sentence, but that’s just an illusion. You got lucky because your
    sentence didn’t contain many repeated tokens. And the latest versions of Python
    (3.6 and above) maintain the order of the keys based on when you insert new keys
    into a dictionary.^([[1](#_footnotedef_1 "View footnote.")]) But you are about
    to create vectors out of these dictionaries of tokens and their counts. You need
    vectors to do linear algebra and machine learning on a collection of documents
    (sentences in this case). Your bag-of-words vectors will keep track of each unique
    token with a consistent index number for a position within your vectors. This
    way the counts for tokens like "and" or the comma add up across all the vectors
    for your documents — the sentences in the Wikipedia article titled "Algorithmic
    Bias."
  prefs: []
  type: TYPE_NORMAL
- en: Important
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: For NLP the order of keys in your dictionary won’t matter, because you will
    maintain a consistent ordering in vectors such as a Pandas Series. Just as in
    Chapter 2, the Counter dictionary orders your vocabulary (`dict` keys) according
    to when you processed each of the documents of your corpus. And sometimes you
    may want to alphabetize your vocabulary to make it easier to analyze. Once you
    assign each token to a dimension in your vectors of counts, be sure to record
    that order for the future, so you can reuse your pipeline without retraining it
    by reprocessing all your documents. And if you are trying to reproduce someone
    else’s NLP pipeline you’ll want to reuse their exact vocabulary (token list),
    in the exact same order. Otherwise, you will need to process their training dataset
    in the same order, using the exact same software, that they did.
  prefs: []
  type: TYPE_NORMAL
- en: For short documents like this one, the jumbled bag of words still contains a
    lot of information about the original intent of the sentence. The information
    in a bag of words is sufficient to do some powerful things such as detect spam,
    compute sentiment (positivity or other emotions), and even detect subtle intent,
    such as sarcasm. It may be a bag, but it’s full of meaning and information. To
    make these words easier to think about and ensure your pipeline is consistent,
    you want to sort them in some consistent order. To rank the tokens by their count,
    the Counter object has a handy method, `most_common`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: That’s handy! The `Counter.most_common` method will give your a ranked list
    of however many tokens you want, paired with their counts in 2-tuples. But this
    isn’t quite what you want. You need a vector representation to be able to easily
    do math on your token counts.
  prefs: []
  type: TYPE_NORMAL
- en: A Pandas `Series` is an efficient data structure for storing the token counts,
    including the 2-tuples from the `most_common` method. The nice thing about a Pandas
    `Series` is that it behaves like a vector (numpy array) whenever you use a math
    operator such as plus (`+`) or (`\*`) or even `.dot()`. And you can still access
    each named (labeled) dimension associated with each token using the normal square
    bracket (`['token']`) syntax.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The Python expression `x[y]` is the same as `x.*getitem*(y)`. Square brackets
    (`[` and `]`) are syntactic sugar (shorthand) for the hidden `.*getitem*()` methods
    on dictionaries, lists, Pandas Series and many other container objects. If you
    ever want to use this operator on a container class of your own, you only need
    to define a `.*getitem*(index_value)` method that retrieves the appropriate element
    from your container.
  prefs: []
  type: TYPE_NORMAL
- en: You can coerce any list of 2-tuples into a dictionary using the built-in `dict`
    type constructor. And you can coerce any dictionary into a Pandas `Series` using
    the `Series` constructor.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The Pandas Series displays nicely when you print it to the screen, which can
    be handy while you are trying to understand what a token count vector contains.
    Now that you’ve created a count vector you can do main on it like any other Pandas
    Series.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: You can see that there are 23 tokens in the sentence, but only 18 unique tokens
    in your vocabulary for that sentence. So each of your document vectors will need
    to have at least 18 values, even if the other documents do not use those same
    18 words. This allows each token to have its own dimension (slot) in your count
    vectors. Each token is assigned a "slot" in your vectors corresponding to its
    position in your lexicon. Some of those token counts in the vector will be zeros,
    which is what you want.
  prefs: []
  type: TYPE_NORMAL
- en: It makes sense that the comma (",") token and the word "and" are at the top
    of your list of `most_common` tokens. Commas were used five times, the word "and"
    was used twice, and all the other words were only used once each in this sentence.
    Your top two terms or tokens in this particular sentence are ",", and "and." This
    is a pretty common problem with natural language text — the most common words
    are often the least meaningful. Stopwords such as these don’t tell you much about
    the meaning of this document, so you might be tempted to ignore them completely.
    A better approach is to scale your token counts using the statistics of words
    in *your* documents rather than someone else’s arbitrary list of stop words from
    their documents.
  prefs: []
  type: TYPE_NORMAL
- en: The number of times a word occurs in a given document is called the *term frequency*,
    commonly abbreviated "TF." One of the first things you will probably want to do
    is to normalize (divide) your token counts by the number of terms in the document.
    This will give you the relative frequency (percentage or fraction) of the document
    that contains a token, regardless of the length of the document. Check out the
    relative frequency of the word "justice" to see if this approach does justice
    to the importance of this word in this text.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The *normalized term frequency* of the term "justice" in the opening sentence
    of this Wikipedia article is about 4%. And you probably won’t expect this percentage
    to go up as you process more sentences in the article. If the sentence and the
    article were both talking about "justice" about the same amount, this *normalized
    term frequency* score would remain roughly the same throughout the document.
  prefs: []
  type: TYPE_NORMAL
- en: According to this term frequency, the word "justice" represents about 4% of
    the meaning of this sentence. That’s not a lot, considering how vital this word
    is to the meaning of the sentence. So you need to do one more normalization step
    to give this word a boost relative to the other words in the sentence.
  prefs: []
  type: TYPE_NORMAL
- en: To give the word "justice" a score for significance or importance you will need
    some statistics about it from more than just this one sentence. You need to find
    out how much "justice" is used elsewhere. Fortunately for budding NLP engineers,
    Wikipedia is full of high quality accurate natural language text in many languages.
    You can use this text to "teach" your machine about the importance of the word
    "justice" across many documents. To demonstrate the power of this approach, all
    you need is a few paragraphs from the Wikipedia article on algorithmic bias. Here
    are some more sentences from the Wikipedia article "Algorithmic Bias."
  prefs: []
  type: TYPE_NORMAL
- en: Algorithmic bias describes systematic and repeatable errors in a computer system
    that create unfair outcomes, such as privileging one arbitrary group of users
    over others. Bias can emerge due to many factors, including but not limited to
    the design of the algorithm or the unintended or unanticipated use or decisions
    relating to the way data is coded, collected, selected or used to train the algorithm.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: …​
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Algorithmic bias has been cited in cases ranging from election outcomes to the
    spread of online hate speech. It has also arisen in criminal justice, healthcare,
    and hiring, compounding existing racial, economic, and gender biases.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: …​
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Problems in understanding, researching, and discovering algorithmic bias persist
    due to the proprietary nature of algorithms, which are typically treated as trade
    secrets.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: — Wikipedia
  prefs: []
  type: TYPE_NORMAL
- en: Algorithmic Bias ([https://en.wikipedia.org/wiki/Algorithmic_bias](wiki.html))
  prefs: []
  type: TYPE_NORMAL
- en: Look at these sentences to see if you can find keywords that are key to your
    understanding of the text. Your algorithm will need to make sure it includes these
    words and computes statistics about them. If you were trying to detect these important
    words with Python automatically (programatically) how would you compute an importance
    score? See if you can figure out how you could use the `Counter` dictionary to
    help your algorithm understand something about algorithmic bias.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Looks like this sentence doesn’t reuse any words at all. The key to frequency
    analysis and term frequency vectors is the statistics of word usage *relative
    to other words*. So we need to input the other sentences and to create useful
    word counts that are normalized based on how words are used elsewhere. To grok
    (understand) "Algorithmic Bias" you could take the time to read and type all of
    the Wikipedia article into a Python string. Or you could download the text from
    Wikipedia using the `nlpia2_wikipedia` Python package and then process it with
    NLP to find the key concepts you need to review. To retrieve the latest "Algorithmic
    bias" text directly from Wikipedia, use `nlpia2.wikipedia` rather than the official
    (but abandoned) Wikipedia package.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: You can also download a text file containing the first 3 paragraphs of the Wikipedia
    article from the `nlpia2` package on GitLab. If you have cloned the `nlpia2` package
    you will see the `src/nlpia2/ch03/bias_intro.txt` on your local hard drive. If
    you haven’t installed `nlpia2` from the source code you can use the code snippet
    here to retrieve the file using the `requests` package. The `requests` package
    is an handy tool for scraping and downloading natural language text from the web.
    All of the text that made ChatGPT, Bard, and YouChat seem so smart was scraped
    from web pages using tools similar to `requests`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `requests` package returns an HTTP response object containing the headers
    (in `.headers`) and body (`.text`) of an HTTP response. The `bias_intro.txt` file
    from the `nlpia2` package data is a 2023 snapshot of the first three paragraphs
    of the Wikipedia article.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: For a plain text document you can use the `response.content` attribute which
    contains the `bytes` of the raw HTML page. If you want to retrieve a string you
    can use the `response.text` property to automatically decode the text bytes to
    create a unicode `str`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Counter` class from the Python standard library in the `collections` module
    is great for efficiently counting any sequence of objects. That’s perfect for
    NLP when you want to count up occurrences of unique words and punctuation in a
    list of tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Okay, those counts are a bit more statistically significant. But there are still
    many meaningless words and punctuation marks that seem to have high counts. It’s
    not likely that this Wikipedia article is really about tokens such as "of", "to",
    commas, or periods. Perhaps paying attention to the least common tokens will be
    more useful than the most common ones.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Well that didn’t work out so well. You were probably hoping to find terms such
    as "bias", "algorithmic", and "data." So you’re going to have to use a formula
    that balances the counts to come up with the "Goldilocks" score for the ones that
    are "just right." The way you can do that is to come up with another useful count — the
    number of documents that a word occurs in, called the "document frequency." This
    is when things get really interesting.
  prefs: []
  type: TYPE_NORMAL
- en: If you had a large corpus of many many documents you could normalize (divide)
    the counts within a document based on how often a token is used across all your
    documents. As you are just getting started with token count vectors it will be
    better to just create some small documents by splitting up a Wikipedia article
    summary, into smaller documents (sentences or paragraphs). This way you can at
    least see all the documents on one page and figure out where all the counts came
    from by running the code in your head. In the next section that is what you will
    do, split the "Algorithm Bias" article text into sentences and play around with
    different ways of normalizing and structuring the count dictionaries to make them
    more useful for NLP.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1 Vectorizing text
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`Counter` dictionaries are great for counting up tokens in text. But vectors
    are where it’s really at. And it turns out that dictionaries can be coerced into
    a `DataFrame` or `Series` just by calling the `DataFrame` constructor on a list
    of dictionaries. Pandas will take care of all the bookkeeping so that each unique
    token or dictionary key has its own column. Pandas will create NaNs whenever the
    `Counter` dictionary for a document is missing a particular key because the document
    doesn’t contain that word or symbol.'
  prefs: []
  type: TYPE_NORMAL
- en: Once you split the "Algorithmic Bias" article into lines you will see the power
    of vector representations. You will soon see why a Pandas Series is a much more
    useful data structure for working with Token counts compared to a standard Python
    `dict.`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: When the dimensions of your vectors are used to hold scores for tokens or strings,
    that’s when you want to use a Pandas `DataFrame` or `Series` to store your vectors.
    That way you can see what each dimension is for. Check out that sentence that
    we started this chapter with. It happens to be the eleventh sentence in the Wikipedia
    article.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Now this Pandas `Series` is a *vector*. That’s something you can do math on.
    And when you do that math, Pandas will keep track of which word is where so that
    "bias" and "justice" aren’t accidentally added together. Your row vectors in this
    DataFrame have a "dimension" for each word in your vocabulary. In fact, the `df.columns`
    attribute contains your vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: But wait, there are more than 30,000 words in a standard English dictionary.
    If you start processing a lot of Wikipedia articles instead of just a few sentences,
    that’ll be a lot of dimensions to deal with. You are probably used to 2D and 3D
    vectors because they are easy to visualize. But do concepts like distance and
    length even work with 30,000 dimensions? It turns out they do, and you’ll learn
    how to improve on these high-dimensional vectors later in the book. For now just
    know that each element of a vector is used to represent the count, weight or importance
    of a word in the document you want the vector to represent.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll find every unique word in each document and then find all the unique
    words in all of your documents. In math, this is the union of all the sets of
    words in each document. This master set of words for your documents is called
    the *vocabulary* of your pipeline. And if you decide to keep track of additional
    linguistic information about each word, such as spelling variations or parts of
    speech, you might call it a *lexicon*. And you might find academics that use the
    term *corpus* to describe a collection of documents will likely also use the word
    "lexicon," just because it is a more precise technical term than "vocabulary."
  prefs: []
  type: TYPE_NORMAL
- en: So take a look at the vocabulary or lexicon for this tiny corpus of three paragraphs.
    First, you will do *case folding* (lowercasing) to ignore the difference between
    capitalized words (such as proper nouns) and group them together into a single
    vocabulary token. This will reduce the number of unique words in your vocabulary
    in the later stages of your pipeline, which can make it easier to see what’s going
    on.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Now that you have tokenized all 28 of these documents (sentences), you can concatenate
    all these token lists together to create one big list of all the tokens, including
    repetitions. The only difference between this list of tokens and the original
    document is that it has been segmented into sentences and tokenized into words.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Create a vocabulary (lexicon) from the sequence of tokens for the entire paragraph.
    Your lexicon or vocabulary is a list of all the unique tokens in your corpus.
    Just like a dictionary of words at the library, a lexicon doesn’t contain any
    duplicates. Which Python data types do you know of that remove duplicates (besides
    the `dict` type)?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Using the `set` data type ensures that no tokens are counted twice. After lowercasing
    (folding) all the tokens, there are only 248 uniquely spelled tokens in your short
    corpus of 498 words. This means that, on average, each token is used almost exactly
    twice (`498 / 248`).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: It is usually best to run through your entire corpus to build up your vocabulary
    before you go back through the documents counting up tokens and putting them in
    the right slot in your vocab. If you do it this way, it allows you to alphabetize
    your vocabulary, making it easier to keep track of approximately where each token
    count should be in the vectors. And you can filter our really frequent or really
    rare tokens so you can ignore them and keep the dimensions low. This is especially
    important when you want to count n-grams longer than 1-grams.
  prefs: []
  type: TYPE_NORMAL
- en: Assuming you want to keep count of all 248 tokens in this all-lowercase 1-gram
    vocabulary, you can reassemble your count vector matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Look through a few of those count vectors and see if you can find the sentence
    they correspond to in the Wikipedia article on "Algorithmic Bias." Can you see
    how you can get a feel for what each sentence is saying by only looking at the
    vector? A count vector puts the gist of a document into a numerical vector. And
    for a machine that knows nothing about the meaning of words, it is helpful to
    normalize these counts by how frequent a token is overall. For that you will use
    the Scikit-Learn package.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 Faster, better, easier token counting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that you’ve manually created your count vectors, you might wonder if someone
    has built a library for all this token counting and accounting. You can usually
    count on the Scikit-Learn (`sklearn`) package to satisfy all your NLP needs.^([[2](#_footnotedef_2
    "View footnote.")]) If you have already installed the `nlpia2` package you will
    already have Scikit-Learn (`sklearn`) installed. If you would rather install it
    manually, here is one way.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: In an `ipython` console or `jupyter notebook` you can run bash commands using
    the exclamation point at the start of a line.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Once you have set up your environment and installed Scikit-Learn here is how
    you would create the term frequency vector. The `CountVectorizer` class is similar
    to that list of `Counter` classes that you used earlier. It is a standard *transformer*
    class with `.fit()` and `.transform()` methods that comply with the sklearn API
    for all machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.1 Using `sklearn` to compute word count vectors
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Now you have a matrix (practically a list of lists in Python) that represents
    the three documents (the three rows of the matrix) and the count of each term,
    token, or word in your lexicon makes up the columns of the matrix. That was fast!
    With just 1 line of code, `vectorize.fit_transform(corpus)`, we have gotten to
    the same result as with dozens of lines you needed to manually tokenize, create
    a lexicon and count the terms. Note that these vectors have a length of 16, rather
    than 18 like the vectors you created manually. That’s because Scikit-Learn tokenizes
    the sentences slightly differently (it only considers words of 2 letters or more
    as tokens) and drops the punctuation.
  prefs: []
  type: TYPE_NORMAL
- en: So, you have three vectors, one for each document. Now what? What can you do
    with them? Your document word-count vectors can do all the cool stuff any vector
    can do, so let’s learn a bit more about vectors and vector spaces first.^([[3](#_footnotedef_3
    "View footnote.")])
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.3 Vectorize your code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you read about "vectorizing code" on the internet means something entirely
    different than "vectorizing text." Vectorizing text is about converting text into
    a meaningful vector representation of that text. Vectorizing code is about speeding
    up your code by taking advantage of powerful compiled libraries like `numpy` and
    using Python to do math as little as possible. The reason it’s called "vectorizing"
    is because you can use vector algebra notation to eliminate the for loops in your
    code, the slowest part of many NLP pipelines. Instead of `for` loops iterating
    through all the elements of a vector or matrix to do math you just use numpy to
    do the for loop for you in compiled C code. And Pandas uses `numpy` under the
    hood for all its vector algebra, so you can mix and match a DataFrame with a numpy
    arrary or a Python float and it will all run really fast.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Python’s dynamic typing design makes all this magic possible. When you multiply
    a `float` by an `array` or `DataFrame`, instead of raising an error because you’re
    doing math on two different types, the interpreter will figure out what you’re
    trying to do and "make is so," just like Sulu. And it will compute what you’re
    looking for in the fastest possible way, using compiled C code rather than a Python
    `for` loop.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: If you use vectorization to eleminate some of the `for` loops in your code,
    you can speed up your NLP pipeline by a 100x or more. This is 100x more models
    that you can try. The Berlin Social Science Center (WZB) has a great tutorial
    on vectorization.^([[4](#_footnotedef_4 "View footnote.")]). And if you poke around
    elsewhere on the site you’ll find perhaps the only trustworthy source of statistics
    and data on the effect NLP and AI are having on society.^([[5](#_footnotedef_5
    "View footnote.")])
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.4 Vector spaces
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Vectors are the primary building blocks of linear algebra, or vector algebra.
    They are an ordered list of numbers, or coordinates, in a vector space. They describe
    a location or position in that space. Or they can be used to identify a particular
    direction and magnitude or distance in that space. A *vector space* is the collection
    of all possible vectors that could appear in that space. So a vector with two
    values would lie in a 2D vector space, a vector with three values in 3D vector
    space, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: A piece of graph paper, or a grid of pixels in an image, are both nice 2D vector
    spaces. You can see how the order of these coordinates matter. If you reverse
    the x and y coordinates for locations on your graph paper, without reversing all
    your vector calculations, all your answers for linear algebra problems would be
    flipped. Graph paper and images are examples of rectilinear, or Euclidean, spaces
    because the x and y coordinates are perpendicular to each other. The vectors you
    talk about in this chapter are all rectilinear, Euclidean spaces.
  prefs: []
  type: TYPE_NORMAL
- en: 'What about latitude and longitude on a map or globe? That geographic coordinate
    space is definitely two-dimensional because it’s an ordered list of two numbers:
    latitude and longitude. But each of the latitude-longitude pairs describes a point
    on an approximately spherical surface — the Earth’s surface. The latitude-longitude
    vector space is not rectilinear, and Euclidean geometry doesn’t exactly work in
    it. That means you have to be careful when you calculate things like distance
    or closeness between two points represented by a pair of 2D geographic coordinates,
    or points in any non-Euclidean space. Think about how you would calculate the
    distance between the latitude and longitude coordinates of Portland, OR and New
    York, NY.^([[6](#_footnotedef_6 "View footnote.")])'
  prefs: []
  type: TYPE_NORMAL
- en: Figure [3.1](#figure-2d-vectors) shows one way to visualize the three 2D vectors
    `(5, 5)`, `(3, 2)`, and `(-1, 1)`. The head of a vector (represented by the pointy
    tip of an arrow) is used to identify a location in a vector space. So the vector
    heads in this diagram will be at those three pairs of coordinates. The tail of
    a position vector (represented by the "rear" of the arrow) is always at the origin,
    or `(0, 0)`.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3\. 1\. 2D vectors
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![vecs](images/vecs.png)'
  prefs: []
  type: TYPE_IMG
- en: What about 3D vector spaces? Positions and velocities in the 3D physical world
    you live in can be represented by x, y, and z coordinates in a 3D vector. But
    you aren’t limited to normal 3D space. You can have 5 dimensions, 10 dimensions,
    5,000, whatever. The linear algebra all works out the same. You might need more
    computing power as the dimensionality grows. And you’ll run into some "curse-of-dimensionality"
    issues, but you can wait to deal with that until chapter 10.^([[7](#_footnotedef_7
    "View footnote.")])
  prefs: []
  type: TYPE_NORMAL
- en: For a natural language document vector space, the dimensionality of your vector
    space is the count of the number of distinct words that appear in the entire corpus.
    For TF (and TF-IDF to come), we call this dimensionality capital letter "K". This
    number of distinct words is also the vocabulary size of your corpus, so in an
    academic paper it’ll usually be called "|V|" You can then describe each document,
    within this K-dimensional vector space by a K-dimensional vector. K = 18 in your
    three-document corpus about Harry and Jill (or 16, if your tokenizer drops the
    punctuation). Because humans can’t easily visualize spaces of more than three
    dimensions, let’s set aside most of those dimensions and look at two for a moment,
    so you can have a visual representation of the vectors on this flat page you’re
    reading. So in figure [3.2](#figure-2d-term-frequency-vectors), K is reduced to
    two for a two-dimensional view of the 18-dimensional Harry and Jill vector space.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3\. 2\. 2D term frequency vectors
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![harry faster vecs](images/harry_faster_vecs.png)'
  prefs: []
  type: TYPE_IMG
- en: K-dimensional vectors work the same way, just in ways you can’t easily visualize.
    Now that you have a representation of each document and know they share a common
    space, you have a path to compare them. You could measure the Euclidean distance
    between the vectors by subtracting them and computing the length of that distance
    between them, which is called the 2-norm distance. It’s the distance a "crow"
    would have to fly (in a straight line) to get from a location identified by the
    tip (head) of one vector and the location of the tip of the other vector. Check
    out appendix C on linear algebra to see why this is a bad idea for word count
    (term frequency) vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Two vectors are "similar" if they share similar direction. They might have similar
    magnitude (length), which would mean that the word count (term frequency) vectors
    are for documents of about the same length. But do you care about document length
    in your similarity estimate for vector representations of words in documents?
    Probably not. You’d like your estimate of document similarity to find use of the
    same words about the same number of times in similar proportions. This accurate
    estimate would give you confidence that the documents they represent are probably
    talking about similar things.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3\. 3\. 2D vectors and the angles between them
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![vecs cosine](images/vecs_cosine.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Cosine similarity*, is the cosine of the angle between two vectors (theta).
    Figure [3.3](#figure-vecs-cosine) shows how you can compute the cosine similarity
    dot product using equation [3.1](#equation_3_3). Cosine similarity is a popular
    among NLP engineers because:'
  prefs: []
  type: TYPE_NORMAL
- en: Fast to compute even for high dimensional vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sensitive to changes in a single dimension
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Work well for high-dimensional vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Has a value between -1 and 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use cosine similarity without bogging down your NLP pipeline because
    you only need to compute the dot product. And you may be surprised to learn that
    you do not need to compute the cosine function to get the cosine similarity. You
    can use the linear algebra dot product, which does not require any trigonometric
    function evaluation. This makes it very efficient (fast) to calculate. And cosine
    similarity considers each dimension independently and their effect on the direction
    of the vector adds up, even for high dimensional vectors. TF-IDF can have thousands
    or even millions of dimensions, so you need to use a metric that doesn’t degrade
    in usefulness as the number of dimensions increases (called the curse of dimensionality).
  prefs: []
  type: TYPE_NORMAL
- en: 'Another big advantage of cosine similarity is that it outputs a value between
    -1 and +1:'
  prefs: []
  type: TYPE_NORMAL
- en: -1 means the vectors point in exactly opposite directions - this can only happen
    for vectors that have negative values (TF-IDF vectors do not)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 means the vectors are perpendicular or orthogonal - this happens whenever
    your two TF-IDF vectors do not share any of the same words (dimensions)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: +1 means the two vectors are perfectly aligned - this can happen whenever your
    two documents use the same words with the same relative frequency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This makes it easier to guess at good thresholds to use in conditional expression
    within your pipeline. Here’s what the normalized dot product looks like in your
    linear algebra textbook:'
  prefs: []
  type: TYPE_NORMAL
- en: Equation 3.1
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: \[\begin{equation} \boldsymbol{A} \cdot \boldsymbol{B} = |\boldsymbol{A}| |\boldsymbol{B}|
    * cos(\theta) \end{equation}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'In Python you might use code like this to compute cosine similarity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'If you solve this equation for `np.cos(angle_between_A_and_B)` (called "cosine
    similarity between vectors A and B") you can derive code to computer the cosine
    similarity:'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.2 Cosine similarity formula in Python
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'In linear algebra notation this becomes equation [3.2](#equation_3_4):'
  prefs: []
  type: TYPE_NORMAL
- en: Equation 3.2 cosine similarity between two vectors
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: \[\begin{equation} cos(\theta) = \frac{\boldsymbol{A} \cdot \boldsymbol{B}}{|\boldsymbol{A}||\boldsymbol{B}|}
    \end{equation}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Or in pure Python without `numpy`:'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.3 Compute cosine similarity in python
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: So you need to take the dot product of two of your vectors in question — multiply
    the elements of each vector pairwise — and then sum those products up. You then
    divide by the norm (magnitude or length) of each vector. The vector norm is the
    same as its Euclidean distance from the head to the tail of the vector — the square
    root of the sum of the squares of its elements. This *normalized dot product*,
    like the output of the cosine function, will be a value between -1 and 1\. It
    is the cosine of the angle between these two vectors. It gives you a value for
    how much the vectors point in the same direction.^([[8](#_footnotedef_8 "View
    footnote.")])
  prefs: []
  type: TYPE_NORMAL
- en: A cosine similarity of **1** represents identical normalized vectors that point
    in exactly the same direction along all dimensions. The vectors may have different
    lengths or magnitudes, but they point in the same direction. Remember you divided
    the dot product by the norm of each vector. So the closer a cosine similarity
    value is to 1, the closer the two vectors are in angle. For NLP document vectors
    that have a cosine similarity close to 1, you know that the documents are using
    similar words in similar proportion. So the documents whose document vectors are
    close to each other are likely talking about the same thing.
  prefs: []
  type: TYPE_NORMAL
- en: A cosine similarity of **0** represents two vectors that share no components.
    They are orthogonal, perpendicular in all dimensions. For NLP TF vectors, this
    situation occurs only if the two documents share no words in common. This doesn’t
    necessarily mean they have different meanings or topics, just that they use completely
    different words.
  prefs: []
  type: TYPE_NORMAL
- en: A cosine similarity of **-1** represents two vectors that are anti-similar,
    completely opposite. They point in opposite directions. This can never happen
    for simple word count (term frequency) vectors or even normalized TF vectors (which
    we talk about later). Counts of words can never be negative. So word count (term
    frequency) vectors will always be in the same "quadrant" of the vector space.
    None of the term frequency vectors can sneak around into one of the quadrants
    in the vector space. None of your term frequency vectors can have components (word
    frequencies) that are the negative of another term frequency vector, because term
    frequencies just can’t be negative.
  prefs: []
  type: TYPE_NORMAL
- en: You won’t see any negative cosine similarity values for pairs of vectors for
    natural language documents in this chapter. But in the next chapter, we develop
    a concept of words and topics that are "opposite" to each other. And this will
    show up as documents, words, and topics that have cosine similarities of less
    than zero, or even **-1**.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to compute cosine similarity for regular `numpy` vectors, such
    as those returned by `CountVectorizer`, you can use Scikit-Learn’s built-in tools.
    Here is how you can calculate the cosine similarity between word vectors 1 and
    2 that we computed in [3.4](#listing-cosine-similarity):'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.4 Cosine similarity
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: That slicing of the term frequency (`tf`) DataFrame probably looks like an odd
    way to retrieve a vector. This is because the SciKit-Learn function for computing
    cosine similarity has been optimized to work efficiently on large arrays of vectors
    (2-D matrices). This code slices off the first and second row of the DataFrame
    as a 1xN array containing the counts of the words in the first sentences of the
    text. This count vector for the first sentence from the "Algorithmic Bias" article
    is only 11.7% similar (cosine similarity of 0.117) to the second sentence of the
    article. It seems that the second sentence shares very few words with the first
    sentence.
  prefs: []
  type: TYPE_NORMAL
- en: To dig in deeper and understand cosine distance you can check that the code
    in [3.3](#listing-compute-cosine-similarity-in-python) will give you the same
    answer for Counter dictionaries as the `sklearn` cosine similarity function gives
    you for equivalent numpy arrays. And while you are at it, use *active learning*
    to guess the cosine similarity for each pair of sentences before seeing what your
    function outputs. Whenever you try to predict the output of an NLP algorithm,
    and then correct yourself based on what really happens, it improves your intuition
    about how NLP works.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Counting n-grams
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have already seen in the last chapter how to create *n*-grams from the tokens
    in your corpus. Now, it’s time to use them to create a better representation of
    documents. Fortunately for you, you can use the same tools you are already familiar
    with, just tweak the parameters slightly.
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s add another sentence to our corpus, which will illustrate why ngram
    vectors can sometimes be more useful than count vectors.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'If you compute the vector of word counts for this new sentence (question),
    using the same vectorizer we trained in Listing 3.2, you will see that it is exactly
    equal to the representation of the second sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Sparse matrices are an efficient way to store token counts, but to build your
    intuition about what’s going on, or debug your code, you will want to *densify*
    a vector. You can convert a sparse vector (row of a sparse matrix) to a numpy
    array or Pandas series using the `.toarray()` method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: You can probably guess which word in the question that is showing up at the
    8th position (dimension) in the count vector. Remember that this is the 8th word
    in the vocabulary computed by the `CountVectorizer`, and it lexically sorts its
    vocabulary when you run `.fit()`. You can pair up the count vector with your vocabulary
    in a Pandas `Series` to see what is going on inside your count vectors.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Now, calculate the cosine similarity between the question vector and all the
    other vectors in your "knowledge base" of sentence vectors. This is what a search
    engine or database full text search will do to find answers to your queries.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The closest (most similar) is for the fourth sentence in the corpus. It has
    a cosine similarity of .433 with the `question_vector`. Check out the fourth sentence
    in your knowledge base of sentences to see if it might be a good match for this
    question.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Not bad! That sentence would be a good start. However, the first sentence of
    the Wikipedia article is probably a better definition of algorithmic bias for
    this question. Think about how you could improve the vectorization pipeline so
    that your search would return the first sentence rather than the 4th.
  prefs: []
  type: TYPE_NORMAL
- en: To find out whether 2-grams might help, do the same vectorization process you
    did a few pages ago with `CountVectorizer`, but instead set the `n-gram` *hyperparameter*
    to count 2-grams instead of individual tokens (1-grams). A hyperparameter is just
    a function name or argument value or anything you may want to adjust to improve
    your NLP pipeline in some way. Finding the best hyperparmeters is called hyperparameter
    tuning. So start tuning up the `ngram_range` parameter to see if it helps.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Looking at the dimensionality of the new count vectors you probably noticed
    that these vectors are significantly longer. There are always more unique 2-grams
    (pairs of words) than unique tokens. Check out the ngram-counts for that "algorithmic
    bias" 2-gram that is so important for your question.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Now that first sentence might be a better match for your query. It is worth
    noting that bag-of-*n*-grams approach has its own challenges. With large texts
    and corpora, the amount of *n*-grams increases exponentially, causing "curse-of-dimensionality"
    issues we mentioned before. However, as you saw in this section, there might be
    cases where you will want to use it instead of single token counting.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 Analyzing this
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Even though until now we only dealt with *n*-grams of word token, *n*-gram of
    characters can be useful too. For example, they can be used for language detection,
    or authorship attribution (deciding who among the set of authors wrote the document
    analyzed). Let’s solve a puzzle using character *n*-grams and the `CountVectorizer`
    class you just learned how to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll start by importing a small and interesting Python package called `this`,
    and examining some of its constants:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: What are these strange words? In what language are they written? H.P. Lovecraft
    fans may think of the ancient language used to summon the dead deity Cthulhu.^([[9](#_footnotedef_9
    "View footnote.")]) But even to them, this message will be incomprehensible.
  prefs: []
  type: TYPE_NORMAL
- en: To figure out the meaning of this cryptic piece of text, you’ll use the method
    you just learned — frequency analysis (counting tokens). Only this time, a little
    bird is telling you it might be worth to start with character tokens rather than
    word tokens! Luckily, `CountVectorizer` can serve you here as well. You can see
    the results of listing [3.5](#listing-countvectorizer-histogram) in figure 3.4a
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.5 CountVectorizer histogram
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Hmmm. Not quite sure what you can do with these frequency counts. But then
    again, you haven’t even seen the frequency counts for any other text yet. Let’s
    choose some big document - for example, the Wikipedia article for Machine Learning,^([[10](#_footnotedef_10
    "View footnote.")]) and try to do the same analysis (check out the results in
    Figure 3.4b):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Now that looks interesting! If you look closely at the two frequency histograms,
    you might notice a pattern. The peaks and valleys of the histograms seem to be
    arranged in the same order. If you’ve worked with frequency spectra before, this
    may make sense. The pattern of character frequency peaks and valleys is similar,
    but shifted.
  prefs: []
  type: TYPE_NORMAL
- en: To determine whether your eyes are seeing a real pattern, you need to check
    to see if the shift in the peaks and valleys is consistent. This signal processing
    approach is called *spectral analysis*. You can compute the relative position
    of the peaks by subtracting the positions of the highest points of each signal
    from each other.
  prefs: []
  type: TYPE_NORMAL
- en: You can use a couple of built-in python functions, `ord()` and `chr()`, to convert
    back and forth between integers and characters. Fortunately these integers and
    character mappings are in alphabetical order "ABC…​".
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'So if you want to decode the letter "R" in this secret message you should probably
    subtract 13 from its *ordinal* (`ord`) value to get the letter "E" - the most
    frequently used letter in English. Likewise to decode the letter "V" you would
    replace it with "I" - the second most frequently use letter. The three most frequent
    letters have been shifted by the same `peak_distance` (13) to create the encoded
    message. And that distance is preserved between the least frequent letters, too:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'By this point, you have probably MetaGered (searched the web) for information
    about this puzzle.^([[11](#_footnotedef_11 "View footnote.")]) Maybe you discovered
    that this secret message is probably encoded using a ROT13 cipher (encoding).^([[12](#_footnotedef_12
    "View footnote.")]) The ROT13 algorithm rotates each letter in a string 13 positions
    forward in the alphabet. To decode a supposedly secret message that has been encoded
    with ROT13 you would only need to apply the inverse algorithm and rotate your
    alphabet backwards 13 positions. You can probably create the encoder and decoder
    functions yourself in a single line of code. Or you can use python’s builtin `codecs`
    package to reveal what `this` is all about:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Now you know The Zen of Python! These words of wisdom were written by one of
    the Python tribe elders, Tim Peters, back in 1999\. Since the poem has been placed
    in the public domain, put to music,^([[13](#_footnotedef_13 "View footnote.")])
    and even parodied.^([[14](#_footnotedef_14 "View footnote.")]) The Zen of Python
    has helps the authors of this book write cleaner, more readable and reusable code.
    And thanks to a character-based `CountVectorizer`, you were able to decode these
    words of wisdom.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Zipf’s Law
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now on to our main topic — Sociology. Okay, not, but you’ll make a quick detour
    into the world of counting people and words, and you’ll learn a seemingly universal
    rule that governs the counting of most things. It turns out, that in language,
    like most things involving living organisms, patterns abound.
  prefs: []
  type: TYPE_NORMAL
- en: In the early twentieth century, the French stenographer Jean-Baptiste Estoup
    noticed a pattern in the frequencies of words that he painstakingly counted by
    hand across many documents (thank goodness for computers and `Python`). In the
    1930s, the American linguist George Kingsley Zipf sought to formalize Estoup’s
    observation, and this relationship eventually came to bear Zipf’s name.
  prefs: []
  type: TYPE_NORMAL
- en: Zipf’s law states that given some corpus of natural language utterances, the
    frequency of any word is inversely proportional to its rank in the frequency table.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: — Wikipedia
  prefs: []
  type: TYPE_NORMAL
- en: Zipf`s Law [https://en.wikipedia.org/wiki/Zipfs_law](wiki.html)
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, *inverse proportionality* refers to a situation where an item
    in a ranked list will appear with a frequency tied explicitly to its rank in the
    list. The first item in the ranked list will appear twice as often as the second,
    and three times as often as the third, for example. One of the quick things you
    can do with any corpus or document is plot the frequencies of word usages relative
    to their rank (in frequency). If you see any outliers that don’t fall along a
    straight line in a log-log plot, it may be worth investigating.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example of how far Zipf’s Law stretches beyond the world of words, figure
    3.6 charts the relationship between the population of US cities and the rank of
    that population. It turns out that Zipf’s Law applies to counts of lots of things.
    Nature is full of systems that experience exponential growth and "network effects"
    like population dynamics, economic output, and resource distribution.^([[15](#_footnotedef_15
    "View footnote.")]) It’s interesting that something as simple as Zipf’s Law could
    hold true across a wide range of natural and manmade phenomena. Nobel Laureate
    Paul Krugman, speaking about economic models and Zipf’s Law, put it succinctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The usual complaint about economic theory is that our models are oversimplified — that
    they offer excessively neat views of complex, messy reality. [With Zipf’s law]
    the reverse is true: You have complex, messy models, yet reality is startlingly
    neat and simple.*'
  prefs: []
  type: TYPE_NORMAL
- en: Here is an updated version of Krugman’s city population plot:^([[16](#_footnotedef_16
    "View footnote.")])
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.4 City population distribution
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![log pop from wikipedia](images/log_pop_from_wikipedia.png)'
  prefs: []
  type: TYPE_IMG
- en: As with cities and social networks, so with words. Let’s first download the
    Brown Corpus from NLTK.
  prefs: []
  type: TYPE_NORMAL
- en: The Brown Corpus was the first million-word electronic corpus of English, created
    in 1961 at Brown University. This corpus contains text from 500 sources, and the
    sources have been categorized by genre, such as news, editorial, and so on.^([[17](#_footnotedef_17
    "View footnote.")])
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: — NLTK Documentation
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: So with over 1 million tokens, you have something meaty to look at.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: A quick glance shows that the word frequencies in the Brown corpus follow the
    logarithmic relationship Zipf predicted. "The" (rank 1 in term frequency) occurs
    roughly twice as often as "of" (rank 2 in term frequency), and roughly three times
    as often as "and" (rank 3 in term frequency). If you don’t believe us, use the
    example code ([https://gitlab.com/tangibleai/nlpia2/-/blob/main/src/nlpia2/ch03/ch03_zipf.py](ch03.html))
    in the `nlpia` package to see this yourself.
  prefs: []
  type: TYPE_NORMAL
- en: In short, if you rank the words of a corpus by the number of occurrences and
    list them in descending order, you’ll find that, for a sufficiently large sample,
    the first word in that ranked list is twice as likely to occur in the corpus as
    the second word in the list. And it is four times as likely to appear as the fourth
    word on the list. So given a large corpus, you can use this breakdown to say statistically
    how likely a given word is to appear in any given document of that corpus.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Inverse Document Frequency (IDF)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now back to your document vectors. Word counts and *n*-gram counts are useful,
    but pure word count, even when normalized by the length of the document, doesn’t
    tell you much about the importance of that word in that document *relative* to
    the rest of the documents in the corpus. If you could suss out that information,
    you could start to describe documents within the corpus. Say you have a corpus
    of every book about artificial intelligence (AI) ever written. "Intelligence"
    would almost surely occur many times in every book (document) you counted, but
    that doesn’t provide any new information, it doesn’t help distinguish between
    those documents. Whereas something like "neural network" or "conversational engine"
    might not be so prevalent across the entire corpus, but for the documents where
    it frequently occurred, you would know more about their nature. For this, you
    need another tool.
  prefs: []
  type: TYPE_NORMAL
- en: '*Inverse document frequency*, or IDF, is your window through Zipf in topic
    analysis. Let’s take your term frequency counter from earlier and expand on it.
    You can count tokens and bin them up two ways: per document and across the entire
    corpus. You’re going to be counting just by document.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s return to the Algorithmic Bias example from Wikipedia and grab another
    section (that deals with algorithmic racial and ethnic discrimination) and say
    it is the second document in your Bias corpus.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms have been criticized as a method for obscuring racial prejudices
    in decision-making. Because of how certain races and ethnic groups were treated
    in the past, data can often contain hidden biases. For example, black people are
    likely to receive longer sentences than white people who committed the same crime.
    This could potentially mean that a system amplifies the original biases in the
    data.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: …​
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A study conducted by researchers at UC Berkeley in November 2019 revealed that
    mortgage algorithms have been discriminatory towards Latino and African Americans
    which discriminated against minorities based on "creditworthiness" which is rooted
    in the U.S. fair-lending law which allows lenders to use measures of identification
    to determine if an individual is worthy of receiving loans. These particular algorithms
    were present in FinTech companies and were shown to discriminate against minorities.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: — Wikipedia
  prefs: []
  type: TYPE_NORMAL
- en: 'Algorithmic Bias: Racial and ethnic discrimination ([https://en.wikipedia.org/wiki/Algorithmic_bias#Racial_and_ethnic_discrimination](wiki.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'First let’s get the total word count for each document in your corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Now with a couple of tokenized documents about bias in hand, let’s look at the
    term frequency of the term "bias" in each document. You will store the TFs you
    find in two dictionaries, one for each document.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Okay, you have a number eight times as large as the other. Is the intro section
    eight times as much about bias? No, not really. So you need to dig a little deeper.
    First, check out how those numbers compare to the scores for some other word,
    say the word "and".
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Great! You know both of these documents are about "and" just as much as they
    are about "bias" - actually, the discrimination chapter is more about "and" than
    about "bias"! Oh, wait.
  prefs: []
  type: TYPE_NORMAL
- en: 'A good way to think of a term’s inverse document frequency is this: How surprising
    is it that this token is in this document? The concept of measuring the surprise
    in a token might not sound like a very mathematical idea. However, in statistics,
    physics and information theory, the surprise of a symbol is used to measure its
    *entropy* or information content. And that is exactly what you need to gage the
    importance of a particular word. If a term appears in one document a lot of times,
    but occurs rarely in the rest of the corpus, it is a word that distinguishes that
    document’s meaning from the other documents. This'
  prefs: []
  type: TYPE_NORMAL
- en: 'A term’s IDF is merely the ratio of the total number of documents to the number
    of documents the term appears in. In the case of "and" and "bias" in your current
    example, the answer is the same for both:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Not very interesting. So let’s look at another word "black".
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Okay, that’s something different. Let’s use this "rarity" measure to weight
    the term frequencies.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'And let’s grab the TF of "black" in the two documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'And finally, the IDF for all three. You’ll store the IDFs in dictionaries per
    document like you did with TF:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'And then for the intro document you find:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'And then for the history document:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 3.4.1 Return of Zipf
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You’re almost there. Let’s say, though, you have a corpus of 1 million documents
    (maybe you’re baby-Google), and someone searches for the word "cat", and in your
    1 million documents you have exactly 1 document that contains the word "cat".
    The raw IDF of this is:'
  prefs: []
  type: TYPE_NORMAL
- en: 1,000,000 / 1 = 1,000,000
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s imagine you have 10 documents with the word "dog" in them. Your IDF for
    "dog" is:'
  prefs: []
  type: TYPE_NORMAL
- en: 1,000,000 / 10 = 100,000
  prefs: []
  type: TYPE_NORMAL
- en: That’s a big difference. Your friend Zipf would say that’s **too** big because
    it’s likely to happen a lot. Zipf’s Law showed that when you compare the frequencies
    of two words, like "cat" and "dog", even if they occur a similar number of times
    the more frequent word will have an exponentially higher frequency than the less
    frequent one. So Zipf’s Law suggests that you scale all your word frequencies
    (and document frequencies) with the `log()` function, the inverse of `exp()`.
    This ensures that words with similar counts, such as "cat" and "dog", aren’t vastly
    different in frequency. And this distribution of word frequencies will ensure
    that your TF-IDF scores are more uniformly distributed. So you should redefine
    IDF to be the log of the original probability of that word occurring in one of
    your documents. You’ll also want to take the log of the term frequency as well.^([[18](#_footnotedef_18
    "View footnote.")])
  prefs: []
  type: TYPE_NORMAL
- en: 'The base of log function is not important, since you just want to make the
    frequency distribution uniform, not to scale it within a particular numerical
    range.^([[19](#_footnotedef_19 "View footnote.")]) If you use a base 10 log function,
    you’ll get:'
  prefs: []
  type: TYPE_NORMAL
- en: 'search: cat'
  prefs: []
  type: TYPE_NORMAL
- en: Equation 3.3
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: \[\begin{equation} \text{idf} = \log \left(\text{1,000,000} / 1 \right) = 6
    \end{equation}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'search: dog'
  prefs: []
  type: TYPE_NORMAL
- en: Equation 3.4
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: \[\begin{equation} \text{idf} = \log \left(\text{1,000,000} / 10 \right) = 5
    \end{equation}\]
  prefs: []
  type: TYPE_NORMAL
- en: So now you’re weighting the TF results of each more appropriately to their occurrences
    in language, in general.
  prefs: []
  type: TYPE_NORMAL
- en: 'And then finally, for a given term, *t*, in a given document, *d*, in a corpus,
    *D*, you get:'
  prefs: []
  type: TYPE_NORMAL
- en: Equation 3.5
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: \[\begin{equation} \text{tf}\left(t, d\right) = \frac{\text{count}(t)}{\text{count}(d)}
    \end{equation}\]
  prefs: []
  type: TYPE_NORMAL
- en: Equation 3.6
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: \[\begin{equation} \text{idf}\left(t,D\right) = \log \left(\frac{\text{number
    of documents}}{\text{number of documents containing t}}\right) \end{equation}\]
  prefs: []
  type: TYPE_NORMAL
- en: Equation 3.7
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: \[\begin{equation} \text{tfidf}\left(t,d,D\right) = \text{tf}(t,d) \ast \text{idf}(t,D)
    \end{equation}\]
  prefs: []
  type: TYPE_NORMAL
- en: The more times a word appears in the document, the TF (and hence the TF-IDF)
    will go up. At the same time, as the number of documents that contain that word
    goes up, the IDF (and hence the TF-IDF) for that word will go down. So now, you
    have a number. Something your computer can chew on. But what is it exactly? It
    relates a specific word or token to a specific document in a specific corpus,
    and then it assigns a numeric value to the importance of that word in the given
    document, given its usage across the entire corpus.
  prefs: []
  type: TYPE_NORMAL
- en: 'In some classes, all the calculations will be done in log space so that multiplications
    become additions and division becomes subtraction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: This single number, the TF-IDF score, is the humble foundation of all search
    engines. Now that you’ve been able to transform words and documents into numbers
    and vectors, it is time for some Python to put all those numbers to work. You
    won’t likely ever have to implement the TF-IDF formulas from scratch, because
    these algorithms are already implemented for you in many software libraries. You
    don’t need to be an expert at linear algebra to understand NLP, but it sure can
    boost your confidence if you have a mental model of the math that goes into a
    number like a TF-IDF score. If you understand the math, you can confidently tweak
    it for your application and perhaps even help an open-source project improve its
    NLP algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.2 Relevance ranking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As you saw earlier, you can easily compare two vectors and get their similarity,
    but you have since learned that merely counting words isn’t as effective as using
    their TF-IDF values. So in each document vector you want to replace each word’s
    count with its TF-IDF value (score). Now your vectors will more thoroughly reflect
    the meaning, or topic, of the document.
  prefs: []
  type: TYPE_NORMAL
- en: When you use a search engine such as MetaGer.org, Duck.com or You.com, the list
    of 10 or so search results is carefully crafted from TF-IDF vectors for each of
    those pages. If you think about it, it is quite amazing that an algorithm is able
    to give you 10 pages that almost always contain an important piece of information
    you are looking for. After all, there are billions of web pages for the search
    engine to choose from. How is that possible? Under the hood, all search engines
    start by computing the similarity between the TF-IDF vector for a query with the
    TF-IDF vector for the billions of web pages in their database. This similarity
    to your query is often called *relevance*. Here’s how you can rank any documents
    by relevance.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: With this setup, you have K-dimensional vector representation of each document
    in the corpus. And now on to the hunt! Or search, in your case. From the previous
    section, you might remember how we defined similarity between vectors. Two vectors
    are considered similar if their cosine similarity is high, so you can find two
    similar vectors near each other if they maximize the cosine similarity.
  prefs: []
  type: TYPE_NORMAL
- en: Now you have all you need to do a basic TF-IDF-based search. You can treat the
    search query itself as a document, and therefore get a TF-IDF-based vector representation
    of it. The last step is then to find the documents whose vectors have the highest
    cosine similarities to the query and return those as the search results.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you take your three documents about Harry, and make the query "How long
    does it take to get to the store?":'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: You can safely say document 0 has the most relevance for your query! And with
    this, you can find relevant documents amidst any corpus, be it articles in Wikipedia,
    books from Project Gutenberg, or toots on ActivityPub (Mastodon). Google look
    out!
  prefs: []
  type: TYPE_NORMAL
- en: Actually, Google’s search engine is safe from competition from us. You have
    to do an "index scan" of your TF-IDF vectors with each query. That’s an \(O(N)\)
    algorithm. Most search engines can respond in constant time (\(O(1)\)) because
    they use an *inverted index*.^([[20](#_footnotedef_20 "View footnote.")]) You
    aren’t going to implement an index that can find these matches in constant time
    here, but if you’re interested you might like exploring the state-of-the-art Python
    implementation in the `Whoosh` ^([[21](#_footnotedef_21 "View footnote.")]) package
    and its source code.^([[22](#_footnotedef_22 "View footnote.")])
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the preceding code, you dropped the keys that were not found in your pipeline’s
    lexicon (vocabulary) to avoid a divide-by-zero error. But a better approach is
    to +1 the denominator of every IDF calculation, which ensures no denominators
    are zero. In fact this approach is so common it has a name, *additive smoothing*
    or "Laplace smoothing" ^([[23](#_footnotedef_23 "View footnote.")]) — will usually
    improve the search results for TF-IDF keyword-based searches.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.3 Another vectorizer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that was a lot of code for things that have long since been automated. The
    `sklearn` package you used at the beginning of this chapter has a tool for TF-IDF
    too. Just as `CountVectorizer` you saw previously, it does tokenization, omits
    punctuation, and computes the tf-idf scores all in one.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s how you can use `sklearn` to build a TF-IDF matrix. The syntax is almost
    exactly the same as for `CountVectorizer`.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.6 Computing TF-IDF matrix using Scikit-Learn
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: With Scikit-Learn, in four lines of code, you created a matrix of your three
    documents and the inverse document frequency for each term in the lexicon. It’s
    very similar to the matrix you got from `CountVectorizer`, only this time it contains
    TF-IDF of each term, token, or word in your lexicon make up the columns of the
    matrix. On large texts this or some other pre-optimized TF-IDF model will save
    you scads of work.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.4 Alternatives
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: TF-IDF matrices (term-document matrices) have been the mainstay of information
    retrieval (search) for decades. As a result, researchers and corporations have
    spent a lot of time trying to optimize that IDF part to try to improve the relevance
    of search results. [3.1](#alternative_tfidf_normalizations_table) lists some of
    the ways you can normalize and smooth your term frequency weights.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3.1 Alternative TF-IDF normalization approaches (Molino 2017) ^([[24](#_footnotedef_24
    "View footnote.")])
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Scheme | Definition |'
  prefs: []
  type: TYPE_TB
- en: '| None | ![table equation 1](images/table_equation_1.png) |'
  prefs: []
  type: TYPE_TB
- en: '| TD-IDF | ![table equation 2](images/table_equation_2.png) |'
  prefs: []
  type: TYPE_TB
- en: '| TF-ICF | ![table equation 3](images/table_equation_3.png) |'
  prefs: []
  type: TYPE_TB
- en: '| Okapi BM25 | ![table equation 4](images/table_equation_4.png) |'
  prefs: []
  type: TYPE_TB
- en: '| ATC | ![table equation 5](images/table_equation_5.png) |'
  prefs: []
  type: TYPE_TB
- en: '| LTU | ![table equation 6](images/table_equation_6.png) |'
  prefs: []
  type: TYPE_TB
- en: '| MI | ![table equation 7](images/table_equation_7.png) |'
  prefs: []
  type: TYPE_TB
- en: '| PosMI | ![table equation 8](images/table_equation_8.png) |'
  prefs: []
  type: TYPE_TB
- en: '| T-Test | ![table equation 9](images/table_equation_9.png) |'
  prefs: []
  type: TYPE_TB
- en: '| chi² | See section 4.3.5 of *From Distributional to Semantic Similarity*
    ([https://www.era.lib.ed.ac.uk/bitstream/handle/1842/563/IP030023.pdf#subsection.4.3.5](563.html))
    by James Richard Curran |'
  prefs: []
  type: TYPE_TB
- en: '| Lin98a | ![table equation 10](images/table_equation_10.png) |'
  prefs: []
  type: TYPE_TB
- en: '| Lin98b | ![table equation 11](images/table_equation_11.png) |'
  prefs: []
  type: TYPE_TB
- en: '| Gref94 | ![table equation 12](images/table_equation_12.png) |'
  prefs: []
  type: TYPE_TB
- en: Search engines (information retrieval systems) match keywords (term) between
    queries and documents in a corpus. If you’re building a search engine and want
    to provide documents that are likely to match what your users are looking for,
    you should spend some time investigating the alternatives described by Piero Molino
    in figure 3.7.
  prefs: []
  type: TYPE_NORMAL
- en: One such alternative to using straight TF-IDF cosine distance to rank query
    results is Okapi BM25, or its most recent variant, BM25F.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.5 Okapi BM25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The smart people at London’s City University came up with a better way to rank
    search results. Rather than merely computing the TF-IDF cosine similarity, they
    normalize and smooth the similarity. They also ignore duplicate terms in the query
    document, effectively clipping the term frequencies for the query vector at 1\.
    And the dot product for the cosine similarity is not normalized by the TF-IDF
    vector norms (number of terms in the document and the query), but rather by a
    nonlinear function of the document length itself.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: You can optimize your pipeline by choosing the weighting scheme that gives your
    users the most relevant results. But if your corpus isn’t too large, you might
    consider forging ahead with us into even more useful and accurate representations
    of the meaning of words and documents.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Using TF-IDF for your bot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, you learned how TF-IDF can be used to represent natural language
    documents with vectors, find similarities between them, and perform keyword search.
    But if you want to build a chatbot, how can you use those capabilities to make
    your first intelligent assistant?
  prefs: []
  type: TYPE_NORMAL
- en: Actually, many chatbots rely heavily on a search engine. And some chatbots use
    their search engine as their only algorithm for generating responses. You just
    need to take one additional step to turn your simple search index (TF-IDF) into
    a chatbot. To make this book as practical as possible, every chapter will show
    you how to make your bot smarter using the skills you picked up in that chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you’re going to make your chatbot answer data science questions.
    The trick is simple: you’re store your training data in pairs of questions and
    appropriate responses. Then you can use TF-IDF to search for a question most similar
    to the user input text. Instead of returning the most similar statement in your
    database, you return the response associated with that statement. And with that,
    you’re chatting!'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s do it step by step. First, let’s load our data. You’ll use the corpus
    of data science questions that Hobson was asked by his mentees in the last few
    years. They are located in the `qary` repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Next, let’s create TF-IDF vectors for the questions in our dataset. You’ll use
    the Scikit-Learn TfidfVectorizer class you’ve seen in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: We’re now ready to implement the question-answering itself. Your bot will reply
    to the user’s question by using the same vectorizer you trained on the dataset,
    and finding the most similar questions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'And your first question-answering chatbot is ready! Let’s ask it its first
    question:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Try to play with it and ask it a couple more questions, such as: - What is
    a Gaussian distribution? - Who came up with the perceptron algorithm?'
  prefs: []
  type: TYPE_NORMAL
- en: You’ll realize quickly, however, that your chatbot fails quite often - and not
    just because the dataset you trained it upon is small.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s try the following question:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: If you looked closely at the dataset, you might have seen it actually has an
    answer about decreasing overfitting for boosting models. However, our vectorizer
    is just a little bit too literal - and when it saw the word "decrease" in the
    wrong question, that caused the dot product to be higher for the wrong question.
    In the next chapter, we’ll see how we can overcome this challenge by looking at
    *meaning* rather than particular words.
  prefs: []
  type: TYPE_NORMAL
- en: 3.6 What’s next
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you can convert natural language text to numbers, you can begin to
    manipulate them and compute with them. Numbers firmly in hand, in the next chapter
    you’ll refine those numbers to try to represent the **meaning** or **topic** of
    natural language text instead of just its words. In subsequent chapters, we show
    you how to implement a semantic search engine that finds documents that "mean"
    something similar to the words in your query rather than just documents that use
    those exact words from your query. Semantic search is much better than anything
    TF-IDF weighting and stemming and lemmatization can ever hope to achieve. State-of-the-art
    search engines combine both TF-IDF vectors and semantic embedding vectors to achieve
    both higher accuracy than conventional search.
  prefs: []
  type: TYPE_NORMAL
- en: The well-funded OpenSearch project, an ElasticSearch fork, is now leading the
    way in search innovation.^([[25](#_footnotedef_25 "View footnote.")]) ElasticSearch
    started walling off their technology garden in 2021\. The only reason Google and
    Bing and other web search engines don’t use the semantic search approach is that
    their corpus is too large. Semantic word and topic vectors don’t scale to billions
    of documents, but millions of documents are no problem. And some scrappy startups
    such as You.com are learning how to use open source to enable semantic search
    and conversational search (chat) on a web scale.
  prefs: []
  type: TYPE_NORMAL
- en: So you only need the most basic TF-IDF vectors to feed into your pipeline to
    get state-of-the-art performance for semantic search, document classification,
    dialog systems, and most of the other applications we mentioned in Chapter 1\.
    TF-IDFs are just the first stage in your pipeline, a basic set of features you’ll
    extract from text. In the next chapter, you will compute topic vectors from your
    TF-IDF vectors. Topic vectors are an even better representation of the meaning
    of a document than these carefully normalized and smoothed TF-IDF vectors. And
    things only get better from there as we move on to Word2vec word vectors in chapter
    6 and deep learning embeddings of the meaning of words and documents in later
    chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 3.7 Test yourself
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What are the differences between the count vectors that `CountVectorizer.transform()`
    creates and a list of python `collections.Counter` objects? Can you convert them
    to identical `DataFrame` objects?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you use `TFIDFVectorizer` on a large corpus (more than 1M documents) with
    a huge vocabulary (more than 1M tokens)? What problems do you expect to encounter?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Think of an example of corpus or task where term frequency (TF) will perform
    better than TF-IDF.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We mentioned that bag of character n-grams can be used for language recognition
    tasks. How would an algorithm that uses character n-grams to distinguish one language
    from another work?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the limitations or disadvantages of TF-IDF you have seen throughout
    this chapter? Can you come up with additional ones that weren’t mentioned?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How would you use TF-IDF as a base to improve how most search engines today
    work?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 3.8 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Any web-scale search engine with millisecond response times has the power of
    a TF-IDF term document matrix hidden under the hood.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zipf’s law can help you predict the frequencies of all sorts of things including
    words, characters, and people.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Term frequencies must be weighted by their inverse document frequency to ensure
    the most important, most meaningful words are given the heft they deserve.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bag-of-words / Bag-of-ngrams and TF-IDF are the most basic algorithms to represent
    natural language documents with a vector of real numbers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Euclidean distance and similarity between pairs of high dimensional vectors
    don’t adequately represent their similarity for most NLP applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cosine distance, the amount of "overlap" between vectors, can be calculated
    efficiently just by multiplying the elements of normalized vectors together and
    summing up those products.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cosine distance is the go-to similarity score for most natural language vector
    representations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[1]](#_footnoteref_1) StackOverflow discussion of whether to rely on this
    feature ( [https://stackoverflow.com/questions/39980323/are-dictionaries-ordered-in-python-3-6/39980744#39980744](are-dictionaries-ordered-in-python-3-6.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[2]](#_footnoteref_2) Scikit-Learn documentation ( [http://scikit-learn.org/](scikit-learn.org.html)).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[3]](#_footnoteref_3) If you would like more details about linear algebra
    and vectors take a look at Appendix C.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[4]](#_footnoteref_4) "Vectorization and Parallelization" by WZB.eu ( [https://datascience.blog.wzb.eu/2018/02/02/vectorization-and-parallelization-in-python-with-numpy-and-pandas/](vectorization-and-parallelization-in-python-with-numpy-and-pandas.html)).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[5]](#_footnoteref_5) "Knowledge and Society in Times of Upheaval" ( [https://wzb.eu/en/node/60041](node.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[6]](#_footnoteref_6) You’d need to use a package like GeoPy (geopy.readthedocs.io)
    to get the math right.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[7]](#_footnoteref_7) The curse of dimensionality is that vectors will get
    exponentially farther and farther away from one another, in Euclidean distance,
    as the dimensionality increases. A lot of simple operations become impractical
    above 10 or 20 dimensions, like sorting a large list of vectors based on their
    distance from a "query" or "reference" vector (approximate nearest neighbor search).
    To dig deeper, check out Wikipedia’s "Curse of Dimensionality" article ( [https://en.wikipedia.org/wiki/Curse_of_dimensionality](wiki.html)).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[8]](#_footnoteref_8) These videos show how to create vectors for words and
    then compute their cosine similarity to each other using SpaCy and numpy ( [https://www.dropbox.com/sh/3p2tt55pqsisy7l/AAB4vwH4hV3S9pUO0n4kTZfGa?dl=0](3p2tt55pqsisy7l.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[9]](#_footnoteref_9) If the reference is unfamiliar to you, check out the
    story *Call of Cthulhu* by H.P. Lovecraft: [https://www.hplovecraft.com/writings/texts/fiction/cc.aspx](fiction.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[10]](#_footnoteref_10) Retrieved on July 9th 2021 from here: [https://en.wikipedia.org/wiki/Machine_learning](wiki.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[11]](#_footnoteref_11) The nonprofit MetaGer search engine takes privacy,
    honesty, and ethics seriously unlike the top search engines you’re already familiar
    with ( [https://metager.org/](metager.org.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[12]](#_footnoteref_12) Wikipedia ROT13 article ( [https://en.wikipedia.org/wiki/ROT13](wiki.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[13]](#_footnoteref_13) Zbwedicon’s YouTube video about the Zen of Python
    ( [https://www.youtube.com/watch?v=i6G6dmVJy74](www.youtube.com.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[14]](#_footnoteref_14) You can install and import PyDanny’s `that` package
    to have a laugh about Python antipatterns ( [https://pypi.org/project/that](project.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[15]](#_footnoteref_15) See the web page titled "There is More than a Power
    Law in Zipf" ( [https://www.nature.com/articles/srep00812](articles.html)).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[16]](#_footnoteref_16) Population data downloaded from Wikipedia using Pandas.
    See the ``nlpia.book.examples` code on GitHub ( [https://gitlab.com/tangibleai/nlpia2/-/blob/main/src/nlpia2/ch03/ch03_zipf.py](ch03.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[17]](#_footnoteref_17) For a complete list, see [http://icame.uib.no/brown/bcm-los.html](brown.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[18]](#_footnoteref_18) Gerard Salton and Chris Buckley first demonstrated
    the usefulness of log scaling for information retrieval in their paper Term Weighting
    Approaches in Automatic Text Retrieval ( [https://ecommons.cornell.edu/bitstream/handle/1813/6721/87-881.pdf](6721.html)).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[19]](#_footnoteref_19) Later we show you how to normalize the TF-IDF vectors
    after all the TF-IDF values have been calculated using this log scaling.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[20]](#_footnoteref_20) See the web page titled "Inverted index - Wikipedia"
    ( [https://en.wikipedia.org/wiki/Inverted_index](wiki.html)).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[21]](#_footnoteref_21) See the web page titled "Whoosh : PyPI" ( [https://pypi.python.org/pypi/Whoosh](pypi.html)).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[22]](#_footnoteref_22) See the web page titled "GitHub - Mplsbeb/whoosh:
    A fast pure-Python search engine" ( [https://github.com/Mplsbeb/whoosh](Mplsbeb.html)).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[23]](#_footnoteref_23) See the web page titled "Additive smoothing - Wikipedia"
    ( [https://en.wikipedia.org/wiki/Additive_smoothing](wiki.html)).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[24]](#_footnoteref_24) *Word Embeddings Past, Present and Future* by Piero
    Molino at AI with the Best 2017'
  prefs: []
  type: TYPE_NORMAL
- en: '[[25]](#_footnoteref_25) "The ABCs of semantic search in OpenSearch" by Milind
    Shyani ( [https://opensearch.org/blog/semantic-science-benchmarks/](semantic-science-benchmarks.html))'
  prefs: []
  type: TYPE_NORMAL
