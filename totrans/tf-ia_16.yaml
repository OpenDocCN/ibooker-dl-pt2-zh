- en: 13 Transformers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 13个Transformer
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Implementing a full Transformer model with all the components
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现一个包含所有组件的完整Transformer模型
- en: Implementing a spam classifier using a pretrained BERT model from TFHub
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TFHub中预训练的BERT模型实现垃圾邮件分类器
- en: Implementing a question-answering model using Hugging Face’s Transformer library
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Hugging Face的Transformer库实现问答模型
- en: In chapters 11 and 12, you learned about sequence-to-sequence models, a powerful
    family of models that allows us to map an arbitrary-length sequence to another
    arbitrary-length sequence. We exemplified this ability through a machine translation
    task. Sequence-to-sequence models consist of an encoder and a decoder. The encoder
    takes in the input sequence (a sentence in the source language) and creates a
    compact representation of that (known as the context vector). The decoder takes
    in the context vector to produce the final target (i.e., a sentence in the target
    language). But we saw how limiting the context vector makes the model and looked
    at two techniques to improve model performance. First, teacher forcing allows
    the decoder to see not only the context vector but also the previous words in
    the target at a given time. This provides the decoder much more information to
    produce the final prediction accurately. Second, the attention mechanism allows
    the decoder to peek into any part of the encoder history of outputs and use that
    information to produce the output as well. However, LSTM and GRU models are still
    quite restrictive as they can only see one output in the sequence at a given time
    and need to rely on a limited state vector (i.e., memory) to remember what they
    have seen.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在第11章和第12章，你学习了序列到序列模型，这是一类强大的模型家族，允许我们将任意长度的序列映射到另一个任意长度的序列。我们通过一个机器翻译任务来举例说明这种能力。序列到序列模型由一个编码器和一个解码器组成。编码器接收输入序列（源语言中的句子）并创建该序列的紧凑表示（称为上下文向量）。解码器接收上下文向量以生成最终的目标（即目标语言中的句子）。但是我们看到上下文向量的限制如何限制了模型，并研究了两种改进模型性能的技术。首先，教师强制允许解码器不仅在给定时间内查看上下文向量，还可以查看目标语言中先前的单词。这为解码器提供了更多的信息，以准确地生成最终的预测。其次，注意机制允许解码器窥视编码器输出历史中的任何部分，并使用该信息来生成输出。然而，LSTM和GRU模型仍然相当受限制，因为它们一次只能看到一个序列输出，并且需要依赖有限的状态向量（即记忆）来记住它们所看到的内容。
- en: But there’s a new rival in town. If there’s one word that most recent state-of-the-art
    NLP and computer vision research have been brimming with, it’s Transformer. Transformer
    models are the latest type of deep learning models that made an unforgettable
    entrance by being crowned as state of the art for many NLP tasks, beating previous
    leaders such as LSTM- and GRU-based models. Inspired by their unprecedented success
    in NLP, they are now being introduced to solve various computer vision problems.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 但是现在镇上出现了一个新的竞争对手。如果有一个词可以概括最近最先进的自然语言处理（NLP）和计算机视觉研究，那就是Transformer。Transformer模型是最新型的深度学习模型，通过被冠以NLP许多任务的最新技术而令人难忘，击败了先前的领导者，如基于LSTM和GRU的模型。受到他们在NLP领域取得的空前成功的启发，它们现在正在被引入来解决各种计算机视觉问题。
- en: By design, Transformer models make remembering or using information present
    in long sequences of data (e.g., a sequence of words) trivial. Unlike LSTM models,
    which have to look at one time step at a time, Transformer models can see the
    full sequence at once. This enables Transformer models to understand language
    better than other models. Furthermore, Transformer models enjoy high parallelizability
    due to the minimization of longitudinal (i.e., temporal) computations that require
    sequential processing of text.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 按设计，Transformer模型使得记忆或使用长序列数据（例如单词序列）中的信息变得轻而易举。与LSTM模型不同，后者必须一次查看一个时间步长，Transformer模型可以一次看到整个序列。这使得Transformer模型比其他模型更好地理解语言。此外，由于纵向（即时间）计算的最小化需要对文本进行顺序处理，Transformer模型享有高度的可并行性。
- en: 'In this chapter, continuing our conversation from chapter 5, we will discuss
    some more details of the Transformer model so that our understanding of it is
    holistic. We will see how the Transformer model employs several embeddings to
    represent tokens as well as the position of those tokens in the sequence. Then
    we will learn BERT, a variant of the Transformer model that has been trained on
    a large corpus of text, ready to be used as a base layer to solve downstream NLP
    tasks easily without the need for complex models. BERT is essentially the encoder
    section of the Transformer model pretrained on large amounts of text using two
    techniques: masked language modeling (i.e., words in the sequence are randomly
    masked where BERT must predict the masked words) and next-sentence prediction
    (i.e., given two sentences, A and B, predict whether B entails A). We will see
    BERT in action when we use it to implement a spam classifier in TensorFlow. Next,
    Hugging Face’s transformers library ([https://huggingface.co/transformers/](https://huggingface.co/transformers/))
    is a popular choice for implementing state-of-the-art Transformer models with
    ease. It is one of the most valuable libraries to look at if you are planning
    to implement Transformer models in TensorFlow. For this reason, we will implement
    a question-answering model using the Transformers library by Hugging Face. Finally,
    we will end the chapter with a discussion about how Transformer models are used
    in computer vision.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，延续我们在第五章的对话，我们将讨论 Transformer 模型的一些更多细节，以便我们对其有一个全面的理解。我们将看到 Transformer
    模型如何使用多个嵌入来表示标记以及这些标记在序列中的位置。然后我们将学习 BERT，这是 Transformer 模型的一个变体，它已经在大型文本语料库上进行了训练，可以作为基础层轻松解决下游
    NLP 任务，而不需要复杂的模型。BERT 本质上是 Transformer 模型的编码器部分，使用了两种技术进行大量文本的预训练：掩码语言建模（即，序列中的单词被随机掩码，BERT
    必须预测掩码单词）和下一个句子预测（即，给定两个句子 A 和 B，预测 B 是否暗示 A）。当我们使用它在 TensorFlow 中实现一个垃圾邮件分类器时，我们将看到
    BERT 在实际中的运行情况。接下来，Hugging Face 的 transformers 库（[https://huggingface.co/transformers/](https://huggingface.co/transformers/)）是实现最先进的
    Transformer 模型的热门选择，易于使用。如果您打算在 TensorFlow 中实现 Transformer 模型，那么这是最有价值的库之一。出于这个原因，我们将使用
    Hugging Face 的 Transformers 库实现一个问答模型。最后，我们将结束本章，讨论 Transformer 模型在计算机视觉中的应用。
- en: As the first step, let’s revisit what we have already learned about Transformer
    models (see figure 13.1) and expand our understanding a bit further.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一步，让我们重新审视一下我们已经学过的关于 Transformer 模型的知识（见图 13.1），并进一步扩展我们的理解。
- en: '![13-01](../../OEBPS/Images/13-01.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![13-01](../../OEBPS/Images/13-01.png)'
- en: Figure 13.1 How Transformer models are used to solve NLP problems
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.1 Transformer 模型如何解决 NLP 问题
- en: 13.1 Transformers in more detail
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.1 更详细的 Transformer
- en: You are working as a data scientist, and it’s been suggested that you use a
    Transformer model in your NLP workflow. You look at the Transformer models provided
    in TensorFlow. However, you are struggling to understand the model from the documentation.
    You think implementing a Transformer network from scratch would be a great way
    to grok the concepts that give life to the Transformer models. Therefore, you
    decide to implement a Transformer model with all the components specified in the
    paper “Attention Is All You Need” ([https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf)).
    This model will have components like embedding layers (token and positional embeddings),
    self-attention layers, normalization layers, and so on.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一名数据科学家，有人建议你在 NLP 工作流中使用 Transformer 模型。你看了 TensorFlow 提供的 Transformer 模型。然而，你很难从文档中理解这个模型。你认为从零开始实现一个
    Transformer 网络是理解赋予 Transformer 模型生命的概念的好方法。因此，你决定按照论文“Attention Is All You Need”（[https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf)）中指定的所有组件来实现一个
    Transformer 模型。这个模型将具有诸如嵌入层（标记和位置嵌入）、自注意层、归一化层等组件。
- en: 13.1.1 Revisiting the basic components of the Transformer
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.1.1 重新审视 Transformer 的基本组件
- en: In chapter 5, we talked about the basics of the Transformer model to implement
    a simplified Transformer. Now let’s deepen our discussion and look at all the
    components that coexist in a Transformer model. The Transformer model is an encoder-decoder-based
    model. The encoder takes a sequence of inputs (e.g., Dogs are great) to create
    a hidden (or latent) representation of those tokens. Next, the decoder consumes
    the encoder’s generated representation of input tokens and generates an output
    (e.g., the French translation of the input).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 5 章，我们讨论了 Transformer 模型的基础，以实现一个简化的 Transformer。现在让我们深入讨论并查看存在于 Transformer
    模型中的所有组件。Transformer 模型是基于编码器-解码器的模型。编码器接受一系列输入（例如，Dogs are great）以创建这些标记的隐藏（或潜在）表示。接下来，解码器使用编码器生成的输入标记的表示，并生成一个输出（例如，输入的法语翻译）。
- en: 'The encoder consists of a stack of layers, where each layer consists of two
    sublayers:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器由一堆层组成，其中每一层包含两个子层：
- en: '*A self-attention layer*—Generates a latent representation for each input token
    in the sequence. For each input token, this layer looks at the entire input sequence
    and selects other tokens in the sequence that enrich the semantics of the generated
    hidden output for that token (i.e., *attended representation*).'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自注意力层*—为序列中的每个输入标记生成潜在表示。对于每个输入标记，此层查看整个输入序列，并选择序列中的其他标记，以丰富为该标记生成的隐藏输出的语义（即，*关注表示*）。'
- en: '*A fully connected layer*—Generates an element-wise hidden representation of
    the attended representation.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*全连接层*—生成与关注表示相关的逐元素隐藏表示。'
- en: 'The decoder consists of three sublayers:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器由三个子层组成：
- en: '*A masked self-attention layer*—For each input token, it looks at all the tokens
    to the left of it. The decoder needs to mask words to the right to prevent the
    model from seeing words in the future, making the prediction task trivial for
    the decoder.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*掩码自注意力层*—对于每个输入标记，它查看其左侧的所有标记。解码器需要屏蔽右侧的单词，以防止模型看到未来的单词，使得解码器的预测任务变得简单。'
- en: '*An encoder-decoder attention layer*—For each input token in the decoder, it
    looks at both the encoder’s outputs as well as the decoder’s masked attended output
    to generate a semantically rich hidden output'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*编码器-解码器注意力层*—对于解码器中的每个输入标记，它查看编码器的输出以及解码器的掩码关注输出，以生成一个语义丰富的隐藏输出。'
- en: '*A fully connected layer*—Generates an element-wise hidden representation of
    the attended representation of the decoder.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*全连接层*—生成解码器关注表示的逐元素隐藏表示。'
- en: 'The hardest thing to navigate in our previous discussions was understanding
    the self-attention layer. Therefore, it’s worthwhile revisiting the computations
    transpiring in the self-attention layer. Computations in the self-attention layer
    revolve around three weight matrices:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的讨论中，最难理解的是自注意力层。因此，值得重新审视自注意力层中发生的计算。自注意力层中的计算围绕着三个权重矩阵展开：
- en: Query weight matrix (*W*[q])
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询权重矩阵（*W*[q]）
- en: Key weight matrix (*W*[k])
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 键权重矩阵（*W*[k]）
- en: Value weight matrix (*W*[v])
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值权重矩阵（*W*[v]）
- en: 'Each of these weight matrices produces three outputs for a given token (at
    position *i*) in a given input sequence: a query, a key, and a value, respectively.
    Let’s refresh our memory on what we said about these entities in chapter 5:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这些权重矩阵中的每一个对于给定输入序列中的给定标记（在位置*i*）产生三个输出：查询、键和值。让我们刷新一下我们在第 5 章中对这些实体所说的话：
- en: '*Query* (*q*[i])—Helps to build a probability matrix that is eventually used
    for indexing values (*v*). Query affects the rows of the matrix and represents
    the index of the current word that’s being processed.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*查询*（*q*[i]）—帮助构建最终用于索引值（*v*）的概率矩阵。查询影响矩阵的行，并表示正在处理的当前单词的索引。'
- en: '*Key* (*k*[i])—Helps to build a probability matrix that is eventually used
    for indexing values (*v*). Key affects the columns of the matrix and represents
    the candidate words that need to be mixed depending on the query word.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*关键*（*k*[i]）—帮助构建最终用于索引值（*v*）的概率矩阵。关键影响矩阵的列，并表示需要根据查询词混合的候选词。'
- en: '*Value* (*v*[i])—Hidden (i.e., latent) representation of the inputs that are
    used to compute the final output by indexing from the probability matrix created
    using query and key. As explained earlier, the final output at position *i* is
    generated not only by using the *i*^(th) token, but also by using other tokens
    in the input sequence, which enhances the semantics captured in the final representation.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Value*（*v*[i]) ——输入的隐藏（即潜在）表示，用于通过查询和密钥创建的概率矩阵索引计算最终输出。正如前面解释的那样，在位置 *i* 处的最终输出不仅使用第
    *i* 个令牌，还使用输入序列中的其他令牌，这增强了最终表示中捕获的语义。'
- en: The high-level purpose of these elements is to generate an attended representation
    (i.e., a latent or hidden representation for a given token that is enriched by
    the information in other tokens of the input sequence) of a given input token.
    To do that, the model
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这些元素的高层目的是生成一个被关注的表示（即给定令牌的潜在或隐藏表示，其由输入序列中其他令牌的信息增强）。为此，模型
- en: Generates a query for each position in the input sequence
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为输入序列中的每个位置生成一个查询
- en: For each query, determines how much each key should contribute (the key also
    represents individual tokens)
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个查询，确定每个密钥应该贡献多少（密钥也代表个别令牌）
- en: Based on the contributions of the keys for a given query, mixes the values corresponding
    to those keys to generate the final attended representation
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于给定查询的密钥的贡献，混合与这些密钥对应的值以生成最终的关注表示
- en: 'All three of the query, key, and values are generated by multiplying a trainable
    weight matrix with an input token’s numerical representation. All this needs to
    happen in a differentiable way to ensure the gradients can be backpropagated through
    the model. The paper proposes the following computation to compute the final representation
    of the self-attention layer for the input tokens:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 查询、密钥和值都是通过将可训练的权重矩阵与输入令牌的数值表示相乘而生成的。所有这些都需要以可微分的方式进行，以确保梯度可以通过模型进行反向传播。论文提出了以下计算来计算输入令牌的自注意力层的最终表示：
- en: '![13_01a](../../OEBPS/Images/13_01a.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![13_01a](../../OEBPS/Images/13_01a.png)'
- en: 'Here, Q represents the queries, K represents the keys, and V represents values
    for all the inputs and all the tokens in each input in a batch of data. This is
    what makes Transformer models so powerful: unlike LSTM models, Transformer models
    aggregate looking at all tokens in a sequence to a single matrix multiplication,
    making these models highly parallelizable.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，Q 表示查询，K 表示密钥，V 表示批量数据中所有输入和每个输入中所有令牌的值。这就是使 Transformer 模型如此强大的原因：与 LSTM
    模型不同，Transformer 模型将所有令牌在一个序列中聚合到一个矩阵乘法中，使得这些模型高度可并行化。
- en: 13.1.2 Embeddings in the Transformer
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Transformer 中的嵌入
- en: One thing we overlooked when discussing the Transformer model is the embeddings
    used in it. We briefly touched on the word embeddings used. Let’s discuss this
    topic in more detail here. Word embeddings provide a semantic-preserving representation
    of words based on the context in which words are used. In other words, if two
    words are used in the same context, they will have similar word vectors. For example,
    the words “cat” and “dog” will have similar representations, whereas “cat” and
    “volcano” will have vastly different representations.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们讨论 Transformer 模型时，有一件事情被忽略了，那就是它使用的嵌入。我们简要地提到了使用的词嵌入。让我们在这里更详细地讨论这个话题。词嵌入根据单词的上下文提供了语义保持的表示。换句话说，如果两个单词在相同的上下文中使用，它们将具有相似的单词向量。例如，“猫”和“狗”将具有相似的表示，而“猫”和“火山”将具有完全不同的表示。
- en: Word vectors were initially introduced in the paper titled “Efficient Estimation
    of Word Representations in Vector Space” by Mikolov et al. ([https://arxiv.org/pdf/1301.3781.pdf](https://arxiv.org/pdf/1301.3781.pdf)).
    It came in two variants, skip-gram and continuous bag-of-words (CBOW). Because
    skip-gram is somewhat more widely accepted than CBOW, let’s discuss the crux of
    the skip-gram algorithm.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 单词向量最初在 Mikolov 等人的论文中被介绍，题为“Efficient Estimation of Word Representations in
    Vector Space” ([https://arxiv.org/pdf/1301.3781.pdf](https://arxiv.org/pdf/1301.3781.pdf))。它有两个变种，skip-gram
    和 continuous bag-of-words（CBOW）。由于 skip-gram 稍微比 CBOW 更广泛地被接受，让我们讨论 skip-gram
    算法的要点。
- en: The first step is to define a large matrix of size *V* × *E*, where *V* is the
    size of the vocabulary and *E* is the size of the embeddings. The size of the
    embeddings (E) is a user-defined hyperparameter, where a larger E typically leads
    to more powerful word embeddings. In practice, you do not need to increase the
    size of embeddings beyond 300.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是定义一个大小为 *V* × *E* 的大矩阵，其中 *V* 是词汇表的大小，*E* 是嵌入的大小。嵌入的大小（*E*）是用户定义的超参数，其中更大的
    *E* 通常会导致更强大的词嵌入。在实践中，你不需要使嵌入的大小超过 300。
- en: Next, you create inputs and targets in a completely unsupervised manner. Given
    a large corpus of text, you select a word form as the input (probe word) and the
    words surrounding the probe word as targets. The surrounding words are captured
    by defining a fixed-sized window around the probe word. For example, for a window
    size of 2 (on each side of the probe word), you can generate the following input-target
    pairs from the sentence “angry John threw a pizza at me.”
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，完全无监督地创建输入和目标。给定大量的文本语料库，选择一个单词形式作为输入（探针词），并以探针词周围的单词作为目标。通过定义固定大小的探针词周围窗口来捕获周围的单词。例如，针对窗口大小为
    2（在探针词的每一侧），你可以从句子 “angry John threw a pizza at me.” 生成以下输入-目标对。
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: With the labeled data, you can frame the problem of learning word embeddings
    as a classification problem. In other words, you train a model (i.e., a function
    of the word-embedding matrix) to predict the target word, given the input word.
    The model consists of two components, the embedding matrix and a fully connected
    layer with a softmax activation to output the predictions. Once the embeddings
    are learned, you can discard the other things around it (e.g., the fully connected
    layer) and use the embedding matrix for a downstream NLP task such as sentiment
    analysis, machine translation, and so on. You just have to look up the embedding
    vector corresponding to a word in order to obtain a numerical representation for
    this word.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 有了带标签的数据，你可以将学习词嵌入的问题框定为分类问题。换句话说，你训练一个模型（即一个词嵌入矩阵的函数），以输入的词为基础预测目标词。该模型包括两个部分，嵌入矩阵和完全连接层，通过
    softmax 激活输出预测结果。一旦学习了嵌入，你可以丢弃其周围的其他内容（例如完全连接层），并使用嵌入矩阵用于下游 NLP 任务，例如情感分析、机器翻译等。只需要查找与单词相对应的嵌入向量，即可获得该单词的数字表示。
- en: 'Motivated by the original word vector algorithms, modern deep learning models
    combine learning word embeddings and the actual decision-support NLP problem to
    a single model training task. In other words, the following general approach is
    taken to incorporate word embeddings to a machine learning model:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现代深度学习模型受到原始词向量算法的启发，将学习词嵌入和实际的决策支持 NLP 问题融合到单个模型训练任务中。换句话说，以下一般方法用于将词嵌入纳入到机器学习模型中：
- en: Define a randomly initialized word-embedding matrix (or pretrained embeddings
    available to download for free).
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义一个随机初始化的词嵌入矩阵（或提供免费下载的预训练的嵌入）。
- en: Define the model (randomly initialized) that uses word embeddings as the inputs
    and produces an output (e.g., sentiment, a language translation, etc.).
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义使用单词嵌入作为输入并产生输出（例如情感、语言翻译等）的模型（随机初始化）。
- en: Train the whole model (embeddings + the model) end to end on the task.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在任务上训练整个模型（嵌入 + 模型）。
- en: 'The same technique is used in Transformer models. However, in Transformer models,
    there are two different embeddings:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 模型中也使用了相同的技术。但是，Transformer 模型中有两个不同的嵌入：
- en: Token embeddings (which provide a unique representation for each token seen
    by the model in an input sequence)
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Token 嵌入（为模型在输入序列中看到的每个 token 提供唯一的表示）
- en: Positional embeddings (which provide a unique representation for each position
    in the input sequence)
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置嵌入（为输入序列中的每个位置提供唯一的表示）
- en: The token embeddings have a unique embedding vector for each token (e.g., character,
    word, sub-word), depending on the model’s tokenizing mechanism.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Token 嵌入（为模型在输入序列中看到的每个 token 提供唯一的表示）
- en: The positional embeddings are used to signal the model where a token is appearing.
    The primary purpose is for the positional embeddings server to tell the Transformer
    model where a word is appearing. This is because, unlike LSTMs/GRUs, Transformer
    models don’t have a notion of sequence, as they process the whole text at once.
    Furthermore, a change to the position in word can alter the meaning of a sentence
    or a word. For example, in the two versions of
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 位置嵌入被用来告诉模型一个标记出现的位置。其主要目的是为了让位置嵌入服务器告诉变压器模型一个单词出现的位置。这是因为，与LSTMs/GRUs不同，变压器模型没有序列的概念，因为它们一次处理整个文本。此外，单词位置的改变可能会改变句子或单词的含义。例如，在两个版本中
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'the word “it” refers to different things, and the position of the word “it”
    can be used as a cue to identify this difference. The original Transformer paper
    uses the following equations to generate positional embeddings:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 单词“it”指的是不同的东西，单词“it”的位置可以用作识别这种差异的线索。原始的变压器论文使用以下方程式生成位置嵌入：
- en: '*PE*(*pos*,2*i*) = sin(*pos*/10000^(21/d[model]))'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*PE*(*pos*,2*i*) = sin(*pos*/10000^(21/d[model]))'
- en: '*PE*(*pos*,2*i +* 1) = cos(*pos*/10000^(21/d[model]))'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*PE*(*pos*,2*i +* 1) = cos(*pos*/10000^(21/d[model]))'
- en: where pos denotes the position in the sequence and *i* denotes the *i*^(th)
    feature dimension (0 ≤ *i* < *d*[model]). Even-numbered features use a sine function,
    whereas odd-numbered features use a cosine function. Figure 13.2 presents how
    positional embeddings change as the time step and the feature position change.
    It can be seen that feature positions with higher indices have lower frequency
    sinusoidal waves. It is not entirely clear how the authors came up with the exact
    equation. However, they do mention that they did not see a significant performance
    difference between the previous equation and letting the model learn positional
    embeddings jointly during the training.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 其中pos表示序列中的位置，*i*表示*i*^(th)特征维度（0 ≤ *i* < *d*[model]）。偶数特征使用正弦函数，而奇数特征使用余弦函数。图13.2展示了当时间步长和特征位置变化时位置嵌入的变化。可以看到，具有较高索引的特征位置具有较低频率的正弦波。作者确切的方程式并不完全清楚。但是，他们确实提到他们没有看到前一个方程式和让模型在训练期间联合学习位置嵌入之间有显著的性能差异。
- en: '![13-02](../../OEBPS/Images/13-02.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![13-02](../../OEBPS/Images/13-02.png)'
- en: Figure 13.2 How positional embeddings change with the time step and the feature
    position. Even-numbered feature positions use the sine function, whereas odd-numbered
    positions use the cosine function. Additionally, the frequency of the signals
    decreases as the feature position increases.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.2 位置嵌入随时间步长和特征位置的变化。偶数特征位置使用正弦函数，而奇数位置使用余弦函数。此外，信号的频率随着特征位置的增加而降低。
- en: It is important to note that both token and positional embeddings will have
    the same dimensionality (i.e., *d*[model]). Finally, as the input to the model,
    the token embeddings and the positional embeddings are summed to form a single
    hybrid embedding vector (figure 13.3).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，标记嵌入和位置嵌入都具有相同的维度（即*d*[model]）。最后，作为模型的输入，标记嵌入和位置嵌入被求和以形成单个混合嵌入向量（图13.3）。
- en: '![13-03](../../OEBPS/Images/13-03.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![13-03](../../OEBPS/Images/13-03.png)'
- en: Figure 13.3 The embeddings generated in a Transformer model and how the final
    embeddings are computed
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.3 在变压器模型中生成的嵌入以及如何计算最终嵌入
- en: 13.1.3 Residuals and normalization
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.1.3 残差和规范化
- en: Another important characteristic of the Transformer models is the existence
    of the residual connections and the normalization layers in between the individual
    layers. We discussed residual connections in depth in chapter 7 when we discussed
    advance techniques for image classification. Let’s briefly revisit the mechanics
    and the motivation for residual connections.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器模型的另一个重要特征是残差连接和单个层之间的规范化层的存在。当我们讨论图像分类的高级技术时，我们在第7章中深入讨论了残差连接。让我们简要地重新讨论残差连接的机制和动机。
- en: Residual connections are formed by adding a given layer’s output to the output
    of one or more layers ahead. This, in turn, forms “shortcut connections” through
    the model and provides a stronger gradient flow by reducing the changes of the
    phenomenon known as the *vanishing gradients* (figure 13.4). Vanishing gradients
    cause the gradients in the layers closest to the inputs to be very small so that
    the training in those layers is hindered.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 残差连接是通过将给定层的输出添加到前面一个或多个层的输出而形成的。这反过来形成了模型中的“快捷连接”，并通过减少所谓的*梯度消失*现象提供了更强的梯度流（见图13.4）。梯度消失导致最靠近输入的层的梯度非常小，以至于这些层的训练受到阻碍。
- en: '![13-04](../../OEBPS/Images/13-04.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![13-04](../../OEBPS/Images/13-04.png)'
- en: Figure 13.4 Mathematical view of residual connections
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.4 残差连接的数学视角
- en: 'In Transformer models, in each layer, residual connections are created in the
    following way:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Transformer 模型中，每个层次都会创建残差连接，具体如下：
- en: The input to the multi-head self-attention sublayer is added to the output of
    the multi-head self-attention sublayer.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多头自注意力子层的输入被添加到多头自注意力子层的输出中。
- en: The input to the fully connected sublayer is added to the output of the fully
    connected sublayer.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全连接子层的输入被添加到完全连接子层的输出中。
- en: Next, the output reinforced by residual connections goes through a layer normalization
    layer. *Layer normalization*, similar to batch normalization, is a way to reduce
    the “covariate shift” in neural networks, allowing them to be trained faster and
    achieve better performance. Covariate shift refers to changes in the distribution
    of neural network activations (caused by changes in the data distribution) that
    transpire as the model goes through model training. Such changes in the distribution
    hurt consistency during model training and negatively impact the model. Layer
    normalization was introduced in the paper “Layer Normalization” by Ba et al. ([https://arxiv.org/pdf/1607.06450.pdf](https://arxiv.org/pdf/1607.06450.pdf)).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，通过残差连接增强的输出经过一层层归一化层。*层归一化*，类似于批归一化，是减少神经网络中“协变量转移”的一种方式，使其能够更快地训练并达到更好的性能。协变量转移是指神经网络激活分布的变化（由数据分布的变化引起），这些变化在模型训练过程中发生。这种分布的变化会在模型训练期间影响一致性，并对模型产生负面影响。层归一化是由
    Ba 等人在论文“Layer Normalization”中介绍的（[https://arxiv.org/pdf/1607.06450.pdf](https://arxiv.org/pdf/1607.06450.pdf)）。
- en: Batch normalization computes the mean and variance of activations as an average
    over the samples in the batch, causing its performance to rely on mini batch sizes,
    which are used to train the model.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 批归一化计算激活的均值和方差作为批次中样本的平均值，导致其性能依赖于用于训练模型的小批量大小。
- en: However, layer normalization computes the mean and variance (i.e., the normalization
    terms) of the activations in such a way that the normalization terms are the same
    for every hidden unit. In other words, layer normalization has a single mean and
    a variance value for all the hidden units in a layer. This is in contrast to batch
    normalization, which maintains individual mean and variance values for each hidden
    unit in a layer. Moreover, unlike batch normalization, layer normalization does
    not average over the samples in the batch, but rather leaves the averaging out
    and has different normalization terms for different inputs. By having a mean and
    variance per sample, layer normalization gets rid of the dependency on the mini
    batch size. For more details about this method, please refer to the original paper.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，层归一化计算激活的均值和方差（即归一化项）的方式是这样的，即归一化项对每个隐藏单元都是相同的。换句话说，层归一化对于层中的所有隐藏单元都有一个单一的均值和方差值。这与批归一化不同，后者对层中的每个隐藏单元维护单独的均值和方差值。此外，与批归一化不同，层归一化不会对批次中的样本进行平均，而是留下了平均化，对不同的输入具有不同的归一化项。通过每个样本具有一个均值和方差，层归一化摆脱了对小批量大小的依赖。有关此方法的更多细节，请参阅原始论文。
- en: Layer normalization in TensorFlow/Keras
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow/Keras 中的层归一化
- en: TensorFlow provides a convenient implementation of the layer normalization algorithm
    at [http://mng.bz/YGRB](http://mng.bz/YGRB). You can use this layer with any model
    you define using TensorFlow Keras APIs.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 提供了层归一化算法的方便实现，网址为 [http://mng.bz/YGRB](http://mng.bz/YGRB)。你可以使用
    TensorFlow Keras API 定义的任何模型来使用这个层。
- en: Figure 13.5 depicts how residual connections and layer normalization are used
    in Transformer models.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.5 展示了在 Transformer 模型中如何使用残差连接和层归一化。
- en: '![13-05](../../OEBPS/Images/13-05.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![13-05](../../OEBPS/Images/13-05.png)'
- en: Figure 13.5 How residual connections and layer normalization layers are used
    in the Transformer model
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.5 残差连接和层归一化层在 Transformer 模型中的使用方式
- en: With that, we end the discussion about the components in the Transformer model.
    We have discussed all the bells and whistles of the Transformer model, namely
    self-attention layers, fully connected layers, embeddings (token and positional),
    layer normalization, and residual connections. In the next section, we will discuss
    how we can use a pretrained Transformer model known as BERT to solve a spam classification
    task.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论关于 Transformer 模型中的组件到此结束。我们已经讨论了 Transformer 模型的所有要点，即自注意力层、全连接层、嵌入（标记和位置）、层归一化和残差连接。在下一节中，我们将讨论如何使用一个称为
    BERT 的预训练 Transformer 模型来解决垃圾邮件分类任务。
- en: Exercise 1
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 1
- en: You are given the following code for the Transformer encoder
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 你被给定了以下 Transformer 编码器的代码
- en: '[PRE2]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: where the EncoderLayer defines a typical Transformer encoder layer that has
    a self-attention sublayer and a fully connected sublayer. You are asked to integrate
    positional encoding using the equation
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 EncoderLayer 定义了一个典型的 Transformer 编码器层，其中包含自注意力子层和全连接子层。你被要求使用以下方程式集成位置编码
- en: '*PE*(*pos*, 2*i*) = sin(*pos*/10000^(2*i*/d[model]))'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '*PE*(*pos*, 2*i*) = sin(*pos*/10000^(2*i*/d[model]))'
- en: '*pos* goes from 0 to 511 (d=512 features) and *i* goes from 0 to 24 (n_steps=25
    time steps) and denotes the time step. In other words, our positional encoding
    will be a tensor of shape [n_steps, d]. You can use tf.math.sin() to generate
    the sin value element-wise for a tensor. You can define the positional embeddings
    as a tensor and not the product of a tf.keras.layers.Layer. The final embedding
    should be generated by summing the token embeddings and the positional embeddings.
    How would you do that?'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*pos* 从 0 到 511（d=512 特征），*i* 从 0 到 24（n_steps=25 时间步），表示时间步。换句话说，我们的位置编码将是一个形状为
    [n_steps, d] 的张量。你可以使用 tf.math.sin() 逐元素生成张量的 sin 值。你可以将位置嵌入定义为张量，而不是 tf.keras.layers.Layer
    的乘积。最终嵌入应通过将标记嵌入和位置嵌入相加来生成。你会如何做？'
- en: 13.2 Using pretrained BERT for spam classification
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.2 使用预训练的 BERT 进行垃圾邮件分类
- en: You are working as a data scientist for a mail service company, and the company
    is dying to implement a spam classification functionality. They want to implement
    this functionality in house and save dollars. Having read about BERT and how powerful
    it is for solving NLP tasks, you explain to the team that all you need to do is
    download the BERT model, fit a classification layer on top of BERT, and train
    the whole model end to end on labeled data. The labeled data consists of a spam
    message and a label indicating whether the message is spam or ham (i.e., not spam).
    You have been put in charge of implementing this model.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 你正在为一家邮件服务公司担任数据科学家，公司渴望实现垃圾邮件分类功能。他们希望在公司内部实现此功能并节省成本。通过阅读关于 BERT 及其在解决 NLP
    任务中的强大性能的文章，你向团队解释，你需要做的就是下载 BERT 模型，在 BERT 顶部拟合一个分类层，并在标记的数据上端到端地训练整个模型。标记的数据包括一个垃圾消息和一个指示消息是否为垃圾或正常的标签。你被委托负责实现此模型。
- en: 'Now that we have discussed all the moving elements of the Transformer architecture,
    it puts us in a very strong position to understand BERT. BERT is a Transformer-based
    model introduced in the paper “BERT: Pre-Training of Deep Bidirectional Transformers
    for Language Understanding” by Devlin et al. ([https://arxiv.org/pdf/1810.04805.pdf](https://arxiv.org/pdf/1810.04805.pdf)),
    and it represents a very important milestone in the history of NLP as it’s a pioneering
    model that proved the ability to apply “transfer learning” in the domain of NLP.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '现在我们已经讨论了 Transformer 架构的所有移动元素，这使得我们非常有能力理解 BERT。BERT 是一种基于 Transformer 的模型，由
    Devlin 等人在论文 “BERT: Pre-Training of Deep Bidirectional Transformers for Language
    Understanding” 中介绍（[https://arxiv.org/pdf/1810.04805.pdf](https://arxiv.org/pdf/1810.04805.pdf)），它代表了自然语言处理历史上的一个非常重要的里程碑，因为它是一个先驱性模型，证明了在
    NLP 领域应用 “迁移学习”的能力。'
- en: BERT is a Transformer model that is pretrained on large amounts of textual data
    in an unsupervised fashion. Therefore, you can use BERT as the basis to get rich,
    semantically sound numerical representations for textual input sequences that
    can be readily fed to your downstream NLP models. Because of the rich textual
    representations provided by BERT, you’re relieving your decision support model
    of the need to understand the language and instead can directly focus on the problem
    at hand. From a technical perspective, if you’re solving a classification problem
    with BERT, all you need to do is
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 是一个在大量文本数据上以无监督方式预训练的 Transformer 模型。因此，你可以使用 BERT 作为基础，获得丰富、语义上准确的文本输入序列的数字表示，这些表示可以直接提供给下游
    NLP 模型。由于 BERT 提供的丰富文本表示，你可以让你的决策支持模型不再需要理解语言，而是可以直接专注于手头的问题。从技术角度来看，如果你正在用 BERT
    解决分类问题，你所需要做的就是
- en: Fit a classifier(s) (e.g., a logistic regression layer) on top of BERT, which
    takes BERT’s output(s) as the input(s)
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 BERT 之上拟合一个分类器（例如逻辑回归层），将 BERT 的输出作为输入
- en: Train the model (i.e., BERT + classifier(s)) end to end on the discriminative
    task
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在判别性任务上（即 BERT + 分类器）端到端地训练模型
- en: History of BERT
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 的历史
- en: Before models like BERT, solving natural language processing (NLP) tasks was
    both repetitive and time-consuming. Every time, you had to train a model from
    scratch. And to exacerbate this suboptimal way of tackling problems, most models
    struggled with long sequences of text, limiting their ability to understand language.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在像 BERT 这样的模型出现之前，解决自然语言处理（NLP）任务既重复又耗时。每次都需要从头开始训练一个模型。更糟糕的是，大多数模型都无法处理长文本序列，限制了它们理解语言的能力。
- en: In 2017, Transformer models for NLP tasks were proposed in the paper “Attention
    Is All You Need” ([https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf)).
    Transformer models beat previous dominants like LSTMs and GRUs across the board
    on a collection of NLP tasks. Transformer models, unlike recurrent models that
    look at one word at a time and maintain a state (i.e., memory), look at the whole
    sequence at once.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 2017 年，NLP 任务的 Transformer 模型在论文“Attention Is All You Need”（[https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf)）中提出。Transformer
    模型在一系列 NLP 任务上击败了之前的主导者，如 LSTMs 和 GRUs。与逐字逐句查看并维护状态（即内存）的循环模型不同，Transformer 模型一次查看整个序列。
- en: Then, in 2018, the “ImageNet moment” arrived for NLP (i.e., transfer learning
    in NLP). The ImageNet moment refers to the moment when ML practitioners realized
    that using a computer vision model that is already trained on the large ImageNet
    image classification data set on other tasks (e.g., object detection, image segmentation)
    yields better performance faster. This essentially gave rise to the concept of
    transfer learning that is heavily used in the computer vision domain. So, until
    2018, the NLP domain did not have a very good way to employ transfer learning
    to uplift the performance on tasks. The paper “Universal Language Model Fine-Tuning
    for Text Classification” ([https://arxiv.org/pdf/1801.06146.pdf](https://arxiv.org/pdf/1801.06146.pdf))
    introduced the idea of using pretraining on a language modeling task and then
    training the model on a discriminative task (e.g., a classification problem).
    The advantage of this approach was that you did not need as many samples as if
    you were to train the model from scratch.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在 2018 年，NLP（即在 NLP 中进行迁移学习）迎来了“ImageNet 时刻”。ImageNet 时刻是指 ML 从业者意识到，在其他任务（如目标检测、图像分割）上使用已经在大型
    ImageNet 图像分类数据集上训练过的计算机视觉模型，可以更快地获得更好的性能。这实际上催生了在计算机视觉领域广泛使用的迁移学习概念。因此，直到 2018
    年，NLP 领域还没有一个非常好的方法来利用迁移学习来提升任务性能。论文“通用语言模型微调用于文本分类”（[https://arxiv.org/pdf/1801.06146.pdf](https://arxiv.org/pdf/1801.06146.pdf)）介绍了在语言建模任务上预训练然后在判别性任务上训练模型的思想（例如，分类问题）。这种方法的优势在于，你不需要像从头训练模型那样多的样本。
- en: In 2018, BERT was introduced. It was a marriage of two of the finest moments
    in the history of NLP. In other words, BERT is a Transformer model that is pretrained
    on large amounts of textual data in an unsupervised fashion.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 2018 年，BERT 被引入。这是自然语言处理历史上两个最出色时刻的结合。换句话说，BERT 是一个在大量文本数据上以无监督方式预训练的 Transformer
    模型。
- en: We will now examine the BERT model in more detail.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将更详细地了解 BERT 模型。
- en: 13.2.1 Understanding BERT
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2.1 理解 BERT
- en: Let’s now inspect BERT more microscopically. As I alluded to earlier, BERT is
    a Transformer model. To be exact, it’s the encoder part of the Transformer model.
    This means that BERT takes an input sequence (a collection of tokens) and produces
    an encoded output sequence. Figure 13.6 depicts the high-level architecture of
    BERT.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们更微观地检查BERT。正如我之前提到的，BERT是一个Transformer模型。确切地说，它是Transformer模型的编码器部分。这意味着BERT接受一个输入序列（一组标记）并生成一个编码的输出序列。图13.6描述了BERT的高层架构。
- en: '![13-06](../../OEBPS/Images/13-06.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![13-06](../../OEBPS/Images/13-06.png)'
- en: Figure 13.6 The high-level architecture of BERT. It takes a set of input tokens
    and produces a sequence of hidden representations generated using several hidden
    layers.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.6 BERT的高层架构。它接受一组输入标记并生成使用几个隐藏层生成的隐藏表示的序列。
- en: When BERT takes an input, it inserts some special tokens into the input. First,
    at the beginning, it inserts a [CLS] (an abbreviated form of the term classification)
    token that is used to generate the final hidden representation for certain types
    of tasks (e.g., sequence classification). It represents the output after attending
    to all the tokens in the sequence. Next, it also inserts an [SEP] (i.e., “separation”)
    token depending on the type of input. The [SEP] token marks the end and beginning
    of different sequences in the input. For example, in question answering, the model
    takes a question and a context (e.g., paragraph) that may have the answer as an
    input, and [SEP] is used in between the question and the context.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 当BERT接受一个输入时，它会在输入中插入一些特殊的标记。首先，在开始时，它插入一个[CLS]（分类的缩写形式）标记，用于生成特定类型任务（例如，序列分类）的最终隐藏表示。它表示在关注序列中的所有标记后的输出。接下来，根据输入类型，它还会插入一个[SEP]（即“分隔”）标记。[SEP]标记标记了输入中不同序列的结束和开始。例如，在问答中，模型接受问题和可能包含答案的上下文（例如，段落）作为输入，并且[SEP]在问题和上下文之间使用。
- en: Next, the final embedding of the tokens is generated using three different embedding
    spaces. The token embedding has a unique vector for each token in the vocabulary.
    The positional embeddings encode the position of each token, as discussed earlier.
    Finally, the segment embedding provides a distinct representation for each subcomponent
    in the input when the input consists of multiple components. For example, in question
    answering, the question will have a unique vector as its segment embedding vector,
    whereas the context will have a different embedding vector. This is done by having
    *n* different embedding vectors for the *n* different components in the input
    sequence. Depending on the component index specified for each token in the input,
    the corresponding segment-embedding vector is retrieved. *n* needs to be specified
    in advance.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用三种不同的嵌入空间生成标记的最终嵌入。标记嵌入为词汇表中的每个标记提供了一个独特的向量。位置嵌入编码了每个标记的位置，如前面讨论的。最后，段落嵌入为输入的每个子组件提供了一个不同的表示。例如，在问答中，问题将具有作为其段落嵌入向量的唯一向量，而上下文将具有不同的嵌入向量。这通过在输入序列中的每个不同组件的*n*个不同嵌入向量来完成。根据输入中为每个标记指定的组件索引，检索相应的段落嵌入向量。*n*需要事先指定。
- en: 'The real value of BERT comes from the fact that it has been pretrained on a
    large corpus of data in a self-supervised fashion. In the pretraining stage, BERT
    is trained on two different tasks:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: BERT的真正价值来自于它是以自监督方式在大型语料库上预训练的事实。在预训练阶段，BERT在两个不同的任务上进行训练：
- en: Masked language modeling (MLM)
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 掩码语言建模（MLM）
- en: Next-sentence prediction (NSP)
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下一句预测（NSP）
- en: The masked language modeling (MLM) task is inspired by the *Cloze task* or the
    *Cloze test*, where a student is given a sentence with one or more blanks and
    is asked to fill the blanks. Similarly, given a text corpus, words are masked
    from sentences and then the model is asked to predict the masked tokens. For example,
    the sentence
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 掩码语言建模（MLM）任务灵感来自*填空题*或*填空测验*，其中学生被给出一句带有一个或多个空白的句子，并被要求填写空白。类似地，给定一个文本语料库，单词从句子中被掩码，然后模型被要求预测掩码的标记。例如，句子
- en: '[PRE3]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: might become
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 可能变成
- en: '[PRE4]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: NOTE There has been a plethora of Transformer-based models, each building on
    the previous. You can read more about these models in appendix C.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：已经有大量基于Transformer的模型，每个模型都在前一个模型的基础上进行了构建。您可以在附录C中了解更多关于这些模型的信息。
- en: 'Figure 13.7 illustrates the main components of BERT during the training of
    the masked language modeling task. BERT uses a special token ([MASK]) to represent
    masked words. Then the target for the model will be the word “bakery.” But this
    introduces a practical issue to the model. The special token [MASK] does not appear
    in the actual text. This means that the text the model will see during the fine-tuning
    phase (i.e., when training on a classification problem) will be different than
    what it will see during pretraining. This is sometimes referred to as the *pretraining-fine-tuning
    discrepancy*. Therefore, the authors of BERT suggest the following approach to
    cope with the issue. When masking a word, do one of the following:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.7显示了在遮蔽语言建模任务训练过程中BERT的主要组成部分。BERT使用特殊标记（[MASK]）来表示被遮蔽的单词。然后模型的目标将是单词"bakery"。但这给模型带来了实际问题。特殊标记[MASK]在实际文本中不会出现。这意味着模型在关键问题的微调阶段（即在分类问题上训练时）看到的文本将与在预训练阶段看到的文本不同。这有时被称为*预训练-微调不一致性*。因此，BERT的作者建议采取以下方法来处理这个问题。当屏蔽一个单词时，做以下之一：
- en: Use the [MASK] token as it is (with 80% probability).
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用[MASK]标记（使用80％的概率）。
- en: Use a random word (with 10% probability).
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用随机单词（以10％的概率）。
- en: Use the true word (with 10% probability).
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用真实的单词（以10％的概率）。
- en: '![13-07](../../OEBPS/Images/13-07.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![13-07](../../OEBPS/Images/13-07.png)'
- en: 'Figure 13.7 The methodology used for pretraining BERT. BERT is pretrained on
    two tasks: a masked language modeling task and the next sentence prediction task.
    In the masked language modeling task, the tokens in the input are masked and the
    model is asked to predict masked tokens. In the next sentence prediction task,
    the model is asked to predict if given two sentences are next to each other.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.7显示了预训练BERT使用的方法。BERT在两个任务上进行预训练：遮蔽语言建模任务和下一句预测任务。在遮蔽语言建模任务中，输入中的标记被遮蔽，模型被要求预测被遮蔽的标记。在下一句预测任务中，模型被要求预测两个句子是否相邻。
- en: 'Next, in the next sentence prediction task, the model is given a pair of sentences,
    A and B (in that order), and is asked to predict whether B is the next sentence
    after A. This can be done by fitting a binary classifier on top of BERT and training
    the whole model end to end on selected pairs of sentences. Generating pairs of
    sentences as inputs to the model is not hard and can be done in an unsupervised
    manner:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在下一句预测任务中，模型会得到一对句子A和B（按照顺序），并被要求预测B是否是A之后的下一句。可以通过在BERT之上拟合一个二元分类器，并在选择的句子对上端到端地训练整个模型来完成。以无监督的方式生成模型的输入对是不难的：
- en: A sample with the label TRUE is generated by picking two sentences that are
    adjacent to each other.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过选择相邻的两个句子生成标签为TRUE的样本。
- en: A sample with the label FALSE is generated by picking two sentences randomly
    that are not adjacent to each other.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过随机选择不相邻的两个句子生成标签为FALSE的样本。
- en: Following this approach, a labeled data set is generated for the next sentence
    prediction task. Then BERT, along with the binary classifier, is trained end to
    end using the labeled data set. Figure 13.7 highlights the data and the model
    architecture in the next sentence prediction task.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 按照这种方法，为下一句预测任务生成了一个带标签的数据集。然后，使用带标签的数据集对BERT和二元分类器进行端到端的训练。图13.7突出显示了下一句预测任务中的数据和模型架构。
- en: You might have noticed in figure 13.6 that the input to BERT has special tokens.
    There are two special tokens (in addition to the [MASK] token we already discussed)
    that serve special purposes.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到图13.6中输入到BERT的是特殊标记。除了我们已讨论过的[MASK]标记之外，还有两个特殊的标记具有特殊的用途。
- en: The [CLS] token is appended to any input sequence fed to BERT. This denotes
    the beginning of the input. It also forms the basis for the input fed into the
    classification head used on top of BERT to solve your NLP task. As you know, BERT
    produces a hidden representation for each input token in the sequence. As a convention,
    the hidden representation corresponding to the [CLS] token is used as the input
    to the classification model that sits on top of BERT.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[CLS]标记被附加到输入到BERT的任何输入序列上。它表示输入的开始。它还为放置在BERT顶部的分类头上使用的输入提供基础。正如您所知，BERT对序列中的每个输入标记生成一个隐藏表示。按照惯例，与[CLS]标记对应的隐藏表示被用作放置在BERT之上的分类模型的输入。'
- en: 'The task-specific NLP tasks solved by BERT can be classified into four different
    categories. These are based on the tasks found in the General Language Understanding
    Evaluation (GLUE) benchmark task suite ([https://gluebenchmark.com](https://gluebenchmark.com)):'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 解决的任务特定 NLP 任务可以分为四种不同的类别。这些基于 General Language Understanding Evaluation（GLUE）基准任务套件中的任务
    ([https://gluebenchmark.com](https://gluebenchmark.com))：
- en: '*Sequence classification*—Here, a single input sequence is given and the model
    is asked to predict a label for the whole sequence (e.g., sentiment analysis,
    spam identification).'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*序列分类*—这里，给定一个单一的输入序列，并要求模型为整个序列预测一个标签（例如，情感分析，垃圾邮件识别）。'
- en: '*Token classification*—Here, a single input sequence is given and the model
    is asked to predict a label for each token in the sequence (e.g., named entity
    recognition, part-of-speech tagging).'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*令牌分类*—这里，给定一个单一的输入序列，并要求模型为序列中的每个令牌预测一个标签（例如，命名实体识别，词性标注）。'
- en: '*Question answering*—Here, the input consists of two sequences: a question
    and a context. The question and the context are separated by an [SEP] token. The
    model is trained to predict the starting and ending indices of the span of tokens
    belonging to the answer.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*问答*—这里，输入包括两个序列：一个问题和一个上下文。问题和上下文之间由一个 [SEP] 令牌分隔。模型被训练来预测属于答案的令牌范围的起始和结束索引。'
- en: '*Multiple choice*—Here the input consists of multiple sequences: a question
    followed by multiple candidates that may or may not be the answer to the question.
    These multiple sequences are separated by the token [SEP] and provided as a single
    input sequence to the model. The model is trained to predict the correct answer
    (i.e., the class label) for that question.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*多选题*—这里的输入由多个序列组成：一个问题，后跟可能是或可能不是问题答案的多个候选项。这些多个序列由令牌 [SEP] 分隔，并作为单个输入序列提供给模型。模型被训练来预测该问题的正确答案（即，类标签）。'
- en: 'BERT is designed in such a way that it can be used to solve these tasks without
    any modifications to the base model. In tasks that involve multiple sequences
    (e.g., question answering, multiple-choice questions), you need to tell the model
    different inputs separately (e.g., which tokens are the question and which tokens
    are the context in the question-answering task). In order to make that distinction,
    the [SEP] token is used. An [SEP] token is inserted between the different sequences.
    For example, if you are solving a question-answering task, you might have an input
    as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 的设计使得它能够在不对基础模型进行任何修改的情况下用于解决这些任务。在涉及多个序列的任务中（例如，问答，多选题），您需要单独告诉模型不同的输入（例如，问题的令牌和问题回答任务中的上下文的令牌）。为了做出这种区分，使用
    [SEP] 令牌。[SEP] 令牌在不同序列之间插入。例如，如果您正在解决一个问答任务，您可能会有一个输入如下：
- en: '[PRE5]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Then the input to BERT might look like
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，BERT 的输入可能如下所示
- en: '[PRE6]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'BERT also uses a segment-embedding space to denote which sequence a token belongs
    to. For example, inputs having only one sequence have the same segment-embedding
    vector for all the tokens (e.g., spam classification task). Inputs having two
    or more sequences use the first or second space depending on which sequence the
    token belongs to. For example, in question answering, the model will use a unique
    segment-embedding vector to encode tokens of the question, where it will use a
    different segment-embedding vector to encode the tokens of the context. Now we
    have discussed all the elements of BERT needed to use it successfully to solve
    a downstream NLP task. Let’s reiterate the key points about BERT:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 还使用段落嵌入空间来表示一个令牌属于哪个序列。例如，只有一个序列的输入对于所有令牌具有相同的段落嵌入向量（例如，垃圾邮件分类任务）。具有两个或更多序列的输入使用第一个或第二个空间，取决于令牌属于哪个序列。例如，在问答中，模型将使用唯一的段落嵌入向量来编码问题的令牌，其中将使用不同的段落嵌入向量来编码上下文的令牌。现在我们已经讨论了使用
    BERT 成功解决下游 NLP 任务所需的所有要素。让我们重申一下有关 BERT 的关键要点：
- en: BERT is an encoder-based Transformer that is pretrained on large amounts of
    text.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT 是一种基于编码器的 Transformer，经过大量文本的预训练。
- en: BERT uses masked language modeling and next-sentence prediction tasks to pretrain
    the model.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT 使用掩码语言建模和下一句预测任务进行模型的预训练。
- en: BERT outputs a hidden representation for every token in the input sequence.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT 为输入序列中的每个令牌输出隐藏表示。
- en: 'BERT has three embedding spaces: token embedding, positional embedding, and
    segment embedding.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT 有三个嵌入空间：令牌嵌入，位置嵌入和段落嵌入。
- en: BERT uses a special token [CLS] to denote the beginning of an input and is used
    as the input to a downstream classification model.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BERT is designed to solve four types of NLP tasks: sequence classification,
    token classification, free-text question answering, and multiple-choice question
    answering.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BERT uses the special token [SEP] to separate sequence A and sequence B.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will learn how we can classify spam messages with BERT.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 13.2.2 Classifying spam with BERT in TensorFlow
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It’s now time to show off your skills by implementing a spam classifier with
    minimal effort. First, let’s download data. The data we will use for this exercise
    is a collection of spam and ham (non-spam) SMS messages available at [http://mng.bz/GE9v](http://mng.bz/GE9v).
    The Python code for downloading the data has been provided in the Ch13-Transormers-with-TF2-and-Huggingface/13.1_Spam_Classification_with_BERT.ipynb
    notebook.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the data
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you download the data and extract it, we can quickly look at what’s in
    the data. It will be a single tab-separated text file. The first three entries
    of the file are as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As shown, each line starts with the word ham or spam, indicating whether it’s
    safe or spam. Then the text in the message is given, followed by a tab. Our next
    task is to load this data into memory and store the inputs and labels in NumPy
    arrays. The following listing shows the steps to do so.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.1 Loading the data from the text file into NumPy arrays
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Inputs (messages) are stored here.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: ❷ The label (0/1) is stored here.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Counts the total number of ham/spam examples
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Read every row in the file.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: ❺ If the line starts with ham, it’s a ham example.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Assign it a label of 0.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Input is the text in the line (except for the word starting ham).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Increase the count n_ham.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: ❾ If the line starts with spam, it’s spam.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: ❿ Assign it a label of 1.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: ⓫ Input is the text in the line (except for the word that starts spam).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: ⓬ Append the input text to inputs.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: ⓭ Append the labels to labels.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: ⓮ Convert inputs to a NumPy array (and reshape it to a matrix with one column).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: ⓯ Convert the labels list to a NumPy array.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: You can print the n_ham and n_spam variables and verify that there are 4,827
    ham examples and 747 spam examples. In other words, there are fewer spam examples
    than ham examples. Therefore, we have to make sure to account for this imbalance
    when training the model.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Treating class imbalance in the data
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'To counteract class imbalance, let’s create balanced training/validation and
    testing data sets. To do that, we will use the imbalanced-learn library, which
    is a great library for manipulating imbalanced data sets (e.g., sampling various
    amounts of data from different classes). There are two primary strategies for
    restoring balance in a data set:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Undersampling the majority class (fewer samples from that class are selected
    for the final data set)
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oversampling the minority class (more samples from that class are generated
    for the final data set)
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will use the first strategy here (i.e., undersampling the majority class).
    More specifically, we will first
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里使用第一种策略（即欠采样多数类）。更具体地说，我们将首先
- en: Create balanced testing and validation data sets by randomly sampling data from
    the data set (n examples per each class)
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过从数据集中随机抽样数据来创建平衡的测试和验证数据集（每个类别n个示例）
- en: Assign the rest of the data to a training set and undersample the majority class
    in the training set using an algorithm known as the *near-miss* algorithm
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将剩余的数据分配给训练集，并使用一种称为*near-miss*算法的算法对训练集中的多数类进行欠采样。
- en: The creation of the training validation and testing data is illustrated in figure
    13.8.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 创建训练验证和测试数据的过程如图13.8所示。
- en: '![13-08](../../OEBPS/Images/13-08.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![13-08](../../OEBPS/Images/13-08.png)'
- en: Figure 13.8 How the training, validation, and testing data sets are created
    from the original data set
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.8 从原始数据集创建训练、验证和测试数据集的过程
- en: 'First, let’s import a few under-samplers from the library and the NumPy library:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们从库和NumPy库中导入一些欠采样器：
- en: '[PRE9]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, we will define a variable, n, which denotes how many examples per class
    we will keep in the validation and testing data sets:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义一个变量n，它表示在验证和测试数据集中每个类别要保留多少个样本：
- en: '[PRE10]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then we will define a random under-sampler. The most important parameter is
    the sampling_strategy parameter, which takes a dictionary with the label as the
    key and the number of samples required for that label as the value. We will also
    pass random_seed to the random_state argument to make sure we get the same result
    every time we run the code:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将定义一个随机欠采样器。最重要的参数是sampling_strategy参数，它接受一个字典，其中键是标签，值是该标签所需的样本数量。我们还将通过random_state参数传递random_seed以确保每次运行代码时都获得相同的结果：
- en: '[PRE11]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Then we call the fit_resample() function of the under-sampler, with the inputs
    and labels arrays we created, to sample data:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们调用欠采样器的fit_resample()函数，使用我们创建的inputs和labels数组来采样数据：
- en: '[PRE12]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Once you fit the under-sampler, you can get the indices of the selected samples
    using the under-sampler’s sample_indices_ attribute. Using those indices, we will
    create a new pair of arrays, test_x and test_y, to hold the test data:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您适应了欠采样器，您可以使用欠采样器的sample_indices_属性获取所选样本的索引。使用这些索引，我们将创建一对新的数组test_x和test_y来保存测试数据：
- en: '[PRE13]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The indices that are not in the test data set are assigned to separate arrays:
    rest_x and rest_y. These will be used to create the validation data set and the
    training data set:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 不在测试数据集中的索引被分配到不同的数组：rest_x 和 rest_y。这些将被用来创建验证数据集和训练数据集：
- en: '[PRE14]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Following the same approach, we undersample data from rest_x and rest_y to
    create the validation data set (valid_x and valid_y). Note that we are not using
    the inputs and labels arrays, but rather the remaining data of those arrays after
    separating the test data:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前的方法类似，我们从rest_x和rest_y中欠采样数据来创建验证数据集（valid_x和valid_y）。请注意，我们不使用inputs和labels数组，而是使用这些数组分离出测试数据后剩余的数据：
- en: '[PRE15]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Finally, we create the training data set, which will hold all the remaining
    elements after creating the test and validation data sets:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们创建训练数据集，该数据集将保存在创建测试和验证数据集后剩余的所有元素：
- en: '[PRE16]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We also have to make sure that the training data set is balanced. In order
    to do that, let’s use a smarter way to undersample data than randomly selecting
    elements. The undersampling algorithm we will use here is called the near-miss
    algorithm. The near-miss algorithm removes samples from the majority class that
    are too close to the samples in the minority class. This helps to increase the
    distance between minority and majority class examples. Here, the minority class
    refers to the class having less data and the majority class refers to the class
    that has more data. To use the near-miss algorithm, it needs to able to compute
    the distance between two samples. Therefore, we need to convert our text into
    some numerical representation. We will represent each message as a bag-of-words
    representation. By using scikit-learn’s CountVectorizer, we can easily do that:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还必须确保训练数据集是平衡的。为了做到这一点，让我们使用比随机选择元素更智能的方式来欠采样数据。我们将在这里使用的欠采样算法称为near-miss算法。near-miss算法会删除与少数类别中的样本太接近的多数类别中的样本。这有助于增加少数类别和多数类别示例之间的距离。在这里，少数类别指的是数据较少的类别，而多数类别指的是数据较多的类别。为了使用near-miss算法，它需要能够计算两个样本之间的距离。因此，我们需要将我们的文本转换为一些数值表示。我们将使用scikit-learn的CountVectorizer来实现这一点：
- en: '[PRE17]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'train_bow will contain the bag-of-word representations of our data. Then we
    can pass this to a NearMiss instance. The way to obtain the data is the same as
    before:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: train_bow 将包含我们数据的词袋表示。然后我们可以将其传递给 NearMiss 实例。获取数据的方式与之前相同：
- en: '[PRE18]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let’s print the sizes of our data sets and see if they have the sizes we wanted
    in the first place:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打印出我们数据集的大小，看看它们是否与我们最初想要的大小相匹配：
- en: '[PRE19]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Excellent! Our data sets are all balanced, and we are ready to press ahead with
    the rest of the workflow.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！我们的数据集都是平衡的，我们准备继续进行工作流的其余部分。
- en: Defining the model
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 定义模型
- en: With the data prepared and ready, we will download the model. The BERT model
    we will use is from the TensorFlow hub ([https://www.tensorflow.org/hub](https://www.tensorflow.org/hub)).
    The TensorFlow hub is a model repository for various models trained on various
    tasks. You can get models for a multitude of tasks, including image classification,
    object detection, language modeling, question answering, and so on. To see the
    full list of available models, check out [https://tfhub.dev/](https://tfhub.dev/).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好数据后，我们将下载模型。我们将使用的 BERT 模型来自 TensorFlow hub ([https://www.tensorflow.org/hub](https://www.tensorflow.org/hub))。TensorFlow
    hub 是各种模型训练任务的模型仓库。您可以获得多种任务的模型，包括图像分类、对象检测、语言建模、问答等等。要查看所有可用模型的完整列表，请访问 [https://tfhub.dev/](https://tfhub.dev/)。
- en: 'In order for us to successfully use BERT for an NLP task, we need three important
    things to work:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 为了成功地利用BERT来完成自然语言处理任务，我们需要三个重要的工作：
- en: '*A tokenizer*—Determines how to split the provided input sequence to tokens'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分词器* —— 确定如何将提供的输入序列分割为标记'
- en: '*An encoder*—Takes in the tokens, computes numerical representations, and finally
    generates a hidden representation for each token as well as a pooled representation
    (a single representation of the whole sequence)'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*编码器* —— 接受标记，计算数值表示，并最终为每个标记生成隐藏表示以及汇总表示（整个序列的单一表示）'
- en: '*A classification head*—Takes in the pooled representation and generates a
    label for the input'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分类头* —— 接受汇总表示并为输入生成标签'
- en: First, let’s examine the tokenizer. The tokenizer takes a single input string
    or a list of strings and converts it to a list of strings or a list of lists of
    strings, respectively. It does this by breaking each string into smaller elements.
    For example, a sentence can be broken into words by splitting it on the space
    character. The tokenizer in BERT uses an algorithm known as the *WordPiece* algorithm
    ([http://mng.bz/z40B](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf)).
    It uses an iterative approach to find sub-words (i.e., parts of words) that appear
    commonly in a data set. The details of the WordPiece algorithm are out of scope
    for this book. Feel free to consult the original paper to learn more details.
    The most important characteristic of the tokenizer for this discussion is that
    it will break a given input string (e.g., a sentence) into a list of smaller tokens
    (e.g., sub-words).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们来看一下分词器。分词器接受单个输入字符串或字符串列表，并将其转换为字符串列表或字符串列表，分别将其拆分为较小的元素。例如，可以通过在空格字符上分割来将句子分割为单词。BERT
    中的分词器使用一种称为 *WordPiece* 算法的算法 ([http://mng.bz/z40B](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf))。它使用迭代方法找到数据集中常见的子词（即单词的部分）。WordPiece
    算法的细节超出了本书的范围。欢迎查阅原始论文以了解更多细节。对于本讨论而言，分词器的最重要特征是它将给定的输入字符串（例如，句子）分解为较小标记的列表（例如，子词）。
- en: Advantage of sub-word approaches like the WordPiece algorithm
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 像 WordPiece 算法这样的子词方法的优点
- en: Sub-word approaches like the WordPiece algorithm learn smaller, commonly occurring
    parts of words and use them to define the vocabulary. There are two main advantages
    to this approach compared to having whole words in our vocabulary.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 像 WordPiece 算法这样的子词方法学习单词的较小且常见的部分，并使用它们来定义词汇表。与将整个单词放入词汇表相比，此方法有两个主要优点。
- en: 'Using sub-words often reduces the size of the vocabulary. Assume the words
    [“walk”, “act”, “walked”, “acted”, “walking”, “acting”]. If individual words are
    used, each word needs to be a single item in the vocabulary. However, if a sub-word
    approach is used, the vocabulary can be reduced to [“walk”, “act”, “##ed”, “##ing”],
    which only has four words. Here, the ## means that it needs to be prefixed with
    another sub-word.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 使用子词通常可以减少词汇量的大小。假设单词为[“walk”, “act”, “walked”, “acted”, “walking”, “acting”]。如果使用单独的单词，每个单词都需要在词汇表中成为一个单一项目。然而，如果使用子词方法，词汇表可以缩减为[“walk”,
    “act”, “##ed”, “##ing”]，只有四个单词。在这里，##表示它需要添加另一个子词作为前缀。
- en: 'Secondly, a sub-word approach can handle out-of-vocabulary words. This means
    the sub-word approach can represent words that appear in the test data set, but
    not in the training set. A word-based approach will not be able to do so and will
    simply replace the unseen word with a special token. Assume the sub-word vocabulary
    [“walk”, “act”, “##ed”, “##ing”, “develop”]. Even if the words “developed” or
    “developing” don’t appear in the training data, the vocabulary can still represent
    these words by combining two sub-words from the vocabulary (e.g., developed =
    develop + ##ed).'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '第二，子词方法可以处理词汇表中未出现的单词。这意味着子词方法可以通过组合来自词汇表的两个子词（例如，developed = develop + ##ed）来表示出现在测试数据集中但未出现在训练集中的单词。基于单词的方法将无法这样做，而只能使用特殊标记替换看不见的单词。假设子词表[“walk”,
    “act”, “##ed”, “##ing”, “develop”]。即使训练数据中没有出现“developed”或“developing”这些单词，词汇表仍然可以通过组合来自词汇表的两个子词来表示这些单词。'
- en: 'To set up the tokenizer, let’s first import the tf-models-official library:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 要设置标记器，让我们首先导入tf-models-official库：
- en: '[PRE20]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Then you can define a tokenizer as follows:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以按以下方式定义标记器：
- en: '[PRE21]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Here, you first get the location of the vocabulary file and define some configurations,
    such as whether the text should be converted to lowercase before tokenizing it.
    The vocabulary file is a text file with one sub-word in each line. This tokenizer
    uses Fast WordPiece Tokenization ([https://arxiv.org/abs/2012.15524.pdf](https://arxiv.org/abs/2012.15524.pdf)),
    an efficient implementation of the original WordPiece algorithm. Note that the
    vocab_file and do_lower_case are settings found in the model artifacts we will
    be downloading from TensorFlow hub in the next step. But for ease of understanding,
    we define them as constants here. You will find the code to automatically extract
    them from the model in the notebook. Next, we can use the tokenizer as follows
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你首先需要获取词汇文件的位置，并定义一些配置，比如在对文本进行标记化之前是否应该将其转换为小写。词汇文件是一个文本文件，每行都有一个子词。这个标记器使用了Fast
    WordPiece Tokenization（[https://arxiv.org/abs/2012.15524.pdf](https://arxiv.org/abs/2012.15524.pdf)），这是原始WordPiece算法的高效实现。请注意，vocab_file和do_lower_case是我们将在下一步从TensorFlow
    hub下载的模型工件中找到的设置。但为了方便理解，我们在这里将它们定义为常量。您将在笔记本中找到自动提取它们的代码。接下来，我们可以按照以下方式使用标记器：
- en: '[PRE22]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: which returns
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 它返回
- en: '[PRE23]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'You can see here how BERT’s tokenizer is tokenizing the sentence. Some words
    are kept as they are, whereas some words are split into sub-words (e.g., seas
    + ##hell + ##s). As discussed earlier, the ## means that it does not mark the
    beginning of a word. In other words, ## shows that this sub-word needs to be prefixed
    with another sub-word to get an actual word. Now, let’s look at the special tokens
    that are used by the BERT model and what IDs are assigned to them. This also validates
    that these tokens exist in the tokenizer:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '您可以在这里看到BERT的标记器如何标记句子。有些单词保持原样，而有些单词则分成子词（例如，seas + ##hell + ##s）。如前所述，##表示它不标记一个单词的开头。换句话说，##表示这个子词需要添加另一个子词作为前缀才能得到一个实际的单词。现在，让我们来看看BERT模型使用的特殊标记以及为它们分配的ID是什么。这也验证了这些标记存在于标记器中：'
- en: '[PRE24]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This returns
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回
- en: '[PRE25]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Here, the [PAD] is another special token used by BERT to denote padded tokens
    (0s). Padding is used very commonly in NLP to bring sentences with different lengths
    to the same length by padding the sentences with zeros. Here, the [PAD] token
    corresponds to zero.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，[PAD]是BERT用来表示填充令牌（0）的另一个特殊标记。在NLP中，常常使用填充来将不同长度的句子填充到相同的长度，填充的句子是用零填充的。在这里，[PAD]标记对应了零。
- en: With the basic functionality of the tokenizer understood, we can define a function
    called encode_sentence() that will encode a given sentence to an input understood
    by the BERT model (see the next listing).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 了解了标记器的基本功能后，我们可以定义一个名为encode_sentence()的函数，将给定的句子编码为BERT模型所理解的输入（请参见下一个清单）。
- en: Listing 13.2 Encoding a given input string using BERT’s tokenizer
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 清单13.2 使用BERT的标记器对给定的输入字符串进行编码
- en: '[PRE26]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: ❶ Add the special [CLS] and [SEP] tokens to the sequence and get the token IDs.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Return the token IDs.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: In this function, we return the tokenized output, first adding the [CLS] token,
    then tokenizing the given string to a list of sub-words, and finally adding the
    [SEP] token to mark the end of the sentence/sequence. For example, the sentence
    “I like ice cream”
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: will return
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: As we can see, token ID 101 (i.e., [CLS]) is at the beginning, and 102 (i.e.,
    [SEP]) is at the end. The rest of the token IDs correspond to the actual string
    we input. It is not enough to tokenize inputs for BERT; we also have to define
    some extra inputs to the model. For example, the sentence
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'should return a data structure like the following:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Let’s discuss the various elements in this data structure. BERT takes in an
    input in the form of a dictionary where
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: The key input_ids represents the token_ids obtained from the encode_sentence
    function previously defined
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The key input_mask represents a mask the same size as the input_ids of 1s and
    0s, where ones indicate values that should not be masked (e.g., actual tokens
    in the input sequence and special tokens like [CLS] token ID 101 and [SEP] token
    ID 102), and where zero indicates tokens that should be masked (e.g., the [PAD]
    token ID 0).
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The key input_type_ids is a matrix/vector of 1s and 0s having the same size
    as input_ids. This indicates which sentence each token belongs to. Remember that
    BERT can take two types of inputs: inputs with one sequence and inputs with two
    sequences, A and B. The input_type_ids matrix denotes which sequence (A or B)
    each token belongs to. Since we only have one sequence in our inputs, we simply
    create a matrix of zeros having the same size as input_ids.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The function get_bert_inputs() will generate the input in this format using
    a set of documents (i.e., a list of strings, where each string is an input; see
    the next listing).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.3 Formatting a given input to the format BERT accepts
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: ❶ Use BertPackInputs to generate token IDs, mask and segment IDs.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Generate outputs for all the messages in docs.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Convert the output of BertPackInputs to a dictionary with keys as string and
    value as a numpy array.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Return the result.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we are using the BertPackInputs object that takes in an array where each
    item is a string containing the message. Then BertPackInputs generates a dictionary
    that contains the following processed outputs:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: input_word_ids—Token IDs with [CLS] and [SEP] token IDs automatically added.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: input_mask—An integer array with each element representing whether it is a real
    token (1) or a padded token (0) at that position.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: input_type_ids—An integer array with each element representing which segment
    each token belongs to. It will be an array of all zeros in this case.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BertPackInputs does a lot of different preprocessing required by the BERT model.
    You can read about various inputs accepted by this layer at [https://www.tensorflow.org/api_docs/python/tfm/nlp/layers/BertPackInputs](https://www.tensorflow.org/api_docs/python/tfm/nlp/layers/BertPackInputs).
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: BertPackInputs 执行了 BERT 模型需要的许多不同的预处理操作。您可以在 [https://www.tensorflow.org/api_docs/python/tfm/nlp/layers/BertPackInputs](https://www.tensorflow.org/api_docs/python/tfm/nlp/layers/BertPackInputs)
    上阅读关于此层接受的各种输入的信息。
- en: 'To generate the prepared training, validation, and testing data for the model,
    simply call the get_bert_inputs() function:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 要为模型生成准备好的训练、验证和测试数据，只需调用 get_bert_inputs() 函数：
- en: '[PRE32]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'After this, let’s shuffle the data in train_inputs as a precaution. Currently,
    the data is ordered such that spam messages appear after ham messages:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，让我们作为预防措施对 train_inputs 中的数据进行洗牌。目前，数据是有序的，即垃圾邮件消息在正常邮件消息之后：
- en: '[PRE33]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Remember to use the same shuffling for both inputs and labels to maintain their
    association. We have done everything to prepare the inputs for the model. Now
    it’s time for the grand unveiling of the model. We need to define BERT with a
    classification head so that the model can be trained end to end on the classification
    data set we have. We will do this in two steps. First, we will download the encoder
    part of BERT from TensorFlow hub and then use the tfm.nlp.models.BertClassifier
    object in the tensorflow-models-official library to generate the final BERT model
    with the classifier head. Let’s examine how we do the first part:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 记得对输入和标签都使用相同的洗牌方式来保持它们的关联。我们已经做好了为模型准备输入的一切。现在是揭晓模型的大时刻了。我们需要定义一个具有分类头的 BERT，以便模型可以在我们的分类数据集上进行端到端的训练。我们将分两步来完成这个过程。首先，我们将从
    TensorFlow hub 下载 BERT 的编码器部分，然后使用 tensorflow-models-official 库中的 tfm.nlp.models.BertClassifier
    对象来生成最终的带有分类器头的 BERT 模型。让我们来看看我们如何完成第一部分：
- en: '[PRE34]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Here, we first define three input layers, where each input layer has one of
    the outputs of the BertPackInputs mapped to. For example, the input_word_ids input
    layer will receive the output found at the key input_word_ids in the dictionary
    produced by the function get_bert_inputs(). Next, we download the pretrained BERT
    encoder by passing a URL to the hub.KerasLayer object. This layer produces two
    outputs: sequence_output, which contains hidden representations of all the time
    steps, and pooled_output, which contains the hidden representation corresponding
    to the position of the [CLS] token. For this problem, we need the latter to pass
    it to a classification head sitting on top of the encoder. We will finally define
    a Keras model using the Functional API. This model needs to have a specific input
    and output signature defined via dictionaries as shown previously. We will use
    this model to define a classifier model based on this encoder:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们首先定义了三个输入层，每个输入层都映射到 BertPackInputs 的一个输出。例如，input_word_ids 输入层将接收到在 get_bert_inputs()
    函数生成的字典中键为 input_word_ids 的输出。接下来，我们通过向 hub.KerasLayer 对象传递一个 URL 来下载预训练的 BERT
    编码器。这个层会生成两个输出：sequence_output，其中包含所有时间步长的隐藏表示，和 pooled_output，其中包含与 [CLS] 标记位置对应的隐藏表示。对于这个问题，我们需要后者，将其传递给位于编码器顶部的分类头。最后，我们将使用
    Functional API 定义一个 Keras 模型。这个模型需要通过字典定义特定的输入和输出签名，如之前所示。我们将使用这个模型来定义一个基于这个编码器的分类器模型：
- en: '[PRE35]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: As you can see, it’s quite straightforward to define the classifier. We simply
    pass our hub_encoder to the BertClassifier and say we have two classes, spam and
    ham (i.e. num_classes=2).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，定义分类器非常简单。我们只需将我们的 hub_encoder 传递给 BertClassifier，并声明我们有两个类别，即垃圾邮件和正常邮件（即
    num_classes=2）。
- en: Alternative way to get the BERT encoder
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 获取 BERT 编码器的另一种方式
- en: 'There’s another method you can use to get a BERT encoder. However, it requires
    manually loading pretrained weights; therefore, we will keep this method as an
    alternative. Firstly, you start with a configuration file containing the encoder
    model’s various hyperparameters. I have provided you with the original configuration
    used for the model as a YAML file (Ch12/data/bert_en_uncased_base.yaml). It contains
    various hyperparameters used by BERT (e.g., hidden dimension size, nonlinear activation,
    etc.). Feel free to look at them to understand the different parameters used for
    the model. We will load these configurations using the yaml library as a dictionary
    and store it in config_dict. Next, we generate encoder_config, an EncoderConfig
    object initiated with the configuration we loaded. With encoder_config defined,
    we will build an encoder BERT model that is able to generate feature representations
    of tokens, and then call bert.bert_models.classifier_model() with this encoder
    as the network. Note that this method gets a randomly initialized BERT model:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: If you want a pretrained version of BERT like this, then you need to download
    the TensorFlow checkpoint. You can do this by going to the link found in bert_url
    and then clicking download. Finally, you load the weights with
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Now you have a pretrained BERT encoder.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s discuss how we can compile the built model.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: Compiling the model
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Here we will define the optimizer to train the model. So far, we have not changed
    much from the default optimizer options provided in TensorFlow/Keras. This time,
    let’s use the optimizer provided in the tf-models-official library. The optimizer
    can be instantiated by calling the nlp.optimization.create_optimizer() function.
    This is outlined in the next listing.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.4 Optimizing BERT on the spam classification task
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: As the default optimizer, *Adam with weight decay* ([https://arxiv.org/pdf/1711.05101.pdf](https://arxiv.org/pdf/1711.05101.pdf))
    is used. Adam with weight decay is a variant of the original Adam optimizer we
    used, but with better generalization properties. The num_warmup_steps denotes
    the duration of the learning rate warm-up. During the warm-up, the learning rate
    is brought up linearly from a small value to init_lr (defined in linear_decay)
    within num_ warmup_steps. After that, the learning rate is decayed all the way
    to end_lr (defined in linear_decay) during num_train_steps using a polynomial
    decay ([http://mng.bz/06lN](http://mng.bz/06lN)). This is depicted in figure 13.9.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '![13-09](../../OEBPS/Images/13-09.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
- en: Figure 13.9 The behavior of the learning rate as training progresses through
    iterations (i.e., steps)
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can compile the model just like we did before. We will define a loss
    (sparse categorical cross-entropy loss) and a metric (accuracy computed using
    labels rather than one-hot vectors) and then pass the optimizer, loss, and metric
    to the hub_ classifier.compile() function:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Training the model
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve come a long way, and now all that’s left is training the model. Model
    training is quite simple and similar to how we trained models using the tf.keras.Model.fit()
    function:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We are passing the train_inputs we prepared using the get_bert_inputs() function
    to the argument x and train_y (i.e., a vector of 1s and 0s indicating whether
    the input is a spam or a ham, respectively) to y. Similarly, we define validation_data
    as a tuple containing valid_inputs and valid_y. We also pass in the batch_size
    (training batch size), validation_batch_size, and the number of epochs to train
    for.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating and interpreting the results
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: 'When you run the training, you should get a result close to the following.
    Here you can see the training loss and the accuracy as well as the validation
    loss and the accuracy:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The console output clearly shows that the training loss went down steadily,
    whereas the accuracy rose from 45% to 76%. The model has reached a validation
    accuracy of 85%. This clearly shows the power of a model like BERT. If you were
    to train an NLP model from scratch, it would be impossible to reach a validation
    accuracy of 85% in such a short time. Since BERT has very strong language understanding,
    the model can focus on learning the task at hand. Note that you may get a different
    level of accuracy from what’s shown here as our validation and testing sets are
    quite small (each having only 200 records).
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: NOTE On an Intel Core i5 machine with an NVIDIA GeForce RTX 2070 8 GB, the training
    took approximately 40 seconds to run 3 epochs.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: Finally, let’s test the model on the test data by calling the evaluate() function
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: which will return
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Again, it’s a great result. After just three epochs and no fancy parameter tweaking,
    we have achieved 79.5% accuracy on the test data. And all we did was fit a logistic
    regression layer on top of BERT.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: 'The next section will discuss how we can define a model that can find answers
    to a given question from a paragraph. To do this, we will use one of the most
    popular libraries for Transformer models to date: Hugging Face’s transformers
    library.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 2
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: You have a classification problem that has five classes, and you would like
    to modify the bert_classifier. Given the data in the correct format, how would
    you change the bert_classifier object defined?
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: 13.3 Question answering with Hugging Face’s Transformers
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Your friend is planning to kick off a start-up that uses ML to find answers
    to open-domain questions. Lacking the ML background, he turns to you to ask whether
    it is feasible to do so using ML. Knowing that question answering is machine learnable
    given labeled data, you decide to create a question-answering prototype using
    a BERT variant and demonstrate it. You will be using the SQUAD v1 question-answering
    data set and training a DistilBERT (a variant of BERT) on the data set. For this,
    you are going to use the Hugging Face’s transformers library ([https://huggingface.co/transformers/](https://huggingface.co/transformers/)).
    Hugging Face’s transformers library provides implementations of different Transformer
    models and easy-to-use APIs to train/evaluate models on data sets.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: 'BERT is designed to solve two different types of tasks:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: Tasks having a single sequence of text as the input
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tasks having two sequences of text (A and B) as the input
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Spam classification falls under the first type. Question answering is a type
    of task that has two input sequences. In question answering, you have a question
    and a context (a paragraph, a sentence, etc.), which may contain the answer to
    the question. Then a model is trained to predict the answer for a given question
    and a context. Let’s get to know the process a bit better. Each record in the
    data set will consist of following elements:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: A question (a sequence of text)
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A context (a sequence of text)
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A starting index of the answer from the context (an integer)
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An ending index of the answer from the context (an integer)
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First, we need to combine the question and the context and add several special
    tokens. At the beginning, we need to add a [CLS] token and then a [SEP] to separate
    the question and the context, as well as an [SEP] to mark the end of the input.
    Furthermore, the question and the context are broken down into tokens (i.e., sub-words)
    using the model’s tokenizer. For the input having
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: 'Question: What did the dog barked at'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer: The dog barked at the mail man'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: 'if we assume individual words as tokens, the input will look as follows:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Next, these tokens are converted to IDs and fed to the BERT model. The output
    of the BERT model is connected to two downstream classification layers: one layer
    predicts the starting index of the answer, whereas the other layer predicts the
    ending index of the answer. Each of these two classification layers has its own
    weights and biases.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: The BERT model will output a hidden representation for each token in the input
    sequence. The token output representations across the entire span of the context
    are fed to the downstream models. Each classification layer then predicts the
    probability of each token being the starting/ending token of the answer. The weights
    of these layers are shared across the time dimension. In other words, the same
    weight matrix is applied to every output representation to predict the probabilities.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: For this section, we are going to use Hugging Face’s transformers library. Please
    refer to the sidebar for more details.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: Hugging Face’s transformers library
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: 'Hugging Face is a company that focuses on solving NLP problems. The company
    provides libraries to train NLP models as well as host data sets and train models
    in a publicly accessible repository. We will use two Python libraries provided
    by Hugging Face: transformers and datasets.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: 'At the time of writing this book, the transformers library ([https://huggingface.co/transformers/](https://huggingface.co/transformers/))
    is the most versatile Python library that provides instant access to many Transformer
    models that have been released (e.g., BERT, XLNet, DistilBERT, Albert, RoBERT,
    etc.) as well as to community-released NLP models ([https://huggingface.co/models](https://huggingface.co/models)).
    The transformers library supports both TensorFlow and PyTorch deep learning frameworks.
    PyTorch ([https://pytorch.org/](https://pytorch.org/)) is another deep learning
    framework like TensorFlow that offers a comprehensive suite of functionality to
    implement and productionize deep learning models. The following are the key advantages
    I see in the transformers library:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: An easy-to-understand API to pretrain and fine-tune models that is consistent
    across all the models
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to download TensorFlow versions of various Transformer models and
    use them as Keras models
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to convert TensorFlow and PyTorch models
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Powerful features such as the Trainer ([http://mng.bz/Kx9j](http://mng.bz/Kx9j)),
    which allows users to create and train models with lot of flexibility
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Highly active community-contributing models and data sets
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 13.3.1 Understanding the data
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As mentioned, we will use the SQuAD v1 data set ([https://rajpurkar.github.io/SQuAD-explorer/](https://rajpurkar.github.io/SQuAD-explorer/)).
    It’s a question-answering data set created by Stanford University. You can easily
    download the data set using Hugging Face’s datasets library as follows:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Let’s print the data set and see the available attributes:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: This will return
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: There are 87,599 training examples and 10,570 validation samples. We will use
    these examples to create the train/validation/test split. We are interested in
    the last three columns in the features section only (i.e., context, question,
    and answers). From these, context and question are simply strings, whereas answers
    is a dictionary. Let’s analyze answers a bit further. You can print a few answers
    with
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: which gives
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: We can see that each answer has a starting index (character based) and a text
    containing the answer. With this information, we can easily calculate the answer’s
    ending index (i.e., end index = start index + len(text)).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: GLUE benchmark task suite
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: The GLUE benchmark ([https://gluebenchmark.com/tasks](https://gluebenchmark.com/tasks))
    is a popular collection of tasks for evaluating NLP models. It tests the natural
    language understanding of models on a variety of tasks. The following tasks are
    included in the GLUE tasks collection.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: '![13_table1](../../OEBPS/Images/13_table1.png)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
- en: 13.3.2 Processing data
  id: totrans-336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This data set has several integrity issues that need to be taken care of. We
    will fix those issues and then create a tf.data pipeline to pump the data. The
    first issues that need to be fixed are alignment issues between the given answer_start
    and the actual answer_start. Some examples tend to have an offset of around two
    characters between the given and the actual answer_start positions. We will write
    a function to correct for this offset, as well as add the end index. The next
    listing outlines the code to perform this operation.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.5 Fixing the unwanted offsets in the answer start/end indices
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: ❶ Track how many were correct and fixed.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: ❷ New fixed answers will be held in this variable.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Iterate through each answer context pair.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Convert the answer from a list of strings to a string.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Convert the start of the answer from a list of integers to an integer.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Compute the end index by adding the answer’s length to the start_idx.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: ❼ If the slice from start_idx to end_idx exactly matches the answer text, no
    changes are required.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: ❽ If the slice from start_idx to end_idx needs to be offset by 1 to match the
    answer, offset accordingly.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: ❾ If the slice from start_idx to end_idx needs to be offset by 2 to match the
    answer, offset accordingly.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: ❿ Print the number of correct answers (requires no change).
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: ⓫ Print the number of answers that required fixing.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can call this function on the two subsets (train and validation) of
    the data set. We will use the validation subset as our testing set (unseen examples).
    A proportion of the training examples will be set aside as validation samples:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'When you run this code, you will see the following statistics:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: Training data corrections
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 87,341/87,599 examples had the correct answer indices.
  id: totrans-355
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 258/87,599 examples had the wrong answer indices.
  id: totrans-356
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Validation data correction
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 10,565/10,570 examples had the correct answer indices.
  id: totrans-358
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 5/10,570 examples had the wrong answer indices.
  id: totrans-359
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We have to make sure that the required number of corrections is not drastically
    high. If the number of corrections is significantly high, typically it could mean
    a bug in the code or problems with the data-loading logic. Here, we can clearly
    see that the number of examples that required corrections is low.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: Defining and using the tokenizer
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: 'All the data we need to solve the problem is available to us. Now it’s time
    to tokenize the data like we did before. Remember that these pretrained NLP models
    come in two parts: the tokenizer and the model itself. The tokenizer tokenizes
    the text into smaller parts (e.g., sub-words) and presents those to the model
    in the form of a sequence of IDs. Then the model takes in those IDs and performs
    embedding lookups on them, as well as various computations, to come up with the
    final token representation that will be used as inputs to the downstream classification
    model (i.e., the question-answering classification layer). In a similar fashion,
    in the transformers library, you have the tokenizer object and the model object:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: You can see that we are using a tokenizer called DistilBertTokenizerFast. This
    tokenizer comes from a model known as the DistilBERT. DistilBERT is a smaller
    version of BERT that shows similar performance as BERT but smaller in size. It
    has been trained using a transfer learning technique known as *knowledge distillation*
    ([https://devopedia.org/knowledge-distillation](https://devopedia.org/knowledge-distillation)).
    To get the tokenizer, all we need to do is call the DistilBertTokenizerFast.from_pretrained()
    function with the model tag (i.e., distilbert-base-uncased). This tag says the
    model is a DistilBERT-type model with base size (there are different-sized models
    available) and ignores the case of the characters (denoted by uncased). The model
    tag points to a model available in Hugging Face’s model repository ([https://huggingface.co/models](https://huggingface.co/models))
    and downloads it for us. It will be stored on your computer.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: 'Hugging Face provides two different variants of tokenizers: standard tokenizers
    (PreTrainedTokenizer objects; [http://mng.bz/95d7](http://mng.bz/95d7)) and fast
    tokenizers (PreTrainedTokenizerFast objects; [http://mng.bz/j2Xr](http://mng.bz/j2Xr)).
    You can read about their differences on [http://mng.bz/WxEa](http://mng.bz/WxEa)).
    They are significantly faster when encoding (i.e., converting strings to token
    sequences) in a batch-wise fashion. Furthermore, fast tokenizers have additional
    methods that will help us to process inputs easily for the question-answering
    model.'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: What is DistilBERT?
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: 'Following BERT, DistilBERT is a model introduced by Hugging Face in the paper
    “DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter”
    by Sanh et al. ([https://arxiv.org/pdf/1910.01108v4.pdf](https://arxiv.org/pdf/1910.01108v4.pdf))
    in 2019\. It is trained using a transfer learning technique known as knowledge
    distillation. The idea is to have a teacher model (i.e., BERT), where a smaller
    model (i.e., DistilBERT) is trying to mimic the teacher’s output, which becomes
    the learning objective for DistilBERT. DistilBERT is smaller compared to BERT
    and has only six layers, as opposed to BERT’s 12\. Another key difference of DistilBERT
    is that it is only trained on the masked language modeling task and not on the
    next-sentence prediction task. This decision is backed up by several studies that
    question the contribution made by next-sentence prediction tasks (compared to
    the masked language modeling task) toward natural language understanding.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: 'With the tokenizer downloaded, let’s examine the tokenizer and how it transforms
    the inputs by providing some sample text:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: This returns
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Next, print the tokens corresponding to the IDs with
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: This will give
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: We can now encode all the training and test data we have with the code in the
    next listing.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.6 Encoding training and test data
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: ❶ Encode train data.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Encode test data.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we are using several arguments when calling the tokenizer. When truncation
    and padding are enabled (i.e., set to True), the tokenizer will pad/truncate input
    sequences as necessary. You can pass an argument to the tokenizer (model_max_length)
    during the creation to pad or truncate text to a certain length. If this argument
    is not given, it will use the default length available to the tokenizer as one
    of the configurations set when it was pretrained. When padding/truncation is enabled,
    your inputs will go through one of the following changes:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: If the sequence is shorter than the length, add the special token [PAD] to the
    end of the sequence until it reaches the length.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the sequence is longer than the length, it will be truncated.
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the sequence has exactly the same length, no change will be introduced.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you run the code in listing 13.6, it prints
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'We can see that all sequences are either padded or truncated until they reach
    a length of 512, the sequence length set during the model pretraining. Let’s look
    at some of the important arguments you need to be aware of when defining a tokenizer
    with the transformers library:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: model_max_length (int, *optional*)—The maximum length (in number of tokens)
    for the inputs to be padded to.
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: padding_side (str, *optional*)—To which side the model should apply padding.
    Can be ['right', 'left']. The default value is picked from the class attribute
    of the same name.
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: model_input_names (List[string], *optional*)—The list of inputs accepted by
    the forward pass of the model (e.g., "token_type_ids" or "attention_mask"). The
    default value is picked from the class attribute of the same name.
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: bos_token (str or tokenizers.AddedToken, *optional*)—A special token representing
    the beginning of a sentence.
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: eos_token (str or tokenizers.AddedToken, *optional*)—A special token representing
    the end of a sentence.
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: unk_token (str or tokenizers.AddedToken, *optional*)—A special token representing
    an out-of-vocabulary token. Out-of-vocabulary tokens are important if the model
    encounters words it has not seen before.
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most of these arguments can be safely ignored as we are using a pretrained tokenizer
    model, which already had these attributes set before the training.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately for us, we still have to do a few more things. One important
    transformation we have to do is to how we represent the start and end of the answer
    to the model. As I said earlier, we are given the starting and ending character
    positions of the answer. But our model understands only token-level decomposition,
    not character-level decomposition. Therefore, we have to find the token positions
    from the given character positions. Fortunately, the fast tokenizers provide a
    convenient function: char_to_token(). Note that this function is only available
    for fast tokenizers (i.e., PreTrainedTokenizerFast objects), not the standard
    tokenizers (i.e., PreTrainedTokenizer objects). The char_to_token() function takes
    in the following arguments:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: batch_or_char_index (int)—The example index in the batch. If the batch has one
    example, it will be used as the character index we’re interested in converting
    to a token index.
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: char_index (int, optional—If the batch index is provided, this will denote the
    character index we want to convert to a token index.
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: sequence_index (int, optional—If the input has multiple sequences, this denotes
    which sequence the character/token belongs to.
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using this function, we will write the update_char_to_token_positions_inplace()
    function to convert the char-based indices to token-based indices (see the next
    listing)..
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.7 Converting the char indices to token-based indices
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: ❶ Go through all the answers.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Get the token position for both start and end char positions.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Keep track of how many samples were missing answers.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: ❹ If a starting position was not found, set it to the last available index.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: ❺ If an ending position was not found, set it to the last available index.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Update the encodings in place.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: This will print
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: In the code in listing 13.7, we go through every answer in the data set and
    call the char_to_token() method on every answer_start and answer_end element that
    marks the start and end (char index) of answers, respectively. The new starting
    and ending token indices of the answers are assigned to new keys, start_positions
    and end_positions. Furthermore, you can see that there is a validation step that
    checks if the starting or ending indices are None (i.e., a suitable token ID was
    not found). This can happen if the answer has been truncated from the context
    when preprocessing. If that’s the case, we assign the very last index of the sequence
    as the positions.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: How many rotten eggs?
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: You can see that we are printing the number of examples that needed modifications
    (e.g., needed correction) or were corrupted (truncated answers). This is an important
    sanity check, as a very high number can indicate problems with data quality or
    a bug in your data processing workflow. Therefore, always print these numbers
    and make sure they are low enough to be safely ignored.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: We will now see how we can define a tf.data pipeline.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: From tokens to the tf.data pipeline
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: 'After all the cleaning and required transformations, our data set is as good
    as new. All that’s left for us is to create a tf.data.Dataset from the data. Our
    data pipeline will be very simple. It will create a training data set, which will
    be broken into two subsets, training and validation, and batch the data set. Then
    the test data set will be created and batched. First let’s import TensorFlow:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Then we will define a generator that will produce the inputs and outputs necessary
    for our model to be trained. As you can see, our input is a tuple of two items.
    It has
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: The padded input token IDs (having the shape [<dataset size>, 512])
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The attention mask (having the shape [<dataset size>, 512])
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The outputs will be comprised of
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: The starting token positions (having the shape [<dataset size>])
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ending token positions (having the shape [<dataset size>]
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Our data generator returns data in a very specific format. It returns an input
    tuple and an output tuple. The input tuple has the token IDs (input_ids) returned
    by the tokenizer and the attention mask (attention_mask), in that order. The output
    tuple has the answer’s start positions (start_positions) and end positions (end_positions)
    of all the inputs.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: 'We must define our data generator as a callable (i.e., it returns a function,
    not the generator object). This is a requirement for the tf.data.Dataset we will
    be defining. To get a callable function from the generator we defined earlier,
    let’s use the partial() function,. partial() function takes in a callable, optional
    keyword argument of the callable and returns a partially populated callable, to
    which you only need to provide the missing arguments (i.e., arguments not specified
    during the partial call):'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'train_data_gen can be treated as a function that has no arguments, as we have
    already provided all arguments in the partial call. As we have defined our data
    in the form of a generator, we can use the tf.data.Dataset.from_generator() function
    to generate the data. Remember that we have to pass the output_types argument
    when defining the data through a generator. All our outputs are int32 type. But
    they come as a pair of tuples:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Next, we shuffle the data to make sure there’s no order. Make sure you pass
    the buffer_size argument that specifies how many samples are brought into memory
    for the shuffle. We will set the buffer_size to 20,000 as we plan to use 10,000
    samples from that as validation samples:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'It’s time to split the train_dataset into training and validation data, as
    we will be using the original validation data subset as the testing data. To split
    the train_dataset into training and validation subsets, we will take the following
    approach. After the shuffle, define the first 10,000 samples as the valid data
    set. We will be batching the data using the tf.data.Dataset.batch() function with
    a batch size of 8:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Skip the first 10,000 datapoints, as they belong to the validation set, and
    have the rest as the train_dataset. We will be batching the data using the tf.data.Dataset.batch()
    function with a batch size of 8:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Finally, the test data is defined using the same data generator:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Next we will work through defining the model.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: 13.3.3 Defining the DistilBERT model
  id: totrans-438
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have looked closely at the data, cleaned and processed it using a tokenizer,
    and defined a tf.data.Dataset to quickly retrieve batches of examples in the format
    our model will accept. It’s time to define the model. To define the model, we
    will import the following module:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'The transformers library provides you with a remarkable selection of ready-made
    model templates that you can download and train on the task. In other words, you
    don’t have to fiddle around with the library to find out how to plug downstream
    models on top of the pretrained transformers. For example, we are solving a question-answering
    problem and we want to use the DistilBERT model for that. The transformers library
    has the built-in question-answering adaptation of the DistilBERT model. All you
    need to do is import the module and call the from_pretrained() function with the
    model tag to download it:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: This will download the model and save it on your local computer.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: What other ready-to-use models does the Transformers library provide?
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: You can a look at [http://mng.bz/8Mwz](http://mng.bz/8Mwz) to get an idea about
    what you can and cannot easily do with the DistilBERT model. transformers is a
    fully fledged library that you can use to solve almost all the common NLP tasks
    with Transformer models. Here we will look at the options for the DistilBERT model.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: '**TFDistilBertForMaskedLM**'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: This allows you to pretrain the DistilBERT model using the masked language modeling
    task. In the masked language modeling task, in a given corpus of text, words are
    masked at random and the model is asked to predict the masked words.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: '**TFDistilBertForSequenceClassification**'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: If your problem has a single input sequence and you want to predict a label
    for the input (e.g., spam classification, sentiment analysis, etc.), you can use
    this model to train the model end to end.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: '**TFDistilBertForMultipleChoice**'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: DistilBERT can be used to solve multiple-choice problems with this variant.
    The input consists of a question and several answers. These are typically combined
    into a single sequence (i.e., [CLS] [Question] [SEP] [Answer 1] [SEP] [Answer
    2] [SEP], etc.), and the model is asked to predict the best answer to the question,
    which is typically done by feeding the representation for the [CLS] token to a
    classification layer that will predict the index of the correct answer (e.g.,
    first answer, second answer, etc.).
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: '**TFDistilBertForTokenClassification**'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: Tasks like named-entity recognition or part-of-speech tagging require the model
    to predict a label (e.g., person, organization, etc.) for every token in the input
    sequence. For such tasks, this type of model can be used simply.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: '**TFDistilBertForQuestionAnswering**'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: This is the model we will use in our scenario. We have a question and a context,
    and the model needs to predict the answer (or the starting/ending positions of
    the answer in the context). Such problems can be solved using this module.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: Table 13.1 is a summary of the models in this sidebar and their usage.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: Table 13.1 Different models in Hugging Face’s transformers library and their
    usage
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: '![13_table2](../../OEBPS/Images/13_table2.png)'
  id: totrans-458
  prefs: []
  type: TYPE_IMG
- en: The blog [http://jalammar.github.io/illustrated-bert/](http://jalammar.github.io/illustrated-bert/)
    provides a very informative diagram of how BERT-like models can be used for different
    tasks (as outlined under the “Task-specific models” section).
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: The training and evaluation of these models will be very similar to how you
    would use them if they were Keras models. You can call model.fit(), model.predict()
    or model.evaluate() on these models given that the data is in the correct format
    the model expects.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: To train these models, you also have the ability to use an advanced Trainer
    ([http://mng.bz/EWZd](http://mng.bz/EWZd)). We will discuss the Trainer object
    in more detail later.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: The next part of this came from a wakeup call that I had while using the library.
    Typically, once the model is defined, you can use it just like a Keras model.
    This means you can call model.fit() with a tf.data.Dataset and train the model.
    TensorFlow and Keras, when training models, expect the model output to be a tensor
    or a tuple of tensors. However, Transformer models’ outputs are specific objects
    (a descendant of the transformers.file_utils.ModelOutput object), as outlined
    in [http://mng.bz/N6Rn](http://mng.bz/N6Rn). This will throw an error like
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: To fix this, the transformers library allows you to set a certain configuration
    called return_dict and make sure the model returns a tuple, not an object. Then
    we define a Config object that has return_dict=False and override the default
    config of the model with the new config object. For example, for the DistilBERT
    model, this can be done with
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Unfortunately, I was not able to get the model to behave in the way I expected
    it to by using this configuration. This goes to show that when writing code, you
    need to anticipate things that can go wrong even when using top-notch libraries.
    The easiest fix would be to let the transformers library output an ModelOutput
    object and write a wrapper function that will extract the required outputs of
    that object and create a tf.keras.Model from those outputs. The function in the
    following listing does that for us.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.8 Wrapping the model in a tf.keras.models.Model object to prevent
    errors
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-468
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: ❶ Define an input layer that will take a batch of a token sequence.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Define an input for the attention mask returned when encoding with the tokenizer.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Get the model output given the input_ids and the attention_mask.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Define a wrapper model that takes the defined inputs as the inputs and output
    logits of the start and end indices’ prediction layers.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: 'You can generate the model that generates the corrected outputs by calling
    the function in listing 13.8:'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-474
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Finally, compile the model with a loss function (sparse categorical cross-entropy,
    as we are using integer labels), metric(s) (sparse accuracy), and an optimizer
    (Adam optimizer):'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-476
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Transformers in vision
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: Transformer models typically thrive in the NLP domain. Until recently, little
    effort was put into understanding their place in the computer vision domain. Then
    came several notable papers from Google and Facebook AI that investigated how
    Transformer models could be used in the computer vision domain.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
- en: '**Vision Transformer (ViT)**'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: 'First, the paper “An Image Is Worth 16X16 Words: Transformers for Image Recognition
    at Scale” by Dosovitskiy et al. was published in October 2020 ([https://arxiv.org/pdf/2010.11929.pdf](https://arxiv.org/pdf/2010.11929.pdf)).
    This can be considered the first substantial step toward vision transformers.
    In this paper, the authors adapt the original Transformer model proposed for the
    NLP domain to a computer vision with minimal changes to the architecture. This
    model is called the Vision Transformer (ViT).'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: The idea is to decompose an image to small patches of 16 × 16 and consider each
    as a separate token. Each image path is flattened to a 1D vector, and its position
    is encoded by a positional-encoding mechanism similar to the original Transformer.
    It is important to note that the positional encoding in the original Transformer
    is 1D. However, images are 2D. The authors argue that a 1D positional encoding
    was adequate and there was no major performance difference between 1D and 2D positional
    encoding. Once the image is broken into patches of 16 × 16 and flattened, each
    image can be presented as a sequence of tokens, just like a textual input sequence.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: Then the model is pretrained in a self-supervised fashion, using a vision data
    set called JFT-300M ([https://paperswithcode.com/dataset/jft-300m](https://paperswithcode.com/dataset/jft-300m)).
    It is not trivial to formulate a self-supervised task in vision as it is in NLP.
    In NLP, you can form an objective simply to predict masked tokens in a text sequence.
    However, in the context of computer vision, a token is a sequence of continuous
    values (normalized pixel values). Therefore, ViT is pretrained to predict the
    mean three-bit RGB color of a given image patch. Each channel (i.e., red, green,
    and blue) is represented with three bits (i.e., each bit having the value of 0
    or 1), which gives 512 possibilities or classes. In other words, for a given image,
    patches are masked randomly (using the same approach as BERT), and the model is
    asked to predict the mean three-bit RGB color of that image patch.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: After pretraining, the model can be fine-tuned for a task-specific problem by
    fitting a classification or a regression head on top of ViT, just like BERT. ViT
    also has the [CLS] token at the beginning of the sequence, which will be used
    as the input representation for downstream vision models that are plugged on top
    of ViT. The following figure illustrates the mechanics of ViT.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: The original code for ViT is written with a framework known as Jax ([https://github
    .com/google-research/vision_transformer](https://github.com/google-research/vision_transformer)).
    However, there are several third-party TensorFlow wrappers for the model (e.g.,
    [https://github.com/emla2805/vision-transformer](https://github.com/emla2805/vision-transformer)).
    If using a third-party version, make sure you read the code and verify the correctness
    as the third party is not affiliated with the original research team.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: '![13-09-unnumb-1](../../OEBPS/Images/13-09-unnumb-1.png)'
  id: totrans-485
  prefs: []
  type: TYPE_IMG
- en: The Vision Transformer (ViT) model architecture and how it is used for a downstream
    image classification task
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: Unified Transformer (UniT)
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
- en: 'Then came an even more groundbreaking paper from Facebook AI called “Transformer
    Is All You Need: Multimodal Multitask Learning with a Unified Transformer” by
    Hu et al. ([https://arxiv.org/pdf/2102.10772.pdf](https://arxiv.org/pdf/2102.10772.pdf)).
    The model is called the Unified Transformer (UniT). UniT can perform a plethora
    of tasks in both computer vision and NLP domains, just by changing the classification
    head.'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
- en: The model, even though it is complex, is straightforward. There are three Transformer
    models. One Transformer encodes the image input (if present) to generate an encoded
    representation of an image. The next Transformer encodes the text input (if present).
    Finally, another Transformer takes a task index as the input, gets the embedding,
    and passes it to a cross self-attention layer that takes the concatenated image
    and text encoding as the query and key and the task embedding (after passing through
    a self-attention layer) as the value. This is similar to how the Transformer decoder,
    in its encoder-decoder attention layer, uses the last encoder output to generate
    the query and key and uses the decoder’s input as the value. This model is depicted
    in the figure on the next page.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: 'UniT is evaluated on seven tasks involving eight data sets. These tasks include
    object detection, visual question answering, object annotation, and four language-only
    tasks. The four language-only tasks are from the GLUE benchmarking data sets,
    which include the following:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: '**Tasks**'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
- en: '*Object detection*—The model predicts the rectangular coordinates of objects
    present in images (data sets COCO and Visual Genome Detection [VGD]).'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: '*Visual question answering* (VQAv2)—An image and a question is given, and then
    the model predicts the answer to the question, which can be found in the image
    (data set: VQAv2).'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: '*Visual entailment task*—A visual entailment task where an image and a text
    sequence are given and the model predicts whether the sentence semantically entails
    the image (SNLI-VE).'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
- en: '*Question Natural Language Inference* (QNLI)—Question answering by extracting
    the answer from a given context.'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
- en: '*Quora Question Pairs* (QQP)—Identify duplicate questions from a given pair
    of questions.'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
- en: '*Textual entailment*—Textual entailment focuses on predicting if sentence A
    entails/ contradicts/is neutral to sentence B. (data set MNLI).'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
- en: '*Sentiment analysis*—Predicts a sentiment (positive/negative/is neutral) for
    a given review/text (data set Stanford Sentiment Treebank [SST-2]).'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
- en: '![13-09-unnumb-2](../../OEBPS/Images/13-09-unnumb-2.png)'
  id: totrans-499
  prefs: []
  type: TYPE_IMG
- en: 'The overall architecture of UniT. The model consists of three components: an
    image encoder, a text encoder, and a task decoder. Finally, there are several
    classification heads fitted on top of the task decoder.'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
- en: Next we will train the model we just defined.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
- en: 13.3.4 Training the model
  id: totrans-502
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have been patiently chipping away at this problem to arrive at this moment.
    Finally, we can train the model we just defined with the train_dataset data set
    we created earlier and use the valid_dataset to monitor the accuracy of the model:'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-504
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'This will print the following output:'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-506
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'The training updates are quite long, so let’s break them down a bit. There
    are two losses:'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
- en: tf_distil_bert_for_question_answering_loss—Measures the loss on the starting
    index prediction head
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: tf_distil_bert_for_question_answering_1_loss—Measures the loss on the ending
    index prediction head
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As mentioned earlier, for question answering, we have two classification heads:
    one to predict the starting index and one to predict the ending index. We have
    something similar for the accuracy. There are two accuracies to measure the performance
    of individual heads:'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
- en: tf_distil_bert_for_question_answering_sparse_categorical_accuracy—Measures the
    accuracy of the classification head predicting the starting index
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: tf_distil_bert_for_question_answering_1_sparse_categorical_accuracy—Measures
    the accuracy of the classification head predicting the ending index
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can see that the model has achieved a training accuracy of around 77% for
    both starting and ending index prediction, where the validation accuracy is around
    70% and 72% for starting/ending index prediction, respectively. These are good
    accuracies given that we only trained the model for three epochs.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
- en: NOTE On an Intel Core i5 machine with an NVIDIA GeForce RTX 2070 8 GB, the training
    took approximately 2 hours and 45 minutes to run three epochs.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
- en: You can see that there are several warnings produced during the model training.
    Since this is a new library, it is extremely important to pay attention to these
    warnings to make sure they make sense in the context we’re using these models.
    You don’t need to worry about these as you would if you saw an error, as warnings
    do not always indicate a problem. Depending on the problem we’re solving, some
    warnings are not applicable and can be safely ignored. The first warning says
    that the parameters output_attentions, output_hidden_states and use_cache cannot
    be updated when calling the model but need to be passed in as config objects.
    We are not worried about this as we are not interested in introducing any custom
    modifications to the model, and we are using one that is already designed for
    question answering.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
- en: 'The second warning says that return_dict will always be set to TRUE. Setting
    return_dict=True means that the Transformer model will return a ModelOutput object
    that is not understood by TensorFlow or Keras. This creates problems down the
    road when we want to use the model with a Keras API. This is one of the reasons
    we create the tf_wrap_model() function: to make sure we get a tf.keras.Model that
    always outputs a tuple and not a ModelOutput object.'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we will save the model:'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  id: totrans-518
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: Make sure to save both the tokenizer and the model. To save the tokenizer, you
    can simply call the save_pretrained() function and provide it a folder path. The
    tokenizer will be saved in that directory. Saving the model requires a bit more
    work. We will not be able to save the model (model_v2) as is, because when your
    model has a custom layer, to be saved correctly, the layer needs to implement
    the get_config() function and specify all the attributes of that layer. However,
    doing this for the Transformer model, which is there as a custom layer, would
    be extremely difficult. Therefore, we will only save the Transformer model by
    calling the model_v2.get_layer() function with the layer name (i.e., tf_distil_bert_for_question_answering)
    and then calling the save_pretrained() method with a folder path. Any time we
    have to build the full model, we can simply call the tf_wrap_model() function
    on that saved model.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
- en: 13.3.5 Ask BERT a question
  id: totrans-520
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Evaluating the model is also a matter of calling the model_v2.evaluate() function
    with the test_dataset we created earlier:'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-522
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: This will print
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  id: totrans-524
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: This is wonderful news! We have achieved around 65.7% accuracy on predicting
    the start index of the answer, where the model can predict the end index with
    approximately 69.4% accuracy. Two things to observe are that both starting and
    ending accuracies are similar, meaning that the model gets the answer correct
    (start to end) around that accuracy. Finally, this accuracy falls close to the
    validation accuracy, meaning that we don’t have unusual overfitting occurring.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
- en: As I have alluded to numerous times, it is usually not enough to look at a number
    alone to judge a model on its performance. Visual inspection has always been a
    natural tendency for humans when evaluating an object. Therefore, as a scrupulous
    data scientist or a machine learning engineer, it should be a part of the machine
    learning workflow whenever possible. In the next listing, we will provide our
    model a question from the test set and see what the model produces.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.9 Inferring the textual answer from the model for a given question
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  id: totrans-528
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: ❶ Define a sample question, context, and answer.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Predict with the model for the sample input. This returns the starting and
    ending index probability vectors.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Get the predicted starting index by getting the maximum index from the starting
    index probability vector.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Get the predicted ending index by getting the maximum index from the ending
    index probability vector.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Get the string answer tokens by getting the text between the starting/ending
    indices.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Return the list of tokens as a single string.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Print the inputs to the model.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Call the ask_bert function on the defined input.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
- en: ❾ Print the answer.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s flesh out the details of the code in listing 13.9\. First, we define
    an index i. This index will be used to retrieve a sample from the test set. sample_q,
    sample_c, and sample_a represent the question, context, and answer of the sample
    we have selected. With these, we can define the sample_input, which will contain
    the encoded representation of the input understood by the model. The function
    ask_bert() takes in an input prepared for the model with the tokenizer (denoted
    by the sample_input) to convert token IDs of the answer back to readable tokens.
    The function first predicts the output for the input and gets the starting and
    ending token IDs of the answer. Finally, the function converts these IDs along
    with the words in between into a single comprehensible answer and returns the
    text. If you print the output of this process, you will get the following:'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  id: totrans-539
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: This ends our conversation about using Hugging Face’s Transformer library to
    implement Transformer models. We have gone through all the steps you are likely
    to come across while solving any NLP task with a Transformer model. And Hugging
    Face’s transformers library still enjoys its reputation as one of the best libraries
    for implementing Transformer models in TensorFlow or PyTorch.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing attention heads
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
- en: Whenever there’s a chance for us to interpret deep learning models and understand
    why a model made a certain decision, it is important to make the most of it. Having
    the ability to dissect and interpret models helps to build trust among users.
    Interpreting Transformer models is made very easy due to the presence of the self-attention
    layers. Using the self-attention layer, we can find out which words the model
    paid attention to while coming up with a hidden representation of a token.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
- en: We can use the bert_viz library ([https://github.com/jessevig/bertviz](https://github.com/jessevig/bertviz))
    to visualize the attention patterns in any attention head found in any layer.
    It is important to note that bert_viz does not support TensorFlow but uses the
    PyTorch library. Despite this small technical difference, using bert_viz is easy
    and straightforward.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import the required libraries:'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  id: totrans-545
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Next, define a BERT model. Make sure to pass the output_attentions=True config
    to output attention outputs, as it is turned off by default:'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  id: totrans-547
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Encode the input text and then get the output of the model:'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  id: totrans-549
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: Finally call the head_view() function. You can get the attention outputs by
    simply calling output.attentions, which will return a tuple of 12 tensors. Each
    tensor corresponds to a separate layer in BERT. Also, make sure you convert them
    to torch tensors. Otherwise, this function errors out. The output is visualized
    in figure 13.10.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  id: totrans-551
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: This ends our discussion about Transformer models. However, it’s important to
    keep in mind that Transformer models are evolving and becoming better even as
    we speak. In the next chapter, we will discuss an important visualization tool,
    called TensorBoard, that is shipped with TensorFlow.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
- en: '![13-10](../../OEBPS/Images/13-10.png)'
  id: totrans-553
  prefs: []
  type: TYPE_IMG
- en: Figure 13.10 The attention output of the bert_viz library. You can select different
    layers from the dropdown menu. The different shades represent different attention
    heads in that layer, which can be switched on or off. The lines between the two
    columns represent which words the model paid attention to when generating the
    hidden representation of a given token.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 3
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
- en: You are asked to implement a named-entity recognition model. Named-entity recognition
    is a token classification task, where each token is assigned a label (e.g., person,
    organization, geographical, other, etc.). There are seven different labels. If
    you want to use the distilbert-base-uncased model for this, how would you define
    the model? Remember that in the transformers library, you can pass the num_labels
    as a keyword to define the number of output classes. For example, if you have
    a config "a" that you would like to set to "abc", you can do
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  id: totrans-557
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: Summary
  id: totrans-558
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The main subcomponents of a Transformer model are embeddings (token and positional),
    a self-attention sublayer, a fully connected sublayer, residual connections, and
    layer normalization sublayers.
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BERT is a Transformer encoder model that produces a hidden (attended) representation
    for each token passed in the input.
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BERT uses special tokens like [CLS] (denotes the beginning and is used to generate
    the output for classification heads), [SEP] (to mark the separation between two
    subsequences; e.g., question and the context in question answering), [PAD] (to
    denote padded tokens to bring all sequences to a fixed length), and [MASK] (to
    mask out tokens in the input sequence; e.g., padded tokens).
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BERT can be used for downstream, task-specific classification tasks by fitting
    a classification head (e.g., a logistic regression layer) on top of the final
    output of BERT. Depending on the type of task, multiple classification heads might
    be required, and the utilization of the classification head might differ.
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hugging Face’s transformers library provides implementations of all the NLP-related
    Transformer models and can be easily downloaded and used in the workflow. The
    pretrained models available for download have two main components: the tokenizer
    that will tokenize a provided string to a sequence of tokens and the model that
    takes in the sequence of tokens to generate the final hidden output.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Answers to exercises
  id: totrans-564
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Exercise 1**'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
- en: '*PE*(*pos*,2*i* ) = sin(*pos*/10000^(21/d[model]))'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  id: totrans-567
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '**Exercise 2**'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  id: totrans-569
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '**Exercise 3**'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  id: totrans-571
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
