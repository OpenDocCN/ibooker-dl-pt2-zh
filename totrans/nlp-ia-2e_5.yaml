- en: 5 Word brain (neural networks)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 字脑（神经网络）
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章包括
- en: Building a base layer for your neural networks
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为你的神经网络构建一个基础层
- en: Understanding backpropagation to train neural networks
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解反向传播以训练神经网络
- en: Implementing a basic neural network in Python
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Python 中实现一个基本的神经网络
- en: Implementing a scalable neural network in PyTorch
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 PyTorch 中实现一个可扩展的神经网络
- en: Stacking network layers for better data representation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 堆叠网络层以获得更好的数据表示
- en: Tuning up your neural network for better performance
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整你的神经网络以获得更好的性能
- en: When you read the title of this chapter, "word brain", the neurons in your brain
    started firing, reminding you where you’d heard something like that before. And
    now that you read the word "heard", your neurons might be connecting the words
    in the title to the part of your brain that processes the *sound* of words. And
    maybe, the neurons in your audio cortex are starting to connect the phrase "word
    brain" to common phrases that rhyme with it, such as "bird brain."
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当你读到这一章的标题，“字脑”，你的大脑中的神经元开始激活，提醒你在哪里听过类似的东西。现在你读到“听到”这个词，你的神经元可能正在连接标题中的单词与处理单词*声音*的大脑部分。也许，你听觉皮层中的神经元开始将短语“字脑”连接到与之押韵的常见短语，比如“鸟脑”。
- en: Even if my brain didn’t predict your brain very well, you’re about to build
    a small brain yourself. And the "word brain" you are about to build will be a
    lot better than both of our human brains, at least for some particularly hard
    NLP tasks. You’re going to build a tiny brain that can process a single word and
    predict something about what it means. And a neural net can do this when the word
    it is processing is a person’s name and it doesn’t seem to *mean* anything at
    all to a human.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我的大脑没有很好地预测到你的大脑，你也将要建立一个小脑。你即将构建的“字脑”将比我们人类的大脑好得多，至少对于某些特别困难的自然语言处理任务来说是这样。你将建立一个可以处理单词并预测其意义的微小脑。当神经网络处理的单词是一个人的名字时，它似乎对人类来说*没有意义*。
- en: Don’t worry if all of this talk about brains and predictions and words has you
    confused. You are going to start simple, with just a single artificial neuron,
    built in Python. And you’ll use PyTorch to handle all the complicated math required
    to connect your neuron up to other neurons and create an artificial neural network.
    Once you understand neural networks, you’ll begin to understand *deep learning*,
    and be able to use it in the real world for fun, positive social impact, and …​
    if you insist, profit.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所有关于大脑、预测和单词的谈论让你感到困惑，不要担心。你将从简单开始，只用一个在 Python 中构建的人工神经元。你将使用 PyTorch 处理连接你的神经元到其他神经元并创建人工神经网络所需的所有复杂数学。一旦你理解了神经网络，你就会开始理解*深度学习*，并能够在现实世界中使用它进行有趣的、积极的社会影响，以及...
    如果你坚持的话，利润。
- en: 5.1 Why neural networks?
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 为什么使用神经网络？
- en: 'When you use a deep neural network for machine learning it is called *deep
    learning*. In the past few years, deep learning has smashed through the accuracy
    and intelligence ceiling on many tough NLP problems:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用深度神经网络进行机器学习时，它被称为*深度学习*。在过去几年里，深度学习已经在许多困难的自然语言处理问题上突破了准确性和智能性的天花板：
- en: question answering
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问答
- en: reading comprehension
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阅读理解
- en: summarization
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总结
- en: '*natural language inference* (NLI)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自然语言推理*（NLI）'
- en: 'And recently deep learning (deep neural networks) enabled previously unimaginable
    applications:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，深度学习（深度神经网络）实现了以前难以想象的应用：
- en: long, engaging conversations
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长时间的、引人入胜的对话
- en: companionship
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陪伴
- en: writing software
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写软件
- en: 'That last one, writing software, is particularly interesting, because NLP neural
    networks are being used to write software …​ wait for it …​ for NLP. This means
    that AI and NLP algorithms are getting closer to the day when they will be able
    to self-replicate and self-improve. This has renewed hope and interest in neural
    networks as a path toward *Artificial General Intelligence* (AGI) - or at least
    *more* generally intelligent machines. And NLP is already being used to directly
    generate software that is advancing the intelligence of those NLP algorithms.
    That virtuous cycle is creating models so complex and powerful that humans have
    a hard time understanding them and explaining how they work. An OpenAI article
    shows a clear inflection point in the complexity of models that happened in 2012,
    when Geoffrey Hinton’s improvement to neural network architectures caught on.
    Since 2012, the amount of compute used in the largest AI training runs have been
    increasing exponentially with a 3.4-month doubling time.^([[1](#_footnotedef_1
    "View footnote.")]) Neural networks make all this possible because they:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个，写软件，尤其有趣，因为 NLP 神经网络正在被用来编写软件……等等……用于 NLP。这意味着人工智能和自然语言处理算法正在接近一天，它们将能够自我复制和自我改进。这使得神经网络成为通向
    *人工通用智能*（AGI）的路径，至少是*更*普遍智能机器的一种。NLP 已经被用来直接生成正在推进那些 NLP 算法智能的软件。这种良性循环正在创造出如此复杂和强大的模型，以至于人类很难理解它们，解释它们的工作原理。一篇
    OpenAI 文章显示了模型复杂性在 2012 年发生的明显拐点，当时 Geoffrey Hinton 对神经网络架构的改进开始流行起来。自 2012 年以来，用于最大
    AI 训练运行的计算量呈指数增长，每 3.4 个月翻一倍。^([[1](#_footnotedef_1 "查看脚注。")]) 之所以所有这些成为可能，是因为神经网络：
- en: Are better at generalizing from a few examples
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于从少量示例中进行泛化更好
- en: Can automatically engineer features from raw data
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以自动从原始数据中提取特征
- en: Can be trained easily on any unlabeled text
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以很容易地在任何未标记的文本上进行训练
- en: Neural networks do the feature engineering for you, and they do it optimally.
    They extract generally useful features and representations of your data according
    to whatever problem you set up in your pipeline. And modern neural networks work
    especially well even for information-rich data such as natural language text.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络为你做特征工程，而且做得非常优秀。它们根据你在管道中设置的问题，提取通常有用的特征和数据的表示。现代神经网络在信息丰富的数据，比如自然语言文本方面尤其表现出色。
- en: 5.1.1 Neural networks for words
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.1 单词的神经网络
- en: With neural networks, you don’t have to guess whether the proper nouns or the
    average word length or hand-crafted word sentiment scores are going to be what
    your model needs. You can avoid the temptation to use readability scores, or sentiment
    analyzers to reduce the dimensionality of your data. You don’t even have to squash
    your vectors with blind (unsupervised) dimension reduction approaches such as
    stop word filtering, stemming, lemmatizing, LDA, PCA, TSNE, or clustering. A neural
    network *mini-brain* can do this for you, and it will do it optimally, based on
    the statistics of the relationship between words and your target.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 有了神经网络，你不需要猜测专有名词、平均词长或手工制作的词语情感分数是否是你的模型所需要的。你可以避免使用可读性分数，或情感分析器来降低数据的维度。你甚至不需要使用盲目（无监督）的降维方法，如停用词过滤、词干提取、词形还原、LDA、PCA、TSNE
    或聚类。一个神经网络的 *小型大脑* 可以为你做到这一切，并且它会根据词与你的目标之间的关系统计学来进行优化。
- en: Warning
  id: totrans-28
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 警告
- en: Don’t use stemmers, lemmatizers or other keyword-based preprocessing in your
    deep learning pipeline unless you’re absolutely sure it is helping your model
    perform better for your application.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的深度学习管道中不要使用词干提取器、词形还原器或其他基于关键字的预处理，除非你确信它能够帮助你的模型在你的应用中表现更好。
- en: If you’re doing stemming, lemmatization, or keyword-based analyses you probably
    want to try your pipeline without those filters. It doesn’t matter whether you
    use NLTK, Stanford Core NLP, or even SpaCy, hand-crafted linguistic algorithms
    like lemmatizers are probably not helping. These algorithms are limited by the
    hand-labeled vocabulary and hand-crafted regular expressions that define the algorithm.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在进行词干提取、词形还原或基于关键字的分析，你可能想要尝试在没有这些过滤器的情况下运行你的管道。无论你使用的是 NLTK，Stanford Core
    NLP，还是 SpaCy，手工制作的词汇算法，比如词形还原器，可能没有帮助。这些算法受到手工标记的词汇表和手工制作的正则表达式的限制。
- en: 'Here are some preprocessing algorithms that will likely trip up your neural
    nets:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些预处理算法可能会使你的神经网络遇到困难：
- en: Porter stemmer
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 波特词干提取器
- en: Penn Treebank lemmatizer
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 宾夕法尼亚树库词形还原器
- en: Flesch-Kincaid readability analyzer
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flesch-Kincaid 可读性分析器
- en: VADER sentiment analyzer
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VADER 情感分析器
- en: In the hyperconnected modern world of machine learning and deep learning, natural
    languages evolve too rapidly and these algorithms can’t keep up. Stemmers and
    lemmatizers are overfit to a bygone era. The words "hyperconnected" and "overfit"
    were nonexistent 50 years ago. Lemmatizers, stemmers, and sentiment analyzers
    often do the wrong thing with unanticipated words such as these.^([[2](#_footnotedef_2
    "View footnote.")])
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习和深度学习的超连通现代世界中，自然语言也发展得太快，这些算法跟不上。词干提取器和词形提取器过度拟合于过去的时代。50 年前，"超连通"和 "过度拟合"
    这样的词根本不存在。词形提取器、词干提取器和情感分析器常常对这些未预料到的词做出错误的处理。^([[2](#_footnotedef_2 "View footnote.")])
- en: Deep learning is a game changer for NLP. In the past, brilliant linguists like
    Julie Beth Lovins needed to hand-craft algorithms to extract stems, lemmas, and
    keywords from text.^([[3](#_footnotedef_3 "View footnote.")]) (Her one-pass stemmer
    and lemmatizer algorithms were later made famous by Martin Porter and others)^([[4](#_footnotedef_4
    "View footnote.")]) Deep neural networks now make all that laborious work unnecessary.
    They directly access the meaning of words based on their statistics, without requiring
    brittle algorithms like stemmers and lemmatizers.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是自然语言处理的游戏规则改变者。在过去，像朱莉·贝丝·洛文斯（Julie Beth Lovins）这样的杰出语言学家需要手工制作算法来从文本中提取词干、词形和关键词。^([[3](#_footnotedef_3
    "View footnote.")])（她的单遍词干提取器和词形提取器算法后来被马丁·波特等人所广为人知）^([[4](#_footnotedef_4 "View
    footnote.")]) 深度神经网络现在使所有这些繁琐的工作都变得不必要。它们根据单词的统计信息直接访问单词的含义，而不需要像词干提取器和词形提取器那样脆弱的算法。
- en: Even powerful feature engineering approaches like the Latent Semantic Analysis
    (LSA) of Chapter 4 can’t match the NLU capabilities of neural nets. The automatic
    learning of decision thresholds with decision trees, random forests, and boosted
    trees does’t provide the depth of language understanding of neural nets. Conventional
    machine learning algorithms made full-text search and universally accessible knowledge
    a reality. But deep learning with neural networks makes artificial intelligence
    and intelligent assistants possible. You no longer needed an information retrieval
    expert or librarian to find what you were looking for, you have a virtual librarian
    to assist you. Deep Learning now powers your thinking in ways you wouldn’t have
    imagined a few years ago.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 即使像第 4 章的潜在语义分析（LSA）这样的强大特征工程方法也无法匹敌神经网络的 NLU 能力。通过决策树、随机森林和提升树自动学习决策阈值并不提供神经网络的语言理解深度。传统的机器学习算法使全文检索和普遍可访问的知识成为现实。但是具有神经网络的深度学习使得人工智能和智能助手成为可能。你不再需要信息检索专家或图书馆员来找到你想要的东西，你有一个虚拟的图书管理员来协助你。深度学习现在以你之前无法想象的方式推动着你的思维。
- en: What is it about deep layers of neurons that has propelled NLP to such prominence
    in our lives? Why is it that we are now so dependent on neural machine translation
    (NMT) recommendation engines, middle button suggestions (There’s a subreddit where
    people post comments made entirely of middle button suggestions from their smartphones?^([[5](#_footnotedef_5
    "View footnote.")])), and auto-reply nudges? If you’ve tried digital detox, you
    may have experienced this sensation of not being fully yourself without NLP helping
    you behind the scenes. And NLP neural nets for have given us hope that Artificial
    General Intelligence (AGI) is within reach. They promise to allow machines to
    learn in the same way we often do, by just reading a lot of text.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 深层神经元有什么特点，使得自然语言处理在我们的生活中如此突出？为什么我们现在如此依赖神经机器翻译（NMT）推荐引擎、中间按钮建议（有一个 Subreddit，人们在那里发布完全由他们智能手机的中间按钮建议组成的评论？^([[5](#_footnotedef_5
    "View footnote.")])), 和自动回复提醒？如果你尝试过数字排毒，你可能会感受到没有 NLP 在幕后帮助你，你就不能完全做自己的这种感觉。而
    NLP 神经网络给了我们希望，认为通用人工智能（AGI）已在触手可及范围内。它们承诺允许机器以我们通常的方式学习，只需阅读大量文本。
- en: The power of NLP that you learned to employ in the previous chapters is about
    to get a lot more powerful. You’ll want to understand how deep, layered networks
    of artificial neurons work in order to ensure that your algorithms benefit society
    instead of destroy it. (Stuart Russell’s *Human Compatible AI* explains the dangers
    and promise of AI and AGI, with some insightful NLP examples.) To wield this power
    for good, you need to get a feeling for how neural networks work all the way down
    deep at the individual neuron.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你学习到的应用于NLP的强大力量即将变得更加强大。您需要了解深层次的人工神经元网络如何工作，以确保您的算法造福于社会而不是毁坏它（Stuart Russell的“Human
    Compatible AI”阐释了人工智能（AI）和人工智能通用（AGI）的危险和承诺，具有一些有见地的NLP示例。）。为了运用这种力量造福于人类，您需要对单个神经元的工作有所了解。
- en: You’ll also want to understand *why* they work so well for many NLP problems…​and
    why they fail miserably on others.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要了解*为什么*它们对许多NLP问题如此有效……以及为什么在其他问题上失败得惨不忍睹。
- en: We want to save you from the "AI winter" that discouraged researchers in the
    past. If you employ neural networks incorrectly you could get frost-bitten by
    an overfit NLP pipeline that works well on your test data, but proves disastrous
    in the real world. As you get to understand how neural networks work, you will
    begin to see how you can build more *robust NLP* neural networks. Neural networks
    for NLP problems are notoriously brittle and vulnerable to adversarial attacks
    such as poisoning. (You can learn more about how to measure a model’s robustness
    and improve it from Robin Jia’s PhD thesis.^([[6](#_footnotedef_6 "View footnote.")]))
    But first, you must build an intuition for how a single neuron works.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望帮助您摆脱过去使研究人员感到沮丧的“人工智能冬天”。如果将神经网络使用不当，则可能会被一个过度拟合的 NLP（自然语言处理）流水线冻伤，该流水线在您的测试数据上表现良好，但在实际应用中会造成灾难性后果。随着您对神经网络如何工作的了解加深，您将会开始看到如何构建更*强大的
    NLP*神经网络。 NLP 问题的神经网络因质量脆弱且容易受到敌意攻击，例如污染（可以从 Robin Jia 的博士学位论文中了解有关如何测量模型的稳健性并改进它的更多信息。^([[6](#_footnotedef_6
    "View footnote.")])) 但是，首先你必须对单个神经元的工作有所了解。
- en: Tip
  id: totrans-43
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: Here are two excellent natural language texts about processing natural language
    text with neural networks. You can even use these texts to train a deep learning
    pipeline to understand the terminology of NLP.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 下面列出了两本关于用神经网络处理自然语言文本的优秀自然语言文本。 您甚至可以使用这些文本来训练深度学习流水线以理解NLP的术语。
- en: '*A Primer on Neural Network Models for Natural Language Processing* by Yoav
    Goldberg ([https://archive.is/BNEgK](archive.is.html))'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yoav Goldberg 的 *自然语言处理神经网络模型入门*（[https://archive.is/BNEgK](archive.is.html)）
- en: '*CS224d: Deep Learning for Natural Language Processing* by Richard Socher ([https://web.stanford.edu/class/cs224d/lectures/](lectures.html))'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Richard Socher 的 *CS224d：自然语言处理的深度学习*（[https://web.stanford.edu/class/cs224d/lectures/](lectures.html)）
- en: You might also want to check *Deep learning for Natural Language Processing*
    by Stephan Raaijmakers on Manning.([https://www.manning.com/books/deep-learning-for-natural-language-processing](books.html))
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能还想查看Manning的Stephan Raaijmakers所写的 *深度学习自然语言处理*（[https://www.manning.com/books/deep-learning-for-natural-language-processing](books.html)）
- en: 5.1.2 Neurons as feature engineers
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.2 神经元作为特征工程师
- en: One of the main limitations of linear regression, logistic regression, and naive
    Bayes models is that they all require you to engineer features one by one. You
    must find the best numerical representation of your text among all the possible
    ways to represent text as numbers. Then you have to parameterize a function that
    takes in these engineered feature representations and outputs your predictions.
    Only then can the optimizer start searching for the parameter values that best
    predict the output variable.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归、逻辑回归和朴素贝叶斯模型的主要限制之一是它们都需要逐个进行特征工程。你必须在所有可能的文本数字表示方法中找到最好的数字表示形式。然后，您必须参数化一个函数，该函数接受这些经过工程化的特征表示并输出您的预测。只有在此之后，优化器才能开始搜索最佳预测输出变量的参数值。
- en: Note
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: In some cases, you will want to manually engineer threshold features for your
    NLP pipeline. This can be especially useful if you need an explainable model that
    you can discuss with your team and relate to real-world phenomena. To create a
    simpler model with few engineered features, without neural networks, requires
    you to examine residual plots for each and every feature. When you see a discontinuity
    or nonlinearity in the residuals at a particular value of the feature, that’s
    a good threshold value to add to your pipeline. Sometimes, you can even find an
    association between your engineered thresholds and real-world phenomena.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，您可能希望为您的NLP流水线手动创建阈值特征。如果您需要一个可与团队讨论并与现实现象相关联的可解释模型，这将非常有用。要创建一个具有少量工程特征的简化模型，而不使用神经网络，您需要检查每个特征的残差图。当您在特征的特定值处看到残差中断或非线性时，那就是要添加到您的流水线中的好的阈值值。有时，您甚至可以找到您工程阈值和现实世界现象之间的关联。
- en: For example, the TF-IDF vector representation you used in Chapter 3 works well
    for information retrieval and full-text search. However, TF-IDF vectors often
    don’t generalize well for semantic search or NLU in the real world where words
    are used in ambiguous ways or mispelled. And the PCA or LSA transformation of
    Chapter 4 may not find the right topic vector representation for your particular
    problem. They are good for visualization but not optimal for NLU applications.
    Multi-layer neural networks promise to do this feature engineering for you and
    do it in a way that’s in some sense optimal. Neural networks search a much broader
    space of possible feature engineering functions.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您在第3章中使用的TF-IDF向量表示对信息检索和全文搜索非常有效。但是，在现实世界中，TF-IDF向量通常无法很好地泛化到语义搜索或NLU，因为单词以模糊的方式使用或拼写错误。并且第4章中的PCA或LSA转换可能无法为您的特定问题找到正确的主题向量表示。它们适用于可视化，但对于NLU应用来说并非最佳选择。多层神经网络承诺为您进行特征工程，并以某种意义上的最佳方式执行此操作。神经网络搜索更广泛的可能特征工程函数空间。
- en: Dealing with the polynomial feature explosion
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 处理多项式特征爆炸
- en: Another example of some feature engineering that neural networks can optimize
    for you is polynomial feature extraction. (Think back to the last time you used
    `sklearn.preprocessing.PolynomialFeatures`) During feature engineering, you might
    guess that the relationship between inputs and outputs is quadratic. In that case
    you would square those input features and retrain a model with these new features
    to see if it improved your model’s accuracy on the test set. Basically, if the
    residuals for a particular feature (prediction minus test-set label) do not look
    like white noise centered on zero, then that is an opportunity for you to take
    out some more error from your model’s predictions by transforming that feature
    with some nonlinear function, such as square (`*\*2`), cube (`\*\*3`), `sqrt`,
    `log`, `exp`. Any function you can dream up is fair game. And you will gradually
    develop an intuition that helps you guess the right function that will improve
    your accuracy the most. And if you don’t know which interactions might be critical
    to solving your problem, you have to multiply all your features by each other.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个神经网络可以为您优化的一些特征工程的例子是多项式特征提取（回想一下您上次使用`sklearn.preprocessing.PolynomialFeatures`的情况）。在特征工程期间，您可能会猜想输入和输出之间的关系是二次的。在这种情况下，您会对这些输入特征进行平方，并使用这些新特征重新训练模型，以查看它是否改善了模型在测试集上的准确性。基本上，如果特定特征（预测值减去测试集标签）的残差看起来不像以零为中心的白噪声，那么这就是您利用一些非线性函数来从模型的预测中消除更多误差的机会，比如平方（`*\*2`），立方（`\*\*3`），`sqrt`，`log`，`exp`。您能想到的任何函数都是公平竞争的。您将逐渐培养出一种直觉，帮助您猜测出最能提高准确性的正确函数。如果您不知道哪些相互作用可能对解决问题至关重要，那么您必须将所有特征互相乘起来。
- en: You know the depth and breadth of this rabbit hole. The number of possible fourth-order
    polynomial features is virtually limitless. You might try to reduce the dimensions
    of your TF-IDF vectors from 10s of thousands to 100s of dimensions using PCA or
    LSA. But throwing in fourth-order polynomial features would exponentially expand
    your dimensionality beyond even the dimensionality of TF-IDF vectors.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 您了解这个兔子洞的深度和广度。可能的四阶多项式特征的数量几乎是无限的。您可以尝试使用PCA或LSA将TF-IDF向量的维度从数万个减少到数百个。但是将四次多项式特征加入其中将会使您的维度远远超过TF-IDF向量的维度。
- en: And even with millions of possible polynomial features, there are still millions
    more threshold features. Random forests of decision trees and boosted decision
    trees have advanced to the point that they do a decent job of feature engineering
    automatically. So finding the right threshold features is essentially a solved
    problem. But these feature representations are difficult to explain and sometimes
    don’t generalize well to the real world. This is where neural nets can help.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 即使有数百万种可能的多项式特征，还有数百万个阈值特征。决策树的随机森林和提升决策树已经发展到一定程度，可以自动进行良好的特征工程。因此，找到合适的阈值特征基本上是一个已解决的问题。但是这些特征表示难以解释，有时在现实世界中的泛化效果不佳。这就是神经网络可以发挥作用的地方。
- en: The Holy Grail of feature engineering is finding representations that say something
    about the physics of the real world. If your features are explainable according
    to real-world phenomena, you can begin to build confidence that it is more than
    just predictive. It may be a truly causal model that says something about the
    world that is true in general and not just for your dataset.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程的圣杯是找到能够反映真实世界物理的表示。如果你的特征可以根据真实世界现象来解释，你就可以开始建立对其不仅仅是预测性的信心。它可能是一个真正的因果模型，它对于世界是普遍真实的，而不仅仅是对你的数据集。
- en: Peter Woit explains how the explosion of possible models in modern physics are
    mostly *Not Even Wrong* .^([[7](#_footnotedef_7 "View footnote.")]) These *not
    even wrong* models are what you create when you use `sklearn.preprocessing.PolynomialFeatures`.
    And that is a real problem. Very few of the millions of these extracted polynomial
    features are even physically possible. In other words the vast majority of polynomial
    features are just noise.^([[8](#_footnotedef_8 "View footnote.")]) So if you `PolynomialFeatures`
    in your preprocessing, limit the `degree` parameter to `2` or less.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Peter Woit 解释了现代物理学中可能模型的激增大部分都是*甚至不错*的。[[7]](#_footnotedef_7 "查看脚注") 当你使用 `sklearn.preprocessing.PolynomialFeatures`
    时，这些*甚至不错*的模型就是你所创建的。而这是一个真正的问题。这些提取的多项式特征中很少有数百万是物理上可能的。换句话说，绝大多数多项式特征只是噪音。[[8]](#_footnotedef_8
    "查看脚注") 因此，如果在预处理中使用 `PolynomialFeatures`，请将 `degree` 参数限制为 `2` 或更低。
- en: Important
  id: totrans-59
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 重要提示
- en: For any machine learning pipeline, make sure your polynomial features never
    include the multiplication of more than 2 physical quantities. If you decide to
    try polynomial features with a degree greater than two you can save yourself some
    grief by filtering out unrealizable (fantasy) 3-way interaction features. For
    example `x1 * x2 \** 2` is a legitimate third-degree polynomial feature to try,
    but `x1 * x2 * x3` is not. Polynomial features involving the interaction (multiplication)
    of more than two features together are not physically realizable. Removing these
    "fantasy features" will improve the robustness of your NLP pipeline and help you
    reduce any hallucinations coming out of your generative models.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何机器学习流水线，确保你的多项式特征永远不包括超过 2 个物理量的乘积。如果你决定尝试高于二次的多项式特征，可以通过滤除不可实现（幻想）的三路交互特征来减少困扰。例如，`x1
    * x2 \** 2` 是一个合法的三次多项式特征，但 `x1 * x2 * x3` 不是。涉及超过两个特征之间交互（乘法）的多项式特征在物理上是不可实现的。移除这些“幻想特征”将提高你的
    NLP 流水线的鲁棒性，并帮助你减少生成模型产生的任何幻觉。
- en: We hope that by now you’re inspired by the possibilities that neural networks
    offer. Let’s start our journey into the world of neural networks building single
    neurons that look a lot like logistic regressions. Ultimately, you will be able
    to combine and stack these neurons in layers that optimize the feature engineering
    for you.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望到目前为止你已经受到了神经网络所提供的可能性的启发。让我们开始我们的神经网络之旅，构建类似逻辑回归的单个神经元。最终，你将能够组合和堆叠这些神经元，以优化特征工程。
- en: 5.1.3 Biological neurons
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.3 生物神经元
- en: Frank Rosenblatt came up with the first artificial neural network based on his
    understanding of how biological neurons in our brains work. He called it a perceptron
    because he was using it to help machines perceive their environment using sensor
    data as input.^([[9](#_footnotedef_9 "View footnote.")]) He hoped they would revolutionize
    machine learning by eliminating the need to hand-craft filters to extract features
    from data. He also wanted to automate the process of finding the right combination
    of functions for any problem.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 弗兰克·罗森布拉特根据自己对大脑中生物神经元工作原理的理解提出了第一个基于人工神经网络的模型。他将其称为感知器，因为他使用它来帮助机器利用传感器数据感知其环境。[[9]](#_footnotedef_9)
    他希望这些感知器通过消除手工设计的滤波器从数据中提取特征的需要来革新机器学习。他还希望自动化找到任何问题的正确功能组合的过程。
- en: He wanted to make it possible for engineers to build AI systems without having
    to design specialized models for each problem. At the time, engineers used linear
    regressions, polynomial regressions, logistic regressions and decision trees to
    help robots make decisions. Rosenblatt’s perceptron was a new kind of machine
    learning algorithm that could approximate any function, not just a line, a logistic
    function, or a polynomial.^([[10](#_footnotedef_10 "View footnote.")]) He based
    it on how biological neurons work.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 他希望使工程师能够构建 AI 系统，而无需为每个问题设计专门的模型。当时，工程师们使用线性回归、多项式回归、逻辑回归和决策树来帮助机器人做出决策。罗森布拉特的感知器是一种新型的机器学习算法，它可以近似任何函数，不仅仅是一条线、一个逻辑函数或一个多项式。[[10]](#_footnotedef_10)
- en: Figure 5.1 Biological neuron cell
  id: totrans-65
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.1 生物神经元细胞
- en: '![biological neuron cell](images/biological_neuron_cell.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![生物神经元细胞](images/biological_neuron_cell.png)'
- en: Rosenblatt was building on a long history of successful logistic regression
    models. He was modifying the optimization algorithm slightly to better mimic what
    neuroscientists were learning about how biological neurons adjust their response
    to the environment over time.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 罗森布拉特在成功的逻辑回归模型的漫长历史上进行了改进。他略微修改了优化算法，以更好地模拟神经科学家对生物神经元如何随着时间调整其对环境的响应的理解。
- en: Electrical signals flow into a biological neuron in your brain through the *dendrites*
    (see Figure 5.1) and into the nucleus. The nucleus accumulates electric charge
    and it builds up over time. When the accumulated charge in the nucleus reaches
    the activation level of that particular neuron it *fires* an electrical signal
    out through the *axon*. However, neurons are not all created equal. The dendrites
    of the neuron in your brain are more "sensitive" for some neuron inputs than for
    others. And the nucleus itself may have a higher or lower activation threshold
    depending on its function in the brain. So for some more sensitive neurons it
    takes less of a signal on the inputs to trigger the output signal being sent out
    the axon.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 电信号通过*树突*（见图 5.1）流入你大脑中的生物神经元，然后进入细胞核。细胞核积累电荷并随着时间的推移逐渐增加。当细胞核中积累的电荷达到特定神经元的激活水平时，它通过*轴突*发出电信号。然而，神经元并非完全相同。你大脑中的神经元的树突对某些输入的神经元更“敏感”而对其他输入的神经元则不那么“敏感”。细胞核本身可能具有较高或较低的激活阈值，这取决于其在大脑中的功能。因此，对于一些更敏感的神经元，输入的信号量较少即可触发通过轴突发送输出信号。
- en: So you can imagine how neuroscientists might measure the sensitivity of individual
    dendrites and neurons with experiments on real neurons. And this sensitivity can
    be given a numerical value. Rosenblatt’s perceptron abstracts this biological
    neuron to create an artificial neuron with a *weight* associated with each input
    (dendrite). For artificial neurons, such as Rosenblatt’s perceptron, we represent
    the sensitivity of individual dendrites as a numerical *weight* or *gain* for
    that particular path. A biological cell *weights* incoming signals when deciding
    when to fire. A higher weight represents a higher sensitivity to small changes
    in the input.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你可以想象神经科学家如何通过对真实神经元进行实验来测量单个树突和神经元的敏感性。这种敏感性可以被赋予数值。罗森布拉特的感知器将这种生物神经元抽象化，创建了一个与每个输入（树突）相关联的*权重*的人工神经元。对于人工神经元，例如罗森布拉特的感知器，我们将单个树突的敏感性表示为该特定路径的数值*权重*或*增益*。生物细胞在决定何时触发时加权输入信号。更高的权重代表对输入的小变化更敏感。
- en: A biological neuron will dynamically change those weights in the decision-making
    process over the course of its life. You are going to mimic that biological learning
    process using the machine learning process called *back propagation*.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 生物神经元会在其生命周期中动态地改变这些权重，在决策过程中。您将使用称为*反向传播*的机器学习过程来模仿这种生物学习过程。
- en: Figure 5.2 Basic perceptron
  id: totrans-71
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.2 基本感知器
- en: '![perceptron](images/perceptron.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![perceptron](images/perceptron.png)'
- en: AI researchers hoped to replace the rigid math of logistic regressions and linear
    regressions and polynomial feature extraction with the more fuzzy and generalized
    logic of neural networks — tiny brains. Rosenblatt’s artificial neurons even worked
    for trigonometric functions and other highly nonlinear functions. Each neuron
    solved one part of the problem and could be combined with other neurons to learn
    more and more complex functions. (Though not all of them - even simple functions,
    like an XOR gate can’t be solved with a single layer perceptron). He called this
    collection of artificial neurons a perceptron.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: AI 研究人员希望用更模糊、更泛化的神经网络逻辑来取代 logistic 回归、线性回归和多项式特征提取的严格数学 —— 小型大脑。Rosenblatt
    的人工神经元甚至可以处理三角函数和其他高度非线性函数。每个神经元解决问题的一部分，并且可以与其他神经元结合来学习更加复杂的函数。（尽管并非所有的都行 - 即使是简单的函数，比如
    XOR 门，也不能用单层感知器解决）。他把这些人工神经元的集合称为感知器。
- en: Rosenblatt didn’t realize it at the time, but his artificial neurons could be
    layered up just as biological neurons connect to each other in clusters. In modern
    *deep learning* we connect the predictions coming out of one group of neurons
    to another collection of neurons to refine the predictions. This allows us to
    create layered networks that can model *any* function. They can now solve any
    machine learning problem …​ if you have enough time and data.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 'Rosenblatt 当时并没有意识到，但他的人工神经元可以像生物神经元一样被层叠起来，连接成簇。在现代* 深度学习* 中，我们将一个组神经元的预测连接到另一个组神经元以细化预测。这使我们能够创建层叠网络，可以模拟*任何*函数。如果您有足够的时间和数据，它们现在可以解决任何机器学习问题……。 '
- en: Figure 5.3 Neural network layers
  id: totrans-75
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.3 神经网络层
- en: '![multilayer perceptron](images/multilayer-perceptron.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![multilayer perceptron](images/multilayer-perceptron.png)'
- en: 5.1.4 Perceptron
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.4 感知器
- en: One of the most complex things neurons do is process language. Think about how
    a perceptron might be used to process natural language text. Does the math shown
    in Figure 5.2 remind you of any of the machine learning models you’ve used before?
    What machine learning models do you know of that multiply the input features by
    a vector of weights or coefficients? Well, that would be a linear regression.
    But what if you used a sigmoid activation function or logistic function on the
    output of a linear regression? It’s starting to look a lot like a *logistic regression*
    to me.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元处理的最复杂的事情之一就是语言处理。想象一下感知器如何用于处理自然语言文本。图 5.2 中显示的数学内容让您想起了以前使用过的任何机器学习模型吗？您知道哪些机器学习模型将输入特征与权重或系数向量相乘吗？好吧，那将是一个线性回归。但是如果您在线性回归的输出上使用
    sigmoid 激活函数或 logistic 函数呢？对我来说，它开始看起来很像* logistic 回归*。
- en: The sigmoid *activation function* used in a perceptron is actually the same
    as the logistic function used within logistic regression. Sigmoid just means s-shaped.
    And the logistic function has exactly the shape we want for creating a soft threshold
    or logical binary output. So really what your neuron is doing here is equivalent
    to a logistic regression on the inputs.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在感知器中使用的 sigmoid *激活函数*实际上与 logistic 回归中使用的逻辑函数相同。Sigmoid 只是表示 s 形状。逻辑函数正好具有我们用于创建软阈值或逻辑二进制输出的形状。因此，你的神经元在这里实际上相当于对输入进行
    logistic 回归。
- en: This is the formula for a logistic function implemented in Python.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 Python 中实现的逻辑函数的公式。
- en: '[PRE0]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: And here is what a logistic function looks like, and how the coefficient (weight)
    and phase (intercept) affect its shape.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这是逻辑函数的样子，以及系数（权重）和相位（截距）如何影响其形状。
- en: '[PRE1]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: What were your inputs when you did a logistic regression on natural language
    sentences in earlier chapters? You first processed the text with a keyword detector,
    `CountVectorizer`, or `TfidfVectorizer`. These models use a tokenizer, like the
    ones you learned about in chapter 2 to split the text into individual words, and
    then count them up. So for NLP it’s common to use the BOW counts or the TF-IDF
    vector as the input to an NLP model, and that’s true for neural networks as well.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Each of Rosenblatt’s input weights (biological dendrites) had an adjustable
    value for the weight or sensitivity of that signal. Rosenblatt implemented this
    weight with a potentiometer, like a volume knob on an old-fashioned stereo receiver.
    This allowed researchers to manually adjust the sensitivity of their neuron to
    each of its inputs individually. A perceptron can be made more or less sensitive
    to the counts of each word in the BOW or TF-IDF vector by adjusting this sensitivity
    knob.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Once the signal for a particular word was increased or decreased according to
    the sensitivity or weight it passed into the main body of the biological neuron
    cell. It’s here in the body of the perceptron, and also in a real biological neuron,
    where the input signals are added together. Then that signal is passed through
    a soft thresholding function like a sigmoid before sending the signal out the
    axon. A biological neuron will only *fire* if the signal is above some threshold.
    The sigmoid function in a perceptron just makes it easy to implement that threshold
    at 50% of the min-max range. If a neuron doesn’t fire for a given combination
    of words or input signals, that means it was a negative classification match.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.5 A Python perceptron
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So a machine can simulate a really simple neuron by multiplying numerical features
    by "weights" and combining them together to create a prediction or make a decision.
    These numerical features represent your object as a numerical vector that the
    machine can "understand". For the home price prediction problem of Zillow’s zestimate,
    how do you think they might build an NLP-only model to predict home prices? But
    how do you represent the natural language description of a house as a vector of
    numbers so that you can predict its price? You could take a verbal description
    of the house and use the counts of each word as a feature, just as you did in
    Chapters 2 and 3\. Or you could use a transformation like PCA to compress these
    thousands of dimensions into topic vectors, as you did with PCA in Chapter 4.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: But these approaches are just a guess at which features are important, based
    on the variability or variance of each feature. Perhaps the key words in the description
    are the numerical values for the square footage and number of bedrooms in the
    home. Your word vectors and topic vectors would miss these numerical values entirely.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: In "normal" machine learning problems, like predicting home prices, you might
    have structured numerical data. You will usually have a table with all the important
    features listed, such as square footage, last sold price, number of bedrooms,
    and even latitude and longitude or zip code. For natural language problems, however,
    we want your model to be able to work with unstructured data, text. Your model
    has to figure out exactly which words and in what combination or sequence are
    predictive of your target variable. Your model must read the home description,
    and, like a human brain, make a guess at the home price. And a neural network
    is the closest thing you have to a machine that can mimic some of your human intuition.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在"正常"的机器学习问题中，比如预测房价，您可能会有结构化的数值数据。通常会有一张列出所有重要特征的表格，比如房屋面积、上次售价、卧室数量，甚至纬度和经度或邮政编码。但是对于自然语言问题，我们希望您的模型能够处理非结构化数据，即文本。您的模型必须准确地找出哪些单词以及以何种组合或顺序对目标变量具有预测能力。您的模型必须阅读房屋描述，并像人脑一样猜测房价。而神经网络是您能找到的最接近模仿人类直觉的机器。
- en: The beauty of deep learning is that you can use as your input every possible
    feature you can dream up. This means you can input the entire text description
    and have your transformer produce a high-dimensional TF-IDF vector and a neural
    network can handle it just fine. You can even go higher dimensional than that.
    You can pass it the raw, unfiltered text as 1-hot encoded sequences of words.
    Do you remember the piano roll we talked about in Chapter 2? Neural networks are
    made for these kinds of raw representations of natural language data.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的美妙之处在于您可以使用您可以想象到的每一个可能的特征作为输入。这意味着您可以输入整个文本描述，并且您的转换器可以生成一个高维的 TF-IDF
    向量，而神经网络可以很好地处理它。您甚至可以更高维度地使用它。您可以将原始、未经过滤的文本作为单词的 1-hot 编码序列传递。您还记得我们在第2章谈到的钢琴卷吗？神经网络就是为这种原始的自然语言数据表示而生的。
- en: Shallow learning
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 浅层学习
- en: For your first deep learning NLP problem, you will keep it shallow. To understand
    the magic of deep learning it helps to see how a single neuron works. A single
    neuron will find a *weight* for each feature you input into the model. You can
    think of these weights as a percentage of the signal that is let into the neuron.
    If you’re familiar with linear regression, then you probably recognize these diagrams
    and can see that the weights are just the slopes of a linear regression. And if
    you throw in a logistic function, these weights are the coefficients that a logistic
    regression learns as you give it examples from your dataset. To put it in different
    words, the weights for the inputs to a single neuron are mathematically equivalent
    to the slopes in a multivariate linear regression or logistic regression.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对于您的第一个深度学习自然语言处理问题，您将保持表面。要理解深度学习的魔力，看看单个神经元如何工作会有所帮助。单个神经元将为输入模型的每个特征找到一个*权重*。您可以将这些权重视为输入神经元的信号的百分比。如果您熟悉线性回归，那么您可能会认出这些图表，并且可以看到权重只是线性回归的斜率。如果你加上了一个
    logistic函数，这些权重就是逻辑回归从您的数据集中学到的系数。换句话说，单个神经元的输入权重在数学上等同于多元线性回归或逻辑回归中的斜率。
- en: Tip
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: Just as with the Scikit-Learn machine learning models, the individual features
    are denoted as `x[i]` or in Python as `x[i]`. The *i* is an indexing integer denoting
    the position within the input vector. And the collection of all features for a
    given example are within the vector **x**.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 就像Scikit-Learn机器学习模型一样，各个特征被表示为`x[i]`或在Python中表示为`x[i]`。*i*表示输入向量中的位置。给定示例的所有特征的集合都在向量**x**中。
- en: '`x = x[1], x[2], …​, x[i], …​, x[n]`'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`x = x[1], x[2], …​, x[i], …​, x[n]`'
- en: And similarly, you’ll see the associate weights for each feature as w[i], where
    *i* corresponds to the integer in x. The weights are generally represented as
    a vector **W**
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，您将看到每个特征的关联权重为w[i]，其中*i*对应于x中的整数。权重通常表示为向量**W**
- en: '`w = w[1], w[2], …​, w[i], …​, w[n]`'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`w = w[1], w[2], …​, w[i], …​, w[n]`'
- en: With the features in hand, you just multiply each feature (x[i]) by the corresponding
    weight (w[i]) and then sum up.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 有了特征之后，您只需将每个特征（x[i]）与相应的权重（w[i]）相乘，然后求和。
- en: '`y = (x[1] * w[1]) + (x[2] * w[2]) + …​ + (x[i] * w[i])`'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`y = (x[1] * w[1]) + (x[2] * w[2]) + …​ + (x[i] * w[i])`'
- en: 'Here’s a fun, simple example to make sure you understand this math. Imagine
    an input BOW vector for a phrase like "green egg egg ham ham ham spam spam spam
    spam":'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个有趣的简单示例，以确保您理解这个数学。想象一个由短语"green egg egg ham ham ham spam spam spam spam"构成的
    BOW 向量：
- en: '[PRE2]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: So this 4-input, 1-output, single-neuron network outputs a value of -0.76 for
    these random weights in a neuron that hasn’t yet been trained.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: There’s one more piece you’re missing here. You need to run a nonlinear function
    on the output (`y`) to change the shape of the output so it’s not just a linear
    regression. Often a thresholding or clipping function is used to decide whether
    the neuron should fire or not. For a thresholding function, if the weighted sum
    is above a certain threshold, the perceptron outputs 1\. Otherwise, it outputs
    0\. You can represent this threshold with a simple *step function* (labeled "Activation
    Function" in Figure 5.2).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the code to apply a step function or thresholding function to the output
    of your neuron:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: And if you want your model to output a continuous probability or likelihood
    rather than a binary `0` or `1`, you probably want to use the logistic activation
    function that we introduced earlier in this chapter.^([[11](#_footnotedef_11 "View
    footnote.")])
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: A neural network works like any other machine learning model — you present it
    with numerical examples of inputs (feature vectors) and outputs (predictions)
    for your model. And like a conventional logistic regression, the neural network
    will use trial and error to find the weights on your inputs that create the best
    predictions. Your *loss function* will measure how much error your model has.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Make sure this Python implementation of the math in a neuron makes sense to
    you. Keep in mind, that the code we’ve written is only for the *feed forward*
    path of a neuron. The math is very similar to what you would see in the `LogisticRegression.predict()`
    function in Scikit-Learn for a 4-input, 1-output logistic regression.^([[12](#_footnotedef_12
    "View footnote.")])
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A *loss function* is a function that outputs a score to measure how bad your
    model is, the total error of its predictions. An *objective function* just measures
    how good your model is based on how small the error is. A *loss function* is like
    the percentage of questions a student got wrong on a test. An *objective function*
    is like the grade or percent score on that test. You can use either one to help
    you learn the right answers and get better and better on your tests.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Why the extra weight?
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Did you notice that you have one additional weight, `w0`? There is no input
    labeled `x0`. So why is there a `w0`? Can you guess why we always give our neural
    neurons an input signal with a constant value of "1.0" for `x0`? Think back to
    the linear and logistic regression models you have built in the past. Do you remember
    the extra coefficient in the single-variable linear regression formula?
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `y` variable is for the output or predictions from the model. The `x` variable
    is for the single independent feature variable in this model. And you probably
    remember that `m` represents the slope. But do you remember what `b` is for?
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now can you guess what the extra weight `w[0]` is for, and why we always make
    sure it isn’t affected by the input (multiply it by an input of 1.0)?
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: It’s the *intercept* from your linear regression, just "rebranded" as the *bias*
    weight (`w0`) for this layer of a neural network.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.2 and this example reference *bias*. What is this? The bias is an "always
    on" input to the neuron. The neuron has a weight dedicated to it just as with
    every other element of the input, and that weight is trained along with the others
    in the exact same way. This is represented in two ways in the various literature
    around neural networks. You may see the input represented as the base input vector,
    say of *n*-elements, with a 1 appended to the beginning or the end of the vector,
    giving you an *n*+1 dimensional vector. The position of the one is irrelevant
    to the network, as long as it is consistent across all of your samples. Other
    times people presume the existence of the bias term and leave it off the input
    in a diagram, but the weight associated with it exists separately and is always
    multiplied by one and added to the dot product of the sample input’s values and
    their associated weights. Both are effectively the same.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: The reason for having the bias weight at all is that you need the neuron to
    be resilient to inputs of all zeros. It may be the case that the network needs
    to learn to output 0 in the face of inputs of 0, but it may not. Without the bias
    term, the neuron would output 0 * weight = 0 for any weights you started with
    or tried to learn. With the bias term, you wouldn’t have the problem. And in case
    the neuron needs to learn to output 0, the neuron can learn to decrement the weight
    associated with the bias term enough to keep the dot product below the threshold.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.3 is a rather neat visualization of the analogy between some of the
    signals within a biological neuron in your brain and the signals of an artificial
    neuron used for deep learning. If you want to get deep, think about how you are
    using a biological neuron to read this book about natural language processing
    to learn about deep learning.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.4 A perceptron and a biological neuron
  id: totrans-127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![artificial neuron vs biological](images/artificial_neuron_vs_biological.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
- en: 'The Python for the simplest possible single neuron looks like this:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Perhaps you are more comfortable with numpy and *vectorized* mathematical operations
    like you learned about in linear algebra class.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note
  id: totrans-133
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Any Python conditional expression will evaluate to a `True` or `False` boolean
    value. If you use that `bool` type in a mathematical operation such as addition
    or multiplication, Python will *coerce* a `True` value into a numerical `int`
    or `float` value of `1` or `1.0`. A `False` value is coerced into a `1` or `0`
    when you multiply a Boolean by, or add it to another number.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: The `w` variable contains the vector of weight parameters for the model. These
    are the values that will be learned as the neuron’s outputs are compared to the
    desired outputs during training. The `x` variable contains the vector of signal
    values coming into the neuron. This is the feature vector, such as a TF-IDF vector
    for a natural language model. For a biological neuron, the inputs are the rate
    of electrical pulses rippling through the dendrites. The input to one neuron is
    often the output from another neuron.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-136
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The sum of the pairwise multiplications of the inputs (`x`) and the weights
    (`w`) is exactly the same as the dot product of the two vectors `x` and `y`. If
    you use numpy, a neuron can be implemented with a single brief Python expression:
    `w.dot(x) > 0`. This is why *linear algebra* is so useful for neural networks.
    Neural networks are mostly just dot products of parameters by inputs. And GPUs
    are computer processing chips designed to do all the multiplications and additions
    of these dot products in parallel, one operation on each GPU core. So a 1-core
    GPU can often perform a dot product 250 times faster than a 4-core CPU.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are familiar with the natural language of mathematics, you might prefer
    the summation notation:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '**Equation 5.1: Threshold activation function**'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '![equation 5 1](images/equation_5_1.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
- en: Your perceptron hasn’t *learned* anything just yet. But you have achieved something
    quite important. You’ve passed data into a model and received an output. That
    output is likely wrong, given you said nothing about where the weight values come
    from. But this is where things will get interesting.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-142
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The base unit of any neural network is the neuron. The basic perceptron is a
    special case of the more generalized neuron. We refer to the perceptron as a neuron
    for now and come back to the terminology when it no longer applies.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Example logistic neuron
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It turns out you are already familiar with a very common kind of perceptron
    or neuron. When you use the logistic function for the *activation function* on
    a neuron, you’ve essentially created a logistic regression model. A single neuron
    with the logistic function for its activation function is mathematically equivalent
    to the `LogisticRegression` model in Scikit-Learn. The only difference is how
    they’re trained. So you are going to first train a logistic regression model and
    compare it to a single-neuron neural network trained on the same data.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1 The logistics of clickbait
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Software (and humans) often need to make decisions based on logical criteria.
    For example, many times a day you probably have to decide whether to click on
    a particular link or title. Sometimes those links lead you to a fake news article.
    So your brain learns some logical rules that it follows before clicking on a particular
    link.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Is it a topic you’re interested in?
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the link look promotional or spammy?
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is it from a reputable source that you like?
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does it look true or factual?
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each one of these decisions could be modeled in an artificial neuron within
    a machine. And you could use that model to create a logic gate in a circuit board
    or a conditional expression (`if` statement) in software. If you did this with
    artificial neurons, the smallest artificial "brain" you could build to handle
    these 4 decisions would use 4 logistic regression gates.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: To mimic your brain’s *clickbait* filter you might decide to train a logistic
    regression model on the length of the headline. Perhaps you have a hunch that
    longer headlines are more likely to be sensational and exaggerated. Here’s a scatter
    plot of fake and authentic news headlines and their headline length in characters.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: The neuron input weight is equivalent to the maximum slope in the middle of
    the logistic regression plot in Figure 5.3 for a fake news classifier with a single
    feature, title length.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.5 Logistic regression - fakeness vs title length
  id: totrans-155
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![fake news title len logistic regression](images/fake_news_title_len_logistic_regression.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
- en: 5.2.2 Sex education
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: How’s that for clickbait? Because the fake news (clickbait) dataset has been
    fully exploited on Kaggle, you’re going to switch to a more fun and useful dataset.
    You’re going to predict the sex of a name with perceptrons (artificial neurons).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: The problem you’re going to solve with this simple architecture is an everyday
    NLU problem that your brain’s millions of neurons try to solve every day. Your
    brain is strongly incentivized to identify the birth sex of the people you interact
    with on social media. (If you’re interested in why this is, Richard McElreath
    and Robert Boyd have a fascinating book on the subject.^([[13](#_footnotedef_13
    "View footnote.")])) A single artificial neuron can solve this challenge with
    about 80% accuracy using only the characters in the first name of a person. You’re
    going to use a sample of names from a database of 317 million birth certificates
    across US states and territories over more than 100 years.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Biologically, identifying someone’s sex is useful to your genes because they
    only survive if you reproduce them by finding a sexual partner to blend your genes
    with. Social interaction with other humans is critical to your genes' existence
    and survival. And your genes are the blueprint for your brain. So your brain is
    likely to contain at least a few neurons dedicated to this critical task. And
    you’re going to find out how many artificial neurons it takes to predict the sex
    associated with a baby’s given name (first name).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Sex
  id: totrans-161
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The word *sex* here refers to the label a doctor assigns to a baby at birth.
    In the US, the name, sex and date of birth are recorded on a birth certificate
    according to the laws that state. And the sex category is subject to interpretation
    and judgment by the person who fills out and signs the birth certificate. In datasets
    derived from US birth certificates, "sex at birth" is usually equivalent to one’s
    *genetic sex*, but that is not always the case. It is possible to create a relatively
    well-defined "genetic sex" category based on the presence of XX chromosomes (female)
    or XY chromosomes (male). But biology and life have a way of blurring the boundaries
    of even this seemingly precise definition of "genetic sex".
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Male and female are not the last word in *birth sex* classification. The CDC
    (Center for Disease Control) in recommends that USCDI (US Core Data Interoperability)
    standards include several nonbinary sex categories for clinical or medical use.^([[14](#_footnotedef_14
    "View footnote.")]) In addition to 'female' and 'male', the categories 'unknown',
    and 'something not listed (specify)' are recommended by most western medical systems.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: You want to make sure that your test set names don’t appear anywhere in your
    training set. You also want to make sure that your test set only has one "right"
    label for each name. But this isn’t what you think. There is not one correct binary
    sex label for any particular name. There is indeed a correct probability score
    (continuous value) of maleness or femaleness of a name based on the ratio of the
    counts of names with a particular sex designation on their birth certificates.
    But that "correct" score will change as you add new examples to your dataset.
    Natural language processing is messy and fluid because the natural world and the
    language that describes it is dynamic and impossible to "pin on the wall."^([[15](#_footnotedef_15
    "View footnote.")])
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: This will enable the possibility that your model could *theoretically* achieve
    100% accuracy. Obviously, this isn’t really possible for a problem like this where
    even humans can’t achieve 100% accuracy. But your accuracy on the test set will
    tell you how close you are to this ideal, but only if you delete the duplicate
    names from your test set.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.3 Pronouns and gender vs sex
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some states in the US allow one to indicate their child’s *gender* on a birth
    certificate. Gender is often what people use to decide what pronouns they prefer.
    And there are various ways that people think about their gender. There’s the apparent
    gender that they present to the world and there’s the gender identity that they
    assign to themselves at various stages of their lives. Identifying either of these
    genders is a sensitive subject because it is fraught with legal and social ramifications.
    In many repressive cultures, it can even be a matter of life and death. And gender
    is a very difficult thing to predict for a machine learning algorithm. For this
    chapter, we utilized a simplified binary sex dataset to prepare the scaffolding
    you need to build your natural language processing skills from the ground up.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'And there are practical uses for sex-estimation model even for machines that
    don’t need it to spread their genes. A sex estimation model can be used to solve
    an important and difficult challenge in NLP called *coreference resolution*.^([[16](#_footnotedef_16
    "View footnote.")]) Coreference resolution is when an NLP algorithm identifies
    the object or words associated with pronouns in natural language text. For example,
    consider the pronouns in these sentences: "Maria was born in Ukraine. Her father
    was a physicist. 15 years later she left there for Israel." You may not realize
    it, but you resolved three coreferences in the blink of an eye. Your brain did
    the statistics on the likelihood that "Maria" was a "she/her" and that "Ukraine"
    is a "there".'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Coreference resolution isn’t always that easy, for machines or for humans. It
    is more difficult to do in languages where pronouns do not have gender. It can
    be even more difficult in languages with pronouns that do not discriminate between
    people and inanimate objects. Even languages with genderless objects like English
    sometimes arbitrarily assign gender to important things, such as sailing ships.
    Ships are referred to with feminine pronouns such as "she" and "her." And they
    are often given feminine names.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: So knowing the sex associated with the names of people (and ships) in your text
    can be helpful in improving your NLU pipeline. This can be helpful even when that
    sex identification is a poor indicator of the presented gender of a person mentioned
    in the text. The author of the text will often expect you to make assumptions
    about sex and gender based on names. In gender-bending SciFi novels, visionary
    authors like Gibson use this to keep you on your toes and expand your mind.^([[17](#_footnotedef_17
    "View footnote.")])
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Important
  id: totrans-171
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Make sure your NLP pipelines and chatbots are kind, inclusive and accessible
    for all human beings. In order to ensure your algorithms are unbiased you can
    *normalize* for any sex and gender information in the text data you process. In
    the next chapter you will see all the surprising ways in which sex and gender
    can affect the decisions your algorithms make. And you will see how gender affects
    the decisions of businesses or employers you deal with every day.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.4 Sex logistics
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First, import Pandas and set the `max_rows` to display only a few rows of your
    `DataFrame`s.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now download the raw data from the `nlpia2` repository and sample only 10,000
    rows, to keep things fast on any computer.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The data spans more than 100 years of US birth certificates, but only includes
    the baby’s first name:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '|  | region | sex | year | name | count | freq |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
- en: '| 6139665 | WV | F | 1987 | Brittani | 10 | 0.000003 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
- en: '| 2565339 | MD | F | 1954 | Ida | 18 | 0.000005 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
- en: '| 22297 | AK | M | 1988 | Maxwell | 5 | 0.000001 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
- en: '| …​ | …​ | …​ | …​ | …​ | …​ | …​ |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
- en: '| 4475894 | OK | F | 1950 | Leah | 9 | 0.000003 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
- en: '| 5744351 | VA | F | 2007 | Carley | 11 | 0.000003 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
- en: '| 5583882 | TX | M | 2019 | Kartier | 10 | 0.000003 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
- en: You can ignore the region and birth year information for now. You only need
    the natural language name to predict sex with reasonable accuracy. If you’re curious
    about names, you can explore these variables as features or targets. Your target
    variable will be sex ('M' or 'F'). There are no other sex categories provided
    in this dataset besides male and female.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: You might enjoy exploring the dataset to discover how often your intuition about
    the names parents choose for their babies. Machine learning and NLP are a great
    way to dispell stereotypes and misconceptions.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: That’s what makes NLP and DataScience so much fun. It gives us a broader view
    of the world that breaks us out of the limited perspective of our biological brains.
    I’ve never met a woman named "Timothy" but at least .1% of babies named Timothy
    in the US have female on their birth certificate.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: To speed up the model training, you can aggregate (combine) your data across
    regions and years if those are not aspects of names that you’d like your model
    to predict. You can accomplish this with a Pandas `DataFrame’s `.groupby()` method.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Because we’ve aggregated the numerical data for the column "count", the `counts`
    object is now a Pandas `Series` object rather than a `DataFrame`. It looks a little
    funny because we created a multilevel index on both name and sex. Can you guess
    why?
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Now the dataset looks like an efficient set of examples for training a logistic
    regression. In fact, if we only wanted to predict the likely sex for the names
    in this database, we could just use the max count (the most common usage) for
    each name.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: But this is a book about NLP and NLU (Natural Language Understanding). You’d
    like your models to *understand* the text of the name in some way. And you’d like
    it to work on odd names that are not even in this database, names such as "Carlana",
    a portmanteau of "Carl" and "Ana", her grandparents, or one-of-a-kind names such
    as "Cason." Examples that are not part of your training set or test set are called
    "out of distribution." In the real world, your model will almost always encounter
    words and phrases never seen before. It’s called "generalization" when a model
    can extrapolate to these out-of-distribution examples.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: But how can you tokenize a single word like a name so that your model can generalize
    to completely new made-up names that its never seen before? You can use the character
    n-grams within each word (or name) as your tokens. You can set up a `TfidfVectorizer`
    to count characters and character n-grams rather than words. You can experiment
    with a wider or narrower `ngram_range` but 3-grams are a good bet for most TF-IDF-based
    information retrieval and NLU algorithms. For example, the state-of-the-art database
    PostgreSQL defaults to character 3-grams for its full-text search indexes. In
    later chapters, you’ll even use word piece and sentence piece tokenization which
    can optimally select a variety of character sequences to use as your tokens.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Shouldn’t you normalize the token counts by something like document frequency?
    You will use the counts of births for that. For name TF-IDF vectors you want to
    use counts of births or people as your *document* frequencies. This will help
    your vector represent the frequency of the name outside of your corpus of unique
    names.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Now that you’ve indexed our `names` series by `name` *and* `sex` aggregating
    counts across states and years, there will be fewer unique rows in your `Series`.
    You can de-duplicate the names before calculating TF-IDF character n-gram term
    frequencies. Don’t forget to keep track of the number of birth certificates so
    you use that as your document frequency.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: You’ve aggregated 10,000 name-sex pairs into only 4238 unique name-sex pairings.
    Now you are ready to split the data into training and test sets.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'To ensure you don’t accidentally swap the sexes for any of the names, recreate
    the `name, sex` multiindex:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As you saw earlier, this dataset contains conflicting labels for many names.
    In real life, many names are used for both male and female babies (or other human
    sex categories). Like all machine learning classification problems, the math treats
    it as a regression problem. The model is actually predicting a continuous value
    rather than a discrete binary category. Linear algebra and real life only work
    on real values. In machine learning all dichotomies are false.^([[18](#_footnotedef_18
    "View footnote.")]) Machines don’t think of words and concepts as hard categories,
    so neither should you.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Because of the duplicates the test set flag can be created from the `not` of
    the `istrain`.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Now you can transfer the `istest` and `istrain` flags over to the original Dataframe,
    being careful to fill `NaNs` with False for both the training set and the test
    set.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Now you can use the training set to fit `TfidfVectorizer` without skewing the
    n-gram counts with the duplicate names.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: You need to be careful when working with sparse data structures. If you convert
    them to normal dense arrays with `.todense()` you may crash your computer by using
    up all its RAM. But this sparse matrix contains only about 17 million elements
    so it should work fine within most laptops. You can use `toarray()` on sparse
    matrices to create a DataFrame and give meaningful labels to the rows and columns.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Aah, notice that the column labels (character n-grams) all start with lowercase
    letters. It looks like the `TfidfVectorizer` folded the case (lowercased everything).
    It’s likely that capitalization will help the model, so let’s revectorize the
    names without lowercasing.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: That’s better. These character 1, 2, and 3-grams should have enough information
    to help a neural network guess the sex for names in this birth certificate database.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Choosing a neural network framework
  id: totrans-218
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Logistic regressions are the perfect machine learning model for any high-dimensional
    feature vector such as a TF-IDF vector. To turn a logistic regression into a neuron
    you just need a way to connect it to other neurons. You need a neuron that can
    learn to predict the outputs of other neurons. And you need to spread the learning
    out so one neuron doesn’t try to do all the work. Each time your neural network
    gets an example from your dataset that shows it the right answer it will be able
    to calculate just how wrong it was, the loss or error. But if you have more than
    one neuron working together to contribute to that prediction, they’ll each need
    to know how much to change their weights to move the output closer to the correct
    answer. And to know that you need to know how much each weight affects the output,
    the gradient (slope) of the weights relative to the error. This process of computing
    gradients (slopes) and telling all the neurons how much to adjust their weights
    up and down so that the loss will go down is called *backpropagation* or backprop.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: A deep learning package like PyTorch can handle all that for you automatically.
    In fact, it can handle any computational graph (network) you can dream up. PyTorch
    can handle any network of connections between mathematical operations. This flexibility
    is why most researchers use it rather than TensorFlow (Keras) for their breakthrough
    NLP algorithms. TensorFlow is designed with a particular kind of computational
    graph in mind, one that can be efficiently computed on specialized chips manufactured
    by one of the BigTech companies. Deep Learning is a powerful money-maker for Big
    Tech and they want to train your brain to use only their tools for building neural
    networks. I had no idea BigTech would assimilate Keras into the TensorFlow "Borg",
    otherwise I would not have recommended it in the first edition.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: The decline in portability for Keras and the rapidly growing popularity of PyTorch
    are the main reasons we decided a second edition of this book was in order. What’s
    so great about PyTorch?
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'Wikipedia has an unbiased and detailed comparison of all DeepLearning frameworks.
    And Pandas lets you load it directly from the web into a `DataFrame`:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Here is how you can use some basic NLP to score the top 10 deep learning frameworks
    from the Wikipedia article that lists each of their pros and cons. You will find
    this kind of code useful whenever you want to turn semi-structured natural language
    into data for your NLP pipelines.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Now that the Wikipedia table is cleaned up, you can compute some sort of "total
    score" for each deep learning framework.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: PyTorch got nearly a perfect score because of its support for Linux, Android
    and all popular deep learning applications.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Another promising one you might want to check out is ONNX. It’s really a meta
    framework and an open standard that allows you to convert back and forth between
    networks designed on another framework. ONNX also has some optimization and pruning
    capabilities that will allow your models to run inference much faster on much
    more limited hardware, such as portable devices.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: And just for comparison, how does SciKit Learn stack up to PyTorch for building
    a neural network model?
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.1 Scikit-Learn vs PyTorch
  id: totrans-231
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Scikit-Learn | PyTorch |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
- en: '| for Machine Learning | for Deep Learning |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
- en: '| Not GPU-friendly | Made for GPUs (parallel processing) |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
- en: '| `model.predict()` | `model.forward()` |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
- en: '| `model.fit()` | trained with custom `for`-loop |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
- en: '| simple, familiar API | flexible, powerful API |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
- en: Enough about frameworks, you are here to learn about neurons. PyTorch is just
    what you need. And there’s a lot left to explore to get familiar with your new
    PyTorch toolbox.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.5 A sleek sexy PyTorch neuron
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finally, it’s time to build a neuron using the PyTorch framework. Let’s put
    all this into practice by predicting the sex of the names you cleaned earlier
    in this chapter.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: You can start by using PyTorch to implement a single neuron with a logistic
    activation function - just like the one you used to learn the toy example at the
    beginning of the chapter.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Let’s see what happened here. Our model is a *class* that extends the PyTorch
    class used to define neural networks, `torch.nn.Module`. As with every Python
    class, it has a *constructor* method called `*init*`. The constructor is where
    you can define all the attributes of your neural network - most importantly, the
    model’s layers. In our case, we have an extremely simple architecture - one layer
    with a single neuron, which means there will be only one output. And the number
    of inputs, or features, will be equal to the length of your TF-IDF vector, the
    dimensionality of your features. There were 3663 unique 1-grams, 2-grams, and
    3-grams in our names dataset, so that’s how many inputs you’ll have for this single-neuron
    network.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: The second crucial method you need to implement for your neural network is the
    `forward()` method. This method defines how the input to your model propagates
    through its layers - the *forward propagation*. If you are asking yourself where
    the backward propagation (backprop) is, you’ll soon see, but it’s not in the constructor.
    We decided to use the logistic, or sigmoid, activation function for our neuron
    - so our `forward()` method will use PyTorch’s built-in function `sigmoid`.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Is this all you need to train our model? Not yet. There are two more crucial
    pieces that your neuron needs to learn. One is the loss function, or cost function
    that you saw earlier in this chapter. The Mean Square Error (MSE) you learned
    about in chapter 4 would be a good candidate for the error metric if this were
    a regression problem. For this problem you are doing binary classification, so
    Binary Cross Entropy is a more common error (loss) metric to use.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s what Binary Cross Entropy looks like for a single classification probability
    *p*:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '**Equation 5.2: Binary Cross Entropy**'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '`BCE = -(_y_ log _p_ + (1 - _y_) log1 - _p_)`'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: The logarithmic nature of the function allows it to penalize a "confidently
    wrong" example, when your model predicts with high probability the sex of a particular
    name is male, when it is actually more commonly labeled as female. We can help
    it to make the penalties even more related to reality by using another piece of
    information available to us - the frequency of the name for a particular sex in
    our dataset.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The last thing we need to choose is how to adjust our weights based on the loss
    - the optimizer algorithm. Remember our discussion about "skiing" down the gradient
    of the loss function? The most common way to implement skiing downward called
    Stochastic Gradient Descent (SGD). Instead of taking all of your dataset into
    account, like your Pythonic perceptron did, it only calculates the gradient based
    on one sample at a time or perhaps a mini-batch of samples.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Your optimizer needs two parameters to know how fast or how to ski along the
    loss slope - *learning rate* and *momentum*. The learning rate determines how
    much your weights change in response to an error - think of it as your "ski velocity".
    Increasing it can help your model converge to the local minimum faster, but if
    it’s too large, you may overshoot the minimum every time you get close. Any optimizer
    you would use in PyTorch would have a learning rate.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Momentum is an attribute of our gradient descent algorithm that allows it to
    "accelerate" when it’s moving in the right direction and "slow down" if it’s getting
    away from its target. How do we decide which values to give these two attributes?
    As with other hyperparameters you see in this book, you’ll need to optimize your
    them to see what’s the most effective one for your problem. For now, you can chose
    some arbitrary values for the hyperparameters `momentum` and `lr` (learning rate).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The last step before running our model training is to get the testing and training
    datasets into a format that PyTorch models can digest.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Finally, you’re ready for the most important part of this chapter - the sex
    learning! Let’s look at it and understand what happens at each step.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: That was fast! It should take only a couple of seconds to train this single
    neuron for about 200 epochs and thousands of examples for each epoch.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Looks easy, right? We made it as simple as possible so that you can see the
    steps clearly. But we don’t even know how our model is performing! Let’s add some
    utility functions that will help us see if our neuron improves over time. This
    is called instrumentation. We can of course look at the loss, but it’s also good
    to gauge how our model is doing with a more intuitive score, such as accuracy.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you’ll need a function to convert the PyTorch tensors we get from the
    module back into `numpy` arrays:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now you use this utility function to measure the accuracy of each iteration
    on the tensors for your outputs (predictions):'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now you can rerun your training using this utility function to see the progress
    of the model’s loss and accuracy with each epoch:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: With just a single set of weights for a single neuron, your simple model was
    able to achieve more than 70% accuracy on our messy, ambiguous, real-world dataset.
    Now you can add some more examples from the real world of Tangible AI and some
    of our contributors.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Earlier we chose to use the value 1 to represent "female" and 0 to represent
    "male." The first three example names, "John," "Greg," and "Vishvesh," are the
    names of men who have generously contributed to open source projects that are
    important to me, including the code in this book. It looks like Vish’s name doesn’t
    appear on as many US birth certificates for male babies as John’s or Greg’s. The
    model is more certain of the maleness in the character n-grams for "John" than
    those for "Vishvesh."
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: The next three names, "Sarah," "Carlana," and 'Ruby', are the first names of
    women at the top of my mind when writing this book.^([[19](#_footnotedef_19 "View
    footnote.")]) ^([[20](#_footnotedef_20 "View footnote.")]) The name "Ruby" may
    have some maleness in its character n-grams because a similar name "Rudy" (often
    used for male babies) is only 1 edit away from "Ruby." Oddly the name "Carlana,"
    which contains within it a common male name "Carl," is confidently predicted to
    be a female name.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Skiing down the error slope
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of training in neural networks is to minimize a loss function by finding
    the best parameters (weights) for your model. At each step of the optimization
    loop your algorithm finds the steepest way down the slope. Keep in mind, this
    error slope is not the error for just one particular example from your data set.
    It is minimizing the cost (loss) for the mean of all the errors on all the points
    in a batch of data taken together.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Creating a visualization of this side of the problem can help build a mental
    model of what you’re doing when you adjust the weights of the network as you go.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: In chapter 4 you learned about Root Mean Square Error (RMSE), which is the most
    common cost function for regression problems. If you imagine plotting the error
    as a function of the possible weights, given a specific input and a specific expected
    output, a point exists where that function is closest to zero; that is your *minimum* — the
    spot where your model has the least error.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: This minimum will be the set of weights that gives the optimal output for a
    given training example. You will often see this represented as a three-dimensional
    bowl with two of the axes being a two-dimensional weight vector and the third
    being the error (see figure 5.8). That description is a vast simplification, but
    the concept is the same in higher dimensional spaces (for cases with more than
    two weights).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.6 Convex error curve
  id: totrans-277
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![smooth error](images/smooth_error.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
- en: Similarly, you can graph the error surface as a function of all possible weights
    across all the inputs of a training set. But you need to tweak the error function
    a little. You need something that represents the aggregate error across all inputs
    for a given set of weights. For this example, you’ll use *mean squared error*
    as the *z* axis. Here again, you’ll find a location on the error surface where
    the coordinates at that location are the vector of weights that minimize the average
    error between your predictions and the classification labels in your training
    set. That set of weights will configure your model fit the entire training set
    as well as possible.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1 Off the chair lift, onto the slope - gradient descent and local minima
  id: totrans-280
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: What does this visualization represent? At each epoch, the algorithm is performing
    *gradient descent* in trying to minimize the error. Each time you adjust the weights
    in a direction that will hopefully reduce your error the next time. A convex error
    surface will be great. Stand on the ski slope, look around, find out which way
    is down, and go that way!
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: But you’re not always so lucky as to have such a smooth-shaped bowl; it may
    have some pits and divots scattered about. This situation is what is known as
    a *nonconvex error curve*. And, as in skiing, if these pits are big enough, they
    can suck you in and you might not reach the bottom of the slope.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Again the diagrams represent the weights for two-dimensional input. But the
    concept is the same if you have a 10-dimensional input, or 50, or 1000\. In those
    higher dimensional spaces, visualizing it doesn’t make sense anymore, so you trust
    the math. Once you start using neural networks, visualizing the error surface
    becomes less important. You get the same information from watching (or plotting)
    the error or a related metric over the training time and seeing if it is tending
    toward zero. That will tell you if your network is on the right track or not.
    But these 3D representations are a helpful tool for creating a mental model of
    the process.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: But what about the nonconvex error space? Aren’t those divots and pits a problem?
    Yes, yes they are. Depending on where you randomly start your weights, you could
    end up at radically different weights and the training would stop, as there is
    no other way to go down from this *local minimum* (see Figure 5.9).
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.7 Nonconvex error curve
  id: totrans-285
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![lumpy error](images/lumpy_error.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
- en: And as you get into even higher-dimensional space, the local minima will follow
    you there as well.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '5.3.2 Shaking things up: stochastic gradient descent'
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Up until now, you have been aggregating the error for all the training examples
    and skiing down the steepest route as fast as you can. But training on the entire
    training set one sample at a time is a little nearsighted. It’s like choosing
    the downhill sections of a snow park and ignoring all the jumps. Sometimes a good
    ski jump can help you skip over some rough terrain.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: And if you try to train on the entire dataset at once, you may run out of RAM,
    bogging down your training in SWAP — swapping data back and forth between RAM
    and your much slower persistent disk storage. And this single static error surface
    can have traps. Because you are starting from a random starting point (the initial
    model weights) you could blindly ski downhill into some local minima (divot, hole,
    or cave). You may not know that better options exist for your weight values. And
    your error surface is static. Once you reach a local minimum in the error surface,
    there is no downhill slope to help your model ski out and on down the mountain.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: So to shake things up you want to add some randomization to the process. You
    want to periodically shuffle the order of the training examples that your model
    is learning from. Typically you reshuffle the order of the training examples after
    each pass through your training dataset. Shuffling your data changes the order
    in which your model considers the prediction error for each sample. So it will
    change the path it follows in search of the global minimum (smallest model error
    for that dataset). This shuffling is the "stochastic" part of stochastic gradient
    descent.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: There’s still some room for improving the "gradient" estimation part of gradient
    descent. You can add a little humility to your optimizer so it doesn’t get overconfident
    and blindly follow every new guess all the way to where it thinks the global minimum
    should be. It’s pretty rare that the ski slope where you are is going to point
    in a straight line directly to the ski lodge at the bottom of the mountain. So
    your model goes a short distance in the direction of the downward slope (gradient)
    without going all the way. This way the gradient for each individual sample doesn’t
    lead your model too far astray and your model doesn’t get lost in the woods. You
    can adjust the *learning rate* hyperparameter of the SGD optimizer (stochastic
    gradient descent) to control how confident your model is in each individual sample
    gradient.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: Another training approach is *batch learning*. A batch is a subset of the training
    data, like maybe 0.1%, 1%, 10% or 20% of your dataset. Each batch creates a new
    error surface to experiment with as you ski around searching for the unknown "global"
    error surface minimum. Your training data is just a sample of the examples that
    will occur in the real world. So your model shouldn’t assume that the "global"
    real-world error surface is shaped the same as the error surface for any portion
    of your training data.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: 'And this leads to the best strategy for most NLP problems: *mini-batch learning*.^([[21](#_footnotedef_21
    "View footnote.")]) Geoffrey Hinton found that a batch size of around 16 to 64
    samples was optimal for most neural network training problems.^([[22](#_footnotedef_22
    "View footnote.")]) This is the right size to balance the shakiness of stochastic
    gradient descent, with your desire to make significant progress in the correct
    direction towards the global minimum. And as you move toward the changing local
    minima on this fluctuating surface, with the right data and right hyperparameters,
    you can more easily bumble toward the global minimum. Mini-batch learning is a
    happy medium between *full batch* learning and individual example training. Mini-batch
    learning gives you the benefits of both *stochastic* learning (wandering randomly)
    and *gradient descent* learning (speeding headlong directly down the presumed
    slope).'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: Although the details of how *backpropagation* works are fascinating ^([[23](#_footnotedef_23
    "View footnote.")]), they aren’t trivial, and we won’t explain the details here.
    A good mental image that can help you train your models is to imagine the error
    surface for your problem as the uncharted terrain of some alien planet. Your optimizer
    can only look at the slope of the ground at your feet. It uses that information
    to take a few steps downhill, before checking the slope (gradient) again. It may
    take a long time to explore the planet this way. But a good optimization algorithm
    helps your neural network remember all the good locations on the map and use them
    to guess a new place on the map to explore in search of the global minimum. On
    Earth this lowest point on the planet’s surface is the bottom of the canyon under
    Denman Glacier in Antarctica — 3.5 km below sea level.^([[24](#_footnotedef_24
    "View footnote.")]) A good mini-batch learning strategy will help you find the
    steepest way down the ski slope or glacier (not a pleasant image if you’re scared
    of heights) to the global minimum. Hopefully, you’ll soon find yourself by the
    fire in the ski lodge at the bottom of the mountain or a campfire in an ice cave
    below Denman Glacier.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: See if you can add additional layers to the perceptron you created in this chapter.
    See if the results you get improve as you increase the network complexity. Bigger
    is not always better, especially for small problems.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Test yourself
  id: totrans-297
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What is the simple AI logic "problem" that Rosenblatt’s artificial neurons couldn’t
    solve?
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What minor change to Rosenblatt’s architecture "fixed" perceptrons and ended
    the first "AI Winter"?
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the equivalent of a PyTorch `model.forward()` function in Scikit-Learn
    models?
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What test set accuracy can you achieve with the sex-predicting `LogisticRegression`
    model if you aggregate names across year and region? Don’t forget to stratify
    your test set to avoid cheating.
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 5.5 Summary
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Minimizing a cost function is how machines gradually learn more and more about
    the words.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A backpropagation algorithm is the means by which a network *learns*.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The amount a weight contributes to a model’s error is directly related to the
    amount it needs to updated.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural networks are at their heart optimization engines.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Watch out for pitfalls (local minima) during training by monitoring the gradual
    reduction in error.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[1]](#_footnoteref_1) See analysis by Dario Amodei and Danny Hernandez here
    ( [https://openai.com/blog/ai-and-compute/](ai-and-compute.html))'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '[[2]](#_footnoteref_2) See the lemmatizing FAQ chatbot example in chapter 3
    failed on the question about "overfitting."'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '[[3]](#_footnoteref_3) Wikipedia article about Julie Beth Lovins: [https://en.wikipedia.org/wiki/Julie_Beth_Lovins](wiki.html)'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '[[4]](#_footnoteref_4) [https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html](htmledition.html)'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '[[5]](#_footnoteref_5) [https://proai.org/middle-button-subreddit](proai.org.html)'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '[[6]](#_footnoteref_6) Robin Jia, *Building Robust NLP Systems* ( [https://robinjia.GitHub.io/assets/pdf/robinjia_thesis.pdf](pdf.html))'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '[[7]](#_footnoteref_7) *Not Even Wrong: The Failure of String Theory and the
    Search for Unity in Physical Law* by Peter Woit'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '[[8]](#_footnoteref_8) Lex Fridman interview with Peter Woit ( [https://lexfridman.com/peter-woit/](peter-woit.html))'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '[[9]](#_footnoteref_9) Rosenblatt, Frank (1957), The perceptron—​a perceiving
    and recognizing automaton. Report 85-460-1, Cornell Aeronautical Laboratory.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '[[10]](#_footnoteref_10) [https://en.wikipedia.org/wiki/Universal_approximation_theorem](wiki.html)'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: '[[11]](#_footnoteref_11) The logistic activation function can be used to turn
    a linear regression into a logistic regression: ( [https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic.html](linear_model.html))'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '[[12]](#_footnoteref_12) [https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression](modules.html)'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '[[13]](#_footnoteref_13) McElreath, Richard, and Robert Boyd, *Mathematical
    Models of Social Evolution: A guide for the perplexed*, University of Chicago
    Press, 2008.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '[[14]](#_footnoteref_14) USCDI (US Core Data Interoperability) ISA (Interoperability
    Standards Advisory) article on "Sex (Assigned at Birth)" ( [https://www.healthit.gov/isa/uscdi-data/sex-assigned-birth](uscdi-data.html))'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '[[15]](#_footnoteref_15) from "When I am pinned and wriggling on the wall"
    in "The Love Song of J. Alfred Prufrock" by T. S. Eliot ( [https://www.poetryfoundation.org/poetrymagazine/poems/44212/the-love-song-of-j-alfred-prufrock](44212.html))'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '[[16]](#_footnoteref_16) Overview of Coreference Resolution at The Stanford
    Natural Language Processing Group: ( [https://nlp.stanford.edu/projects/coref.shtml](projects.html))'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '[[17]](#_footnoteref_17) The Perifpheral by William Gibson on wikipedia ( [https://en.wikipedia.org/wiki/The_Peripheral](wiki.html))'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '[[18]](#_footnoteref_18) False dichotomy article on wikipedia ( [https://en.wikipedia.org/wiki/False_dilemma](wiki.html))'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '[[19]](#_footnoteref_19) Sarah Goode Wikipedia article ( [https://en.wikipedia.org/wiki/Sarah_E._Goode](wiki.html))'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '[[20]](#_footnoteref_20) Ruby Bridges Wikipedia article ( [https://en.wikipedia.org/wiki/Ruby_Bridges](wiki.html))'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '[[21]](#_footnoteref_21) "Faster SGD training by minibatch persistency", by
    Fischetti et al ( [https://arxiv.org/pdf/1806.07353.pdf](pdf.html))'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '[[22]](#_footnoteref_22) Neural Networks for Machine Learning - Overview of
    mini-batch gradient descent by Geoffrey Hinton ( [https://www.cs.toronto.edu/~hinton/coursera/lecture6/lec6.pdf](lecture6.html))'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '[[23]](#_footnoteref_23) Wikpedia, [https://en.wikipedia.org/wiki/Backpropagation](wiki.html)'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: '[[24]](#_footnoteref_24) Wikipedia list of places below sea level ( [https://en.wikipedia.org/wiki/List_of_places_on_land_with_elevations_below_sea_level](wiki.html))'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
