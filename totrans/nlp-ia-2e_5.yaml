- en: 5 Word brain (neural networks)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 字脑（神经网络）
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章包括
- en: Building a base layer for your neural networks
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为你的神经网络构建一个基础层
- en: Understanding backpropagation to train neural networks
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解反向传播以训练神经网络
- en: Implementing a basic neural network in Python
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Python 中实现一个基本的神经网络
- en: Implementing a scalable neural network in PyTorch
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 PyTorch 中实现一个可扩展的神经网络
- en: Stacking network layers for better data representation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 堆叠网络层以获得更好的数据表示
- en: Tuning up your neural network for better performance
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整你的神经网络以获得更好的性能
- en: When you read the title of this chapter, "word brain", the neurons in your brain
    started firing, reminding you where you’d heard something like that before. And
    now that you read the word "heard", your neurons might be connecting the words
    in the title to the part of your brain that processes the *sound* of words. And
    maybe, the neurons in your audio cortex are starting to connect the phrase "word
    brain" to common phrases that rhyme with it, such as "bird brain."
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当你读到这一章的标题，“字脑”，你的大脑中的神经元开始激活，提醒你在哪里听过类似的东西。现在你读到“听到”这个词，你的神经元可能正在连接标题中的单词与处理单词*声音*的大脑部分。也许，你听觉皮层中的神经元开始将短语“字脑”连接到与之押韵的常见短语，比如“鸟脑”。
- en: Even if my brain didn’t predict your brain very well, you’re about to build
    a small brain yourself. And the "word brain" you are about to build will be a
    lot better than both of our human brains, at least for some particularly hard
    NLP tasks. You’re going to build a tiny brain that can process a single word and
    predict something about what it means. And a neural net can do this when the word
    it is processing is a person’s name and it doesn’t seem to *mean* anything at
    all to a human.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我的大脑没有很好地预测到你的大脑，你也将要建立一个小脑。你即将构建的“字脑”将比我们人类的大脑好得多，至少对于某些特别困难的自然语言处理任务来说是这样。你将建立一个可以处理单词并预测其意义的微小脑。当神经网络处理的单词是一个人的名字时，它似乎对人类来说*没有意义*。
- en: Don’t worry if all of this talk about brains and predictions and words has you
    confused. You are going to start simple, with just a single artificial neuron,
    built in Python. And you’ll use PyTorch to handle all the complicated math required
    to connect your neuron up to other neurons and create an artificial neural network.
    Once you understand neural networks, you’ll begin to understand *deep learning*,
    and be able to use it in the real world for fun, positive social impact, and …​
    if you insist, profit.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所有关于大脑、预测和单词的谈论让你感到困惑，不要担心。你将从简单开始，只用一个在 Python 中构建的人工神经元。你将使用 PyTorch 处理连接你的神经元到其他神经元并创建人工神经网络所需的所有复杂数学。一旦你理解了神经网络，你就会开始理解*深度学习*，并能够在现实世界中使用它进行有趣的、积极的社会影响，以及...
    如果你坚持的话，利润。
- en: 5.1 Why neural networks?
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 为什么使用神经网络？
- en: 'When you use a deep neural network for machine learning it is called *deep
    learning*. In the past few years, deep learning has smashed through the accuracy
    and intelligence ceiling on many tough NLP problems:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用深度神经网络进行机器学习时，它被称为*深度学习*。在过去几年里，深度学习已经在许多困难的自然语言处理问题上突破了准确性和智能性的天花板：
- en: question answering
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问答
- en: reading comprehension
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阅读理解
- en: summarization
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总结
- en: '*natural language inference* (NLI)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自然语言推理*（NLI）'
- en: 'And recently deep learning (deep neural networks) enabled previously unimaginable
    applications:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，深度学习（深度神经网络）实现了以前难以想象的应用：
- en: long, engaging conversations
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长时间的、引人入胜的对话
- en: companionship
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陪伴
- en: writing software
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写软件
- en: 'That last one, writing software, is particularly interesting, because NLP neural
    networks are being used to write software …​ wait for it …​ for NLP. This means
    that AI and NLP algorithms are getting closer to the day when they will be able
    to self-replicate and self-improve. This has renewed hope and interest in neural
    networks as a path toward *Artificial General Intelligence* (AGI) - or at least
    *more* generally intelligent machines. And NLP is already being used to directly
    generate software that is advancing the intelligence of those NLP algorithms.
    That virtuous cycle is creating models so complex and powerful that humans have
    a hard time understanding them and explaining how they work. An OpenAI article
    shows a clear inflection point in the complexity of models that happened in 2012,
    when Geoffrey Hinton’s improvement to neural network architectures caught on.
    Since 2012, the amount of compute used in the largest AI training runs have been
    increasing exponentially with a 3.4-month doubling time.^([[1](#_footnotedef_1
    "View footnote.")]) Neural networks make all this possible because they:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个，写软件，尤其有趣，因为 NLP 神经网络正在被用来编写软件……等等……用于 NLP。这意味着人工智能和自然语言处理算法正在接近一天，它们将能够自我复制和自我改进。这使得神经网络成为通向
    *人工通用智能*（AGI）的路径，至少是*更*普遍智能机器的一种。NLP 已经被用来直接生成正在推进那些 NLP 算法智能的软件。这种良性循环正在创造出如此复杂和强大的模型，以至于人类很难理解它们，解释它们的工作原理。一篇
    OpenAI 文章显示了模型复杂性在 2012 年发生的明显拐点，当时 Geoffrey Hinton 对神经网络架构的改进开始流行起来。自 2012 年以来，用于最大
    AI 训练运行的计算量呈指数增长，每 3.4 个月翻一倍。^([[1](#_footnotedef_1 "查看脚注。")]) 之所以所有这些成为可能，是因为神经网络：
- en: Are better at generalizing from a few examples
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于从少量示例中进行泛化更好
- en: Can automatically engineer features from raw data
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以自动从原始数据中提取特征
- en: Can be trained easily on any unlabeled text
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以很容易地在任何未标记的文本上进行训练
- en: Neural networks do the feature engineering for you, and they do it optimally.
    They extract generally useful features and representations of your data according
    to whatever problem you set up in your pipeline. And modern neural networks work
    especially well even for information-rich data such as natural language text.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络为你做特征工程，而且做得非常优秀。它们根据你在管道中设置的问题，提取通常有用的特征和数据的表示。现代神经网络在信息丰富的数据，比如自然语言文本方面尤其表现出色。
- en: 5.1.1 Neural networks for words
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.1 单词的神经网络
- en: With neural networks, you don’t have to guess whether the proper nouns or the
    average word length or hand-crafted word sentiment scores are going to be what
    your model needs. You can avoid the temptation to use readability scores, or sentiment
    analyzers to reduce the dimensionality of your data. You don’t even have to squash
    your vectors with blind (unsupervised) dimension reduction approaches such as
    stop word filtering, stemming, lemmatizing, LDA, PCA, TSNE, or clustering. A neural
    network *mini-brain* can do this for you, and it will do it optimally, based on
    the statistics of the relationship between words and your target.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 有了神经网络，你不需要猜测专有名词、平均词长或手工制作的词语情感分数是否是你的模型所需要的。你可以避免使用可读性分数，或情感分析器来降低数据的维度。你甚至不需要使用盲目（无监督）的降维方法，如停用词过滤、词干提取、词形还原、LDA、PCA、TSNE
    或聚类。一个神经网络的 *小型大脑* 可以为你做到这一切，并且它会根据词与你的目标之间的关系统计学来进行优化。
- en: Warning
  id: totrans-28
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 警告
- en: Don’t use stemmers, lemmatizers or other keyword-based preprocessing in your
    deep learning pipeline unless you’re absolutely sure it is helping your model
    perform better for your application.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的深度学习管道中不要使用词干提取器、词形还原器或其他基于关键字的预处理，除非你确信它能够帮助你的模型在你的应用中表现更好。
- en: If you’re doing stemming, lemmatization, or keyword-based analyses you probably
    want to try your pipeline without those filters. It doesn’t matter whether you
    use NLTK, Stanford Core NLP, or even SpaCy, hand-crafted linguistic algorithms
    like lemmatizers are probably not helping. These algorithms are limited by the
    hand-labeled vocabulary and hand-crafted regular expressions that define the algorithm.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在进行词干提取、词形还原或基于关键字的分析，你可能想要尝试在没有这些过滤器的情况下运行你的管道。无论你使用的是 NLTK，Stanford Core
    NLP，还是 SpaCy，手工制作的词汇算法，比如词形还原器，可能没有帮助。这些算法受到手工标记的词汇表和手工制作的正则表达式的限制。
- en: 'Here are some preprocessing algorithms that will likely trip up your neural
    nets:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些预处理算法可能会使你的神经网络遇到困难：
- en: Porter stemmer
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 波特词干提取器
- en: Penn Treebank lemmatizer
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 宾夕法尼亚树库词形还原器
- en: Flesch-Kincaid readability analyzer
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flesch-Kincaid 可读性分析器
- en: VADER sentiment analyzer
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VADER 情感分析器
- en: In the hyperconnected modern world of machine learning and deep learning, natural
    languages evolve too rapidly and these algorithms can’t keep up. Stemmers and
    lemmatizers are overfit to a bygone era. The words "hyperconnected" and "overfit"
    were nonexistent 50 years ago. Lemmatizers, stemmers, and sentiment analyzers
    often do the wrong thing with unanticipated words such as these.^([[2](#_footnotedef_2
    "View footnote.")])
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习和深度学习的超连通现代世界中，自然语言也发展得太快，这些算法跟不上。词干提取器和词形提取器过度拟合于过去的时代。50 年前，"超连通"和 "过度拟合"
    这样的词根本不存在。词形提取器、词干提取器和情感分析器常常对这些未预料到的词做出错误的处理。^([[2](#_footnotedef_2 "View footnote.")])
- en: Deep learning is a game changer for NLP. In the past, brilliant linguists like
    Julie Beth Lovins needed to hand-craft algorithms to extract stems, lemmas, and
    keywords from text.^([[3](#_footnotedef_3 "View footnote.")]) (Her one-pass stemmer
    and lemmatizer algorithms were later made famous by Martin Porter and others)^([[4](#_footnotedef_4
    "View footnote.")]) Deep neural networks now make all that laborious work unnecessary.
    They directly access the meaning of words based on their statistics, without requiring
    brittle algorithms like stemmers and lemmatizers.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是自然语言处理的游戏规则改变者。在过去，像朱莉·贝丝·洛文斯（Julie Beth Lovins）这样的杰出语言学家需要手工制作算法来从文本中提取词干、词形和关键词。^([[3](#_footnotedef_3
    "View footnote.")])（她的单遍词干提取器和词形提取器算法后来被马丁·波特等人所广为人知）^([[4](#_footnotedef_4 "View
    footnote.")]) 深度神经网络现在使所有这些繁琐的工作都变得不必要。它们根据单词的统计信息直接访问单词的含义，而不需要像词干提取器和词形提取器那样脆弱的算法。
- en: Even powerful feature engineering approaches like the Latent Semantic Analysis
    (LSA) of Chapter 4 can’t match the NLU capabilities of neural nets. The automatic
    learning of decision thresholds with decision trees, random forests, and boosted
    trees does’t provide the depth of language understanding of neural nets. Conventional
    machine learning algorithms made full-text search and universally accessible knowledge
    a reality. But deep learning with neural networks makes artificial intelligence
    and intelligent assistants possible. You no longer needed an information retrieval
    expert or librarian to find what you were looking for, you have a virtual librarian
    to assist you. Deep Learning now powers your thinking in ways you wouldn’t have
    imagined a few years ago.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 即使像第 4 章的潜在语义分析（LSA）这样的强大特征工程方法也无法匹敌神经网络的 NLU 能力。通过决策树、随机森林和提升树自动学习决策阈值并不提供神经网络的语言理解深度。传统的机器学习算法使全文检索和普遍可访问的知识成为现实。但是具有神经网络的深度学习使得人工智能和智能助手成为可能。你不再需要信息检索专家或图书馆员来找到你想要的东西，你有一个虚拟的图书管理员来协助你。深度学习现在以你之前无法想象的方式推动着你的思维。
- en: What is it about deep layers of neurons that has propelled NLP to such prominence
    in our lives? Why is it that we are now so dependent on neural machine translation
    (NMT) recommendation engines, middle button suggestions (There’s a subreddit where
    people post comments made entirely of middle button suggestions from their smartphones?^([[5](#_footnotedef_5
    "View footnote.")])), and auto-reply nudges? If you’ve tried digital detox, you
    may have experienced this sensation of not being fully yourself without NLP helping
    you behind the scenes. And NLP neural nets for have given us hope that Artificial
    General Intelligence (AGI) is within reach. They promise to allow machines to
    learn in the same way we often do, by just reading a lot of text.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 深层神经元有什么特点，使得自然语言处理在我们的生活中如此突出？为什么我们现在如此依赖神经机器翻译（NMT）推荐引擎、中间按钮建议（有一个 Subreddit，人们在那里发布完全由他们智能手机的中间按钮建议组成的评论？^([[5](#_footnotedef_5
    "View footnote.")])), 和自动回复提醒？如果你尝试过数字排毒，你可能会感受到没有 NLP 在幕后帮助你，你就不能完全做自己的这种感觉。而
    NLP 神经网络给了我们希望，认为通用人工智能（AGI）已在触手可及范围内。它们承诺允许机器以我们通常的方式学习，只需阅读大量文本。
- en: The power of NLP that you learned to employ in the previous chapters is about
    to get a lot more powerful. You’ll want to understand how deep, layered networks
    of artificial neurons work in order to ensure that your algorithms benefit society
    instead of destroy it. (Stuart Russell’s *Human Compatible AI* explains the dangers
    and promise of AI and AGI, with some insightful NLP examples.) To wield this power
    for good, you need to get a feeling for how neural networks work all the way down
    deep at the individual neuron.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你学习到的应用于NLP的强大力量即将变得更加强大。您需要了解深层次的人工神经元网络如何工作，以确保您的算法造福于社会而不是毁坏它（Stuart Russell的“Human
    Compatible AI”阐释了人工智能（AI）和人工智能通用（AGI）的危险和承诺，具有一些有见地的NLP示例。）。为了运用这种力量造福于人类，您需要对单个神经元的工作有所了解。
- en: You’ll also want to understand *why* they work so well for many NLP problems…​and
    why they fail miserably on others.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要了解*为什么*它们对许多NLP问题如此有效……以及为什么在其他问题上失败得惨不忍睹。
- en: We want to save you from the "AI winter" that discouraged researchers in the
    past. If you employ neural networks incorrectly you could get frost-bitten by
    an overfit NLP pipeline that works well on your test data, but proves disastrous
    in the real world. As you get to understand how neural networks work, you will
    begin to see how you can build more *robust NLP* neural networks. Neural networks
    for NLP problems are notoriously brittle and vulnerable to adversarial attacks
    such as poisoning. (You can learn more about how to measure a model’s robustness
    and improve it from Robin Jia’s PhD thesis.^([[6](#_footnotedef_6 "View footnote.")]))
    But first, you must build an intuition for how a single neuron works.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望帮助您摆脱过去使研究人员感到沮丧的“人工智能冬天”。如果将神经网络使用不当，则可能会被一个过度拟合的 NLP（自然语言处理）流水线冻伤，该流水线在您的测试数据上表现良好，但在实际应用中会造成灾难性后果。随着您对神经网络如何工作的了解加深，您将会开始看到如何构建更*强大的
    NLP*神经网络。 NLP 问题的神经网络因质量脆弱且容易受到敌意攻击，例如污染（可以从 Robin Jia 的博士学位论文中了解有关如何测量模型的稳健性并改进它的更多信息。^([[6](#_footnotedef_6
    "View footnote.")])) 但是，首先你必须对单个神经元的工作有所了解。
- en: Tip
  id: totrans-43
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: Here are two excellent natural language texts about processing natural language
    text with neural networks. You can even use these texts to train a deep learning
    pipeline to understand the terminology of NLP.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 下面列出了两本关于用神经网络处理自然语言文本的优秀自然语言文本。 您甚至可以使用这些文本来训练深度学习流水线以理解NLP的术语。
- en: '*A Primer on Neural Network Models for Natural Language Processing* by Yoav
    Goldberg ([https://archive.is/BNEgK](archive.is.html))'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yoav Goldberg 的 *自然语言处理神经网络模型入门*（[https://archive.is/BNEgK](archive.is.html)）
- en: '*CS224d: Deep Learning for Natural Language Processing* by Richard Socher ([https://web.stanford.edu/class/cs224d/lectures/](lectures.html))'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Richard Socher 的 *CS224d：自然语言处理的深度学习*（[https://web.stanford.edu/class/cs224d/lectures/](lectures.html)）
- en: You might also want to check *Deep learning for Natural Language Processing*
    by Stephan Raaijmakers on Manning.([https://www.manning.com/books/deep-learning-for-natural-language-processing](books.html))
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能还想查看Manning的Stephan Raaijmakers所写的 *深度学习自然语言处理*（[https://www.manning.com/books/deep-learning-for-natural-language-processing](books.html)）
- en: 5.1.2 Neurons as feature engineers
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.2 神经元作为特征工程师
- en: One of the main limitations of linear regression, logistic regression, and naive
    Bayes models is that they all require you to engineer features one by one. You
    must find the best numerical representation of your text among all the possible
    ways to represent text as numbers. Then you have to parameterize a function that
    takes in these engineered feature representations and outputs your predictions.
    Only then can the optimizer start searching for the parameter values that best
    predict the output variable.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归、逻辑回归和朴素贝叶斯模型的主要限制之一是它们都需要逐个进行特征工程。你必须在所有可能的文本数字表示方法中找到最好的数字表示形式。然后，您必须参数化一个函数，该函数接受这些经过工程化的特征表示并输出您的预测。只有在此之后，优化器才能开始搜索最佳预测输出变量的参数值。
- en: Note
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: In some cases, you will want to manually engineer threshold features for your
    NLP pipeline. This can be especially useful if you need an explainable model that
    you can discuss with your team and relate to real-world phenomena. To create a
    simpler model with few engineered features, without neural networks, requires
    you to examine residual plots for each and every feature. When you see a discontinuity
    or nonlinearity in the residuals at a particular value of the feature, that’s
    a good threshold value to add to your pipeline. Sometimes, you can even find an
    association between your engineered thresholds and real-world phenomena.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，您可能希望为您的NLP流水线手动创建阈值特征。如果您需要一个可与团队讨论并与现实现象相关联的可解释模型，这将非常有用。要创建一个具有少量工程特征的简化模型，而不使用神经网络，您需要检查每个特征的残差图。当您在特征的特定值处看到残差中断或非线性时，那就是要添加到您的流水线中的好的阈值值。有时，您甚至可以找到您工程阈值和现实世界现象之间的关联。
- en: For example, the TF-IDF vector representation you used in Chapter 3 works well
    for information retrieval and full-text search. However, TF-IDF vectors often
    don’t generalize well for semantic search or NLU in the real world where words
    are used in ambiguous ways or mispelled. And the PCA or LSA transformation of
    Chapter 4 may not find the right topic vector representation for your particular
    problem. They are good for visualization but not optimal for NLU applications.
    Multi-layer neural networks promise to do this feature engineering for you and
    do it in a way that’s in some sense optimal. Neural networks search a much broader
    space of possible feature engineering functions.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您在第3章中使用的TF-IDF向量表示对信息检索和全文搜索非常有效。但是，在现实世界中，TF-IDF向量通常无法很好地泛化到语义搜索或NLU，因为单词以模糊的方式使用或拼写错误。并且第4章中的PCA或LSA转换可能无法为您的特定问题找到正确的主题向量表示。它们适用于可视化，但对于NLU应用来说并非最佳选择。多层神经网络承诺为您进行特征工程，并以某种意义上的最佳方式执行此操作。神经网络搜索更广泛的可能特征工程函数空间。
- en: Dealing with the polynomial feature explosion
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 处理多项式特征爆炸
- en: Another example of some feature engineering that neural networks can optimize
    for you is polynomial feature extraction. (Think back to the last time you used
    `sklearn.preprocessing.PolynomialFeatures`) During feature engineering, you might
    guess that the relationship between inputs and outputs is quadratic. In that case
    you would square those input features and retrain a model with these new features
    to see if it improved your model’s accuracy on the test set. Basically, if the
    residuals for a particular feature (prediction minus test-set label) do not look
    like white noise centered on zero, then that is an opportunity for you to take
    out some more error from your model’s predictions by transforming that feature
    with some nonlinear function, such as square (`*\*2`), cube (`\*\*3`), `sqrt`,
    `log`, `exp`. Any function you can dream up is fair game. And you will gradually
    develop an intuition that helps you guess the right function that will improve
    your accuracy the most. And if you don’t know which interactions might be critical
    to solving your problem, you have to multiply all your features by each other.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个神经网络可以为您优化的一些特征工程的例子是多项式特征提取（回想一下您上次使用`sklearn.preprocessing.PolynomialFeatures`的情况）。在特征工程期间，您可能会猜想输入和输出之间的关系是二次的。在这种情况下，您会对这些输入特征进行平方，并使用这些新特征重新训练模型，以查看它是否改善了模型在测试集上的准确性。基本上，如果特定特征（预测值减去测试集标签）的残差看起来不像以零为中心的白噪声，那么这就是您利用一些非线性函数来从模型的预测中消除更多误差的机会，比如平方（`*\*2`），立方（`\*\*3`），`sqrt`，`log`，`exp`。您能想到的任何函数都是公平竞争的。您将逐渐培养出一种直觉，帮助您猜测出最能提高准确性的正确函数。如果您不知道哪些相互作用可能对解决问题至关重要，那么您必须将所有特征互相乘起来。
- en: You know the depth and breadth of this rabbit hole. The number of possible fourth-order
    polynomial features is virtually limitless. You might try to reduce the dimensions
    of your TF-IDF vectors from 10s of thousands to 100s of dimensions using PCA or
    LSA. But throwing in fourth-order polynomial features would exponentially expand
    your dimensionality beyond even the dimensionality of TF-IDF vectors.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 您了解这个兔子洞的深度和广度。可能的四阶多项式特征的数量几乎是无限的。您可以尝试使用PCA或LSA将TF-IDF向量的维度从数万个减少到数百个。但是将四次多项式特征加入其中将会使您的维度远远超过TF-IDF向量的维度。
- en: And even with millions of possible polynomial features, there are still millions
    more threshold features. Random forests of decision trees and boosted decision
    trees have advanced to the point that they do a decent job of feature engineering
    automatically. So finding the right threshold features is essentially a solved
    problem. But these feature representations are difficult to explain and sometimes
    don’t generalize well to the real world. This is where neural nets can help.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 即使有数百万种可能的多项式特征，还有数百万个阈值特征。决策树的随机森林和提升决策树已经发展到一定程度，可以自动进行良好的特征工程。因此，找到合适的阈值特征基本上是一个已解决的问题。但是这些特征表示难以解释，有时在现实世界中的泛化效果不佳。这就是神经网络可以发挥作用的地方。
- en: The Holy Grail of feature engineering is finding representations that say something
    about the physics of the real world. If your features are explainable according
    to real-world phenomena, you can begin to build confidence that it is more than
    just predictive. It may be a truly causal model that says something about the
    world that is true in general and not just for your dataset.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程的圣杯是找到能够反映真实世界物理的表示。如果你的特征可以根据真实世界现象来解释，你就可以开始建立对其不仅仅是预测性的信心。它可能是一个真正的因果模型，它对于世界是普遍真实的，而不仅仅是对你的数据集。
- en: Peter Woit explains how the explosion of possible models in modern physics are
    mostly *Not Even Wrong* .^([[7](#_footnotedef_7 "View footnote.")]) These *not
    even wrong* models are what you create when you use `sklearn.preprocessing.PolynomialFeatures`.
    And that is a real problem. Very few of the millions of these extracted polynomial
    features are even physically possible. In other words the vast majority of polynomial
    features are just noise.^([[8](#_footnotedef_8 "View footnote.")]) So if you `PolynomialFeatures`
    in your preprocessing, limit the `degree` parameter to `2` or less.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Peter Woit 解释了现代物理学中可能模型的激增大部分都是*甚至不错*的。[[7]](#_footnotedef_7 "查看脚注") 当你使用 `sklearn.preprocessing.PolynomialFeatures`
    时，这些*甚至不错*的模型就是你所创建的。而这是一个真正的问题。这些提取的多项式特征中很少有数百万是物理上可能的。换句话说，绝大多数多项式特征只是噪音。[[8]](#_footnotedef_8
    "查看脚注") 因此，如果在预处理中使用 `PolynomialFeatures`，请将 `degree` 参数限制为 `2` 或更低。
- en: Important
  id: totrans-59
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 重要提示
- en: For any machine learning pipeline, make sure your polynomial features never
    include the multiplication of more than 2 physical quantities. If you decide to
    try polynomial features with a degree greater than two you can save yourself some
    grief by filtering out unrealizable (fantasy) 3-way interaction features. For
    example `x1 * x2 \** 2` is a legitimate third-degree polynomial feature to try,
    but `x1 * x2 * x3` is not. Polynomial features involving the interaction (multiplication)
    of more than two features together are not physically realizable. Removing these
    "fantasy features" will improve the robustness of your NLP pipeline and help you
    reduce any hallucinations coming out of your generative models.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何机器学习流水线，确保你的多项式特征永远不包括超过 2 个物理量的乘积。如果你决定尝试高于二次的多项式特征，可以通过滤除不可实现（幻想）的三路交互特征来减少困扰。例如，`x1
    * x2 \** 2` 是一个合法的三次多项式特征，但 `x1 * x2 * x3` 不是。涉及超过两个特征之间交互（乘法）的多项式特征在物理上是不可实现的。移除这些“幻想特征”将提高你的
    NLP 流水线的鲁棒性，并帮助你减少生成模型产生的任何幻觉。
- en: We hope that by now you’re inspired by the possibilities that neural networks
    offer. Let’s start our journey into the world of neural networks building single
    neurons that look a lot like logistic regressions. Ultimately, you will be able
    to combine and stack these neurons in layers that optimize the feature engineering
    for you.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望到目前为止你已经受到了神经网络所提供的可能性的启发。让我们开始我们的神经网络之旅，构建类似逻辑回归的单个神经元。最终，你将能够组合和堆叠这些神经元，以优化特征工程。
- en: 5.1.3 Biological neurons
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.3 生物神经元
- en: Frank Rosenblatt came up with the first artificial neural network based on his
    understanding of how biological neurons in our brains work. He called it a perceptron
    because he was using it to help machines perceive their environment using sensor
    data as input.^([[9](#_footnotedef_9 "View footnote.")]) He hoped they would revolutionize
    machine learning by eliminating the need to hand-craft filters to extract features
    from data. He also wanted to automate the process of finding the right combination
    of functions for any problem.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 弗兰克·罗森布拉特根据自己对大脑中生物神经元工作原理的理解提出了第一个基于人工神经网络的模型。他将其称为感知器，因为他使用它来帮助机器利用传感器数据感知其环境。[[9]](#_footnotedef_9)
    他希望这些感知器通过消除手工设计的滤波器从数据中提取特征的需要来革新机器学习。他还希望自动化找到任何问题的正确功能组合的过程。
- en: He wanted to make it possible for engineers to build AI systems without having
    to design specialized models for each problem. At the time, engineers used linear
    regressions, polynomial regressions, logistic regressions and decision trees to
    help robots make decisions. Rosenblatt’s perceptron was a new kind of machine
    learning algorithm that could approximate any function, not just a line, a logistic
    function, or a polynomial.^([[10](#_footnotedef_10 "View footnote.")]) He based
    it on how biological neurons work.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 他希望使工程师能够构建 AI 系统，而无需为每个问题设计专门的模型。当时，工程师们使用线性回归、多项式回归、逻辑回归和决策树来帮助机器人做出决策。罗森布拉特的感知器是一种新型的机器学习算法，它可以近似任何函数，不仅仅是一条线、一个逻辑函数或一个多项式。[[10]](#_footnotedef_10)
- en: Figure 5.1 Biological neuron cell
  id: totrans-65
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.1 生物神经元细胞
- en: '![biological neuron cell](images/biological_neuron_cell.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![生物神经元细胞](images/biological_neuron_cell.png)'
- en: Rosenblatt was building on a long history of successful logistic regression
    models. He was modifying the optimization algorithm slightly to better mimic what
    neuroscientists were learning about how biological neurons adjust their response
    to the environment over time.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 罗森布拉特在成功的逻辑回归模型的漫长历史上进行了改进。他略微修改了优化算法，以更好地模拟神经科学家对生物神经元如何随着时间调整其对环境的响应的理解。
- en: Electrical signals flow into a biological neuron in your brain through the *dendrites*
    (see Figure 5.1) and into the nucleus. The nucleus accumulates electric charge
    and it builds up over time. When the accumulated charge in the nucleus reaches
    the activation level of that particular neuron it *fires* an electrical signal
    out through the *axon*. However, neurons are not all created equal. The dendrites
    of the neuron in your brain are more "sensitive" for some neuron inputs than for
    others. And the nucleus itself may have a higher or lower activation threshold
    depending on its function in the brain. So for some more sensitive neurons it
    takes less of a signal on the inputs to trigger the output signal being sent out
    the axon.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 电信号通过*树突*（见图 5.1）流入你大脑中的生物神经元，然后进入细胞核。细胞核积累电荷并随着时间的推移逐渐增加。当细胞核中积累的电荷达到特定神经元的激活水平时，它通过*轴突*发出电信号。然而，神经元并非完全相同。你大脑中的神经元的树突对某些输入的神经元更“敏感”而对其他输入的神经元则不那么“敏感”。细胞核本身可能具有较高或较低的激活阈值，这取决于其在大脑中的功能。因此，对于一些更敏感的神经元，输入的信号量较少即可触发通过轴突发送输出信号。
- en: So you can imagine how neuroscientists might measure the sensitivity of individual
    dendrites and neurons with experiments on real neurons. And this sensitivity can
    be given a numerical value. Rosenblatt’s perceptron abstracts this biological
    neuron to create an artificial neuron with a *weight* associated with each input
    (dendrite). For artificial neurons, such as Rosenblatt’s perceptron, we represent
    the sensitivity of individual dendrites as a numerical *weight* or *gain* for
    that particular path. A biological cell *weights* incoming signals when deciding
    when to fire. A higher weight represents a higher sensitivity to small changes
    in the input.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你可以想象神经科学家如何通过对真实神经元进行实验来测量单个树突和神经元的敏感性。这种敏感性可以被赋予数值。罗森布拉特的感知器将这种生物神经元抽象化，创建了一个与每个输入（树突）相关联的*权重*的人工神经元。对于人工神经元，例如罗森布拉特的感知器，我们将单个树突的敏感性表示为该特定路径的数值*权重*或*增益*。生物细胞在决定何时触发时加权输入信号。更高的权重代表对输入的小变化更敏感。
- en: A biological neuron will dynamically change those weights in the decision-making
    process over the course of its life. You are going to mimic that biological learning
    process using the machine learning process called *back propagation*.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 生物神经元会在其生命周期中动态地改变这些权重，在决策过程中。您将使用称为*反向传播*的机器学习过程来模仿这种生物学习过程。
- en: Figure 5.2 Basic perceptron
  id: totrans-71
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.2 基本感知器
- en: '![perceptron](images/perceptron.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![perceptron](images/perceptron.png)'
- en: AI researchers hoped to replace the rigid math of logistic regressions and linear
    regressions and polynomial feature extraction with the more fuzzy and generalized
    logic of neural networks — tiny brains. Rosenblatt’s artificial neurons even worked
    for trigonometric functions and other highly nonlinear functions. Each neuron
    solved one part of the problem and could be combined with other neurons to learn
    more and more complex functions. (Though not all of them - even simple functions,
    like an XOR gate can’t be solved with a single layer perceptron). He called this
    collection of artificial neurons a perceptron.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: AI 研究人员希望用更模糊、更泛化的神经网络逻辑来取代 logistic 回归、线性回归和多项式特征提取的严格数学 —— 小型大脑。Rosenblatt
    的人工神经元甚至可以处理三角函数和其他高度非线性函数。每个神经元解决问题的一部分，并且可以与其他神经元结合来学习更加复杂的函数。（尽管并非所有的都行 - 即使是简单的函数，比如
    XOR 门，也不能用单层感知器解决）。他把这些人工神经元的集合称为感知器。
- en: Rosenblatt didn’t realize it at the time, but his artificial neurons could be
    layered up just as biological neurons connect to each other in clusters. In modern
    *deep learning* we connect the predictions coming out of one group of neurons
    to another collection of neurons to refine the predictions. This allows us to
    create layered networks that can model *any* function. They can now solve any
    machine learning problem …​ if you have enough time and data.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 'Rosenblatt 当时并没有意识到，但他的人工神经元可以像生物神经元一样被层叠起来，连接成簇。在现代* 深度学习* 中，我们将一个组神经元的预测连接到另一个组神经元以细化预测。这使我们能够创建层叠网络，可以模拟*任何*函数。如果您有足够的时间和数据，它们现在可以解决任何机器学习问题……。 '
- en: Figure 5.3 Neural network layers
  id: totrans-75
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.3 神经网络层
- en: '![multilayer perceptron](images/multilayer-perceptron.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![multilayer perceptron](images/multilayer-perceptron.png)'
- en: 5.1.4 Perceptron
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.4 感知器
- en: One of the most complex things neurons do is process language. Think about how
    a perceptron might be used to process natural language text. Does the math shown
    in Figure 5.2 remind you of any of the machine learning models you’ve used before?
    What machine learning models do you know of that multiply the input features by
    a vector of weights or coefficients? Well, that would be a linear regression.
    But what if you used a sigmoid activation function or logistic function on the
    output of a linear regression? It’s starting to look a lot like a *logistic regression*
    to me.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元处理的最复杂的事情之一就是语言处理。想象一下感知器如何用于处理自然语言文本。图 5.2 中显示的数学内容让您想起了以前使用过的任何机器学习模型吗？您知道哪些机器学习模型将输入特征与权重或系数向量相乘吗？好吧，那将是一个线性回归。但是如果您在线性回归的输出上使用
    sigmoid 激活函数或 logistic 函数呢？对我来说，它开始看起来很像* logistic 回归*。
- en: The sigmoid *activation function* used in a perceptron is actually the same
    as the logistic function used within logistic regression. Sigmoid just means s-shaped.
    And the logistic function has exactly the shape we want for creating a soft threshold
    or logical binary output. So really what your neuron is doing here is equivalent
    to a logistic regression on the inputs.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在感知器中使用的 sigmoid *激活函数*实际上与 logistic 回归中使用的逻辑函数相同。Sigmoid 只是表示 s 形状。逻辑函数正好具有我们用于创建软阈值或逻辑二进制输出的形状。因此，你的神经元在这里实际上相当于对输入进行
    logistic 回归。
- en: This is the formula for a logistic function implemented in Python.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 Python 中实现的逻辑函数的公式。
- en: '[PRE0]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: And here is what a logistic function looks like, and how the coefficient (weight)
    and phase (intercept) affect its shape.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这是逻辑函数的样子，以及系数（权重）和相位（截距）如何影响其形状。
- en: '[PRE1]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: What were your inputs when you did a logistic regression on natural language
    sentences in earlier chapters? You first processed the text with a keyword detector,
    `CountVectorizer`, or `TfidfVectorizer`. These models use a tokenizer, like the
    ones you learned about in chapter 2 to split the text into individual words, and
    then count them up. So for NLP it’s common to use the BOW counts or the TF-IDF
    vector as the input to an NLP model, and that’s true for neural networks as well.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，当你对自然语言句子进行逻辑回归时，你的输入是什么？你首先用关键词检测器、`CountVectorizer` 或 `TfidfVectorizer`
    处理文本。这些模型使用分词器，就像你在第 2 章中学到的那些，将文本分割成单个词，并对其进行计数。因此，对于自然语言处理，使用 BOW 计数或 TF-IDF
    向量作为 NLP 模型的输入是很常见的，这对神经网络也是如此。
- en: Each of Rosenblatt’s input weights (biological dendrites) had an adjustable
    value for the weight or sensitivity of that signal. Rosenblatt implemented this
    weight with a potentiometer, like a volume knob on an old-fashioned stereo receiver.
    This allowed researchers to manually adjust the sensitivity of their neuron to
    each of its inputs individually. A perceptron can be made more or less sensitive
    to the counts of each word in the BOW or TF-IDF vector by adjusting this sensitivity
    knob.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Rosenblatt 的每个输入权重（生物树突）都有一个用于该信号的权重或敏感性的可调值。Rosenblatt 使用电位计实现了这个权重，就像老式立体声接收机上的音量旋钮一样。这使得研究人员能够手动调整神经元对其每个输入的敏感性。通过调整这个敏感性旋钮，感知器可以对
    BOW 或 TF-IDF 向量中每个词的计数更敏感或不太敏感。
- en: Once the signal for a particular word was increased or decreased according to
    the sensitivity or weight it passed into the main body of the biological neuron
    cell. It’s here in the body of the perceptron, and also in a real biological neuron,
    where the input signals are added together. Then that signal is passed through
    a soft thresholding function like a sigmoid before sending the signal out the
    axon. A biological neuron will only *fire* if the signal is above some threshold.
    The sigmoid function in a perceptron just makes it easy to implement that threshold
    at 50% of the min-max range. If a neuron doesn’t fire for a given combination
    of words or input signals, that means it was a negative classification match.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦某个特定词的信号根据其敏感性或权重增加或减少，它就会传入生物神经元细胞的主体。就在这里，在感知器的主体部分，以及在真正的生物神经元中，输入信号被加在一起。然后，该信号通过类似于
    S 型函数的软阈值函数传递，然后将信号发送到轴突。如果信号高于某个阈值，生物神经元只会 *激活*。感知器中的 S 型函数只是使得在最小-最大范围的 50%
    处实现该阈值变得容易。如果对于给定的词组或输入信号，神经元不激活，这意味着它是一个负分类匹配。
- en: 5.1.5 A Python perceptron
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.5 一个 Python 感知器
- en: So a machine can simulate a really simple neuron by multiplying numerical features
    by "weights" and combining them together to create a prediction or make a decision.
    These numerical features represent your object as a numerical vector that the
    machine can "understand". For the home price prediction problem of Zillow’s zestimate,
    how do you think they might build an NLP-only model to predict home prices? But
    how do you represent the natural language description of a house as a vector of
    numbers so that you can predict its price? You could take a verbal description
    of the house and use the counts of each word as a feature, just as you did in
    Chapters 2 and 3\. Or you could use a transformation like PCA to compress these
    thousands of dimensions into topic vectors, as you did with PCA in Chapter 4.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，机器可以通过将数字特征与 "权重" 相乘，并将它们组合在一起来模拟一个非常简单的神经元，以创建预测或做出决策。这些数值特征代表你的对象作为机器可以
    "理解" 的数值向量。对于 Zillow 的 zestimate 的房价预测问题，你认为他们可能如何构建一个仅基于 NLP 的模型来预测房价？但是，你如何将房屋的自然语言描述表示为数字向量，以便你可以预测它的价格呢？你可以采用房屋的口头描述，并将每个词的计数作为特征，就像你在第
    2 章和第 3 章中所做的那样。或者，你可以像在第 4 章中使用 PCA 那样，将这些成千上万的维度压缩成主题向量。
- en: But these approaches are just a guess at which features are important, based
    on the variability or variance of each feature. Perhaps the key words in the description
    are the numerical values for the square footage and number of bedrooms in the
    home. Your word vectors and topic vectors would miss these numerical values entirely.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 但这些方法只是根据每个特征的可变性或方差猜测哪些特征是重要的。也许描述中的关键词是房屋面积和卧室数量的数值。你的词向量和主题向量会完全忽略这些数值。
- en: In "normal" machine learning problems, like predicting home prices, you might
    have structured numerical data. You will usually have a table with all the important
    features listed, such as square footage, last sold price, number of bedrooms,
    and even latitude and longitude or zip code. For natural language problems, however,
    we want your model to be able to work with unstructured data, text. Your model
    has to figure out exactly which words and in what combination or sequence are
    predictive of your target variable. Your model must read the home description,
    and, like a human brain, make a guess at the home price. And a neural network
    is the closest thing you have to a machine that can mimic some of your human intuition.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在"正常"的机器学习问题中，比如预测房价，您可能会有结构化的数值数据。通常会有一张列出所有重要特征的表格，比如房屋面积、上次售价、卧室数量，甚至纬度和经度或邮政编码。但是对于自然语言问题，我们希望您的模型能够处理非结构化数据，即文本。您的模型必须准确地找出哪些单词以及以何种组合或顺序对目标变量具有预测能力。您的模型必须阅读房屋描述，并像人脑一样猜测房价。而神经网络是您能找到的最接近模仿人类直觉的机器。
- en: The beauty of deep learning is that you can use as your input every possible
    feature you can dream up. This means you can input the entire text description
    and have your transformer produce a high-dimensional TF-IDF vector and a neural
    network can handle it just fine. You can even go higher dimensional than that.
    You can pass it the raw, unfiltered text as 1-hot encoded sequences of words.
    Do you remember the piano roll we talked about in Chapter 2? Neural networks are
    made for these kinds of raw representations of natural language data.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的美妙之处在于您可以使用您可以想象到的每一个可能的特征作为输入。这意味着您可以输入整个文本描述，并且您的转换器可以生成一个高维的 TF-IDF
    向量，而神经网络可以很好地处理它。您甚至可以更高维度地使用它。您可以将原始、未经过滤的文本作为单词的 1-hot 编码序列传递。您还记得我们在第2章谈到的钢琴卷吗？神经网络就是为这种原始的自然语言数据表示而生的。
- en: Shallow learning
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 浅层学习
- en: For your first deep learning NLP problem, you will keep it shallow. To understand
    the magic of deep learning it helps to see how a single neuron works. A single
    neuron will find a *weight* for each feature you input into the model. You can
    think of these weights as a percentage of the signal that is let into the neuron.
    If you’re familiar with linear regression, then you probably recognize these diagrams
    and can see that the weights are just the slopes of a linear regression. And if
    you throw in a logistic function, these weights are the coefficients that a logistic
    regression learns as you give it examples from your dataset. To put it in different
    words, the weights for the inputs to a single neuron are mathematically equivalent
    to the slopes in a multivariate linear regression or logistic regression.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对于您的第一个深度学习自然语言处理问题，您将保持表面。要理解深度学习的魔力，看看单个神经元如何工作会有所帮助。单个神经元将为输入模型的每个特征找到一个*权重*。您可以将这些权重视为输入神经元的信号的百分比。如果您熟悉线性回归，那么您可能会认出这些图表，并且可以看到权重只是线性回归的斜率。如果你加上了一个
    logistic函数，这些权重就是逻辑回归从您的数据集中学到的系数。换句话说，单个神经元的输入权重在数学上等同于多元线性回归或逻辑回归中的斜率。
- en: Tip
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: Just as with the Scikit-Learn machine learning models, the individual features
    are denoted as `x[i]` or in Python as `x[i]`. The *i* is an indexing integer denoting
    the position within the input vector. And the collection of all features for a
    given example are within the vector **x**.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 就像Scikit-Learn机器学习模型一样，各个特征被表示为`x[i]`或在Python中表示为`x[i]`。*i*表示输入向量中的位置。给定示例的所有特征的集合都在向量**x**中。
- en: '`x = x[1], x[2], …​, x[i], …​, x[n]`'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`x = x[1], x[2], …​, x[i], …​, x[n]`'
- en: And similarly, you’ll see the associate weights for each feature as w[i], where
    *i* corresponds to the integer in x. The weights are generally represented as
    a vector **W**
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，您将看到每个特征的关联权重为w[i]，其中*i*对应于x中的整数。权重通常表示为向量**W**
- en: '`w = w[1], w[2], …​, w[i], …​, w[n]`'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`w = w[1], w[2], …​, w[i], …​, w[n]`'
- en: With the features in hand, you just multiply each feature (x[i]) by the corresponding
    weight (w[i]) and then sum up.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 有了特征之后，您只需将每个特征（x[i]）与相应的权重（w[i]）相乘，然后求和。
- en: '`y = (x[1] * w[1]) + (x[2] * w[2]) + …​ + (x[i] * w[i])`'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`y = (x[1] * w[1]) + (x[2] * w[2]) + …​ + (x[i] * w[i])`'
- en: 'Here’s a fun, simple example to make sure you understand this math. Imagine
    an input BOW vector for a phrase like "green egg egg ham ham ham spam spam spam
    spam":'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个有趣的简单示例，以确保您理解这个数学。想象一个由短语"green egg egg ham ham ham spam spam spam spam"构成的
    BOW 向量：
- en: '[PRE2]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: So this 4-input, 1-output, single-neuron network outputs a value of -0.76 for
    these random weights in a neuron that hasn’t yet been trained.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 4 输入、1 输出、单神经元网络在一个尚未经过训练的神经元中针对这些随机权重输出了一个值为 -0.76。
- en: There’s one more piece you’re missing here. You need to run a nonlinear function
    on the output (`y`) to change the shape of the output so it’s not just a linear
    regression. Often a thresholding or clipping function is used to decide whether
    the neuron should fire or not. For a thresholding function, if the weighted sum
    is above a certain threshold, the perceptron outputs 1\. Otherwise, it outputs
    0\. You can represent this threshold with a simple *step function* (labeled "Activation
    Function" in Figure 5.2).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这里还有一件你错过的事情。你需要在输出（`y`）上运行一个非线性函数，以改变输出的形状，使其不仅仅是线性回归。通常使用阈值或截断函数来决定神经元是否应该触发。对于阈值函数，如果加权和超过某个阈值，感知器输出
    1。否则，输出 0。你可以用一个简单的*步骤函数*（在图 5.2 中标记为"激活函数"）来表示这个阈值。
- en: 'Here’s the code to apply a step function or thresholding function to the output
    of your neuron:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这是将步骤函数或阈值函数应用于你的神经元输出的代码：
- en: '[PRE6]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: And if you want your model to output a continuous probability or likelihood
    rather than a binary `0` or `1`, you probably want to use the logistic activation
    function that we introduced earlier in this chapter.^([[11](#_footnotedef_11 "View
    footnote.")])
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望你的模型输出连续的概率或可能性，而不是二进制的`0`或`1`，你可能希望使用我们在本章前面介绍的逻辑激活函数。^([[11](#_footnotedef_11
    "查看脚注。")])
- en: '[PRE7]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: A neural network works like any other machine learning model — you present it
    with numerical examples of inputs (feature vectors) and outputs (predictions)
    for your model. And like a conventional logistic regression, the neural network
    will use trial and error to find the weights on your inputs that create the best
    predictions. Your *loss function* will measure how much error your model has.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络就像任何其他的机器学习模型一样——你向其提供输入（特征向量）和输出（预测）的数字示例来训练它。就像传统的逻辑回归一样，神经网络将使用试错法来找到在输入上的权重，从而产生最佳预测。你的*损失函数*将衡量模型的误差有多大。
- en: Make sure this Python implementation of the math in a neuron makes sense to
    you. Keep in mind, that the code we’ve written is only for the *feed forward*
    path of a neuron. The math is very similar to what you would see in the `LogisticRegression.predict()`
    function in Scikit-Learn for a 4-input, 1-output logistic regression.^([[12](#_footnotedef_12
    "View footnote.")])
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你能理解这个神经元中的数学在 Python 实现中是否合理。请记住，我们编写的代码仅用于神经元的*前向传播*路径。这个数学在 Scikit-Learn
    中的`LogisticRegression.predict()`函数中的逻辑回归的 4 输入、1 输出中是非常相似的。^([[12](#_footnotedef_12
    "查看脚注。")])
- en: Note
  id: totrans-114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: A *loss function* is a function that outputs a score to measure how bad your
    model is, the total error of its predictions. An *objective function* just measures
    how good your model is based on how small the error is. A *loss function* is like
    the percentage of questions a student got wrong on a test. An *objective function*
    is like the grade or percent score on that test. You can use either one to help
    you learn the right answers and get better and better on your tests.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 一个*损失函数*是一个输出评分的函数，用于衡量模型的糟糕程度，即其预测的总误差。一个*目标函数*仅根据误差的大小来衡量模型的好坏。*损失函数*就像学生在测试中答错的问题的百分比。*目标函数*就像那个测试的等级或百分比分数。你可以使用任何一个来帮助你学会正确答案，并在你的测试中变得越来越好。
- en: Why the extra weight?
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么额外的权重？
- en: Did you notice that you have one additional weight, `w0`? There is no input
    labeled `x0`. So why is there a `w0`? Can you guess why we always give our neural
    neurons an input signal with a constant value of "1.0" for `x0`? Think back to
    the linear and logistic regression models you have built in the past. Do you remember
    the extra coefficient in the single-variable linear regression formula?
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 你有没有注意到你有一个额外的权重，`w0`？没有标记为`x0`的输入。那么为什么会有一个`w0`？你能猜到为什么我们总是给我们的神经元一个常数值为"1.0"的输入信号作为`x0`吗？回想一下你过去构建的线性回归和逻辑回归模型。你还记得单变量线性回归公式中的额外系数吗？
- en: '[PRE8]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `y` variable is for the output or predictions from the model. The `x` variable
    is for the single independent feature variable in this model. And you probably
    remember that `m` represents the slope. But do you remember what `b` is for?
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`y`变量是模型的输出或预测。`x`变量是这个模型中的单个独立特征变量。你可能还记得`m`代表斜率。但你还记得`b`是干什么的吗？'
- en: '[PRE9]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now can you guess what the extra weight `w[0]` is for, and why we always make
    sure it isn’t affected by the input (multiply it by an input of 1.0)?
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你能猜到额外的权重`w[0]`是用来干什么的，为什么我们总是确保它不受输入的影响（将其乘以一个1.0的输入）吗？
- en: '[PRE10]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: It’s the *intercept* from your linear regression, just "rebranded" as the *bias*
    weight (`w0`) for this layer of a neural network.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这是你线性回归的*截距*，只是作为神经网络这一层的*偏差*权重(`w0`)重新包装了一下。
- en: Figure 5.2 and this example reference *bias*. What is this? The bias is an "always
    on" input to the neuron. The neuron has a weight dedicated to it just as with
    every other element of the input, and that weight is trained along with the others
    in the exact same way. This is represented in two ways in the various literature
    around neural networks. You may see the input represented as the base input vector,
    say of *n*-elements, with a 1 appended to the beginning or the end of the vector,
    giving you an *n*+1 dimensional vector. The position of the one is irrelevant
    to the network, as long as it is consistent across all of your samples. Other
    times people presume the existence of the bias term and leave it off the input
    in a diagram, but the weight associated with it exists separately and is always
    multiplied by one and added to the dot product of the sample input’s values and
    their associated weights. Both are effectively the same.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 和这个例子提到了*偏差*。这是什么？偏差是神经元的一个“始终存在”的输入。神经元有一个专门的权重与它相关联，就像输入的每个其他元素一样，并且该权重与其他权重一样被训练。这在围绕神经网络的各种文献中以两种方式表示。你可能会看到输入表示为基本输入向量，比如*n*个元素，其前面或后面附加了一个1，从而给出一个*n*+1维向量。其中一个的位置对网络来说是无关紧要的，只要它在所有样本中保持一致即可。其他时候，人们假设偏差项的存在并将其从图表中省略，但与之相关联的权重存在并且总是乘以1并添加到样本输入的值及其相关权重的点积中。两者实际上是一样的。
- en: The reason for having the bias weight at all is that you need the neuron to
    be resilient to inputs of all zeros. It may be the case that the network needs
    to learn to output 0 in the face of inputs of 0, but it may not. Without the bias
    term, the neuron would output 0 * weight = 0 for any weights you started with
    or tried to learn. With the bias term, you wouldn’t have the problem. And in case
    the neuron needs to learn to output 0, the neuron can learn to decrement the weight
    associated with the bias term enough to keep the dot product below the threshold.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 之所以要有偏差权重，是因为你需要神经元对所有零输入都具有弹性。也许网络需要学习在面对零输入时输出0，但也可能不需要。如果没有偏差项，神经元将对任何你开始或尝试学习的权重都输出0
    * 权重 = 0。有了偏差项，你就不会遇到这个问题。并且如果神经元需要学会输出0，那么神经元可以学会递减与偏差项相关联的权重，以使点积保持在阈值以下。
- en: Figure 5.3 is a rather neat visualization of the analogy between some of the
    signals within a biological neuron in your brain and the signals of an artificial
    neuron used for deep learning. If you want to get deep, think about how you are
    using a biological neuron to read this book about natural language processing
    to learn about deep learning.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 是对比较生物神经元内部信号与用于深度学习的人工神经元信号之间类比的一种相当简洁的可视化。如果你想深入理解，想一想你是如何使用生物神经元来阅读这本关于自然语言处理的书来学习深度学习。
- en: Figure 5.4 A perceptron and a biological neuron
  id: totrans-127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.4 感知器和生物神经元
- en: '![artificial neuron vs biological](images/artificial_neuron_vs_biological.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![人工神经元 vs 生物神经元](images/artificial_neuron_vs_biological.png)'
- en: 'The Python for the simplest possible single neuron looks like this:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的单个神经元的 Python 代码如下：
- en: '[PRE11]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Perhaps you are more comfortable with numpy and *vectorized* mathematical operations
    like you learned about in linear algebra class.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 或许你更熟悉 numpy 和*向量化*的数学操作，就像你在线性代数课上学到的那样。
- en: '[PRE12]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note
  id: totrans-133
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Any Python conditional expression will evaluate to a `True` or `False` boolean
    value. If you use that `bool` type in a mathematical operation such as addition
    or multiplication, Python will *coerce* a `True` value into a numerical `int`
    or `float` value of `1` or `1.0`. A `False` value is coerced into a `1` or `0`
    when you multiply a Boolean by, or add it to another number.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 任何 Python 条件表达式都会评估为一个`True`或`False`布尔值。如果你在数学运算中使用`bool`类型，比如加法或乘法，Python 将会将`True`值强制转换为数值型`int`或`float`值，即`1`或`1.0`。当你将布尔值与另一个数字相乘或相加时，`False`值会被强制转换为`1`或`0`。
- en: The `w` variable contains the vector of weight parameters for the model. These
    are the values that will be learned as the neuron’s outputs are compared to the
    desired outputs during training. The `x` variable contains the vector of signal
    values coming into the neuron. This is the feature vector, such as a TF-IDF vector
    for a natural language model. For a biological neuron, the inputs are the rate
    of electrical pulses rippling through the dendrites. The input to one neuron is
    often the output from another neuron.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 变量`w`包含模型的权重参数向量。这些值在训练期间，当神经元的输出与期望输出进行比较时，将会被学习。变量`x`包含进入神经元的信号值向量。这是特征向量，比如自然语言模型的TF-IDF向量。对于生物神经元，输入是通过树突传播的电脉冲的速率。一个神经元的输入通常是来自另一个神经元的输出。
- en: Tip
  id: totrans-136
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: 'The sum of the pairwise multiplications of the inputs (`x`) and the weights
    (`w`) is exactly the same as the dot product of the two vectors `x` and `y`. If
    you use numpy, a neuron can be implemented with a single brief Python expression:
    `w.dot(x) > 0`. This is why *linear algebra* is so useful for neural networks.
    Neural networks are mostly just dot products of parameters by inputs. And GPUs
    are computer processing chips designed to do all the multiplications and additions
    of these dot products in parallel, one operation on each GPU core. So a 1-core
    GPU can often perform a dot product 250 times faster than a 4-core CPU.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 输入（`x`）和权重（`w`）的成对乘积之和恰好等于两个向量`x`和`y`的点积。如果你使用numpy，一个神经元可以用一个简短的Python表达式实现：`w.dot(x)
    > 0`。这就是为什么*线性代数*对于神经网络非常有用的原因。神经网络主要是参数和输入的点积。而GPU是专门设计用于并行执行所有这些点积的乘法和加法的计算处理芯片，每个GPU核心上执行一次操作。因此，一个1核心GPU通常比一个4核心CPU执行点积快250倍。
- en: 'If you are familiar with the natural language of mathematics, you might prefer
    the summation notation:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你熟悉数学的自然语言，你可能更喜欢使用总和符号表示：
- en: '**Equation 5.1: Threshold activation function**'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**方程 5.1：阈值激活函数**'
- en: '![equation 5 1](images/equation_5_1.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![方程 5 1](images/equation_5_1.png)'
- en: Your perceptron hasn’t *learned* anything just yet. But you have achieved something
    quite important. You’ve passed data into a model and received an output. That
    output is likely wrong, given you said nothing about where the weight values come
    from. But this is where things will get interesting.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 你的感知器还没有*学习*任何东西。但你已经实现了一些非常重要的东西。你已经将数据传递到模型中并收到了一个输出。考虑到你没有提到权重值来自哪里，这个输出很可能是错误的。但这就是事情变得有趣的地方。
- en: Tip
  id: totrans-142
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: The base unit of any neural network is the neuron. The basic perceptron is a
    special case of the more generalized neuron. We refer to the perceptron as a neuron
    for now and come back to the terminology when it no longer applies.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 任何神经网络的基本单位都是神经元。基本感知器是更广义的神经元的特殊情况。我们暂时将感知器称为神经元，并在不适用时再回到术语。
- en: 5.2 Example logistic neuron
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 示例逻辑神经元
- en: It turns out you are already familiar with a very common kind of perceptron
    or neuron. When you use the logistic function for the *activation function* on
    a neuron, you’ve essentially created a logistic regression model. A single neuron
    with the logistic function for its activation function is mathematically equivalent
    to the `LogisticRegression` model in Scikit-Learn. The only difference is how
    they’re trained. So you are going to first train a logistic regression model and
    compare it to a single-neuron neural network trained on the same data.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，你已经对一种非常常见的感知器或神经元很熟悉了。当你在神经元上使用逻辑函数作为*激活函数*时，你实际上创建了一个逻辑回归模型。一个使用逻辑函数作为其激活函数的单个神经元，在数学上等价于Scikit-Learn中的`LogisticRegression`模型。唯一的区别在于它们的训练方式。所以你首先要训练一个逻辑回归模型，并将其与在相同数据上训练的单个神经元神经网络进行比较。
- en: 5.2.1 The logistics of clickbait
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.1 点击诱饵的物流
- en: Software (and humans) often need to make decisions based on logical criteria.
    For example, many times a day you probably have to decide whether to click on
    a particular link or title. Sometimes those links lead you to a fake news article.
    So your brain learns some logical rules that it follows before clicking on a particular
    link.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 软件（和人类）通常需要根据逻辑标准做出决定。例如，你可能每天都要决定是否点击特定的链接或标题。有时这些链接会引导你进入一篇虚假新闻文章。因此，你的大脑学会了一些逻辑规则，它在点击特定链接之前遵循这些规则。
- en: Is it a topic you’re interested in?
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是你感兴趣的话题吗？
- en: Does the link look promotional or spammy?
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 链接看起来是宣传性的还是垃圾邮件？
- en: Is it from a reputable source that you like?
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它来自你喜欢的可靠来源吗？
- en: Does it look true or factual?
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它看起来是真实的还是事实的？
- en: Each one of these decisions could be modeled in an artificial neuron within
    a machine. And you could use that model to create a logic gate in a circuit board
    or a conditional expression (`if` statement) in software. If you did this with
    artificial neurons, the smallest artificial "brain" you could build to handle
    these 4 decisions would use 4 logistic regression gates.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这些决定中的每一个都可以在机器内的人工神经元中建模。你可以使用该模型创建电路板上的逻辑门或软件中的条件表达式（`if`语句）。如果你使用人工神经元进行这些操作，那么用来处理这4个决定的最小人工“大脑”将使用4个逻辑回归门。
- en: To mimic your brain’s *clickbait* filter you might decide to train a logistic
    regression model on the length of the headline. Perhaps you have a hunch that
    longer headlines are more likely to be sensational and exaggerated. Here’s a scatter
    plot of fake and authentic news headlines and their headline length in characters.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 想要模仿你大脑中的*点击率*过滤器，你可能会决定对新闻标题的长度进行逻辑回归模型训练。也许你有一种直觉，认为更长的标题更有可能是耸人听闻和夸张的。下面是假新闻和真实新闻标题以及它们的字符长度的散点图。
- en: The neuron input weight is equivalent to the maximum slope in the middle of
    the logistic regression plot in Figure 5.3 for a fake news classifier with a single
    feature, title length.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在图5.3的逻辑回归图中，对于具有单个特征（标题长度）的假新闻分类器，神经元输入权重等于中间最大斜率。
- en: Figure 5.5 Logistic regression - fakeness vs title length
  id: totrans-155
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.5 逻辑回归 - 虚假与标题长度
- en: '![fake news title len logistic regression](images/fake_news_title_len_logistic_regression.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![假新闻标题长度逻辑回归](images/fake_news_title_len_logistic_regression.png)'
- en: 5.2.2 Sex education
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.2 性教育
- en: How’s that for clickbait? Because the fake news (clickbait) dataset has been
    fully exploited on Kaggle, you’re going to switch to a more fun and useful dataset.
    You’re going to predict the sex of a name with perceptrons (artificial neurons).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 点击率怎么样？因为假新闻（点击率）数据集在Kaggle上已经充分利用，所以你将转而使用一个更有趣和有用的数据集。你将使用感知器（人工神经元）来预测一个名字的性别。
- en: The problem you’re going to solve with this simple architecture is an everyday
    NLU problem that your brain’s millions of neurons try to solve every day. Your
    brain is strongly incentivized to identify the birth sex of the people you interact
    with on social media. (If you’re interested in why this is, Richard McElreath
    and Robert Boyd have a fascinating book on the subject.^([[13](#_footnotedef_13
    "View footnote.")])) A single artificial neuron can solve this challenge with
    about 80% accuracy using only the characters in the first name of a person. You’re
    going to use a sample of names from a database of 317 million birth certificates
    across US states and territories over more than 100 years.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 你要用这个简单的架构解决的问题是大脑的数百万神经元每天都试图解决的日常自然语言理解问题。你的大脑强烈地激励于识别你在社交媒体上与之互动的人的出生性别。（如果你对此感兴趣，理查德·麦克尔雷斯和罗伯特·博伊德有一本关于这个主题的引人入胜的书。^([[13](#_footnotedef_13
    "查看脚注。")])) 一个单一的人工神经元可以使用一个人的名字中的字符来解决这个挑战，准确率约为80%。你将使用来自美国各州和领土超过100年的3.17亿出生证明的数据库中的名字样本。
- en: Biologically, identifying someone’s sex is useful to your genes because they
    only survive if you reproduce them by finding a sexual partner to blend your genes
    with. Social interaction with other humans is critical to your genes' existence
    and survival. And your genes are the blueprint for your brain. So your brain is
    likely to contain at least a few neurons dedicated to this critical task. And
    you’re going to find out how many artificial neurons it takes to predict the sex
    associated with a baby’s given name (first name).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 从生物学上讲，识别某人的性别对你的基因是有用的，因为只有通过找到性伴侣将你的基因与之融合，它们才能存活。与其他人类的社交互动对你的基因的存在和生存至关重要。而你的基因是你大脑的蓝图。因此，你的大脑很可能至少包含一些专门用于这一关键任务的神经元。你将找出预测与婴儿名字（名字）相关的性别需要多少人工神经元。
- en: Sex
  id: totrans-161
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 性别
- en: The word *sex* here refers to the label a doctor assigns to a baby at birth.
    In the US, the name, sex and date of birth are recorded on a birth certificate
    according to the laws that state. And the sex category is subject to interpretation
    and judgment by the person who fills out and signs the birth certificate. In datasets
    derived from US birth certificates, "sex at birth" is usually equivalent to one’s
    *genetic sex*, but that is not always the case. It is possible to create a relatively
    well-defined "genetic sex" category based on the presence of XX chromosomes (female)
    or XY chromosomes (male). But biology and life have a way of blurring the boundaries
    of even this seemingly precise definition of "genetic sex".
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的“性别”一词指的是医生在婴儿出生时分配的标签。在美国，根据法律规定，出生证明上记录了姓名、性别和出生日期。性别类别由填写和签署出生证明的人员进行解释和判断。在源自美国出生证明的数据集中，“出生时的性别”通常等同于一个人的遗传性别，但情况并非总是如此。可以根据XX染色体（女性）或XY染色体（男性）的存在创建一个相对明确的“遗传性别”类别。但是生物学和生活有一种模糊即使是这种看似精确的“遗传性别”定义的界限的方式。
- en: Male and female are not the last word in *birth sex* classification. The CDC
    (Center for Disease Control) in recommends that USCDI (US Core Data Interoperability)
    standards include several nonbinary sex categories for clinical or medical use.^([[14](#_footnotedef_14
    "View footnote.")]) In addition to 'female' and 'male', the categories 'unknown',
    and 'something not listed (specify)' are recommended by most western medical systems.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 男性和女性并不是“出生性别”分类的最后一句话。美国疾病控制中心（CDC）建议，美国核心数据互操作性（USCDI）标准包括几个非二元性别类别，用于临床或医学用途。除了“女性”和“男性”之外，“未知”和“未列出的其他内容（请具体说明）”是大多数西方医疗系统推荐的类别。
- en: You want to make sure that your test set names don’t appear anywhere in your
    training set. You also want to make sure that your test set only has one "right"
    label for each name. But this isn’t what you think. There is not one correct binary
    sex label for any particular name. There is indeed a correct probability score
    (continuous value) of maleness or femaleness of a name based on the ratio of the
    counts of names with a particular sex designation on their birth certificates.
    But that "correct" score will change as you add new examples to your dataset.
    Natural language processing is messy and fluid because the natural world and the
    language that describes it is dynamic and impossible to "pin on the wall."^([[15](#_footnotedef_15
    "View footnote.")])
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你的测试集名称在训练集中没有出现。你还要确保你的测试集中每个名称只有一个“正确”的标签。但这并不是你想象的那样。任何特定名称都没有一个正确的二元性别标签。事实上，根据出生证明上具有特定性别标识的名称计数的比率，存在一个关于名称的雌雄性别的正确概率分数（连续值）。但是，随着您向数据集添加新示例，该“正确”分数将发生变化。自然语言处理是混乱而流动的，因为自然界及其描述语言是动态的，不可能“固定不变”。
- en: This will enable the possibility that your model could *theoretically* achieve
    100% accuracy. Obviously, this isn’t really possible for a problem like this where
    even humans can’t achieve 100% accuracy. But your accuracy on the test set will
    tell you how close you are to this ideal, but only if you delete the duplicate
    names from your test set.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使您的模型从理论上讲可以达到100%的准确率。显然，对于一个即使人类也无法达到100%准确率的问题来说，这并不真实。但是，只有删除测试集中的重复名称，您才能了解您距离理想状态有多接近。
- en: 5.2.3 Pronouns and gender vs sex
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.3 代词和性别与性别
- en: Some states in the US allow one to indicate their child’s *gender* on a birth
    certificate. Gender is often what people use to decide what pronouns they prefer.
    And there are various ways that people think about their gender. There’s the apparent
    gender that they present to the world and there’s the gender identity that they
    assign to themselves at various stages of their lives. Identifying either of these
    genders is a sensitive subject because it is fraught with legal and social ramifications.
    In many repressive cultures, it can even be a matter of life and death. And gender
    is a very difficult thing to predict for a machine learning algorithm. For this
    chapter, we utilized a simplified binary sex dataset to prepare the scaffolding
    you need to build your natural language processing skills from the ground up.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 美国的一些州允许在出生证明上指示他们的孩子的*性别*。性别通常是人们用来决定他们喜欢什么代词的依据。人们对待自己的性别有各种各样的方式。有他们展示给世界的显而易见的性别，也有他们在生活的不同阶段为自己分配的性别认同。识别这两种性别中的任何一种都是一个敏感的话题，因为它带有法律和社会的影响。在许多压抑的文化中，甚至可能是生死攸关的问题。而对于机器学习算法来说，性别是一个非常难以预测的东西。对于本章节，我们利用了一个简化的二进制性别数据集来准备你从零开始构建自然语言处理技能所需的脚手架。
- en: 'And there are practical uses for sex-estimation model even for machines that
    don’t need it to spread their genes. A sex estimation model can be used to solve
    an important and difficult challenge in NLP called *coreference resolution*.^([[16](#_footnotedef_16
    "View footnote.")]) Coreference resolution is when an NLP algorithm identifies
    the object or words associated with pronouns in natural language text. For example,
    consider the pronouns in these sentences: "Maria was born in Ukraine. Her father
    was a physicist. 15 years later she left there for Israel." You may not realize
    it, but you resolved three coreferences in the blink of an eye. Your brain did
    the statistics on the likelihood that "Maria" was a "she/her" and that "Ukraine"
    is a "there".'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 即使对于不需要传播基因的机器来说，性别估计模型也有实际用途。性别估计模型可以用来解决自然语言处理中一个重要而困难的挑战，称为*共指消解*。^([[16](#_footnotedef_16
    "View footnote.")]) 共指消解是指当自然语言处理算法识别出与代词在自然语言文本中相关的对象或词语时。例如，请考虑以下句子中的代词："玛丽亚出生在乌克兰。她的父亲是一位物理学家。15年后她离开了那里去了以色列。"你可能没有意识到，但你在一眨眼的功夫就解决了三个共指。你的大脑对于"玛丽亚"是"她/她"和"乌克兰"是"那里"的概率进行了统计。
- en: Coreference resolution isn’t always that easy, for machines or for humans. It
    is more difficult to do in languages where pronouns do not have gender. It can
    be even more difficult in languages with pronouns that do not discriminate between
    people and inanimate objects. Even languages with genderless objects like English
    sometimes arbitrarily assign gender to important things, such as sailing ships.
    Ships are referred to with feminine pronouns such as "she" and "her." And they
    are often given feminine names.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 共指消解对于机器或人类来说并不总是那么容易。在没有性别的语言中，进行这个过程更加困难。在那些不区分人和无生命物体的代词语言中，这可能更加困难。即使是没有性别的英语有时也会随意给重要的事物分配性别，比如帆船。船只被称为"她"和"她的"。它们通常会被赋予女性的名字。
- en: So knowing the sex associated with the names of people (and ships) in your text
    can be helpful in improving your NLU pipeline. This can be helpful even when that
    sex identification is a poor indicator of the presented gender of a person mentioned
    in the text. The author of the text will often expect you to make assumptions
    about sex and gender based on names. In gender-bending SciFi novels, visionary
    authors like Gibson use this to keep you on your toes and expand your mind.^([[17](#_footnotedef_17
    "View footnote.")])
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 了解文本中人名（和船只名）所关联的性别，对于改善你的自然语言理解管道很有帮助。即使当这种性别辨识是一个贫乏的指示器来表明文本中提到的人的性别时，这也是有帮助的。文本的作者通常期望你根据名字来对性别和性别进行假设。在变性科幻小说中，像吉布森这样的有远见的作者利用这一点来让你警惕，并拓展你的思维。^([[17](#_footnotedef_17
    "View footnote.")])
- en: Important
  id: totrans-171
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 重要
- en: Make sure your NLP pipelines and chatbots are kind, inclusive and accessible
    for all human beings. In order to ensure your algorithms are unbiased you can
    *normalize* for any sex and gender information in the text data you process. In
    the next chapter you will see all the surprising ways in which sex and gender
    can affect the decisions your algorithms make. And you will see how gender affects
    the decisions of businesses or employers you deal with every day.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您的 NLP 流水线和聊天机器人对所有人类都是友好、包容和可访问的。为了确保您的算法是无偏的，您可以 *规范化* 处理的文本数据中的任何性别和性别信息。在下一章中，您将看到性别和性别如何影响您的算法做出的决策的所有令人惊讶的方式。您还将看到性别如何影响您每天处理的企业或雇主的决策。
- en: 5.2.4 Sex logistics
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.4 性别信息学
- en: First, import Pandas and set the `max_rows` to display only a few rows of your
    `DataFrame`s.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，导入 Pandas 并设置 `max_rows` 以仅显示您的 `DataFrame` 的几行。
- en: '[PRE13]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now download the raw data from the `nlpia2` repository and sample only 10,000
    rows, to keep things fast on any computer.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在从 `nlpia2` 存储库下载原始数据并仅采样10000行，以使任何计算机都可以快速运行。
- en: '[PRE14]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The data spans more than 100 years of US birth certificates, but only includes
    the baby’s first name:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 数据覆盖美国出生证书超过100年，但仅包括婴儿的名字：
- en: '|  | region | sex | year | name | count | freq |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '|  | 地区 | 性别 | 年份 | 名字 | 数量 | 频率 |'
- en: '| 6139665 | WV | F | 1987 | Brittani | 10 | 0.000003 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 6139665 | WV | 女 | 1987 | Brittani | 10 | 0.000003 |'
- en: '| 2565339 | MD | F | 1954 | Ida | 18 | 0.000005 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 2565339 | MD | 女 | 1954 | Ida | 18 | 0.000005 |'
- en: '| 22297 | AK | M | 1988 | Maxwell | 5 | 0.000001 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 22297 | AK | 男 | 1988 | Maxwell | 5 | 0.000001 |'
- en: '| …​ | …​ | …​ | …​ | …​ | …​ | …​ |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| …​ | …​ | …​ | …​ | …​ | …​ | …​ |'
- en: '| 4475894 | OK | F | 1950 | Leah | 9 | 0.000003 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 4475894 | OK | 女 | 1950 | Leah | 9 | 0.000003 |'
- en: '| 5744351 | VA | F | 2007 | Carley | 11 | 0.000003 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 5744351 | VA | 女 | 2007 | Carley | 11 | 0.000003 |'
- en: '| 5583882 | TX | M | 2019 | Kartier | 10 | 0.000003 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 5583882 | TX | 男 | 2019 | Kartier | 10 | 0.000003 |'
- en: You can ignore the region and birth year information for now. You only need
    the natural language name to predict sex with reasonable accuracy. If you’re curious
    about names, you can explore these variables as features or targets. Your target
    variable will be sex ('M' or 'F'). There are no other sex categories provided
    in this dataset besides male and female.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以忽略地区和出生年份信息。您只需要使用自然语言名称就可以以合理的准确性预测性别。如果您对名字感到好奇，您可以将这些变量探索为特征或目标。您的目标变量将是性别（'M'或'F'）。除男性和女性外，此数据集中没有提供其他性别分类。
- en: You might enjoy exploring the dataset to discover how often your intuition about
    the names parents choose for their babies. Machine learning and NLP are a great
    way to dispell stereotypes and misconceptions.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会喜欢探索数据集，以发现父母为他们的宝宝选择名字的频率。机器学习和 NLP 是消除成见和误解的好方法。
- en: '[PRE15]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: That’s what makes NLP and DataScience so much fun. It gives us a broader view
    of the world that breaks us out of the limited perspective of our biological brains.
    I’ve never met a woman named "Timothy" but at least .1% of babies named Timothy
    in the US have female on their birth certificate.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 NLP 和数据科学如此有趣的原因。它为我们提供了一个更广泛的世界视角，打破了我们生物大脑的有限视角。我从来没有见过一个叫“Timothy”的女人，但在美国出生证书上的至少0.1%的婴儿名字为Timothy的是女性。
- en: To speed up the model training, you can aggregate (combine) your data across
    regions and years if those are not aspects of names that you’d like your model
    to predict. You can accomplish this with a Pandas `DataFrame’s `.groupby()` method.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如果地区和年份不是名称的要素，不需要预测，可以在跨地区和年份聚合（组合）数据以加快模型训练。您可以使用 Pandas 的 `DataFrame's` `.groupby()`
    方法来实现这一点。
- en: '[PRE16]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Because we’ve aggregated the numerical data for the column "count", the `counts`
    object is now a Pandas `Series` object rather than a `DataFrame`. It looks a little
    funny because we created a multilevel index on both name and sex. Can you guess
    why?
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们已经聚合了 "count" 列的数字数据，所以 `counts` 对象现在是 Pandas 的 `Series` 对象，而不是 `DataFrame`。它看起来有点奇怪，因为我们在名称和性别上创建了多级
- en: Now the dataset looks like an efficient set of examples for training a logistic
    regression. In fact, if we only wanted to predict the likely sex for the names
    in this database, we could just use the max count (the most common usage) for
    each name.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据集看起来像是训练逻辑回归的有效示例集。实际上，如果我们只想预测该数据库中的名称可能的性别，我们可以仅使用每个名称的最大计数（最常用法）。
- en: But this is a book about NLP and NLU (Natural Language Understanding). You’d
    like your models to *understand* the text of the name in some way. And you’d like
    it to work on odd names that are not even in this database, names such as "Carlana",
    a portmanteau of "Carl" and "Ana", her grandparents, or one-of-a-kind names such
    as "Cason." Examples that are not part of your training set or test set are called
    "out of distribution." In the real world, your model will almost always encounter
    words and phrases never seen before. It’s called "generalization" when a model
    can extrapolate to these out-of-distribution examples.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 但这是一本关于NLP和NLU（自然语言理解）的书。你希望你的模型以某种方式*理解*姓名的文本。而且你希望它能够处理不在这个数据库中的奇怪姓名，比如"Carlana"，一个由她的祖父母"Carl"和"Ana"组成的混合词，或者像"Cason"这样的独一无二的姓名。不在你的训练集或测试集中的示例被称为"分布外"。在现实世界中，你的模型几乎总是会遇到以前从未见过的词语和短语。当一个模型能够推广到这些分布外示例时，这被称为"泛化"。
- en: But how can you tokenize a single word like a name so that your model can generalize
    to completely new made-up names that its never seen before? You can use the character
    n-grams within each word (or name) as your tokens. You can set up a `TfidfVectorizer`
    to count characters and character n-grams rather than words. You can experiment
    with a wider or narrower `ngram_range` but 3-grams are a good bet for most TF-IDF-based
    information retrieval and NLU algorithms. For example, the state-of-the-art database
    PostgreSQL defaults to character 3-grams for its full-text search indexes. In
    later chapters, you’ll even use word piece and sentence piece tokenization which
    can optimally select a variety of character sequences to use as your tokens.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 但是你如何对一个单词像一个姓名进行标记化，以便你的模型可以泛化到完全新的虚构的从未见过的名字？你可以使用每个单词（或姓名）中的字符n-gram作为你的标记。你可以设置一个`TfidfVectorizer`来计算字符和字符n-gram而不是单词。你可以尝试更宽或更窄的`ngram_range`，但对于大多数基于TF-IDF的信息检索和NLU算法来说，3-gram是一个不错的选择。例如，最先进的数据库PostgreSQL将其全文搜索索引默认设置为字符3-gram。在后面的章节中，你甚至将使用词块和句子块标记化，它们可以选择最佳的字符序列作为你的标记。
- en: '[PRE17]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Shouldn’t you normalize the token counts by something like document frequency?
    You will use the counts of births for that. For name TF-IDF vectors you want to
    use counts of births or people as your *document* frequencies. This will help
    your vector represent the frequency of the name outside of your corpus of unique
    names.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该按照文档频率来归一化标记计数，例如出生率。你将使用出生率来计算。对于姓名TF-IDF向量，你希望使用出生率或人口作为*文档*频率。这将帮助你的向量表示语料库之外的姓名频率。
- en: Now that you’ve indexed our `names` series by `name` *and* `sex` aggregating
    counts across states and years, there will be fewer unique rows in your `Series`.
    You can de-duplicate the names before calculating TF-IDF character n-gram term
    frequencies. Don’t forget to keep track of the number of birth certificates so
    you use that as your document frequency.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经通过姓名*和*性别对我们的`names`系列进行了索引，跨州和年份聚合计数，你的`Series`中将会有更少的唯一行。在计算TF-IDF字符n-gram术语频率之前，你可以去重姓名。不要忘记跟踪出生证明的数量，以便将其用作文档频率。
- en: '[PRE18]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: You’ve aggregated 10,000 name-sex pairs into only 4238 unique name-sex pairings.
    Now you are ready to split the data into training and test sets.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经将 10,000 个姓名-性别对聚合成了只有 4238 个唯一的姓名-性别配对。现在你可以将数据分割成训练集和测试集了。
- en: '[PRE19]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'To ensure you don’t accidentally swap the sexes for any of the names, recreate
    the `name, sex` multiindex:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保你不会意外地交换任何姓名的性别，重新创建`name, sex`的多索引：
- en: '[PRE20]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As you saw earlier, this dataset contains conflicting labels for many names.
    In real life, many names are used for both male and female babies (or other human
    sex categories). Like all machine learning classification problems, the math treats
    it as a regression problem. The model is actually predicting a continuous value
    rather than a discrete binary category. Linear algebra and real life only work
    on real values. In machine learning all dichotomies are false.^([[18](#_footnotedef_18
    "View footnote.")]) Machines don’t think of words and concepts as hard categories,
    so neither should you.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你之前看到的，这个数据集对许多姓名包含冲突的标签。在现实生活中，许多姓名都被用于男性和女性婴儿（或其他人类性别类别）。像所有的机器学习分类问题一样，数学将其视为回归问题。模型实际上是在预测一个连续值，而不是离散的二进制类别。线性代数和现实生活只适用于实值。在机器学习中，所有的二分法都是错误的。^([[18](#_footnotedef_18
    "View footnote.")]) 机器不会将单词和概念视为严格的类别，因此你也不应该这样做。
- en: '[PRE21]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Because of the duplicates the test set flag can be created from the `not` of
    the `istrain`.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 由于重复，测试集标志可以从`istrain`的非运算中创建。
- en: '[PRE22]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Now you can transfer the `istest` and `istrain` flags over to the original Dataframe,
    being careful to fill `NaNs` with False for both the training set and the test
    set.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以将`istest`和`istrain`标志传递到原始数据框中，要小心，对于训练集和测试集，将`NaNs`填充为 False。
- en: '[PRE23]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Now you can use the training set to fit `TfidfVectorizer` without skewing the
    n-gram counts with the duplicate names.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以使用训练集来适应`TfidfVectorizer`，而不会因为重复的名称而使 n-gram 计数偏差。
- en: '[PRE24]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: You need to be careful when working with sparse data structures. If you convert
    them to normal dense arrays with `.todense()` you may crash your computer by using
    up all its RAM. But this sparse matrix contains only about 17 million elements
    so it should work fine within most laptops. You can use `toarray()` on sparse
    matrices to create a DataFrame and give meaningful labels to the rows and columns.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用稀疏数据结构时，你需要小心。如果你用`.todense()`将它们转换为普通的稠密数组，可能会因为使用了所有的内存而导致计算机崩溃。但是这个稀疏矩阵只包含大约
    1700 万个元素，所以它应该可以在大多数笔记本电脑上正常工作。你可以对稀疏矩阵使用`toarray()`来创建一个数据框，并为行和列提供有意义的标签。
- en: '[PRE25]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Aah, notice that the column labels (character n-grams) all start with lowercase
    letters. It looks like the `TfidfVectorizer` folded the case (lowercased everything).
    It’s likely that capitalization will help the model, so let’s revectorize the
    names without lowercasing.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到列标签（字符 n-grams）全部以小写字母开头。看起来`TfidfVectorizer`将大小写折叠了（将所有内容都转换为小写）。大写可能会帮助模型，所以让我们重新对名称进行向量化而不是转换为小写。
- en: '[PRE26]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: That’s better. These character 1, 2, and 3-grams should have enough information
    to help a neural network guess the sex for names in this birth certificate database.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 好多了。这些字符 1、2 和 3 克就应该包含足够的信息来帮助神经网络猜测出这份出生证明数据库中姓名的性别。
- en: Choosing a neural network framework
  id: totrans-218
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 选择神经网络框架
- en: Logistic regressions are the perfect machine learning model for any high-dimensional
    feature vector such as a TF-IDF vector. To turn a logistic regression into a neuron
    you just need a way to connect it to other neurons. You need a neuron that can
    learn to predict the outputs of other neurons. And you need to spread the learning
    out so one neuron doesn’t try to do all the work. Each time your neural network
    gets an example from your dataset that shows it the right answer it will be able
    to calculate just how wrong it was, the loss or error. But if you have more than
    one neuron working together to contribute to that prediction, they’ll each need
    to know how much to change their weights to move the output closer to the correct
    answer. And to know that you need to know how much each weight affects the output,
    the gradient (slope) of the weights relative to the error. This process of computing
    gradients (slopes) and telling all the neurons how much to adjust their weights
    up and down so that the loss will go down is called *backpropagation* or backprop.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归是任何高维特征向量（如 TF-IDF 向量）的完美机器学习模型。要将逻辑回归转换为神经元，你只需要一种方法将其连接到其他神经元。你需要一个能够学习预测其他神经元输出的神经元。并且你需要将学习分散开来，这样一个神经元就不会尝试做所有的工作。每当你的神经网络从数据集中获得一个显示正确答案的示例时，它就能计算出自己的错误程度，即损失或误差。但是如果有多个神经元共同贡献给该预测，它们每个都需要知道如何改变自己的权重，以使输出更接近正确答案。要知道这一点，你需要知道每个权重对输出的影响程度，即权重相对于误差的梯度（斜率）。计算梯度（斜率）并告诉所有神经元如何调整它们的权重以使损失降低的过程称为*反向传播*或反向传播。
- en: A deep learning package like PyTorch can handle all that for you automatically.
    In fact, it can handle any computational graph (network) you can dream up. PyTorch
    can handle any network of connections between mathematical operations. This flexibility
    is why most researchers use it rather than TensorFlow (Keras) for their breakthrough
    NLP algorithms. TensorFlow is designed with a particular kind of computational
    graph in mind, one that can be efficiently computed on specialized chips manufactured
    by one of the BigTech companies. Deep Learning is a powerful money-maker for Big
    Tech and they want to train your brain to use only their tools for building neural
    networks. I had no idea BigTech would assimilate Keras into the TensorFlow "Borg",
    otherwise I would not have recommended it in the first edition.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 像PyTorch这样的深度学习软件包可以自动处理所有这些。事实上，它可以处理您可以想象的任何计算图（网络）。PyTorch可以处理任何数学操作之间的网络连接。正是因为这种灵活性，大多数研究人员使用它而不是TensorFlow（Keras）来开发他们的突破性NLP算法。TensorFlow设计了一种特定类型的计算图，可以在由大型技术公司制造的专用芯片上高效计算。深度学习对于大型技术公司来说是一个强大的赚钱方式，他们希望训练你的大脑只使用他们自己的工具来构建神经网络。我不知道大型技术公司会将Keras整合到TensorFlow中，否则我不会在第一版中推荐它。
- en: The decline in portability for Keras and the rapidly growing popularity of PyTorch
    are the main reasons we decided a second edition of this book was in order. What’s
    so great about PyTorch?
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: Keras的可移植性下降和PyTorch的快速增长的受欢迎程度是我们决定第二版书的原因。PyTorch有什么好处呢？
- en: 'Wikipedia has an unbiased and detailed comparison of all DeepLearning frameworks.
    And Pandas lets you load it directly from the web into a `DataFrame`:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 维基百科上有一个公正和详细的所有深度学习框架的比较。而Pandas可以让您直接从网络加载它并放入一个`DataFrame`中：
- en: '[PRE27]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Here is how you can use some basic NLP to score the top 10 deep learning frameworks
    from the Wikipedia article that lists each of their pros and cons. You will find
    this kind of code useful whenever you want to turn semi-structured natural language
    into data for your NLP pipelines.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这是如何使用基本的自然语言处理（NLP）通过维基百科的文章对前十个深度学习框架进行评分的方法，该文章列出了它们的优点和缺点。每当您想将半结构化自然语言转化为NLP管道中的数据时，您会发现这种代码非常有用。
- en: '[PRE28]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Now that the Wikipedia table is cleaned up, you can compute some sort of "total
    score" for each deep learning framework.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 现在维基百科的表格都整理好了，你可以为每个深度学习框架计算某种"总分"。
- en: '[PRE29]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: PyTorch got nearly a perfect score because of its support for Linux, Android
    and all popular deep learning applications.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch几乎得到了满分，因为它支持Linux、Android和所有流行的深度学习应用程序。
- en: Another promising one you might want to check out is ONNX. It’s really a meta
    framework and an open standard that allows you to convert back and forth between
    networks designed on another framework. ONNX also has some optimization and pruning
    capabilities that will allow your models to run inference much faster on much
    more limited hardware, such as portable devices.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个值得注意的是ONNX。它实际上是一个元框架和一个允许在其他框架上进行网络转换的开放标准。ONNX还具有一些优化和剪枝功能，可以使您的模型在非常有限的硬件上（例如便携设备）运行推理速度更快。
- en: And just for comparison, how does SciKit Learn stack up to PyTorch for building
    a neural network model?
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较一下，使用SciKit Learn构建神经网络模型与使用PyTorch相比如何？
- en: Table 5.1 Scikit-Learn vs PyTorch
  id: totrans-231
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表5.1 Scikit-Learn与PyTorch
- en: '| Scikit-Learn | PyTorch |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| Scikit-Learn | PyTorch |'
- en: '| for Machine Learning | for Deep Learning |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 用于机器学习 | 用于深度学习 |'
- en: '| Not GPU-friendly | Made for GPUs (parallel processing) |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 不适合GPU | 适用于GPU（并行处理） |'
- en: '| `model.predict()` | `model.forward()` |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| `model.predict()` | `model.forward()` |'
- en: '| `model.fit()` | trained with custom `for`-loop |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| `model.fit()` | 通过自定义`for`循环训练 |'
- en: '| simple, familiar API | flexible, powerful API |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 简单熟悉的API | 灵活强大的API |'
- en: Enough about frameworks, you are here to learn about neurons. PyTorch is just
    what you need. And there’s a lot left to explore to get familiar with your new
    PyTorch toolbox.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 足够讨论框架了，你在这里是为了学习神经元。PyTorch正是你所需要的。而且还有很多东西等着你去探索，以熟悉你的新的PyTorch工具箱。
- en: 5.2.5 A sleek sexy PyTorch neuron
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.5 一个时尚性感的PyTorch神经元
- en: Finally, it’s time to build a neuron using the PyTorch framework. Let’s put
    all this into practice by predicting the sex of the names you cleaned earlier
    in this chapter.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，是时候使用PyTorch框架构建一个神经元了。让我们通过预测您在本章前面清理过的姓名的性别来将所有这些付诸实践中。
- en: You can start by using PyTorch to implement a single neuron with a logistic
    activation function - just like the one you used to learn the toy example at the
    beginning of the chapter.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Let’s see what happened here. Our model is a *class* that extends the PyTorch
    class used to define neural networks, `torch.nn.Module`. As with every Python
    class, it has a *constructor* method called `*init*`. The constructor is where
    you can define all the attributes of your neural network - most importantly, the
    model’s layers. In our case, we have an extremely simple architecture - one layer
    with a single neuron, which means there will be only one output. And the number
    of inputs, or features, will be equal to the length of your TF-IDF vector, the
    dimensionality of your features. There were 3663 unique 1-grams, 2-grams, and
    3-grams in our names dataset, so that’s how many inputs you’ll have for this single-neuron
    network.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: The second crucial method you need to implement for your neural network is the
    `forward()` method. This method defines how the input to your model propagates
    through its layers - the *forward propagation*. If you are asking yourself where
    the backward propagation (backprop) is, you’ll soon see, but it’s not in the constructor.
    We decided to use the logistic, or sigmoid, activation function for our neuron
    - so our `forward()` method will use PyTorch’s built-in function `sigmoid`.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Is this all you need to train our model? Not yet. There are two more crucial
    pieces that your neuron needs to learn. One is the loss function, or cost function
    that you saw earlier in this chapter. The Mean Square Error (MSE) you learned
    about in chapter 4 would be a good candidate for the error metric if this were
    a regression problem. For this problem you are doing binary classification, so
    Binary Cross Entropy is a more common error (loss) metric to use.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s what Binary Cross Entropy looks like for a single classification probability
    *p*:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '**Equation 5.2: Binary Cross Entropy**'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '`BCE = -(_y_ log _p_ + (1 - _y_) log1 - _p_)`'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: The logarithmic nature of the function allows it to penalize a "confidently
    wrong" example, when your model predicts with high probability the sex of a particular
    name is male, when it is actually more commonly labeled as female. We can help
    it to make the penalties even more related to reality by using another piece of
    information available to us - the frequency of the name for a particular sex in
    our dataset.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The last thing we need to choose is how to adjust our weights based on the loss
    - the optimizer algorithm. Remember our discussion about "skiing" down the gradient
    of the loss function? The most common way to implement skiing downward called
    Stochastic Gradient Descent (SGD). Instead of taking all of your dataset into
    account, like your Pythonic perceptron did, it only calculates the gradient based
    on one sample at a time or perhaps a mini-batch of samples.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Your optimizer needs two parameters to know how fast or how to ski along the
    loss slope - *learning rate* and *momentum*. The learning rate determines how
    much your weights change in response to an error - think of it as your "ski velocity".
    Increasing it can help your model converge to the local minimum faster, but if
    it’s too large, you may overshoot the minimum every time you get close. Any optimizer
    you would use in PyTorch would have a learning rate.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 你的优化器需要两个参数来知道沿着损失斜率滑行的速度有多快或如何滑行 - *学习速率*和*动量*。学习速率决定了在出现错误时，你的权重发生多大的变化 -
    可以将其视为你的“滑行速度”。增加它可以帮助你的模型更快地收敛到局部最小值，但如果太大，每次接近最小值时都可能超过。在PyTorch中使用的任何优化器都会有一个学习速率。
- en: Momentum is an attribute of our gradient descent algorithm that allows it to
    "accelerate" when it’s moving in the right direction and "slow down" if it’s getting
    away from its target. How do we decide which values to give these two attributes?
    As with other hyperparameters you see in this book, you’ll need to optimize your
    them to see what’s the most effective one for your problem. For now, you can chose
    some arbitrary values for the hyperparameters `momentum` and `lr` (learning rate).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 动量是我们梯度下降算法的一个属性，它可以在朝正确方向移动时进行加速，并在远离目标时减速。我们如何决定这两个属性的值？和本书中看到的其他超参数一样，你需要优化它们以找出对你的问题最有效的值。现在，你可以选择一些任意的值作为超参数`momentum`和`lr`（学习率）。
- en: '[PRE32]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The last step before running our model training is to get the testing and training
    datasets into a format that PyTorch models can digest.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行我们的模型训练之前，最后一步是将测试和训练数据集转换为PyTorch模型可以处理的格式。
- en: '[PRE33]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Finally, you’re ready for the most important part of this chapter - the sex
    learning! Let’s look at it and understand what happens at each step.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你准备好了这一章最重要的部分——性别学习！让我们来看一看，了解每一步发生了什么。
- en: '[PRE34]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: That was fast! It should take only a couple of seconds to train this single
    neuron for about 200 epochs and thousands of examples for each epoch.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 真快！训练这个单一神经元大约需要几秒钟，大约200个纪元和每个纪元数以千计的例子。
- en: Looks easy, right? We made it as simple as possible so that you can see the
    steps clearly. But we don’t even know how our model is performing! Let’s add some
    utility functions that will help us see if our neuron improves over time. This
    is called instrumentation. We can of course look at the loss, but it’s also good
    to gauge how our model is doing with a more intuitive score, such as accuracy.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来很简单，对吧？我们尽可能将步骤简化，让你能清楚地看到。但我们甚至不知道我们的模型表现如何！让我们添加一些实用函数，帮助我们观察神经元是否随着时间的推移而改进。这被称为仪器化。当然，我们可以看损失，但也可以用更直观的分数来评估我们的模型表现，比如准确性。
- en: 'First, you’ll need a function to convert the PyTorch tensors we get from the
    module back into `numpy` arrays:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要一个函数将我们从模块中获得的PyTorch张量转换回`numpy`数组：
- en: '[PRE35]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now you use this utility function to measure the accuracy of each iteration
    on the tensors for your outputs (predictions):'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以使用这个实用程序函数来测量每次迭代在输出（预测）的张量上的准确性：
- en: '[PRE36]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now you can rerun your training using this utility function to see the progress
    of the model’s loss and accuracy with each epoch:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以使用这个实用程序函数重新运行训练，以查看模型在每个纪元中损失和准确度的进展：
- en: '[PRE37]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: With just a single set of weights for a single neuron, your simple model was
    able to achieve more than 70% accuracy on our messy, ambiguous, real-world dataset.
    Now you can add some more examples from the real world of Tangible AI and some
    of our contributors.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 只需使用一个神经元的一组权重，你的简单模型就能在我们混乱、不确定、真实世界的数据集上达到超过70%的准确率。现在你可以添加一些真实世界中的有形人工智能的例子以及一些我们的贡献者。
- en: '[PRE39]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Earlier we chose to use the value 1 to represent "female" and 0 to represent
    "male." The first three example names, "John," "Greg," and "Vishvesh," are the
    names of men who have generously contributed to open source projects that are
    important to me, including the code in this book. It looks like Vish’s name doesn’t
    appear on as many US birth certificates for male babies as John’s or Greg’s. The
    model is more certain of the maleness in the character n-grams for "John" than
    those for "Vishvesh."
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 早些时候，我们选择使用值1来表示“女性”，使用值0来表示“男性”。前三个例子的名字，“John”，“Greg”和“Vishvesh”，是男人的名字，他们慷慨地为对我重要的开源项目做出了贡献，包括本书中的代码。看起来Vish的名字在美国男婴的出生证明上出现的次数不如John或Greg多。对于“John”而言，模型对于“Vishvesh”的字符n-gram中的男性意味更加确定。
- en: The next three names, "Sarah," "Carlana," and 'Ruby', are the first names of
    women at the top of my mind when writing this book.^([[19](#_footnotedef_19 "View
    footnote.")]) ^([[20](#_footnotedef_20 "View footnote.")]) The name "Ruby" may
    have some maleness in its character n-grams because a similar name "Rudy" (often
    used for male babies) is only 1 edit away from "Ruby." Oddly the name "Carlana,"
    which contains within it a common male name "Carl," is confidently predicted to
    be a female name.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Skiing down the error slope
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of training in neural networks is to minimize a loss function by finding
    the best parameters (weights) for your model. At each step of the optimization
    loop your algorithm finds the steepest way down the slope. Keep in mind, this
    error slope is not the error for just one particular example from your data set.
    It is minimizing the cost (loss) for the mean of all the errors on all the points
    in a batch of data taken together.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Creating a visualization of this side of the problem can help build a mental
    model of what you’re doing when you adjust the weights of the network as you go.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: In chapter 4 you learned about Root Mean Square Error (RMSE), which is the most
    common cost function for regression problems. If you imagine plotting the error
    as a function of the possible weights, given a specific input and a specific expected
    output, a point exists where that function is closest to zero; that is your *minimum* — the
    spot where your model has the least error.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: This minimum will be the set of weights that gives the optimal output for a
    given training example. You will often see this represented as a three-dimensional
    bowl with two of the axes being a two-dimensional weight vector and the third
    being the error (see figure 5.8). That description is a vast simplification, but
    the concept is the same in higher dimensional spaces (for cases with more than
    two weights).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.6 Convex error curve
  id: totrans-277
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![smooth error](images/smooth_error.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
- en: Similarly, you can graph the error surface as a function of all possible weights
    across all the inputs of a training set. But you need to tweak the error function
    a little. You need something that represents the aggregate error across all inputs
    for a given set of weights. For this example, you’ll use *mean squared error*
    as the *z* axis. Here again, you’ll find a location on the error surface where
    the coordinates at that location are the vector of weights that minimize the average
    error between your predictions and the classification labels in your training
    set. That set of weights will configure your model fit the entire training set
    as well as possible.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1 Off the chair lift, onto the slope - gradient descent and local minima
  id: totrans-280
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: What does this visualization represent? At each epoch, the algorithm is performing
    *gradient descent* in trying to minimize the error. Each time you adjust the weights
    in a direction that will hopefully reduce your error the next time. A convex error
    surface will be great. Stand on the ski slope, look around, find out which way
    is down, and go that way!
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 这个可视化代表了什么？在每个时期，算法都在进行*梯度下降*，试图最小化误差。每次你调整权重的方向都希望下次能减少你的误差。一个凸错误曲面会很棒。站在滑雪坡上，四处看看，找出哪个方向是下坡，然后朝那个方向走！
- en: But you’re not always so lucky as to have such a smooth-shaped bowl; it may
    have some pits and divots scattered about. This situation is what is known as
    a *nonconvex error curve*. And, as in skiing, if these pits are big enough, they
    can suck you in and you might not reach the bottom of the slope.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 但是你并不总是有这样一个光滑的碗形；它可能有一些分散的凹陷和坑洞。这种情况被称为*非凸误差曲线*。而且，就像滑雪一样，如果这些坑洞足够大，它们会吸引你，你可能就无法到达斜坡底部了。
- en: Again the diagrams represent the weights for two-dimensional input. But the
    concept is the same if you have a 10-dimensional input, or 50, or 1000\. In those
    higher dimensional spaces, visualizing it doesn’t make sense anymore, so you trust
    the math. Once you start using neural networks, visualizing the error surface
    becomes less important. You get the same information from watching (or plotting)
    the error or a related metric over the training time and seeing if it is tending
    toward zero. That will tell you if your network is on the right track or not.
    But these 3D representations are a helpful tool for creating a mental model of
    the process.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，这些图表代表了二维输入的权重。但是如果你有一个 10 维、50 维或 1000 维的输入，概念是一样的。在那些更高维的空间中，再也无法将其可视化，所以你要相信数学。一旦你开始使用神经网络，可视化错误曲面就变得不那么重要了。你可以通过观察（或绘制）训练时间内的错误或相关指标，看它是否趋向于零来获取相同的信息。这将告诉你你的网络是否在正确的轨道上。但是这些
    3D 表示法对于创建过程的心理模型是一个有用的工具。
- en: But what about the nonconvex error space? Aren’t those divots and pits a problem?
    Yes, yes they are. Depending on where you randomly start your weights, you could
    end up at radically different weights and the training would stop, as there is
    no other way to go down from this *local minimum* (see Figure 5.9).
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 但是非凸错误空间呢？那些凹陷和坑洞是个问题吗？是的，是的。取决于你随机开始权重的位置，你可能会以截然不同的权重结束，训练会停止，因为从这个局部最小值往下走别无选择（见图
    5.9）。
- en: Figure 5.7 Nonconvex error curve
  id: totrans-285
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.7 非凸误差曲线
- en: '![lumpy error](images/lumpy_error.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![凹凸错误](images/lumpy_error.png)'
- en: And as you get into even higher-dimensional space, the local minima will follow
    you there as well.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 随着你进入更高维的空间，局部最小值也会跟随你到那里。
- en: '5.3.2 Shaking things up: stochastic gradient descent'
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.2 改变方式：随机梯度下降
- en: Up until now, you have been aggregating the error for all the training examples
    and skiing down the steepest route as fast as you can. But training on the entire
    training set one sample at a time is a little nearsighted. It’s like choosing
    the downhill sections of a snow park and ignoring all the jumps. Sometimes a good
    ski jump can help you skip over some rough terrain.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经在尽力快速地聚合所有训练样本的错误，并且尽可能快地滑向最陡峭的路线。但是一次处理整个训练集一个样本有点短视。这就像选择雪地公园的下坡路段，忽略了所有的跳跃。有时，一个好的滑雪跳台可以帮助你跳过一些崎岖的地形。
- en: And if you try to train on the entire dataset at once, you may run out of RAM,
    bogging down your training in SWAP — swapping data back and forth between RAM
    and your much slower persistent disk storage. And this single static error surface
    can have traps. Because you are starting from a random starting point (the initial
    model weights) you could blindly ski downhill into some local minima (divot, hole,
    or cave). You may not know that better options exist for your weight values. And
    your error surface is static. Once you reach a local minimum in the error surface,
    there is no downhill slope to help your model ski out and on down the mountain.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你试图一次性训练整个数据集，你可能会耗尽内存，导致你的训练在 SWAP 中陷入困境 —— 在 RAM 和你的更慢的持久性磁盘存储之间来回交换数据。而这个单一静态的错误曲面可能会有陷阱。因为你是从一个随机起点开始的（初始模型权重），你可能会盲目地滑下山坡进入一些局部最小值（凹陷、坑洞或洞穴）。你可能不知道存在更好的权重值选项。而且你的错误曲面是静态的。一旦你在错误曲面上达到一个局部最小值，就没有下坡的斜度来帮助你的模型滑出去，然后滑下山去。
- en: So to shake things up you want to add some randomization to the process. You
    want to periodically shuffle the order of the training examples that your model
    is learning from. Typically you reshuffle the order of the training examples after
    each pass through your training dataset. Shuffling your data changes the order
    in which your model considers the prediction error for each sample. So it will
    change the path it follows in search of the global minimum (smallest model error
    for that dataset). This shuffling is the "stochastic" part of stochastic gradient
    descent.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了让事情更有变化，您希望向这个过程添加一些随机性。您希望周期性地对模型学习的训练样本的顺序进行洗牌。通常在每次通过训练数据集后重新洗牌训练样本的顺序。洗牌您的数据会改变模型考虑每个样本的预测误差的顺序。因此，它将改变其寻找全局最小值（该数据集的最小模型误差）的路径。这种洗牌是随机梯度下降的"随机"部分。
- en: There’s still some room for improving the "gradient" estimation part of gradient
    descent. You can add a little humility to your optimizer so it doesn’t get overconfident
    and blindly follow every new guess all the way to where it thinks the global minimum
    should be. It’s pretty rare that the ski slope where you are is going to point
    in a straight line directly to the ski lodge at the bottom of the mountain. So
    your model goes a short distance in the direction of the downward slope (gradient)
    without going all the way. This way the gradient for each individual sample doesn’t
    lead your model too far astray and your model doesn’t get lost in the woods. You
    can adjust the *learning rate* hyperparameter of the SGD optimizer (stochastic
    gradient descent) to control how confident your model is in each individual sample
    gradient.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '"梯度下降"方法的"梯度"估计部分仍然有改进的余地。您可以向优化器添加一些谦卑，这样它就不会过于自信，盲目地跟随每一个新的猜测，一直到它认为全局最小值应该在哪里。在您所在的滑雪道很少会直接指向山底滑雪小屋的直线方向。因此，你的模型沿着向下坡的方向（梯度）行进了一小段距离，而不是一直走到底。这样，每个独立样本的梯度就不会使您的模型偏离太远，您的模型也不会迷失在树林中。您可以调整SGD优化器（随机梯度下降）的*学习率*超参数，以控制您的模型对每个独立样本的梯度有多自信。'
- en: Another training approach is *batch learning*. A batch is a subset of the training
    data, like maybe 0.1%, 1%, 10% or 20% of your dataset. Each batch creates a new
    error surface to experiment with as you ski around searching for the unknown "global"
    error surface minimum. Your training data is just a sample of the examples that
    will occur in the real world. So your model shouldn’t assume that the "global"
    real-world error surface is shaped the same as the error surface for any portion
    of your training data.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种训练方法是*批量学习*。一个批次是训练数据的一个子集，比如，0.1%、1%、10%或20%的数据集。每个批次都会创建一个新的错误表面，让你在搜索未知的"全局"错误表面最小值时进行实验。你的训练数据只是真实世界中会发生的例子的样本。因此，您的模型不应假设"全局"真实世界的错误表面形状与训练数据的任何部分的错误表面相同。
- en: 'And this leads to the best strategy for most NLP problems: *mini-batch learning*.^([[21](#_footnotedef_21
    "View footnote.")]) Geoffrey Hinton found that a batch size of around 16 to 64
    samples was optimal for most neural network training problems.^([[22](#_footnotedef_22
    "View footnote.")]) This is the right size to balance the shakiness of stochastic
    gradient descent, with your desire to make significant progress in the correct
    direction towards the global minimum. And as you move toward the changing local
    minima on this fluctuating surface, with the right data and right hyperparameters,
    you can more easily bumble toward the global minimum. Mini-batch learning is a
    happy medium between *full batch* learning and individual example training. Mini-batch
    learning gives you the benefits of both *stochastic* learning (wandering randomly)
    and *gradient descent* learning (speeding headlong directly down the presumed
    slope).'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了大多数NLP问题的最佳策略：*小批量学习*。^([[21](#_footnotedef_21 "查看脚注。")]) Geoffrey Hinton发现，对于大多数神经网络训练问题，大约16到64个样本的批次大小是最佳的。^([[22](#_footnotedef_22
    "查看脚注。")]) 这是平衡了随机梯度下降的不稳定性和您希望朝向全局最小值正确方向取得显著进展的正确大小。当您朝着这个变化的局部最小值前进，并且使用正确的数据和正确的超参数时，您可以更容易地朝全局最小值迈进。小批量学习是*完整批次*学习和单个样本训练之间的一种折衷。小批量学习使您既能享受*随机*学习（随机徘徊）的好处，又能享受*梯度下降*学习（直接加速下坡）的好处。
- en: Although the details of how *backpropagation* works are fascinating ^([[23](#_footnotedef_23
    "View footnote.")]), they aren’t trivial, and we won’t explain the details here.
    A good mental image that can help you train your models is to imagine the error
    surface for your problem as the uncharted terrain of some alien planet. Your optimizer
    can only look at the slope of the ground at your feet. It uses that information
    to take a few steps downhill, before checking the slope (gradient) again. It may
    take a long time to explore the planet this way. But a good optimization algorithm
    helps your neural network remember all the good locations on the map and use them
    to guess a new place on the map to explore in search of the global minimum. On
    Earth this lowest point on the planet’s surface is the bottom of the canyon under
    Denman Glacier in Antarctica — 3.5 km below sea level.^([[24](#_footnotedef_24
    "View footnote.")]) A good mini-batch learning strategy will help you find the
    steepest way down the ski slope or glacier (not a pleasant image if you’re scared
    of heights) to the global minimum. Hopefully, you’ll soon find yourself by the
    fire in the ski lodge at the bottom of the mountain or a campfire in an ice cave
    below Denman Glacier.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: See if you can add additional layers to the perceptron you created in this chapter.
    See if the results you get improve as you increase the network complexity. Bigger
    is not always better, especially for small problems.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Test yourself
  id: totrans-297
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What is the simple AI logic "problem" that Rosenblatt’s artificial neurons couldn’t
    solve?
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What minor change to Rosenblatt’s architecture "fixed" perceptrons and ended
    the first "AI Winter"?
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the equivalent of a PyTorch `model.forward()` function in Scikit-Learn
    models?
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What test set accuracy can you achieve with the sex-predicting `LogisticRegression`
    model if you aggregate names across year and region? Don’t forget to stratify
    your test set to avoid cheating.
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 5.5 Summary
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Minimizing a cost function is how machines gradually learn more and more about
    the words.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A backpropagation algorithm is the means by which a network *learns*.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The amount a weight contributes to a model’s error is directly related to the
    amount it needs to updated.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural networks are at their heart optimization engines.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Watch out for pitfalls (local minima) during training by monitoring the gradual
    reduction in error.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[1]](#_footnoteref_1) See analysis by Dario Amodei and Danny Hernandez here
    ( [https://openai.com/blog/ai-and-compute/](ai-and-compute.html))'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '[[2]](#_footnoteref_2) See the lemmatizing FAQ chatbot example in chapter 3
    failed on the question about "overfitting."'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '[[3]](#_footnoteref_3) Wikipedia article about Julie Beth Lovins: [https://en.wikipedia.org/wiki/Julie_Beth_Lovins](wiki.html)'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '[[4]](#_footnoteref_4) [https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html](htmledition.html)'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '[[5]](#_footnoteref_5) [https://proai.org/middle-button-subreddit](proai.org.html)'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '[[6]](#_footnoteref_6) Robin Jia, *Building Robust NLP Systems* ( [https://robinjia.GitHub.io/assets/pdf/robinjia_thesis.pdf](pdf.html))'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '[[6]](#_footnoteref_6) Robin Jia, *Building Robust NLP Systems* ( [https://robinjia.GitHub.io/assets/pdf/robinjia_thesis.pdf](pdf.html))'
- en: '[[7]](#_footnoteref_7) *Not Even Wrong: The Failure of String Theory and the
    Search for Unity in Physical Law* by Peter Woit'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '[[7]](#_footnoteref_7) *Not Even Wrong: The Failure of String Theory and the
    Search for Unity in Physical Law* by Peter Woit'
- en: '[[8]](#_footnoteref_8) Lex Fridman interview with Peter Woit ( [https://lexfridman.com/peter-woit/](peter-woit.html))'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '[[8]](#_footnoteref_8) Lex Fridman interview with Peter Woit ( [https://lexfridman.com/peter-woit/](peter-woit.html))'
- en: '[[9]](#_footnoteref_9) Rosenblatt, Frank (1957), The perceptron—​a perceiving
    and recognizing automaton. Report 85-460-1, Cornell Aeronautical Laboratory.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '[[9]](#_footnoteref_9) Rosenblatt, Frank (1957), The perceptron—​a perceiving
    and recognizing automaton. Report 85-460-1, Cornell Aeronautical Laboratory.'
- en: '[[10]](#_footnoteref_10) [https://en.wikipedia.org/wiki/Universal_approximation_theorem](wiki.html)'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '[[10]](#_footnoteref_10) [https://en.wikipedia.org/wiki/Universal_approximation_theorem](wiki.html)'
- en: '[[11]](#_footnoteref_11) The logistic activation function can be used to turn
    a linear regression into a logistic regression: ( [https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic.html](linear_model.html))'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '[[11]](#_footnoteref_11) The logistic activation function can be used to turn
    a linear regression into a logistic regression: ( [https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic.html](linear_model.html))'
- en: '[[12]](#_footnoteref_12) [https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression](modules.html)'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '[[12]](#_footnoteref_12) [https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression](modules.html)'
- en: '[[13]](#_footnoteref_13) McElreath, Richard, and Robert Boyd, *Mathematical
    Models of Social Evolution: A guide for the perplexed*, University of Chicago
    Press, 2008.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '[[13]](#_footnoteref_13) McElreath, Richard, and Robert Boyd, *Mathematical
    Models of Social Evolution: A guide for the perplexed*, University of Chicago
    Press, 2008.'
- en: '[[14]](#_footnoteref_14) USCDI (US Core Data Interoperability) ISA (Interoperability
    Standards Advisory) article on "Sex (Assigned at Birth)" ( [https://www.healthit.gov/isa/uscdi-data/sex-assigned-birth](uscdi-data.html))'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '[[14]](#_footnoteref_14) USCDI (US Core Data Interoperability) ISA (Interoperability
    Standards Advisory) article on "Sex (Assigned at Birth)" ( [https://www.healthit.gov/isa/uscdi-data/sex-assigned-birth](uscdi-data.html))'
- en: '[[15]](#_footnoteref_15) from "When I am pinned and wriggling on the wall"
    in "The Love Song of J. Alfred Prufrock" by T. S. Eliot ( [https://www.poetryfoundation.org/poetrymagazine/poems/44212/the-love-song-of-j-alfred-prufrock](44212.html))'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '[[15]](#_footnoteref_15) from "When I am pinned and wriggling on the wall"
    in "The Love Song of J. Alfred Prufrock" by T. S. Eliot ( [https://www.poetryfoundation.org/poetrymagazine/poems/44212/the-love-song-of-j-alfred-prufrock](44212.html))'
- en: '[[16]](#_footnoteref_16) Overview of Coreference Resolution at The Stanford
    Natural Language Processing Group: ( [https://nlp.stanford.edu/projects/coref.shtml](projects.html))'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '[[16]](#_footnoteref_16) Overview of Coreference Resolution at The Stanford
    Natural Language Processing Group: ( [https://nlp.stanford.edu/projects/coref.shtml](projects.html))'
- en: '[[17]](#_footnoteref_17) The Perifpheral by William Gibson on wikipedia ( [https://en.wikipedia.org/wiki/The_Peripheral](wiki.html))'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '[[17]](#_footnoteref_17) The Perifpheral by William Gibson on wikipedia ( [https://en.wikipedia.org/wiki/The_Peripheral](wiki.html))'
- en: '[[18]](#_footnoteref_18) False dichotomy article on wikipedia ( [https://en.wikipedia.org/wiki/False_dilemma](wiki.html))'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '[[18]](#_footnoteref_18) False dichotomy article on wikipedia ( [https://en.wikipedia.org/wiki/False_dilemma](wiki.html))'
- en: '[[19]](#_footnoteref_19) Sarah Goode Wikipedia article ( [https://en.wikipedia.org/wiki/Sarah_E._Goode](wiki.html))'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '[[19]](#_footnoteref_19) Sarah Goode Wikipedia article ( [https://en.wikipedia.org/wiki/Sarah_E._Goode](wiki.html))'
- en: '[[20]](#_footnoteref_20) Ruby Bridges Wikipedia article ( [https://en.wikipedia.org/wiki/Ruby_Bridges](wiki.html))'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '[[20]](#_footnoteref_20) Ruby Bridges Wikipedia article ( [https://en.wikipedia.org/wiki/Ruby_Bridges](wiki.html))'
- en: '[[21]](#_footnoteref_21) "Faster SGD training by minibatch persistency", by
    Fischetti et al ( [https://arxiv.org/pdf/1806.07353.pdf](pdf.html))'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '[[21]](#_footnoteref_21) "Faster SGD training by minibatch persistency", by
    Fischetti et al ( [https://arxiv.org/pdf/1806.07353.pdf](pdf.html))'
- en: '[[22]](#_footnoteref_22) Neural Networks for Machine Learning - Overview of
    mini-batch gradient descent by Geoffrey Hinton ( [https://www.cs.toronto.edu/~hinton/coursera/lecture6/lec6.pdf](lecture6.html))'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '[[22]](#_footnoteref_22) Neural Networks for Machine Learning - Overview of
    mini-batch gradient descent by Geoffrey Hinton ( [https://www.cs.toronto.edu/~hinton/coursera/lecture6/lec6.pdf](lecture6.html))'
- en: '[[23]](#_footnoteref_23) Wikpedia, [https://en.wikipedia.org/wiki/Backpropagation](wiki.html)'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '[[23]](#_footnoteref_23) Wikpedia, [https://en.wikipedia.org/wiki/Backpropagation](wiki.html)'
- en: '[[24]](#_footnoteref_24) Wikipedia list of places below sea level ( [https://en.wikipedia.org/wiki/List_of_places_on_land_with_elevations_below_sea_level](wiki.html))'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '[[24]](#_footnoteref_24) Wikipedia list of places below sea level ( [https://en.wikipedia.org/wiki/List_of_places_on_land_with_elevations_below_sea_level](wiki.html))'
