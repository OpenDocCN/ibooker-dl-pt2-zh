- en: '4 Getting started with neural networks: Classification and regression'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*This chapter covers*'
  prefs: []
  type: TYPE_NORMAL
- en: Your first examples of real-world machine learning workflows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling classification problems over vector data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling continuous regression problems over vector data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This chapter is designed to get you started using neural networks to solve
    real problems. You’ll consolidate the knowledge you gained from chapters 2 and
    3, and you’ll apply what you’ve learned to the following three new tasks covering
    the three most common use cases of neural networks:'
  prefs: []
  type: TYPE_NORMAL
- en: Classifying movie reviews as positive or negative (binary classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying news wires by topic (multiclass classification)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimating the price of a house, given real estate data (scalar regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These examples will be your first contact with end-to-end machine learning
    workflows: you’ll be introduced to data preprocessing, basic model architecture
    principles, and model evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: Classification and regression glossary
  prefs: []
  type: TYPE_NORMAL
- en: 'Classification and regression involve many specialized terms. You’ve come across
    some of them in earlier examples, and you’ll see more of them in future chapters.
    They have the following precise, machine learning–specific definitions, and you
    should be familiar with them:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Sample or input*—One data point that goes into your model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Prediction or output*—What comes out of your model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Target*—The truth. What your model should ideally have predicted, according
    to an external source of data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Prediction error or loss value*—A measure of the distance between your model’s
    prediction and the target.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Classes*—A set of possible labels to choose from in a classification problem.
    For example, when classifying cat and dog pictures, “dog” and “cat” are the two
    classes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Label*—A specific instance of a class annotation in a classification problem.
    For instance, if picture #1234 is annotated as containing the class “dog,” then
    “dog” is a label of picture #1234.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ground-truth or annotations*—All targets for a dataset, typically collected
    by humans.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Binary classification*—A classification task where each input sample should
    be categorized into two exclusive categories.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Multiclass classification*—A classification task where each input sample should
    be categorized into more than two categories, for instance, classifying handwritten
    digits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Multilabel classification*—A classification task where each input sample can
    be assigned multiple labels. For instance, a given image may contain both a cat
    and a dog and should be annotated with both the “cat” label and the “dog” label.
    The number of labels per image is usually variable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Scalar regression*—A task where the target is a continuous scalar value. Predicting
    house prices is a good example: the different target prices form a continuous
    space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Vector regression*—A task where the target is a set of continuous values,
    for example, a continuous vector. If you’re doing regression against multiple
    values (such as the coordinates of a bounding box in an image), then you’re doing
    vector regression.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Mini-batch or batch*—A small set of samples (typically between 8 and 128)
    that are processed simultaneously by the model. The number of samples is often
    a power of 2, to facilitate memory allocation on GPU. When training, a mini-batch
    is used to compute a single gradient descent update applied to the weights of
    the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you’ll be able to use neural networks to handle
    simple classification and regression tasks over vector data. You’ll then be ready
    to start building a more principled, theory-driven understanding of machine learning
    in chapter 5.
  prefs: []
  type: TYPE_NORMAL
- en: '4.1 Classifying movie reviews: A binary classification example'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Two-class classification, or binary classification, is one of the most common
    kinds of machine learning problems. In this example, you’ll learn to classify
    movie reviews as positive or negative, based on the text content of the reviews.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1 The IMDB dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You’ll work with the *IMDB dataset*: a set of 50,000 highly polarized reviews
    from the Internet Movie Database. They’re split into 25,000 reviews for training
    and 25,000 reviews for testing, each set consisting of 50% negative and 50% positive
    reviews.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like the MNIST dataset, the IMDB dataset comes packaged with Keras. It
    has already been preprocessed: the reviews (sequences of words) have been turned
    into sequences of integers, where each integer stands for a specific word in a
    dictionary. This enables us to focus on model building, training, and evaluation.
    In chapter 11, you’ll learn how to process raw text input from scratch.'
  prefs: []
  type: TYPE_NORMAL
- en: The following code will load the dataset (when you run it the first time, about
    80 MB of data will be downloaded to your machine).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.1 Loading the IMDB dataset
  prefs: []
  type: TYPE_NORMAL
- en: library(keras)
  prefs: []
  type: TYPE_NORMAL
- en: imdb <- dataset_imdb(num_words = 10000)
  prefs: []
  type: TYPE_NORMAL
- en: c(c(train_data, train_labels), c(test_data, test_labels)) %<-% imdb
  prefs: []
  type: TYPE_NORMAL
- en: Using the multiassignment (%<-%) operator
  prefs: []
  type: TYPE_NORMAL
- en: The argument num_words = 10000 means you’ll keep only the top 10,000 most frequently
    occurring words in the training data. Rare words will be discarded. This allows
  prefs: []
  type: TYPE_NORMAL
- en: imdb <- dataset_imdb(num_words = 10000)
  prefs: []
  type: TYPE_NORMAL
- en: train_data <- imdb$train$x
  prefs: []
  type: TYPE_NORMAL
- en: train_labels <- imdb$train$y
  prefs: []
  type: TYPE_NORMAL
- en: test_data <- imdb$test$x
  prefs: []
  type: TYPE_NORMAL
- en: test_labels <- imdb$test$y
  prefs: []
  type: TYPE_NORMAL
- en: 'The datasets built into Keras are all nested lists of training and test data.
    Here, we use the multiassignment operator (%<-%) from the zeallot package to unpack
    the list into a set of distinct variables. This could equally be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The multiassignment version is preferable because it’s more compact. The %<-%
    operator is automatically available whenever the R Keras package is attached.
    us to work with vector data of manageable size. If we didn’t set this limit, we’d
    be working with 88,585 unique words in the training data, which is unnecessarily
    large. Many of these words occur only in a single sample and thus can’t be meaningfully
    used for classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'The variables train_data and test_data are lists of reviews; each review is
    a list of word indices (encoding a sequence of words). train_labels and test_labels
    are lists of 0s and 1s, where 0 stands for *negative* and 1 stands for *positive*:'
  prefs: []
  type: TYPE_NORMAL
- en: str(train_data)
  prefs: []
  type: TYPE_NORMAL
- en: List of 25000
  prefs: []
  type: TYPE_NORMAL
- en: '$ : int [1:218] 1 14 22 16 43 530 973 1622 1385 65 …'
  prefs: []
  type: TYPE_NORMAL
- en: '$ : int [1:189] 1 194 1153 194 8255 78 228 5 6 1463 …'
  prefs: []
  type: TYPE_NORMAL
- en: '$ : int [1:141] 1 14 47 8 30 31 7 4 249 108 …'
  prefs: []
  type: TYPE_NORMAL
- en: '$ : int [1:550] 1 4 2 2 33 2804 4 2040 432 111 …'
  prefs: []
  type: TYPE_NORMAL
- en: '$ : int [1:147] 1 249 1323 7 61 113 10 10 13 1637 …'
  prefs: []
  type: TYPE_NORMAL
- en: '$ : int [1:43] 1 778 128 74 12 630 163 15 4 1766 …'
  prefs: []
  type: TYPE_NORMAL
- en: '$ : int [1:123] 1 6740 365 1234 5 1156 354 11 14 5327 …'
  prefs: []
  type: TYPE_NORMAL
- en: '$ : int [1:562] 1 4 2 716 4 65 7 4 689 4367 …'
  prefs: []
  type: TYPE_NORMAL
- en: '[list output truncated]'
  prefs: []
  type: TYPE_NORMAL
- en: str(train_labels)
  prefs: []
  type: TYPE_NORMAL
- en: int [1:25000] 1 0 0 1 0 0 1 0 1 0 …
  prefs: []
  type: TYPE_NORMAL
- en: 'Because we’re restricting ourselves to the top 10,000 most frequent words,
    no word index will exceed 10,000:'
  prefs: []
  type: TYPE_NORMAL
- en: max(sapply(train_data, max))
  prefs: []
  type: TYPE_NORMAL
- en: '[1] 9999'
  prefs: []
  type: TYPE_NORMAL
- en: For kicks, here’s how you can quickly decode one of these reviews back to English
    words.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.2 Decoding reviews back to text
  prefs: []
  type: TYPE_NORMAL
- en: word_index <- dataset_imdb_word_index()➊
  prefs: []
  type: TYPE_NORMAL
- en: reverse_word_index <- names(word_index)➋
  prefs: []
  type: TYPE_NORMAL
- en: names(reverse_word_index) <- as.character(word_index)➋
  prefs: []
  type: TYPE_NORMAL
- en: decoded_words <- train_data[[1]] %>%
  prefs: []
  type: TYPE_NORMAL
- en: sapply(function(i) {
  prefs: []
  type: TYPE_NORMAL
- en: if (i > 3) reverse_word_index[[as.character(i - 3)]]➌
  prefs: []
  type: TYPE_NORMAL
- en: else "?"
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  prefs: []
  type: TYPE_NORMAL
- en: decoded_review <- paste0(decoded_words, collapse = " ")
  prefs: []
  type: TYPE_NORMAL
- en: cat(decoded_review, "\n")
  prefs: []
  type: TYPE_NORMAL
- en: '? this film was just brilliant casting location scenery story direction everyone''s
    really suited the part they played and you could just imagine being there robert
    ? is an amazing actor and now the same being director …'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **word_index is a named vector mapping words to an integer index.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Reverses it, mapping integer indices to words**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Decodes the review. Note that the indices are offset by 3 because 0, 1,
    and 2 are reserved indices for "padding," "start of sequence," and "unknown."**
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 Preparing the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can’t directly feed lists of integers into a neural network. They all have
    different lengths, but a neural network expects to process contiguous batches
    of data. You have to turn your lists into tensors. You can do that in the following
    two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Pad your lists so that they all have the same length, turn them into an integer
    tensor of shape (samples, max_length), and start your model with a layer capable
    of handling such integer tensors (the Embedding layer, which we’ll cover in detail
    later in the book).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Multi-hot encode* your lists to turn them into vectors of 0s and 1s. This
    would mean, for instance, turning the sequence [8, 5] into a 10,000-dimensional
    vector that would be all 0s except for indices 8 and 5, which would be 1s. Then
    you could use a layer_dense(), capable of handling floating-point vector data,
    as the first layer in your model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s go with the latter solution to vectorize the data, which you’ll do manually
    for maximum clarity.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.3 Encoding the integer sequences via multi-hot encoding**'
  prefs: []
  type: TYPE_NORMAL
- en: vectorize_sequences <- function(sequences, dimension = 10000)
  prefs: []
  type: TYPE_NORMAL
- en: '{ results <- array(0, dim = c(length(sequences), dimension))➊'
  prefs: []
  type: TYPE_NORMAL
- en: for (i in seq_along(sequences)) {
  prefs: []
  type: TYPE_NORMAL
- en: sequence <- sequences[[i]]
  prefs: []
  type: TYPE_NORMAL
- en: for (j in sequence)
  prefs: []
  type: TYPE_NORMAL
- en: results[i, j] <- 1➋
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: results
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: x_train <- vectorize_sequences(train_data)➌
  prefs: []
  type: TYPE_NORMAL
- en: x_test <- vectorize_sequences(test_data)➍
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Create an all-zero matrix of shape (length(sequences), dimension).**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Set specific indices of results to 1s.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Vectorize training data.**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Vectorize test data.**
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s what the samples look like now:'
  prefs: []
  type: TYPE_NORMAL
- en: str(x_train)
  prefs: []
  type: TYPE_NORMAL
- en: num [1:25000, 1:10000] 1 1 1 1 1 1 1 1 1 1 …
  prefs: []
  type: TYPE_NORMAL
- en: 'You should also vectorize your labels, which is a straightforward cast of integers
    to floats:'
  prefs: []
  type: TYPE_NORMAL
- en: y_train <- as.numeric(train_labels)
  prefs: []
  type: TYPE_NORMAL
- en: y_test <- as.numeric(test_labels)
  prefs: []
  type: TYPE_NORMAL
- en: Now the data is ready to be fed into a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.3 Building your model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The input data is vectors, and the labels are scalars (1s and 0s): this is
    one of the simplest problem setups you’ll ever encounter. A type of model that
    performs well on such a problem is a plain stack of densely connected layers (layer_dense())
    with relu activations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You need to make two key architecture decisions about such a stack of dense
    layers:'
  prefs: []
  type: TYPE_NORMAL
- en: How many layers to us
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many units to choose for each laye
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Image](../images/f0108-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 4.1 The three-layer model**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In chapter 5, you’ll learn formal principles to guide you in making these choices.
    For the time being, you’ll have to trust me with the following architecture choices:'
  prefs: []
  type: TYPE_NORMAL
- en: Two intermediate layers with 16 units each
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A third layer that will output the scalar prediction regarding the sentiment
    of the current revie
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Figure 4.1](#fig4-1) shows what the model looks like. And the following listing
    shows the Keras implementation, similar to the MNIST example you saw previously.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.4 Model definition**'
  prefs: []
  type: TYPE_NORMAL
- en: model <- keras_model_sequential() %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(16, activation = "relu") %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(16, activation = "relu") %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(1, activation = "sigmoid")
  prefs: []
  type: TYPE_NORMAL
- en: 'The first argument being passed to each layer_dense() is the number of *units*
    in the layer: the dimensionality of representation space of the layer. Remember
    from chapters 2 and 3 that each such layer_dense() with a relu activation implements
    the following chain of tensor operations:'
  prefs: []
  type: TYPE_NORMAL
- en: output <- relu(dot(input, W) + b)
  prefs: []
  type: TYPE_NORMAL
- en: 'Having 16 units means the weight matrix W will have shape (input_dimension,
    16): the dot product with W will project the input data onto a 16-dimensional
    representation space (and then you’ll add the bias vector b and apply the relu
    operation). You can intuitively understand the dimensionality of your representation
    space as “how much freedom you’re allowing the model to have when learning internal
    representations.” Having more units (a higher-dimensional representation space)
    allows your model to learn more complex representations, but it makes the model
    more computationally expensive and may lead to learning unwanted patterns (patterns
    that will improve performance on the training data but not on the test data).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The intermediate layers use relu as their activation function, and the final
    layer uses a sigmoid activation so as to output a probability (a score between
    0 and 1 indicating how likely the sample is to have the target “1”: how likely
    the review is to be positive). A relu (rectified linear unit) is a function meant
    to zero out negative values (see [figure 4.2](#fig4-2)), whereas a sigmoid “squashes”
    arbitrary values into the [0, 1] interval (see [figure 4.3](#fig4-3)), outputting
    something that can be interpreted as a probability.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0109-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 4.2 The relu function**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0109-02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 4.3 The sigmoid function**'
  prefs: []
  type: TYPE_NORMAL
- en: What are activation functions, and why are they necessary?
  prefs: []
  type: TYPE_NORMAL
- en: 'Without an activation function like relu (also called a *nonlinearity*), layer_dense
    would consist of two linear operations—a dot product and an addition:'
  prefs: []
  type: TYPE_NORMAL
- en: output <- dot(input, W) + b
  prefs: []
  type: TYPE_NORMAL
- en: 'The layer could learn only *linear transformations* (affine transformations)
    of the input data: the *hypothesis space* of the layer would be the set of all
    possible linear transformations of the input data into a 16-dimensional space.
    Such a hypothesis space is too restricted and wouldn’t benefit from multiple layers
    of representations, because a deep stack of linear layers would still implement
    a linear operation: adding more layers wouldn’t extend the hypothesis space (as
    you saw in chapter 2).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To get access to a much richer hypothesis space that will benefit from deep
    representations, you need a nonlinearity, or activation function. relu is the
    most popular activation function in deep learning, but many other candidates exist,
    which all come with similarly strange names: prelu, elu, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, you need to choose a loss function and an optimizer. Because you’re
    facing a binary classification problem and the output of your model is a probability
    (you end your model with a single-unit layer with a sigmoid activation), it’s
    best to use the binary_crossentropy loss. It isn’t the only viable choice: for
    instance, you could use mean_squared_error. But cross-entropy is usually the best
    choice when you’re dealing with models that output probabilities. *Cross-entropy*
    is a quantity from the field of information theory that measures the distance
    between probability distributions or, in this case, between the ground-truth distribution
    and your predictions. As for the choice of the optimizer, we’ll go with rmsprop,
    which is a usually a good default choice for virtually any problem.'
  prefs: []
  type: TYPE_NORMAL
- en: Here’s the step where we configure the model with the rmsprop optimizer and
    the binary_crossentropy loss function. Note that we’ll also monitor accuracy during
    training.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.5 Compiling the model**'
  prefs: []
  type: TYPE_NORMAL
- en: model %>% compile(optimizer = "rmsprop",
  prefs: []
  type: TYPE_NORMAL
- en: loss = "binary_crossentropy",
  prefs: []
  type: TYPE_NORMAL
- en: metrics = "accuracy")
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.4 Validating your approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As you learned in chapter 3, a deep learning model should never be evaluated
    on its training data—it’s standard practice to use a validation set to monitor
    the accuracy of the model during training. Here, we’ll create a validation set
    by setting apart 10,000 samples from the original training data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.6 Setting aside a validation set**'
  prefs: []
  type: TYPE_NORMAL
- en: x_val <- x_train[seq(10000), ]
  prefs: []
  type: TYPE_NORMAL
- en: partial_x_train <- x_train[-seq(10000), ]
  prefs: []
  type: TYPE_NORMAL
- en: y_val <- y_train[seq(10000)]
  prefs: []
  type: TYPE_NORMAL
- en: partial_y_train <- y_train[-seq(10000)]
  prefs: []
  type: TYPE_NORMAL
- en: We will now train the model for 20 epochs (20 iterations over all samples in
    the training data) in mini-batches of 512 samples. At the same time, we will monitor
    loss and accuracy on the 10,000 samples that we set apart. We do so by passing
    the validation data as the validation_data argument.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.7 Training your model**'
  prefs: []
  type: TYPE_NORMAL
- en: history <- model %>% fit(
  prefs: []
  type: TYPE_NORMAL
- en: partial_x_train,
  prefs: []
  type: TYPE_NORMAL
- en: partial_y_train,
  prefs: []
  type: TYPE_NORMAL
- en: epochs = 20,
  prefs: []
  type: TYPE_NORMAL
- en: batch_size = 512,
  prefs: []
  type: TYPE_NORMAL
- en: validation_data = list(x_val, y_val)
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: On a CPU, this will take less than 2 seconds per epoch—training is over in 20
    seconds. At the end of every epoch is a slight pause as the model computes its
    loss and accuracy on the 10,000 samples of the validation data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the call to model %>% fit() returns a history object, as you saw
    in chapter 3\. This object has a member metrics, which is a named list containing
    data about everything that happened during training. Let’s look at it:'
  prefs: []
  type: TYPE_NORMAL
- en: str(history$metrics)
  prefs: []
  type: TYPE_NORMAL
- en: List of 4
  prefs: []
  type: TYPE_NORMAL
- en: '$ loss : num [1:20] 0.526 0.326 0.241 0.191 0.154 …'
  prefs: []
  type: TYPE_NORMAL
- en: '$ accuracy : num [1:20] 0.799 0.899 0.921 0.937 0.951 …'
  prefs: []
  type: TYPE_NORMAL
- en: '$ val_loss : num [1:20] 0.415 0.327 0.286 0.276 0.285 …'
  prefs: []
  type: TYPE_NORMAL
- en: '$ val_accuracy: num [1:20] 0.857 0.876 0.891 0.89 0.886 …'
  prefs: []
  type: TYPE_NORMAL
- en: 'The metrics list contains four entries: one per metric that was being monitored
    during training and during validation. We’ll use the plot() method for the history
    object to plot the training and validation loss side by side, as well as the training
    and validation accuracy (see [figure 4.4](#fig4-4)). Note that your own results
    may vary slightly due to a different random initialization of your model.'
  prefs: []
  type: TYPE_NORMAL
- en: plot(history)
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0111-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 4.4 Training and validation loss and accuracy metrics**'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, the training loss decreases with every epoch, and the training
    accuracy increases with every epoch. That’s what you would expect when running
    gradient descent optimization—the quantity you’re trying to minimize should be
    less with every iteration. But that isn’t the case for the validation loss and
    accuracy: they seem to peak at the fourth epoch. This is an example of what we
    warned against earlier: a model that performs better on the training data isn’t
    necessarily a model that will do better on data it has never seen before. In precise
    terms, what you’re seeing is *overfitting*: after the fourth epoch, you’re overoptimizing
    on the training data, and you end up learning representations that are specific
    to the training data and don’t generalize to data outside of the training set.'
  prefs: []
  type: TYPE_NORMAL
- en: Training history plot() method
  prefs: []
  type: TYPE_NORMAL
- en: 'The plot() method for training history objects uses ggplot2 for plotting if
    it’s available (if it isn’t, base graphics are used). The plot includes all specified
    metrics as well as the loss; it draws a smoothing line if there are 10 or more
    epochs. You can customize all of this behavior via various arguments to the plot()
    method. If you want to create a custom visualization, call the as.data.frame()
    method on history to obtain a data frame with factors for each metric as well
    as training versus validation:'
  prefs: []
  type: TYPE_NORMAL
- en: history_df <- as.data.frame(history)
  prefs: []
  type: TYPE_NORMAL
- en: str(history_df)
  prefs: []
  type: TYPE_NORMAL
- en: '''data.frame'': 80 obs. of 4 variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '$ epoch : int 1 2 3 4 5 6 7 8 9 10 …'
  prefs: []
  type: TYPE_NORMAL
- en: '$ value : num 0.526 0.326 0.241 0.191 0.154 …'
  prefs: []
  type: TYPE_NORMAL
- en: '$ metric: Factor w/ 2 levels "loss","accuracy": 1 1 1 1 1 1 1 1 1 1 …'
  prefs: []
  type: TYPE_NORMAL
- en: '$ data : Factor w/ 2 levels "training","validation": 1 1 1 1 1 1 1 …'
  prefs: []
  type: TYPE_NORMAL
- en: In this case, to prevent overfitting, you could stop training after four epochs.
    In general, you can use a range of techniques to mitigate overfitting, which we’ll
    cover in chapter 5\. Let’s train a new model from scratch for four epochs and
    then evaluate it on the test data.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.8 Retraining a model from scratch
  prefs: []
  type: TYPE_NORMAL
- en: model <- keras_model_sequential() %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(16, activation = "relu") %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(16, activation = "relu") %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(1, activation = "sigmoid")
  prefs: []
  type: TYPE_NORMAL
- en: model %>% compile(optimizer = "rmsprop",
  prefs: []
  type: TYPE_NORMAL
- en: loss = "binary_crossentropy",
  prefs: []
  type: TYPE_NORMAL
- en: metrics = "accuracy")
  prefs: []
  type: TYPE_NORMAL
- en: model %>% fit(x_train, y_train, epochs = 4, batch_size = 512)
  prefs: []
  type: TYPE_NORMAL
- en: results <- model %>% evaluate(x_test, y_test)
  prefs: []
  type: TYPE_NORMAL
- en: 'The final results are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: results
  prefs: []
  type: TYPE_NORMAL
- en: loss  accuracy
  prefs: []
  type: TYPE_NORMAL
- en: 0.2999835 0.8819600➊
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **The first number, 0.29, is the test loss, and the second number, 0.88, is
    the test accuracy.**
  prefs: []
  type: TYPE_NORMAL
- en: This fairly naive approach achieves an accuracy of 88%. With state-of-the-art
    approaches, you should be able to get close to 95%.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.5 Using a trained model to generate predictions on new data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After having trained a model, you’ll want to use it in a practical setting.
    You can generate the likelihood of reviews being positive by using the predict()
    method, as you learned in chapter 3:'
  prefs: []
  type: TYPE_NORMAL
- en: model %>% predict(x_test)
  prefs: []
  type: TYPE_NORMAL
- en: '[,1]'
  prefs: []
  type: TYPE_NORMAL
- en: '[1,] 0.20960191'
  prefs: []
  type: TYPE_NORMAL
- en: '[2,] 0.99959260'
  prefs: []
  type: TYPE_NORMAL
- en: '[3,] 0.93098557'
  prefs: []
  type: TYPE_NORMAL
- en: '[4,] 0.83782458'
  prefs: []
  type: TYPE_NORMAL
- en: '[5,] 0.94010764'
  prefs: []
  type: TYPE_NORMAL
- en: '[6,] 0.79225385'
  prefs: []
  type: TYPE_NORMAL
- en: '[7,] 0.99964178'
  prefs: []
  type: TYPE_NORMAL
- en: '[8,] 0.01294626'
  prefs: []
  type: TYPE_NORMAL
- en: …
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the model is confident for some samples (0.99 or more, or 0.01
    or less) but less confident for others (0.6, 0.4).
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.6 Further experiments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following experiments will help convince you that the architecture choices
    you’ve made are all fairly reasonable, although there’s still room for improvement:'
  prefs: []
  type: TYPE_NORMAL
- en: You used two representation layers before the final classification layer. Try
    using one or three representation layers, and see how doing so affects validation
    and test accuracy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Try using layers with more units or fewer units: 32 units, 64 units, and so
    on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try using the mse loss function instead of binary_crossentropy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try using the tanh activation (an activation that was popular in the early days
    of neural networks) instead of relu
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4.1.7 Wrapping up
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here’s what you should take away from this example:'
  prefs: []
  type: TYPE_NORMAL
- en: You usually need to do quite a bit of preprocessing on your raw data to be able
    to feed it—as tensors—into a neural network. Sequences of words can be encoded
    as binary vectors, but other encoding options are also available
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stacks of layer_dense() with relu activations can solve a wide range of problems
    (including sentiment classification), and you’ll likely use them frequently.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In a binary classification problem (two output classes), your model should
    end with a layer_dense() with one unit and a sigmoid activation: the output of
    your model should be a scalar between 0 and 1, encoding a probability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With such a scalar sigmoid output on a binary classification problem, the loss
    function you should use is binary_crossentropy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The rmsprop optimizer is generally a good enough choice, whatever your problem.
    That’s one less thing for you to worry about.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As they get better on their training data, neural networks eventually start
    over-fitting and end up obtaining increasingly worse results on data they’ve never
    seen before. Be sure to always monitor performance on data that is outside of
    the training set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '4.2 Classifying newswires: A multiclass classification example'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, you saw how to classify vector inputs into two mutually
    exclusive classes using a densely connected neural network. But what happens when
    you have more than two classes?
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ll build a model to classify Reuters newswires into 46 mutually
    exclusive topics. Because we have many classes, this problem is an instance of
    *multi-class classification*, and because each data point should be classified
    into only one category, the problem is more specifically an instance of *single-label
    multiclass classification*. If each data point could belong to multiple categories
    (in this case, topics), we’d be facing a *multilabel multiclass classification*
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 The Reuters dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You’ll work with the *Reuters dataset*, a set of short newswires and their topics,
    published by Reuters in 1986\. It’s a simple, widely used toy dataset for text
    classification. The dataset contains 46 different topics; some topics are more
    represented than others, but each topic has at least 10 examples in the training
    set. Like IMDB and MNIST, the Reuters dataset comes packaged as part of Keras.
    Let’s take a look.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.9 Loading the Reuters dataseta
  prefs: []
  type: TYPE_NORMAL
- en: reuters <- dataset_reuters(num_words = 10000)
  prefs: []
  type: TYPE_NORMAL
- en: c(c(train_data, train_labels), c(test_data, test_labels)) %<-% reuters
  prefs: []
  type: TYPE_NORMAL
- en: 'As with the IMDB dataset, the argument num_words = 10000 restricts the data
    to the 10,000 most frequently occurring words found in the data. You have 8,982
    training examples and 2,246 test examples:'
  prefs: []
  type: TYPE_NORMAL
- en: length(train_data)
  prefs: []
  type: TYPE_NORMAL
- en: '[1] 8982'
  prefs: []
  type: TYPE_NORMAL
- en: length(test_data)
  prefs: []
  type: TYPE_NORMAL
- en: '[1] 2246'
  prefs: []
  type: TYPE_NORMAL
- en: 'As with the IMDB reviews, each example is a list of integers (word indices):'
  prefs: []
  type: TYPE_NORMAL
- en: str(train_data)
  prefs: []
  type: TYPE_NORMAL
- en: List of 8982
  prefs: []
  type: TYPE_NORMAL
- en: '$ : int [1:87] 1 2 2 8 43 10 447 5 25 207 …'
  prefs: []
  type: TYPE_NORMAL
- en: '$ : int [1:56] 1 3267 699 3434 2295 56 2 7511 9 56 …'
  prefs: []
  type: TYPE_NORMAL
- en: '$ : int [1:139] 1 53 12 284 15 14 272 26 53 959 …'
  prefs: []
  type: TYPE_NORMAL
- en: '$ : int [1:224] 1 4 686 867 558 4 37 38 309 2276 …'
  prefs: []
  type: TYPE_NORMAL
- en: '$ : int [1:101] 1 8295 111 8 25 166 40 638 10 436 …'
  prefs: []
  type: TYPE_NORMAL
- en: '$ : int [1:116] 1 4 37 38 309 213 349 1632 48 193 …'
  prefs: []
  type: TYPE_NORMAL
- en: '$ : int [1:100] 1 56 5539 925 149 8 16 23 931 3875 …'
  prefs: []
  type: TYPE_NORMAL
- en: '$ : int [1:100] 1 53 648 26 14 749 26 39 6207 5466 …'
  prefs: []
  type: TYPE_NORMAL
- en: '[list output truncated]'
  prefs: []
  type: TYPE_NORMAL
- en: Here’s how you can decode it back to words, in case you’re curious.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.10 Decoding newswires back to text
  prefs: []
  type: TYPE_NORMAL
- en: word_index <- dataset_reuters_word_index()
  prefs: []
  type: TYPE_NORMAL
- en: reverse_word_index <- names(word_index)
  prefs: []
  type: TYPE_NORMAL
- en: names(reverse_word_index) <- as.character(word_index)
  prefs: []
  type: TYPE_NORMAL
- en: decoded_words <- train_data[[1]] %>%
  prefs: []
  type: TYPE_NORMAL
- en: sapply(function(i) {
  prefs: []
  type: TYPE_NORMAL
- en: if (i > 3) reverse_word_index[[as.character(i - 3)]]➊
  prefs: []
  type: TYPE_NORMAL
- en: else "?"
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  prefs: []
  type: TYPE_NORMAL
- en: decoded_review <- paste0(decoded_words, collapse = " ")
  prefs: []
  type: TYPE_NORMAL
- en: decoded_review
  prefs: []
  type: TYPE_NORMAL
- en: '[1] "? ? ? said as a result of its december acquisition of space co it'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common01.jpg) expects …"'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Note that the indices are offset by 3 because 0, 1, and 2 are reserved indices
    for "padding," "start of sequence," and "unknown."**
  prefs: []
  type: TYPE_NORMAL
- en: 'The label associated with an example is an integer between 0 and 45—a topic
    index:'
  prefs: []
  type: TYPE_NORMAL
- en: str(train_labels)
  prefs: []
  type: TYPE_NORMAL
- en: int [1:8982] 3 4 3 4 4 4 4 3 3 16 …
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Preparing the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can vectorize the data with the same approach as in the previous example.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.11 Encoding the input data
  prefs: []
  type: TYPE_NORMAL
- en: vectorize_sequences <- function(sequences, dimension = 10000) {
  prefs: []
  type: TYPE_NORMAL
- en: results <- matrix(0, nrow = length(sequences), ncol = dimension)
  prefs: []
  type: TYPE_NORMAL
- en: for (i in seq_along(sequences))
  prefs: []
  type: TYPE_NORMAL
- en: results[i, sequences[[i]]] <- 1
  prefs: []
  type: TYPE_NORMAL
- en: results
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: x_train <- vectorize_sequences(train_data)➊
  prefs: []
  type: TYPE_NORMAL
- en: x_test <- vectorize_sequences(test_data)➋
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Vectorized training data**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Vectorized test data**
  prefs: []
  type: TYPE_NORMAL
- en: 'To vectorize the labels, you have two possibilities: you can cast the label
    list as an integer tensor, or you can use *one-hot encoding*. One-hot encoding
    is a widely used format for categorical data, also called *categorical encoding*.
    In this case, one-hot encoding of the labels consists of embedding each label
    as an all-zero vector with a 1 in the place of the label index. The following
    listing shows an example.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.12 Encoding the labels
  prefs: []
  type: TYPE_NORMAL
- en: to_one_hot <- function(labels, dimension = 46) {
  prefs: []
  type: TYPE_NORMAL
- en: results <- matrix(0, nrow = length(labels), ncol = dimension)
  prefs: []
  type: TYPE_NORMAL
- en: labels <- labels + 1➊
  prefs: []
  type: TYPE_NORMAL
- en: for(i in seq_along(labels)) {
  prefs: []
  type: TYPE_NORMAL
- en: j <- labels[[i]]
  prefs: []
  type: TYPE_NORMAL
- en: results[i, j] <- 1
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: results
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: y_train <- to_one_hot(train_labels)➋
  prefs: []
  type: TYPE_NORMAL
- en: y_test <- to_one_hot(test_labels)➌
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Vectorized training labels**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Vectorized test labels**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Labels are 0-based**a
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that there is a built-in way to do this in Keras:'
  prefs: []
  type: TYPE_NORMAL
- en: y_train <- to_categorical(train_labels)
  prefs: []
  type: TYPE_NORMAL
- en: y_test <- to_categorical(test_labels)
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3 Building your model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This topic-classification problem looks similar to the previous movie-review
    classification problem: in both cases, we’re trying to classify short snippets
    of text. But there is a new constraint here: the number of output classes has
    gone from 2 to 46\. The dimensionality of the output space is much larger.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In a stack of layer_dense()s like those we’ve been using, each layer can access
    only information present in the output of the previous layer. If one layer drops
    some information relevant to the classification problem, this information can
    never be recovered by later layers: each layer can potentially become an information
    bottleneck. In the previous example, we used 16-dimensional intermediate layers,
    but a 16-dimensional space may be too limited to learn to separate 46 different
    classes: such small layers may act as information bottlenecks, permanently dropping
    relevant information. For this reason we’ll use larger layers. Let’s go with 64
    units.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.13 Model definition
  prefs: []
  type: TYPE_NORMAL
- en: model <- keras_model_sequential() %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(64, activation = "relu") %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(64, activation = "relu") %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(46, activation = "softmax")
  prefs: []
  type: TYPE_NORMAL
- en: You should note two other things about this architecture. First, we end the
    model with a layer_dense() of size 46\. This means for each input sample, the
    network will output a 46-dimensional vector. Each entry in this vector (each dimension)
    will encode a different output class.
  prefs: []
  type: TYPE_NORMAL
- en: Second, the last layer uses a softmax activation. You saw this pattern in the
    MNIST example. It means the model will output a *probability distribution* over
    the 46 different output classes—for every input sample, the model will produce
    a 46-dimensional output vector, where output[i] is the probability that the sample
    belongs to class i. The 46 scores will sum to 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'The best loss function to use in this case is categorical_crossentropy. It
    measures the distance between two probability distributions: here, between the
    probability distribution output by the model and the true distribution of the
    labels. By minimizing the distance between these two distributions, you train
    the model to output something as close as possible to the true labels.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.14 Compiling the model
  prefs: []
  type: TYPE_NORMAL
- en: model %>% compile(optimizer = "rmsprop",
  prefs: []
  type: TYPE_NORMAL
- en: loss = "categorical_crossentropy",
  prefs: []
  type: TYPE_NORMAL
- en: metrics = "accuracy")
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.4 Validating your approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s set apart 1,000 samples in the training data to use as a validation set.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.15 Setting aside a validation set
  prefs: []
  type: TYPE_NORMAL
- en: val_indices <- 1:1000
  prefs: []
  type: TYPE_NORMAL
- en: x_val <- x_train[val_indices, ]
  prefs: []
  type: TYPE_NORMAL
- en: partial_x_train <- x_train[-val_indices, ]
  prefs: []
  type: TYPE_NORMAL
- en: y_val <- y_train[val_indices, ]
  prefs: []
  type: TYPE_NORMAL
- en: partial_y_train <- y_train[-val_indices, ]
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s train the model for 20 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.16 Training the model
  prefs: []
  type: TYPE_NORMAL
- en: history <- model %>% fit(
  prefs: []
  type: TYPE_NORMAL
- en: partial_x_train,
  prefs: []
  type: TYPE_NORMAL
- en: partial_y_train,
  prefs: []
  type: TYPE_NORMAL
- en: epochs = 20,
  prefs: []
  type: TYPE_NORMAL
- en: batch_size = 512,
  prefs: []
  type: TYPE_NORMAL
- en: validation_data = list(x_val, y_val)
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: And finally, let’s display its loss and accuracy curves (see [figure 4.5](#fig4-5)).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.17 Plotting the training and validation loss and accuracy
  prefs: []
  type: TYPE_NORMAL
- en: plot(history)
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0118-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 4.5 Loss and accuracy curves of the validation set**'
  prefs: []
  type: TYPE_NORMAL
- en: The model begins to overfit after nine epochs. Let’s train a new model from
    scratch for nine epochs and then evaluate it on the test set.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.18 Retraining a model from scratch
  prefs: []
  type: TYPE_NORMAL
- en: model <- keras_model_sequential() %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(64, activation = "relu") %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(64, activation = "relu") %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(46, activation = "softmax")
  prefs: []
  type: TYPE_NORMAL
- en: model %>% compile(optimizer = "rmsprop",
  prefs: []
  type: TYPE_NORMAL
- en: loss = "categorical_crossentropy",
  prefs: []
  type: TYPE_NORMAL
- en: metrics = "accuracy")
  prefs: []
  type: TYPE_NORMAL
- en: model %>% fit(x_train, y_train, epochs = 9, batch_size = 512)
  prefs: []
  type: TYPE_NORMAL
- en: results <- model %>% evaluate(x_test, y_test)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the final results:'
  prefs: []
  type: TYPE_NORMAL
- en: results
  prefs: []
  type: TYPE_NORMAL
- en: loss  accuracy
  prefs: []
  type: TYPE_NORMAL
- en: 0.9562974 0.7898486
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach reaches an accuracy of ~80%. With a balanced binary classification
    problem, the accuracy reached by a purely random classifier would be 50%. But
    in this case, we have 46 classes, and they may not be equally represented. What
    would be the accuracy of a random baseline? We could try quickly checking this
    empirically:'
  prefs: []
  type: TYPE_NORMAL
- en: mean(test_labels == sample(test_labels))
  prefs: []
  type: TYPE_NORMAL
- en: '[1] 0.190561'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, a random classifier would score around 19% classification accuracy,
    so the results of our model seem pretty good in that light.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.5 Generating predictions on new data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Calling the model’s predict() method on new samples returns a class probability
    distribution over all 46 topics for each sample. Let’s generate topic predictions
    for all of the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: predictions <- model %>% predict(x_test)
  prefs: []
  type: TYPE_NORMAL
- en: 'Each entry (row) in “predictions” is a vector of length 46:'
  prefs: []
  type: TYPE_NORMAL
- en: str(predictions)
  prefs: []
  type: TYPE_NORMAL
- en: num [1:2246, 1:46] 0.0000873 0.0013171 0.0094679 0.0001123 0.0001032 …
  prefs: []
  type: TYPE_NORMAL
- en: 'The coefficients in this vector sum to 1, because they form a probability distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: sum(predictions[1, ])
  prefs: []
  type: TYPE_NORMAL
- en: '[1] 1'
  prefs: []
  type: TYPE_NORMAL
- en: 'The largest entry is the predicted class—the class with the highest probability:'
  prefs: []
  type: TYPE_NORMAL
- en: which.max(predictions[1, ])
  prefs: []
  type: TYPE_NORMAL
- en: '[1] 5'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.6 A different way to handle the labels and the loss
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We mentioned earlier that another way to encode the labels would be to preserve
    their integer values, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: y_train <- train_labels
  prefs: []
  type: TYPE_NORMAL
- en: y_test <- test_labels
  prefs: []
  type: TYPE_NORMAL
- en: 'The only thing this approach would change is the choice of the loss function.
    The loss function used in listing 4.18, categorical_crossentropy, expects the
    labels to follow a categorical encoding. With integer labels, you should use sparse_categorical_
    crossentropy:'
  prefs: []
  type: TYPE_NORMAL
- en: model %>% compile(
  prefs: []
  type: TYPE_NORMAL
- en: optimizer = "rmsprop",
  prefs: []
  type: TYPE_NORMAL
- en: loss = "sparse_categorical_crossentropy",
  prefs: []
  type: TYPE_NORMAL
- en: metrics = "accuracy")
  prefs: []
  type: TYPE_NORMAL
- en: This new loss function is still mathematically the same as categorical_crossentropy;
    it just has a different interface.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.7 The importance of having sufficiently large intermediate layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We mentioned earlier that because the final outputs are 46-dimensional, you
    should avoid intermediate layers with many fewer than 46 units. Now let’s see
    what happens when we introduce an information bottleneck by having intermediate
    layers that are significantly less than 46-dimensional: for example, 4-dimensional.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.19 A model with an information bottleneck
  prefs: []
  type: TYPE_NORMAL
- en: model <- keras_model_sequential() %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(64, activation = "relu") %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(4, activation = "relu") %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(46, activation = "softmax")
  prefs: []
  type: TYPE_NORMAL
- en: model %>% compile(optimizer = "rmsprop",
  prefs: []
  type: TYPE_NORMAL
- en: loss = "categorical_crossentropy",
  prefs: []
  type: TYPE_NORMAL
- en: metrics = "accuracy")
  prefs: []
  type: TYPE_NORMAL
- en: model %>% fit(
  prefs: []
  type: TYPE_NORMAL
- en: partial_x_train,
  prefs: []
  type: TYPE_NORMAL
- en: partial_y_train,
  prefs: []
  type: TYPE_NORMAL
- en: epochs = 20,
  prefs: []
  type: TYPE_NORMAL
- en: batch_size = 128,
  prefs: []
  type: TYPE_NORMAL
- en: validation_data = list(x_val, y_val)
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: The model now peaks at ~71% validation accuracy, an 8% absolute drop. This drop
    is mostly because we’re trying to compress a lot of information (enough information
    to recover the separation hyperplanes of 46 classes) into an intermediate space
    that is too low-dimensional. The model is able to cram *most* of the necessary
    information into these 4-dimensional representations, but not all of it.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.8 Further experiments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Like in the previous example, I encourage you to try out the following experiments
    to train your intuition about the kind of configuration decisions you have to
    make with such models:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Try using larger or smaller layers: 32 units, 128 units, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You used two intermediate layers before the final softmax classification layer.
    Now try using a single intermediate layer, or three intermediate layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4.2.9 Wrapping up
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here’s what you should take away from this example: if you’re trying to classify
    data points among *N* classes, your model should end with a layer_dense() of size
    *N*.'
  prefs: []
  type: TYPE_NORMAL
- en: In a single-label, multiclass classification problem, your model should end
    with a softmax activation so that it will output a probability distribution over
    the *N* output classes.
  prefs: []
  type: TYPE_NORMAL
- en: categorical_crossentropy is almost always the loss function you should use for
    such problems. It minimizes the distance between the probability distributions
    output by the model and the true distribution of the targets.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can handle labels in multiclass classification in the following two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Encode the labels via categorical encoding (also known as one-hot encoding)
    and use categorical_crossentropy as a loss function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encode the labels as integers and use the sparse_categorical_crossentropy loss
    function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you need to classify data into a large number of categories, you should avoid
    creating information bottlenecks in your model due to intermediate layers that
    are too small.
  prefs: []
  type: TYPE_NORMAL
- en: '4.3 Predicting house prices: A regression example'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The two previous examples were considered classification problems, where the
    goal was to predict a single discrete label of an input data point. Another common
    type of machine learning problem is *regression*, which consists of predicting
    a continuous value instead of a discrete label, for instance, predicting the temperature
    tomorrow, given meteorological data, or predicting the time that a software project
    will take to complete, given its specifications.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t confuse *regression* with the *logistic regression* algorithm. Confusingly,
    logistic regression isn’t a regression algorithm—it’s a classification algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1 The Boston housing price dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we’ll attempt to predict the median price of homes in a given
    Boston suburb in the mid-1970s, given data points about the suburb at the time,
    such as the crime rate, the local property tax rate, and so on. The dataset we’ll
    use has an interesting difference from the two previous examples. It has relatively
    few data points: only 506, split between 404 training samples and 102 test samples.
    And each *feature* in the input data (e.g., the crime rate) has a different scale.
    For instance, some values are proportions, which take values between 0 and 1,
    others take values between 1 and 12, others between 0 and 100, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.20 Loading the Boston housing dataset
  prefs: []
  type: TYPE_NORMAL
- en: boston <- dataset_boston_housing()
  prefs: []
  type: TYPE_NORMAL
- en: c(c(train_data, train_targets), c(test_data, test_targets)) %<-% boston
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the data:'
  prefs: []
  type: TYPE_NORMAL
- en: str(train_data)
  prefs: []
  type: TYPE_NORMAL
- en: num [1:404, 1:13] 1.2325 0.0218 4.8982 0.0396 3.6931 …
  prefs: []
  type: TYPE_NORMAL
- en: str(test_data)
  prefs: []
  type: TYPE_NORMAL
- en: num [1:102, 1:13] 18.0846 0.1233 0.055 1.2735 0.0715 …
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, we have 404 training samples and 102 test samples, each with
    13 numerical features, such as per capita crime rate, average number of rooms
    per dwelling, accessibility to highways, and so on. The targets are the median
    values of owner-occupied homes, in thousands of dollars:'
  prefs: []
  type: TYPE_NORMAL
- en: str(train_targets)
  prefs: []
  type: TYPE_NORMAL
- en: num [1:404(1d)] 15.2 42.3 50 21.1 17.7 18.5 11.3 15.6 15.6 14.4 …
  prefs: []
  type: TYPE_NORMAL
- en: The prices are typically between $10,000 and $50,000\. If that sounds cheap,
    remember that this was the mid-1970s, and these prices aren’t adjusted for inflation.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2 Preparing the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It would be problematic to feed into a neural network values that all take
    wildly different ranges. The model might be able to automatically adapt to such
    heterogeneous data, but it would definitely make learning more difficult. A widespread
    best practice for dealing with such data is to do feature-wise normalization:
    for each feature in the input data (a column in the input data matrix), we subtract
    the mean of the feature and divide by the standard deviation, so that the feature
    is centered around 0 and has a unit standard deviation. This is easily done in
    R using the scale() function.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.21 Normalizing the data
  prefs: []
  type: TYPE_NORMAL
- en: mean <- apply(train_data, 2, mean)
  prefs: []
  type: TYPE_NORMAL
- en: sd <- apply(train_data, 2, sd)
  prefs: []
  type: TYPE_NORMAL
- en: train_data <- scale(train_data, center = mean, scale = sd)
  prefs: []
  type: TYPE_NORMAL
- en: test_data <- scale(test_data, center = mean, scale = sd)
  prefs: []
  type: TYPE_NORMAL
- en: Note that the quantities used for normalizing the test data are computed using
    the training data. You should never use any quantity computed on the test data
    in your workflow, even for something as simple as data normalization.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.3 Building your model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Because so few samples are available, we’ll use a very small model with two
    intermediate layers, each with 64 units. In general, the less training data you
    have, the worse overfitting will be, and using a small model is one way to mitigate
    overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.22 Model definition
  prefs: []
  type: TYPE_NORMAL
- en: build_model <- function() {➊
  prefs: []
  type: TYPE_NORMAL
- en: model <- keras_model_sequential() %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(64, activation = "relu") %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(64, activation = "relu") %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(1)
  prefs: []
  type: TYPE_NORMAL
- en: model %>% compile(optimizer = "rmsprop",
  prefs: []
  type: TYPE_NORMAL
- en: loss = "mse",
  prefs: []
  type: TYPE_NORMAL
- en: metrics = "mae")
  prefs: []
  type: TYPE_NORMAL
- en: model
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Because we need to instantiate the same model multiple times, we use a function
    to construct it.**
  prefs: []
  type: TYPE_NORMAL
- en: 'The model ends with a single unit and no activation (it will be a linear layer).
    This is a typical setup for scalar regression (a regression where you’re trying
    to predict a single continuous value). Applying an activation function would constrain
    the range the output can take: for instance, if you applied a sigmoid activation
    function to the last layer, the model could learn to predict values only between
    0 and 1\. Here, because the last layer is purely linear, the model is free to
    learn to predict values in any range.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that we compile the model with the mse loss function—*mean squared error*
    (MSE), the square of the difference between the predictions and the targets. This
    is a widely used loss function for regression problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re also monitoring a new metric during training: *mean absolute error* (MAE).
    It’s the absolute value of the difference between the predictions and the targets.
    For instance, an MAE of 0.5 on this problem would mean your predictions are off
    by $500 on average.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.4 Validating your approach using K-fold validation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To evaluate our model while we keep adjusting its parameters (such as the number
    of epochs used for training), we could split the data into a training set and
    a validation set, as we did in the previous examples. But because we have so few
    data points, the validation set would end up being very small (e.g., about 100
    examples). As a consequence, the validation scores might change a lot depending
    on which data points we chose for validation and which we chose for training:
    the validation scores might have a high *variance* with regard to the validation
    split. This would prevent us from reliably evaluating our model.'
  prefs: []
  type: TYPE_NORMAL
- en: The best practice in such situations is to use *K*-fold cross-validation (see
    [figure 4.6](#fig4-6)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0124-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 4.6 *K*-fold cross-validation with K = 3**'
  prefs: []
  type: TYPE_NORMAL
- en: It consists of splitting the available data into *K* partitions (typically *K*
    = 4 or 5), instantiating *K* identical models, and training each one on *K*– 1
    partitions while evaluating on the remaining partition. The validation score for
    the model used is then the average of the *K* validation scores obtained. In terms
    of code, this is straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.23 *K*-fold validation
  prefs: []
  type: TYPE_NORMAL
- en: k <- 4
  prefs: []
  type: TYPE_NORMAL
- en: fold_id <- sample(rep(1:k, length.out = nrow(train_data)))
  prefs: []
  type: TYPE_NORMAL
- en: num_epochs <- 100
  prefs: []
  type: TYPE_NORMAL
- en: all_scores <- numeric()
  prefs: []
  type: TYPE_NORMAL
- en: for (i in 1:k) {
  prefs: []
  type: TYPE_NORMAL
- en: 'cat("Processing fold #", i, "\n")'
  prefs: []
  type: TYPE_NORMAL
- en: val_indices <- which(fold_id == i)➊
  prefs: []
  type: TYPE_NORMAL
- en: val_data <- train_data[val_indices, ]➊
  prefs: []
  type: TYPE_NORMAL
- en: val_targets <- train_targets[val_indices]➊
  prefs: []
  type: TYPE_NORMAL
- en: partial_train_data <- train_data[-val_indices, ]➋
  prefs: []
  type: TYPE_NORMAL
- en: partial_train_targets <- train_targets[-val_indices]➋
  prefs: []
  type: TYPE_NORMAL
- en: model <- build_model()➌
  prefs: []
  type: TYPE_NORMAL
- en: model %>% fit (➍
  prefs: []
  type: TYPE_NORMAL
- en: partial_train_data,
  prefs: []
  type: TYPE_NORMAL
- en: partial_train_targets,
  prefs: []
  type: TYPE_NORMAL
- en: epochs = num_epochs,
  prefs: []
  type: TYPE_NORMAL
- en: batch_size = 16,
  prefs: []
  type: TYPE_NORMAL
- en: verbose = 0
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: results <- model %>%
  prefs: []
  type: TYPE_NORMAL
- en: evaluate(val_data, val_targets, verbose = 0)➎
  prefs: []
  type: TYPE_NORMAL
- en: all_scores[[i]] <- results[['mae']]
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: 'Processing fold # 1'
  prefs: []
  type: TYPE_NORMAL
- en: 'Processing fold # 2'
  prefs: []
  type: TYPE_NORMAL
- en: 'Processing fold # 3'
  prefs: []
  type: TYPE_NORMAL
- en: 'Processing fold # 4'
  prefs: []
  type: TYPE_NORMAL
- en: '➊ **Prepare the validation data: data from partition #k.**'
  prefs: []
  type: TYPE_NORMAL
- en: '➋ **Prepare the training data: data from all other partitions.**'
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Build the Keras model (already compiled).**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Train the model (in silent mode, verbose = 0).**
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **Evaluate the model on the validation data.**
  prefs: []
  type: TYPE_NORMAL
- en: 'Running this with num_epochs = 100 yields the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: all_scores
  prefs: []
  type: TYPE_NORMAL
- en: '[1] 2.435980 2.165334 2.252230 2.362636'
  prefs: []
  type: TYPE_NORMAL
- en: mean(all_scores)
  prefs: []
  type: TYPE_NORMAL
- en: '[1] 2.304045'
  prefs: []
  type: TYPE_NORMAL
- en: The different runs do indeed show rather different validation scores, from 2.1
    to 2.4\. The average (2.3) is a much more reliable metric than any single score—that’s
    the entire point of *K*-fold cross-validation. In this case, we’re off by $2,300
    on average, which is significant considering that the prices range from $10,000
    to $50,000.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try training the model a bit longer: 500 epochs. To keep a record of
    how well the model does at each epoch, we’ll modify the training loop to save
    the per-epoch validation score log for each fold.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.24 Saving the validation logs at each fold
  prefs: []
  type: TYPE_NORMAL
- en: num_epochs <- 500
  prefs: []
  type: TYPE_NORMAL
- en: all_mae_histories <- list()
  prefs: []
  type: TYPE_NORMAL
- en: for (i in 1:k) {
  prefs: []
  type: TYPE_NORMAL
- en: 'cat("Processing fold #", i, "\n")'
  prefs: []
  type: TYPE_NORMAL
- en: val_indices <- which(fold_id == i)➊
  prefs: []
  type: TYPE_NORMAL
- en: val_data <- train_data[val_indices, ]➊
  prefs: []
  type: TYPE_NORMAL
- en: val_targets <- train_targets[val_indices]➊
  prefs: []
  type: TYPE_NORMAL
- en: partial_train_data <- train_data[-val_indices, ]➋
  prefs: []
  type: TYPE_NORMAL
- en: partial_train_targets <- train_targets[-val_indices]➋
  prefs: []
  type: TYPE_NORMAL
- en: model <- build_model()➌
  prefs: []
  type: TYPE_NORMAL
- en: history <- model %>% fit(➍
  prefs: []
  type: TYPE_NORMAL
- en: partial_train_data, partial_train_targets,
  prefs: []
  type: TYPE_NORMAL
- en: validation_data = list(val_data, val_targets),
  prefs: []
  type: TYPE_NORMAL
- en: epochs = num_epochs, batch_size = 16, verbose = 0
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: mae_history <- history$metrics$val_mae
  prefs: []
  type: TYPE_NORMAL
- en: all_mae_histories[[i]] <- mae_history
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: 'Processing fold # 1'
  prefs: []
  type: TYPE_NORMAL
- en: 'Processing fold # 2'
  prefs: []
  type: TYPE_NORMAL
- en: 'Processing fold # 3'
  prefs: []
  type: TYPE_NORMAL
- en: 'Processing fold # 4'
  prefs: []
  type: TYPE_NORMAL
- en: all_mae_histories <- do.call(cbind, all_mae_histories)
  prefs: []
  type: TYPE_NORMAL
- en: '➊ **Prepare the validation data: data from partition #k.**'
  prefs: []
  type: TYPE_NORMAL
- en: '➋ **Prepare the training data: data from all other partitions.**'
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Build the Keras model (already compiled).**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Train the model (in silent mode, verbose = 0).**
  prefs: []
  type: TYPE_NORMAL
- en: We can then compute the average of the per-epoch MAE scores for all folds.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.25 Building the history of successive mean *K*-fold validation scores
  prefs: []
  type: TYPE_NORMAL
- en: average_mae_history <- rowMeans(all_mae_histories)
  prefs: []
  type: TYPE_NORMAL
- en: Let’s plot this; see [figure 4.7](#fig4-7).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.26 Plotting validation scores
  prefs: []
  type: TYPE_NORMAL
- en: plot(average_mae_history, xlab = "epoch", type = 'l')
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0126-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 4.7 Validation MAE by epoch**'
  prefs: []
  type: TYPE_NORMAL
- en: 'It may be a little difficult to read the plot, due to a scaling issue: the
    validation MAE for the first few epochs is dramatically higher than the values
    that follow. Let’s omit the first 10 data points, which are on a different scale
    than the rest of the curve.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.27 Plotting validation scores, excluding the first 10 data points
  prefs: []
  type: TYPE_NORMAL
- en: truncated_mae_history <- average_mae_history[-(1:10)]
  prefs: []
  type: TYPE_NORMAL
- en: plot(average_mae_history, xlab = "epoch", type = 'l',
  prefs: []
  type: TYPE_NORMAL
- en: ylim = range(truncated_mae_history))
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in [figure 4.8](#fig4-8), validation MAE stops improving significantly
    after 100– 140 epochs (this number includes the 10 epochs we omitted). Past that
    point, we start overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0127-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 4.8 Validation MAE by epoch, excluding the first 10 data points**'
  prefs: []
  type: TYPE_NORMAL
- en: Once you’re finished tuning other parameters of the model (in addition to the
    number of epochs, you could also adjust the size of the intermediate layers),
    you can train a final production model on all of the training data, with the best
    parameters, and then look at its performance on the test data.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.28 Training the final model
  prefs: []
  type: TYPE_NORMAL
- en: model <- build_model()➊
  prefs: []
  type: TYPE_NORMAL
- en: model %>% fit(train_data, train_targets,➋
  prefs: []
  type: TYPE_NORMAL
- en: epochs = 120, batch_size = 16, verbose = 0)
  prefs: []
  type: TYPE_NORMAL
- en: result <- model %>% evaluate(test_data, test_targets)
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Get a fresh, compiled model.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Train it on the entirety of the data.**
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the final result:'
  prefs: []
  type: TYPE_NORMAL
- en: result["mae"]
  prefs: []
  type: TYPE_NORMAL
- en: mae
  prefs: []
  type: TYPE_NORMAL
- en: '2.476283'
  prefs: []
  type: TYPE_NORMAL
- en: We’re still off by a bit under $2,500\. It’s an improvement! Just like with
    the two previous tasks, you can try varying the number of layers in the model,
    or the number of units per layer, to see if you can squeeze out a lower test error.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.5 Generating predictions on new data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When calling predict() on our binary classification model, we retrieved a scalar
    score between 0 and 1 for each input sample. With our multiclass classification
    model, we retrieved a probability distribution over all classes for each sample.
    Now, with this scalar regression model, predict() returns the model’s guess for
    the sample’s price in thousands of dollars:'
  prefs: []
  type: TYPE_NORMAL
- en: predictions <- model %>% predict(test_data)
  prefs: []
  type: TYPE_NORMAL
- en: predictions[1, ]
  prefs: []
  type: TYPE_NORMAL
- en: '[1] 10.27619'
  prefs: []
  type: TYPE_NORMAL
- en: The first house in the test set is predicted to have a price of about $10,000.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.6 Wrapping up
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here’s what you should take away from this scalar regression example:'
  prefs: []
  type: TYPE_NORMAL
- en: Regression is done using different loss functions than we used for classification.
    Mean squared error (MSE) is a loss function commonly used for regression.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, evaluation metrics to be used for regression differ from those used
    for classification; naturally, the concept of accuracy doesn’t apply for regression.
    A common regression metric is mean absolute error (MAE).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When features in the input data have values in different ranges, you should
    scale each feature independently as a preprocessing step.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When there is little data available, using *K*-fold validation is a great way
    to reliably evaluate a model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When little training data is available, it’s preferable to use a small model
    with few intermediate layers (typically only one or two), in order to avoid severe
    overfitting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The three most common kinds of machine learning tasks on vector data are binary
    classification, multiclass classification, and scalar regression.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The “Wrapping up” sections earlier in the chapter summarize the important points
    you’ve learned regarding each task.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression uses different loss functions and different evaluation metrics than
    classification
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You’ll usually need to preprocess raw data before feeding it into a neural network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When your data has features with different ranges, scale each feature independently
    as part of preprocessing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As training progresses, neural networks eventually begin to overfit and obtain
    worse results on never-before-seen data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you don’t have much training data, use a small model with only one or two
    intermediate layers, to avoid severe overfitting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your data is divided into many categories, you may cause information bottlenecks
    if you make the intermediate layers too small.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you’re working with little data, *K*-fold validation can help reliably
    evaluate your model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
