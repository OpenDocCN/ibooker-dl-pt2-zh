- en: 'Chapter 5\. Transfer learning: Reusing pretrained neural networks'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*This chapter covers*'
  prefs: []
  type: TYPE_NORMAL
- en: What transfer learning is and why it is better than training models from scratch
    for many types of problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to leverage the feature-extraction power of state-of-the-art pretrained
    convnets by converting them from Keras to TensorFlow.js
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The detailed mechanisms of transfer-learning techniques including layer freezing,
    creating new transfer heads, and fine-tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use transfer learning to train a simple object-detection model in TensorFlow.js
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In [chapter 4](kindle_split_015.html#ch04), we saw how to train convnets to
    classify images. Now consider the following scenario. Our convnet for classifying
    handwritten digits performs poorly for a user because their handwriting is very
    different from the original training data. Can we improve the model to serve the
    user better by using a small amount of data (say, 50 examples) we can collect
    from them? Consider another scenario: an e-commerce website wishes to automatically
    classify pictures of commodity items uploaded by users. But none of the publicly
    available convnets (such as MobileNet^([[1](#ch05fn1)])) are trained on such domain-specific
    images. Is it possible to use a publicly available image model to address the
    custom classification problem, given a modest number (say, a few hundred) of labeled
    pictures?'
  prefs: []
  type: TYPE_NORMAL
- en: ¹
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Andrew G. Howard et al., “MobileNets: Efficient Convolutional Neural Networks
    for Mobile Vision Applications,” submitted 17 Apr. 2017, [https://arxiv.org/abs/1704.04861](https://arxiv.org/abs/1704.04861).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Fortunately, a technique called *transfer learning*, the main focus of this
    chapter, can help solve tasks like these.
  prefs: []
  type: TYPE_NORMAL
- en: '5.1\. Introduction to transfer learning: Reusing pretrained models'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In essence, transfer learning is about speeding up a new learning task by reusing
    the results of previous learning. It involves using a model already trained on
    a dataset to perform a *different but related* machine-learning task. The already-trained
    model is referred to as the *base model*. Transfer learning sometimes involves
    retraining the base model and sometimes involves creating a new model on top of
    the base model. We refer to the new model as the *transfer model*. As [figure
    5.1](#ch05fig01) shows, the amount of data used for this retraining process is
    usually much smaller compared to the data that went into training the base model
    (as with the two examples given at the beginning of this chapter). As such, transfer
    learning is often much less time-and resource-consuming compared to the base model’s
    training process. This makes it feasible to perform transfer learning in a resource-restricted
    environment like the browser using TensorFlow.js. And thus transfer learning is
    an important topic for TensorFlow.js learners.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.1\. The general workflow of transfer learning. A large dataset goes
    into the training of the base model. This initial training process is often long
    and computationally heavy. The base model is then retrained, possibly by becoming
    part of a new model. The retraining process usually involves a dataset much smaller
    than the original one. The computation involved in the retraining is significantly
    less than the initial training and can happen on an edge device, such as a laptop
    or a phone running TensorFlow.js.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](05fig01a_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The key phrase “different but related” in the description of transfer learning
    can mean different things in different cases:'
  prefs: []
  type: TYPE_NORMAL
- en: The first scenario mentioned at the beginning of this chapter involves adapting
    a model to the data from a specific user. Although the data is different from
    the original training set, the task is exactly the same—classifying an image into
    the 10 digits. This type of transfer learning is referred to as *model adaptation*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other transfer-learning problems involve targets (labels) that are different
    from the original ones. The commodity image-classification scenario mentioned
    at the beginning of this chapter belongs to this category.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'What is the advantage of transfer learning over training a new model from scratch?
    The answer is two-fold:'
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning is more efficient in terms of both the amount of data it requires
    and the amount of computation it takes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It builds on the gains of previous training by reusing the feature-extracting
    power of the base model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These points are valid regardless of the type of problem (for instance, classification
    and regression). On the first point, transfer learning uses the trained weights
    from the base model (or a subset of them). As a result, it requires less training
    data and training time to converge to a given level of accuracy compared to training
    a new model from scratch. In this regard, transfer learning is analogous to how
    humans learn new tasks: once you have mastered a task (playing a card game, for
    example), learning similar tasks (such as playing similar card games) becomes
    significantly easier and faster in the future. The saved cost of training time
    may seem relatively small for a neural network like the convnet we built for MNIST.
    However, for larger models trained on larger datasets (such as industrial-scale
    convnets trained on terabytes of image data), the savings can be substantial.'
  prefs: []
  type: TYPE_NORMAL
- en: On the second point, the core idea of transfer learning is reusing previous
    training results. Through learning from a very large dataset, the original neural
    network has become very good at extracting useful features from the original input
    data. These features will be useful for the new task as long as the new data in
    the transfer-learning task is not too different from the original data. Researchers
    have assembled very large datasets for common machine-learning domains. In computer
    vision, there is ImageNet,^([[2](#ch05fn2)]) which contains millions of labeled
    images from about a thousand categories. Deep-learning researchers have trained
    deep convnets using the ImageNet dataset, including ResNet, Inception, and MobileNet
    (the last of which we will soon lay our hands on). Due to the large number and
    diversity of the images in ImageNet, convnets trained on it are good feature extractors
    for general types of images. These feature extractors will be useful for working
    with small datasets like those in the aforementioned scenarios, but training such
    effective feature extractors is impossible with small datasets like those. Opportunities
    for transfer learning exist in other domains as well. For example, in natural
    language processing, people have trained word embeddings (that is, vector representation
    of all common words in a language) on large text corpora consisting of billions
    of words. These embeddings are useful for language-understanding tasks where much
    smaller text datasets are available. Without further ado, let’s see how transfer
    learning works in practice through an example.
  prefs: []
  type: TYPE_NORMAL
- en: ²
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Don’t be confused by the name. “ImageNet” refers to a dataset, not a neural
    network.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '5.1.1\. Transfer learning based on compatible output shapes: Freezing layers'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let’s start by looking at a relatively simple example. We will train a convnet
    on only the first five digits of the MNIST dataset (0 through 4). We will then
    use the resulting model to recognize the remaining five digits (5 through 9),
    which the model never saw during the original training. Although this example
    is somewhat contrived, it illustrates the basic workflow of transfer learning.
    The example can be checked out and run with the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the demo web page that opens, start the transfer learning process by clicking
    the Retrain button. You can see the process reach an accuracy of about 96% on
    the new set of five digits (5 through 9), which takes about 30 seconds on a reasonably
    powerful laptop. As we will show, this is significantly faster than the non-transfer-learning
    alternative (namely, training a new model from scratch). Let’s see how this is
    done, step-by-step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our example loads the pretrained base model from an HTTP server instead of
    training it from scratch so as not to obscure the workflow’s key parts. Recall
    from section 4.3.3 that TensorFlow.js provides the `tf.loadLayersModel()` method
    for loading pretrained models. This is called in the loader.js file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The printed summary of the model looks like [figure 5.2](#ch05fig02). As you
    can see, this model consists of 12 layers.^([[3](#ch05fn3)]) All its 600,000 or
    so weight parameters are trainable, just like all the TensorFlow.js models we
    have seen so far. Note that `loadLayersModel()` loads not only the model’s topology
    but also all its weight values. As a result, the loaded model is ready to predict
    the class of digits 0 through 4\. However, this is not how we will use the model.
    Instead, we will train the model to recognize new digits (5 through 9).
  prefs: []
  type: TYPE_NORMAL
- en: ³
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'You may not have seen the *activation* layer type in this model. Activation
    layers are simple layers that perform only an activation function (such as relu
    and softmax) on the input. Suppose you have a dense layer with the default (linear)
    activation; stacking an activation layer on top of it is equivalent to using a
    dense layer with the nondefault activation included. The latter is what we did
    for the examples in [chapter 4](kindle_split_015.html#ch04). But the former style
    is also sometimes seen. In TensorFlow.js, you can get such a model topology by
    using code like the following: `const` `model` `=` `tf.sequential();` `model.add(tf.layers.dense({untis:`
    `5,` `inputShape})); model.add(tf.layers.activation({activation: ''relu''})`.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Figure 5.2\. A printed summary of the convnet for recognition of MNIST images
    and transfer learning
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](05fig01_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Looking at the callback function for the Retrain button (in the `retrainModel()`
    function of index.js), you will notice a few lines of code that set the `trainable`
    property of the first seven layers of the model to `false` if the option Freeze
    Feature Layers is selected (it is selected by default).
  prefs: []
  type: TYPE_NORMAL
- en: What does that do? By default, the `trainable` property of each of the model’s
    layers is `true` after the model is loaded via the `loadLayersModel()` method
    or created from scratch. The `trainable` property is used during training (that
    is, calls to the `fit()` or `fitDataset()` method). It tells the optimizer whether
    the layer’s weights should be updated. By default, the weights of all layers of
    a model are updated during training. But if you set the property to `false` for
    some of the model’s layers, the weights of those layers will *not* be updated
    during training. In TensorFlow.js terminology, those layers become *untrainable*,
    or *frozen*. The code in [listing 5.1](#ch05ex01) freezes the first seven layers
    of the model, from the input conv2d layer to the flatten layer, while leaving
    the last several layers (the dense layers) trainable.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.1\. “Freezing” the first several layers of the convnet for transfer
    learning
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '***1*** Freezes the layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***2*** Makes a new model with the same topology as the old one, but with reinitialized
    weight values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***3*** The freezing will not take effect during fit() calls unless you compile
    the model first.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***4*** Prints the model summary again after compile(). You should see that
    a number of the model’s weights have become nontrainable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, setting the layers’ `trainable` property alone is not enough: if you
    just modify the `trainable` property and call the model’s `fit()` method right
    away, you will see the weights of those layers still get updated during the `fit()`
    call. You need to call `Model.compile()` before calling `Model.fit()` in order
    for the `trainable` property changes to take effect, as is done in [listing 5.1](#ch05ex01).
    We mentioned previously that the `compile()` call configures the optimizer, loss
    function, and metrics. However, the method also lets the model refresh the list
    of weight variables to be updated during those calls. After the `compile()` call,
    we call `summary()` again to print a new summary of the model. As you can see
    by comparing the new summary with the old one in [figure 5.2](#ch05fig02), some
    of the model’s weights become nontrainable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: You can verify that the number of nontrainable parameters, 9,568, is the sum
    of weight parameters in the two frozen layers with weights (the two conv2d layers).
    Note that some of the layers we’ve frozen contain no weights (such as the maxPooling2d
    layer and the flatten layer) and therefore don’t contribute to the count of nontrainable
    parameters when they are frozen.
  prefs: []
  type: TYPE_NORMAL
- en: 'The actual transfer-learning code is shown in [listing 5.2](#ch05ex02). Here,
    we use the same `fit()` method that we’ve used to train models from scratch. In
    this call, we use the `validationData` field to get a measure of how accurate
    the model is doing on data it hasn’t seen during training. In addition, we connect
    two callbacks to the `fit()` call, one for updating the progress bar in the UI
    and the other for plotting the loss and accuracy curves using the tfjs-vis module
    (more details coming in [chapter 7](kindle_split_019.html#ch07)). This shows an
    aspect of the `fit()` API that we haven’t mentioned before: you can give a callback
    or an array of multiple callbacks to a `fit()` call. In the latter case, all the
    callbacks will be invoked (in the order they are specified in the array) during
    training.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.2\. Using `Model.fit()` to perform transfer learning
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '***1*** Giving multiple callbacks to a fit() call is allowed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***2*** Uses tfjs-vis to plot the validation loss and accuracy during transfer
    learning'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does the result of the transfer learning turn out? As you can see in panel
    A of [figure 5.3](#ch05fig03), it reaches an accuracy of around 0.968 after 10
    epochs of training, which takes roughly 15 seconds on a relatively up-to-date
    laptop—not bad. But how does this compare to training a model from scratch? One
    way in which we can demonstrate the value of starting from a pretrained model
    over starting from scratch is to do an experiment in which we randomly reinitialize
    the weights of the pretrained model right before the `fit()` call. This is what
    happens if you select the Reinitialize Weights option from the Training Mode drop-down
    menu before clicking the Retrain button. The result is shown in panel B of the
    same figure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.3\. The loss and validation curves for transfer learning on the MNIST
    convnet. Panel A: the curves obtained with the first seven layers of the pretrained
    model frozen. Panel B: the curves obtained with all the weights of the model reinitialized
    randomly. Panel C: the curves obtained without freezing any layers of the pretrained
    model. Note that the y-axes differ among the three panels. Panel D: a multiseries
    plot that shows the loss and accuracy curves from panels A–C on the same axes
    to facilitate comparison.'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](05fig02_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see by comparing panel B with panel A, the random reinitialization
    of the model weights causes the loss to start at a significantly higher value
    (0.36 versus 0.30) and the accuracy to start from a significantly lower value
    (0.88 versus 0.91). The reinitialized model also ends up with a lower final validation
    accuracy (~0.954) than the model that reuses weights from the base model (~0.968).
    These differences reflect the advantage of transfer learning: by reusing weights
    in the early layers (the feature-extracting layers) of the model, the model gets
    a nice head start relative to learning everything from scratch. This is because
    the data encountered in the transfer-learning task is similar to the data used
    to train the original model. The images of digits 5 through 9 have a lot in common
    with those of digits 0 through 4: they are all grayscale images with a black background;
    they have similar visual patterns (strokes of comparable widths and curvatures).
    So, the features the model learned how to extract from digits 0 through 4 turn
    out to be useful for learning to classifying the new digits (5 through 9), too.'
  prefs: []
  type: TYPE_NORMAL
- en: 'What if we don’t freeze the weights of the feature layers? The Don’t Freeze
    Feature Layers option of the Training Mode drop-down menu allows you to perform
    this experiment. The result is shown in panel C of [figure 5.3](#ch05fig03). There
    are a few noteworthy differences from the results in panel A:'
  prefs: []
  type: TYPE_NORMAL
- en: 'With no feature-layer freezing, the loss value starts off higher (for instance,
    after the first epoch: 0.37 versus 0.27); the accuracy starts off lower (0.87
    versus 0.91). Why is this the case? When the pretrained model is first starting
    to be trained on the new dataset, the predictions will contain a large number
    of errors because the pretrained weights generate essentially random predictions
    for the five new digits. As a result, the loss function will have very high values
    and steep slopes. This causes the gradients calculated in the early phases of
    the training to be very large, which in turn leads to large fluctuations in all
    the model’s weights. As a result, all layers’ weights will undergo a period of
    large fluctuations, which leads to the higher initial loss seen in panel C. In
    the normal transfer-learning approach (panel A), the model’s first few layers
    are frozen and are therefore “shielded” from these large initial weight perturbations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Partly due to these large initial perturbations, the final accuracy achieved
    by the no-freezing approach (~0.945, panel C) is *not* appreciably higher compared
    to that from the normal transfer-learning approach with layer freezing (~0.968,
    panel A).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The training takes much longer when none of the model’s layers are frozen. For
    example, on one of the laptops that we use, training the model with frozen feature
    layers takes about 30 seconds, whereas training the model without any layer freezing
    takes approximately twice as long (60 seconds). [Figure 5.4](#ch05fig04) illustrates
    the reason behind this in a schematic way. The frozen layers are taken out from
    the equation during backpropagation, which causes each batch of the `fit()` call
    to go much faster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Figure 5.4\. A schematic explanation for why freezing some layers of a model
    speeds up training. In this figure, the backpropagation path is shown by the black
    arrows pointing to the left. Panel A: when no layer is frozen, all the model’s
    weights (v[1]–v[5]) need to be updated during each training step (each batch)
    and hence will be involved in backpropagation, represented by the black arrows.
    Note that the features (x) and targets (y) are never included in backpropagation
    because their values don’t need to be updated. Panel B: by freezing the first
    few layers of the model, a subset of the weights (v[1]–v[3]) are no longer a part
    of backpropagation. Instead, they become analogous to x and y, which are just
    treated as constants that factor into the computation of the loss. As a result,
    the amount of computation it takes to perform the backpropagation decreases, and
    the training speed increases.'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](05fig03_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'These points provide justification for the layer-freezing approach of transfer
    learning: it leverages the feature-extracting layers from the base model and protects
    them from large weight perturbations during the early phases of the new training,
    thereby achieving a higher accuracy in a shorter training period.'
  prefs: []
  type: TYPE_NORMAL
- en: Two final remarks before we move on to the next section. First, model adaptation—the
    process of retraining a model to make it work better on the input data from a
    particular user—uses techniques very similar to the ones shown here, that is,
    freezing the base layers while letting the weights of the top few layers be altered
    through training on the user-specific data. This is despite the fact that the
    problem we solved in this section didn’t involve data from a different user, but
    rather involved data with different labels. Second, you might wonder how to verify
    that a weight of a frozen layer (the conv2d layers, in this case) is indeed the
    same before and after a `fit()` call. It is not very hard to do this verification.
    We leave it as an exercise for you (see exercise 2 at the end of this chapter).
  prefs: []
  type: TYPE_NORMAL
- en: '5.1.2\. Transfer learning on incompatible output shapes: Creating a new m-
    model using outputs from the base model'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the example of transfer learning seen in the previous section, the base
    model had the same output shape as the new output shape. This property doesn’t
    hold in many other transfer-learning cases (see [figure 5.5](#ch05fig05)). For
    example, if you want to use the base model trained initially on the five digits
    to classify *four* new digits, the approach previously described will not work.
    A more common scenario is the following: given a deep convnet that has been trained
    on the ImageNet classification dataset consisting of 1,000 output classes, you
    have an image-classification task at hand that involves a much smaller number
    of output classes (case B in [figure 5.5](#ch05fig05)). Perhaps it is a binary-classification
    problem—whether the image contains a human face or not—or perhaps it is a multiclass-classification
    problem with only a handful of classes—what kind of commodity item a picture contains
    (recall the example at the beginning of this chapter). In such cases, the base
    model’s output shape doesn’t work for the new problem.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.5\. Transfer learning can be divided into three types according to
    whether the output shape and activation of the new model are the same as or different
    from those of the original model. Case A: the output shape and the activation
    function of the new model match those of the base model. The transfer of the MNIST
    model onto new digits in [section 5.1.1](#ch05lev2sec1) is an example of this
    type of transfer learning. Case B: the new model has the same activation type
    as the base model because the original task and the new task are of the same type
    (for example, both are multiclass classification). However, the output shapes
    are different (for instance, the new task involves a different number of classes).
    Examples of this type of transfer learning can be found in [section 5.1.2](#ch05lev2sec2)
    (controlling a video game in the style of Pac-Man^(TM 4) through a webcam) and
    5.1.3 (recognizing a new set of spoken words). Case C: the new task is of a different
    type from the original one (such as regression versus classification). The object-detection
    model based on MobileNet is an example of this type.'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](05fig04_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In some cases, even the *type* of machine-learning task is different from the
    one the base model has been trained on. For instance, you can perform a regression
    task (predict a number, as in case C in [figure 5.5](#ch05fig05)) by applying
    transfer learning on the base model trained on a classification task. In [section
    5.2](#ch05lev1sec2), you will see a still more intriguing use of transfer learning—predicting
    an array of numbers, instead of a single one, for the purpose of detecting and
    localizing objects in images.
  prefs: []
  type: TYPE_NORMAL
- en: These cases all involve a desired output shape that differs from that of the
    base model. This makes it necessary to construct a new model. But because we are
    doing transfer learning, the new model will not be created from scratch. Instead,
    it will use the base model. We will illustrate how to do this in the webcam-transfer-learning
    example in the tfjs-examples repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see this example in action, make sure your machine has a front-facing camera—the
    example will collect the data for transfer learning from the camera. Most laptops
    and tablet computers come with a built-in front-facing camera nowadays. If you
    are using a desktop computer, however, you may need to find a webcam and attach
    it to the machine. Similar to the previous examples, you can use the following
    commands to check out and run the demo:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This fun demo turns your webcam into a game controller by applying transfer
    learning on a TensorFlow.js implementation of MobileNet, and lets you play the
    Pac-Man game with it. Let’s walk through the three steps it takes to run the demo:
    data collection, model transfer learning, and playing.^([[4](#ch05fn4)])'
  prefs: []
  type: TYPE_NORMAL
- en: ⁴
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Pac-Man is a trademark of Bandai Namco Entertainment Inc.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The data for transfer learning is collected from your webcam. Once the demo
    is running in your browser, you will see four black squares in the bottom-right
    part of the page. They are arranged in a way similar to the four direction buttons
    on a Nintendo Family Computer controller. They correspond to the four classes
    that the model will be trained to recognize in real time. These four classes correspond
    to the four directions in which Pac-Man will go. When you click and hold one of
    them, images will be collected via the webcam at a rate of 20–30 frames per second.
    A number beneath the square tells you how many images have been collected for
    this controller direction so far.
  prefs: []
  type: TYPE_NORMAL
- en: For the best transfer-learning quality, make sure you 1) collect at least 50
    images per class, and 2) move and wiggle your head and face around a little bit
    during the data collection so that the training images contain more diversity,
    which benefits the robustness of the model you’ll get from the transfer learning.
    In this demo, most people turn their heads in the four directions (up, down, left,
    and right; see [figure 5.6](#ch05fig06)) to indicate which way Pac-Man should
    go. But you can use any head positions, facial expressions, or even hand gestures
    that you desire as the input images, as long as the inputs are sufficiently visually
    distinct from one class to another.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.6\. The UI of the webcam-transfer-learning example^([[5](#ch05fn5)])
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: ⁵
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The UI of this webcam-transfer-learning example is the work of Jimbo Wilson
    and Shan Carter. A video recording of this fun example in action is available
    at [https://youtu.be/YB-kfeNIPCE?t=941](https://youtu.be/YB-kfeNIPCE?t=941).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](05fig05_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: After collecting the training images, click the Train Model button, which will
    start the transfer-learning process. Transfer learning should take only a few
    seconds. As it progresses, you should see the loss value displayed on the screen
    get smaller and smaller until it reaches a very small positive value (such as
    0.00010) and stops changing. At this point, the transfer-learning model has been
    trained, and you can use it to play the game. To start the game, just click the
    Play button and wait for game state to settle. The model will then start performing
    real-time inference on the stream of images from the webcam. At each video frame,
    the winning class (the class with the highest probability score assigned by the
    transfer-learning model) will be indicated in the bottom-right part of the UI
    with bright yellow highlighting. In addition, it will cause Pac-Man to move in
    the corresponding direction (unless blocked by a wall).
  prefs: []
  type: TYPE_NORMAL
- en: This demo might look like magic to those unfamiliar with machine learning, but
    it is based on nothing more than a transfer-learning algorithm that uses MobileNet
    to perform a four-class classification task. The algorithm uses the small amount
    of image data collected through the webcam. Those images are conveniently labeled
    through the click-and-hold action you performed while collecting the images. Thanks
    to the power of transfer learning, this process doesn’t need much data or much
    training time (it even works on a smartphone). So, that is how this demo works
    in a nutshell. If you wish to understand the technical details, dive deep with
    us into the underlying TensorFlow.js code in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Deep dive into webcam transfer learning
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The code in [listing 5.3](#ch05ex03) (from webcam-transfer-learning/index.js)
    is responsible for loading the base model. In particular, we load a version of
    MobileNet that can run efficiently in TensorFlow.js. [Info box 5.1](#ch05sb01)
    describes how this model is converted from the Keras deep-learning library in
    Python. As soon as the model is loaded, we use the `getLayer()` method to get
    hold of one of its layers. `getLayer()` allows you to specify a layer by its name
    (`'conv_pw_13_relu'` in this case). You may recall another way to access a model’s
    layers from [section 2.4.2](kindle_split_013.html#ch02lev2sec17)—that is, by indexing
    into the model’s `layers` attribute, which holds all the model’s layers as a JavaScript
    array. This approach is easy to use only when the model consists of a small number
    of layers. The MobileNet model we are dealing with here has 93 layers, which makes
    that approach fragile (for example, what if more layers get added to the model
    in the future?). Therefore, the name-based `getLayer()` approach is more reliable,
    if we assume the authors of MobileNet will keep the names of the key layers unchanged
    when they release new versions of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.3\. Loading MobileNet and creating a “truncated” model from it
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '***1*** URLs under [storage.google.com/tfjs-models](http://storage.google.com/tfjs-models)
    are designed to be permanent and stable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***2*** Gets an intermediate layer of the MobileNet. This layer contains features
    useful for the custom image-classification task.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***3*** Creates a new model that is the same as MobileNet except that it ends
    at the ''conv_pw_13_relu'' layer, that is, with the last few layers (referred
    to as the “head”) truncated'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '**Converting models from Python Keras into the TensorFlow .js format**'
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow.js features a high degree of compatibility and interoperability
    with Keras, one of the most popular Python deep-learning libraries. One of the
    benefits that stems from this compatibility is that you can utilize many of the
    so-called “applications” from Keras. These applications are a set of pretrained
    deep convnets (see [https://keras.io/applications/](https://keras.io/applications/)).
    The authors of Keras have painstakingly trained these convnets on large datasets
    such as ImageNet and made them available via the library so that they are ready
    for reuse, including inference and transfer learning, as we are doing here. For
    those who use Keras in Python, importing an application takes just one line of
    code. Due to the interoperability previously mentioned, it is also easy for a
    TensorFlow.js user to use these applications. Here are the steps it takes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Make sure that the Python package called `tensorflowjs` is installed. The easiest
    way to install it is via the `pip` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following code through a Python source file or in an interactive Python
    REPL such as ipython:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The first two lines import the required `keras` and `tensorflowjs` modules.
    The third line loads MobileNet into a Python object (`model`). You can, in fact,
    print a summary of the model in pretty much the same way as you print the summary
    of a TensorFlow.js model: that is, `model.summary()`. You can see that the last
    layer of the model (the model’s output) indeed has a shape of `(None, 1000)` (equivalent
    to `[null, 1000]` in JavaScript), reflecting the 1,000-class ImageNet classification
    task that the MobileNet model was trained on. The keyword argument `alpha=0.25`
    that we specified for this constructor call chooses a version of MobileNet that
    is smaller in size. You may choose larger values of `alpha` (such as `0.75, 1`),
    and the same conversion code will continue to work.'
  prefs: []
  type: TYPE_NORMAL
- en: The last line in the previous code snippet saves the model to the specified
    directory on the disk using a method from the tensorflowjs module. After the line
    finishes running, there will be a new directory at /tmp/mobilenet_0.25, with content
    that looks like
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This is exactly the same format as the one we saw in section 4.3.3, when we
    showed how to save a trained TensorFlow.js model to disk using its `save()` method
    in the Node.js version of TensorFlow.js. Therefore, to the TensorFlow.js-based
    programs that load this converted model from disk, the saved format is identical
    to a model created and trained in TensorFlow.js: it can simply call the `tf.loadLayersModel()`
    method and point at the path to the model.json file (either in the browser or
    in Node.js), which is exactly what happens in [listing 5.3](#ch05ex03).'
  prefs: []
  type: TYPE_NORMAL
- en: The loaded MobileNet model is ready to perform the machine-learning task that
    the model was originally trained on—classify input images into the 1,000 classes
    of the ImageNet dataset. Note that this particular dataset has a heavy emphasis
    on animals, especially various breeds of cats and dogs (which is probably related
    to the abundance of such images on the internet!). For those interested in this
    particular usage, the MobileNet example in the tfjs-example repository illustrates
    how to do that ([https://github.com/tensorflow/tfjs-examples/tree/master/mobilenet](https://github.com/tensorflow/tfjs-examples/tree/master/mobilenet)).
    However, this direct usage of MobileNet is not what we focus on in this chapter;
    instead, we explore how to use the loaded MobileNet to perform transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: The `tfjs.converters.save_keras_model()` method shown previously is capable
    of converting and saving not only MobileNet but also other Keras applications,
    such as DenseNet and NasNet. In exercise 3 at the end of this chapter, you will
    practice converting another Keras application (MobileNetV2) into the TensorFlow.js
    format and loading it in the browser. Furthermore, it should be pointed out that
    `tfjs.converters .save_keras_model()` is generally applicable to any model objects
    you have created or trained in Keras, not just models from `keras.applications`.
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: What do we do with the `conv_pw_13_relu` layer once we get hold of it? We create
    a new model that contains the layers of the original MobiletNet model from its
    first (input) layer to the `conv_pw_13_relu` layer. This is the first time you
    see this kind of model construction in this book, so it requires some careful
    explanation. For that, we need to introduce the concept of a *symbolic tensor*
    first.
  prefs: []
  type: TYPE_NORMAL
- en: Creating models from symbolic tensors
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: You have seen tensors so far. `Tensor` is the basic data type (also abbreviated
    as *dtype*) in TensorFlow.js. A tensor object carries concrete numeric values
    of a given shape and dtype, backed by storage on WebGL textures (if in a WebGL-enabled
    browser) or CPU/GPU memory (if in Node.js). However, `SymbolicTensor` is another
    important class in TensorFlow.js. Instead of holding concrete values, a symbolic
    tensor specifies only a shape and a dtype. A symbolic tensor can be thought of
    as a “slot” or a “placeholder,” into which an actual tensor value may be inserted
    later, given that the tensor value has a compatible shape and dtype. In TensorFlow.js,
    a layer or model object takes one or more inputs (so far, you’ve only seen cases
    of one input), and those are represented as one or more symbolic tensors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use an analogy that might help you understand a symbolic tensor. Consider
    a function in a programming language like Java or TypeScript (or any other statically
    typed language you are familiar with). The function takes one or more input arguments.
    Each argument of a function has a type, which stipulates what kind of variables
    may be passed in as the argument. However, the argument *itself* doesn’t hold
    any concrete values. By itself, the argument is just a placeholder. A symbolic
    tensor is analogous to a function argument: it specifies what kind (combination
    of shape^([[6](#ch05fn6)]) and dtype) of tensors may be used in that slot. By
    parallel, a function in a statically typed language has a return type. This is
    comparable to the output symbolic tensor of a model or layer object. It is a “blueprint”
    for the shape and dtype of the actual tensor values that the model or layer object
    will output.'
  prefs: []
  type: TYPE_NORMAL
- en: ⁶
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A difference between a tensor’s shape and a symbolic tensor’s shape is that
    the former always has fully specified dimensions (such as `[8, 32, 20]`), while
    the latter may have undetermined dimensions (such as `[null, null, 20]`). You
    have already seen this in the “Output shape” column of the model summaries.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In TensorFlow.js, two important attributes of a model object are its inputs
    and outputs. Each of these is an array of symbolic tensors. For a model with exactly
    one input and exactly one output, both arrays have a length of 1\. Similarly,
    a layer object has two attributes: input and output, each of which is a symbolic
    tensor. Symbolic tensors can be used to create a new model. This is a new way
    of creating models in TensorFlow.js, which is different from the approach you’ve
    seen before: namely, creating sequential models with `tf.sequential()` and subsequent
    calls to the `add()` method. In the new approach, we use the `tf.model()` function,
    which takes a configuration object with two mandatory fields: `inputs` and `outputs`.
    The `inputs` field is required to be a symbolic tensor (or, alternatively, an
    array of symbolic tensors), and likewise for the `outputs` field. Therefore, we
    can obtain the symbolic tensors from the original MobileNet model and feed them
    to a `tf.model()` call. The result is a new model that consists of a part of the
    original MobileNet.'
  prefs: []
  type: TYPE_NORMAL
- en: This process is illustrated schematically in [figure 5.7](#ch05fig07). (Note
    that the figure reduces the number of layers from the actual MobileNet model for
    the sake of a simple-looking diagram.) The important thing to realize is that
    the symbolic tensors taken from the original model and handed to the `tf.model()`
    call are *not* isolated objects. Instead, they carry information about what layers
    they belong to and how the layers are connected to each other. For readers familiar
    with graphs in data structure, the original model is a graph of symbolic tensors,
    with the connecting edges being the layers. By specifying the inputs and outputs
    of the new model as symbolic tensors in the original model, we are extracting
    a subgraph of the original MobileNet graph. The subgraph, which becomes the new
    model, contains the first few (in particular, the first 87) layers of MobileNet,
    while the last 6 layers are left out. The last few layers of a deep convnet are
    sometimes referred to as the *head*. What we are doing with the `tf.model()` call
    can be referred to as *truncating* the model. The truncated MobileNet preserves
    the feature-extracting layers while discarding the head. Why does the head contain
    *six* layers? This is because those layers are specific to the 1,000-class classification
    task that the MobileNet was originally trained on. The layers are not useful for
    the four-class classification task we are facing.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.7\. A schematic drawing that explains how the new (“truncated”) model
    is created from MobileNet. See the `tf.model()` call in [listing 5.3](#ch05ex03)
    for the corresponding code. Each layer has an input and an output, both of which
    are `SymbolicTensor` instances. In the original model, SymbolicTensor0 is the
    input of the first layer and the input of the entire model. It is used as the
    input symbolic tensor of the new model. In addition, we take the output symbolic
    tensor of an intermediate layer (equivalent to `conv_pw_13_relu`) as the output
    tensor of the new model. Hence, we get a model that consists of the first two
    layers of the original model, shown in the bottom part of the diagram. The last
    layer of the original model, which is the output layer and sometimes referred
    to as the model’s head, is discarded. This is why approaches like this are sometimes
    referred to as *truncating* a model. Note that this diagram depicts models with
    small numbers of layers for the sake of clarity. What actually happens with the
    code in [listing 5.3](#ch05ex03) involves a model with many more (93) layers compared
    to the one shown in this diagram.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](05fig06_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Transfer learning based on embeddings
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The output of the truncated MobileNet is the activation of an intermediate layer
    of the original MobileNet.^([[7](#ch05fn7)]) But how is intermediate-layer activation
    from MobileNet useful to us? The answer can be seen in the function that handles
    the events of clicking and holding each of the four black squares ([listing 5.4](#ch05ex04).)
    Every time an input image is available from the webcam (via the `capture()` method),
    we call the `predict()` method of the truncated MobileNet and save the output
    in an object called `controllerDataset`, which will be used for transfer learning
    later.
  prefs: []
  type: TYPE_NORMAL
- en: ⁷
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A frequently asked question about TensorFlow.js models is how to obtain the
    activations of intermediate layers. The approach we showed here is the answer.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: But how to interpret the output of the truncated MobileNet? For every image
    input, it is a tensor of shape `[1, 7, 7, 256]`. It is not the probabilities for
    any classification problem, nor is it the values predicted for any regression
    problem. It is a representation of the input image in a certain high-dimensional
    space. This space has 7 * 7 * 256, or approximately 12.5k, dimensions. Although
    the space has a lot of dimensions, it is lower-dimensional compared to the original
    image, which, due to the 224 × 224 image dimensions and three color channels,
    has 224 * 224 * 3 ≈ 150k dimensions. So, the output from the truncated MobileNet
    can be viewed as an efficient representation of the image. This kind of lower-dimension
    representation of inputs is often referred to as an *embedding*. Our transfer
    learning will be based on the embeddings of the four sets of images collected
    from the webcam.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.4\. Obtaining image embeddings using a truncated MobileNet
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '***1*** Uses tf.tidy() to clean up intermediate tensors such as img. See [appendix
    B](kindle_split_030.html#app02), section B.3 for a tutorial on TensorFlow.js memory
    management in the browser.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***2*** Gets MobileNet’s internal activation for the input image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have a way to get the embeddings of the webcam images, how do we
    use them to predict what direction a given image corresponds to? For this, we
    need a new model, one that takes the embedding as its input and outputs the probability
    values for the four direction classes. The code in the following listing (from
    index.js) creates such a model.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.5\. Predicting controller direction using image embeddings
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '***1*** Flattens the [7, 7, 256] embedding from the truncated MobileNet. The
    slice(1) operation discards the first (batch) dimension, which is present in the
    output shape but unwanted by the inputShape attribute of the layer’s factory method,
    so it can be used with a dense layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***2*** A first (hidden) dense layer with nonlinear (relu) activation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***3*** The number of units of the last layer should correspond to the number
    of classes we want to predict.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Compared to the truncated MobileNet, the new model created in [listing 5.5](#ch05ex05)
    has a much smaller size. It consists of only three layers:'
  prefs: []
  type: TYPE_NORMAL
- en: The input layer is a flatten layer. It transforms the 3D embedding from the
    truncated model into a 1D tensor that subsequent dense layers can take. We have
    seen similar uses of flatten layers in the MNIST convnets in [chapter 4](kindle_split_015.html#ch04).
    We let its `inputShape` match the output shape of the truncated MobileNet (without
    the batch dimension) because the new model will be fed embeddings that come out
    of the truncated MobileNet.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second layer is a hidden layer. It is hidden because it is neither the input
    layer nor the output layer of the model. Instead, it is sandwiched between two
    other layers in order to enhance the model’s capacity. This is very similar to
    the MLPs you encountered in [chapter 3](kindle_split_014.html#ch03). It is a hidden
    dense layer with a relu activation. Recall that in the [chapter 3](kindle_split_014.html#ch03)
    section “[Avoiding the fallacy of stacking layers without nonlinearity](kindle_split_014.html#ch03lev3sec2),”
    we discussed the importance of using a nonlinear activation for hidden layers
    like this.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The third layer is the final (output) layer of the new model. It has a softmax
    activation that suits the multiclass classification problem we are facing (that
    is, four classes: one for each Pac-Man direction).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, we have essentially built an MLP on top of MobileNet’s feature-extraction
    layers. The MLP can be thought of as a new head for MobileNet, even though the
    feature extractor (the truncated MobileNet) and the head are two separate models
    in this case (see [figure 5.8](#ch05fig08)). As a result of the two-model setup,
    it is not possible to train the new head directly using the image tensors (of
    the shape `[numExamples, 224, 224, 3]`). Instead, the new head must be trained
    on the embeddings of the images—the output of the truncated MobileNet. Luckily,
    we have already collected those embedding tensors ([listing 5.4](#ch05ex04)).
    All we need to do to train the new head is call its `fit()` method on the embedding
    tensors. The code that does that inside the `train()` function in index.js is
    straightforward, and we won’t elaborate on that further.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.8\. A schematic of the transfer-learning algorithm that underlies the
    webcam-transfer-learning example
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](05fig07_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Once the transfer learning has finished, the truncated model and the new head
    will be used together to obtain probability scores from input images from the
    webcam. You can find the code in the `predict()` function in index.js, shown in
    [listing 5.6](#ch05ex06). In particular, two `predict()` calls are involved. The
    first call converts the image tensor into its embedding using the truncated MobileNet;
    the second one converts the embedding into the probability scores for the four
    directions using the new head trained with transfer learning. Subsequent code
    in [listing 5.6](#ch05ex06) obtains the winning index (the index that corresponds
    to the maximum probability score among the four directions) and uses it to steer
    the Pac-Man and update UI states. As in the previous examples, we don’t cover
    the UI part of the example because it is not central to the machine-learning algorithms.
    You may study and play with the UI code at your own pleasure using the code in
    the next listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.6\. Getting the prediction from a webcam input image after transfer
    learning
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '***1*** Captures a frame from the webcam'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***2*** Gets the embedding from the truncatedMobileNet'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***3*** Converts the embedding into the probability scores of the four directions
    using the new head model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***4*** Gets the index of the maximum probability score'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***5*** Downloads the index from GPU to CPU'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***6*** Updates the UI according to the winning direction: steers the Pac-Man
    and updates other UI states, such as the highlighting of the corresponding “button”
    on the controller'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This concludes our discussion of the part of the webcam-transfer-learning example
    relevant to the transfer-learning algorithm. One interesting aspect of the method
    we used in this example is that the training and inference process involves two
    separate model objects. This is good for our educational purpose of illustrating
    how to get embeddings from the intermediate layers of a pretrained model. Another
    advantage of this approach is that it exposes the embeddings and makes it easier
    to apply machine-learning techniques that make direct use of these embeddings.
    An example of such techniques is *k-nearest neighbors* (kNN, discussed in [info
    box 5.2](#ch05sb02)). However, exposing the embeddings directly may also be viewed
    as a shortcoming for the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: It leads to slightly more complex code. For example, the inference requires
    two `predict()` calls in order to perform inference on a single image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suppose we want to save the models for use in later sessions or for conversion
    to a non-TensorFlow.js library. Then the truncated model and the new head model
    need to be saved separately, as two separate artifacts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In some special cases, transfer learning will involve backpropagation over certain
    parts of the base model (such as the first few layers of the truncated MobileNet).
    This is not possible when the base and the head are two separate objects.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will show a way to overcome these limitations by forming
    a single model object for transfer learning. It will be an end-to-end model in
    the sense that it can transform input data in the original format into the final
    desired output.
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '**k-nearest neighbors classification based on embeddings**'
  prefs: []
  type: TYPE_NORMAL
- en: There are non-neural network approaches to solving classification problems in
    machine learning. One of the most famous is the k-nearest neighbors (kNN) algorithm.
    Unlike neural networks, the kNN algorithm doesn’t involve a training step and
    is easier to understand.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can describe how the kNN classification works in a few sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: You pick a positive integer *k* (for instance, 3).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You collect a number of reference examples, each labeled with the true class.
    Usually the number of reference examples collected is at least several times larger
    than *k*. Each example is represented as a series of real-valued numbers, or a
    *vector*. This step is similar to the collection of training examples in the neural
    network approach.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In order to predict the class of a new input, you compute the distances between
    the vector representation of the new input and those of all the reference examples.
    You then sort the distances. By doing so, you can find the *k* reference examples
    that are the closest to the input in the vector space. These are called the “*k*
    nearest neighbors” of the input (the namesake of the algorithm).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You look at the classes of the *k* nearest neighbors and use the most common
    class among them as the prediction for the input. In other words, you let the
    *k* nearest neighbors “vote” on the predicted class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An example of this algorithm is shown in the following figure.
  prefs: []
  type: TYPE_NORMAL
- en: '![](05fig08_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: An example of kNN classification in a 2D embedding space. In this case, *k*
    = 3, and there are two classes (triangles and circles). There are five reference
    examples for the triangle class and seven for the circle class. The input example
    is represented as a square. The three nearest neighbors to the input are indicated
    by the line segments that connect them with the input. Because two of the three
    nearest neighbors are circles, the predicted class for the input example will
    be a circle.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the previous description, one of the key requirements of
    the kNN algorithm is that every input example is represented as a vector. Embeddings
    like the one we obtained from the truncated MobileNet are good candidates for
    such vector representations for two reasons. First, they often have a lower dimensionality
    compared to the original inputs and hence reduce the amount of storage and computation
    required by the distance calculation. Second, the embeddings usually capture more
    important features in the input (such as important geometric features in images;
    see [figure 4.5](kindle_split_015.html#ch04fig05)) and ignore less important ones
    (for example, brightness and size) owing to the fact that they have been trained
    on a large classification dataset. In some cases, embeddings give us vector representations
    for things that are not even originally represented as numbers (such as the word
    embeddings in [chapter 9](kindle_split_021.html#ch09)).
  prefs: []
  type: TYPE_NORMAL
- en: Compared to the neural network approach, kNN doesn’t require any training. In
    cases where the number of reference examples is not too large, and the dimensionality
    of the input is not too high, using kNN can be computationally more efficient
    than training a neural network and running it for inference.
  prefs: []
  type: TYPE_NORMAL
- en: However, kNN inference doesn’t scale well with the amount of data. In particular,
    given *N* reference examples, a kNN classifier must compute *N* distances in order
    to make a prediction for every input.^([[a](#ch05fn1a)]) When *N* gets large,
    the amount of computation can get intractable. By contrast, the inference with
    a neural network doesn’t change with the amount of training data. Once the network
    has been trained, it doesn’t matter how many examples went into the training.
    The amount of computation that the forward pass on the network takes is only a
    function of the network’s topology.
  prefs: []
  type: TYPE_NORMAL
- en: ^a
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'But see research efforts to design algorithms that approximate the kNN algorithm
    but run faster and scale better than kNN: Gal Yona, “Fast Near-Duplicate Image
    Search Using Locality Sensitive Hashing,” Towards Data Science, 5 May 2018, [http://mng.bz/1wm1](http://mng.bz/1wm1).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'If you are interested in using kNN for your applications, check out the WebGL-accelerated
    kNN library built on top of TensorFlow.js: [http://mng.bz/2Jp8](http://mng.bz/2Jp8).'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '5.1.3\. Getting the most out of transfer learning through fine-tuning: An audio
    example'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the previous sections, the examples of transfer learning dealt with visual
    inputs. In this example, we will show that transfer learning works on audio data
    represented as spectrogram images as well. Recall that we introduced the convnet
    for recognizing speech commands (isolated, short spoken words) in [section 4.4](kindle_split_015.html#ch04lev1sec4).
    The speech-command recognizer we built was capable of recognizing only 18 different
    words (such as “one,” “two,” “up,” and “down”). What if you want to train a recognizer
    for other words? Perhaps your particular application requires the user to say
    specific words such as “red” or “blue,” or even words that are picked by the users
    themselves; or perhaps your application is intended for users who speak languages
    other than English. This is a classic example of transfer learning: with the small
    amount of data at hand, you *could* try to train a model entirely from scratch,
    but using a pretrained model as the base allows you to spend a smaller amount
    of time and computation resources while getting a higher degree of accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do transfer learning in the speech-command example app
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Before we describe how transfer learning works in this example, it will be
    good for you to get familiar with how to use the transfer-learning feature through
    the UI. To use the UI, make sure your machine has an audio-input device (a microphone)
    attached and that the audio-input volume is set to a nonzero value in your system
    settings. To download the code of the demo and run it, do the following (the same
    procedure as in [section 4.4.1](kindle_split_015.html#ch04lev2sec10)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: When the UI starts up, answer Yes to the browser’s request for your permission
    to access the microphone. [Figure 5.9](#ch05fig09) shows an example screenshot
    of the demo. When it starts up, the demo page will automatically load a pretrained
    speech-commands model from the internet, using the `tf.loadLayersModel()` method
    pointing to an HTTPS URL. After the model is loaded, the Start and Enter Transfer
    Words buttons will be enabled. If you click the Start button, the demo will enter
    an inference mode in which it detects the 18 basic words (as displayed on the
    screen) in a continuous fashion. Each time a word is detected, the corresponding
    word box will light up on the screen. However, if you click the Enter Transfer
    Words button, a number of additional buttons will appear on the screen. These
    buttons are created from the comma-separated words in the text-input box to the
    right. The default words are “noise,” “red,” and “green.” These are the words
    that the transfer-learning model will be trained to recognize. But you are free
    to modify the content of the input box if you want to train a transfer model for
    other words, as long as you preserve the “noise” item. The “noise” item is a special
    one, for which you should collect background noise samples—that is, samples without
    any speech sound in them. This allows the transfer model to tell moments in which
    a word is spoken from moments of silence (background noise). When you click these
    buttons, the demo will record a 1-second audio snippet from the microphone and
    display its spectrogram next to the button. The number in the word button keeps
    track of how many examples you have collected for the particular word so far.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.9\. An example screenshot of the transfer-learning feature of the
    speech-command example. Here, the user has entered a custom set of words for transfer
    learning: “feel,” “seal,” “veal,” and “zeal,” in addition to the always-required
    “noise” item. Furthermore, the user has collected 20 examples for each of the
    word and noise categories.'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](f0173_01_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As is the general case in machine-learning problems, the more data you can
    collect (as permitted by the time and resources available), the better the trained
    model will turn out. The example app requires at least eight examples for every
    word. If you don’t want to or cannot collect sound samples yourself, you can download
    a precollected dataset from [http://mng.bz/POGY](http://mng.bz/POGY) (file size:
    9 MB) and upload it by using the Upload button in the Dataset IO section of the
    UI.'
  prefs: []
  type: TYPE_NORMAL
- en: Once the dataset is ready, through either file uploading or your own sample
    collection, the Start Transfer Learning button will become enabled. You can click
    the button to kick off the training of the transfer model. The app performs a
    3:1 split on the audio spectrograms you have collected so that a randomly selected
    75% of them will be used for training, while the remaining 25% will be used for
    validation.^([[8](#ch05fn8)]) The app displays the training-set loss and accuracy
    values along with the validation-set values as the transfer learning happens.
    Once the training is complete, you can click the Start button to let the demo
    start a continuous recognition of the transfer words, during which time you can
    assess the accuracy of the transfer model empirically.
  prefs: []
  type: TYPE_NORMAL
- en: ⁸
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is the reason why the demo requires you to collect at least eight samples
    per word. With fewer words, the number of samples for each word will be small
    in the validation set, leading to potentially unreliable loss and accuracy estimates.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: You should experiment with different sets of words and see how they affect the
    accuracy you can get after doing transfer learning on them. In the default set,
    “red” and “green,” the words are fairly distinct from each other in terms of their
    phonemic content. For example, their onset consonants are two very distinct sounds,
    “r” and “g.” Their vowels also sound fairly distinct (“e” versus “ee”); so do
    their ending consonants (“d” versus “n”). Therefore, you should be able to get
    near-perfect validation accuracy at the end of the transfer training, as long
    as the number of examples you collect for each word is not too small (say >= 8),
    and you don’t use an epoch number that’s too small (which leads to underfitting)
    or too large (which leads to overfitting; see [chapter 8](kindle_split_020.html#ch08)).
  prefs: []
  type: TYPE_NORMAL
- en: 'To make the transfer-learning task more challenging for the model, use a set
    consisting of 1) more confusable words and 2) a larger vocabulary. This is what
    we did for the screenshot in [figure 5.9](#ch05fig09). There, a set of four words
    that sound similar to each other are used: “feel,” “seal,” “veal,” and “zeal.”
    These words have identical vowel and ending consonants, as well as four similar-sounding
    onset consonants. They might even confuse a human listener not paying attention
    or someone listening over a bad phone line. From the accuracy curve at the bottom-right
    part of the figure, you can see that it is not an easy task for the model to reach
    an accuracy higher than 90%, for which an initial phase of transfer learning has
    to be supplemented by an additional phase of *fine-tuning*—that is, a transfer-learning
    trick.'
  prefs: []
  type: TYPE_NORMAL
- en: Deep dive into fine-tuning in transfer learning
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Fine-tuning is a technique that helps you reach levels of accuracy not achievable
    just by training the new head of the transfer model. If you wish to understand
    how fine-tuning works, this section explains it in greater detail. There will
    be a few technical points to digest. But the deepened understanding of transfer
    learning and the related TensorFlow.js implementation that you’ll get out of it
    will be worth the effort.
  prefs: []
  type: TYPE_NORMAL
- en: Constructing a single model for transfer learning
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: First, we need to understand how the speech transfer-learning app creates the
    model for transfer learning. The code in [listing 5.7](#ch05ex07) (from speech-commands/src/browser_
    fft_recognizer.ts) creates a model from the base speech-command model (the one
    you learned in [section 4.4.1](kindle_split_015.html#ch04lev2sec10)). It first
    finds the penultimate (the second-last) dense layer of the model and gets its
    output symbolic tensor `(`truncatedBaseOutput` in the code). It then creates a
    new head model consisting of only one dense layer. The input shape of this new
    head matches the shape of the `truncatedBaseOutput` symbolic tensor, and its output
    shape matches the number of words in the transfer dataset (five, in the case of
    [figure 5.9](#ch05fig09)). The dense layer is configured to use the softmax activation,
    which suits the multiclass-classification task. (Note that unlike most of the
    other code listings in the book, the following code is written in TypeScript.
    If you’re unfamiliar with TypeScript, you can simply ignore the type notations
    such as `void` and `tf.SymbolicTensor`.)`
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.7\. Creating the transfer-learning model as a single `tf.Model` object^([[9](#ch05fn9)])
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: ⁹
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Two notes about this code listing: 1) The code is written in TypeScript because
    it is a part of the reusable @tensorflow-models/speech-commands library. 2) Some
    error-checking code has been removed from this code for the sake of simplicity.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '***1*** Finds the second-last dense layer of the base model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***2*** Gets the layer that will be unfrozen during fine-tuning later (see
    [listing 5.8](#ch05ex08))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***3*** Finds the symbolic tensor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***4*** Creates the new head of the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***5*** “Applies” the new head on the output of the truncated base model’s
    output to get the final output of the new model as a symbolic tensor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***6*** Uses the tf.model() API to create a new model for transfer learning,
    specifying the original model’s inputs as its input and the new symbolic tensor
    as the output'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The new head is used in a novel way: its `apply()` method is called using the
    `truncatedBaseOutput` symbolic tensor as the input argument. `apply()` is a method
    that’s available on every layer and model object in TensorFlow.js. What does the
    `apply()` method do? As its name suggests, it “applies” the new head model on
    an input and gives you an output. The important things to realize are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Both the input and output involved here are symbolic—they are placeholders for
    concrete tensor values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Figure 5.10](#ch05fig10) shows a graphical illustration of this: the symbolic
    input (`truncatedBaseOutput`) is not an isolated entity; instead, it is the output
    of the second-last dense layer of the base model. The dense layer receives inputs
    from another layer, which in turn receives inputs from its upstream layer, and
    so forth. Therefore, `truncatedBaseOutput` carries with it a subgraph of the base
    model: namely, the subgraph between the base model’s input and the second-last
    dense layer’s output. In other words, it is the entire graph of the base model,
    minus the part after the second-last dense layer. As a result, the output of the
    `apply()` call carries a graph consisting of that subgraph plus the new dense
    layer. The output and the original input are used together in a call to the `tf.model()`
    function, which yields a new model. This new model is the same as the base model
    except that its head has been replaced with the new dense layer (see the bottom
    part of [figure 5.10](#ch05fig10)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Figure 5.10\. A schematic illustration of the way in which the new, end-to-end
    model is created for transfer learning. This figure should be read in conjunction
    with [listing 5.7](#ch05ex07). Some parts of the figure that correspond to variables
    in [listing 5.7](#ch05ex07) are labeled in fixed-width font. Step 1: the output
    symbolic tensor of the second-to-last dense layer of the original model is obtained
    (indicated by the thick arrow). It will later be used in step 3\. Step 2: the
    new head model, consisting of a single output dense layer (labeled “dense 3”)
    is created. Step 3: the `apply()` method of the new head model is invoked with
    the symbolic tensor from step 1 as the input argument. The call connects the input
    to the new head model with the truncated base model from step 1\. Step 4: the
    return value of the `apply()` call is used in conjunction with the input symbolic
    tensor of the original model during a call to the `tf.model()` function. This
    call returns a new model that contains all the layers of the original model from
    the first layer to the second-last dense layer, in addition to the dense layer
    in the new head. In effect, this swaps the old head of the original model with
    the new head, setting the stage for subsequent training on the transfer data.
    Note that some (seven) layers of the actual speech-command model are omitted in
    this diagram for the sake of visual simplicity. In this figure, the tinted layers
    are trainable, while the white-colored layers are untrainable.'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](05fig09_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that the approach here is different from how we fused models in [section
    5.1.2](#ch05lev2sec2). There, we created a truncated base and a new head model
    as two separate model instances. As a result, running inference on each input
    example involves two `predict()` calls. Here, the inputs expected by the new model
    are identical to the audio-spectrogram tensors expected by the base model. At
    the same time, the new model directly outputs the probability scores for the new
    words. Every inference takes only one `predict()` call and is therefore a more
    streamlined process. By encapsulating all the layers in a single model, our new
    approach has an additional advantage important for our application: it allows
    us to perform backpropagation through any of the layers involved in recognizing
    the new words. This enables us to perform the fine-tuning trick. This is what
    we will explore in the next section.'
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning through layer unfreezing
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Fine-tuning is an optional step of transfer learning that follows an initial
    phase of model training. In the initial phase, all the layers from the base model
    were frozen (their `trainable` attribute was set to `false`), and weight updating
    happened only to the head layers. We have seen this type of initial training in
    the mnist-transfer-cnn and webcam-transfer-learning examples earlier in this chapter.
    During fine-tuning, some of the layers of the base model are unfrozen (their `trainable`
    attribute is set to `true`), and then the model is trained on the transfer data
    again. This layer unfreezing is shown schematically in [figure 5.11](#ch05fig11).
    The code in [listing 5.8](#ch05ex08) (from speech-commands/src/browser_fft_recognizer.ts)
    shows how that’s done in TensorFlow.js for the speech-command example.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.11\. Illustrating frozen and unfrozen (that is, trainable) layers during
    the initial (panel A) and fine-tuning (panel B) phases of the transfer learning
    as done by the code in [listing 5.8](#ch05ex08). Note that the reason dense1 is
    followed immediately by dense3 is that dense2 (the original output of the base
    model) has been truncated as the first step of the transfer learning (see [figure
    5.10](#ch05fig10)).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](05fig10_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Listing 5.8\. Initial transfer learning, followed by fine-tuning^([[10](#ch05fn10)])
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: ^(10)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Some error-checking code has been removed so as to focus on the key parts of
    the algorithm.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '***1*** Makes sure all layers of the truncated base model, including the one
    that will be fine-tuned later, are frozen for the initial phase of transfer training'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***2*** Compiles the model for the initial transfer training'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***3*** If validationSplit is required, splits the transfer data into a training
    set and a validation set in a balanced way'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***4*** Calls Model.fit() for initial transfer training'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***5*** For fine-tuning, unfreezes the second-last dense layer of the base
    model (the last layer of the truncated base model)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***6*** Recompiles the model after unfreezing the layer (or the unfreezing
    won’t take effect)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***7*** Calls Model.fit() for fine-tuning'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are several important things to point out about the code in [listing
    5.8](#ch05ex08):'
  prefs: []
  type: TYPE_NORMAL
- en: Each time you freeze or unfreeze any layers by changing their `trainable` attribute,
    you need to call the `compile()` method of the model again in order for the change
    to take effect. We’ve already covered that when talking about the MNIST transfer-learning
    example in [section 5.1.1](#ch05lev2sec1).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We reserve a fraction of the training data for validation. This ensures that
    the loss and accuracy we look at reflect how well the model works on inputs it
    hasn’t seen during backpropagation. However, the way in which we split a fraction
    of the collected data out for validation is different from before and deserves
    some attention. In the MNIST convnet example ([listing 4.2](kindle_split_015.html#ch04ex02)
    in [chapter 4](kindle_split_015.html#ch04)), we used the `validationSplit` parameter
    to let the `Model.fit()` reserve the last 15–20% of the data for validation. The
    same approach won’t work very well here. Why? Because we have a much smaller training
    set here compared to the size of the data in the earlier examples. As a result,
    blindly splitting the last several examples for validation may very well result
    in scenarios in which some words are underrepresented in the validation subset.
    For example, suppose you have collected eight examples for each of the four words
    “feel,” “seal,” “veal,” and “zeal” and choose the last 25% of the 32 samples (8
    examples) for validation. Then, on average, there will be only two examples for
    each word in the validation subset. Due to randomness, some of the words may end
    up having only one example in the validation subset, and others may have no example
    there at all! Obviously, if the validation set lacks certain words, it won’t be
    a very good set to measure the model’s accuracy on. This is why we use a custom
    function (`balancedTrainValSplit` in [listing 5.8](#ch05ex08)). This function
    takes into account the true word label of the examples and ensures that all the
    different words get fair representation in both the training and validation subsets.
    If you have a transfer-learning application involving a similarly small dataset,
    it is a good idea to do the same.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, what does fine-tuning do for us? What added value does it provide on top
    of the initial phase of transfer learning? To illustrate that, we plot the loss
    and accuracy curves from the initial and fine-tuning phases concatenated as continuous
    curves in panel A of [figure 5.12](#ch05fig12). The transfer dataset involved
    here consists of the same four words we saw in [figure 5.9](#ch05fig09). The first
    100 epochs of each curve correspond to the initial phase, while the last 300 epochs
    correspond to fine-tuning. You can see that toward the end of the 100 epochs of
    initial training, the loss and accuracy curves begin to flatten out and start
    to enter regimes of diminishing returns. The accuracy on the validation subset
    levels off around 84%. (Notice how misleading it would be to look at only the
    accuracy curve from the *training subset*, which easily approaches 100%.) However,
    unfreezing the dense layer in the base model, recompiling the model, and starting
    the fine-tuning phase of training, the validation accuracy gets unstuck and could
    go up to 90–92%, which is a very decent 6–8 percentage point gain in accuracy.
    A similar effect can be seen in the validation loss curve.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.12\. Panel A: example loss and accuracy curves from transfer learning
    and the subsequent fine-tuning (FT in the plot legends). Notice the inflection
    point at the junction between the initial and fine-tuning parts of the curves.
    Fine-tuning accelerates the reduction in the loss and gain in the accuracy, which
    is due to the unfreezing of the top few layers of the base model and the resulting
    increase in the model’s capacity, and its adaptation toward the unique features
    in the transfer-learning data. Panel B: the loss and accuracy curves from training
    the transfer model an equal number of epochs (400 epochs) without fine-tuning.
    Notice that without the fine-tuning, the validation loss converges to a higher
    value and the validation accuracy to a lower value compared to panel A. Note that
    while the final accuracy reaches about *0.9* with fine-tuning (panel A), it gets
    stuck at about *0.85* without the fine-tuning but with the same number of total
    epochs (panel B).'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](05fig11_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: To illustrate the value of fine-tuning over transfer learning without fine-tuning,
    we show in panel B of [figure 5.12](#ch05fig12) what happens if the transfer model
    is trained for an equal number of (400) epochs without fine-tuning the top few
    layers of the base model. There is no “inflection point” in the loss or accuracy
    curves that happened in panel A at epoch 100 when the fine-tuning kicks in. Instead,
    the loss and accuracy curves level off and converge to worse values.
  prefs: []
  type: TYPE_NORMAL
- en: 'So why does fine-tuning help? It can be understood as an increase in the model
    capacity. By unfreezing some of the topmost layers of the base model, we allow
    the transfer model to minimize the loss function in a higher-dimensional parameter
    space than the initial phase. This is similar to adding hidden layers to a neural
    network. The weight parameters of the unfrozen dense layer have been optimized
    for the original dataset (the one consisting of words like “one,” “two,” “yes,”
    and “no”), which may not be optimal for the transfer words. This is because the
    internal representations that help the model distinguish between those original
    words may not be the representations that make the transfer words easiest to distinguish
    from one another. By allowing those parameters to be optimized further (that is,
    fine-tuned) for the transfer words, we allow the representation to be optimized
    for the transfer words. Therefore, we get a boost in validation accuracy on the
    transfer words. Note that this boost is easier to see when the transfer-learning
    task is hard (as with the four confusable words: “feel,” “seal,” “veal,” and “zeal”).
    With easier tasks (more distinct words like “red” and “green”), the validation
    accuracy may well reach 100% with only the initial transfer learning.'
  prefs: []
  type: TYPE_NORMAL
- en: One question you might want to ask is, here we unfreeze only one layer in the
    base model, but will unfreezing more layers help? The short answer is, it depends,
    because unfreezing even more layers gives the model even higher capacity. But
    as we mentioned in [chapter 4](kindle_split_015.html#ch04) and will discuss in
    greater detail in [chapter 8](kindle_split_020.html#ch08), higher capacity leads
    to a higher risk of overfitting, especially when we are faced with a small dataset
    like the audio examples collected in the browser here. This is not to mention
    the additional computation load required to train more layers. You are encouraged
    to experiment with it yourself as a part of exercise 4 at the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s wrap up this section on transfer learning in TensorFlow.js. We introduced
    three different ways to reuse a pretrained model on new tasks. In order to help
    you decide which approach to use in your future transfer-learning projects, we
    summarize the three approaches and their relative pros and cons in [table 5.1](#ch05table01).
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.1\. A summary of three approaches to transfer learning in TensorFlow.js
    and their relative advantages and shortcomings
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Approach | Pros | Cons |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Use the original model and freeze its first several (feature-extracting)
    layers ([section 5.1.1](#ch05lev2sec1)). |'
  prefs: []
  type: TYPE_TB
- en: Simple and convenient
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Works only if the output shape and activation required by the transfer learning
    match those of the base model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Obtain internal activations from the original model as embeddings for the
    input example, and create a new model that takes the embedding as the input ([section
    5.1.2](#ch05lev2sec2)). |'
  prefs: []
  type: TYPE_TB
- en: Applicable to transfer-learning cases that require an output shape different
    from the original one
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embedding tensors are directly accessible, making methods such as k-nearest
    neighbors (kNN, see [info box 5.2](#ch05sb02)) classifiers possible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Need to manage two separate model instances
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Difficult to fine-tune layers of the original model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Create a new model that contains the feature-extracting layers of the original
    model and the layers of the new head ([section 5.1.3](#ch05lev2sec3)). |'
  prefs: []
  type: TYPE_TB
- en: Applicable to transfer-learning cases that require an output shape different
    from the original one
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Only a single model instance to manage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enables fine-tuning of feature-extracting layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Internal activations (embeddings) that are not directly accessible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2\. Object detection through transfer learning on a convnet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The examples of transfer learning you have seen in this chapter so far have
    a commonality: the nature of the machine-learning task stays the same after the
    transfer. In particular, they take a computer-vision model trained on a multiclass-classification
    task and apply it on another multiclass-classification task. In this section,
    we will show that this doesn’t have to be the case. The base model can be used
    on a task very different from the original one—for example, when you want to use
    a base model trained on a classification task to perform regression (fitting a
    number). This type of cross-domain transfer is a good example of the versatility
    and reusability of deep learning, which is one of the main reasons behind the
    success of the field.'
  prefs: []
  type: TYPE_NORMAL
- en: The new task we will use to illustrate this point is *object detection*, the
    first nonclassification computer-vision problem type you encounter in this book.
    Object detection involves detecting certain classes of objects in an image. How
    is it different from classification? In object detection, the detected object
    is reported in terms of not only its class (what type of object it is) but also
    some additional information regarding the location of the object inside the image
    (where the object is). The latter is a piece of information that a mere classifier
    doesn’t provide. For example, in a typical object-detection system used by self-driving
    cars, a frame of input image is analyzed so that the system outputs not only the
    types of interesting objects that are present in the image (such as vehicles and
    pedestrians) but also the location, apparent size, and pose of such objects within
    the image’s coordinate system.
  prefs: []
  type: TYPE_NORMAL
- en: The example code is in the simple-object-detection directory of the tfjs-examples
    repository. Note that this example is different from the ones you have seen so
    far in that it combines model training in Node.js with inference in the browser.
    Specifically, the model training happens with tfjs-node (or tfjs-node-gpu), and
    the trained model is saved to disk. A parcel server is then used to serve the
    saved model files, along with the static index.html and index.js, in order to
    showcase the inference on the model in the browser.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sequence of commands you can use for running the example is as follows
    (with some comment strings that you don’t need to include when entering the commands):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The `yarn train` command performs model training on your machine and saves
    the model inside the ./dist folder when it’s finished. Note that this is a long-running
    training job and is best handled if you have a CUDA-enabled GPU, which boosts
    the training speed by a factor of 3 to 4\. To do this, you just need to add the
    `--gpu` flag to the `yarn train` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'However, if you don’t have the time or resources to train the model on your
    own machine, don’t worry: you can just skip the `yarn train` command and proceed
    directly to `yarn watch`. The inference page that runs in the browser will allow
    you to load a model we’ve already trained for you from a centralized location
    via HTTP.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1\. A simple object-detection problem based on synthesized scenes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: State-of-the-art object-detection techniques involve many tricks that are not
    suitable for a beginning tutorial on the topic. Our goal here is to show the essence
    of how object detection works without being bogged down by too many technical
    details. To this end, we designed a simple object-detection problem that involves
    synthesized image scenes (see [figure 5.13](#ch05fig13)). These synthesized images
    have a dimension of 224 × 224 and color depth of 3 (RGB channels) and hence match
    the input specification of the MobileNet model that will form the base of our
    model. As the example in [figure 5.13](#ch05fig13) shows, each scene has a white
    background. The object to detect is either an equilateral triangle or a rectangle.
    If the object is a triangle, its size and orientation are random; if the object
    is a rectangle, its height and width vary randomly. If the scene consisted of
    only the white background and the object of interest, the task would be too easy
    to show the power of our technique. To add to the difficulty of the task, a number
    of “noise objects” are randomly sprinkled in the scene. These include 10 circles
    and 10 line segments in every image. The locations and sizes of the circles are
    generated randomly, and so are the locations and lengths of the line segments.
    Some of the noise objects may lie on top of the target object, partially obscuring
    it. All the target and noise objects have randomly generated colors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.13\. An example of the synthesized scenes used by simple object detection.
    Panel A: a rotated equilateral triangle as the target object. Panel B: a rectangle
    as the target object. The boxes labeled “true” are the true bounding box for the
    object of interest. Note that the object of interest can sometimes be partially
    obscured by some of the noise objects (line segments and circles).'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](05fig12_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'With the input data fully characterized, we can now define the task for the
    model we are about to create and train. The model will output five numbers, which
    are organized into two groups:'
  prefs: []
  type: TYPE_NORMAL
- en: The first group contains a single number, indicating whether the detected object
    is a triangle or a rectangle (regardless of its location, size, orientation, and
    color).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The remaining four numbers form the second group. They are the coordinates of
    the bounding box around the detected object. Specifically, they are the left x-coordinate,
    right x-coordinate, top y-coordinate, and bottom y-coordinate of the bounding
    box, respectively. See [figure 5.13](#ch05fig13) for an example.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The nice things about using synthesized data are 1) the true label values are
    automatically known, and 2) we can generate as much data as we want. Every time
    we generate a scene image, the type of the object and its bounding box are automatically
    available to us from the generation process. So, there is no need for any labor-intensive
    labeling of the training images. This very efficient process in which the input
    features and labels are synthesized together is used in many testing and prototyping
    environments for deep-learning models and is a technique you should be familiar
    with. However, training object-detection models meant for real-life image inputs
    requires manually labeled real scenes. Luckily, there are such labeled datasets
    available. The Common Object in Context (COCO) dataset is one of them (see [http://cocodataset.org](http://cocodataset.org)).
  prefs: []
  type: TYPE_NORMAL
- en: After the training completes, the model should be able to localize and classify
    the target objects with reasonably good accuracy (as shown by the examples in
    [figure 5.13](#ch05fig13)). To understand how the model learns this object-detection
    task, dive with us into the code in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2\. Deep dive into simple object detection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now let’s build the neural network to solve the synthesized object-detection
    problem. As before, we build our model on the pretrained MobileNet model in order
    to use the powerful general visual feature extractor in the model’s convolutional
    layers. This is what the `loadTruncatedBase()` method in [listing 5.9](#ch05ex09)
    does. However, a new challenge our new model faces is how to predict two things
    at the same time: determining what shape the target object is and finding its
    coordinates in the image. We haven’t seen this type of “dual-task prediction”
    before. The trick we use here is to let the model output a tensor that encapsulates
    both predictions, and we will design a new loss function that measures how well
    the model is doing in both tasks at once. We *could* train two separate models,
    one for classifying the shape and one for predicting the bounding box. But compared
    with using a single model to perform both tasks, running two models will involve
    more computation and more memory usage and doesn’t leverage the fact that feature-extracting
    layers can be shared between the two tasks. (The following code is from simple-object-detection/train.js.)'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.9\. Defining a model for simple object learning based on truncating
    MobileNet^([[11](#ch05fn11)])
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: ^(11)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Some code for checking error conditions is removed for clarity.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '***1*** Sets what layers to unfreeze for fine-tuning'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***2*** Gets an intermediate layer: the last feature-extraction layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***3*** Forms the truncated MobileNet'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***4*** Freezes all feature-extraction layers for the initial phase of transfer
    learning'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***5*** Keeps track of layers that will be unfrozen during fine-tuning'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***6*** Builds the new head model for the simple object-detection task'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***7*** The length-5 output consists of a length-1 shape indicator and a length-4
    bounding box (see [figure 5.14](#ch05fig14)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***8*** Puts the new head model on top of the truncated MobileNet to form the
    entire model for object detection'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The key part of the “dual-task” model is built by the `buildNewHead()` method
    in [listing 5.9](#ch05ex09). A schematic drawing of the model is shown in the
    left part of [figure 5.14](#ch05fig14). The new head consists of three layers.
    A flatten layer shapes the output of the last convolutional layer of the truncated
    MobileNet base so that dense layers can be added later on. The first dense layer
    is a hidden one with a relu nonlinearity. The second dense layer is the final
    output of the head and hence the final output of the entire object-detection model.
    This layer has the default linear activation. It is the key to understanding how
    this model works and therefore needs to be looked at carefully.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.14\. The object-detection model and the custom loss function that it
    is based on. See [listing 5.9](#ch05ex09) for how the model (the left part) is
    constructed. See [listing 5.10](#ch05ex10) for how the custom loss function is
    written.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](05fig13_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see from the code, the final dense layer has an output unit count
    of 5\. What do the five numbers represent? They combine the shape prediction and
    the bounding-box prediction. Interestingly, what determines their meaning is not
    the model itself, but rather the loss function that will be used on it. Previously,
    you saw various types of loss functions that can be straightforward string names
    such as `"meanSquaredError"` and are suitable to their respective machine-learning
    tasks (for example, see [table 3.6](kindle_split_014.html#ch03table06) in [chapter
    3](kindle_split_014.html#ch03)). However, this is only one of two ways to specify
    loss functions in TensorFlow.js. The other way, which is what we are using here,
    involves defining a custom JavaScript function that satisfies a certain signature.
    The signature is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Two input arguments: 1) the true labels of the input examples and 2) the corresponding
    predictions of the model. Each of them is represented as a 2D tensor. The shape
    of the two tensors ought to be identical, with the first dimension of each tensor
    being the batch size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The return value is a scalar tensor (a tensor of shape `[]`) whose value is
    the mean loss of the examples in the batch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our custom loss function, written according to this signature, is shown in [listing
    5.10](#ch05ex10) and graphically illustrated in the right part of [figure 5.14](#ch05fig14).
    The first input to `customLossFunction` (`yTrue`) is the true label tensor, which
    has a shape of `[batchSize, 5]`.
  prefs: []
  type: TYPE_NORMAL
- en: The second input (`yPred`) is the model’s output prediction, with exactly the
    same shape as `yTrue`. Of the five dimensions along the second axis of `yTrue`
    (the five columns, if we view it as a matrix), the first one is a 0–1 indicator
    for the shape of the target object (0 for triangle and 1 for rectangle). This
    is determined by how the data is synthesized (see simple-object-detection/synthetic_images.js).
    The remaining four columns are the target object’s bounding box—that is, its left,
    right, top, and bottom values—each of which ranges from 0 to `CANVAS_SIZE` (224).
    The number 224 is the height and width of the input images and comes from the
    input image size to MobileNet, which our model is based on.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.10\. Defining the custom loss function for the object-detection task
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '***1*** The shape-indicator column of yTrue is scaled by CANVAS_SIZE (224)
    to ensure approximately equal contribution to the loss by shape prediction and
    bounding-box prediction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The custom loss function takes `yTrue` and scales its first column (the 0–1
    shape indicator) by `CANVAS_SIZE`, while leaving the other columns unchanged.
    It then calculates the MSE between `yPred` and the scaled `yTrue`. Why do we scale
    the 0–1 shape label in `yTrue`? We want the model to output a number that represents
    whether it predicts the shape to be a triangle or a rectangle. Specifically, it
    outputs a number close to 0 for triangle and a number close to `CANVAS_SIZE` (224)
    for rectangle. So, during inference time, we can just compare the first value
    in the model’s output with `CANVAS_SIZE/2` (112) to get the model’s prediction
    of whether the shape is more like a triangle or a rectangle. The question is then
    how to measure the accuracy of this shape prediction in order to come up with
    a loss function. Our answer is to compute the difference between this number and
    the 0–1 indicator, multiplied by `CANVAS_SIZE`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Why do we do this instead of using binary cross entropy as we did for the phishing-detection
    example in [chapter 3](kindle_split_014.html#ch03)? We need to combine two metrics
    of accuracy here: one for the shape prediction and one for the bounding-box prediction.
    The latter task involves predicting continuous values and can be viewed as a regression
    task. Therefore, MSE is a natural metric for bounding boxes. In order to combine
    the metrics, we just “pretend” that the shape prediction is also a regression
    task. This trick allows us to use a single metric function (the `tf.metric.meanSquaredError()`
    call in [listing 5.10](#ch05ex10)) to encapsulate the loss for both predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: But why do we scale the 0–1 indicator by `CANVAS_SIZE`? Well, if we didn’t do
    this scaling, our model would end up generating a number in the neighborhood of
    0–1 as an indicator for whether it predicts the shape to be a triangle (close
    to 0) or a rectangle (close to 1). The difference between numbers around the `[0,
    1]` interval would clearly be much smaller compared to the differences we get
    from comparing the true bounding box and the predicted ones, which are in the
    range of 0–224\. As a result, the error signal from the shape prediction would
    be totally overshadowed by the error signal from the bounding-box prediction,
    which would not help us get accurate shape predictions. By scaling the 0–1 shape
    indicator, we make sure the shape prediction and bounding-box prediction contribute
    about equally to the final loss value (the return value of `customLossFunction()`),
    so that when the model is trained, it will optimize both types of predictions
    at once. In exercise 4 at the end of this chapter, you are encouraged to experiment
    with this scaling yourself.^([[12](#ch05fn12)])
  prefs: []
  type: TYPE_NORMAL
- en: ^(12)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An alternative to the scaling and `meanSquaredError`-based approach here is
    to take the first column of `yPred` as the shape probability score and compute
    the binary cross entropy with the first column of `yTrue`. Then the binary cross
    entropy value can be summed together with the MSE calculated on the remaining
    columns of `yTrue` and `yPred`. But in this alternative approach, the cross entropy
    needs to be scaled properly to ensure the balance with the bounding-box loss,
    just like in our current approach. The scaling involves a free parameter whose
    value needs to be carefully selected. In practice, it becomes an additional hyperparameter
    of the model and requires time and compute resources to tune, which is a downside
    of the approach. We opted against the approach in favor of our current approach
    for simplicity.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'With the data prepared and the model and loss function defined, we are ready
    to train our model! The key parts of the model training code are shown in [listing
    5.11](#ch05ex11) (from simple-object-detection/train.js). Like the fine-tuning
    we’ve seen before ([section 5.1.3](#ch05lev2sec3)), the training proceeds in two
    phases: an initial phase, during which only the new head layers are trained, and
    a fine-tuning phase, during which the new head layers are trained together with
    the top few layers of the truncated MobileNet base. It should be noted that the
    `compile()` method must be invoked (again) right before the fine-tuning `fit()`
    call in order for the changes to the `trainable` property of the layers to take
    effect. If you run the training on your own machine, it’ll be easy to observe
    a significant downward jump in the loss values as soon as the fine-tuning phase
    starts, reflecting an increase in the capacity of the model and the adaptation
    of the unfrozen feature-extraction layers to the unique features in the object-detection
    data as a result of their unfreezing. The list of layers unfrozen during the fine-tuning
    is determined by the `fineTuningLayers` array, which is populated when we truncate
    the MobileNet (see the `loadTruncatedBase()` function in [listing 5.9](#ch05ex09)).
    These are the top nine layers of the truncated MobileNet. In exercise 3 at the
    end of the chapter, you can experiment with unfreezing fewer or more top layers
    of the base and observe how they change the accuracy of the model produced by
    the training process.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.11\. Phase two of training the object-detection model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '***1*** Uses a relatively high learning rate for the initial phase'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***2*** Performs the initial phase of transfer learning'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***3*** Fine-tuning phase starts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***4*** Unfreezes some layers for fine-tuning'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***5*** Uses a slightly lower learning rate for the fine-tuning phase'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***6*** During the fine-tuning phase, we reduce batchSize to avoid out-of-memory
    issues caused by the fact that backpropagation involves more weights and consumes
    more memory than the initial phase.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***7*** Performs the fine-tuning phase'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After the fine-tuning ends, the model is saved to the disk and is then loaded
    during the in-browser inference step (started by the `yarn watch` command). If
    you load a hosted model, or if you have spent the time and compute resources to
    train a reasonably good model on your own machine, the shape and bounding-box
    prediction you’ll see in the inference page should be fairly good (validation
    loss at <100 after 100 epochs of initial training and 200 epochs of fine-tuning).
    The inference results are good but not perfect (see the examples in [figure 5.13](#ch05fig13)).
    When you examine the results, keep in mind that the in-browser evaluation is a
    fair one and reflects the model’s true generalization power because the examples
    the trained model is tasked to solve in the browser are different from the training
    and validation examples that it has seen during the transfer-learning process.
  prefs: []
  type: TYPE_NORMAL
- en: 'To wrap up this section, we showed how a model trained previously on image
    classification can be applied successfully to a different task: object detection.
    In doing this, we demonstrated how to define a custom loss function to fit the
    “dual-task” (shape classification + bounding-box regression) nature of the object-detection
    problem and how to use the custom loss during model training. This example not
    only illustrates the basic principles behind object detection but also highlights
    the flexibility of transfer learning and the range of problems it may be used
    on. Object-detection models used in production applications are, of course, more
    complex and involve more tricks than the toy example we built using a synthesized
    dataset here. [Info box 5.3](#ch05sb03) briefly presents some interesting facts
    about advanced object-detection models, and describes how they are different from
    the simple example you just saw and how you can use one of them through TensorFlow.js.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '**Production object-detection models**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](05fig14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: An example object-detection result from the TensorFlow.js version of the Single-Shot
    Detection (SSD) model. Notice the multiple bounding boxes and their associated
    object class and confidence scores.
  prefs: []
  type: TYPE_NORMAL
- en: 'Object detection is an important task of interest to many types of applications,
    such as image understanding, industrial automation, and self-driving cars. The
    most well-known state-of-the-art object-detection models include the Single-Shot
    Detection^([[a](#ch05fn2a)]) (SSD, for which an example inference result is shown
    in the figure) and You Only Look Once (YOLO).^([[b](#ch05fn2b)]) These models
    are similar to the model we saw in our simple-object-detection example in the
    following regards:'
  prefs: []
  type: TYPE_NORMAL
- en: ^a
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Wei Liu et al., “SSD: Single Shot MultiBox Detector,” *Lecture Notes in Computer
    Science* 9905, 2016, [http://mng.bz/G4qD](http://mng.bz/G4qD).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ^b
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Joseph Redmon et al., “You Only Look Once: Unified, Real-Time Object Detection,”
    *Proceedings IEEE Conference on Computer Vision and Pattern Recognition* (CVPR),
    2016, pp. 779–788, [http://mng.bz/zlp1](http://mng.bz/zlp1).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: They predict both the class and location of objects.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are built on pretrained image-classification models such as MobileNet and
    VGG16^([[c](#ch05fn2c)]) and are trained through transfer learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ^c
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: Karen Simonyan and Andrew Zisserman, “Very Deep Convolutional Networks for Large-Scale
    Image Recognition,” submitted 4 Sept. 2014, [https://arxiv.org/abs/1409.1556](https://arxiv.org/abs/1409.1556).
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'However, they are also different from our toy model in many regards:'
  prefs: []
  type: TYPE_NORMAL
- en: Real object-detection models predict many more classes of objects than our simple
    model (for example, the COCO dataset has 80 object categories; see [http://cocodataset.org/#home](http://cocodataset.org/#home)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are capable of detecting multiple objects in the same image (see the example
    figure).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Their model architectures are more complex than the one in our simple model.
    For example, the SSD model adds multiple new heads on top of a truncated pretrained
    image model in order to predict the class confidence score and bounding boxes
    for multiple objects in the input image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Instead of using a single `meanSquaredError` metric as the loss function, the
    loss function of a real object-detection model is a weighted sum of two types
    of losses: 1) a softmax cross-entropy-like loss for the probability scores predicted
    for object classes and 2) a `meanSquaredError` or `meanAbsoluteError`-like loss
    for bounding boxes. The relative weight between the two types of loss values is
    carefully tuned to ensure balanced contributions from both sources of error.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real object-detection models produce a large number of candidate bounding boxes
    per input image. These bounding boxes are “pruned” so that the ones with the highest
    object-class probability scores are retained in the final output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some real object-detection models incorporate “prior knowledge” about the location
    of object bounding boxes. These are educated guesses for where the bounding boxes
    are in the image, based on analysis of a larger number of labeled real images.
    The priors help speed up the training of the models by starting from a reasonable
    initial state instead of from complete random guesses (as is in our simple-object-detection
    example).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A few real object-detection models have been ported to TensorFlow.js. For example,
    one of the best ones you can play with is in the coco-ssd directory of the tfjs-models
    repository. To see it in action, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are interested in learning more about real object-detection models,
    you can read the following blog posts. They are for the SSD model and YOLO model,
    respectively, which use different model architecture and postprocessing techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '“Understanding SSD MultiBox—Real-Time Object Detection In Deep Learning” by
    Eddie Forson: [http://mng.bz/07dJ](http://mng.bz/07dJ).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '“Real-time Object Detection with YOLO, YOLOv2, and now YOLOv3” by Jona-than
    Hui: [http://mng.bz/KEqX](http://mng.bz/KEqX).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: So far in this book, we’ve tackled machine-learning datasets that are handed
    to us and ready to be explored. They are well-formatted, having been cleaned up
    through the painstaking work by data scientists and machine-learning researchers
    before us, to the degree that we can focus on modeling without worrying too much
    about how to ingest the data and whether the data is correct. This is true for
    the MNIST and audio datasets used in this chapter; it’s also certainly true for
    the phishing-website and iris-flower datasets we used in [chapter 3](kindle_split_014.html#ch03).
  prefs: []
  type: TYPE_NORMAL
- en: We can safely say that this is *never* the case for real-world machine-learning
    problems you will encounter. Most of a machine-learning practitioner’s time is
    in fact spent on acquiring, preprocessing, cleaning, verifying, and formatting
    the data.^([[13](#ch05fn13)]) In the next chapter, we’ll teach you the tools available
    in TensorFlow.js to make these data-wrangling and ingestion workflows easier.
  prefs: []
  type: TYPE_NORMAL
- en: ^(13)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Gil Press, “Cleaning Big Data: Most Time-Consuming, Least Enjoyable Data Science
    Task, Survey Says,” *Forbes*, 23 Mar. 2016, [http://mng.bz/9wqj](http://mng.bz/9wqj).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When we visited the mnist-transfer-cnn example in [section 5.1.1](#ch05lev2sec1),
    we pointed out that setting the `trainable` property of a model’s layers won’t
    take effect during training, unless the model’s `compile()` method is called before
    the training. Verify that by making some changes to the `retrainModel()` method
    in the index.js file of the example. Specifically,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a `this.model.summary()` call right before the line with `this.model .compile()`,
    and observe the numbers of trainable and nontrainable parameters. What do they
    show? How are they different from the numbers you get after the `compile()` call?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Independent from the previous item, move the `this.model.compile()` call to
    the part right before the setting of the `trainable` property of the feature layers.
    In other words, set the property of those layers after the `compile()` call. How
    does that change the training speed? Is the speed consistent with only the last
    several layers of the model being updated? Can you find other ways to confirm
    that, in this case, the weights of the first several layers of the models are
    updated during training?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'During the transfer learning in [section 5.1.1](#ch05lev2sec1) ([listing 5.1](#ch05ex01)),
    we froze the first two conv2d layers by setting their `trainable` properties to
    `false` before starting the `fit()` call. Can you add some code to the index.js
    in the mnist-transfer-cnn example to verify that the weights of the conv2d layers
    are indeed unaltered by the `fit()` call? Another approach we experimented with
    in the same section was calling `fit()` without freezing the layers. Can you verify
    that the weight values of the layers are indeed altered by the `fit()` call in
    that case? (Hint: recall that in [section 2.4.2](kindle_split_013.html#ch02lev2sec17)
    of [chapter 2](kindle_split_013.html#ch02), we used the `layers` attribute of
    a model object and its `getWeights()` method to access the value of weights.)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert the Keras MobileNetV2^([[14](#ch05fn14)]) (not MobileNetV1!—we already
    did that) application into the TensorFlow.js format, and load it into TensorFlow.js
    in the browser. Refer to [info box 5.1](#ch05sb01) for detailed steps. Can you
    use the `summary()` method to examine the topology of MobileNetV2 and identify
    its main differences from MobileNetV1?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ^(14)
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Mark Sandler et al., “MobileNetV2: Inverted Residuals and Linear Bottlenecks,”
    revised 21 Mar. 2019, [https://arxiv.org/abs/1801.04381](https://arxiv.org/abs/1801.04381).'
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: One of the important things about the fine-tuning code in [listing 5.8](#ch05ex08)
    is that the `compile()` method of the model is called again after unfreezing the
    dense layer in the base model. Can you do the following?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the same method from exercise 2 to verify that the weights (kernel and bias)
    of the dense layer are indeed not altered by the first `fit()` call (the one for
    the initial phase of transfer learning) and that they indeed are by the second
    `fit()` call (the one for the fine-tuning phase).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Try commenting out the `compile()` call after the unfreezing line (the line
    that changes the value of the trainable attribute) and see how that affects the
    weight value changes you just observed. Convince yourself that the `compile()`
    call is indeed necessary for letting changes in the frozen/unfrozen states of
    the model take effect.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Change the code and try unfreezing more weight-carrying layers of the base speech-command
    model (for instance, the conv2d layer before the second-last dense layer) and
    see how that affects the outcome of the fine-tuning.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the custom loss function we defined for the simple object-detection task,
    we scaled the 0–1 shape label so the error signal from the shape prediction could
    match the error signal from the bounding-box prediction (see [listing 5.10](#ch05ex10)).
    Experiment with what happens if this scaling is not done by removing the `mul()`
    call in the code in [listing 5.10](#ch05ex10). Convince yourself that this scaling
    is necessary for ensuring reasonably accurate shape predictions. This can also
    be done by simply replacing the instances of `customLossFunction` with `meanSquaredError`
    during the `compile()` call (see [listing 5.11](#ch05ex11)). Also note that removal
    of the scaling during training needs to be accompanied by a change in the thresholding
    during inference time: change the threshold from `CANVAS_SIZE/2` to `1/2` in the
    inference logic (in simple-object-detection/index.js).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The fine-tuning phase in the simple object-detection example involved unfreezing
    the nine top layers of the truncated MobileNet base (see how `fineTuningLayers`
    is populated in [listing 5.9](#ch05ex09)). A natural question to ask is, why nine?
    In this exercise, change the number of unfrozen layers by including fewer or more
    layers in the `fineTuningLayers` array. What do you expect to see in the following
    quantities when you unfreeze fewer layers during fine-tuning: 1) the final loss
    value and 2) the time each epoch takes in the fine-tuning phase? Does the experiment
    result match your expectations? How about unfreezing more layers during fine-tuning?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Transfer learning is the process of reusing a pretrained model or a part of
    it on a learning task related to, but different from, the one that the model was
    originally trained for. This reusing speeds up the new learning task.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In practical applications of transfer learning, people often reuse convnets
    that have been trained on very large classification datasets, such as MobileNet
    trained on the ImageNet dataset. Due to the sheer size of the original dataset
    and the diversity of the examples it contains, such pretrained models bring with
    them convolutional layers that are powerful, general-purpose feature extractors
    for a wide variety of compute-vision problems. Such layers are difficult, if not
    impossible, to train with the small amount of data that are available in typical
    transfer-learning problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We discussed several general approaches of transfer learning in TensorFlow.js,
    which differ from each other in terms of 1) whether new layers are created as
    the “new head” for transfer learning and 2) whether the transfer learning is done
    with one model instance or two. Each approach has its pros and cons and is suited
    for different use cases (see [table 5.1](#ch05table01)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By setting the `trainable` attribute of a model’s layer, we can prevent its
    weights from being updated during training (`Model.fit()` calls). This is referred
    to as freezing and is used to “protect” the base model’s feature-extraction layers
    during transfer learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In some transfer-learning problems, we can boost the new model’s performance
    by unfreezing a few top layers of the base model after an initial phase of training.
    This reflects the adaptation of the unfrozen layers to the unique features in
    the new dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transfer learning is a versatile and flexible technique. The base model can
    help us solve problems that are different from the one that it is originally trained
    on. We illustrated this point by showing how to train an object-detection model
    based on MobileNet.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loss functions in TensorFlow.js can be defined as custom JavaScript functions
    that operate on tensor inputs and outputs. As we showed in the simple object-detection
    example, custom loss functions are often needed to solve practical machine-learning
    problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
