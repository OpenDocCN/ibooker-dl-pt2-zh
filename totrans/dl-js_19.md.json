["```js\ngit clone https://github.com/tensorflow/tfjs-examples.git\ncd tfjs-examples/cart-pole\nyarn && yarn watch\n```", "```js\ncreateModel(hiddenLayerSizes) {                      ***1***\n    if (!Array.isArray(hiddenLayerSizes)) {\n      hiddenLayerSizes = [hiddenLayerSizes];\n    }\n    this.model = tf.sequential();\n    hiddenLayerSizes.forEach((hiddenLayerSize, i) => {\n      this.model.add(tf.layers.dense({\n        units: hiddenLayerSize,\n        activation: 'elu',\n        inputShape: i === 0 ? [4] : undefined        ***2***\n      }));\n    });\n    this.model.add(tf.layers.dense({units: 1}));     ***3***\n  }\n}\n```", "```js\ngetLogitsAndActions(inputs) {\n  return tf.tidy(() => {\n    const logits = this.policyNet.predict(inputs);\n\n    const leftProb = tf.sigmoid(logits);         ***1***\n    const leftRightProbs = tf.concat(            ***2***\n        [leftProb, tf.sub(1, leftProb)], 1);     ***2***\n    const actions = tf.multinomial(              ***3***\n        leftRightProbs, 1, null, true);          ***3***\n    return [logits, actions];\n  });\n}\n```", "```js\n  getGradientsAndSaveActions(inputTensor) {\n    const f = () => tf.tidy(() => {\n      const [logits, actions] =\n          this.getLogitsAndActions(inputTensor);                       ***1***\n      this.currentActions_ = actions.dataSync();\n      const labels =\n          tf.sub(1, tf.tensor2d(this.currentActions_, actions.shape));\n      return tf.losses.sigmoidCrossEntropy(                            ***2***\n          labels, logits).asScalar();                                  ***2***\n    });\n    return tf.variableGrads(f);                                        ***3***\n  }\n```", "```js\n  async train(\n      cartPoleSystem, optimizer, discountRate, numGames, maxStepsPerGame) {\n    const allGradients = [];\n    const allRewards = [];\n    const gameSteps = [];\n    onGameEnd(0, numGames);\n    for (let i = 0; i < numGames; ++i) {                              ***1***\n      cartPoleSystem.setRandomState();                                ***2***\n      const gameRewards = [];\n      const gameGradients = [];\n      for (let j = 0; j < maxStepsPerGame; ++j) {                     ***3***\n        const gradients = tf.tidy(() => {\n          const inputTensor = cartPoleSystem.getStateTensor();\n          return this.getGradientsAndSaveActions(                     ***4***\n              inputTensor).grads;                                     ***4***\n        });\n        this.pushGradients(gameGradients, gradients);\n        const action = this.currentActions_[0];\n        const isDone = cartPoleSystem.update(action);                 ***5***\n\n        await maybeRenderDuringTraining(cartPoleSystem);\n\n        if (isDone) {\n          gameRewards.push(0);\n          break;\n        } else {\n          gameRewards.push(1);                                        ***6***\n        }\n      }\n      onGameEnd(i + 1, numGames);\n      gameSteps.push(gameRewards.length);\n      this.pushGradients(allGradients, gameGradients);\n      allRewards.push(gameRewards);\n      await tf.nextFrame();\n    }\n    tf.tidy(() => {\n      const normalizedRewards =                                       ***7***\n          discountAndNormalizeRewards(allRewards, discountRate);      ***7***\n      optimizer.applyGradients(                                       ***8***\n          scaleAndAverageGradients(allGradients, normalizedRewards)); ***8***\n    });\n    tf.dispose(allGradients);\n    return gameSteps;\n  }\n```", "```js\ngit clone https://github.com/tensorflow/tfjs-examples.git\ncd tfjs-examples/snake-dqn\nyarn\nyarn watch\n```", "```js\nconst game = new SnakeGame({height, width, numFruits, initLen});\n```", "```js\nconst {state, reward, done, fruitEaten} = game.step(action);\n```", "```js\nexport function createDeepQNetwork(h, w, numActions) {\n  const model = tf.sequential();\n  model.add(tf.layers.conv2d({                            ***1***\n    filters: 128,\n    kernelSize: 3,\n    strides: 1,\n    activation: 'relu',\n    inputShape: [h, w, 2]                                 ***2***\n  }));\n  model.add(tf.layers.batchNormalization());              ***3***\n  model.add(tf.layers.conv2d({\n    filters: 256,\n    kernelSize: 3,\n    strides: 1,\n    activation: 'relu'\n  }));\n  model.add(tf.layers.batchNormalization());\n  model.add(tf.layers.conv2d({\n    filters: 256,\n    kernelSize: 3,\n    strides: 1,\n    activation: 'relu'\n  }));\n  model.add(tf.layers.flatten());                         ***4***\n  model.add(tf.layers.dense({units: 100, activation: 'relu'}));\n  model.add(tf.layers.dropout({rate: 0.25}));             ***5***\n  model.add(tf.layers.dense({units: numActions}));\n  return model;\n}\n```", "```js\n    x = Sample a random number uniformly between 0 and 1.\n    if x < epsilon:\n      Choose an action randomly\n    else:\n      qValues = DQN.predict(observation)\n      Choose the action that corresponds to the maximum element of qValues\n```", "```js\nlet action;\nconst state = this.game.getState();\nif (Math.random() < this.epsilon) {\n  action = getRandomAction();                          ***1***\n} else {\n  tf.tidy(() => {                                      ***2***\n    const stateTensor =                                ***2***\n        getStateTensor(state,                          ***2***\n                       this.game.height,               ***2***\n                       this.game.width);               ***2***\n    action = ALL_ACTIONS[\n        this.onlineNetwork.predict(                    ***3***\n            stateTensor).argMax(-1).dataSync()[0]];    ***3***\n  });\n}\n```", "```js\nconst batch = this.replayMemory.sample(batchSize);       ***1***\nconst stateTensor = getStateTensor(\n    batch.map(example => example[0]),                    ***2***\n    this.game.height, this.game.width);\nconst actionTensor = tf.tensor1d(\n    batch.map(example => example[1]),                    ***3***\n    'int32');\nconst qs = this.onlineNetwork.apply(                     ***4***\n    stateTensor, {training: true})                       ***4***\n    .mul(tf.oneHot(actionTensor, NUM_ACTIONS)).sum(-1);  ***5***\n```", "```js\nconst rewardTensor = tf.tensor1d(\n    batch.map(example => example[2]));             ***1***\nconst nextStateTensor = getStateTensor(\n    batch.map(example => example[4]),              ***2***\n    this.game.height, this.game.width);\nconst nextMaxQTensor =\n    this.targetNetwork.predict(nextStateTensor)    ***3***\n    .max(-1);                                      ***4***\nconst doneMask = tf.scalar(1).sub(\n    tf.tensor1d(batch.map(example => example[3]))\n        .asType('float32'));                       ***5***\nconst targetQs =                                   ***6***\n    rewardTensor.add(nextMaxQTensor.mul(           ***6***\n        doneMask).mul(gamma));                     ***6***\n```", "```js\ntrainOnReplayBatch(batchSize, gamma, optimizer) {\n  const batch = this.replayMemory.sample(batchSize);              ***1***\n  const lossFunction = () => tf.tidy(() => {                      ***2***\n    const stateTensor = getStateTensor(\n        batch.map(example => example[0]),\n                  this.game.height,\n                  this.game.width);\n\n    const actionTensor = tf.tensor1d(\n        batch.map(example => example[1]), 'int32');\n    const qs = this.onlineNetwork                                 ***3***\n        .apply(stateTensor, {training: true})\n        .mul(tf.oneHot(actionTensor, NUM_ACTIONS)).sum(-1);\n\n    const rewardTensor = tf.tensor1d(batch.map(example => example[2]));\n    const nextStateTensor = getStateTensor(\n        batch.map(example => example[4]),\n                  this.game.height, this.game.width);\n    const nextMaxQTensor =\n        this.targetNetwork.predict(nextStateTensor).max(-1);\n    const doneMask = tf.scalar(1).sub(\n        tf.tensor1d(batch.map(example => example[3])).asType('float32'));\n    const targetQs =                                              ***4***\n        rewardTensor.add(nextMaxQTensor.mul(doneMask).mul(gamma));\n    return tf.losses.meanSquaredError(targetQs, qs);              ***5***\n  });\n  const grads = tf.variableGrads(                                 ***6***\n      lossFunction, this.onlineNetwork.getWeights());\n  optimizer.applyGradients(grads.grads);                          ***7***\n  tf.dispose(grads);\n}\n```", "```js\nyarn train --logDir /tmp/snake_logs\n```", "```js\npip install tensorboard tensorboard --logdir /tmp/snake_logs\n```"]