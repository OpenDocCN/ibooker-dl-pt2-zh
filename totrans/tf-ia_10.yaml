- en: '8 Telling things apart: Image segmentation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Understanding segmentation data and working with it in Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a fully fledged segmentation data pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing an advanced segmentation model (DeepLab v3)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compiling models with custom-built image segmentation loss functions/metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training the image segmentation model on the clean and processed image data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the trained segmentation model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the last chapter, we learned about various advanced computer vision models
    and techniques to push the performance of an image classifier. We learned about
    the architecture of Inception net v1 as well as its successors (e.g., Inception
    net v2, v3, and v4). Our objective was to lift the performance of the model on
    an image classification data set with 64 × 64-sized RGB images of objects belonging
    to 200 different classes. While trying to train a model on this data set, we learned
    many important concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Inception blocks*—A way to group convolutional layers having different-sized
    windows (or kernels) to encourage learning features at different scales while
    making the model parameter efficient due to the smaller-sized kernels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Auxiliary outputs*—Inception net uses a classification layer (i.e., a fully
    connected layer with softmax activation) not only at the end of the network, but
    also in the middle of the network. This enables the gradients from the final layer
    to flow strongly all the way to the first layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Augmenting data*—Using various image transformation techniques (adjusting
    brightness/contrast, rotating, translating, etc.) to increase the amount of labeled
    data using the tf.keras.preprocessing.image.ImageDataGenerator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Dropout*—Switching on and off nodes in the layers randomly. This forces the
    neural networks to learn more robust features as the network does not always have
    all the nodes activated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Early stopping*—Using the performance on the validation data set as a way
    to control when the training stops. If the validation performance has not increased
    in a certain number of epochs, training is halted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Transfer learning*—Downloading and using a pretrained model (e.g., Inception-ResNet
    v2) trained on a larger, similar data set as the initialization and fine-tuning
    it to perform well on the task at hand.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn about another important task in computer vision:
    image segmentation. In image classification, we only care if an object exists
    in a given image. Image segmentation, on the other hand, recognizes multiple objects
    in the same image as well as where they are in the image. It is a very important
    topic of computer vision, and applications like self-driving cars live and breathe
    image segmentation models. Self-driving cars need to precisely locate objects
    in their surroundings, which is where image segmentation comes into play. As you
    might have guessed already, they also have their roots in many other applications:'
  prefs: []
  type: TYPE_NORMAL
- en: Image retrieval
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying galaxies ([http://mng.bz/gwVx](http://mng.bz/gwVx))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Medical image analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are a computer vision/deep learning engineer/researcher working on image-related
    problems, there is a high chance that your path will cross with image segmentation.
    Image segmentation models classify each pixel in the image to one of a predefined
    set of object categories. Image segmentation has ties to the image classification
    task we saw earlier. Both solve a classification task. Additionally, pretrained
    image classification models are used as the backbone of segmentation models, as
    they can provide crucial image features at different granularities to solve the
    segmentation task better and faster. A key difference is that image classifiers
    are solving a sparse prediction task, where each image has a single class label
    associated, as opposed to segmentation models that solve a dense prediction task
    that has a class label associated with every pixel in the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Any image segmentation algorithm can be classified as one of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Semantic segmentation*—The algorithm is only interested in identifying different
    categories of objects present in the image. For example, if there are multiple
    persons in the image, the pixels corresponding to all of them will be tagged with
    the same class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Instance segmentation*—The algorithm is interested in identifying different
    objects separately. For example, if there are multiple persons in the image, pixels
    belonging to each person are represented by a unique class. Instance-based segmentation
    is considered more difficult than semantic segmentation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figure 8.1 depicts the difference between the data found in a semantic segmentation
    task and an instance-based segmentation task. In this chapter, we will focus on
    semantic segmentation ([http://mng.bz/5QAZ](http://mng.bz/5QAZ)).
  prefs: []
  type: TYPE_NORMAL
- en: '![08-01](../../OEBPS/Images/08-01.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 Semantic segmentation versus instance segmentation
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look at the data we are dealing with more closely.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1 Understanding the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You are experimenting with a startup idea. The idea is to develop a navigation
    algorithm for small remote-control (RC) toys. Users can choose between how safe
    or adventurous the navigation needs to be. As the first step, you plan to develop
    an image segmentation model. The output of the image segmentation model will later
    feed to a different model that will predict the navigation path depending on what
    the user requests.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this task, you feel the Pascal VOC 2012 data set will be a good fit as
    it mostly comprises indoor and outdoor images that are found in urban/domestic
    environments. It contains pairs of images: an input image containing some objects
    and an annotated image. In the annotated image, each pixel has an assigned color,
    depending on which object that pixel belongs to. Here, you plan to download the
    data set and load the data successfully into Python.'
  prefs: []
  type: TYPE_NORMAL
- en: After having a good understanding/framing of the problem you want to solve,
    your next focus point should be understanding and exploring the data. Segmentation
    data is different from the image classification data sets we’ve seen thus far.
    One major difference is that both the input and target are images. The input image
    is a standard image, similar to what you’d find in an image classification task.
    Unlike in image classification, the target is not a label, but an image, where
    each pixel has a color from a predefined palette of colors. In other words, each
    object we’re interested in segmenting is assigned a color. Then a pixel corresponding
    to that object in the input image is colored with that color. The number of available
    colors is the same as the number of different objects (plus background) that you’re
    interested in identifying (figure 8.2).
  prefs: []
  type: TYPE_NORMAL
- en: '![08-02](../../OEBPS/Images/08-02.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 Inputs and outputs of an image classifier versus an image segmentation
    model
  prefs: []
  type: TYPE_NORMAL
- en: For this task, we will be using the PASCAL VOC 2012 data set, which is popular
    and consists of real-world scenes. The data set has labels for 22 different classes,
    as outlined in table 8.1.
  prefs: []
  type: TYPE_NORMAL
- en: Table 8.1 Different classes and their respective labels in the PASCAL VOC 2012
    data set
  prefs: []
  type: TYPE_NORMAL
- en: '| **Class** | **Assigned Label** | **Class** | **Assigned Label** |'
  prefs: []
  type: TYPE_TB
- en: '| Background | 0 | Dining table | 11 |'
  prefs: []
  type: TYPE_TB
- en: '| Aeroplane | 1 | Dog | 12 |'
  prefs: []
  type: TYPE_TB
- en: '| Bicycle | 2 | Horse | 13 |'
  prefs: []
  type: TYPE_TB
- en: '| Bird | 3 | Motorbike | 14 |'
  prefs: []
  type: TYPE_TB
- en: '| Boat | 4 | Person | 15 |'
  prefs: []
  type: TYPE_TB
- en: '| Bottle | 5 | Potted plant | 16 |'
  prefs: []
  type: TYPE_TB
- en: '| Bus | 6 | Sheep | 17 |'
  prefs: []
  type: TYPE_TB
- en: '| Car | 7 | Sofa | 18 |'
  prefs: []
  type: TYPE_TB
- en: '| Cat | 8 | Train | 19 |'
  prefs: []
  type: TYPE_TB
- en: '| Chair | 9 | TV/monitor | 20 |'
  prefs: []
  type: TYPE_TB
- en: '| Cow | 10 | Boundaries/unknown object | 255 |'
  prefs: []
  type: TYPE_TB
- en: The white pixels represent object boundaries or unknown objects. Figure 8.3
    illustrates the data set by showing a sample for every single object class present.
  prefs: []
  type: TYPE_NORMAL
- en: '![08-03](../../OEBPS/Images/08-03.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 Samples from the PASCAL VOC 2012 data set. The data set shows a single
    example image, along with the annotated segmentation of it for the 20 different
    object classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In figure 8.4, diving a bit deeper, you can see a single sample datapoint (best
    viewed in color) up close. It has two objects: a chair and a dog. As it is shown,
    different colors are assigned to different object categories. While the figure
    is best viewed in color, you still can distinguish different objects by paying
    attention to the white border that outlines the objects in the figure.'
  prefs: []
  type: TYPE_NORMAL
- en: '![08-04](../../OEBPS/Images/08-04.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 An original input image in image segmentation and the corresponding
    target annotated/segmented image
  prefs: []
  type: TYPE_NORMAL
- en: First, we’ll download the data set, if it does not exist, from [http://mng.bz/6XwZ](http://mng.bz/6XwZ)
    (see the next listing).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.1 Downloading data
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Check if the file is already downloaded. If so, don’t download again.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Get the content from the URL.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Save the file to disk.
  prefs: []
  type: TYPE_NORMAL
- en: ❹ If the file exists but is not extracted, extract the file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data set download is quite similar to our past experience. The data exists
    as a tarfile. We download the file if it doesn’t exist and extract it. Next, we
    will discuss how to use the image library Pillow and NumPy to load the images
    into memory. Here, the target images will need special treatment, as you will
    see that they are not stored using the conventional approach. There are no surprises
    involved with loading input images to memory. Using the PIL (i.e., Pillow) library,
    they can be loaded with a single line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, you can inspect the image’s attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: It’s time to load the corresponding annotated/segmented target images. As mentioned
    earlier, target images require special attention. The target images are not stored
    as standard images but as *palettized* images. Palettization is a technique to
    reduce memory footprint while storing images with a fixed number of colors in
    the image. The crux of the method is to maintain a palette of colors. The palette
    is stored as a sequence of integers, which has a length of the number of colors
    or the number of channels. (E.g., in the case of RGB, where a pixel is made of
    three values corresponding to red, green, and blue, the number of channels is
    three. A grayscale image has a single channel, where each pixel is made of a single
    value). The image itself then stores an array of indices (size = height × width),
    where each index maps to a color in the palette. Finally, by mapping the palette
    indices from the image to palette colors, you can compute the original image.
    Figure 8.5 provides a visual exposition of this discussion.
  prefs: []
  type: TYPE_NORMAL
- en: '![08-05](../../OEBPS/Images/08-05.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 The numerical representation of input images and target images in
    the PASCAL VOC 2012 data set
  prefs: []
  type: TYPE_NORMAL
- en: The next listing shows the code for reconstructing the original image pixels
    from the palettized image.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.2 Reconstructing the original image from a palettized image
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Get the color palette from the image.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ The palette is stored as a vector. We reshape it to an array, where each row
    represents a single RGB color.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Get the image’s height and width.
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Convert the palettized image stored as an array to a vector (helps with our
    next steps).
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Get the image as a vector if the image is provided as an array instead of
    a Pillow image.
  prefs: []
  type: TYPE_NORMAL
- en: ❻ We first define a vector of zeros that has the same length as our image. Then,
    for all the indices found in the image, we gather corresponding colors from the
    palette and assign them to the same position in the rgb_image.
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Restore the original shape.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we first obtain the palette of the image using the get_palette() function.
    This will be present as a one-dimensional array (of length number of classes ×
    number of channels). Next, we need to reshape the array to a (number of classes,
    number of channels)-sized array. In our case, this will be converted to a (22,3)-sized
    array. As we define the first dimension of the reshape as -1, it will be automatically
    inferred from the original size of the data and the other dimensions of the reshape
    operation. Finally, we define an array of zeros, which will ultimately store the
    actual colors the indices found in the image. To do that, we index the rgb_image
    vector using the image (which contains indices) and assign matching colors from
    the palette to those indices.
  prefs: []
  type: TYPE_NORMAL
- en: With the data we have looked at thus far, let’s define a TensorFlow data pipeline
    that can transform and convert the data to a format acceptable by the model.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 1
  prefs: []
  type: TYPE_NORMAL
- en: You have been provided with an rgb_image in RGB format, where each pixel belongs
    to one of n distinctive colors and has been given a palette called palette, which
    is a [n,3]-sized array. How would you convert the rgb_image to a palettized image?
  prefs: []
  type: TYPE_NORMAL
- en: 'Hint You can create the naïve solution by using three for loops: two loops
    to get a single pixel of rgb_image and then a final loop to traverse each color
    in the palette.'
  prefs: []
  type: TYPE_NORMAL
- en: '8.2 Getting serious: Defining a TensorFlow data pipeline'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far, we have discussed the data that will help us build a navigation algorithm
    for the RC toy. Before building a model, an important task to complete is having
    a scalable data ingestion method from disk to the model. Doing this upfront will
    save us a lot of time when we’re ready to scale or productionize. You think the
    best way is to implement a tf.data pipeline to retrieve images from the disk,
    preprocess them, transform them, and have them ready for the model to grab them.
    This pipeline should read images in, reshape them to a fixed size (in the case
    of variable-sized images), augment them (during the training stage), batch them,
    and repeat this process for a desired number of epochs. Finally, we will define
    three pipelines: a training data pipeline, a validation data pipeline, and a testing
    data pipeline.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our goal at the end of the data exploration stage should be to build a reliable
    data pipeline from the disk to the model. This is what we will be looking at here.
    At a high level, we will build a TensorFlow data pipeline that will perform the
    following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Get the filenames belonging to a certain subset (e.g., training, validation,
    or testing).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Read the specified images from the disk.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocess the images (this involves normalizing/resizing/cropping images).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform augmentation on the images to increase the volume of data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batch the data in small batches.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimize data retrieval using several built-in optimization techniques.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As the first step, we will write a function that returns a generator that will
    generate filenames of the data that we want to be fetched. We will also provide
    the ability to specify which subset the user wants to be fetched (e.g., training,
    validation, or testing). Returning data through a generator will make writing
    a tf.data pipeline easier (see the following listing).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.3 Retrieving the filenames for a given subset of data
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Read the CSV file that contains the training instance filenames.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ For validation/test subsets, perform a one-time shuffle to make sure we get
    a good mix with a fixed seed.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Read the CSV file that contains validation/test filenames.
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Shuffle the data after fixing the seed.
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Get the first half as the validation set.
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Get the second half as the test set.
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Form absolute paths to the input image files we captured (depending on the
    subset argument).
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Return the filename pairs (input and annotations) as a generator.
  prefs: []
  type: TYPE_NORMAL
- en: ❾ Form absolute paths to the segmented image files.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see that we’re passing a few arguments when reading the CSV files.
    These arguments characterize the file we’re reading. These files are extremely
    simple and contain just a single image filename on a single line. index_col=None
    means that the file does not have an index column, header=None means there is
    no header in the file, and squeeze=True means that the output will be presented
    as a pandas Series, not a pandas Dataframe. With that, we can define a TensorFlow
    data set (tf.data.Dataset) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: TensorFlow has several different functions for generating data sets using different
    sources. As we have defined the function get_subset_filenames() to return a generator,
    we will use the tf.data.Dataset.from_generator() function. Note that we need to
    provide the format as well as the datatypes of the returned data, by the generator,
    using the output_types argument. The function subset_filename_gen_func returns
    two strings; therefore, we define output types as a tuple of two tf.string elements.
  prefs: []
  type: TYPE_NORMAL
- en: 'One other important aspect is the different txt files we read from depending
    on the subset. There are three different files in the relative path: the data\VOCtrainval_11-May-2012\VOCdevkit\VOC2012\ImageSets\Segmentation
    folder; train.txt, val.txt, and trainval.txt. Here, train.txt contains the filenames
    of the training images, whereas val.txt contains the filenames of the validation/testing
    images. We will use these files to create different pipelines that produce different
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: Where does tf.data come from?
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow’s tf.data pipeline can consume data from various sources. Here are
    some of the commonly used methods to retrieve data:'
  prefs: []
  type: TYPE_NORMAL
- en: tf.data.Dataset.from_generator(gen_fn)—You have already seen this function in
    action. If you have a generator (i.e., gen_fn) that produces data, you want it
    to be processed through a tf.data pipeline. This is the easiest method to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'tf.data.Dataset.from_tensor_slices(t)—This is a very useful function if you
    have data already loaded as a big matrix. t can be an N-dimensional matrix, and
    this function will extract element by element on the first dimension. For example,
    assume that you have loaded a tensor t of size 3 × 4 to memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Then you can easily set up a tf.data pipeline as follows. tf.data.Dataset.from_
    tensor_slices(t) *will return* [1,2,3,4], *then* [2,3,4,5], *and finally* [6,7,8,9]
    when you iterate this data pipeline. In other words, you are seeing one row (i.e.,
    a slice from the batch dimension, hence the name from_tensor_slices) at a time.
    You can now incorporate functions like tf.data.Dataset.batch() to get a batch
    of rows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now it’s time to read in the images found in the file paths we obtained in
    the previous step. TensorFlow has support to easily load an image, where the path
    to a filename is img_filename, using the functions tf.io.read_file and tf.image.decode_image.
    Here, img_filename is a tf.string (i.e., a string in TensorFlow):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use this pattern to load input images. However, we need to implement
    a custom image load function to load the target image. If you use the previous
    approach, it will automatically convert the image to an array with pixel values
    (instead of palette indices). But if we don’t perform that conversion, we will
    have a target array that is in the exact format we need because the palette indices
    that are in the target image are the actual class labels for each corresponding
    pixel in the input image. We will use PIL.Image within our TensorFlow data pipeline
    to load the image as a palettized image and avoid converting it to RGB:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'However, you can’t yet use custom functions as part of the tf.data pipeline.
    They need to be streamlined with the data-flow graph of the data pipeline by wrapping
    it as a TensorFlow operation. This can be easily achieved by using the tf.numpy_function
    operation, which allows you to wrap a custom function that returns a NumPy array
    as a TensorFlow operation. If we have the target image’s file path represented
    by y, you can use the following code to load the image into TensorFlow with a
    custom image-loading function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The dark side of tf.numpy_function
  prefs: []
  type: TYPE_NORMAL
- en: NumPy has larger coverage for various scientific computations than TensorFlow,
    so you might think that tf.numpy_funtion makes things very convenient. This is
    not quite true, as you can infest your TensorFlow code with terrible performance
    degradations. When TensorFlow executes NumPy code, it can create very inefficient
    data flow graphs and introduce overheads. Therefore, always try to stick to TensorFlow
    operations and use custom NumPy code only if you have to. In our case, since there
    is no alternative way for us to load a palletized image without mapping palletized
    values to actual RGB, we used a custom function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice how we’re passing both the input (i.e., inp=[y]) and its data type (i.e.,
    Tout=[tf.uint8]) to this function. They both need to be in the form of a Python
    list. Finally, let’s collate everything we discussed in one place:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The tf.data.Dataset.map() function will be used quite heavily throughout this
    discussion. You can find a lengthy explanation of the map() function in the sidebar.
  prefs: []
  type: TYPE_NORMAL
- en: 'A Refresher: tf.data.Dataset.map() function'
  prefs: []
  type: TYPE_NORMAL
- en: This tf.data pipeline will make extensive use of the tf.data.Dataset.map() function.
    Therefore, it is extremely helpful for us to remind ourselves what this function
    accomplishes.
  prefs: []
  type: TYPE_NORMAL
- en: The td.data.Dataset.map() function applies a given function or functions across
    all the records in a data set. In other words, it transforms the data points in
    the data set using a specified transformation. For example, assume the tf.data.Dataset
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: to get the square of each element, you can use the map function as
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'If you have multiple elements in a single record, leveraging the flexibility
    of map(), you can transform them individually:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: As a normalization step we will bring the pixel values to [0,1] range by using
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Note that we are keeping our target image (y) as it is. Before I continue with
    any more steps in our pipeline, I want to direct your attention to an important
    matter. This is a caveat that is quite common, and it is thus worthwhile to be
    aware of it. After the step we just completed, you might feel like, if you want,
    you can batch the data and feed it to the model. For example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'If you do that for this data set, you will get an error like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This is because you ignored a crucial characteristic and a sanity check of the
    data set. Unless you’re using a curated data set, you are unlikely to find images
    with the same dimensions. If you look at images in the data set, you will notice
    that they are not of the same size; they have different heights and widths. In
    TensorFlow, unless you use a special data structure like tf.RaggedTensor, you
    cannot batch unequally sized images together. That is exactly what TensorFlow
    is complaining about in the error.
  prefs: []
  type: TYPE_NORMAL
- en: To alleviate the problem, we need to bring all the images to a standard size
    (see listing 8.4). To do that, we will define the following function. It will
    either
  prefs: []
  type: TYPE_NORMAL
- en: Resize the image to a larger size (resize_to_before_crop) and then crop the
    image to the desired size (input_size) or
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resize the image to the desired size (input_size)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Listing 8.4 Bringing images to a fixed size using random cropping or resizing
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Define a function to randomly crop images after resizing.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Resize the input image using bilinear interpolation to a larger size.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Resize the target image using the nearest interpolation to a larger size.
  prefs: []
  type: TYPE_NORMAL
- en: ❹ To resize, we first swap the axis of y as it has the shape [1, height, width].
    We convert this back to [height, width, 1] (i.e., a single channel image) using
    the tf.transpose() function.
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Define a random variable to offset images on height during cropping.
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Define a random variable to offset images on width during cropping.
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Crop the input image and the target image using the same cropping parameters.
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Resize both the input image and the target image to a desired size (no cropping).
  prefs: []
  type: TYPE_NORMAL
- en: ❾ Define a random variable (used to perform augmentations).
  prefs: []
  type: TYPE_NORMAL
- en: ❿ If augmentation is enabled and the resized image is larger than the input
    size we requested, perform augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: ⓫ During augmentation, the rand_crop or resize function is executed randomly.
  prefs: []
  type: TYPE_NORMAL
- en: ⓬ If augmentation is disabled, only resize images.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we define a function called randomly_crop_or_resize, which has two nested
    functions, rand_crop and resize. The rand_crop first resizes the image to the
    size specified in resize_to_before_crop and creates a random crop. It is imperative
    to check that you applied the exact same crop to both the input and the target.
    For example, same-crop parameters should be used to crop both the input and the
    target. In order to crop images, we use
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The arguments are self-explanatory: image takes an image to be cropped, offset_height
    and offset_width decide the starting point for the crop, and target_height and
    target_width specify the final size after the crop. The resize function will simply
    resize the input and the target to a specified size using the tf.image.resize
    operation.'
  prefs: []
  type: TYPE_NORMAL
- en: When resizing, we use *bilinear interpolation* for the input images and *nearest
    interpolation* for targets. Bilinear interpolation resizes the images by computing
    the resulting pixels, as an average of neighboring pixels, whereas nearest interpolation
    computes the output pixel as the nearest most common pixel from the neighbors.
    Bilinear interpolation leads to a smoother result after resizing. However, you
    must use nearest interpolation for the target image, as bilinear interpolation
    will lead to fractional outputs, corrupting the integer-based annotations. The
    interpolation techniques described are visualized in figure 8.6.
  prefs: []
  type: TYPE_NORMAL
- en: '![08-06](../../OEBPS/Images/08-06.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 Nearest interpolation and bilinear interpolation for both up-sampling
    and down-sampling tasks
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will introduce an additional step to the way we’re going to use these
    two nested functions. If augmentation is enabled, we want the cropping or resizing
    to take place randomly within the pipeline. We will define a random variable (drawn
    from a uniform distribution between 0 and 1) and perform crop or resize depending
    on the value of the random variable at a given time. This conditioning can be
    achieved using the tf.cond function, which takes three arguments and returns output
    according to these arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: Condition—This is a computation that results in a Boolean value (i.e., is the
    random variable rand greater than 0.5).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: true_fn—If the condition is true, then this function will be executed (i.e.,
    perform rand_crop on both x and y)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: false_fn—If the condition is false, then this function will be executed (i.e.,
    perform a resize on both x and y)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If augmentation is disabled (i.e., by setting the augmentation variable to
    False), only resizing is performed. With the details fleshed out, we can use the
    randomly_crop_ or_resize function in our data pipeline as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: At this point, we have a globally fixed-sized image coming out of our pipeline.
    The next thing we address is very important. Factors such as variable size of
    images and custom NumPy functions used to load images make it impossible for TensorFlow
    to infer the shape of its final tensor (though it’s a fixed-sized tensor) after
    a few steps. If you check the shapes of the tensors produced at this point, you
    will probably perceive them as
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This means that TensorFlow was unable to infer the shape of the tensors. To
    avoid any ambiguities or problems moving forward, we will set the shape of the
    output we have in the pipeline. For a tensor t, if the shape is ambiguous but
    you know the shape, you can set the shape manually using
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In our data pipeline, we can set the shape as
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We know that the outputs following the resize or crop are going to be
  prefs: []
  type: TYPE_NORMAL
- en: '*Input image*—An RGB image with input_size height and width'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Target image*—A single-channel image with input_size height and width'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will set the shape accordingly using the tf.data.Dataset.map() function.
    We cannot underestimate the power of data augmentation, so we will introduce several
    data augmentation steps to our data pipeline (see the next listing).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.5 Functions used for random augmentation of images
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Define a random variable.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Define a function to flip images deterministically.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Using the same pattern as before, we use tf.cond to randomly perform horizontal
    flipping.
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Randomly flip images in the data set.
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Randomly adjust the hue (i.e., color) of the input image (target stays the
    same).
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Randomly adjust the brightness of the input image (target stays the same).
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Randomly adjust the contrast of the input image (target stays the same).
  prefs: []
  type: TYPE_NORMAL
- en: 'In listing 8.5, we perform the following translations:'
  prefs: []
  type: TYPE_NORMAL
- en: Randomly flipping images horizontally
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Randomly changing the hue of the images (up to 10%)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Randomly changing the brightness of the images (up to 10%)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Randomly changing the contrast of the images (up to 20%)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'By using the tf.data.Dataset.map() function, we can easily perform the specified
    random augmentation steps, should the user enable augmentation in the pipeline
    (i.e., by setting the augmentation variable to True). Note that we’re performing
    some augmentations (e.g., random hue, brightness, and contrast adjustments) on
    the input image only. We will also give the user the option to have different-sized
    inputs and targets (i.e., outputs). This is achieved by resizing the output to
    a desired size, defined by the output_size argument. The model we use for this
    task has different-sized input and output dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, here we use the nearest interpolation to resize the target. Next, we
    will shuffle the data (if the user set the shuffle argument to True):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The shuffle function takes an important argument called buffer_size, which
    determines how many samples are loaded to memory in order to select a sample randomly.
    The higher the buffer_size, the more randomness you are introducing. On the other
    hand, a higher buffer_size implies higher memory consumption. It’s now time to
    batch the data, so instead of a single data point, we get a batch of data when
    we iterate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: This is done using the tf.data.Dataset.batch() function and passing the desired
    batch size as the argument. When using the tf.data pipeline, if you are running
    it for multiple epochs, you also need to use the tf.data.Dataset.repeat() function
    to repeat the pipeline for a given number of epochs.
  prefs: []
  type: TYPE_NORMAL
- en: Why do we need tf.data.Dataset.repeat()?
  prefs: []
  type: TYPE_NORMAL
- en: tf.data.Dataset is a generator. A unique characteristic of a generator is that
    you only can iterate it once. After the generator reaches the end of the sequence
    it’s iterating, it will exit by throwing an exception. Therefore, if you need
    to iterate through a generator multiple times, you need to redefine the generator
    as many times as needed. By adding tf.data.Dataset.repeat(epochs), the generate
    is redefined as many times as we would like (epochs times in this example).
  prefs: []
  type: TYPE_NORMAL
- en: 'One more step is needed before our tf.data pipeline is done and dusted. If
    you look at the shape of the target (y) output, you will see that it has a channel
    dimension of 1\. However, for the loss function we will be using, we need to get
    rid of that dimension:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'For this, we will use the tf.squeeze() operation, which removes any dimensions
    that are of size 1 and returns a tensor. For example, if you squeeze a tensor
    of size [1,3,2,1,5], you will get a [3,2,5] sized tensor. The final code is provided
    in listing 8.6\. You might notice two steps that are highlighted. These are two
    popular optimization steps available: caching and prefetching.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.6 The final tf.data pipeline
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: ❶ If augmentation is enabled, resize_to_before_crop needs to be defined.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Return a list of filenames depending on the subset of data requested.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Load the images into memory. cache() is an optimization step and will be discussed
    in the text.
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Normalize the input images.
  prefs: []
  type: TYPE_NORMAL
- en: ❺ The function that randomly crops or resizes images
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Perform random crop or resize on the images.
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Set the shape of the resulting images.
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Randomly perform various augmentations on the data.
  prefs: []
  type: TYPE_NORMAL
- en: ❾ Resize the output image if needed.
  prefs: []
  type: TYPE_NORMAL
- en: ❿ Shuffle the data using a buffer.
  prefs: []
  type: TYPE_NORMAL
- en: ⓫ Batch the data and repeat the process for a desired number of epochs.
  prefs: []
  type: TYPE_NORMAL
- en: ⓬ This is an optimization step discussed in detail in the text.
  prefs: []
  type: TYPE_NORMAL
- en: ⓭ Remove the unnecessary dimension from target images.
  prefs: []
  type: TYPE_NORMAL
- en: ⓮ Get the final tf.data pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'It wasn’t an easy journey, but it was a rewarding one. We have learned some
    important skills in defining the data pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining a generator that returns the filenames of the data to be fetched
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading images within a tf.data pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manipulating images (resizing, cropping, brightness adjustment, etc.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batching and repeating data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining multiple pipelines for different data sets with different requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will look at some optimization techniques to turn our mediocre data
    pipeline into an impressive data highway.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.1 Optimizing tf.data pipelines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'TensorFlow is a framework meant for consuming large data sets, where consuming
    data in an efficient manner is a key priority. One thing still missing from our
    conversation is what kind of optimization steps are available for tf.data pipelines,
    so let us nudge this discussion in that direction. Two steps were set in bold
    in listing 8.6: caching and prefetching. If you are interested in other optimization
    techniques, you can read more at [https://www.tensorflow.org/guide/data_performance](https://www.tensorflow.org/guide/data_performance).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Caching will store the data in memory as it flows through the pipeline. This
    means that, when cached, that step (e.g., loading the data from the disk) happens
    only in the first epoch. The subsequent epochs will read from the cached data
    that’s held in memory. Here, you can see that we’re caching the images after we
    load them to memory. This way, TensorFlow loads the images in the first epoch
    only:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Prefetching is another powerful weapon you have at your disposal, and it allows
    you to leverage the multiprocessing power of your device:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The argument provided to the function decides how much data is prefetched. By
    setting it to AUTOTUNE, TensorFlow will decide the best amount of data to be fetched
    depending on the resources available. Assume a simple data pipeline that loads
    images from the disk and trains a model. Then, the data read and model training
    will happen in interleaved steps. This leads to significant idling time, as the
    model idles while the data is loading, and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: However, thanks to prefetching, this doesn’t need to be the case. Prefetching
    employs background threads and an internal buffer to load the data in advance
    while the model is training. When the next iteration comes, the model can seamlessly
    continue the training as data is already fetched into the memory. The differences
    between sequential execution and prefetching are shown in figure 8.7.
  prefs: []
  type: TYPE_NORMAL
- en: '![08-07](../../OEBPS/Images/08-07.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 Sequential execution versus pre-fetching-based execution in model
    training
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will look at the finished tf.data pipeline for the image segmentation
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.2 The final tf.data pipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Finally, you can define the data pipeline(s) using the functions we have defined
    so far. Here, we define three different data pipelines for three different purposes:
    training, validation, and testing (see the following listing).'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.7 Creating the train/validation/test data pipelines instances
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Directory where the input images are
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Directory where the annotated images (targets) are
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Directory where the text files containing train/validation/test filenames
    are
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Define a reusable partial function from get_subset_filenames.
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Define three generators for train/validation/test data.
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Define input image size.
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Define a train data pipeline that uses data augmentation and shuffling.
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Define a validation data pipeline that doesn’t use data augmentation or shuffling.
  prefs: []
  type: TYPE_NORMAL
- en: ❾ Define a test data pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define several important paths:'
  prefs: []
  type: TYPE_NORMAL
- en: orig_dir—Directory containing input images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: seg_dir—Directory containing the target images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: subset_dir—Directory containing text files (train.txt, val.txt) that enlist
    training and validation instances, respectively
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then we will define a partial function from the get_subset_filenames() function
    we defined earlier so that we can get a generator just by setting the subset argument
    of the function. Using this technique, we will define three generators: train_subset_fn,
    val_subset_fn, and test_subset_fn. Finally, we will define three tf.data.Datasets
    using the get_subset_tf_dataset() function. Our pipelines will have the following
    characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Training pipeline*—Performs data augmentation and data shuffling on every
    epoch'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Validation pipeline and test pipeline*—No augmentation or shuffling'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The model we will define expects a 384 × 384-sized input and an output. In
    the training data pipeline, we will resize images to 444 × 444 and then randomly
    crop a 384 × 384-sized image. Following this, we will look at the core part of
    the solution: defining the image segmentation model.'
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 2
  prefs: []
  type: TYPE_NORMAL
- en: 'You have been given a small set of data that contains two tensors: tensor a
    contains 100 64 × 64 × 3-sized images (i.e., 100 × 64 × 64 × 3 shaped), and tensor
    b contains 100 32 × 32 × 1-sized segmentation masks (i.e., 100 × 32 × 32 × 1 shaped).
    You have been asked to define a tf.data.Dataset using the functions discussed
    that will'
  prefs: []
  type: TYPE_NORMAL
- en: Resize the segmentation masks to match the input image size (using nearest interpolation)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalize the input images using the transformation (x - 128)/255 where a single
    image is x
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batch the data to batches of 32 and repeat for five epochs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prefetch the data with an auto-tuning feature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '8.3 DeepLabv3: Using pretrained networks to segment images'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It’s now time to create the brains of the pipeline: the deep learning model.
    Based on feedback from a colleague at a self-driving car company working on similar
    problems, you will implement a DeepLab v3 model. This is a model built on the
    back of a pretrained ResNet 50 model (trained on image classification) but with
    the last several layers changed to perform *atrous convolution* instead of standard
    convolution. It uses a pyramidal aggregation module that uses atrous convolution
    at different scales to generate image features at different scales to produce
    the final output. Finally, it uses a bilinear interpolation layer to resize the
    final output to a desired size. You are confident that DeepLab v3 can deliver
    good initial results.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep neural network-based segmentation models can be broadly categorized into
    two types:'
  prefs: []
  type: TYPE_NORMAL
- en: Encoder decoder models (e.g., U-Net model)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fully convolutional network (FCN) followed by a pyramidal aggregation module
    (e.g., DeepLab v3 model)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A well-known example of the encoder-decoder model is the U-Net model. In other
    words, U-Net has an encoder that gradually creates smaller, coarser representations
    of the input. This is followed by a decoder that takes the representations the
    encoder built and gradually up-samples (i.e., increases the size of) the output
    until it reaches the size of the input image. The up-sampling is achieved through
    an operation known as *transpose convolution*. Finally, you train the whole structure
    end to end, where an input is the input image and the target is the segmentation
    mask for the corresponding image. We will not discuss this type of model in this
    chapter. However, I have included a detailed walkthrough in appendix B (along
    with an implementation of the model).
  prefs: []
  type: TYPE_NORMAL
- en: The other type of segmentation models introduces a special model that replaces
    the decoder. We call this module a *pyramidal aggregation module*. Its purpose
    is to garner spatial information at different scales (e.g., different-sized outputs
    from various interim convolution layers) that provides fine-grained contextual
    information about the objects present in the image. DeepLab v3 is a prime example
    of this approach. We will put the DeepLab v3 model under the microscope and use
    it to excel at the segmentation task.
  prefs: []
  type: TYPE_NORMAL
- en: Researchers and engineers gravitate toward methods that use pyramidal aggregation
    modules more. There could be many reasons for this. One lucrative reason is that
    there are less parameters in networks that use pyramidal aggregation than an encoder-decoder
    based counterpart. Another reason may be that, typically, introducing a novel
    module offers more flexibility (compared to an encoder-decoder) to engineer efficient
    and accurate feature extraction methods at multiple scales.
  prefs: []
  type: TYPE_NORMAL
- en: How important is the pyramidal aggregation module? To know that, we have to
    first understand what the fully convolutional part of the network looks like.
    Figure 8.8 illustrates the generic structure of such a segmentation model.
  prefs: []
  type: TYPE_NORMAL
- en: '![08-08](../../OEBPS/Images/08-08.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 General structure and organization of a fully convolutional network
    that uses a pyramidal aggregation module
  prefs: []
  type: TYPE_NORMAL
- en: The best way to understand the importance of the pyramidal aggregation module
    is to see what happens if we don’t have it. If that is the case, then the last
    convolutional layer will have the enormous and unrealistic responsibility of building
    the final segmentation mask (which is typically 16-32x times larger than the layer
    output). It is no surprise that there is a massive representational bottleneck
    between the final convolution layer and the final segmentation mask, leading to
    poor performance. The pyramidal structure typically enforced in CNNs results in
    a very small output width and height in the final layer.
  prefs: []
  type: TYPE_NORMAL
- en: The pyramidal aggregation module bridges this gap. It does so by combining several
    different interim outputs. This way, the network has ample fine-grained (from
    earlier layers) and coarser (from deeper layers) details to construct the desired
    segmentation mask. Fine-grained representations provide spatial/contextual information
    about the image, whereas the coarser representations provide high-level information
    about the image (e.g., what objects are present). By fusing both types of these
    representations, the task of generating the final output becomes more achievable.
  prefs: []
  type: TYPE_NORMAL
- en: Why not a skyscraper instead of a pyramid?
  prefs: []
  type: TYPE_NORMAL
- en: You might be tempted to ponder, if making the outputs smaller as you go causes
    loss of information, “Why not keep it the same size?” (hence the term *skyscraper*).
    This is an impractical solution for two main reasons.
  prefs: []
  type: TYPE_NORMAL
- en: First, decreasing the size of the outputs through pooling or striding is an
    important regularization method that forces the network to learn translation-invariant
    features (as we discussed in chapter 6). By taking this away, we can hinder the
    generalizability of the network.
  prefs: []
  type: TYPE_NORMAL
- en: Second, not decreasing the output size will increase the memory footprint of
    the model significantly. This will, in turn, restrict the depth of the network
    dramatically, making it more difficult to create deeper networks.
  prefs: []
  type: TYPE_NORMAL
- en: DeepLab v3 is the golden child of a lineage of models that emerged from and
    was introduced in the paper “Rethinking Atrous Convolution for Semantic Image
    Segmentation” ([https://arxiv.org/pdf/1706.05587.pdf](https://arxiv.org/pdf/1706.05587.pdf))
    by several researchers from Google.
  prefs: []
  type: TYPE_NORMAL
- en: Most segmentation models face an adverse side effect caused by a common and
    beneficial design principle. Vision models incorporate stride/pooling to make
    network translation invariant. But an ill-favored outcome of that is the compounding
    reduction of the size of the outputs produced. This typically leads to a final
    output that is 16-32 times smaller than the input. Being a dense prediction task,
    image segmentation tasks suffer heavily from this design idea. Therefore, most
    of the groundbreaking networks that have surfaced have been about solving this.
    The DeepLab model came into the world for exactly that purpose. Let’s now see
    how DeepLab v3 solves this problem.
  prefs: []
  type: TYPE_NORMAL
- en: DeepLab v3 uses a ResNet-50 ([https://arxiv.org/pdf/1512.03385.pdf](https://arxiv.org/pdf/1512.03385.pdf))
    pretrained on an ImageNet image classification data set as its backbone for extracting
    features of an image. It is one of the pioneering residual networks that made
    waves in the computer vision community a few years ago. DeepLab v3 introduces
    several architectural changes to the model to alleviate this issue. Furthermore,
    DeepLab v3 introduces a shiny new component called *atrous spatial pyramid pooling*
    (ASPP). We will discuss each of these in more detail in the coming sections.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.1 A quick overview of the ResNet-50 model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The ResNet-50 model consists of several convolution blocks, followed by a global
    average pooling layer and a fully connected final prediction layer with softmax
    activation. The convolution block is the innovative part of the model. The original
    model has 16 convolution blocks organized into five groups. A single block consists
    of three convolution layers (1 × 1 convolution layer with stride 2, 3 × 3 convolution
    layer, and 1 × 1 convolution layer), batch normalization, and residual connections.
    We discussed residual connections in depth in chapter 7\. Next, we will discuss
    a core computation used throughout the model known as atrous convolution.
  prefs: []
  type: TYPE_NORMAL
- en: '8.3.2 Atrous convolution: Increasing the receptive field of convolution layers
    with holes'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Compared to the standard ResNet-50, a major change that DeepLab v3 boasts is
    the use of atrous convolutions. Atrous (meaning “holes” in French) convolution,
    also known as dilated convolution, is a variant of the standard convolution. Atrous
    convolution works by inserting “holes” in between the convolution parameters.
    The increase in the receptive field is controlled by a parameter called *dilation
    rate*. A higher dilation rate means more holes between actual parameters in the
    convolution. A major benefit of atrous convolution is the ability to increase
    the size of the receptive field without compromising the parameter efficiency
    of a convolution layer.
  prefs: []
  type: TYPE_NORMAL
- en: '![08-09](../../OEBPS/Images/08-09.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.9 Atrous convolution compared to standard convolution. Standard convolution
    is a special case of atrous convolution, where the rate is 1\. As you increase
    the dilation rate, the receptive field of the layer increases.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.9 shows how a large dilation rate leads to a larger receptive field.
    The number of shaded gray boxes represents the number of parameters, whereas the
    dashed, lightly shaded box represents the size of the receptive field. As you
    can see, the number of parameters stays constant, while the receptive field increases.
    Computationally, it is quite straightforward to extend standard convolution to
    atrous convolution. All you need to do is insert zeros for the holes in the atrous
    convolution operation.
  prefs: []
  type: TYPE_NORMAL
- en: Wait! How does atrous convolution help segmentation models?
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed, the main issue presented by the pyramidal structure of CNNs
    is that the output gets gradually smaller. The easiest solution, leaving the learned
    parameters untouched, is to reduce the stride of the layers. Though technically
    that will increase output size, conceptually there is a problem.
  prefs: []
  type: TYPE_NORMAL
- en: To understand it, assume the i^(th) layer of a CNN has a stride of 2 and gets
    a h × w-sized input. Then the i+1^(th) layer gets a h/2 × w/2-sized input. By
    removing the stride of the i^(th) layer, it gets a h × w-sized output. However,
    the kernel of the i+1^(th) layer has been trained to see a smaller output, so
    by increasing the size of the input, we are disrupting (or reducing) the receptive
    field of the layer. By introducing atrous convolution, we compensate for that
    reduction of the receptive field.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now see how the ResNet-50 is repurposed for image segmentation. First,
    we download it from the tf.keras.applications module. The architecture of the
    ResNet-50 model has the following format. To start, it has a stride 2 convolution
    layer and a stride 2 pooling layer. After that, it has sequence of convolution
    blocks and finally an average pooling layer and fully connected output layer.
    These convolution blocks have a hierarchical organization of convolution layers.
    Each convolution block consists of several subblocks, which consist of three convolution
    layers (i.e., a 1 × 1 convolution, a 3 × 3 convolution, and a 1 × 1 convolution)
    along with batch normalization.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.3 Implementing DeepLab v3 using the Keras functional API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The network starting from the input up to the conv4 block remains unchanged.
    Following the notation from the original ResNet paper, these blocks are identified
    as conv2, conv3, and conv4 block groups. Our first task is to create a model containing
    the input layer up to the conv4 block of the original ResNet-50 model. After that,
    we will focus on recreating the final convolution block (i.e., conv5) as per the
    DeepLab v3 paper:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: As shown here, we find the last layer in the ResNet-50 model just before the
    "conv5_ block1_1_conv", which would be the last layer of the conv4 block group.
    With that, we can define a makeshift model that contains layers from the input
    to the final output of the conv4 block group. Later, we will focus on augmenting
    this model by introducing modifications and novel components from the paper. We
    will redefine the conv5 block with dilated convolutions. To do this, we need to
    understand the composition of a ResNet block (figure 8.10). We can assume it has
    three different levels.
  prefs: []
  type: TYPE_NORMAL
- en: '![08-10](../../OEBPS/Images/08-10.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10 Anatomy of a convolution block in ResNet-50\. For this example,
    we show the very first convolution block of ResNet-50\. The organization of a
    convolution block group consists of three different levels.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now implement a function to represent each level while using dilated convolution.
    In order to convert a standard convolution layer to a dilated convolution, we
    just have to pass in the desired rate to the dilation_rate parameter in the tf.keras.layers.Conv2D
    layer. First, we will implement a function that represents a level 3 block, as
    shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.8 A level 3 convolution block in ResNet-50
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Here, inp takes a 4D input having shape [batch size, height, width, channels].
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Perform 2D convolution on the input with a given number of filters, kernel_size,
    and dilation rate.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Perform batch normalization on the output of the convolution layer.
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Apply ReLU activation if activation is set to True.
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Return the output without an activation if activation is set to False.
  prefs: []
  type: TYPE_NORMAL
- en: A level 3 block has a single convolution layer with a desired dilation rate
    and a batch normalization layer followed by a nonlinear ReLU activation layer.
    Next, we will write a function for the level 2 block (see the next listing).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.9 A level 2 convolution block in ResNet-50
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'A level 2 block consists of three level 3 blocks with a given dilation rate
    that have convolution layers with the following specifications:'
  prefs: []
  type: TYPE_NORMAL
- en: 1 × 1 convolution layer having 512 filters and a desired dilation rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3 × 3 convolution layer having 512 filters and a desired dilation rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 × 1 convolution layer having 2048 filters and a desired dilation rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apart from using atrous convolution, this is identical to a level 2 block of
    the original conv5 block in the ResNet-50 model. With all the building blocks
    ready, we can implement the fully fledged conv5 block with atrous convolution
    (see the next listing).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.10 Implementing the final ResNet-50 convolution block group (level
    1)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Create a level 3 block (block0) to create residual connections for the first
    block.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Define the first level 2 block, which has a dilation rate of 2 (block1).
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Create a residual connection from block0 to block1.
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Apply ReLU activation to the result.
  prefs: []
  type: TYPE_NORMAL
- en: ❺ The second level 2 block with a dilation rate of 2 (block2)
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Create a residual connection from block1 to block2.
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Apply ReLU activation.
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Apply a similar procedure to block1 and block2 to create block3.
  prefs: []
  type: TYPE_NORMAL
- en: 'There’s no black magic here. The function resnet_block lays the outputs of
    the functions we already discussed to assemble the final convolution block. Particularly,
    it has three level 2 blocks with residual connections going from the previous
    block to the next. Finally, we can get the final output of the conv5 block with
    a dilation rate of 2 by calling the resnet_block function with the output of the
    interim model (resnet50_ upto_conv4) we defined as the input and a dilation rate
    of 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 8.3.4 Implementing the atrous spatial pyramid pooling module
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here, we will discuss the most exciting innovation of the DeepLab v3 model.
    The atrous spatial pyramid pooling (ASPP) module serves two purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: Aggregates multiscale information about an image, obtained through outputs produced
    using different dilation rates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combines highly summarized information obtained through global average pooling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ASPP module gathers multiscale information by performing different convolutions
    on the last ResNet-50 output. Specifically, the ASPP module performs 1 × 1 convolution,
    3 × 3 convolution (r = 6), 3 × 3 convolution (r = 12), and 3 × 3 convolution (r
    = 18), where r is the dilation rate. All of these convolutions have 256 output
    channels and are implemented as level 3 blocks (provided by the function block_level3()).
  prefs: []
  type: TYPE_NORMAL
- en: ASRP captures high-level information by performing global average pooling, followed
    by a 1 × 1 convolution with 256 output channels to match the output size of multiscale
    outputs, and finally a bilinear up-sampling layer to up-sample the height and
    width dimensions shrunk by the global average pooling. Remember that bilinear
    interpolation up-samples the images by computing the resulting pixels as an average
    of neighboring pixels. Figure 8.11 illustrates the ASPP module.
  prefs: []
  type: TYPE_NORMAL
- en: '![08-11](../../OEBPS/Images/08-11.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.11 The ASPP module used in the DeepLab v3 model
  prefs: []
  type: TYPE_NORMAL
- en: The job of the ASPP module can be summarized as a concise function. We have
    all the tools we need to implement this function from the previous work we have
    done (see the following listing).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.11 Implementing ASPP
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Define a 1 × 1 convolution.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Define a 3 x 3 convolution with 256 filters and a dilation rate of 6.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Define a 3 x 3 convolution with 256 filters and a dilation rate of 12.
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Define a 3 x 3 convolution with 256 filters and a dilation rate of 18.
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Define a global average pooling layer.
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Define a 1 × 1 convolution with 256 filters.
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Up-sample the output using bilinear interpolation.
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Concatenate all the outputs.
  prefs: []
  type: TYPE_NORMAL
- en: ❾ Create an instance of ASPP.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ASPP module consists of four level 3 blocks, as outlined in the code. The
    first block comprises a 1 × 1 convolution with 256 filters without dilation (this
    produces outa_1_conv). The latter three blocks consist of 3 × 3 convolutions with
    256 filters but with varying dilation rates (i.e., 6, 12, 18; they produce outa_2_conv,
    outa_3_conv, and outa_4_conv, respectively). This covers aggregating features
    from the image at multiple scales. However, we also need to preserve the global
    information about the image, similar to a global average pooling layer (outb_1_avg).
    This is achieved through a lambda layer that averages the input over the height
    and width dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the averaging is then followed by a 1 × 1 convolution filter
    with 256 filters. Then, to bring the output to the same size as previous outputs,
    an up-sampling layer that uses bilinear interpolation is used (this produces outb_1_up):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Finally, all these outputs are concatenated to a single output using a Concatenate
    layer to produce the final output out_aspp.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.5 Putting everything together
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now it’s time to collate all the different components to create one majestic
    segmentation model. The next listing outlines the steps required to build the
    final model.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.12 The final DeepLab v3 model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Define the RGB input layer.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Download and define the resnet50.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Get the output of the last layer we’re interested in.
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Define an interim model from the input up to the last layer of the conv4 block.
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Define the removed conv5 resnet block.
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Define the ASPP module.
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Define the final output.
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Define the final model.
  prefs: []
  type: TYPE_NORMAL
- en: Note how the model has a linear layer that does not have any activation present
    (e.g., sigmoid or softmax). This is because we are planning to use a special loss
    function that uses logits (unnormalized scores obtained from the last layer before
    applying softmax) instead of normalized probability scores. Due to that, we will
    keep the last layer a linear output with no activation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have one final housekeeping step to perform: copying the weights from the
    original conv5 block to the newly created conv5 block in our model. To do that,
    first we need to store the weights from the original model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: We cannot copy the weights to the new model until we compile the model, as weights
    are not initialized until the model is compiled. Before we do that, we need to
    learn loss functions and evaluation metrics that are used in segmentation tasks.
    To do that, we will need to implement custom loss functions and metrics and use
    them to compile the model. This will be discussed in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 3
  prefs: []
  type: TYPE_NORMAL
- en: 'You want to create a new pyramidal aggregation module called aug-ASPP. The
    idea is similar to the ASPP module we implemented earlier, but with a few differences.
    Let’s say you have been given two interim outputs from the model: out_1 and out_2
    (same size). You have to write a function, aug_aspp, that will take these two
    outputs and do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Perform atrous convolution with r = 16, 128 filters, 3 × 3 convolution, stride
    1, and ReLU activation on out_1 (output will be called atrous_out_1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform atrous convolution with r = 8, 128 filters, 3 × 3 convolution, stride
    1, and ReLU activation on both out_1 and out_2 (output will be called atrous_
    out_2_1 and atrous_out_2_2)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concatenate atrous_out_2_1 and atrous_out_2_2 (output will be called atrous_out_2)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply 1 × 1 convolution with 64 filters to both atrous_out_1 and atrous_out_2
    and concatenate (output will be called conv_out)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use bilinear up-sampling to double the size of conv_out (on height and width
    dimensions) and apply sigmoid activation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '8.4 Compiling the model: Loss functions and evaluation metrics in image segmentation'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to finalize the DeepLab v3 model (built using mostly the ResNet-50
    structure and the ASPP module), we have to define a suitable loss function and
    metrics to measure the performance of the model. Image segmentation is quite different
    from image classification tasks, so the loss function and metrics don’t necessarily
    translate to the segmentation problem. One key difference is that there is typically
    a large class imbalance in segmentation data, as a “background” class typically
    dominates an image compared to other object-related pixels. To get started, you
    read a few blog posts and research papers and identify weighted categorical cross-entropy
    loss and dice loss as good candidates. You focus on three different metrics: pixel
    accuracy, mean (class-weighted) accuracy, and mean IoU.'
  prefs: []
  type: TYPE_NORMAL
- en: Loss functions and evaluation metrics used in image segmentation models are
    different from what is used in image classifiers. To start, image classifiers
    take in a single class label for a single image, whereas a segmentation model
    predicts a class for every single pixel in the image. This highlights the necessity
    of not only reimagining existing loss functions and metrics, but also inventing
    new losses and evaluation metrics that are more appropriate for the output produced
    by segmentation models. We will first discuss loss functions and then metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.1 Loss functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A *loss function* is what is used to optimize the model whose purpose is to
    find the parameters that minimize a defined loss. A loss function used in a deep
    network must be *differentiable*, as the minimization of the loss happens with
    the help of gradients. The loss functions we’ll use comprise two loss functions:'
  prefs: []
  type: TYPE_NORMAL
- en: Cross-entropy loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dice loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross-entropy loss
  prefs: []
  type: TYPE_NORMAL
- en: Cross-entropy loss is one of the most common losses used in segmentation tasks
    and can be implemented with just one line in Keras. We already used cross-entropy
    loss quite a few times but didn’t analyze it in detail. However, it is worthwhile
    to review the underpinning mechanics that govern cross-entropy loss.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-entropy loss takes in a predicted target and a true target. Both these
    tensors are of shape [batch size, height, width, object classes]. The object class
    dimension is a one-hot encoded representation of which object class a given pixel
    belongs to. The cross-entropy loss is then computed for every pixel independently
    using
  prefs: []
  type: TYPE_NORMAL
- en: '![08_11a](../../OEBPS/Images/08_11a.png)'
  prefs: []
  type: TYPE_IMG
- en: where *CE*(*i, j*) represents the cross-entropy loss for pixel at position (*i,
    j*) on the image, *c* is the number of classes, and *y*[k] and *ŷ*[k] represent
    the elements in the one-hot encoded vector and the predicted probability distribution
    over classes of that pixel. This is then summed across all the pixels to get the
    final loss.
  prefs: []
  type: TYPE_NORMAL
- en: Beneath the simplicity of the method, a critical issue lurks. Class imbalance
    is almost certain to rear its ugly head in image segmentation problems. You will
    find hardly any real-world images where each object occupies an equal area in
    the image. The good news is it is not very difficult to deal with this issue.
    This can be mitigated by assigning a weight for each pixel in the image, depending
    on the dominance of the class it represents. Pixels belonging to large objects
    will have smaller weights, whereas pixels belonging to smaller objects will have
    larger weights, providing an equal say despite the size in the final loss. The
    next listing shows how to do this in TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.13 Computing the label weights for a given batch of data
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Get the total pixels per-class in y_true.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Get the total pixels in y_true.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Compute the weights per-class. Rarer classes get more weight.
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Reshape y_true to a [batch size, height*width]-sized tensor.
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Create a weight vector by gathering the weights corresponding to indices in
    y_true.
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Make y_weights a vector.
  prefs: []
  type: TYPE_NORMAL
- en: Here, for a given batch, we compute the weights as a sequence/vector that has
    a number of elements equal to y_true. First, we get the total number of pixels
    for each class by computing the sum over the width and height of the one-hot encoded
    y_true (i.e., has dimensions batch, height, width, and class). Here, a class that
    has a value larger than num_classes will be ignored. Next, we compute the total
    number of pixels per sample by taking the sum over the class dimension resulting
    in *tot* (a [batch size, 1]-sized tensor). Now the weights can be computed per
    sample and per class using
  prefs: []
  type: TYPE_NORMAL
- en: '![08_11b](../../OEBPS/Images/08_11b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where *n* is the total number of pixels and *n*^i is the total number of pixels
    belonging to the *i*^(th) class. After that, we reshape y_true to shape [batch
    size, -1] as preparation for an important step in weight computation. As the final
    output, we want to create a tensor out of weights, where we gather elements from
    the y_weights that correspond to elements in y_true. In other words, we fetch
    the value from y_weights, where the index to fetch is given by the values in y_true.
    At the end, the result will be of the same shape and size as y_true. This is all
    we need to weigh the samples: multiply weights element-wise with the loss value
    for each pixel. To achieve this, we will use the function tf.gather(), which gathers
    the elements from a given tensor (params) while taking a tensor that represents
    indices (indices) and returns a tensor that is of the same shape as the indices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Here, to ignore the batch dimension during performing the gather, we pass the
    argument batch_dims indicating how many batch dimensions we have. With that, we
    will define a function that outputs the weighted cross-entropy loss given a batch
    of predicted and true targets.
  prefs: []
  type: TYPE_NORMAL
- en: With the weights ready, we can now implement our first segmentation loss function.
    We will implement weighted cross-entropy loss. At a glance, the function masks
    irrelevant pixels (e.g., pixels belonging to unknown objects) and unwraps the
    predicted and true labels to get rid of the height and width dimensions. Finally,
    we can compute the cross-entropy loss using the built-in function in TensorFlow
    (see the next listing).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.14 Implementing the weighted cross-entropy loss
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Define the valid mask, masking unnecessary pixels.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Some initial setup that casts y_true to int and sets the shape
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Get the label weights.
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Unwrap y_pred and y_true so that batch, height, and width dimensions are squashed.
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Compute the cross-entropy loss with y_true, y_pred, and the mask.
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Return the function that computes the loss.
  prefs: []
  type: TYPE_NORMAL
- en: You might be thinking, “Why is the loss defined as a nested function?” This
    is a standard pattern we have to follow if we need to include extra arguments
    to our loss function (i.e., num_classes). All we are doing is capturing the computations
    of the loss function in the loss_fn function and then creating an outer function
    ce_weighted_from_logits() that will return the function that encapsulates the
    loss computations (i.e., loss_fn).
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, a valid mask is created to indicate whether the labels in y_true
    are less than the number of classes. Any label that has a value larger than the
    number of classes is ignored (e.g., unknown objects). Next, we get the weight
    vector and indicate a weight for each pixel using the get_label_weights() function.
    We will unwrap y_pred to a [-1, num_classes]-sized tensor, as y_pred contains
    *logits* (i.e., unnormalized probability scores output by the model) across all
    classes in the data set. y_true will be unwrapped to a vector (i.e., a single
    dimension), as y_true only contains the class label. Finally, we use tf.nn.sparse_softmax_cross_entropy_with_logits()
    to compute the loss over masked predicted and true targets. The function takes
    two arguments, labels and logits, which are self-explanatory. We can make two
    salient observations:'
  prefs: []
  type: TYPE_NORMAL
- en: We are computing sparse cross-entropy loss (i.e., not standard cross-entropy
    loss).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are computing cross-entropy loss from logits.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When using sparse cross entropy, we don’t have to one-hot encode the labels,
    so we can skip this, which leads to a more memory-efficient data pipeline. This
    is because one-hot encoding is handled internally by the model. By using a sparse
    loss, we have less to worry about.
  prefs: []
  type: TYPE_NORMAL
- en: Computing the loss from logits (i.e., unnormalized scores) instead of from normalized
    probabilities leads to better and more stable gradients. Therefore, whenever possible,
    make sure to use logits instead of normalized probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Dice loss
  prefs: []
  type: TYPE_NORMAL
- en: The second loss we will discuss is called the *dice loss*, which is computed
    as
  prefs: []
  type: TYPE_NORMAL
- en: '![08_11c](../../OEBPS/Images/08_11c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the intersection between the prediction and target tensors can be computed
    with element-wise multiplication, whereas the union can be computed using element-wise
    addition between the prediction and the target tensors. You might be thinking
    that using element-wise operations is a strange way to compute intersection and
    union. To understand the reason behind this, I want to refer to a statement made
    earlier: a loss function used in a deep network must be *differentiable*.'
  prefs: []
  type: TYPE_NORMAL
- en: This means that we cannot use the standard conventions we use to compute intersection
    and union from a given set of values. Rather, we need to resort to a differentiable
    computation, leading to intersection and union between two tensors. Intersection
    can be computed by taking element-wise multiplication between the predicted and
    true targets. Union can be computed by taking the element-wise addition between
    the predicted and true targets. Figure 8.12 clarifies how these operations lead
    to intersection and union between two tensors.
  prefs: []
  type: TYPE_NORMAL
- en: '![08-12](../../OEBPS/Images/08-12.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.12 Computations involved in dice loss. The intersection can be computed
    as a differentiable function by taking element-wise multiplication, whereas union
    can be computed as the element-wise sum.
  prefs: []
  type: TYPE_NORMAL
- en: This loss is predominantly focused on maximizing the intersection between the
    predicted and true targets. The multiplier of 2 is used to balance out the duplication
    of values that comes from the overlap between the intersection and the union,
    found in the denominator (see the following listing).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.15 Implementing the dice loss
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Initial setup for y_true
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Get the label weights and reshape it to a [-1, 1] shape.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Apply softmax on y_pred to get normalized probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Unwrap y_pred and one-hot-encoded y_true to the [-1, num_classes] shape.
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Compute intersection using element-wise multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Compute union using element-wise addition.
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Compute the dice coefficient.
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Compute the dice loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, smooth is a smoothing parameter that we’ll use to avoid potential NaN
    values resulting in division by zero. After that we do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Obtain weights for each y_true label
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply a softmax activation to y_pred
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unwrap y_pred to the [-1, num_classes] tensor and y_true to a [-1]-sized vector
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then intersection and union are computed for y_pred and y_true. Specifically,
    intersection is computed as the result of element-wise multiplication of y_pred
    and y_true and the union as the result of the element-wise addition of y_pred
    and y_true.
  prefs: []
  type: TYPE_NORMAL
- en: Focal loss
  prefs: []
  type: TYPE_NORMAL
- en: '*Focal loss* is a relatively novel loss introduced in the paper “Focal Loss
    for Dense Object Prediction” ([https://arxiv.org/pdf/1708.02002.pdf](https://arxiv.org/pdf/1708.02002.pdf)).
    Focal loss was introduced to combat the severe class imbalance found in segmentation
    tasks. Specifically, it solves a problem in many easy examples (e.g., samples
    from common classes with smaller loss), over-powering small numbers of hard examples
    (e.g., samples from rare classes with larger loss). Focal loss solves this problem
    by introducing a modulating factor that will down-weight easy examples, so, naturally,
    the loss function focuses more on learning hard examples.'
  prefs: []
  type: TYPE_NORMAL
- en: The loss function we will use to optimize the segmentation model will be the
    loss resulting from addition of sparse cross-entropy loss and dice loss (see the
    next listing).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.16 Final combined loss function
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will discuss evaluation metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.2 Evaluation metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Evaluation metrics play a vital role in model training as a health check for
    the model. This means low performance/issues can be quickly identified by making
    sure evaluation metrics behave in a reasonable way. Here we will discuss three
    different metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: Pixel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean IoU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will implement these as custom metrics by leveraging some of the existing
    metrics in TensorFlow, where you have to subclass from the tf.keras.metrics.Metric
    class or one of the existing metrics. This means that you create a new Python
    class, which inherits from the base tf.keras.metrics.Metric base class of one
    of the existing concrete metrics classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The first thing you need to understand about a metric is that it is a stateful
    object, meaning it maintains a state. For example, a single epoch has multiple
    iterations and assumes you’re interested in computing the accuracy. The metric
    needs to accumulate the values required to compute the accuracy over all the iterations
    so that at the end, it can compute the average accuracy for that epoch. When defining
    a metric, there are three functions you need to be mindful of: __init__, update_state,
    result, and reset_states.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s get concrete by assuming that we are implementing an accuracy metric
    (i.e., the number of elements in y_pred that matched y_true as a percentage).
    It needs to maintain a total: the sum of all the accuracy values we passed and
    the count (number of accuracy values we passed). With these two state elements,
    we can compute the mean accuracy at any time. When implementing the accuracy metric,
    you implement these functions:'
  prefs: []
  type: TYPE_NORMAL
- en: __init__—Defines two states; total and count
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: update_state—Updates total and count based on y_true and y_pred
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: result—Computes the mean accuracy as total/count
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: reset_states—Resets both the total and count (this needs to happen at the beginning
    of an epoch)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s see how this knowledge translates to the evaluation metrics we’re interested
    in solving.
  prefs: []
  type: TYPE_NORMAL
- en: Pixel and mean accuracies
  prefs: []
  type: TYPE_NORMAL
- en: Pixel accuracy is the simplest metric you can think of. It measures the pixel-wise
    accuracy between the prediction and the true target (see the next listing).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.17 Implementing the pixel accuracy metric
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Set the shape of y_true (in case it is undefined).
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Reshape y_true to a vector.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Reshape y_pred after taking argmax to a vector.
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Define a valid mask (mask out unnecessary pixels).
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Gather pixels/labels that satisfy the valid_mask condition.
  prefs: []
  type: TYPE_NORMAL
- en: ❻ With the processed y_true and y_pred, compute the accuracy using the update_state()
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pixel accuracy computes the one-to-one match between predicted pixels and true
    pixels. To compute this, we subclass from tf.keras.metrics.Accuracy as it has
    all the computations we need. To do this, we override the update_state function
    as shown. There are a few things we need to take care of:'
  prefs: []
  type: TYPE_NORMAL
- en: We need to set the shape of y_true as a precaution. This is because when working
    with tf.data.Dataset, sometimes the shape is lost.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reshape y_true to a vector.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get the class labels of y_pred by performing tf.argmax() and reshape it to a
    vector.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define a valid mask that ignores unwanted classes (e.g., unknown objects).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get the pixels that satisfy only the valid_mask filter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once we complete these tasks, we simply call the parent object’s (i.e., tf.keras
    .metrics.Accuracy) update_state method with the corresponding arguments. We don’t
    have to override result() and reset_states() functions, as they already contain
    the correct computations.
  prefs: []
  type: TYPE_NORMAL
- en: We said that class imbalance is prevalent in image segmentation problems. Typically,
    background pixels will spread in a large region of the image, potentially leading
    to misguided conclusions. Therefore, a slightly better approach might be to compute
    the accuracy individually per class and then average it. Enter mean accuracy,
    which prevents the undesired characteristics of pixel accuracy (see the next listing).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.18 Implementing the mean accuracy metric
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Initial setup
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Compute the confusion matrix using y_true and y_pred.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Get the true positives (elements on the diagonal).
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Compute the mean accuracy using true positives and true class counts for each
    class.
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Compute the average of mean_accuracy using the update_state() function.
  prefs: []
  type: TYPE_NORMAL
- en: The MeanAccuracyMetric will branch out from tf.keras.metrics.Mean, which computes
    the average over a given sequence of values. The plan is to compute the mean_accuracy
    within the update_state() function and then pass the value to the parent’s update_state()
    function so that we get the average value of mean accuracy. First, we perform
    the initial setup and clean-up of y_true and y_pred we discussed earlier.
  prefs: []
  type: TYPE_NORMAL
- en: '![08-13](../../OEBPS/Images/08-13.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.13 Illustration of a confusion matrix for a five-class classification
    problem. The shaded boxes represent true positives.
  prefs: []
  type: TYPE_NORMAL
- en: 'Afterward, we compute the confusion matrix (figure 8.13) from predicted and
    true targets. A confusion matrix for an *n*-way classification problem (i.e.,
    a classification problem with *n* possible classes) is defined as a *n* × *n*
    matrix. Here, the element at the (*i*, *j*) position indicates how many instances
    were predicted as belonging to the *i* ^(th) class but actually belong to the
    *j* ^(th) class. Figure 8.13 portrays this type of confusion matrix. We can get
    the true positives by extracting the diagonal (i.e., (*i*, *i*) elements in the
    matrix for all 1 < = *i* < = *n*). We can now compute the mean accuracy in two
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Perform element-wise division on the true positive count by actual counts for
    all the classes. This produces a vector whose elements represents per-class accuracy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the vector mean that resulted from step 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we pass the mean accuracy to its parent’s update_state() function.
  prefs: []
  type: TYPE_NORMAL
- en: Mean IoU
  prefs: []
  type: TYPE_NORMAL
- en: '*Mean IoU* (mean intersection over union) is a popular evaluation metric pick
    for segmentation tasks and has close ties to the dice loss we discussed earlier,
    as they both use the concept of intersection and union to compute the final result
    (see the next listing).'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.19 Implementing the mean IoU metric
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: ❶ After the initial setup of y_true and y_pred, all we need to do is call the
    parent’s update_state() function.
  prefs: []
  type: TYPE_NORMAL
- en: The mean IoU computations are already found in tf.keras.metrics.MeanIoU. Therefore,
    we will use that as our parent class. All we need to do is perform the aforementioned
    setup for y_true and y_pred and then call the parent’s update_state() function.
    Mean IoU is computed as
  prefs: []
  type: TYPE_NORMAL
- en: '![08_13a](../../OEBPS/Images/08_13a.png)'
  prefs: []
  type: TYPE_IMG
- en: Various elements used in this computation are depicted in figure 8.14.
  prefs: []
  type: TYPE_NORMAL
- en: '![08-14](../../OEBPS/Images/08-14.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.14 Confusion matrix and how it can be used to compute false positives,
    false negatives, and true positives
  prefs: []
  type: TYPE_NORMAL
- en: 'We now understand the loss functions and evaluation metrics that are available
    to us and have already implemented them. We can incorporate these losses to compile
    the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Remember that we stored the weights from a convolution block we removed earlier.
    Now that we have compiled the model, we can copy the weights to the new model
    using the following syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: We now move on to training the model with the data pipeline and the model we
    defined.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 4
  prefs: []
  type: TYPE_NORMAL
- en: You are coming up with a new loss function that computes the disjunctive union
    between y_true and y_pred. The disjunctive union between two sets A and B is the
    set of elements that are in either A or B but not in the intersection. You know
    you can compute the intersection with element-wise multiplication and union with
    element-wise addition of y_true and y_pred. Write the equation to compute the
    disjunctive union as a function of y_true and y_pred.
  prefs: []
  type: TYPE_NORMAL
- en: 8.5 Training the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You’re coming to the final stages of the first iteration of your product. Now
    it’s time to put the data and knowledge you garnered to good use (i.e., train
    the model). We will train the model for 25 epochs and monitor the pixel accuracy,
    mean accuracy, and mean IoU metrics. During the training, we will measure the
    performance on validation data set.
  prefs: []
  type: TYPE_NORMAL
- en: Training the model is the easiest part, as we have done the hard work that leads
    up to training. It is now just a matter of calling fit() with the correct parameters
    on the DeepLab v3 we just defined, as the following listing shows.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.20 Training the model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Train logger
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Set the mode for the following callbacks automatically by looking at the metric
    name.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Learning rate scheduler
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Early stopping callback
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Train the model while using the validation set for learning rate adaptation
    and early stopping.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will define a directory called eval if it does not exist. The training
    logs will be saved in this directory. Next, we define three different callbacks
    to be used during the training:'
  prefs: []
  type: TYPE_NORMAL
- en: csv_logger—Logs the training loss/metrics and validation loss/metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: lr_callback—Reduces the learning rate by a factor of 10, if the validation loss
    does not decrease within three epochs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: es_callback—Performs early stopping if the validation loss does not decrease
    within six epochs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NOTE On an Intel Core i5 machine with an NVIDIA GeForce RTX 2070 8GB, the training
    took approximately 45 minutes to run 25 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: 'With that, we call deeplabv3.fit() with the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: x—The tf.data pipeline producing training instances (set to tr_image_ds).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: steps_per_epoch—Number of steps per epoch. This is obtained by computing the
    number of training instances and dividing it by the batch size (set to n_train).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: validation_data—The tf.data pipeline producing validation instances. This is
    obtained by computing the number of validation instances and dividing it by the
    batch size (set to val_image_ds).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: epochs—Number of epochs (set to epochs).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: callbacks—The callbacks we set up earlier (set to [lr_callback, csv_logger,
    es_callback]).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After the model is trained, we will evaluate it on the test set. We will also
    visualize segmentations generated by the model.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 5
  prefs: []
  type: TYPE_NORMAL
- en: You have a data set of 10,000 samples and have split it into 90% training data
    and 10% validation data. You use a batch size of 10 for training and a batch size
    of 20 for validation. How many training and validation steps will be there in
    a single epoch?
  prefs: []
  type: TYPE_NORMAL
- en: 8.6 Evaluating the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s take a moment to reflect on what we have done so far. We defined a data
    pipeline to read images and prepare them as inputs and targets for the model.
    Then we defined a model known as DeepLab v3 that uses a pretrained ResNet-50 as
    its backbone and a special module called atrous spatial pyramid pooling to predict
    the final segmentation mask. Then we defined task-specific losses and metrics
    to make sure we could evaluate the model with a variety of metrics. Afterward,
    we trained the model. Now it’s time for the ultimate reveal. We will measure the
    performance on an unseen test data set to see how well the model does. We will
    also visualize the model outputs and compare them against the real targets by
    plotting them side by side.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can run the model over the unseen test images and gauge how well it is performing.
    To do that, we execute the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: The size of the test set is the same as the validation set, as we split the
    images listed in val.txt into two equal validation and test sets. This will return
    around
  prefs: []
  type: TYPE_NORMAL
- en: 62% mean IoU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 87% mean accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 91% pixel accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are very respectable scores given our circumstances. Our training data
    set consists of less than 1,500 segmented images. Using this data, we were able
    to train a model that achieves around 62% mean IoU on a test data set of approximately
    size 725.
  prefs: []
  type: TYPE_NORMAL
- en: What does state of the art look like?
  prefs: []
  type: TYPE_NORMAL
- en: The state-of-the-art performance on Pascal VOC 2012 reports around 90% mean
    IoU ([http://mng.bz/o2m2](http://mng.bz/o2m2)). However, these are models that
    are much larger and complex than what we used here. Furthermore, they are typically
    trained with significantly more data by using an auxiliary data set known as the
    semantic boundary data set (SBD) (introduced in the paper [http://mng.bz/nNve](http://mng.bz/nNve)).
    This will push the training datapoint count to over 10,000 (close to seven times
    the size of our current training set).
  prefs: []
  type: TYPE_NORMAL
- en: You can further investigate the model by visually inspecting some of the results
    our module produces. After all, it is a vision model that we are developing. Therefore,
    we should not rely solely on numbers to make decisions and conclusions. We should
    also visually analyze the results before settling on a conclusion.
  prefs: []
  type: TYPE_NORMAL
- en: What would the results from a U-Net based network look like?
  prefs: []
  type: TYPE_NORMAL
- en: Under similar conditions provided for the DeepLab v3 model, the U-Net model
    built with a pretrained ResNet-50 model as the encoder was only able to achieve
    approximately 32.5% mean IoU, 78.5% mean accuracy, and 81.5% pixel accuracy. The
    implementation is provided in the Jupyter notebook in the ch08 folder.
  prefs: []
  type: TYPE_NORMAL
- en: On an Intel Core i5 machine with an NVIDIA GeForce RTX 2070 8GB, the training
    took approximately 55 minutes to run 25 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: There is a detailed explanation of the U-Net model in appendix B.
  prefs: []
  type: TYPE_NORMAL
- en: To complete this investigation, we will get a random sample from the test set
    and ask the model to predict the segmentation map for each of those images. Then
    we will plot the results side by side to ensure that our model is doing a good
    job (figure 8.15).
  prefs: []
  type: TYPE_NORMAL
- en: '![08-15](../../OEBPS/Images/08-15.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.15 Comparing the true annotated targets to model predictions. You can
    see that the model is quite good at separating objects from different backgrounds.
  prefs: []
  type: TYPE_NORMAL
- en: We can see that unless it is an extremely difficult image (e.g., the top-left
    image, where there’s a car obscured by a gate), our model does a very good job.
    It can identify almost all the images found in the sample we analyzed with high
    accuracy. The code for visualizing the images is provided in the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our discussion of image segmentation. In the next few chapters,
    we will discuss several natural language processing problems.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 6
  prefs: []
  type: TYPE_NORMAL
- en: You are given
  prefs: []
  type: TYPE_NORMAL
- en: A model (called model)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A batch of images called batch_image (already preprocessed and ready to be fed
    to a model)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A corresponding batch of targets, batch_targets (the true segmentation mask
    in one-hot encoded format)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write a function called get_top_bad_examples(model, batch_images, batch_targets,
    n) that will return the top n indices of the hardest (highest loss) images in
    batch_ images. Given a predicted mask and a target mask, you can use the sum over
    element-wise multiplication as the loss of a given image.
  prefs: []
  type: TYPE_NORMAL
- en: You can use the model.predict() function to make a prediction on batch_ images,
    and it will return a predicted mask as the same size as batch_targets. Once you
    compute the losses for the batch (batch_loss), you can use the tf.math.top_k(batch_
    loss, n) function to get the indices of elements with the highest value. tf.math
    .top_k() returns a tuple containing the top values and indices of a given vector,
    in that order.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Segmentation models fall into two broad categories: semantic segmentation and
    instance segmentation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The tf.data API provides various functionality to implement complex data pipelines,
    such as using custom NumPy functions, performing quick transformations using tf.data.Dataset.map(),
    and I/O optimization techniques like prefetch and cache.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DeepLab v3 is a popular segmentation model that uses a pretrained ResNet-50
    model as its backbone and atrous convolutions to increase the receptive field
    by inserting holes (i.e., zeros) between the kernel weights.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The DeepLab v3 model uses a module known as atrous spatial pyramid pooling to
    aggregate information at multiple scales, which helps to create a fine-grained
    segmented output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In segmentation tasks, cross entropy and dice loss are two popular losses, whereas
    pixel accuracy, mean accuracy, and mean IoU are popular evaluation metrics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In TensorFlow, loss functions can be implemented as stateless functions. But
    metrics must be implemented as stateful objects by subclassing from the tf.keras.metrics.Metric
    base class or a suitable class.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The DeepLab v3 model achieved a very good accuracy of 62% mean IoU on the Pascal
    VOC 2010 data set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Answers to exercises
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Exercise 1**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '**Exercise 2**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '**Exercise 3**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '**Exercise 4**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '**Exercise 5**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 6**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
