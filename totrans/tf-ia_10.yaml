- en: '8 Telling things apart: Image segmentation'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8区分事物：图像分割
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容涵盖
- en: Understanding segmentation data and working with it in Python
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解分割数据并在Python中处理它
- en: Implementing a fully fledged segmentation data pipeline
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现一个完整的分割数据管道
- en: Implementing an advanced segmentation model (DeepLab v3)
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现高级分割模型（DeepLab v3）
- en: Compiling models with custom-built image segmentation loss functions/metrics
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自定义构建的图像分割损失函数/度量编译模型
- en: Training the image segmentation model on the clean and processed image data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对清洁和处理后的图像数据进行图像分割模型训练
- en: Evaluating the trained segmentation model
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估经过训练的分割模型
- en: 'In the last chapter, we learned about various advanced computer vision models
    and techniques to push the performance of an image classifier. We learned about
    the architecture of Inception net v1 as well as its successors (e.g., Inception
    net v2, v3, and v4). Our objective was to lift the performance of the model on
    an image classification data set with 64 × 64-sized RGB images of objects belonging
    to 200 different classes. While trying to train a model on this data set, we learned
    many important concepts:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了各种先进的计算机视觉模型和技术，以提高图像分类器的性能。我们了解了Inception net v1的架构以及它的后继者（例如Inception
    net v2、v3和v4）。我们的目标是提高模型在一个包含200个不同类别的对象的64×64大小的RGB图像的图像分类数据集上的性能。在尝试在此数据集上训练模型时，我们学到了许多重要的概念：
- en: '*Inception blocks*—A way to group convolutional layers having different-sized
    windows (or kernels) to encourage learning features at different scales while
    making the model parameter efficient due to the smaller-sized kernels.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Inception blocks*—一种将具有不同尺寸窗口（或核）的卷积层分组在一起的方法，以鼓励学习不同尺度的特征，同时由于更小尺寸的核而使模型参数高效。'
- en: '*Auxiliary outputs*—Inception net uses a classification layer (i.e., a fully
    connected layer with softmax activation) not only at the end of the network, but
    also in the middle of the network. This enables the gradients from the final layer
    to flow strongly all the way to the first layer.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*辅助输出*—Inception net不仅在网络末端使用分类层（即具有softmax激活的完全连接层），而且还在网络中间使用。这使得从最终层到第一层的梯度能够强劲地传播。'
- en: '*Augmenting data*—Using various image transformation techniques (adjusting
    brightness/contrast, rotating, translating, etc.) to increase the amount of labeled
    data using the tf.keras.preprocessing.image.ImageDataGenerator.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据增强*—使用各种图像转换技术（调整亮度/对比度、旋转、平移等）使用tf.keras.preprocessing.image.ImageDataGenerator增加标记数据的数量。'
- en: '*Dropout*—Switching on and off nodes in the layers randomly. This forces the
    neural networks to learn more robust features as the network does not always have
    all the nodes activated.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Dropout*—随机打开和关闭层中的节点。这迫使神经网络学习更健壮的特征，因为网络并不总是激活所有节点。'
- en: '*Early stopping*—Using the performance on the validation data set as a way
    to control when the training stops. If the validation performance has not increased
    in a certain number of epochs, training is halted.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*提前停止*—使用验证数据集上的性能作为控制训练何时停止的方法。如果在一定数量的epochs中验证性能没有提高，则停止训练。'
- en: '*Transfer learning*—Downloading and using a pretrained model (e.g., Inception-ResNet
    v2) trained on a larger, similar data set as the initialization and fine-tuning
    it to perform well on the task at hand.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*迁移学习*—下载并使用在更大、类似数据集上训练的预训练模型（例如Inception-ResNet v2）作为初始化，并对其进行微调以在手头的任务上表现良好。'
- en: 'In this chapter, we will learn about another important task in computer vision:
    image segmentation. In image classification, we only care if an object exists
    in a given image. Image segmentation, on the other hand, recognizes multiple objects
    in the same image as well as where they are in the image. It is a very important
    topic of computer vision, and applications like self-driving cars live and breathe
    image segmentation models. Self-driving cars need to precisely locate objects
    in their surroundings, which is where image segmentation comes into play. As you
    might have guessed already, they also have their roots in many other applications:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习计算机视觉中另一个重要任务：图像分割。在图像分类中，我们只关心给定图像中是否存在对象。另一方面，图像分割不仅识别同一图像中的多个对象，还识别它们在图像中的位置。这是计算机视觉的一个非常重要的主题，像自动驾驶汽车这样的应用程序依赖于图像分割模型。自动驾驶汽车需要精确定位其周围的物体，这就是图像分割发挥作用的地方。你可能已经猜到，它们在许多其他应用程序中也有它们的根基：
- en: Image retrieval
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像检索
- en: Identifying galaxies ([http://mng.bz/gwVx](http://mng.bz/gwVx))
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别星系 ([http://mng.bz/gwVx](http://mng.bz/gwVx))
- en: Medical image analysis
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 医学图像分析
- en: If you are a computer vision/deep learning engineer/researcher working on image-related
    problems, there is a high chance that your path will cross with image segmentation.
    Image segmentation models classify each pixel in the image to one of a predefined
    set of object categories. Image segmentation has ties to the image classification
    task we saw earlier. Both solve a classification task. Additionally, pretrained
    image classification models are used as the backbone of segmentation models, as
    they can provide crucial image features at different granularities to solve the
    segmentation task better and faster. A key difference is that image classifiers
    are solving a sparse prediction task, where each image has a single class label
    associated, as opposed to segmentation models that solve a dense prediction task
    that has a class label associated with every pixel in the image.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您是从事与图像相关问题的计算机视觉/深度学习工程师/研究人员，您的道路很可能会与图像分割相交。图像分割模型将图像中的每个像素分类为预定义的一组对象类别之一。图像分割与我们之前看到的图像分类任务有关。两者都解决了一个分类任务。此外，预训练的图像分类模型被用作分割模型的骨干，因为它们可以提供不同粒度的关键图像特征，以更好更快地解决分割任务。一个关键区别是图像分类器解决了一个稀疏预测任务，其中每个图像都有一个与之关联的单个类标签，而分割模型解决了一个密集预测任务，其中图像中的每个像素都有一个与之关联的类标签。
- en: 'Any image segmentation algorithm can be classified as one of the following:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 任何图像分割算法都可以分类为以下类型之一：
- en: '*Semantic segmentation*—The algorithm is only interested in identifying different
    categories of objects present in the image. For example, if there are multiple
    persons in the image, the pixels corresponding to all of them will be tagged with
    the same class.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*语义分割*—该算法仅对图像中存在的不同类别的对象感兴趣。例如，如果图像中有多个人，则与所有人对应的像素将被标记为相同的类。'
- en: '*Instance segmentation*—The algorithm is interested in identifying different
    objects separately. For example, if there are multiple persons in the image, pixels
    belonging to each person are represented by a unique class. Instance-based segmentation
    is considered more difficult than semantic segmentation.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*实例分割*—该算法对单独识别不同对象感兴趣。例如，如果图像中有多个人，属于每个人的像素将被表示为唯一的类。与语义分割相比，实例分割被认为更难。'
- en: Figure 8.1 depicts the difference between the data found in a semantic segmentation
    task and an instance-based segmentation task. In this chapter, we will focus on
    semantic segmentation ([http://mng.bz/5QAZ](http://mng.bz/5QAZ)).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 描述了语义分割任务中找到的数据与实例分割任务中找到的数据之间的区别。在本章中，我们将重点关注语义分割 ([http://mng.bz/5QAZ](http://mng.bz/5QAZ))。
- en: '![08-01](../../OEBPS/Images/08-01.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![08-01](../../OEBPS/Images/08-01.png)'
- en: Figure 8.1 Semantic segmentation versus instance segmentation
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 语义分割与实例分割的比较
- en: In the next section, we will look at the data we are dealing with more closely.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将更仔细地研究我们正在处理的数据。
- en: 8.1 Understanding the data
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 理解数据
- en: You are experimenting with a startup idea. The idea is to develop a navigation
    algorithm for small remote-control (RC) toys. Users can choose between how safe
    or adventurous the navigation needs to be. As the first step, you plan to develop
    an image segmentation model. The output of the image segmentation model will later
    feed to a different model that will predict the navigation path depending on what
    the user requests.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 您正在尝试一个创业想法。这个想法是为小型遥控（RC）玩具开发一种导航算法。用户可以选择导航需要多安全或者冒险。作为第一步，您计划开发一个图像分割模型。图像分割模型的输出将稍后馈送到另一个模型，该模型将根据用户的请求预测导航路径。
- en: 'For this task, you feel the Pascal VOC 2012 data set will be a good fit as
    it mostly comprises indoor and outdoor images that are found in urban/domestic
    environments. It contains pairs of images: an input image containing some objects
    and an annotated image. In the annotated image, each pixel has an assigned color,
    depending on which object that pixel belongs to. Here, you plan to download the
    data set and load the data successfully into Python.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个任务，您觉得 Pascal VOC 2012 数据集会是一个很好的选择，因为它主要包含了城市/家庭环境中的室内和室外图像。它包含图像对：一个包含一些对象的输入图像和一个带有注释的图像。在注释图像中，每个像素都有一个分配的颜色，取决于该像素属于哪个对象。在这里，您计划下载数据集并成功将数据加载到
    Python 中。
- en: After having a good understanding/framing of the problem you want to solve,
    your next focus point should be understanding and exploring the data. Segmentation
    data is different from the image classification data sets we’ve seen thus far.
    One major difference is that both the input and target are images. The input image
    is a standard image, similar to what you’d find in an image classification task.
    Unlike in image classification, the target is not a label, but an image, where
    each pixel has a color from a predefined palette of colors. In other words, each
    object we’re interested in segmenting is assigned a color. Then a pixel corresponding
    to that object in the input image is colored with that color. The number of available
    colors is the same as the number of different objects (plus background) that you’re
    interested in identifying (figure 8.2).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解/界定您想解决的问题之后，下一个重点应该是了解和探索数据。分割数据与我们迄今为止见过的图像分类数据集不同。一个主要的区别是输入和目标都是图像。输入图像是一个标准图像，类似于您在图像分类任务中找到的图像。与图像分类不同，目标不是标签，而是图像，其中每个像素都有来自预定义颜色调色板的颜色。换句话说，我们感兴趣的每个对象都被分配了一种颜色。然后，在输入图像中对应于该对象的像素以该颜色着色。可用颜色的数量与您想要识别的不同对象（加上背景）的数量相同（图8.2）。
- en: '![08-02](../../OEBPS/Images/08-02.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![08-02](../../OEBPS/Images/08-02.png)'
- en: Figure 8.2 Inputs and outputs of an image classifier versus an image segmentation
    model
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 图像分类器与图像分割模型的输入和输出
- en: For this task, we will be using the PASCAL VOC 2012 data set, which is popular
    and consists of real-world scenes. The data set has labels for 22 different classes,
    as outlined in table 8.1.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个任务，我们将使用流行的PASCAL VOC 2012数据集，该数据集由真实场景组成。数据集为22个不同类别提供了标签，如表8.1所述。
- en: Table 8.1 Different classes and their respective labels in the PASCAL VOC 2012
    data set
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 表8.1 PASCAL VOC 2012数据集中的不同类别及其相应的标签
- en: '| **Class** | **Assigned Label** | **Class** | **Assigned Label** |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| **类别** | **指定标签** | **类别** | **指定标签** |'
- en: '| Background | 0 | Dining table | 11 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 背景 | 0 | 餐桌 | 11 |'
- en: '| Aeroplane | 1 | Dog | 12 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 飞机 | 1 | 狗 | 12 |'
- en: '| Bicycle | 2 | Horse | 13 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 自行车 | 2 | 马 | 13 |'
- en: '| Bird | 3 | Motorbike | 14 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 鸟 | 3 | 摩托车 | 14 |'
- en: '| Boat | 4 | Person | 15 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 船 | 4 | 人 | 15 |'
- en: '| Bottle | 5 | Potted plant | 16 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 瓶子 | 5 | 盆栽植物 | 16 |'
- en: '| Bus | 6 | Sheep | 17 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 公共汽车 | 6 | 羊 | 17 |'
- en: '| Car | 7 | Sofa | 18 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 汽车 | 7 | 沙发 | 18 |'
- en: '| Cat | 8 | Train | 19 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 猫 | 8 | 火车 | 19 |'
- en: '| Chair | 9 | TV/monitor | 20 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 椅子 | 9 | 电视/显示器 | 20 |'
- en: '| Cow | 10 | Boundaries/unknown object | 255 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 牛 | 10 | 边界/未知对象 | 255 |'
- en: The white pixels represent object boundaries or unknown objects. Figure 8.3
    illustrates the data set by showing a sample for every single object class present.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 白色像素代表对象边界或未知对象。图8.3通过显示每个单独的对象类别的样本来说明数据集。
- en: '![08-03](../../OEBPS/Images/08-03.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![08-03](../../OEBPS/Images/08-03.png)'
- en: Figure 8.3 Samples from the PASCAL VOC 2012 data set. The data set shows a single
    example image, along with the annotated segmentation of it for the 20 different
    object classes.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 PASCAL VOC 2012数据集的样本。数据集显示了单个示例图像，以及用于20种不同对象类别的注释分割。
- en: 'In figure 8.4, diving a bit deeper, you can see a single sample datapoint (best
    viewed in color) up close. It has two objects: a chair and a dog. As it is shown,
    different colors are assigned to different object categories. While the figure
    is best viewed in color, you still can distinguish different objects by paying
    attention to the white border that outlines the objects in the figure.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在图8.4中，深入挖掘一下，你可以近距离看到单个样本数据点（最好是以彩色视图查看）。它有两个对象：一把椅子和一只狗。正如所示，不同的颜色分配给不同的对象类别。虽然最好以彩色查看图像，但您仍然可以通过注意在图像中勾勒对象的白色边框来区分不同的对象。
- en: '![08-04](../../OEBPS/Images/08-04.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![08-04](../../OEBPS/Images/08-04.png)'
- en: Figure 8.4 An original input image in image segmentation and the corresponding
    target annotated/segmented image
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 图像分割中的原始输入图像及其相应的目标标注/分割图像
- en: First, we’ll download the data set, if it does not exist, from [http://mng.bz/6XwZ](http://mng.bz/6XwZ)
    (see the next listing).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将从[http://mng.bz/6XwZ](http://mng.bz/6XwZ)下载数据集（请参阅下一个清单）。
- en: Listing 8.1 Downloading data
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 清单8.1 下载数据
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Check if the file is already downloaded. If so, don’t download again.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 检查文件是否已经下载。如果已下载，则不要重新下载。
- en: ❷ Get the content from the URL.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 从URL获取内容。
- en: ❸ Save the file to disk.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将文件保存到磁盘。
- en: ❹ If the file exists but is not extracted, extract the file.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 如果文件存在但尚未提取，则提取文件。
- en: 'The data set download is quite similar to our past experience. The data exists
    as a tarfile. We download the file if it doesn’t exist and extract it. Next, we
    will discuss how to use the image library Pillow and NumPy to load the images
    into memory. Here, the target images will need special treatment, as you will
    see that they are not stored using the conventional approach. There are no surprises
    involved with loading input images to memory. Using the PIL (i.e., Pillow) library,
    they can be loaded with a single line of code:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的下载与我们以前的经验非常相似。数据作为tar文件存在。如果文件不存在，我们会下载文件并解压缩。接下来，我们将讨论如何使用图像库Pillow和NumPy将图像加载到内存中。在这里，目标图像将需要特殊处理，因为您将看到它们不是使用常规方法存储的。加载输入图像到内存中没有任何意外情况。使用PIL（即Pillow）库，可以通过一行代码加载它们：
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, you can inspect the image’s attributes:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您可以检查图像的属性：
- en: '[PRE2]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: It’s time to load the corresponding annotated/segmented target images. As mentioned
    earlier, target images require special attention. The target images are not stored
    as standard images but as *palettized* images. Palettization is a technique to
    reduce memory footprint while storing images with a fixed number of colors in
    the image. The crux of the method is to maintain a palette of colors. The palette
    is stored as a sequence of integers, which has a length of the number of colors
    or the number of channels. (E.g., in the case of RGB, where a pixel is made of
    three values corresponding to red, green, and blue, the number of channels is
    three. A grayscale image has a single channel, where each pixel is made of a single
    value). The image itself then stores an array of indices (size = height × width),
    where each index maps to a color in the palette. Finally, by mapping the palette
    indices from the image to palette colors, you can compute the original image.
    Figure 8.5 provides a visual exposition of this discussion.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候加载相应的注释/分割的目标图像了。如前所述，目标图像需要特殊关注。目标图像不是作为标准图像存储的，而是作为*调色板化*图像存储的。调色板化是一种在图像中存储具有固定颜色数量的图像时减少内存占用的技术。该方法的关键在于维护一个颜色调色板。调色板被存储为整数序列，其长度为颜色数量或通道数量（例如，对于RGB的情况，一个像素由三个值对应于红、绿和蓝，通道数量为三。灰度图像具有单个通道，其中每个像素由单个值组成）。然后，图像本身存储了一个索引数组（大小为高度×宽度），其中每个索引映射到调色板中的一种颜色。最后，通过将图像中的调色板索引映射到调色板颜色，可以计算出原始图像。图8.5提供了这个讨论的视觉展示。
- en: '![08-05](../../OEBPS/Images/08-05.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![08-05](../../OEBPS/Images/08-05.png)'
- en: Figure 8.5 The numerical representation of input images and target images in
    the PASCAL VOC 2012 data set
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5 显示了PASCAL VOC 2012数据集中输入图像和目标图像的数值表示。
- en: The next listing shows the code for reconstructing the original image pixels
    from the palettized image.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个清单展示了从调色板图像中重新构造原始图像像素的代码。
- en: Listing 8.2 Reconstructing the original image from a palettized image
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 代码清单8.2 从调色板图像中重建原始图像
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Get the color palette from the image.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从图像中获取颜色调色板。
- en: ❷ The palette is stored as a vector. We reshape it to an array, where each row
    represents a single RGB color.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 调色板以向量形式存储。我们将其重新整形为一个数组，其中每一行表示一个单独的RGB颜色。
- en: ❸ Get the image’s height and width.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 获取图像的高度和宽度。
- en: ❹ Convert the palettized image stored as an array to a vector (helps with our
    next steps).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将以数组形式存储的调色板图像转换为向量（有助于接下来的步骤）。
- en: ❺ Get the image as a vector if the image is provided as an array instead of
    a Pillow image.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 如果图像是以数组而不是Pillow图像提供的，将图像作为向量获取。
- en: ❻ We first define a vector of zeros that has the same length as our image. Then,
    for all the indices found in the image, we gather corresponding colors from the
    palette and assign them to the same position in the rgb_image.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 首先，我们定义一个与图像长度相同的零向量。然后，对于图像中的所有索引，我们从调色板中获取相应的颜色，并将其分配到rgb_image的相同位置。
- en: ❼ Restore the original shape.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 恢复原始形状。
- en: Here, we first obtain the palette of the image using the get_palette() function.
    This will be present as a one-dimensional array (of length number of classes ×
    number of channels). Next, we need to reshape the array to a (number of classes,
    number of channels)-sized array. In our case, this will be converted to a (22,3)-sized
    array. As we define the first dimension of the reshape as -1, it will be automatically
    inferred from the original size of the data and the other dimensions of the reshape
    operation. Finally, we define an array of zeros, which will ultimately store the
    actual colors the indices found in the image. To do that, we index the rgb_image
    vector using the image (which contains indices) and assign matching colors from
    the palette to those indices.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们首先使用get_palette()函数获取图像的调色板。这将作为一个一维数组存在（长度为类别数×通道数）。接下来，我们需要将数组重塑为一个（类别数，通道数）大小的数组。在我们的情况下，这将转换为一个（22,3）大小的数组。由于我们将重塑的第一维定义为-1，它将从原始数据的大小和重塑操作的其他维度中自动推断出来。最后，我们定义一个全零数组，它最终将存储图像中找到的索引的实际颜色。为此，我们使用图像（包含索引）索引rgb_image向量，并将调色板中匹配的颜色分配给这些索引。
- en: With the data we have looked at thus far, let’s define a TensorFlow data pipeline
    that can transform and convert the data to a format acceptable by the model.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 利用我们迄今为止看到的数据，让我们定义一个TensorFlow数据管道，将数据转换和转换为模型可接受的格式。
- en: Exercise 1
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 练习1
- en: You have been provided with an rgb_image in RGB format, where each pixel belongs
    to one of n distinctive colors and has been given a palette called palette, which
    is a [n,3]-sized array. How would you convert the rgb_image to a palettized image?
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经提供了一个以RGB格式表示的rgb_image，其中每个像素属于n种独特的颜色之一，并且已经给出了一个称为调色板的调色板，它是一个[n,3]大小的数组。你将如何将rgb_image转换为调色板图像？
- en: 'Hint You can create the naïve solution by using three for loops: two loops
    to get a single pixel of rgb_image and then a final loop to traverse each color
    in the palette.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 提示 你可以通过使用三个for循环来创建一个简单的解决方案：两个循环用于获取rgb_image的单个像素，然后最后一个循环用于遍历调色板中的每种颜色。
- en: '8.2 Getting serious: Defining a TensorFlow data pipeline'
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 认真对待：定义一个TensorFlow数据管道
- en: 'So far, we have discussed the data that will help us build a navigation algorithm
    for the RC toy. Before building a model, an important task to complete is having
    a scalable data ingestion method from disk to the model. Doing this upfront will
    save us a lot of time when we’re ready to scale or productionize. You think the
    best way is to implement a tf.data pipeline to retrieve images from the disk,
    preprocess them, transform them, and have them ready for the model to grab them.
    This pipeline should read images in, reshape them to a fixed size (in the case
    of variable-sized images), augment them (during the training stage), batch them,
    and repeat this process for a desired number of epochs. Finally, we will define
    three pipelines: a training data pipeline, a validation data pipeline, and a testing
    data pipeline.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了将帮助我们为RC玩具构建导航算法的数据。在构建模型之前，一个重要的任务是完成从磁盘到模型的可扩展数据摄取方法。提前完成这项工作将节省大量时间，当我们准备扩展或投产时。你认为最好的方法是实现一个tf.data管道，从磁盘中检索图像，对它们进行预处理、转换，并使其准备好供模型获取。该管道应该读取图像，将它们重塑为固定大小（对于变尺寸图像），对它们进行数据增强（在训练阶段），分批处理它们，并为所需的epoch数重复此过程。最后，我们将定义三个管道：一个训练数据管道，一个验证数据管道和一个测试数据管道。
- en: 'Our goal at the end of the data exploration stage should be to build a reliable
    data pipeline from the disk to the model. This is what we will be looking at here.
    At a high level, we will build a TensorFlow data pipeline that will perform the
    following tasks:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据探索阶段结束时，我们的目标应该是建立一个从磁盘到模型的可靠数据管道。这就是我们将在这里看到的。从高层次来看，我们将建立一个TensorFlow数据管道，执行以下任务：
- en: Get the filenames belonging to a certain subset (e.g., training, validation,
    or testing).
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取属于某个子集（例如，训练、验证或测试）的文件名。
- en: Read the specified images from the disk.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从磁盘中读取指定的图像。
- en: Preprocess the images (this involves normalizing/resizing/cropping images).
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预处理图像（包括对图像进行归一化/调整大小/裁剪）。
- en: Perform augmentation on the images to increase the volume of data.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对图像执行增强以增加数据量。
- en: Batch the data in small batches.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据分批处理成小批次。
- en: Optimize data retrieval using several built-in optimization techniques.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用几种内置优化技术优化数据检索。
- en: As the first step, we will write a function that returns a generator that will
    generate filenames of the data that we want to be fetched. We will also provide
    the ability to specify which subset the user wants to be fetched (e.g., training,
    validation, or testing). Returning data through a generator will make writing
    a tf.data pipeline easier (see the following listing).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一步，我们将编写一个函数，返回一个生成器，该生成器将生成我们要获取的数据的文件名。我们还将提供指定用户想要获取的子集（例如，训练、验证或测试）的能力。通过生成器返回数据将使编写`tf.data`流水线更容易（参见下面的代码清单）。
- en: Listing 8.3 Retrieving the filenames for a given subset of data
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 检索给定数据子集的文件名列表
- en: '[PRE4]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Read the CSV file that contains the training instance filenames.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 读取包含训练实例文件名的CSV文件。
- en: ❷ For validation/test subsets, perform a one-time shuffle to make sure we get
    a good mix with a fixed seed.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 对验证/测试子集执行一次洗牌，以确保我们使用固定的种子得到良好的混合。
- en: ❸ Read the CSV file that contains validation/test filenames.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 读取包含验证/测试文件名的CSV文件。
- en: ❹ Shuffle the data after fixing the seed.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 修复种子后对数据进行洗牌。
- en: ❺ Get the first half as the validation set.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将第一半部分作为验证集。
- en: ❻ Get the second half as the test set.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将第二半部分作为测试集。
- en: ❼ Form absolute paths to the input image files we captured (depending on the
    subset argument).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 形成我们捕获的输入图像文件的绝对路径（取决于子集参数）。
- en: ❽ Return the filename pairs (input and annotations) as a generator.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 将文件名对（输入和注释）作为生成器返回。
- en: ❾ Form absolute paths to the segmented image files.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 形成分段图像文件的绝对路径。
- en: 'You can see that we’re passing a few arguments when reading the CSV files.
    These arguments characterize the file we’re reading. These files are extremely
    simple and contain just a single image filename on a single line. index_col=None
    means that the file does not have an index column, header=None means there is
    no header in the file, and squeeze=True means that the output will be presented
    as a pandas Series, not a pandas Dataframe. With that, we can define a TensorFlow
    data set (tf.data.Dataset) as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，在读取CSV文件时我们传递了一些参数。这些参数描述了我们正在读取的文件。这些文件非常简单，每行只包含一个图像文件名。`index_col=None`表示文件没有索引列，`header=None`表示文件没有标题，`squeeze=True`表示输出将被呈现为pandas
    Series，而不是pandas Dataframe。有了这些，我们可以定义一个TensorFlow数据集（`tf.data.Dataset`），如下所示：
- en: '[PRE5]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: TensorFlow has several different functions for generating data sets using different
    sources. As we have defined the function get_subset_filenames() to return a generator,
    we will use the tf.data.Dataset.from_generator() function. Note that we need to
    provide the format as well as the datatypes of the returned data, by the generator,
    using the output_types argument. The function subset_filename_gen_func returns
    two strings; therefore, we define output types as a tuple of two tf.string elements.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow有几个不同的函数，用于使用不同的来源生成数据集。由于我们已经定义了函数`get_subset_filenames()`来返回一个生成器，我们将使用`tf.data.Dataset.from_generator()`函数。注意，我们需要提供返回数据的格式和数据类型，通过生成器使用`output_types`参数。函数`subset_filename_gen_func`返回两个字符串；因此，我们将输出类型定义为两个`tf.string`元素的元组。
- en: 'One other important aspect is the different txt files we read from depending
    on the subset. There are three different files in the relative path: the data\VOCtrainval_11-May-2012\VOCdevkit\VOC2012\ImageSets\Segmentation
    folder; train.txt, val.txt, and trainval.txt. Here, train.txt contains the filenames
    of the training images, whereas val.txt contains the filenames of the validation/testing
    images. We will use these files to create different pipelines that produce different
    data.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要方面是我们根据子集从不同的txt文件中读取的情况。在相对路径中有三个不同的文件：`data\VOCtrainval_11-May-2012\VOCdevkit\VOC2012\ImageSets\Segmentation`
    文件夹；`train.txt`、`val.txt` 和 `trainval.txt`。在这里，`train.txt` 包含训练图像的文件名，而 `val.txt`
    包含验证/测试图像的文件名。我们将使用这些文件创建不同的流水线，产生不同的数据。
- en: Where does tf.data come from?
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.data` 是从哪里来的？'
- en: 'TensorFlow’s tf.data pipeline can consume data from various sources. Here are
    some of the commonly used methods to retrieve data:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow的`tf.data`流水线可以从各种来源消耗数据。以下是一些常用的检索数据的方法：
- en: tf.data.Dataset.from_generator(gen_fn)—You have already seen this function in
    action. If you have a generator (i.e., gen_fn) that produces data, you want it
    to be processed through a tf.data pipeline. This is the easiest method to use.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.data.Dataset.from_generator(gen_fn)` ——你已经在实际操作中见过这个函数。如果你有一个生成器（即 `gen_fn`）产生数据，你希望它通过一个
    `tf.data` 流水线进行处理。这是使用的最简单的方法。'
- en: 'tf.data.Dataset.from_tensor_slices(t)—This is a very useful function if you
    have data already loaded as a big matrix. t can be an N-dimensional matrix, and
    this function will extract element by element on the first dimension. For example,
    assume that you have loaded a tensor t of size 3 × 4 to memory:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: tf.data.Dataset.from_tensor_slices(t)——如果你已经将数据加载为一个大矩阵，这是一个非常有用的函数。t 可以是一个
    N 维矩阵，这个函数将在第一个维度上逐个元素地提取。例如，假设你已经将一个大小为 3 × 4 的张量 t 加载到内存中：
- en: '[PRE6]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Then you can easily set up a tf.data pipeline as follows. tf.data.Dataset.from_
    tensor_slices(t) *will return* [1,2,3,4], *then* [2,3,4,5], *and finally* [6,7,8,9]
    when you iterate this data pipeline. In other words, you are seeing one row (i.e.,
    a slice from the batch dimension, hence the name from_tensor_slices) at a time.
    You can now incorporate functions like tf.data.Dataset.batch() to get a batch
    of rows.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以轻松地设置一个 tf.data 管道，如下所示。tf.data.Dataset.from_tensor_slices(t) *将返回* [1,2,3,4]，*然后*
    [2,3,4,5]，*最后* [6,7,8,9] 当你迭代这个数据管道时。换句话说，你每次看到一行（即从批处理维度中切片，因此称为 from_tensor_slices）。
- en: 'Now it’s time to read in the images found in the file paths we obtained in
    the previous step. TensorFlow has support to easily load an image, where the path
    to a filename is img_filename, using the functions tf.io.read_file and tf.image.decode_image.
    Here, img_filename is a tf.string (i.e., a string in TensorFlow):'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候读取我们在上一步获取的文件路径中找到的图像了。TensorFlow 提供了支持，可以轻松加载图像，其中文件名路径为 img_filename，使用函数
    tf.io.read_file 和 tf.image.decode_image。在这里，img_filename 是一个 tf.string（即 TensorFlow
    中的字符串）：
- en: '[PRE7]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We will use this pattern to load input images. However, we need to implement
    a custom image load function to load the target image. If you use the previous
    approach, it will automatically convert the image to an array with pixel values
    (instead of palette indices). But if we don’t perform that conversion, we will
    have a target array that is in the exact format we need because the palette indices
    that are in the target image are the actual class labels for each corresponding
    pixel in the input image. We will use PIL.Image within our TensorFlow data pipeline
    to load the image as a palettized image and avoid converting it to RGB:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这种模式来加载输入图像。然而，我们需要实现一个自定义图像加载函数来加载目标图像。如果你使用前面的方法，它将自动将图像转换为具有像素值的数组（而不是调色板索引）。但如果我们不执行该转换，我们将得到一个精确符合我们需要的格式的目标数组，因为目标图像中的调色板索引是输入图像中每个对应像素的实际类标签。我们将在
    TensorFlow 数据管道中使用 PIL.Image 来加载图像作为调色板图像，并避免将其转换为 RGB：
- en: '[PRE8]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'However, you can’t yet use custom functions as part of the tf.data pipeline.
    They need to be streamlined with the data-flow graph of the data pipeline by wrapping
    it as a TensorFlow operation. This can be easily achieved by using the tf.numpy_function
    operation, which allows you to wrap a custom function that returns a NumPy array
    as a TensorFlow operation. If we have the target image’s file path represented
    by y, you can use the following code to load the image into TensorFlow with a
    custom image-loading function:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，你还不能将自定义函数作为 tf.data 管道的一部分使用。它们需要通过将其包装为 TensorFlow 操作来与数据管道的数据流图相协调。这可以通过使用
    tf.numpy_function 操作轻松实现，它允许你将返回 NumPy 数组的自定义函数包装为 TensorFlow 操作。如果我们用 y 表示目标图像的文件路径，你可以使用以下代码将图像加载到
    TensorFlow 中并使用自定义图像加载函数：
- en: '[PRE9]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The dark side of tf.numpy_function
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**tf.numpy_function** 的黑暗面'
- en: NumPy has larger coverage for various scientific computations than TensorFlow,
    so you might think that tf.numpy_funtion makes things very convenient. This is
    not quite true, as you can infest your TensorFlow code with terrible performance
    degradations. When TensorFlow executes NumPy code, it can create very inefficient
    data flow graphs and introduce overheads. Therefore, always try to stick to TensorFlow
    operations and use custom NumPy code only if you have to. In our case, since there
    is no alternative way for us to load a palletized image without mapping palletized
    values to actual RGB, we used a custom function.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy 对各种科学计算有比 TensorFlow 更广泛的覆盖，所以你可能会认为 tf.numpy_function 让事情变得非常方便。但事实并非如此，因为你可能会在
    TensorFlow 代码中引入可怕的性能下降。当 TensorFlow 执行 NumPy 代码时，它可能会创建非常低效的数据流图并引入开销。因此，尽量坚持使用
    TensorFlow 操作，并且只在必要时使用自定义的 NumPy 代码。在我们的情况下，由于没有其他方法可以加载调色板图像而不将调色板值映射到实际的 RGB，我们使用了一个自定义函数。
- en: 'Notice how we’re passing both the input (i.e., inp=[y]) and its data type (i.e.,
    Tout=[tf.uint8]) to this function. They both need to be in the form of a Python
    list. Finally, let’s collate everything we discussed in one place:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们将输入（即，inp=[y]）和其数据类型（即，Tout=[tf.uint8]）都传递给此函数。它们都需要以 Python 列表的形式存在。最后，让我们把我们讨论的所有内容都整理到一个地方：
- en: '[PRE10]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The tf.data.Dataset.map() function will be used quite heavily throughout this
    discussion. You can find a lengthy explanation of the map() function in the sidebar.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: tf.data.Dataset.map() 函数将在本讨论中大量使用。您可以在侧边栏中找到 map() 函数的详细解释。
- en: 'A Refresher: tf.data.Dataset.map() function'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 刷新器：tf.data.Dataset.map() 函数
- en: This tf.data pipeline will make extensive use of the tf.data.Dataset.map() function.
    Therefore, it is extremely helpful for us to remind ourselves what this function
    accomplishes.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 此 tf.data 管道将大量使用 tf.data.Dataset.map() 函数。因此，我们提醒自己此函数实现了什么功能是非常有帮助的。
- en: The td.data.Dataset.map() function applies a given function or functions across
    all the records in a data set. In other words, it transforms the data points in
    the data set using a specified transformation. For example, assume the tf.data.Dataset
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: td.data.Dataset.map() 函数将给定的函数或多个函数应用于数据集中的所有记录。换句话说，它使用指定的转换来转换数据集中的数据点。例如，假设
    tf.data.Dataset
- en: '[PRE11]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: to get the square of each element, you can use the map function as
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取每个元素的平方，可以使用 map 函数如下
- en: '[PRE12]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'If you have multiple elements in a single record, leveraging the flexibility
    of map(), you can transform them individually:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在单个记录中有多个元素，则可以利用map()的灵活性来分别转换它们：
- en: '[PRE13]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As a normalization step we will bring the pixel values to [0,1] range by using
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 作为规范化步骤，我们将通过使用将像素值带到 [0,1] 范围的方法
- en: '[PRE14]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note that we are keeping our target image (y) as it is. Before I continue with
    any more steps in our pipeline, I want to direct your attention to an important
    matter. This is a caveat that is quite common, and it is thus worthwhile to be
    aware of it. After the step we just completed, you might feel like, if you want,
    you can batch the data and feed it to the model. For example
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们保留了目标图像（y）。在我们的管道中继续进行更多步骤之前，我想引起您的注意。这是一个相当常见的警告，因此值得注意。在我们刚刚完成的步骤之后，您可能会觉得，如果您愿意，您可以将数据进行批处理并将其馈送到模型中。例如
- en: '[PRE15]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'If you do that for this data set, you will get an error like the following:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对此数据集进行此操作，将会收到以下错误：
- en: '[PRE16]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This is because you ignored a crucial characteristic and a sanity check of the
    data set. Unless you’re using a curated data set, you are unlikely to find images
    with the same dimensions. If you look at images in the data set, you will notice
    that they are not of the same size; they have different heights and widths. In
    TensorFlow, unless you use a special data structure like tf.RaggedTensor, you
    cannot batch unequally sized images together. That is exactly what TensorFlow
    is complaining about in the error.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为您忽略了数据集的一个关键特征和一个健全性检查。除非您使用的是经过筛选的数据集，否则您不太可能找到具有相同尺寸的图像。如果您查看数据集中的图像，您会注意到它们的尺寸不同；它们具有不同的高度和宽度。在
    TensorFlow 中，除非您使用像 tf.RaggedTensor 这样的特殊数据结构，否则无法将大小不同的图像一起进行批处理。这正是 TensorFlow
    在错误中抱怨的内容。
- en: To alleviate the problem, we need to bring all the images to a standard size
    (see listing 8.4). To do that, we will define the following function. It will
    either
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓解问题，我们需要将所有图像调整为标准大小（请参见列表 8.4）。为此，我们将定义以下函数。它将
- en: Resize the image to a larger size (resize_to_before_crop) and then crop the
    image to the desired size (input_size) or
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将图像调整为较大的尺寸（resize_to_before_crop），然后将图像裁剪为所需大小（input_size），或者
- en: Resize the image to the desired size (input_size)
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将图像调整为所需大小（input_size）
- en: Listing 8.4 Bringing images to a fixed size using random cropping or resizing
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.4 使用随机裁剪或调整大小将图像调整为固定大小
- en: '[PRE17]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Define a function to randomly crop images after resizing.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义一个函数，在调整大小后随机裁剪图像。
- en: ❷ Resize the input image using bilinear interpolation to a larger size.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用双线性插值将输入图像调整为较大的尺寸。
- en: ❸ Resize the target image using the nearest interpolation to a larger size.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用最近邻插值将目标图像调整为较大的尺寸。
- en: ❹ To resize, we first swap the axis of y as it has the shape [1, height, width].
    We convert this back to [height, width, 1] (i.e., a single channel image) using
    the tf.transpose() function.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 要调整大小，我们首先交换 y 轴的轴，因为它的形状为 [1, height, width]。我们使用 tf.transpose() 函数将其转换回
    [height, width, 1]（即，单通道图像）。
- en: ❺ Define a random variable to offset images on height during cropping.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 定义一个随机变量，在裁剪期间偏移图像的高度。
- en: ❻ Define a random variable to offset images on width during cropping.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 定义一个随机变量，在裁剪期间在宽度上对图像进行偏移。
- en: ❼ Crop the input image and the target image using the same cropping parameters.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 使用相同的裁剪参数裁剪输入图像和目标图像。
- en: ❽ Resize both the input image and the target image to a desired size (no cropping).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 将输入图像和目标图像都调整为所需大小（不裁剪）。
- en: ❾ Define a random variable (used to perform augmentations).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 定义一个随机变量（用于执行增强）。
- en: ❿ If augmentation is enabled and the resized image is larger than the input
    size we requested, perform augmentation.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 如果启用增强并且调整大小后的图像大于我们请求的输入大小，则执行增强。
- en: ⓫ During augmentation, the rand_crop or resize function is executed randomly.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ⓫ 在增强期间，随机执行 rand_crop 或 resize 函数。
- en: ⓬ If augmentation is disabled, only resize images.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ⓬ 如果禁用增强，则只调整大小。
- en: Here, we define a function called randomly_crop_or_resize, which has two nested
    functions, rand_crop and resize. The rand_crop first resizes the image to the
    size specified in resize_to_before_crop and creates a random crop. It is imperative
    to check that you applied the exact same crop to both the input and the target.
    For example, same-crop parameters should be used to crop both the input and the
    target. In order to crop images, we use
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们定义了一个名为 randomly_crop_or_resize 的函数，其中包含两个嵌套函数 rand_crop 和 resize。rand_crop
    首先将图像调整为 resize_to_before_crop 中指定的大小，并创建一个随机裁剪。务必检查是否对输入和目标应用了完全相同的裁剪。例如，应使用相同的裁剪参数对输入和目标进行裁剪。为了裁剪图像，我们使用
- en: '[PRE18]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The arguments are self-explanatory: image takes an image to be cropped, offset_height
    and offset_width decide the starting point for the crop, and target_height and
    target_width specify the final size after the crop. The resize function will simply
    resize the input and the target to a specified size using the tf.image.resize
    operation.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 参数的含义不言而喻：image 接受要裁剪的图像，offset_height 和 offset_width 决定裁剪的起点，target_height
    和 target_width 指定裁剪后的最终大小。resize 函数将使用 tf.image.resize 操作简单地将输入和目标调整为指定大小。
- en: When resizing, we use *bilinear interpolation* for the input images and *nearest
    interpolation* for targets. Bilinear interpolation resizes the images by computing
    the resulting pixels, as an average of neighboring pixels, whereas nearest interpolation
    computes the output pixel as the nearest most common pixel from the neighbors.
    Bilinear interpolation leads to a smoother result after resizing. However, you
    must use nearest interpolation for the target image, as bilinear interpolation
    will lead to fractional outputs, corrupting the integer-based annotations. The
    interpolation techniques described are visualized in figure 8.6.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在调整大小时，我们对输入图像使用*双线性插值*，对目标使用*最近邻插值*。双线性插值通过计算结果像素的邻近像素的平均值来调整图像大小，而最近邻插值通过从邻居中选择最近的常见像素来计算输出像素。双线性插值在调整大小后会导致更平滑的结果。然而，必须对目标图像使用最近邻插值，因为双线性插值会导致分数输出，破坏基于整数的注释。图
    8.6 可视化了所描述的插值技术。
- en: '![08-06](../../OEBPS/Images/08-06.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![08-06](../../OEBPS/Images/08-06.png)'
- en: Figure 8.6 Nearest interpolation and bilinear interpolation for both up-sampling
    and down-sampling tasks
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.6 最近邻插值和双线性插值用于上采样和下采样任务
- en: 'Next, we will introduce an additional step to the way we’re going to use these
    two nested functions. If augmentation is enabled, we want the cropping or resizing
    to take place randomly within the pipeline. We will define a random variable (drawn
    from a uniform distribution between 0 and 1) and perform crop or resize depending
    on the value of the random variable at a given time. This conditioning can be
    achieved using the tf.cond function, which takes three arguments and returns output
    according to these arguments:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将在使用这两个嵌套函数的方式上引入一个额外的步骤。如果启用了增强，我们希望裁剪或调整大小在管道中随机地发生。我们将定义一个随机变量（从介于
    0 和 1 之间的均匀分布中抽取）并根据随机变量的值在给定时间内执行裁剪或调整大小。可以使用 tf.cond 函数实现这种条件，该函数接受三个参数，并根据这些参数返回输出：
- en: Condition—This is a computation that results in a Boolean value (i.e., is the
    random variable rand greater than 0.5).
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Condition——这是一个计算结果为布尔值的计算（即随机变量 rand 是否大于 0.5）。
- en: true_fn—If the condition is true, then this function will be executed (i.e.,
    perform rand_crop on both x and y)
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: true_fn——如果条件为真，则执行此函数（即对 x 和 y 执行 rand_crop）
- en: false_fn—If the condition is false, then this function will be executed (i.e.,
    perform a resize on both x and y)
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: false_fn——如果条件为假，则执行此函数（即对 x 和 y 执行调整大小）
- en: 'If augmentation is disabled (i.e., by setting the augmentation variable to
    False), only resizing is performed. With the details fleshed out, we can use the
    randomly_crop_ or_resize function in our data pipeline as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如果禁用了增强（即通过将`augmentation`变量设置为`False`），则仅执行调整大小操作。详细信息澄清后，我们可以在我们的数据管道中使用`randomly_crop_or_resize`函数如下：
- en: '[PRE19]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: At this point, we have a globally fixed-sized image coming out of our pipeline.
    The next thing we address is very important. Factors such as variable size of
    images and custom NumPy functions used to load images make it impossible for TensorFlow
    to infer the shape of its final tensor (though it’s a fixed-sized tensor) after
    a few steps. If you check the shapes of the tensors produced at this point, you
    will probably perceive them as
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们的管道中出现了一个全局固定大小的图像。接下来我们要处理的事情非常重要。诸如图像大小可变和用于加载图像的自定义 NumPy 函数等因素使得 TensorFlow
    在几个步骤之后无法推断其最终张量的形状（尽管它是一个固定大小的张量）。如果您检查此时产生的张量的形状，您可能会将它们视为
- en: '[PRE20]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This means that TensorFlow was unable to infer the shape of the tensors. To
    avoid any ambiguities or problems moving forward, we will set the shape of the
    output we have in the pipeline. For a tensor t, if the shape is ambiguous but
    you know the shape, you can set the shape manually using
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着 TensorFlow 无法推断张量的形状。为了避免任何歧义或问题，我们将设置管道中输出的形状。对于张量`t`，如果形状不明确但您知道形状，您可以使用手动设置形状
- en: '[PRE21]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In our data pipeline, we can set the shape as
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的数据管道中，我们可以设置形状为
- en: '[PRE22]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We know that the outputs following the resize or crop are going to be
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道跟随调整大小或裁剪的输出将会是
- en: '*Input image*—An RGB image with input_size height and width'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输入图像* —— 一个具有`input_size`高度和宽度的 RGB 图像'
- en: '*Target image*—A single-channel image with input_size height and width'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*目标图像* —— 一个具有`input_size`高度和宽度的单通道图像'
- en: We will set the shape accordingly using the tf.data.Dataset.map() function.
    We cannot underestimate the power of data augmentation, so we will introduce several
    data augmentation steps to our data pipeline (see the next listing).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`tf.data.Dataset.map()`函数相应地设置形状。不能低估数据增强的威力，因此我们将向我们的数据管道引入几个数据增强步骤（见下一篇列表）。
- en: Listing 8.5 Functions used for random augmentation of images
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.5 用于图像随机增强的函数
- en: '[PRE23]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ Define a random variable.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义一个随机变量。
- en: ❷ Define a function to flip images deterministically.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义一个函数来确定性地翻转图像。
- en: ❸ Using the same pattern as before, we use tf.cond to randomly perform horizontal
    flipping.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用与之前相同的模式，我们使用`tf.cond`随机执行水平翻转。
- en: ❹ Randomly flip images in the data set.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 在数据集中随机翻转图像。
- en: ❺ Randomly adjust the hue (i.e., color) of the input image (target stays the
    same).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 随机调整输入图像的色调（即颜色）（目标保持不变）。
- en: ❻ Randomly adjust the brightness of the input image (target stays the same).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 随机调整输入图像的亮度（目标保持不变）。
- en: ❼ Randomly adjust the contrast of the input image (target stays the same).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 随机调整输入图像的对比度（目标保持不变）。
- en: 'In listing 8.5, we perform the following translations:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表 8.5 中，我们执行以下翻译：
- en: Randomly flipping images horizontally
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机水平翻转图像
- en: Randomly changing the hue of the images (up to 10%)
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机改变图像的色调（最多10%）
- en: Randomly changing the brightness of the images (up to 10%)
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机改变图像的亮度（最多10%）
- en: Randomly changing the contrast of the images (up to 20%)
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机改变图像的对比度（最多20%）
- en: 'By using the tf.data.Dataset.map() function, we can easily perform the specified
    random augmentation steps, should the user enable augmentation in the pipeline
    (i.e., by setting the augmentation variable to True). Note that we’re performing
    some augmentations (e.g., random hue, brightness, and contrast adjustments) on
    the input image only. We will also give the user the option to have different-sized
    inputs and targets (i.e., outputs). This is achieved by resizing the output to
    a desired size, defined by the output_size argument. The model we use for this
    task has different-sized input and output dimensions:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`tf.data.Dataset.map()`函数，我们可以在管道中轻松执行指定的随机增强步骤，如果用户在管道中启用了增强（即通过将`augmentation`变量设置为`True`）。请注意，我们仅对输入图像执行一些增强（例如，随机色调、亮度和对比度调整）。我们还将给用户提供具有不同尺寸的输入和目标（即输出）的选项。这通过将输出调整为由`output_size`参数定义的所需大小来实现。我们用于此任务的模型具有不同尺寸的输入和输出维度：
- en: '[PRE24]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Again, here we use the nearest interpolation to resize the target. Next, we
    will shuffle the data (if the user set the shuffle argument to True):'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，这里我们使用最近邻插值来调整目标的大小。接下来，我们将对数据进行洗牌（如果用户将`shuffle`参数设置为`True`）：
- en: '[PRE25]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The shuffle function takes an important argument called buffer_size, which
    determines how many samples are loaded to memory in order to select a sample randomly.
    The higher the buffer_size, the more randomness you are introducing. On the other
    hand, a higher buffer_size implies higher memory consumption. It’s now time to
    batch the data, so instead of a single data point, we get a batch of data when
    we iterate:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 混洗函数有一个重要参数称为`buffer_size`，它确定了加载到内存中以随机选择样本的样本数量。`buffer_size`越高，引入的随机性就越多。另一方面，较高的`buffer_size`意味着更高的内存消耗。现在是批处理数据的时候了，所以在迭代时不是单个数据点，而是当我们迭代时获得一批数据：
- en: '[PRE26]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This is done using the tf.data.Dataset.batch() function and passing the desired
    batch size as the argument. When using the tf.data pipeline, if you are running
    it for multiple epochs, you also need to use the tf.data.Dataset.repeat() function
    to repeat the pipeline for a given number of epochs.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用`tf.data.Dataset.batch()`函数完成的，将所需的批次大小作为参数传递。在使用`tf.data`管道时，如果要运行多个周期，还需要使用`tf.data.Dataset.repeat()`函数重复管道给定次数的周期。
- en: Why do we need tf.data.Dataset.repeat()?
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为什么需要`tf.data.Dataset.repeat()`？
- en: tf.data.Dataset is a generator. A unique characteristic of a generator is that
    you only can iterate it once. After the generator reaches the end of the sequence
    it’s iterating, it will exit by throwing an exception. Therefore, if you need
    to iterate through a generator multiple times, you need to redefine the generator
    as many times as needed. By adding tf.data.Dataset.repeat(epochs), the generate
    is redefined as many times as we would like (epochs times in this example).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.data.Dataset`是一个生成器。生成器的一个独特特点是您只能迭代一次。当生成器到达正在迭代的序列的末尾时，它将通过抛出异常退出。因此，如果您需要多次迭代生成器，您需要根据需要重新定义生成器。通过添加`tf.data.Dataset.repeat(epochs)`，生成器将根据需要重新定义（在此示例中为epochs次）。'
- en: 'One more step is needed before our tf.data pipeline is done and dusted. If
    you look at the shape of the target (y) output, you will see that it has a channel
    dimension of 1\. However, for the loss function we will be using, we need to get
    rid of that dimension:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的`tf.data`管道完成之前，还需要一步。如果查看目标（y）输出的形状，您将看到它具有通道维度为1。但是，对于我们将要使用的损失函数，我们需要摆脱该维度：
- en: '[PRE27]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'For this, we will use the tf.squeeze() operation, which removes any dimensions
    that are of size 1 and returns a tensor. For example, if you squeeze a tensor
    of size [1,3,2,1,5], you will get a [3,2,5] sized tensor. The final code is provided
    in listing 8.6\. You might notice two steps that are highlighted. These are two
    popular optimization steps available: caching and prefetching.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 对此，我们将使用`tf.squeeze()`操作，该操作会删除尺寸为1的任何维度并返回一个张量。例如，如果您压缩一个尺寸为`[1,3,2,1,5]`的张量，您将得到一个尺寸为`[3,2,5]`的张量。最终的代码在清单8.6中提供。您可能会注意到两个突出显示的步骤。这是两个流行的优化步骤：缓存和预提取。
- en: Listing 8.6 The final tf.data pipeline
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 清单8.6 最终的`tf.data`管道
- en: '[PRE28]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: ❶ If augmentation is enabled, resize_to_before_crop needs to be defined.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 如果启用了增强，则需要定义`resize_to_before_crop`。
- en: ❷ Return a list of filenames depending on the subset of data requested.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 根据所请求的数据子集返回文件名列表。
- en: ❸ Load the images into memory. cache() is an optimization step and will be discussed
    in the text.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将图像加载到内存中。`cache()`是一个优化步骤，将在文本中讨论。
- en: ❹ Normalize the input images.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 规范化输入图像。
- en: ❺ The function that randomly crops or resizes images
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 随机裁剪或调整图像大小的函数
- en: ❻ Perform random crop or resize on the images.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 在图像上执行随机裁剪或调整大小。
- en: ❼ Set the shape of the resulting images.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 设置结果图像的形状。
- en: ❽ Randomly perform various augmentations on the data.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 在数据上随机执行各种增强。
- en: ❾ Resize the output image if needed.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 根据需要调整输出图像的大小。
- en: ❿ Shuffle the data using a buffer.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 使用缓冲区对数据进行随机混洗。
- en: ⓫ Batch the data and repeat the process for a desired number of epochs.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ⓫ 批处理数据并为所需的周期重复该过程。
- en: ⓬ This is an optimization step discussed in detail in the text.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ⓬ 这是文本中详细讨论的优化步骤。
- en: ⓭ Remove the unnecessary dimension from target images.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ⓭ 从目标图像中删除不必要的维度。
- en: ⓮ Get the final tf.data pipeline.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: ⓮ 获取最终的`tf.data`管道。
- en: 'It wasn’t an easy journey, but it was a rewarding one. We have learned some
    important skills in defining the data pipeline:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是一次轻松的旅程，但却是一次有益的旅程。我们学到了一些定义数据管道的重要技能：
- en: Defining a generator that returns the filenames of the data to be fetched
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义一个生成器，返回要获取的数据的文件名
- en: Loading images within a tf.data pipeline
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`tf.data`管道中加载图像
- en: Manipulating images (resizing, cropping, brightness adjustment, etc.)
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作图像（调整大小、裁剪、亮度调整等）
- en: Batching and repeating data
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据批处理和重复
- en: Defining multiple pipelines for different data sets with different requirements
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为不同数据集定义多个流水线，这些数据集具有不同的要求
- en: Next, we will look at some optimization techniques to turn our mediocre data
    pipeline into an impressive data highway.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将查看一些优化技术，将我们平庸的数据流水线转变为令人印象深刻的数据高速公路。
- en: 8.2.1 Optimizing tf.data pipelines
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.1 优化 tf.data 流水线
- en: 'TensorFlow is a framework meant for consuming large data sets, where consuming
    data in an efficient manner is a key priority. One thing still missing from our
    conversation is what kind of optimization steps are available for tf.data pipelines,
    so let us nudge this discussion in that direction. Two steps were set in bold
    in listing 8.6: caching and prefetching. If you are interested in other optimization
    techniques, you can read more at [https://www.tensorflow.org/guide/data_performance](https://www.tensorflow.org/guide/data_performance).'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 是一个用于消耗大型数据集的框架，高效地消耗数据是一个关键优先事项。我们对 tf.data 流水线的讨论中仍然缺少一件事，即 tf.data
    流水线可用的优化步骤。在列表 8.6 中，缓存和预取两个步骤被加粗设置。如果您对其他优化技术感兴趣，可以在 [https://www.tensorflow.org/guide/data_performance](https://www.tensorflow.org/guide/data_performance)
    上阅读更多。
- en: 'Caching will store the data in memory as it flows through the pipeline. This
    means that, when cached, that step (e.g., loading the data from the disk) happens
    only in the first epoch. The subsequent epochs will read from the cached data
    that’s held in memory. Here, you can see that we’re caching the images after we
    load them to memory. This way, TensorFlow loads the images in the first epoch
    only:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存将在数据通过流水线时将其存储在内存中。这意味着当缓存时，该步骤（例如，从磁盘加载数据）仅在第一个时期发生。随后的时期将从内存中保存的缓存数据中读取。在这里，您可以看到我们将图像加载到内存后进行缓存。这样，TensorFlow
    仅在第一个时期加载图像：
- en: '[PRE29]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Prefetching is another powerful weapon you have at your disposal, and it allows
    you to leverage the multiprocessing power of your device:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: Prefetching 是你可以使用的另一个强大武器，它允许你利用设备的多进程能力：
- en: '[PRE30]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The argument provided to the function decides how much data is prefetched. By
    setting it to AUTOTUNE, TensorFlow will decide the best amount of data to be fetched
    depending on the resources available. Assume a simple data pipeline that loads
    images from the disk and trains a model. Then, the data read and model training
    will happen in interleaved steps. This leads to significant idling time, as the
    model idles while the data is loading, and vice versa.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 提供给函数的参数决定了预取多少数据。通过将其设置为 AUTOTUNE，TensorFlow 将根据可用资源决定要获取的最佳数据量。假设一个简单的数据流水线从磁盘加载图像并训练模型。然后，数据读取和模型训练将交替进行。这导致了显着的空闲时间，因为模型在数据加载时空闲，反之亦然。
- en: However, thanks to prefetching, this doesn’t need to be the case. Prefetching
    employs background threads and an internal buffer to load the data in advance
    while the model is training. When the next iteration comes, the model can seamlessly
    continue the training as data is already fetched into the memory. The differences
    between sequential execution and prefetching are shown in figure 8.7.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，多亏了预取，情况就不一样了。预取利用后台线程和内部缓冲区，在模型训练时提前加载数据。当下一次迭代到来时，模型可以无缝地继续训练，因为数据已经被提前加载到内存中。图
    8.7 显示了顺序执行和预取之间的差异。
- en: '![08-07](../../OEBPS/Images/08-07.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![08-07](../../OEBPS/Images/08-07.png)'
- en: Figure 8.7 Sequential execution versus pre-fetching-based execution in model
    training
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.7 模型训练中的顺序执行与基于预取的执行的区别
- en: Next, we will look at the finished tf.data pipeline for the image segmentation
    problem.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将查看图像分割问题的完整 tf.data 流水线。
- en: 8.2.2 The final tf.data pipeline
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.2 最终的 tf.data 流水线
- en: 'Finally, you can define the data pipeline(s) using the functions we have defined
    so far. Here, we define three different data pipelines for three different purposes:
    training, validation, and testing (see the following listing).'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您可以使用我们迄今为止定义的函数来定义数据流水线（们）。在这里，我们为三个不同的目的定义了三种不同的数据流水线：训练、验证和测试（见下面的列表）。
- en: Listing 8.7 Creating the train/validation/test data pipelines instances
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.7 创建训练/验证/测试数据流水线实例
- en: '[PRE31]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: ❶ Directory where the input images are
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 包含输入图像的目录
- en: ❷ Directory where the annotated images (targets) are
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 包含注释图像（目标）的目录
- en: ❸ Directory where the text files containing train/validation/test filenames
    are
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 包含训练/验证/测试文件名的文本文件所在的目录
- en: ❹ Define a reusable partial function from get_subset_filenames.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 从 get_subset_filenames 定义一个可重用的部分函数。
- en: ❺ Define three generators for train/validation/test data.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 为训练/验证/测试数据定义三个生成器。
- en: ❻ Define input image size.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 定义输入图像尺寸。
- en: ❼ Define a train data pipeline that uses data augmentation and shuffling.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 定义一个使用数据增强和洗牌的训练数据流水线。
- en: ❽ Define a validation data pipeline that doesn’t use data augmentation or shuffling.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 定义一个不使用数据增强或洗牌的验证数据流水线。
- en: ❾ Define a test data pipeline.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 定义一个测试数据流水线。
- en: 'First, we define several important paths:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义了几个重要的路径：
- en: orig_dir—Directory containing input images
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: orig_dir—包含输入图像的目录
- en: seg_dir—Directory containing the target images
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: seg_dir—包含目标图像的目录
- en: subset_dir—Directory containing text files (train.txt, val.txt) that enlist
    training and validation instances, respectively
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: subset_dir—包含列出训练和验证实例的文本文件（train.txt、val.txt）的目录
- en: 'Then we will define a partial function from the get_subset_filenames() function
    we defined earlier so that we can get a generator just by setting the subset argument
    of the function. Using this technique, we will define three generators: train_subset_fn,
    val_subset_fn, and test_subset_fn. Finally, we will define three tf.data.Datasets
    using the get_subset_tf_dataset() function. Our pipelines will have the following
    characteristics:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将从我们之前定义的 get_subset_filenames() 函数定义一个偏函数，以便我们可以通过设置函数的 subset 参数来获取一个生成器。利用这种技术，我们将定义三个生成器：train_subset_fn、val_subset_fn
    和 test_subset_fn。最后，我们将使用 get_subset_tf_dataset() 函数定义三个 tf.data.Datasets。我们的流水线将具有以下特征：
- en: '*Training pipeline*—Performs data augmentation and data shuffling on every
    epoch'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*训练流水线*—在每个 epoch 上执行数据增强和数据洗牌'
- en: '*Validation pipeline and test pipeline*—No augmentation or shuffling'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*验证流水线和测试流水线*—无增强或洗牌'
- en: 'The model we will define expects a 384 × 384-sized input and an output. In
    the training data pipeline, we will resize images to 444 × 444 and then randomly
    crop a 384 × 384-sized image. Following this, we will look at the core part of
    the solution: defining the image segmentation model.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义的模型期望一个 384 × 384 大小的输入和一个输出。在训练数据流水线中，我们将图像调整大小为 444 × 444，然后随机裁剪一个 384
    × 384 大小的图像。接下来，我们将看一下解决方案的核心部分：定义图像分割模型。
- en: Exercise 2
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 2
- en: 'You have been given a small set of data that contains two tensors: tensor a
    contains 100 64 × 64 × 3-sized images (i.e., 100 × 64 × 64 × 3 shaped), and tensor
    b contains 100 32 × 32 × 1-sized segmentation masks (i.e., 100 × 32 × 32 × 1 shaped).
    You have been asked to define a tf.data.Dataset using the functions discussed
    that will'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经获得了一个包含两个张量的小数据集：张量 a 包含 100 个大小为 64 × 64 × 3 的图像（即，100 × 64 × 64 × 3 的形状），张量
    b 包含 100 个大小为 32 × 32 × 1 的分割蒙版（即，100 × 32 × 32 × 1 的形状）。您被要求使用讨论过的函数定义一个 tf.data.Dataset，它将
- en: Resize the segmentation masks to match the input image size (using nearest interpolation)
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将分割蒙版调整大小以匹配输入图像大小（使用最近的插值）
- en: Normalize the input images using the transformation (x - 128)/255 where a single
    image is x
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用转换（x - 128）/255 对输入图像进行标准化，其中单个图像是 x
- en: Batch the data to batches of 32 and repeat for five epochs
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据批处理为大小为 32 的批次，并重复五个 epochs
- en: Prefetch the data with an auto-tuning feature
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自动调优功能预取数据
- en: '8.3 DeepLabv3: Using pretrained networks to segment images'
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 DeepLabv3：使用预训练网络对图像进行分割
- en: 'It’s now time to create the brains of the pipeline: the deep learning model.
    Based on feedback from a colleague at a self-driving car company working on similar
    problems, you will implement a DeepLab v3 model. This is a model built on the
    back of a pretrained ResNet 50 model (trained on image classification) but with
    the last several layers changed to perform *atrous convolution* instead of standard
    convolution. It uses a pyramidal aggregation module that uses atrous convolution
    at different scales to generate image features at different scales to produce
    the final output. Finally, it uses a bilinear interpolation layer to resize the
    final output to a desired size. You are confident that DeepLab v3 can deliver
    good initial results.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是创建流水线的核心部分的时候了：深度学习模型。根据一位在类似问题上工作的自动驾驶汽车公司同事的反馈，您将实现一个 DeepLab v3 模型。这是一个建立在预训练的
    ResNet 50 模型（在图像分类上训练）的基础上的模型，但最后几层被改为执行 *空洞卷积* 而不是标准卷积。它使用金字塔聚合模块，在不同尺度上使用空洞卷积来生成不同尺度上的图像特征，以产生最终输出。最后，它使用双线性插值层将最终输出调整大小为所需大小。您相信
    DeepLab v3 能够提供良好的初始结果。
- en: 'Deep neural network-based segmentation models can be broadly categorized into
    two types:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度神经网络的分割模型可以广泛分为两类：
- en: Encoder decoder models (e.g., U-Net model)
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器解码器模型（例如，U-Net 模型）
- en: Fully convolutional network (FCN) followed by a pyramidal aggregation module
    (e.g., DeepLab v3 model)
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全卷积网络（FCN）后跟金字塔聚合模块（例如，DeepLab v3模型）
- en: A well-known example of the encoder-decoder model is the U-Net model. In other
    words, U-Net has an encoder that gradually creates smaller, coarser representations
    of the input. This is followed by a decoder that takes the representations the
    encoder built and gradually up-samples (i.e., increases the size of) the output
    until it reaches the size of the input image. The up-sampling is achieved through
    an operation known as *transpose convolution*. Finally, you train the whole structure
    end to end, where an input is the input image and the target is the segmentation
    mask for the corresponding image. We will not discuss this type of model in this
    chapter. However, I have included a detailed walkthrough in appendix B (along
    with an implementation of the model).
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器-解码器模型的一个著名例子是U-Net模型。换句话说，U-Net具有逐渐创建输入的较小、更粗略表示的编码器。然后，解码器接收编码器生成的表示，并逐渐上采样（即增加输出的大小）直到达到输入图像的大小为止。上采样是通过一种称为*转置卷积*的操作实现的。最后，你以端到端的方式训练整个结构，其中输入是输入图像，目标是相应图像的分割掩码。我们不会在本章讨论这种类型的模型。然而，我在附录B中包含了一个详细的步骤说明（以及模型的实现）。
- en: The other type of segmentation models introduces a special model that replaces
    the decoder. We call this module a *pyramidal aggregation module*. Its purpose
    is to garner spatial information at different scales (e.g., different-sized outputs
    from various interim convolution layers) that provides fine-grained contextual
    information about the objects present in the image. DeepLab v3 is a prime example
    of this approach. We will put the DeepLab v3 model under the microscope and use
    it to excel at the segmentation task.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种分割模型引入了一个特殊的模块来替换解码器。我们称之为*金字塔聚合模块*。它的目的是在不同尺度上收集空间信息（例如来自各种中间卷积层的不同大小的输出），以提供关于图像中存在的对象的细粒度上下文信息。DeepLab
    v3是这种方法的一个典型示例。我们将对DeepLab v3模型进行详细分析，并借此在分割任务上取得卓越成果。
- en: Researchers and engineers gravitate toward methods that use pyramidal aggregation
    modules more. There could be many reasons for this. One lucrative reason is that
    there are less parameters in networks that use pyramidal aggregation than an encoder-decoder
    based counterpart. Another reason may be that, typically, introducing a novel
    module offers more flexibility (compared to an encoder-decoder) to engineer efficient
    and accurate feature extraction methods at multiple scales.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员和工程师更倾向于使用金字塔聚合模块的方法。可能有很多原因。一个有利可图的原因是，使用金字塔聚合的网络参数较少，比采用基于编码器-解码器的对应网络更少。另一个原因可能是，通常引入新模块（与编码器-解码器相比）提供更多灵活性，可以在多个尺度上设计高效准确的特征提取方法。
- en: How important is the pyramidal aggregation module? To know that, we have to
    first understand what the fully convolutional part of the network looks like.
    Figure 8.8 illustrates the generic structure of such a segmentation model.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 金字塔聚合模块有多重要？为了了解这一点，我们首先必须了解完全卷积网络的结构是什么样的。图8.8说明了这种分割模型的通用结构。
- en: '![08-08](../../OEBPS/Images/08-08.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![08-08](../../OEBPS/Images/08-08.png)'
- en: Figure 8.8 General structure and organization of a fully convolutional network
    that uses a pyramidal aggregation module
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8 全卷积网络使用金字塔聚合模块的一般结构和组织方式
- en: The best way to understand the importance of the pyramidal aggregation module
    is to see what happens if we don’t have it. If that is the case, then the last
    convolutional layer will have the enormous and unrealistic responsibility of building
    the final segmentation mask (which is typically 16-32x times larger than the layer
    output). It is no surprise that there is a massive representational bottleneck
    between the final convolution layer and the final segmentation mask, leading to
    poor performance. The pyramidal structure typically enforced in CNNs results in
    a very small output width and height in the final layer.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 了解金字塔聚合模块的重要性的最佳方式是看看如果没有它会发生什么。如果是这种情况，那么最后一个卷积层将承担建立最终分割掩码（通常是最后一层输出的16-32倍大）的巨大且不切实际的责任。毫不奇怪，在最终卷积层和最终分割掩码之间存在巨大的表征瓶颈，从而导致性能不佳。在卷积神经网络中通常强制执行的金字塔结构导致最后一层的输出宽度和高度非常小。
- en: The pyramidal aggregation module bridges this gap. It does so by combining several
    different interim outputs. This way, the network has ample fine-grained (from
    earlier layers) and coarser (from deeper layers) details to construct the desired
    segmentation mask. Fine-grained representations provide spatial/contextual information
    about the image, whereas the coarser representations provide high-level information
    about the image (e.g., what objects are present). By fusing both types of these
    representations, the task of generating the final output becomes more achievable.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 金字塔聚合模块弥合了这一差距。 它通过组合几个不同的中间输出来做到这一点。 这样，网络就有了充足的细粒度（来自较早层）和粗糙的（来自更深层）细节，以构建所需的分割掩模。
    细粒度的表示提供了关于图像的空间/上下文信息，而粗糙的表示提供了关于图像的高级信息（例如，存在哪些对象）。 通过融合这两种类型的表示，生成最终输出的任务变得更加可行。
- en: Why not a skyscraper instead of a pyramid?
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么不是金字塔而是摩天大楼呢？
- en: You might be tempted to ponder, if making the outputs smaller as you go causes
    loss of information, “Why not keep it the same size?” (hence the term *skyscraper*).
    This is an impractical solution for two main reasons.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，如果随着时间的推移使输出变小会导致信息的丢失，“为什么不保持相同的大小呢？”（因此有了*摩天大楼*这个术语）。 这是一个不切实际的解决方案，主要有两个原因。
- en: First, decreasing the size of the outputs through pooling or striding is an
    important regularization method that forces the network to learn translation-invariant
    features (as we discussed in chapter 6). By taking this away, we can hinder the
    generalizability of the network.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，通过池化或步幅减小输出大小是一种重要的正则化方法，它迫使网络学习平移不变特征（正如我们在第6章讨论的那样）。 如果去掉这个过程，我们就会阻碍网络的泛化能力。
- en: Second, not decreasing the output size will increase the memory footprint of
    the model significantly. This will, in turn, restrict the depth of the network
    dramatically, making it more difficult to create deeper networks.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，不减小输出大小将显著增加模型的内存占用。 这反过来会极大地限制网络的深度，使得创建更深层次的网络更加困难。
- en: DeepLab v3 is the golden child of a lineage of models that emerged from and
    was introduced in the paper “Rethinking Atrous Convolution for Semantic Image
    Segmentation” ([https://arxiv.org/pdf/1706.05587.pdf](https://arxiv.org/pdf/1706.05587.pdf))
    by several researchers from Google.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: DeepLab v3是一系列模型的金童，这些模型起源于并且是由来自谷歌的几位研究人员在论文“重新思考用于语义图像分割的空洞卷积”（[https://arxiv.org/pdf/1706.05587.pdf](https://arxiv.org/pdf/1706.05587.pdf)）中提出的。
- en: Most segmentation models face an adverse side effect caused by a common and
    beneficial design principle. Vision models incorporate stride/pooling to make
    network translation invariant. But an ill-favored outcome of that is the compounding
    reduction of the size of the outputs produced. This typically leads to a final
    output that is 16-32 times smaller than the input. Being a dense prediction task,
    image segmentation tasks suffer heavily from this design idea. Therefore, most
    of the groundbreaking networks that have surfaced have been about solving this.
    The DeepLab model came into the world for exactly that purpose. Let’s now see
    how DeepLab v3 solves this problem.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数分割模型都面临着由常见且有益的设计原则引起的不利副作用。 视觉模型将步幅/池化结合起来，使网络平移不变。 但这个设计思想的一个不受欢迎的结果是输出大小的不断减小。
    这通常导致最终输出比输入小16-32倍。 作为密集预测任务，图像分割任务严重受到这种设计思想的影响。 因此，大多数涌现出来的具有突破性的网络都是为了解决这个问题。
    DeepLab模型就是为了解决这个问题而诞生的。 现在让我们看看DeepLab v3是如何解决这个问题的。
- en: DeepLab v3 uses a ResNet-50 ([https://arxiv.org/pdf/1512.03385.pdf](https://arxiv.org/pdf/1512.03385.pdf))
    pretrained on an ImageNet image classification data set as its backbone for extracting
    features of an image. It is one of the pioneering residual networks that made
    waves in the computer vision community a few years ago. DeepLab v3 introduces
    several architectural changes to the model to alleviate this issue. Furthermore,
    DeepLab v3 introduces a shiny new component called *atrous spatial pyramid pooling*
    (ASPP). We will discuss each of these in more detail in the coming sections.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: DeepLab v3使用在ImageNet图像分类数据集上预训练的ResNet-50（[https://arxiv.org/pdf/1512.03385.pdf](https://arxiv.org/pdf/1512.03385.pdf)）作为提取图像特征的主干。
    几年前，它是计算机视觉社区中引起轰动的开创性残差网络之一。 DeepLab v3对模型进行了几个架构上的改变，以缓解这个问题。 此外，DeepLab v3引入了一个全新的组件，称为*空洞空间金字塔池化*（ASPP）。
    我们将在接下来的章节中更详细地讨论每个组件。
- en: 8.3.1 A quick overview of the ResNet-50 model
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.1 ResNet-50模型的快速概述
- en: The ResNet-50 model consists of several convolution blocks, followed by a global
    average pooling layer and a fully connected final prediction layer with softmax
    activation. The convolution block is the innovative part of the model. The original
    model has 16 convolution blocks organized into five groups. A single block consists
    of three convolution layers (1 × 1 convolution layer with stride 2, 3 × 3 convolution
    layer, and 1 × 1 convolution layer), batch normalization, and residual connections.
    We discussed residual connections in depth in chapter 7\. Next, we will discuss
    a core computation used throughout the model known as atrous convolution.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet-50 模型由多个卷积块组成，后跟一个全局平均池化层和一个具有 softmax 激活的完全连接的最终预测层。卷积块是模型的创新部分。原始模型有
    16 个卷积块，组织成五个组。一个单独的块由三个卷积层组成（1 × 1 卷积层，步长为 2，3 × 3 卷积层，1 × 1 卷积层），批量归一化和残差连接。我们在第
    7 章深入讨论了残差连接。接下来，我们将讨论模型中始终使用的核心计算，称为孔卷积。
- en: '8.3.2 Atrous convolution: Increasing the receptive field of convolution layers
    with holes'
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.2 孔卷积：用孔扩大卷积层的感受野
- en: Compared to the standard ResNet-50, a major change that DeepLab v3 boasts is
    the use of atrous convolutions. Atrous (meaning “holes” in French) convolution,
    also known as dilated convolution, is a variant of the standard convolution. Atrous
    convolution works by inserting “holes” in between the convolution parameters.
    The increase in the receptive field is controlled by a parameter called *dilation
    rate*. A higher dilation rate means more holes between actual parameters in the
    convolution. A major benefit of atrous convolution is the ability to increase
    the size of the receptive field without compromising the parameter efficiency
    of a convolution layer.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 与标准 ResNet-50 相比，DeepLab v3 骄傲地使用孔卷积的主要变化。孔（法语意为“孔”）卷积，也称为扩张卷积，是标准卷积的变体。孔卷积通过在卷积参数之间插入“孔”来工作。感受野的增加由一个称为
    *扩张率* 的参数控制。更高的扩张率意味着卷积中实际参数之间有更多的孔。孔卷积的一个主要好处是能够增加感受野的大小，而不会损害卷积层的参数效率。
- en: '![08-09](../../OEBPS/Images/08-09.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![08-09](../../OEBPS/Images/08-09.png)'
- en: Figure 8.9 Atrous convolution compared to standard convolution. Standard convolution
    is a special case of atrous convolution, where the rate is 1\. As you increase
    the dilation rate, the receptive field of the layer increases.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.9 孔卷积与标准卷积的比较。标准卷积是孔卷积的特例，其中速率为 1。随着扩张率的增加，层的感受野也会增加。
- en: Figure 8.9 shows how a large dilation rate leads to a larger receptive field.
    The number of shaded gray boxes represents the number of parameters, whereas the
    dashed, lightly shaded box represents the size of the receptive field. As you
    can see, the number of parameters stays constant, while the receptive field increases.
    Computationally, it is quite straightforward to extend standard convolution to
    atrous convolution. All you need to do is insert zeros for the holes in the atrous
    convolution operation.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.9 显示了较大的扩张率导致更大的感受野。阴影灰色框的数量表示参数数量，而虚线，轻微阴影的框表示感受野的大小。正如你所见，参数数量保持不变，而感受野增加。从计算上讲，将标准卷积扩展到孔卷积非常简单。你所需要做的就是在孔卷积操作中插入零。
- en: Wait! How does atrous convolution help segmentation models?
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 等等！孔卷积如何帮助分割模型？
- en: As we discussed, the main issue presented by the pyramidal structure of CNNs
    is that the output gets gradually smaller. The easiest solution, leaving the learned
    parameters untouched, is to reduce the stride of the layers. Though technically
    that will increase output size, conceptually there is a problem.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们讨论的那样，CNN 的金字塔结构提出的主要问题是输出逐渐变小。最简单的解决方案，不改变学习的参数，是减小层的步幅。尽管技术上会增加输出大小，但在概念上存在问题。
- en: To understand it, assume the i^(th) layer of a CNN has a stride of 2 and gets
    a h × w-sized input. Then the i+1^(th) layer gets a h/2 × w/2-sized input. By
    removing the stride of the i^(th) layer, it gets a h × w-sized output. However,
    the kernel of the i+1^(th) layer has been trained to see a smaller output, so
    by increasing the size of the input, we are disrupting (or reducing) the receptive
    field of the layer. By introducing atrous convolution, we compensate for that
    reduction of the receptive field.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解这一点，假设 CNN 的第 i 层的步长为 2，并且获得了 h × w 大小的输入。然后，第 i+1 层获得了 h/2 × w/2 大小的输入。通过移除第
    i 层的步长，它获得了 h × w 大小的输出。然而，第 i+1 层的核已经被训练成看到一个更小的输出，所以通过增加输入的大小，我们破坏了（或减少了）层的感受野。通过引入空洞卷积，我们补偿了该感受野的减小。
- en: Let’s now see how the ResNet-50 is repurposed for image segmentation. First,
    we download it from the tf.keras.applications module. The architecture of the
    ResNet-50 model has the following format. To start, it has a stride 2 convolution
    layer and a stride 2 pooling layer. After that, it has sequence of convolution
    blocks and finally an average pooling layer and fully connected output layer.
    These convolution blocks have a hierarchical organization of convolution layers.
    Each convolution block consists of several subblocks, which consist of three convolution
    layers (i.e., a 1 × 1 convolution, a 3 × 3 convolution, and a 1 × 1 convolution)
    along with batch normalization.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看 ResNet-50 如何被重新用于图像分割。首先，我们从`tf.keras.applications`模块下载它。ResNet-50 模型的架构如下所示。首先，它有一个步幅为
    2 的卷积层和一个步幅为 2 的池化层。之后，它有一系列卷积块，最后是一个平均池化层和完全连接的输出层。这些卷积块具有卷积层的分层组织。每个卷积块由几个子块组成，每个子块由三个卷积层组成（即
    1 × 1 卷积、3 × 3 卷积和 1 × 1 卷积），以及批量归一化。
- en: 8.3.3 Implementing DeepLab v3 using the Keras functional API
  id: totrans-296
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Keras 函数 API 实现 DeepLab v3
- en: 'The network starting from the input up to the conv4 block remains unchanged.
    Following the notation from the original ResNet paper, these blocks are identified
    as conv2, conv3, and conv4 block groups. Our first task is to create a model containing
    the input layer up to the conv4 block of the original ResNet-50 model. After that,
    we will focus on recreating the final convolution block (i.e., conv5) as per the
    DeepLab v3 paper:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 从输入开始直到`conv4`块的网络保持不变。根据原始 ResNet 论文的符号，这些块被标识为`conv2`、`conv3`和`conv4`块组。我们的第一个任务是创建一个包含输入层到原始
    ResNet-50 模型的`conv4`块的模型。之后，我们将专注于根据 DeepLab v3 论文重新创建最终卷积块（即`conv5`）：
- en: '[PRE32]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: As shown here, we find the last layer in the ResNet-50 model just before the
    "conv5_ block1_1_conv", which would be the last layer of the conv4 block group.
    With that, we can define a makeshift model that contains layers from the input
    to the final output of the conv4 block group. Later, we will focus on augmenting
    this model by introducing modifications and novel components from the paper. We
    will redefine the conv5 block with dilated convolutions. To do this, we need to
    understand the composition of a ResNet block (figure 8.10). We can assume it has
    three different levels.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，我们找到了 ResNet-50 模型中位于“conv5_block1_1_conv”之前的最后一层，这将是`conv4`块组的最后一层。有了这个，我们可以定义一个临时模型，该模型包含从输入到`conv4`块组的最终输出的层。后来，我们将专注于通过引入论文中的修改和新组件来增强这个模型。我们将使用扩张卷积重新定义`conv5`块。为此，我们需要了解
    ResNet 块的构成（图 8.10）。我们可以假设它有三个不同的级别。
- en: '![08-10](../../OEBPS/Images/08-10.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![08-10](../../OEBPS/Images/08-10.png)'
- en: Figure 8.10 Anatomy of a convolution block in ResNet-50\. For this example,
    we show the very first convolution block of ResNet-50\. The organization of a
    convolution block group consists of three different levels.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.10 ResNet-50 中卷积块的解剖。对于这个示例，我们展示了 ResNet-50 的第一个卷积块。卷积块组的组织包括三个不同的级别。
- en: Let’s now implement a function to represent each level while using dilated convolution.
    In order to convert a standard convolution layer to a dilated convolution, we
    just have to pass in the desired rate to the dilation_rate parameter in the tf.keras.layers.Conv2D
    layer. First, we will implement a function that represents a level 3 block, as
    shown in the following listing.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们实现一个函数来表示使用扩张卷积时的每个级别。为了将标准卷积层转换为扩张卷积，我们只需将所需的速率传递给`tf.keras.layers.Conv2D`层的`dilation_rate`参数即可。首先，我们将实现一个表示级别
    3 块的函数，如下清单所示。
- en: Listing 8.8 A level 3 convolution block in ResNet-50
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 8.8 ResNet-50 中的级别 3 卷积块
- en: '[PRE33]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: ❶ Here, inp takes a 4D input having shape [batch size, height, width, channels].
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Perform 2D convolution on the input with a given number of filters, kernel_size,
    and dilation rate.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Perform batch normalization on the output of the convolution layer.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Apply ReLU activation if activation is set to True.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Return the output without an activation if activation is set to False.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: A level 3 block has a single convolution layer with a desired dilation rate
    and a batch normalization layer followed by a nonlinear ReLU activation layer.
    Next, we will write a function for the level 2 block (see the next listing).
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.9 A level 2 convolution block in ResNet-50
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'A level 2 block consists of three level 3 blocks with a given dilation rate
    that have convolution layers with the following specifications:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: 1 × 1 convolution layer having 512 filters and a desired dilation rate
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3 × 3 convolution layer having 512 filters and a desired dilation rate
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 × 1 convolution layer having 2048 filters and a desired dilation rate
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apart from using atrous convolution, this is identical to a level 2 block of
    the original conv5 block in the ResNet-50 model. With all the building blocks
    ready, we can implement the fully fledged conv5 block with atrous convolution
    (see the next listing).
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.10 Implementing the final ResNet-50 convolution block group (level
    1)
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: ❶ Create a level 3 block (block0) to create residual connections for the first
    block.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Define the first level 2 block, which has a dilation rate of 2 (block1).
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Create a residual connection from block0 to block1.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Apply ReLU activation to the result.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: ❺ The second level 2 block with a dilation rate of 2 (block2)
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Create a residual connection from block1 to block2.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Apply ReLU activation.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Apply a similar procedure to block1 and block2 to create block3.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: 'There’s no black magic here. The function resnet_block lays the outputs of
    the functions we already discussed to assemble the final convolution block. Particularly,
    it has three level 2 blocks with residual connections going from the previous
    block to the next. Finally, we can get the final output of the conv5 block with
    a dilation rate of 2 by calling the resnet_block function with the output of the
    interim model (resnet50_ upto_conv4) we defined as the input and a dilation rate
    of 2:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 8.3.4 Implementing the atrous spatial pyramid pooling module
  id: totrans-330
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here, we will discuss the most exciting innovation of the DeepLab v3 model.
    The atrous spatial pyramid pooling (ASPP) module serves two purposes:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: Aggregates multiscale information about an image, obtained through outputs produced
    using different dilation rates
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combines highly summarized information obtained through global average pooling
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ASPP module gathers multiscale information by performing different convolutions
    on the last ResNet-50 output. Specifically, the ASPP module performs 1 × 1 convolution,
    3 × 3 convolution (r = 6), 3 × 3 convolution (r = 12), and 3 × 3 convolution (r
    = 18), where r is the dilation rate. All of these convolutions have 256 output
    channels and are implemented as level 3 blocks (provided by the function block_level3()).
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: ASRP captures high-level information by performing global average pooling, followed
    by a 1 × 1 convolution with 256 output channels to match the output size of multiscale
    outputs, and finally a bilinear up-sampling layer to up-sample the height and
    width dimensions shrunk by the global average pooling. Remember that bilinear
    interpolation up-samples the images by computing the resulting pixels as an average
    of neighboring pixels. Figure 8.11 illustrates the ASPP module.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '![08-11](../../OEBPS/Images/08-11.png)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
- en: Figure 8.11 The ASPP module used in the DeepLab v3 model
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: The job of the ASPP module can be summarized as a concise function. We have
    all the tools we need to implement this function from the previous work we have
    done (see the following listing).
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.11 Implementing ASPP
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: ❶ Define a 1 × 1 convolution.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Define a 3 x 3 convolution with 256 filters and a dilation rate of 6.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Define a 3 x 3 convolution with 256 filters and a dilation rate of 12.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Define a 3 x 3 convolution with 256 filters and a dilation rate of 18.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Define a global average pooling layer.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Define a 1 × 1 convolution with 256 filters.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Up-sample the output using bilinear interpolation.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Concatenate all the outputs.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: ❾ Create an instance of ASPP.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: 'The ASPP module consists of four level 3 blocks, as outlined in the code. The
    first block comprises a 1 × 1 convolution with 256 filters without dilation (this
    produces outa_1_conv). The latter three blocks consist of 3 × 3 convolutions with
    256 filters but with varying dilation rates (i.e., 6, 12, 18; they produce outa_2_conv,
    outa_3_conv, and outa_4_conv, respectively). This covers aggregating features
    from the image at multiple scales. However, we also need to preserve the global
    information about the image, similar to a global average pooling layer (outb_1_avg).
    This is achieved through a lambda layer that averages the input over the height
    and width dimensions:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The output of the averaging is then followed by a 1 × 1 convolution filter
    with 256 filters. Then, to bring the output to the same size as previous outputs,
    an up-sampling layer that uses bilinear interpolation is used (this produces outb_1_up):'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Finally, all these outputs are concatenated to a single output using a Concatenate
    layer to produce the final output out_aspp.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.5 Putting everything together
  id: totrans-355
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now it’s time to collate all the different components to create one majestic
    segmentation model. The next listing outlines the steps required to build the
    final model.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.12 The final DeepLab v3 model
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: ❶ Define the RGB input layer.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Download and define the resnet50.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 下载并定义 ResNet50。
- en: ❸ Get the output of the last layer we’re interested in.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 获取我们感兴趣的最后一层的输出。
- en: ❹ Define an interim model from the input up to the last layer of the conv4 block.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 从输入定义一个中间模型，到 conv4 块的最后一层。
- en: ❺ Define the removed conv5 resnet block.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 定义删除的 conv5 ResNet 块。
- en: ❻ Define the ASPP module.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 定义 ASPP 模块。
- en: ❼ Define the final output.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 定义最终输出。
- en: ❽ Define the final model.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 定义最终模型。
- en: Note how the model has a linear layer that does not have any activation present
    (e.g., sigmoid or softmax). This is because we are planning to use a special loss
    function that uses logits (unnormalized scores obtained from the last layer before
    applying softmax) instead of normalized probability scores. Due to that, we will
    keep the last layer a linear output with no activation.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 注意观察模型中的线性层，它没有任何激活函数（例如 sigmoid 或 softmax）。这是因为我们计划使用一种特殊的损失函数，该函数使用 logits（在应用
    softmax 之前从最后一层获得的未归一化分数）而不是归一化的概率分数。因此，我们将保持最后一层为线性输出，没有激活函数。
- en: 'We have one final housekeeping step to perform: copying the weights from the
    original conv5 block to the newly created conv5 block in our model. To do that,
    first we need to store the weights from the original model as follows:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要执行最后一步操作：将原始 conv5 块的权重复制到我们的模型中新创建的 conv5 块。为此，首先需要将原始模型的权重存储如下：
- en: '[PRE41]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: We cannot copy the weights to the new model until we compile the model, as weights
    are not initialized until the model is compiled. Before we do that, we need to
    learn loss functions and evaluation metrics that are used in segmentation tasks.
    To do that, we will need to implement custom loss functions and metrics and use
    them to compile the model. This will be discussed in the next section.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 在编译模型之前，我们无法将权重复制到新模型中，因为在编译模型之前权重不会被初始化。在这之前，我们需要学习在分割任务中使用的损失函数和评估指标。为此，我们需要实现自定义损失函数和指标，并使用它们编译模型。这将在下一节中讨论。
- en: Exercise 3
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 3
- en: 'You want to create a new pyramidal aggregation module called aug-ASPP. The
    idea is similar to the ASPP module we implemented earlier, but with a few differences.
    Let’s say you have been given two interim outputs from the model: out_1 and out_2
    (same size). You have to write a function, aug_aspp, that will take these two
    outputs and do the following:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 您想要创建一个新的金字塔聚合模块称为 aug-ASPP。这个想法与我们之前实现的 ASPP 模块类似，但有一些区别。假设您已经从模型中获得了两个中间输出：out_1
    和 out_2（大小相同）。您必须编写一个函数，aug_aspp，它将获取这两个输出并执行以下操作：
- en: Perform atrous convolution with r = 16, 128 filters, 3 × 3 convolution, stride
    1, and ReLU activation on out_1 (output will be called atrous_out_1)
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对 out_1 进行 atrous 卷积，r=16，128 个过滤器，3×3 卷积，步幅为 1，并应用 ReLU 激活函数（输出将被称为 atrous_out_1）
- en: Perform atrous convolution with r = 8, 128 filters, 3 × 3 convolution, stride
    1, and ReLU activation on both out_1 and out_2 (output will be called atrous_
    out_2_1 and atrous_out_2_2)
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对 out_1和 out_2 进行 atrous 卷积，r=8，128 个过滤器，3×3 卷积，步幅为 1，并对两者应用 ReLU 激活函数（输出将被称为
    atrous_out_2_1 和 atrous_out_2_2）
- en: Concatenate atrous_out_2_1 and atrous_out_2_2 (output will be called atrous_out_2)
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拼接 atrous_out_2_1 和 atrous_out_2_2（输出将被称为 atrous_out_2）
- en: Apply 1 × 1 convolution with 64 filters to both atrous_out_1 and atrous_out_2
    and concatenate (output will be called conv_out)
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对 atrous_out_1 和 atrous_out_2 进行 1×1 卷积，使用 64 个过滤器并进行拼接（输出将被称为 conv_out）
- en: Use bilinear up-sampling to double the size of conv_out (on height and width
    dimensions) and apply sigmoid activation
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用双线性上采样将 conv_out 的大小加倍（在高度和宽度尺寸上），并应用 sigmoid 激活函数
- en: '8.4 Compiling the model: Loss functions and evaluation metrics in image segmentation'
  id: totrans-378
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.4编译模型：图像分割中的损失函数和评估指标
- en: 'In order to finalize the DeepLab v3 model (built using mostly the ResNet-50
    structure and the ASPP module), we have to define a suitable loss function and
    metrics to measure the performance of the model. Image segmentation is quite different
    from image classification tasks, so the loss function and metrics don’t necessarily
    translate to the segmentation problem. One key difference is that there is typically
    a large class imbalance in segmentation data, as a “background” class typically
    dominates an image compared to other object-related pixels. To get started, you
    read a few blog posts and research papers and identify weighted categorical cross-entropy
    loss and dice loss as good candidates. You focus on three different metrics: pixel
    accuracy, mean (class-weighted) accuracy, and mean IoU.'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成DeepLab v3模型的最终构建（主要采用ResNet-50结构和ASPP模块），我们必须定义适当的损失函数和度量来衡量模型的性能。图像分割与图像分类任务有很大的不同，因此损失函数和度量不一定适用于分割问题。一个关键的区别是，在分割数据中通常存在较大的类别不平衡，因为与其他与对象相关的像素相比，“背景”类通常占据了图像的主导地位。为了开始，您阅读了几篇博客文章和研究论文，并将加权分类交叉熵损失和Dice损失视为很好的候选项。您专注于三个不同的度量：像素精度，平均（类别加权）精度和平均IoU。
- en: Loss functions and evaluation metrics used in image segmentation models are
    different from what is used in image classifiers. To start, image classifiers
    take in a single class label for a single image, whereas a segmentation model
    predicts a class for every single pixel in the image. This highlights the necessity
    of not only reimagining existing loss functions and metrics, but also inventing
    new losses and evaluation metrics that are more appropriate for the output produced
    by segmentation models. We will first discuss loss functions and then metrics.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分割模型中使用的损失函数和评估指标与图像分类器中使用的不同。首先，图像分类器接受单个图像的单个类标签，而分割模型预测图像中每个单个像素的类别。这凸显了不仅需要重新构想现有的损失函数和评估指标，而且需要发明适用于分割模型产生的输出的新的损失和评估指标。我们首先讨论损失函数，然后是指标。
- en: 8.4.1 Loss functions
  id: totrans-381
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.1 损失函数
- en: 'A *loss function* is what is used to optimize the model whose purpose is to
    find the parameters that minimize a defined loss. A loss function used in a deep
    network must be *differentiable*, as the minimization of the loss happens with
    the help of gradients. The loss functions we’ll use comprise two loss functions:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '*损失函数*是用于优化其目的是找到最小化定义的损失的参数的模型的。深度网络中使用的损失函数必须是可微分的，因为损失的最小化是通过梯度进行的。我们将使用的损失函数包含两个损失函数：'
- en: Cross-entropy loss
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交叉熵损失
- en: Dice loss
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dice损失
- en: Cross-entropy loss
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵损失
- en: Cross-entropy loss is one of the most common losses used in segmentation tasks
    and can be implemented with just one line in Keras. We already used cross-entropy
    loss quite a few times but didn’t analyze it in detail. However, it is worthwhile
    to review the underpinning mechanics that govern cross-entropy loss.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵损失是分割任务中最常用的损失之一，可以在Keras中仅用一行代码实现。我们已经使用了交叉熵损失很多次，但没有详细分析它。然而，回顾支配交叉熵损失的基础机制是值得的。
- en: Cross-entropy loss takes in a predicted target and a true target. Both these
    tensors are of shape [batch size, height, width, object classes]. The object class
    dimension is a one-hot encoded representation of which object class a given pixel
    belongs to. The cross-entropy loss is then computed for every pixel independently
    using
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 对于交叉熵损失函数，需要输入预测目标和真实目标。这两个张量都具有[batch size, height, width, object classes]的形状。对象类维度是给定像素属于哪个对象类别的一种独热编码表示。然后，对每个像素独立地计算交叉熵损失。
- en: '![08_11a](../../OEBPS/Images/08_11a.png)'
  id: totrans-388
  prefs: []
  type: TYPE_IMG
  zh: 8_11a
- en: where *CE*(*i, j*) represents the cross-entropy loss for pixel at position (*i,
    j*) on the image, *c* is the number of classes, and *y*[k] and *ŷ*[k] represent
    the elements in the one-hot encoded vector and the predicted probability distribution
    over classes of that pixel. This is then summed across all the pixels to get the
    final loss.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*CE*（*i, j*）表示图像位置（*i, j*）处像素的交叉熵损失，*c*是类别数，*y*[k]和*ŷ*[k]分别表示该像素的独热编码向量中元素和预测概率分布的元素。然后在所有像素上求和以获得最终损失。
- en: Beneath the simplicity of the method, a critical issue lurks. Class imbalance
    is almost certain to rear its ugly head in image segmentation problems. You will
    find hardly any real-world images where each object occupies an equal area in
    the image. The good news is it is not very difficult to deal with this issue.
    This can be mitigated by assigning a weight for each pixel in the image, depending
    on the dominance of the class it represents. Pixels belonging to large objects
    will have smaller weights, whereas pixels belonging to smaller objects will have
    larger weights, providing an equal say despite the size in the final loss. The
    next listing shows how to do this in TensorFlow.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.13 Computing the label weights for a given batch of data
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: ❶ Get the total pixels per-class in y_true.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Get the total pixels in y_true.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Compute the weights per-class. Rarer classes get more weight.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Reshape y_true to a [batch size, height*width]-sized tensor.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Create a weight vector by gathering the weights corresponding to indices in
    y_true.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Make y_weights a vector.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: Here, for a given batch, we compute the weights as a sequence/vector that has
    a number of elements equal to y_true. First, we get the total number of pixels
    for each class by computing the sum over the width and height of the one-hot encoded
    y_true (i.e., has dimensions batch, height, width, and class). Here, a class that
    has a value larger than num_classes will be ignored. Next, we compute the total
    number of pixels per sample by taking the sum over the class dimension resulting
    in *tot* (a [batch size, 1]-sized tensor). Now the weights can be computed per
    sample and per class using
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: '![08_11b](../../OEBPS/Images/08_11b.png)'
  id: totrans-400
  prefs: []
  type: TYPE_IMG
- en: 'where *n* is the total number of pixels and *n*^i is the total number of pixels
    belonging to the *i*^(th) class. After that, we reshape y_true to shape [batch
    size, -1] as preparation for an important step in weight computation. As the final
    output, we want to create a tensor out of weights, where we gather elements from
    the y_weights that correspond to elements in y_true. In other words, we fetch
    the value from y_weights, where the index to fetch is given by the values in y_true.
    At the end, the result will be of the same shape and size as y_true. This is all
    we need to weigh the samples: multiply weights element-wise with the loss value
    for each pixel. To achieve this, we will use the function tf.gather(), which gathers
    the elements from a given tensor (params) while taking a tensor that represents
    indices (indices) and returns a tensor that is of the same shape as the indices:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Here, to ignore the batch dimension during performing the gather, we pass the
    argument batch_dims indicating how many batch dimensions we have. With that, we
    will define a function that outputs the weighted cross-entropy loss given a batch
    of predicted and true targets.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: With the weights ready, we can now implement our first segmentation loss function.
    We will implement weighted cross-entropy loss. At a glance, the function masks
    irrelevant pixels (e.g., pixels belonging to unknown objects) and unwraps the
    predicted and true labels to get rid of the height and width dimensions. Finally,
    we can compute the cross-entropy loss using the built-in function in TensorFlow
    (see the next listing).
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 现在权重准备好了，我们可以实现第一个分割损失函数。我们将实现加权交叉熵损失。一眼看去，该函数会屏蔽不相关的像素（例如属于未知对象的像素），并展开预测标签和真实标签以消除高度和宽度维度。最后，我们可以使用
    TensorFlow 中的内置函数计算交叉熵损失（请参见下一个清单）。
- en: Listing 8.14 Implementing the weighted cross-entropy loss
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 8.14 实现加权交叉熵损失
- en: '[PRE44]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: ❶ Define the valid mask, masking unnecessary pixels.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义有效掩码，用于屏蔽不必要的像素。
- en: ❷ Some initial setup that casts y_true to int and sets the shape
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 对 y_true 进行了一些初步设置，将其转换为 int 并设置形状。
- en: ❸ Get the label weights.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 获取标签权重。
- en: ❹ Unwrap y_pred and y_true so that batch, height, and width dimensions are squashed.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 展开 y_pred 和 y_true，以消除批处理、高度和宽度维度。
- en: ❺ Compute the cross-entropy loss with y_true, y_pred, and the mask.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用 y_true、y_pred 和掩码计算交叉熵损失。
- en: ❻ Return the function that computes the loss.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 返回计算损失的函数。
- en: You might be thinking, “Why is the loss defined as a nested function?” This
    is a standard pattern we have to follow if we need to include extra arguments
    to our loss function (i.e., num_classes). All we are doing is capturing the computations
    of the loss function in the loss_fn function and then creating an outer function
    ce_weighted_from_logits() that will return the function that encapsulates the
    loss computations (i.e., loss_fn).
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，“为什么将损失定义为嵌套函数？”如果我们需要向损失函数中包含额外的参数（例如 num_classes），则必须遵循此标准模式。我们正在将损失函数的计算捕获在
    loss_fn 函数中，然后创建一个外部函数 ce_weighted_from_logits()，该函数将返回封装损失计算（即 loss_fn）的函数。
- en: 'Specifically, a valid mask is created to indicate whether the labels in y_true
    are less than the number of classes. Any label that has a value larger than the
    number of classes is ignored (e.g., unknown objects). Next, we get the weight
    vector and indicate a weight for each pixel using the get_label_weights() function.
    We will unwrap y_pred to a [-1, num_classes]-sized tensor, as y_pred contains
    *logits* (i.e., unnormalized probability scores output by the model) across all
    classes in the data set. y_true will be unwrapped to a vector (i.e., a single
    dimension), as y_true only contains the class label. Finally, we use tf.nn.sparse_softmax_cross_entropy_with_logits()
    to compute the loss over masked predicted and true targets. The function takes
    two arguments, labels and logits, which are self-explanatory. We can make two
    salient observations:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 具体地，创建有效掩码以指示 y_true 中的标签是否小于类数。任何值大于类数的标签都会被忽略（例如未知对象）。接下来，我们获取权重向量，并使用 get_label_weights()
    函数为每个像素指定权重。我们将 y_pred 展开成 [-1, num_classes] 大小的张量，因为 y_pred 包含数据集中所有类别的 logits（即模型输出的未归一化概率分数）。y_true
    将展开为一个向量（也就是单维），因为 y_true 只包含类别标签。最后，我们使用 tf.nn.sparse_softmax_cross_entropy_with_logits()
    来计算掩码预测和真实目标的损失。该函数有两个参数，标签和 logits，很容易理解。我们可以得出两个重要的观察结果：
- en: We are computing sparse cross-entropy loss (i.e., not standard cross-entropy
    loss).
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们正在计算稀疏交叉熵损失（而不是标准交叉熵损失）。
- en: We are computing cross-entropy loss from logits.
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们从 logits 中计算交叉熵损失。
- en: When using sparse cross entropy, we don’t have to one-hot encode the labels,
    so we can skip this, which leads to a more memory-efficient data pipeline. This
    is because one-hot encoding is handled internally by the model. By using a sparse
    loss, we have less to worry about.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用稀疏交叉熵时，我们不需要对标签进行独热编码，因此可以跳过此步骤，这会导致数据管道更具内存效率。这是因为独热编码在模型内部处理。通过使用稀疏损失，我们需要担心的东西就更少了。
- en: Computing the loss from logits (i.e., unnormalized scores) instead of from normalized
    probabilities leads to better and more stable gradients. Therefore, whenever possible,
    make sure to use logits instead of normalized probabilities.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 从 logits（即未归一化分数）而不是从归一化概率计算损失会导致更好、更稳定的渐变。因此，尽可能地使用 logits 而不是归一化概率。
- en: Dice loss
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: Dice 损失
- en: The second loss we will discuss is called the *dice loss*, which is computed
    as
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论的第二种损失函数称为 Dice 损失，其计算方式如下：
- en: '![08_11c](../../OEBPS/Images/08_11c.png)'
  id: totrans-421
  prefs: []
  type: TYPE_IMG
  zh: '![08_11c](../../OEBPS/Images/08_11c.png)'
- en: 'Here, the intersection between the prediction and target tensors can be computed
    with element-wise multiplication, whereas the union can be computed using element-wise
    addition between the prediction and the target tensors. You might be thinking
    that using element-wise operations is a strange way to compute intersection and
    union. To understand the reason behind this, I want to refer to a statement made
    earlier: a loss function used in a deep network must be *differentiable*.'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，可以通过逐元素乘法计算预测和目标张量之间的交集，而可以通过逐元素加法计算预测和目标张量之间的并集。你可能会觉得使用逐元素操作来计算交集和并集是一种奇怪的方式。为了理解背后的原因，我想引用之前提到过的一句话：深度网络中使用的损失函数必须是*可微分的*。
- en: This means that we cannot use the standard conventions we use to compute intersection
    and union from a given set of values. Rather, we need to resort to a differentiable
    computation, leading to intersection and union between two tensors. Intersection
    can be computed by taking element-wise multiplication between the predicted and
    true targets. Union can be computed by taking the element-wise addition between
    the predicted and true targets. Figure 8.12 clarifies how these operations lead
    to intersection and union between two tensors.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们不能使用我们通常用来计算交集和并集的标准方法。相反，我们需要采用可微分计算的方法，得到两个张量之间的交集和并集。交集可以通过预测值和真实目标之间的逐元素乘法来计算。并集可以通过预测值和真实目标之间的逐元素加法来计算。图
    8.12 阐明了这些操作如何导致两个张量之间的交集和并集。
- en: '![08-12](../../OEBPS/Images/08-12.png)'
  id: totrans-424
  prefs: []
  type: TYPE_IMG
  zh: '![08-12](../../OEBPS/Images/08-12.png)'
- en: Figure 8.12 Computations involved in dice loss. The intersection can be computed
    as a differentiable function by taking element-wise multiplication, whereas union
    can be computed as the element-wise sum.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.12 显示了 dice 损失中涉及的计算。交集可以通过逐元素乘法计算为一个可微分函数，而并集可以通过逐元素求和计算。
- en: This loss is predominantly focused on maximizing the intersection between the
    predicted and true targets. The multiplier of 2 is used to balance out the duplication
    of values that comes from the overlap between the intersection and the union,
    found in the denominator (see the following listing).
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 这个损失函数主要关注最大化预测值和真实目标之间的交集。乘数 2 的使用是为了平衡来自交集和并集之间重叠的值的重复，其出现在分母中（参见下面的列表）。
- en: Listing 8.15 Implementing the dice loss
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.15 实现 dice 损失
- en: '[PRE45]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: ❶ Initial setup for y_true
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ y_true 的初始设置
- en: ❷ Get the label weights and reshape it to a [-1, 1] shape.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 获取标签权重并将其重塑为 [-1, 1] 的形状。
- en: ❸ Apply softmax on y_pred to get normalized probabilities.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 对 y_pred 应用 softmax 函数以得到归一化概率。
- en: ❹ Unwrap y_pred and one-hot-encoded y_true to the [-1, num_classes] shape.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将 y_pred 和 one-hot 编码的 y_true 展开为 [-1, num_classes] 的形状。
- en: ❺ Compute intersection using element-wise multiplication.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用逐元素乘法计算交集。
- en: ❻ Compute union using element-wise addition.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 使用逐元素加法计算并集。
- en: ❼ Compute the dice coefficient.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 计算 dice 系数。
- en: ❽ Compute the dice loss.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 计算 dice 损失。
- en: 'Here, smooth is a smoothing parameter that we’ll use to avoid potential NaN
    values resulting in division by zero. After that we do the following:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，smooth 是一个平滑参数，我们将用它来避免可能导致除以零而产生 NaN 值的情况。然后我们进行以下操作：
- en: Obtain weights for each y_true label
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取每个 y_true 标签的权重
- en: Apply a softmax activation to y_pred
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对 y_pred 应用 softmax 激活函数
- en: Unwrap y_pred to the [-1, num_classes] tensor and y_true to a [-1]-sized vector
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 y_pred 展开为 [-1, num_classes] 的张量，将 y_true 展开为大小为 [-1] 的向量
- en: Then intersection and union are computed for y_pred and y_true. Specifically,
    intersection is computed as the result of element-wise multiplication of y_pred
    and y_true and the union as the result of the element-wise addition of y_pred
    and y_true.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 然后计算 y_pred 和 y_true 的交集和并集。具体来说，交集是通过 y_pred 和 y_true 的逐元素乘法计算出来的，而并集是通过 y_pred
    和 y_true 的逐元素加法计算出来的。
- en: Focal loss
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 焦点损失
- en: '*Focal loss* is a relatively novel loss introduced in the paper “Focal Loss
    for Dense Object Prediction” ([https://arxiv.org/pdf/1708.02002.pdf](https://arxiv.org/pdf/1708.02002.pdf)).
    Focal loss was introduced to combat the severe class imbalance found in segmentation
    tasks. Specifically, it solves a problem in many easy examples (e.g., samples
    from common classes with smaller loss), over-powering small numbers of hard examples
    (e.g., samples from rare classes with larger loss). Focal loss solves this problem
    by introducing a modulating factor that will down-weight easy examples, so, naturally,
    the loss function focuses more on learning hard examples.'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: The loss function we will use to optimize the segmentation model will be the
    loss resulting from addition of sparse cross-entropy loss and dice loss (see the
    next listing).
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.16 Final combined loss function
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Next, we will discuss evaluation metrics.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.2 Evaluation metrics
  id: totrans-448
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Evaluation metrics play a vital role in model training as a health check for
    the model. This means low performance/issues can be quickly identified by making
    sure evaluation metrics behave in a reasonable way. Here we will discuss three
    different metrics:'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: Pixel
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean accuracy
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean IoU
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will implement these as custom metrics by leveraging some of the existing
    metrics in TensorFlow, where you have to subclass from the tf.keras.metrics.Metric
    class or one of the existing metrics. This means that you create a new Python
    class, which inherits from the base tf.keras.metrics.Metric base class of one
    of the existing concrete metrics classes:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The first thing you need to understand about a metric is that it is a stateful
    object, meaning it maintains a state. For example, a single epoch has multiple
    iterations and assumes you’re interested in computing the accuracy. The metric
    needs to accumulate the values required to compute the accuracy over all the iterations
    so that at the end, it can compute the average accuracy for that epoch. When defining
    a metric, there are three functions you need to be mindful of: __init__, update_state,
    result, and reset_states.'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s get concrete by assuming that we are implementing an accuracy metric
    (i.e., the number of elements in y_pred that matched y_true as a percentage).
    It needs to maintain a total: the sum of all the accuracy values we passed and
    the count (number of accuracy values we passed). With these two state elements,
    we can compute the mean accuracy at any time. When implementing the accuracy metric,
    you implement these functions:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: __init__—Defines two states; total and count
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: update_state—Updates total and count based on y_true and y_pred
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: result—Computes the mean accuracy as total/count
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: reset_states—Resets both the total and count (this needs to happen at the beginning
    of an epoch)
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s see how this knowledge translates to the evaluation metrics we’re interested
    in solving.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: Pixel and mean accuracies
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: Pixel accuracy is the simplest metric you can think of. It measures the pixel-wise
    accuracy between the prediction and the true target (see the next listing).
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.17 Implementing the pixel accuracy metric
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: ❶ Set the shape of y_true (in case it is undefined).
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Reshape y_true to a vector.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Reshape y_pred after taking argmax to a vector.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Define a valid mask (mask out unnecessary pixels).
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Gather pixels/labels that satisfy the valid_mask condition.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: ❻ With the processed y_true and y_pred, compute the accuracy using the update_state()
    function.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: 'Pixel accuracy computes the one-to-one match between predicted pixels and true
    pixels. To compute this, we subclass from tf.keras.metrics.Accuracy as it has
    all the computations we need. To do this, we override the update_state function
    as shown. There are a few things we need to take care of:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: We need to set the shape of y_true as a precaution. This is because when working
    with tf.data.Dataset, sometimes the shape is lost.
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reshape y_true to a vector.
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get the class labels of y_pred by performing tf.argmax() and reshape it to a
    vector.
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define a valid mask that ignores unwanted classes (e.g., unknown objects).
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get the pixels that satisfy only the valid_mask filter.
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once we complete these tasks, we simply call the parent object’s (i.e., tf.keras
    .metrics.Accuracy) update_state method with the corresponding arguments. We don’t
    have to override result() and reset_states() functions, as they already contain
    the correct computations.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
- en: We said that class imbalance is prevalent in image segmentation problems. Typically,
    background pixels will spread in a large region of the image, potentially leading
    to misguided conclusions. Therefore, a slightly better approach might be to compute
    the accuracy individually per class and then average it. Enter mean accuracy,
    which prevents the undesired characteristics of pixel accuracy (see the next listing).
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.18 Implementing the mean accuracy metric
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-481
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: ❶ Initial setup
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Compute the confusion matrix using y_true and y_pred.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Get the true positives (elements on the diagonal).
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Compute the mean accuracy using true positives and true class counts for each
    class.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Compute the average of mean_accuracy using the update_state() function.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: The MeanAccuracyMetric will branch out from tf.keras.metrics.Mean, which computes
    the average over a given sequence of values. The plan is to compute the mean_accuracy
    within the update_state() function and then pass the value to the parent’s update_state()
    function so that we get the average value of mean accuracy. First, we perform
    the initial setup and clean-up of y_true and y_pred we discussed earlier.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
- en: '![08-13](../../OEBPS/Images/08-13.png)'
  id: totrans-488
  prefs: []
  type: TYPE_IMG
- en: Figure 8.13 Illustration of a confusion matrix for a five-class classification
    problem. The shaded boxes represent true positives.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: 'Afterward, we compute the confusion matrix (figure 8.13) from predicted and
    true targets. A confusion matrix for an *n*-way classification problem (i.e.,
    a classification problem with *n* possible classes) is defined as a *n* × *n*
    matrix. Here, the element at the (*i*, *j*) position indicates how many instances
    were predicted as belonging to the *i* ^(th) class but actually belong to the
    *j* ^(th) class. Figure 8.13 portrays this type of confusion matrix. We can get
    the true positives by extracting the diagonal (i.e., (*i*, *i*) elements in the
    matrix for all 1 < = *i* < = *n*). We can now compute the mean accuracy in two
    steps:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，从预测和真实目标计算混淆矩阵（图8.13）。对于一种*n*种分类问题（即，具有*n*个可能类别的分类问题），混淆矩阵被定义为一个*n*×*n*矩阵。这里，位置（*i*，*j*）处的元素表示预测为属于第*i*个类别的实例实际上属于第*j*个类别。图8.13描绘了这种类型的混淆矩阵。我们可以通过提取对角线（即，所有1
    <= *i* <= *n*的矩阵中的(*i*，*i*)元素）来获取真阳性。现在我们可以通过两个步骤计算平均准确度：
- en: Perform element-wise division on the true positive count by actual counts for
    all the classes. This produces a vector whose elements represents per-class accuracy.
  id: totrans-491
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于所有类别，对真阳性计数进行逐元素除法。这将产生一个向量，其元素表示每个类别的准确性。
- en: Compute the vector mean that resulted from step 1.
  id: totrans-492
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算步骤1产生的向量平均值。
- en: Finally, we pass the mean accuracy to its parent’s update_state() function.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将平均准确度传递给其父对象的update_state()函数。
- en: Mean IoU
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 平均交并比
- en: '*Mean IoU* (mean intersection over union) is a popular evaluation metric pick
    for segmentation tasks and has close ties to the dice loss we discussed earlier,
    as they both use the concept of intersection and union to compute the final result
    (see the next listing).'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: '*平均交并比*(mean intersection over union)是用于分割任务的流行评估度量，与我们之前讨论的Dice loss密切相关，因为它们都使用交和并的概念来计算最终结果（见下一个列表）。'
- en: Listing 8.19 Implementing the mean IoU metric
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.19 实现平均交并比度量
- en: '[PRE50]'
  id: totrans-497
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: ❶ After the initial setup of y_true and y_pred, all we need to do is call the
    parent’s update_state() function.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在y_true和y_pred的初始设置之后，我们只需调用父对象的update_state()函数即可。
- en: The mean IoU computations are already found in tf.keras.metrics.MeanIoU. Therefore,
    we will use that as our parent class. All we need to do is perform the aforementioned
    setup for y_true and y_pred and then call the parent’s update_state() function.
    Mean IoU is computed as
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 平均交并比的计算已经在tf.keras.metrics.MeanIoU中找到。因此，我们将使用它作为我们的父类。我们所需做的就是为y_true和y_pred进行前述设置，然后调用父对象的update_state()函数。平均交并比计算为
- en: '![08_13a](../../OEBPS/Images/08_13a.png)'
  id: totrans-500
  prefs: []
  type: TYPE_IMG
  zh: '![08_13a](../../OEBPS/Images/08_13a.png)'
- en: Various elements used in this computation are depicted in figure 8.14.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 在该计算中使用的各种元素如图8.14所示。
- en: '![08-14](../../OEBPS/Images/08-14.png)'
  id: totrans-502
  prefs: []
  type: TYPE_IMG
  zh: '![08-14](../../OEBPS/Images/08-14.png)'
- en: Figure 8.14 Confusion matrix and how it can be used to compute false positives,
    false negatives, and true positives
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.14 混淆矩阵，以及如何用它来计算假阳性，假阴性和真阳性
- en: 'We now understand the loss functions and evaluation metrics that are available
    to us and have already implemented them. We can incorporate these losses to compile
    the model:'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在理解了可用于我们的损失函数和评估指标，并已经实现了它们。我们可以将这些损失编译到模型中：
- en: '[PRE51]'
  id: totrans-505
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Remember that we stored the weights from a convolution block we removed earlier.
    Now that we have compiled the model, we can copy the weights to the new model
    using the following syntax:'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们从前面删除的卷积块中存储了权重。现在我们已经编译了模型，我们可以使用以下语法将权重复制到新模型中：
- en: '[PRE52]'
  id: totrans-507
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: We now move on to training the model with the data pipeline and the model we
    defined.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在开始使用数据管道和我们定义的模型来训练模型。
- en: Exercise 4
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 练习4
- en: You are coming up with a new loss function that computes the disjunctive union
    between y_true and y_pred. The disjunctive union between two sets A and B is the
    set of elements that are in either A or B but not in the intersection. You know
    you can compute the intersection with element-wise multiplication and union with
    element-wise addition of y_true and y_pred. Write the equation to compute the
    disjunctive union as a function of y_true and y_pred.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 您正在设计一个新的损失函数，用于计算y_true和y_pred之间的不相交并集。两个集合A和B之间的不相交并集是在A或B中但不在交集中的元素的集合。您知道可以通过y_true和y_pred的元素乘法计算交并通过y_true和y_pred的元素加法计算。编写函数的等式，以根据y_true和y_pred计算不相交并集。
- en: 8.5 Training the model
  id: totrans-511
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.5 训练模型
- en: You’re coming to the final stages of the first iteration of your product. Now
    it’s time to put the data and knowledge you garnered to good use (i.e., train
    the model). We will train the model for 25 epochs and monitor the pixel accuracy,
    mean accuracy, and mean IoU metrics. During the training, we will measure the
    performance on validation data set.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
- en: Training the model is the easiest part, as we have done the hard work that leads
    up to training. It is now just a matter of calling fit() with the correct parameters
    on the DeepLab v3 we just defined, as the following listing shows.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.20 Training the model
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-515
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: ❶ Train logger
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Set the mode for the following callbacks automatically by looking at the metric
    name.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Learning rate scheduler
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Early stopping callback
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Train the model while using the validation set for learning rate adaptation
    and early stopping.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will define a directory called eval if it does not exist. The training
    logs will be saved in this directory. Next, we define three different callbacks
    to be used during the training:'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
- en: csv_logger—Logs the training loss/metrics and validation loss/metrics
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: lr_callback—Reduces the learning rate by a factor of 10, if the validation loss
    does not decrease within three epochs
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: es_callback—Performs early stopping if the validation loss does not decrease
    within six epochs
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NOTE On an Intel Core i5 machine with an NVIDIA GeForce RTX 2070 8GB, the training
    took approximately 45 minutes to run 25 epochs.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
- en: 'With that, we call deeplabv3.fit() with the following parameters:'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
- en: x—The tf.data pipeline producing training instances (set to tr_image_ds).
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: steps_per_epoch—Number of steps per epoch. This is obtained by computing the
    number of training instances and dividing it by the batch size (set to n_train).
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: validation_data—The tf.data pipeline producing validation instances. This is
    obtained by computing the number of validation instances and dividing it by the
    batch size (set to val_image_ds).
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: epochs—Number of epochs (set to epochs).
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: callbacks—The callbacks we set up earlier (set to [lr_callback, csv_logger,
    es_callback]).
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After the model is trained, we will evaluate it on the test set. We will also
    visualize segmentations generated by the model.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 5
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
- en: You have a data set of 10,000 samples and have split it into 90% training data
    and 10% validation data. You use a batch size of 10 for training and a batch size
    of 20 for validation. How many training and validation steps will be there in
    a single epoch?
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
- en: 8.6 Evaluating the model
  id: totrans-535
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s take a moment to reflect on what we have done so far. We defined a data
    pipeline to read images and prepare them as inputs and targets for the model.
    Then we defined a model known as DeepLab v3 that uses a pretrained ResNet-50 as
    its backbone and a special module called atrous spatial pyramid pooling to predict
    the final segmentation mask. Then we defined task-specific losses and metrics
    to make sure we could evaluate the model with a variety of metrics. Afterward,
    we trained the model. Now it’s time for the ultimate reveal. We will measure the
    performance on an unseen test data set to see how well the model does. We will
    also visualize the model outputs and compare them against the real targets by
    plotting them side by side.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
- en: 'We can run the model over the unseen test images and gauge how well it is performing.
    To do that, we execute the following:'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-538
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: The size of the test set is the same as the validation set, as we split the
    images listed in val.txt into two equal validation and test sets. This will return
    around
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
- en: 62% mean IoU
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 87% mean accuracy
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 91% pixel accuracy
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are very respectable scores given our circumstances. Our training data
    set consists of less than 1,500 segmented images. Using this data, we were able
    to train a model that achieves around 62% mean IoU on a test data set of approximately
    size 725.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
- en: What does state of the art look like?
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
- en: The state-of-the-art performance on Pascal VOC 2012 reports around 90% mean
    IoU ([http://mng.bz/o2m2](http://mng.bz/o2m2)). However, these are models that
    are much larger and complex than what we used here. Furthermore, they are typically
    trained with significantly more data by using an auxiliary data set known as the
    semantic boundary data set (SBD) (introduced in the paper [http://mng.bz/nNve](http://mng.bz/nNve)).
    This will push the training datapoint count to over 10,000 (close to seven times
    the size of our current training set).
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
- en: You can further investigate the model by visually inspecting some of the results
    our module produces. After all, it is a vision model that we are developing. Therefore,
    we should not rely solely on numbers to make decisions and conclusions. We should
    also visually analyze the results before settling on a conclusion.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
- en: What would the results from a U-Net based network look like?
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
- en: Under similar conditions provided for the DeepLab v3 model, the U-Net model
    built with a pretrained ResNet-50 model as the encoder was only able to achieve
    approximately 32.5% mean IoU, 78.5% mean accuracy, and 81.5% pixel accuracy. The
    implementation is provided in the Jupyter notebook in the ch08 folder.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
- en: On an Intel Core i5 machine with an NVIDIA GeForce RTX 2070 8GB, the training
    took approximately 55 minutes to run 25 epochs.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
- en: There is a detailed explanation of the U-Net model in appendix B.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
- en: To complete this investigation, we will get a random sample from the test set
    and ask the model to predict the segmentation map for each of those images. Then
    we will plot the results side by side to ensure that our model is doing a good
    job (figure 8.15).
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
- en: '![08-15](../../OEBPS/Images/08-15.png)'
  id: totrans-552
  prefs: []
  type: TYPE_IMG
- en: Figure 8.15 Comparing the true annotated targets to model predictions. You can
    see that the model is quite good at separating objects from different backgrounds.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
- en: We can see that unless it is an extremely difficult image (e.g., the top-left
    image, where there’s a car obscured by a gate), our model does a very good job.
    It can identify almost all the images found in the sample we analyzed with high
    accuracy. The code for visualizing the images is provided in the notebook.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our discussion of image segmentation. In the next few chapters,
    we will discuss several natural language processing problems.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 6
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
- en: You are given
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
- en: A model (called model)
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A batch of images called batch_image (already preprocessed and ready to be fed
    to a model)
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A corresponding batch of targets, batch_targets (the true segmentation mask
    in one-hot encoded format)
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write a function called get_top_bad_examples(model, batch_images, batch_targets,
    n) that will return the top n indices of the hardest (highest loss) images in
    batch_ images. Given a predicted mask and a target mask, you can use the sum over
    element-wise multiplication as the loss of a given image.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
- en: You can use the model.predict() function to make a prediction on batch_ images,
    and it will return a predicted mask as the same size as batch_targets. Once you
    compute the losses for the batch (batch_loss), you can use the tf.math.top_k(batch_
    loss, n) function to get the indices of elements with the highest value. tf.math
    .top_k() returns a tuple containing the top values and indices of a given vector,
    in that order.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-563
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Segmentation models fall into two broad categories: semantic segmentation and
    instance segmentation.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The tf.data API provides various functionality to implement complex data pipelines,
    such as using custom NumPy functions, performing quick transformations using tf.data.Dataset.map(),
    and I/O optimization techniques like prefetch and cache.
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DeepLab v3 is a popular segmentation model that uses a pretrained ResNet-50
    model as its backbone and atrous convolutions to increase the receptive field
    by inserting holes (i.e., zeros) between the kernel weights.
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The DeepLab v3 model uses a module known as atrous spatial pyramid pooling to
    aggregate information at multiple scales, which helps to create a fine-grained
    segmented output.
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In segmentation tasks, cross entropy and dice loss are two popular losses, whereas
    pixel accuracy, mean accuracy, and mean IoU are popular evaluation metrics.
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In TensorFlow, loss functions can be implemented as stateless functions. But
    metrics must be implemented as stateful objects by subclassing from the tf.keras.metrics.Metric
    base class or a suitable class.
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The DeepLab v3 model achieved a very good accuracy of 62% mean IoU on the Pascal
    VOC 2010 data set.
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Answers to exercises
  id: totrans-571
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Exercise 1**'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-573
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '**Exercise 2**'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-575
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '**Exercise 3**'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-577
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '**Exercise 4**'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-579
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '**Exercise 5**'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 6**'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-582
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
