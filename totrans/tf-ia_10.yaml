- en: '8 Telling things apart: Image segmentation'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8区分事物：图像分割
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容涵盖
- en: Understanding segmentation data and working with it in Python
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解分割数据并在Python中处理它
- en: Implementing a fully fledged segmentation data pipeline
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现一个完整的分割数据管道
- en: Implementing an advanced segmentation model (DeepLab v3)
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现高级分割模型（DeepLab v3）
- en: Compiling models with custom-built image segmentation loss functions/metrics
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自定义构建的图像分割损失函数/度量编译模型
- en: Training the image segmentation model on the clean and processed image data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对清洁和处理后的图像数据进行图像分割模型训练
- en: Evaluating the trained segmentation model
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估经过训练的分割模型
- en: 'In the last chapter, we learned about various advanced computer vision models
    and techniques to push the performance of an image classifier. We learned about
    the architecture of Inception net v1 as well as its successors (e.g., Inception
    net v2, v3, and v4). Our objective was to lift the performance of the model on
    an image classification data set with 64 × 64-sized RGB images of objects belonging
    to 200 different classes. While trying to train a model on this data set, we learned
    many important concepts:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了各种先进的计算机视觉模型和技术，以提高图像分类器的性能。我们了解了Inception net v1的架构以及它的后继者（例如Inception
    net v2、v3和v4）。我们的目标是提高模型在一个包含200个不同类别的对象的64×64大小的RGB图像的图像分类数据集上的性能。在尝试在此数据集上训练模型时，我们学到了许多重要的概念：
- en: '*Inception blocks*—A way to group convolutional layers having different-sized
    windows (or kernels) to encourage learning features at different scales while
    making the model parameter efficient due to the smaller-sized kernels.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Inception blocks*—一种将具有不同尺寸窗口（或核）的卷积层分组在一起的方法，以鼓励学习不同尺度的特征，同时由于更小尺寸的核而使模型参数高效。'
- en: '*Auxiliary outputs*—Inception net uses a classification layer (i.e., a fully
    connected layer with softmax activation) not only at the end of the network, but
    also in the middle of the network. This enables the gradients from the final layer
    to flow strongly all the way to the first layer.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*辅助输出*—Inception net不仅在网络末端使用分类层（即具有softmax激活的完全连接层），而且还在网络中间使用。这使得从最终层到第一层的梯度能够强劲地传播。'
- en: '*Augmenting data*—Using various image transformation techniques (adjusting
    brightness/contrast, rotating, translating, etc.) to increase the amount of labeled
    data using the tf.keras.preprocessing.image.ImageDataGenerator.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据增强*—使用各种图像转换技术（调整亮度/对比度、旋转、平移等）使用tf.keras.preprocessing.image.ImageDataGenerator增加标记数据的数量。'
- en: '*Dropout*—Switching on and off nodes in the layers randomly. This forces the
    neural networks to learn more robust features as the network does not always have
    all the nodes activated.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Dropout*—随机打开和关闭层中的节点。这迫使神经网络学习更健壮的特征，因为网络并不总是激活所有节点。'
- en: '*Early stopping*—Using the performance on the validation data set as a way
    to control when the training stops. If the validation performance has not increased
    in a certain number of epochs, training is halted.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*提前停止*—使用验证数据集上的性能作为控制训练何时停止的方法。如果在一定数量的epochs中验证性能没有提高，则停止训练。'
- en: '*Transfer learning*—Downloading and using a pretrained model (e.g., Inception-ResNet
    v2) trained on a larger, similar data set as the initialization and fine-tuning
    it to perform well on the task at hand.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*迁移学习*—下载并使用在更大、类似数据集上训练的预训练模型（例如Inception-ResNet v2）作为初始化，并对其进行微调以在手头的任务上表现良好。'
- en: 'In this chapter, we will learn about another important task in computer vision:
    image segmentation. In image classification, we only care if an object exists
    in a given image. Image segmentation, on the other hand, recognizes multiple objects
    in the same image as well as where they are in the image. It is a very important
    topic of computer vision, and applications like self-driving cars live and breathe
    image segmentation models. Self-driving cars need to precisely locate objects
    in their surroundings, which is where image segmentation comes into play. As you
    might have guessed already, they also have their roots in many other applications:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习计算机视觉中另一个重要任务：图像分割。在图像分类中，我们只关心给定图像中是否存在对象。另一方面，图像分割不仅识别同一图像中的多个对象，还识别它们在图像中的位置。这是计算机视觉的一个非常重要的主题，像自动驾驶汽车这样的应用程序依赖于图像分割模型。自动驾驶汽车需要精确定位其周围的物体，这就是图像分割发挥作用的地方。你可能已经猜到，它们在许多其他应用程序中也有它们的根基：
- en: Image retrieval
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像检索
- en: Identifying galaxies ([http://mng.bz/gwVx](http://mng.bz/gwVx))
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Medical image analysis
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are a computer vision/deep learning engineer/researcher working on image-related
    problems, there is a high chance that your path will cross with image segmentation.
    Image segmentation models classify each pixel in the image to one of a predefined
    set of object categories. Image segmentation has ties to the image classification
    task we saw earlier. Both solve a classification task. Additionally, pretrained
    image classification models are used as the backbone of segmentation models, as
    they can provide crucial image features at different granularities to solve the
    segmentation task better and faster. A key difference is that image classifiers
    are solving a sparse prediction task, where each image has a single class label
    associated, as opposed to segmentation models that solve a dense prediction task
    that has a class label associated with every pixel in the image.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'Any image segmentation algorithm can be classified as one of the following:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '*Semantic segmentation*—The algorithm is only interested in identifying different
    categories of objects present in the image. For example, if there are multiple
    persons in the image, the pixels corresponding to all of them will be tagged with
    the same class.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Instance segmentation*—The algorithm is interested in identifying different
    objects separately. For example, if there are multiple persons in the image, pixels
    belonging to each person are represented by a unique class. Instance-based segmentation
    is considered more difficult than semantic segmentation.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figure 8.1 depicts the difference between the data found in a semantic segmentation
    task and an instance-based segmentation task. In this chapter, we will focus on
    semantic segmentation ([http://mng.bz/5QAZ](http://mng.bz/5QAZ)).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '![08-01](../../OEBPS/Images/08-01.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 Semantic segmentation versus instance segmentation
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look at the data we are dealing with more closely.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 8.1 Understanding the data
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You are experimenting with a startup idea. The idea is to develop a navigation
    algorithm for small remote-control (RC) toys. Users can choose between how safe
    or adventurous the navigation needs to be. As the first step, you plan to develop
    an image segmentation model. The output of the image segmentation model will later
    feed to a different model that will predict the navigation path depending on what
    the user requests.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'For this task, you feel the Pascal VOC 2012 data set will be a good fit as
    it mostly comprises indoor and outdoor images that are found in urban/domestic
    environments. It contains pairs of images: an input image containing some objects
    and an annotated image. In the annotated image, each pixel has an assigned color,
    depending on which object that pixel belongs to. Here, you plan to download the
    data set and load the data successfully into Python.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: After having a good understanding/framing of the problem you want to solve,
    your next focus point should be understanding and exploring the data. Segmentation
    data is different from the image classification data sets we’ve seen thus far.
    One major difference is that both the input and target are images. The input image
    is a standard image, similar to what you’d find in an image classification task.
    Unlike in image classification, the target is not a label, but an image, where
    each pixel has a color from a predefined palette of colors. In other words, each
    object we’re interested in segmenting is assigned a color. Then a pixel corresponding
    to that object in the input image is colored with that color. The number of available
    colors is the same as the number of different objects (plus background) that you’re
    interested in identifying (figure 8.2).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '![08-02](../../OEBPS/Images/08-02.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 Inputs and outputs of an image classifier versus an image segmentation
    model
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: For this task, we will be using the PASCAL VOC 2012 data set, which is popular
    and consists of real-world scenes. The data set has labels for 22 different classes,
    as outlined in table 8.1.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Table 8.1 Different classes and their respective labels in the PASCAL VOC 2012
    data set
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '| **Class** | **Assigned Label** | **Class** | **Assigned Label** |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
- en: '| Background | 0 | Dining table | 11 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
- en: '| Aeroplane | 1 | Dog | 12 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
- en: '| Bicycle | 2 | Horse | 13 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
- en: '| Bird | 3 | Motorbike | 14 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
- en: '| Boat | 4 | Person | 15 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
- en: '| Bottle | 5 | Potted plant | 16 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
- en: '| Bus | 6 | Sheep | 17 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
- en: '| Car | 7 | Sofa | 18 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
- en: '| Cat | 8 | Train | 19 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
- en: '| Chair | 9 | TV/monitor | 20 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
- en: '| Cow | 10 | Boundaries/unknown object | 255 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
- en: The white pixels represent object boundaries or unknown objects. Figure 8.3
    illustrates the data set by showing a sample for every single object class present.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '![08-03](../../OEBPS/Images/08-03.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 Samples from the PASCAL VOC 2012 data set. The data set shows a single
    example image, along with the annotated segmentation of it for the 20 different
    object classes.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: 'In figure 8.4, diving a bit deeper, you can see a single sample datapoint (best
    viewed in color) up close. It has two objects: a chair and a dog. As it is shown,
    different colors are assigned to different object categories. While the figure
    is best viewed in color, you still can distinguish different objects by paying
    attention to the white border that outlines the objects in the figure.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '![08-04](../../OEBPS/Images/08-04.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 An original input image in image segmentation and the corresponding
    target annotated/segmented image
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: First, we’ll download the data set, if it does not exist, from [http://mng.bz/6XwZ](http://mng.bz/6XwZ)
    (see the next listing).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.1 Downloading data
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Check if the file is already downloaded. If so, don’t download again.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Get the content from the URL.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Save the file to disk.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: ❹ If the file exists but is not extracted, extract the file.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 'The data set download is quite similar to our past experience. The data exists
    as a tarfile. We download the file if it doesn’t exist and extract it. Next, we
    will discuss how to use the image library Pillow and NumPy to load the images
    into memory. Here, the target images will need special treatment, as you will
    see that they are not stored using the conventional approach. There are no surprises
    involved with loading input images to memory. Using the PIL (i.e., Pillow) library,
    they can be loaded with a single line of code:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的下载与我们以前的经验非常相似。数据作为tar文件存在。如果文件不存在，我们会下载文件并解压缩。接下来，我们将讨论如何使用图像库Pillow和NumPy将图像加载到内存中。在这里，目标图像将需要特殊处理，因为您将看到它们不是使用常规方法存储的。加载输入图像到内存中没有任何意外情况。使用PIL（即Pillow）库，可以通过一行代码加载它们：
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, you can inspect the image’s attributes:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您可以检查图像的属性：
- en: '[PRE2]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: It’s time to load the corresponding annotated/segmented target images. As mentioned
    earlier, target images require special attention. The target images are not stored
    as standard images but as *palettized* images. Palettization is a technique to
    reduce memory footprint while storing images with a fixed number of colors in
    the image. The crux of the method is to maintain a palette of colors. The palette
    is stored as a sequence of integers, which has a length of the number of colors
    or the number of channels. (E.g., in the case of RGB, where a pixel is made of
    three values corresponding to red, green, and blue, the number of channels is
    three. A grayscale image has a single channel, where each pixel is made of a single
    value). The image itself then stores an array of indices (size = height × width),
    where each index maps to a color in the palette. Finally, by mapping the palette
    indices from the image to palette colors, you can compute the original image.
    Figure 8.5 provides a visual exposition of this discussion.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候加载相应的注释/分割的目标图像了。如前所述，目标图像需要特殊关注。目标图像不是作为标准图像存储的，而是作为*调色板化*图像存储的。调色板化是一种在图像中存储具有固定颜色数量的图像时减少内存占用的技术。该方法的关键在于维护一个颜色调色板。调色板被存储为整数序列，其长度为颜色数量或通道数量（例如，对于RGB的情况，一个像素由三个值对应于红、绿和蓝，通道数量为三。灰度图像具有单个通道，其中每个像素由单个值组成）。然后，图像本身存储了一个索引数组（大小为高度×宽度），其中每个索引映射到调色板中的一种颜色。最后，通过将图像中的调色板索引映射到调色板颜色，可以计算出原始图像。图8.5提供了这个讨论的视觉展示。
- en: '![08-05](../../OEBPS/Images/08-05.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![08-05](../../OEBPS/Images/08-05.png)'
- en: Figure 8.5 The numerical representation of input images and target images in
    the PASCAL VOC 2012 data set
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5 显示了PASCAL VOC 2012数据集中输入图像和目标图像的数值表示。
- en: The next listing shows the code for reconstructing the original image pixels
    from the palettized image.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个清单展示了从调色板图像中重新构造原始图像像素的代码。
- en: Listing 8.2 Reconstructing the original image from a palettized image
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 代码清单8.2 从调色板图像中重建原始图像
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Get the color palette from the image.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从图像中获取颜色调色板。
- en: ❷ The palette is stored as a vector. We reshape it to an array, where each row
    represents a single RGB color.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 调色板以向量形式存储。我们将其重新整形为一个数组，其中每一行表示一个单独的RGB颜色。
- en: ❸ Get the image’s height and width.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 获取图像的高度和宽度。
- en: ❹ Convert the palettized image stored as an array to a vector (helps with our
    next steps).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将以数组形式存储的调色板图像转换为向量（有助于接下来的步骤）。
- en: ❺ Get the image as a vector if the image is provided as an array instead of
    a Pillow image.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 如果图像是以数组而不是Pillow图像提供的，将图像作为向量获取。
- en: ❻ We first define a vector of zeros that has the same length as our image. Then,
    for all the indices found in the image, we gather corresponding colors from the
    palette and assign them to the same position in the rgb_image.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 首先，我们定义一个与图像长度相同的零向量。然后，对于图像中的所有索引，我们从调色板中获取相应的颜色，并将其分配到rgb_image的相同位置。
- en: ❼ Restore the original shape.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 恢复原始形状。
- en: Here, we first obtain the palette of the image using the get_palette() function.
    This will be present as a one-dimensional array (of length number of classes ×
    number of channels). Next, we need to reshape the array to a (number of classes,
    number of channels)-sized array. In our case, this will be converted to a (22,3)-sized
    array. As we define the first dimension of the reshape as -1, it will be automatically
    inferred from the original size of the data and the other dimensions of the reshape
    operation. Finally, we define an array of zeros, which will ultimately store the
    actual colors the indices found in the image. To do that, we index the rgb_image
    vector using the image (which contains indices) and assign matching colors from
    the palette to those indices.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: With the data we have looked at thus far, let’s define a TensorFlow data pipeline
    that can transform and convert the data to a format acceptable by the model.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 1
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: You have been provided with an rgb_image in RGB format, where each pixel belongs
    to one of n distinctive colors and has been given a palette called palette, which
    is a [n,3]-sized array. How would you convert the rgb_image to a palettized image?
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 'Hint You can create the naïve solution by using three for loops: two loops
    to get a single pixel of rgb_image and then a final loop to traverse each color
    in the palette.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '8.2 Getting serious: Defining a TensorFlow data pipeline'
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far, we have discussed the data that will help us build a navigation algorithm
    for the RC toy. Before building a model, an important task to complete is having
    a scalable data ingestion method from disk to the model. Doing this upfront will
    save us a lot of time when we’re ready to scale or productionize. You think the
    best way is to implement a tf.data pipeline to retrieve images from the disk,
    preprocess them, transform them, and have them ready for the model to grab them.
    This pipeline should read images in, reshape them to a fixed size (in the case
    of variable-sized images), augment them (during the training stage), batch them,
    and repeat this process for a desired number of epochs. Finally, we will define
    three pipelines: a training data pipeline, a validation data pipeline, and a testing
    data pipeline.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'Our goal at the end of the data exploration stage should be to build a reliable
    data pipeline from the disk to the model. This is what we will be looking at here.
    At a high level, we will build a TensorFlow data pipeline that will perform the
    following tasks:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Get the filenames belonging to a certain subset (e.g., training, validation,
    or testing).
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Read the specified images from the disk.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocess the images (this involves normalizing/resizing/cropping images).
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform augmentation on the images to increase the volume of data.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batch the data in small batches.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimize data retrieval using several built-in optimization techniques.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As the first step, we will write a function that returns a generator that will
    generate filenames of the data that we want to be fetched. We will also provide
    the ability to specify which subset the user wants to be fetched (e.g., training,
    validation, or testing). Returning data through a generator will make writing
    a tf.data pipeline easier (see the following listing).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.3 Retrieving the filenames for a given subset of data
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Read the CSV file that contains the training instance filenames.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: ❷ For validation/test subsets, perform a one-time shuffle to make sure we get
    a good mix with a fixed seed.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Read the CSV file that contains validation/test filenames.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Shuffle the data after fixing the seed.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Get the first half as the validation set.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Get the second half as the test set.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Form absolute paths to the input image files we captured (depending on the
    subset argument).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Return the filename pairs (input and annotations) as a generator.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: ❾ Form absolute paths to the segmented image files.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see that we’re passing a few arguments when reading the CSV files.
    These arguments characterize the file we’re reading. These files are extremely
    simple and contain just a single image filename on a single line. index_col=None
    means that the file does not have an index column, header=None means there is
    no header in the file, and squeeze=True means that the output will be presented
    as a pandas Series, not a pandas Dataframe. With that, we can define a TensorFlow
    data set (tf.data.Dataset) as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: TensorFlow has several different functions for generating data sets using different
    sources. As we have defined the function get_subset_filenames() to return a generator,
    we will use the tf.data.Dataset.from_generator() function. Note that we need to
    provide the format as well as the datatypes of the returned data, by the generator,
    using the output_types argument. The function subset_filename_gen_func returns
    two strings; therefore, we define output types as a tuple of two tf.string elements.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 'One other important aspect is the different txt files we read from depending
    on the subset. There are three different files in the relative path: the data\VOCtrainval_11-May-2012\VOCdevkit\VOC2012\ImageSets\Segmentation
    folder; train.txt, val.txt, and trainval.txt. Here, train.txt contains the filenames
    of the training images, whereas val.txt contains the filenames of the validation/testing
    images. We will use these files to create different pipelines that produce different
    data.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Where does tf.data come from?
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow’s tf.data pipeline can consume data from various sources. Here are
    some of the commonly used methods to retrieve data:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: tf.data.Dataset.from_generator(gen_fn)—You have already seen this function in
    action. If you have a generator (i.e., gen_fn) that produces data, you want it
    to be processed through a tf.data pipeline. This is the easiest method to use.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'tf.data.Dataset.from_tensor_slices(t)—This is a very useful function if you
    have data already loaded as a big matrix. t can be an N-dimensional matrix, and
    this function will extract element by element on the first dimension. For example,
    assume that you have loaded a tensor t of size 3 × 4 to memory:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Then you can easily set up a tf.data pipeline as follows. tf.data.Dataset.from_
    tensor_slices(t) *will return* [1,2,3,4], *then* [2,3,4,5], *and finally* [6,7,8,9]
    when you iterate this data pipeline. In other words, you are seeing one row (i.e.,
    a slice from the batch dimension, hence the name from_tensor_slices) at a time.
    You can now incorporate functions like tf.data.Dataset.batch() to get a batch
    of rows.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'Now it’s time to read in the images found in the file paths we obtained in
    the previous step. TensorFlow has support to easily load an image, where the path
    to a filename is img_filename, using the functions tf.io.read_file and tf.image.decode_image.
    Here, img_filename is a tf.string (i.e., a string in TensorFlow):'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We will use this pattern to load input images. However, we need to implement
    a custom image load function to load the target image. If you use the previous
    approach, it will automatically convert the image to an array with pixel values
    (instead of palette indices). But if we don’t perform that conversion, we will
    have a target array that is in the exact format we need because the palette indices
    that are in the target image are the actual class labels for each corresponding
    pixel in the input image. We will use PIL.Image within our TensorFlow data pipeline
    to load the image as a palettized image and avoid converting it to RGB:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'However, you can’t yet use custom functions as part of the tf.data pipeline.
    They need to be streamlined with the data-flow graph of the data pipeline by wrapping
    it as a TensorFlow operation. This can be easily achieved by using the tf.numpy_function
    operation, which allows you to wrap a custom function that returns a NumPy array
    as a TensorFlow operation. If we have the target image’s file path represented
    by y, you can use the following code to load the image into TensorFlow with a
    custom image-loading function:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The dark side of tf.numpy_function
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: NumPy has larger coverage for various scientific computations than TensorFlow,
    so you might think that tf.numpy_funtion makes things very convenient. This is
    not quite true, as you can infest your TensorFlow code with terrible performance
    degradations. When TensorFlow executes NumPy code, it can create very inefficient
    data flow graphs and introduce overheads. Therefore, always try to stick to TensorFlow
    operations and use custom NumPy code only if you have to. In our case, since there
    is no alternative way for us to load a palletized image without mapping palletized
    values to actual RGB, we used a custom function.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice how we’re passing both the input (i.e., inp=[y]) and its data type (i.e.,
    Tout=[tf.uint8]) to this function. They both need to be in the form of a Python
    list. Finally, let’s collate everything we discussed in one place:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们将输入（即，inp=[y]）和其数据类型（即，Tout=[tf.uint8]）都传递给此函数。它们都需要以 Python 列表的形式存在。最后，让我们把我们讨论的所有内容都整理到一个地方：
- en: '[PRE10]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The tf.data.Dataset.map() function will be used quite heavily throughout this
    discussion. You can find a lengthy explanation of the map() function in the sidebar.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: tf.data.Dataset.map() 函数将在本讨论中大量使用。您可以在侧边栏中找到 map() 函数的详细解释。
- en: 'A Refresher: tf.data.Dataset.map() function'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 刷新器：tf.data.Dataset.map() 函数
- en: This tf.data pipeline will make extensive use of the tf.data.Dataset.map() function.
    Therefore, it is extremely helpful for us to remind ourselves what this function
    accomplishes.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 此 tf.data 管道将大量使用 tf.data.Dataset.map() 函数。因此，我们提醒自己此函数实现了什么功能是非常有帮助的。
- en: The td.data.Dataset.map() function applies a given function or functions across
    all the records in a data set. In other words, it transforms the data points in
    the data set using a specified transformation. For example, assume the tf.data.Dataset
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: td.data.Dataset.map() 函数将给定的函数或多个函数应用于数据集中的所有记录。换句话说，它使用指定的转换来转换数据集中的数据点。例如，假设
    tf.data.Dataset
- en: '[PRE11]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: to get the square of each element, you can use the map function as
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取每个元素的平方，可以使用 map 函数如下
- en: '[PRE12]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'If you have multiple elements in a single record, leveraging the flexibility
    of map(), you can transform them individually:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在单个记录中有多个元素，则可以利用map()的灵活性来分别转换它们：
- en: '[PRE13]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As a normalization step we will bring the pixel values to [0,1] range by using
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 作为规范化步骤，我们将通过使用将像素值带到 [0,1] 范围的方法
- en: '[PRE14]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note that we are keeping our target image (y) as it is. Before I continue with
    any more steps in our pipeline, I want to direct your attention to an important
    matter. This is a caveat that is quite common, and it is thus worthwhile to be
    aware of it. After the step we just completed, you might feel like, if you want,
    you can batch the data and feed it to the model. For example
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们保留了目标图像（y）。在我们的管道中继续进行更多步骤之前，我想引起您的注意。这是一个相当常见的警告，因此值得注意。在我们刚刚完成的步骤之后，您可能会觉得，如果您愿意，您可以将数据进行批处理并将其馈送到模型中。例如
- en: '[PRE15]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'If you do that for this data set, you will get an error like the following:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对此数据集进行此操作，将会收到以下错误：
- en: '[PRE16]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This is because you ignored a crucial characteristic and a sanity check of the
    data set. Unless you’re using a curated data set, you are unlikely to find images
    with the same dimensions. If you look at images in the data set, you will notice
    that they are not of the same size; they have different heights and widths. In
    TensorFlow, unless you use a special data structure like tf.RaggedTensor, you
    cannot batch unequally sized images together. That is exactly what TensorFlow
    is complaining about in the error.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为您忽略了数据集的一个关键特征和一个健全性检查。除非您使用的是经过筛选的数据集，否则您不太可能找到具有相同尺寸的图像。如果您查看数据集中的图像，您会注意到它们的尺寸不同；它们具有不同的高度和宽度。在
    TensorFlow 中，除非您使用像 tf.RaggedTensor 这样的特殊数据结构，否则无法将大小不同的图像一起进行批处理。这正是 TensorFlow
    在错误中抱怨的内容。
- en: To alleviate the problem, we need to bring all the images to a standard size
    (see listing 8.4). To do that, we will define the following function. It will
    either
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓解问题，我们需要将所有图像调整为标准大小（请参见列表 8.4）。为此，我们将定义以下函数。它将
- en: Resize the image to a larger size (resize_to_before_crop) and then crop the
    image to the desired size (input_size) or
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将图像调整为较大的尺寸（resize_to_before_crop），然后将图像裁剪为所需大小（input_size），或者
- en: Resize the image to the desired size (input_size)
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将图像调整为所需大小（input_size）
- en: Listing 8.4 Bringing images to a fixed size using random cropping or resizing
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.4 使用随机裁剪或调整大小将图像调整为固定大小
- en: '[PRE17]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Define a function to randomly crop images after resizing.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义一个函数，在调整大小后随机裁剪图像。
- en: ❷ Resize the input image using bilinear interpolation to a larger size.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用双线性插值将输入图像调整为较大的尺寸。
- en: ❸ Resize the target image using the nearest interpolation to a larger size.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用最近邻插值将目标图像调整为较大的尺寸。
- en: ❹ To resize, we first swap the axis of y as it has the shape [1, height, width].
    We convert this back to [height, width, 1] (i.e., a single channel image) using
    the tf.transpose() function.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 要调整大小，我们首先交换 y 轴的轴，因为它的形状为 [1, height, width]。我们使用 tf.transpose() 函数将其转换回
    [height, width, 1]（即，单通道图像）。
- en: ❺ Define a random variable to offset images on height during cropping.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 定义一个随机变量，在裁剪期间偏移图像的高度。
- en: ❻ Define a random variable to offset images on width during cropping.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 定义一个随机变量，在裁剪期间在宽度上对图像进行偏移。
- en: ❼ Crop the input image and the target image using the same cropping parameters.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 使用相同的裁剪参数裁剪输入图像和目标图像。
- en: ❽ Resize both the input image and the target image to a desired size (no cropping).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 将输入图像和目标图像都调整为所需大小（不裁剪）。
- en: ❾ Define a random variable (used to perform augmentations).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 定义一个随机变量（用于执行增强）。
- en: ❿ If augmentation is enabled and the resized image is larger than the input
    size we requested, perform augmentation.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 如果启用增强并且调整大小后的图像大于我们请求的输入大小，则执行增强。
- en: ⓫ During augmentation, the rand_crop or resize function is executed randomly.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ⓫ 在增强期间，随机执行 rand_crop 或 resize 函数。
- en: ⓬ If augmentation is disabled, only resize images.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ⓬ 如果禁用增强，则只调整大小。
- en: Here, we define a function called randomly_crop_or_resize, which has two nested
    functions, rand_crop and resize. The rand_crop first resizes the image to the
    size specified in resize_to_before_crop and creates a random crop. It is imperative
    to check that you applied the exact same crop to both the input and the target.
    For example, same-crop parameters should be used to crop both the input and the
    target. In order to crop images, we use
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们定义了一个名为 randomly_crop_or_resize 的函数，其中包含两个嵌套函数 rand_crop 和 resize。rand_crop
    首先将图像调整为 resize_to_before_crop 中指定的大小，并创建一个随机裁剪。务必检查是否对输入和目标应用了完全相同的裁剪。例如，应使用相同的裁剪参数对输入和目标进行裁剪。为了裁剪图像，我们使用
- en: '[PRE18]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The arguments are self-explanatory: image takes an image to be cropped, offset_height
    and offset_width decide the starting point for the crop, and target_height and
    target_width specify the final size after the crop. The resize function will simply
    resize the input and the target to a specified size using the tf.image.resize
    operation.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 参数的含义不言而喻：image 接受要裁剪的图像，offset_height 和 offset_width 决定裁剪的起点，target_height
    和 target_width 指定裁剪后的最终大小。resize 函数将使用 tf.image.resize 操作简单地将输入和目标调整为指定大小。
- en: When resizing, we use *bilinear interpolation* for the input images and *nearest
    interpolation* for targets. Bilinear interpolation resizes the images by computing
    the resulting pixels, as an average of neighboring pixels, whereas nearest interpolation
    computes the output pixel as the nearest most common pixel from the neighbors.
    Bilinear interpolation leads to a smoother result after resizing. However, you
    must use nearest interpolation for the target image, as bilinear interpolation
    will lead to fractional outputs, corrupting the integer-based annotations. The
    interpolation techniques described are visualized in figure 8.6.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在调整大小时，我们对输入图像使用*双线性插值*，对目标使用*最近邻插值*。双线性插值通过计算结果像素的邻近像素的平均值来调整图像大小，而最近邻插值通过从邻居中选择最近的常见像素来计算输出像素。双线性插值在调整大小后会导致更平滑的结果。然而，必须对目标图像使用最近邻插值，因为双线性插值会导致分数输出，破坏基于整数的注释。图
    8.6 可视化了所描述的插值技术。
- en: '![08-06](../../OEBPS/Images/08-06.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![08-06](../../OEBPS/Images/08-06.png)'
- en: Figure 8.6 Nearest interpolation and bilinear interpolation for both up-sampling
    and down-sampling tasks
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.6 最近邻插值和双线性插值用于上采样和下采样任务
- en: 'Next, we will introduce an additional step to the way we’re going to use these
    two nested functions. If augmentation is enabled, we want the cropping or resizing
    to take place randomly within the pipeline. We will define a random variable (drawn
    from a uniform distribution between 0 and 1) and perform crop or resize depending
    on the value of the random variable at a given time. This conditioning can be
    achieved using the tf.cond function, which takes three arguments and returns output
    according to these arguments:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将在使用这两个嵌套函数的方式上引入一个额外的步骤。如果启用了增强，我们希望裁剪或调整大小在管道中随机地发生。我们将定义一个随机变量（从介于
    0 和 1 之间的均匀分布中抽取）并根据随机变量的值在给定时间内执行裁剪或调整大小。可以使用 tf.cond 函数实现这种条件，该函数接受三个参数，并根据这些参数返回输出：
- en: Condition—This is a computation that results in a Boolean value (i.e., is the
    random variable rand greater than 0.5).
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Condition——这是一个计算结果为布尔值的计算（即随机变量 rand 是否大于 0.5）。
- en: true_fn—If the condition is true, then this function will be executed (i.e.,
    perform rand_crop on both x and y)
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: true_fn——如果条件为真，则执行此函数（即对 x 和 y 执行 rand_crop）
- en: false_fn—If the condition is false, then this function will be executed (i.e.,
    perform a resize on both x and y)
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: false_fn——如果条件为假，则执行此函数（即对 x 和 y 执行调整大小）
- en: 'If augmentation is disabled (i.e., by setting the augmentation variable to
    False), only resizing is performed. With the details fleshed out, we can use the
    randomly_crop_ or_resize function in our data pipeline as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: At this point, we have a globally fixed-sized image coming out of our pipeline.
    The next thing we address is very important. Factors such as variable size of
    images and custom NumPy functions used to load images make it impossible for TensorFlow
    to infer the shape of its final tensor (though it’s a fixed-sized tensor) after
    a few steps. If you check the shapes of the tensors produced at this point, you
    will probably perceive them as
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This means that TensorFlow was unable to infer the shape of the tensors. To
    avoid any ambiguities or problems moving forward, we will set the shape of the
    output we have in the pipeline. For a tensor t, if the shape is ambiguous but
    you know the shape, you can set the shape manually using
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In our data pipeline, we can set the shape as
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We know that the outputs following the resize or crop are going to be
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '*Input image*—An RGB image with input_size height and width'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Target image*—A single-channel image with input_size height and width'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will set the shape accordingly using the tf.data.Dataset.map() function.
    We cannot underestimate the power of data augmentation, so we will introduce several
    data augmentation steps to our data pipeline (see the next listing).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.5 Functions used for random augmentation of images
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ Define a random variable.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Define a function to flip images deterministically.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Using the same pattern as before, we use tf.cond to randomly perform horizontal
    flipping.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Randomly flip images in the data set.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Randomly adjust the hue (i.e., color) of the input image (target stays the
    same).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Randomly adjust the brightness of the input image (target stays the same).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Randomly adjust the contrast of the input image (target stays the same).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'In listing 8.5, we perform the following translations:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Randomly flipping images horizontally
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Randomly changing the hue of the images (up to 10%)
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Randomly changing the brightness of the images (up to 10%)
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Randomly changing the contrast of the images (up to 20%)
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'By using the tf.data.Dataset.map() function, we can easily perform the specified
    random augmentation steps, should the user enable augmentation in the pipeline
    (i.e., by setting the augmentation variable to True). Note that we’re performing
    some augmentations (e.g., random hue, brightness, and contrast adjustments) on
    the input image only. We will also give the user the option to have different-sized
    inputs and targets (i.e., outputs). This is achieved by resizing the output to
    a desired size, defined by the output_size argument. The model we use for this
    task has different-sized input and output dimensions:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Again, here we use the nearest interpolation to resize the target. Next, we
    will shuffle the data (if the user set the shuffle argument to True):'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The shuffle function takes an important argument called buffer_size, which
    determines how many samples are loaded to memory in order to select a sample randomly.
    The higher the buffer_size, the more randomness you are introducing. On the other
    hand, a higher buffer_size implies higher memory consumption. It’s now time to
    batch the data, so instead of a single data point, we get a batch of data when
    we iterate:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This is done using the tf.data.Dataset.batch() function and passing the desired
    batch size as the argument. When using the tf.data pipeline, if you are running
    it for multiple epochs, you also need to use the tf.data.Dataset.repeat() function
    to repeat the pipeline for a given number of epochs.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Why do we need tf.data.Dataset.repeat()?
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: tf.data.Dataset is a generator. A unique characteristic of a generator is that
    you only can iterate it once. After the generator reaches the end of the sequence
    it’s iterating, it will exit by throwing an exception. Therefore, if you need
    to iterate through a generator multiple times, you need to redefine the generator
    as many times as needed. By adding tf.data.Dataset.repeat(epochs), the generate
    is redefined as many times as we would like (epochs times in this example).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'One more step is needed before our tf.data pipeline is done and dusted. If
    you look at the shape of the target (y) output, you will see that it has a channel
    dimension of 1\. However, for the loss function we will be using, we need to get
    rid of that dimension:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'For this, we will use the tf.squeeze() operation, which removes any dimensions
    that are of size 1 and returns a tensor. For example, if you squeeze a tensor
    of size [1,3,2,1,5], you will get a [3,2,5] sized tensor. The final code is provided
    in listing 8.6\. You might notice two steps that are highlighted. These are two
    popular optimization steps available: caching and prefetching.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.6 The final tf.data pipeline
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: ❶ If augmentation is enabled, resize_to_before_crop needs to be defined.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Return a list of filenames depending on the subset of data requested.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Load the images into memory. cache() is an optimization step and will be discussed
    in the text.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Normalize the input images.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: ❺ The function that randomly crops or resizes images
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Perform random crop or resize on the images.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Set the shape of the resulting images.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Randomly perform various augmentations on the data.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: ❾ Resize the output image if needed.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: ❿ Shuffle the data using a buffer.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: ⓫ Batch the data and repeat the process for a desired number of epochs.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: ⓬ This is an optimization step discussed in detail in the text.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: ⓭ Remove the unnecessary dimension from target images.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: ⓮ Get the final tf.data pipeline.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 'It wasn’t an easy journey, but it was a rewarding one. We have learned some
    important skills in defining the data pipeline:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Defining a generator that returns the filenames of the data to be fetched
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading images within a tf.data pipeline
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manipulating images (resizing, cropping, brightness adjustment, etc.)
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batching and repeating data
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining multiple pipelines for different data sets with different requirements
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will look at some optimization techniques to turn our mediocre data
    pipeline into an impressive data highway.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.1 Optimizing tf.data pipelines
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'TensorFlow is a framework meant for consuming large data sets, where consuming
    data in an efficient manner is a key priority. One thing still missing from our
    conversation is what kind of optimization steps are available for tf.data pipelines,
    so let us nudge this discussion in that direction. Two steps were set in bold
    in listing 8.6: caching and prefetching. If you are interested in other optimization
    techniques, you can read more at [https://www.tensorflow.org/guide/data_performance](https://www.tensorflow.org/guide/data_performance).'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 'Caching will store the data in memory as it flows through the pipeline. This
    means that, when cached, that step (e.g., loading the data from the disk) happens
    only in the first epoch. The subsequent epochs will read from the cached data
    that’s held in memory. Here, you can see that we’re caching the images after we
    load them to memory. This way, TensorFlow loads the images in the first epoch
    only:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Prefetching is another powerful weapon you have at your disposal, and it allows
    you to leverage the multiprocessing power of your device:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The argument provided to the function decides how much data is prefetched. By
    setting it to AUTOTUNE, TensorFlow will decide the best amount of data to be fetched
    depending on the resources available. Assume a simple data pipeline that loads
    images from the disk and trains a model. Then, the data read and model training
    will happen in interleaved steps. This leads to significant idling time, as the
    model idles while the data is loading, and vice versa.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: However, thanks to prefetching, this doesn’t need to be the case. Prefetching
    employs background threads and an internal buffer to load the data in advance
    while the model is training. When the next iteration comes, the model can seamlessly
    continue the training as data is already fetched into the memory. The differences
    between sequential execution and prefetching are shown in figure 8.7.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '![08-07](../../OEBPS/Images/08-07.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 Sequential execution versus pre-fetching-based execution in model
    training
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will look at the finished tf.data pipeline for the image segmentation
    problem.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.2 The final tf.data pipeline
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Finally, you can define the data pipeline(s) using the functions we have defined
    so far. Here, we define three different data pipelines for three different purposes:
    training, validation, and testing (see the following listing).'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.7 Creating the train/validation/test data pipelines instances
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: ❶ Directory where the input images are
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Directory where the annotated images (targets) are
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Directory where the text files containing train/validation/test filenames
    are
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Define a reusable partial function from get_subset_filenames.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Define three generators for train/validation/test data.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Define input image size.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Define a train data pipeline that uses data augmentation and shuffling.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Define a validation data pipeline that doesn’t use data augmentation or shuffling.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: ❾ Define a test data pipeline.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define several important paths:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: orig_dir—Directory containing input images
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: seg_dir—Directory containing the target images
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: subset_dir—Directory containing text files (train.txt, val.txt) that enlist
    training and validation instances, respectively
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then we will define a partial function from the get_subset_filenames() function
    we defined earlier so that we can get a generator just by setting the subset argument
    of the function. Using this technique, we will define three generators: train_subset_fn,
    val_subset_fn, and test_subset_fn. Finally, we will define three tf.data.Datasets
    using the get_subset_tf_dataset() function. Our pipelines will have the following
    characteristics:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '*Training pipeline*—Performs data augmentation and data shuffling on every
    epoch'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Validation pipeline and test pipeline*—No augmentation or shuffling'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The model we will define expects a 384 × 384-sized input and an output. In
    the training data pipeline, we will resize images to 444 × 444 and then randomly
    crop a 384 × 384-sized image. Following this, we will look at the core part of
    the solution: defining the image segmentation model.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 2
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 'You have been given a small set of data that contains two tensors: tensor a
    contains 100 64 × 64 × 3-sized images (i.e., 100 × 64 × 64 × 3 shaped), and tensor
    b contains 100 32 × 32 × 1-sized segmentation masks (i.e., 100 × 32 × 32 × 1 shaped).
    You have been asked to define a tf.data.Dataset using the functions discussed
    that will'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Resize the segmentation masks to match the input image size (using nearest interpolation)
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalize the input images using the transformation (x - 128)/255 where a single
    image is x
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batch the data to batches of 32 and repeat for five epochs
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prefetch the data with an auto-tuning feature
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '8.3 DeepLabv3: Using pretrained networks to segment images'
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It’s now time to create the brains of the pipeline: the deep learning model.
    Based on feedback from a colleague at a self-driving car company working on similar
    problems, you will implement a DeepLab v3 model. This is a model built on the
    back of a pretrained ResNet 50 model (trained on image classification) but with
    the last several layers changed to perform *atrous convolution* instead of standard
    convolution. It uses a pyramidal aggregation module that uses atrous convolution
    at different scales to generate image features at different scales to produce
    the final output. Finally, it uses a bilinear interpolation layer to resize the
    final output to a desired size. You are confident that DeepLab v3 can deliver
    good initial results.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep neural network-based segmentation models can be broadly categorized into
    two types:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Encoder decoder models (e.g., U-Net model)
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fully convolutional network (FCN) followed by a pyramidal aggregation module
    (e.g., DeepLab v3 model)
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A well-known example of the encoder-decoder model is the U-Net model. In other
    words, U-Net has an encoder that gradually creates smaller, coarser representations
    of the input. This is followed by a decoder that takes the representations the
    encoder built and gradually up-samples (i.e., increases the size of) the output
    until it reaches the size of the input image. The up-sampling is achieved through
    an operation known as *transpose convolution*. Finally, you train the whole structure
    end to end, where an input is the input image and the target is the segmentation
    mask for the corresponding image. We will not discuss this type of model in this
    chapter. However, I have included a detailed walkthrough in appendix B (along
    with an implementation of the model).
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: The other type of segmentation models introduces a special model that replaces
    the decoder. We call this module a *pyramidal aggregation module*. Its purpose
    is to garner spatial information at different scales (e.g., different-sized outputs
    from various interim convolution layers) that provides fine-grained contextual
    information about the objects present in the image. DeepLab v3 is a prime example
    of this approach. We will put the DeepLab v3 model under the microscope and use
    it to excel at the segmentation task.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Researchers and engineers gravitate toward methods that use pyramidal aggregation
    modules more. There could be many reasons for this. One lucrative reason is that
    there are less parameters in networks that use pyramidal aggregation than an encoder-decoder
    based counterpart. Another reason may be that, typically, introducing a novel
    module offers more flexibility (compared to an encoder-decoder) to engineer efficient
    and accurate feature extraction methods at multiple scales.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: How important is the pyramidal aggregation module? To know that, we have to
    first understand what the fully convolutional part of the network looks like.
    Figure 8.8 illustrates the generic structure of such a segmentation model.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '![08-08](../../OEBPS/Images/08-08.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 General structure and organization of a fully convolutional network
    that uses a pyramidal aggregation module
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: The best way to understand the importance of the pyramidal aggregation module
    is to see what happens if we don’t have it. If that is the case, then the last
    convolutional layer will have the enormous and unrealistic responsibility of building
    the final segmentation mask (which is typically 16-32x times larger than the layer
    output). It is no surprise that there is a massive representational bottleneck
    between the final convolution layer and the final segmentation mask, leading to
    poor performance. The pyramidal structure typically enforced in CNNs results in
    a very small output width and height in the final layer.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: The pyramidal aggregation module bridges this gap. It does so by combining several
    different interim outputs. This way, the network has ample fine-grained (from
    earlier layers) and coarser (from deeper layers) details to construct the desired
    segmentation mask. Fine-grained representations provide spatial/contextual information
    about the image, whereas the coarser representations provide high-level information
    about the image (e.g., what objects are present). By fusing both types of these
    representations, the task of generating the final output becomes more achievable.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Why not a skyscraper instead of a pyramid?
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: You might be tempted to ponder, if making the outputs smaller as you go causes
    loss of information, “Why not keep it the same size?” (hence the term *skyscraper*).
    This is an impractical solution for two main reasons.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: First, decreasing the size of the outputs through pooling or striding is an
    important regularization method that forces the network to learn translation-invariant
    features (as we discussed in chapter 6). By taking this away, we can hinder the
    generalizability of the network.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: Second, not decreasing the output size will increase the memory footprint of
    the model significantly. This will, in turn, restrict the depth of the network
    dramatically, making it more difficult to create deeper networks.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: DeepLab v3 is the golden child of a lineage of models that emerged from and
    was introduced in the paper “Rethinking Atrous Convolution for Semantic Image
    Segmentation” ([https://arxiv.org/pdf/1706.05587.pdf](https://arxiv.org/pdf/1706.05587.pdf))
    by several researchers from Google.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Most segmentation models face an adverse side effect caused by a common and
    beneficial design principle. Vision models incorporate stride/pooling to make
    network translation invariant. But an ill-favored outcome of that is the compounding
    reduction of the size of the outputs produced. This typically leads to a final
    output that is 16-32 times smaller than the input. Being a dense prediction task,
    image segmentation tasks suffer heavily from this design idea. Therefore, most
    of the groundbreaking networks that have surfaced have been about solving this.
    The DeepLab model came into the world for exactly that purpose. Let’s now see
    how DeepLab v3 solves this problem.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: DeepLab v3 uses a ResNet-50 ([https://arxiv.org/pdf/1512.03385.pdf](https://arxiv.org/pdf/1512.03385.pdf))
    pretrained on an ImageNet image classification data set as its backbone for extracting
    features of an image. It is one of the pioneering residual networks that made
    waves in the computer vision community a few years ago. DeepLab v3 introduces
    several architectural changes to the model to alleviate this issue. Furthermore,
    DeepLab v3 introduces a shiny new component called *atrous spatial pyramid pooling*
    (ASPP). We will discuss each of these in more detail in the coming sections.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.1 A quick overview of the ResNet-50 model
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The ResNet-50 model consists of several convolution blocks, followed by a global
    average pooling layer and a fully connected final prediction layer with softmax
    activation. The convolution block is the innovative part of the model. The original
    model has 16 convolution blocks organized into five groups. A single block consists
    of three convolution layers (1 × 1 convolution layer with stride 2, 3 × 3 convolution
    layer, and 1 × 1 convolution layer), batch normalization, and residual connections.
    We discussed residual connections in depth in chapter 7\. Next, we will discuss
    a core computation used throughout the model known as atrous convolution.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '8.3.2 Atrous convolution: Increasing the receptive field of convolution layers
    with holes'
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Compared to the standard ResNet-50, a major change that DeepLab v3 boasts is
    the use of atrous convolutions. Atrous (meaning “holes” in French) convolution,
    also known as dilated convolution, is a variant of the standard convolution. Atrous
    convolution works by inserting “holes” in between the convolution parameters.
    The increase in the receptive field is controlled by a parameter called *dilation
    rate*. A higher dilation rate means more holes between actual parameters in the
    convolution. A major benefit of atrous convolution is the ability to increase
    the size of the receptive field without compromising the parameter efficiency
    of a convolution layer.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '![08-09](../../OEBPS/Images/08-09.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
- en: Figure 8.9 Atrous convolution compared to standard convolution. Standard convolution
    is a special case of atrous convolution, where the rate is 1\. As you increase
    the dilation rate, the receptive field of the layer increases.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.9 shows how a large dilation rate leads to a larger receptive field.
    The number of shaded gray boxes represents the number of parameters, whereas the
    dashed, lightly shaded box represents the size of the receptive field. As you
    can see, the number of parameters stays constant, while the receptive field increases.
    Computationally, it is quite straightforward to extend standard convolution to
    atrous convolution. All you need to do is insert zeros for the holes in the atrous
    convolution operation.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Wait! How does atrous convolution help segmentation models?
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed, the main issue presented by the pyramidal structure of CNNs
    is that the output gets gradually smaller. The easiest solution, leaving the learned
    parameters untouched, is to reduce the stride of the layers. Though technically
    that will increase output size, conceptually there is a problem.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: To understand it, assume the i^(th) layer of a CNN has a stride of 2 and gets
    a h × w-sized input. Then the i+1^(th) layer gets a h/2 × w/2-sized input. By
    removing the stride of the i^(th) layer, it gets a h × w-sized output. However,
    the kernel of the i+1^(th) layer has been trained to see a smaller output, so
    by increasing the size of the input, we are disrupting (or reducing) the receptive
    field of the layer. By introducing atrous convolution, we compensate for that
    reduction of the receptive field.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now see how the ResNet-50 is repurposed for image segmentation. First,
    we download it from the tf.keras.applications module. The architecture of the
    ResNet-50 model has the following format. To start, it has a stride 2 convolution
    layer and a stride 2 pooling layer. After that, it has sequence of convolution
    blocks and finally an average pooling layer and fully connected output layer.
    These convolution blocks have a hierarchical organization of convolution layers.
    Each convolution block consists of several subblocks, which consist of three convolution
    layers (i.e., a 1 × 1 convolution, a 3 × 3 convolution, and a 1 × 1 convolution)
    along with batch normalization.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.3 Implementing DeepLab v3 using the Keras functional API
  id: totrans-296
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The network starting from the input up to the conv4 block remains unchanged.
    Following the notation from the original ResNet paper, these blocks are identified
    as conv2, conv3, and conv4 block groups. Our first task is to create a model containing
    the input layer up to the conv4 block of the original ResNet-50 model. After that,
    we will focus on recreating the final convolution block (i.e., conv5) as per the
    DeepLab v3 paper:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: As shown here, we find the last layer in the ResNet-50 model just before the
    "conv5_ block1_1_conv", which would be the last layer of the conv4 block group.
    With that, we can define a makeshift model that contains layers from the input
    to the final output of the conv4 block group. Later, we will focus on augmenting
    this model by introducing modifications and novel components from the paper. We
    will redefine the conv5 block with dilated convolutions. To do this, we need to
    understand the composition of a ResNet block (figure 8.10). We can assume it has
    three different levels.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '![08-10](../../OEBPS/Images/08-10.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10 Anatomy of a convolution block in ResNet-50\. For this example,
    we show the very first convolution block of ResNet-50\. The organization of a
    convolution block group consists of three different levels.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now implement a function to represent each level while using dilated convolution.
    In order to convert a standard convolution layer to a dilated convolution, we
    just have to pass in the desired rate to the dilation_rate parameter in the tf.keras.layers.Conv2D
    layer. First, we will implement a function that represents a level 3 block, as
    shown in the following listing.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.8 A level 3 convolution block in ResNet-50
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: ❶ Here, inp takes a 4D input having shape [batch size, height, width, channels].
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Perform 2D convolution on the input with a given number of filters, kernel_size,
    and dilation rate.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Perform batch normalization on the output of the convolution layer.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Apply ReLU activation if activation is set to True.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Return the output without an activation if activation is set to False.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: A level 3 block has a single convolution layer with a desired dilation rate
    and a batch normalization layer followed by a nonlinear ReLU activation layer.
    Next, we will write a function for the level 2 block (see the next listing).
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.9 A level 2 convolution block in ResNet-50
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'A level 2 block consists of three level 3 blocks with a given dilation rate
    that have convolution layers with the following specifications:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: 1 × 1 convolution layer having 512 filters and a desired dilation rate
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3 × 3 convolution layer having 512 filters and a desired dilation rate
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 × 1 convolution layer having 2048 filters and a desired dilation rate
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apart from using atrous convolution, this is identical to a level 2 block of
    the original conv5 block in the ResNet-50 model. With all the building blocks
    ready, we can implement the fully fledged conv5 block with atrous convolution
    (see the next listing).
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.10 Implementing the final ResNet-50 convolution block group (level
    1)
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: ❶ Create a level 3 block (block0) to create residual connections for the first
    block.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Define the first level 2 block, which has a dilation rate of 2 (block1).
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Create a residual connection from block0 to block1.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Apply ReLU activation to the result.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: ❺ The second level 2 block with a dilation rate of 2 (block2)
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Create a residual connection from block1 to block2.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Apply ReLU activation.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Apply a similar procedure to block1 and block2 to create block3.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: 'There’s no black magic here. The function resnet_block lays the outputs of
    the functions we already discussed to assemble the final convolution block. Particularly,
    it has three level 2 blocks with residual connections going from the previous
    block to the next. Finally, we can get the final output of the conv5 block with
    a dilation rate of 2 by calling the resnet_block function with the output of the
    interim model (resnet50_ upto_conv4) we defined as the input and a dilation rate
    of 2:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 8.3.4 Implementing the atrous spatial pyramid pooling module
  id: totrans-330
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here, we will discuss the most exciting innovation of the DeepLab v3 model.
    The atrous spatial pyramid pooling (ASPP) module serves two purposes:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: Aggregates multiscale information about an image, obtained through outputs produced
    using different dilation rates
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combines highly summarized information obtained through global average pooling
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ASPP module gathers multiscale information by performing different convolutions
    on the last ResNet-50 output. Specifically, the ASPP module performs 1 × 1 convolution,
    3 × 3 convolution (r = 6), 3 × 3 convolution (r = 12), and 3 × 3 convolution (r
    = 18), where r is the dilation rate. All of these convolutions have 256 output
    channels and are implemented as level 3 blocks (provided by the function block_level3()).
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: ASRP captures high-level information by performing global average pooling, followed
    by a 1 × 1 convolution with 256 output channels to match the output size of multiscale
    outputs, and finally a bilinear up-sampling layer to up-sample the height and
    width dimensions shrunk by the global average pooling. Remember that bilinear
    interpolation up-samples the images by computing the resulting pixels as an average
    of neighboring pixels. Figure 8.11 illustrates the ASPP module.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '![08-11](../../OEBPS/Images/08-11.png)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
- en: Figure 8.11 The ASPP module used in the DeepLab v3 model
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: The job of the ASPP module can be summarized as a concise function. We have
    all the tools we need to implement this function from the previous work we have
    done (see the following listing).
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.11 Implementing ASPP
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: ❶ Define a 1 × 1 convolution.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Define a 3 x 3 convolution with 256 filters and a dilation rate of 6.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Define a 3 x 3 convolution with 256 filters and a dilation rate of 12.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Define a 3 x 3 convolution with 256 filters and a dilation rate of 18.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Define a global average pooling layer.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Define a 1 × 1 convolution with 256 filters.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Up-sample the output using bilinear interpolation.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Concatenate all the outputs.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: ❾ Create an instance of ASPP.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: 'The ASPP module consists of four level 3 blocks, as outlined in the code. The
    first block comprises a 1 × 1 convolution with 256 filters without dilation (this
    produces outa_1_conv). The latter three blocks consist of 3 × 3 convolutions with
    256 filters but with varying dilation rates (i.e., 6, 12, 18; they produce outa_2_conv,
    outa_3_conv, and outa_4_conv, respectively). This covers aggregating features
    from the image at multiple scales. However, we also need to preserve the global
    information about the image, similar to a global average pooling layer (outb_1_avg).
    This is achieved through a lambda layer that averages the input over the height
    and width dimensions:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The output of the averaging is then followed by a 1 × 1 convolution filter
    with 256 filters. Then, to bring the output to the same size as previous outputs,
    an up-sampling layer that uses bilinear interpolation is used (this produces outb_1_up):'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Finally, all these outputs are concatenated to a single output using a Concatenate
    layer to produce the final output out_aspp.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.5 Putting everything together
  id: totrans-355
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now it’s time to collate all the different components to create one majestic
    segmentation model. The next listing outlines the steps required to build the
    final model.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.12 The final DeepLab v3 model
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: ❶ Define the RGB input layer.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Download and define the resnet50.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Get the output of the last layer we’re interested in.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Define an interim model from the input up to the last layer of the conv4 block.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Define the removed conv5 resnet block.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Define the ASPP module.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Define the final output.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Define the final model.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: Note how the model has a linear layer that does not have any activation present
    (e.g., sigmoid or softmax). This is because we are planning to use a special loss
    function that uses logits (unnormalized scores obtained from the last layer before
    applying softmax) instead of normalized probability scores. Due to that, we will
    keep the last layer a linear output with no activation.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: 'We have one final housekeeping step to perform: copying the weights from the
    original conv5 block to the newly created conv5 block in our model. To do that,
    first we need to store the weights from the original model as follows:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: We cannot copy the weights to the new model until we compile the model, as weights
    are not initialized until the model is compiled. Before we do that, we need to
    learn loss functions and evaluation metrics that are used in segmentation tasks.
    To do that, we will need to implement custom loss functions and metrics and use
    them to compile the model. This will be discussed in the next section.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 3
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: 'You want to create a new pyramidal aggregation module called aug-ASPP. The
    idea is similar to the ASPP module we implemented earlier, but with a few differences.
    Let’s say you have been given two interim outputs from the model: out_1 and out_2
    (same size). You have to write a function, aug_aspp, that will take these two
    outputs and do the following:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: Perform atrous convolution with r = 16, 128 filters, 3 × 3 convolution, stride
    1, and ReLU activation on out_1 (output will be called atrous_out_1)
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform atrous convolution with r = 8, 128 filters, 3 × 3 convolution, stride
    1, and ReLU activation on both out_1 and out_2 (output will be called atrous_
    out_2_1 and atrous_out_2_2)
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concatenate atrous_out_2_1 and atrous_out_2_2 (output will be called atrous_out_2)
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply 1 × 1 convolution with 64 filters to both atrous_out_1 and atrous_out_2
    and concatenate (output will be called conv_out)
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use bilinear up-sampling to double the size of conv_out (on height and width
    dimensions) and apply sigmoid activation
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '8.4 Compiling the model: Loss functions and evaluation metrics in image segmentation'
  id: totrans-378
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to finalize the DeepLab v3 model (built using mostly the ResNet-50
    structure and the ASPP module), we have to define a suitable loss function and
    metrics to measure the performance of the model. Image segmentation is quite different
    from image classification tasks, so the loss function and metrics don’t necessarily
    translate to the segmentation problem. One key difference is that there is typically
    a large class imbalance in segmentation data, as a “background” class typically
    dominates an image compared to other object-related pixels. To get started, you
    read a few blog posts and research papers and identify weighted categorical cross-entropy
    loss and dice loss as good candidates. You focus on three different metrics: pixel
    accuracy, mean (class-weighted) accuracy, and mean IoU.'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: Loss functions and evaluation metrics used in image segmentation models are
    different from what is used in image classifiers. To start, image classifiers
    take in a single class label for a single image, whereas a segmentation model
    predicts a class for every single pixel in the image. This highlights the necessity
    of not only reimagining existing loss functions and metrics, but also inventing
    new losses and evaluation metrics that are more appropriate for the output produced
    by segmentation models. We will first discuss loss functions and then metrics.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.1 Loss functions
  id: totrans-381
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A *loss function* is what is used to optimize the model whose purpose is to
    find the parameters that minimize a defined loss. A loss function used in a deep
    network must be *differentiable*, as the minimization of the loss happens with
    the help of gradients. The loss functions we’ll use comprise two loss functions:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: Cross-entropy loss
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dice loss
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross-entropy loss
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: Cross-entropy loss is one of the most common losses used in segmentation tasks
    and can be implemented with just one line in Keras. We already used cross-entropy
    loss quite a few times but didn’t analyze it in detail. However, it is worthwhile
    to review the underpinning mechanics that govern cross-entropy loss.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: Cross-entropy loss takes in a predicted target and a true target. Both these
    tensors are of shape [batch size, height, width, object classes]. The object class
    dimension is a one-hot encoded representation of which object class a given pixel
    belongs to. The cross-entropy loss is then computed for every pixel independently
    using
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '![08_11a](../../OEBPS/Images/08_11a.png)'
  id: totrans-388
  prefs: []
  type: TYPE_IMG
- en: where *CE*(*i, j*) represents the cross-entropy loss for pixel at position (*i,
    j*) on the image, *c* is the number of classes, and *y*[k] and *ŷ*[k] represent
    the elements in the one-hot encoded vector and the predicted probability distribution
    over classes of that pixel. This is then summed across all the pixels to get the
    final loss.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: Beneath the simplicity of the method, a critical issue lurks. Class imbalance
    is almost certain to rear its ugly head in image segmentation problems. You will
    find hardly any real-world images where each object occupies an equal area in
    the image. The good news is it is not very difficult to deal with this issue.
    This can be mitigated by assigning a weight for each pixel in the image, depending
    on the dominance of the class it represents. Pixels belonging to large objects
    will have smaller weights, whereas pixels belonging to smaller objects will have
    larger weights, providing an equal say despite the size in the final loss. The
    next listing shows how to do this in TensorFlow.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.13 Computing the label weights for a given batch of data
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: ❶ Get the total pixels per-class in y_true.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Get the total pixels in y_true.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Compute the weights per-class. Rarer classes get more weight.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Reshape y_true to a [batch size, height*width]-sized tensor.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Create a weight vector by gathering the weights corresponding to indices in
    y_true.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Make y_weights a vector.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: Here, for a given batch, we compute the weights as a sequence/vector that has
    a number of elements equal to y_true. First, we get the total number of pixels
    for each class by computing the sum over the width and height of the one-hot encoded
    y_true (i.e., has dimensions batch, height, width, and class). Here, a class that
    has a value larger than num_classes will be ignored. Next, we compute the total
    number of pixels per sample by taking the sum over the class dimension resulting
    in *tot* (a [batch size, 1]-sized tensor). Now the weights can be computed per
    sample and per class using
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: '![08_11b](../../OEBPS/Images/08_11b.png)'
  id: totrans-400
  prefs: []
  type: TYPE_IMG
- en: 'where *n* is the total number of pixels and *n*^i is the total number of pixels
    belonging to the *i*^(th) class. After that, we reshape y_true to shape [batch
    size, -1] as preparation for an important step in weight computation. As the final
    output, we want to create a tensor out of weights, where we gather elements from
    the y_weights that correspond to elements in y_true. In other words, we fetch
    the value from y_weights, where the index to fetch is given by the values in y_true.
    At the end, the result will be of the same shape and size as y_true. This is all
    we need to weigh the samples: multiply weights element-wise with the loss value
    for each pixel. To achieve this, we will use the function tf.gather(), which gathers
    the elements from a given tensor (params) while taking a tensor that represents
    indices (indices) and returns a tensor that is of the same shape as the indices:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Here, to ignore the batch dimension during performing the gather, we pass the
    argument batch_dims indicating how many batch dimensions we have. With that, we
    will define a function that outputs the weighted cross-entropy loss given a batch
    of predicted and true targets.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: With the weights ready, we can now implement our first segmentation loss function.
    We will implement weighted cross-entropy loss. At a glance, the function masks
    irrelevant pixels (e.g., pixels belonging to unknown objects) and unwraps the
    predicted and true labels to get rid of the height and width dimensions. Finally,
    we can compute the cross-entropy loss using the built-in function in TensorFlow
    (see the next listing).
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.14 Implementing the weighted cross-entropy loss
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: ❶ Define the valid mask, masking unnecessary pixels.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Some initial setup that casts y_true to int and sets the shape
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Get the label weights.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Unwrap y_pred and y_true so that batch, height, and width dimensions are squashed.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Compute the cross-entropy loss with y_true, y_pred, and the mask.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Return the function that computes the loss.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: You might be thinking, “Why is the loss defined as a nested function?” This
    is a standard pattern we have to follow if we need to include extra arguments
    to our loss function (i.e., num_classes). All we are doing is capturing the computations
    of the loss function in the loss_fn function and then creating an outer function
    ce_weighted_from_logits() that will return the function that encapsulates the
    loss computations (i.e., loss_fn).
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, a valid mask is created to indicate whether the labels in y_true
    are less than the number of classes. Any label that has a value larger than the
    number of classes is ignored (e.g., unknown objects). Next, we get the weight
    vector and indicate a weight for each pixel using the get_label_weights() function.
    We will unwrap y_pred to a [-1, num_classes]-sized tensor, as y_pred contains
    *logits* (i.e., unnormalized probability scores output by the model) across all
    classes in the data set. y_true will be unwrapped to a vector (i.e., a single
    dimension), as y_true only contains the class label. Finally, we use tf.nn.sparse_softmax_cross_entropy_with_logits()
    to compute the loss over masked predicted and true targets. The function takes
    two arguments, labels and logits, which are self-explanatory. We can make two
    salient observations:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: We are computing sparse cross-entropy loss (i.e., not standard cross-entropy
    loss).
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are computing cross-entropy loss from logits.
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When using sparse cross entropy, we don’t have to one-hot encode the labels,
    so we can skip this, which leads to a more memory-efficient data pipeline. This
    is because one-hot encoding is handled internally by the model. By using a sparse
    loss, we have less to worry about.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: Computing the loss from logits (i.e., unnormalized scores) instead of from normalized
    probabilities leads to better and more stable gradients. Therefore, whenever possible,
    make sure to use logits instead of normalized probabilities.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: Dice loss
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: The second loss we will discuss is called the *dice loss*, which is computed
    as
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: '![08_11c](../../OEBPS/Images/08_11c.png)'
  id: totrans-421
  prefs: []
  type: TYPE_IMG
- en: 'Here, the intersection between the prediction and target tensors can be computed
    with element-wise multiplication, whereas the union can be computed using element-wise
    addition between the prediction and the target tensors. You might be thinking
    that using element-wise operations is a strange way to compute intersection and
    union. To understand the reason behind this, I want to refer to a statement made
    earlier: a loss function used in a deep network must be *differentiable*.'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: This means that we cannot use the standard conventions we use to compute intersection
    and union from a given set of values. Rather, we need to resort to a differentiable
    computation, leading to intersection and union between two tensors. Intersection
    can be computed by taking element-wise multiplication between the predicted and
    true targets. Union can be computed by taking the element-wise addition between
    the predicted and true targets. Figure 8.12 clarifies how these operations lead
    to intersection and union between two tensors.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: '![08-12](../../OEBPS/Images/08-12.png)'
  id: totrans-424
  prefs: []
  type: TYPE_IMG
- en: Figure 8.12 Computations involved in dice loss. The intersection can be computed
    as a differentiable function by taking element-wise multiplication, whereas union
    can be computed as the element-wise sum.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: This loss is predominantly focused on maximizing the intersection between the
    predicted and true targets. The multiplier of 2 is used to balance out the duplication
    of values that comes from the overlap between the intersection and the union,
    found in the denominator (see the following listing).
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.15 Implementing the dice loss
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: ❶ Initial setup for y_true
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Get the label weights and reshape it to a [-1, 1] shape.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Apply softmax on y_pred to get normalized probabilities.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Unwrap y_pred and one-hot-encoded y_true to the [-1, num_classes] shape.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Compute intersection using element-wise multiplication.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Compute union using element-wise addition.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Compute the dice coefficient.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Compute the dice loss.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, smooth is a smoothing parameter that we’ll use to avoid potential NaN
    values resulting in division by zero. After that we do the following:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: Obtain weights for each y_true label
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply a softmax activation to y_pred
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unwrap y_pred to the [-1, num_classes] tensor and y_true to a [-1]-sized vector
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then intersection and union are computed for y_pred and y_true. Specifically,
    intersection is computed as the result of element-wise multiplication of y_pred
    and y_true and the union as the result of the element-wise addition of y_pred
    and y_true.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: Focal loss
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: '*Focal loss* is a relatively novel loss introduced in the paper “Focal Loss
    for Dense Object Prediction” ([https://arxiv.org/pdf/1708.02002.pdf](https://arxiv.org/pdf/1708.02002.pdf)).
    Focal loss was introduced to combat the severe class imbalance found in segmentation
    tasks. Specifically, it solves a problem in many easy examples (e.g., samples
    from common classes with smaller loss), over-powering small numbers of hard examples
    (e.g., samples from rare classes with larger loss). Focal loss solves this problem
    by introducing a modulating factor that will down-weight easy examples, so, naturally,
    the loss function focuses more on learning hard examples.'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: The loss function we will use to optimize the segmentation model will be the
    loss resulting from addition of sparse cross-entropy loss and dice loss (see the
    next listing).
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.16 Final combined loss function
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Next, we will discuss evaluation metrics.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.2 Evaluation metrics
  id: totrans-448
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Evaluation metrics play a vital role in model training as a health check for
    the model. This means low performance/issues can be quickly identified by making
    sure evaluation metrics behave in a reasonable way. Here we will discuss three
    different metrics:'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: Pixel
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean accuracy
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean IoU
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will implement these as custom metrics by leveraging some of the existing
    metrics in TensorFlow, where you have to subclass from the tf.keras.metrics.Metric
    class or one of the existing metrics. This means that you create a new Python
    class, which inherits from the base tf.keras.metrics.Metric base class of one
    of the existing concrete metrics classes:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The first thing you need to understand about a metric is that it is a stateful
    object, meaning it maintains a state. For example, a single epoch has multiple
    iterations and assumes you’re interested in computing the accuracy. The metric
    needs to accumulate the values required to compute the accuracy over all the iterations
    so that at the end, it can compute the average accuracy for that epoch. When defining
    a metric, there are three functions you need to be mindful of: __init__, update_state,
    result, and reset_states.'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s get concrete by assuming that we are implementing an accuracy metric
    (i.e., the number of elements in y_pred that matched y_true as a percentage).
    It needs to maintain a total: the sum of all the accuracy values we passed and
    the count (number of accuracy values we passed). With these two state elements,
    we can compute the mean accuracy at any time. When implementing the accuracy metric,
    you implement these functions:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: __init__—Defines two states; total and count
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: update_state—Updates total and count based on y_true and y_pred
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: result—Computes the mean accuracy as total/count
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: reset_states—Resets both the total and count (this needs to happen at the beginning
    of an epoch)
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s see how this knowledge translates to the evaluation metrics we’re interested
    in solving.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: Pixel and mean accuracies
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: Pixel accuracy is the simplest metric you can think of. It measures the pixel-wise
    accuracy between the prediction and the true target (see the next listing).
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.17 Implementing the pixel accuracy metric
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: ❶ Set the shape of y_true (in case it is undefined).
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Reshape y_true to a vector.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Reshape y_pred after taking argmax to a vector.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Define a valid mask (mask out unnecessary pixels).
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Gather pixels/labels that satisfy the valid_mask condition.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: ❻ With the processed y_true and y_pred, compute the accuracy using the update_state()
    function.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: 'Pixel accuracy computes the one-to-one match between predicted pixels and true
    pixels. To compute this, we subclass from tf.keras.metrics.Accuracy as it has
    all the computations we need. To do this, we override the update_state function
    as shown. There are a few things we need to take care of:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: We need to set the shape of y_true as a precaution. This is because when working
    with tf.data.Dataset, sometimes the shape is lost.
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reshape y_true to a vector.
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get the class labels of y_pred by performing tf.argmax() and reshape it to a
    vector.
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define a valid mask that ignores unwanted classes (e.g., unknown objects).
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get the pixels that satisfy only the valid_mask filter.
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once we complete these tasks, we simply call the parent object’s (i.e., tf.keras
    .metrics.Accuracy) update_state method with the corresponding arguments. We don’t
    have to override result() and reset_states() functions, as they already contain
    the correct computations.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
- en: We said that class imbalance is prevalent in image segmentation problems. Typically,
    background pixels will spread in a large region of the image, potentially leading
    to misguided conclusions. Therefore, a slightly better approach might be to compute
    the accuracy individually per class and then average it. Enter mean accuracy,
    which prevents the undesired characteristics of pixel accuracy (see the next listing).
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.18 Implementing the mean accuracy metric
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-481
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: ❶ Initial setup
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Compute the confusion matrix using y_true and y_pred.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Get the true positives (elements on the diagonal).
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Compute the mean accuracy using true positives and true class counts for each
    class.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Compute the average of mean_accuracy using the update_state() function.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: The MeanAccuracyMetric will branch out from tf.keras.metrics.Mean, which computes
    the average over a given sequence of values. The plan is to compute the mean_accuracy
    within the update_state() function and then pass the value to the parent’s update_state()
    function so that we get the average value of mean accuracy. First, we perform
    the initial setup and clean-up of y_true and y_pred we discussed earlier.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
- en: '![08-13](../../OEBPS/Images/08-13.png)'
  id: totrans-488
  prefs: []
  type: TYPE_IMG
- en: Figure 8.13 Illustration of a confusion matrix for a five-class classification
    problem. The shaded boxes represent true positives.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: 'Afterward, we compute the confusion matrix (figure 8.13) from predicted and
    true targets. A confusion matrix for an *n*-way classification problem (i.e.,
    a classification problem with *n* possible classes) is defined as a *n* × *n*
    matrix. Here, the element at the (*i*, *j*) position indicates how many instances
    were predicted as belonging to the *i* ^(th) class but actually belong to the
    *j* ^(th) class. Figure 8.13 portrays this type of confusion matrix. We can get
    the true positives by extracting the diagonal (i.e., (*i*, *i*) elements in the
    matrix for all 1 < = *i* < = *n*). We can now compute the mean accuracy in two
    steps:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: Perform element-wise division on the true positive count by actual counts for
    all the classes. This produces a vector whose elements represents per-class accuracy.
  id: totrans-491
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the vector mean that resulted from step 1.
  id: totrans-492
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we pass the mean accuracy to its parent’s update_state() function.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: Mean IoU
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
- en: '*Mean IoU* (mean intersection over union) is a popular evaluation metric pick
    for segmentation tasks and has close ties to the dice loss we discussed earlier,
    as they both use the concept of intersection and union to compute the final result
    (see the next listing).'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.19 Implementing the mean IoU metric
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-497
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: ❶ After the initial setup of y_true and y_pred, all we need to do is call the
    parent’s update_state() function.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
- en: The mean IoU computations are already found in tf.keras.metrics.MeanIoU. Therefore,
    we will use that as our parent class. All we need to do is perform the aforementioned
    setup for y_true and y_pred and then call the parent’s update_state() function.
    Mean IoU is computed as
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
- en: '![08_13a](../../OEBPS/Images/08_13a.png)'
  id: totrans-500
  prefs: []
  type: TYPE_IMG
- en: Various elements used in this computation are depicted in figure 8.14.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
- en: '![08-14](../../OEBPS/Images/08-14.png)'
  id: totrans-502
  prefs: []
  type: TYPE_IMG
- en: Figure 8.14 Confusion matrix and how it can be used to compute false positives,
    false negatives, and true positives
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
- en: 'We now understand the loss functions and evaluation metrics that are available
    to us and have already implemented them. We can incorporate these losses to compile
    the model:'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-505
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Remember that we stored the weights from a convolution block we removed earlier.
    Now that we have compiled the model, we can copy the weights to the new model
    using the following syntax:'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-507
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: We now move on to training the model with the data pipeline and the model we
    defined.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 4
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
- en: You are coming up with a new loss function that computes the disjunctive union
    between y_true and y_pred. The disjunctive union between two sets A and B is the
    set of elements that are in either A or B but not in the intersection. You know
    you can compute the intersection with element-wise multiplication and union with
    element-wise addition of y_true and y_pred. Write the equation to compute the
    disjunctive union as a function of y_true and y_pred.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
- en: 8.5 Training the model
  id: totrans-511
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You’re coming to the final stages of the first iteration of your product. Now
    it’s time to put the data and knowledge you garnered to good use (i.e., train
    the model). We will train the model for 25 epochs and monitor the pixel accuracy,
    mean accuracy, and mean IoU metrics. During the training, we will measure the
    performance on validation data set.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
- en: Training the model is the easiest part, as we have done the hard work that leads
    up to training. It is now just a matter of calling fit() with the correct parameters
    on the DeepLab v3 we just defined, as the following listing shows.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.20 Training the model
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-515
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: ❶ Train logger
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Set the mode for the following callbacks automatically by looking at the metric
    name.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Learning rate scheduler
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Early stopping callback
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Train the model while using the validation set for learning rate adaptation
    and early stopping.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will define a directory called eval if it does not exist. The training
    logs will be saved in this directory. Next, we define three different callbacks
    to be used during the training:'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
- en: csv_logger—Logs the training loss/metrics and validation loss/metrics
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: lr_callback—Reduces the learning rate by a factor of 10, if the validation loss
    does not decrease within three epochs
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: es_callback—Performs early stopping if the validation loss does not decrease
    within six epochs
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NOTE On an Intel Core i5 machine with an NVIDIA GeForce RTX 2070 8GB, the training
    took approximately 45 minutes to run 25 epochs.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
- en: 'With that, we call deeplabv3.fit() with the following parameters:'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
- en: x—The tf.data pipeline producing training instances (set to tr_image_ds).
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: steps_per_epoch—Number of steps per epoch. This is obtained by computing the
    number of training instances and dividing it by the batch size (set to n_train).
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: validation_data—The tf.data pipeline producing validation instances. This is
    obtained by computing the number of validation instances and dividing it by the
    batch size (set to val_image_ds).
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: epochs—Number of epochs (set to epochs).
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: callbacks—The callbacks we set up earlier (set to [lr_callback, csv_logger,
    es_callback]).
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After the model is trained, we will evaluate it on the test set. We will also
    visualize segmentations generated by the model.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 5
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
- en: You have a data set of 10,000 samples and have split it into 90% training data
    and 10% validation data. You use a batch size of 10 for training and a batch size
    of 20 for validation. How many training and validation steps will be there in
    a single epoch?
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
- en: 8.6 Evaluating the model
  id: totrans-535
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s take a moment to reflect on what we have done so far. We defined a data
    pipeline to read images and prepare them as inputs and targets for the model.
    Then we defined a model known as DeepLab v3 that uses a pretrained ResNet-50 as
    its backbone and a special module called atrous spatial pyramid pooling to predict
    the final segmentation mask. Then we defined task-specific losses and metrics
    to make sure we could evaluate the model with a variety of metrics. Afterward,
    we trained the model. Now it’s time for the ultimate reveal. We will measure the
    performance on an unseen test data set to see how well the model does. We will
    also visualize the model outputs and compare them against the real targets by
    plotting them side by side.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
- en: 'We can run the model over the unseen test images and gauge how well it is performing.
    To do that, we execute the following:'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-538
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: The size of the test set is the same as the validation set, as we split the
    images listed in val.txt into two equal validation and test sets. This will return
    around
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
- en: 62% mean IoU
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 87% mean accuracy
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 91% pixel accuracy
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are very respectable scores given our circumstances. Our training data
    set consists of less than 1,500 segmented images. Using this data, we were able
    to train a model that achieves around 62% mean IoU on a test data set of approximately
    size 725.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
- en: What does state of the art look like?
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
- en: The state-of-the-art performance on Pascal VOC 2012 reports around 90% mean
    IoU ([http://mng.bz/o2m2](http://mng.bz/o2m2)). However, these are models that
    are much larger and complex than what we used here. Furthermore, they are typically
    trained with significantly more data by using an auxiliary data set known as the
    semantic boundary data set (SBD) (introduced in the paper [http://mng.bz/nNve](http://mng.bz/nNve)).
    This will push the training datapoint count to over 10,000 (close to seven times
    the size of our current training set).
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
- en: You can further investigate the model by visually inspecting some of the results
    our module produces. After all, it is a vision model that we are developing. Therefore,
    we should not rely solely on numbers to make decisions and conclusions. We should
    also visually analyze the results before settling on a conclusion.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
- en: What would the results from a U-Net based network look like?
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
- en: Under similar conditions provided for the DeepLab v3 model, the U-Net model
    built with a pretrained ResNet-50 model as the encoder was only able to achieve
    approximately 32.5% mean IoU, 78.5% mean accuracy, and 81.5% pixel accuracy. The
    implementation is provided in the Jupyter notebook in the ch08 folder.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
- en: On an Intel Core i5 machine with an NVIDIA GeForce RTX 2070 8GB, the training
    took approximately 55 minutes to run 25 epochs.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
- en: There is a detailed explanation of the U-Net model in appendix B.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
- en: To complete this investigation, we will get a random sample from the test set
    and ask the model to predict the segmentation map for each of those images. Then
    we will plot the results side by side to ensure that our model is doing a good
    job (figure 8.15).
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
- en: '![08-15](../../OEBPS/Images/08-15.png)'
  id: totrans-552
  prefs: []
  type: TYPE_IMG
- en: Figure 8.15 Comparing the true annotated targets to model predictions. You can
    see that the model is quite good at separating objects from different backgrounds.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
- en: We can see that unless it is an extremely difficult image (e.g., the top-left
    image, where there’s a car obscured by a gate), our model does a very good job.
    It can identify almost all the images found in the sample we analyzed with high
    accuracy. The code for visualizing the images is provided in the notebook.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our discussion of image segmentation. In the next few chapters,
    we will discuss several natural language processing problems.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 6
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
- en: You are given
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
- en: A model (called model)
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A batch of images called batch_image (already preprocessed and ready to be fed
    to a model)
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A corresponding batch of targets, batch_targets (the true segmentation mask
    in one-hot encoded format)
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write a function called get_top_bad_examples(model, batch_images, batch_targets,
    n) that will return the top n indices of the hardest (highest loss) images in
    batch_ images. Given a predicted mask and a target mask, you can use the sum over
    element-wise multiplication as the loss of a given image.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
- en: You can use the model.predict() function to make a prediction on batch_ images,
    and it will return a predicted mask as the same size as batch_targets. Once you
    compute the losses for the batch (batch_loss), you can use the tf.math.top_k(batch_
    loss, n) function to get the indices of elements with the highest value. tf.math
    .top_k() returns a tuple containing the top values and indices of a given vector,
    in that order.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-563
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Segmentation models fall into two broad categories: semantic segmentation and
    instance segmentation.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The tf.data API provides various functionality to implement complex data pipelines,
    such as using custom NumPy functions, performing quick transformations using tf.data.Dataset.map(),
    and I/O optimization techniques like prefetch and cache.
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DeepLab v3 is a popular segmentation model that uses a pretrained ResNet-50
    model as its backbone and atrous convolutions to increase the receptive field
    by inserting holes (i.e., zeros) between the kernel weights.
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The DeepLab v3 model uses a module known as atrous spatial pyramid pooling to
    aggregate information at multiple scales, which helps to create a fine-grained
    segmented output.
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In segmentation tasks, cross entropy and dice loss are two popular losses, whereas
    pixel accuracy, mean accuracy, and mean IoU are popular evaluation metrics.
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In TensorFlow, loss functions can be implemented as stateless functions. But
    metrics must be implemented as stateful objects by subclassing from the tf.keras.metrics.Metric
    base class or a suitable class.
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The DeepLab v3 model achieved a very good accuracy of 62% mean IoU on the Pascal
    VOC 2010 data set.
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Answers to exercises
  id: totrans-571
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Exercise 1**'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-573
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '**Exercise 2**'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-575
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '**Exercise 3**'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-577
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '**Exercise 4**'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-579
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '**Exercise 5**'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 6**'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-582
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
