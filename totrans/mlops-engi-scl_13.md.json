["```py\npip install pytorch_lightning\n```", "```py\nimport torch as pt\nimport pytorch_lightning as pl                                               ❶\n\npt.set_default_dtype(pt.float64)                                             ❷\n\nclass DcTaxiModel(pl.LightningModule):                                       ❸\n  def __init__(self, **kwargs):                                              ❹\n    super().__init__()                                                       ❺\n    self.save_hyperparameters()                                              ❻\n    pt.manual_seed(int(self.hparams.seed))                                   ❼\n    self.layers = pt.nn.Linear(int(self.hparams.num_features), 1)            ❽\n\n  def batchToXy(batch):                                                      ❾\n    batch = batch.squeeze_()\n    X, y = batch[:, 1:], batch[:, 0]\n    return X, y\n\n  def forward(X):                                                            ❿\n    y_est = self.model(X)\n    return y_est.squeeze_()\n\n  def training_step(self, batch, batch_idx):\n    X, y = self.batchToXy(batch)\n    y_est = self.forward(X)\n    loss = pt.nn.functional.mse_loss(y_est, y)\n    for k,v in {\n        \"train_mse\": loss.item(),\n        \"train_rmse\": loss.sqrt().item(),\n    }.items():\n      self.log(k, v, on_step=True, on_epoch=True, prog_bar=True, logger=True)⓫\n\n    return loss                                                              ⓬\n\n  def configure_optimizers(self):                                            ⓭\n    optimizers = {'Adam': pt.optim.AdamW,\n                    'SGD': pt.optim.SGD}\n    optimizer = optimizers[self.hparams.optimizer]\n\n    return optimizer(self.layers.parameters(),                               ⓮\n                        lr = float(self.hparams.lr))\n\nmodel = DcTaxiModel(**{                                                      ⓯\n    \"seed\": \"1686523060\",\n    \"num_features\": \"8\",\n    \"optimizer\": \"Adam\",\n    \"lr\": \"0.03\",\n    \"max_batches\": \"100\",\n    \"batch_size\": \"64\",\n})\n```", "```py\nfrom pytorch_lightning.loggers import CSVLogger\ncsvLog = \\\n  CSVLogger(save_dir = \"logs\",                         ❶\n            name = \"dctaxi\",\n            version = f\"seed_{model.hparams.seed}\")    ❷\n\ntrainer = \\\n  pl.Trainer(gpus = pt.cuda.device_count() \\           ❸\n                      if pt.cuda.is_available() else 0,\n    max_epochs = 1,                                    ❹\n    limit_train_batches = \\                            ❺\n      int( model.hparams.max_batches ) \\\n            if 'max_batches' in model.hparams else 1,\n    log_every_n_steps = 1,                             ❻\n    logger = [csvLog])                                 ❼\n```", "```py\npip install kaen[osds]\n```", "```py\nfrom torch.utils.data import DataLoader\nfrom kaen.torch import ObjectStorageDataset as osds\n\ntrain_ds = osds('https://raw.githubusercontent.com/osipov/\n➥                   smlbook/master/train.csv',\n                batch_size = int(model.hparams.batch_size) )\n\ntrain_dl = DataLoader(train_ds,\n                      pin_memory = True)\ntrainer.fit(model,\n            train_dataloaders = train_dl)\n\ntrainer.fit(model, train_dl)\n```", "```py\nimport pandas as pd\nmetrics_df = pd.read_csv(f'logs/dctaxi/seed_{model.hparams.seed}/\n➥                             metrics.csv')\nax = metrics_df.plot('step', 'train_rmse_step')\n```", "```py\nax = metrics_df.iloc[-25:].plot('step', 'train_rmse_step')\nax.plot(metrics_df.iloc[-25:]['step'],\n        pt.full([25], metrics_df[-25:]['train_rmse_step'].mean())),\n```", "```py\nimport torch as pt\nimport pytorch_lightning as pl\nfrom torch.utils.data import DataLoader\nfrom kaen.torch import ObjectStorageDataset as osds\n\npt.set_default_dtype(pt.float64)\n\nclass DcTaxiModel(pl.LightningModule):\n    def __init__(self, **kwargs):\n        super().__init__()\n        self.save_hyperparameters()\n\n        pt.manual_seed(int(self.hparams.seed))\n\n        self.layers = pt.nn.Linear(int(self.hparams.num_features), 1)\n\n    def batchToXy(self, batch):\n      batch = batch.squeeze_()\n      X, y = batch[:, 1:], batch[:, 0]\n      return X, y\n\n    def forward(self, X):\n      y_est = self.layers(X)\n      return y_est.squeeze_()\n\n    def training_step(self, batch, batch_idx):\n\n        X, y = self.batchToXy(batch) #unpack batch into features and label\n\n        y_est = self.forward(X)\n\n        loss = pt.nn.functional.mse_loss(y_est, y)\n\n        for k,v in {\n          \"train_mse\": loss.item(),\n          \"train_rmse\": loss.sqrt().item(),\n        }.items():\n          self.log(k, v, on_step=True,\n                          on_epoch=True, prog_bar=True, logger=True)\n\n        return loss\n\n    def configure_optimizers(self):\n        optimizers = {'Adam': pt.optim.AdamW,\n                      'SGD': pt.optim.SGD}\n        optimizer = optimizers[self.hparams.optimizer]\n\n        return optimizer(self.layers.parameters(),\n                            lr = float(self.hparams.lr))\n\ndef build(model):\n  csvLog = CSVLogger(save_dir = \"logs\",\n                    name = \"dctaxi\",\n                    version = f\"seed_{model.hparams.seed}\"\n                    )\n\n  trainer = pl.Trainer(gpus = pt.cuda.device_count() \\\n                              if pt.cuda.is_available() else 0,\n    max_epochs = 1,\n    limit_train_batches = int( model.hparams.max_batches ) \\\n                          if 'max_batches' in model.hparams else 1,\n    progress_bar_refresh_rate = 20,\n    weights_summary = None,\n    log_every_n_steps = 1,\n    logger = csvLog)\n\n  train_ds = osds('https://raw.githubusercontent.com/osipov/smlbook/\n➥                   master/train.csv',\n                  batch_size = int(model.hparams.batch_size) )\n\n  train_dl = DataLoader(train_ds,\n                        pin_memory = True)\n\n  trainer.fit(model,\n              train_dataloaders = train_dl)\n\n  return model, trainer\n\nmodel = build(DcTaxiModel(**{\n        \"seed\": \"1686523060\",\n        \"num_features\": \"8\",\n        \"optimizer\": \"Adam\",\n        \"lr\": \"0.03\",\n        \"max_batches\": \"100\",\n        \"batch_size\": \"100\",\n}))\n```", "```py\ndef test_step(self, batch, batch_idx):\n    X, y = self.batchToXy(batch)\n\n    with pt.no_grad():                                 ❶\n        loss = pt.nn.functional.mse_loss(self.forward(X), y)\n\n    for k,v in {\n        \"test_mse\": loss.item(),                       ❷\n        \"test_rmse\": loss.sqrt().item(),               ❸\n    }.items():\n        self.log(k, v, on_step=True, on_epoch=True,\n                        prog_bar=True, logger=True)\n```", "```py\ndef build(model, train_glob, test_glob):                            ❶\n  csvLog = CSVLogger(save_dir = \"logs\",\n                    name = \"dctaxi\",\n                     version = f\"seed_{model.hparams.seed}\")\n\n  trainer = pl.Trainer(gpus = pt.cuda.device_count() \\\n                              if pt.cuda.is_available() else 0,\n    max_epochs = 1,\n    limit_train_batches = int( model.hparams.max_batches ) \\\n                          if 'max_batches' in model.hparams else 1,\n    limit_test_batches = 1,                                         ❷\n    log_every_n_steps = 1,\n    logger = csvLog)\n\n  train_ds = osds(train_glob,                                       ❸\n                  batch_size = int(model.hparams.batch_size) )\n\n  train_dl = DataLoader(train_ds,                                   ❹\n                        pin_memory = True)\n\n  trainer.fit(model,\n              train_dataloaders = train_dl)\n\n  test_ds = osds(test_glob, \n                  batch_size = int(model.hparams.batch_size) )\n\n  test_dl = DataLoader(test_ds, \n                        pin_memory = True)\n\n  trainer.test(model,                                               ❺\n              test_dataloaders=test_dl)\n\n  return model, trainer\n```", "```py\nmodel = build(DcTaxiModel(**{\n        \"seed\": \"1686523060\",\n        \"num_features\": \"8\",\n        \"optimizer\": \"Adam\",\n        \"lr\": \"0.03\",\n        \"max_batches\": \"100\",\n        \"batch_size\": \"100\",}),\n  train_glob = 'https://raw.githubusercontent.com/osipov/smlbook/\n➥                 master/train.csv',\n  test_glob = 'https://raw.githubusercontent.com/osipov/smlbook/\n➥                 master/train.csv')\n```", "```py\n-----------------------------------------------------------------------------\nDATALOADER:0 TEST RESULTS\n{'test_mse': 9.402312278747559,\n 'test_mse_epoch': 9.402312278747559,\n 'test_rmse': 3.066318988800049,\n 'test_rmse_epoch': 3.066318988800049}\n-----------------------------------------------------------------------------\n```", "```py\ndef validation_step(self, batch, batch_idx):\n    X, y = self.batchToXy(batch)\n\n    with pt.no_grad():\n        loss = pt.nn.functional.mse_loss(self.forward(X), y)\n\n    for k,v in {\n    \"val_mse\": loss.item(),\n    \"val_rmse\": loss.sqrt().item(),\n    }.items():\n    self.log(k, v, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n\n    return loss\n```", "```py\ntrainer = pl.Trainer(gpus = pt.cuda.device_count() \\\n                            if pt.cuda.is_available() else 0,\n    max_epochs = 1,\n    limit_train_batches = int( model.hparams.max_batches ) \\\n                          if 'max_batches' in model.hparams else 1,\n    limit_val_batches = 1,                         ❶\n    num_sanity_val_steps = 1,                      ❷\n    val_check_interval = min(20,                   ❸\n                            int( model.hparams.max_batches ) ),\n    limit_test_batches = 1,\n    log_every_n_steps = 1,\n    logger = csvLog,\n    progress_bar_refresh_rate = 20,\n    weights_summary = None,)\n```", "```py\nimport torch as pt\nimport pytorch_lightning as pl\nfrom torch.utils.data import DataLoader\nfrom kaen.torch import ObjectStorageDataset as osds\n\npt.set_default_dtype(pt.float64)\n\nclass DcTaxiModel(pl.LightningModule):\n    def __init__(self, **kwargs):\n        super().__init__()\n        self.save_hyperparameters()\n\n        pt.manual_seed(int(self.hparams.seed))\n\n        self.layers = pt.nn.Linear(int(self.hparams.num_features), 1)\n\n    def batchToXy(self, batch):\n      batch = batch.squeeze_()\n      X, y = batch[:, 1:], batch[:, 0]\n      return X, y\n\n    def forward(self, X):\n      y_est = self.layers(X)\n      return y_est.squeeze_()\n\n    def training_step(self, batch, batch_idx):\n\n        X, y = self.batchToXy(batch)\n\n        y_est = self.forward(X)\n\n        loss = pt.nn.functional.mse_loss(y_est, y)\n\n        for k,v in {\n          \"train_mse\": loss.item(),\n          \"train_rmse\": loss.sqrt().item(),\n        }.items():\n          self.log(k, v, on_step=True, on_epoch=True,\n                          prog_bar=True, logger=True)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n      X, y = self.batchToXy(batch)\n      with pt.no_grad():\n          loss = pt.nn.functional.mse_loss(self.forward(X), y)\n\n      for k,v in {\n        \"val_mse\": loss.item(),\n        \"val_rmse\": loss.sqrt().item(),\n      }.items():\n        self.log(k, v, on_step=True, on_epoch=True,\n                        prog_bar=True, logger=True)\n\n      return loss\n\n    def test_step(self, batch, batch_idx):\n      X, y = self.batchToXy(batch)\n\n      with pt.no_grad():\n          loss = pt.nn.functional.mse_loss(self.forward(X), y)\n\n      for k,v in {\n          \"test_mse\": loss.item(),\n          \"test_rmse\": loss.sqrt().item(),\n      }.items():\n          self.log(k, v, on_step=True, on_epoch=True,\n                          prog_bar=True, logger=True)\n\n    def configure_optimizers(self):\n        optimizers = {'Adam': pt.optim.AdamW,\n                      'SGD': pt.optim.SGD}\n        optimizer = optimizers[self.hparams.optimizer]\n\n        return optimizer(self.layers.parameters(),\n                            lr = float(self.hparams.lr))\n\ndef build(model, train_glob, val_glob, test_glob):\n  csvLog = CSVLogger(save_dir = \"logs\",\n                    name = \"dctaxi\",\n                     version = f\"seed_{model.hparams.seed}\")\n\n  trainer = pl.Trainer(gpus = pt.cuda.device_count() \\\n                              if pt.cuda.is_available() else 0,\n    max_epochs = 1,\n    limit_train_batches = int( model.hparams.max_batches ) \\\n                          if 'max_batches' in model.hparams else 1,\n    limit_val_batches = 1,\n    num_sanity_val_steps = 1,\n    val_check_interval = min(20, int( model.hparams.max_batches ) ),\n    limit_test_batches = 1,\n    log_every_n_steps = 1,\n    logger = csvLog,\n    progress_bar_refresh_rate = 20,\n    weights_summary = None,)\n\n  train_dl = \\\n    DataLoader(osds(train_glob,\n                    batch_size = int(model.hparams.batch_size) ),\n               pin_memory = True)\n\n  val_dl = \\\n    DataLoader(osds(val_glob,\n                    batch_size = int(model.hparams.batch_size) ),\n               pin_memory = True)\n\n  trainer.fit(model,\n              train_dataloaders = train_dl,\n              val_dataloaders = val_dl)\n\n  test_dl = \\\n    DataLoader(osds(test_glob,\n                    batch_size = int(model.hparams.batch_size) ),\n               pin_memory = True)\n\n  trainer.test(model,\n               dataloaders=test_dl)\n\n  return model, trainer\n```", "```py\nmodel, trainer = build(DcTaxiModel(**{\n        \"seed\": \"1686523060\",\n        \"num_features\": \"8\",\n        \"optimizer\": \"Adam\",\n        \"lr\": \"0.03\",\n        \"max_batches\": \"100\",\n        \"batch_size\": \"100\",}),\n  train_glob = 'https://raw.githubusercontent.com/osipov/smlbook/\n➥                 master/train.csv',\n  val_glob = 'https://raw.githubusercontent.com/osipov/smlbook/\n➥                 master/valid.csv',\n  test_glob = 'https://raw.githubusercontent.com/osipov/smlbook/\n➥                 master/train.csv').\n```", "```py\nimport pandas as pd\nmetrics_df = \\\n  pd.read_csv(f'logs/dctaxi/seed_{model.hparams.seed}/metrics.csv')\n\nax = (metrics_df[['step', 'train_rmse_step']][20:]\n      .dropna()\n      .plot('step', 'train_rmse_step'))\n\nax = (metrics_df[['step', 'val_rmse_step']][20:]\n    .fillna(method='ffill')['val_rmse_step']\n    .plot(ax = ax))\n```"]