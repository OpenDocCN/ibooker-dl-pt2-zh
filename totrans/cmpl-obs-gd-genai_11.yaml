- en: Appendix A. Important Definitions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I’m afraid there’s no way around it: if we want to get the *full* benefits
    of AI, we’re going to have to swallow hard and absorb some serious concepts. Technology
    is complicated by design, and AI is a particularly complicated subset of technology.
    The good news is that we’re not trying to qualify as physicists and engineers,
    so a very basic grasp of these ideas and their history will work just fine for
    our purposes. But prepare for some turbulence all the same.'
  prefs: []
  type: TYPE_NORMAL
- en: A.1 Some critical AI definitions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To get you started, here’s a helpful diagram illustrating the complex relationships
    between the many computational elements behind generative AI models.
  prefs: []
  type: TYPE_NORMAL
- en: Figure A.1 A left-to-right oriented mindmap of AI relationships
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![gai app 1](images/gai-app-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Having said that, even if you choose to skip this section altogether, you’ll
    still be able to successfully follow along with everything else in the book. But
    you might have trouble identifying some of the nuance (and weaknesses) in the
    AI responses you get. And some instructions and processes may feel a bit arbitrary.
  prefs: []
  type: TYPE_NORMAL
- en: I should note that the definitions for many of these concepts will reference
    other concepts. I’ll do my best to refer to only things that have been previously
    defined, but there are too many twisted (and recursive) relationships to make
    that happen every time. With that warning, here’s some fundamental knowledge that’ll
    make you more effective at working with generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: '**Machine learning** is a branch of artificial intelligence that focuses on
    developing algorithms and models capable of automatically learning and improving
    from data without explicit programming. It involves training a system on a large
    dataset to recognize patterns, make predictions, or perform tasks. By iteratively
    adjusting model parameters, machine learning enables computers to learn from experience
    and adapt to new inputs, enabling them to make informed decisions and perform
    complex tasks with minimal human intervention.'
  prefs: []
  type: TYPE_NORMAL
- en: In the context of AI, a **model** refers to a mathematical representation or
    computational system that learns patterns, structures, or relationships from data.
    It’s a trained algorithm or network that can take input and generate meaningful
    output based on its learned knowledge or trained parameters. In generative AI,
    a model refers specifically to a system that can generate new data samples that
    resemble the training data, whether it’s generating images, text, music, or other
    forms of creative content. The model encapsulates the learned information and
    the ability to generate new instances based on that knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: '**Labels** are categorizations or annotations assigned to data points. They
    provide explicit information about the characteristics or attributes associated
    with the input. Labels act as guiding signals to help the model learn and generate
    output that aligns with the desired attributes or properties. One place where
    labels are commonly used is for sentiment analysis. Sentiment analysis involves
    training a model to classify text as either positive, negative, or neutral based
    on its emotional tone. To perform this task, we need to label our training data
    with appropriate sentiments (e.g., "this review is positive," "this tweet is negative").'
  prefs: []
  type: TYPE_NORMAL
- en: '**Weighting** refers to the numerical values assigned to the connections between
    neurons or features in a model. These weights determine the strength or importance
    of each connection and play a crucial role in the model’s learning and decision-making
    process. During training, the weights are adjusted iteratively based on the observed
    errors or differences between predicted and actual outputs, enabling the model
    to learn from the data and improve its performance by assigning appropriate weights
    to different inputs and connections. Weighting is commonly used for named entity
    recognition (NER), which involves identifying and categorizing entities mentioned
    in text into predefined categories like person, organization, and location. A
    weighted NER model, for instance, be used for a chatbot application to extract
    and respond to user queries about specific topics or entities.'
  prefs: []
  type: TYPE_NORMAL
- en: A **parser** is a software component or algorithm that analyzes the structure
    of a given input, typically in the form of a sequence of symbols or text, and
    generates a structured representation based on a predefined grammar or set of
    rules. It is commonly used in natural language processing to parse sentences and
    extract syntactic or semantic information. Parsers break down the input into constituent
    parts, such as words or phrases, and establish relationships between them, enabling
    further analysis, understanding, or processing of the input data.
  prefs: []
  type: TYPE_NORMAL
- en: By understanding the **dependencies** between words, sentences, or visual elements,
    generative AI models can generate meaningful sequences or images that maintain
    contextual consistency. Modeling dependencies allows the generated output to exhibit
    logical flow, semantic coherence, and adherence to patterns observed in the training
    data. Accurately capturing dependencies is essential for generating high-quality
    and coherent outputs in generative AI applications.
  prefs: []
  type: TYPE_NORMAL
- en: '**Regression** is a supervised machine learning technique used to predict or
    estimate a continuous output variable based on input features. It models the relationship
    between the input variables and the output variable by fitting a mathematical
    function to the training data. The goal is to find the best-fitting function that
    minimizes the difference between the predicted values and the actual values. Regression
    algorithms analyze the patterns and trends in the data to make predictions or
    infer relationships. Regression can be another tool for sentiment analysis. For
    customer service-related tasks, for instance, it’s important to be able to automatically
    classify customer complaints or praise to allow organizations to accurately route
    issues to the appropriate support agents.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Classification** is a fundamental task in machine learning where the goal
    is to assign input data points to predefined categories or classes. It involves
    training a model on labeled data, where each data point is associated with a known
    class. The model learns patterns and relationships in the training data to make
    predictions on new, unseen data. The output of a classification model is a discrete
    class label that represents the predicted category to which the input belongs.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Optimization algorithms** are mathematical procedures used to find the optimal
    solution for a given problem. In the context of machine learning and neural networks,
    these algorithms are employed to minimize an objective function, typically represented
    by a loss or cost function. The goal is to adjust the model’s parameters iteratively
    to reach the optimal set of values that minimize the objective function. In the
    world of optimizing models, there are some popular techniques like "stochastic
    gradient descent" and its variations. These methods help the model get better
    by adjusting its internal settings according to how much it’s improving or getting
    worse. By doing this, the model gets closer to finding the best possible solution
    and performs much better at its tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Vectors** are mathematical entities used to represent both magnitude and
    direction in a multi-dimensional space. In the context of machine learning and
    data analysis, vectors are often used to represent features or data points. Each
    dimension of a vector corresponds to a specific attribute or variable, allowing
    for efficient storage and manipulation of data. Vectors can be operated upon using
    mathematical operations like addition, subtraction, and dot product, enabling
    calculations of similarity, distances, and transformations. Vectors play a fundamental
    role in various algorithms and models, such as clustering, classification, and
    dimensionality reduction.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Vector embeddings** help LLMs generalize knowledge across similar words and
    phrases, even if they were not encountered during training. This allows the model
    to handle out-of-vocabulary words effectively. Pre-trained embeddings can be used
    as starting points for various NLP tasks, enabling transfer learning and improving
    performance on downstream tasks with limited data. One practical application of
    all this would be medical imaging, where vector embeddings can be used to analyze
    and compare images of organs or tissues. A deep learning model can be trained
    to map brain scans onto a vector space, where similar scans are clustered together.
    This enables doctors to quickly identify patterns and abnormalities in patient
    scans, leading to earlier diagnosis and treatment of diseases such as cancer or
    neurological disorders.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Word embeddings** are a way of representing words as vectors in a high-dimensional
    space, such that similar words are close together in that space. Word embeddings
    are typically represented as tensors, where each dimension represents a different
    aspect of the word’s meaning. For example, a word embedding tensor might have
    dimensions for the word’s synonyms, antonyms, and part of speech.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Parallelization** refers to the technique of dividing a computational task
    into smaller subtasks that can be executed simultaneously on multiple computing
    resources. It leverages the power of parallel processing to speed up the overall
    computation and improve efficiency. In parallel computing, tasks are allocated
    to different processors, threads, or computing units, allowing them to work concurrently.
    This approach enables tasks to be completed faster by distributing the workload
    across multiple resources. Parallelization is widely used in various fields, including
    machine learning, scientific simulations, and data processing, to achieve significant
    performance gains and handle large-scale computations efficiently.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Regularization techniques** are methods used to improve the generalization
    performance of models. These techniques add a penalty term to the loss function
    during training, discouraging the model from relying too heavily on complex or
    noisy patterns in the data. Regularization techniques help control model complexity,
    reduce overfitting, and improve the model’s ability to generalize to unseen data.'
  prefs: []
  type: TYPE_NORMAL
- en: A common practical application of regularization techniques is in text classification,
    specifically when dealing with imbalanced datasets. Let’s say we have a dataset
    of movie reviews, where the majority class is positive reviews (e.g., "good movie")
    and the minority class is negative reviews (e.g., "bad movie"). Without regularization,
    the model might become biased towards the positive reviews and fail to accurately
    classify the negative reviews. To address this imbalance, we can add a regularization
    term to the loss function that penalizes the model for misclassifying negative
    reviews.
  prefs: []
  type: TYPE_NORMAL
- en: '**Convergence** refers to the process of training multiple models on the same
    dataset until they produce similar outputs. This is done to reduce the risk of
    overfitting and improve the generalization of the models. Convergence is typically
    evaluated using metrics such as validation loss or accuracy, and the training
    process is stopped once the models converge to a stable solution.'
  prefs: []
  type: TYPE_NORMAL
- en: 'All of which bring us to:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Natural Language Processing (NLP)** focuses on the interaction between computers
    and human language. It involves the development of algorithms and models to enable
    computers to understand, interpret, and generate human language in a meaningful
    way. NLP encompasses tasks such as text classification, sentiment analysis, machine
    translation, information extraction, and question answering. It utilizes techniques
    from various disciplines, including computational linguistics, machine learning,
    and deep learning, to process and analyze textual data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'And, finally, to:'
  prefs: []
  type: TYPE_NORMAL
- en: A **Large Language Model (LLM)**, is a tool in natural language processing (NLP)
    that leverages deep learning techniques to understand and generate human-like
    text. It analyzes patterns, contexts, and semantics within a given text corpus
    to learn the underlying structures of language. With its ability to comprehend
    and generate coherent and contextually relevant responses, an LLM can be used
    for various tasks, such as chatbots, language translation, text completion, and
    summarization. By capturing the intricacies of language, an LLM allows machines
    to communicate directly with humans.
  prefs: []
  type: TYPE_NORMAL
- en: Or, in other words, it enables generative AI.
  prefs: []
  type: TYPE_NORMAL
