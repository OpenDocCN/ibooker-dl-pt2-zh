- en: 1 An introduction to deep learning systems
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 深度学习系统简介
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容包括
- en: Defining a deep learning system
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义深度学习系统
- en: The product development cycle and how a deep learning system supports it
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品开发周期及深度学习系统如何支持其
- en: An overview of a basic deep learning system and its components
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本深度学习系统及其组成部分概述
- en: Differences between building a deep learning system and developing models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搭建深度学习系统与开发模型之间的差异
- en: This chapter will prepare you with a big-picture mental model of a deep learning
    system. We will review some definitions and provide a reference system architecture
    design and a complete sample implementation of the architecture. We hope this
    mental model will prime you to see how the rest of the chapters, which address
    each system component in detail, fit into the whole picture.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将为您提供一个深度学习系统的全貌思维模型。我们将回顾一些定义，并提供一个参考系统架构设计和该架构的完整示例实现。我们希望这个思维模型能够让您看到其他章节如何详细介绍每个系统组件，并将其融入整体图景。
- en: 'To begin this chapter, we will discuss an even bigger picture beyond the deep
    learning system: something we call *the deep learning development cycle*. This
    cycle outlines the various roles and stages involved in bringing products based
    on deep learning to market. The model and the platform do not exist in a vacuum;
    they affect and are affected by product management, market research, production,
    and other stages. We believe that engineers design better systems when they understand
    this cycle and know what each team does and what it needs to do its job.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始本章之前，我们将讨论一个超越深度学习系统的更大图景：我们称之为*深度学习开发周期*。这个周期概述了基于深度学习的产品推向市场所涉及的各种角色和阶段。模型和平台并不是孤立存在的；它们受产品管理、市场调研、生产和其他阶段的影响，也会影响这些阶段。我们相信，当工程师了解这个周期以及每个团队的工作内容和所需时，他们设计的系统会更好。
- en: In section 1.2, we start our discussion of deep learning system design with
    a sample architecture of a typical system that can be adapted for designing your
    own deep learning system. The components described in this section will be explored
    in greater detail in their own chapters. Finally, we will emphasize the differences
    between developing a model and developing a deep learning system. This distinction
    is often a point of confusion, so we want to clear it up right away.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在1.2节中，我们将从典型系统的示例架构开始讨论深度学习系统设计。本节描述的组件将在各自的章节中进行更详细的探讨。最后，我们将强调开发模型与开发深度学习系统之间的区别。这种区别常常是一个让人困惑的焦点，所以我们想要立即澄清。
- en: After reading this introductory chapter, you will have a solid understanding
    of the deep learning landscape. You will also be able to start creating your own
    deep learning system design, as well as understand existing designs and how to
    use and extend them, so you don’t have to build everything from scratch. As you
    continue reading this book, you will see how everything connects and works together
    as a deep learning system.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读完这个入门章节之后，您将对深度学习的概况有了扎实的理解。您还将能够开始创建自己的深度学习系统设计，并理解现有设计以及如何使用和扩展它们，这样您就不必从头开始构建一切。随着您继续阅读本书，您将看到一切是如何连接和共同作为一个深度学习系统运作的。
- en: Terminology
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 术语
- en: Before we proceed with the rest of the chapter (and the rest of the book), let’s
    define and clarify a few terms that we use throughout the book.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续本章（以及本书的其余部分）之前，让我们定义和澄清本书中始终使用的一些术语。
- en: Deep learning vs. machine learning
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习与机器学习的比较
- en: Deep learning is machine learning, but it is considered an evolution of machine
    learning. Machine learning, by definition, is an application of artificial intelligence
    that includes algorithms that parse data, learn from that data, and then apply
    what it has learned to make informed decisions. Deep learning is a special form
    of machine learning that uses a programmable neural network as the algorithm to
    learn from data and make accurate decisions.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是机器学习的一种，但被认为是机器学习的一种演变。机器学习按定义是人工智能的一种应用，包括解析数据、从数据中学习，然后应用所学内容做出明智决策的算法。深度学习是机器学习的一种特殊形式，它使用可编程神经网络作为算法，从数据中学习并做出准确的决策。
- en: Although this book primarily focuses on teaching you how to build the system
    or infrastructure to facilitate deep learning development (all the examples are
    neural network algorithms), the design and project development concepts we discuss
    are all applicable to machine learning as well. So, in this book we use the terms
    *deep learning* and *machine learning* somewhat interchangeably. For example,
    the deep learning development cycle introduced in this chapter and the data management
    service introduced in chapter 2 work in the machine learning context, too.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本书主要关注于教授如何构建系统或基础设施来促进深度学习开发（所有示例都是神经网络算法），但我们讨论的设计和项目开发概念在机器学习中也适用。因此，在本书中，我们有时将术语*深度学习*和*机器学习*互换使用。例如，本章介绍的深度学习开发周期和第2章介绍的数据管理服务也适用于机器学习上下文。
- en: Deep learning use case
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习用例
- en: A deep learning use case refers to a scenario that utilizes deep learning technology—in
    other words, a problem that you want to solve using deep learning. Examples include
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习用例是指利用深度学习技术解决问题的场景，换句话说，是您想使用深度学习解决的问题。例如：
- en: '*Chatbot*—A user can initiate a text-based conversation with a virtual agent
    on a customer support website. The virtual agent uses a deep learning model to
    understand sentences that the user enters and carries on a conversation with the
    user like a real human.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*聊天机器人*—用户可以在客户支持网站上与虚拟代理进行基于文本的对话。虚拟代理使用深度学习模型理解用户输入的句子，并像真正的人类一样与用户进行对话。'
- en: '*Self-driving car*—A driver can put a car into an assistive driving mode that
    automatically steers itself according to road markings. Markings are captured
    by multiple cameras on board the car to form a perception of the road using deep
    learning–based computer vision technology.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自动驾驶汽车*—驾驶员可以将汽车置于辅助驾驶模式，根据道路标线自动转向。车载多个摄像头捕捉标线，利用基于深度学习的计算机视觉技术形成对道路的感知。'
- en: Model, prediction and inference, and model serving
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 模型、预测和推断，以及模型服务
- en: 'These three terms are described as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个术语的描述如下：
- en: '*Model*—A deep learning model can be seen as an executable program that contains
    an algorithm (model architecture) and required data to make a prediction.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型*—深度学习模型可以被视为包含算法（模型架构）和进行预测所需数据的可执行程序。'
- en: '*Prediction and inference*—Both model prediction and model inference refer
    to executing the model with given data to get a set of outputs. As prediction
    and inference are used widely in the context of model serving, they are used interchangeably
    in this book.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预测和推断*—模型预测和模型推断都是指使用给定数据执行模型以获得一组输出。由于在模型服务的上下文中广泛使用预测和推断，它们在本书中可以互换使用。'
- en: '*Model serving* (prediction service)—This book describes model serving as hosting
    machine learning models in a web application (on the cloud or on premises) and
    allowing deep learning applications to integrate the model functionality into
    their systems through an API. The model serving web program is usually referred
    to as the prediction service or model serving service.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型服务*（预测服务）—本书将模型服务描述为在Web应用程序（在云端或本地）中托管机器学习模型，并允许深度学习应用程序通过API将模型功能集成到其系统中。模型服务Web程序通常称为预测服务或模型服务。'
- en: Deep learning application
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习应用
- en: 'A deep learning application is a piece of software that utilizes deep learning
    technologies to solve problems. It usually does not perform any computationally
    intensive tasks, such as data crunching, deep learning model training, and model
    serving (with the exception of hosting models at the edge, such as an autonomous
    vehicle). Examples include:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习应用是利用深度学习技术解决问题的软件。它通常不执行任何计算密集型任务，例如数据处理、深度学习模型训练和模型服务（除了在边缘托管模型，例如自动驾驶汽车）。例如：
- en: A *chatbot application* that provides a UI or APIs to take natural sentences
    as input from a user, interprets them, takes actions, and provides a meaningful
    response to the user. Based on the model output calculated in the deep learning
    system (from model serving service), the chatbot responds and takes action.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供UI或API以接受用户的自然句子作为输入，解释它们，采取行动并向用户提供有意义的响应的*聊天机器人应用程序*。基于深度学习系统中计算的模型输出（来自模型服务），聊天机器人做出响应并采取行动。
- en: '*Self-driving software* that takes input from multiple sensors, such as video
    cameras, proximity sensors, and LiDAR, to form a perception of a car’s surroundings
    with the help of deep learning models and drives the car accordingly.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自动驾驶软件*从多个传感器接收输入，如视频摄像头、接近传感器和激光雷达，借助深度学习模型形成对汽车周围环境的感知，并相应地驾驶汽车。'
- en: Platform vs. system vs. infrastructure
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 平台 vs. 系统 vs. 基础设施
- en: 'In this book, the terms *deep learning platform*, *deep learning system*, and
    *deep learning infrastructure* all share the same meaning: an underlying system
    that provides all necessary support for building deep learning applications efficiently
    and to scale. We tend to use *system* most commonly, but in the context of this
    book, all three terms have the same meaning.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，术语 *深度学习平台*、*深度学习系统* 和 *深度学习基础设施* 具有相同的含义：为高效构建深度学习应用程序提供所有必要支持的基础系统。我们倾向于最常使用
    *系统*，但在本书的上下文中，所有三个术语都具有相同的含义。
- en: Now that we’re all on the same page about the terms, let’s get started!
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对术语有了共识，让我们开始吧！
- en: 1.1 The deep learning development cycle
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 深度学习开发周期
- en: As we’ve said, deep learning systems are the infrastructure necessary for deep
    learning *project development* to progress efficiently. So, before we dive into
    the structure of a deep learning system, it’s prudent to look at the development
    paradigm that a deep learning system enables. We call this paradigm the *deep
    learning development cycle*.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所说，深度学习系统是深度学习 *项目开发* 高效进行所必需的基础设施。因此，在深入探讨深度学习系统的结构之前，审视一下深度学习系统所启用的开发范式是明智的。我们称这个范式为
    *深度学习开发周期*。
- en: You may wonder why, in a technical book, we want to emphasize something that
    is as nontechnical as product development. The fact is that the goal of most deep
    learning work is, in the end, to bring a product or service to market. Yet many
    engineers are not familiar with the other stages of product development, just
    as many product developers do not know about engineering or modeling. From our
    experience in building deep learning systems, we have learned that persuading
    people in multiple roles in a company to adopt a system largely depends on whether
    the system will actually fix their particular problems. We believe that outlining
    the various stages and roles in the deep learning development cycle helps to articulate,
    address, communicate, and eventually solve everyone’s pain points.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，在一本技术书中，为什么我们要强调像产品开发这样与技术无关的东西。事实上，大多数深度学习工作的目标最终是将产品或服务推向市场。然而，许多工程师并不熟悉产品开发的其他阶段，就像许多产品开发者不了解工程或建模一样。从我们构建深度学习系统的经验中，我们已经了解到，说服公司中多个角色的人员采用系统主要取决于该系统是否实际上能解决他们的特定问题。我们相信，概述深度学习开发周期中的各个阶段和角色有助于表达、解决、沟通和最终解决每个人的痛点。
- en: Understanding this cycle solves a few other problems, as well. In the last decade,
    many new deep learning software packages have been developed to address different
    areas. Some of them tackle model training and serving, whereas others handle model
    performance tracking and experimentation. Data scientists and engineers would
    piece these tools together each time they needed to solve a specific application
    or use case; this is called MLOps (machine learning operations). As the number
    of these applications grows, piecing these tools together every time from scratch
    for a new application becomes repetitive and time-consuming. At the same time,
    as the importance of these applications grows, so do the expectations for their
    quality. Both of these concerns call for a consistent approach to developing and
    delivering deep learning features quickly and reliably. This consistent approach
    starts with everyone working under the same deep learning development paradigm
    or cycle.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 了解这一周期也能解决其他一些问题。在过去的十年里，许多新的深度学习软件包已经被开发出来，以解决不同的领域。其中一些处理模型训练和服务，而另一些处理模型性能跟踪和实验。数据科学家和工程师每次需要解决特定应用程序或用例时都会组合这些工具；这被称为
    MLOps（机器学习运维）。随着这些应用程序数量的增长，为新的应用程序每次从头开始组合这些工具变得重复和耗时。同时，随着这些应用程序的重要性增长，对其质量的期望也在增加。这两个问题都需要一种一致的方法来快速可靠地开发和交付深度学习功能。这种一致的方法始于所有人都在同一深度学习开发范式或周期下工作。
- en: How does the deep learning *system* fit into the deep learning *cycle*? A well-built
    deep learning system would support the product development cycle and make performing
    the cycle easy, quick, and reliable. Ideally, data scientists can use a deep learning
    system as the infrastructure to complete the entire deep learning cycle without
    learning all the engineering details of the underlying complex systems.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习*系统*如何适应深度学习*周期*？一个良好构建的深度学习系统应该支持产品开发周期，并使执行周期变得轻松、快速和可靠。理想情况下，数据科学家可以使用深度学习系统作为基础设施完成整个深度学习周期，而无需学习底层复杂系统的所有工程细节。
- en: Because every product and organization is unique, it is crucial for system builders
    to understand the unique requirements of the various roles to build a successful
    system. By “successful,” we mean one that helps stakeholders collaborate productively
    to deliver deep learning features quickly. Throughout this book, as we go through
    the design principles of deep learning systems and look at how each component
    works, your understanding of your stakeholder requirements will help you adapt
    this knowledge to form your own system design. As we discuss the technical details,
    we will point out when you need to pay attention to certain types of stakeholders
    during the design of the system. The deep learning development cycle will serve
    as the guiding framework when we consider the design requirements of each component
    of a deep learning system.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 因为每个产品和组织都是独特的，对系统构建者来说，理解各种角色的独特需求以构建成功的系统至关重要。所谓的“成功”，是指帮助利益相关者高效协作，快速交付深度学习特性的系统。在本书中，当我们讨论深度学习系统的设计原则，并查看每个组件的工作方式时，你对利益相关者需求的理解将帮助你调整这些知识，形成自己的系统设计。在讨论技术细节时，我们将指出在设计系统时需要注意某些类型的利益相关者。深度学习开发周期将作为指导框架，帮助我们考虑深度学习系统的每个组件的设计要求。
- en: Let’s start with a picture. Figure 1.1 illustrates what a typical cycle looks
    like. It shows the machine learning (especially deep learning) development progress
    phase by phase. As you can see, cross-functional collaboration happens at almost
    every step. We will discuss each phase and role involved in this diagram in the
    following two sections.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一张图片开始。图1.1展示了典型周期的样貌。它展示了机器学习（特别是深度学习）的开发进度逐个阶段的过程。正如你所见，跨职能协作几乎在每一步都发生。我们将在接下来的两个部分讨论此图中涉及的每个阶段和角色。
- en: '![](../Images/01-01.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01-01.png)'
- en: Figure 1.1 A typical scenario to bring deep learning from research to a product.
    We call this *the deep learning development cycle*.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1 将深度学习从研究带入产品的典型场景。我们称之为*深度学习开发周期*。
- en: 1.1.1 Phases in the deep learning product development cycle
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1.1 深度学习产品开发周期中的阶段
- en: 'The deep learning development cycle typically begins with a business opportunity
    and is driven by a product plan and its management. After that, the cycle normally
    goes through four phases: data exploration, prototyping, productionization (shipping
    to production), and application integration. Let’s look at these phases one at
    a time. Then we’ll look at all the roles involved (denoted by the icon of a person
    in figure 1.1).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习开发周期通常从一个业务机会开始，并由产品计划及其管理驱动。之后，周期通常经历四个阶段：数据探索、原型制作、产品化（投入生产）和应用集成。让我们逐一查看这些阶段。然后我们将查看所有涉及的角色（在图1.1中以人物图标表示）。
- en: Note The number in parentheses next to each following subsection corresponds
    to the same number in figure 1.1.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：每个后续小节旁边括号中的数字与图1.1中的相同数字对应。
- en: Product initiation (1)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 产品启动（1）
- en: First, the business stakeholder (product owner or project manager) analyzes
    the business and identifies a potential business opportunity or problem that can
    be addressed with machine learning.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，业务利益相关者（产品所有者或项目经理）分析业务，并确定可以通过机器学习解决的潜在业务机会或问题。
- en: Data exploration (2)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 数据探索（2）
- en: When data scientists have a clear understanding of business requirements, they
    begin to work with data engineers to collect as much data as possible, label it,
    and build datasets. Data collection can include searching publicly available data
    and exploring internal sources. Data cleaning may also occur. Data labeling can
    either be outsourced or performed in-house.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据科学家清楚了解业务需求时，他们开始与数据工程师合作，尽可能收集、标记数据并构建数据集。数据收集可以包括搜索公开可用数据和探索内部来源。数据清理也可能会发生。数据标记可以外包或在内部执行。
- en: Compared to the following phases, this early phase of data exploration is unstructured
    and often done casually. It might be a Python script or shell script, or even
    a manual copy of data. Data scientists often use web-based data analysis applications,
    such as Jupyter Notebook (open source; [https://jupyter.org](https://jupyter.org)),
    Amazon SageMaker Data Wrangler ([https://aws.amazon.com/sagemaker/data-wrangler](https://aws.amazon.com/sagemaker/data-wrangler)),
    and Databricks ([www.databricks.com](http://www.databricks.com)), to analyze data.
    There is no formal data collection pipeline that needs to be built.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 与以下阶段相比，数据探索的早期阶段是非结构化的，通常是随意进行的。它可能是一个 Python 脚本或 shell 脚本，甚至是数据的手动复制。数据科学家经常使用基于
    Web 的数据分析应用程序，例如 Jupyter Notebook（开源；[https://jupyter.org](https://jupyter.org)）、Amazon
    SageMaker Data Wrangler（[https://aws.amazon.com/sagemaker/data-wrangler](https://aws.amazon.com/sagemaker/data-wrangler)）和
    Databricks（[www.databricks.com](http://www.databricks.com)）来分析数据。不需要构建正式的数据收集管道。
- en: Data exploration is not only important but also critical to the success of a
    deep learning project. The more relevant data is available, the higher the likelihood
    of building effective and efficient deep learning models.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 数据探索不仅重要，而且对深度学习项目的成功至关重要。可用的相关数据越多，建立有效和高效深度学习模型的可能性就越高。
- en: Research and prototyping (3, 4)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 研究和原型设计（3, 4）
- en: The goal of prototyping is to find the most feasible algorithm/approach to address
    the business requirement (from product owner) with the given data. In this phase,
    data scientists can work with AI researchers to propose and evaluate different
    training algorithms with datasets built from the previous data exploration phase.
    Data scientists usually pilot multiple ideas in this phase and build proof-of-concept
    (POC) models to evaluate them.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 原型设计的目标是找到最可行的算法/方法，以解决给定数据的业务需求（来自产品所有者）。在此阶段，数据科学家可以与 AI 研究人员合作，提出并评估来自前期数据探索阶段构建的不同训练算法。数据科学家通常在此阶段尝试多种想法，并构建概念验证（POC）模型来评估它们。
- en: Although newly published algorithms are often considered, most of them will
    not be adopted. The accuracy of an algorithm is not the only factor to be considered;
    one also must consider computing resource requirements, data volume, and algorithm
    implementation cost when evaluating an algorithm. The most practical approach
    is usually the winner.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管新发布的算法通常会受到考虑，但大多数算法都不会被采纳。算法的准确性不是唯一要考虑的因素；在评估算法时还必须考虑计算资源需求、数据量和算法实现成本。最实用的方法通常是获胜者。
- en: Note that due to resource constraints, researchers are not always involved in
    the prototyping phase. Frequently, data scientists do the research work as well
    as build the POC.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于资源限制，研究人员并不总是参与原型设计阶段。经常情况下，数据科学家既做研究工作，又构建 POC。
- en: 'You may also notice that in figure 1.1, there is an inner loop (loop A) in
    the big development cycle: Product Initiation > Data Exploration > Deep Learning
    Research > Prototyping > Model > Product Initiation. The purpose of this loop
    is to obtain product feedback in the early phase by building a POC model. We may
    run through this loop multiple times until all stakeholders (data scientists,
    product owners) arrive at a consensus on the algorithms and data that will be
    used to address the business requirement.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可能注意到，在图1.1中，大型开发周期中有一个内部循环（循环A）：产品启动 > 数据探索 > 深度学习研究 > 原型设计 > 模型 > 产品启动。该循环的目的是通过构建
    POC 模型在早期阶段获得产品反馈。我们可能会多次执行此循环，直到所有利益相关者（数据科学家、产品所有者）就将用于满足业务需求的算法和数据达成一致意见。
- en: 'Multiple hard lessons finally taught us that we must vet the solution with
    the product team or the customer (even better) before starting the expensive process
    of productionization—building production data and training pipelines and hosting
    models. The purpose of a deep learning project is no different from any other
    software development project: to solve a business need. Vetting the approach with
    the product team in the early stage will prevent the expensive and demoralizing
    process of reworking it in later stages.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 多次痛苦的教训最终教会了我们，在开始昂贵的生产过程——构建生产数据和训练管道以及托管模型之前，我们必须与产品团队或客户（甚至更好）审查解决方案。深度学习项目的目的与任何其他软件开发项目并无不同：解决业务需求。在早期阶段与产品团队审查方法将防止在后期重新制定方法的昂贵和令人沮丧的过程。
- en: Productionization aka MLOps (5)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 生产化（也称为 MLOps）（5）
- en: Productionization, also called “shipping to production,” is the process of making
    a product production worthy and ready to be consumed by its users. Production
    worthiness is commonly defined as being able to serve customer requests, withstand
    a certain level of request load, and gracefully handle adverse situations such
    as malformed input and request overload. Production worthiness also includes postproduction
    efforts, such as continuous model metric monitoring and evaluation, feedback gathering,
    and model retraining.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 生产化，也称为“投入生产”，是使产品具备生产价值并准备好被用户消费的过程。生产价值通常定义为能够服务客户请求，承受一定程度的请求负载，并优雅地处理诸如格式错误的输入和请求超载等不利情况。生产价值还包括后期工作，如持续的模型指标监控和评估、反馈收集和模型重新训练。
- en: Productionization is the most engineering-intensive part of the development
    cycle because we’ll be converting prototyping experiments into serious production
    processes. A nonexhaustive to-do list of productionization can include
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 生产化是开发周期中最需要工程投入的部分，因为我们将把原型实验转化为严肃的生产流程。生产化的非详尽待办事项列表可以包括
- en: Building a data pipeline to pull data from different data sources repeatedly
    and keep the dataset versioned and updated.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立一个数据管道，重复从不同的数据源中提取数据，并使数据集版本化和更新。
- en: Building a data pipeline to preprocess dataset, such as data enhancement or
    enrichment and integrating with external labeling tools.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立数据管道对数据集进行预处理，例如数据增强或增强和与外部标记工具集成。
- en: Refactoring and dockerizing the prototyping code to production-quality model
    training code.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重构和将原型代码docker化为生产质量的模型训练代码。
- en: Making the result of training and serving codes reproducible by versioning and
    tracking the inputs and outputs. For example, we could enable the training code
    to report the training metadata (training date and time, duration, hyperparameters)
    and model metadata (performance metrics, data, and code used) to ensure the full
    traceability of every model training run.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过版本控制和跟踪输入和输出使训练和服务代码的结果可再现。例如，我们可以使训练代码报告训练元数据（训练日期和时间、持续时间、超参数）和模型元数据（性能指标、使用的数据和代码），以确保对每次模型训练运行的完全可追溯性。
- en: Setting up continuous integration (Jenkins, GitLab CI) and continuous deployment
    to automate the code building, validation, and deployment.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置持续集成（Jenkins、GitLab CI）和持续部署以自动化代码构建、验证和部署。
- en: Building a continuous model training and evaluation pipeline so the model training
    can automatically consume the latest dataset and produce models in a repeatable,
    auditable, and reliable manner.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立连续的模型训练和评估管道，以便模型训练可以自动使用最新的数据集，并以可重复、可审计和可靠的方式生成模型。
- en: Building a model deployment pipeline that automatically releases models that
    have passed the quality gate, so the model serving component can access them;
    `async` or real-time model prediction can be performed depending on the business
    requirements. The model serving component hosts the model and exposes it via a
    web API.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立一个模型部署管道，自动发布通过质量门的模型，以便模型服务组件可以访问它们；根据业务需求可以执行`async`或实时模型预测。模型服务组件托管模型并通过Web
    API公开它。
- en: Building continuous-monitoring pipelines that periodically assess the dataset,
    model, and model serving performance to detect potential feature drift (data distribution
    change) in dataset or model performance degradation (concept drifting) and alert
    developers or retrain the model.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立持续监控管道，定期评估数据集、模型和模型服务性能，以检测数据集的潜在特征漂移（数据分布变化）或模型性能下降（概念漂移）并警告开发人员或重新训练模型。
- en: 'These days, the productionization step has a new alias with buzz: MLOps (machine
    learning operation), which is a vague term, and its definition is ambiguous for
    researchers and professionals. We interpret MLOps to mean bridging the gap between
    model development (experimentation) and operations in production environments
    (Ops) to facilitate productionization of machine learning projects. An example
    might be streamlining the process of taking machine learning models to production
    and then monitoring and maintaining them.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，生产化步骤有一个新的热门别名：MLOps（机器学习运营），这是一个模糊的术语，对研究人员和专业人员的定义模糊不清。我们解释MLOps的含义是弥合模型开发（实验）和生产环境运营（Ops）之间的鸿沟，以促进机器学习项目的生产化。例如，简化将机器学习模型推向生产的过程，然后对其进行监视和维护。
- en: 'MLOps is a paradigm rooted in the application of similar principles that DevOps
    has to software development. It leverages three disciplines: machine learning,
    software engineering (especially the operation), and data engineering. See figure
    1.2 for a look at deep learning through the lens of MLOps.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps 是一种根植于 DevOps 原则的范式，应用了类似的原则到软件开发中。它利用了三个学科：机器学习、软件工程（特别是运维）和数据工程。查看图
    1.2，了解通过 MLOps 视角看深度学习。
- en: '![](../Images/01-02.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01-02.png)'
- en: 'Figure 1.2 MLOps applies DevOps approaches to deep learning for the productionization
    phase, when models get shipped to production. (Source: *Machine Learning Engineering
    in Action*, by Ben Wilson, Manning, 2022, figure 2.7)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2 MLOps 在深度学习的产品化阶段应用了 DevOps 方法，当模型被推向生产时。（来源：*Machine Learning Engineering
    in Action*，作者 Ben Wilson，Manning 出版社，2022年，图 2.7）
- en: Because this book is about building machine learning systems that support ML
    operations, we won’t go into details about the practices shown in figure 1.2\.
    But, as you can see, the engineering effort that supports the development of machine
    learning models in production is huge. Compared to what data scientists used to
    do during the previous phase of data exploration and model prototyping, the tooling
    (software), engineering standards, and processes have dramatically changed and
    become much more complex.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这本书是关于构建支持 ML 运营的机器学习系统，所以我们不会详细介绍图 1.2 中所示的实践。但是，正如你所看到的，支持将机器学习模型开发到生产环境中的工程工作量是巨大的。与数据科学家在数据探索和模型原型阶段所做的工作相比，工具（软件）、工程标准和流程已经发生了巨大变化，并变得更加复杂。
- en: Why is shipping models to production difficult?
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么将模型部署到生产环境很困难？
- en: The massive underlying infrastructure (tools, services, servers) and heavy cross-team
    collaboration are the two biggest hurdles for shipping models to production. This
    section on productionization (aka MLOps) establishes that data scientists need
    to work with data engineers, platform developers, DevOps engineers, and machine
    learning engineers, as well as learn a massive infrastructure (deep learning system),
    to ship an algorithm/model from prototype to production. It's no wonder that productionizing
    a model takes so much time.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 庞大的基础设施（工具、服务、服务器）和团队间的密集合作是将模型部署到生产环境的两个最大障碍。这个关于产品化（又称 MLOps）的部分建立在一个事实上，即数据科学家需要与数据工程师、平台开发人员、DevOps
    工程师和机器学习工程师一起工作，并且要了解庞大的基础设施（深度学习系统），才能将算法/模型从原型推向生产。难怪产品化模型需要花费如此多的时间。
- en: To solve these challenges, we need to abstract away the complexity from data
    scientists when designing and building a deep learning system. As with building
    a car, we want to put data scientists behind the wheel but without asking them
    to know much about the car itself.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些挑战，我们需要在设计和构建深度学习系统时，将复杂性从数据科学家那里抽象出来。就像建造汽车一样，我们希望让数据科学家坐在驾驶座上，但不要求他们对汽车本身了解太多。
- en: Now, returning to the development cycle, you may notice there is *another* inner
    loop (loop B) in figure 1.1 that goes from Productionization (box 5) and Model
    to Product Initiation (box 1). This is the second vet with the product team before
    we integrate the model inference with an AI application.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，回到开发周期，你可能会注意到图 1.1 中还有一个*另一个*内部循环（循环 B），从产品化（方框 5）到模型到产品启动（方框 1）。这是在我们将模型推理与
    AI 应用集成之前与产品团队进行的第二次审查。
- en: Our second vet (loop B) compares the model and data between prototyping and
    production. We want to ensure the model performance and scalability (for example,
    model serving capacity) match business requirements.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第二次审查（循环 B）在原型和生产之间比较模型和数据。我们要确保模型性能和可扩展性（例如，模型服务容量）符合业务需求。
- en: 'Note The following two papers are recommended; if you want to learn more about
    MLOps, they are great starting points: “Operationalizing Machine Learning: An
    Interview Study” (arXiv:2209.09125) and “Machine Learning Operations (MLOps):
    Overview, Definition, and Architecture” (arXiv:2205.02302).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '注意：以下两篇论文是推荐的；如果你想了解更多关于 MLOps 的内容，它们是很好的起点：“Operationalizing Machine Learning:
    An Interview Study”（arXiv:2209.09125）和“Machine Learning Operations (MLOps): Overview,
    Definition, and Architecture”（arXiv:2205.02302）。'
- en: Application integration (6)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 应用集成（6）
- en: The last step of the product development cycle is to integrate the model prediction
    to the AI application. The common pattern is to host the models in the model serving
    service (which will be discussed in section 1.2.2) of the deep learning system
    and integrate the business application logic with the model by sending model prediction
    requests over the internet.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 产品开发周期的最后一步是将模型预测集成到AI应用中。常见的模式是将模型托管在深度学习系统的模型服务服务中，并通过互联网发送模型预测请求将业务应用逻辑与模型集成。
- en: As a sample user scenario, a chatbot user interacts with the chatbot user interface
    by typing or voicing questions. When the chatbot application receives input from
    the customer, it calls the remote model serving service to run a model prediction
    and then takes action or responds to the customer based on the model prediction
    result.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个示例用户场景，一个聊天机器人用户通过键入或发声问题与聊天机器人用户界面进行交互。当聊天机器人应用程序接收到来自客户的输入时，它调用远程模型服务服务来运行模型预测，然后根据模型预测结果采取行动或回应客户。
- en: Along with integrating model serving with application logic, this phase also
    involves evaluating metrics important to the product, such as clickthrough rate
    and churn rate. Nice ML-specific metrics (good precision–recall curve) do not
    always guarantee the business requirement is met. So the business stakeholders
    often perform customer interviews and product metric evaluation at this stage.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 除了将模型服务与应用逻辑集成外，此阶段还涉及评估对产品重要的指标，如点击率和流失率。良好的ML特定指标（良好的精确度-召回率曲线）并不总是能保证满足业务需求。因此，业务利益相关者通常在此阶段进行客户访谈和产品指标评估。
- en: 1.1.2 Roles in the development cycle
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1.2 开发周期中的角色
- en: Because you now have a clear idea of each step in a typical development cycle,
    let’s look at the key roles that collaborate in this cycle. The definitions, job
    titles, and responsibilities of each role may vary across organizations. So make
    sure you clarify who does what in your organization and adjust your system’s design
    appropriately.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 因为您现在对典型开发周期中的每个步骤有了清晰的了解，让我们来看看在这个周期中合作的关键角色。每个角色的定义、职称和职责可能因组织而异。所以确保您澄清了您的组织中谁做什么，并相应调整您系统的设计。
- en: Business stakeholders (product owner)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 业务利益相关者（产品所有者）
- en: 'Many organizations assign the stakeholder role to multiple positions, such
    as product managers, engineering managers, and senior developers. Business stakeholders
    define the business goal of a product and are responsible for communication and
    execution of the product development cycle. The following are their responsibilities:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 许多组织将利益相关者角色分配给多个职位，如产品经理、工程经理和高级开发人员。业务利益相关者定义产品的业务目标，并负责产品开发周期的沟通和执行。以下是他们的责任：
- en: Getting inspiration from deep learning research, discussing potential application
    of deep learning features in products, and driving product requirements that in
    turn drive model development
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从深度学习研究中获得灵感，讨论在产品中应用深度学习特性的潜在应用，并驱动推动模型开发的产品需求
- en: Owning the product! Communicating with customers and making sure the engineering
    solution meets the business requirement and delivers the results
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拥有产品！与客户沟通，确保工程解决方案符合业务需求并产生结果
- en: Coordinating cross-functional collaborations between different roles and teams
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 协调不同角色和团队之间的跨职能协作
- en: Running project development execution; providing guidance or feedback during
    the entire development cycle to ensure the deep learning features offer real value
    to the customers of the product
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行项目开发执行；在整个开发周期内提供指导或反馈，以确保深度学习特性为产品的客户提供真正的价值
- en: Evaluating the product metrics (such as user churn rate and feature usage)—not
    the model metrics (precision or accuracy)—and driving improvement of model development,
    productionization, or product integration
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估产品指标（如用户流失率和功能使用情况）—而不是模型指标（精度或准确性）—并推动模型开发、产品化或产品集成的改进
- en: Researchers
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员
- en: Machine learning researchers research and develop new and novel neural network
    architectures. They also develop techniques for improving model accuracy and efficiency
    in training models. These architectures and techniques can be used during model
    development.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习研究人员研究和开发新颖的神经网络架构。他们还开发提高模型准确性和训练模型效率的技术。这些架构和技术可以在模型开发过程中使用。
- en: Note The machine learning researcher role is often associated with big tech
    companies like Google, Microsoft, and Salesforce. In many other companies, data
    scientists fulfill the same role.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Data scientists
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Data scientists may wear a research hat, but most of the time, they translate
    a business problem into a machine learning problem and implement it using machine
    learning methods. Data scientists are motivated by the product’s need and apply
    research techniques to production data rather than standard benchmark datasets.
    Besides researching model algorithms, a data scientist’s responsibilities may
    also include
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Combining multiple deep learning neural network architectures and/or techniques
    from different research into a solution. Sometimes they apply additional machine
    learning techniques besides pure deep learning.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring available data, determining what data is useful, and deciding on how
    to preprocess it before supplying it for training.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prototyping different approaches (writing experimental code) to tackle the business
    problem.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting model prototyping code into production code with workflow automation.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Following the engineering process to ship models to production by using the
    deep learning system.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iterating on the need for any additional data that may help with model development.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuously monitoring and evaluating data and model performance in production.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Troubleshooting model-related problems, such as model degradation.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data engineers
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Data engineers help collect data and set up data pipelines for continuous data
    ingestion and processing, including data transformation, enrichment, and labeling.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: MLOps engineer/ML engineer
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: An MLOps engineer fulfills a number of roles across multiple domains, including
    that of data engineer, DevOps (operation) engineer, data scientist, and platform
    engineer. As well as setting up and operating the machine learning infrastructure
    (both systems and hardware), they manage automation pipelines to create datasets
    and train and deploy models. ML infrastructures and user activities, such as training
    and serving, are also monitored by MLOps engineers.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, MLOps is hard, because it requires people to master a set of
    practices across software development, operation, maintenance, and machine learning
    development. MLOps engineers’ goal is to ensure the creation, deployment, monitoring,
    and maintenance of machine learning models are efficient and reliable.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning system/platform engineer
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning system engineers build and maintain the general pieces of the
    machine learning infrastructure—the primary focus of this book—to support all
    the machine learning development activities for data scientists, data engineers,
    MLOps engineers, and AI applications. Among the components of the machine learning
    system are data warehouses, compute platforms, workflow orchestration services,
    model metadata and artifact stores, model training services, model serving services,
    and more.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Application engineer
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Application engineers build customer-facing applications (both frontend and
    backend) to address given business requirements. The application logic will make
    decisions or take actions based on the model prediction for a given customer request.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Note In the future, as machine learning systems (infrastructure) mature, the
    roles involved in deep learning development cycle will merge into fewer and fewer.
    Eventually, data scientists will be able to complete the entire cycle on their
    own.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 1.1.3 Deep learning development cycle walk-through
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'By giving an example, we can demonstrate the roles and the process in a more
    concrete manner. Suppose you have been assigned the task of building a customer
    support system that answers questions automatically about the company’s product
    lines. The following steps will guide you through the process of bringing that
    product to market:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: The product requirement is to build a customer support application that presents
    a menu, so customers can navigate to find answers to commonly asked questions.
    As the number of questions grows, the menu becomes larger, with many layers of
    navigation. Analytics has shown that many customers are confused by the navigation
    system and drop off from navigating the menu while trying to find answers.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The product manager (PM) who owns the product is motivated to improve the user
    retention rate and experience (finding the answers quickly). After conducting
    some research with customers, the PM finds that a majority of customers would
    like to obtain answers without a complex menu system, preferably as simple as
    asking questions in their natural language.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The PM reaches out to machine learning researchers for a potential solution.
    It turns out that deep learning may help. Experts think the technology is mature
    enough for this use case and suggest a few approaches based on deep learning models.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The PM writes a product spec indicating that the application should take one
    question from a customer at a time, recognize intent from the question, and match
    it with relevant answers.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data scientists receive product requirements and start to prototype deep learning
    models that fit the need. They first start data exploration to collect available
    training data and consult with researchers for the choices of algorithms. And
    then data scientists start to build prototyping code to produce experimental models.
    Eventually, they arrive at some datasets, a few training algorithms, and several
    models. After careful evaluation, one natural language process model is selected
    from various experiments.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then the PM assembles a team of platform engineers, MLOps engineers, and data
    engineers to work with the data scientist to onboard the prototyping code, made
    in step 5, to production. The work includes building a continuous data processing
    pipeline and a continuous model training, deployment, and evaluation pipeline,
    as well as setting up the model serving functionality. The PM also specifies the
    number of predictions per second and the latency required.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once a production setup is complete, the application engineers integrate the
    customer support service’s backend with the model serving service (built in step
    6), so when a user types in a question, the service will return answers based
    on the model prediction. The PM also defines product metrics, such as average
    time spent finding an answer, to evaluate the end result and use it to drive the
    next round of improvement.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 1.1.4 Scaling project development
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As you saw in section 1.1.2, we need to fill seven different roles to complete
    a deep learning project. The cross-functional collaboration between these roles
    happens at almost every step. For example, data engineers, platform developers,
    and data scientists work together to productionize a project. Anyone who has been
    involved in a project that requires many stakeholders knows how much communication
    and coordination are required to keep a project like this moving forward.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: These challenges make deep learning development hard to scale because we either
    don’t have the resources to fill all the required roles or we can’t meet the product
    timeline due to the communication costs and slowdowns. To reduce the enormous
    amount of operational work, communication, and cross-team coordination costs,
    companies are investing in machine learning infrastructure and reducing the number
    of people and the scope of knowledge required to build a machine learning project.
    The goal of a deep learning infrastructure stack is not only to automate model
    building and data processing but also to make it possible to merge the technical
    roles so that the data scientist is empowered to take care of all these functions
    autonomously within a project.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: A key success indicator of a deep learning system is to see how smooth the model
    productionization process can be. With a good infrastructure, the data scientist,
    who is not expected to suddenly become an expert DevOps or data engineer, should
    be able to implement models in a scalable manner, set up data pipelines, and deploy
    and monitor models in production independently.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: By using an efficient deep learning system, data scientists will be able to
    complete the development cycle with minimal additional overhead—less communication
    required and less time wasted waiting by others—and focus on the most important
    data science tasks, such as understanding the data and experimenting with algorithms.
    The ability to scale deep learning project development is the true value proposition
    of a deep learning system.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Deep learning system design overview
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With the context of section 1.1 in mind, let’s dive into the focus of this
    book: the deep learning system itself. Designing a system—any system—is the art
    of achieving goals under a set of constraints that are unique to your situation.
    This is also true for deep learning systems. For instance, let’s say you have
    a few deep learning models that need to be served at the same time, but your budget
    does not allow you to operate a machine that has enough memory to fit all of them
    at the same time. You may need to design a caching mechanism to swap models between
    memory and disk. Swapping, however, will increase inference latency. Whether this
    solution is feasible will depend on latency requirements. Another possibility
    is to operate multiple smaller machines for each model, if your model sizes and
    budget will allow it.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Or, for another example, imagine your company’s product must comply with certain
    certification standards. It may mandate data access policies that pose significant
    limitations to anyone who wants to gain access to data collected by the company’s
    product. You may need to design a framework to allow data access in a compliant
    fashion so that researchers, data scientists, and data engineers can troubleshoot
    problems and develop new models that require such data access in your deep learning
    system.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, there are many knobs that can be turned. It will certainly be
    an iterative process to arrive at a design that will satisfy as many requirements
    as possible. But to shorten the iterative process, it is desirable to start with
    a design that is as close to the end state as possible.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we first propose a deep learning system design with only essential
    components and then explain the responsibility of each of the components and user
    workflows. In our experience of designing and tailoring deep learning systems,
    a few key components are common across different designs. We think that they can
    be used as a reasonable starting point for your design. We call this the *reference
    system architecture*.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: You can make a copy of this reference for your design project, go through your
    list of goals and constraints, and start by identifying knobs in each component
    that you can adjust to your needs. Because this isn’t an authoritative architecture,
    you should also assess whether all components are really necessary and add or
    remove components as you see fit.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.1 Reference system architecture
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Figure 1.3 shows the high-level overview of the reference deep learning system.
    The deep learning system has two major parts. The first is the application programming
    interface (API; box A) for the system, located in the middle of the diagram. The
    second is the collection of components of the deep learning system, which is represented
    by all the rectangular boxes located within the large box, outlined in a dotted
    line and taking up the lower half of the diagram. These boxes each represent a
    system component:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: API (box A)
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: API（框A）
- en: Dataset manager (box B)
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集管理器（框B）
- en: Model trainer (box C)
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型训练器（框C）
- en: Model serving (box D)
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型服务（框D）
- en: Metadata and artifacts store (box E)
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元数据和工件存储（框E）
- en: Workflow orchestration (box F)
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作流编排（框F）
- en: Interactive data science environment (box G)
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交互式数据科学环境（框G）
- en: In this book, we assume that these system components are *microservices*.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们假设这些系统组件是*微服务*。
- en: '![](../Images/01-03.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01-03.png)'
- en: Figure 1.3 An overview of a typical deep learning system that includes basic
    components to support a deep learning development cycle. This reference architecture
    can be used as a starting point and further tailored. In later chapters, we discuss
    each component in detail and explain how it fits into this big picture.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3 典型深度学习系统的概览，包括支持深度学习开发周期的基本组件。这个参考架构可以作为一个起点，进行进一步的定制。在后面的章节中，我们将详细讨论每个组件，并解释它们如何融入这一大局。
- en: Definition There is no single definition for microservices. Here, we will use
    the term to mean processes that communicate over a network with the HTTP or the
    gRPC protocol.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 定义：对于微服务，没有单一的定义。在这里，我们将使用该术语来指代使用HTTP或gRPC协议与网络通信的进程。
- en: This assumption means we can expect these components to reasonably support multiple
    users with different roles securely and are readily accessible over a network
    or the internet. (This book, however, will not cover all engineering aspects of
    how microservices are designed or built. We will focus our discussion on specifics
    that are relevant to deep learning systems.)
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这一假设意味着我们可以合理地期望这些组件能够安全地支持具有不同角色的多个用户，并且可以方便地通过网络或互联网访问。（然而，本书将不涵盖微服务的所有工程方面的设计或构建。我们将重点讨论与深度学习系统相关的具体内容。）
- en: NOTE You may wonder whether you need to design, build, and host all deep learning
    system components on your own. Indeed, there are open source (Kubeflow) and hosted
    alternatives (Amazon SageMaker) for them. We hope that after you have learned
    the fundamentals of each component, how they fit in the big picture, and how they
    are used by different roles, you will make the best decision for your use case.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：你可能会想知道，你是否需要自己设计、构建和托管所有深度学习系统组件。实际上，有开源（Kubeflow）和托管的替代方案（Amazon SageMaker）可供选择。我们希望在你学习了每个组件的基本知识、它们如何融入整体架构以及不同角色如何使用后，你能为你的使用场景做出最佳决策。
- en: 1.2.2 Key components
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.2 关键组件
- en: Now let’s walk through the key components that we consider essential to a basic
    deep learning system, as shown in figure 1.3\. You may want to add additional
    components or simplify them further as you see fit for your requirements.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们详细介绍我们认为对基本深度学习系统至关重要的关键组件，如图1.3所示。你可能希望根据自己的需求添加其他组件或进一步简化。
- en: Application programming interface
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序编程接口
- en: The entry point (box A in figure 1.3) of our deep learning system is an API
    that is accessible over a network. We opted for an API because the system needs
    to support not only graphical user interfaces that will be used by researchers,
    data scientists, data engineers, and the like but also applications and possibly
    other systems—for example, a data warehouse from a partner organization.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们深度学习系统的入口点（图1.3中的框A）是一个通过网络访问的API。我们选择API是因为系统不仅需要支持研究人员、数据科学家、数据工程师等使用的图形用户界面，还需要支持应用程序和可能来自合作伙伴组织的数据仓库等其他系统。
- en: Although conceptually the API is the single point of entry of the system, it
    is entirely possible that the API is defined as the sum of all APIs provided by
    each component, without an extra layer that aggregates everything under a single-service
    endpoint. Throughout this book, we will use the sum of all APIs provided by each
    component directly and skip the aggregation for simplicity.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在概念上API是系统的唯一入口点，但完全有可能将API定义为每个组件提供的所有API的总和，而没有额外的层将所有内容聚合在单一服务端点下。在本书中，我们将直接使用每个组件提供的所有API的总和，并跳过聚合以简化问题。
- en: Note Should you use a centralized or distributed deep learning system API? In
    the reference architecture (figure 1.3), the deep learning system API is shown
    as a single box. It should be interpreted as a logical container for the complete
    set of your deep learning system API, regardless of whether it is implemented
    on single (e.g., an API gateway that proxies for all components) or multiple service
    endpoints (direct interaction with each component). Each implementation has its
    own merits and shortcomings, and you should work with your team to figure out
    what functions best. Direct interaction with each component may be easier if you
    start with a small use case and team.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Dataset manager
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning is based on data. There is no doubt that the data management component
    is a central piece of a deep learning system. Every learning system is a garbage-in,
    garbage-out system, so ensuring good data quality for learning is of paramount
    importance. A good data management component should provide the solution to this
    problem. It enables collecting, organizing, describing, and storing data, which
    in turn makes it possible for data to be explored, labeled, and used for training
    models.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'In figure 1.3, we can see at least four relationships of the dataset manager
    (box B) with other parties:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Data collectors push raw data to the dataset manager to create or update datasets.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The workflow orchestration service (box F) executes the data process pipeline,
    which pulls data from the dataset manager to enhance the training dataset or transform
    the data format and pushes the result back.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data scientists, researchers, and data engineers use Jupyter Notebook (box G)
    to pull data from the data manager for data exploration and examination.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model training service (box C) pulls training data from the data manger
    for model training.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In chapter 2, we will discuss dataset management in depth. Throughout the book,
    we use the term *dataset* as a unit of collected data that may be related.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Model trainer
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Model trainer (aka, model training service; box C) responds to provide foundational
    computation resources, such as CPUs, RAM, and GPUs, and job management logics
    to run the model training code and produce model files. In figure 1.3, we can
    see that the workflow orchestration service (box F) tells the model trainer to
    execute a model training code. The trainer takes input training data from the
    dataset manager (box B) and produces a model. Then it uploads the model with training
    metrics and metadata to the metadata and artifacts store (box E).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'It is usually necessary to perform intense computation on a large dataset to
    produce high-quality deep learning models that can make accurate predictions.
    Adoption of new algorithms and training libraries/frameworks is also a critical
    requirement. These requirements produce challenges on several levels:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '*Capability of reducing model training time*—Despite the growing size of training
    data and complexity of model architecture, training systems must keep training
    times reasonable.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Horizontal scalability*—An effective production training system should be
    able to support multiple training requests from different applications and users
    simultaneously.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cost of adopting new technologies*—The deep learning community is a vigorous
    one, with constant updates and improvements to algorithms and tooling (SDK, framework).
    The training system should be flexible enough to accommodate new innovations easily
    without interfering with the existing workload.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In chapter 3, we will investigate different approaches to solving the aforementioned
    problems. We will not go deep into the theoretical aspect of training algorithms
    in this book, as they do not affect how we design the system. In chapter 4, we
    will look at how we can distribute training to accelerate the process. In chapter
    5, we will explore a few different approaches for optimizing training hyperparameters.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Model serving
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'Models can be used in various settings, such as online inference for real-time
    predictions or offline inference for batch predictions using large volumes of
    input data. This is where model serving surfaces—when a system hosts the model,
    takes input prediction requests, runs model prediction, and returns the prediction
    to users. There are a few key questions to be answered:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Are your inference requests coming from over the network? Or are they coming
    from sensors that need to be served locally?
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is an acceptable latency? Are inference requests ad hoc or streaming?
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many models are being served? Is each model individually serving a type
    of inference request, or is an ensemble of models doing so?
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How large are model sizes? How much memory capacity do you need to budget?
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What model architectures need to be supported? Does it require a GPU? How much
    computing resources do you need to produce inferences? Are there other supporting
    serving components—for example, embeddings, normalization, aggregation, etc.?
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are there sufficient resources to keep models online? Or is a swapping (such
    as moving models between memory and disk) strategy needed?
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From figure 1.3, the main input and output of the model serving (box D) are
    inference requests and the prediction returned, respectively. To produce inferences,
    models are retrieved from the metadata and artifacts store (box E). Some requests
    and their responses may be logged and sent to the model monitoring and evaluation
    service (not shown in figure 1.3 or covered in this book), which detects anomalies
    from this data and produces alerts. In chapters 6 and 7, we will go deeper into
    model serving architecture, explore these key aspects, and discuss their solutions.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Metadata and artifacts store
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Imagine working on a simple deep learning application as a one-person team,
    where you have to work with only a few datasets and train and deploy only one
    type of model. You can probably keep track of how datasets, training codes, models,
    inference codes, and inferences are related to one another. These relationships
    are essential for model development and troubleshooting as you need to be able
    to trace certain observations back to the cause.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Now imagine adding more applications, more people, and more model types. The
    number of these relationships will grow exponentially. In a deep learning system
    that is designed to support multiple types of users working on multiple datasets,
    code, and models at various stages, there is a need for a component that keeps
    track of the web of relationships. The metadata and artifacts store in a deep
    learning system does just that. Artifacts include code that trains models and
    produces inferences, as well as any generated data such as trained models, inferences,
    and metrics. Metadata is any data that describes an artifact or the relationship
    between artifacts. Some concrete examples are
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: The author and version of a piece of training code
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A reference of the input training dataset and the training environment of a
    trained model
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training metrics of a trained model, such as training date and time, duration,
    and the owner of the training job
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model-specific metrics, such as model version, model lineage (data and code
    used in training), and performance metrics
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model, request, and inference code that produce a certain inference
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Workflow history, tracking each step of the model training and data process
    pipeline runs
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are just a few examples of what a baseline metadata and artifacts store
    would help track. You should tailor the component to the needs of your team or
    organization.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Every other component that generates metadata and artifacts in figure 1.3 would
    flow into the metadata and artifacts store (box E). The store also plays an important
    role in model serving because it provides model files and their metadata to the
    model serving service (box D). Although not shown in the figure, custom tools
    for trace lineage and troubleshooting are usually built at the user interface
    layer, powered by the metadata and artifacts store.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: As we proceed through chapter 8, we will look at a baseline metadata and artifacts
    store. This store is usually the central component of a deep learning system’s
    user interface.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Workflow orchestration
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Workflow orchestration (figure 1.3, box F) is ubiquitous in many systems, where
    it helps to automatically launch computation tasks triggered by programmatic conditions.
    In the context of a machine learning system, the workflow orchestration is the
    driving force behind all the automations running in a deep learning system. It
    allows people to define workflows or pipelines—directed acyclic graphs (DAGs)—to
    glue individual tasks together with an execution order. The workflow orchestration
    component orchestrates the task executions of these workflows. Some typical examples
    are
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Launching model training whenever a new dataset is built
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring upstream data sources, augmenting new data, transferring its format,
    notifying external labelers, and merging the new data into existing datasets
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying the trained model to the model server if it passes some accepted criteria
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continually monitoring model performance metrics and alerting developers when
    degradation is detected
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will learn how to build or set up a workflow orchestration system in chapter
    9.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Interactive data science environment
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Customer data and models cannot be downloaded to a local workstation from production
    for compliance and security reasons. For data scientists to interactively explore
    data, troubleshoot pipeline execution in workflow orchestration, and debug models,
    a remote interactive data science environment (figure 1.3, box G) located inside
    the deep learning system is required.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: It is common for companies to set up their own trusted data science environment
    by using open source Jupyter Notebooks ([https://jupyter.org/](https://jupyter.org/))
    or by utilizing cloud vendors’ JupyterLab-based solutions, such as Amazon SageMaker
    Studio ([https://aws.amazon.com/sagemaker/studio/](https://aws.amazon.com/sagemaker/studio/)).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical interactive data science environment should provide the following
    functionalities:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '*Data exploration*—Offers data scientists easy access to customer data but
    keeps it secure and compliant; there are no data leaks, and any unauthorized data
    access will be rejected.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model prototyping*—Provides the much-needed tooling for data scientists to
    develop quick POC models inside the deep learning system.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Troubleshooting*—Enables engineers to debug any activity happening inside
    the deep learning system, such as downloading the model and playing with it to
    analyze its behavior or checking all the input/output artifacts (intermediate
    datasets or configurations) from a failed pipeline.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1.2.3 Key user scenarios
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To better understand how deep learning systems can be used during the development
    cycle (figure 1.1), we prepared sample scenarios that illustrate how they could
    be used. Let’s start with programmatic consumers, shown in figure 1.4\. Data collectors
    that push data to the system will usually end up at the data management service
    via the API, which collects and organizes raw data for model training.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/01-04.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
- en: Figure 1.4 Data is pushed from sources or collectors, through the API to the
    data management service, where the data is further organized and stored in formats
    that are more friendly for model training.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning applications will usually hit the model inference service to obtain
    inferences from a trained model, which is used to power deep learning features
    that end users will consume. Figure 1.5 shows the sequence of this interaction.
    Scripts, or even full-fledged management services, can be programmatic consumers,
    too. Because they are optional, we omitted them from the figure for simplicity.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/01-05.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
- en: Figure 1.5 Deep learning applications request inferences through the API. The
    model inference service accepts and processes these requests against trained models
    and produces inferences that are returned back to applications.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Between human consumers and the API usually lies an extra layer—the user interface.
    The interface can be web based or command-line based. Some power users may even
    skip this interface and consume the API directly. Let’s walk through each persona
    one by one.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: A typical scenario of researchers using the system is illustrated in figure
    1.6\. Researchers can look up available data to try out their new modeling technique.
    They access the user interface and visit the data exploration and visualization
    section, which pulls data from the data management service. A great deal of manual
    data processing might be involved in massaging it into forms that can be consumed
    by new training techniques. Once researchers settle with a technique, they can
    package it as a library for others’ consumption.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/01-06.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
- en: Figure 1.6 A usage sequence of a researcher who is interested in seeing what
    data is available for researching and developing a new modeling technique. The
    researcher interacts with a user interface that is supported by the API and data
    management behind the scenes.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Data scientists and engineers can work on use cases by first looking at what
    data is available, similar to what researchers would initially do in the previous
    paragraph. This would be supported by the data management service. They make hypotheses
    and put together data processing and training techniques as code. These steps
    can be combined to form a workflow using the workflow management service.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: When the workflow management service performs a run of the workflow, it contacts
    the data management service and the model training service to perform actual duties
    and track their progress. Hyperparameters, code versions, model training metrics,
    and test results are all stored to the metadata and artifacts store by each service
    and training code.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Through the user interface, data scientists and engineers can compare experimental
    runs and deduce the best way to train models. This aforementioned scenario is
    shown in figure 1.7.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/01-07.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
- en: Figure 1.7 A usage sequence of a data scientist defining model training workflow,
    running it, and reviewing results
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Product managers can also look at and query all kinds of metrics throughout
    the system through the user interface. The metrics data can be supplied by the
    metadata and artifacts store.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.4 Derive your own design
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have gone over all aspects of the reference system architecture,
    let’s discuss some guidelines for customizing your own version.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Gathering goals and requirements
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: The first step to designing any successful system design is to have a set of
    clear goals and requirements with which to work. These should ideally come from
    users of your system, either directly or indirectly through the product management
    team or engineering management. This short list of goals and requirements will
    help you form a vision of what your system will look like. This vision, in turn,
    should be the guideline that drives you throughout the design and development
    phases of your system.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Note Sometimes engineers are asked to develop a system to support one or more
    deep learning applications that already exist. In this case, you may instead start
    with identifying the set of common requirements among these applications and how
    your system can be designed to help bring innovation quickly to these applications.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: To collect the system goals and requirements, you need to identify the different
    types of users and stakeholders, or *personas*, of the system. (This is a general
    concept that can be applied to most system design problems.) It is the users,
    after all, who will help you articulate the goals and requirements of the system.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'Our recommendation is to start with use cases or application requirements if
    you are not sure of a good starting point. Here are some example questions that
    you can ask your users:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '*To data engineers and product managers*—Does the system allow applications
    to collect data for training? Does the system need to handle streaming input data?
    How much data is being collected?'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*To data scientists and engineers*—How do we process and label the data? Does
    the system need to provide labeling tools for external vendors? How do we evaluate
    the model? How do we handle the test dataset? Is an interactive notebooking user
    interface needed for data science work?'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*To researchers and data scientists*—How large of a volume of data is needed
    for training models? What’s the average time of model training runs? How much
    computing and data capacity is needed for research and data science? What kind
    of experiments should the system support? What metadata and metrics need to be
    collected to evaluate different experiments?'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*To product managers and software engineers*—Is model serving done on the remote
    server or on the client? Is it a real-time model inference or offline batch prediction?
    Is there a latency requirement?'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*To product managers*—What problems are we trying to solve at our organization?
    What is our business model? How are we going to gauge the effectiveness of our
    implementations?'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*To security teams*—What level of security is needed in your system? Is data
    access wide open or strictly restricted/isolated? Is there an audit requirement?
    Is there a certain level of compliance or certification (e.g., General Data Protection
    Regulation, System and Organization Controls 2, etc.) that the system needs to
    achieve?'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customizing the reference architecture
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Once the design requirement and scope are clear, we can start to customize the
    reference architecture in figure 1.3\. First, we can decide whether we need to
    add or remove any components. For example, if the requirement is purely managing
    model training in a remote server farm, we could remove the workflow management
    component. If data scientists want to evaluate model performance effectively with
    production data, they could also add an experiment management component. This
    component allows data scientists to perform training and validation using full-scale
    data that already exists in the system and conduct online A/B testing against
    production traffic with previously unseen data.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: The second step is to design and implement each key component suite to your
    specific needs. Depending on the requirement, you might exclude the data streaming
    API from the dataset management service and add distributed training support if
    training speed is a concern. You can either build each key component from scratch
    or use open source software. In the rest of the book, we cover both options for
    each key component to ensure you know what to do.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: TIP Keep the system design simple and user friendly. The purpose of creating
    such a large deep learning system is to improve the productivity of deep learning
    development, so please keep this in mind when designing it. We want to make it
    easy for data scientists to build high-quality models without the need to learn
    what’s going on in the underlying system.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.5 Building components on top of Kubernetes
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have introduced a list of key components that are implemented as services.
    With this number of services, you may want to manage them with a sophisticated
    system at the infrastructure level, such as Kubernetes.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes is an open source system for automating deployment, scaling, and
    management of containerized applications, which are applications that run in isolated
    runtime environments—for example, docker containers. We have seen a number of
    deep learning systems that are built on top of Kubernetes. Some people learn how
    to use Kubernetes without ever knowing why it is used to run deep learning services,
    so we want to explain the thinking behind it. If you are familiar with Kubernetes,
    please feel free to skip this section.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Note Kubernetes is a complex platform that would require a book-length of material
    to teach, so we are only discussing its merits for a deep learning system. If
    you want to learn Kubernetes, we highly recommend you check out *Kubernetes in
    Action* (Manning, 2018), by Marko Lukša.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Challenges for managing computing resources
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Executing one docker container on a remote server seems to be a simple task,
    but running 200 containers on 30 different servers is a different story. There
    are many challenges, such as monitoring all remote servers to determine on which
    one to run the container, needing to failover a container to a healthy server,
    restarting a container when it’s stuck, following up each container run and getting
    notified when it completes, etc. To address these challenges, we must monitor
    the hardware, OS processes, and networking ourselves. Not only is it technically
    challenging, but it is also a huge amount of work.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: How Kubernetes helps
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes is an open source container orchestration platform for scheduling
    and automating the deployment, management, and scaling of containerized applications.
    Once you set up a Kubernetes cluster, your server groups’ operation (deployment,
    patching, updates) and resources become manageable. Here is a deployment example:
    you can tell Kubernetes to run a docker image with 16 GB memory and 1 GPU with
    a command; Kubernetes will allocate the resource to run this docker image for
    you.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: This is a huge benefit for software developers because not every one of them
    has extensive experience with hardware and deployment. With Kubernetes, we only
    need to declare the end state of our cluster, and Kubernetes will do the actual
    job to meet our goals.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides container deployment benefits, the following are some other key Kubernetes
    functionalities that are crucial for managing our training containers:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '*Autoscaling features*—Kubernetes automatically resizes the number of nodes
    in the cluster based on the workload. This means if there is a sudden burst of
    user requests, Kubernetes will add the capacity automatically, which is called
    *elastic compute management*.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Self-healing capabilities*—Kubernetes restarts, replaces, or reschedules pods
    when they fail or when nodes die. It also kills pods that do not respond to user-defined
    health checks.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Resource utilization and isolation*—Kubernetes takes care of computing resource
    saturation; it ensures every server is fully utilized. Internally, Kubernetes
    launches application containers in *pods*. Each pod is an isolated environment
    with a computing resource guarantee, and it runs a function unit. In Kubernetes,
    multiple pods can be in one node (server) as long as their combined resource requirements
    (CPU, memory, disk) don’t exceed the node’s limitations, so servers can be shared
    by different function units easily with guaranteed isolation.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Namespaces*—Kubernetes supports splitting a physical cluster into different
    virtual clusters. These virtual clusters are called *namespaces*. You can define
    resource quota per namespace, which allows you to isolate resources for different
    teams by assigning them to different namespaces.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the flip side, these benefits come at a cost—they consume resources as well.
    When you run a Kubernetes pod, the pod itself takes some amount of system resources
    (CPU, memory). These resources are consumed on top of those that are needed to
    run containers inside pods. Kubernetes’s overhead seems reasonable in many situations;
    for example, from an experiment published in the article “How We Minimized the
    Overhead of Kubernetes in Our Job System” ([http://mng.bz/DZBV](http://mng.bz/DZBV))
    by Lally Singh and Ashwin Venkatesan (February 2021), the CPU overhead per pod
    was about 10 ms per second.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Note We recommend you check out appendix B to see how existing deep learning
    systems relate to the concepts presented in this chapter. In that appendix, we
    compare the reference architecture described in section 1.2.1 with Amazon SageMaker,
    Google Vertex AI, Microsoft Azure Machine Learning, and Kubeflow.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 Building a deep learning system vs. developing a model
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A final piece of groundwork before we begin: we think it is crucial to call
    out the differences between *building a deep learning system* and *developing
    a deep learning model*. In this book, we define *the practice of developing a
    deep learning model* to solve a problem as the process of'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: Exploring available data and how it can be transformed for training
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determining the effective training algorithm(s) to use for the problem
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training models and developing inference code to test against unseen data
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recall that a deep learning system should support not only all tasks required
    by model development but also those that need to be performed by other roles and
    make collaboration between these roles seamless. When building a deep learning
    system, you are not developing deep learning models; you are building a system
    that supports the development of deep learning models, making that process more
    efficient and scalable.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: We have found an abundance of material published about building the models.
    But we have seen precious little written about designing and building the platforms
    or systems that support those models. And that is why we wrote this book.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A typical machine learning project development goes through the following cycle:
    product initiation, data exploration, model prototyping, productionization, and
    production integration.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are seven different roles involved in deep learning project development:
    a product manager, researchers, data scientists, data engineers, MLOps engineers,
    machine learning system engineers, and application engineers.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A deep learning system should reduce complexity in the deep learning development
    cycle.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the help of deep learning systems, the data scientist, who is not expected
    to suddenly become an expert DevOps or data engineer, should be able to implement
    models in a scalable manner, set up data pipelines, and deploy and monitor models
    in production independently.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An efficient deep learning system should allow data scientists to focus on interesting
    and important data science tasks.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A high-level reference architecture like the one we present in figure 1.3 can
    help you quickly start a new design. First, make your own copy and then collect
    goals and requirements. Finally, add, modify, or subtract components and their
    relationships as you see fit.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A basic deep learning system consists of the following key components: dataset
    manager, model trainer, model serving, metadata and artifacts store, workflow
    orchestration, and data science environment.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data management component helps collect, organize, describe, and store data
    as datasets that can be used as training input. It also supports data exploration
    activities and tracks lineage between datasets. Chapter 2 will discuss data management
    in detail.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model training component is responsible for handling multiple training requests
    and running them efficiently provided a given, limited set of computing resources.
    Chapters 3 and 4 will review the model training component.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model serving component handles incoming inference requests, produces inferences
    with models, and returns them to requesters. It will be covered in chapters 6
    and 7.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The metadata and artifacts store component records metadata and stores artifacts
    from the rest of the system. Any data produced by the system can be treated as
    artifacts. Most of them would be models, which come with metadata that will be
    stored in the same component. This provides complete lineage information to support
    experimentation and troubleshooting. We will talk about this component in chapter
    8.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The workflow management component stores workflow definitions that chain together
    different steps in data processing and model training. It is responsible for triggering
    periodic workflow runs and tracking the progress of each run step that is being
    executed on other components—for instance, a model training step being executed
    on the model training service. In chapter 9, we will provide a walk-through of
    this component.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A deep learning system should support the deep learning development cycle and
    make collaboration between multiple roles easy.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a deep learning system is different from developing a deep learning
    model. The system is the infrastructure to support deep learning model development.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
