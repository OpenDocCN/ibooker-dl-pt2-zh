- en: 1 An introduction to deep learning systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Defining a deep learning system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The product development cycle and how a deep learning system supports it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An overview of a basic deep learning system and its components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Differences between building a deep learning system and developing models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter will prepare you with a big-picture mental model of a deep learning
    system. We will review some definitions and provide a reference system architecture
    design and a complete sample implementation of the architecture. We hope this
    mental model will prime you to see how the rest of the chapters, which address
    each system component in detail, fit into the whole picture.
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin this chapter, we will discuss an even bigger picture beyond the deep
    learning system: something we call *the deep learning development cycle*. This
    cycle outlines the various roles and stages involved in bringing products based
    on deep learning to market. The model and the platform do not exist in a vacuum;
    they affect and are affected by product management, market research, production,
    and other stages. We believe that engineers design better systems when they understand
    this cycle and know what each team does and what it needs to do its job.'
  prefs: []
  type: TYPE_NORMAL
- en: In section 1.2, we start our discussion of deep learning system design with
    a sample architecture of a typical system that can be adapted for designing your
    own deep learning system. The components described in this section will be explored
    in greater detail in their own chapters. Finally, we will emphasize the differences
    between developing a model and developing a deep learning system. This distinction
    is often a point of confusion, so we want to clear it up right away.
  prefs: []
  type: TYPE_NORMAL
- en: After reading this introductory chapter, you will have a solid understanding
    of the deep learning landscape. You will also be able to start creating your own
    deep learning system design, as well as understand existing designs and how to
    use and extend them, so you don’t have to build everything from scratch. As you
    continue reading this book, you will see how everything connects and works together
    as a deep learning system.
  prefs: []
  type: TYPE_NORMAL
- en: Terminology
  prefs: []
  type: TYPE_NORMAL
- en: Before we proceed with the rest of the chapter (and the rest of the book), let’s
    define and clarify a few terms that we use throughout the book.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning vs. machine learning
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning is machine learning, but it is considered an evolution of machine
    learning. Machine learning, by definition, is an application of artificial intelligence
    that includes algorithms that parse data, learn from that data, and then apply
    what it has learned to make informed decisions. Deep learning is a special form
    of machine learning that uses a programmable neural network as the algorithm to
    learn from data and make accurate decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Although this book primarily focuses on teaching you how to build the system
    or infrastructure to facilitate deep learning development (all the examples are
    neural network algorithms), the design and project development concepts we discuss
    are all applicable to machine learning as well. So, in this book we use the terms
    *deep learning* and *machine learning* somewhat interchangeably. For example,
    the deep learning development cycle introduced in this chapter and the data management
    service introduced in chapter 2 work in the machine learning context, too.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning use case
  prefs: []
  type: TYPE_NORMAL
- en: A deep learning use case refers to a scenario that utilizes deep learning technology—in
    other words, a problem that you want to solve using deep learning. Examples include
  prefs: []
  type: TYPE_NORMAL
- en: '*Chatbot*—A user can initiate a text-based conversation with a virtual agent
    on a customer support website. The virtual agent uses a deep learning model to
    understand sentences that the user enters and carries on a conversation with the
    user like a real human.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Self-driving car*—A driver can put a car into an assistive driving mode that
    automatically steers itself according to road markings. Markings are captured
    by multiple cameras on board the car to form a perception of the road using deep
    learning–based computer vision technology.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model, prediction and inference, and model serving
  prefs: []
  type: TYPE_NORMAL
- en: 'These three terms are described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Model*—A deep learning model can be seen as an executable program that contains
    an algorithm (model architecture) and required data to make a prediction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Prediction and inference*—Both model prediction and model inference refer
    to executing the model with given data to get a set of outputs. As prediction
    and inference are used widely in the context of model serving, they are used interchangeably
    in this book.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model serving* (prediction service)—This book describes model serving as hosting
    machine learning models in a web application (on the cloud or on premises) and
    allowing deep learning applications to integrate the model functionality into
    their systems through an API. The model serving web program is usually referred
    to as the prediction service or model serving service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning application
  prefs: []
  type: TYPE_NORMAL
- en: 'A deep learning application is a piece of software that utilizes deep learning
    technologies to solve problems. It usually does not perform any computationally
    intensive tasks, such as data crunching, deep learning model training, and model
    serving (with the exception of hosting models at the edge, such as an autonomous
    vehicle). Examples include:'
  prefs: []
  type: TYPE_NORMAL
- en: A *chatbot application* that provides a UI or APIs to take natural sentences
    as input from a user, interprets them, takes actions, and provides a meaningful
    response to the user. Based on the model output calculated in the deep learning
    system (from model serving service), the chatbot responds and takes action.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Self-driving software* that takes input from multiple sensors, such as video
    cameras, proximity sensors, and LiDAR, to form a perception of a car’s surroundings
    with the help of deep learning models and drives the car accordingly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Platform vs. system vs. infrastructure
  prefs: []
  type: TYPE_NORMAL
- en: 'In this book, the terms *deep learning platform*, *deep learning system*, and
    *deep learning infrastructure* all share the same meaning: an underlying system
    that provides all necessary support for building deep learning applications efficiently
    and to scale. We tend to use *system* most commonly, but in the context of this
    book, all three terms have the same meaning.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’re all on the same page about the terms, let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 The deep learning development cycle
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we’ve said, deep learning systems are the infrastructure necessary for deep
    learning *project development* to progress efficiently. So, before we dive into
    the structure of a deep learning system, it’s prudent to look at the development
    paradigm that a deep learning system enables. We call this paradigm the *deep
    learning development cycle*.
  prefs: []
  type: TYPE_NORMAL
- en: You may wonder why, in a technical book, we want to emphasize something that
    is as nontechnical as product development. The fact is that the goal of most deep
    learning work is, in the end, to bring a product or service to market. Yet many
    engineers are not familiar with the other stages of product development, just
    as many product developers do not know about engineering or modeling. From our
    experience in building deep learning systems, we have learned that persuading
    people in multiple roles in a company to adopt a system largely depends on whether
    the system will actually fix their particular problems. We believe that outlining
    the various stages and roles in the deep learning development cycle helps to articulate,
    address, communicate, and eventually solve everyone’s pain points.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding this cycle solves a few other problems, as well. In the last decade,
    many new deep learning software packages have been developed to address different
    areas. Some of them tackle model training and serving, whereas others handle model
    performance tracking and experimentation. Data scientists and engineers would
    piece these tools together each time they needed to solve a specific application
    or use case; this is called MLOps (machine learning operations). As the number
    of these applications grows, piecing these tools together every time from scratch
    for a new application becomes repetitive and time-consuming. At the same time,
    as the importance of these applications grows, so do the expectations for their
    quality. Both of these concerns call for a consistent approach to developing and
    delivering deep learning features quickly and reliably. This consistent approach
    starts with everyone working under the same deep learning development paradigm
    or cycle.
  prefs: []
  type: TYPE_NORMAL
- en: How does the deep learning *system* fit into the deep learning *cycle*? A well-built
    deep learning system would support the product development cycle and make performing
    the cycle easy, quick, and reliable. Ideally, data scientists can use a deep learning
    system as the infrastructure to complete the entire deep learning cycle without
    learning all the engineering details of the underlying complex systems.
  prefs: []
  type: TYPE_NORMAL
- en: Because every product and organization is unique, it is crucial for system builders
    to understand the unique requirements of the various roles to build a successful
    system. By “successful,” we mean one that helps stakeholders collaborate productively
    to deliver deep learning features quickly. Throughout this book, as we go through
    the design principles of deep learning systems and look at how each component
    works, your understanding of your stakeholder requirements will help you adapt
    this knowledge to form your own system design. As we discuss the technical details,
    we will point out when you need to pay attention to certain types of stakeholders
    during the design of the system. The deep learning development cycle will serve
    as the guiding framework when we consider the design requirements of each component
    of a deep learning system.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with a picture. Figure 1.1 illustrates what a typical cycle looks
    like. It shows the machine learning (especially deep learning) development progress
    phase by phase. As you can see, cross-functional collaboration happens at almost
    every step. We will discuss each phase and role involved in this diagram in the
    following two sections.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/01-01.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.1 A typical scenario to bring deep learning from research to a product.
    We call this *the deep learning development cycle*.
  prefs: []
  type: TYPE_NORMAL
- en: 1.1.1 Phases in the deep learning product development cycle
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The deep learning development cycle typically begins with a business opportunity
    and is driven by a product plan and its management. After that, the cycle normally
    goes through four phases: data exploration, prototyping, productionization (shipping
    to production), and application integration. Let’s look at these phases one at
    a time. Then we’ll look at all the roles involved (denoted by the icon of a person
    in figure 1.1).'
  prefs: []
  type: TYPE_NORMAL
- en: Note The number in parentheses next to each following subsection corresponds
    to the same number in figure 1.1.
  prefs: []
  type: TYPE_NORMAL
- en: Product initiation (1)
  prefs: []
  type: TYPE_NORMAL
- en: First, the business stakeholder (product owner or project manager) analyzes
    the business and identifies a potential business opportunity or problem that can
    be addressed with machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Data exploration (2)
  prefs: []
  type: TYPE_NORMAL
- en: When data scientists have a clear understanding of business requirements, they
    begin to work with data engineers to collect as much data as possible, label it,
    and build datasets. Data collection can include searching publicly available data
    and exploring internal sources. Data cleaning may also occur. Data labeling can
    either be outsourced or performed in-house.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to the following phases, this early phase of data exploration is unstructured
    and often done casually. It might be a Python script or shell script, or even
    a manual copy of data. Data scientists often use web-based data analysis applications,
    such as Jupyter Notebook (open source; [https://jupyter.org](https://jupyter.org)),
    Amazon SageMaker Data Wrangler ([https://aws.amazon.com/sagemaker/data-wrangler](https://aws.amazon.com/sagemaker/data-wrangler)),
    and Databricks ([www.databricks.com](http://www.databricks.com)), to analyze data.
    There is no formal data collection pipeline that needs to be built.
  prefs: []
  type: TYPE_NORMAL
- en: Data exploration is not only important but also critical to the success of a
    deep learning project. The more relevant data is available, the higher the likelihood
    of building effective and efficient deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Research and prototyping (3, 4)
  prefs: []
  type: TYPE_NORMAL
- en: The goal of prototyping is to find the most feasible algorithm/approach to address
    the business requirement (from product owner) with the given data. In this phase,
    data scientists can work with AI researchers to propose and evaluate different
    training algorithms with datasets built from the previous data exploration phase.
    Data scientists usually pilot multiple ideas in this phase and build proof-of-concept
    (POC) models to evaluate them.
  prefs: []
  type: TYPE_NORMAL
- en: Although newly published algorithms are often considered, most of them will
    not be adopted. The accuracy of an algorithm is not the only factor to be considered;
    one also must consider computing resource requirements, data volume, and algorithm
    implementation cost when evaluating an algorithm. The most practical approach
    is usually the winner.
  prefs: []
  type: TYPE_NORMAL
- en: Note that due to resource constraints, researchers are not always involved in
    the prototyping phase. Frequently, data scientists do the research work as well
    as build the POC.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may also notice that in figure 1.1, there is an inner loop (loop A) in
    the big development cycle: Product Initiation > Data Exploration > Deep Learning
    Research > Prototyping > Model > Product Initiation. The purpose of this loop
    is to obtain product feedback in the early phase by building a POC model. We may
    run through this loop multiple times until all stakeholders (data scientists,
    product owners) arrive at a consensus on the algorithms and data that will be
    used to address the business requirement.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiple hard lessons finally taught us that we must vet the solution with
    the product team or the customer (even better) before starting the expensive process
    of productionization—building production data and training pipelines and hosting
    models. The purpose of a deep learning project is no different from any other
    software development project: to solve a business need. Vetting the approach with
    the product team in the early stage will prevent the expensive and demoralizing
    process of reworking it in later stages.'
  prefs: []
  type: TYPE_NORMAL
- en: Productionization aka MLOps (5)
  prefs: []
  type: TYPE_NORMAL
- en: Productionization, also called “shipping to production,” is the process of making
    a product production worthy and ready to be consumed by its users. Production
    worthiness is commonly defined as being able to serve customer requests, withstand
    a certain level of request load, and gracefully handle adverse situations such
    as malformed input and request overload. Production worthiness also includes postproduction
    efforts, such as continuous model metric monitoring and evaluation, feedback gathering,
    and model retraining.
  prefs: []
  type: TYPE_NORMAL
- en: Productionization is the most engineering-intensive part of the development
    cycle because we’ll be converting prototyping experiments into serious production
    processes. A nonexhaustive to-do list of productionization can include
  prefs: []
  type: TYPE_NORMAL
- en: Building a data pipeline to pull data from different data sources repeatedly
    and keep the dataset versioned and updated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a data pipeline to preprocess dataset, such as data enhancement or
    enrichment and integrating with external labeling tools.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refactoring and dockerizing the prototyping code to production-quality model
    training code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making the result of training and serving codes reproducible by versioning and
    tracking the inputs and outputs. For example, we could enable the training code
    to report the training metadata (training date and time, duration, hyperparameters)
    and model metadata (performance metrics, data, and code used) to ensure the full
    traceability of every model training run.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up continuous integration (Jenkins, GitLab CI) and continuous deployment
    to automate the code building, validation, and deployment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a continuous model training and evaluation pipeline so the model training
    can automatically consume the latest dataset and produce models in a repeatable,
    auditable, and reliable manner.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a model deployment pipeline that automatically releases models that
    have passed the quality gate, so the model serving component can access them;
    `async` or real-time model prediction can be performed depending on the business
    requirements. The model serving component hosts the model and exposes it via a
    web API.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building continuous-monitoring pipelines that periodically assess the dataset,
    model, and model serving performance to detect potential feature drift (data distribution
    change) in dataset or model performance degradation (concept drifting) and alert
    developers or retrain the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These days, the productionization step has a new alias with buzz: MLOps (machine
    learning operation), which is a vague term, and its definition is ambiguous for
    researchers and professionals. We interpret MLOps to mean bridging the gap between
    model development (experimentation) and operations in production environments
    (Ops) to facilitate productionization of machine learning projects. An example
    might be streamlining the process of taking machine learning models to production
    and then monitoring and maintaining them.'
  prefs: []
  type: TYPE_NORMAL
- en: 'MLOps is a paradigm rooted in the application of similar principles that DevOps
    has to software development. It leverages three disciplines: machine learning,
    software engineering (especially the operation), and data engineering. See figure
    1.2 for a look at deep learning through the lens of MLOps.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/01-02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.2 MLOps applies DevOps approaches to deep learning for the productionization
    phase, when models get shipped to production. (Source: *Machine Learning Engineering
    in Action*, by Ben Wilson, Manning, 2022, figure 2.7)'
  prefs: []
  type: TYPE_NORMAL
- en: Because this book is about building machine learning systems that support ML
    operations, we won’t go into details about the practices shown in figure 1.2\.
    But, as you can see, the engineering effort that supports the development of machine
    learning models in production is huge. Compared to what data scientists used to
    do during the previous phase of data exploration and model prototyping, the tooling
    (software), engineering standards, and processes have dramatically changed and
    become much more complex.
  prefs: []
  type: TYPE_NORMAL
- en: Why is shipping models to production difficult?
  prefs: []
  type: TYPE_NORMAL
- en: The massive underlying infrastructure (tools, services, servers) and heavy cross-team
    collaboration are the two biggest hurdles for shipping models to production. This
    section on productionization (aka MLOps) establishes that data scientists need
    to work with data engineers, platform developers, DevOps engineers, and machine
    learning engineers, as well as learn a massive infrastructure (deep learning system),
    to ship an algorithm/model from prototype to production. It's no wonder that productionizing
    a model takes so much time.
  prefs: []
  type: TYPE_NORMAL
- en: To solve these challenges, we need to abstract away the complexity from data
    scientists when designing and building a deep learning system. As with building
    a car, we want to put data scientists behind the wheel but without asking them
    to know much about the car itself.
  prefs: []
  type: TYPE_NORMAL
- en: Now, returning to the development cycle, you may notice there is *another* inner
    loop (loop B) in figure 1.1 that goes from Productionization (box 5) and Model
    to Product Initiation (box 1). This is the second vet with the product team before
    we integrate the model inference with an AI application.
  prefs: []
  type: TYPE_NORMAL
- en: Our second vet (loop B) compares the model and data between prototyping and
    production. We want to ensure the model performance and scalability (for example,
    model serving capacity) match business requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note The following two papers are recommended; if you want to learn more about
    MLOps, they are great starting points: “Operationalizing Machine Learning: An
    Interview Study” (arXiv:2209.09125) and “Machine Learning Operations (MLOps):
    Overview, Definition, and Architecture” (arXiv:2205.02302).'
  prefs: []
  type: TYPE_NORMAL
- en: Application integration (6)
  prefs: []
  type: TYPE_NORMAL
- en: The last step of the product development cycle is to integrate the model prediction
    to the AI application. The common pattern is to host the models in the model serving
    service (which will be discussed in section 1.2.2) of the deep learning system
    and integrate the business application logic with the model by sending model prediction
    requests over the internet.
  prefs: []
  type: TYPE_NORMAL
- en: As a sample user scenario, a chatbot user interacts with the chatbot user interface
    by typing or voicing questions. When the chatbot application receives input from
    the customer, it calls the remote model serving service to run a model prediction
    and then takes action or responds to the customer based on the model prediction
    result.
  prefs: []
  type: TYPE_NORMAL
- en: Along with integrating model serving with application logic, this phase also
    involves evaluating metrics important to the product, such as clickthrough rate
    and churn rate. Nice ML-specific metrics (good precision–recall curve) do not
    always guarantee the business requirement is met. So the business stakeholders
    often perform customer interviews and product metric evaluation at this stage.
  prefs: []
  type: TYPE_NORMAL
- en: 1.1.2 Roles in the development cycle
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Because you now have a clear idea of each step in a typical development cycle,
    let’s look at the key roles that collaborate in this cycle. The definitions, job
    titles, and responsibilities of each role may vary across organizations. So make
    sure you clarify who does what in your organization and adjust your system’s design
    appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: Business stakeholders (product owner)
  prefs: []
  type: TYPE_NORMAL
- en: 'Many organizations assign the stakeholder role to multiple positions, such
    as product managers, engineering managers, and senior developers. Business stakeholders
    define the business goal of a product and are responsible for communication and
    execution of the product development cycle. The following are their responsibilities:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting inspiration from deep learning research, discussing potential application
    of deep learning features in products, and driving product requirements that in
    turn drive model development
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Owning the product! Communicating with customers and making sure the engineering
    solution meets the business requirement and delivers the results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coordinating cross-functional collaborations between different roles and teams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running project development execution; providing guidance or feedback during
    the entire development cycle to ensure the deep learning features offer real value
    to the customers of the product
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the product metrics (such as user churn rate and feature usage)—not
    the model metrics (precision or accuracy)—and driving improvement of model development,
    productionization, or product integration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Researchers
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning researchers research and develop new and novel neural network
    architectures. They also develop techniques for improving model accuracy and efficiency
    in training models. These architectures and techniques can be used during model
    development.
  prefs: []
  type: TYPE_NORMAL
- en: Note The machine learning researcher role is often associated with big tech
    companies like Google, Microsoft, and Salesforce. In many other companies, data
    scientists fulfill the same role.
  prefs: []
  type: TYPE_NORMAL
- en: Data scientists
  prefs: []
  type: TYPE_NORMAL
- en: Data scientists may wear a research hat, but most of the time, they translate
    a business problem into a machine learning problem and implement it using machine
    learning methods. Data scientists are motivated by the product’s need and apply
    research techniques to production data rather than standard benchmark datasets.
    Besides researching model algorithms, a data scientist’s responsibilities may
    also include
  prefs: []
  type: TYPE_NORMAL
- en: Combining multiple deep learning neural network architectures and/or techniques
    from different research into a solution. Sometimes they apply additional machine
    learning techniques besides pure deep learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring available data, determining what data is useful, and deciding on how
    to preprocess it before supplying it for training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prototyping different approaches (writing experimental code) to tackle the business
    problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting model prototyping code into production code with workflow automation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Following the engineering process to ship models to production by using the
    deep learning system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iterating on the need for any additional data that may help with model development.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuously monitoring and evaluating data and model performance in production.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Troubleshooting model-related problems, such as model degradation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data engineers
  prefs: []
  type: TYPE_NORMAL
- en: Data engineers help collect data and set up data pipelines for continuous data
    ingestion and processing, including data transformation, enrichment, and labeling.
  prefs: []
  type: TYPE_NORMAL
- en: MLOps engineer/ML engineer
  prefs: []
  type: TYPE_NORMAL
- en: An MLOps engineer fulfills a number of roles across multiple domains, including
    that of data engineer, DevOps (operation) engineer, data scientist, and platform
    engineer. As well as setting up and operating the machine learning infrastructure
    (both systems and hardware), they manage automation pipelines to create datasets
    and train and deploy models. ML infrastructures and user activities, such as training
    and serving, are also monitored by MLOps engineers.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, MLOps is hard, because it requires people to master a set of
    practices across software development, operation, maintenance, and machine learning
    development. MLOps engineers’ goal is to ensure the creation, deployment, monitoring,
    and maintenance of machine learning models are efficient and reliable.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning system/platform engineer
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning system engineers build and maintain the general pieces of the
    machine learning infrastructure—the primary focus of this book—to support all
    the machine learning development activities for data scientists, data engineers,
    MLOps engineers, and AI applications. Among the components of the machine learning
    system are data warehouses, compute platforms, workflow orchestration services,
    model metadata and artifact stores, model training services, model serving services,
    and more.
  prefs: []
  type: TYPE_NORMAL
- en: Application engineer
  prefs: []
  type: TYPE_NORMAL
- en: Application engineers build customer-facing applications (both frontend and
    backend) to address given business requirements. The application logic will make
    decisions or take actions based on the model prediction for a given customer request.
  prefs: []
  type: TYPE_NORMAL
- en: Note In the future, as machine learning systems (infrastructure) mature, the
    roles involved in deep learning development cycle will merge into fewer and fewer.
    Eventually, data scientists will be able to complete the entire cycle on their
    own.
  prefs: []
  type: TYPE_NORMAL
- en: 1.1.3 Deep learning development cycle walk-through
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'By giving an example, we can demonstrate the roles and the process in a more
    concrete manner. Suppose you have been assigned the task of building a customer
    support system that answers questions automatically about the company’s product
    lines. The following steps will guide you through the process of bringing that
    product to market:'
  prefs: []
  type: TYPE_NORMAL
- en: The product requirement is to build a customer support application that presents
    a menu, so customers can navigate to find answers to commonly asked questions.
    As the number of questions grows, the menu becomes larger, with many layers of
    navigation. Analytics has shown that many customers are confused by the navigation
    system and drop off from navigating the menu while trying to find answers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The product manager (PM) who owns the product is motivated to improve the user
    retention rate and experience (finding the answers quickly). After conducting
    some research with customers, the PM finds that a majority of customers would
    like to obtain answers without a complex menu system, preferably as simple as
    asking questions in their natural language.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The PM reaches out to machine learning researchers for a potential solution.
    It turns out that deep learning may help. Experts think the technology is mature
    enough for this use case and suggest a few approaches based on deep learning models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The PM writes a product spec indicating that the application should take one
    question from a customer at a time, recognize intent from the question, and match
    it with relevant answers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data scientists receive product requirements and start to prototype deep learning
    models that fit the need. They first start data exploration to collect available
    training data and consult with researchers for the choices of algorithms. And
    then data scientists start to build prototyping code to produce experimental models.
    Eventually, they arrive at some datasets, a few training algorithms, and several
    models. After careful evaluation, one natural language process model is selected
    from various experiments.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then the PM assembles a team of platform engineers, MLOps engineers, and data
    engineers to work with the data scientist to onboard the prototyping code, made
    in step 5, to production. The work includes building a continuous data processing
    pipeline and a continuous model training, deployment, and evaluation pipeline,
    as well as setting up the model serving functionality. The PM also specifies the
    number of predictions per second and the latency required.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once a production setup is complete, the application engineers integrate the
    customer support service’s backend with the model serving service (built in step
    6), so when a user types in a question, the service will return answers based
    on the model prediction. The PM also defines product metrics, such as average
    time spent finding an answer, to evaluate the end result and use it to drive the
    next round of improvement.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 1.1.4 Scaling project development
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As you saw in section 1.1.2, we need to fill seven different roles to complete
    a deep learning project. The cross-functional collaboration between these roles
    happens at almost every step. For example, data engineers, platform developers,
    and data scientists work together to productionize a project. Anyone who has been
    involved in a project that requires many stakeholders knows how much communication
    and coordination are required to keep a project like this moving forward.
  prefs: []
  type: TYPE_NORMAL
- en: These challenges make deep learning development hard to scale because we either
    don’t have the resources to fill all the required roles or we can’t meet the product
    timeline due to the communication costs and slowdowns. To reduce the enormous
    amount of operational work, communication, and cross-team coordination costs,
    companies are investing in machine learning infrastructure and reducing the number
    of people and the scope of knowledge required to build a machine learning project.
    The goal of a deep learning infrastructure stack is not only to automate model
    building and data processing but also to make it possible to merge the technical
    roles so that the data scientist is empowered to take care of all these functions
    autonomously within a project.
  prefs: []
  type: TYPE_NORMAL
- en: A key success indicator of a deep learning system is to see how smooth the model
    productionization process can be. With a good infrastructure, the data scientist,
    who is not expected to suddenly become an expert DevOps or data engineer, should
    be able to implement models in a scalable manner, set up data pipelines, and deploy
    and monitor models in production independently.
  prefs: []
  type: TYPE_NORMAL
- en: By using an efficient deep learning system, data scientists will be able to
    complete the development cycle with minimal additional overhead—less communication
    required and less time wasted waiting by others—and focus on the most important
    data science tasks, such as understanding the data and experimenting with algorithms.
    The ability to scale deep learning project development is the true value proposition
    of a deep learning system.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Deep learning system design overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With the context of section 1.1 in mind, let’s dive into the focus of this
    book: the deep learning system itself. Designing a system—any system—is the art
    of achieving goals under a set of constraints that are unique to your situation.
    This is also true for deep learning systems. For instance, let’s say you have
    a few deep learning models that need to be served at the same time, but your budget
    does not allow you to operate a machine that has enough memory to fit all of them
    at the same time. You may need to design a caching mechanism to swap models between
    memory and disk. Swapping, however, will increase inference latency. Whether this
    solution is feasible will depend on latency requirements. Another possibility
    is to operate multiple smaller machines for each model, if your model sizes and
    budget will allow it.'
  prefs: []
  type: TYPE_NORMAL
- en: Or, for another example, imagine your company’s product must comply with certain
    certification standards. It may mandate data access policies that pose significant
    limitations to anyone who wants to gain access to data collected by the company’s
    product. You may need to design a framework to allow data access in a compliant
    fashion so that researchers, data scientists, and data engineers can troubleshoot
    problems and develop new models that require such data access in your deep learning
    system.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, there are many knobs that can be turned. It will certainly be
    an iterative process to arrive at a design that will satisfy as many requirements
    as possible. But to shorten the iterative process, it is desirable to start with
    a design that is as close to the end state as possible.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we first propose a deep learning system design with only essential
    components and then explain the responsibility of each of the components and user
    workflows. In our experience of designing and tailoring deep learning systems,
    a few key components are common across different designs. We think that they can
    be used as a reasonable starting point for your design. We call this the *reference
    system architecture*.
  prefs: []
  type: TYPE_NORMAL
- en: You can make a copy of this reference for your design project, go through your
    list of goals and constraints, and start by identifying knobs in each component
    that you can adjust to your needs. Because this isn’t an authoritative architecture,
    you should also assess whether all components are really necessary and add or
    remove components as you see fit.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.1 Reference system architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Figure 1.3 shows the high-level overview of the reference deep learning system.
    The deep learning system has two major parts. The first is the application programming
    interface (API; box A) for the system, located in the middle of the diagram. The
    second is the collection of components of the deep learning system, which is represented
    by all the rectangular boxes located within the large box, outlined in a dotted
    line and taking up the lower half of the diagram. These boxes each represent a
    system component:'
  prefs: []
  type: TYPE_NORMAL
- en: API (box A)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset manager (box B)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model trainer (box C)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model serving (box D)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metadata and artifacts store (box E)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Workflow orchestration (box F)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interactive data science environment (box G)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this book, we assume that these system components are *microservices*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/01-03.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.3 An overview of a typical deep learning system that includes basic
    components to support a deep learning development cycle. This reference architecture
    can be used as a starting point and further tailored. In later chapters, we discuss
    each component in detail and explain how it fits into this big picture.
  prefs: []
  type: TYPE_NORMAL
- en: Definition There is no single definition for microservices. Here, we will use
    the term to mean processes that communicate over a network with the HTTP or the
    gRPC protocol.
  prefs: []
  type: TYPE_NORMAL
- en: This assumption means we can expect these components to reasonably support multiple
    users with different roles securely and are readily accessible over a network
    or the internet. (This book, however, will not cover all engineering aspects of
    how microservices are designed or built. We will focus our discussion on specifics
    that are relevant to deep learning systems.)
  prefs: []
  type: TYPE_NORMAL
- en: NOTE You may wonder whether you need to design, build, and host all deep learning
    system components on your own. Indeed, there are open source (Kubeflow) and hosted
    alternatives (Amazon SageMaker) for them. We hope that after you have learned
    the fundamentals of each component, how they fit in the big picture, and how they
    are used by different roles, you will make the best decision for your use case.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.2 Key components
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now let’s walk through the key components that we consider essential to a basic
    deep learning system, as shown in figure 1.3\. You may want to add additional
    components or simplify them further as you see fit for your requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Application programming interface
  prefs: []
  type: TYPE_NORMAL
- en: The entry point (box A in figure 1.3) of our deep learning system is an API
    that is accessible over a network. We opted for an API because the system needs
    to support not only graphical user interfaces that will be used by researchers,
    data scientists, data engineers, and the like but also applications and possibly
    other systems—for example, a data warehouse from a partner organization.
  prefs: []
  type: TYPE_NORMAL
- en: Although conceptually the API is the single point of entry of the system, it
    is entirely possible that the API is defined as the sum of all APIs provided by
    each component, without an extra layer that aggregates everything under a single-service
    endpoint. Throughout this book, we will use the sum of all APIs provided by each
    component directly and skip the aggregation for simplicity.
  prefs: []
  type: TYPE_NORMAL
- en: Note Should you use a centralized or distributed deep learning system API? In
    the reference architecture (figure 1.3), the deep learning system API is shown
    as a single box. It should be interpreted as a logical container for the complete
    set of your deep learning system API, regardless of whether it is implemented
    on single (e.g., an API gateway that proxies for all components) or multiple service
    endpoints (direct interaction with each component). Each implementation has its
    own merits and shortcomings, and you should work with your team to figure out
    what functions best. Direct interaction with each component may be easier if you
    start with a small use case and team.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset manager
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning is based on data. There is no doubt that the data management component
    is a central piece of a deep learning system. Every learning system is a garbage-in,
    garbage-out system, so ensuring good data quality for learning is of paramount
    importance. A good data management component should provide the solution to this
    problem. It enables collecting, organizing, describing, and storing data, which
    in turn makes it possible for data to be explored, labeled, and used for training
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 'In figure 1.3, we can see at least four relationships of the dataset manager
    (box B) with other parties:'
  prefs: []
  type: TYPE_NORMAL
- en: Data collectors push raw data to the dataset manager to create or update datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The workflow orchestration service (box F) executes the data process pipeline,
    which pulls data from the dataset manager to enhance the training dataset or transform
    the data format and pushes the result back.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data scientists, researchers, and data engineers use Jupyter Notebook (box G)
    to pull data from the data manager for data exploration and examination.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model training service (box C) pulls training data from the data manger
    for model training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In chapter 2, we will discuss dataset management in depth. Throughout the book,
    we use the term *dataset* as a unit of collected data that may be related.
  prefs: []
  type: TYPE_NORMAL
- en: Model trainer
  prefs: []
  type: TYPE_NORMAL
- en: Model trainer (aka, model training service; box C) responds to provide foundational
    computation resources, such as CPUs, RAM, and GPUs, and job management logics
    to run the model training code and produce model files. In figure 1.3, we can
    see that the workflow orchestration service (box F) tells the model trainer to
    execute a model training code. The trainer takes input training data from the
    dataset manager (box B) and produces a model. Then it uploads the model with training
    metrics and metadata to the metadata and artifacts store (box E).
  prefs: []
  type: TYPE_NORMAL
- en: 'It is usually necessary to perform intense computation on a large dataset to
    produce high-quality deep learning models that can make accurate predictions.
    Adoption of new algorithms and training libraries/frameworks is also a critical
    requirement. These requirements produce challenges on several levels:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Capability of reducing model training time*—Despite the growing size of training
    data and complexity of model architecture, training systems must keep training
    times reasonable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Horizontal scalability*—An effective production training system should be
    able to support multiple training requests from different applications and users
    simultaneously.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cost of adopting new technologies*—The deep learning community is a vigorous
    one, with constant updates and improvements to algorithms and tooling (SDK, framework).
    The training system should be flexible enough to accommodate new innovations easily
    without interfering with the existing workload.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In chapter 3, we will investigate different approaches to solving the aforementioned
    problems. We will not go deep into the theoretical aspect of training algorithms
    in this book, as they do not affect how we design the system. In chapter 4, we
    will look at how we can distribute training to accelerate the process. In chapter
    5, we will explore a few different approaches for optimizing training hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Model serving
  prefs: []
  type: TYPE_NORMAL
- en: 'Models can be used in various settings, such as online inference for real-time
    predictions or offline inference for batch predictions using large volumes of
    input data. This is where model serving surfaces—when a system hosts the model,
    takes input prediction requests, runs model prediction, and returns the prediction
    to users. There are a few key questions to be answered:'
  prefs: []
  type: TYPE_NORMAL
- en: Are your inference requests coming from over the network? Or are they coming
    from sensors that need to be served locally?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is an acceptable latency? Are inference requests ad hoc or streaming?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many models are being served? Is each model individually serving a type
    of inference request, or is an ensemble of models doing so?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How large are model sizes? How much memory capacity do you need to budget?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What model architectures need to be supported? Does it require a GPU? How much
    computing resources do you need to produce inferences? Are there other supporting
    serving components—for example, embeddings, normalization, aggregation, etc.?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are there sufficient resources to keep models online? Or is a swapping (such
    as moving models between memory and disk) strategy needed?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From figure 1.3, the main input and output of the model serving (box D) are
    inference requests and the prediction returned, respectively. To produce inferences,
    models are retrieved from the metadata and artifacts store (box E). Some requests
    and their responses may be logged and sent to the model monitoring and evaluation
    service (not shown in figure 1.3 or covered in this book), which detects anomalies
    from this data and produces alerts. In chapters 6 and 7, we will go deeper into
    model serving architecture, explore these key aspects, and discuss their solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Metadata and artifacts store
  prefs: []
  type: TYPE_NORMAL
- en: Imagine working on a simple deep learning application as a one-person team,
    where you have to work with only a few datasets and train and deploy only one
    type of model. You can probably keep track of how datasets, training codes, models,
    inference codes, and inferences are related to one another. These relationships
    are essential for model development and troubleshooting as you need to be able
    to trace certain observations back to the cause.
  prefs: []
  type: TYPE_NORMAL
- en: Now imagine adding more applications, more people, and more model types. The
    number of these relationships will grow exponentially. In a deep learning system
    that is designed to support multiple types of users working on multiple datasets,
    code, and models at various stages, there is a need for a component that keeps
    track of the web of relationships. The metadata and artifacts store in a deep
    learning system does just that. Artifacts include code that trains models and
    produces inferences, as well as any generated data such as trained models, inferences,
    and metrics. Metadata is any data that describes an artifact or the relationship
    between artifacts. Some concrete examples are
  prefs: []
  type: TYPE_NORMAL
- en: The author and version of a piece of training code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A reference of the input training dataset and the training environment of a
    trained model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training metrics of a trained model, such as training date and time, duration,
    and the owner of the training job
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model-specific metrics, such as model version, model lineage (data and code
    used in training), and performance metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model, request, and inference code that produce a certain inference
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Workflow history, tracking each step of the model training and data process
    pipeline runs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are just a few examples of what a baseline metadata and artifacts store
    would help track. You should tailor the component to the needs of your team or
    organization.
  prefs: []
  type: TYPE_NORMAL
- en: Every other component that generates metadata and artifacts in figure 1.3 would
    flow into the metadata and artifacts store (box E). The store also plays an important
    role in model serving because it provides model files and their metadata to the
    model serving service (box D). Although not shown in the figure, custom tools
    for trace lineage and troubleshooting are usually built at the user interface
    layer, powered by the metadata and artifacts store.
  prefs: []
  type: TYPE_NORMAL
- en: As we proceed through chapter 8, we will look at a baseline metadata and artifacts
    store. This store is usually the central component of a deep learning system’s
    user interface.
  prefs: []
  type: TYPE_NORMAL
- en: Workflow orchestration
  prefs: []
  type: TYPE_NORMAL
- en: Workflow orchestration (figure 1.3, box F) is ubiquitous in many systems, where
    it helps to automatically launch computation tasks triggered by programmatic conditions.
    In the context of a machine learning system, the workflow orchestration is the
    driving force behind all the automations running in a deep learning system. It
    allows people to define workflows or pipelines—directed acyclic graphs (DAGs)—to
    glue individual tasks together with an execution order. The workflow orchestration
    component orchestrates the task executions of these workflows. Some typical examples
    are
  prefs: []
  type: TYPE_NORMAL
- en: Launching model training whenever a new dataset is built
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring upstream data sources, augmenting new data, transferring its format,
    notifying external labelers, and merging the new data into existing datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying the trained model to the model server if it passes some accepted criteria
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continually monitoring model performance metrics and alerting developers when
    degradation is detected
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will learn how to build or set up a workflow orchestration system in chapter
    9.
  prefs: []
  type: TYPE_NORMAL
- en: Interactive data science environment
  prefs: []
  type: TYPE_NORMAL
- en: Customer data and models cannot be downloaded to a local workstation from production
    for compliance and security reasons. For data scientists to interactively explore
    data, troubleshoot pipeline execution in workflow orchestration, and debug models,
    a remote interactive data science environment (figure 1.3, box G) located inside
    the deep learning system is required.
  prefs: []
  type: TYPE_NORMAL
- en: It is common for companies to set up their own trusted data science environment
    by using open source Jupyter Notebooks ([https://jupyter.org/](https://jupyter.org/))
    or by utilizing cloud vendors’ JupyterLab-based solutions, such as Amazon SageMaker
    Studio ([https://aws.amazon.com/sagemaker/studio/](https://aws.amazon.com/sagemaker/studio/)).
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical interactive data science environment should provide the following
    functionalities:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Data exploration*—Offers data scientists easy access to customer data but
    keeps it secure and compliant; there are no data leaks, and any unauthorized data
    access will be rejected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model prototyping*—Provides the much-needed tooling for data scientists to
    develop quick POC models inside the deep learning system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Troubleshooting*—Enables engineers to debug any activity happening inside
    the deep learning system, such as downloading the model and playing with it to
    analyze its behavior or checking all the input/output artifacts (intermediate
    datasets or configurations) from a failed pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1.2.3 Key user scenarios
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To better understand how deep learning systems can be used during the development
    cycle (figure 1.1), we prepared sample scenarios that illustrate how they could
    be used. Let’s start with programmatic consumers, shown in figure 1.4\. Data collectors
    that push data to the system will usually end up at the data management service
    via the API, which collects and organizes raw data for model training.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/01-04.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.4 Data is pushed from sources or collectors, through the API to the
    data management service, where the data is further organized and stored in formats
    that are more friendly for model training.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning applications will usually hit the model inference service to obtain
    inferences from a trained model, which is used to power deep learning features
    that end users will consume. Figure 1.5 shows the sequence of this interaction.
    Scripts, or even full-fledged management services, can be programmatic consumers,
    too. Because they are optional, we omitted them from the figure for simplicity.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/01-05.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.5 Deep learning applications request inferences through the API. The
    model inference service accepts and processes these requests against trained models
    and produces inferences that are returned back to applications.
  prefs: []
  type: TYPE_NORMAL
- en: Between human consumers and the API usually lies an extra layer—the user interface.
    The interface can be web based or command-line based. Some power users may even
    skip this interface and consume the API directly. Let’s walk through each persona
    one by one.
  prefs: []
  type: TYPE_NORMAL
- en: A typical scenario of researchers using the system is illustrated in figure
    1.6\. Researchers can look up available data to try out their new modeling technique.
    They access the user interface and visit the data exploration and visualization
    section, which pulls data from the data management service. A great deal of manual
    data processing might be involved in massaging it into forms that can be consumed
    by new training techniques. Once researchers settle with a technique, they can
    package it as a library for others’ consumption.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/01-06.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.6 A usage sequence of a researcher who is interested in seeing what
    data is available for researching and developing a new modeling technique. The
    researcher interacts with a user interface that is supported by the API and data
    management behind the scenes.
  prefs: []
  type: TYPE_NORMAL
- en: Data scientists and engineers can work on use cases by first looking at what
    data is available, similar to what researchers would initially do in the previous
    paragraph. This would be supported by the data management service. They make hypotheses
    and put together data processing and training techniques as code. These steps
    can be combined to form a workflow using the workflow management service.
  prefs: []
  type: TYPE_NORMAL
- en: When the workflow management service performs a run of the workflow, it contacts
    the data management service and the model training service to perform actual duties
    and track their progress. Hyperparameters, code versions, model training metrics,
    and test results are all stored to the metadata and artifacts store by each service
    and training code.
  prefs: []
  type: TYPE_NORMAL
- en: Through the user interface, data scientists and engineers can compare experimental
    runs and deduce the best way to train models. This aforementioned scenario is
    shown in figure 1.7.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/01-07.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.7 A usage sequence of a data scientist defining model training workflow,
    running it, and reviewing results
  prefs: []
  type: TYPE_NORMAL
- en: Product managers can also look at and query all kinds of metrics throughout
    the system through the user interface. The metrics data can be supplied by the
    metadata and artifacts store.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.4 Derive your own design
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have gone over all aspects of the reference system architecture,
    let’s discuss some guidelines for customizing your own version.
  prefs: []
  type: TYPE_NORMAL
- en: Gathering goals and requirements
  prefs: []
  type: TYPE_NORMAL
- en: The first step to designing any successful system design is to have a set of
    clear goals and requirements with which to work. These should ideally come from
    users of your system, either directly or indirectly through the product management
    team or engineering management. This short list of goals and requirements will
    help you form a vision of what your system will look like. This vision, in turn,
    should be the guideline that drives you throughout the design and development
    phases of your system.
  prefs: []
  type: TYPE_NORMAL
- en: Note Sometimes engineers are asked to develop a system to support one or more
    deep learning applications that already exist. In this case, you may instead start
    with identifying the set of common requirements among these applications and how
    your system can be designed to help bring innovation quickly to these applications.
  prefs: []
  type: TYPE_NORMAL
- en: To collect the system goals and requirements, you need to identify the different
    types of users and stakeholders, or *personas*, of the system. (This is a general
    concept that can be applied to most system design problems.) It is the users,
    after all, who will help you articulate the goals and requirements of the system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our recommendation is to start with use cases or application requirements if
    you are not sure of a good starting point. Here are some example questions that
    you can ask your users:'
  prefs: []
  type: TYPE_NORMAL
- en: '*To data engineers and product managers*—Does the system allow applications
    to collect data for training? Does the system need to handle streaming input data?
    How much data is being collected?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*To data scientists and engineers*—How do we process and label the data? Does
    the system need to provide labeling tools for external vendors? How do we evaluate
    the model? How do we handle the test dataset? Is an interactive notebooking user
    interface needed for data science work?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*To researchers and data scientists*—How large of a volume of data is needed
    for training models? What’s the average time of model training runs? How much
    computing and data capacity is needed for research and data science? What kind
    of experiments should the system support? What metadata and metrics need to be
    collected to evaluate different experiments?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*To product managers and software engineers*—Is model serving done on the remote
    server or on the client? Is it a real-time model inference or offline batch prediction?
    Is there a latency requirement?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*To product managers*—What problems are we trying to solve at our organization?
    What is our business model? How are we going to gauge the effectiveness of our
    implementations?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*To security teams*—What level of security is needed in your system? Is data
    access wide open or strictly restricted/isolated? Is there an audit requirement?
    Is there a certain level of compliance or certification (e.g., General Data Protection
    Regulation, System and Organization Controls 2, etc.) that the system needs to
    achieve?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customizing the reference architecture
  prefs: []
  type: TYPE_NORMAL
- en: Once the design requirement and scope are clear, we can start to customize the
    reference architecture in figure 1.3\. First, we can decide whether we need to
    add or remove any components. For example, if the requirement is purely managing
    model training in a remote server farm, we could remove the workflow management
    component. If data scientists want to evaluate model performance effectively with
    production data, they could also add an experiment management component. This
    component allows data scientists to perform training and validation using full-scale
    data that already exists in the system and conduct online A/B testing against
    production traffic with previously unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: The second step is to design and implement each key component suite to your
    specific needs. Depending on the requirement, you might exclude the data streaming
    API from the dataset management service and add distributed training support if
    training speed is a concern. You can either build each key component from scratch
    or use open source software. In the rest of the book, we cover both options for
    each key component to ensure you know what to do.
  prefs: []
  type: TYPE_NORMAL
- en: TIP Keep the system design simple and user friendly. The purpose of creating
    such a large deep learning system is to improve the productivity of deep learning
    development, so please keep this in mind when designing it. We want to make it
    easy for data scientists to build high-quality models without the need to learn
    what’s going on in the underlying system.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.5 Building components on top of Kubernetes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have introduced a list of key components that are implemented as services.
    With this number of services, you may want to manage them with a sophisticated
    system at the infrastructure level, such as Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes is an open source system for automating deployment, scaling, and
    management of containerized applications, which are applications that run in isolated
    runtime environments—for example, docker containers. We have seen a number of
    deep learning systems that are built on top of Kubernetes. Some people learn how
    to use Kubernetes without ever knowing why it is used to run deep learning services,
    so we want to explain the thinking behind it. If you are familiar with Kubernetes,
    please feel free to skip this section.
  prefs: []
  type: TYPE_NORMAL
- en: Note Kubernetes is a complex platform that would require a book-length of material
    to teach, so we are only discussing its merits for a deep learning system. If
    you want to learn Kubernetes, we highly recommend you check out *Kubernetes in
    Action* (Manning, 2018), by Marko Lukša.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges for managing computing resources
  prefs: []
  type: TYPE_NORMAL
- en: Executing one docker container on a remote server seems to be a simple task,
    but running 200 containers on 30 different servers is a different story. There
    are many challenges, such as monitoring all remote servers to determine on which
    one to run the container, needing to failover a container to a healthy server,
    restarting a container when it’s stuck, following up each container run and getting
    notified when it completes, etc. To address these challenges, we must monitor
    the hardware, OS processes, and networking ourselves. Not only is it technically
    challenging, but it is also a huge amount of work.
  prefs: []
  type: TYPE_NORMAL
- en: How Kubernetes helps
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes is an open source container orchestration platform for scheduling
    and automating the deployment, management, and scaling of containerized applications.
    Once you set up a Kubernetes cluster, your server groups’ operation (deployment,
    patching, updates) and resources become manageable. Here is a deployment example:
    you can tell Kubernetes to run a docker image with 16 GB memory and 1 GPU with
    a command; Kubernetes will allocate the resource to run this docker image for
    you.'
  prefs: []
  type: TYPE_NORMAL
- en: This is a huge benefit for software developers because not every one of them
    has extensive experience with hardware and deployment. With Kubernetes, we only
    need to declare the end state of our cluster, and Kubernetes will do the actual
    job to meet our goals.
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides container deployment benefits, the following are some other key Kubernetes
    functionalities that are crucial for managing our training containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Autoscaling features*—Kubernetes automatically resizes the number of nodes
    in the cluster based on the workload. This means if there is a sudden burst of
    user requests, Kubernetes will add the capacity automatically, which is called
    *elastic compute management*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Self-healing capabilities*—Kubernetes restarts, replaces, or reschedules pods
    when they fail or when nodes die. It also kills pods that do not respond to user-defined
    health checks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Resource utilization and isolation*—Kubernetes takes care of computing resource
    saturation; it ensures every server is fully utilized. Internally, Kubernetes
    launches application containers in *pods*. Each pod is an isolated environment
    with a computing resource guarantee, and it runs a function unit. In Kubernetes,
    multiple pods can be in one node (server) as long as their combined resource requirements
    (CPU, memory, disk) don’t exceed the node’s limitations, so servers can be shared
    by different function units easily with guaranteed isolation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Namespaces*—Kubernetes supports splitting a physical cluster into different
    virtual clusters. These virtual clusters are called *namespaces*. You can define
    resource quota per namespace, which allows you to isolate resources for different
    teams by assigning them to different namespaces.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the flip side, these benefits come at a cost—they consume resources as well.
    When you run a Kubernetes pod, the pod itself takes some amount of system resources
    (CPU, memory). These resources are consumed on top of those that are needed to
    run containers inside pods. Kubernetes’s overhead seems reasonable in many situations;
    for example, from an experiment published in the article “How We Minimized the
    Overhead of Kubernetes in Our Job System” ([http://mng.bz/DZBV](http://mng.bz/DZBV))
    by Lally Singh and Ashwin Venkatesan (February 2021), the CPU overhead per pod
    was about 10 ms per second.
  prefs: []
  type: TYPE_NORMAL
- en: Note We recommend you check out appendix B to see how existing deep learning
    systems relate to the concepts presented in this chapter. In that appendix, we
    compare the reference architecture described in section 1.2.1 with Amazon SageMaker,
    Google Vertex AI, Microsoft Azure Machine Learning, and Kubeflow.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 Building a deep learning system vs. developing a model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A final piece of groundwork before we begin: we think it is crucial to call
    out the differences between *building a deep learning system* and *developing
    a deep learning model*. In this book, we define *the practice of developing a
    deep learning model* to solve a problem as the process of'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring available data and how it can be transformed for training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determining the effective training algorithm(s) to use for the problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training models and developing inference code to test against unseen data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recall that a deep learning system should support not only all tasks required
    by model development but also those that need to be performed by other roles and
    make collaboration between these roles seamless. When building a deep learning
    system, you are not developing deep learning models; you are building a system
    that supports the development of deep learning models, making that process more
    efficient and scalable.
  prefs: []
  type: TYPE_NORMAL
- en: We have found an abundance of material published about building the models.
    But we have seen precious little written about designing and building the platforms
    or systems that support those models. And that is why we wrote this book.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A typical machine learning project development goes through the following cycle:
    product initiation, data exploration, model prototyping, productionization, and
    production integration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are seven different roles involved in deep learning project development:
    a product manager, researchers, data scientists, data engineers, MLOps engineers,
    machine learning system engineers, and application engineers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A deep learning system should reduce complexity in the deep learning development
    cycle.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the help of deep learning systems, the data scientist, who is not expected
    to suddenly become an expert DevOps or data engineer, should be able to implement
    models in a scalable manner, set up data pipelines, and deploy and monitor models
    in production independently.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An efficient deep learning system should allow data scientists to focus on interesting
    and important data science tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A high-level reference architecture like the one we present in figure 1.3 can
    help you quickly start a new design. First, make your own copy and then collect
    goals and requirements. Finally, add, modify, or subtract components and their
    relationships as you see fit.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A basic deep learning system consists of the following key components: dataset
    manager, model trainer, model serving, metadata and artifacts store, workflow
    orchestration, and data science environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data management component helps collect, organize, describe, and store data
    as datasets that can be used as training input. It also supports data exploration
    activities and tracks lineage between datasets. Chapter 2 will discuss data management
    in detail.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model training component is responsible for handling multiple training requests
    and running them efficiently provided a given, limited set of computing resources.
    Chapters 3 and 4 will review the model training component.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model serving component handles incoming inference requests, produces inferences
    with models, and returns them to requesters. It will be covered in chapters 6
    and 7.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The metadata and artifacts store component records metadata and stores artifacts
    from the rest of the system. Any data produced by the system can be treated as
    artifacts. Most of them would be models, which come with metadata that will be
    stored in the same component. This provides complete lineage information to support
    experimentation and troubleshooting. We will talk about this component in chapter
    8.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The workflow management component stores workflow definitions that chain together
    different steps in data processing and model training. It is responsible for triggering
    periodic workflow runs and tracking the progress of each run step that is being
    executed on other components—for instance, a model training step being executed
    on the model training service. In chapter 9, we will provide a walk-through of
    this component.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A deep learning system should support the deep learning development cycle and
    make collaboration between multiple roles easy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a deep learning system is different from developing a deep learning
    model. The system is the infrastructure to support deep learning model development.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
