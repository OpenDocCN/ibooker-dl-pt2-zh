- en: 2 Gaussian processes as distributions over functions
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 高斯过程作为函数分布
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容包括
- en: A crash course on multivariate Gaussian distributions and their properties
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对多元高斯分布及其属性的速成课程
- en: Understanding GPs as multivariate Gaussian distributions in infinite dimensions
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 GPs 理解为无限维度中的多元高斯分布
- en: Implementing GPs in Python
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Python 中实现 GP
- en: 'Having seen what BayesOpt can help us do, we are now ready to embark on our
    journey toward mastering BayesOpt. As we saw in chapter 1, a BayesOpt workflow
    consists of two main parts: a Gaussian process (GP) as a predictive, or surrogate,
    model and a policy for decision-making. With a GP, we don’t obtain only point
    estimates as predictions for a test data point, but instead, we have an entire
    *probability distribution* representing our belief about the prediction.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到贝叶斯优化可以帮助我们做什么，我们已经准备好踏上掌握贝叶斯优化的旅程。正如我们在第 1 章中看到的，贝叶斯优化工作流程由两个主要部分组成：高斯过程（GP）作为预测模型或替代模型，以及用于决策的策略。使用
    GP，我们不仅获得测试数据点的点估计作为预测，而且我们有一个完整的*概率分布*表示我们对预测的信念。
- en: With a GP, we produce similar predictions from similar data points. For example,
    in weather forecasting, when estimating today’s temperature, a GP will look at
    the climatic data of days that are similar to today, either the last few days
    or this exact day a year ago. Days in another season wouldn’t inform the GP when
    making this prediction. Similarly, when predicting the price of a house, a GP
    will say that similar houses in the same neighborhood as the prediction target
    are more informative than houses in another state.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 GP，我们从相似的数据点产生相似的预测。例如，在天气预报中，当估计今天的温度时，GP 会查看与今天相似的几天的气候数据，即最近几天或一年前的这一天。另一个季节的天数不会在进行此预测时通知
    GP。同样，当预测房屋价格时，GP 将会说预测目标所在地区的相似房屋比另一个州的房屋更具信息量。
- en: How similar a data point is to another is encoded using the covariance function
    of a GP, which, in addition, models the uncertainty in the GP’s predictions. Remember
    from chapter 1 our comparison of a ridge regression model and a GP, shown again
    in figure 2.1\. Here, while the ridge regressor only produces single-valued predictions,
    the GP outputs a normal distribution at each test point. Uncertainty quantification
    is what sets the GP apart from other ML models, specifically in the context of
    decision-making under uncertainty.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 数据点之间的相似程度是使用 GP 的协方差函数来编码的，此外，该函数还模拟了 GP 预测的不确定性。请记住，在第 1 章中我们对比了岭回归模型和 GP
    的模型，再次显示在图 2.1 中。在这里，虽然岭回归器只产生单值预测，但 GP 在每个测试点输出一个正态分布。不确定性量化是将 GP 与其他 ML 模型区分开来的因素，特别是在不确定性决策的背景下。
- en: '![](../../OEBPS/Images/02-01.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/02-01.png)'
- en: Figure 2.1 Predictions by ridge regression vs. those by a GP. While the mean
    prediction of the GP is the same as the prediction of ridge, the GP also offers
    CIs indicating the predictive level of uncertainty.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1 岭回归和 GP 的预测。尽管 GP 的平均预测与岭回归的预测相同，但 GP 还提供了表示预测不确定性的 CI。
- en: We will see how correlation modeling and uncertainty quantification are mathematically
    realized with Gaussian distributions and learn to actually implement a GP in GPyTorch,
    the premiere GP modeling tool in Python. Being able to model a function with a
    GP is the first step toward BayesOpt—a step we will take in this chapter.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到如何通过高斯分布在数学上实现相关建模和不确定性量化，并学习如何在 GPyTorch 中实际实现 GP，这是 Python 中首选的 GP 建模工具。能够用
    GP 对函数进行建模是迈向贝叶斯优化的第一步，我们将在本章中完成这一步。
- en: Why GPyTorch?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么选择 GPyTorch？
- en: There are other GP modeling libraries in Python, such as GPy or GPflow, but
    we have chosen GPyTorch for this book. Built on top of PyTorch and actively maintained,
    GPyTorch offers a streamlined workflow from array manipulation to GP modeling
    and, eventually, to BayesOpt with BoTorch, which we start using in chapter 4.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中还有其他的 GP 建模库，如 GPy 或 GPflow，但我们选择了 GPyTorch 作为本书的工具。基于 PyTorch 构建且处于积极维护状态，GPyTorch
    提供了从数组操作到 GP 建模再到使用 BoTorch 进行贝叶斯优化的简化工作流程，我们将在第 4 章开始使用 BoTorch。
- en: The library is also actively maintained and has many state-of-the-art methods
    implemented. For example, chapter 12 covers using GPyTorch to scale a GP to large
    datasets, and in chapter 13, we learn to integrate a neural network into a GP
    model.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 该库也在积极维护，并且已实现了许多最先进的方法。例如，第 12 章介绍了使用 GPyTorch 对大型数据集进行缩放的方法，在第 13 章中，我们学习将神经网络集成到
    GP 模型中。
- en: 2.1 How to sell your house the Bayesian way
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 如何以贝叶斯方式出售您的房屋
- en: Before we jump right into GPs, let’s consider an example scenario in the domain
    of housing price modeling and how the price of a house is determined in relation
    to other houses. This discussion serves as an example of how correlation works
    in a multivariate Gaussian distribution, which is a central part of a GP.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们立即进入高斯过程之前，让我们考虑一个房价建模领域的示例场景，以及房子价格如何与其他房子相关确定的例子。这个讨论作为多元高斯分布中相关性如何工作的示例，是高斯过程的核心部分。
- en: 'Say you are a homeowner in Missouri who is looking to sell your house. You
    are trying to set an appropriate asking price and are talking to a friend about
    how to do this:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你是密苏里州的一位房主，正打算出售你的房子。你正在尝试确定一个合适的要价，并与朋友讨论如何做到这一点：
- en: '**You:**     I’m not sure what to do. I just don’t know exactly how much my
    house is worth.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**你：**     我不确定该怎么办。我只是不知道我的房子值多少钱。'
- en: '**Friend:** Do you have a rough estimate?'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**朋友：** 你有个大概的估算吗？'
- en: '**You:**     Somewhere between 150k and 300k would be my guess.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**你：**     我猜大概在 15 万到 30 万之间。'
- en: '**Friend:** That’s a pretty big range.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**朋友：** 这个范围挺大的。'
- en: '**You:**     Yeah, I wish I knew people who have sold their houses. I need
    some references.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**你：**     是啊，我希望我认识已经卖掉房子的人。我需要一些参考。'
- en: '**Friend:** I heard Alice sold her house for 250k.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**朋友：** 我听说爱丽丝卖了她的房子 25 万。'
- en: '**You:**     Alix who’s in California? That’s really surprising! Also, I don’t
    think a house in California will help me make a better estimate for my own house.
    It could still be anything between 150k and 300k.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**你：**     在加利福尼亚的阿利克斯吗？这真让人吃惊！而且，我不认为加利福尼亚的房子会帮助我更好地估算自己的房子。它可能仍然在 15 万到 30
    万之间。'
- en: '**Friend:** No, it’s Alice who lives right next to you.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**朋友：** 不，是住在你隔壁的爱丽丝。'
- en: '**You:**     Oh, I see. That’s very useful actually, since her house is really
    similar to mine! Now, I would guess that my house will be valued at somewhere
    between 230k and 270k. Time to talk to my realtor!'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**你：**     哦，我明白了。这实际上非常有用，因为她的房子和我的非常相似！现在，我猜我的房子估价在 23 万到 27 万之间。是时候和我的房地产经纪人谈谈了！'
- en: '**Friend:** Glad I could help.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**朋友：** 很高兴我能帮上忙。'
- en: 'In this conversation, you said that using your neighbor Alice’s house as a
    reference is a good strategy for estimating your own price. This is because the
    two houses are similar in attributes and physically close to each other, so you
    expect them to sell for similar amounts. Alix’s house, on the other hand, is in
    California and is entirely irrelevant to our house, so even if you know how much
    she sold her house for, you won’t be able to gain any new information about what
    you’re interested in: how much your own house is worth.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这次对话中，你说使用你的邻居爱丽丝的房子作为参考是估算你自己价格的好策略。这是因为这两个房子在属性上相似，并且彼此物理上靠近，所以你期望它们卖出的价格相似。另一方面，阿利克斯的房子位于加利福尼亚，与我们的房子毫不相关，所以即使你知道她的房子卖了多少钱，你也无法获得任何关于你感兴趣的新信息：你自己的房子值多少钱。
- en: The calculation we just went through is a Bayesian update to our belief about
    the price of our house. You might be familiar with Bayes’ theorem, which is shown
    in figure 2.2\. For an excellent introduction to Bayes’ theorem and Bayesian learning,
    check out chapter 8 of Luis Serrano’s *Grokking Machine Learning* (Manning, 2021).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚进行的计算是关于我们对房子价格的信念的贝叶斯更新。你可能熟悉贝叶斯定理，如图 2.2 所示。有关贝叶斯定理和贝叶斯学习的优秀介绍，请参阅路易斯·塞拉诺的《精通机器学习》（Manning，2021）第
    8 章。
- en: Bayes’ theorem gives us a way of updating our belief about a quantity we’re
    interested in, which, in this case, is the appropriate price for our house. When
    applying Bayes’ theorem, we go from our prior belief, which is our first guess,
    to a posterior belief about the quantity in question. This posterior belief combines
    the prior belief and the likelihood of any data we observe.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理给了我们一种更新我们对我们感兴趣的数量的信念的方法，这种数量在这种情况下是我们房子的合适价格。在应用贝叶斯定理时，我们从先验信念，即我们的第一个猜测，到关于所讨论数量的后验信念。这个后验信念结合了先验信念和我们观察到的任何数据的可能性。
- en: '![](../../OEBPS/Images/02-02.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/02-02.png)'
- en: Figure 2.2 Bayes' theorem, which gives a method of updating a belief about a
    quantity of interest, represented as a probability distribution of a random variable.
    Before observing any data, we have the prior belief about X. After being updated
    with data, we obtain the posterior belief about X.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2 贝叶斯定理，它提供了一种更新对感兴趣的数量的信念的方法，表示为一个随机变量的概率分布。在观察到任何数据之前，我们对 X 有先验信念。在使用数据更新后，我们获得了关于
    X 的后验信念。
- en: In our example, we start out with a prior belief that the price is between 150k
    and 300k. The range from 150k to 300k, like your friend remarked, is quite big,
    so there’s not much information contained in this initial prior belief—anything
    between 150k and 300k is possible for this price. Now, an interesting thing happens
    when we *update* this range to a posterior belief, considering new information
    about either of the two houses’ price.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们首先有一个先验置信度，认为房价在 150k 到 300k 之间。正如你的朋友所说的那样，150k 到 300k 的范围很大，所以在这个初始先验置信度中没有太多信息，任何在
    150k 到 300k 之间的价格都是可能的。当我们根据两个房子中任意一个的价格的新信息*更新*这个范围到后验置信度时，一件有趣的事情发生了。
- en: 'First, assuming that Alix’s house in California is valued at 250k, our posterior
    belief about our own house remains unchanged: from 150k to 300k. Again, this is
    because Alix’s house is not relevant to ours, and the price of her house doesn’t
    inform us about the quantity we’re interested in.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，假设 Alix 在加利福尼亚的房子价值为 250k，我们对我们自己房子的后验置信度保持不变：从 150k 到 300k。同样，这是因为 Alix
    的房子与我们的房子无关，她的房子的价格也无法告诉我们我们感兴趣的东西的数量。
- en: 'Second, if the new information is that Alice’s house, which is right next to
    our own, is valued at 250k, then our posterior belief significantly changes from
    the prior: to the 230k–270k range. Having Alice’s house as a reference, we have
    updated our belief to be around the observed value, 250k, while narrowing down
    the range of our belief (going from a 150k difference to a 40k difference). This
    is a very reasonable thing to do, as the price of Alice’s house is very informative
    with respect to the price of our house. Figure 2.3 visualizes this entire process.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，如果新的信息是 Alice 的房子，它就在我们的旁边，价值为 250k，那么我们的后验置信度就会从先验置信度大幅改变：变为 230k 到 270k
    的范围。有了 Alice 的房子作为参考，我们已经根据观察值 250k 更新了我们的置信度，同时缩小了置信度的范围（从 150k 的差异缩小到 40k 的差异）。这是非常合理的事情，因为
    Alice 的房子对我们房子的价格非常具有信息量。图2.3 可视化了整个过程。
- en: '![](../../OEBPS/Images/02-03.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/02-03.png)'
- en: Figure 2.3 Updating the belief about the price of our house in a Bayesian way.
    Depending on how similar the house whose price was observed is to our house, the
    posterior belief either stays the same or is drastically updated.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3 以贝叶斯方式更新我们房价的置信度。根据观察的房价与我们房子的相似程度，后验置信度要么保持不变，要么发生 drastīc 更新。
- en: Note that the numbers in the example are not exact and are only used so that
    the example makes intuitive sense. However, we will see that by using a multivariate
    Gaussian distribution to model our belief, we can realize this intuitive update
    procedure in a quantifiable way. Further, with such a Gaussian distribution, we
    can determine whether a variable (someone’s house) is similar enough to the variable
    we’re interested in (our own house) to influence our posterior belief and to what
    extent.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，示例中的数字不是精确的，只是为了使例子更具直观性。然而，我们将看到，使用多元高斯分布来建模我们的置信度，可以以可量化的方式实现这种直观的更新过程。此外，利用这样的高斯分布，我们可以确定一个变量（某人的房子）是否与我们感兴趣的变量（我们自己的房子）足够相似，以影响我们的后验置信度的程度。
- en: 2.2 Modeling correlations with multivariate Gaussian distributions and Bayesian
    updates
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 用多元高斯分布和贝叶斯更新建模相关性
- en: In this section, we learn about multivariate Gaussian distributions (or multivariate
    Gaussians—or simply Gaussians) and see how they facilitate the update rule we
    saw previously. This serves as the basis for our subsequent discussion on GPs.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习多元高斯分布（或多元高斯分布，或简称高斯分布）以及它们如何促进我们之前看到的更新规则。这为我们后续讨论高斯过程奠定了基础。
- en: 2.2.1 Using multivariate Gaussian distributions to jointly model multiple variables
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.1 使用多元高斯分布共同建模多个变量
- en: Here, we first cover what multivariate Gaussians are and what they can model.
    We will see that with a covariance matrix, a multivariate Gaussian (MVN) describes
    not only the behavior of the individual random variables but also the correlation
    of these variables.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们首先介绍了多元高斯分布是什么以及它们可以模拟的内容。我们将看到，通过使用协方差矩阵，多元高斯分布描述了不仅是单个随机变量的行为，而且还描述了这些变量之间的相关性。
- en: First, let’s consider normal distributions—aka the bell curve. Normal distributions
    are highly common in the real world and are used to model a wide range of quantities;
    examples include height, IQ, income, and weight at birth.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们来考虑正态分布，也叫钟形曲线。正态分布在现实世界中非常常见，被用来模拟各种量，比如身高、智商、收入和出生体重。
- en: An MVN distribution is what we would use when we want to model more than one
    quantity. To do this, we aggregate these quantities into a vector of random variables,
    and this vector is then said to follow an MVN distribution. This aggregation is
    depicted in figure 2.4.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要建模多于一个量时，我们将使用MVN分布。为此，我们将这些量聚合成一个随机变量向量，然后称此向量遵循MVN分布。这个聚合如图2.4所示。
- en: '![](../../OEBPS/Images/02-04.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/02-04.png)'
- en: Figure 2.4 An MVN distribution groups multiple normally distributed random variables
    together. While the mean vector of the MVN concatenates the mean values, the covariance
    matrix models the correlations between the individual variables.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4 MVN分布将多个正态分布的随机变量组合在一起。虽然MVN的均值向量连接了均值，但协方差矩阵模拟了各个变量之间的相关性。
- en: Definition Consider a random vector *X* = [*X*[1]*X*[2] ... *X[n]*] that follows
    a Gaussian distribution denoted by *N*(*μ*, Σ), where *μ* is a vector of length
    *n* and Σ is an *n*-by-*n* matrix. Here, *μ* is called the mean vector, whose
    individual elements denote the expected values of corresponding random variables
    in *X*, and Σ is the covariance matrix, which describes the variance of individual
    variables as well as correlations between variables.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 考虑一个随机向量*X* = [*X*[1]*X*[2] ... *X*[n]]，它遵循一个被标记为*N*(*μ*, Σ)的高斯分布，其中*μ*是长度为*n*的向量，Σ是一个*n*乘以*n*的矩阵。在这里，*μ*被称为均值向量，其各个元素表示*X*中相应随机变量的期望值，Σ是协方差矩阵，描述了各个变量的方差以及变量之间的相关性。
- en: 'Let’s take a moment to parse the definition of an MVN distribution:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花一点时间解析MVN分布的定义：
- en: First, due to the convenient properties of an MVN, each random variable in the
    vector *X* follows a normal distribution. Specifically, the *i*-th variable *X[i]*
    has the mean value of *μ[i]*, which is the *i*-th element of the mean vector *μ*
    of the MVN.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，由于MVN的便利性质，向量*X*中的每个随机变量都遵循正态分布。具体来说，第*i*个变量*X[i]*的平均值为*μ[i]*，这是MVN的均值向量*μ*的第*i*个元素。
- en: Further, the variance of *X[i]* is the *i*-th *diagonal* entry of the covariance
    matrix Σ.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，*X[i]*的方差是协方差矩阵Σ的第*i*个*对角*条目。
- en: If we have a vector of random variables following an MVN, then each individual
    variable corresponds to a known normal distribution.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们有一个遵循MVN的随机变量向量，那么每个单独的变量对应于一个已知的正态分布。
- en: 'If the diagonal entries in the covariance matrix Σ are the variances of the
    individual variables, what about the off-diagonal entries? The entry in the *i*-th
    row and *j*-th column of this matrix denotes the covariance between *X[i]* and
    *X[j]*, which is related to the correlation between the two random variables.
    Assuming the correlation is positive, the following is true:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果协方差矩阵Σ中的对角线条目是各个变量的方差，那么非对角线条目呢？该矩阵中第*i*行和第*j*列的条目表示*X[i]*和*X[j]*之间的协方差，这与两个随机变量之间的相关性有关。假设相关性为正，则以下结论成立：
- en: If this correlation is high, then the two random variables *X[i]* and *X[j]*
    are said to be correlated. This means that if the value of one increases, the
    value of the other also tends to increase, and if the value of one decreases,
    the value of the other will decrease. Your neighbor Alice’s house and your own
    are examples of correlated variables.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果这种相关性很高，那么两个随机变量*X[i]*和*X[j]*被认为是相关的。这意味着如果一个值增加，另一个值也倾向于增加，如果一个值减少，另一个值也会减少。你的邻居爱丽丝的房子和你自己的房子就是相关变量的例子。
- en: On the other hand, if this correlation is low and close to zero, then no matter
    what the value of *X[i]* is, what we know about the value of *X[j]* most likely
    does not change much. This is because there is no correlation between the two
    variables. Alix’s house in California and our house fall into this category.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一方面，如果这种相关性很低且接近零，则无论*X[i]*的值是什么，我们关于*X[j]*值的了解很可能不会发生太大变化。这是因为两个变量之间没有相关性。加利福尼亚州的阿利克斯的房子和我们的房子属于这个类别。
- en: Negative correlations
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 负相关性
- en: 'The previous description is for positive correlations. A correlation can also
    be negative, indicating that the variables move in opposite directions: if one
    variable increases, the other will decrease, and vice versa. Positive correlations
    illustrate the important concepts we aim to learn here, so we won’t worry about
    the negative correlation case.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的描述是针对正相关性的。相关性也可以是负的，表示变量朝相反的方向移动：如果一个变量增加，另一个变量就会减少，反之亦然。正相关性展示了我们在这里想要学习的重要概念，所以我们不会担心负相关性的情况。
- en: 'To make our discussion more concrete, let’s define an MVN distribution that
    jointly models three random variables: the price of our house, A; the price of
    our neighbor Alice’s house, B; and the price of Alix’s house in California, C.
    This three-dimensional Gaussian also has a covariance matrix described in figure
    2.4.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使我们的讨论更具体，让我们定义一个MVN分布，同时对三个随机变量进行建模：我们房子的价格A；邻居爱丽丝房子的价格B；以及加利福尼亚的阿利克斯房子的价格C。这个三维高斯分布的协方差矩阵也在图2.4中描述。
- en: Note It’s usually convenient to assume that the mean vector of this Gaussian
    is normalized to be the zero vector. This normalization is usually done, in practice,
    to simplify a lot of mathematical details.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 通常方便假设这个高斯分布的均值向量归一化为零向量。这种归一化通常在实践中完成，以简化许多数学细节。
- en: Once again, the diagonal cells tell us the variances of individual random variables.
    B has a slightly larger variance (3) than A (1), meaning we are more uncertain
    about the value of B because we don’t know everything about our neighbor’s house,
    so a more accurate estimate cannot be done. The third variable, C, on the other
    hand, has the largest variance to denote the fact that houses in California have
    an overall wider price range.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，对角线单元格告诉我们单个随机变量的方差。B的方差（3）略大于A（1），这意味着我们对B的值更不确定，因为我们对邻居的房子不了解所有情况，所以不能做出更准确的估计。另一方面，第三个变量C具有最大的方差，表示加利福尼亚的房屋价格范围更广。
- en: Note The values being used here (1, 3, 10) are example values to make the point
    that the larger a variance of a random variable, the more uncertain we are about
    the value of the variable (before learning what its value is).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 这里使用的值（1, 3, 10）是示例值，目的是说明随机变量方差越大，我们对该变量的值越不确定（在了解其值之前）。
- en: Further, the covariance between our house, A, and our neighbor’s, B, is 0.9,
    which indicates the prices of the two houses are correlated in a non-negligible
    way. This makes sense because if we know the price of our neighbor’s house, we
    will be able to obtain a better estimate for our own house that is on the same
    street. Also notice that neither A nor B has any correlation with the price of
    the house in California, since location-wise, C has nothing in common with A or
    B. Another way to say this is that even if we know how much the house in California
    costs, we won’t learn anything about the price of our own house. Let’s now visualize
    this three-dimensional Gaussian using a parallel coordinates plot in figure 2.5.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们家（A）和邻居家（B）之间的协方差为0.9，这意味着两栋房子的价格存在着显著的相关性。这是有道理的，因为如果我们知道邻居房子的价格，我们就能更好地估算出我们自己房子的价格，因为它们位于同一条街上。还要注意的是，无论是A还是B与加利福尼亚房价都没有任何相关性，因为位置上来看，C与A或B没有任何共同之处。另一种说法是，即使我们知道加利福尼亚房子的价格，我们也不会对我们自己房子的价格了解多少。现在让我们在图2.5中使用平行坐标图来可视化这个三维高斯分布。
- en: '![](../../OEBPS/Images/02-05.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/02-05.png)'
- en: Figure 2.5 Parallel coordinates plot visualizing a mean-normalized MVN from
    the housing price example. The error bars indicate 95% CIs of corresponding normal
    distributions, while the faded lines show samples drawn from the multivariate
    Gaussian.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5 平行坐标图可视化了来自房价示例的均值归一化MVN。误差条表示相应正态分布的95% CI，而淡化的线显示了从多元高斯中绘制的样本。
- en: 'Note the bold diamonds and corresponding error bars in the figure:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 注意图中的粗体菱形和相应的误差条：
- en: The bold diamonds represent the mean vector of our Gaussian, which is simply
    the zero vector.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 粗体菱形代表我们高斯分布的均值向量，即零向量。
- en: The error bars denote the 95% credible intervals (CIs) of the three individual
    variables. Going from A to B to C, we observe larger and larger CIs, corresponding
    to the increasing values of the respective variances.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 误差条表示三个单独变量的95%可信区间（CI）。从A到B到C，我们观察到越来越大的CI，对应着相应方差的增加。
- en: Credible intervals
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 可信区间
- en: A (1 – α) CI of a normal distribution of a random variable *x* is a range in
    which the probability that *x* falls inside this range is exactly (1 – α). Statisticians
    typically use 95% CIs for normal distributions. There is nothing really special
    about the number 95% here, except for the fact that 95% is the threshold many
    statistical procedures use to determine whether something is meaningful or not.
    For example, a *t* test typically uses a confidence level of 1 – α = 0.95, corresponding
    to the fact that a *p*-value less than α = 0.05 indicates significant results.
    A convenient fact about normal distributions is that *μ* ± 1.96σ is a 95% CI (some
    even use *μ* ± 2σ), where *μ* and σ are the mean and standard deviation of the
    variable *x*, which is an easy-to-compute quantity.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 正态分布随机变量*x*的(1 – α) CI是一个范围，其中*x*落入这个范围的概率恰好为(1 – α)。统计学家通常对正态分布使用 95% CI。这里并没有什么特别之处，只是因为
    95% 是许多统计程序用来确定某事是否有意义的阈值。例如，一个*t*检验通常使用置信水平 1 – α = 0.95，对应着*p*值小于 α = 0.05 表示显著结果。关于正态分布的一个方便事实是*μ*
    ± 1.96σ是一个 95% CI（有些甚至使用*μ* ± 2σ），其中*μ*和σ是变量*x*的均值和标准差，这是一个容易计算的量。
- en: Figure 2.5 represents our *prior belief* about the normalized prices of the
    three houses. Starting from this prior, we guess that all three have a normalized
    price of zero, and we have varying levels of uncertainty regarding our guesses.
    Further, as we are working with a random distribution, we could draw samples from
    this MVN. These samples are shown as connected faded diamonds.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5 表示我们关于三栋房子标准化价格的*先验信念*。从这个先验开始，我们猜测所有三栋房子的标准化价格都为零，并且对我们的猜测有不同程度的不确定性。此外，由于我们正在处理一个随机分布，我们可以从这个
    MVN 中抽取样本。这些样本显示为相连的半透明菱形。
- en: 2.2.2 Updating MVN distributions
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.2 更新 MVN 分布
- en: With an MVN distribution in hand, we will see how we can update this distribution
    given some data we observe in this subsection. Specifically, following our example
    at the beginning of the chapter, we’d like to derive the *posterior belief* about
    these prices upon observing the value of either B or C. This is an important task
    as it is how an MVN, as well as a GP, learns from data.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 有了一个 MVN 分布，我们将看到如何在本小节中观察到一些数据后更新这个分布。具体地说，跟随本章开头的示例，我们想要根据观察到的 B 或 C 的值推导出关于这些价格的*后验信念*。这是一个重要的任务，因为这是
    MVN 以及 GP 从数据中学习的方式。
- en: 'Definition This update process is sometimes called *conditioning*: deriving
    the *conditional distribution* of a variable, given that we know the value of
    some other variable. More specifically, we are conditioning our belief—which is
    a joint trivariate Gaussian—on the value of either B or C, obtaining the joint
    posterior distribution for these three variables.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 这个更新过程有时被称为*条件设定*：推导出一个变量的*条件分布*，在我们已知某个其他变量的值的情况下。更具体地说，我们正在将我们的信念——一个联合三元高斯——条件设定为
    B 或 C 的值，获得这三个变量的联合后验分布。
- en: Here, using the Bayes’ theorem in figure 2.2, we can derive this posterior distribution
    in closed form. However, the derivation is rather math-heavy, so we won’t go into
    it here. We just need to know that we have a formula in which we can plug the
    value of B or C that we’d like to condition on, and the formula will tell us what
    the posterior distributions of A, B, and C are. Surprisingly, the posterior distribution
    of a Gaussian is conditioned on data that is also Gaussian, and we can obtain
    the exact posterior means and variances that specify the posterior Gaussians.
    (Later in the chapter, we see that when we implement a GP in Python, GPyTorch
    takes care of this math-heavy update for us.)
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，利用图 2.2 中的贝叶斯定理，我们可以得出这个后验分布的闭式形式。然而，推导过程相当数学密集，所以我们不会在这里详细介绍。我们只需要知道，我们有一个公式，可以插入我们想要条件的
    B 或 C 的值，然后这个公式会告诉我们 A、B 和 C 的后验分布是什么。令人惊讶的是，高斯的后验分布是根据同样是高斯的数据进行条件设定的，我们可以获得指定后验高斯的确切后验均值和方差。（在本章后面，我们会看到当我们在
    Python 中实现 GP 时，GPyTorch 会为我们处理这个数学密集的更新。）
- en: Note For the interested reader, this formula and its derivation can be found
    in chapter 2, section 2 of the book *Gaussian Processes for Machine Learning*
    by Carl Edward Rasmussen and Christopher K. I. Williams (MIT Press, 2006), which
    is often considered the bible of GPs.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 对于感兴趣的读者，这个公式及其推导可以在 Carl Edward Rasmussen 和 Christopher K. I. Williams（MIT
    Press，2006）的书籍*Gaussian Processes for Machine Learning*的第 2 章第 2 节中找到，这本书通常被认为是
    GP 的圣经。
- en: '![](../../OEBPS/Images/02-05-unnumb-1.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/02-05-unnumb-1.png)'
- en: Let’s now regenerate the parallel coordinates plot, conditioning the MVN on
    B = 4 as an example value for B. The result is shown in figure 2.6.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们重新生成平行坐标图，以 B = 4 作为 B 的示例值进行条件限制。结果如图 2.6 所示。
- en: '![](../../OEBPS/Images/02-06.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/02-06.png)'
- en: Figure 2.6 Parallel coordinates plot visualizing the MVN from figure 2.5, conditioned
    on B = 4\. Here, the distribution of A is updated, and all drawn samples interpolate
    B = 4.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6 平行坐标图可视化了图 2.5 中 MVN 在 B = 4 条件下的情况。在这里，A 的分布被更新，所有绘制的样本都插值为 B = 4。
- en: 'Upon updating our belief with an observation about B, a few things have changed
    in our posterior belief:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在用关于 B 的观察更新我们的信念后，我们的后验信念发生了一些变化：
- en: The distribution of A changes, taking on a slightly larger mean value due to
    the positive correlation between A and B. Further, its error bars now have a smaller
    range.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: A 的分布会发生变化，由于 A 和 B 之间的正相关关系，其均值会略微增加。此外，其误差范围现在变得更小。
- en: The posterior distribution of B simply becomes a special normal distribution
    with zero variance since we know for sure what its value is in the posterior.
    In other words, there is no uncertainty about the value of B anymore.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: B 的后验分布简单地变成了一个具有零方差的特殊正态分布，因为我们现在对后验中的 B 值完全确定。换句话说，对于 B 的值不再存在不确定性。
- en: Meanwhile, the distribution of C stays the same after the update as it has no
    correlation with B.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与此同时，C 的分布在更新后保持不变，因为它与 B 没有相关性。
- en: All of this makes sense and corresponds with our intuition from the housing
    price example. Specifically, when we find out about our neighbor’s house, the
    belief about our own house is updated to be similar to the observed price, and
    our uncertainty also decreases.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都是有道理的，并且与我们从房价示例中得到的直觉相符。具体来说，当我们得知邻居的房子价格时，关于我们自己房子的信念被更新为类似于观察到的价格，并且我们的不确定性也减少了。
- en: What happens when we condition on C? As you might have guessed, for the same
    reason that C stays the same after conditioning on a value of B, both the posterior
    distribution of A and that of B remain unchanged when we condition on C. Figure
    2.7 shows this for C = 4.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们以 C 为条件时会发生什么？正如您可能猜到的那样，由于在 B 的值上进行条件限制后 C 保持不变的原因，当我们以 C 为条件时，A 和 B 的后验分布都保持不变。图
    2.7 展示了 C = 4 的情况。
- en: '![](../../OEBPS/Images/02-07.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/02-07.png)'
- en: Figure 2.7 Parallel coordinates plot visualizing the MVN from figure 2.5, conditioned
    on C = 4\. Here, no other marginal distribution changes. All drawn samples interpolate
    C = 4.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.7 平行坐标图可视化了图 2.5 中 MVN 在 C = 4 条件下的情况。在这里，没有其他边缘分布改变。所有绘制的样本都插值为 C = 4。
- en: This is when we find out that a house in California was sold. As this house
    has nothing to do with our own house in Missouri, the belief about the price of
    our house stays the same.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们得知加利福尼亚州的一栋房子被卖掉时，我们发现这栋房子与我们在密苏里州的房子无关，因此对我们房子价格的信念保持不变。
- en: There is another interesting thing about figures 2.6 and 2.7\. Notice that when
    we condition on B = 2 in figure 2.6, all the samples we draw of the posterior
    MVN pass through the point (B, 2). This is because in our posterior belief, we
    don’t have any uncertainty about what value B takes anymore, and any sample drawn
    from the posterior distribution needs to satisfy the constraints from this condition.
    The same thing goes for the point (C, 2) in figure 2.7.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6 和图 2.7 还有另一个有趣之处。请注意，在图 2.6 中，当我们以 B = 2 为条件时，我们绘制的所有后验 MVN 的样本都经过点 (B,
    2)。这是因为在我们的后验信念中，我们对于 B 取值不再有任何不确定性，因此从后验分布中绘制的任何样本都需要满足此条件的约束。图 2.7 中的点 (C, 2)
    也是同样道理。
- en: Visually, you could think of this as meaning that when we condition on a variable,
    we “tie” the samples drawn from the prior distribution (in figure 2.5) into a
    knot at the same point corresponding to the variable we condition on, as shown
    in figure 2.8.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 从视觉上来看，您可以将这理解为当我们对一个变量进行条件限制时，我们将从先验分布（在图 2.5 中）绘制的样本在相应的变量条件处“绑定”成一个结，如图 2.8
    所示。
- en: '![](../../OEBPS/Images/02-08.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/02-08.png)'
- en: Figure 2.8 Conditioning a Gaussian on an observation is similar to tying a knot
    around that observation. All samples from the posterior distribution need to pass
    through the knot, and there’s no uncertainty at the observed point.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.8 在观察上对高斯进行条件限制类似于在该观察周围打结。后验分布中的所有样本都需要通过这个结，且在观察点没有不确定性。
- en: Finally, we can picture the Bayesian conditioning procedure we have just gone
    through with a diagram analogous to figure 2.3\. This is shown in figure 2.9.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以通过类似于图2.3的图表来描述我们刚刚进行过的贝叶斯条件过程。这在图2.9中显示。
- en: '![](../../OEBPS/Images/02-09.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/02-09.png)'
- en: Figure 2.9 Updating the belief about the price of our house in a Bayesian way.
    Depending on how similar the house whose price was observed in relation to our
    house is, the posterior belief either stays the same or is drastically updated.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.9：以贝叶斯方式更新关于我们房子价格的信念。根据观察到的房价与我们房子的相似程度，后验信念要么保持不变，要么得到极大更新。
- en: Again, if we condition our Gaussian on C, the posterior distributions of the
    uncorrelated variables remain unchanged. If we condition on B, however, the variable
    that is correlated to it, A, gets updated.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，如果我们将高斯分布条件设置为C，则不相关变量的后验分布保持不变。然而，如果我们将其设置为B，与之相关的变量A会得到更新。
- en: 2.2.3 Modeling many variables with high-dimensional Gaussian distributions
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.3：用高维高斯分布建模多个变量
- en: An MVN distribution need not only contain three random variables; it can, in
    fact, simultaneously model any finite number of variables. In this subsection,
    we learn that a higher-dimensional Gaussian works in the same way as what we have
    seen so far. So, let’s say that instead of a 3-dimensional Gaussian representing
    three houses, we have a 20-dimensional Gaussian encoding information about an
    array of houses on a street. An even higher-dimensional Gaussian would model houses
    in a city or country.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: MVN分布不仅需要包含三个随机变量；事实上，它可以同时模拟任意数量的变量。在本小节中，我们了解到高维高斯分布的工作方式与我们迄今所见的相同。所以，我们可以说，一个代表三座房子的3维高斯分布，我们可以将其替换为一个编码街道上一系列房屋信息的20维高斯分布。甚至更高维的高斯分布可以模拟城市或国家中的房屋。
- en: Further, with these parallel coordinates plots, we can visualize all individual
    variables of a high-dimensional Gaussian at the same time. This is because each
    variable corresponds to a single error bar, occupying only one slot on the *x*-axis.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通过这些平行坐标图，我们可以同时可视化高维高斯分布的所有单个变量。这是因为每个变量对应一个单独的误差条，只占用*x*轴上的一个位置。
- en: We once again normalize its mean vector to be the zero vector, and while it’s
    not convenient to show its 20-by-20 covariance matrix, we could plot a heat map
    visualizing this matrix, as in figure 2.10.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次将其均值向量归一化为零向量，虽然显示其20×20的协方差矩阵不方便，但我们可以绘制一个热图来可视化这个矩阵，就像图2.10中所示的那样。
- en: '![](../../OEBPS/Images/02-10.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/02-10.png)'
- en: Figure 2.10 Heat map showing the covariance matrix of a 20-dimensional Gaussian
    distribution. Neighboring variables are more correlated than those that are far
    away, as indicated by a darker shade.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.10：显示20维高斯分布的协方差矩阵的热图。相邻变量之间的相关性比远离的变量更大，这由较深的色调表示。
- en: The diagonal entries, or the variance of individual variables, are all 1s, in
    this case. Further, variables are ordered so that those that are close to each
    other are correlated; that is, their covariance takes a larger value. Variables
    that are far away from each other are, on the other hand, less correlated, and
    their covariances are close to zero. For example, any pair of consecutive variables
    in this Gaussian (the first and the second, the second and the third, etc.) have
    a covariance of roughly 0.87\. That is, any two houses that are next to each other
    have a covariance of 0.87\. If we consider the 1st and the 20th variable—that
    is, the house at one end of the street and the house at the other end—their covariance
    is effectively zero.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 对角线条目，或者单个变量的方差，在这种情况下都是1。此外，变量被排序，使得彼此靠近的变量是相关的；也就是说，它们的协方差取值较大。相反，彼此远离的变量则不太相关，它们的协方差接近于零。例如，在这个高斯分布中的任意一对连续变量（第一个和第二个，第二个和第三个等等）的协方差大约为0.87。也就是说，任意两个相邻的房子的协方差为0.87。如果我们考虑第1个和第20个变量——也就是，街道一端的房子和另一端的房子——它们的协方差实际上是零。
- en: This is very intuitive as we expect houses that are close by to have similar
    prices, so once we know the price of a house, we gain more information about the
    prices of those that are around that area than about the prices of those that
    are far away.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常直观，因为我们期望附近的房子价格相似，所以一旦我们知道一座房子的价格，我们就可以更多地了解该地区周围房屋的价格，而不是远处房屋的价格。
- en: '![](../../OEBPS/Images/02-11.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/02-11.png)'
- en: Figure 2.11 Error bars and samples drawn from a prior (left) and a posterior
    (right) Gaussian distribution, conditioned on the 10th variable having a value
    of 2\. Uncertainty in variables close to the 10th reduces in the posterior, and
    their mean values are updated to be close to 2.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.11 误差线和从先验（左）和后验（右）高斯分布中抽取的样本，以第 10 个变量的值为 2 条件。接近第 10 个变量的变量在后验中的不确定性减少，其均值更新为接近
    2。
- en: 'How does this play out in a parallel coordinates plot? Figure 2.11 shows our
    prior Gaussian on the left and the posterior Gaussian conditioned on the 10th
    variable having a value of 2 on the right. Basically, we are simulating the event
    in which we find out that the price of the 10th house is 2 (whose exact unit is
    omitted):'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这在平行坐标图中如何体现？图 2.11 显示了我们左侧的先验高斯和右侧第 10 个变量的值为 2 的后验高斯。基本上，我们正在模拟以下事件，即我们发现第
    10 座房子的价格为 2（其确切单位被省略）：
- en: First, we once again see this phenomenon in which the error bars and samples
    are tied into a knot around the observation we condition on in the posterior distribution.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们再次看到这种现象，在后验分布中，误差线和样本围绕我们条件观察到的观察点打成结。
- en: Second, due to the correlation structure imposed by the covariance matrix, variables
    close to the 10th have their mean values “dragged up” so that the mean vector
    now smoothly interpolates the point (10, 2). This means we have updated our belief,
    as the surrounding houses now have their prices increased to reflect the information
    we have learned.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，由于协方差矩阵施加的相关结构，接近第 10 个变量的变量其均值被“拉高”，以便均值向量现在平滑地插值到点 (10, 2)。这意味着我们更新了我们的信念，因为周围的房屋现在其价格增加以反映我们学到的信息。
- en: Finally, the uncertainty (denoted by the error bars) around this point (10,
    2) decreases after the conditioning. This is a very good property to have, as
    intuitively, if we know the value of a variable, we should become more certain
    about the values of other variables correlated with the variable we know. That
    is, if we know the price of a house, we become more certain about the prices of
    nearby houses. This property is the basis of the calibrated quantification of
    uncertainty that GPs offer, which we see in the next section of this chapter.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，在这一点 (10, 2) 条件之后，围绕这一点的不确定性（由误差线表示）减小。这是一个非常好的属性，因为直觉上，如果我们知道一个变量的值，我们应该对与我们所知变量相关的其他变量的值更加确定。也就是说，如果我们知道一座房子的价格，我们就会更加确定附近房屋的价格。这个属性是高斯过程提供的校准不确定性量化的基础，我们在本章的下一节中会看到。
- en: 2.3 Going from a finite to an infinite Gaussian
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 从有限到无限的高斯分布
- en: We are now ready to discuss what a GP is. In the same manner as when we have
    three variables, A, B, and C, or 20, as in the previous section, let’s say we
    now have an infinite number of variables, all belonging to an MVN. This *infinite-dimensional*
    Gaussian is called a *Gaussian process*.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备讨论什么是高斯过程。与前面三个变量 A、B 和 C，或者之前章节中的 20 个变量相同的方式，我们假设现在有无限多个变量，所有这些变量都属于多元正态分布。这个
    *无限维* 高斯被称为 *高斯过程*。
- en: Imagine predicting housing prices across a very large, densely populated area.
    The scale of the entire area is so large that if we move away from a house by
    a very small amount, we will arrive at a different house. Given the high density
    of the variables (houses) in this Gaussian, we can treat this whole area as having
    infinitely many houses; that is, the Gaussian distribution has infinitely many
    variables.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下在一个非常大而密集的区域内预测房价。整个区域的规模如此之大，以至于如果我们离一座房子移动了一个非常小的距离，我们就会到达另一座房子。鉴于高斯分布中变量（房屋）的高密度，我们可以将整个区域视为有无限多个房屋；也就是说，高斯分布有无限多个变量。
- en: '![](../../OEBPS/Images/02-12.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/02-12.png)'
- en: Figure 2.12 Modeling housing prices in California using different numbers of
    variables. The more variables we have, the smoother our model becomes and the
    closer we approach an infinite-dimensional model.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.12 使用不同数量的变量对加州房价建模。我们拥有的变量越多，我们的模型就越平滑，我们越接近无限维模型。
- en: This is illustrated in figure 2.12 using a dataset containing 5,000 house prices
    in California. In the top-left panel, we show the individual data points in a
    scatter plot. In the remaining panels, we model the data using various numbers
    of variables, where each variable corresponds to a region inside the map of California.
    As the number of variables increases, our model becomes more fine-grained. When
    this number is infinite—that is, when we can make a prediction in any region on
    this map, however small—our model exists in an infinite-dimensional space.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这在图 2.12 中使用一个包含加利福尼亚州 5,000 个房价的数据集进行了说明。在左上角的面板中，我们展示了散点图中的个别数据点。在其余的面板中，我们使用各种数量的变量来对数据进行建模，其中每个变量对应于加利福尼亚地图内的一个区域。随着变量数量的增加，我们的模型变得更加精细。当这个数量是无限的时候——也就是说，当我们可以在这张地图上的任何区域进行预测，无论多么小——我们的模型存在于一个无限维空间中。
- en: 'This is exactly what a Gaussian process is: a Gaussian distribution in an infinite-dimensional
    space. The ability to make a prediction in any region helps us move away from
    a finite-dimensional MVN and obtain an ML model. Strictly speaking, the concept
    of a Gaussian distribution doesn’t apply when there are infinitely many variables,
    so the technical definition is the following.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是高斯过程的含义：在无限维空间中的高斯分布。在任何区域进行预测的能力帮助我们摆脱了有限维多元正态分布，并获得了一个机器学习模型。严格来说，当变量有无穷多个时，高斯分布的概念并不适用，因此技术上的定义如下。
- en: Definition A Gaussian process is a collection of random variables such that
    the joint distribution of every finite subset of those variables is an MVN.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 高斯过程是一组随机变量，使得这些变量的任意有限子集的联合分布是一个多元正态分布。
- en: This definition means that if we have a GP model to describe a function *ƒ*,
    then the function values at *any* set of points are modeled by an MVN distribution.
    For example, the vector of variables [*ƒ*(1) *ƒ*(2) *ƒ*(3)] follows a three-dimensional
    Gaussian; [*ƒ*(1) *ƒ*(0) *ƒ*(10) *ƒ*(5) *ƒ*(3)] follows a different, five-dimensional
    Gaussian; and [*ƒ*(0.1) *ƒ*(0.2) ... *ƒ*(9.9) *ƒ*(10)] follows yet another Gaussian.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这个定义意味着，如果我们有一个高斯过程模型来描述一个函数 *ƒ*，那么在*任何*一组点处的函数值都由一个多元正态分布来建模。例如，变量向量 [*ƒ*(1)
    *ƒ*(2) *ƒ*(3)] 遵循一个三维高斯分布；[*ƒ*(1) *ƒ*(0) *ƒ*(10) *ƒ*(5) *ƒ*(3)] 遵循另一个不同的、五维的高斯分布；以及
    [*ƒ*(0.1) *ƒ*(0.2) ... *ƒ*(9.9) *ƒ*(10)] 遵循另一个高斯分布。
- en: '![](../../OEBPS/Images/02-13.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/02-13.png)'
- en: Figure 2.13 Parallel coordinates plots of different Gaussian distributions.
    Any finite subset of a GP is an MVN. As the number of variables approaches infinity,
    we obtain a GP and can make predictions everywhere in the domain.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.13 不同高斯分布的平行坐标图。高斯过程的任何有限子集都是多元正态分布。随着变量数量趋于无穷，我们得到一个高斯过程，并且可以在域中的任何地方进行预测。
- en: This is illustrated in figure 2.13\. The first three panels show, in parallel
    coordinates plots, a trivariate Gaussian for [*ƒ*(–2) *ƒ*(1) *ƒ*(4)], an 11-variate
    Gaussian for [*ƒ*(–4.5) *ƒ*(–4) ... *ƒ*(4) *ƒ*(4.5)], and a 101-variate Gaussian
    across a denser grid. Finally, in the last panel, we have infinitely many variables,
    which gives us a GP.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这在图 2.13 中有所说明。前三个面板显示了平行坐标图中的三元高斯分布，分别为 [*ƒ*(–2) *ƒ*(1) *ƒ*(4)]，一个 11 元高斯分布，对应
    [*ƒ*(–4.5) *ƒ*(–4) ... *ƒ*(4) *ƒ*(4.5)]，以及一个在更密集的网格上的 101 元高斯分布。最后，在最后一个面板中，我们有无限多个变量，这给了我们一个高斯过程。
- en: 'Since we are now in infinite dimensions, it doesn’t make sense to talk about
    the mean vector and covariance matrix anymore. Instead, what we have with a GP
    is a mean *function* and a covariance *function*, but the respective roles of
    these two objects are still the same as with an MVN:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们现在处于无限维空间，讨论均值向量和协方差矩阵已经没有意义了。相反，我们在高斯过程中有的是一个均值 *函数* 和一个协方差 *函数*，但这两个对象的角色仍然与多元正态分布相同：
- en: First, the mean function, which takes in one input, *x*, computes the expectation
    of the function value *ƒ*(*x*).
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先是均值函数，它接受一个输入 *x*，计算函数值 *ƒ*(*x*) 的期望。
- en: Second, the covariance function takes in two inputs, *x*[1] and *x*[2], and
    computes the covariance between the two variables, *ƒ*(*x*[1]) and *ƒ*(*x*[2]).
    If *x*[1] is the same as *x*[2], then this covariance value is simply the variance
    of the normal distribution of *ƒ*(*x*). If *x*[1] is different from *x*[2], the
    covariance denotes the correlation between the two variables.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二，协方差函数接受两个输入，*x*[1] 和 *x*[2]，并计算两个变量 *ƒ*(*x*[1]) 和 *ƒ*(*x*[2]) 之间的协方差。如果 *x*[1]
    等于 *x*[2]，那么这个协方差值就是 *ƒ*(*x*) 的正态分布的方差。如果 *x*[1] 不同于 *x*[2]，协方差表示两个变量之间的相关性。
- en: As the mean and covariance are functions, we are no longer tied to a fixed number
    of variables—instead, we effectively have infinitely many variables and can make
    our predictions *anywhere*, as illustrated in figure 2.13\. This is why although
    a GP has all the properties of an MVN, the GP exists in infinite dimensions.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 由于均值和协方差是函数，我们不再受限于固定数量的变量——相反，我们有效地拥有无限数量的变量，并且可以在*任何地方*进行我们的预测，如图2.13所示。这就是为什么尽管高斯过程具有多元正态分布的所有特性，但高斯过程存在于无限维度的原因。
- en: For the same reason, a GP can be considered as a distribution over functions,
    as the title of this chapter suggests. The progression we have gone through in
    this chapter, from a one-dimensional normal to a GP, is summarized in table 2.1.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 出于同样的原因，高斯过程可以被视为对函数的分布，就像本章的标题所暗示的那样。本章我们经历的从一维正态分布到高斯过程的过程，在表2.1中总结。
- en: Table 2.1 Gaussian distribution objects and what they model. With a GP, we operate
    under infinite dimensions, modeling functions instead of numbers or vectors.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.1 高斯分布对象及其模拟对象。使用高斯过程时，我们在无限维度下操作，模拟函数而不是数字或向量。
- en: '| Distribution type | Number of modeled variables | Description |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 分布类型 | 模拟变量数量 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| A one-dimensional normal distribution | One | A distribution over numbers
    |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 一维正态分布 | 一个 | 数字的分布 |'
- en: '| An MVN distribution | Finitely many | A distribution over vectors of finite
    length |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 多元正态分布 | 有限数量 | 有限长度向量的分布 |'
- en: '| A GP | Infinitely many | A distribution over functions |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 高斯过程 | 无限数量 | 函数的分布 |'
- en: To see a GP in action, let’s reexamine the curve-fitting procedure in figure
    2.1 at the beginning of this chapter, where we limit our domain to between –5
    and 5\. This is shown in figure 2.14.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 要看高斯过程的实际应用，让我们重新审视本章开头图2.1中的曲线拟合过程，我们将我们的域限制在-5到5之间。如图2.14所示。
- en: '![](../../OEBPS/Images/02-14.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/02-14.png)'
- en: Figure 2.14 Predictions made by a GP conditioned on zero, one, two, and four
    observations
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.14 高斯过程对零、一个、两个和四个观测条件下的预测
- en: 'In each of the panels, the following is true:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个面板中，以下内容为真：
- en: The solid line in the middle is the mean function, which is analogous to the
    solid line connecting the diamonds in figure 2.11.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中间的实线是平均函数，类似于图2.11中连接菱形的实线。
- en: The shaded region, on the other hand, is the 95% CI across the domain, corresponding
    to the error bars in figure 2.11.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一方面，阴影区域是域上的95% CI，对应于图2.11中的误差条。
- en: The various wiggly lines are samples drawn from the corresponding GP.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各种曲线是从相应高斯过程中抽取的样本。
- en: Before observing any data, we start out with the *prior GP* in the top-left
    panel. Just like a prior MVN, our prior GP produces constant mean prediction and
    uncertainty in the absence of training data. This is a reasonable behavior to
    have.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在观察任何数据之前，我们从左上角的*先验高斯过程*开始。就像先验多元正态分布一样，在没有训练数据的情况下，我们的先验高斯过程产生恒定的均值预测和不确定性。这是一个合理的行为。
- en: The interesting part comes when we condition our GP on various data points.
    This is visualized in the remaining panels of figure 2.14\. Exactly like the discrete
    case of an MVN, with a GP working in a continuous domain, the mean prediction
    as well as samples drawn from the posterior distribution smoothly interpolate
    data points in the training set, while our uncertainty about the function value,
    quantified by the CI, smoothly decreases in the areas around these observations.
    This is what we call a *calibrated quantification of uncertainty*, which is one
    of the biggest selling points of a GP.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将高斯过程与各种数据点进行条件化时，有趣的部分就出现了。这在图2.14剩余的面板中进行了可视化。正如多元正态分布的离散情况一样，高斯过程在连续域中工作时，均值预测以及从后验分布中抽取的样本在训练集的数据点之间平滑插值，而关于函数值的不确定性，由置信区间(CI)量化，在这些观测点周围的区域平滑减少。这就是我们所说的*校准不确定性量化*，这是高斯过程的最大卖点之一。
- en: Smoothness of a GP
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯过程的平滑性
- en: The *smoothness* property refers to the constraint that requires similar points
    to be correlated with each other. In other words, points that are similar should
    result in similar function values. This is, again, why when we condition on the
    data point at 3 in the top-right panel, the mean predictions at 2.9 and 3.1 are
    updated to take on larger values than their prior means. These points, 2.9 and
    3.1, are similar to 3 because they are close to each other. This smoothness is
    set using the covariance function of the GP, which is the topic of chapter 3\.
    While the examples we have seen so far are in one dimension, this smoothness is
    preserved when our search space is higher-dimensional, as we see later.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Overall, we have seen that a GP is an MVN, distribution when extended to infinite
    dimensions, and thanks to many convenient mathematical properties of Gaussian
    distributions, a GP not only produces a mean prediction but also quantifies our
    uncertainty about the function values in a principled way via its predictive covariances.
    The mean prediction goes exactly through the training data points, and the uncertainty
    collapses at these data points.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Modeling non-Gaussian data
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: In real life, not all data follows Gaussian distributions. For example, for
    values that are limited within a numerical range or variables that don’t follow
    bell-shaped distributions, Gaussian distributions are inappropriate and might
    lead to low-quality predictions.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: In these cases, we can apply various data processing techniques to “convert”
    our data points to follow Gaussian distribution. For example, the Box–Muller transform
    is an algorithm that generates pairs of normally distributed random numbers from
    uniformly distributed random numbers. The interested reader can find more details
    about this algorithm on Wolfram’s MathWorld ([https://mathworld.wolfram.com/Box-MullerTransformation.xhtml](https://mathworld.wolfram.com/Box-MullerTransformation.xhtml)).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Implementing GPs in Python
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this final section of the chapter, we take our first step toward implementing
    GPs in Python. Our goal is to become familiar with the syntax and API of the libraries
    we will be using for this task and learn how to recreate the visualizations we
    have seen thus far. This hands-on section will also help us understand GPs more
    deeply.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: First, make sure you have downloaded the accompanying code for the book and
    installed the necessary libraries. Detailed instructions on how to do this are
    included in the front matter. We use the code included in the Jupyter notebook
    CH02/01 - Gaussian processes.ipynb.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.1 Setting up the training data
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before we start implementing the code for our GP model, let’s first spend some
    time creating an objective function we’d like to model and a training dataset.
    To do this, we need to import PyTorch for calculating and manipulating tensors
    and Matplotlib for data visualization:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Our objective function in this example is the one-dimensional Forrester function.
    The Forrester function is multimodal with one global maximum and one local maximum
    ([https://www.sfu.ca/~ssurjano/forretal08.xhtml](https://www.sfu.ca/~ssurjano/forretal08.xhtml)),
    making fitting and finding the maximum of the function a nontrivial task. The
    function has the following formula:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/02-14-Equations_ch-2.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
- en: 'This is implemented as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let us quickly plot this function in a graph. Here, we restrict ourselves to
    the domain between –3 and 3 and compute this Forrester function on a dense grid
    of 100 points in this range. We also need some sample points for training, which
    we generate by randomly sampling with `torch.rand()` and store in `train_x`; `train_y`
    contains the labels of these training points, which can be obtained by evaluating
    `forrester_1d(train_x)`. This plot is generated by the following code, which produces
    figure 2.15:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../../OEBPS/Images/02-15.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
- en: Figure 2.15 The objective function that is used in the current example, as shown
    by the solid line. The markers indicate points in the training dataset.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'The three markers we see are points that we randomly select to include in our
    training dataset. The locations of these training data points are stored in `train_x`,
    and their labels (the values of the Forrester function at these locations) are
    stored in `train_y`. This sets up our regression task: implementing and training
    a GP on these three data points and visualizing its predictions on the range between
    –3 and 3\. Here, we have also created `xs`, which is a dense grid over this range.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.2 Implementing a GP class
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this subsection, we learn how to implement a GP model in Python. We use the
    GPyTorch library, a state-of-the-art tool for modern GP modeling.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Important The design philosophy of GPyTorch is to follow the DL library PyTorch
    and have all of its model classes extend a base model class. If you are familiar
    with implementing neural networks in PyTorch, you might know that this base is
    `torch.nn.Module`. With GPyTorch, we typically extend the `gpytorch.models.ExactGP`
    class.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement our model class, we use the following structure:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here, we implement a class called `BaseGPModel`, which has two specific methods:
    `__init__()` and `forward()`. The behavior of our GP model heavily depends on
    how we write these two methods, and no matter what kind of GP model we’d like
    to implement, our model class needs to have these methods.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s discuss the `__init__()` method first. Its job is to take in the training
    dataset defined by the first and second arguments, `train_x` and `train_y`, as
    well as a likelihood function, stored in the variable `likelihood`, and initialize
    the GP model, which is a `BaseGPModel` object. We implement the method as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here, we simply pass the three input arguments to the `__init__()` method of
    our super class, and the built-in implementation of `gpytorch.models.ExactGP`
    takes care of the heavy lifting for us. What remains is the definition of the
    mean and the covariance functions, which, as we have said, are the two main components
    of a GP.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'In GPyTorch, there is a wide range of choices for both the mean and the covariance
    function, which we explore in chapter 3\. For now, we use the most common options
    for a GP:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '`gpytorch.means.ZeroMean()` for the mean function, which outputs zero mean
    predictions in prior mode'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gpytorch.kernels.RBFKernel()` for the covariance functions, which implements
    the *radial basis function* (RBF) kernel—one of the most commonly used covariance
    function for GPs, which implements the idea that data points close to each other
    are correlated to each other'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We store these objects in the `mean_module` and `covar_module` class attributes,
    respectively. That’s all we need to do for the `__init__()` method. Now, let’s
    turn our attention to the `forward()` method.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: The `forward()` method is very important as it defines how the model should
    process its input. If you have worked with neural networks in PyTorch, you know
    that the `forward()` method of a network class sequentially passes its input through
    the layers of the network, and the output of the final layer is what the neural
    network produces. In PyTorch, each layer is implemented as a *module*, which is
    a term to denote the basic building block of any object in PyTorch that processes
    data.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'The `forward()` method of a GP in GPyTorch works in a similar way: the GP’s
    mean and covariance functions are implemented as modules, and the input of the
    method is passed to these modules. Instead of sequentially passing the result
    through different modules, we pass the input to the mean and the covariance functions
    simultaneously. The output of these modules is then combined to create an MVN
    distribution. This difference between PyTorch and GPyTorch is illustrated in figure
    2.16.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/02-16.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
- en: Figure 2.16 How PyTorch and GPyTorch process data in their respective `forward()`
    methods. The input is processed by different modules to produce the final output,
    either a number for a feed-forward neural network or an MVN distribution for a
    GP.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'The `forward()` method is implemented in the following code:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The logic here is pretty straightforward: As we have a mean function and a
    covariance function, we simply call them on the input `x` to compute the mean
    and covariance predictions. Finally, what we need to return is an MVN distribution,
    implemented by the `gpytorch.distributions.MultivariateNormal` class, with corresponding
    mean and covariance. In other words, we are doing nothing more than creating an
    MVN distribution with a mean vector and a covariance matrix computed from the
    `mean_ module` and `covar_module` attributes of our model class.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: And that is all there is to it! It’s quite surprising how easy it is to implement
    a GP model with GPyTorch. The biggest takeaway for us is that we need to implement
    a mean and covariance function in the `__init__()` method. In the `forward()`
    method, when we need to make a prediction, we simply call these two functions
    on the input passed in.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！用 GPyTorch 实现 GP 模型是多么容易，令人惊讶。对我们来说最大的收获是，我们需要在 `__init__()` 方法中实现均值和协方差函数。在
    `forward()` 方法中，当我们需要进行预测时，我们只需在输入上调用这两个函数即可。
- en: 2.4.3 Making predictions with a GP
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.3 用 GP 进行预测
- en: 'With the `BaseGPModel` class in hand, we are ready to make predictions with
    a GP! Recall that in the `__init__()` method, we need to pass in a likelihood
    function, `likelihood`, in addition to our training data. In many regression tasks,
    a `gpytorch.likelihoods.GaussianLikelihood` object suffices. We create this object
    like so:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 有了 `BaseGPModel` 类，我们可以开始用 GP 进行预测了！回想一下，在 `__init__()` 方法中，我们需要传递一个似然函数 `likelihood`，以及我们的训练数据。在许多回归任务中，`gpytorch.likelihoods.GaussianLikelihood`
    对象就足够了。我们可以这样创建这个对象：
- en: '[PRE6]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now, we can initialize our `BaseGPModel` object. But before we initialize it
    with our three-entry training data, we would like to first make predictions with
    the prior GP.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以初始化我们的 `BaseGPModel` 对象了。但在我们将其初始化为我们的三项训练数据之前，我们首先想要用先验 GP 进行预测。
- en: '![](../../OEBPS/Images/02-16-unnumb-2.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/02-16-unnumb-2.png)'
- en: 'To initialize a GP object without any training data, we pass `None` as both
    the training features (`train_x`) and labels (`train_y`). So our prior GP is created
    as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 要初始化一个没有任何训练数据的 GP 对象，我们将 `None` 传递给训练特征 (`train_x`) 和标签 (`train_y`)。因此，我们的先验
    GP 被创建如下：
- en: '[PRE7]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Finally, some bookkeeping is necessary before we can make any predictions.
    First, we set the hyperparameters of the GP:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在我们进行任何预测之前，需要进行一些簿记工作。首先，我们设置 GP 的超参数：
- en: '[PRE8]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We discuss what each of these hyperparameters controls in chapter 3\. For now,
    we just use the values that I personally like to use as the default: 1 for the
    length scale and 0.0001 for the noise variance. The very last detail is to enable
    prediction mode in both the GP model and its likelihood by calling the `eval()`
    method from the corresponding objects.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第三章讨论每个超参数控制的内容。目前，我们只使用我个人喜欢的默认值：长度尺度为 1，噪声方差为 0.0001。最后一个细节是通过调用相应对象的
    `eval()` 方法在 GP 模型和其似然性中启用预测模式。
- en: 'With these bookkeeping tasks out of the way, we can now finally call this GP
    model on our test data to make predictions. We do this like so:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 处理完这些簿记任务后，我们现在终于可以在我们的测试数据上调用这个 GP 模型进行预测了。我们可以这样做：
- en: '[PRE9]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Remember, in the `forward()` method of the model class, we return the MVN distribution,
    so that is the output when we pass some test data through our model using `model(xs)`.
    (In the syntax of PyTorch, calling `model(xs)` is a shorthand for calling the
    `forward()` method on the test data `xs`.) We also pass that same output through
    the likelihood function `likelihood`, which incorporates the noise variance into
    our predictions. In short, what we store in `predictive_distribution` is an MVN
    distribution that represents our prediction for the test points `xs`. Moreover,
    we compute this object within a `torch.no_grad()` context, which is good practice
    when we don’t want PyTorch to keep track of the gradients of these computations.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在模型类的 `forward()` 方法中，我们返回 MVN 分布，因此当我们通过 `model(xs)` 使用我们的模型传递一些测试数据时，这就是输出。（在
    PyTorch 的语法中，调用 `model(xs)` 是对测试数据 `xs` 调用 `forward()` 方法的一种简写。）我们还将相同的输出通过似然函数
    `likelihood`，将噪声方差合并到我们的预测中。简而言之，我们在 `predictive_distribution` 中存储的是代表测试点 `xs`
    的预测的 MVN 分布。此外，我们在 `torch.no_grad()` 上下文中计算此对象，当我们不希望 PyTorch 跟踪这些计算的梯度时，这是一个好的做法。
- en: Note We only want to compute the gradients of operations when we’d like to optimize
    some parameters of our model using gradient descent. But when we want to make
    predictions, we should keep our model completely fixed, so disabling gradient
    checking is appropriate.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 我们只想在优化模型参数时计算操作的梯度。但是当我们想要进行预测时，我们应该完全固定我们的模型，因此禁用梯度检查是适当的。
- en: 2.4.4 Visualizing predictions of a GP
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.4 可视化 GP 的预测
- en: With this predictive Gaussian distribution in hand, we can now recreate the
    GP plots we have seen so far. Each of these plots consists of a mean function,
    *μ*, which we could obtain from the MVN with
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个预测的高斯分布，我们现在可以重新创建我们迄今为止见过的 GP 图。这些图中的每一个都包括一个均值函数 *μ*，我们可以从 MVN 中获取
- en: '[PRE10]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Additionally, we want to show the 95% CI. Mathematically, this can be done by
    extracting the diagonal elements of the predictive covariance matrix, Σ (remember
    that these elements denote the individual variances σ²), taking the square roots
    of these values to compute the standard deviations, σ, and computing the CI range
    of *μ* ± 1.96σ.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately for us, computing the 95% CI is a common operation when working
    with a GP, so GPyTorch offers a convenient helper method called `confidence_`
    `region()` that we may call directly from an MVN distribution object:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This method returns a tuple of two Torch tensors, which store the lower and
    upper endpoints of the CI, respectively.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we may want samples drawn for our plot from our current GP model.
    We can do this directly by calling the method `sample()` from the Gaussian object
    `predictive_distribution`. If we don’t pass in any input argument, the method
    will return a single sample. Here, we want to sample from our GP five times, which
    is done as follows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We pass in a `torch.Size()` object to denote that we want five samples to be
    returned. Setting the random seed before sampling is a good practice to ensure
    reproducibility of our code. And with that, we are ready to make some plots!
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing to do is simply plot the mean function:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'As for the 95% CI, we typically use a shaded region like what we have seen
    so far, which can be done using Matplotlib’s `fill_between()` function:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Finally, we plot the individual samples:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This code will produce the plot in figure 2.17.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/02-17.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
- en: Figure 2.17 Predictions made by a prior GP with a zero mean and RBF kernel.
    While the mean and CI are constant, individual samples exhibit complex, nonlinear
    behaviors.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: We see that across the domain, our prior GP produces a mean function that is
    constant at zero, and our 95% CI is constant. This is to be expected as we used
    a `gpytorch .means.ZeroMean()` object to implement the mean function, and without
    any training data, our prior predictions default to this 0 value.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'With that said, the mean and CI are only measurements of expectation: they
    denote the average behavior of our predictions across many, many different realizations
    of what could be. When we draw individual samples, however, we see that each of
    these samples has a very complex shape that is not at all constant. All of this
    is to say that while the expected value of our prediction at any point is zero,
    there is a wide range of values it could take. This demonstrates that a GP can
    model complex, nonlinear behaviors in a flexible manner.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we have learned to make and visualize predictions from a prior GP without
    any training data. Now, let’s actually train a GP model on the training set we
    randomly generated and see how the predictions change. The nice thing about what
    we have coded so far is that everything may be repeated exactly, except we now
    initialize our GP with our training data (remember that we used `None` for the
    first and second arguments before):'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学会了在没有任何训练数据的情况下生成和可视化预测的先前 GP 的过程。现在，让我们实际上在我们随机生成的训练集上训练一个 GP 模型，并观察预测结果的变化。迄今为止，我们编码的好处就是这一切可以完全重复进行，只不过现在我们要用我们的训练数据来初始化
    GP（请记住，之前我们在第一个和第二个参数中使用了 `None`）：
- en: '[PRE16]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Rerunning the code will give us figure 2.18.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 重新运行代码将给我们 Figure 2.18。
- en: '![](../../OEBPS/Images/02-18.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/02-18.png)'
- en: Figure 2.18 Predictions made by a posterior GP. The mean function and randomly
    drawn samples smoothly interpolate the training data points, while uncertainty
    vanishes in the regions surrounding these data points.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.18 由后验 GP 生成的预测。均值函数和随机绘制样本能很好地插值训练数据点，同时不确定性在这些数据点周围消失。
- en: 'This is exactly the type of prediction we’d like to see: the mean line and
    samples nicely interpolate our observed data points, and our uncertainty (measured
    by the CI) also reduces around those data points.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们想要看到的预测类型：均值线和样本完美地插值了我们观测到的数据点，并且我们的不确定性（由 CI 表示）在这些数据点周围也减小了。
- en: We can already see how this uncertainty quantification is useful in terms of
    modeling the objective function. After only observing three data points, our GP
    has obtained a rather good approximation of the true objective function. In fact,
    almost all of the objective lies inside the 95% CI, indicating our GP is successfully
    accounting for how the objective function might behave, even in regions where
    we don’t have any data from the function yet. This calibrated quantification is
    especially beneficial when we actually need to make decisions based on our GP
    model—that is, when we decide at which points we should observe the function value
    to find the optimum—but let’s save that for the next part of the book.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经可以看到这种不确定性量化在建模目标函数方面的作用。在仅观测到三个数据点之后，我们的 GP 对真实目标函数有一个相当好的近似。实际上，几乎所有的目标函数都在
    95% CI 内，表明我们的 GP 成功地考虑了目标函数的行为方式，即使在我们尚未从函数中获得任何数据的区域也是如此。这种校准的量化在我们实际需要根据 GP
    模型做出决策的情况下特别有益——也就是说，在我们决定在哪些点上观察函数值以找到最优值时——但是让我们将其保留到本书的下一部分。
- en: 2.4.5 Going beyond one-dimensional objective functions
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.5 超越一维目标函数
- en: So far, we have only seen examples of GPs trained on one-dimensional objective
    functions. However, there is nothing inherent about a GP that confines us to just
    one dimension. In fact, as long as our mean and covariance function can handle
    high-dimensional inputs, a GP can operate in high dimensions without difficulty.
    In this subsection, we learn how to train a GP on a two-dimensional dataset.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只看到了在一维目标函数上训练的 GP 的示例。然而，GP 并不局限于一个维度。实际上，只要我们的均值和协方差函数能够处理高维输入，GP
    就可以在高维上高效地运算。在本小节中，我们将学习如何训练一个基于二维数据集的 GP。
- en: We follow the procedure in the previous section. First, we need a training dataset.
    Here, I’m artificially creating a dummy set with points at (0, 0), (1, 2), and
    (–1, 1) with respective labels of 0, –1, and 0.5\. In other words, the objective
    function we’re learning from has a value of 0 at (0, 0), –1 at (1, 2), and 0.5
    at (–1, 1). We’d like to make predictions within the [–3, 3]-by-[–3, 3] square.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按照前一节的步骤进行。首先，我们需要一个训练数据集。在这里，我将自行创建一个带有点(0, 0)，(1, 2)，和(–1, 1)以及相应标签为0，–1和0.5的虚拟集合。换句话说，我们从中学习的目标函数在(0,
    0)处的值为0，在(1, 2)处的值为–1，在(–1, 1)处的值为0.5。我们希望在[–3, 3]-by-[–3, 3]的正方形内进行预测。
- en: 'This is set up in Python as follows:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中的设置如下：
- en: '[PRE17]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ One-dimensional grid
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 一维点阵
- en: ❷ Two-dimensional grid
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 二维点阵
- en: The variable `xs` is a 10,201-by-2 matrix that contains the lattices of a grid
    over the square we’d like to predict on.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 变量 `xs` 是一个 10,201×2 的矩阵，其中包含了我们希望进行预测的正方形上的点阵。
- en: Important There are 10,201 points because we are taking a 101-endpoint grid
    in each of the two dimensions. Now, we simply rerun the GP code we previously
    ran to train a GP and make predictions on this two-dimensional dataset. Note that
    no modification to our `BaseGPModel` class or any of the prediction code is needed,
    which is quite amazing!
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 'One thing we do need to change, though, is how we visualize our predictions.
    As we are operating in two dimensions, plotting the predictive mean and CI in
    a single plot becomes more difficult. Here, a typical solution is to draw a heat
    map for the predictive mean and another heat map for the predictive standard deviation.
    While the standard deviation is not exactly the 95% CI, these two objects, in
    essence, do quantify the same thing: our uncertainty about the function values.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'So, instead of calling `predictive_distribution.confidence_region()`, as we
    did before, we now extract the predictive standard deviation like so:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, to draw the heat maps, we use the `imshow()` function from Matplotlib.
    We need to be careful with the shape of our predictions in `predictive_mean` and
    `predictive_` `stddev` here. Each of them is a tensor of length 10,000, so it
    will need to be reshaped into a square matrix before being passed to the `imshow()`
    function. This can be done as follows:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ The first heat map for the predictive mean
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: ❷ The second heat map for the predictive standard deviation
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: This code produces the two heat maps in figure 2.19.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/02-19.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
- en: Figure 2.19 Predictions made by a two-dimensional GP. The mean function still
    agrees with the training data, and uncertainty once again vanishes in the regions
    surrounding these data points.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'We see that what we have in the one-dimensional case extends to this example
    too:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 'The left panel shows that our mean prediction agrees with our training data:
    the bright blob on the left corresponds to (–1, 1), which has a value of 0.5,
    while the dark blob on the right corresponds to (1, 2), which has a value of –1
    (our observation at (0, 0) has a value of 0, which is also the prior mean, so
    it is not as obvious to point out in the left panel as the other two).'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our uncertainty (measured by the predictive standard deviation) is close to
    zero around the three points in our training data, as demonstrated by the right
    panel. Going away from these data points, the standard deviation smoothly increases
    to the normalized maximum uncertainty of 1.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This means all of the nice properties of a GP, such as smooth interpolation
    and uncertainty quantification, are preserved when we go to higher dimensions.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: This marks the end of chapter 2\. We have gained a conceptual understanding
    of what a GP is and learned how to implement a base GP model in Python using GPyTorch.
    As mentioned, we dive deeper into the mean and covariance functions of a GP in
    chapter 3, including their hyperparameters, and see how each of these components
    controls the behavior of our GP model.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: 2.5 Exercise
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, we train a GP on a real-world dataset we saw in chapter 1,
    which is shown again in table 2.2\. Each data point (row) corresponds to an alloy
    (a kind of metal) created by mixing lead (Pb), tin (Sn), germanium (Ge), and manganese
    (Mn)—these are called the *parent compounds*—at different ratios. The features
    are contained in the first four columns, which are the percentages of the parent
    compounds. The prediction target, mixing temperature, is in the last column, denoting
    the lowest temperature at which an alloy can form. The task is to predict mixing
    temperature given the compositional percentages of an alloy.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Table 2.2 Data from a materials discovery task. The features are the structure
    of a material expressed in percentages of parent compounds, and the prediction
    target is the mixing temperature.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '| % of Pb | % of Sn | % of Ge | % of Mn | Mixing temp. (°F) |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
- en: '| 0.50 | 0.50 | 0.00 | 0.00 | 192.08 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
- en: '| 0.33 | 0.33 | 0.33 | 0.00 | 258.30 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
- en: '| 0.00 | 0.50 | 0.50 | 0.00 | 187.24 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
- en: '| 0.00 | 0.33 | 0.33 | 0.33 | 188.54 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
- en: 'There are multiple steps:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Create the four-dimensional dataset included in table 2.2.
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Normalize the fifth column by subtracting the mean from all values and dividing
    the results by their standard deviation.
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Treat the first four columns as features and the fifth as labels. Train a GP
    on this data. You can reuse the GP model class we implemented in the chapter.
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a test dataset containing compositions with zero percent germanium and
    manganese. In other words, the test set is a grid over the unit square whose axes
    are percentages of lead and tin.
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The test set should look like the following PyTorch tensor:'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Notice that the third and fourth columns are all zeros.
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Predict the mixing temperature on this test set. That is, compute the posterior
    mean and standard deviation of the normalized mixing temperature for every point
    in the test set.
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visualize the predictions. This involves showing the mean and standard deviation
    as heat maps in the same way as in figure 2.19\. The solution is included in CH02/02
    - Exercise.ipynb.
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A multivariate Gaussian (MVN) distribution models the joint distribution of
    many random variables. The mean vector denotes the expected values of the variables,
    while the covariance matrix models both the variances and the covariances among
    these variables.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By applying Bayes’ theorem, we can compute the posterior distribution of an
    MVN. Through this Bayesian update, variables that are similar to the observed
    variable are updated to reflect this similarity. Overall, similar variables produce
    similar predictions.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A GP extends an MVN distribution to infinite dimensions, making it a distribution
    over functions. However, the behavior of a GP is still similar to that of an MVN
    distribution.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even without any training data, a GP may still produce predictions specified
    by the prior GP.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once trained on a dataset, the mean prediction of a GP smoothly interpolates
    the training data points.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One of the biggest advantages of using a GP is the calibrated quantification
    of uncertainty the model offers: predictions around observed data points are more
    confident; predictions far away from training data, on the other hand, are more
    uncertain.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conditioning with an MVN distribution or a GP is visually similar to tying a
    knot at an observation. This forces the model to exactly go through the observation
    and reduces the uncertainty to zero.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When using GPyTorch to implement GPs, we can write a model class that extends
    the base class in a modular way. Specifically, we implement two specific methods:
    `__init__()`, which declares the mean and covariance functions of the GP, and
    `forward()`, which constructs an MVN distribution for a given input.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
