["```py\n>>> text = (\"Trust me, though, the words were on their way, and when \"\n...         \"they arrived, Liesel would hold them in her hands like \"\n...         \"the clouds, and she would wring them out, like the rain.\")\n>>> tokens = text.split()  # #1\n>>> tokens[:8]\n['Trust', 'me,', 'though,', 'the', 'words', 'were', 'on', 'their']\n```", "```py\n>>> import re\n>>> pattern = r'\\w+(?:\\'\\w+)?|[^\\w\\s]'  # #1\n>>> texts = [text]\n>>> texts.append(\"There's no such thing as survival of the fittest. \"\n...              \"Survival of the most adequate, maybe.\")\n>>> tokens = list(re.findall(pattern, texts[-1]))\n>>> tokens[:8]\n[\"There's\", 'no', 'such', 'thing', 'as', 'survival', 'of', 'the']\n>>> tokens[8:16]\n['fittest', '.', 'Survival', 'of', 'the', 'most', 'adequate', ',']\n>>> tokens[16:]\n['maybe', '.']\n```", "```py\n>>> import numpy as np\n>>> vocab = sorted(set(tokens))  # #1\n>>> '  '.join(vocab[:12])  # #2\n\", . Survival There's adequate as fittest maybe most no of such\"\n>>> num_tokens = len(tokens)\n>>> num_tokens\n18\n>>> vocab_size = len(vocab)\n>>> vocab_size\n15\n```", "```py\n>>> import spacy  # #1\n>>> spacy.cli.download('en_core_web_sm')  # #2\n>>> nlp = spacy.load('en_core_web_sm')  # #3\n>>> doc = nlp(texts[-1])\n>>> type(doc)\nspacy.tokens.doc.Doc\n\n>>> tokens = [tok.text for tok in doc]\n>>> tokens[:9]\n['There', \"'s\", 'no', 'such', 'thing', 'as', 'survival', 'of', 'the']\n\n>>> tokens[9:17]\n['fittest', '.', 'Survival', 'of', 'the', 'most', 'adequate', ',']\n```", "```py\n>>> from spacy import displacy\n>>> sentence = list(doc.sents)[0]  # #1\n>>> svg = displacy.render(sentence, style=\"dep\",\n...     jupyter=False)  # #2\n>>> open('sentence_diagram.svg', 'w').write(svg)  # #3\n>>> # displacy.serve(sentence, style=\"dep\") # #4\n>>> # !firefox 127.0.0.1:5000\n>>> displacy.render(sentence, style=\"dep\")  # #5\n```", "```py\n>>> import requests\n>>> text = requests.get('https://proai.org/nlpia2-ch2.adoc').text\n>>> f'{round(len(text) / 10_000)}0k'  # #1\n'60k'\n```", "```py\n>>> import spacy\n>>> nlp = spacy.load('en_core_web_sm')\n>>> %timeit nlp(text)  # #1\n4.67 s ± 45.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n>>> f'{round(len(text) / 10_000)}0k'\n'160k'\n>>> doc = nlp(text)\n>>> f'{round(len(list(doc)) / 10_000)}0k'\n'30k'\n>>> f'{round(len(doc) / 1_000 / 4.67)}kWPS'  # #2\n'7kWPS'\n```", "```py\n>>> nlp.pipe_names  # #1\n['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n>>> nlp = spacy.load('en_core_web_sm', disable=nlp.pipe_names)\n>>> %timeit nlp(text)\n199 ms ± 6.63 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n```", "```py\n>>> import nltk\n>>> nltk.download('punkt')\nTrue\n>>> from nltk.tokenize import word_tokenize\n>>> %timeit word_tokenize(text)\n156 ms ± 1.01 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n>>> tokens = word_tokenize(text)\n>>> f'{round(len(tokens) / 10_000)}0k'\n'10k'\n```", "```py\n>>> pattern = r'\\w+(?:\\'\\w+)?|[^\\w\\s]'\n>>> tokens = re.findall(pattern, text)  # #1\n>>> f'{round(len(tokens) / 10_000)}0k'\n'20k'\n>>> %timeit re.findall(pattern, text)\n8.77 ms ± 29.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n```", "```py\n>>> import pandas as pd\n>>> from sklearn.feature_extraction.text import CountVectorizer\n>>> vectorizer = CountVectorizer(ngram_range=(1, 2), analyzer='char')\n>>> vectorizer.fit(texts)\nCountVectorizer(analyzer='char', ngram_range=(1, 2))\n```", "```py\n>>> bpevocab_list = [\n...    sorted((i, s) for s, i in vectorizer.vocabulary_.items())]\n>>> bpevocab_dict = dict(bpevocab_list[0])\n>>> list(bpevocab_dict.values())[:7]\n['  ', ' a', ' c', ' f', ' h', ' i', ' l']\n```", "```py\n>>> vectors = vectorizer.transform(texts)\n>>> df = pd.DataFrame(\n...     vectors.todense(),\n...     columns=vectorizer.vocabulary_)\n>>> df.index = [t[:8] + '...' for t in texts]\n>>> df = df.T\n>>> df['total'] = df.T.sum()\n>>> df\n    Trust me...  There's ... total\n t           31      14          45\n r            3       2           5\n u            1       0           1\n s            0       1           1\n              3       0           3\n..           ...     ...        ...\nat            1       0           1\nma            2       1           3\nyb            1       0           1\nbe            1       0           1\ne.            0       1           1\n<BLANKLINE>\n[148 rows x 3 columns]\n```", "```py\n>>> df.sort_values('total').tail()\n        Trust me...  There's ... total\n    en        10           3       13\n    an        14           5       19\n    uc        11           9       20\n    e         18           8       26\n    t         31          14       45\n```", "```py\n>>> df['n'] = [len(tok) for tok in vectorizer.vocabulary_]\n>>> df[df['n'] > 1].sort_values('total').tail()\n    Trust me...  There's ... total n\nur           8          4            12  2\nen          10          3            13  2\nan          14          5            19  2\nuc          11          9            20  2\ne           18          8            26  2\n```", "```py\n>>> hi_text = 'Hiking home now'\n>>> hi_text.startswith('Hi')\nTrue\n>>> pattern = r'\\w+(?:\\'\\w+)?|[^\\w\\s]'  # #1\n>>> 'Hi' in re.findall(pattern, hi_text)  # #2\nFalse\n>>> 'Hi' == re.findall(pattern, hi_text)[0]  # #3\nFalse\n```", "```py\n>>> import pandas as pd\n>>> onehot_vectors = np.zeros(\n...     (len(tokens), vocab_size), int)  # #1\n>>> for i, tok in enumerate(tokens):\n...     if tok not in vocab:\n...         continue\n...     onehot_vectors[i, vocab.index(tok)] = 1  # #2\n>>> df_onehot = pd.DataFrame(onehot_vectors, columns=vocab)\n>>> df_onehot.shape\n(18, 15)\n>>> df_onehot.iloc[:,:8].replace(0, '')  # #3\n    ,  .  Survival  There's adequate as fittest maybe\n0                       1\n1\n2\n3\n4                                   1\n5\n6\n7\n8                                           1\n9      1\n10              1\n11\n12\n13\n14                               1\n15  1\n16                                                1\n17     1\n```", "```py\n>>> import spacy  # #1\n>>> from nlpia2.spacy_language_model import load  # #2\n>>> nlp = load('en_core_web_sm')  # #3\n>>> nlp\n<spacy.lang.en.English...>\n```", "```py\n[source,python]\n>>> doc = nlp(texts[-1])\n>>> type(doc)\n<class 'spacy.tokens.doc.Doc'>\n```", "```py\n>>> tokens = [tok.text for tok in doc]  # #1\n>>> tokens[:9]\n['There', \"'s\", 'no', 'such', 'thing', 'as', 'survival', 'of', 'the']\n>>> tokens[9:17]\n['fittest', '.', 'Survival', 'of', 'the', 'most', 'adequate', ',']\n```", "```py\n>>> from spacy import displacy\n>>> sentence = list(doc.sents)[0]  # #1\n>>> # displacy.serve(sentence, style=\"dep\") # #2\n>>> # !firefox 127.0.0.1:5000\n>>> displacy.render(sentence, style=\"dep\")\n```", "```py\n>>> import requests\n>>> text = requests.get('https://proai.org/nlpia2-ch2.adoc').text\n>>> f'{round(len(text) / 10_000)}0k'  # #1\n'170k'\n```", "```py\n>>> from nlpia2.spacy_language_model import load\n>>> nlp = load('en_core_web_sm')\n>>> %timeit nlp(text)  # #1\n4.67 s ± 45.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n>>> f'{round(len(text) / 10_000)}0k'\n'170k'\n>>> doc = nlp(text)\n>>> f'{round(len(list(doc)) / 10_000)}0k'\n'40k'\n>>> f'{round(len(doc) / 1_000 / 4.67)}kWPS'  # #2\n'8kWPS'\n```", "```py\n>>> nlp.pipe_names  # #1\n['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n>>> nlp = load('en_core_web_sm', disable=['tok2vec', 'tagger', 'parser'])\n>>> nlp.pipe_names\n['attribute_ruler', 'lemmatizer', 'ner']\n>>> %timeit nlp(text)\n199 ms ± 6.63 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n```", "```py\n>>> import nltk\n>>> nltk.download('punkt')\nTrue\n>>> from nltk.tokenize import word_tokenize\n>>> %timeit word_tokenize(text)\n156 ms ± 1.01 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n>>> tokens = word_tokenize(text)\n>>> f'{round(len(tokens) / 10_000)}0k'\n'30k'\n```", "```py\n>>> pattern = r'\\w+(?:\\'\\w+)?|[^\\w\\s]'\n>>> tokens = re.findall(pattern, text)  # #1\n>>> f'{round(len(tokens) / 10_000)}0k'\n'40k'\n>>> %timeit re.findall(pattern, text)\n8.77 ms ± 29.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n```", "```py\n>>> import pandas as pd\n>>> from sklearn.feature_extraction.text import CountVectorizer\n>>> vectorizer = CountVectorizer(ngram_range=(1, 2), analyzer='char')\n>>> vectorizer.fit(texts)\nCountVectorizer(analyzer='char', ngram_range=(1, 2))\n```", "```py\n>>> bpevocab_list = [\n...    sorted((i, s) for s, i in vectorizer.vocabulary_.items())]\n>>> bpevocab_dict = dict(bpevocab_list[0])\n>>> list(bpevocab_dict.values())[:7]\n['  ', ' a', ' c', ' f', ' h', ' i', ' l']\n```", "```py\n>>> vectors = vectorizer.transform(texts)\n>>> df = pd.DataFrame(\n...     vectors.todense(),\n...     columns=vectorizer.vocabulary_)\n>>> df.index = [t[:8] + '...' for t in texts]\n>>> df = df.T\n>>> df['total'] = df.T.sum()\n>>> df\n        Trust me...  There's ... total\n    t            31           14     45\n    r             3            2      5\n    u             1            0      1\n    s             0            1      1\n                  3            0      3\n    ..          ...          ...    ...\n    at            1            0      1\n    ma            2            1      3\n    yb            1            0      1\n    ...\n```", "```py\n>>> df.sort_values('total').tail(3)\n        Trust me...  There's ... total\n    uc           11            9     20\n    e            18            8     26\n    t            31           14     45\n```", "```py\n>>> df['n'] = [len(tok) for tok in vectorizer.vocabulary_]\n>>> df[df['n'] > 1].sort_values('total').tail()\n        Trust me...  There's ... total n\n     c            8            4     12  2\n    en           10            3     13  2\n    an           14            5     19  2\n    uc           11            9     20  2\n    e            18            8     26  2\n```", "```py\n>>> hi_text = 'Hiking home now'\n>>> hi_text.startswith('Hi')\nTrue\n>>> pattern = r'\\w+(?:\\'\\w+)?|[^\\w\\s]'  # #1\n>>> 'Hi' in re.findall(pattern, hi_text)  # #2\nFalse\n>>> 'Hi' == re.findall(pattern, hi_text)[0]  # #3\nFalse\n```", "```py\n>>> bow = sorted(set(re.findall(pattern, text)))\n>>> bow[:9]\n [',', '.', 'Liesel', 'Trust', 'and', 'arrived', 'clouds', 'hands', 'her']\n>>> bow[9:19]\n['hold', 'in', 'like', 'me', 'on', 'out', 'rain', 'she', 'the', 'their']\n>>> bow[19:27]\n['them', 'they', 'though', 'way', 'were', 'when', 'words', 'would']\n```", "```py\n>>> v1 = np.array([1, 2, 3])\n>>> v2 = np.array([2, 3, 4])\n>>> v1.dot(v2)\n20\n>>> (v1 * v2).sum()  # #1\n20\n>>> sum([x1 * x2 for x1, x2 in zip(v1, v2)])  # #2\n20\n```", "```py\n>>> from nltk.tokenize import TreebankWordTokenizer\n>>> texts.append(\n...   \"If conscience and empathy were impediments to the advancement of \"\n...   \"self-interest, then we would have evolved to be amoral sociopaths.\"\n...   )  # #1\n>>> tokenizer = TreebankWordTokenizer()\n>>> tokens = tokenizer.tokenize(texts[-1])[:6]\n>>> tokens[:8]\n['If', 'conscience', 'and', 'empathy', 'were', 'impediments', 'to', 'the']\n>>> tokens[8:16]\n['advancement', 'of', 'self-interest', ',', 'then', 'we', 'would', 'have']\n>>> tokens[16:]\n['evolved', 'to', 'be', 'amoral', 'sociopaths', '.']\n```", "```py\n>>> import spacy\n>>> nlp = spacy.load(\"en_core_web_sm\")\n>>> text = \"Nice guys finish first.\"  # #1\n>>> doc = nlp(text)\n>>> for token in doc:\n>>>     print(f\"{token.text:<11}{token.pos_:<10}{token.dep:<10}\")\nNice       ADJ       402\nguys       NOUN      429\nfinish     VERB      8206900633647566924\nfirst      ADV       400\n.          PUNCT     445\n```", "```py\n>>> import jieba\n>>> seg_list = jieba.cut(\"西安是一座举世闻名的文化古城\")  # #1\n>>> list(seg_list)\n['西安', '是', '一座', '举世闻名', '的', '文化', '古城']\n```", "```py\n>>> import jieba\n... seg_list = jieba.cut(\"西安是一座举世闻名的文化古城\", cut_all=True)  # #1\n>>> list(seg_list)\n['西安', '是', '一座', '举世', '举世闻名', '闻名', '的', '文化', '古城']\n```", "```py\n>>> seg_list = jieba.cut_for_search(\"西安是一座举世闻名的文化古城\")  # #1\n>>> list(seg_list)\n['西安', '是', '一座', '举世', '闻名', '举世闻名', '的', '文化', '古城']\n```", "```py\n>>> import jieba\n>>> from jieba import posseg\n>>> words = posseg.cut(\"西安是一座举世闻名的文化古城\")\n>>> jieba.enable_paddle()  # #1\n>>> words = posseg.cut(\"西安是一座举世闻名的文化古城\", use_paddle=True)\n>>> list(words)\n[pair('西安', 'ns'),\n pair('是', 'v'),\n pair('一座', 'm'),\n pair('举世闻名', 'i'),\n pair('的', 'uj'),\n pair('文化', 'n'),\n pair('古城', 'ns')]\n```", "```py\n>>> import spacy\n>>> spacy.cli.download(\"zh_core_web_sm\")  # #1\n>>> nlpzh = spacy.load(\"zh_core_web_sm\")\n>>> doc = nlpzh(\"西安是一座举世闻名的文化古城\")\n>>> [(tok.text, tok.pos_) for tok in doc]\n[('西安', 'PROPN'),\n ('是', 'VERB'),\n ('一', 'NUM'),\n ('座', 'NUM'),\n ('举世闻名', 'VERB'),\n ('的', 'PART'),\n ('文化', 'NOUN'),\n ('古城', 'NOUN')]\n```", "```py\n>>> from nltk.tokenize.casual import casual_tokenize\n>>> texts.append(\"@rickrau mind BLOOOOOOOOWWWWWN by latest lex :*) !!!!!!!!\")\n>>> casual_tokenize(texts[-1], reduce_len=True)\n['@rickrau', 'mind', 'BLOOOWWWN', 'by', 'latest', 'lex', ':*)', '!', '!', '!']\n```", "```py\n>>> import requests\n>>> url = (\"https://gitlab.com/tangibleai/nlpia/-/raw/master/\"\n...        \"src/nlpia/data/stopword_lists.json\")\n>>> response = requests.get(url)\n>>> stopwords = response.json()['exhaustive']  # #1\n>>> tokens = 'the words were just as I remembered them'.split()  # #2\n>>> tokens_without_stopwords = [x for x in tokens if x not in stopwords]\n>>> print(tokens_without_stopwords)\n['I', 'remembered']\n```", "```py\n>>> import nltk\n>>> nltk.download('stopwords')\n>>> stop_words = nltk.corpus.stopwords.words('english')\n>>> len(stop_words)\n179\n>>> stop_words[:7]\n['i', 'me', 'my', 'myself', 'we', 'our', 'ours']\n>>> [sw for sw in stopwords if len(sw) == 1]\n['i', 'a', 's', 't', 'd', 'm', 'o', 'y']\n```", "```py\n>>> resp = requests.get(url)\n>>> len(resp.json()['exhaustive'])\n667\n>>> len(resp.json()['sklearn'])\n318\n>>> len(resp.json()['spacy'])\n326\n>>> len(resp.json()['nltk'])\n179\n>>> len(resp.json()['reuters'])\n28\n```", "```py\n>>> tokens = ['House', 'Visitor', 'Center']\n>>> normalized_tokens = [x.lower() for x in tokens]\n>>> print(normalized_tokens)\n['house', 'visitor', 'center']\n```", "```py\n>>> def stem(phrase):\n...     return '  '.join([re.findall('^(.*ss|.*?)(s)?$',\n...         word)[0][0].strip(\"'\") for word in phrase.lower().split()])\n>>> stem('houses')\n'house'\n>>> stem(\"Doctor House's calls\")\n'doctor house call'\n```", "```py\n>>> from nltk.stem.porter import PorterStemmer\n>>> stemmer = PorterStemmer()\n>>> '  '.join([stemmer.stem(w).strip(\"'\") for w in\n...   \"dish washer's fairly washed dishes\".split()])\n'dish washer fairli wash dish'\n```", "```py\ndef step1a(self, word):\n    if word.endswith('sses'):\n        word = self.replace(word, 'sses', 'ss')  # #1\n    elif word.endswith('ies'):\n        word = self.replace(word, 'ies', 'i')\n    elif word.endswith('ss'):\n        word = self.replace(word, 'ss', 'ss')\n    elif word.endswith('s'):\n        word = self.replace(word, 's', '')\n    return word\n```", "```py\n>>> from nltk.stem.snowball import SnowballStemmer\n>>> stemmer = SnowballStemmer(language='english')\n>>> '  '.join([stemmer.stem(w).strip(\"'\") for w in\n...   \"dish washer's fairly washed dishes\".split()])\n'dish washer fair wash dish'\n```", "```py\n>>> nltk.download('wordnet')\nTrue\n>>> nltk.download('omw-1.4')\nTrue\n>>> from nltk.stem import WordNetLemmatizer\n>>> lemmatizer = WordNetLemmatizer()\n>>> lemmatizer.lemmatize(\"better\")  # #1\n'better'\n>>> lemmatizer.lemmatize(\"better\", pos=\"a\")  # #2\n'good'\n>>> lemmatizer.lemmatize(\"good\", pos=\"a\")\n'good'\n>>> lemmatizer.lemmatize(\"goods\", pos=\"a\")\n'goods'\n>>> lemmatizer.lemmatize(\"goods\", pos=\"n\")\n'good'\n>>> lemmatizer.lemmatize(\"goodness\", pos=\"n\")\n'goodness'\n>>> lemmatizer.lemmatize(\"best\", pos=\"a\")\n'best'\n```", "```py\n>>> stemmer.stem('goodness')\n'good'\n```", "```py\n>>> import spacy\n>>> nlp = spacy.load(\"en_core_web_sm\")\n>>> doc = nlp(\"better good goods goodness best\")\n>>> for token in doc:\n>>>     print(token.text, token.lemma_)\nbetter well\ngood good\ngoods good\ngoodness goodness\nbest good\n```", "```py\n>>> from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n>>> sa = SentimentIntensityAnalyzer()\n>>> sa.lexicon  # #1\n{ ...\n':(': -1.9,  # #2\n':)': 2.0,\n...\n'pls': 0.3,  # #3\n'plz': 0.3,\n...\n'great': 3.1,\n... }\n>>> [(tok, score) for tok, score in sa.lexicon.items()\n...   if \"  \" in tok]  # #4\n[(\"( '}{' )\", 1.6),\n (\"can't stand\", -2.0),\n ('fed up', -1.8),\n ('screwed up', -1.5)]\n>>> sa.polarity_scores(text=\\\n...   \"Python is very readable and it's great for NLP.\")\n{'compound': 0.6249, 'neg': 0.0, 'neu': 0.661,\n'pos': 0.339}  # #5\n>>> sa.polarity_scores(text=\\\n...   \"Python is not a bad choice for most applications.\")\n{'compound': 0.431, 'neg': 0.0, 'neu': 0.711,\n'pos': 0.289}  # #6\n```", "```py\n>>> corpus = [\"Absolutely perfect! Love it! :-) :-) :-)\",\n...           \"Horrible! Completely useless. :(\",\n...           \"It was OK. Some good and some bad things.\"]\n>>> for doc in corpus:\n...     scores = sa.polarity_scores(doc)\n...     print('{:+}: {}'.format(scores['compound'], doc))\n+0.9428: Absolutely perfect! Love it! :-) :-) :-)\n-0.8768: Horrible! Completely useless. :(\n-0.1531: It was OK. Some good and some bad things.\n```", "```py\n>>> movies = pd.read_csv('https://proai.org/movie-reviews.csv.gz',\n...     index_col=0)\n>>> movies.head().round(2)\n    sentiment                                               text\nid\n1        2.27  The Rock is destined to be the 21st Century's ...\n2        3.53  The gorgeously elaborate continuation of ''The...\n3       -0.60                     Effective but too tepid biopic\n4        1.47  If you sometimes like to go to the movies to h...\n5        1.73  Emerges as something rare, an issue movie that...\n\n>>> movies.describe().round(2)\n       sentiment\ncount   10605.00\nmean        0.00  # #1\nstd         1.92\nmin        -3.88  # #2\n...\nmax         3.94  # #3\n```", "```py\n>>> import pandas as pd\n>>> pd.options.display.width = 75  # #1\n>>> from nltk.tokenize import casual_tokenize  # #2\n>>> bows = []\n>>> from collections import Counter  # #3\n>>> for text in movies.text:\n...     bows.append(Counter(casual_tokenize(text)))\n>>> df_movies = pd.DataFrame.from_records(bows)  # #4\n>>> df_movies = df_movies.fillna(0).astype(int)  # #5\n>>> df_movies.shape  # #6\n(10605, 20756)\n\n>>> df_movies.head()\n   !  \" # $ % & ' ... zone zoning zzzzzzzzz ½ élan – ’\n0  0  0  0  0  0  0  4 ...     0       0          0  0     0  0  0\n1  0  0  0  0  0  0  4 ...     0       0          0  0     0  0  0\n2  0  0  0  0  0  0  0 ...     0       0          0  0     0  0  0\n3  0  0  0  0  0  0  0 ...     0       0          0  0     0  0  0\n4  0  0  0  0  0  0  0 ...     0       0          0  0     0  0  0\n\n>>> df_movies.head()[list(bows[0].keys())]\n   The  Rock  is  destined  to  be ...  Van  Damme  or  Steven  Segal  .\n0    1     1   1         1   2   1 ...    1      1   1       1      1  1\n1    2     0   1         0   0   0 ...    0      0   0       0      0  4\n2    0     0   0         0   0   0 ...    0      0   0       0      0  0\n3    0     0   1         0   4   0 ...    0      0   0       0      0  1\n4    0     0   0         0   0   0 ...    0      0   0       0      0  1\n\n[5 rows x 33 columns]\n```", "```py\n>>> from sklearn.naive_bayes import MultinomialNB\n>>> nb = MultinomialNB()\n>>> nb = nb.fit(df_movies, movies.sentiment > 0)  # #1\n>>> movies['pred_senti'] = (\n...   nb.predict_proba(df_movies))[:, 1] * 8 - 4  # #2\n>>> movies['error'] = movies.pred_senti - movies.sentiment\n>>> mae = movies['error'].abs().mean().round(1)  # #3\n>>> mae\n1.9\n```", "```py\n>>> movies['senti_ispos'] = (movies['sentiment'] > 0).astype(int)\n>>> movies['pred_ispos'] = (movies['pred_senti'] > 0).astype(int)\n>>> columns = [c for c in movies.columns if 'senti' in c or 'pred' in c]\n>>> movies[columns].head(8)\n    sentiment  pred_senti  senti_ispos  pred_ispos\nid\n1    2.266667    2.511515            1           1\n2    3.533333    3.999904            1           1\n3   -0.600000   -3.655976            0           0\n4    1.466667    1.940954            1           1\n5    1.733333    3.910373            1           1\n6    2.533333    3.995188            1           1\n7    2.466667    3.960466            1           1\n8    1.266667   -1.918701            1           0\n\n>>> (movies.pred_ispos ==\n...   movies.senti_ispos).sum() / len(movies)\n0.9344648750589345  # #1\n```", "```py\n>>> products = pd.read_csv('https://proai.org/product-reviews.csv.gz')\n>>> products.columns\nIndex(['id', 'sentiment', 'text'], dtype='object')\n>>> products.head()\n    id  sentiment                                               text\n0  1_1      -0.90  troubleshooting ad-2500 and ad-2600 no picture...\n1  1_2      -0.15  repost from january 13, 2004 with a better fit...\n2  1_3      -0.20  does your apex dvd player only play dvd audio ...\n3  1_4      -0.10  or does it play audio and video but scrolling ...\n4  1_5      -0.50  before you try to return the player or waste h...\n```", "```py\n>>> bows = []\n>>> for text in products['text']:\n...     bows.append(Counter(casual_tokenize(text)))\n>>> df_products = pd.DataFrame.from_records(bows)\n>>> df_products = df_products.fillna(0).astype(int)\n>>> df_products.shape # #1\n```", "```py\n>>> df_all_bows = pd.concat([df_movies, df_products])\n>>> df_all_bows.columns  # #1\nIndex(['!', '\"',\n       ...\n       'zoomed', 'zooming', 'zooms', 'zx', 'zzzzzzzzz', ...],\n      dtype='object', length=23302)\n```", "```py\n>>> vocab = list(df_movies.columns)  # #1\n>>> df_products = df_all_bows.iloc[len(movies):]  # #2\n>>> df_products = df_products[vocab]  # #3\n>>> df_products.shape\n(3546, 20756)\n>>> df_movies.shape  # #4\n(10605, 20756)\n```", "```py\n>>> products['senti_ispos'] = (products['sentiment'] > 0).astype(int)\n>>> products['pred_ispos'] = nb.predict(df_products).astype(int)\n>>> correct = (products['pred_ispos']\n...         == products['senti_ispos'])  # #1\n>>> correct.sum() / len(products)\n0.557...\n```"]