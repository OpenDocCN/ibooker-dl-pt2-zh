- en: 10 ALBERT, adapters, and multitask adaptation strategies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Applying embedding factorization and parameter sharing across layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning a model from the BERT family on multiple tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Splitting a transfer learning experiment into multiple steps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying adapters to a model from the BERT family
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the previous chapter, we began our coverage of some adaptation strategies
    for the deep NLP transfer learning modeling architectures that we have covered
    so far. In other words, given a pretrained architecture such as ELMo, BERT, or
    GPT, how can transfer learning be carried out more efficiently? We covered two
    critical ideas behind the method ULMFiT, namely the concepts of *discriminative
    fine-tuning* and *gradual unfreezing*.
  prefs: []
  type: TYPE_NORMAL
- en: The first adaptation strategy we will touch on in this chapter revolves around
    two ideas aimed at creating transformer-based language models that scale more
    favorably with a bigger vocabulary and longer input length. The first idea essentially
    involves clever factorization, or splitting up a larger matrix of weights into
    two smaller matrices, allowing you to increase the dimensions of one without affecting
    the dimensions of the other. The second idea involves sharing parameters across
    all layers. These two strategies are the bedrock of the method known as ALBERT,
    A Lite BERT.[¹](#pgfId-1258076) We use the implementation in the transformers
    library to get some hands-on experience with the method.
  prefs: []
  type: TYPE_NORMAL
- en: In chapter 4, we introduced the idea of multitask learning, where a model is
    trained to perform a variety of tasks at once. The resulting model is usually
    more generalizable to new scenarios and can result in better transfer. Unsurprisingly,
    this idea reappears in the context of adaptation strategies for pretrained NLP
    language models. When faced with a transfer scenario where there is insufficient
    training data to fine-tune on a given task, why not fine-tune on multiple tasks?
    Discussing this idea provides a great opportunity to introduce the (GLUE) dataset:[²](#pgfId-1258081)
    a collection of data for several tasks representative of human language reasoning.
    These tasks include detecting similarity between sentences, similarity between
    questions, paraphrasing, sentiment analysis, and question answering. We show how
    to quickly leverage the transformers library for multitask fine-tuning using this
    dataset. This exercise also demonstrates how to similarly fine-tune a model from
    the BERT family on a custom dataset from one of these important classes of problems.
  prefs: []
  type: TYPE_NORMAL
- en: In chapter 4, where we also discussed domain adaptation, we found that the similarity
    of the source and target domains plays a crucial role in the effectiveness of
    transfer learning. Greater similarity implies an easier transfer learning process
    in general. When the source and target are too dissimilar, you may find it impossible
    to carry out the process in a single step. In those circumstances, the idea of
    *sequential adaptation* may be used to break the overall desired transfer into
    simpler, more manageable steps. A language-based tool that fails to transfer between
    West and East Africa, for instance, may transfer successfully first between West
    Africa and Central Africa, and then between Central Africa and East Africa. In
    this chapter, we sequentially adapt “fill-in-the-blanks” objective pretrained
    BERT to a low-resource sentence similarity-detection scenario, by first adapting
    to a data-rich question similarity scenario.
  prefs: []
  type: TYPE_NORMAL
- en: The final adaptation strategy we will explore is the use of so-called *adaptation
    modules* or *adapters*. These are newly introduced modules of only a few parameters
    between layers of a pretrained neural network. Fine-tuning this modified model
    for new tasks requires training only these few additional parameters. The weights
    of the original network are kept the same. Virtually no loss in performance, compared
    to fine-tuning the entire model, is often observed when adding just 3-4% additional
    parameters per task.[³](#pgfId-1258089) These adapters are also modular and easily
    shared between researchers.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1 Embedding factorization and cross-layer parameter sharing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The adaptation strategies we discuss in this section revolve around two ideas
    aimed at creating transformer-based language models that scale more favorably
    with a bigger vocabulary and longer maximum input length. The first idea essentially
    involves clever factorization of a larger matrix of weights into two smaller matrices,
    allowing one to increase in dimension without affecting the dimensions of the
    other one. The second idea involves sharing parameters across all layers. These
    two strategies are the bedrock of the method known as ALBERT.[⁴](#pgfId-1258098)
    We again use the implementation in the transformers library to get some hands-on
    experience with the method. This serves both to give you a sense of what sorts
    of improvements are attained, as well as to arm you with the ability to use it
    in your own projects. We will be using the Amazon book reviews in the Multi-Domain
    Sentiment Dataset[⁵](#pgfId-1258102) from chapter 4 as our custom corpus for this
    experiment. This will allow you to gain further experience fine-tuning a pretrained
    transformer-based language model on a custom corpus, this time in English!
  prefs: []
  type: TYPE_NORMAL
- en: The first strategy, namely embedding factorization, is motivated by the observation
    that in BERT, the size of the input embedding is intrinsically linked to the dimension
    of its hidden layers. The tokenizer creates a one-hot encoded vector for each
    token—this vector is equal to 1 in the dimension corresponding to the token and
    0 otherwise. The dimension of this one-hot encoded vector is equal to the size
    of the vocabulary, *V*. The input embedding can be thought of as a matrix of dimension
    *V* by *E*, multiplying the one-hot encoded vector and projecting it into a dimension
    of size *E*. In earlier models such as BERT, this was equal to the hidden layer
    dimension *H*, so that this projection happened directly into the hidden layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'This means that when the size of the hidden layer increases, the dimension
    of the input embedding must increase as well, which can be very inefficient. On
    the other hand, the ALBERT authors observed that the role for the input embedding
    is to learn context-independent representations, whereas that of the hidden layers
    is to learn context-dependent representations—a harder problem. Motivated by this,
    they propose splitting the single-input embedding matrix into two matrices: one
    of dimension *V* by *E* and the other *E* by *H*, allowing *H* and *E* to be completely
    independent. Said differently, the one-hot encoded vectors can be first projected
    into an intermediate embedding of a smaller size and only then fed into the hidden
    layers. This allows the input embedding to have a significantly smaller size,
    even when the hidden layer dimensions are large or need to be scaled up. This
    design decision alone leads to an 80% reduction in the size of the matrix/matrices
    projecting the one-hot embedded vector into the hidden layers.'
  prefs: []
  type: TYPE_NORMAL
- en: The second strategy, cross-layer parameter sharing, is related to the *soft-parameter
    sharing multitask learning* scenario we discussed in chapter 4\. Corresponding
    weights across all layers are encouraged to be similar to each other by imposing
    appropriate constraints on them during learning. This serves as a regularization
    effect, reducing the risk of overfitting by reducing the number of degrees of
    available freedom. Taken together, the two techniques allowed the authors to build
    pretrained language models that outperformed both the GLUE and the SQuAD record
    performances at the time (February 2020). Compared to BERT, an approximately 90%
    reduction in parameter size was achieved with only a slight reduction in performance
    (less than 1% on SQuAD).
  prefs: []
  type: TYPE_NORMAL
- en: Again, because a variety of checkpoints have been made available for direct
    loading and our focus here is on transfer learning, we do not repeat the training
    steps from scratch here. Instead, we work with a checkpoint analogous to the “base”
    BERT checkpoint we used in the previous chapter and in chapter 8 for our cross-lingual
    transfer learning experiments. This allows us to directly compare the performance
    and benefits of using this architecture over the original BERT, while also teaching
    you how to start using this architecture in your own projects.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.1 Fine-tuning pretrained ALBERT on MDSD book reviews
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We prepare the data using the same steps as in section 4.4, which we will not
    repeat here. These are also repeated in the Kaggle notebooks made available with
    this book. We start with the variable `data` produced by listing 4.6\. Assuming
    the same hyperparameter settings as section 4.4, this is a NumPy array of 2,000
    book review texts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Write this NumPy array to file using Pandas with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We start by initializing an Albert tokenizer to the pretrained checkpoint from
    the base ALBERT model as follows. We are using version 2 because it is the latest
    available version at the moment. You can find the list of all available ALBERT
    models at any point on the Hugging Face website.[⁶](#pgfId-1258124)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Loads the ALBERT tokenizer
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Uses the pretrained ALBERT tokenizer
  prefs: []
  type: TYPE_NORMAL
- en: 'Having prepared the tokenizer, load the base ALBERT checkpoint into an ALBERT
    masked language model, and display the number of parameters as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Uses masked language modeling
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Initializes to the ALBERT checkpoint
  prefs: []
  type: TYPE_NORMAL
- en: The output indicates that the model has 11.8 million parameters—this is a huge
    reduction in size versus BERT’s 178.6 million parameters from chapter 8 and DistilBERT’s
    135.5 million parameters from the previous chapter. In fact, this is a reduction
    of 15 times from the BERT model. Wow!
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, as before, build a dataset with the tokenizer from the monolingual Twi
    text, using the convenient `LineByLineTextDataset` method included with transformers,
    shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: ❶ How many lines to read at a time
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a “data collator”—a helper method that creates a special object out
    of a batch of sample data lines (of length `block_size`)—as shown next. This special
    object is consummable by PyTorch for neural network training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Uses masked language modeling, and masks words with a probability of 0.15
  prefs: []
  type: TYPE_NORMAL
- en: Here we use masked language modeling with 15% of the words to be randomly masked
    in our input data, and the model is asked to predict them during training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define standard training arguments, such as output directory and training batch
    size, as shown in the next code snippet. Note that we are training for 10 epochs
    this time, because the dataset is so much smaller than the monolingual Twi sample
    of over 600,000 used in the previous chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, use training arguments with the previously defined dataset and collator
    to define a “trainer” for one training epoch across the data as shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Train and time how long training takes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Over this small dataset, the 10 epochs take only approximately five minutes
    to finish training. The loss reaches a value of around 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Save the model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let’s apply the pipelines API to predict the masked word in a fictional
    book review as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Defines the fill-in-the-blanks pipeline
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Predicts the masked token
  prefs: []
  type: TYPE_NORMAL
- en: 'This yields the following, very plausible, output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: You may have observed by now that the sequence of steps we executed to fine-tune
    ALBERT on the custom book review corpus here is very similar to the sequence of
    steps we used with DistilBERT in the previous chapter. That sequence of steps
    is, in turn, quite similar to the sequence of steps we used with mBERT in chapter
    8\. We stress yet again that this recipe can be used as a blueprint with virtually
    any other architecture available in transformers. Although it is impossible for
    us to provide an example of fine-tuning on every possible type of application,
    this recipe should generalize, or at least serve as a good starting point, for
    many use cases. Consider, for instance, the scenario where you wanted to teach
    GPT-2 to write in some chosen style. Simply copy over the same code we have used
    here, point the dataset paths to a corpus of your chosen writing style, and change
    the tokenizer and model references from `AlbertTokenizer` / `AlbertForMaskedLM`
    to `GPT2Tokenizer` / `GPT2LMHeadModel`.
  prefs: []
  type: TYPE_NORMAL
- en: 'One thing to note is that all the PyTorch transformers models have all the
    layers unfrozen for training by default. To freeze all layers, you can execute
    the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: You can freeze only some parameters using analogous code snippets.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, where we will discuss multitask fine-tuning, we will have
    yet another opportunity to look at fine-tuning of these types of models, this
    time for various tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2 Multitask fine-tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In section 3 of chapter 4, we introduced the idea of multitask learning, where
    a model is trained to perform a variety of tasks instead of just one. The resulting
    model is usually more generalizable to new scenarios and can result in better
    transfer and performance. Unsurprisingly, this idea reappears in the context of
    adaptation strategies for pretrained NLP language models, where models fine-tuned
    on multiple tasks have been observed to be more robust and performant.[⁷](#pgfId-1258231)
  prefs: []
  type: TYPE_NORMAL
- en: Our discussion of this idea here provides a great opportunity to introduce the
    *General Language Understanding Evaluation* (GLUE) dataset,[⁸](#pgfId-1258236)
    a collection of data for several tasks representative of human language reasoning.
    This dataset includes tasks such as detecting similarity between sentences, similarity
    between questions, paraphrasing, sentiment analysis, and question answering. In
    this section, we demonstrate how to quickly leverage the transformers library
    to fine-tune the various transformer-based pretrained models we have discussed
    on various tasks from the GLUE dataset. This exercise also demonstrates how to
    analogously fine-tune a model from the BERT family on a custom dataset from one
    of the important classes of problems included in GLUE.
  prefs: []
  type: TYPE_NORMAL
- en: We also demonstrate *sequential adaptation*—the process of breaking up an overall
    desired transfer experiment into simpler, more manageable steps. Consider a hypothetical
    scenario where a language-based tool fails to transfer between West and East Africa—it
    still may transfer successfully first between West Africa and Central Africa,
    and then between Central Africa and East Africa. This is related to the idea of
    multitask fine-tuning in the sense that it is essentially that carried out sequentially,
    one after the other. Instead of fine-tuning the model on several tasks simultaneously—which
    is how multitask fine-tuning is typically conceptualized—sequential adaptation
    fine-tunes on one task first, and then the other.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we demo multitask fine-tuning and sequential adaptation by
    fine-tuning some pretrained transformer-based language models on several tasks
    from the GLUE dataset. Specifically, we focus on a question similarity task known
    as the *Quora Question Pair* (QQP) task, as well as the *Semantic Textual Similarity
    Benchmark* (SST-B) task for measuring similarity between a pair of sentences.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.1 General Language Understanding Dataset (GLUE)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The General Language Understanding Dataset (GLUE) was introduced to provide
    a challenging set of benchmark datasets for a diverse set of natural language
    understanding tasks. These tasks were selected to represent the implicit agreement
    among researchers in NLP over the years about what constitutes an interesting,
    challenging, and relevant set of problems. In table 10.1, we summarize the tasks
    available in the dataset and data counts for each task.
  prefs: []
  type: TYPE_NORMAL
- en: Table 10.1 List of tasks, descriptions, and data counts for each task made available
    in the original General Language Understanding Dataset (GLUE)
  prefs: []
  type: TYPE_NORMAL
- en: '| Task Name | Data Count | Description |'
  prefs: []
  type: TYPE_TB
- en: '| The Corpus of Linguistic Acceptability (CoLA) | 8,500 train, 1,000 test |
    Determines whether or not an English sentence is grammatical |'
  prefs: []
  type: TYPE_TB
- en: '| The Stanford Sentiment Treebank (SST2) | 67,000 train, 1,800 test | Detects
    the sentiment of a given sentence—positive or negative |'
  prefs: []
  type: TYPE_TB
- en: '| Microsoft Research Paraphrase Corpus (MRPC) | 3,700 train, 1,700 test | Determines
    whether one sentence is a paraphrase of another |'
  prefs: []
  type: TYPE_TB
- en: '| Semantic Textual Similarity Benchmark (STS-B) | 7,000 train, 1,400 test |
    On a scale of 1 to 5, predicts the similarity score between a pair of sentences
    |'
  prefs: []
  type: TYPE_TB
- en: '| Quora Question Pairs (QQP) | 3,640,000 train, 391,000 test | Determines whether
    a pair of Quora questions are semantically equivalent |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-Genre Natural Language Inference (MultiNLI) | 393,000 train, 20,000
    test | Determines whether a premise sentence implies/entails or contradicts a
    hypothesis sentence |'
  prefs: []
  type: TYPE_TB
- en: '| Question-Answering Natural Language Inference (QNLI) | 105,000 train, 5,400
    test | Detects whether the context sentence contains an answer to the question
    |'
  prefs: []
  type: TYPE_TB
- en: '| Recognizing Textual Entailment (RTE) | 2,500 train, 3,000 test | Measures
    the textual entailment between the premise and the hypothesis, similarly to MultiNLI
    |'
  prefs: []
  type: TYPE_TB
- en: '| Winograd Schema Challenge (WNLI) | 634 train, 146 test | Determines to which
    noun from a set of possible options an ambiguous pronoun refers |'
  prefs: []
  type: TYPE_TB
- en: As can be seen from the table, the original GLUE dataset covered a variety of
    tasks with different amounts of data available. This is to encourage the sharing
    of knowledge between different tasks, which is the essence of the multitask fine-tuning
    idea we are exploring in this section of the chapter. We now briefly describe
    the various tasks in the table.
  prefs: []
  type: TYPE_NORMAL
- en: The first two tasks—the *Corpus of Linguistic Acceptability* (CoLA) and the
    *Stanford Sentiment Treebank* (SST2)—are single-sentence tasks. The former tries
    to determine if a given English sentence is grammatically correct, whereas the
    latter tries to detect whether the sentiment expressed in a sentence is positive
    or negative.
  prefs: []
  type: TYPE_NORMAL
- en: The following three tasks—*Microsoft Research Paraphrase Corpus* (MRPC), *Semantic
    Textual Similarity Benchmark* (STS-B), and *Quora Question Pairs* (QQP)—are classified
    as similarity tasks. These involve comparisons between two sentences in various
    ways. MRPC tries to detect if one sentence is a paraphrase of another, that is,
    if it expresses the same concepts. STS-B measures the similarity between a pair
    of sentences on a continuous scale between 1 and 5\. QQP tries to detect if one
    Quora question is equivalent to another.
  prefs: []
  type: TYPE_NORMAL
- en: The remaining four tasks are classified as inference tasks. The *Multi-Genre
    Natural Language Inference* (MultiNLI) task attempts to determine if a given sentence
    implies another sentence or contradicts it—it measures *entailment*. The *Question-Answering
    Natural Language Inference* (QNLI) task is similar to the SQuAD[⁹](#pgfId-1258329)
    dataset we discussed and used in chapter 8 to illustrate question answering. As
    a reminder, that dataset is composed of a context paragraph, a question about
    it, and the start and end positional indicators of an answer to the question in
    the context paragraph, if one exists. QNLI essentially turns this idea into a
    sentence-pair task by pairing each context sentence with the question and attempting
    to predict if the answer is in that context sentence. The *Recognizing Textual
    Entailment* (RTE) task is similar to MultiNLI in that it measures entailment between
    a pair of sentences. Finally, the *Winograd Schema Challenge* (WNLI) dataset attempts
    to detect to which noun from a set of available options an ambiguous pronoun in
    a sentence refers.
  prefs: []
  type: TYPE_NORMAL
- en: Since the inception of GLUE, another dataset called SuperGLUE[^(10)](#pgfId-1258335)
    has been introduced as well. This new version became necessary as modern methods
    recently began achieving close to perfect performances on many parts of GLUE.
    SuperGLUE was developed to be more challenging and thus to provide more “dynamic
    range” for comparing methods. We focus on GLUE here, but we do think it is important
    to keep the existence of SuperGLUE in mind as you become more of an expert on
    NLP.
  prefs: []
  type: TYPE_NORMAL
- en: We will do some experiments with QQP and STS-B GLUE tasks as illustrative examples
    in the rest of this section. To start off, in the next subsection we demonstrate
    how to fine-tune pretrained BERT on any one of the tasks we have presented. We
    underscore that while we use STS-B as the example fine-tuning task in this case,
    the same sequence of steps is directly applicable for any of the presented tasks.
    We also alert you that this exercise prepares you to fine-tune BERT on your own
    custom dataset from any of the task categories presented.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.2 Fine-tuning on a single GLUE task
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this subsection, we see how we can quickly fine-tune a pretrained model from
    the transformers family on a task from the GLUE benchmark set. Recall that BERT
    was pretrained on the “fill-in-the-blanks” and “next-sentence prediction” objectives.
    Here, we further fine-tune this pretrained BERT on the STS-B similarity task GLUE
    data. This exercise serves as an example of how you could carry this out on any
    other task in GLUE, as well as on any of your own custom datasets belonging to
    one of these important classes of problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing we do is clone the transformers repository and install the
    necessary requirements using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Clones the (specified version of) transformers repository
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Installs the necessary requirements
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Fixes the transformers version for reproducibility
  prefs: []
  type: TYPE_NORMAL
- en: Please ignore dependency conflict messages in our Kaggle notebook—they are irrelevant
    to the libraries we are using here, as long as you fork our notebook instead of
    starting a new one from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, download GLUE data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Downloads the GLUE data for all tasks
  prefs: []
  type: TYPE_NORMAL
- en: 'This creates a GLUE directory, with a subdirectory in it named after each GLUE
    task and containing the data for that task. We can take a look at what is contained
    in GLUE/STS-B as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This produced the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Moreover, we can take a peek at a slice of the STS-B training data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Before proceeding, we note that in order to use the scripts discussed here to
    fine-tune the model on your own custom data, you just need to convert your data
    into the format shown and point the scripts to its location!
  prefs: []
  type: TYPE_NORMAL
- en: 'To fine-tune the “vanilla” `bert-base-cased` BERT checkpoint on the STS-B GLUE
    task for three epochs—with a batch size of 32, a maximum input sequence length
    of 256, and a learning rate of 2e-5—we execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: ❶ This is a “magic” command for timing in a Jupyter notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'This takes under 10 minutes to execute. Note that in the code, we specified
    the output directory to be /tmp/STS-B/. This folder contains the fine-tuned model
    and evaluation results. Then, to see the performance that was attained, we simply
    execute the following to print the results to screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This yields the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: These represent the final figures for the metrics used on this problem, namely
    the Pearson and Spearman correlation coefficients. Without delving into too much
    detail, these coefficients measure the correlation between the ground truth similarities
    provided in the dataset and the similarities obtained by our fine-tuned model
    on the test set. Higher values for these coefficients indicate a better model
    due to greater correlation with the ground truth. We see that a performance approaching
    89% is attained for both coefficients. A quick look at the current GLUE leaderboard[^(11)](#pgfId-1258396)
    as of this writing (early October 2020) indicates that the top 20 performances
    recorded worldwide vary between approximately 87% at the low end and 93% at the
    high end. These top performances also perform well on the other tasks in GLUE,
    although we have only fine-tuned on a single task so far. It is nevertheless impressive
    that we can obtain a performance so close to the state of the art this quickly.
    Note from table 10.1 that the amount of training data for this task is only 7,000
    samples.
  prefs: []
  type: TYPE_NORMAL
- en: In the next subsection, we will fine-tune the model further on an additional
    task—Quora Question Pairs (QQP)—and thereby further illustrate the concepts of
    multitask learning and sequential adaptation.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.3 Sequential adaptation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this subsection, we will see if fine-tuning on the Quora Question Pairs
    (QQP) task, before fine-tuning on the STS-B task, can yield a better performance.
    Recall from table 10.1 that QQP has 364,000 training samples whereas STS-B has
    7,000 samples. Clearly, QQP has considerably more data. Training on QQP first
    can then be interpreted as applying a sequential adaptation multitask learning
    strategy to handle a low-resource scenario where the amount of training data is
    less than ideal: only 7,000 samples.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start the exercise assuming the transformers repository has been cloned,
    necessary requirements have been installed, and the GLUE data has been downloaded,
    as shown in the previous subsection. Now, the next thing to do is to fine-tune
    the “vanilla” `bert-base-cased` BERT checkpoint on the QQP GLUE task for one epoch,
    with a batch size of 32, a maximum input sequence length of 256, and a learning
    rate of 2e-5\. Note that we use just one epoch this time, instead of three as
    in the previous subsection, because the training data is now so much larger. Each
    epoch—which involves passing over the training set once—now covers 364,000 samples,
    which we gauge to be sufficient. We use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The training epoch takes about 2 hours and 40 minutes to execute. As before,
    we can check the attained performance on the QQP task as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This attains the following performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then load the QQP-fine-tuned model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Initializes to our fine-tuned model checkpoint
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Uses sequence classification this time, because it is the form of the problem
  prefs: []
  type: TYPE_NORMAL
- en: Having loaded the fine-tuned model, let’s extract its encoder so that we can
    use it in a successive model that can then be further fine-tuned on the STS-B
    task. Note that this is similar to the hard-parameter sharing scenario we analyzed
    in chapter 4\. We illustrate this scenario in figure 10.1.
  prefs: []
  type: TYPE_NORMAL
- en: '![10_01](../Images/10_01.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 The hard-parameter sharing multitask learning scenario we are exploring
    in this section. The model is first fine-tuned on QQP, which is a data-rich scenario,
    and then STS-B, which is a low-resource scenario. The sequential nature of this
    experiment classifies it as sequential adaptation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The encoder shared between tasks is clearly shown in the figure. The encoder
    is extracted and used to initialize a model for fine-tuning on STS-B using the
    following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Gets the fine-tuned QQP model encoder
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Makes sure the vocabulary and output sizes of an STS-B configuration are set
    to be consistent
  prefs: []
  type: TYPE_NORMAL
- en: ❸ STS-B is a regression problem and requires only one output; QQP is a binary
    classification task and thus has two outputs.
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Initializes the STS-B model with similar settings to QQP
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Set its encoder to the QQP encoder
  prefs: []
  type: TYPE_NORMAL
- en: 'Save the initialized STS-B model for further fine-tuning as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Make sure the vocabulary from the QQP model is available, as shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now fine-tune the previously QQP fine-tuned model on STS-B, using the same
    settings as in the previous subsection, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The three training epochs take only about seven and half minutes to execute,
    given that the training set size is only 7,000\. We check the attained performance
    as usual using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The following performance is observed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: We have attained an improvement over the previous subsection, where fine-tuning
    only on STS-B was carried out. The `eval_corr` attained there was about 88.9%,
    whereas we attain 89.3% here. The successive adaptation multitask learning experiment
    has thus been observed to be beneficial and to have resulted in a measurable improvement
    in performance.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will see if we can fine-tune similarly to new scenarios
    even more efficiently than we did here. We will investigate introducing so-called
    adaptation modules, or adapters, in between the layers of a pretrained language
    model to adapt to new scenarios. This approach holds promise because the number
    of introduced parameters is very small, and they can be pretrained and shared
    by the NLP community efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: 10.3 Adapters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next adaptation strategy we explore is the use of so-called *adaptation
    modules* or *adapters*. The key idea behind them is shown in figure 10.2, which
    introduces them as additional layers in the vanilla transformer encoder from figure
    7.6 in chapter 7.
  prefs: []
  type: TYPE_NORMAL
- en: '![10_02](../Images/10_02.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 Newly introduced adapter layers in the “vanilla” transformer encoder
    from figure 7.6
  prefs: []
  type: TYPE_NORMAL
- en: As can be seen in the figure, these adapters are newly introduced modules of
    only a few parameters between layers of a pretrained neural network. Fine-tuning
    the modified model for new tasks requires training only these few additional parameters—the
    weights of the original network are kept the same. Virtually no loss in performance,
    compared to fine-tuning the entire model, is often observed when adding just 3-4%
    additional parameters per task.[^(12)](#pgfId-1258480) In practice, this additional
    number of parameters is equivalent to the disk space of about 1 additional megabyte,
    which is very low by modern standards.
  prefs: []
  type: TYPE_NORMAL
- en: These adapters are modular, allowing for ready extendibility and easy sharing
    of experience among researchers. In fact, a project named AdapterHub,[^(13)](#pgfId-1258485)
    which is built on the transformers library we have been using, aims to be the
    central repository for sharing such modules. In this section, we will use this
    project to build a BERT model fine-tuned on the Stanford Sentiment Treebank (SST2)
    task. This is equivalent to what we did in the previous section when fine-tuning
    on the STS-B subset of GLUE and will allow you to quickly gain an appreciation
    for the advantages afforded by the adapter framework versus what we did before.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s install the AdapterHub library as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the required classes and load the required adapter with just three lines
    of code as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Checkpoint to fine-tune
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Task-specific adapter selection specification
  prefs: []
  type: TYPE_NORMAL
- en: Available adapters and usage instructions are listed on the AdapterHub website.[^(14)](#pgfId-1258498)
    That is literally all we had to do to adapt the BERT checkpoint to the SST2 sentiment-classification
    task. Comparing this with our fine-tuning steps from the previous section should
    make the utility of the adapter methodology obvious. Instead of fine-tuning, we
    just load additional modules and keep moving!
  prefs: []
  type: TYPE_NORMAL
- en: Note that in our code we used the `bert-base-uncased` checkpoint, and the adapter
    we are loading was fine-tuned on the UKP Sentential Argument Mining Corpus,[^(15)](#pgfId-1258504)
    due to the constraints of what is currently available in the AdapterHub repository.
    AdapterHub is an early-stage project, and we expect that significantly more adapters
    will be made available over time. At the time of writing in October of 2020, close
    to 200 adapters are available.[^(16)](#pgfId-1258508)
  prefs: []
  type: TYPE_NORMAL
- en: 'As a final action of this section and the chapter, let’s convince ourselves
    that the model we have built here is actually working as a sentiment-classification
    engine. We do this using the following code snippet, which compares the sentiment
    of two sentences: “That was an amazing contribution, good!” and “That is very
    bad for the environment.”'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Uses a regular pretrained tokenizer
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Sentence A
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Sentence B
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Makes prediction A
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Makes prediction B
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Displays the prediction probabilities for sentence A
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Displays the prediction probabilities for sentence B
  prefs: []
  type: TYPE_NORMAL
- en: 'This produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The shown predictions can be interpreted as a pair of probabilities, the first
    of which indicates the probability of the input being “negative” and the second,
    the probability of being “positive.” We see that the sentence “That was an amazing
    contribution, good!” is strongly positive with a probability of 99.9%. The sentence
    “That is very bad for the environment,” on the other hand, is negative, with a
    probability of 81.6%. That certainly makes sense and validates our experiment.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Applying embedding factorization and parameter sharing across layers yields
    a more parameter-efficient model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning a model from the BERT family on multiple tasks simultaneously, that
    is, multitask fine-tuning, yields a more generalizable model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Employing adapters on a model from the BERT family can simplify fine-tuning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '1. Z. Lan et al., “ALBERT: A Lite BERT for Self-Supervised Learning of Language
    Representations,” ICLR (2020).'
  prefs: []
  type: TYPE_NORMAL
- en: '2. A. Wang et al., “Glue: A Multi-Task Benchmark and Analysis Platform for
    Natural Language Understanding,” ICLR (2019).'
  prefs: []
  type: TYPE_NORMAL
- en: 3. N. Houlsby et al., “Parameter-Efficient Transfer Learning for NLP,” ICML
    (2019).
  prefs: []
  type: TYPE_NORMAL
- en: '4. Z. Lan et al., “ALBERT: A Lite BERT for Self-Supervised Learning of Language
    Representations,” ICLR (2020).'
  prefs: []
  type: TYPE_NORMAL
- en: 5. [https://www.cs.jhu.edu/~mdredze/datasets/sentiment/](https://www.cs.jhu.edu/~mdredze/datasets/sentiment/)
  prefs: []
  type: TYPE_NORMAL
- en: 6. [https://huggingface.co/models?filter=albert](https://huggingface.co/models?filter=albert)
  prefs: []
  type: TYPE_NORMAL
- en: 7. X. Liu et al., “Multi-Task Deep Neural Networks for Natural Language Understanding,”
    ACL Proceedings (2019).
  prefs: []
  type: TYPE_NORMAL
- en: '8. A. Wang et al., “GLUE: A Multi-Task Benchmark and Analysis Platform for
    Natural Language Understanding,” ICLR (2019).'
  prefs: []
  type: TYPE_NORMAL
- en: '9. P. Rajpurkar et al., “SQuAD: 100,000+ Questions for Machine Comprehension
    of Text,” arXiv (2016).'
  prefs: []
  type: TYPE_NORMAL
- en: '10. A. Wang et al., “Glue: A Multi-Task Benchmark and Analysis Platform for
    Natural Language Understanding,” ICLR (2019).'
  prefs: []
  type: TYPE_NORMAL
- en: 11. [https://gluebenchmark.com/leaderboard](https://gluebenchmark.com/leaderboard)
  prefs: []
  type: TYPE_NORMAL
- en: 12. N. Houlsby et al., “Parameter-Efficient Transfer Learning for NLP,” ICML
    (2019).
  prefs: []
  type: TYPE_NORMAL
- en: 13. [https://adapterhub.ml/](https://adapterhub.ml/)
  prefs: []
  type: TYPE_NORMAL
- en: 14. [https://adapterhub.ml/explore](https://adapterhub.ml/explore)
  prefs: []
  type: TYPE_NORMAL
- en: 15. [http://mng.bz/7j0e](http://mng.bz/7j0e)
  prefs: []
  type: TYPE_NORMAL
- en: 16. [https://adapterhub.ml/explore](https://adapterhub.ml/explore)
  prefs: []
  type: TYPE_NORMAL
