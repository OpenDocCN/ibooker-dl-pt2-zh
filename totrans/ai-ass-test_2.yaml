- en: 2 Large language models and prompt engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Outlining the fundamentals of how large language models work
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the risks of using large language models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining prompt engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experimenting with prompt engineering to return various outputs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving problems with prompt engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the previous chapter, we learnt that it’s important to take the time to familiarize
    ourselves with new tools, and it’s that mindset we’ll be adopting in this chapter.
    Throughout this book, we’ll be exploring how to specifically use generative AI
    tools such as Open AI’s ChatGPT and GitHub Copilot, which are built on large language
    models, or LLMs. There are many ways in which AI is used in testing, but what
    makes LLMs so interesting is their adaptability to different situations—hence,
    their rise in popularity. So before we look at how we can adopt LLM tools into
    our day-to-day testing, let’s first learn a bit about what LLMs are and how they
    work, and how to get the most benefit out of them, by learning the concept of
    prompt engineering.
  prefs: []
  type: TYPE_NORMAL
- en: 'What has made LLMs like ChatGPT dominate tech headlines throughout 2023? Consider
    this sample interaction with ChatGPT that I had:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on this quick “conversation,” we can see that:'
  prefs: []
  type: TYPE_NORMAL
- en: I can interact with ChatGPT with natural language. No traditional programming
    experience was required to get results from ChatGPT.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output from ChatGPT is also in natural language. It’s easy to understand
    and react to.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advocates of LLMs are celebrating that these types of AI tools have democratized
    the use of AI, allowing anyone to use them to get results. However, this democratization
    is a double-edged sword. The nature in which we interact with LLMs can give us
    the illusion that we’re talking with a machine that reasons in the same way we,
    as humans, do. But making that assumption can impact our ability to get the most
    out of an LLM. So to get the best results out of tools like ChatGPT, it helps
    to understand how they work (at least in layman’s terms) to better understand
    how they can fit into our testing activities and how to extract the most value
    from them.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Large language models, explained
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How does a person with relatively little experience in building AI systems explain
    how a complex LLM system works? Fortunately, in the Computerphile video “AI Language
    Models & Transformers” ([www.youtube.com/watch?v=rURRYI66E54](www.youtube.com.html)),
    Rob Miles offers an example that can help us gain a fundamental grasp on what
    LLMs do. (I strongly recommend watching all his videos on AI.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Take out your phone and open up a messaging app, or any other app that causes
    your keyboard to appear. Above the keyboard, you’ll likely see a selection of
    suggested words to insert into your message. For example, my keyboard offers me
    these suggestions: *I,* *I am,* and *The.* Selecting one of these options, such
    as *I am,* causes the suggestions to update. For me, it offered the options *away,*
    *away for,* and *now.* Selecting the option away *for* once again updates the
    options to select from. So how does the keyboard know which options to show and
    which ones not to?'
  prefs: []
  type: TYPE_NORMAL
- en: In your keyboard is an AI model that behaves in a similar manner to LLMs. This
    description is an oversimplification, but at its core, the keyboard on your phone
    is using the same machine learning approach as an LLM, by leveraging probability.
    Language is a complex and fluid set of rules, meaning any attempt to codify relationships
    explicitly is almost impossible. So instead, a model is trained on massive data
    sets to implicitly learn the relationships in language and to create a probability
    distribution that is used to predict what the next word might be. This can best
    be described by visualizing the options available from the keyboard example, as
    shown in Figure 2.1
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.1 Probability distribution in action
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/02__image001.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see when we select the term *I am,* our model within our keyboard
    has been trained to assign probabilities to a vast range of words. Some of these
    will have a high probability of coming after ‘I am’ such as ‘away’ and some will
    have a low probability, such as ‘sandalwood’. As mentioned before, these probabilities
    are coming from a model that has completed a training process, known as unsupervised
    learning, in which vast amounts of data have been sent to an algorithm to process.
    It’s from that training process that a model is created with complex weights and
    balances within it that give the model its predictive abilities.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning and unsupervised learning
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: When training AI, two of the more dominant techniques to use are supervised
    learning and unsupervised learning. Which learning approach is used will determine
    how data has been structured and sent to an algorithm. *Supervised* learning uses
    data that has been organized, labeled, and paired with an output. For example,
    a medical data set might contain labeled data that includes BMI, Age, and Sex,
    for example, which is paired with a labeled outcome about whether they suffered
    a specific illness—say, a heart attack or stroke. *Unsupervised* learning, on
    the other hand, uses data that isn’t labeled and has no output data. The idea
    is that when an algorithm is trained on this type of data, it learns the implicit
    patterns within the data.
  prefs: []
  type: TYPE_NORMAL
- en: Chances are good that if you play around with the predictive function on your
    keyboard, the output will differ from mine—even if we have the same phone and
    operating system. This is because once the model has been trained and is utilized
    within our phones, it’s still being fine-tuned by what we type into our phones.
    I travel for work, so I have to let people know when I am away and when I’m available.
    (It is perhaps a damning indictment of my work-life balance!) So terms such as
    ‘I am’ and ‘away’ have an increased probability as they are words I use more regularly.
    This is known as Reinforcement Learning with Human Feedback, or RLHF.
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, comparing predictive messaging on a phone to an LLM is an oversimplification,
    but the comparison holds true. LLMs also use non-supervised learning and RLHF.
    The difference, however, is that although an AI model on a phone can look at perhaps
    the last five words typed to predict the next, LLMs use cutting-edge techniques,
    such as these:'
  prefs: []
  type: TYPE_NORMAL
- en: '*G*enerative *p*retrained *t*ransformers (which is what makes the GPT abbreviation
    in *ChatGPT*)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Powerful hardware infrastructure using thousands of servers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training data on a scale that would dwarf what our humble keyboard model will
    have been trained on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do we need to know the intricacies of each of these points? Not really, but
    it helps us to appreciate a key aspect of LLMs. The output of LLMs, no matter
    how powerful, is probabilistic. LLMs are not a repository of information, there
    is structured knowledge stored within them like we would see on the wider internet.
    This means that how it comes to conclusions differs from how we humans come to
    conclusions (probability rather than experience), which is what makes them so
    powerful—but also risky to use if we aren’t vigilant about how we use them.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Avoiding the risks of using large language models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having an AI predict what word goes after another isn’t an easy task, and though
    current LLMs have seen an explosion in ability, there are risks we need to be
    conscious of. Let’s take a look at a few of them.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.1 Hallucinations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The challenge with text prediction is to ensure that the output of an LLM makes
    sense and is rooted in reality. For example, back in chapter 1, when I asked ChatGPT
    to write me an introduction to this book, it shared the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Initially, when LLMs were being developed, their output didn’t make much sense.
    The text would be readable, but it lacked structure or grammatical sense. If we
    read this example, it parses perfectly well, and it makes sense. However, as I
    mentioned, the book that ChatGPT describes doesn’t exist. This is known, in the
    context of an LLM, as a *hallucination.* The LLM is able to output a clear statement
    in a way that grants it some authority, but what has been written is false.
  prefs: []
  type: TYPE_NORMAL
- en: 'Why an LLM hallucinates is not entirely clear. One of the challenges of working
    with LLMs is that they act like a black box. It’s difficult to monitor how an
    LLM reached a specific conclusion, which is compounded by its indeterminate nature.
    Just because I got an output that contained a hallucination doesn’t mean that
    others will do the same in the future. (This is where Reinforced Learning with
    Human Feedback (RLHF) helps to combat hallucinations: we can inform the model
    whether its output is false and it will learn from that).'
  prefs: []
  type: TYPE_NORMAL
- en: The risk of hallucinations means we must always maintain an element of skepticism
    when interpreting the output of an LLM. We need to be mindful that what is being
    returned from an LLM is predictive and not always correct. We can’t turn off our
    critical thinking just because a tool appears to be behaving in a way that mimics
    how a human might.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.2 Data provenance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For most users of LLMs, it’s not just how the model precisely works that is
    a black box to us, but also the data it has been trained on. Since ChatGPT's explosion
    in popularity, the conversation around data ownership and copyright has intensified.
    Companies like X (formerly known as Twitter) and Reddit have accused OpenAI of
    stealing their data wholesale, and, at the time of writing, a class action lawsuit
    against OpenAI has been filed by a collection of authors who accuse the company
    of breaching copyright law by training models on their works ([www.theguardian.com/books/2023/jul/05/authors-file-a-lawsuit-against-openai-for-unlawfully-ingesting-their-books](05.html)).
  prefs: []
  type: TYPE_NORMAL
- en: The results from these debates are yet to be seen, but if we bring this topic
    back to the world of software development, we must be mindful of what material
    an LLM has been trained on. For example, ChatGPT at one point would return nonsensical
    responses when specific phrases were sent to it, all because it had been trained
    on data from the subreddit r/counting, which is full of data that is, on the surface,
    seemingly nonsensical itself. You can learn more from Computerphile about this
    weird behavior at [www.youtube.com/watch?v=WO2X3oZEJOA](www.youtube.com.html)).
    If an LLM has been trained on garbage, it will output garbage.
  prefs: []
  type: TYPE_NORMAL
- en: This becomes important when we consider tools such as GitHub Copilot, for example,
    uses the same GPT model that ChatGPT uses. Copilot has been fine-tuned differently,
    using the billions of lines of code stored in GitHub so that it can act as an
    assistant and suggest code snippets as we develop our codebase. We’ll explore
    in later chapters how we can put Copilot to good use, but again we should be critical
    of what it suggests and not blindly accept everything it offers as a suggestion.
    Why? Ask yourself, are you happy with the code you’ve created in the past? Do
    you trust all the code others have created? If a large population of engineers
    is prone to implementing bad patterns, then that is what tools like Copilot will
    have been trained on. The point is a little hyperbolic, because a lot of good
    developers and testers out there do good work—good work that Copilot is trained
    on. But it’s a thought exercise worth considering every now and then just to ensure
    that we remember who is in the driver’s seat when building applications with LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.3 Data privacy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Just as we need to be mindful of what an LLM outputs, we also have to consider
    what we enter into them. The temptation to share material with LLMs to find answers
    to problems we’re facing will be strong. But we have to ask ourselves, where is
    the data we send being stored? As mentioned earlier, LLMs are continuously being
    tweaked through RLFH feedback. Companies like OpenAI and GitHub will take the
    information we share, store it, and use it for future model training (GitHub does
    offer some privacy controls over what it can store though).
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be problematic when working for companies (or for ourselves) who want
    to keep their intellectual property private. Take Samsung, whose employees accidentally
    leaked confidential material through the use of ChatGPT, as described here by
    TechRadar:'
  prefs: []
  type: TYPE_NORMAL
- en: The company allowed engineers at its semiconductor arm to use the AI writer
    to help fix problems with their source code. But in doing so, the workers inputted
    confidential data, such as the source code itself for a new program and internal
    meeting notes data relating to their hardware.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: You can read about it at [www.techradar.com/news/samsung-workers-leaked-company-secrets-by-using-chatgpt](news.html).
  prefs: []
  type: TYPE_NORMAL
- en: As the adoption of LLMs begins to increase across organizations, we may begin
    to see an increase in policies that restrict what we can and can’t use LLMs for.
    Some may ban the use of third-party LLMs and some organisations will opt to train
    and deploy their own internal LLMs for internal use (a topic we will explore in
    part 3 of this book). The result of those decisions will be highly contextual
    but they will impact what type of LLMs we use and what data we can and cannot
    send, underlying our need to be mindful of what we send to LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: It’s also important to keep customer privacy in mind, as we have an obligation
    to not only the companies we work for (especially for those who sign Non-disclosure
    agreements) but also our users. We have a legal and moral duty to protect user
    data from being spread into the wild, where we have no oversight.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, though LLMs provide a wealth of opportunities, we must avoid
    the trap of anthropomorphizing them. Treating LLMs as if they have come to conclusions
    in the same way as we as humans do is a fallacy. It can entrench a level of trust
    in the output that is dangerous and likely mean that we aren’t getting the most
    benefit out of them. However, if we learn to leverage the probabilistic nature
    of LLMs when we instruct them, we can increase our chances of creating outputs
    that can help us improve efficiency—which is where prompt engineering can help
    us.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Improving results with prompt engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are using natural language to prompt an LLM to return a desired outcome,
    but because they are probabilistic, we can communicate with them in a way that
    differs from normal interaction with humans. As LLMs have developed, a new field
    of engineering has appeared, known as *prompt engineering,* which contains a collection
    of patterns and techniques that we can use to increase the likelihood that we
    get a desired output from an LLM.
  prefs: []
  type: TYPE_NORMAL
- en: What is a prompt?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In this book, we’ll use the term *prompt* regularly, as this will be our primary
    means of communicating with LLMs. When we use the term prompt we are simply referring
    to the natural language input that is sent to an LLM. For example, in the first
    example of this chapter I sent the following prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: which prompted the LLM to return a response to me.
  prefs: []
  type: TYPE_NORMAL
- en: In the coming chapters, we will use prompt engineering heavily to trigger LLMs
    to deliver a range of useful content for various testing activities. But before
    we begin, it’s worthwhile learning the fundamentals of prompt engineering so that
    we can see how prompts are built to maximize output from LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'To help us better understand what prompt engineering is, consider these two
    prompts sent to an LLM. The first is a general question:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The second is a more detailed prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Comparing the two, we can see that the second example is more detailed, with
    explicit requests and examples to outline what we might expect the LLM to return.
    Although the intention is similar, the output from each is drastically different.
  prefs: []
  type: TYPE_NORMAL
- en: 'Compare the output from the first example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'against the second example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Imagine that we want to use the proposed risks in each example to guide our
    testing. The first example has outputted suggestions that are vague and abstract.
    We would still need to do a fair bit of work to break down the large topics, such
    as security risks, whereas with the second example, we have specific, actionable
    risks that we could use easily. And the goal of using tools like LLM is to *reduce*
    the workload, not increase it.
  prefs: []
  type: TYPE_NORMAL
- en: Our second prompt yields better results because the instructions it gives have
    been considered, and are detailed and clear, which is what prompt engineering,
    at its core, is about. Though both prompts are using natural language, with prompt
    engineering we are aware of how an LLM works and what we want it to return carefully
    to inform how we write a prompt so that we maximize the chances of a desired outcome.
    When using prompt engineering, we appreciate that although an LLM communicates
    in plain language, how it processes our request differs from how a human might
    do so, which means we can adopt specific techniques to steer an LLM in the direction
    we want.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Examining the principles of prompt engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As LLMs have developed, so have the patterns and techniques of prompt engineering.
    Many courses and blog posts have been written around prompt engineering, but one
    notable collection of principles, which we’ll explore shortly, has been created
    by Isa Fulford and Andrew Ng and their respective teams. A collaboration from
    OpenAI’s LLM knowledge and Deeplearning.ai’s teaching platform has created a course
    called ChatGPT Prompt Engineering for Developers, which features a series of principles
    and tactics that can be used in prompts to get the most out of LLMs. If you have
    the time, I encourage you to take the short course found at [https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers](chatgpt-prompt-engineering-for-developers.html).
    ([https://www.promptingguide.ai/](www.promptingguide.ai.html) is also a useful
    reference.) Though the course references ChatGPT specifically, the principles
    taught there can be applied across many LLMs. So let’s explore these principles
    and tactics to get comfortable with prompting LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '2.4.1 Principle 1: Write clear and specific instructions'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This first principle might seem obvious at first glance—it’s always sensible
    to provide instructions to others that are clear and specific. But what this principle
    is actually suggesting is that we want to write prompts that are clear and specific
    *for an LLM.* And that will mean something different from what might be clear
    and specific to a human. To achieve this concept, Fulford and Ng teach four tactics
    to achieve clear and specific prompts: use delimiters, ask for structured output,
    check for assumptions, and few-shot prompting. In the next few sections, let’s
    examine each one in more detail.'
  prefs: []
  type: TYPE_NORMAL
- en: '2.4.2 Tactic 1: Use delimiters'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When writing prompts, we may need to provide different content and data that
    serve different purposes. For example, the start of our prompt might include instructions
    on what we want an LLM to produce, whereas the end of our prompt might include
    raw data that we want to process. LLMs are capable of guessing our intentions
    for different sections of our prompts, but because our goal is to be as clear
    as possible, we can aid the process by using *delimiters,* which are characters
    used to separate strings, to state our intentions for different parts of our prompt.
    Take this prompt as an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'When running this prompt within ChatGPT, I received the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As we can see through the use of delimiters, the correct table name `rooms`
    has been added and the column names, formats and ranges have been correctly randomized.
    This is made possible by the clear expectations and rules we set that are distinguished
    by various delimiters throughout the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 'The delimiters help to make the prompt clearer, but they also make it easy
    to modify. For example, if we want to reconfigure the generated data, we could
    enter another line such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: So, when writing prompts that contain a large amount of data that changes context,
    we can use delimiters to make clear what is being provided in a prompt at a specific
    point.
  prefs: []
  type: TYPE_NORMAL
- en: Activity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Using the delimiter prompt example, create new instructions—this time, for a
    booking that would include information about who made the booking, contact details,
    and check-in and check-out dates.
  prefs: []
  type: TYPE_NORMAL
- en: '2.4.3 Tactic 2: Ask for structured output'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One facet of LLMs that makes them useful is that they have the ability to provide
    outputs in structured formats. As we make our way through this book, we’ll explore
    just how useful this ability is, but as a rule, we must always remember that we
    need to be clear in a prompt about what structured format we want to see used.
    Take this prompt as an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We begin the prompt by clearly stating what format we want to see our object
    in, in this case, JSON, before we then start outlining the structure of the object.
    When I sent this prompt to ChatGPT, the following result was returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'As the prompt clearly states, the expected format and structure are two distinct
    instructions, meaning we can modify our instructions to quickly change the format
    structure by sending an additional prompt, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Sending this prompt to ChatGPT returned the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Notice the structure is the same (as well as the randomized data). By explicitly
    stating what format we desire, we can instruct an LLM with exactly what format
    we want at a given time and then alternate formats with ease.
  prefs: []
  type: TYPE_NORMAL
- en: Activity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Attempt to create an object that contains multiple parameters with different
    data types in different formats. Try prompting an LLM to convert your object from
    one format to another—for example, from JSON to XML.
  prefs: []
  type: TYPE_NORMAL
- en: '2.4.4 Tactic 3: Check for assumptions'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we learned earlier, LLMs can hallucinate, by generating output that contains
    information that is incorrect or disconnected from reality. It goes without saying
    that we want to reduce the risk of producing hallucinations, which is where our
    third tactic, checking for assumptions, can help. LLMs are more likely to hallucinate
    if they are provided with prompts that are focused on edge cases around a problem
    we want to solve. If an LLM is not properly instructed, it’s more likely to make
    a guess at an answer than to outright inform us that it cannot provide a useful
    answer. So, if we want to avoid guesswork, we need to provide instructions in
    our prompt to allow the LLM to bail out if it cannot execute our request. Consider
    this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this prompt with a collection of email addresses that can be extracted
    returned the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'But here’s what happens when I ran the prompt again without email addresses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Then I received the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This response from the LLM was a direct reference to this section of the prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'which prevented the LLM from providing an output that is incorrect. For example,
    when I ran the prompt without the assumption check, the following was returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This is an excellent example of an LLM hallucination. The object we provided
    contained no email addresses, so the LLM used guesswork and incorrectly started
    generating new email addresses based on existing data. However, with the assumption
    check in place, we prevented the hallucination from occurring in the first place.
  prefs: []
  type: TYPE_NORMAL
- en: Activity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Create a prompt that filters specific data out of a list of raw data and then
    outputs the results (For example, filter fruit from a list of different foodstuffs.)
    Then modify the data to include an edge case (for example, incorrect or missing
    data). Observe what the LLM outputs, and then attempt to correct the output by
    adding to the prompt some instructions that follow the check-for-assumptions prompt.
  prefs: []
  type: TYPE_NORMAL
- en: '2.4.5 Tactic 4: Few-shot prompting'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we’ve progressed through this chapter and learned new concepts about LLMs
    and prompting, key points have been clarified through the use of examples. They
    are essential tools when it comes to not just teaching but also communicating
    with one another. This is no different for LLMs. *Few-shot prompting* basically
    means providing explicit examples to clarify instructions (The word *few* in this
    context indicates how many examples you share. A prompt with no examples would
    be a *zero-shot prompt*). Take this example of a prompt that uses examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Sending this prompt to ChatGPT returned the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Consider the format of `Explore <Target> using <Resource> to discover <Information>.`
    It is rule-based, but it leans heavily on natural language to communicate the
    rules (unlike JSON or XML structures that rely on explicit delimiter rules). By
    providing examples in our prompt, we can help contextualize what we mean by `<Target>`,
    `<Resource>` and `<Information>` and reduce the risk of the LLM guessing what
    that means.
  prefs: []
  type: TYPE_NORMAL
- en: Activity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Create a prompt that uses the few-shot tactic. In your prompt, provide the instructions
    you expect the LLM to follow and then add at least two examples to help guide
    it in providing a desired outcome.
  prefs: []
  type: TYPE_NORMAL
- en: '2.4.6 Principle 2: Give the model time to “think”'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It may seem a little unusual, given that we’ve been considering LLMs as probabilistic
    machines and not entities that can think, to see a principle that encourages us
    to give a model time to “think.” Surely the value of using LLMs is that they can
    respond with answers to complex questions much faster than we humans can?
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the focus of principle 2 isn’t on how an LLM thinks, but rather on
    how we present complex tasks to a probabilistic engine. In the ChatGPT Prompt
    Engineering for Developers course mentioned a little earlier, Fulford gives a
    useful analogy as she introduces this principle:'
  prefs: []
  type: TYPE_NORMAL
- en: “If you give a model a task that’s too complex to do in a short amount of time
    . . . it may make up a guess which is likely to be incorrect.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: She explains that the same situation would happen if humans were given a complex
    task with limited time to respond. We’d rely on educated guesswork and likely
    come up with a less than satisfactory answer. So principle 2 offers tactics to
    help us write prompts that break down tasks and encourage LLMs to evaluate output
    to once again maximize the chances of a desired response.
  prefs: []
  type: TYPE_NORMAL
- en: '2.4.7 Tactic 1: Specify the steps to complete the task'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first tactic is relatively straightforward once we are confident about
    using the tactics of principle 1 (Write clear and specific instructions.) By using
    delimiters, we can break a complex task into individual steps for an LLM to take
    to solve the larger task. Take a look at this prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'When sent to ChatGPT, it returned this result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: This is a snippet from the output I received. It created many more risks and
    charters, but the first example from each section demonstrates the LLM responding
    to each subtask, one by one, to create an output.
  prefs: []
  type: TYPE_NORMAL
- en: Activity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Create a prompt that requires a complex task to be carried out. Attempt to break
    out the complex task into multiple subtasks that the LLM can carry out.
  prefs: []
  type: TYPE_NORMAL
- en: '2.4.8 Tactic 2: Instruct the model to work out its own solution first'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our final tactic focuses not on the process of creating an output, but rather
    on evaluating the output itself. Similar to checking for assumptions, ask an LLM
    to evaluate the output to confirm that it aligns with what it has been instructed
    to produce. Let’s look at an example to gain a better understanding of how this
    would work. First, let’s look at a prompt that doesn’t ask an LLM to work out
    its solution first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Sending this to ChatGPT, the following result was returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This looks like reasonable Java code for a unit test, but if this were added
    to a suite of unit checks, it would fail. It would fail because the method `authDB.deleteToken`
    in the production code provided has not been handled correctly. Specifically,
    if we wanted this unit check to work, we would need to mock `authDB.deleteToken`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if we run the prompt again but this time have it evaluate its solution
    before outputting a final answer, we get a different result. So first we change
    the prompt to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Sending this to ChatGPT returned this result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This time, we can see that because we asked the LLM to evaluate its solution
    before returning a result, the outputted unit check uses `Mockito` to mock the
    `authDB.deleteToken`. So, if we observe issues with LLMs outputting erroneous
    solutions or they start hallucinating, we can add an instruction to evaluate solutions
    first to minimize the occurrence of hallucinations.
  prefs: []
  type: TYPE_NORMAL
- en: Activity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Create a prompt that requires an LLM to work out a solution to a problem. Observe
    its output and see if the solution it produces is correct. Then add instructions
    to have the LLM evaluate the solution. What happens? Does the solution change?
    Is it an improvement?
  prefs: []
  type: TYPE_NORMAL
- en: 2.5 Working with different LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we’ve spoken about LLMs in a broad sense while using OpenAI’s ChatGPT
    in the previous examples to demonstrate how they work in general. However, ChatGPT
    is just one of the many different LLMs that are available to us to use. So, before
    we conclude the chapter, let’s familiarize ourselves with the ways in which LLMs
    are different from one another and learn about some of the currently popular models
    and communities so that we can increase our chances of finding the right LLM for
    the job.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.1 Comparing large language models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'What makes an LLM good? How do we determine whether a model is worth using?
    These are not easy questions to answer. The complex nature of LLMs and how they’re
    trained and what data was used close these systems off to deep analysis, and comprise
    an area that some researchers are trying to improve or shed light upon. However,
    that doesn’t mean we shouldn’t educate ourselves on some of the key aspects of
    LLMs and how they impact them. We might not all be AI researchers attempting to
    explore the deep inner workings of LLMs, but we are, or will be, users of them
    and we will want to know that what we spending resources on is giving us value.
    So to help us break down some of the jargon and give us some grounding on how
    LLMs differ from one another, let’s go through some key attributes that are discussed
    in the world of LLMs:'
  prefs: []
  type: TYPE_NORMAL
- en: Parameter count
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Take a look at different LLMs and you’ll likely see talk of LLMs having a “175
    Billion” or “1 Trillion” parameter count. It sometimes can feel like marketing
    speak, but parameter count does have an impact on how an LLM performs. The parameter
    count essentially relates to the amount of statistical weights that exist within
    a model. Each individual weight provides a piece of the statistical puzzle that
    makes up an LLM. So, roughly speaking, the more parameters an LLM has, the better
    it will perform. The parameter count also can give us a sense of cost. The higher
    the parameter count, the more expensive it is to run, a cost that may be, in part,
    handed down to users.
  prefs: []
  type: TYPE_NORMAL
- en: Training Data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: LLMs require huge quantities of data to be trained on, so the size of data and
    quality of data it has been trained on will have an impact on the quality of an
    LLM. If we want an LLM to be accurate in how it responds to requests, it’s not
    enough to just throw as much data as possible. It needs to be data that can help
    influence the probability of a model in a sensible manner. For example, the Reddit
    example we explored earlier in the chapter, in which the subreddit r/counting
    being used to train ChatGPT caused it to hallucinate in strange ways, demonstrates
    that more isn’t necessarily better. Still, similar to parameter count, the more
    high-quality data an LLM has been trained on, the better it will likely perform.
    The challenge lies in knowing what data an LLM has been trained on—something that
    corporate creators of AI are keen to keep a secret.
  prefs: []
  type: TYPE_NORMAL
- en: Extensibility and Integration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Just as with any other tool, the value of an LLM can be increased further if
    it can offer other features beyond its core abilities, such as integrating into
    existing systems or training models further for our specific needs. What features
    are available to integrate and extend LLMs largely depends on who was responsible
    for training them.
  prefs: []
  type: TYPE_NORMAL
- en: For example, OpenAI offers paid-for API access to their models. But beyond an
    instruction feature that allows you to tweak output with a simple prompt, there
    is no ability to further fine-tune and deploy one of their GPT models for private
    use. Compare this to Meta’s LlaMa model, which has been open-sourced, allowing
    the AI community to download and further train to their own requirements, though
    they have to build their own infrastructure to deploy the model.
  prefs: []
  type: TYPE_NORMAL
- en: As LLM platforms grow, we will see advances in not just their ability to respond
    to prompts but also the features around them and their access. So it’s necessary
    to keep said features in mind when evaluating what to work with.
  prefs: []
  type: TYPE_NORMAL
- en: Quality of responses
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Arguably, the most important factor to consider is whether an LLM is providing
    responses that are legible, useful and free (or as close to free) of hallucination
    as possible. Although criteria such as parameter count and training data are useful
    indicators of an LLM's performance, it’s up to us to understand what we want to
    use an LLM for and then determine how each responds toward our prompts and help
    in solving our specific problems. Not all challenges we face need the largest,
    most expensive LLM in the market. So it’s important that we take the time to try
    out different models, compare their outputs and then make a judgment for ourselves.
    For example, GPT models from OpenAI are found to perform better with code examples
    than, say, Google Bard. These details have been discovered through experimentation
    and observation.
  prefs: []
  type: TYPE_NORMAL
- en: These criteria we’ve explored are by no means an exhaustive list, but once again
    they demonstrate that there is more to consider about LLMs once we get past the
    initial glamour of how they respond. Different LLMs perform in different ways,
    helping us with different challenges. So let’s take a look at some of the more
    popular models and platforms that are currently available.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.2 Examining popular Large Language Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since OpenAI’s launch of ChatGPT, there has been an explosion in releases of
    LLMs from various organisations. It’s not to say that these models and related
    work weren’t around before ChatGPT’s release, but the public focus has certainly
    intensified and more and more marketing and release announcements have focused
    on companies releasing their LLM offerings. Here are some of the more common/popular
    LLMs that have been released since the end of 2022.
  prefs: []
  type: TYPE_NORMAL
- en: Keeping up with LLMs
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: It’s worth noting that the situation with the launch of LLMs and their related
    features is extremely fluid and has grown at quite a fast pace. Therefore, it’s
    likely that some of what we’ll explore will differ from the time of writing in
    mid-2023 to the time you are reading this book. However, what this list demonstrates
    is some of the bigger names in the LLM space that are worth exploring.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: At the time of writing, OpenAI is the most ubiquitous of organizations offering
    LLMs for use. Although OpenAI have been working on LLM models for quite some time,
    releasing their GPT-3 model in 2020, it was their release of ChatGPT in November
    2022 that kickstarted the popular wave of interest and use of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenAI offers a range of different LLM models, but the two that stand out are
    GPT-3.5-Turbo and GPT-4, which you can learn about more in their docs: [https://platform.openai.com/docs/models/overview](models.html).
    These two models are used as *foundation* models, or models that can be trained
    further for specific purposes, for a range of products such as ChatGPT, GitHub
    Copilot and Microsoft Bing AI.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to their models, OpenAI has offered a range of features such as
    API access to their direct GPT-3.5-Turbo and GPT-4 models, and a collection of
    apps that integrate with ChatGPT (if you subscribe to their plus membership).
    It’s by far the most popular LLM (for now) and has kicked started a race with
    organizations to release their own LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Although we’ve already explored some prompts with ChatGPT, you can always access
    and experiment with ChatGPT at [https://chat.openai.com/](chat.openai.com.html).
  prefs: []
  type: TYPE_NORMAL
- en: Sticking with OpenAI
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Though there are many different large language models that I encourage you to
    use, for the sake of consistency, we will stick with ChatGPT-3.5-Turbo. It’s not
    necessarily the most powerful LLM at this time, but it is the most ubiquitous—and
    free. That said, if you want to try out these prompts with other LLM models, feel
    free. But be aware that their responses will likely differ from what is shared
    in this book.
  prefs: []
  type: TYPE_NORMAL
- en: PaLM
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As soon as OpenAI released ChatGPT it was only a matter of time before Google
    released their own LLM with chat services, and they did so in March 2023\. Based
    on their PaLM 2 LLM, a 540 billion parameter model, Google sought to compete with
    ChatGPT and offered a similar chat-based experience with Bard.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to OpenAI, Google offers access to PaLM 2 via their Google Cloud platform
    (which can be found at [https://developers.generativeai.google/](developers.generativeai.google.html))
    and has recently started offering apps that work similarly to OpenAI’s ChatGPT
    apps, with the added integration into other Google Suite tools such as Google
    Drive and Gmail.
  prefs: []
  type: TYPE_NORMAL
- en: You can access and experiment with Bard at [https://bard.google.com/chat](bard.google.com.html).
  prefs: []
  type: TYPE_NORMAL
- en: LLaMa
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'LLaMa, which is the name for a collection of models, was first released by
    Meta in July 2023\. What sets LLaMa apart from OpenAI’s GPT models and Google’s
    PaLM is that LLaMa is open source. In addition to the open-source license, LLaMa
    comes in a range of sizes: 7, 13 and 70 billion parameters, respectively. The
    combination of these sizes and their access means that LLaMa has been adopted
    by the AI community as a popular foundational model. The flip side of this access,
    though, is that Meta doesn’t provide a public platform to train and run versions
    of LLaMa. So data sets, and infrastructure, have to be personally sourced to use
    it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'More details on LLaMa can be found at the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.llama2.ai/](www.llama2.ai.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://llama-2.ai/download/](download.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huggingface
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Unlike the other entries in our list, Huggingface offers no proprietary model
    but instead facilitates an AI community that contains a wide variety of different
    models, most of which are open source. Looking at their index page of models,
    found at [https://huggingface.co/models](huggingface.co.html), we can see hundreds
    of thousands of differently trained models that have come from different companies
    and research labs. Huggingface also offers datasets for training, apps, and documentation
    that allow the reader to dive deeper into how models are built. All of these resources
    are available so that the AI community can access pre-trained models, tweak them
    and further train them for specific use, something that we’ll be exploring further
    in Part 3 of this book.
  prefs: []
  type: TYPE_NORMAL
- en: The marketplace for LLMs has sizeably grown in a short amount of time, both
    commercially and in open-source, and similar to other areas of software development,
    being proactive in what new LLMs are appearing can be beneficial. However, it
    can also be overwhelming and not necessarily feasible to keep up with everything
    that is happening at once. So instead of attempting to keep abreast of all the
    comings and goings in the AI community, we can opt to explore LLMs when we want
    to use LLMs to solve specific problems. Having a problem can help frame our criteria
    around which tools work best for us and which don’t.
  prefs: []
  type: TYPE_NORMAL
- en: Activity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Either select an earlier prompt from this chapter or create one of your own
    and submit it to different LLMs. Note how each responds and compares. Do some
    feel more conversational? How do they handle either receiving or sending code
    examples? Which ones provide the best response in your opinion?
  prefs: []
  type: TYPE_NORMAL
- en: 2.6 Creating a library of prompts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the benefits of prompts is that once they are created, they can be used
    repeatedly. As a consequence, a lot of collections of prompts for different roles
    and tasks are appearing online. For example, here are a few collections that I’ve
    seen shared recently:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Awesome ChatGPT Prompts, GitHub: [https://github.com/f/awesome-chatgpt-prompts](f.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '50 ChatGPT Prompts for Developers, Dev.to: [https://dev.to/mursalfk/50-chatgpt-prompts-for-developers-4bp6](mursalfk.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ChatGPT Cheat Sheet, Hackr.io: [https://hackr.io/blog/chatgpt-cheat-sheet-for-developer](blog.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This list is not at all exhaustive and the sample collections aren’t necessarily
    related to testing, but they are worth looking through to learn how others have
    created prompts, as well as giving us the opportunity to determine which prompts
    would actually be effective and which wouldn’t.
  prefs: []
  type: TYPE_NORMAL
- en: Though prompt collections shared publicly can be useful, it’s likely we’ll end
    up creating prompts that are used for specific contexts. So it’s worthwhile getting
    into the habit of storing prompts that prove to be useful in some sort of repository
    for us, and others, to quickly use. Where you store these will depend on what
    and who they are used for. If they’re for public use, then sharing a repository
    of prompts or adding to existing collections might be valuable. If we’re creating
    and using them while developing company products, then we need to treat them in
    the same way as our production code and store them somewhere private so that we
    don’t violate any policies around intellectual property. Finally, we may also
    consider version control so that we can tweak and track prompts as we learn more
    about working with LLMs and as the LLMs themselves evolve.
  prefs: []
  type: TYPE_NORMAL
- en: Wherever they are stored, the idea is to create a repository of prompts that
    are quick and easy to access so that once a prompt has been created for a specific
    activity, it can be reused multiple times rapidly so that we can get as much value
    from them to improve our productivity.
  prefs: []
  type: TYPE_NORMAL
- en: Activity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Create a space where you can store future prompts for you and your team to use.
  prefs: []
  type: TYPE_NORMAL
- en: Using prompts from this book
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In the spirit of storing prompts for future use and to help you, the reader,
    with trying out the prompt examples within this book, you can find each prompt
    example in the following GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/mwinteringham/llm-prompts-for-testing](mwinteringham.html)'
  prefs: []
  type: TYPE_NORMAL
- en: This will enable you to quickly copy and paste the prompts into your chosen
    LLM as we go through each chapter. Saving you the task of having to type the whole
    prompt in manually. There will be sections in certain prompts where you will need
    to add your own custom content or context to use them. To make them clear, instructions
    on what is required to add to the prompt will be found in the prompt and will
    be formatted in all caps and inside square brackets.
  prefs: []
  type: TYPE_NORMAL
- en: 2.7 Solving problems by using prompts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The tactics and tooling we’ve learned about in this chapter help provide us
    with a framework to use LLMs and design specific prompts for specific testing
    activities. We should be mindful, though, that although these tactics improve
    our chances of getting desired results, they are not foolproof. For example, when
    we ask an LLM to evaluate its output, the LLM isn’t actually evaluating its output
    like a traditional application might. It’s simply moving the predictive needle
    further toward an output that aligns with our requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore it is necessary for us to develop the skills to write prompts that
    help us solve our problems effectively and in a way that doesn’t end up diminishing
    the time saved using LLMs (For example, we don’t want to spend hours tweaking
    prompts.)
  prefs: []
  type: TYPE_NORMAL
- en: Single prompting versus multi-prompting
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Throughout this chapter we’ve explored how to use principles and tactics to
    create individual prompts that are effective as possible at maximizing desired
    output from an LLM. However, tools like ChatGPT, Bard and Claude allow us to conduct
    ‘conversations’ with LLMs in which the history of the conversations influence
    the output of future responses in said ‘conversation’. This raises the question,
    would it be easier to try multiple prompts in a conversation to tweak output?
    Though this can be effective, we do run the risk that the longer a conversation
    progresses, the higher the risk of hallucinations occurring as an LLM attempts
    to overfit responses to our requests. This is why tools like BingAI are limited
    with the amount of responses they can give in a given conversation. However, more
    importantly, more doesn’t necessarily mean better. The garbage in, garbage out
    rule applies with both single and multiple prompts. Relying on multiple prompts
    in one conversation means we become less clear and precise in what we are asking
    for, which adds delays and increases hallucination, thus negating the value of
    using an LLM in the first place. In conclusion, whether we want to send a single
    prompt to get what we want or send multiple prompts, adopting the principles and
    tactics created by Isa Fulford and Andrew Ng will increase our productivity with
    LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'This means being able to identify specific issues that LLMs can help with and
    then utilizing prompt engineering to maximize the chances of extracting valuable
    information from an LLM. This is what we’ll explore throughout the rest of this
    book: when and how to use LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: As we progress, we’ll also learn that prompts come in many shapes and sizes.
    Throughout this chapter, we’ve looked at prompts that are manually written by
    us humans. But, as we’ll learn, tools like GitHub Copilot auto-generate prompts
    as we write our code. That doesn’t mean we can’t still infuse the principles and
    tactics into our ways of working, but it does take time, awareness, and practice
    to develop.
  prefs: []
  type: TYPE_NORMAL
- en: Activity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Before you continue reading this book and learn about different types of prompts
    for different testing activities, use the knowledge of chapters 1 and 2 and consider
    a specific testing task that you do and attempt to build a prompt that can help
    you with your work.
  prefs: []
  type: TYPE_NORMAL
- en: 2.8 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs are trained on vast amounts of data using sophisticated algorithms to analyze
    our requests and predict an output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The predictive nature of LLMs makes them quite adaptable but also means they
    come with some risks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can sometimes output *hallucinations,* or text that sounds authoritative
    and correct when in fact it is completely false.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data that LLMs are trained on may contain errors, gaps, and assumptions,
    and we must be mindful of that when using them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We must also be mindful of the data we share with LLMs so as not to cause unauthorized
    leaks of business or user information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt engineering is a collection of principles and tactics we can use to maximize
    the chances of an LLM returning a desired output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can leverage the knowledge that LLMs are predictive in nature and use it
    to our advantage through the use of prompt engineering.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using delimiters can help us clarify instructions and parameters in a prompt.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An LLM can output data in various formats, but it requires us to explicitly
    state which structure format we want in a prompt.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can reduce hallucinations from LLMs by using the check-for-assumption tactic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Providing examples in a prompt can help ensure that an LLM provides an output
    in a desired format or context.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specifying specific subtasks in a prompt can help an LLM process complex tasks
    successfully.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Asking LLMs to evaluate solutions to problems can also reduce errors and maximize
    outcomes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knowing when to use LLMs and developing skills with prompt engineering is the
    key to success, regardless of what tooling we use.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
