- en: appendix C Natural language processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'C.1 Touring around the zoo: Meeting other Transformer models'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In chapter 13, we discussed a powerful Transformer-based model known as BERT
    (bidirectional encoder representations from Transformers). But BERT was just the
    beginning of a wave of Transformer models. These models grew stronger and better,
    either by solving theoretical issues with BERT or re-engineering various aspects
    of the model to perform faster and better. Let’s understand some of these popular
    models to learn what sets them apart from BERT.
  prefs: []
  type: TYPE_NORMAL
- en: C.1.1 Generative pre-training (GPT) model (2018)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The story actually starts even before BERT. OpenAI introduced a model called
    GPT in the paper “Improving Language Understanding by Generative Pre-Training”
    by Radford et al. ([http://mng.bz/1oXV](http://mng.bz/1oXV)). It is trained in
    a similar fashion to BERT, pretraining on a large corpus of text followed by fine-tuning
    on a discriminative task. The GPT model is a *Transformer decoder* compared to
    BERT, which is a *Transformer encoder*. The difference is that the GPT model has
    left-to-right (or causal) attention, whereas BERT has bidirectional (i.e., left-to-right
    and right-to-left) attention used when computing self-attention outputs. In other
    words, the GPU model pays attention only to the words to the left of it as it
    computes the self-attention output of a given word. This is the same as the masked
    attention component we discussed in chapter 5\. Due to this, GPT is also called
    an *autoregressive model*, whereas BERT is called an *autoencoder*. In addition,
    unlike BERT, adapting GPT to different tasks like sequence classification, token
    classification, or question answering requires slight architectural changes, which
    is cumbersome. GPT has three versions (GPT-1, GPT-2, and GPT-3); each model became
    bigger while introducing slight changes to improve performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE OpenAI, TensorFlow: [https://github.com/openai/gpt-2](https://github.com/openai/gpt-2).'
  prefs: []
  type: TYPE_NORMAL
- en: C.1.2 DistilBERT (2019)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Following BERT, DistilBERT is a model introduced by Hugging Face in the paper
    “DistilBERT, a distilled version of BERT: Smaller, faster, cheaper and lighter”
    by Sanh et al. ([https://arxiv.org/pdf/1910.01108v4.pdf](https://arxiv.org/pdf/1910.01108v4.pdf))
    in 2019\. The primary focus of DistilBERT is to compress BERT while keeping the
    performance similar. It is trained using a transfer learning technique known as
    *knowledge distillation* ([http://mng.bz/qYV2](http://mng.bz/qYV2)). The idea
    is to have a teacher model (i.e., BERT), and a smaller model (i.e., DistilBERT)
    that tries to mimic the teacher’s output. The DistilBERT model is smaller compared
    to BERT and only has 6 layers, as opposed to BERT, which has 12 layers. The DistilBERT
    model is initialized with the initialization of every other layer of BERT (because
    DistilBERT has exactly half the layers of BERT). Another key difference of DistilBERT
    is that it is only trained on the masked language modeling task and not on the
    next-sentence prediction task.'
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE Hugging Face’s Transformers: [https://huggingface.co/transformers/model_doc/distilbert.xhtml](https://huggingface.co/transformers/model_doc/distilbert.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: C.1.3 RoBERT/ToBERT (2019)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RoBERT (recurrence over BERT) and ToBERT (Transformer over BERT) are two models
    introduced in the paper “Hierarchical Transformer Models for Long Document Classification”
    by Pappagari et al. ([https://arxiv.org/pdf/1910.10781.pdf](https://arxiv.org/pdf/1910.10781.pdf)).
    The main problem addressed by this paper is the inability or the performance degradation
    experienced for long text sequences (e.g., call transcripts) in BERT. This is
    because the self-attention layer has a computational complexity of O(n²) for a
    sequence of length n. The solution proposed in these models is to factorize long
    sequences to smaller segments of length k (with overlap) and feed each segment
    to BERT to generate the pooled output (i.e., the output of the [CLS] token) or
    the posterior probabilities (from a task-specific classification layer). Next,
    stack the outputs returned by BERT for each segment and pass them on to a recurrent
    model like LSTM (RoBERT) or a smaller Transformer (ToBERT).
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE Hugging Face’s Transformers: [https://huggingface.co/transformers/model_doc/roberta.xhtml](https://huggingface.co/transformers/model_doc/roberta.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: C.1.4 BART (2019)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'BART (bidirectional and auto-regressive Transformers), proposed in “BART: Denoising
    Sequence-to-Sequence Pre-training for Natural Language Generation, Translation,
    and Comprehension” by Lewis et al. ([https://arxiv.org/pdf/1910.13461.pdf](https://arxiv.org/pdf/1910.13461.pdf))
    is a sequence-to-sequence model. We discussed sequence-to-sequence models in chapters
    11 and 12, and BART draws on the same concepts. BART has an encoder and a decoder.
    If you remember from chapter 5, the Transformer model also has an encoder and
    a decoder and can be thought of as a sequence-to-sequence model. The encoder of
    a Transformer has bidirectional attention, whereas the decoder of a Transformer
    has left-to-right attention (i.e., is autoregressive).'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the vanilla Transformer model, BART uses several innovative pre-training
    techniques (document reconstruction) to pretrain the model. Particularly, BART
    is trained as a denoising autoencoder, where a noisy input is provided and the
    model needs to reconstruct the true input. In this case, the input is a document
    (a collection of sentences). The documents are corrupted using the methods listed
    in table C.1.
  prefs: []
  type: TYPE_NORMAL
- en: Table C.1 Various methods employed in corrupting documents. The true document
    is “I was hungry. I went to the café.” The “_” character denotes the mask token.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Method** | **Description** | **Example** |'
  prefs: []
  type: TYPE_TB
- en: '| Token masking | Tokens in the sentences are randomly masked. | I was _ .
    I _ to the cafe |'
  prefs: []
  type: TYPE_TB
- en: '| Token deletion | Tokens are randomly deleted. | I hungry . I went to café
    |'
  prefs: []
  type: TYPE_TB
- en: '| Sentence permutation | Change the order of the sentences. | I went to the
    café . I was hungry |'
  prefs: []
  type: TYPE_TB
- en: '| Document rotation | Rotate the document so that the starting and ending of
    the document changes. | Café . I was hungry . I went to the |'
  prefs: []
  type: TYPE_TB
- en: '| Text infilling | Instead of a single token, mask spans tokens with a single
    mask token. A 0 length span would insert the mask token. | I was _ hungry . I
    _ the café |'
  prefs: []
  type: TYPE_TB
- en: With the corruption logic, we generate inputs to BART, and the target will be
    the true document without corruptions. Initially, the corrupted document is input
    to the encoder, and then the decoder is asked to recursively predict the true
    sequence while using the previously predicted output(s) as the next input. This
    is similar to how we predicted translations using a machine translation model
    in chapter 11.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the model is pretrained, you can use BART for any of the NLP tasks that
    Transformer models are typically used for. For example, BART can be used for sequence
    classification tasks (e.g., sentiment analysis) in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: Input the token sequence (e.g., movie review) to both the encoder and the decoder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a special token (e.g., [CLS]) to the end of the sequence when feeding the
    decoder input. We added the special token to the beginning of the sequence when
    working with BERT.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get the hidden representation output for the special token by the decoder and
    feed that to a downstream classifier that will predict the final output (e.g.,
    positive/negative prediction).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To use BART for sequence-to-sequence problems (e.g., machine translation),
    follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Input the source sequence to the encoder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a starting (e.g., [SOS]) and ending (e.g., [EOS]) special token to the start
    and end of the target sequence, respectively.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: During training, train the decoder with all tokens in the target sequence except
    the last as the input and all tokens but the first as the target (i.e., teacher
    forcing).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: During inference, provide the starting token as the first input to the decoder
    and recursively predict the next output while using the previous output(s) as
    the input (i.e., autoregressive),
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'NOTE Hugging Face’s Transformers: [http://mng.bz/7ygy](http://mng.bz/7ygy).'
  prefs: []
  type: TYPE_NORMAL
- en: C.1.5 XLNet (2020)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'XLNet was introduced in the paper “XLNet: Generalized Autoregressive Pretraining
    for Language Understanding” by Yang et al. ([https://arxiv.org/pdf/1906.08237.pdf](https://arxiv.org/pdf/1906.08237.pdf))
    in early 2020\. Its primary focus was to capture the best of both worlds in autoencoder-based
    models (e.g., BERT) and autoregressive models (e.g., GPT). For this discourse,
    it is important to understand the advantages and drawbacks of the two approaches.'
  prefs: []
  type: TYPE_NORMAL
- en: A key advantage that BERT has as an autoencoder model is that the task-specific
    classification head has hidden representations of tokens that are enriched by
    bidirectional context, as it can pay attention to both sides of a given token.
    And as you can imagine, knowing what comes before as well as after for a given
    token yield results in better downstream tasks. Conversely, GPT pays attention
    to only the left side of a given word to generate the representation. Therefore,
    GPT’s token representations are suboptimal in the sense that they only pay unidirectional
    attention (left side) to the token.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the pretraining methodology of BERT involves introducing
    the special token [MASK]. Though this token appears in the pretraining context,
    it never appears in the fine-tuning context, creating a pretraining fine-tuning
    discrepancy.
  prefs: []
  type: TYPE_NORMAL
- en: There’s a more critical issue that is lurking in BERT. BERT formulates the language
    modeling under the assumption that masked tokens are separately constructed (i.e.,
    independence assumption). In other words, if you have the sentence “I love [MASK][1]
    [MASK][2] city,” the second mask token is generated with no attention to what
    was chosen for the [MASK][1] token. This is wrong because to generate a valid
    city name, you must know the value of [MASK][1] before generating [MASK][2]. However,
    the autoregressive nature of GPT allows the model to first predict the value for
    [MASK][1] and then use that along with other words to its left to generate the
    value for [MASK][2] about the first word in the city before generating the second
    word (i.e., context aware).
  prefs: []
  type: TYPE_NORMAL
- en: 'XLNet blends these two language modeling approaches into one so that you have
    the bidirectional context coming from the approach used in BERT and the context
    awareness from GPT’s approach. The new approach is called *permutation language
    modeling*. The idea is as follows. Consider a T elements-long sequence of words.
    There are T! permutations for that sequence. For example, the sentence “Bob loves
    cats” will have 3! = 3 × 2 × 1 = 6 permutations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If the parameters of the language model used to learn this are shared across
    all the permutations, not only can we use an autoregressive approach to learn
    it, but we can also capture information from both sides of the text for a given
    word. This is the main idea explored in the paper.
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE Hugging Face’s Transformers: [http://mng.bz/mOl2](http://mng.bz/mOl2).'
  prefs: []
  type: TYPE_NORMAL
- en: C.1.6 Albert (2020)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Albert is a variant BERT model that delivers competitive performance to BERT
    with fewer parameters. Albert makes two important contributions: reducing the
    model size and introducing a new self-supervised loss that helps the model capture
    language better.'
  prefs: []
  type: TYPE_NORMAL
- en: Factorization of the embedding layer
  prefs: []
  type: TYPE_NORMAL
- en: First, Albert factorizes the embedding matrix used in BERT. In BERT the embeddings
    are metrics of size V × H, where V is the vocabulary size and H is the hidden
    state size. In other words, there is a tight coupling between the embedding size
    (i.e., length of an embedding vector) and the final hidden representation size.
    However, the embeddings (e.g., WordPiece embeddings in BERT) are not designed
    to capture context, whereas hidden states are computed taking both the token and
    its context into account. Therefore, it makes sense to have a large hidden state
    size H, as the hidden state captures a more informative representation of a token
    than embeddings. But doing so increases the size of the embedding matrix due to
    the tight coupling present. Therefore, Albert suggests factorizing the embedding
    matrix to two matrices, V × E and E × H, decoupling the embedding size and the
    hidden state size. With this design, one can increase the hidden state size while
    keeping the embedding size small.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-layer parameter sharing
  prefs: []
  type: TYPE_NORMAL
- en: 'Cross-layer parameter sharing is another technique introduced in Albert to
    reduce the parameter space. Since all layers in BERT (as well the Transformer
    model in general) have uniform layers from top to bottom, parameter sharing is
    trivial. Parameter sharing can happen in one of the following three ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Across all self-attention sublayers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Across all the fully connected sublayers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Across self-attention and fully connected sublayers (separately)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Albert uses sharing all parameters across layers as the default strategy. By
    using this strategy, Albert achieves a 71%-86% parameter reduction without compromising
    the performance of the model significantly.
  prefs: []
  type: TYPE_NORMAL
- en: Sentence-order prediction instead of next sentence prediction
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the authors of the paper argue that the value added by the next-sentence
    prediction pretraining task in BERT is doubtful, which is supported by several
    previous studies. Therefore, they introduce a new, more challenging model that
    focuses primarily on language coherence: sentence-order prediction. In this, the
    model is trained with a binary classification head to predict whether a given
    pair of sentences are in the correct order. The data can be easily generated,
    where positive samples are taken as sentences next to each other in that order,
    and negative samples are generated by swapping two adjacent sentences. The authors
    argue that this is more challenging than next-sentence prediction, leading to
    a more informed model than BERT.'
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE TFHub: ([https://tfhub.dev/google/albert_base/3](https://tfhub.dev/google/albert_base/3)).
    Hugging Face’s Transformers: ([http://mng.bz/5QM1](http://mng.bz/5QM1)).'
  prefs: []
  type: TYPE_NORMAL
- en: C.1.7 Reformer (2020)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Reformer was one of the latest to join the family of Transformers. The
    main idea behind the Reformer is its ability to scale to sequences that are several
    tens of thousands of tokens long. The Reformer was introduced in the paper “Reformer:
    The Efficient Transformer” by Kitaev et al. ([https://arxiv.org/pdf/2001.04451.pdf](https://arxiv.org/pdf/2001.04451.pdf))
    in early 2020.'
  prefs: []
  type: TYPE_NORMAL
- en: The main limitation of the vanilla Transformers that prevents them from being
    used for long sequences is the computational complexity of the self-attention
    layer. It needs to look at every other word for every word to generate the final
    representation, which has a O(L²) complexity for a sequence that has L tokens.
    The reformer uses locality-sensitive hashing (LSH) to reduce this complexity to
    O(L logL). The idea of LSH is to assign every input a hash; the inputs having
    the same hash are considered similar and assigned to the same bucket. With that,
    similar inputs are placed in one bucket. To do that, we have to introduce several
    modifications to the self-attention sublayer.
  prefs: []
  type: TYPE_NORMAL
- en: Locality-sensitive hashing in the self-attention layer
  prefs: []
  type: TYPE_NORMAL
- en: First, we need to make sure the Q and K matrices are identical. This is a necessity
    as the idea is to compute similarity between queries and keys. This is easily
    done by sharing the weights between Q and K weight matrices. Next, a hashing function
    needs to be developed, which can generate a hash for a given query/key so that
    similar queries/keys (shared-qk) get similar hashes. Also remember that this must
    be done in a differentiable way to ensure end-to-end training of the model. The
    following hashing function is used
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: where R is a random matrix of size [d_model, b/2] for a user-defined b (i.e.,
    number of buckets) and x is the input of shape [b, L, d_model]. By using this
    hashing function, you get a bucket ID for each input token in a batch in a given
    position. To learn more about this technique, refer to the original paper “Practical
    and Optimal LSH for Angular Distance” by Andoni et al. ([https://arxiv.org/pdf/1509.02897.pdf](https://arxiv.org/pdf/1509.02897.pdf)).
    According to the bucket ID, the shared-qk items are sorted.
  prefs: []
  type: TYPE_NORMAL
- en: Then the sorted shared-qk items are chunked using a fixed chunk size. A larger
    chunk size means more computations (i.e., more words are considered for a given
    token), whereas a smaller chunk size can mean underperformance (i.e., not enough
    tokens to look at).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the self-attention is computed as follows. For a given token, look
    in the same chunk that it’s in as well as the previous chunk and attend to the
    words with the same bucket ID in both of those chunks. This will produce the self-attention
    output for all the tokens provided in the input. This way, the model does not
    have to look at every other word for every token and can focus on a subset of
    words or tokens for a given token. This makes the model scalable to sequences
    several tens of thousands of tokens long.
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE Hugging Face’s Transformers: [http://mng.bz/6XaD](http://mng.bz/6XaD).'
  prefs: []
  type: TYPE_NORMAL
