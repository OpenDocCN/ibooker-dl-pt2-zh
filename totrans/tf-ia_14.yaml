- en: '11 Sequence-to-sequence learning: Part 1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Understanding sequence-to-sequence data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a sequence-to-sequence machine translation model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training and evaluating sequence-to-sequence models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Repurposing the trained model to generate translations for unseen text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the previous chapter, we discussed solving an NLP task known as language
    modeling with deep recurrent neural networks. In this chapter, we are going to
    further our discussion and learn how we can use recurrent neural networks to solve
    more complex tasks. We will learn about a variety of tasks in which an arbitrary-length
    input sequence is mapped to another arbitrary-length sequence. Machine translation
    is a very appropriate example of this that involves converting a sequence of words
    in one language to a sequence of words in another.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, our primary focus is on building an English-to-German machine
    translator. To arrive at that, we will first download a machine translation data
    set, look at the structure of that data set, and apply some processing to prepare
    it for the model. Then we will define a machine translation model that can learn
    to map arbitrarily long sequences to other arbitrarily long sequences. This is
    an encoder-decoder-based model, meaning there is an encoder that takes in one
    sequence (e.g., an English phrase) to produce a latent representation of the sequence
    and a decoder that decodes that information to produce a target sequence (e.g.,
    a German phrase). A special characteristic of this model will be its ability to
    take in raw strings and convert them to numerical representations internally.
    Therefore, this model is more end-to-end than other NLP models we have created
    in previous chapters. Once the model is defined, we will train it using the data
    set we processed and evaluate it on two metrics: per-word accuracy of the sequences
    produced and BLEU (biLingual evaluation understudy). BLEU is a more advanced metric
    than accuracy that mimics how a human would evaluate the quality of a translation.
    Finally, we will define a slightly modified decoder that can recursively produce
    words (starting from an initial seed) while taking the previous prediction as
    the input for the current time step. In the first section, we will discuss the
    data a bit before diving into modeling.'
  prefs: []
  type: TYPE_NORMAL
- en: 11.1 Understanding the machine translation data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You are developing a machine translation service for tourists who are visiting
    Germany. You found a bilingual parallel corpus of English and German text (available
    at [http://www.manythings.org/anki/deu-eng.zip](http://www.manythings.org/anki/deu-eng.zip)).
    It contains English text and a corresponding German translation side by side in
    a text file. The idea is to use this to train a sequence-to-sequence model, and
    before doing that, you have to understand the organization of the data, load it
    into memory, and analyze the vocabulary size and the sequence length. Furthermore,
    you will process the text so that it has the special token “sos” (denotes “start
    of sentence”) at the beginning of the German translation and “eos” (denotes “end
    of sentence”) at the end of the translation. These are important tags that will
    help us at the time of generating translations from the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first download the data set and take a tour of it. You will need to manually
    download this data set (available at [http://www.manythings.org/anki/deu-eng.zip](http://www.manythings.org/anki/deu-eng.zip)),
    as this web page does not support automatic retrieval through script. Once downloaded,
    we will extract the data, which has a text file containing the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'If you open the text file, it will have entries as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The data is in tab-separated format and has a <German phrase><tab><English
    phrase><tab><Attribution> format. We really care about the first two tab-separated
    values in a record. Once the data is downloaded, we can easily load the data to
    a pandas DataFrame. Here we will load the data, set up column names, and extract
    the columns that are of interest to us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We can also compute the size of the DataFrame through
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: which will return
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: NOTE The data here is updated over time. Therefore, you can get slightly different
    results (e.g., data set size, vocabulary size, vocabular distribution, etc.) than
    shown here.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have around 227,000 examples in our data set. Each example contains an English
    phrase/sentence/paragraph and the corresponding German translation. We will do
    one more cleaning step. It seems that some of the entries in the text file have
    some Unicode issues. These are handled fine by pandas, but are problematic for
    some downstream TensorFlow components. Therefore, let''s run the following cleanup
    step to ignore those problematic lines in the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Let’s analyze some of the samples by calling df.head() (table 11.1) and df.tail()
    (table 11.2). df.head() returns the contents of table 11.1, whereas df.tail()
    produces the contents of table 11.2.
  prefs: []
  type: TYPE_NORMAL
- en: Table 11.1 Some of the examples at the beginning of the data
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **EN** | **DE** |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | Go. | Geh. |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Hi. | Hallo! |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Hi. | Grüß Gott! |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Run! | Lauf! |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Run. | Lauf! |'
  prefs: []
  type: TYPE_TB
- en: Table 11.2 Some of the examples at the end of the data
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **EN** | **DE** |'
  prefs: []
  type: TYPE_TB
- en: '| 227075 | Even if some by non-native speakers... | Auch wenn Sätze von Nichtmuttersprachlern
    mitu... |'
  prefs: []
  type: TYPE_TB
- en: '| 227076 | If someone who doesn’t your background sa... | Wenn jemand, der
    deine Herkunft nicht kennt, s... |'
  prefs: []
  type: TYPE_TB
- en: '| 227077 | If someone who doesn’t your background sa... | Wenn jemand Fremdes
    dir sagt, dass du dich wie... |'
  prefs: []
  type: TYPE_TB
- en: '| 227078 | If someone who doesn’t your background sa... | Wenn jemand, der
    nicht weiß, woher man kommt, ... |'
  prefs: []
  type: TYPE_TB
- en: '| 227079 | Doubtless there exists in this world precisely... | Ohne Zweifel
    findet sich auf dieser Welt zu je... |'
  prefs: []
  type: TYPE_TB
- en: 'The examples are sorted by their length, and you can see that they start with
    examples of a single word and end up with examples with approximately 50 words.
    We will only use a sample of 50,000 phrases from this data set to speed up our
    workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We set the random seed as random_seed=4321.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we will introduce two special tokens to the German translations: sos
    and eos. sos marks the start of the translation, whereas eos marks the end of
    the translation. As you will see, these tokens serve an important purpose when
    it comes to generating translations after the model trained. But for consistency
    during training and inference (or generation), we will introduce these tokens
    to all of our examples. This can be easily done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: SOS and EOS tokens
  prefs: []
  type: TYPE_NORMAL
- en: The choice of SOS and EOS is just a convenience, and technically they could
    be represented by any two unique tokens, as long as they are not words from the
    corpus itself. It is important to make these tokens unique, as they play an important
    role when generating translations from previously unseen English sentences. The
    specifics of the role will be discussed in a later section.
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s a very straightforward transformation. This will convert the phrase "Grüß
    Gott!" to "sos Grüß Gott! eos". Next, we’re going to generate a training/validation/test
    subset from the data we sampled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We will keep 10% of the data as test data, another 10% as validation data,
    and the remaining 80% as training data. The data will be randomly sampled (without
    replacement) for the data sets. We then move on to analyze two important characteristics
    of text data sets, as we have done over and over again: the vocabulary size (listing
    11.1) and the sequence length (listing 11.2).'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.1 Analyzing the vocabulary size
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Create a flattened list from English words.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Create a flattened list of German words.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Get the vocabulary size of words appearing more than or equal to 10 times.
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Generate a counter object (i.e., dict word -> frequency).
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Create a pandas series from the counter, and then sort most frequent to least.
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Print the most common words.
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Get the count of words that appear at least 10 times.
  prefs: []
  type: TYPE_NORMAL
- en: which will return
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Next, sequence analysis is done in the following function.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.2 Analyzing the sequence length
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Create a pd.Series, which contains the sequence length for each review.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Get the median as well as summary statistics of the sequence length.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Get the quantiles at given marks (i.e., 1% and 99% percentiles).
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Print the summary stats of the data between the defined quantiles.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, call this function on the data to get the statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This produces
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s print out the vocabulary size and the sequence length parameters
    for the two languages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This will return
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Now we have the language-specific parameters needed to define the model. In
    the next section, we’ll look at how we can define a model to translate between
    languages.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 1
  prefs: []
  type: TYPE_NORMAL
- en: 'You have been given a pandas Series ser in the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Write a function called vocab_size(ser) to return the vocabulary size.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2 Writing an English-German seq2seq machine translator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You have a clean data set that is ready to go into a model. You will be using
    a sequence-to-sequence deep learning model as the machine translation model. It
    consists of two parts: an encoder that produces a hidden representation of the
    English (source) text and a decoder that decodes that representation to German
    (target) text. Both the encoder and the decoder are recurrent neural networks.
    Moreover, the model will accept raw text and will convert the raw text to token
    IDs using a TextVectorization layer provided in TensorFlow. These token IDs will
    go to an embedding layer that will return word vectors of the token IDs.'
  prefs: []
  type: TYPE_NORMAL
- en: We have the data prepared and ready to go. Now let’s learn about the model that
    can consume this data. Sequence-to-sequence learning maps an arbitrarily long
    sequence to another arbitrarily long sequence. This poses a unique challenge for
    us, as the model not only needs to be able to consume a sequence of arbitrary
    length, but also needs to be able to produce a sequence of arbitrary length as
    the output. For example, in machine translation, it is very common for translations
    to have fewer or more words than the input. Because of this, they require a special
    type of model. These models are known as *encoder-decoder* or *seq2seq* (short
    for sequence-to-sequence) models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Encoder-decoder models are, in fact, two different models interconnected in
    a certain way. Conceptually, the encoder takes in a sequence and produces a context
    vector (or a thought vector) that embeds the information present in the input
    sequence. The decoder takes in the representation produced by the encoder and
    decodes it to generate another sequence. Since the two parts (i.e., the encoder
    and the decoder) operate on separate things (i.e., the encoder consumes the input
    sequence while the decoder generates the output sequence), encoder-decoder models
    are well suited for solving sequence-to-sequence tasks. Another way to understand
    what the encoder and the decoder do is as follows: the encoder processes the source
    language input (i.e., the language to translate from), and the decoder processes
    the target language input (i.e., the language to translate to). This is depicted
    in figure 11.1.'
  prefs: []
  type: TYPE_NORMAL
- en: '![11-01](../../OEBPS/Images/11-01.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 High-level components of the encoder-decoder architecture in the
    context of machine translation
  prefs: []
  type: TYPE_NORMAL
- en: Particularly, the encoder contains a recurrent neural network. We will be using
    a gated recurrent unit (GRU) model. It goes through the input sequence and produces
    a final output, which is the final output of the GRU cell after it processes the
    last element in the input sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Thought vector
  prefs: []
  type: TYPE_NORMAL
- en: A *thought vector* is a term popularized by Geoffery Hinten, a luminary in deep
    learning who has been involved since its inception. A thought vector refers to
    a vectorized representation of a thought. The ability to generate accurate numerical
    representations of thoughts would revolutionize the way we search documents or
    search on the web (e.g., Google). This is similar to how a numerical representation
    of a word is called a *word vector*. In the context of machine translation, the
    context vector can be called a thought vector as it captures the essence of a
    sentence or a phrase in a single vector.
  prefs: []
  type: TYPE_NORMAL
- en: You can read more about this at [https://wiki.pathmind.com/thought-vectors](https://wiki.pathmind.com/thought-vectors).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we have the decoder, which also consists of a GRU model and several Dense
    layers. The purpose of the Dense layers is to generate a final prediction (a word
    from the target vocabulary). The weights of the Dense layers present in the decoder
    are shared across time. This means that, just as the GRU layer updates the same
    weights as it moves from one input to the other, the Dense layer reuses the same
    weights across the time steps. This process is depicted in figure 11.2.
  prefs: []
  type: TYPE_NORMAL
- en: '![11-02](../../OEBPS/Images/11-02.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 Specific components in the encoder and decoder modules. The encoder
    has a GRU layer, and the decoder consists of a GRU layer followed by one or more
    Dense layers, whose weights are shared across time.
  prefs: []
  type: TYPE_NORMAL
- en: Up until now, when solving NLP tasks, converting string tokens to numerical
    IDs was considered a preprocessing step. In other words, we would perform the
    tokens-to-ID conversion and input the IDs to the model. But it doesn’t have to
    be that way. We can define more versatile models that do such text processing
    internally as well as learn to solve the tasks. Keras provides certain layers
    that can plug into your model to be more end-to-end. The tensorflow.keras.layers.experimental.preprocessing.TextVectorization
    layer is one such layer. Let’s examine the usage of this layer.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.1 The TextVectorization layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The TextVectorization layer takes in a string, tokenizes it, and converts the
    tokens to IDs by means of a vocabulary (or dictionary) lookup. It takes a list
    of strings (or an array of strings) as the input, where each string can be a word/phrase/sentence
    (and so on). Then it learns the vocabulary from that corpus. Finally, the layer
    can be used to convert a list of strings to a tensor that contains a sequence
    of token IDs for each string in the list provided. Let’s see this layer in action.
    First import the layer with
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Then define the layer as follows. Here we’re defining the layer for the English
    language. Keep in mind that we need two TextVectorization layers in our model,
    one for English and one for German:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'It’s worthwhile to stop here and look at the different arguments we’re providing:'
  prefs: []
  type: TYPE_NORMAL
- en: max_tokens—Specifies the number of words in the vocabulary. Any word that is
    not present in the vocabulary (i.e., an out-of-vocabulary word) is converted to
    [UNK].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: output_mode—Specifies the type of the final output. Can be one of "int", "binary",
    "count", and "tf-idf". "int" means the layer will output a token ID for each token.
    "binary" implies that the output will be a [<batch size>, <vocab size>] tensor,
    where a value of 1 is given at an index, if the token indicated by that index
    is present in that example. "count" gives a similar output as "binary", but instead
    of 1s, it contains the number of times a token appeared in that example. "tf-id"
    gives a similar output as "binary", but the TF-IDF value at each position.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: output_sequence_length—Specifies the length of the batched input sequence after
    converting to token IDs. If set to None, it means the sequence length will be
    set to the length of the longest sequence in the batch. The shorter sequences
    are padded with a special token (special token defaults to "").
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To make the best out of this layer, we have to fit it on a text corpus so that
    it can learn the vocabulary. Calling the adapt() function and passing the list
    of strings (or an array of strings) to it achieves that. In other words, adapt()
    yields the same results as the fit() method of a scikit-learn model ([http://mng.bz/aJmB](http://mng.bz/aJmB)).
    It takes in some data and trains (or adapts) the model according to the data.
    In the case of the tokenizer, among other things, it builds out a dictionary (a
    mapping from word to ID):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: After fitting the layer, you can get the vocabulary
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: which prints
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In other words, the vocabulary is a list of tokens where the ID corresponds
    to their indexes in the list. You can compute the size of the vocabulary by
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: which returns
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, to use this layer to convert strings to numerical IDs, we must wrap it
    in a model. To do so, let’s first define a Keras Sequential model. Let’s name
    the model toy_model as this will only be used to learn the behavior of the text
    vectorizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Define an input layer, set its size to accept a tensor with a single column
    (i.e., a list of strings), and set the data type to tf.string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Then add the text vectorization layer we defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'You can use this just like any other Keras model and convert arbitrary text
    to numerical ID sequences. Specifically, you use the model.predict() function
    on some input data, which takes in the input and transforms it accordingly depending
    on the layers used in the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Finally, print the inputs and results as follows
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: which gives
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The layer does everything accordingly. Let’s first looks at the shape of the
    output. The shape, since we set the output_sequence_length=None, pads all the
    examples in the input up to the length of the longest input in the inputs. Here,
    “how are you” is the longest and has three words in it. Therefore, all the rows
    are padded with zeros, such that each example has three columns. Generally, the
    layer returns a [<batch size>, sequence_length]-sized output.
  prefs: []
  type: TYPE_NORMAL
- en: If the word is found in the vocabulary, it is converted to some number (e.g.,
    “run” is converted to 427). If the word is not found in the vocabulary (e.g.,
    “ectoplasmic”), it is replaced with a special ID (1) that corresponds to out-of-vocabulary
    words.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.2 Defining the TextVectorization layers for the seq2seq model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With a good understanding of the TextVectorization layer, let’s define a function
    to return a text vectorization layer wrapped in a Keras Model object. This function,
    named get_vectorizer(), takes in the following arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: corpus—Accepts a list (or array) of strings (i.e., a corpus to build the vocabulary).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: n_vocab—Vocabulary size. The most common n_vocab words are kept to build the
    vocabulary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: max_length (optional)—Length of the resulting token sequences. It defaults to
    None, in which case the sequence length will be the length of the longest text
    sequence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: return_vocabulary (optional)—Whether to return the vocabulary (i.e., list of
    string tokens).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: name (optional)—String to set the model name
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It defines an input layer that accepts a batch of strings (having a total shape
    of [None, 1]). Next, the function defines a text vectorizer layer. Note that the
    layer has a vocabulary size of n_vocab + 2. The extra 2 is necessary to accommodate
    the special tokens " " and "[UNK]". The layer is fitted with the text corpus passed
    into the function. Finally, we define a Keras model with the input layer (inp)
    and the output of the text vectorization layer (vectorize_out). If the return_vocabulary
    is set to True, it will also return the vocabulary of the vectorize_layer, as
    shown in the next listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.3 Defining the text vectorizers for the encoder-decoder model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Define an input layer that takes a list of strings (or an array of strings).
  prefs: []
  type: TYPE_NORMAL
- en: ❷ When defining the vocab size, we use n_vocab + 2 ,as there are two special
    tokens, "(Padding)" and "[UNK]", added automatically.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Fit the vectorizer layer on the data.
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Get the token IDs for the data fed to the input.
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Return the model only. The model takes an array of strings and outputs a tensor
    of token IDs.
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Return the vocabulary in addition to the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we have defined the function, let’s use it and define two vectorizers,
    one for the English inputs and one for the German inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Here, the corpus takes in a list or an array of text. Each text is a string
    containing an English or German phrase/sentence. n_vocab defines the size of the
    vocabulary, and max_length defines the sequence length to which we should pad
    the data. Note how we use de_seq_length-1 for the decoder. This Subtraction of
    1 here is a necessity due to the way data is presented to the decoder during the
    model training. We will discuss the specific details when we reach model training.
    Finally, we can define a name to keep track of different layers.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.3 Defining the encoder
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Moving on to the encoder, we will use a GRU model at the core of our encoder.
    The encoder is responsible for processing the source input sequence. Its responsibility
    is to process the source input and produce a *context vector* (sometimes called
    a *thought vector*). This vector captures the essence of the input sequence in
    a compact, vectorized representation. Normally, this context vector would be the
    GRU cell’s last output state after it processes the full input sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see the steps involved in getting the encoder ready. For this, we will
    use the Keras Functional layer. Sequence-to-sequence models are not sequential
    and involve nonlinear connections between the encoder and the decoder. Therefore,
    we cannot use the Keras Sequential API. First, we define the input layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The input accepts a list of strings. Note how we are setting the shape to (1,)
    to make sure the model accepts a tensor with just one column and dtype to tf.string.
    Next, we vectorize the text input fed forward by inp:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Here, the vectorizer is a model that performs text vectorization, which is output
    by the get_vectorizer() function we defined earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we define an embedding layer that will convert the token IDs returned
    by the vectorizer to word vectors. This is a layer that has trainable weights.
    Therefore, during the training, the model will tune the word embeddings to reflect
    useful representations to solve the task at hand:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: When defining the embedding layer, you need to always pass the vocabulary size
    (input_dim) and the output_dim. Note that the vocabulary size has been increased
    by 2 to accommodate the two special tokens (i.e., UNK and PAD) that are introduced.
    We will set the output_dim to 128\. We also want to mask excessive zeros that
    were padded from the final computations and thus set mask_zero=True. Finally,
    we will also pass a name to identify the layer easily.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we are coming to the core of our model: the recurrent neural network (RNN).
    As mentioned earlier, we will use a GRU model but with an added twist! We are
    going to make our GRU model bidirectional! A bidirectional RNN is a special type
    of RNN that processes a sequence both forward and backward. This is in contrast
    to a standard RNN, which only processes the sequence forward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Bidirectional RNN: Reading text forward and backward'
  prefs: []
  type: TYPE_NORMAL
- en: The standard RNN reads the text forward, one time step at a time, and outputs
    a sequence of outputs. Bidirectional RNNs, as the name suggests, not only read
    the text forward, but read it backward. This means that bidirectional RNNs have
    two sequences of outputs. Then these two sequences are combined using a combination
    strategy (e.g., *concatenation*) to produce the final output. Bidirectional RNNs
    typically outperform standard RNNs because they understand relationships in text
    both forward and backward, as shown in the following figure.
  prefs: []
  type: TYPE_NORMAL
- en: '![11-02-unnumb](../../OEBPS/Images/11-02-unnumb.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparison between standard RNNs and bidirectional RNNs
  prefs: []
  type: TYPE_NORMAL
- en: Why does reading text backward help? There are some languages that are read
    backward (e.g., Arabic, Hebrew). Unless the text is specifically processed to
    account for this writing style, a standard RNN would have a very difficult time
    understanding the language. By having a bidirectional RNN, you are removing the
    model's dependency on a language to always be left to right or right to left.
  prefs: []
  type: TYPE_NORMAL
- en: If we consider the English language, there can be instances where it's impossible
    to infer a relationship going only forward. Consider the two sentences
  prefs: []
  type: TYPE_NORMAL
- en: John went toward the bank on Clarence Street.
  prefs: []
  type: TYPE_NORMAL
- en: John went towards the bank of the river.
  prefs: []
  type: TYPE_NORMAL
- en: Since the two sentences are identical up to the word "bank," it is not possible
    to know if the bank is referring to the financial institution or a river bank
    until you read the rest. For a bidirectional RNN, this is trivial.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we get the output of the gru_layer and assign it to gru_out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we define the encoder model as a tf.keras.models.Model object. It
    takes inp (i.e., a single-column tensor of type tf.string) and outputs gru_out
    (i.e., the final state of the bidirectional GRU model). This final state of the
    GRU model is what is considered the context vector that provides the decoder information
    about the source language sentence/phrase input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: You can observe the step-by-step build of the encoder model encapsulated in
    a function, as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.4 The function that returns the encoder
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: ❶ The input is (None,1) shaped and accepts an array of strings.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Vectorize the data (assign token IDs)
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Define an embedding layer to convert IDs to word vectors.
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Get the embeddings of the token IDs.
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Define a bidirectional GRU layer. The encoder looks at the English text (i.e.,
    the input) both backward and forward.
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Get the output of the gru the last (the last output state vector returned
    by the model).
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Define the encoder model; it takes in a list/array of strings and returns
    the last output state of the GRU model.
  prefs: []
  type: TYPE_NORMAL
- en: 'After defining the function, you can simply call it to build the encoder model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 11.2.4 Defining the decoder and the final model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The encoder is done and dusted, and it’s time to look at the decoder. The decoder
    is going to look slightly more complex than the encoder. The core model of the
    decoder is again a GRU model. It is then followed by a fully connected hidden
    layer and a fully connected prediction layer. The prediction layer outputs a word
    from the German vocabulary (by computing probability over all the words in the
    vocabulary) for each time step.
  prefs: []
  type: TYPE_NORMAL
- en: 'During model training, the decoder predicts the next word in a given target
    sequence. For example, the decoder, given the target sequence [A, B, C, D], will
    train the model for three time steps on the following input-output tuples: (A,
    B), (B, C), and (C, D). In other words, given the token A, predict token B; given
    the token B, predict token C; and so on. If you think about the end-to-end process
    of the encoder-decoder model, the following steps take place:'
  prefs: []
  type: TYPE_NORMAL
- en: The encoder processes the source input sequence (i.e., English) and produces
    a context vector (i.e., the last output state of the GRU model).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The decoder uses the context vector produced by the encoder as its initial state
    for the recurrent component.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The decoder takes in the target input sequence (i.e., German) and predicts the
    next token given the previous token. For each time step, it predicts a token over
    the complete target vocabulary using a fully connected layer and a softmax layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This way of training the model is known as *teacher forcing*, as you are guiding
    the decoder with the target sequence (i.e., the teacher). Using teacher forcing
    quickly leads to better performance, compared to not using teacher forcing during
    the training of encoder-decoder-type models. Let’s look at the decoder and see
    how the encoder and the decoder tie in to create the final model in more depth,
    shown in figure 11.3.
  prefs: []
  type: TYPE_NORMAL
- en: '![11-03](../../OEBPS/Images/11-03.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 The implementation of the final sequence-to-sequence model with
    the focus on various layers and outputs involved
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s time to discuss the specifics of building the decoder and the final model.
    The first thing we have to do is get the encoder’s output by passing an input.
    We define an input layer identical to the input of the encoder and pass that to
    the encoder model we defined earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we pass the e_inp to the encoder model, which will give us the last output
    state of the GRU model as the output. This is an important input for the decoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'As the starting point of the decoder, we define an input layer with identical
    specifications to the encoder’s input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We then pass the input to a text vectorization model given the get_vectorizer()
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'We define an embedding layer as we did with the encoder so that token IDs produced
    by the text vectorization layer are converted to word vectors. Note that we have
    two separate embedding layers for the encoder and the decoder, as they use sequences
    from two different languages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'It’s now time to implement the recurrent component of the decoder. Similar
    to the encoder, we are using a GRU model to process the sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'But note that, in contrast to the encoder, in the decoder we are not using
    a bidirectional wrapper on the GRU model. The decoder cannot rely on a backward
    reading capability because it should generate the next output depending only on
    the previous and the current input. Note also that we have set return_sequences=True:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Finally, passing the output of the embedding layer to the gru_layer, we get
    the output. We stated earlier that the d_init_state (i.e., the encoder’s output)
    is one of the important inputs to the decoder. Here, we pass the d_init_state
    as the initial state to the decoder’s GRU layer. This means that, instead of starting
    out with a zero-initialized state vector, the decoder will use the encoder’s context
    vector as the initial state. Since we have set return_sequences=True, the output
    will contain output state vectors from all the time steps, not just the last one.
    This means the output will be of size [<batch size>, <time steps>, 256].
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.5 Defining the decoder and the final model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Define an encoder input layer and get the encoder output (i.e., the context
    vector).
  prefs: []
  type: TYPE_NORMAL
- en: ❷ The input is (None,1) shaped and accepts an array of strings. We feed the
    German sequence as the input and ask the model to predict it with the words offset
    by 1 (i.e., the next word).
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Get the decoder’s vectorized output.
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Define an embedding layer to convert IDs to word vectors. This is a different
    layer from the encoder's embedding layer.
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Define a GRU layer. Unlike the encoder, we cannot define a bidirectional GRU
    for the decoder.
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Get the output of the GRU layer of the decoder.
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Define an intermediate Dense layer and get the output.
  prefs: []
  type: TYPE_NORMAL
- en: ❽ The final prediction layer with softmax
  prefs: []
  type: TYPE_NORMAL
- en: ❾ Define the full model.
  prefs: []
  type: TYPE_NORMAL
- en: We can now define everything needed for our final model as
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Here, we are defining the English and German vectorizers (en_vectorizer and
    de_vectorizer, respectively). An encoder is then defined using the English vocabulary
    size and the English vectorizer. Finally, the final encoder-decoder model is defined
    using the German vocabulary size (de_vocab) encoder and the German vectorizer
    (de_vectorizer).
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.5 Compiling the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The last thing to do have the model ready for training is compile the model.
    We will use sparse categorical cross-entropy loss, the Adam optimizer, and the
    accuracy as metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Finally, let’s print the model summary
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: which will output
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will learn how the model we just defined can be trained
    with the data we prepared.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 2
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to teacher forcing, another technique used to define encoder-decoder
    models is by defining a model where
  prefs: []
  type: TYPE_NORMAL
- en: The encoder takes in the English token sequence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The decoder takes in the context vector repeated on the time axis as inputs
    so that the same context vector is fed to the decoder for every time step
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You have been provided the following encoder. Define the decoder that has a
    GRU layer and two fully connected hidden layers and the final model that starts
    with en_inp, and produce the final predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: You can use the tf.keras.layers.RepeatVector layer to repeat the context vector
    for any number of times. For example, if you pass a [None, 32]-sized tensor to
    the tf.keras.layers.RepeatVector(5) layer, it returns a [None, 5, 32]-sized tensor
    by repeating the [None, 32] tensor five times on the time dimension.
  prefs: []
  type: TYPE_NORMAL
- en: 11.3 Training and evaluating the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You have defined an end-to-end model that can consume raw text and generate
    translations. Next, you will train this model on the data that was prepared earlier.
    You will use the training set to train the model and the validation set to monitor
    the performance as it trains. Finally, the model will be tested on test data.
    To evaluate the model, we will use two metrics: accuracy and BLEU. BLEU is a popular
    metric used in sequence-to-sequence problems to measure the quality of the output
    sequence (e.g., translation).'
  prefs: []
  type: TYPE_NORMAL
- en: We have defined and compiled the model. We will now train the model on training
    data and evaluate its performance on validation and testing data across several
    metrics. We will use a performance metric known as BLEU to measure the performance
    of our model. It is not a standard metric provided in Keras and will be implemented
    using standard Python/NumPy functions. Due to this, we will write custom train/
    evaluation loops to train and evaluate the model, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: To facilitate the model training and evaluation, we are going to create several
    helper functions. First, we will create a function to create inputs and targets
    from the Python DataFrame objects we defined at the beginning (see the next listing).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.6 Preparing the training/validation/test data for model training
    and evaluation
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Define a dictionary for containing train/validation/test data.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Iterate through the train, valid, and test DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Define the encoder inputs as the English text.
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Define the decoder inputs as all of the German text but the last token.
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Define the decoder outputs as all of the German text but the first token.
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Update the dictionary with encoder inputs, decoder inputs, and the decoder
    outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'This function takes in the three data frames, train_df, valid_df, and test_df,
    and performs some transformations on them to return a dictionary that contains
    three keys: train, valid, and test. Under each key, you will find the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Encoder inputs (i.e., an English word sequence)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decoder inputs (i.e., a German word sequence)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decoder outputs (i.e., a German word sequence)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As we stated earlier, we are using a technique known as teacher forcing to
    lift the model’s performance. Therefore, the decoder’s object becomes predicting
    the next word given the previous word(s). For instance, for the example (“I want
    a piece of chocolate cake”, “Ich möchte ein Stück Schokoladenkuchen”), encoder
    inputs, decoder inputs, and decoder outputs become the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[“I”, “want”, “a”, “piece”, “of”, “chocolate”, “cake”] (encoder inputs)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[“Ich”, “möchte”, “ein”, “Stück”] (decoder inputs)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[“möchte”, “ein”, “Stück”, “Schokoladenkuchen”] (decoder outputs)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As can be seen, at every time step, the decoder is predicting the next word
    given the previous word(s). The prepare_data(...) function does this, as the next
    listing shows. Then we will write a function to shuffle the data. This function
    will then be used to shuffle data at the beginning of every epoch during training.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.7 Shuffling the training data
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: ❶ If shuffle_indices are not passed, create shuffled indices automatically.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Shuffle the provided shuffle_indices.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Return shuffled data.
  prefs: []
  type: TYPE_NORMAL
- en: The shuffle_data() function takes the data output by the prepare_data function
    (i.e., encoder inputs, decoder inputs, and decoder outputs). Optionally, it takes
    in a shuffled representation of the data indices. We allow the shuffle indices
    to be passed into the function so that, by shuffling the already shuffled indices,
    you get a new permutation of the order of the data. This is useful for generating
    different shuffle configurations in every epoch during the training.
  prefs: []
  type: TYPE_NORMAL
- en: The shuffle_data() function, if shuffle_indices is not passed, will generate
    a random permutation of the data indices. Data indices are generated by np.arange(en_
    inputs.shape[0]), which creates an ordered number sequence from 0 to the number
    of examples in en_inputs. A random permutation of a given array can be generated
    by calling the np.random.permutation() function on the array. If an array has
    been passed to the shuffle_indices argument, then the array passed into shuffle_indices
    will be shuffled, generating a new shuffled data configuration. Finally, we return
    encoder inputs (en_inputs), decoder inputs (de_inputs), and decoder outputs (de_labels)
    shuffled, as determined by the shuffle_indices array.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will write a function to evaluate the model. In this function, we
    evaluate a given model on given data using the defined batch_size. Particularly,
    we evaluate the machine translation model on three metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Cross-entropy loss*—The standard multiclass cross-entropy loss calculated
    between the prediction probabilities and true targets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Accuracy*—Standard accuracy measured on whether the model predicts the same
    word as the true target at a given time step. In other words, the prediction must
    match the true target exactly, from word to word.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*BLEU score*—A more powerful metric than the accuracy that is based on precision
    but takes into account many n-grams for different values of n.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bilingual Evaluation Understudy (BLEU)
  prefs: []
  type: TYPE_NORMAL
- en: 'BLEU is a metric used to measure the quality of generated text sequences (e.g.,
    translations) by measuring how close the translation is to a given ground truth
    (or multiple ground truths per translation, as the same thing can be said differently
    in languages). It was introduced in the paper “BLEU: A Method for Automatic Evaluation
    of Machine Translation” by Papineni et al. ([https://www.aclweb.org/anthology/P02-1040.pdf](https://www.aclweb.org/anthology/P02-1040.pdf)).
    A BLEU score is a variant of the precision metric that is used to compute the
    similarity between a candidate text (i.e., prediction) against *multiple* reference
    translations (i.e., ground truths).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the BLEU metric, let’s consider the following candidates and
    references:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Candidate 1 (C1): the cat was on the red mat'
  prefs: []
  type: TYPE_NORMAL
- en: 'Candidate 2 (C2): the cat the cat the cat the'
  prefs: []
  type: TYPE_NORMAL
- en: 'Reference 1: the cat is on the floor'
  prefs: []
  type: TYPE_NORMAL
- en: 'Reference 2: there was a cat on the mat'
  prefs: []
  type: TYPE_NORMAL
- en: The precision for candidate 1 and 2 can be computed as
  prefs: []
  type: TYPE_NORMAL
- en: precision = Number of words matched any reference/Number of words in the candidate
  prefs: []
  type: TYPE_NORMAL
- en: which means
  prefs: []
  type: TYPE_NORMAL
- en: Precision(C1) = 6/7 and Precision(C2) = 7/7
  prefs: []
  type: TYPE_NORMAL
- en: This contradicts intuition. Obviously, C1 is a much better choice as a match
    for the references. But the precision tells another story. Therefore, BLEU introduces
    a modified precision. In modified precision, for a given unique word in the candidate,
    you compute the number of times that the word appears in any single reference
    and take the maximum of it. Then you sum this value for all the unique words in
    the candidate text. For example, for C1 and C2, modified unigram precision is
  prefs: []
  type: TYPE_NORMAL
- en: ModPrecision(C1) = (2 + 1 + 1 + 2 + 0 + 1) /7 = 5/7 and ModPrecision(C2) = (2
    + 1)/7 = 3/7
  prefs: []
  type: TYPE_NORMAL
- en: 'This is much better: C1 has a higher precision than C2, which is what we wanted.
    BLEU extends the modified unigram precision to modified n-gram precision and computes
    the modified precision for several n-grams (e.g., unigrams, bigrams, trigrams,
    etc.). Computing modified precision over many different n-grams enables BLEU to
    favor translations or candidates that have longer subsequences matching the reference.'
  prefs: []
  type: TYPE_NORMAL
- en: We will define an object called BLEUMetric, which will compute the BLEU score
    for a given batch of predictions and targets, as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.8 Defining the BLEUMetric for evaluating the machine translation
    model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Get the vocabulary from the fitted TextVectorizer.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Define a StringLookup layer, which can convert token IDs to words.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Get the predicted token IDs.
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Convert token IDs to words using the vocabulary and the StringLookup.
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Strip the string of any extra white spaces.
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Replace everything after the EOS token with a blank.
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Join all the tokens to one string in each sequence.
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Decode the byte stream to a string.
  prefs: []
  type: TYPE_NORMAL
- en: ❾ If the string is empty, add a [UNK] token. If not, it can lead to numerical
    errors.
  prefs: []
  type: TYPE_NORMAL
- en: ❿ Split the sequences into individual tokens.
  prefs: []
  type: TYPE_NORMAL
- en: ⓫ Get the clean versions of the predictions and real sequences.
  prefs: []
  type: TYPE_NORMAL
- en: ⓬ We have to wrap each real sequence in a list to make use of a third-party
    function to compute BLEU.
  prefs: []
  type: TYPE_NORMAL
- en: ⓭ Get the BLEU value for the given batch of targets and predictions.
  prefs: []
  type: TYPE_NORMAL
- en: First, we define an __init__(...) function and several attributes of this class,
    such as vocab, which will have the decoder’s vocabulary returned by the TextVectorization
    layer. Next, we define a TensorFlow StringLookup layer that can return the string
    token given the token ID, and vice versa. All that is required by the StringLookup
    function is the vocabulary of the decoder’s TextVectorization layer. By default,
    the StringLookup layer converts a given string token to a token ID. Setting invert=true
    means that this layer will convert a given token ID to a string token. We also
    need to say that we don't want this layer to automatically add representations
    for out-of-vocabulary words. For that we set num_oov_indices=0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we define a function called calculate_bleu_from_predictions(...) that
    takes a batch of true targets and a batch of prediction probabilities given by
    the model to compute the BLEU score for that batch. First, it computes the predicted
    token IDs by taking the maximum index of each probability vector for each time
    step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, the string tokens are generated using the StringLookup layer defined
    earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Specifically, we pass the token ID matrices (predicted and target) to the StringLookup
    layer. For example, if
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: then
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we define a function to perform some cleaning. The defined function
    will truncate the predictions such that everything after the EOS token is removed
    (inclusive) and will tokenize the sentences into lists of words. The input to
    this function is a tensor, where each row is a list of tokens (i.e., pred_tokens).
    Let’s make this an opportunity to hone our understanding of the TensorFlow string
    operations. TensorFlow has a namespace known as tf.strings ([http://mng.bz/gw7E](http://mng.bz/gw7E))
    that provides a variety of basic string manipulation functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s look at the first line of code that calls several tf.string operations
    on the inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: It executes a sequence of transformations on the input string tensor. First,
    calling tf.strings.join() on tokens will join all the tokens in one column using
    a given separator. For example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: becomes
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Since our sentences are across the rows, we first need to transpose tokens,
    such that sentences are in the columns. Next, tf.strings.regex_replace() is called
    on the tensor, where each item is a sentence resulting from the join. It will
    remove everything followed by the EOS token. This string pattern is captured by
    the eos.* regex. Finally, we strip any starting and ending spaces from the resulting
    strings.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow keeps strings in byte format. In order to convert the string to
    UTF-8 encoding, we have a series of conversions to get it into the correct format:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we have to convert the array to a NumPy array. The elements in this array
    would be in Object format.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we convert this to an array of bytes by calling translations_in_bytes.numpy().astype(np.bytes_).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we decode the array of bytes and convert it to a desired encoding (in
    our case, UTF-8).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The rest of the code is straightforward to understand and has been delineated
    in the code as annotations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we call the clean_text() function on both predicted and real token
    tensors and feed the final results to a third-party implementation of the BLEU
    metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'The clean_text() function will convert both predicted translations and true
    translations (sometimes referred to as *references*) to a list of a list of tokens.
    Here, the outer list represents individual examples, whereas the inner list represents
    the tokens in a given example. As the final step, we will wrap each reference
    in another list structure so that real_tokens becomes a list of a list of a list
    of tokens. This is a necessity, as we will be using a third-party implementation
    of the BLEU metric. compute_bleu, used here, is a third-party implementation found
    in the TensorFlow repository ([http://mng.bz/e7Ev](http://mng.bz/e7Ev)). The compute_bleu()
    function expects two main arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Translation*—A list of a list of tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*References*—A list of a list of list of tokens. In other words, each translation
    can have multiple references, where each reference is a list of tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then it returns
  prefs: []
  type: TYPE_NORMAL
- en: bleu—BLEU score for a given batch of candidate reference pairs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: precisions—Individual n-gram precisions that build up the final BLEU score.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: bp—Brevity penalty (special part of BLEU score that penalizes short candidates).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ratio—Candidate length divided by reference length.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: translation_length—Sum of lengths of candidates in the batch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: reference_length—Sum of lengths of references in the batch. In the case of multiple
    references per candidate, minimum is selected.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s test the compute_bleu() function in action. Let’s imagine a translation
    and a reference. In the first scenario, the translation has [UNK] tokens appearing
    at the beginning and the remaining match the reference completely. In the second
    scenario, we again have two [UNK] tokens, but they appear at the beginning and
    in the middle. Let’s see the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: This will print
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: If you were to compute the word-to-word accuracy of the translation compared
    to the reference, you would get the same result, as only the two [UNK] tokens
    are mismatched. However, the BLEU score is different for the two instances. It
    clearly shows that BLEU prefers the translation that gets more words continuously
    right, without breaks.
  prefs: []
  type: TYPE_NORMAL
- en: We have all the bells and whistles we need to write the training and evaluation
    loops for the model. Let’s first write the evaluation loop (see the next listing),
    as it will be used in the training loop to evaluate the model.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.9 Evaluating the encoder-decoder model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Define the metric.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Get the number of batches.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Evaluate one batch at a time.
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Status update
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Get the inputs and targets.
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Get the evaluation metrics.
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Get the predictions to compute BLEU.
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Update logs that contain loss, accuracy, and BLEU metrics.
  prefs: []
  type: TYPE_NORMAL
- en: ❾ Compute the BLEU metric.
  prefs: []
  type: TYPE_NORMAL
- en: 'The evaluate_model() function takes in several important arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: model—The encoder-decoder model we defined.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: en_inputs_raw—The encoder inputs (text). This will be an array of strings, where
    each string is an English sentence/phrase.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: de_inputs_raw—The decoder inputs (text). This will be an array of strings. It
    will have all the words but the last word in every German translation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: de_labels_raw—The decoder labels (text). This will be an array of strings. It
    will have all the words but the first one in every German translation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: de_vectorizer—The decoder vectorizer to convert decoder_labels_raw (text) to
    token IDs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The function defines a BLEUMetric object we defined earlier. It defines placeholders
    for accumulating loss, accuracy, and BLEU scores for each batch in the given data
    set. Then it goes through each batch of data and does the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Creates the batch input as the corresponding batch of data from en_inputs_ raw
    and de_inputs_raw
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creates the targets as token IDs using de_labels_raw
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluates the model using batch inputs and targets to get the loss and accuracy
    scores of the batch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computes the BLEU score using the true targets and predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accumulates the metrics in the placeholders defined earlier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, after the model has iterated through all the batches of data, it will
    return the mean loss, accuracy, and BLEU scores as the final evaluation benchmark
    for the data set.
  prefs: []
  type: TYPE_NORMAL
- en: 'With that, we jump into defining the training loop (see the next listing).
    We will define a function called train_model() that will do following four core
    tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Train the model with the training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate the model with the training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate the model with validation data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate the model with testing data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Listing 11.10 Training the model using a custom training/evaluation loop
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Define the metric.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Define the data.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Reset metric logs at the beginning of every epoch.
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Shuffle data at the beginning of every epoch.
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Get the number of training batches.
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Train one batch at a time.
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Status update
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Get a batch of inputs (English and German sequences).
  prefs: []
  type: TYPE_NORMAL
- en: ❾ Get a batch of targets (German sequences offset by 1).
  prefs: []
  type: TYPE_NORMAL
- en: ❿ Train for a single step.
  prefs: []
  type: TYPE_NORMAL
- en: ⓫ Evaluate the model to get the metrics.
  prefs: []
  type: TYPE_NORMAL
- en: ⓬ Get the final prediction to compute BLEU.
  prefs: []
  type: TYPE_NORMAL
- en: ⓭ Compute the BLEU metric.
  prefs: []
  type: TYPE_NORMAL
- en: ⓮ Update the epoch's log records of the metrics.
  prefs: []
  type: TYPE_NORMAL
- en: ⓯ Define validation data.
  prefs: []
  type: TYPE_NORMAL
- en: ⓰ Evaluate the model on validation data.
  prefs: []
  type: TYPE_NORMAL
- en: ⓱ Print the evaluation metrics of each epoch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s analyze the function in listing 11.10 to understand its behavior. If
    you take a step back, all it’s doing is calling the functions we defined earlier
    and displaying results. First it prepares the data (using the prepare_data() function)
    by presenting it as a dictionary that has train, valid, and test keys. Next, it
    goes through several epochs of training. In each epoch, it shuffles the training
    data and goes through the data batch by batch. For every training batch, the model
    is trained on the data and evaluated on the same batch. The train data evaluation
    logs are used to compute the training performance, just as we saw in the evaluate_model()
    function. Then, after the training loop finishes, the model is evaluated on the
    validation data. Finally, at the end of training the model, the model is evaluated
    on the test data. You can call the train_ model() function as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'This will output a result similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: NOTE On an Intel Core i5 machine with an NVIDIA GeForce RTX 2070 8 GB, the training
    took approximately 4 minutes and 10 seconds to run five epochs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first observation we can make is that the training has progressed in the
    right direction. Both training and validation metrics have improved over time.
    The model has kicked off with a training accuracy of 24% and a BLEU score of 0.001,
    and ended up with an accuracy of 52% and a BLEU score of 0.14\. During validation,
    the model has increased the accuracy from 33% to 51%, whereas the BLEU score has
    gone from 0.01 to 0.12\. As we did before, let’s save the model so that it can
    later be used in the real world. We will save the model as well as the vocabulary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: When training the model, we used the target (i.e., target language tokens) to
    provide an input to the decoder. This is not possible when using the model to
    translate where the target is unknown. For this, in the following section, we
    modify our trained model while using the same model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 3
  prefs: []
  type: TYPE_NORMAL
- en: 'You have the following function for training a model. Here, en_inputs_raw represents
    the encoder inputs, de_inputs_raw represents decoder inputs, and de_labels_raw
    represents decoder labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: You want to change the code so that, if the mean training BLEU score of a given
    epoch is smaller than the last epoch, the training is stopped. How would you change
    the code?
  prefs: []
  type: TYPE_NORMAL
- en: '11.4 From training to inference: Defining the inference model'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have trained a sequence-to-sequence machine translation model and plan to
    use it to generate German translations for some unseen English phrases. It has
    been trained using teacher forcing, meaning that the words in the translation
    have been fed as inputs. You realize that this is not possible during inference,
    as the task itself is to generate the translation. Therefore, you are going to
    create a new encoder-decoder model using the trained weights of the original model.
    In this model, the decoder operates recursively, where it feeds its previous prediction
    as the input in the next time step. The decoder starts off with the SOS token
    and continues in this manner until it outputs the EOS token.
  prefs: []
  type: TYPE_NORMAL
- en: The ultimate objective of training a machine translator is to use it in the
    real world to translate unseen source language sentences (e.g., English) to a
    target language (e.g., German). However, unlike most of the other models we trained,
    we cannot take this off of the shelf and use it straight away for inference. There’s
    extra effort required to bridge the gap between using a trained model for inference.
    Before coming up with a solution, let’s first understand the underlying problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'During model training, we used teacher forcing to improve the performance of
    the model. In teacher forcing, the decoder is provided target language inputs
    (e.g., German) and asked to predict the next word in the sequence at each time
    step. This means that the trained model relies on two inputs: English sequences
    and German sequences. However, during inference, we don’t have access to the German
    sequences. Our task is to generate those German sequences for given English sequences.
    Therefore, we need to repurpose our trained model to be able to generate German
    translations without relying on whole German sequences to be available.'
  prefs: []
  type: TYPE_NORMAL
- en: The solution is to keep the encoder as it is and introduce several modifications
    to the decoder. We will make our decoder a recursive decoder. By that, we mean
    that the decoder will use its previous predicted word as an input to the next
    time step, until it reaches the end of the sequence (figure 11.4). Specifically,
    we do the following. For a given English sequence
  prefs: []
  type: TYPE_NORMAL
- en: Get the context vector by inputting the English sequence to the encoder.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We first input the SOS token (*x*^d[0]) (denoted by start_token in the code)
    along with the context vector (si^d[1]) and get the decoder’s prediction (*ŷ*^d[1])
    and the output state (so^d[1]).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Until the decoder’s prediction (*ŷ*^d[t] [+1]) is EOS (denoted by end_token
    in the code)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feed the decoder’s prediction (*ŷ*^d[t]) and the output state (so^d[t]) at
    time t as the input (*x*^d[t] [+1]) and the initial state (si^d[t] [+1]) for the
    next time step (t + 1).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Get the decoder’s prediction (*ŷ*^d[t] [+1]) and the output state (so^d[t]
    [+1]) in the next time step.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![11-04](../../OEBPS/Images/11-04.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 Using the sequence-to-sequence model for inference (i.e., generating
    translations from English inputs)
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve this, we have to introduce two major changes to the trained model:'
  prefs: []
  type: TYPE_NORMAL
- en: Separate the encoder and the decoder as separate models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Change the decoder such that it takes an input token and an initial state as
    the input and outputs the predicted token and the next state as the output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s see how we can do this in TensorFlow. First, we will load the model we
    just saved:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: It’s very easy to get the encoder model because we have encapsulated the encoder
    as a nested model in the final model. It can be taken out by calling
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we define two inputs to represent the two inputs of the decoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'As we discussed earlier, we define two inputs: one to represent the input to
    the decoder (d_inp) and another to represent the state input to the decoder’s
    GRU layer (d_state_inp). Analyzing the shapes, d_inp takes in an array of strings,
    just like before. d_state_inp represents the state vector of the GRU model and
    has 256 feature dimensionality.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After we define the inputs, we will retrace all the steps we followed during
    the decoder build in the trained model. However, instead of creating new randomly
    initialized layers, we will get layers from the trained model. Particularly, we
    will have the following layers to flow the inputs through:'
  prefs: []
  type: TYPE_NORMAL
- en: Decoder’s vectorization layer (produces d_vectorized_out)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decoder’s embedding layer (produces d_emb_out)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decoder’s GRU layer (produces d_gru_out)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decoder’s fully connected hidden layer (produces d_dense1_out)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Final prediction layer (produces d_final_out)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It is worth highlighting an important change we are introducing to the GRU
    model. Note that we are setting return_sequences=False to make sure the decoder
    GRU only returns the last output (i.e., not a sequence of outputs). In other words,
    the output of the GRU layer is a [None, 256]-sized tensor. This helps us match
    the shape of the output to the d_state_inp we defined earlier, making it easier
    to build a recursive model. Furthermore, the GRU layer takes d_state_inp as the
    initial_state in the model. This way, we can feed the output state vector as a
    recursive input to the decoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'It’s time to define the final decoder model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: The model takes the d_inp and d_state_inp as inputs and produces d_final_out
    (i.e., the final prediction) and d_gru_out (i.e., the GRU output state) as the
    output. Finally, let’s take a step back and encapsulate the work we did in a single
    function called get_inference_model(), as shown in the next listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.11 Defining the recursive inference model for the machine translation
    model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Load the saved trained model.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Get the encoder model from the loaded model by calling the get_layer() function.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Define the first input to the new inference decoder, an input layer that takes
    a batch of strings.
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Define the second input to the new inference decoder, an input layer that
    takes an initial state to pass to the decoder GRU as the input state.
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Generate the vectorized output of the string input to the decoder.
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Generate the embeddings from the vectorized input.
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Get the decoder’s GRU layer.
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Since we generate one word at a time, we will not need the return_sequences.
  prefs: []
  type: TYPE_NORMAL
- en: ❾ Get the GRU out while using d_state_inp from earlier as the initial state.
  prefs: []
  type: TYPE_NORMAL
- en: ❿ Get the dense output.
  prefs: []
  type: TYPE_NORMAL
- en: ⓫ Get the final output.
  prefs: []
  type: TYPE_NORMAL
- en: ⓬ Define the final decoder.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will then define a function to load back the vocabularies we just saved.
    We saved the vocabularies using the JSON format, and all we need to do to load
    the vocabularies is call json.load() with the opened vocabulary files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: Steaming ahead, we now have everything to generate new translations. As discussed,
    we are going to create a process/function where we input the English sentence
    (denoted by sample_en_text) to the encoder to produce the context vector. Next,
    make a prediction with the SOS token and the context vector to get the first German
    token prediction and the next state output. Finally, recursively feed the outputs
    of the decoder as inputs to the decoder until the predicted token is EOS. The
    following listing delineates this functionality using the inference model we just
    built.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.12 Generating translations with the new inference model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Print the input.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Get the initial state for the decoder.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ The first input word to the decoder will always be the start_token (i.e.,
    it has the value sos).
  prefs: []
  type: TYPE_NORMAL
- en: ❹ We collect the translation in this list.
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Keep predicting until we get the end_token (i.e., it has the value eos).
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Override the previous state input with the new state.
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Get the actual word from the token ID of the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Add that to the translation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s run this for several test inputs in our data set and see what we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: This will output
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: You can run these English phrases/sentences against Google Translate and see
    how closely our model is getting them. For a model trained on a relatively simple
    and small data set, our model is doing very well. The [UNK] token is present in
    the translation because all the less-frequent words are replaced with [UNK] in
    the corpus. Therefore, when the model is uncertain of the word that should be
    filled in a certain position, it is likely to output [UNK].
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 4
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of using the GRU model, you have decided to use an LSTM model. As you
    know, an LSTM model has two states: a cell state and an output state. You have
    built the encoder and are now building the decoder. You are planning to adapt
    the following code to use an LSTM model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: If you set return_state=True in an LSTM layer and call it on some compatible
    input x, the output is as follows
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: where state_h and state_c represent the output state and the cell state respectively.
  prefs: []
  type: TYPE_NORMAL
- en: We have trained a machine translation model using the sequence-to-sequence architecture.
    In the next chapter, we will look at how we can improve this model further by
    using a technique known as attention.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The encoder-decoder pattern is common for sequence-to-sequence tasks such as
    machine translation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The encoder takes in source language inputs and produces a context vector.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The context vector is used by the decoder to produce target language outputs
    (i.e., translation).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The tf.keras.layers.experimental.preprocessing.TextVectorization layer allows
    you to integrate tokenization (i.e., converting strings to list of tokens and
    then to token IDs) into your model. This enables the model to take in strings
    rather than numerical values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When training models on sequence-to-sequence tasks, you can employ teacher
    forcing:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In teacher forcing, the encoder consumes the source language input and produces
    the context vector as usual. Then the decoder consumes and predicts the words
    in the translation. In other words, the decoder is trained in such a way that
    the decoder predicts the next word given the previous word(s) in the translation.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The quality of translations produced by a machine translation model is measured
    using the BLEU score:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The BLEU score uses a modified version of the precision metric along with measuring
    precision on different n-grams of the translation to come up with a score.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once a model is trained using teacher forcing, a separate inference model needs
    to be defined using the trained weights:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This inference model has the same encoder, but the decoder takes in the previously
    predicted word as an input for the next step and recursively predicts words until
    a predefined ending criterion is met.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Answers to exercises
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Exercise 1**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: '**Exercise 2**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: '**Exercise 3**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: '**Exercise 4**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
