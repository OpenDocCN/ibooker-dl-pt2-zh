- en: 10 Adopting PyTorch Lightning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 采用 PyTorch Lightning
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Implementing PyTorch Lightning to reduce boilerplate code
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 PyTorch Lightning 减少样板代码
- en: Adding training, validation, and test support for the DC taxi model
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为 DC 出租车模型添加训练、验证和测试支持
- en: Analyzing DC taxi model training and validation using pandas
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 pandas 分析 DC 出租车模型的训练和验证
- en: Thus far, you have written your own implementation related to training and testing
    your machine learning model. However, much of the code you wrote was unrelated
    to your machine learning model architecture and could have applied to a broad
    range of distinct models. Building on this observation, this chapter introduces
    you to PyTorch Lightning, a framework that can help you reduce the amount of boilerplate
    engineering code in your machine learning system, and consequently help you focus
    on evolving your model design and implementation.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经编写了与训练和测试您的机器学习模型相关的实现。然而，您编写的大部分代码与您的机器学习模型架构无关，可以适用于广泛范围的不同模型。基于这一观察结果，本章介绍了
    PyTorch Lightning，这是一个可以帮助您减少机器学习系统中样板工程代码量的框架，并因此帮助您专注于发展您的模型设计和实现的框架。
- en: 10.1 Understanding PyTorch Lightning
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解 PyTorch Lightning
- en: This section introduces the PyTorch Lightning framework for your PyTorch DC
    taxi fare estimation model and teaches you the steps involved in enabling PyTorch
    Lightning training, validation, and test features.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了 PyTorch Lightning 框架，用于您的 PyTorch DC 出租车车费估算模型，并教您如何启用 PyTorch Lightning
    训练、验证和测试特性的步骤。
- en: Thus far, you have implemented a sizable portion of Python and PyTorch boilerplate
    code for your machine learning model. This meant that only a few parts of your
    implementation were model specific, such as the code to
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 目前为止，您已经为您的机器学习模型实现了大部分的 Python 和 PyTorch 样板代码。这意味着你的实现中只有少部分是模型特定的，比如
- en: Package the feature values as tensors
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将特征值打包为张量
- en: Configure the neural net layers
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置神经网络层
- en: Calculate the tensors for the loss
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算损失的张量
- en: Report on the model metrics
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型指标报告
- en: Much of the remaining code, such as the code to iterate over the training batches,
    validation batches, and epochs of training, is largely boilerplate, meaning that
    it can be re-used unmodified across various changes to the model-specific code.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的大部分代码，比如迭代培训批次、验证批次和培训纪元的代码，主要是样板代码，这意味着它可以在模型特定的代码发生各种变化时不加修改地重复使用。
- en: 'As your machine learning system grows more complex, so does the boilerplate
    code in your system’s implementation. For example, mature machine learning systems
    require periodic saving (checkpointing) of the model’s weight values to storage
    in order to enable reproducibility. Having model checkpoints also enables machine
    learning training pipelines to resume from a pre-trained model. Other examples
    include the code involved in integration with hyperparameter optimization services,
    metric tracking, and other experiment management tools that control repeated executions
    of a machine learning pipeline. This should not come as a surprise: recall from
    chapter 1 that over 90% of the components of a production machine learning system
    are complementary to the core machine learning code.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 随着您的机器学习系统变得更加复杂，系统实现中的样板代码也会变得更加复杂。例如，成熟的机器学习系统需要周期性保存（检查点）模型的权重值到存储介质，以便实现可复制性。拥有模型检查点还可以使机器学习训练流程从预训练模型中恢复。其他例子包括与超参数优化服务、指标跟踪和控制机器学习流程的其他实验管理工具集成的代码。这不应该令人惊讶：回想一下第一章中提到的，一个生产级机器学习系统的组件中有超过
    90% 是辅助于核心机器学习代码的。
- en: 'The PyTorch Lightning framework ([https://www.pytorchlightning.ai](https://www.pytorchlightning.ai))
    aims to make PyTorch developers more productive by helping them focus on developing
    core machine learning code instead of getting distracted by the boilerplate. In
    case of the DC taxi model, adopting PyTorch Lightning is straightforward. Before
    starting, you need to ensure that you have the pip package for PyTorch Lightning
    installed by running the following in your shell environment:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 'PyTorch Lightning 框架 ([https://www.pytorchlightning.ai](https://www.pytorchlightning.ai))
    的目标是通过帮助开发者专注于开发核心机器学习代码而不被样板代码分散注意力来提高 PyTorch 开发者的生产力。就 DC 出租车模型而言，采用 PyTorch
    Lightning 是直接的。在开始之前，您需要确保已经在您的 shell 环境中运行以下内容安装了 PyTorch Lightning 的 pip 包:'
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: PyTorch Lightning is a comprehensive framework with a sizable feature set for
    machine learning model development. This book does not aim to replace existing
    PyTorch Lightning tutorials or documentation; instead, the upcoming sections focus
    on the features of the framework you can adopt for the DC taxi model.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.1 Converting PyTorch model training to PyTorch Lightning
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section teaches you about the PyTorch Lightning __init__, training_step
    and configure_optimizers methods and then illustrates how to implement these methods
    for the DC taxi model and how to train the PyTorch Lighting—based DC taxi model
    using a small, sample training data set.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Assuming that the PyTorch Lighting package is installed correctly in your environment,
    you can implement a minimal, trainable version of the DC taxi model.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.1 A basic PyTorch Lightining DC taxi model with support
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Import the PyTorch Lightning library and alias it as pl.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Use torch.float64 for dtype of the model parameters.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: ❸ PyTorch Lightning models must extend from LightningModule.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: ❹ The **kwargs are used to pass hyperparameters to the model.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: ❺ The LightningModule subclass must call parent __init__ first.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Save hyperparameters from **kwargs to self.hparams.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Set the pseudorandom number generator per hyperparameter settings.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Use a simple linear regression model for this illustration.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: ❾ Re-use functions from chapter 7 for the batchToXy . . .
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: ❿ . . . and forward implementations.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: ⓫ Use PyTorch Lightning built-in logging for MSE and RMSE measures.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: ⓬ The training_step method must return the loss tensor.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: ⓭ A LightningModule subclass must have a configure_optimizers method.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: ⓮ Return a configured optimizer instance specified by hyperparameters.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: ⓯ Instantiate a PyTorch Lightning version of the DcTaxiModel as model.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that the DcTaxiModel class inherits from the base pl.LightningModule
    class, which requires a separate initialization in the __init__ method by the
    super() .__init__() method call. The rest of the __init__ method of the class
    is simplified here for illustration and to highlight the following key concepts:
    storage of a reference to the model’s hyperparameters in self.hparams as well
    as the instantiation of the model parameters in the self.layers instance.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: The training_step method is the workhorse of the PyTorch Lightning implementation,
    performing the forward step through the model layers, computing the loss, and
    returning the loss value. Notice that it relies on the batchToXy method (introduced
    in chapter 7) responsible for converting a batch of training examples into the
    format suitable for model training.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: The conversion amounts to eliminating any dimensions that have a shape of 1
    using the squeeze_ method. For example, a tensor with a shape of [1, 128, 5, 1]
    is reshaped to [128, 5] after the application of squeeze_. The use of squeeze_
    (with the trailing underscore) instead of the squeeze method is a minor performance
    optimization. Recall from chapter 5 that the trailing underscore in squeeze_ indicates
    that this PyTorch method performs the operation in place, mutating the tensor,
    as opposed to returning a new tensor instance.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 转换就是使用 squeeze_ 方法消除任何形状为 1 的维度。例如，形状为 [1, 128, 5, 1] 的张量在 squeeze_ 应用后被重新调整为
    [128，5]。在 squeeze_ 中使用下划线进行了小幅度的性能优化。回想一下第五章所讲的，squeeze_ 中的下划线表示 PyTorch 方法将就地执行操作，对张量进行突变，而不是返回新的张量实例。
- en: The DcTaxiModel implementation assumes that the first column in the tensor is
    the label with the remaining columns as features, so the concluding portion of
    the batchToXy code simply aliases the label as y and features as X, returning
    the result.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: DcTaxiModel 的实现假定张量中的第一列是标签，其余列是特征。因此，在 batchToXy 代码的结尾部分，只需将标签简单地别名为 y，将特征别名为
    X 并返回结果。
- en: The calls to self.log in the training_step method report the training MSE and
    RMSE values computed by the model. As explained in chapter 5, in the PyTorch tensor
    API the item method of a PyTorch scalar tensor returns a regular Python value
    instead of a tensor. Hence, the values that are logged using self.log are Python
    numerics rather than PyTorch tensors. The self.log method of PyTorch Lightning
    is a generic API for an extensible logging framework, which will be covered in
    more detail later in this chapter.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在 training_step 方法中调用 self.log 报告模型计算出的训练 MSE 和 RMSE 值。正如第五章所解释的那样，在 PyTorch
    张量 API 中，标量张量的 item 方法返回常规的 Python 值而不是张量。因此，使用 self.log 记录的值是 Python 数值而不是 PyTorch
    张量。PyTorch Lightning 的 self.log 方法是一个可扩展的日志框架的通用 API，稍后在本章中有更详细的介绍。
- en: The configure_optimizers method in the example uses a dictionary of optimizers
    in order to enable the model to switch between different optimization algorithms
    (Adam versus SGD) based on the value of the optimizer hyperparameter. Although
    this implementation of model training does not yet use hyperparameter optimization,
    the dictionary-based lookup approach shown in configure_optimizers ensures that
    the model code does not need to be changed at the later stages of development
    when hyperparameter optimization is enabled.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 示例中的 configure_optimizers 方法使用优化器字典，以便使模型能够根据优化器超参数的值在不同的优化算法（Adam 和 SGD）之间切换。尽管这种模型训练的实现尚未使用超参数优化，但在
    configure_optimizers 中展示的基于字典查找的方法确保了当后续开发启用超参数优化时，模型代码无需改变。
- en: In PyTorch Lightning, training of a model is performed using an instance of
    a Trainer.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyTorch Lightning 中，使用一个 Trainer 实例来训练模型。
- en: Listing 10.2 PyTorch Lightining Trainer to train a subclass
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 代码清单 10.2：使用 PyTorch Lightining Trainer 训练子类
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ CSVLogger is used to illustrate analysis with pandas.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用 CSVLogger 可以用 pandas 进行分析。
- en: ❷ The seed hyperparameter is used to uniquely identify the model log.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ seed 超参数被用于唯一标识模型日志。
- en: ❸ Use multiple GPUs for training when available.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在有多个 GPU 可用时使用多个 GPU 进行训练。
- en: ❹ Use 1 since the training duration is controlled by max_batches.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 在此将其设置为 1，因为训练持续时间由 max_batches 控制。
- en: ❺ Use max_batches to set the number of the training iterations.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用 max_batches 来设置训练迭代的次数。
- en: ❻ Ensure that every call to self.log is persisted in the log.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 确保每次调用 self.log 的记录都被保存在日志中。
- en: ❼ Persist the values sent to self.log based on csvLog setup.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 基于 csvLog 设置将发送到 self.log 的值保存下来。
- en: 'The hyperparameter values can be applied to the machine learning pipeline and
    not just to the model: for example, the max_batches hyperparameter controls the
    duration of the model training. As you will see in the remainder of this chapter,
    hyperparameter values can be used throughout the stages of the machine learning
    pipeline. The max_epochs settings in the code example is designed to ensure that
    the training pipeline can support both Iterable as well as Map PyTorch data sets.
    Recall from chapter 7 that IterableDataset instances have a variable number of
    examples per training data set; hence, training with this category is controlled
    by limiting the number of training batches. This number is specified using the
    limit_train_batches parameter of the Trainer.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: The progress_bar_refresh_rate and weight_summary settings in listing 10.2 are
    reasonable defaults to use with the Trainer to minimize the amount of the logging
    information reported during training. If you prefer to have a report on the training
    model parameters, you can change weights_summary to "full" to report all weights,
    or to "top" to report just the weights of the top (the ones connected to the trunk
    of the model) layers in the model. Similarly, the progress_bar_refresh_rate can
    be changed to an integer value representing the frequency (in terms of the number
    of training steps) for how often to redraw a progress bar showing progress toward
    training completion.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: To provide the model with the training examples, you can use the ObjectStorageDataset
    introduced in chapter 7\. Before executing the code snippet in the next example,
    ensure that you have the Kaen framework installed using
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, to execute just the training of the model, you can invoke the fit method
    on the instance of pl.Trainer, passing in a PyTorch DataLoader with the training
    examples:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the example, the sample of the DC taxi data set available from [http://mng.bz/nr9a](http://mng.bz/nr9a)
    is used to simply illustrate how to use PyTorch Lightning. In the next chapter,
    you will see how to scale to larger data sets by simply changing the URL string
    passed to osds.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Since during training the loss and metric values are logged to a CSV file, once
    the training is finished, you can load the values into a pandas DataFrame and
    plot the results using
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: which should output a graph resembling figure 10.1.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '![10-01](Images/10-01.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 On a small sample, the trivial linear regression model converges
    as expected.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: According to figure 10.1, the naive linear regression model from listing 10.1
    converges to a consistent loss value. To examine the details of the loss value
    at the trailing 25 steps of the convergence, you can again take advantage of pandas
    DataFrame APIs,
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: which plots figure 10.2.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '![10-02](Images/10-02.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 Trailing 25 steps of training converge to a RMSE of roughly 4.0.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: You can confirm that toward the last 25 steps of training, the model converges
    at an average RMSE of about 4.0\. Even with the trivial linear regression model,
    this should not come as a surprise since this illustration used a tiny training
    sample.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: At this point, it is useful to introduce a build function that can be invoked
    to instantiate, train, and later validate, as well as test the model. For convenience,
    here’s the entire implementation of this version of the model with the training
    steps encapsulated.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.3 Basic PyTorch Lightining DC taxi model
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 10.1.2 Enabling test and reporting for a trained model
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section describes the test_step method for a PyTorch Lightning model and
    how the method can be used to test and report on the metrics of a trained model.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the model is trained, the Trainer instance can also be used to report
    on the model loss and metrics against a test data set. However, in order to support
    testing in PyTorch Lightning, a LightningModule subclass must be extended with
    an implementation of a test_step method. The following code snippet describes
    the corresponding implementation for DcTaxiModel:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Ignore the gradient graph during testing for better performance.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Use test_mse instead of train_mse . . .
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: ❸ . . . and test_rmse instead of train_rmse when logging test measurements.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: The PyTorch Lightning test_step does not require any return values; instead,
    the code is expected to report the metrics computed using a trained model. Recall
    from the discussion of autodiff in chapter 6 that maintaining the backward graph
    of the gradients carries additional performance overhead. Since the model gradients
    are not needed during model testing (or validation), the forward and mse_loss
    methods are invoked in the context of with pt.no_grad():, which disables the tracking
    needed for gradient calculations of the loss.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Outside of the minor change related to renaming the logged loss and metric measures
    (e.g., test_rmse versus train_rmse), the implementation of test_step logging is
    identical to the one from training_step function.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'To introduce configuration changes to the Trainer instance for testing and
    to create the DataLoader for the test data, the build function needs to be modified
    ❶—❹:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Pass URL globs to instantiate DataLoader for training and test data.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Use the test data set only once to report on the loss and metric measures.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Use test_glob to instantiate the train_ds . . .
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: ❹ . . . and train_dl instances.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Test and report on the model performance using the Trainer.test method.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: After executing the training and test using the updated model and build implementation
    using
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'you should get test results resembling the following:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 10.1.3 Enabling validation during model training
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section illustrates how to use the validation_step method in a LightningModule
    subclass to enable support for PyTorch model validation.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantages of using PyTorch Lightning become more evident when you modify
    your implementation to support recurring validation steps during training. For
    example, to add model validation to the DcTaxiModel implementation, you simply
    introduce the validation_step method:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The following code describes the remaining changes needed to configure the
    trainer instance to perform validation over a fixed-sized data set (as opposed
    to k-fold cross validation):'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Validate just 1 batch of validation DataLoader data.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Validate before training to ensure the validation data set is available.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Validate after every 20 training iterations (steps) of gradient descent.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: The limit_val_batches acts similarly to limit_train_batches in specifying the
    number of batches from the validation Dataset to use for validation. The num_sanity_val_
    steps parameter to the Trainer controls a feature of PyTorch Lightning that uses
    the validation data set to ensure that the model, as well as the validation DataLoader,
    are instantiated correctly and are ready for training. In the example, setting
    the value of num_sanity_val_steps to 1 performs a single validation step and reports
    the corresponding metrics. The val_check_interval parameter specifies that at
    most after every 20 iterations of training, PyTorch Lightning should perform validation
    using the number of batches specified by the limit_val_batches parameters. The
    use of the min function with val_check_interval ensures that if the hyperparameter
    for max_batches is set to be less than 20, the validation is performed at the
    conclusion of training.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.4 PyTorch Lightning DC taxi linear regression model
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: You can train, validate, and test the entire model by running
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: If you load the resulting logs from the logs/dctaxi/version_1686523060 folder
    as a pandas DataFrame and plot the result using
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: you should observe a graph resembling figure 10.3\. Since the val_check_interval
    parameter was set to 20, most of the values for the val_rmse_step column in the
    data frame are missing. The fillna(method='ffill') call fills the missing values
    forward, for example by setting missing values for steps 81, 82, and so on, based
    on the validation RMSE from step 80.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '![10-03](Images/10-03.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 Despite reasonable test performance, validation RMSE signals overfitting.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: The lackluster performance against the validation data set, as shown in figure
    10.3, points at the likelihood of the model overfitting on the training data set.
    Before putting the code in production, the model implementation should be refactored
    to become more generalizable and less reliant on memorization of the training
    data set. This means that to move forward, you need a model development approach
    with more comprehensive support for experimentation and hyperparameter optimization.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Adopting the PyTorch Lightning framework can help you refactor your machine
    learning implementation to reduce the fraction of incidental, boilerplate code
    and to focus on model-specific development.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 采用 PyTorch Lightning 框架可以帮助您重构机器学习实现，减少附带的样板代码比例，并专注于模型特定开发。
- en: In a PyTorch Lightning-based implementation for a machine learning model, you
    can incrementally add support for model training, validation, and test support,
    along with pluggable features such as a logging framework for analysis.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在基于 PyTorch Lightning 的机器学习模型实现中，您可以逐步添加对模型训练、验证和测试的支持，以及可插拔的特性，例如用于分析的日志框架。
- en: The CSVLogger from PyTorch Lightning saves the results of model training, validation,
    and test features to CSV files that you analyzed using pandas.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch Lightning 中的 CSVLogger 将模型训练、验证和测试结果保存到 CSV 文件中，您可以使用 pandas 进行分析。
