- en: '10 Natural language processing with TensorFlow: Language modeling'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用TensorFlow进行自然语言处理：语言建模
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Implementing an NLP data pipeline with TensorFlow
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TensorFlow构建自然语言处理数据管道
- en: Implementing a GRU-based language model
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现基于GRU的语言模型
- en: Using a perplexity metric for evaluating language models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用困惑度指标评估语言模型
- en: Defining an inference model to generate new text from the trained model
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义一个推断模型，从训练好的模型中生成新的文本
- en: Implementing beam search to uplift the quality of generated text
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现束搜索以提升生成文本的质量
- en: In the last chapter, we discussed an important NLP task called sentiment analysis.
    In that chapter, you used a data set of video game reviews and trained a model
    to predict whether a review carried a negative or positive sentiment by analyzing
    the text. You learned about various preprocessing steps that you can perform to
    improve the quality of the text, such as removing stop words and lemmatizing (i.e.,
    converting words to a base form; e.g., plural to singular). You used a special
    type of model known as long short-term memory (LSTM). LSTM models can process
    sequences such as sentences and learn the relationships and dependencies in them
    to produce an outcome. LSTM models do this by maintaining a state (or memory)
    containing information about the past, as it processes a sequence one element
    at a time. The LSTM model can use the memory of past inputs it has seen along
    with the current input to produce an output at any given time.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了一个重要的自然语言处理任务，称为情感分析。在那一章中，你使用了一个视频游戏评论数据集，并训练了一个模型来分析文本，预测评论是否带有负面或正面情感。你学习了各种预处理步骤，可以执行这些步骤以提高文本的质量，例如去除停用词和词形还原（即将单词转换为基本形式；例如，将复数转换为单数）。你使用了一种特殊类型的模型，称为长短期记忆（LSTM）。LSTM模型可以处理诸如句子之类的序列，并学习它们之间的关系和依赖以产生结果。LSTM模型通过保持一个包含有关过去信息的状态（或记忆）来执行这一任务，因为它逐个元素地处理序列。LSTM模型可以使用它已经看到的过去输入的记忆以及当前输入，在任何给定时间产生一个输出。
- en: In this chapter, we will discuss a new task known as *language modeling*. Language
    modeling has been at the heart of NLP. Language modeling refers to the task of
    predicting the next word given a sequence of previous words. For example, given
    the sentence, “I went swimming in the ____,” the model would predict the word
    “pool.” Ground-shattering models like BERT (bidirectional encoder representation
    from Transformers, which is a type of Transformer-based model) are trained using
    language modeling tasks. This is a prime example of how language modelling can
    help to actualize innovative models that go on to be used in a plethora of areas
    and use cases.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论一项称为*语言建模*的新任务。语言建模一直是自然语言处理的核心。语言建模是指在给定一系列先前单词的情况下预测下一个单词的任务。例如，给定句子“I
    went swimming in the ____”，模型将预测单词“pool”。像BERT（双向编码器表示来自Transformers的模型，这是一种基于Transformer的模型）这样的开创性模型是使用语言建模任务进行训练的。这是语言建模如何帮助实现创新模型，并在各种领域和用例中得到应用的一个典型例子。
- en: In my opinion, language modeling is an underdog in the world of NLP. It is not
    appreciated enough, mostly due to the limited use cases the task itself helps
    to realize. However, language modeling can provision the much-needed linguistic
    knowledge (e.g., semantics, grammar, dependency parsing, etc.) to solve other
    downstream use cases (e.g., information retrieval, question answering, machine
    translation, etc.). Therefore, as an NLP practitioner, you must understand the
    language modeling task.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在我看来，语言建模在自然语言处理领域是一个被忽视的角色。它没有得到足够的重视，主要是因为该任务本身的使用案例有限。然而，语言建模可以为解决其他下游使用案例（例如信息检索、问答、机器翻译等）提供急需的语言知识（例如语义、语法、依赖解析等）。因此，作为一个自然语言处理从业者，你必须理解语言建模任务。
- en: In this chapter, you will build a language model. You will learn about the various
    preprocessing steps involved, such as using n-grams instead of full words as features
    for the model to reduce the size of the vocabulary. You can convert any text to
    n-grams by splitting it every n characters (e.g., if you use bi-grams, aabbbccd
    becomes aa, bb, bc, and cd). You will define a tf.data pipeline that will do most
    of this preprocessing for us. Next, you will use a close relative of the LSTM
    model known as *gated recurrent unit* (GRU) to do the language modeling task.
    GRU is much simpler than the LSTM model, making it faster to train while maintaining
    similar performance to the LSTM model. We will use a special metric called perplexity
    to measure how good our model is. Perplexity measures how surprised the model
    was to see the next word in the corpus given the previous words. You will learn
    more about this metric later in the chapter. Finally, you will learn about a technique
    known as *beam search*, which can uplift the quality of the text generated by
    the model by a significant margin.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将构建一个语言模型。您将学习有关各种预处理步骤的知识，例如使用n-grams而不是完整单词作为模型的特征来减小词汇量的大小。您可以通过每n个字符分割文本来将任何文本转换为n-grams（例如，如果使用bi-grams，则aabbbccd变为aa、bb、bc和cd）。您将定义一个tf.data数据流水线，它将为我们完成大部分预处理工作。接下来，您将使用一种被称为门控循环单元（GRU）的LSTM模型的密切关联方法来执行语言建模任务。GRU比LSTM模型简单得多，训练速度更快，同时保持与LSTM模型相似的性能。我们将使用一种称为困惑度的特殊度量标准来衡量模型的好坏。困惑度衡量了模型在给定前几个词的情况下看到下一个词时的惊讶程度。在本章后面的部分，您将更多地了解这个度量标准。最后，您将学习一种称为波束搜索的技术，可以显著提高模型生成的文本的质量。
- en: 10.1 Processing the data
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.1 处理数据
- en: You’ve been closely following a new generation of deep learning models that
    have emerged, known as Transformers. These models have been trained using language
    modeling. It is a technique that can be used to train NLP models to generate stories/
    Python code/movie scripts, depending on the training data used. The idea is that
    when a sequence of n words, predict the n + 1 word. The training data can easily
    be generated from a corpus of text by taking a sequence of text as the input and
    shifting it right by 1 to generate the targets. This can be done at a character
    level, word level, or n-gram level. We will use two-grams for the language modeling
    task. We will use a children’s story data set from Facebook known as bAbI ([https://research.fb.com/downloads/babi/](https://research.fb.com/downloads/babi/)).
    You will create a TensorFlow data pipeline that performs these transformations
    to generate inputs and targets from text.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 您一直在密切关注一批新一代的深度学习模型，被称为Transformer。这些模型是使用语言建模进行训练的。这是一种可以用来训练自然语言处理模型以生成故事/Python代码/电影剧本的技术，具体取决于使用的训练数据。其思想是当一个由n个单词组成的序列中，预测第n+1个单词。训练数据可以从文本语料库中轻松生成，只需将一个文本序列作为输入，将其向右移动1位以生成目标序列。这可以在字符级、单词级或n-gram级别上进行。我们将使用两个连续的单词作为语言建模任务的n-gram。我们将使用Facebook的一个名为bAbI的儿童故事数据集（[https://research.fb.com/downloads/babi/](https://research.fb.com/downloads/babi/)）。您将创建一个TensorFlow数据流水线，用于执行这些转换以生成输入和目标序列。
- en: 10.1.1 What is language modeling?
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.1 什么是语言建模？
- en: We have briefly discussed the task of language modeling. In a nutshell, language
    modeling, for the text *w*[1], *w*[2], ..., *w*[n][-1], *w*[n], where *w*[i] is
    the *i*^(th) word in the text, computes the probability of *w*[n] given *w*[1],
    *w*[2], ..., *w*[n][-1]. In mathematical notation
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经简要讨论了语言建模的任务。简而言之，语言建模对于文本*w*[1]、*w*[2]，...、*w*[n][-1]、*w*[n]，其中*w*[i]是文本中的第*i*个词，计算给定*w*[1]、*w*[2]，...、*w*[n][-1]时，*w*[n]的概率。在数学上表示为：
- en: '*P*(*w* [n]|*w*[1], *w*[2], ..., *w*[n][-1])'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*w* [n]|*w*[1], *w*[2], ..., *w*[n][-1])'
- en: In other words, it predicts *w*[n] given *w*[1], *w*[2], ..., *w*[n][-1]. When
    training the model, we train it to maximize this probability; in other words
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，它预测给定*w*[1]、*w*[2]，...、*w*[n][-1]时的*w*[n]。在训练模型时，我们训练它最大化这个概率；换句话说：
- en: argmax[W] *P*(*w* [n]|*w*[1], *w*[2], ..., *w*[n][-1])
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: argmax[W] *P*(*w* [n]|*w*[1], *w*[2], ..., *w*[n][-1])
- en: where the probability is computed using a model that has the trainable weights/
    parameters W. This computation becomes computationally infeasible for large texts,
    as we need to look at it from the current word all the way to the very first word.
    To make this computationally realizable, let’s use the *Markov property*, which
    states that you can approximate this sequence with limited history; in other words
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种概率计算中，使用了具有可训练权重/参数`W`的模型。对于大文本来说，这种计算变得计算上不可行，因为我们需要从当前单词一直回溯到第一个单词。为了使其计算可行，让我们使用*马尔科夫性质*，它指出你可以使用有限的历史来近似这个序列；换句话说
- en: '*P*(*w* [n]|*w*[1], *w*[2], ..., *w*[n][-1]) ≈ *P*(*w* [n]|*w*[k], *w*[k+1],
    ..., *w*[n][-1]) for some *k*'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*w* [n]|*w*[1], *w*[2], ..., *w*[n][-1]) ≈ *P*(*w* [n]|*w*[k], *w*[k+1],
    ..., *w*[n][-1]) for some *k*'
- en: As you can imagine, the smaller the *k*, the better the approximation is.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所想象的那样，*k*越小，近似效果越好。
- en: Cloze task
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 闭合任务
- en: Transformer models like BERT use a variant of language modeling called m*asked
    language modeling*. Masked language modeling is inspired by the *Cloze* task or
    the Cloze test. The idea is to ask the student, when given a sentence with one
    or more blanks, to fill in the blanks. This has been used in language assessment
    tests to measure the linguistic competency of students. In masked language modeling,
    the model becomes the student. Words are removed from inputs at random, and the
    model is asked to predict the missing word. This forms the foundation of the training
    process used in models like BERT.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 类似BERT的Transformer模型使用了一种称为*m*asked language modeling*的语言模型变体。蒙面语言建模受到了*闭合*任务或者填空测试的启发。这个想法是在给出一个带有一个或多个空白处的句子时，要求学生填写空白处的单词。这在语言评估测试中被用来衡量学生的语言能力。在蒙面语言建模中，模型变成了学生。单词随机地从输入中删除，并且要求模型预测缺失的单词。这构成了像BERT这样的模型中使用的训练过程的基础。
- en: 10.1.2 Downloading and playing with data
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.2下载和玩弄数据
- en: As the very first step, let’s download the data set using the code in the following
    listing.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一步，让我们使用以下列表中的代码下载数据集。
- en: Listing 10.1 Downloading the Amazon review data set
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.1下载亚马逊评论数据集
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ If the tgz file containing data has not been downloaded, download the data.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 如果尚未下载包含数据的`tgz`文件，则下载数据。
- en: ❷ Write the downloaded data to the disk.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将下载的数据写入磁盘。
- en: ❸ If the tgz file is available but has not been extracted, extract it to the
    given directory.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 如果`tgz`文件可用但尚未解压缩，则将其解压缩到给定的目录。
- en: 'Listing 10.1 will download the data to a local folder if it doesn’t already
    exist and extract the content. If you look in the data folder (specifically, data/lm/CBTest/data),
    you will see that it has three text files: cbt_train.txt, cbt_valid.txt, and cbt_test.txt.
    Each file contains a set of stories. We are going to read these files into memory.
    We will define a simple function to read these files into memory in the next listing.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据不存在，列表10.1将会下载到本地文件夹并解压缩内容。如果您查看数据文件夹（具体地说，data/lm/CBTest/data），您会看到它有三个文本文件：cbt_train.txt，cbt_valid.txt和cbt_test.txt。每个文件包含一组故事。我们将在内存中读取这些文件。我们将在下一个列表中定义一个简单的函数来将这些文件读入内存。
- en: Listing 10.2 Reading the stories in Python
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.2在Python中读取故事
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Define a list to hold all the stories.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义一个列表来保存所有的故事。
- en: ❷ Define a list to hold a story.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义一个列表来保存一个故事。
- en: ❸ Whenever, we encounter a line that starts with _BOOK_TITLE, it’s a new story.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 当我们遇到以_BOOK_TITLE开头的行时，它是一个新的故事。
- en: ❹ If we saw the beginning of a new story, add the already existing story to
    the list stories.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 如果我们看到了一个新故事的开始，将已存在的故事添加到故事列表中。
- en: ❺ Reset the list containing the current story.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 重置包含当前故事的列表。
- en: ❻ Append the current row of text to the list s.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将当前文本行添加到列表`s`中。
- en: ❼ Handle the edge case of the last story remaining in s once the loop is over.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 在循环结束后处理最后一个故事仍然存在于`s`中的边界情况。
- en: 'This code opens a given file and reads it row by row. We do have some additional
    logic to break the text into individual stories. As said earlier, each file contains
    multiple stories. And we want to create a list of strings at the end, where each
    string is a single story. The previous function does that. Next, we can read the
    text files and store them in variables like this:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码打开给定的文件并逐行读取它。我们还有一些额外的逻辑来将文本分割成单独的故事。正如前面所说，每个文件包含多个故事。我们想在最后创建一个字符串列表，其中每个字符串是一个单独的故事。前一个函数就是这样做的。接下来，我们可以读取文本文件并将它们存储在变量中，如下所示：
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here, stories will contain the training data, val_stories will contain the
    validation data, and finally, test_stories will contain test data. Let’s quickly
    look at some high-level information about the data set:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，故事将包含训练数据，val_stories 将包含验证数据，最后，test_stories 将包含测试数据。让我们快速查看一些关于数据集的高级信息：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This code checks how many stories are in each data set and prints the first
    100 characters in the 11^(th) story in the training set:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码检查每个数据集中有多少个故事，并打印训练集中第 11 个故事的前 100 个字符：
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Out of curiosity, let’s also analyze the vocabulary size we have to work with.
    To analyze the vocabulary size, we will first convert our list of strings to a
    list of lists of strings, where each string is a single word. Then we can leverage
    the built-in Counter object to get the word frequency of the text corpus. After
    that, we will create a pandas Series object with the frequencies as the values
    and words as indices and see how many words occur more than 10 times:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 出于好奇，让我们也分析一下我们要处理的词汇量。为了分析词汇量，我们将首先将我们的字符串列表转换为字符串列表的列表，其中每个字符串都是一个单词。然后，我们可以利用内置的
    Counter 对象来获取文本语料库的单词频率。之后，我们将创建一个 pandas Series 对象，其中频率作为值，单词作为索引，并查看有多少单词出现超过
    10 次：
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This will return
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Nearly 15,000 words; that’s quite a vocabulary—and that’s just the words that
    appear more than 10 times. In the previous chapter, we dealt with a vocabulary
    of approximately 11,000 words. So why should we be worried about the extra 4,000?
    Because more words mean more features for the model, and that means a larger number
    of parameters and more chances of overfitting. The short answer is it really depends
    on your use case.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 近 15,000 个单词；这是相当大的词汇量——而且这只是出现超过 10 次的单词。在上一章中，我们处理了大约 11,000 个单词的词汇。那么为什么我们应该担心额外的
    4,000 个单词呢？因为更多的单词意味着模型的特征更多，这意味着参数的数量更多，过拟合的机会也更多。简短的答案是这取决于你的用例。
- en: For example, in the sentiment analysis model we had in the last chapter, the
    final prediction layer was a single-node fully connected layer, regardless of
    the vocabulary size. However, in language modeling, the final prediction layer’s
    dimensionality depends on the vocabulary size, as the final goal is to predict
    the next word. This is done through a softmax layer that represents the probabilistic
    likelihood of the next word over the whole vocabulary, given a sequence of words.
    Not only the memory requirement, but also the computational time, increase as
    the softmax layer grows. Therefore, it is worthwhile investigating other techniques
    to reduce the vocabulary size.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在上一章中我们拥有的情感分析模型中，最终预测层是一个单节点完全连接层，而不管词汇量大小如何。然而，在语言建模中，最终预测层的维度取决于词汇量大小，因为最终目标是预测下一个单词。这是通过一个
    softmax 层来完成的，它表示在给定单词序列的情况下，下一个单词在整个词汇表中的概率似然。随着 softmax 层的增长，不仅内存要求增加，计算时间也要增加。因此，值得研究其他减少词汇量的技术。
- en: Is large vocabulary size the ultimate weakness of the softmax layer?
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 大词汇量是 softmax 层的最终弱点吗？
- en: A major weakness of the softmax layer is its computational complexity. The softmax
    layer needs to first perform a matrix multiplication to get the logits (i.e.,
    unnormalized scores output by the final layer of the network). Then it needs to
    sum over the last axis to compute softmax probabilities of the output. Specifically,
    for the input h, the logits of the softmax layer are computed as
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: softmax 层的一个主要弱点是其计算复杂度。softmax 层首先需要执行矩阵乘法以获取 logits（即网络最终层输出的未归一化分数）。然后，它需要对最后一个轴进行求和以计算输出的
    softmax 概率。具体来说，对于输入 h，softmax 层的 logits 计算为
- en: '*s* = *h*. *W* + *b* where *W* ∈ *R*^(|h|×|V|) [∧] *b* [∈] *R*^(|V|)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*s* = *h*。*W* + *b* 其中 *W* ∈ *R*^(|h|×|V|) [∧] *b* [∈] *R*^(|V|)'
- en: where W is the weight matrix, b is the bias of that final layer, |h| is the
    size of the input, and |V| is the size of the vocabulary. Then softmax normalization
    is applied as
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 W 是权重矩阵，b 是该最终层的偏置，|h| 是输入的大小，|V| 是词汇量的大小。然后应用 softmax 归一化
- en: '![10_00a](../../OEBPS/Images/10_00a.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![10_00a](../../OEBPS/Images/10_00a.png)'
- en: These computations should make it evident to you that a large vocabulary (whose
    size can easily reach hundreds of thousands for a real-world application) will
    create problems in executing this computation in a limited time during model training.
    Having to do this for thousands of batches of data makes the problem even worse.
    Therefore, better techniques to compute the loss without using all the logits
    have emerged. Two popular choices are
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这些计算应该使您明白，大词汇量（对于现实世界的应用程序，其大小很容易达到数十万）将在模型训练期间的有限时间内执行此计算时会产生问题。在数千批数据上执行此操作会使问题变得更加严重。因此，已经出现了更好的技术来计算损失，而无需使用所有logit。两种流行选择是
- en: Noise contrastive estimation (NCE) loss
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 噪声对比估计（NCE）损失
- en: Hierarchical softmax
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分层softmax
- en: '**Noise contrastive estimation (NCE)**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**噪声对比估计（NCE）**'
- en: 'We will look at the prime motivation that drives these methods but not delve
    into the specifics, as this is considered out of scope for this book. For the
    details on these topics, refer to [https://ruder.io/word-embeddings-softmax](https://ruder.io/word-embeddings-softmax).
    NCE only uses the logit indexed by the true target and a small sample of k random
    set of logits (termed *noise*) to compute the loss. The more you match the true
    data distribution in the noise sample, the better the results will be. Specifically,
    if the true target is *s* and the logit at the index *s* is termed *s*[i], the
    following loss is used:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将研究驱动这些方法的主要动机，但不会深入探讨具体细节，因为这被认为超出了本书的范围。有关这些主题的详细信息，请参阅[https://ruder.io/word-embeddings-softmax](https://ruder.io/word-embeddings-softmax)。NCE仅使用由真实目标和k个随机logit样本（称为*噪声*）索引的logit来计算损失。您在噪声样本中匹配真实数据分布的越多，结果就越好。具体来说，如果真实目标是*s*，并且在索引*s*处的logit称为*s*[i]，则使用以下损失：
- en: '![10_00b](../../OEBPS/Images/10_00b.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![10_00b](../../OEBPS/Images/10_00b.png)'
- en: Here, *σ* denotes that the sigmoidal activation is a common activation function
    used in neural networks and is computed as *σ* (*x*) = 1/(1 + *e*^(-x)), where
    *s*[i] represents the logit value of the true target i, and j represents an index
    sampled from a predefined distribution over the vocabulary *P*[n].
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*σ*表示sigmoid激活是神经网络中常用的激活函数，并且计算为*σ* (*x*) = 1/(1 + *e*^(-x))，其中*s*[i]表示真实目标i的logit值，j表示从词汇表*P*[n]上的预定义分布中采样的索引。
- en: '**Hierarchical softmax**'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**分层softmax**'
- en: Unlike standard softmax, which has a flat structure where each node represents
    an element in the vocabulary, hierarchical softmax represents the words in the
    vocabulary as the leaf nodes in a binary tree, and the task becomes choosing whether
    to go left or right in order to reach the right node. The following figure depicts
    the formation
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 与标准softmax不同，其中每个节点代表词汇表中的一个元素的平坦结构相比，分层softmax将词汇表中的单词表示为二叉树中的叶节点，并且任务变成选择向左还是向右以达到正确的节点。以下图示了形成过程
- en: of the layer when hierarchical softmax is used. As is evident, to infer the
    probability of a word given the previous sequence of words, the layer only has
    to go through three steps of computation at max (shown by the dark path), as opposed
    to evaluating across all seven possible words.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 使用分层softmax时，计算层的最大层数。显然，为了推断给定前一序列的单词的概率，该层最多只需进行三步计算（由黑色路径表示），而不是评估所有七个可能的单词。
- en: '![10-00unnumb](../../OEBPS/Images/10-00unnumb.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![10-00unnumb](../../OEBPS/Images/10-00unnumb.png)'
- en: Hierarchical softmax representation of the final layer. The dark path represents
    the path the model must follow to compute the probability of P(home| I went).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 最终层的分层softmax表示。黑色路径表示模型必须遵循的路径，以计算P(home| I went)的概率。
- en: Next, we will see how we can deal with language in the case of a large vocabulary.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看到如何在词汇量很大的情况下处理语言。
- en: 10.1.3 Too large vocabulary? N-grams to the rescue
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.3 词汇量太大？N-gram拯救
- en: Here, we start the first step of defining various text preprocessors and the
    data pipeline. We suspect that going forward with a large vocabulary size can
    have adverse repercussions on our modeling journey. Let’s find some ways to reduce
    the vocabulary size. Given that children’s stories use a relatively simple language
    style, we can represent text as n-grams (at the cost of the expressivity of our
    model). N-grams are an approach where a word is decomposed to finer sub-words
    of fixed length. For example, the bigrams (or two-grams) of the sentence
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们开始定义各种文本预处理器和数据流水线的第一步。我们怀疑在大词汇量的情况下继续前进会对我们的建模之旅产生不利影响。让我们找到一些减少词汇量的方法。鉴于儿童故事使用相对简单的语言风格，我们可以将文本表示为n-gram（以牺牲我们模型的表达能力为代价）。
    N-grams是一种方法，其中单词被分解为固定长度的更细的子单词。例如，句子的二元组（或二元组）
- en: '[PRE7]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: are
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 是
- en: '[PRE8]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: where three grams would be
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 三个元组将是
- en: '[PRE9]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The unigrams (or one-grams) would simply be the individual characters. In other
    words, we are moving a window of fixed length (with a stride of 1), while reading
    the characters within that window at a time. You could also generate n-grams without
    overlaps by moving the window at a stride equal to the length of the window. For
    example, bigrams without overlapping would be
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 单元组（或一元组）将简单地是各个字符。换句话说，我们正在移动一个固定长度的窗口（步长为1），同时每次读取该窗口内的字符。您也可以通过将窗口移动到长度相等的步长来生成没有重叠的n-gram。例如，没有重叠的二元组将是
- en: '[PRE10]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Which one to use depends on your use case. For the language modeling task, it
    makes sense to use the non-overlapping approach. This is because by joining the
    n-grams that we generated, we can easily generate readable text. For certain use
    cases, the non-overlapping approach can be disadvantageous as it leads to a coarser
    representation of text because it doesn’t capture all the different n-grams that
    appear in text.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用哪种方法取决于您的用例。对于语言建模任务，使用非重叠方法是有意义的。这是因为通过连接我们生成的n-gram，我们可以轻松生成可读的文本。对于某些用例，非重叠方法可能不利，因为它导致文本的表示更粗糙，因为它没有捕获文本中出现的所有不同的n-gram。
- en: 'By using bigrams instead of words to develop your vocabulary, you can cut down
    the size of the vocabulary by a significant factor. There are many other advantages
    that come with the n-gram approach, as we will see soon. We will write a function
    to generate n-grams given a text string:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用二元组而不是单词来开发您的词汇表，您可以将词汇表的大小减少到一个显著的因子。随着我们很快会看到的n-gram方法的许多其他优势。我们将编写一个函数来生成给定文本字符串的n-gram：
- en: '[PRE11]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'All we do here is go from the beginning to the end of the text with a stride
    equal to n and read the sequence of characters from position i to i+n. We can
    test how this performs on sample text:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里所做的一切就是从文本的开头到结尾以步长n进行移动，并从位置i到i+n读取字符序列。我们可以测试这在样本文本上的表现如何：
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This will print the following output:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印以下输出：
- en: '[PRE13]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let’s now repeat the process for analyzing the vocabulary size, but with n-grams
    instead of words:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们重复使用n-gram而不是单词来分析词汇量的过程：
- en: '[PRE14]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Now, if we check the number of words that appear at least 10 times in the text
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们检查文本中至少出现10次的单词数
- en: '[PRE15]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: we will see
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将会看到
- en: '[PRE16]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Wow! Compared to the 15,000 words we had, 735 is much smaller and more manageable.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！与我们有的15,000个单词相比，735个要小得多，更易于管理。
- en: Advantages of n-grams
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: n-gram的优势
- en: 'Here are some of the main advantages of using n-grams over words:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用n-gram而不是单词的主要优势之一：
- en: The limited number of n-grams for small n limits the vocabulary size, leading
    to both memory and computational advantages.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于小n的有限数量的n-gram限制了词汇表的大小，从而导致了记忆和计算优势。
- en: N-grams lead to fewer chances of out-of-vocabulary words, as an unseen word
    can usually be constructed using n-grams seen in the past.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: N-grams导致词汇表外单词的机会减少，因为通常可以使用过去看到的n-grams构造看不见的单词。
- en: 10.1.4 Tokenizing text
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.4 文本分词
- en: 'We will *tokenize* the text now (i.e., split a string into a list of smaller
    tokens—words). By the end of this section, you will have defined and fitted a
    tokenizer on the bigrams generated for your text. First, let’s import the Tokenizer
    from TensorFlow:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将*对文本进行分词*（即，将字符串拆分为一组较小的标记——单词）。在本节结束时，您将已经为文本生成的二元组定义并拟合了一个标记生成器。首先，让我们从
    TensorFlow 中导入 Tokenizer：
- en: '[PRE17]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We don’t have to do any preprocessing and want to convert text to word IDs
    as it is. We will define the num_words argument to limit the vocabulary size as
    well as an oov_token that will be used to replace all the n-grams that appear
    less than 10 times in the training corpus:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要进行任何预处理，希望将文本按原样转换为单词 ID。我们将定义 num_words 参数来限制词汇表的大小，以及一个 oov_token，该 token
    将用于替换训练语料库中出现次数少于 10 次的所有 n-gram：
- en: '[PRE18]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let’s generate n-grams from the stories in training data. train_ngram_stories
    will be a list of lists of strings, where the inner list represents a list of
    bigrams for a single story and the outer list represents all the stories in the
    training data set:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从训练数据的故事中生成 n-gram。train_ngram_stories 将是一个字符串列表的列表，其中内部列表表示单个故事的二元组列表，外部列表表示训练数据集中的所有故事：
- en: '[PRE19]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We will fit the Tokenizer on the two-grams of the training stories:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在训练故事的二元组上拟合 Tokenizer：
- en: '[PRE20]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now convert all training, validation, and testing stories to sequences of IDs,
    using the already fitted Tokenizer trained using two-grams from the training data:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在使用已经拟合 Tokenizer，该 Tokenizer 使用训练数据的二元组将所有训练、验证和测试故事转换为 ID 序列：
- en: '[PRE21]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Let’s analyze how the data looks after converting to word IDs by printing some
    test data. Specifically, we’ll print the first three story strings (test_stories),
    n-gram strings (test_ngram_stories), and word ID sequences (test_data_seq):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 通过打印一些测试数据来分析转换为单词 ID 后数据的样子。具体来说，我们将打印前三个故事字符串（test_stories）、n-gram 字符串（test_ngram_stories）和单词
    ID 序列（test_data_seq）：
- en: '[PRE22]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 10.1.5 Defining a tf.data pipeline
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.5 定义一个 tf.data pipeline
- en: Now the preprocessing has happened, and we have text converted to word ID sequences.
    We can define the tf.data pipeline that will deliver the final processed data
    ready to be consumed by the model. The main steps involved in the process are
    illustrated in figure 10.1.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在预处理已经完成，我们将文本转换为单词 ID 序列。我们可以定义 tf.data pipeline，该 pipeline 将提供最终处理好的数据，准备好被模型使用。流程中涉及的主要步骤如图
    10.1 所示。
- en: '![10-01](../../OEBPS/Images/10-01.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![10-01](../../OEBPS/Images/10-01.png)'
- en: Figure 10.1 The high-level steps of the data pipeline we will be developing.
    First the individual stories are broken down to fixed-length sequences (or windows).
    Then, from the windowed sequences, inputs and targets are generated.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.1 数据 pipeline 的高级步骤。首先将单个故事分解为固定长度的序列（或窗口）。然后，从窗口化的序列中生成输入和目标。
- en: 'As we did before, let’s define the word ID corpus as a tf.RaggedTensor object,
    as the sentences in the corpus have variable sequence lengths:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前一样，让我们将单词 ID 语料库定义为 tf.RaggedTensor 对象，因为语料库中的句子具有可变的序列长度：
- en: '[PRE23]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Remember that a ragged tensor is a tensor that has variable-sized dimensions.
    Then we will shuffle the data so that stories come at a random order if shuffle
    is set to True (e.g., training time):'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，不规则张量是具有可变大小维度的张量。然后，如果 shuffle 设置为 True（例如，训练时间），我们将对数据进行洗牌，以便故事以随机顺序出现：
- en: '[PRE24]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Now comes the tricky part. In this section, we will see how to generate fixed-sized
    windowed sequences from an arbitrarily long text. We will do that through a series
    of steps. This section can be slightly complex compared to the rest of the pipeline.
    This is because there will be interim steps that result in data sets nested up
    to three levels. Let’s try to go through this in as much detail as possible.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在来看棘手的部分。在本节中，我们将看到如何从任意长的文本中生成固定大小的窗口化序列。我们将通过一系列步骤来实现这一点。与 pipeline 的其余部分相比，本节可能略微复杂。这是因为将会有中间步骤导致三级嵌套的数据集。让我们尽可能详细地了解一下这一点。
- en: 'First, let’s make it clear what we need to achieve. The first steps we need
    to perform are to do the following for each individual story S:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们清楚我们需要实现什么。我们需要为每个单独的故事 S 执行以下步骤：
- en: Create a tf.data.Dataset() object containing the word IDs of the story S as
    its items.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个包含故事 S 的单词 ID 的 tf.data.Dataset() 对象作为其项。
- en: Call the tf.data.Dataset.window() function to window word IDs with a n_seq+1-sized
    window and a predefined shift. The window() function returns a WindowDataset object
    for each story S.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 n_seq+1 大小的窗口和预定义的移位调用 tf.data.Dataset.window() 函数来窗口化单词 ID，窗口() 函数为每个故事
    S 返回一个 WindowDataset 对象。
- en: After this, you will have a three-level nested data structure having the specification
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，您将获得一个三级嵌套的数据结构，其规范为
- en: '[PRE25]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We will need to flatten this data set and untangle the nesting in our data
    set to end up with a flat tf.data.Dataset. You can remove these inner nestings
    using the tf.data.Dataset.flat_map() function. We will soon see how to use the
    flat_ map() function. To be specific, we have to use two flat_map calls to remove
    two levels of nesting so that we end up with only the flat original data set containing
    simple tensors as elements. In TensorFlow, this process can be achieved using
    the following line of code:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要展开这个数据集并解开数据集中的嵌套，最终得到一个平坦的tf.data.Dataset。您可以使用tf.data.Dataset.flat_map()函数来去除这些内部嵌套。我们将很快看到如何使用flat_map()函数。具体来说，我们需要使用两个flat_map调用来去除两层嵌套，以便最终得到只包含简单张量作为元素的平坦原始数据集。在TensorFlow中，可以使用以下代码行来实现此过程：
- en: '[PRE26]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This is what we are doing here: first, we create a tf.data.Dataset object from
    a single story (x) and then call the tf.data.Dataset.window() function on that
    to create the windowed sequences. This windowed sequence contains windows, where
    each window is a sequence with n_seq+1 consecutive elements in the story x.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们所做的是：首先，我们从一个单一的故事(x)创建一个tf.data.Dataset对象，然后在此上调用tf.data.Dataset.window()函数以创建窗口序列。此窗口序列包含窗口，其中每个窗口都是故事x中n_seq+1个连续元素的序列。
- en: Then we call the tf.data.Dataset.flat_map() function, where, for each window
    element, we get all the individual IDs as a single batch. In other words, a single
    window element produces a single batch with all the elements in that window. Make
    sure you use drop_remainder=True; otherwise, the data set will return smaller
    subwindows within that window that contain fewer elements. Using tf.data.Dataset.flat_map()
    instead of map makes sure that the inner-most nesting will be removed. This whole
    thing is called with a tf.data.Dataset.flat_map() call, which gets rid of the
    next level of nesting immediately following the innermost nesting we removed.
    It is quite a complex process for a single liner. I suggest you go through it
    again if you have not fully understood the process.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们调用tf.data.Dataset.flat_map()函数，在每个窗口元素上，我们将所有单独的ID作为一个批次。换句话说，单个窗口元素会产生一个包含该窗口中所有元素的单个批次。确保使用drop_remainder=True；否则，数据集将返回该窗口中包含较少元素的较小子窗口。使用tf.data.Dataset.flat_map()而不是map，确保去除最内层的嵌套。这整个过程称为tf.data.Dataset.flat_map()调用，该调用立即消除了我们所删除的最内层嵌套后面的下一层嵌套。对于一行代码来说，这是一个相当复杂的过程。如果您还没有完全理解这个过程，我建议您再次仔细阅读一下。
- en: You might notice that we are defining the window size as n_seq+1 and not n_seq.
    The reason for this will become evident later, but using n_seq+1 makes our life
    so much easier when we have to generate inputs and targets from the windowed sequences.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会注意到，我们把窗口大小定义为n_seq+1而不是n_seq。稍后会看到这样做的原因，但是当我们需要从窗口序列生成输入和目标时，使用n_seq+1会让我们的生活变得更加容易。
- en: Difference between map and flat_map in tf.data.Dataset
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: tf.data.Dataset中map和 flat_map的区别
- en: Both functions tf.data.Dataset.map() and tf.data.Dataset.flat_map() achieve
    the same result but with different data set specifications. For example, assume
    the data set
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: tf.data.Dataset.map()和tf.data.Dataset.flat_map()这两个函数可以实现同样的结果，但具有不同的数据集规范。例如，假设数据集
- en: '[PRE27]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Using the tf.data.Dataset.map() function to square the elements as
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 使用tf.data.Dataset.map()函数对元素进行平方
- en: '[PRE28]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: will result in a data set that has the elements
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 将导致一个具有元素的数据集
- en: '[PRE29]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: As you can see, the result has the same structure as the original tensor. Using
    the tf.data.Dataset.flat_map() function to square the elements as
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，结果与原始张量具有相同的结构。使用tf.data.Dataset.flat_map()函数对元素进行平方
- en: '[PRE30]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: will result in a data set that has
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 将导致一个具有的数据集
- en: '[PRE31]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: As you can see, that inner-most nesting has been flattened to produce a flat
    sequence of elements.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，该最内层嵌套已被展平，产生了一个平坦的元素序列。
- en: 'The hardest part of our data pipeline is done. By now, you have a flat data
    set, and each item is n_seq+1 consecutive word IDs belonging to a single story.
    Next, we will perform a window-level shuffle on the data. This is different from
    the first shuffle we did as that was on the story level (i.e., not the window
    level):'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据管道中最困难的部分已经完成。现在，您已经有了一个平坦的数据集，每个项目都是属于单个故事的n_seq+1个连续单词ID。接下来，我们将在数据上执行窗口级别的洗牌。这与我们进行的第一个洗牌不同，因为那是在故事级别上进行的（即不是窗口级别）：
- en: '[PRE32]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We’re then going to batch the data so that we will get a batch of windows every
    time we iterate the data set:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将批处理数据，以便每次迭代数据集时都会获得一批窗口：
- en: '[PRE33]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Finally, the reason we chose the sequence length as n_seq+1 will become clearer.
    Now we will split the sequences into two versions, where one sequence will be
    the other shifted to the right by 1\. In other words, the targets to this model
    will be inputs shifted to the right by 1\. For example, if the sequence is [0,1,2,3,4],
    then the two resulting sequences will be [0,1,2,3] and [1,2,3,4]. Furthermore,
    we will use prefetching to speed up the data ingestion:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们选择序列长度为n_seq+1的原因将变得更清晰。现在我们将序列分为两个版本，其中一个序列将是另一个向右移动1位的序列。换句话说，该模型的目标将是向右移动1位的输入。例如，如果序列是[0,1,2,3,4]，那么得到的两个序列将是[0,1,2,3]和[1,2,3,4]。此外，我们将使用预取来加速数据摄取：
- en: '[PRE34]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Finally, the full code can be encapsulated in a function as in the next listing.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，完整的代码可以封装在下一个清单中的函数中。
- en: Listing 10.3 The tf.data pipeline from free text sequences
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3 从自由文本序列创建的 tf.data 管道
- en: '[PRE35]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: ❶ Define a tf.dataset from a ragged tensor created from data_seq.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从数据_seq 创建的不规则张量定义一个 tf.dataset。
- en: ❷ If shuffle is set, shuffle the data (shuffle story order).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 如果设置了 shuffle，对数据进行洗牌（洗牌故事顺序）。
- en: ❸ Here we create windows from longer sequences, given a window size and a shift,
    and then use a series of flat_map operations to remove the nesting that’s created
    in the process.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在这里，我们从更长的序列中创建窗口，给定窗口大小和偏移量，然后使用一系列 flat_map 操作来删除在此过程中创建的嵌套。
- en: ❹ Shuffle the data (shuffle the order of the windows generated).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 对数据进行洗牌（洗牌窗口生成的顺序）。
- en: ❺ Batch the data.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 批量处理数据。
- en: ❻ Split each sequence into an input and a target and enable pre-fetching.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将每个序列拆分为输入和目标，并启用预取。
- en: All this hard work wouldn’t mean as much as it should unless we looked at the
    generated data
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些辛勤工作，如果不看生成的数据，就不会有太多意义。
- en: '[PRE36]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: which will show you
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这将向您展示
- en: '[PRE37]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Great, you can see that we get a tuple of tensors as a single batch: inputs
    and targets. Moreover, you can validate the correctness of the results, as we
    can clearly see that the targets are the input shifted to the right by 1\. One
    last thing: we will save the same hyperparameters to the disk. Particularly'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，你可以看到我们得到了一个张量元组作为单个批处理：输入和目标。此外，您可以验证结果的正确性，因为我们清楚地看到目标是向右移动 1 位的输入。最后一件事：我们将将相同的超参数保存到磁盘上。
- en: n in n-grams
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: n-gram 中的 n
- en: Vocabulary size
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词汇量大小
- en: Sequence length
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列长度
- en: '[PRE38]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Here, we are defining the sequence length n_seq=100; this is the number of bigrams
    we will have in a single input/label sequence.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义序列长度n_seq=100；这是单个输入/标签序列中我们将拥有的二元组数目。
- en: In this section, we learned about the data used for language modeling and defined
    a capable tf.data pipeline that can convert sequences of text into input label
    sequences that can be used to train the model directly. Next, we will define a
    machine learning model to generate text with.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们了解了用于语言建模的数据，并定义了一个强大的 tf.data 管道，该管道可以将文本序列转换为可直接用于训练模型的输入标签序列。接下来，我们将定义一个用于生成文本的机器学习模型。
- en: Exercise 1
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 1
- en: You are given a sequence x that has values [1,2,3,4,5,6,7,8,9,0]. You have been
    asked to write a tf.data pipeline that generates an input and target tuple, and
    the target is the input shifted two elements to the right (i.e., the target of
    the input 1 is 3). You have to do this so that a single input/target has three
    elements and no overlap between the consecutive input sequences. For the previous
    sequence it should generate [([1,2,3], [3,4,5]), ([6,7,8], [8,9,0])].
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 你有一个序列 x，其值为[1,2,3,4,5,6,7,8,9,0]。你被要求编写一个 tf.data 管道，该管道生成一个输入和目标元组，其中目标是将输入向右移动两个元素（即，输入
    1 的目标是 3）。你必须这样做，以便一个单独的输入/目标具有三个元素，并且连续输入序列之间没有重叠。对于前面的序列，它应该生成[([1,2,3], [3,4,5]),
    ([6,7,8], [8,9,0])]。
- en: '10.2 GRUs in Wonderland: Generating text with deep learning'
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.2 仙境中的 GRU：使用深度学习生成文本
- en: 'Now we’re on to the rewarding part: implementing a cool machine learning model.
    In the last chapter, we talked about deep sequential models. Given the sequential
    nature of the data, you probably have guessed that we’re going to use one of the
    deep sequential models like LSTMs. In this chapter, we will use a slightly different
    model called *gated recurrent units* (GRUs). The principles that drive the computations
    in the model remain the same as LSTMs. To maintain the clarity of our discussion,
    it’s worthwhile to remind ourselves how LSTM models work.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来到了有奖励的部分：实现一个酷炫的机器学习模型。在上一章中，我们讨论了深度时序模型。鉴于数据的时序性，你可能已经猜到我们将使用其中之一深度时序模型，比如LSTMs。在本章中，我们将使用一个略有不同的模型，称为*门控循环单元*（GRUs）。驱动该模型计算的原理与LSTMs相同。为了保持我们讨论的清晰度，值得提醒自己LSTM模型是如何工作的。
- en: LSTMs are a family of deep neural networks that are specifically designed to
    process sequential data. They process a sequence of inputs, one input at a time.
    An LSTM cell goes from one input to the next while producing outputs (or states)
    at each time step (figure 10.2). Additionally, to produce the outputs of a given
    time step, LSTMs uses previous outputs (or states) it produced. This property
    is very important for LSTMs and gives them the ability to memorize things over
    time.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM是一类专门设计用于处理时序数据的深度神经网络。它们逐个输入地处理一系列输入。LSTM单元从一个输入到下一个输入，同时在每个时间步产生输出（或状态）（图10.2）。此外，为了产生给定时间步的输出，LSTMs使用了它产生的先前输出（或状态）。这个属性对于LSTMs非常重要，使它们能够随时间记忆事物。
- en: '![10-02](../../OEBPS/Images/10-02.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![10-02](../../OEBPS/Images/10-02.png)'
- en: Figure 10.2 Overview of the LSTM model and how it processes a sequence of inputs
    spread over time
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2 LSTM模型概述以及它如何处理随时间展开的输入序列
- en: 'Let’s summarize what we learned about LSTMs in the previous chapter, as that
    will help us to compare LSTMs and GRUs. An LSTM has two states known as the cell
    state and the output state. The cell state is responsible for maintaining long-term
    memory, whereas the output state can be thought of as the short-term memory. The
    outputs and interim results within an LSTM cell are controlled by three gates:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下我们在上一章中学到的关于LSTMs的知识，因为这将帮助我们比较LSTMs和GRUs。一个LSTM有两个状态，称为单元状态和输出状态。单元状态负责维护长期记忆，而输出状态可以被视为短期记忆。在LSTM单元内部，输出和中间结果受三个门的控制：
- en: '*Input gate*—Controls the amount of the current input that will contribute
    to the final output at a given time step'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输入门*——控制了在给定时间步中当前输入的多少会对最终输出产生影响'
- en: '*Forget gate*—Controls how much of the previous cell state affects the current
    cell state computation'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*遗忘门*——控制了多少先前的单元状态影响当前单元状态的计算'
- en: '*Output gate*—Controls how much the current cell state contributes to the final
    output produced by the LSTM model'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输出门*——控制了当前单元状态对LSTM模型产生的最终输出的贡献'
- en: 'The GRU model was introduced in the paper “Learning Phrase Representations
    using RNN Encoder-Decoder for Statistical Machine Translation” by Cho et al. ([https://arxiv.org/pdf/1406.1078v3.pdf](https://arxiv.org/pdf/1406.1078v3.pdf)).
    The GRU model can be considered a simplification of the LSTM model while preserving
    on-par performance. The GRU cell has two gates:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: GRU模型是在Cho等人的论文“Learning Phrase Representations using RNN Encoder-Decoder for
    Statistical Machine Translation”中介绍的（[https://arxiv.org/pdf/1406.1078v3.pdf](https://arxiv.org/pdf/1406.1078v3.pdf)）。GRU模型可以被认为是LSTM模型的简化，同时保持了相当的性能。GRU单元有两个门：
- en: '*Update gate* (*z*[t])—Controls how much of the previous hidden state is carried
    to the current hidden state'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*更新门*（*z*[t]）——控制了多少先前的隐藏状态传递到当前的隐藏状态'
- en: '*Reset gate* (*r*[t])—Controls how much of the hidden state is reset with the
    new input'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*重置门*（*r*[t]）——控制了多少隐藏状态与新输入一起被重置'
- en: 'Unlike the LSTM cell, a GRU cell has only one state vector. In summary, there
    are two major changes in a GRU compared to an LSTM model:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 与LSTM单元不同，GRU单元只有一个状态向量。总的来说，与LSTM模型相比，GRU有两个主要变化：
- en: Both the input gate and forget gate are combined into a single gate called the
    update gate (*z*[t]). The input gate is computed as (1-*z*[t]), whereas the forget
    gate stays *z*[t].
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入门和遗忘门被合并成一个称为更新门的门（*z*[t]）。输入门被计算为（1-*z*[t]），而遗忘门保持*z*[t]。
- en: There’s only one state (*h*[t]) in contrast to the two states found in an LSMT
    cell (i.e., cell state and the output state).
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与LSMT单元中的两个状态（即单元状态和输出状态）相比，这里只有一个状态（*h*[t]）。
- en: 'Figure 10.3 depicts the various components of the GRU cell. Here is the full
    list of equations that make a GRU spin:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '*r*[t] = σ(*W*[rh]*h*[t-1] + *W*[rx]*x*[t] + *b*[r])'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '*z*[t] = σ(*W*[zh]*h*[t-1] + *W*[zx]*x*[t] + *b*[z])'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '*h̃*[t] = tanh(*W*[h]*(rh*[t-1]) + *W*[x]*x*[t] + *b*)'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '*h*[t] = (*z*[th]*h*[t-1] + (1 - *z*[t] )*h̃*[t]'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '![10-03](../../OEBPS/Images/10-03.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 Overview of the computations transpiring in a GRU cell
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'This discussion was immensely helpful to not only understand the GRU model
    but also to learn how it’s different from an LSTM cell. You can define a GRU cell
    in TensorFlow as follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The parameter units, return_state and return_sequences, have the same meaning
    as they do in the context of an LSTM cell. However, note that the GRU cell has
    only one state; therefore, if return_state=true, the same state is duplicated
    to mimic the output state and the cell state of the LSTM layer. Figure 10.4 shows
    what these parameters do for a GRU layer.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '![10-04](../../OEBPS/Images/10-04.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 Changes in results depending on the return_state and return_sequences
    arguments of the GRU cell
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: We now know everything we need to define the final model (listing 10.4). Our
    final model will consist of
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: An embedding layer
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A GRU layer (1,024 units) that returns all the final state vectors as a tensor
    that has shape [batch size, sequence length, number of units]
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Dense layer with 512 units and ReLU activation
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Dense layer with n_vocab units and softmax activation
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Listing 10.4 Implementing the language model
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: ❶ Define an embedding layer to learn word vectors of the bigrams.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Define an LSTM layer.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Define a Dense layer.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Define a final Dense layer and softmax activation.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: You will notice that the Dense layer after the GRU receives a three-dimensional
    tensor (as opposed to the typical two-dimensional tensor passed to Dense layers).
    Dense layers are smart enough to work with both two-dimensional and three-dimensional
    inputs. If the input is three-dimensional (like in our case), then a Dense layer
    that accepts a [batch size, number of units] tensor is passed through all the
    steps in the sequence to generate the Dense layer’s output. Also note how we are
    separating the softmax activation from the Dense layer. This is actually an equivalent
    of
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: We will not prolong the conversation by reiterating what’s shown in listing
    10.4, as it is self-explanatory.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 2
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: You have been given the model as follows and have been asked to add another
    GRU layer with 512 units that returns all the state outputs, on top of the existing
    GRU layer. What changes would you make to the following code?
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: In this section, we learned about gated recurrent units (GRUs) and how they
    compare to LSTMs. Finally, we defined a language model that can be trained on
    the data we downloaded and processed earlier. In the next section, we are going
    to learn about evaluation metrics for assessing the quality of generated text.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 10.3 Measuring the quality of the generated text
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Performance monitoring has been an integral part of our modeling journey in
    every chapter. It is no different here. Performance monitoring is an important
    aspect of our language model, and we need to find metrics that are suited for
    language models. Naturally, given that this is a classification task, you might
    be thinking, “Wouldn’t accuracy be a good choice for a metric?” Well, not quite
    in this task.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: For example, if the language model is given the sentence “I like my pet dog,”
    then when asked to predict the missing word given “I like my pet ____,” the model
    might predict “cat,” and the accuracy would be zero. But that’s not correct; “cat”
    makes as much sense as “dog” in this example. Is there a better solution here?
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Enter perplexity! Intuitively, *perplexity* measures how “surprised” the model
    was to see a target given the previous word sequence. Before understanding perplexity,
    you need to understand what “entropy” means.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '*Entropy* is a term coined by the famous Claude Shannon, who is considered
    the father of information theory. Entropy measures the surprise/uncertainty/randomness
    of an event. The event can have some outcome generated by an underlying probability
    distribution over all the possible outcomes. For example, if you consider tossing
    a coin (with a probability p of landing on heads) an event, if p = 0.5, you will
    have the maximum entropy, as that is the most uncertain scenario for a coin toss.
    If p = 1 or p = 0, then you have the minimum entropy, as you know what the outcome
    is before tossing the coin.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'The original interpretation of entropy is the expected value of the number
    of bits required to send a signal or a message informing of an event. A bit is
    a unit of memory, which can be 1 or 0\. For example, you are the commander of
    an army that’s at war with countries A and B. Now you have four possibilities:
    A and B both surrender, A wins and B loses, A loses and B wins, and both A and
    B win. If all these events are equally likely to happen, you need two bits to
    send a message, where each bit represents whether that country won. Entropy of
    a random variable X is quantified by the equation'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '![10_04a](../../OEBPS/Images/10_04a.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
- en: where *x* is an outcome of *X*. Believe it or not, we have been using this equation
    without knowing it every time we used the categorical cross-entropy loss. The
    crux of the categorical cross entropy is this equation. Coming back to the perplexity
    measure, perplexity is simply
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '*Perplexity =* 2^(H(X))'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'Since perplexity is a function of entropy, it measures how surprised/uncertain
    the model was to see the target word, given the sequence of previous words. Perplexity
    can also be thought of as the number of all possible combinations of a given signal.
    For example, assume you are sending a message with two bits, where all the events
    are equally likely; then the entropy = 2, which means perplexity = 2^2 = 4\. In
    other words, there are four combinations two bits can be in: 00, 01, 10, and 11.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: In more of a modeling perspective, you can think of perplexity as the number
    of different targets that the model thinks fit the blank as the next word, given
    a sequence of previous words. The smaller this number, the better, as that means
    the model is trying to find a word from a smaller subset, indicating signs of
    language understanding.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 从建模角度来看，你可以将困惑度理解为在给定一系列前序词的情况下，模型认为有多少不同的目标适合作为下一个词的空白。这个数字越小越好，因为这意味着模型试图从更小的子集中找到一个词，表明语言理解的迹象。
- en: To implement perplexity, we will define a custom metric. The computation is
    very simple. We compute the categorical cross entropy and then exponentiate it
    to get the perplexity. The categorical cross entropy is simply an extension of
    the entropy to measure entropy in classification problems with more than two classes.
    For an input example (*x*[i],*y*[i]), it is typically defined as
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现困惑度，我们将定义一个自定义指标。计算非常简单。我们计算分类交叉熵，然后对其进行指数化以获得困惑度。分类交叉熵简单地是熵的扩展，用于在具有两个以上类别的分类问题中测量熵。对于输入示例（*x*[i],*y*[i]），它通常定义为
- en: '![10_04b](../../OEBPS/Images/10_04b.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![10_04b](../../OEBPS/Images/10_04b.png)'
- en: where *y[i]* denotes the one-hot encoded vector representing the true class
    the example belongs to and *ŷ*[i] is the predicted class probability vector of
    *C* elements, with *ŷ*[i,c] denoting the probability of the example belonging
    to class *c*. Note that in practice, an exponential (natural) base is used instead
    of base, 2 as the computations are faster. The following listing delineates the
    process.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*y[i]*表示真实类别示例所属的独热编码向量，*ŷ*[i]是*C*个元素的预测类别概率向量，其中*ŷ*[i,c]表示示例属于类别*c*的概率。请注意，在实践中，计算更快的是使用指数（自然）基数，而不是使用2作为基数。以下清单描述了这个过程。
- en: Listing 10.5 Implementation of the perplexity metric
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 10.5 实现困惑度指标
- en: '[PRE43]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: ❶ Define a function to compute perplexity given real and predicted targets.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义一个函数来计算给定真实和预测目标的困惑度。
- en: ❷ Compute the categorical cross-entropy loss.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 计算分类交叉熵损失。
- en: ❸ Compute the mean of the loss.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 计算损失的均值。
- en: ❹ Compute the exponential of the mean loss (perplexity).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 计算均值损失的指数（困惑度）。
- en: 'What we’re doing is very simple. First, we subclass the tf.keras.metrics.Mean
    class. The tf.keras.metrics.Mean class will keep track of the mean value of any
    outputted metric passed into its update_state() function. In other words, when
    we subclass the tf.keras.metrics.Mean class, we don’t specifically need to manually
    compute the mean of the accumulated perplexity metric as the training continues.
    It will be automatically done by that parent class. We will define the loss function
    we will use in the self.cross_entropy variable. Then we write the function _calculate_perplexity(),
    which takes the real targets and the predictions from the model. We compute the
    sample-wise loss and then compute the mean. Finally, to get the perplexity, we
    exponentiate the mean loss. With that, we can compile the model:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在做的事情非常简单。首先，我们对tf.keras.metrics.Mean类进行子类化。tf.keras.metrics.Mean类将跟踪传递给其update_state()函数的任何输出指标的均值。换句话说，当我们对tf.keras.metrics.Mean类进行子类化时，我们不需要手动计算累积困惑度指标的均值，因为训练继续进行。这将由该父类自动完成。我们将定义我们将在self.cross_entropy变量中使用的损失函数。然后，我们编写函数_calculate_perplexity()，该函数接受模型的真实目标和预测。我们计算逐样本损失，然后计算均值。最后，为了得到困惑度，我们对均值损失进行指数化。有了这个，我们可以编译模型：
- en: '[PRE44]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: In this section, we learned about the performance metrics, such as entropy and
    perplexity, used to evaluate language models. Furthermore, we implemented a custom
    perplexity metric that is used to compile the final model. Next, we’ll train our
    model on the data we have prepared and evaluate the quality of generated text.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了用于评估语言模型的性能指标，如熵和困惑度。此外，我们实现了一个自定义的困惑度指标，用于编译最终模型。接下来，我们将在准备好的数据上训练我们的模型，并评估生成文本的质量。
- en: Exercise 3
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 3
- en: 'Imagine a classification problem that has three outputs. There are two scenarios
    with different predictions:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个有三个输出的分类问题。有两种不同预测的情况：
- en: 'Scenario A: Labels [0, 2, 1]'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 情景 A：标签 [0, 2, 1]
- en: 'Predictions: [[0.6, 0.2, 0.2], [0.1, 0.1, 0.8], [0.3, 0.5, 0.2]]'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 预测：[[0.6, 0.2, 0.2], [0.1, 0.1, 0.8], [0.3, 0.5, 0.2]]
- en: 'Scenario B: Labels [0, 2, 1]'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 情景 B：标签 [0, 2, 1]
- en: 'Predictions: [[0.3, 0.3, 0.4], [0.4, 0.3, 0.3], [0.3, 0.3, 0.4]]'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 预测：[[0.3, 0.3, 0.4], [0.4, 0.3, 0.3], [0.3, 0.3, 0.4]]
- en: Which one will have the lowest perplexity?
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 哪一个会有最低的困惑度？
- en: 10.4 Training and evaluating the language model
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.4 训练和评估语言模型
- en: 'In this section, we will train the model. Before training the model, let’s
    instantiate the training and validation data sets using the previously implemented
    get_tf_pipeline() function. We will only use the first 50 stories (out of a total
    of 98) in the training set to save time. We will take a sequence of 100 bigrams
    at a time and hop the story by shifting the window by 25 bigrams. This means the
    starting index of the sequences for a single story is 0, 25, 50, . . ., and so
    on. We will use a batch size of 128:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们将训练模型。在训练模型之前，让我们使用之前实现的get_tf_pipeline()函数来实例化训练和验证数据集。我们将只使用前50个故事（共98个）作为训练集，以节省时间。我们每次取100个二元组作为一个序列，通过移动窗口来跳过故事，每次移动25个二元组。这意味着单个故事序列的起始索引是0、25、50等等。我们将使用批大小为128：
- en: '[PRE45]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: To train the model, we will define the callbacks as before. We will define
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练模型，我们将像以前一样定义回调。我们将定义
- en: A CSV logger that will log performance over time during training
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个CSV记录器，将在训练期间记录性能
- en: A learning rate scheduler that will reduce the learning rate when performance
    has plateaued
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个学习率调度器，当性能达到平台期时会减小学习率
- en: An early stopping callback to terminate the training if the performance is not
    improving
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果性能没有提高，则使用早期停止回调来终止训练。
- en: '[PRE46]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Finally, it’s time to train the model. I wonder what sort of cool stories I
    can squeeze out from the trained model:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，是时候训练模型了。我想知道我能从训练好的模型中挤出什么样的酷故事：
- en: '[PRE47]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: NOTE On an Intel Core i5 machine with an NVIDIA GeForce RTX 2070 8 GB, the training
    took approximately 1 hour and 45 minutes to run 25 epochs.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 在配有NVIDIA GeForce RTX 2070 8 GB的Intel Core i5机器上，训练大约需要1小时45分钟运行25个epochs。
- en: After training the model, you will see a validation perplexity close to 9.5\.
    In other words, this means that, for a given word sequence, the model thinks there
    can be 9.5 different next words that are the right word (not exactly, but it is
    a close-enough approximation). Perplexity needs to be judged carefully as the
    goodness of the measure tends to be subjective. For example, as the size of the
    vocabulary increases, this number can go up. But this doesn’t necessarily mean
    that the model is bad. The number can go up because the model has seen more words
    that fit the occasion compared to when the vocabulary was smaller.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型后，您将看到接近9.5的验证困惑度。换句话说，这意味着对于给定的单词序列，模型认为可能有9.5个不同的下一个单词是正确的单词（不完全准确，但是这是一个足够接近的近似值）。困惑度需要仔细判断，因为其好坏倾向于主观。例如，随着词汇量的增加，这个数字可能会上升。但这并不一定意味着模型不好。数字之所以上升是因为模型看到了更多适合场合的词汇，与词汇量较小时相比。
- en: 'We will evaluate the model on the test data to gauge how well our model can
    anticipate some of the unseen stories without being surprised:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在测试数据上评估模型，以了解我们的模型可以多大程度地预测一些未见的故事，而不会感到惊讶：
- en: '[PRE48]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: This will give you approximately
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给您约
- en: '[PRE49]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: which is on par with the validation performance we saw. Finally, save the model
    with
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们看到的验证性能相当。最后，保存模型
- en: '[PRE50]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: In this section, you learned to train and evaluate the model. You trained the
    model on the training data set and evaluated the model on validation and testing
    sets. In the next section, you will learn how to use the trained model to generate
    new children’s stories. Then, in the following section, you will learn how to
    generate text with the model we just trained.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您学习了如何训练和评估模型。您在训练数据集上训练了模型，并在验证和测试集上评估了模型。在接下来的部分中，您将学习如何使用训练好的模型生成新的儿童故事。然后，在接下来的部分中，您将学习如何使用我们刚刚训练的模型生成文本。
- en: Exercise 4
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 练习4
- en: Assume you want to use the validation accuracy (val_accuracy) instead of validation
    perplexity (val_perlexity) to define the early stopping callback. How would you
    change the following callback?
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您想使用验证准确性（val_accuracy）而不是验证困惑度（val_perplexity）来定义早期停止回调。您将如何更改以下回调？
- en: '[PRE51]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '10.5 Generating new text from the language model: Greedy decoding'
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.5 从语言模型生成新文本：贪婪解码
- en: One of the coolest things about a language model is the generative property
    it possesses. This means that the model can generate new data. In our case, the
    language model can generate new children’s stories using the knowledge it garnered
    from the training phrase.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型最酷的一点是其具有的生成特性。这意味着模型可以生成新数据。在我们的情况下，语言模型可以利用从训练阶段获取的知识生成新的儿童故事。
- en: But to do so, we have to use some extra elbow grease. The text-generation process
    is different from the training process. During the training, we had the full sequence
    end to end. This means you can process a sequence of arbitrary length in one go.
    But when generating new text, you don’t have a sequence of text available to you;
    in fact, you are trying to generate one. You start with a random word or words
    and get an output word, and then recursively feed the current output as the next
    input to generate new text. In order to facilitate this process, we need to define
    a new version of the trained model. Let’s flesh out the generative process a bit
    more. Figure 10.5 compares the training process versus the generation/inference
    process.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 但要这么做，我们必须付出额外的努力。文本生成过程与训练过程不同。训练期间我们拥有完整的序列，可以一次性处理任意长度的序列。但是在生成新的文本时，你没有一个可用的文本序列；事实上，你正在尝试生成一个。你从一个随机的词开始，得到一个输出词，然后递归地将当前输出作为下一个输入来生成新的文本。为了促进这个过程，我们需要定义训练模型的一个新版本。让我们更详细地阐述生成过程。图10.5比较了训练过程和生成/推理过程。
- en: Define an initial word *w*[t] (random word from the vocabulary).
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义一个初始单词*w*[t](从词汇表中随机选择)。
- en: Define an initial state vector *h*[t] (initialized with zeros).
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义一个初始状态向量*h*[t](初始化为零)。
- en: Define a list, words, to hold the predicted words and initialize it with the
    initial word.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义一个列表words，用于保存预测的单词，并将其初始化为初始单词。
- en: 'For t from 1 to n:'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于t从1到n：
- en: Get the next word (*w*[t + 1]) and the state vector (*h*[t + 1]) from the model
    and assign to *w*[t] and *h*[t], respectively. This creates a recursive process,
    enabling us to generate as many words as we like.
  id: totrans-273
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从模型中获取下一个单词(*w*[t+1])和状态向量(*h*[t+1])并分别赋值给*w*[t]和*h*[t]，这样就创建了一个递归过程，使我们能够生成尽可能多的单词。
- en: Append the new word to words.
  id: totrans-274
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将新单词添加到words中。
- en: '![10-05](../../OEBPS/Images/10-05.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![10-05](../../OEBPS/Images/10-05.png)'
- en: Figure 10.5 Comparison between the language model at training time and the inference/decoding
    phrase. In the inference phase, we predict one time step at a time. In each time
    step, we get the predicted word as the input and the new hidden state as the previous
    hidden state for the next time step.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.5训练时间和推断/解码阶段的语言模型比较。在推断阶段，我们逐个时间步预测。在每个时间步中，我们将预测的单词作为输入，将新的隐藏状态作为下一个时间步的先前隐藏状态。
- en: We will use the Keras functional API to build this model, as shown in the next
    listing. First, let’s define two inputs.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Keras的函数式API构建这个模型，如下一个列表所示。首先，让我们定义两个输入。
- en: Listing 10.6 Implementation of the inference/decoding language model
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.6是推断/解码语言模型的实现。
- en: '[PRE52]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: ❶ Define an input that can take an arbitrarily long sequence of word IDs.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义一个能够接受任意长度的单词ID序列的输入。
- en: ❷ Define another input that will feed in the previous state.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义另一个输入，将上一个状态输入进去。
- en: ❸ Define an embedding layer.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 定义一个嵌入层。
- en: ❹ Get the embedding vectors from the input word ID.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 从输入的单词ID中获取嵌入向量。
- en: ❺ Define a GRU layer that returns both the output and the state. However, note
    that they will be the same for a GRU.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 定义一个GRU层，返回输出和状态。但要注意，对于GRU来说它们是相同的。
- en: ❻ Get the GRU output and the state from the model.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 从模型中获取GRU输出和状态。
- en: ❼ Compute the first fully connected layer output.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 计算第一个全连接层的输出。
- en: ❽ Define a final layer that is the same size as the vocabulary and get the final
    output of the model.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 定义一个与词汇表大小相同的最终层，并获取模型的最终输出。
- en: ❾ Define the final model that takes an input and a state vector as the inputs
    and produces the next word prediction and the new state vector as the outputs.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 定义最终模型，该模型接受一个输入和一个状态向量作为输入，并产生下一个单词预测和新的状态向量作为输出。
- en: 'We have to perform an important step after defining the model. We must transfer
    the weights from the trained model to the newly defined inference model. For that,
    we have to identify layers with trainable weights, get the weights for those layers
    from the trained model, and assign them to the new model:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义模型之后，我们必须执行一个重要的步骤。我们必须将训练模型的权重转移到新定义的推断模型中。为此，我们必须识别具有可训练权重的层，从训练模型中获取这些层的权重，并将它们分配给新模型：
- en: '[PRE53]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: To get weights of a specific layer in the trained model, you can call
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取训练模型中特定层的权重，可以调用
- en: '[PRE54]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: This will return a NumPy array with the weights. Next, to assign those weights
    to a layer, call
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 将返回一个带有权重的NumPy数组。接下来，为了将这些权重分配给一个层，调用
- en: '[PRE55]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We can now call the newly defined model recursively to generate as many bigrams
    as we like. We will discuss the process to do that in more detail. Instead of
    starting with a single random word, let’s start with a sequence of text. We will
    convert the text to bigrams and subsequently to word IDs using the Tokenizer:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Next, let’s reset the states of the model (this is not needed here because
    we’re starting fresh, but it’s good to know that we can do that). We will define
    a state vector with all zeros:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Then we will recursively predict on each bigram in the seq variable to update
    the state of the GRU model with the input sequence. Once we sweep through the
    whole sequence, we will get the final predicted bigram (that will be our first
    predicted bigram) and append that to the original bigram sequence:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'We will define a new input x with the last word ID that was predicted:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: The fun begins now. We will predict 500 bigrams (i.e., 1,000 characters) using
    the approach discussed earlier. In every iteration, we predict a new bigram and
    the new state with the infer_model using the input x and the state vector state.
    The new bigram and the new state recursively replace x and state variables with
    these new outputs (see the next listing).
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.7 Recursively predicting a new word using the previous word as an
    input
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: ❶ Get the next output and state.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Get the word ID and the word from out.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: ❸ If the word ends with space, we introduce a bit of randomness to break repeating
    text.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Essentially pick one of the top three outputs for that timestep depending
    on their likelihood.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Append the prediction cumulatively to text.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Recursively make the current prediction the next input.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that a little bit of work is required to get the final value of x as the
    model predicts a probability prediction (assigned to out) as its output, not a
    word ID. Furthermore, we’re going to use some additional logic to improve the
    randomness (or entropy, one could say) in the generated text by picking a word
    randomly from the top three words. But we don’t pick them with equal probability.
    Rather, let’s use their predicted probabilities to predict the word. To make sure
    we don’t get too much randomness and to avoid getting random tweaks in the middle
    of words, let’s do this only when the last character is a space character. The
    final word ID (picked either as the word with the highest probability or at random)
    is assigned to the variable x. This process will repeat for 500 steps, and by
    the end, you’ll have a cool machine-generated story on your hands. You can print
    the final text and see how it looks. To do that, simply join the bigrams in the
    text sequence as follows:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: This will display
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: It’s certainly not bad for a simple single layer GRU model. Most of the time,
    the model spits out actual words. But there are occasional spelling mistakes and
    more frequent grammatical errors haunting the text. Can we do better? In the next
    section, we are going to learn about a new technique to generate text called beam
    search.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 5
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume you have the following code that chooses the next word greedily without
    randomness. You run this code and realize that the results are very bad:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: What do you think is the reason for the poor performance?
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '10.6 Beam search: Enhancing the predictive power of sequential models'
  id: totrans-321
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can do better that greedy decoding. Beam search is a popular decoding algorithm
    used in sequential/time-series tasks like this to generate more accurate predictions.
    The idea behind beam search is very simple. Unlike in greedy decoding, where you
    predict a single timestep, with beam search you predict several time steps ahead.
    In each time step, you get the top k predictions and branch out from them. Beam
    search has two important parameters: beam width and beam depth. Beam width controls
    how many candidates are considered at each step, whereas beam depth determines
    how many steps to search. For example, for a beam width of 3 and a beam depth
    of 5, the number of possible options become 3⁵ = 243\. Figure 10.6 further illustrates
    how beam search works.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '![10-06](../../OEBPS/Images/10-06.png)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6 Beam search in action. Beam search leads to better solutions as
    it looks several steps into the future to make a prediction. Here, we are performing
    a beam search with beam width = 3 and beam depth = 5.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: 'First let’s define a function that will take a model, input, and state and
    return the output and the new state:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Then, using this function, we will define a recursive function (recursive_fn)
    that will recursively predict the next word from the previous prediction for a
    predefined depth (defined by beam_depth). At each time step, we consider the top
    k candidates (defined by beam_width) to branch out from. The recursive_fn function
    will populate a variable called results. results will contain a list of tuples,
    where each tuple represents a single path in the search. Specifically, each tuple
    contains the
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: Elements in the path
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The joint log probability of that sequence
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final state vector to pass to the GRU
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This function is outlined in the following listing.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.8 Implementation of the beam search as a recursive function
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: ❶ Define an outer wrapper for the computational function of beam search.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Define an inner function that is called recursively to find the beam paths.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Define the base case for terminating the recursion.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Append the result we got at the termination so we can use it later.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: ❺ During recursion, get the output word and the state by calling the model.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Get the top k candidates for that step.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: ❼ For each candidate, compute the joint probability. We will do this in log
    space to have numerical stability.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Penalize joint probability whenever the same symbol repeats.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: ❾ Append the current candidate to the sequence that maintains the current search
    path at the time.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: ❿ Call the function recursively to find the next candidates.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: ⓫ Make a call to the recursive function to trigger the recursion.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: ⓬ Sort the results by log probability.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can use this beam_search function as follows: we will use a beam
    depth of 7 and a beam width of 2\. Up until the for loop, things are identical
    to how we did things using greedy decoding. In the for loop, we get the results
    list (sorted high to low on joint probability). Then, similar to what we did previously,
    we’ll get the next prediction randomly from the top 10 predictions based on their
    likelihood as the next prediction. The following listing delineates the code to
    do so.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.9 Implementation of beam search decoding to generate a new story
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: ❶ Define a sequence of ngrams from an initial sequence of text.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Convert the bigrams to word IDs.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Build up model state using the given string.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Get the predicted word after processing the sequence.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Predict for 100 time steps.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Get the results from beam search.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Get one of the top 10 results based on their likelihood.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Replace x and state with the new values computed.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: 'Running the code in listing 10.9, you should get text similar to the following:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: The text generated with beam search reads much better than the text we saw with
    greedy decoding. You see improved grammar and less spelling mistakes when text
    is generated with beam search.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: Diverse beam search
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: 'Different alternatives to beam search have surfaced over time. One popular
    alternative is called *diverse beam search*, introduced in the paper, “Diverse
    Beam Search: Decoding Diverse Solutions from Neural Sequence Models” by Vijayakumar
    et al. ([https://arxiv.org/pdf/1610.02424.pdf](https://arxiv.org/pdf/1610.02424.pdf)).
    Diverse beam search overcomes a critical limitation of vanilla beam search. That
    is, if you analyze the most preferred candidate sequences proposed by beam search,
    you’ll find that they differ only by a few elements. This also can lead to repeating
    text that lacks variety. Diverse beam search comes up with an optimization problem
    that incentivizes diversity of the proposed candidates during the search. You
    can read more about this in the paper.'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our discussion on language modeling. In the next chapter, we
    will learn about a new type of NLP problem known as a sequence-to-sequence problem.
    Let’s summarize the key highlights of this chapter.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 6
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: You used the line result = beam_search(infer_model, x, state, 7, 2) to perform
    beam search. You want to consider five candidates at a given time and search only
    three levels deep into the search space. How would you change the line?
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-365
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Language modeling is the task of predicting the next word given a sequence of
    words.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Language modeling is the workhorse of some of the top-performing models in the
    field, such as BERT (a type of Transformer-based model).
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To limit the size of the vocabulary and avoid computational issues, n-gram representation
    can be used.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In n-gram representation, text is split into fixed-length tokens, as opposed
    to doing character-level or word-level tokenization. Then, a fixed sized window
    is moved over the sequence of text to generate inputs and targets for the model.
    In TensorFlow, you can use the tf.data.Dataset.window() function to implement
    this functionality.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The gated recurrent unit (GRU) is a sequential model that operates similarly
    to an LSTM, where it jumps from one input to the next in a sequence while generating
    a state at each time step.
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The GRU is a compact version of an LSTM model that maintains a single state
    and two gates but delivers on-par performance.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perplexity measures how surprised the model was to see the target word given
    the input sequence.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The computation of the perplexity measure is inspired by information theory,
    in which the measure entropy is used to quantify the uncertainty in a random variable
    that represents an event where the outcomes are generated with some underlying
    probability distribution.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The language models after training can be used to generate new text. There
    are two popular techniques—greedy decoding and beam search decoding:'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Greedy decoding predicts one word at a time, where the predicted word is used
    as the input in the next time step.
  id: totrans-375
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Beam search decoding predicts several steps into the future and selects the
    sequence that gives the highest joint probability.
  id: totrans-376
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Answers to exercises
  id: totrans-377
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Exercise 1**'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '**Exercise 2**'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '**Exercise 3**'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: Scenario A will have the lowest perplexity.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 4**'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '**Exercise 5**'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: The line out, new_s = infer_model.predict([x, s]), is wrong. The state is not
    recursively updated in the inference model. This will lead to a working model
    but with poor performance. It should be corrected as out, s = infer_model.predict([x,
    s]).
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 6**'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
