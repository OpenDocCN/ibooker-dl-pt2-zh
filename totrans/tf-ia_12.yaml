- en: '10 Natural language processing with TensorFlow: Language modeling'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Implementing an NLP data pipeline with TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a GRU-based language model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a perplexity metric for evaluating language models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining an inference model to generate new text from the trained model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing beam search to uplift the quality of generated text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the last chapter, we discussed an important NLP task called sentiment analysis.
    In that chapter, you used a data set of video game reviews and trained a model
    to predict whether a review carried a negative or positive sentiment by analyzing
    the text. You learned about various preprocessing steps that you can perform to
    improve the quality of the text, such as removing stop words and lemmatizing (i.e.,
    converting words to a base form; e.g., plural to singular). You used a special
    type of model known as long short-term memory (LSTM). LSTM models can process
    sequences such as sentences and learn the relationships and dependencies in them
    to produce an outcome. LSTM models do this by maintaining a state (or memory)
    containing information about the past, as it processes a sequence one element
    at a time. The LSTM model can use the memory of past inputs it has seen along
    with the current input to produce an output at any given time.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss a new task known as *language modeling*. Language
    modeling has been at the heart of NLP. Language modeling refers to the task of
    predicting the next word given a sequence of previous words. For example, given
    the sentence, “I went swimming in the ____,” the model would predict the word
    “pool.” Ground-shattering models like BERT (bidirectional encoder representation
    from Transformers, which is a type of Transformer-based model) are trained using
    language modeling tasks. This is a prime example of how language modelling can
    help to actualize innovative models that go on to be used in a plethora of areas
    and use cases.
  prefs: []
  type: TYPE_NORMAL
- en: In my opinion, language modeling is an underdog in the world of NLP. It is not
    appreciated enough, mostly due to the limited use cases the task itself helps
    to realize. However, language modeling can provision the much-needed linguistic
    knowledge (e.g., semantics, grammar, dependency parsing, etc.) to solve other
    downstream use cases (e.g., information retrieval, question answering, machine
    translation, etc.). Therefore, as an NLP practitioner, you must understand the
    language modeling task.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will build a language model. You will learn about the various
    preprocessing steps involved, such as using n-grams instead of full words as features
    for the model to reduce the size of the vocabulary. You can convert any text to
    n-grams by splitting it every n characters (e.g., if you use bi-grams, aabbbccd
    becomes aa, bb, bc, and cd). You will define a tf.data pipeline that will do most
    of this preprocessing for us. Next, you will use a close relative of the LSTM
    model known as *gated recurrent unit* (GRU) to do the language modeling task.
    GRU is much simpler than the LSTM model, making it faster to train while maintaining
    similar performance to the LSTM model. We will use a special metric called perplexity
    to measure how good our model is. Perplexity measures how surprised the model
    was to see the next word in the corpus given the previous words. You will learn
    more about this metric later in the chapter. Finally, you will learn about a technique
    known as *beam search*, which can uplift the quality of the text generated by
    the model by a significant margin.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1 Processing the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You’ve been closely following a new generation of deep learning models that
    have emerged, known as Transformers. These models have been trained using language
    modeling. It is a technique that can be used to train NLP models to generate stories/
    Python code/movie scripts, depending on the training data used. The idea is that
    when a sequence of n words, predict the n + 1 word. The training data can easily
    be generated from a corpus of text by taking a sequence of text as the input and
    shifting it right by 1 to generate the targets. This can be done at a character
    level, word level, or n-gram level. We will use two-grams for the language modeling
    task. We will use a children’s story data set from Facebook known as bAbI ([https://research.fb.com/downloads/babi/](https://research.fb.com/downloads/babi/)).
    You will create a TensorFlow data pipeline that performs these transformations
    to generate inputs and targets from text.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.1 What is language modeling?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have briefly discussed the task of language modeling. In a nutshell, language
    modeling, for the text *w*[1], *w*[2], ..., *w*[n][-1], *w*[n], where *w*[i] is
    the *i*^(th) word in the text, computes the probability of *w*[n] given *w*[1],
    *w*[2], ..., *w*[n][-1]. In mathematical notation
  prefs: []
  type: TYPE_NORMAL
- en: '*P*(*w* [n]|*w*[1], *w*[2], ..., *w*[n][-1])'
  prefs: []
  type: TYPE_NORMAL
- en: In other words, it predicts *w*[n] given *w*[1], *w*[2], ..., *w*[n][-1]. When
    training the model, we train it to maximize this probability; in other words
  prefs: []
  type: TYPE_NORMAL
- en: argmax[W] *P*(*w* [n]|*w*[1], *w*[2], ..., *w*[n][-1])
  prefs: []
  type: TYPE_NORMAL
- en: where the probability is computed using a model that has the trainable weights/
    parameters W. This computation becomes computationally infeasible for large texts,
    as we need to look at it from the current word all the way to the very first word.
    To make this computationally realizable, let’s use the *Markov property*, which
    states that you can approximate this sequence with limited history; in other words
  prefs: []
  type: TYPE_NORMAL
- en: '*P*(*w* [n]|*w*[1], *w*[2], ..., *w*[n][-1]) ≈ *P*(*w* [n]|*w*[k], *w*[k+1],
    ..., *w*[n][-1]) for some *k*'
  prefs: []
  type: TYPE_NORMAL
- en: As you can imagine, the smaller the *k*, the better the approximation is.
  prefs: []
  type: TYPE_NORMAL
- en: Cloze task
  prefs: []
  type: TYPE_NORMAL
- en: Transformer models like BERT use a variant of language modeling called m*asked
    language modeling*. Masked language modeling is inspired by the *Cloze* task or
    the Cloze test. The idea is to ask the student, when given a sentence with one
    or more blanks, to fill in the blanks. This has been used in language assessment
    tests to measure the linguistic competency of students. In masked language modeling,
    the model becomes the student. Words are removed from inputs at random, and the
    model is asked to predict the missing word. This forms the foundation of the training
    process used in models like BERT.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.2 Downloading and playing with data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As the very first step, let’s download the data set using the code in the following
    listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.1 Downloading the Amazon review data set
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: ❶ If the tgz file containing data has not been downloaded, download the data.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Write the downloaded data to the disk.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ If the tgz file is available but has not been extracted, extract it to the
    given directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 10.1 will download the data to a local folder if it doesn’t already
    exist and extract the content. If you look in the data folder (specifically, data/lm/CBTest/data),
    you will see that it has three text files: cbt_train.txt, cbt_valid.txt, and cbt_test.txt.
    Each file contains a set of stories. We are going to read these files into memory.
    We will define a simple function to read these files into memory in the next listing.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.2 Reading the stories in Python
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Define a list to hold all the stories.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Define a list to hold a story.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Whenever, we encounter a line that starts with _BOOK_TITLE, it’s a new story.
  prefs: []
  type: TYPE_NORMAL
- en: ❹ If we saw the beginning of a new story, add the already existing story to
    the list stories.
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Reset the list containing the current story.
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Append the current row of text to the list s.
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Handle the edge case of the last story remaining in s once the loop is over.
  prefs: []
  type: TYPE_NORMAL
- en: 'This code opens a given file and reads it row by row. We do have some additional
    logic to break the text into individual stories. As said earlier, each file contains
    multiple stories. And we want to create a list of strings at the end, where each
    string is a single story. The previous function does that. Next, we can read the
    text files and store them in variables like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, stories will contain the training data, val_stories will contain the
    validation data, and finally, test_stories will contain test data. Let’s quickly
    look at some high-level information about the data set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This code checks how many stories are in each data set and prints the first
    100 characters in the 11^(th) story in the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Out of curiosity, let’s also analyze the vocabulary size we have to work with.
    To analyze the vocabulary size, we will first convert our list of strings to a
    list of lists of strings, where each string is a single word. Then we can leverage
    the built-in Counter object to get the word frequency of the text corpus. After
    that, we will create a pandas Series object with the frequencies as the values
    and words as indices and see how many words occur more than 10 times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This will return
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Nearly 15,000 words; that’s quite a vocabulary—and that’s just the words that
    appear more than 10 times. In the previous chapter, we dealt with a vocabulary
    of approximately 11,000 words. So why should we be worried about the extra 4,000?
    Because more words mean more features for the model, and that means a larger number
    of parameters and more chances of overfitting. The short answer is it really depends
    on your use case.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in the sentiment analysis model we had in the last chapter, the
    final prediction layer was a single-node fully connected layer, regardless of
    the vocabulary size. However, in language modeling, the final prediction layer’s
    dimensionality depends on the vocabulary size, as the final goal is to predict
    the next word. This is done through a softmax layer that represents the probabilistic
    likelihood of the next word over the whole vocabulary, given a sequence of words.
    Not only the memory requirement, but also the computational time, increase as
    the softmax layer grows. Therefore, it is worthwhile investigating other techniques
    to reduce the vocabulary size.
  prefs: []
  type: TYPE_NORMAL
- en: Is large vocabulary size the ultimate weakness of the softmax layer?
  prefs: []
  type: TYPE_NORMAL
- en: A major weakness of the softmax layer is its computational complexity. The softmax
    layer needs to first perform a matrix multiplication to get the logits (i.e.,
    unnormalized scores output by the final layer of the network). Then it needs to
    sum over the last axis to compute softmax probabilities of the output. Specifically,
    for the input h, the logits of the softmax layer are computed as
  prefs: []
  type: TYPE_NORMAL
- en: '*s* = *h*. *W* + *b* where *W* ∈ *R*^(|h|×|V|) [∧] *b* [∈] *R*^(|V|)'
  prefs: []
  type: TYPE_NORMAL
- en: where W is the weight matrix, b is the bias of that final layer, |h| is the
    size of the input, and |V| is the size of the vocabulary. Then softmax normalization
    is applied as
  prefs: []
  type: TYPE_NORMAL
- en: '![10_00a](../../OEBPS/Images/10_00a.png)'
  prefs: []
  type: TYPE_IMG
- en: These computations should make it evident to you that a large vocabulary (whose
    size can easily reach hundreds of thousands for a real-world application) will
    create problems in executing this computation in a limited time during model training.
    Having to do this for thousands of batches of data makes the problem even worse.
    Therefore, better techniques to compute the loss without using all the logits
    have emerged. Two popular choices are
  prefs: []
  type: TYPE_NORMAL
- en: Noise contrastive estimation (NCE) loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hierarchical softmax
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Noise contrastive estimation (NCE)**'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will look at the prime motivation that drives these methods but not delve
    into the specifics, as this is considered out of scope for this book. For the
    details on these topics, refer to [https://ruder.io/word-embeddings-softmax](https://ruder.io/word-embeddings-softmax).
    NCE only uses the logit indexed by the true target and a small sample of k random
    set of logits (termed *noise*) to compute the loss. The more you match the true
    data distribution in the noise sample, the better the results will be. Specifically,
    if the true target is *s* and the logit at the index *s* is termed *s*[i], the
    following loss is used:'
  prefs: []
  type: TYPE_NORMAL
- en: '![10_00b](../../OEBPS/Images/10_00b.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *σ* denotes that the sigmoidal activation is a common activation function
    used in neural networks and is computed as *σ* (*x*) = 1/(1 + *e*^(-x)), where
    *s*[i] represents the logit value of the true target i, and j represents an index
    sampled from a predefined distribution over the vocabulary *P*[n].
  prefs: []
  type: TYPE_NORMAL
- en: '**Hierarchical softmax**'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike standard softmax, which has a flat structure where each node represents
    an element in the vocabulary, hierarchical softmax represents the words in the
    vocabulary as the leaf nodes in a binary tree, and the task becomes choosing whether
    to go left or right in order to reach the right node. The following figure depicts
    the formation
  prefs: []
  type: TYPE_NORMAL
- en: of the layer when hierarchical softmax is used. As is evident, to infer the
    probability of a word given the previous sequence of words, the layer only has
    to go through three steps of computation at max (shown by the dark path), as opposed
    to evaluating across all seven possible words.
  prefs: []
  type: TYPE_NORMAL
- en: '![10-00unnumb](../../OEBPS/Images/10-00unnumb.png)'
  prefs: []
  type: TYPE_IMG
- en: Hierarchical softmax representation of the final layer. The dark path represents
    the path the model must follow to compute the probability of P(home| I went).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will see how we can deal with language in the case of a large vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.3 Too large vocabulary? N-grams to the rescue
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here, we start the first step of defining various text preprocessors and the
    data pipeline. We suspect that going forward with a large vocabulary size can
    have adverse repercussions on our modeling journey. Let’s find some ways to reduce
    the vocabulary size. Given that children’s stories use a relatively simple language
    style, we can represent text as n-grams (at the cost of the expressivity of our
    model). N-grams are an approach where a word is decomposed to finer sub-words
    of fixed length. For example, the bigrams (or two-grams) of the sentence
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: are
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: where three grams would be
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The unigrams (or one-grams) would simply be the individual characters. In other
    words, we are moving a window of fixed length (with a stride of 1), while reading
    the characters within that window at a time. You could also generate n-grams without
    overlaps by moving the window at a stride equal to the length of the window. For
    example, bigrams without overlapping would be
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Which one to use depends on your use case. For the language modeling task, it
    makes sense to use the non-overlapping approach. This is because by joining the
    n-grams that we generated, we can easily generate readable text. For certain use
    cases, the non-overlapping approach can be disadvantageous as it leads to a coarser
    representation of text because it doesn’t capture all the different n-grams that
    appear in text.
  prefs: []
  type: TYPE_NORMAL
- en: 'By using bigrams instead of words to develop your vocabulary, you can cut down
    the size of the vocabulary by a significant factor. There are many other advantages
    that come with the n-gram approach, as we will see soon. We will write a function
    to generate n-grams given a text string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'All we do here is go from the beginning to the end of the text with a stride
    equal to n and read the sequence of characters from position i to i+n. We can
    test how this performs on sample text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This will print the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s now repeat the process for analyzing the vocabulary size, but with n-grams
    instead of words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Now, if we check the number of words that appear at least 10 times in the text
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: we will see
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Wow! Compared to the 15,000 words we had, 735 is much smaller and more manageable.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages of n-grams
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of the main advantages of using n-grams over words:'
  prefs: []
  type: TYPE_NORMAL
- en: The limited number of n-grams for small n limits the vocabulary size, leading
    to both memory and computational advantages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: N-grams lead to fewer chances of out-of-vocabulary words, as an unseen word
    can usually be constructed using n-grams seen in the past.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 10.1.4 Tokenizing text
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will *tokenize* the text now (i.e., split a string into a list of smaller
    tokens—words). By the end of this section, you will have defined and fitted a
    tokenizer on the bigrams generated for your text. First, let’s import the Tokenizer
    from TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We don’t have to do any preprocessing and want to convert text to word IDs
    as it is. We will define the num_words argument to limit the vocabulary size as
    well as an oov_token that will be used to replace all the n-grams that appear
    less than 10 times in the training corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s generate n-grams from the stories in training data. train_ngram_stories
    will be a list of lists of strings, where the inner list represents a list of
    bigrams for a single story and the outer list represents all the stories in the
    training data set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We will fit the Tokenizer on the two-grams of the training stories:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now convert all training, validation, and testing stories to sequences of IDs,
    using the already fitted Tokenizer trained using two-grams from the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s analyze how the data looks after converting to word IDs by printing some
    test data. Specifically, we’ll print the first three story strings (test_stories),
    n-gram strings (test_ngram_stories), and word ID sequences (test_data_seq):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 10.1.5 Defining a tf.data pipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now the preprocessing has happened, and we have text converted to word ID sequences.
    We can define the tf.data pipeline that will deliver the final processed data
    ready to be consumed by the model. The main steps involved in the process are
    illustrated in figure 10.1.
  prefs: []
  type: TYPE_NORMAL
- en: '![10-01](../../OEBPS/Images/10-01.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 The high-level steps of the data pipeline we will be developing.
    First the individual stories are broken down to fixed-length sequences (or windows).
    Then, from the windowed sequences, inputs and targets are generated.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we did before, let’s define the word ID corpus as a tf.RaggedTensor object,
    as the sentences in the corpus have variable sequence lengths:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Remember that a ragged tensor is a tensor that has variable-sized dimensions.
    Then we will shuffle the data so that stories come at a random order if shuffle
    is set to True (e.g., training time):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Now comes the tricky part. In this section, we will see how to generate fixed-sized
    windowed sequences from an arbitrarily long text. We will do that through a series
    of steps. This section can be slightly complex compared to the rest of the pipeline.
    This is because there will be interim steps that result in data sets nested up
    to three levels. Let’s try to go through this in as much detail as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s make it clear what we need to achieve. The first steps we need
    to perform are to do the following for each individual story S:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a tf.data.Dataset() object containing the word IDs of the story S as
    its items.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Call the tf.data.Dataset.window() function to window word IDs with a n_seq+1-sized
    window and a predefined shift. The window() function returns a WindowDataset object
    for each story S.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After this, you will have a three-level nested data structure having the specification
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We will need to flatten this data set and untangle the nesting in our data
    set to end up with a flat tf.data.Dataset. You can remove these inner nestings
    using the tf.data.Dataset.flat_map() function. We will soon see how to use the
    flat_ map() function. To be specific, we have to use two flat_map calls to remove
    two levels of nesting so that we end up with only the flat original data set containing
    simple tensors as elements. In TensorFlow, this process can be achieved using
    the following line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This is what we are doing here: first, we create a tf.data.Dataset object from
    a single story (x) and then call the tf.data.Dataset.window() function on that
    to create the windowed sequences. This windowed sequence contains windows, where
    each window is a sequence with n_seq+1 consecutive elements in the story x.'
  prefs: []
  type: TYPE_NORMAL
- en: Then we call the tf.data.Dataset.flat_map() function, where, for each window
    element, we get all the individual IDs as a single batch. In other words, a single
    window element produces a single batch with all the elements in that window. Make
    sure you use drop_remainder=True; otherwise, the data set will return smaller
    subwindows within that window that contain fewer elements. Using tf.data.Dataset.flat_map()
    instead of map makes sure that the inner-most nesting will be removed. This whole
    thing is called with a tf.data.Dataset.flat_map() call, which gets rid of the
    next level of nesting immediately following the innermost nesting we removed.
    It is quite a complex process for a single liner. I suggest you go through it
    again if you have not fully understood the process.
  prefs: []
  type: TYPE_NORMAL
- en: You might notice that we are defining the window size as n_seq+1 and not n_seq.
    The reason for this will become evident later, but using n_seq+1 makes our life
    so much easier when we have to generate inputs and targets from the windowed sequences.
  prefs: []
  type: TYPE_NORMAL
- en: Difference between map and flat_map in tf.data.Dataset
  prefs: []
  type: TYPE_NORMAL
- en: Both functions tf.data.Dataset.map() and tf.data.Dataset.flat_map() achieve
    the same result but with different data set specifications. For example, assume
    the data set
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Using the tf.data.Dataset.map() function to square the elements as
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: will result in a data set that has the elements
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the result has the same structure as the original tensor. Using
    the tf.data.Dataset.flat_map() function to square the elements as
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: will result in a data set that has
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, that inner-most nesting has been flattened to produce a flat
    sequence of elements.
  prefs: []
  type: TYPE_NORMAL
- en: 'The hardest part of our data pipeline is done. By now, you have a flat data
    set, and each item is n_seq+1 consecutive word IDs belonging to a single story.
    Next, we will perform a window-level shuffle on the data. This is different from
    the first shuffle we did as that was on the story level (i.e., not the window
    level):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We’re then going to batch the data so that we will get a batch of windows every
    time we iterate the data set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the reason we chose the sequence length as n_seq+1 will become clearer.
    Now we will split the sequences into two versions, where one sequence will be
    the other shifted to the right by 1\. In other words, the targets to this model
    will be inputs shifted to the right by 1\. For example, if the sequence is [0,1,2,3,4],
    then the two resulting sequences will be [0,1,2,3] and [1,2,3,4]. Furthermore,
    we will use prefetching to speed up the data ingestion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Finally, the full code can be encapsulated in a function as in the next listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.3 The tf.data pipeline from free text sequences
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Define a tf.dataset from a ragged tensor created from data_seq.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ If shuffle is set, shuffle the data (shuffle story order).
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Here we create windows from longer sequences, given a window size and a shift,
    and then use a series of flat_map operations to remove the nesting that’s created
    in the process.
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Shuffle the data (shuffle the order of the windows generated).
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Batch the data.
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Split each sequence into an input and a target and enable pre-fetching.
  prefs: []
  type: TYPE_NORMAL
- en: All this hard work wouldn’t mean as much as it should unless we looked at the
    generated data
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: which will show you
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Great, you can see that we get a tuple of tensors as a single batch: inputs
    and targets. Moreover, you can validate the correctness of the results, as we
    can clearly see that the targets are the input shifted to the right by 1\. One
    last thing: we will save the same hyperparameters to the disk. Particularly'
  prefs: []
  type: TYPE_NORMAL
- en: n in n-grams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vocabulary size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequence length
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Here, we are defining the sequence length n_seq=100; this is the number of bigrams
    we will have in a single input/label sequence.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned about the data used for language modeling and defined
    a capable tf.data pipeline that can convert sequences of text into input label
    sequences that can be used to train the model directly. Next, we will define a
    machine learning model to generate text with.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 1
  prefs: []
  type: TYPE_NORMAL
- en: You are given a sequence x that has values [1,2,3,4,5,6,7,8,9,0]. You have been
    asked to write a tf.data pipeline that generates an input and target tuple, and
    the target is the input shifted two elements to the right (i.e., the target of
    the input 1 is 3). You have to do this so that a single input/target has three
    elements and no overlap between the consecutive input sequences. For the previous
    sequence it should generate [([1,2,3], [3,4,5]), ([6,7,8], [8,9,0])].
  prefs: []
  type: TYPE_NORMAL
- en: '10.2 GRUs in Wonderland: Generating text with deep learning'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now we’re on to the rewarding part: implementing a cool machine learning model.
    In the last chapter, we talked about deep sequential models. Given the sequential
    nature of the data, you probably have guessed that we’re going to use one of the
    deep sequential models like LSTMs. In this chapter, we will use a slightly different
    model called *gated recurrent units* (GRUs). The principles that drive the computations
    in the model remain the same as LSTMs. To maintain the clarity of our discussion,
    it’s worthwhile to remind ourselves how LSTM models work.'
  prefs: []
  type: TYPE_NORMAL
- en: LSTMs are a family of deep neural networks that are specifically designed to
    process sequential data. They process a sequence of inputs, one input at a time.
    An LSTM cell goes from one input to the next while producing outputs (or states)
    at each time step (figure 10.2). Additionally, to produce the outputs of a given
    time step, LSTMs uses previous outputs (or states) it produced. This property
    is very important for LSTMs and gives them the ability to memorize things over
    time.
  prefs: []
  type: TYPE_NORMAL
- en: '![10-02](../../OEBPS/Images/10-02.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 Overview of the LSTM model and how it processes a sequence of inputs
    spread over time
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s summarize what we learned about LSTMs in the previous chapter, as that
    will help us to compare LSTMs and GRUs. An LSTM has two states known as the cell
    state and the output state. The cell state is responsible for maintaining long-term
    memory, whereas the output state can be thought of as the short-term memory. The
    outputs and interim results within an LSTM cell are controlled by three gates:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Input gate*—Controls the amount of the current input that will contribute
    to the final output at a given time step'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Forget gate*—Controls how much of the previous cell state affects the current
    cell state computation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Output gate*—Controls how much the current cell state contributes to the final
    output produced by the LSTM model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The GRU model was introduced in the paper “Learning Phrase Representations
    using RNN Encoder-Decoder for Statistical Machine Translation” by Cho et al. ([https://arxiv.org/pdf/1406.1078v3.pdf](https://arxiv.org/pdf/1406.1078v3.pdf)).
    The GRU model can be considered a simplification of the LSTM model while preserving
    on-par performance. The GRU cell has two gates:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Update gate* (*z*[t])—Controls how much of the previous hidden state is carried
    to the current hidden state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Reset gate* (*r*[t])—Controls how much of the hidden state is reset with the
    new input'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unlike the LSTM cell, a GRU cell has only one state vector. In summary, there
    are two major changes in a GRU compared to an LSTM model:'
  prefs: []
  type: TYPE_NORMAL
- en: Both the input gate and forget gate are combined into a single gate called the
    update gate (*z*[t]). The input gate is computed as (1-*z*[t]), whereas the forget
    gate stays *z*[t].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There’s only one state (*h*[t]) in contrast to the two states found in an LSMT
    cell (i.e., cell state and the output state).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Figure 10.3 depicts the various components of the GRU cell. Here is the full
    list of equations that make a GRU spin:'
  prefs: []
  type: TYPE_NORMAL
- en: '*r*[t] = σ(*W*[rh]*h*[t-1] + *W*[rx]*x*[t] + *b*[r])'
  prefs: []
  type: TYPE_NORMAL
- en: '*z*[t] = σ(*W*[zh]*h*[t-1] + *W*[zx]*x*[t] + *b*[z])'
  prefs: []
  type: TYPE_NORMAL
- en: '*h̃*[t] = tanh(*W*[h]*(rh*[t-1]) + *W*[x]*x*[t] + *b*)'
  prefs: []
  type: TYPE_NORMAL
- en: '*h*[t] = (*z*[th]*h*[t-1] + (1 - *z*[t] )*h̃*[t]'
  prefs: []
  type: TYPE_NORMAL
- en: '![10-03](../../OEBPS/Images/10-03.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 Overview of the computations transpiring in a GRU cell
  prefs: []
  type: TYPE_NORMAL
- en: 'This discussion was immensely helpful to not only understand the GRU model
    but also to learn how it’s different from an LSTM cell. You can define a GRU cell
    in TensorFlow as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The parameter units, return_state and return_sequences, have the same meaning
    as they do in the context of an LSTM cell. However, note that the GRU cell has
    only one state; therefore, if return_state=true, the same state is duplicated
    to mimic the output state and the cell state of the LSTM layer. Figure 10.4 shows
    what these parameters do for a GRU layer.
  prefs: []
  type: TYPE_NORMAL
- en: '![10-04](../../OEBPS/Images/10-04.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 Changes in results depending on the return_state and return_sequences
    arguments of the GRU cell
  prefs: []
  type: TYPE_NORMAL
- en: We now know everything we need to define the final model (listing 10.4). Our
    final model will consist of
  prefs: []
  type: TYPE_NORMAL
- en: An embedding layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A GRU layer (1,024 units) that returns all the final state vectors as a tensor
    that has shape [batch size, sequence length, number of units]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Dense layer with 512 units and ReLU activation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Dense layer with n_vocab units and softmax activation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Listing 10.4 Implementing the language model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Define an embedding layer to learn word vectors of the bigrams.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Define an LSTM layer.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Define a Dense layer.
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Define a final Dense layer and softmax activation.
  prefs: []
  type: TYPE_NORMAL
- en: You will notice that the Dense layer after the GRU receives a three-dimensional
    tensor (as opposed to the typical two-dimensional tensor passed to Dense layers).
    Dense layers are smart enough to work with both two-dimensional and three-dimensional
    inputs. If the input is three-dimensional (like in our case), then a Dense layer
    that accepts a [batch size, number of units] tensor is passed through all the
    steps in the sequence to generate the Dense layer’s output. Also note how we are
    separating the softmax activation from the Dense layer. This is actually an equivalent
    of
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: We will not prolong the conversation by reiterating what’s shown in listing
    10.4, as it is self-explanatory.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 2
  prefs: []
  type: TYPE_NORMAL
- en: You have been given the model as follows and have been asked to add another
    GRU layer with 512 units that returns all the state outputs, on top of the existing
    GRU layer. What changes would you make to the following code?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we learned about gated recurrent units (GRUs) and how they
    compare to LSTMs. Finally, we defined a language model that can be trained on
    the data we downloaded and processed earlier. In the next section, we are going
    to learn about evaluation metrics for assessing the quality of generated text.
  prefs: []
  type: TYPE_NORMAL
- en: 10.3 Measuring the quality of the generated text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Performance monitoring has been an integral part of our modeling journey in
    every chapter. It is no different here. Performance monitoring is an important
    aspect of our language model, and we need to find metrics that are suited for
    language models. Naturally, given that this is a classification task, you might
    be thinking, “Wouldn’t accuracy be a good choice for a metric?” Well, not quite
    in this task.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if the language model is given the sentence “I like my pet dog,”
    then when asked to predict the missing word given “I like my pet ____,” the model
    might predict “cat,” and the accuracy would be zero. But that’s not correct; “cat”
    makes as much sense as “dog” in this example. Is there a better solution here?
  prefs: []
  type: TYPE_NORMAL
- en: Enter perplexity! Intuitively, *perplexity* measures how “surprised” the model
    was to see a target given the previous word sequence. Before understanding perplexity,
    you need to understand what “entropy” means.
  prefs: []
  type: TYPE_NORMAL
- en: '*Entropy* is a term coined by the famous Claude Shannon, who is considered
    the father of information theory. Entropy measures the surprise/uncertainty/randomness
    of an event. The event can have some outcome generated by an underlying probability
    distribution over all the possible outcomes. For example, if you consider tossing
    a coin (with a probability p of landing on heads) an event, if p = 0.5, you will
    have the maximum entropy, as that is the most uncertain scenario for a coin toss.
    If p = 1 or p = 0, then you have the minimum entropy, as you know what the outcome
    is before tossing the coin.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The original interpretation of entropy is the expected value of the number
    of bits required to send a signal or a message informing of an event. A bit is
    a unit of memory, which can be 1 or 0\. For example, you are the commander of
    an army that’s at war with countries A and B. Now you have four possibilities:
    A and B both surrender, A wins and B loses, A loses and B wins, and both A and
    B win. If all these events are equally likely to happen, you need two bits to
    send a message, where each bit represents whether that country won. Entropy of
    a random variable X is quantified by the equation'
  prefs: []
  type: TYPE_NORMAL
- en: '![10_04a](../../OEBPS/Images/10_04a.png)'
  prefs: []
  type: TYPE_IMG
- en: where *x* is an outcome of *X*. Believe it or not, we have been using this equation
    without knowing it every time we used the categorical cross-entropy loss. The
    crux of the categorical cross entropy is this equation. Coming back to the perplexity
    measure, perplexity is simply
  prefs: []
  type: TYPE_NORMAL
- en: '*Perplexity =* 2^(H(X))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since perplexity is a function of entropy, it measures how surprised/uncertain
    the model was to see the target word, given the sequence of previous words. Perplexity
    can also be thought of as the number of all possible combinations of a given signal.
    For example, assume you are sending a message with two bits, where all the events
    are equally likely; then the entropy = 2, which means perplexity = 2^2 = 4\. In
    other words, there are four combinations two bits can be in: 00, 01, 10, and 11.'
  prefs: []
  type: TYPE_NORMAL
- en: In more of a modeling perspective, you can think of perplexity as the number
    of different targets that the model thinks fit the blank as the next word, given
    a sequence of previous words. The smaller this number, the better, as that means
    the model is trying to find a word from a smaller subset, indicating signs of
    language understanding.
  prefs: []
  type: TYPE_NORMAL
- en: To implement perplexity, we will define a custom metric. The computation is
    very simple. We compute the categorical cross entropy and then exponentiate it
    to get the perplexity. The categorical cross entropy is simply an extension of
    the entropy to measure entropy in classification problems with more than two classes.
    For an input example (*x*[i],*y*[i]), it is typically defined as
  prefs: []
  type: TYPE_NORMAL
- en: '![10_04b](../../OEBPS/Images/10_04b.png)'
  prefs: []
  type: TYPE_IMG
- en: where *y[i]* denotes the one-hot encoded vector representing the true class
    the example belongs to and *ŷ*[i] is the predicted class probability vector of
    *C* elements, with *ŷ*[i,c] denoting the probability of the example belonging
    to class *c*. Note that in practice, an exponential (natural) base is used instead
    of base, 2 as the computations are faster. The following listing delineates the
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.5 Implementation of the perplexity metric
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Define a function to compute perplexity given real and predicted targets.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Compute the categorical cross-entropy loss.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Compute the mean of the loss.
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Compute the exponential of the mean loss (perplexity).
  prefs: []
  type: TYPE_NORMAL
- en: 'What we’re doing is very simple. First, we subclass the tf.keras.metrics.Mean
    class. The tf.keras.metrics.Mean class will keep track of the mean value of any
    outputted metric passed into its update_state() function. In other words, when
    we subclass the tf.keras.metrics.Mean class, we don’t specifically need to manually
    compute the mean of the accumulated perplexity metric as the training continues.
    It will be automatically done by that parent class. We will define the loss function
    we will use in the self.cross_entropy variable. Then we write the function _calculate_perplexity(),
    which takes the real targets and the predictions from the model. We compute the
    sample-wise loss and then compute the mean. Finally, to get the perplexity, we
    exponentiate the mean loss. With that, we can compile the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we learned about the performance metrics, such as entropy and
    perplexity, used to evaluate language models. Furthermore, we implemented a custom
    perplexity metric that is used to compile the final model. Next, we’ll train our
    model on the data we have prepared and evaluate the quality of generated text.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 3
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine a classification problem that has three outputs. There are two scenarios
    with different predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Scenario A: Labels [0, 2, 1]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Predictions: [[0.6, 0.2, 0.2], [0.1, 0.1, 0.8], [0.3, 0.5, 0.2]]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Scenario B: Labels [0, 2, 1]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Predictions: [[0.3, 0.3, 0.4], [0.4, 0.3, 0.3], [0.3, 0.3, 0.4]]'
  prefs: []
  type: TYPE_NORMAL
- en: Which one will have the lowest perplexity?
  prefs: []
  type: TYPE_NORMAL
- en: 10.4 Training and evaluating the language model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will train the model. Before training the model, let’s
    instantiate the training and validation data sets using the previously implemented
    get_tf_pipeline() function. We will only use the first 50 stories (out of a total
    of 98) in the training set to save time. We will take a sequence of 100 bigrams
    at a time and hop the story by shifting the window by 25 bigrams. This means the
    starting index of the sequences for a single story is 0, 25, 50, . . ., and so
    on. We will use a batch size of 128:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: To train the model, we will define the callbacks as before. We will define
  prefs: []
  type: TYPE_NORMAL
- en: A CSV logger that will log performance over time during training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A learning rate scheduler that will reduce the learning rate when performance
    has plateaued
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An early stopping callback to terminate the training if the performance is not
    improving
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, it’s time to train the model. I wonder what sort of cool stories I
    can squeeze out from the trained model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: NOTE On an Intel Core i5 machine with an NVIDIA GeForce RTX 2070 8 GB, the training
    took approximately 1 hour and 45 minutes to run 25 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: After training the model, you will see a validation perplexity close to 9.5\.
    In other words, this means that, for a given word sequence, the model thinks there
    can be 9.5 different next words that are the right word (not exactly, but it is
    a close-enough approximation). Perplexity needs to be judged carefully as the
    goodness of the measure tends to be subjective. For example, as the size of the
    vocabulary increases, this number can go up. But this doesn’t necessarily mean
    that the model is bad. The number can go up because the model has seen more words
    that fit the occasion compared to when the vocabulary was smaller.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will evaluate the model on the test data to gauge how well our model can
    anticipate some of the unseen stories without being surprised:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: This will give you approximately
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: which is on par with the validation performance we saw. Finally, save the model
    with
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: In this section, you learned to train and evaluate the model. You trained the
    model on the training data set and evaluated the model on validation and testing
    sets. In the next section, you will learn how to use the trained model to generate
    new children’s stories. Then, in the following section, you will learn how to
    generate text with the model we just trained.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 4
  prefs: []
  type: TYPE_NORMAL
- en: Assume you want to use the validation accuracy (val_accuracy) instead of validation
    perplexity (val_perlexity) to define the early stopping callback. How would you
    change the following callback?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '10.5 Generating new text from the language model: Greedy decoding'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the coolest things about a language model is the generative property
    it possesses. This means that the model can generate new data. In our case, the
    language model can generate new children’s stories using the knowledge it garnered
    from the training phrase.
  prefs: []
  type: TYPE_NORMAL
- en: But to do so, we have to use some extra elbow grease. The text-generation process
    is different from the training process. During the training, we had the full sequence
    end to end. This means you can process a sequence of arbitrary length in one go.
    But when generating new text, you don’t have a sequence of text available to you;
    in fact, you are trying to generate one. You start with a random word or words
    and get an output word, and then recursively feed the current output as the next
    input to generate new text. In order to facilitate this process, we need to define
    a new version of the trained model. Let’s flesh out the generative process a bit
    more. Figure 10.5 compares the training process versus the generation/inference
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Define an initial word *w*[t] (random word from the vocabulary).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define an initial state vector *h*[t] (initialized with zeros).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define a list, words, to hold the predicted words and initialize it with the
    initial word.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For t from 1 to n:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get the next word (*w*[t + 1]) and the state vector (*h*[t + 1]) from the model
    and assign to *w*[t] and *h*[t], respectively. This creates a recursive process,
    enabling us to generate as many words as we like.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Append the new word to words.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![10-05](../../OEBPS/Images/10-05.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 Comparison between the language model at training time and the inference/decoding
    phrase. In the inference phase, we predict one time step at a time. In each time
    step, we get the predicted word as the input and the new hidden state as the previous
    hidden state for the next time step.
  prefs: []
  type: TYPE_NORMAL
- en: We will use the Keras functional API to build this model, as shown in the next
    listing. First, let’s define two inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.6 Implementation of the inference/decoding language model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Define an input that can take an arbitrarily long sequence of word IDs.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Define another input that will feed in the previous state.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Define an embedding layer.
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Get the embedding vectors from the input word ID.
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Define a GRU layer that returns both the output and the state. However, note
    that they will be the same for a GRU.
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Get the GRU output and the state from the model.
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Compute the first fully connected layer output.
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Define a final layer that is the same size as the vocabulary and get the final
    output of the model.
  prefs: []
  type: TYPE_NORMAL
- en: ❾ Define the final model that takes an input and a state vector as the inputs
    and produces the next word prediction and the new state vector as the outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have to perform an important step after defining the model. We must transfer
    the weights from the trained model to the newly defined inference model. For that,
    we have to identify layers with trainable weights, get the weights for those layers
    from the trained model, and assign them to the new model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: To get weights of a specific layer in the trained model, you can call
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: This will return a NumPy array with the weights. Next, to assign those weights
    to a layer, call
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now call the newly defined model recursively to generate as many bigrams
    as we like. We will discuss the process to do that in more detail. Instead of
    starting with a single random word, let’s start with a sequence of text. We will
    convert the text to bigrams and subsequently to word IDs using the Tokenizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s reset the states of the model (this is not needed here because
    we’re starting fresh, but it’s good to know that we can do that). We will define
    a state vector with all zeros:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we will recursively predict on each bigram in the seq variable to update
    the state of the GRU model with the input sequence. Once we sweep through the
    whole sequence, we will get the final predicted bigram (that will be our first
    predicted bigram) and append that to the original bigram sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'We will define a new input x with the last word ID that was predicted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: The fun begins now. We will predict 500 bigrams (i.e., 1,000 characters) using
    the approach discussed earlier. In every iteration, we predict a new bigram and
    the new state with the infer_model using the input x and the state vector state.
    The new bigram and the new state recursively replace x and state variables with
    these new outputs (see the next listing).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.7 Recursively predicting a new word using the previous word as an
    input
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Get the next output and state.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Get the word ID and the word from out.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ If the word ends with space, we introduce a bit of randomness to break repeating
    text.
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Essentially pick one of the top three outputs for that timestep depending
    on their likelihood.
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Append the prediction cumulatively to text.
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Recursively make the current prediction the next input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that a little bit of work is required to get the final value of x as the
    model predicts a probability prediction (assigned to out) as its output, not a
    word ID. Furthermore, we’re going to use some additional logic to improve the
    randomness (or entropy, one could say) in the generated text by picking a word
    randomly from the top three words. But we don’t pick them with equal probability.
    Rather, let’s use their predicted probabilities to predict the word. To make sure
    we don’t get too much randomness and to avoid getting random tweaks in the middle
    of words, let’s do this only when the last character is a space character. The
    final word ID (picked either as the word with the highest probability or at random)
    is assigned to the variable x. This process will repeat for 500 steps, and by
    the end, you’ll have a cool machine-generated story on your hands. You can print
    the final text and see how it looks. To do that, simply join the bigrams in the
    text sequence as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: This will display
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: It’s certainly not bad for a simple single layer GRU model. Most of the time,
    the model spits out actual words. But there are occasional spelling mistakes and
    more frequent grammatical errors haunting the text. Can we do better? In the next
    section, we are going to learn about a new technique to generate text called beam
    search.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 5
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume you have the following code that chooses the next word greedily without
    randomness. You run this code and realize that the results are very bad:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: What do you think is the reason for the poor performance?
  prefs: []
  type: TYPE_NORMAL
- en: '10.6 Beam search: Enhancing the predictive power of sequential models'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can do better that greedy decoding. Beam search is a popular decoding algorithm
    used in sequential/time-series tasks like this to generate more accurate predictions.
    The idea behind beam search is very simple. Unlike in greedy decoding, where you
    predict a single timestep, with beam search you predict several time steps ahead.
    In each time step, you get the top k predictions and branch out from them. Beam
    search has two important parameters: beam width and beam depth. Beam width controls
    how many candidates are considered at each step, whereas beam depth determines
    how many steps to search. For example, for a beam width of 3 and a beam depth
    of 5, the number of possible options become 3⁵ = 243\. Figure 10.6 further illustrates
    how beam search works.'
  prefs: []
  type: TYPE_NORMAL
- en: '![10-06](../../OEBPS/Images/10-06.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6 Beam search in action. Beam search leads to better solutions as
    it looks several steps into the future to make a prediction. Here, we are performing
    a beam search with beam width = 3 and beam depth = 5.
  prefs: []
  type: TYPE_NORMAL
- en: 'First let’s define a function that will take a model, input, and state and
    return the output and the new state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Then, using this function, we will define a recursive function (recursive_fn)
    that will recursively predict the next word from the previous prediction for a
    predefined depth (defined by beam_depth). At each time step, we consider the top
    k candidates (defined by beam_width) to branch out from. The recursive_fn function
    will populate a variable called results. results will contain a list of tuples,
    where each tuple represents a single path in the search. Specifically, each tuple
    contains the
  prefs: []
  type: TYPE_NORMAL
- en: Elements in the path
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The joint log probability of that sequence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final state vector to pass to the GRU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This function is outlined in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.8 Implementation of the beam search as a recursive function
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Define an outer wrapper for the computational function of beam search.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Define an inner function that is called recursively to find the beam paths.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Define the base case for terminating the recursion.
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Append the result we got at the termination so we can use it later.
  prefs: []
  type: TYPE_NORMAL
- en: ❺ During recursion, get the output word and the state by calling the model.
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Get the top k candidates for that step.
  prefs: []
  type: TYPE_NORMAL
- en: ❼ For each candidate, compute the joint probability. We will do this in log
    space to have numerical stability.
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Penalize joint probability whenever the same symbol repeats.
  prefs: []
  type: TYPE_NORMAL
- en: ❾ Append the current candidate to the sequence that maintains the current search
    path at the time.
  prefs: []
  type: TYPE_NORMAL
- en: ❿ Call the function recursively to find the next candidates.
  prefs: []
  type: TYPE_NORMAL
- en: ⓫ Make a call to the recursive function to trigger the recursion.
  prefs: []
  type: TYPE_NORMAL
- en: ⓬ Sort the results by log probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can use this beam_search function as follows: we will use a beam
    depth of 7 and a beam width of 2\. Up until the for loop, things are identical
    to how we did things using greedy decoding. In the for loop, we get the results
    list (sorted high to low on joint probability). Then, similar to what we did previously,
    we’ll get the next prediction randomly from the top 10 predictions based on their
    likelihood as the next prediction. The following listing delineates the code to
    do so.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.9 Implementation of beam search decoding to generate a new story
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Define a sequence of ngrams from an initial sequence of text.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Convert the bigrams to word IDs.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Build up model state using the given string.
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Get the predicted word after processing the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Predict for 100 time steps.
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Get the results from beam search.
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Get one of the top 10 results based on their likelihood.
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Replace x and state with the new values computed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Running the code in listing 10.9, you should get text similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: The text generated with beam search reads much better than the text we saw with
    greedy decoding. You see improved grammar and less spelling mistakes when text
    is generated with beam search.
  prefs: []
  type: TYPE_NORMAL
- en: Diverse beam search
  prefs: []
  type: TYPE_NORMAL
- en: 'Different alternatives to beam search have surfaced over time. One popular
    alternative is called *diverse beam search*, introduced in the paper, “Diverse
    Beam Search: Decoding Diverse Solutions from Neural Sequence Models” by Vijayakumar
    et al. ([https://arxiv.org/pdf/1610.02424.pdf](https://arxiv.org/pdf/1610.02424.pdf)).
    Diverse beam search overcomes a critical limitation of vanilla beam search. That
    is, if you analyze the most preferred candidate sequences proposed by beam search,
    you’ll find that they differ only by a few elements. This also can lead to repeating
    text that lacks variety. Diverse beam search comes up with an optimization problem
    that incentivizes diversity of the proposed candidates during the search. You
    can read more about this in the paper.'
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our discussion on language modeling. In the next chapter, we
    will learn about a new type of NLP problem known as a sequence-to-sequence problem.
    Let’s summarize the key highlights of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 6
  prefs: []
  type: TYPE_NORMAL
- en: You used the line result = beam_search(infer_model, x, state, 7, 2) to perform
    beam search. You want to consider five candidates at a given time and search only
    three levels deep into the search space. How would you change the line?
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Language modeling is the task of predicting the next word given a sequence of
    words.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Language modeling is the workhorse of some of the top-performing models in the
    field, such as BERT (a type of Transformer-based model).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To limit the size of the vocabulary and avoid computational issues, n-gram representation
    can be used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In n-gram representation, text is split into fixed-length tokens, as opposed
    to doing character-level or word-level tokenization. Then, a fixed sized window
    is moved over the sequence of text to generate inputs and targets for the model.
    In TensorFlow, you can use the tf.data.Dataset.window() function to implement
    this functionality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The gated recurrent unit (GRU) is a sequential model that operates similarly
    to an LSTM, where it jumps from one input to the next in a sequence while generating
    a state at each time step.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The GRU is a compact version of an LSTM model that maintains a single state
    and two gates but delivers on-par performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perplexity measures how surprised the model was to see the target word given
    the input sequence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The computation of the perplexity measure is inspired by information theory,
    in which the measure entropy is used to quantify the uncertainty in a random variable
    that represents an event where the outcomes are generated with some underlying
    probability distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The language models after training can be used to generate new text. There
    are two popular techniques—greedy decoding and beam search decoding:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Greedy decoding predicts one word at a time, where the predicted word is used
    as the input in the next time step.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Beam search decoding predicts several steps into the future and selects the
    sequence that gives the highest joint probability.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Answers to exercises
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Exercise 1**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '**Exercise 2**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '**Exercise 3**'
  prefs: []
  type: TYPE_NORMAL
- en: Scenario A will have the lowest perplexity.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 4**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '**Exercise 5**'
  prefs: []
  type: TYPE_NORMAL
- en: The line out, new_s = infer_model.predict([x, s]), is wrong. The state is not
    recursively updated in the inference model. This will lead to a working model
    but with poor performance. It should be corrected as out, s = infer_model.predict([x,
    s]).
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 6**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
