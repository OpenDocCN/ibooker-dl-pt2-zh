["```py\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 460.56       Driver Version: 460.32.03    CUDA Version: 11.2     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   39C    P8     9W /  70W |      3MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n\n```", "```py\n    \"trainer\": {\n        \"optimizer\": {\n            \"type\": \"huggingface_adamw\",\n            \"lr\": 1.0e-5\n        },\n        \"num_epochs\": 20,\n        \"patience\": 10,\n        \"cuda_device\": 0\n}\n```", "```py\ndevice = torch.device('cuda:0')\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2-large\")\nmodel = AutoModelWithLMHead.from_pretrained(\"gpt2-large\")\n\ngenerated = tokenizer.encode(\"On our way to the beach \")\ncontext = torch.tensor([generated])\n\nmodel = model.to(device)\ncontext = context.to(device)\n```", "```py\npip install torchserve torch-model-archiver\n```", "```py\ngit lfs install\ngit clone https://huggingface.co/distilgpt2\n```", "```py\nfrom abc import ABC\nimport logging\n\nimport torch\nfrom ts.torch_handler.base_handler import BaseHandler\n\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\n\nlogger = logging.getLogger(__name__)\n\nclass TransformersLanguageModelHandler(BaseHandler, ABC):\n    def __init__(self):\n        super(TransformersLanguageModelHandler, self).__init__()\n        self.initialized = False\n        self.length = 256\n        self.top_k = 0\n        self.top_p = .9\n        self.temperature = 1.\n        self.repetition_penalty = 1.\n\n    def initialize(self, ctx):                        ❶\n        self.manifest = ctx.manifest\n        properties = ctx.system_properties\n        model_dir = properties.get(\"model_dir\")\n        self.device = torch.device(\n            \"cuda:\" + str(properties.get(\"gpu_id\"))\n            if torch.cuda.is_available()\n            else \"cpu\"\n        )\n\n        self.model = GPT2LMHeadModel.from_pretrained(model_dir)\n        self.tokenizer = GPT2Tokenizer.from_pretrained(model_dir)\n\n        self.model.to(self.device)\n        self.model.eval()\n\n        logger.info('Transformer model from path {0} loaded successfully'.format(model_dir))\n        self.initialized = True\n\n    def preprocess(self, data):                   ❷\n        text = data[0].get(\"data\")\n        if text is None:\n            text = data[0].get(\"body\")\n        text = text.decode('utf-8')\n\n        logger.info(\"Received text: '%s'\", text)\n\n        encoded_text = self.tokenizer.encode(\n            text,\n            add_special_tokens=False,\n            return_tensors=\"pt\")\n\n        return encoded_text\n\n    def inference(self, inputs):                  ❸\n        output_sequences = self.model.generate(\n            input_ids=inputs.to(self.device),\n            max_length=self.length + len(inputs[0]),\n            temperature=self.temperature,\n            top_k=self.top_k,\n            top_p=self.top_p,\n            repetition_penalty=self.repetition_penalty,\n            do_sample=True,\n            num_return_sequences=1,\n        )\n\n        text = self.tokenizer.decode(\n            output_sequences[0],\n            clean_up_tokenization_spaces=True)\n\n        return [text]\n\n    def postprocess(self, inference_output):      ❹\n        return inference_output\n\n_service = TransformersLanguageModelHandler()\n\ndef handle(data, context):                        ❺\n    try:\n        if not _service.initialized:\n            _service.initialize(context)\n\n        if data is None:\n            return None\n\n        data = _service.preprocess(data)\n        data = _service.inference(data)\n        data = _service.postprocess(data)\n\n        return data\n    except Exception as e:\n        raise e\n```", "```py\ntorch-model-archiver \\\n    --model-name distilgpt2 \\\n    --version 1.0 \\\n    --serialized-file distilgpt2/pytorch_model.bin \\\n    --extra-files \"distilgpt2/config.json,distilgpt2/vocab.json,distilgpt2/tokenizer.json,distilgpt2/merges.txt\" \\\n    --handler ./torchserve_handler.py\n```", "```py\nmkdir model_store\nmv distilgpt2.mar model_store\n```", "```py\ntorchserve --start --model-store model_store --models distilgpt2=distilgpt2.mar\n```", "```py\ncurl -d \"data=In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\" -X POST http://127.0.0.1:8080/predictions/distilgpt2\n```", "```py\ntorchserve --stop\n```", "```py\n!git clone https://github.com/shashankprasanna/torchserve-examples.git\n!cd torchserve-examples\n\n!git clone https://github.com/pytorch/serve.git\n!pip install serve/model-archiver/\n\nimport boto3, time, json\nsess    = boto3.Session()\nsm      = sess.client('sagemaker')\nregion  = sess.region_name\naccount = boto3.client('sts').get_caller_identity().get('Account')\n\nimport sagemaker\nrole = sagemaker.get_execution_role()\nsagemaker_session = sagemaker.Session(boto_session=sess)\n\nbucket_name = sagemaker_session.default_bucket()\n```", "```py\ncd model_store\ntar cvfz distilgpt2.tar.gz distilgpt2.mar\naws s3 cp distilgpt2.tar.gz s3://sagemaker-xxx-yyy/torchserve/models/\n```", "```py\nRUN pip install --no-cache-dir psutil \\\n                --no-cache-dir torch \\\n                --no-cache-dir torchvision \\\n                --no-cache-dir transformers\n```", "```py\nregistry_name = 'torchserve'\n!aws ecr create-repository --repository-name torchserve\n\nimage_label = 'v1'\nimage = f'{account}.dkr.ecr.{region}.amazonaws.com/{registry_name}:{image_label}'\n\n!docker build -t {registry_name}:{image_label} .\n!$(aws ecr get-login --no-include-email --region {region})\n!docker tag {registry_name}:{image_label} {image}\n!docker push {image}\n```", "```py\nimport sagemaker\nfrom sagemaker.model import Model\nfrom sagemaker.predictor import RealTimePredictor\nrole = sagemaker.get_execution_role()\n\nmodel_file_name = 'distilgpt2'\n\nmodel_data = f's3://{bucket_name}/torchserve/models/{model_file_name}.tar.gz'\nsm_model_name = 'torchserve-distilgpt2'\n\ntorchserve_model = Model(model_data = model_data, \n                         image_uri = image,\n                         role = role,\n                         predictor_cls=RealTimePredictor,\n                         name = sm_model_name)\nendpoint_name = 'torchserve-endpoint-' + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\npredictor = torchserve_model.deploy(instance_type='ml.m4.xlarge',\n                                    initial_instance_count=1,\n                                    endpoint_name = endpoint_name)\n```", "```py\nresponse = predictor.predict(data=\"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\")\n```", "```py\nb'In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English. The unicorns said they would take a stroll in the direction of scientists over the next month or so.\\n\\n\\n\\n\\nWhen contacted by Animal Life and Crop.com, author Enrique Martinez explained how he was discovered and how the unicorns\\' journey has surprised him. According to Martinez, the experience makes him more interested in research and game development.\\n\"This is really what I want to see this year, and in terms of medical research, I want to see our population increase.\"<|endoftext|>'\n```", "```py\npip install lit-nlp\n```", "```py\nimport numpy as np\n\nfrom allennlp.models.archival import load_archive\nfrom allennlp.predictors.predictor import Predictor\nfrom lit_nlp import dev_server\nfrom lit_nlp import server_flags\nfrom lit_nlp.api import dataset as lit_dataset\nfrom lit_nlp.api import model as lit_model\nfrom lit_nlp.api import types as lit_types\n\nfrom examples.sentiment.sst_classifier import LstmClassifier\nfrom examples.sentiment.sst_reader import StanfordSentimentTreeBankDatasetReaderWithTokenizer\n```", "```py\nclass SSTData(lit_dataset.Dataset):\n    def __init__(self, labels):\n        self._labels = labels\n        self._examples = [\n            {'sentence': 'This is the best movie ever!!!', 'label': '4'},\n            {'sentence': 'A good movie.', 'label': '3'},\n            {'sentence': 'A mediocre movie.', 'label': '1'},\n            {'sentence': 'It was such an awful movie...', 'label': '0'}\n        ]\n\n    def spec(self):\n        return {\n            'sentence': lit_types.TextSegment(),\n            'label': lit_types.CategoryLabel(vocab=self._labels)\n        }\n```", "```py\nclass SentimentClassifierModel(lit_model.Model):\n    def __init__(self):\n        cuda_device = 0\n        archive_file = 'model/model.tar.gz'\n        predictor_name = 'sentence_classifier_predictor'\n\n        archive = load_archive(                                       ❶\n            archive_file=archive_file,\n            cuda_device=cuda_device\n        )\n\n        predictor = Predictor.from_archive(archive, predictor_name=predictor_name)\n\n        self.predictor = predictor                                    ❷\n        label_map = archive.model.vocab.get_index_to_token_vocabulary('labels')\n        self.labels = [label for _, label in sorted(label_map.items())]\n\n    def predict_minibatch(self, inputs):\n        for inst in inputs:\n            pred = self.predictor.predict(inst['sentence'])           ❸\n            tokens = self.predictor._tokenizer.tokenize(inst['sentence'])\n            yield {\n                'tokens': tokens,\n                'probas': np.array(pred['probs']),\n                'cls_emb': np.array(pred['cls_emb'])\n            }\n\n    def input_spec(self):\n        return {\n            \"sentence\": lit_types.TextSegment(),\n            \"label\": lit_types.CategoryLabel(vocab=self.labels, required=False)\n        }\n\n    def output_spec(self):\n        return {\n            \"tokens\": lit_types.Tokens(),\n            \"probas\": lit_types.MulticlassPreds(parent=\"label\", vocab=self.labels),\n            \"cls_emb\": lit_types.Embeddings()\n        }\n```", "```py\nmodel = SentimentClassifierModel()\nmodels = {\"sst\": model}\ndatasets = {\"sst\": SSTData(labels=model.labels)}\n\nlit_demo = dev_server.Server(models, datasets, **server_flags.get_flags())\nlit_demo.serve()\n```"]