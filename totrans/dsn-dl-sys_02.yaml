- en: 2 Dataset management service
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 数据集管理服务
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包括
- en: Understanding dataset management
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解数据集管理
- en: Using design principles to build a dataset management service
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用设计原则构建数据集管理服务
- en: Building a sample dataset management service
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个样本数据集管理服务
- en: Using open source approaches to dataset management
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用开源方法进行数据集管理
- en: After our general discussion of deep learning systems, we are ready for the
    rest of the chapters, which focus on specific components in those systems. We
    present dataset management first not only because deep learning projects are data-driven
    but also because we want to remind you how important it is to think about data
    management before building other services.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们对深度学习系统做一般讨论之后，我们已经准备好了章节的其他部分，这些部分专注于这些系统中的特定组件。我们首先介绍数据集管理，不仅因为深度学习项目是数据驱动的，还因为我们希望提醒您在构建其他服务之前考虑数据管理有多重要。
- en: Dataset management (DM) often gets overlooked in the deep learning model development
    process, whereas data processing and model training and serving attract the most
    attention. A common thought in data engineering is that good data processing pipelines,
    such as ETL (extract, transform, and load) pipelines, are all we need. But if
    you avoid managing your datasets as your project proceeds, your data collection
    and dataset consumption logic become more and more complicated, model performance
    improvement becomes difficult, and eventually, the entire project slows down.
    A good DM system can expedite model development by decoupling training data collection
    and consumption; it also enables model reproducibility by versioning the training
    data.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习模型开发过程中，数据集管理（DM）往往被忽视，而数据处理、模型训练和服务则吸引了最多的注意力。数据工程中的一个普遍观点是，好的数据处理流程，比如ETL（抽取、转换和加载）流程，已经足够了。但是，如果在项目进行过程中避免管理数据集，你的数据收集和数据集消耗逻辑将变得越来越复杂，模型性能改进会变得困难，最终整个项目也会变慢。一个好的DM系统可以通过解耦训练数据的收集和消耗加快模型的开发；它还可以通过对训练数据进行版本控制来实现模型的可重复性。
- en: We guarantee that you will appreciate your wise decision to build or at least
    set up a dataset management component in addition to your existing data processing
    pipelines. And build it before working on the training and serving components.
    Your deep learning project development will go faster and can produce better results
    and simpler models in the long run. Because the DM component shields the upstream
    data complexity from your model training code, your model algorithm development
    and data development can run parallel.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们保证您将感谢自己明智的决定，构建或至少设置一个数据集管理组件，以补充现有的数据处理流程。并且在着手训练和服务组件之前构建它。您的深度学习项目开发将会更快，长远来看可以产生更好的结果和更简单的模型。因为DM组件将上游数据的复杂性屏蔽在您的模型训练代码之外，您的模型算法开发和数据开发可以并行运行。
- en: This chapter is about building dataset management functionality for your deep
    learning project. Because of the variety of deep learning algorithms, data pipelines,
    and data sources, dataset management is an often-discussed topic in the deep learning
    industry. There is no unified approach, and it seems there will never be one.
    To be beneficial to you in practice, therefore, we will focus on teaching the
    design principles instead of advocating one single approach. The sample dataset
    management service we build in this chapter demonstrates one possible way to implement
    these principles.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涉及为您的深度学习项目构建数据集管理功能。由于深度学习算法、数据流程和数据源的多样性，数据集管理是深度学习行业中经常讨论的话题。目前还没有统一的方法，而且似乎永远不会有一个。因此，为了在实践中对您有益，我们将专注于教授设计原则，而不是倡导单一方法。我们在本章构建的样本数据集管理服务展示了实施这些原则的一种可能方法。
- en: In section 2.1, you will learn why dataset management is needed, the challenges
    it should address, and the crucial role it plays in a deep learning system. We
    will also introduce its key design principles to prepare you for the concrete
    examples in the next section.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2.1节，您将了解到为什么需要数据集管理，它应该解决哪些挑战以及它在深度学习系统中所扮演的关键角色。我们还将介绍其关键设计原则，为下一节的具体示例做好准备。
- en: In section 2.2, we will demonstrate a dataset management service based on the
    concepts and design principles introduced in section 2.1\. First, we will set
    up the service on your local machine and experiment with it. Second, we will discuss
    the internal dataset storage and data schema, user scenarios, data ingestion API,
    and dataset fetching API, as well as provide an overview of design and user scenarios.
    During the tour, we will also discuss the pros and cons of some important decisions
    we made in the service design.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2.2节中，我们将基于第2.1节介绍的概念和设计原则演示一个数据集管理服务。首先，我们将在您的本地机器上设置该服务并进行实验。其次，我们将讨论内部数据集存储和数据模式、用户场景、数据摄取
    API 和数据集提取 API，以及提供设计和用户场景的概述。在这个过程中，我们还将讨论在服务设计中作出的一些重要决策的优点和缺点。
- en: In section 2.3, we will look at two open source approaches. If you don’t want
    a DIY dataset management service, you can use the components that are already
    built, available, and adaptable. For instance, you can use Delta Lake with Petastorm
    for dataset management if your existing data pipeline is built on top of Apache
    Spark. Or you can adopt Pachyderm if your data comes directly from a cloud object
    storage such as AWS Simple Storage Service (S3) or Azure Blob. We use image dataset
    preparation as an example to show how these two approaches can work with unstructured
    data in practice. By the end of this chapter, you will have a deep understanding
    of the intrinsic characteristics of dataset management and its design principles,
    so you can either build a dataset management service on your own or improve an
    existing system in your work.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2.3节，我们将看两种开源方法。如果你不想要自行构建的数据集管理服务，你可以使用已经构建好、可用和可适应的组件。例如，如果您的现有数据流水线是基于Apache
    Spark构建的，您可以使用Delta Lake与Petastorm进行数据集管理。或者，如果您的数据直接来自于云对象存储，例如AWS简单存储服务（S3）或Azure
    Blob，您可以选择采用Pachyderm。我们以图像数据集准备为例，展示这两种方法如何在实践中处理非结构化数据。在本章结束时，您将对数据集管理的内在特性和设计原则有深入的了解，这样您可以自己构建数据集管理服务或改进工作中的现有系统。
- en: 2.1 Understanding dataset management service
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 理解数据集管理服务
- en: A dataset management component or service is a specialized data store for organizing
    data in favor of model training and model performance troubleshooting. It processes
    raw data fed from upstream data sources and returns training data in a well-defined
    structure—a dataset—for use in model training. Figure 2.1 shows the core value
    a dataset management service delivers. In the figure, we see that a dataset management
    component converts the raw data into a consistent data format that favors model
    training, so downstream model training applications can just focus on algorithm
    development.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集管理组件或服务是一个专门的数据存储，用于组织数据以支持模型训练和模型性能故障排除。它处理来自上游数据源的原始数据，并以一种明确定义的结构（数据集）返回用于模型训练的训练数据。图2.1显示了数据集管理服务提供的核心价值。在图中，我们可以看到数据集管理组件将原始数据转换为有利于模型训练的一致的数据格式，因此下游模型训练应用程序只需关注算法开发。
- en: '![](../Images/02-01.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-01.png)'
- en: Figure 2.1 A dataset management service is a specialized data store; it ingests
    data into its internal storage with its own raw data format. During training,
    it converts the raw data into training data in a consistent data format that favors
    model training.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 数据集管理服务是一个专门的数据存储；它使用自己的原始数据格式将数据导入其内部存储。在训练期间，它将原始数据转换为一致的数据格式，以便于模型训练。
- en: 2.1.1 Why deep learning systems need dataset management
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.1 深度学习系统为什么需要数据集管理
- en: Let’s take a moment to explain why DM is a crucial part of any deep learning
    system before we start looking at the sample dataset management service. This
    section is important because, from our experience, it is impossible to design
    a system that solves a real problem unless you fully understand the *why*.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始查看示例数据集管理服务之前，让我们花一点时间解释为什么 DM 是任何深度学习系统的重要组成部分。这一部分很重要，因为根据我们的经验，除非你完全理解*为什么*，否则无法设计解决实际问题的系统。
- en: There are two answers to the why question. First, DM can help to expedite model
    development by decoupling the *collection* of training data from the *consumption*
    of that data. Second, a well-designed DM service supports model reproducibility
    by having version tracking on training datasets. Let’s look at both of these points
    in detail.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于为什么这个问题，有两个答案。第一，DM 可以通过将训练数据的*收集*与*使用*分离来加快模型的开发。第二，一个设计良好的 DM 服务通过对训练数据集进行版本跟踪来支持模型的可复现性。让我们详细讨论这两个观点。
- en: Decoupling the training data collection and consumption
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 解耦训练数据收集与消费
- en: 'If you work on a deep learning project completely by yourself, the project
    development workflow is an iterative loop of the following steps: data collection,
    dataset preprocess, training, and evaluation (see figure 2.2). Although you can
    break the downstream dataset preprocess code or training code if you change the
    data format in the data collection component, it’s not a big problem. Because
    you are the single code owner, you make free changes; no other people are affected.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你完全独自开发深度学习项目，项目开发工作流程是以下步骤的迭代循环：数据收集、数据集预处理、训练和评估（见图2.2）。虽然如果你在数据收集组件中更改数据格式，可能会破坏下游数据集预处理代码或训练代码，但这不是一个大问题。因为你是唯一的代码所有者，你可以自由更改；没有其他人受到影响。
- en: '![](../Images/02-02.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-02.png)'
- en: Figure 2.2 The workflow for a single-person deep learning project development
    is an iterative loop of linear steps.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 单人深度学习项目开发的工作流程是一系列线性步骤的迭代循环。
- en: When we are building a serious deep learning platform catering to tens of different
    deep learning projects and opening it to multiple people and teams, the simple
    data flow chart will dilate quickly to a bewildering 3D diagram (figure 2.3).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们正在构建一个面向数十个不同深度学习项目并向多人和团队开放的严肃深度学习平台时，简单的数据流程图将迅速扩张到令人困惑的3D图（图2.3）。
- en: '![](../Images/02-03.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-03.png)'
- en: Figure 2.3 Deep learning model development in enterprise runs in multidimensions.
    Multiple teams of people work together to ship a project in different phases.
    Each team focuses on one step of the workflow and also works on multiple projects
    in parallel.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3 企业中的深度学习模型开发在多个维度上运行。多个团队合作以在不同阶段完成项目。每个团队专注于工作流程的一个步骤，同时还在多个项目上工作。
- en: 'Figure 2.3 shows the complexity of an enterprise deep learning development
    environment. In this setting, each person only works on a single step instead
    of the entire workflow, and they develop their work for multiple projects. Ideally,
    this process is efficient because people build their expertise by focusing on
    one particular problem. But here is the catch: the communication cost is often
    ignored.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3 显示了企业深度学习开发环境的复杂性。在这种情况下，每个人只负责一个步骤而不是整个工作流程，并且他们为多个项目开发他们的工作。理想情况下，这个过程是有效的，因为人们通过专注于一个特定问题来建立他们的专业知识。但这里有个问题：通信成本经常被忽视。
- en: When we divide the steps of a workflow (figure 2.2) between multiple teams,
    data schemas are needed for the handshake. Without a data contract, the downstream
    team doesn’t know how to read the data sent from the upstream team. Let’s go back
    to figure 2.3\. Imagine how many data schemas we need to communicate between teams
    if there are 10 projects developed by four teams in parallel, especially if every
    team handles different steps of the workflow.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将工作流程的步骤（图2.2）分配给多个团队时，需要数据模式进行握手。没有数据合同，下游团队不知道如何读取上游团队发送的数据。让我们回到图2.3。想象一下，如果有四个团队并行开发的10个项目，尤其是每个团队处理工作流程的不同步骤，我们需要多少数据模式来在团队之间进行通信。
- en: Now, if we want to add a new feature or attribute (such as text language) to
    a training dataset, we need to gather every team, obtain a consensus on the new
    data format, and implement the change. This is a huge effort because cross-team
    collaboration in corporations is complicated. It often takes months to make a
    small change; because each team has its own priority, you have to wait on its
    to-do list.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们想要向训练数据集添加一个新特征或属性（如文本语言），我们需要召集每个团队，在新数据格式上达成共识，并实施更改。这是一项巨大的工作，因为公司内部的跨团队协作是复杂的。通常需要几个月的时间来做出一个小改变；因为每个团队都有自己的优先事项，你必须等待他们的待办事项清单。
- en: To make the situation worse, deep learning model development is an iterative
    process. It demands constantly tuning the training dataset (including the upstream
    data pipelines) to improve model accuracy. This requires data scientists, data
    developers, and platform developers to interact at a high frequency, but because
    of the cross-team workflow setting, the data iteration happens slowly, which is
    one of the reasons why model development is so slow in a production environment.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 更糟糕的是，深度学习模型开发是一个迭代过程。它要求不断调整训练数据集（包括上游数据管道）以提高模型准确性。这需要数据科学家、数据开发人员和平台开发人员高频率地进行交互，但由于跨团队工作流程的设置，数据迭代发生缓慢，这是在生产环境中模型开发如此缓慢的原因之一。
- en: Another problem is that when we have multiple types of projects (image, video,
    and text) developing in parallel, the number of data schemas will explode. If
    we let each team define new data schemas freely and don’t manage them properly,
    keeping the system backward compatible is almost impossible. The new data updates
    will become more and more difficult because we have to spend extra time to make
    sure the new data update doesn’t break the projects built in the past. As a consequence,
    the project development velocity will slow down significantly.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是，当我们同时开发多种类型的项目（图像、视频和文本）时，数据模式的数量将会激增。如果让每个团队自由定义新的数据模式，并且不进行适当管理，那么保持系统向后兼容几乎是不可能的。新数据的更新将变得越来越困难，因为我们必须花费额外的时间来确保新数据更新不会破坏过去构建的项目。因此，项目开发速度将会显著减慢。
- en: To address the slow iteration and data schema management problem, we can build
    a dataset management service. Let’s look at figure 2.4 to help determine the changes
    in the project development workflow after introducing the dataset management service.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为解决迭代缓慢和数据模式管理问题，我们可以构建一个数据集管理服务。让我们看一下图2.4，以帮助确定引入数据集管理服务后项目开发工作流程的变化。
- en: 'In figure 2.4, we see a dataset management service that splits the model development
    workflow into two separate spaces: data developer space and data scientist space.
    The long iteration loop (figure 2.2) is now divided into two small loops (figure
    2.4), and each loop is owned by a single team, so the data developer and data
    scientist can iterate on data collection and model training separately; therefore,
    the deep learning project can iterate much faster.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在图2.4中，我们看到一个数据集管理服务将模型开发工作流程分成了两个独立的空间：数据开发者空间和数据科学家空间。长迭代循环（图2.2）现在被分成了两个小循环（图2.4），每个循环由一个团队拥有，因此数据开发者和数据科学家可以分别迭代数据收集和模型训练；因此，深度学习项目可以更快地迭代。
- en: '![](../Images/02-04.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-04.png)'
- en: Figure 2.4 A dataset management component creates a good separation between
    training data collection and consumption by defining strongly typed schemas for
    both, which allows data development and model algorithm development to iterate
    in their own loop, thus expediting the project development.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4 一个数据集管理组件通过为训练数据收集和消耗定义强类型模式，为两者之间创建了良好的分离，这使得数据开发和模型算法开发可以在自己的循环中迭代，从而加快了项目的开发速度。
- en: 'You may also notice that we now have all data schemas in one place: a dataset
    management service, which manages two strongly typed data schemas—the ingestion
    data schema and the training data schema—for each type of dataset. By having two
    separate data schemas for data ingestion and training while doing data transformation
    inside DM, you ensure that the data changes in the upstream data collection won’t
    break the downstream model training. Because the data schemas are strongly typed,
    future data upgrades can easily be made backward compatible.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能也注意到，现在我们把所有的数据模式都放在了一个地方：一个数据集管理服务，它管理着两种强类型的数据模式——每种类型的数据集都有一个摄取数据模式和一个训练数据模式。通过在数据转换过程中在DM内部进行数据摄取和训练的两个单独的数据模式，你可以确保上游数据收集中的数据更改不会破坏下游的模型训练。由于数据模式是强类型的，未来的数据升级可以轻松地保持向后兼容。
- en: Defining a strongly typed dataset may not be a good idea for projects in the
    beginning or experimental phase because we are still exploring all kinds of data
    options. Therefore, we also recommend defining a special schema-free dataset type,
    such as `GENERIC` type, which has no strong schema restriction. For data in this
    dataset type, DM just accepts the data as is and does not perform data validation
    and transformation (for a detailed example, see section 2.2.6). The data collected
    from the data processing pipeline can be consumed directly by the training process.
    Although the whole workflow would be fragile, a free dataset type addresses the
    need to be flexible for projects in the early phase. Once the project matures,
    we can create strongly typed schemas and define a dataset type for them.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为项目定义强类型数据集可能并不是一个好主意，因为我们仍在探索各种数据选项。因此，我们还建议定义一种特殊的无模式数据集类型，例如`GENERIC`类型，它没有强类型模式限制。对于此数据集类型中的数据，DM只接受原样数据，并且不执行数据验证和转换（有关详细示例，请参见第2.2.6节）。从数据处理管道中收集的数据可以直接由训练流程使用。虽然整个工作流程可能会很脆弱，但自由数据集类型满足了在早期阶段项目需要灵活性的需求。一旦项目成熟，我们可以创建强类型模式并为它们定义数据集类型。
- en: To summarize this section, managing two data schemas of a dataset type is the
    secret sauce that decouples data scientists and data developers. In section 2.2.6,
    we will show how these schemas can be implemented in our sample dataset management
    service.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 总结本节，管理数据集类型的两个数据架构是解耦数据科学家和数据开发者的秘密武器。在第 2.2.6 节，我们将展示如何在我们的示例数据集管理服务中实现这些架构。
- en: Enabling model reproducibility
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 实现模型可重现性
- en: A well-designed dataset management service supports model reproducibility by
    having version tracking on training datasets—for example, using a version string
    to obtain the exact training files used in previous model training runs. The advantage
    of model reproducibility with respect to the data scientist (model algorithm development)
    is that you can repeatedly run a deep learning algorithm (such as the self-attention
    transformer in NLP) on a certain dataset and gain the same or similar quality
    of results. This is called *algorithm reproducibility*.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一个设计良好的数据集管理服务通过在训练数据集上进行版本跟踪来支持模型可重现性，例如，使用版本字符串来获取在以前模型训练运行中使用的确切训练文件。相对于数据科学家（模型算法开发），模型可重现性的优势在于，你可以重复在某个数据集上运行深度学习算法（例如
    NLP 中的自注意力变换器），并获得相同或相似质量的结果。这被称为*算法可重现性*。
- en: From the view of a deep learning system developer, model reproducibility is
    the superset of algorithm reproducibility. It requires the dataset management
    system to be able to reproduce its output artifacts (datasets). For example, we
    need to obtain the exact training data and training configuration to reproduce
    models that were trained in the past.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 从深度学习系统开发者的角度来看，模型可重现性是算法可重现性的超集。它要求数据集管理系统能够复现其输出物件（数据集）。例如，我们需要获取确切的训练数据和训练配置来复现过去训练过的模型。
- en: Model reproducibility is crucial to machine learning projects for two main reasons.
    The first is trust. Reproducibility creates trust and credibility for the system
    that produces the model. For any system, if the output can’t be reproduced, people
    simply won’t trust the system. This is extremely relevant to machine learning
    projects because applications will make decisions based on model output—for example,
    a chatbot will transfer a user call to proper service departments according to
    the user intent prediction. If we can’t reproduce a model, the applications built
    on top of the model are nondeterministic and untrustworthy.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 模型可重现性对于机器学习项目至关重要，有两个主要原因。第一个是信任。可重现性为生成模型的系统创造了信任和可信度。对于任何系统，如果输出无法复现，人们简单地不会信任该系统。这在机器学习项目中非常相关，因为应用程序将根据模型输出做出决策——例如，聊天机器人将根据用户意图预测将用户呼叫转接到适当的服务部门。如果我们无法复现模型，构建在模型之上的应用程序是不确定性的和不可信的。
- en: The second reason is that model reproducibility facilitates performance troubleshooting.
    When detecting a model performance regression, people first want to find out what
    has changed in the training dataset and training algorithm code. If model reproducibility
    is not supported, performance troubleshooting will be very difficult.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个原因是模型可重现性有助于性能故障排除。在检测到模型性能退化时，人们首先想要找出训练数据集和训练算法代码发生了什么变化。如果不支持模型可重现性，性能故障排除将非常困难。
- en: 2.1.2 Dataset management design principles
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.2 数据集管理设计原则
- en: We want to outline five design principles for DM before we start building one.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始构建之前，我们想要概述 DM 的五个设计原则。
- en: Note We consider these five principles to be the most important elements in
    this chapter. For data applications, the principles we follow in design are more
    important than the actual design. Because data could be anything in any form,
    there is no paradigm for data storage, in general, and there is no standard design
    that suits all kinds of data processing use cases. So, in practice, we build our
    own data application by following certain general principles. Therefore, these
    principles are critical.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我们认为这五个原则是本章最重要的元素。对于数据应用程序，我们在设计中遵循的原则比实际设计更重要。因为数据可能是任何形式的任何东西，在一般情况下，没有适用于所有数据处理用例的数据存储范式，也没有适用于所有数据处理用例的标准设计。因此，在实践中，我们通过遵循某些通用原则来构建我们自己的数据应用程序。因此，这些原则至关重要。
- en: The five principles here will give you clear design targets for building a new
    DM service or improving your existing DM service.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的五个原则将为您建立新的 DM 服务或改进现有的 DM 服务提供明确的设计目标。
- en: 'Principle 1: Support dataset reproducibility for reproducing models'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 原则1：支持数据集可重现性以重现模型
- en: Dataset reproducibility means that the DM always returns the same exact training
    examples it has returned in the past. For instance, when the training team starts
    training a model, the DM provides a dataset with a version string. Anytime the
    training team—or any other team—needs to retrieve the same training data, it can
    use this version string to query DM to retrieve the same training data.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的可重现性意味着 DM 总是返回过去返回的完全相同的训练示例。例如，当训练团队开始训练模型时，DM 提供了一个带有版本字符串的数据集。每当训练团队——或任何其他团队——需要检索相同的训练数据时，它都可以使用此版本字符串查询
    DM 以检索相同的训练数据。
- en: We believe all DM systems should support dataset reproducibility. Even better
    would be to also offer data diff functionally, so we can see the data difference
    between two different dataset versions easily. This is very convenient for troubleshooting.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们相信所有的 DM 系统都应该支持数据集的可重现性。更好的是还能提供数据差异功能，这样我们就可以轻松地看到两个不同数据集版本之间的数据差异。这对故障排除非常方便。
- en: 'Principle 2: Provide unified API across different types of datasets'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 原则 2：在不同类型的数据集上提供统一的 API
- en: A dataset of deep learning might be structured (text, such as sales records
    or the transcript of a user conversation) or unstructured (image, voice recording
    file). No matter how a DM system processes and stores these different forms of
    data internally, it should provide a unified API interface for uploading and fetching
    different types of datasets. The API interface also abstracts away the data source
    from the data consumer; no matter what happens under the hood, such as data parsing
    changes and internal storage format changes, downstream consumers shouldn’t be
    affected.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的数据集可能是结构化的（文本，如销售记录或用户对话的文字稿）或非结构化的（图像、语音记录文件）。无论 DM 系统如何在内部处理和存储这些不同形式的数据，它都应该为上传和获取不同类型的数据集提供统一的
    API 接口。API 接口还将数据源与数据使用者抽象出来；无论发生什么情况，比如数据解析更改和内部存储格式更改，下游使用者都不应受到影响。
- en: Therefore, our users, both data scientists and data developers, only need to
    learn one API to work with all the different types of datasets. This makes the
    system simple and easy to use. Also, the code maintenance cost will be greatly
    reduced because we only expose one public API.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的用户，包括数据科学家和数据开发人员，只需要学习一个 API 就能处理所有不同类型的数据集。这使系统简单易用。此外，由于我们只公开一个公共 API，代码维护成本将大大降低。
- en: 'Principle 3: Adopt a strongly typed data schema'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 原则 3：采用强类型数据模式
- en: A strongly typed data schema is the key to avoiding unexpected failures caused
    by data changes. With data schema enforcement, the DM service can guarantee that
    the raw data it ingests and the training data it produces are consistent with
    our specs.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 强类型数据模式是避免由数据更改引起的意外故障的关键。通过数据模式强制执行，DM 服务可以保证其摄取的原始数据和生成的训练数据与我们的规范一致。
- en: The strongly typed data schema acts as a safety guard to ensure the downstream
    model training code does not get affected by the upstream data collection changes,
    and it also ensures backward compatibility for both upstream and downstream clients
    of DM. Without data schema protection, the dataset consumer—the downstream model
    training code—can easily be broken by upstream data changes.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 强类型数据模式充当安全防护，以确保下游模型训练代码不受上游数据收集更改的影响，并确保 DM 的上游和下游客户的向后兼容性。如果没有数据模式保护，则数据集使用者——下游模型训练代码——很容易受到上游数据更改的影响。
- en: Data schemas can be versioned as well, but this will add another layer of complexity
    to management. An additional option is to only have one schema per dataset. When
    introducing new data changes, make sure that the schema update is backward compatible.
    If a new data requirement requires a breaking change, create a new dataset type
    with a new schema instead of updating the existing one.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 数据模式也可以进行版本控制，但这会增加管理的复杂性。另一个选项是每个数据集只有一个模式。在引入新的数据更改时，确保模式更新是向后兼容的。如果新的数据需求需要破坏性更改，则创建一个具有新模式的新数据集类型，而不是更新现有的数据集。
- en: 'Principle 4: Ensure API consistency and handle scaling internally'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 原则 4：确保 API 一致性并在内部处理扩展
- en: The current trend in the deep learning field is model architecture that keeps
    getting bigger as datasets continue to grow larger. For example, GPT-3 (a generative
    pretrained transformer language model for language understanding) uses more than
    250 TB of text materials with hundreds of billions of words; in Tesla, the autonomous
    driving model consumes an immense amount of data at the petabyte level. On the
    other hand, we still use small datasets (around 50 MB) for some easy tasks in
    narrow domains, such as customer support ticket classification. Dataset management
    systems should handle the data scaling challenges internally, and the API exposed
    to users (data developers and data scientists) should be consistent for both large-
    and small-sized datasets.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习领域的当前趋势是，随着数据集不断增大，模型架构也变得越来越复杂。例如，GPT-3（一个用于语言理解的生成预训练转换器语言模型）使用超过250 TB的文本材料，其中包含数百亿个单词；在特斯拉中，自动驾驶模型消耗了海量的数据，达到了PB级。另一方面，对于一些简单的窄领域任务（如客户支持工单分类），我们仍然使用小型数据集（约50
    MB）。数据集管理系统应该在内部解决数据扩展的挑战，并且向用户（数据开发者和数据科学家）提供的API对于大型和小型数据集应该是一致的。
- en: 'Principle 5: Guarantee data persistency'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 原则5：保证数据持久性
- en: Ideally, datasets used for deep learning training should be stored immutably
    to reproduce training data and troubleshoot. Data removal should be soft deletions
    with only a few exceptions for hard deletions, such as deleting customer data
    permanently when a customer chooses to opt out of or cancel their account.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，用于深度学习训练的数据集应该以不可变的方式存储，以便复现训练数据和进行故障排查。数据删除应该是软删除，只有极少数例外情况才需要进行硬删除，例如当客户选择退出或取消账户时永久删除客户数据。
- en: 2.1.3 The paradoxical character of datasets
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.3 数据集的矛盾性
- en: To close out our conceptual discussion on dataset management, we would like
    to clarify an ambiguous aspect of datasets. We have seen dozens of poorly designed
    dataset management systems fail on this point.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了结束我们关于数据集管理的概念讨论，我们想要澄清数据集一个模糊的方面。我们见过许多设计不良的数据集管理系统在这一点上失败。
- en: 'A dataset has a paradoxical trait: it is both dynamic and static. From a data
    scientist’s point of view, a dataset is static: it is a fixed set of files with
    annotations (also known as labels). From a data developer’s point of view, a dataset
    is dynamic: it is a file-saving destination in a remote storage to which we keep
    adding data.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集具有矛盾的特性：它既是动态的又是静态的。从数据科学家的角度来看，数据集是静态的：它是一组带有注释（也称为标签）的固定文件。从数据开发者的角度来看，数据集是动态的：它是一个远程存储中的文件保存目的地，我们不断向其添加数据。
- en: So, from a DM perspective, a dataset should be a logic file group and satisfy
    both data collection and data training needs. To help you get a concrete understanding
    of how to accommodate both the dynamic and static nature of datasets, let’s look
    at figure 2.5.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，从数据管理的角度来看，数据集应该是一个逻辑文件组，同时满足数据收集和数据训练的需求。为了帮助您对数据集的动态和静态特性有具体的理解，让我们看一下图2.5。
- en: '![](../Images/02-05.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-05.png)'
- en: 'Figure 2.5 A dataset is a logic file group: it’s both dynamic and static, and
    it''s editable for data collection but fixed for model training.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5 数据集是一个逻辑文件组：它既是动态的又是静态的，对于数据收集来说是可编辑的，但对于模型训练来说是固定的。
- en: 'We can read figure 2.5 from two angles: data ingestion and data fetching. First,
    from the data ingestion side, we see that the data collection pipeline (from the
    left of the graph) keeps pumping in new data, such as text utterances and labels.
    For example, at time T0, an example data batch (example batch T0) is created in
    the dataset—the same for time T1, T2, and T3; we have a total of four data batches
    created over time. So, from the data developer’s view, this dataset is mutable,
    because the pipeline keeps adding data to it.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从数据摄入和数据获取两个角度来阅读图2.5。首先，从数据摄入的角度来看，我们看到数据收集管道（图中左侧）不断地注入新数据，例如文本话语和标签。例如，在时间T0，数据集中创建了一个示例数据批次（示例批次T0）——T1、T2和T3时间也是如此；随着时间的推移，我们总共创建了四个数据批次。因此，从数据开发者的角度来看，这个数据集是可变的，因为管道不断向其中添加数据。
- en: Second, on the training data fetching side (from the top of the graph), we see
    that when fetching training data, the DM reads all the current data from the dataset
    at the same time point. We see that the data is returned as a static versioned
    snapshot, which has a version string to uniquely identify the actual data it picked
    from a dataset. For example, when we fetch training data from the dataset at time
    T2, the dataset has three data batches (batch T0, batch T1, and batch T2). We
    package these three data batches into a snapshot, assign a version string (“version1”),
    and return it as training data.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，在训练数据获取方面（从图的顶部），我们可以看到在获取训练数据时，DM同时读取数据集中的所有当前数据。我们可以看到数据以静态的版本快照方式返回，该快照具有一个版本字符串，用于唯一标识从数据集中选择的实际数据。例如，当我们从时间点T2的数据集中获取训练数据时，数据集有三个数据批次（批次T0、批次T1和批次T2）。我们将这三个数据批次打包成一个快照，分配一个版本字符串（“version1”）并将其作为训练数据返回。
- en: From a model training perspective, the dataset fetched from DM is a static snapshot
    of the dataset—a time-filtered plus customer logic-filtered dataset. The static
    snapshot is crucial to model reproducibility because it represents the exact training
    files used in a training run. When we need to rebuild the model, we can use the
    snapshot version string to find the snapshot that was used in the past model training.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 从模型训练的角度来看，从 DM 获取的数据集是数据集的静态快照——一个经过时间过滤和客户逻辑过滤的数据集。静态快照对于模型的可复制性至关重要，因为它代表了训练过程中使用的确切训练文件。当我们需要重新构建模型时，我们可以使用快照版本字符串来找到过去模型训练中使用的快照。
- en: Our theoretical introductions have been thoroughly covered, and you should be
    able to grasp the needs, goals, and unique characteristics of the dataset management
    component. The next section is a concrete example of how to design a dataset management
    service.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对理论介绍已经进行了全面的介绍，您应该能够掌握数据集管理组件的需求、目标和独特特性。下一节是如何设计数据集管理服务的具体示例。
- en: 2.2 Touring a sample dataset management service
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 浏览示例数据集管理服务
- en: In this section, we will walk you through a sample DM service. We built this
    sample to give you an idea of how the principles presented in section 2.1.2 can
    be implemented. We will first run the service locally, play with it, and then
    look at its API design and internal implementation.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将带您了解一个示例DM服务。我们构建了这个示例，以便让您了解第2.1.2节中介绍的原理如何实施。我们首先在本地运行服务，与之互动，然后查看其API设计和内部实现。
- en: 2.2.1 Playing with the sample service
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.1 与示例服务交互
- en: To make this easy for you, we built seven shell scripts to automate the entire
    DM lab. These shell scripts are the recommended way to experience the demo scenarios
    in this section because they not only automate services’ local setup but also
    take care of setting the environment variables, preparing sample data, and initializing
    the local network.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便您操作，我们构建了七个shell脚本来自动化整个DM实验室。这些shell脚本是本节演示场景的推荐方式，因为它们不仅自动配置本地服务，还会处理环境变量设置、准备示例数据和初始化本地网络。
- en: 'You can find these scripts at [https://github.com/orca3/MiniAutoML/tree/main/scripts](https://github.com/orca3/MiniAutoML/tree/main/scripts),
    starting with the search: “dm”. The “function demo” doc in our GitHub repo ([https://github.com/orca3/MiniAutoML/tree/main/data-management](https://github.com/orca3/MiniAutoML/tree/main/data-management))
    provides detailed instructions for how to complete the lab and sample outputs
    of these scripts.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[https://github.com/orca3/MiniAutoML/tree/main/scripts](https://github.com/orca3/MiniAutoML/tree/main/scripts)找到这些脚本，从搜索词“dm”开始。我们GitHub仓库中的“功能演示”文档([https://github.com/orca3/MiniAutoML/tree/main/data-management](https://github.com/orca3/MiniAutoML/tree/main/data-management))提供了完成实验以及这些脚本的示例输出的详细说明。
- en: Note Before running the function demo, the system requirement should be met
    first. Please refer to [https://github.com/orca3/MiniAutoML#system-requirements](https://github.com/orca3/MiniAutoML#system-requirements).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行功能演示之前，请确保已满足系统要求。请参考[https://github.com/orca3/MiniAutoML#system-requirements](https://github.com/orca3/MiniAutoML#system-requirements)。
- en: 'This lab consists of three sections: first, run the sample dataset management
    service; second, create a dataset and upload data to it; and third, fetch training
    data from the dataset just created.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实验室分为三个部分：首先，运行示例数据集管理服务；其次，创建一个数据集并上传数据；再次，从刚创建的数据集中获取训练数据。
- en: Setting up service locally
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地设置服务
- en: The sample service is written in Java 11\. It uses MinIO as the file blob server
    to mimic cloud object storage (such as Amazon S3), so we can run everything locally
    without any remote dependency. If you have set up the lab in appendix A, you can
    run the following commands (listing 2.1) in your terminal at the root of the scripts
    folder to start the service.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 示例服务是用 Java 11 编写的。它使用 MinIO 作为文件 Blob 服务器来模拟云对象存储（如 Amazon S3），因此我们可以在本地运行而无需任何远程依赖。如果您在附录
    A 中设置了实验，您可以在终端中的脚本文件夹的根目录运行以下命令（列表 2.1）来启动服务。
- en: Note Starting with a clean setup is highly recommended before running DM demo
    scripts. You can execute `./scripts/lab-999-tear-down.sh` to clean up previous
    labs.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在运行 DM demo 脚本之前，强烈建议从干净的设置开始。您可以执行 `./scripts/lab-999-tear-down.sh` 来清除之前的实验。
- en: Listing 2.1 Starting the service locally
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.1 在本地启动服务
- en: '[PRE0]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note To keep the service setup to a bare minimum, we maintain all the dataset
    records in memory to avoid using databases. Please be aware that you will lose
    all datasets if you restart the dataset management service.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持服务的最简设置，我们将所有数据集记录保存在内存中，以避免使用数据库。请注意，如果重新启动数据集管理服务，你将丢失所有数据集。
- en: Creating and updating a language intent dataset
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 创建和更新语言意图数据集
- en: Our sample DM service offers three API methods for users to create/update a
    dataset and check the result. These API methods are `CreateDataset`, `UpdateDataset`,
    and `GetDatasetSummary`. We will discuss them in detail in the next few sections.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的示例 DM 服务为用户提供了三种 API 方法来创建/更新数据集并检查结果。这些 API 方法是 `CreateDataset`、`UpdateDataset`
    和 `GetDatasetSummary`。我们将在接下来的几节中详细讨论它们。
- en: In this example scenario, first we call the `CreateDataset` API method on the
    data management service to create a new language intent dataset; then we use the
    `UpdateDataset` API method to append more data to the dataset. Finally, we use
    the `GetDatasetSummary` API method to obtain the dataset’s statistics and commit
    (data change) history.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例场景中，我们首先调用数据管理服务上的 `CreateDataset` API 方法来创建一个新的语言意图数据集；然后我们使用 `UpdateDataset`
    API 方法向数据集追加更多数据。最后，我们使用 `GetDatasetSummary` API 方法获取数据集的统计信息和提交（数据更改）历史记录。
- en: Note The scripts dm-003-create-dataset.sh and dm-004-add-commits.sh automate
    the previous steps. Please use them to run the demo scenario. Please note that
    the following code listings are only for illustration purposes.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 注意脚本 dm-003-create-dataset.sh 和 dm-004-add-commits.sh 自动化了之前的步骤。请使用它们来运行演示场景。请注意，以下代码列表仅供说明目的。
- en: Let’s run the lab now. First, we’ll create a dataset using the following listing.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们运行实验。首先，我们将使用以下列表创建一个数据集。
- en: Listing 2.2 Creating a language intent dataset
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.2 创建语言意图数据集
- en: '[PRE1]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Uploads raw data (upload/001.csv) to cloud storage
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将原始数据（upload/001.csv）上传到云存储
- en: ❷ gRPC request to create a dataset
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建数据集的 gRPC 请求
- en: ❸ Dataset type
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 数据集类型
- en: ❹ Data URL of the raw data in MinIO, for example, upload/001.csv
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ MinIO 中原始数据的数据 URL，例如，upload/001.csv
- en: ❺ API name
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ API 名称
- en: 'It should be noted that the `CreateDataset` API expects users to provide a
    downloadable URL in the gRPC request, not the actual data, which is why we first
    upload the 001.csv file to the local MinIO server. After the dataset is created,
    the `CreateDataset` API will return a JSON object that contains a data summary
    and commits the history of the dataset. See a sample result as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 应注意，`CreateDataset` API 预期用户在 gRPC 请求中提供可下载的 URL，而不是实际数据，这就是为什么我们首先将 001.csv
    文件上传到本地 MinIO 服务器的原因。数据集创建完成后，`CreateDataset` API 将返回一个包含数据摘要和数据集历史提交的 JSON 对象。以下是一个示例结果：
- en: '[PRE2]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Commits are the snapshot dataset updates.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 提交是数据集更新的快照。
- en: ❷ Commit ID; this commit captures the data from upload/001.csv.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 提交 ID；此提交捕获了来自 upload/001.csv 的数据。
- en: ❸ Commit tags are used to filter commits when building a training dataset.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 提交标签用于在构建训练数据集时过滤提交。
- en: ❹ Data summary of the commit
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 提交的数据摘要
- en: After creating a dataset, you can keep updating it by appending more data; see
    the dataset update gRPC request as follows.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 创建数据集后，你可以通过追加更多数据来持续更新它；请参见以下的数据集更新 gRPC 请求。
- en: Listing 2.3 Updating a language intent dataset
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.3 更新语言意图数据集
- en: '[PRE3]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Uploads raw data (upload/002.csv) to cloud storage
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将原始数据（upload/002.csv）上传到云存储
- en: ❷ A request to append more data (upload/002.csv)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 请求追加更多数据（upload/002.csv）
- en: ❸ Replace the dataset ID with the value returned from the CreateDataset API.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 用从 CreateDataset API 返回的值替换数据集 ID。
- en: ❹ The data URL of raw data, created by raw data upload
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 原始数据的数据 URL，由原始数据上传创建
- en: ❺ Updates the dataset API name
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 更新数据集的 API 名称
- en: 'Once the dataset update completes, the `UpdateDataset` API returns a data summary
    JSON object in the same way as the `CreateDataset` API does; see a sample responsible
    object as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据集更新完成，`UpdateDataset` API 会以与 `CreateDataset` API 相同的方式返回一个数据摘要 JSON 对象；请参考以下示例响应对象：
- en: '[PRE4]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ The commit created by the create dataset request
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 由创建数据集请求创建的提交
- en: ❷ Commit ID; this commit captures the data from upload/002.csv.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 提交 ID；此提交捕获来自 upload/002.csv 的数据。
- en: 'You can also fetch the data summary and commit history of a dataset by using
    the `GetDatasetSummary` API. See the following sample gRPC request:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用 `GetDatasetSummary` API 来获取数据集的数据摘要和提交历史。请参考以下示例的 gRPC 请求：
- en: '[PRE5]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ The ID of the dataset to query
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 要查询的数据集的 ID
- en: Fetch training dataset
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 获取训练数据集
- en: Now we have a dataset (ID = 1) created with raw data; let’s try to build a training
    dataset from it. In our sample service, it’s a two-step process.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了一个数据集（ID = 1），包含原始数据；让我们尝试从中构建一个训练数据集。在我们的示例服务中，这是一个两步骤的过程。
- en: We first call the `PrepareTrainingDataset` API to start the dataset-building
    process. And then we use the `FetchTrainingDataset` API to query the dataset preparation
    progress until the request completes.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先调用 `PrepareTrainingDataset` API 开始数据集构建过程。然后，我们使用 `FetchTrainingDataset`
    API 查询数据集准备进度，直到请求完成。
- en: Note Scripts dm-005-prepare-dataset.sh, dm-006-prepare-partial-dataset.sh, and
    dm-007-fetch-dataset-version.sh automate the steps that follow. Please try to
    use them to run the sample dataset fetching demo in code listings 2.4 and 2.5.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，脚本 dm-005-prepare-dataset.sh、dm-006-prepare-partial-dataset.sh 和 dm-007-fetch-dataset-version.sh
    自动化了接下来的步骤。请尝试使用它们来运行代码清单 2.4 和 2.5 中的示例数据集获取演示。
- en: To use the `PrepareTrainingDataset` API, we only need to provide a dataset ID.
    If you just want a portion of data to be in the training dataset, you can use
    `tag` as a filter in the request. See a sample request as follows.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 `PrepareTrainingDataset` API，我们只需提供一个数据集 ID。如果你只想让部分数据进入训练数据集，你可以在请求中使用
    `tag` 作为过滤器。请参考以下的示例请求。
- en: Listing 2.4 Preparing a training dataset
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 2.4 准备训练数据集
- en: '[PRE6]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ A request to prepare the training dataset with all data commits
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 准备包含所有数据提交的训练数据集的请求
- en: ❷ A request to prepare the training dataset with partial data commits by defining
    filter tags
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 通过定义过滤标签，准备包含部分数据提交的训练数据集的请求
- en: ❸ Data filters
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 数据过滤器
- en: 'Once the data preparation gRPC request succeeds, it returns a JSON object as
    follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据准备的 gRPC 请求成功，它将返回一个如下的 JSON 对象：
- en: '[PRE7]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ ID of the training dataset snapshot
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 训练数据集快照的 ID
- en: ❷ The selected data commits of the raw dataset
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 原始数据集的选定数据提交
- en: Among the data that the `PrepareTrainingDataset` API returns is the `"version_hash"`
    string. It is used to identify the data snapshot produced by the API. Using this
    hash as an ID, we can call the `FetchTrainingDatasetc` API to track the progress
    of building the training dataset; see the following example.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '`PrepareTrainingDataset` API 返回的数据中包含 `"version_hash"` 字符串。它用于识别该 API 生成的数据快照。使用这个哈希作为
    ID，我们可以调用 `FetchTrainingDatasetc` API 来跟踪训练数据集建立的进度；请参考以下示例。'
- en: Listing 2.5 Checking dataset prepare progress
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 2.5 检查数据集准备进度
- en: '[PRE8]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ ID of the training dataset snapshot
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 训练数据集快照的 ID
- en: 'The `FetchTrainingDatasetc` API returns a JSON object that describes the training
    dataset. It tells us the status of the background dataset-building process: `RUNNING`,
    `READY`, or `FAILED`. If the training data is ready for consumption, the response
    object will display a list of downloadable URLs for the training data. In this
    demo, the URLs point to the local MinIO server. See a sample response as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`FetchTrainingDatasetc` API 返回一个描述训练数据集的 JSON 对象。它告诉我们背景数据集构建过程的状态：`RUNNING`、`READY`
    或 `FAILED`。如果训练数据已准备好供使用，响应对象将显示训练数据的可下载 URL 列表。在这个演示中，URL 指向本地的 MinIO 服务器。请参考以下的示例响应：'
- en: '[PRE9]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Status of the training dataset
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 训练数据集的状态
- en: ❷ Data URLs of the training data
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 训练数据的数据 URL
- en: Good job! You just experienced all the major data APIs offered by our sample
    dataset management service. By trying to upload data and build training datasets
    by yourself, we hope you have gained a feeling for how this service can be used.
    In the next few sections, we will look at user scenarios, service architecture
    overview, and code implementation of our sample dataset management services.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 做得好！你刚刚体验了我们示例数据集管理服务提供的所有主要数据 API。通过尝试自己上传数据和构建训练数据集，我们希望你能感受到这项服务的用途。在接下来的几个部分，我们将查看用户场景、服务架构概览以及我们的示例数据集管理服务的代码实现。
- en: Note If you encounter any problems when running the mentioned scripts, please
    refer to the instructions in the “function demo” doc of our GitHub repo. Also,
    if you want to try the labs in chapters 3 and 4, please keep the containers running
    because they are the prerequisites for the model training labs.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在运行上述脚本时遇到任何问题，请参考我们 GitHub 仓库中“function demo”文档中的说明。另外，如果您想尝试第 3 章和第 4 章的实验，请保持容器运行，因为它们是模型训练实验的先决条件。
- en: 2.2.2 Users, user scenarios, and the big picture
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.2 用户、用户场景和整体情况
- en: When designing backend services, the method we found very useful is thinking
    from the outside in. First, figure out who the users are, what value the service
    will provide, and how customers will interact with the service. Then the inner
    logic and storage layout should come naturally to you. For touring this sample
    DM service, we will show you using the same approach. So let’s look at our users
    and user scenario first.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计后端服务时，我们发现非常有用的方法是从外部到内部思考。首先，弄清楚用户是谁，服务将提供什么价值，以及客户将如何与服务进行交互。然后，内部逻辑和存储布局应该自然而然地出现。对于浏览此示例
    DM 服务，我们将使用相同的方法展示给您。因此，让我们首先看看我们的用户和用户场景。
- en: Note The reason we look at the use cases first is that we believe any system
    design should consider the user the most. Our approach to efficiency and scalability
    will come up naturally if we identify how customers use the system. If the design
    is taken in the reverse order (consider technology first and usability second),
    the system often is clumsy to use because it’s designed for technology and not
    for customers.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们之所以首先考虑用例，是因为我们认为任何系统设计都应该最大程度地考虑用户。如果我们确定了客户如何使用系统，那么我们的效率和可扩展性的方法将自然而然地出现。如果设计是以相反的顺序进行的（首先考虑技术，其次考虑可用性），那么系统通常很难使用，因为它是为技术而设计的，而不是为客户设计的。
- en: Users and user scenarios
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 用户和用户场景
- en: 'Our sample DM service is built for two fictional users: Jianguo, a data engineer,
    and Julia, a data scientist. They work together to train a language-intent classification
    model.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的示例 DM 服务是为两个虚构用户而构建的：建国，一名数据工程师，和朱莉娅，一名数据科学家。他们共同合作训练语言意图分类模型。
- en: Jianguo works on training data collection. He continuously collects data from
    different data sources (such as parsing user activity logs and conducting customer
    surveys) and labels them. Jianguo uses a DM data ingestion API to create datasets,
    append new data to existing datasets, and query datasets’ summary and status.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 建国负责训练数据收集。他不断地从不同的数据源（如解析用户活动日志和进行客户调查）中收集数据并对其进行标记。建国使用 DM 数据摄取 API 创建数据集，将新数据附加到现有数据集中，并查询数据集的摘要和状态。
- en: Julia uses the dataset built by Jianguo to train intent classification models
    (usually written in PyTorch or Tensorflow). At the training time, Julia’s training
    code will first call the DM service’s fetch training data API to get the training
    dataset from the DM and then start the training process.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 朱莉娅使用建国构建的数据集来训练意图分类模型（通常使用 PyTorch 或 TensorFlow 编写）。在训练时，朱莉娅的训练代码将首先调用 DM 服务的获取训练数据
    API 从 DM 获取训练数据集，然后开始训练过程。
- en: The service’s overall architecture
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 服务的整体架构
- en: 'Our sample DM service is built in three layers: the data ingestion layer, dataset
    fetching layer, and dataset internal storage layer. The data ingestion API set
    is built so that Jianguo can upload new training data and query dataset status.
    The dataset fetching API is built so that Julia can obtain the training dataset.
    See figures 2.6 and 2.7 for the whole picture.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的示例 DM 服务分为三层：数据摄取层、数据集获取层和数据集内部存储层。数据摄取 API 集是为了让建国能够上传新的训练数据并查询数据集状态而构建的。数据集获取
    API 是为了让朱莉娅能够获取训练数据集。有关整体情况，请参见图 2.6 和图 2.7。
- en: '![](../Images/02-06.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-06.png)'
- en: Figure 2.6 System overview of the sample dataset management service. The sample
    service contains three main components, a data ingestion API, internal storage,
    and a dataset fetching API.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6 示例数据集管理服务的系统概述。该示例服务包含三个主要组件，即数据摄取 API、内部存储和数据集获取 API。
- en: 'The central big box in figure 2.6 shows the overall design of our sample dataset
    management service. It has an internal dataset storage system and two public-facing
    interfaces: a data ingestion API and a dataset fetching API—one for data ingestion
    and another for dataset fetching. The system supports both strongly typed schema
    datasets (text and image types) and nonschema datasets (`GENERIC` type).'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6 的中心大框显示了我们样本数据集管理服务的整体设计。它有一个内部数据集存储系统和两个面向公众的接口：一个用于数据摄入的数据摄入API和一个用于数据集获取的数据集获取API——一个用于数据摄入，另一个用于数据集获取。该系统支持强类型架构数据集（文本和图像类型）和非架构数据集（`GENERIC`类型）。
- en: '![](../Images/02-07.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-07.png)'
- en: Figure 2.7 The internal storage structure for storing a dataset
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.7 用于存储数据集的内部存储结构
- en: Figure 2.7 displays the overall data structure the sample DM service uses to
    store a dataset. The commits are created by the data ingestion API, and versioned
    snapshots are created by the data fetching API. The concepts of commit and versioned
    snapshot are introduced to address the dynamic and static nature of a dataset.
    We will discuss storage in detail in section 2.2.5.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.7 显示了样本DM服务用于存储数据集的整体数据结构。提交是由数据摄入API创建的，版本化的快照是由数据获取API创建的。提交和版本化快照的概念被引入以应对数据集的动态和静态特性。我们将在第
    2.2.5 节详细讨论存储。
- en: In the remaining subsections, we will walk you through every detail of the previous
    two diagrams, component by component. We first start with the API and then move
    to the internal storage and data schema.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的小节中，我们将逐个详细介绍前两个图表中的每个细节，从API开始，然后转向内部存储和数据架构。
- en: 2.2.3 Data ingestion API
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.3 数据摄入API
- en: The data ingestion API allows creating, updating, and querying datasets in the
    sample dataset management service. The gray box in figure 2.8 shows the definition
    of four service methods in the data ingestion layer that support ingesting data
    into DM. Their names are self-explanatory; let’s look at their gRPC method definition
    in listing 2.6.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 数据摄入API允许在样本数据集管理服务中创建、更新和查询数据集。图 2.8 中的灰色框显示了数据摄入层中支持将数据摄入到DM中的四种服务方法的定义。它们的名称不言自明；让我们在列表
    2.6 中查看它们的gRPC方法定义。
- en: '![](../Images/02-08.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-08.png)'
- en: 'Figure 2.8 Four methods to support data ingestion: create the dataset, update
    the dataset, get the dataset summary, and list the datasets'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.8 支持数据摄入的四种方法：创建数据集、更新数据集、获取数据集摘要和列出数据集
- en: Note To reduce boilerplate code, we chose gRPC to implement the public interface
    for our sample DM service. This doesn’t mean gRPC is the best approach for a dataset
    management service, but compared to the RESTful interface, gRPC’s concise coding
    style is perfect for demonstrating our idea without exposing you to unnecessary
    Spring Framework details.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：为了减少样板代码，我们选择了gRPC来实现样本DM服务的公共接口。这并不意味着gRPC是数据集管理服务的最佳方法，但与RESTful接口相比，gRPC的简洁编码风格非常适合演示我们的想法，而不会让您接触到不必要的Spring框架细节。
- en: Definition of data ingestion methods
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 数据摄入方法定义
- en: Let's take a look at what our sample data ingestion API looks like.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们的样本数据摄入API是什么样子的。
- en: Listing 2.6 Data ingestion API service definition
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.6 数据摄入API服务定义
- en: '[PRE10]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Defines dataset type, "TEXT_INTENT" or "GENERIC"
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义数据集类型，为"TEXT_INTENT"或"GENERIC"
- en: ❷ Defines the file URL of the uploading data in MinIO server
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义了在MinIO服务器中上传数据的文件URL。
- en: ❸ Sets data filter by using tags
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 通过使用标签设置数据过滤器。
- en: Note The topic of data deletion and modification is not covered in this sample
    service, but the service can be easily extended to support them.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本示例服务不涵盖数据删除和修改的主题，但该服务可以轻松扩展以支持这些功能。
- en: Data URL vs. data streaming
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 数据URL与数据流
- en: 'You may notice in our API design that we require users to provide data URLs
    as raw data input instead of uploading files directly to our service. In section
    2.2.4, we also choose to return data URLs as a training dataset instead of returning
    files directly via a streaming endpoint. The main reason is that we want to offload
    the file-transferring work to a cloud object storage service, such as Amazon S3
    or Azure Blob. Doing this has two benefits: first, it saves network bandwidth
    because there are no actual files passed between client and service, and second,
    it reduces code complexity because keeping data streaming working with high availability
    can be complicated when files are large and API usage is high.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的API设计中，您可能会注意到我们要求用户提供数据URL作为原始数据输入，而不是直接将文件上传到我们的服务。在第2.2.4节中，我们还选择将数据URL作为训练数据集返回，而不是通过流式传输端点直接返回文件。主要原因是我们希望将文件传输工作卸载到云对象存储服务（如Amazon
    S3或Azure Blob）。这样做有两个好处：首先，它节省了网络带宽，因为客户端和服务之间没有实际的文件传递；其次，它降低了代码复杂性，因为在文件较大且API使用量较高时，保持数据流工作的高可用性可能会更加复杂。
- en: Creating a new dataset
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 创建新的数据集
- en: Let’s look at how the gRPC `CreateDataset` method is implemented. Before calling
    the DM (c`reateDataset` API) to create a dataset, the user (Jianguo) needs to
    prepare a downloadable URL for the data they want to upload (steps 1 and 2); the
    URL can be a downloadable link in a cloud object storage service, like Amazon
    S3 or Azure Blob. In our sample service, we use the MinIO server to run on your
    local to mock Amazon S3\. Jianguo also can name the dataset and assign tags in
    the dataset creation request. Listing 2.7 highlights the key pieces of code (`dataManagement/DataManagementService
    .java`) that implement the workflow pictured in figure 2.9.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看gRPC `CreateDataset` 方法是如何实现的。在调用DM（`createDataset` API）创建数据集之前，用户（Jianguo）需要准备一个可下载的URL，用于上传他们想要的数据（步骤1和2）；这个URL可以是云对象存储服务中的可下载链接，如Amazon
    S3或Azure Blob。在我们的示例服务中，我们使用MinIO服务器在本地模拟Amazon S3。Jianguo还可以在数据集创建请求中命名数据集并分配标签。清单2.7突出显示了代码（`dataManagement/DataManagementService
    .java`）的关键部分，它实现了图2.9中所示的工作流程。
- en: Listing 2.7 New dataset creation implementation
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 清单2.7是新数据集创建的实现
- en: '[PRE11]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Receives dataset creation request (step 3)
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ❶接收数据集创建请求（步骤3）
- en: ❷ Creates a dataset object with metadata from user request (step 4a)
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ❷使用用户请求中的元数据创建数据集对象（步骤4a）
- en: ❸ Downloads data from URL and uploads it to DM’s cloud storage (step 4b)
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ❸从URL下载数据并将其上传到DM的云存储（步骤4b）
- en: ❹ Saves the dataset with downloaded data as the initial commit (step 5)
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ❹将具有下载数据的数据集保存为初始提交（步骤5）
- en: ❺ Returns the dataset summary to the client (steps 6 and 7)
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ❺将数据集摘要返回给客户端（步骤6和7）
- en: '![](../Images/02-09.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-09.png)'
- en: 'Figure 2.9 A high-level overview of the seven steps to creating a new dataset:
    (1) upload data to the cloud object storage; (2) get a data link; (3) call `createDataset`
    API with a data link as the payload; (4) DM first downloads data from the data
    link and then finds the right dataset transformer (`IntentTextTransformer``)`
    to do data parsing and conversion; (5) DM saves the transformed data; and (6 and
    7) DM returns the dataset summary (ID, commit history, data statistics) to the
    user.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.9是创建新数据集的七个步骤的高级概述：（1）将数据上传到云对象存储；（2）获取数据链接；（3）调用`createDataset` API，并将数据链接作为有效负载；（4）DM首先从数据链接下载数据，然后找到正确的数据集转换器（`IntentTextTransformer`）来进行数据解析和转换；（5）DM保存转换后的数据；（6和7）DM将数据集摘要（ID，提交历史，数据统计）返回给用户。
- en: The implementation details of `DatasetIngestion.ingest()` will be discussed
    in section 2.2.5.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '`DatasetIngestion.ingest()`的具体实现细节将在第2.2.5节中讨论。'
- en: Updating an existing dataset
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 更新现有数据集
- en: Deep learning model development is a continuous process. Once we create a dataset
    for a model training project, data engineers (like Jianguo) will keep adding data
    to it. To accommodate this need, we provide the `UpdateDataset` API.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型的开发是一个持续的过程。一旦我们为模型训练项目创建了一个数据集，数据工程师（比如Jianguo）将继续向其中添加数据。为了满足这个需求，我们提供了`UpdateDataset`
    API。
- en: To use the `UpdateDataset` API, we need to prepare a data URL for the new data.
    We can also pass in a commit message and some customer tags to describe the data
    change; these metadata are useful for data history tracking and data filtering.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`UpdateDataset` API，我们需要为新数据准备一个数据URL。我们还可以传递提交消息和一些客户标签来描述数据的更改；这些元数据对于数据历史记录和数据过滤非常有用。
- en: The dataset update workflow is almost identical to the dataset creation workflow
    (figure 2.9). It creates a new commit with the given data and appends the commit
    to the dataset’s commit list. The only difference is that the dataset update workflow
    won’t create a new dataset but will work on an existing dataset. See the following
    code listing.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集更新工作流程与数据集创建工作流程几乎相同（图 2.9）。它使用给定数据创建一个新的提交，并将提交附加到数据集的提交列表中。唯一的区别是数据集更新工作流程不会创建新的数据集，而是在现有数据集上工作。请参阅以下代码清单。
- en: Note Because every dataset update is saved as a commit, we could easily remove
    or soft delete those commits with some dataset management API if Jianguo mistakenly
    uploads some mislabeled data to a dataset. Because of space limitations, these
    management APIs are not discussed.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 每次数据集更新都保存为提交，如果建国错误地将一些标记错误的数据上传到数据集中，我们可以很容易地使用一些数据集管理API删除或软删除这些提交。由于空间限制，这些管理API没有讨论。
- en: Listing 2.8 Dataset update implementation
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.8 数据集更新实现
- en: '[PRE12]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Receives dataset creation request (step 3)
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 接收数据集创建请求（步骤 3）
- en: ❷ Finds the existing dataset and creates a new commit object (step 4a)
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 查找现有数据集并创建新的提交对象（步骤 4a）
- en: We will talk more about the concept of commits in section 2.2.3\. For now, you
    just need to be aware that every dataset update request creates a new commit object.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2.2.3节，我们将更多地讨论提交的概念。目前，你只需知道每个数据集更新请求都会创建一个新的提交对象。
- en: 'Note Why save data updates in commits? Can we merge the new data with the current
    data so we only store the latest state? In our update dataset implementation,
    we create a new commit every time the `UpdateDataset` API is called. There are
    two reasons we want to avoid an in-place data merge: first, an in-place data merge
    can cause irreversible data modification and silent data loss. Second, to reproduce
    the training dataset used in the past, we need to make sure the data batches DM
    receives are stored immutably because they are the source data we used to create
    the training dataset at any time.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 为什么要将数据更新保存在提交中？我们能否将新数据与当前数据合并，以便只存储最新状态？在我们的更新数据集实现中，每次调用`UpdateDataset`
    API时，我们都会创建一个新的提交。我们要避免就地数据合并有两个原因：首先，就地数据合并可能导致不可逆转的数据修改和悄悄的数据丢失。其次，为了重现过去使用的训练数据集，我们需要确保DM接收的数据批次是不可变的，因为它们是我们随时用来创建训练数据集的源数据。
- en: List datasets and get datasets summary
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 列出数据集并获取数据集摘要
- en: 'Besides `CreateDataset` and `UpdateDataset` API, our users need methods to
    list existing datasets and query the overview of a dataset, such as the number
    of a dataset’s examples and labels and its audit history. To accommodate these
    needs, we build two APIs: `ListDatasets` and `GetDatasetSummary`. The first one
    can list all the existing datasets, and the second one provides detailed information
    about a dataset, such as commit history, example and label count, and dataset
    ID and type. The implementation for these two APIs is straightforward; you can
    find them in our Git repo (`miniAutoML/DataManagementService.java)`.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`CreateDataset`和`UpdateDataset` API外，我们的用户还需要方法来列出现有数据集并查询数据集的概述，例如数据集的示例数和标签数以及其审计历史记录。为满足这些需求，我们构建了两个API：`ListDatasets`和`GetDatasetSummary`。第一个API可以列出所有现有的数据集，而第二个API提供了有关数据集的详细信息，例如提交历史记录、示例和标签计数以及数据集ID和类型。这两个API的实现很简单；你可以在我们的Git仓库中找到它们（`miniAutoML/DataManagementService.java)`。
- en: 2.2.4 Training dataset fetching API
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.4 训练数据集获取API
- en: In this section, we will look at the dataset fetching layer, which is highlighted
    as a gray box in figure 2.10\. To build training data, we designed two APIs. The
    data scientist (Julia) first calls the `PrepareTrainingDataset` API to issue a
    training data preparation request; our DM service will kick off a background thread
    to start building the training data and return a version string as a reference
    handle for the training data. Next, Julia can call the `FetchTrainingDataset`
    API to obtain the training data if the background thread is completed.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看一下数据集获取层，它在图 2.10 中被标记为灰色方框。为了构建训练数据，我们设计了两个API。数据科学家（朱莉娅）首先调用`PrepareTrainingDataset`
    API发出训练数据准备请求；我们的DM服务将启动一个后台线程来开始构建训练数据，并返回一个版本字符串作为训练数据的参考句柄。接下来，朱莉娅可以调用`FetchTrainingDataset`
    API来获取训练数据，如果后台线程已完成。
- en: '![](../Images/02-10.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-10.png)'
- en: 'Figure 2.10 Two methods in the dataset fetching layer to support dataset fetching:
    `PrepareTrainingDataset` and `FetchTrainingDataset`'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.10 数据集获取层中支持数据集获取的两种方法：`PrepareTrainingDataset`和`FetchTrainingDataset`
- en: Definition of dataset fetching methods
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集获取方法的定义
- en: First, let’s see the gRPC service method definition (`grpc-contract/src/main/proto/`
    `data_management.proto)` for the two dataset fetching methods—`PrepareTrainingDataset`
    and `FetchTrainingDataset`.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看一下 gRPC 服务方法定义（`grpc-contract/src/main/proto/` `data_management.proto`）中的两个数据集获取方法——`PrepareTrainingDataset`
    和 `FetchTrainingDataset`。
- en: Listing 2.9 Training dataset fetching service definition
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.9 训练数据集获取服务定义
- en: '[PRE13]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Prepares training dataset API
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 准备训练数据集 API
- en: ❷ Fetches training dataset API
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 获取训练数据集 API
- en: ❸ The payload of dataset preparation API
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 数据集准备 API 的有效载荷
- en: ❹ Specifies which dataset to build training data
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 指定要构建训练数据的数据集
- en: ❺ Specifies which commit of a dataset to build training data, optional
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 指定要构建训练数据的数据集的提交，可选
- en: ❻ Filters data by commit tags, optional
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 按提交标签过滤数据，可选
- en: ❼ The payload of the training dataset fetch API
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 训练数据集获取 API 的有效载荷
- en: ❽ Version hash string represents the training dataset snapshot.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 版本哈希字符串代表训练数据集快照。
- en: Why we need two APIs (two steps) to fetch a dataset
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们需要两个 API（两个步骤）来获取数据集
- en: If we only publish one API for acquiring training data, the caller needs to
    wait on the API call until the backend data preparation completes to obtain the
    final training data. If the data preparation takes a long time, this request will
    time out.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只发布一个用于获取训练数据的 API，则调用者需要在后端数据准备完成后等待 API 调用，以获取最终的训练数据。如果数据准备需要很长时间，则此请求将超时。
- en: A deep learning dataset is normally big (at the gigabyte level); it can take
    minutes or hours to complete the network I/O data transfer and local data aggregation.
    So the common solution to acquiring large data is to offer two APIs—one for submitting
    data preparation requests and another for querying the data status—and pull down
    the result when the request is complete. In this way, the dataset fetching API
    performs consistently regardless of the size of the dataset.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习数据集通常很大（以 GB 为单位）；进行网络 I/O 数据传输和本地数据聚合可能需要几分钟或几小时。因此，获取大型数据的常见解决方案是提供两个
    API——一个用于提交数据准备请求，另一个用于查询数据状态，并在请求完成时拉取结果。通过这种方式，数据集获取 API 的性能始终如一，无论数据集的大小如何。
- en: Sending the prepare training dataset request
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 发送准备训练数据集请求
- en: Now let’s look at the code workflow of the `PrepareTrainingDataset` API. Figure
    2.11 shows how our sample service handles Julia’s preparation training dataset
    request.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看一下 `PrepareTrainingDataset` API 的代码工作流程。图 2.11 显示了我们的示例服务如何处理 Julia 的准备训练数据集请求。
- en: '![](../Images/02-11.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-11.png)'
- en: 'Figure 2.11 A high-level overview of the eight steps to responding to a dataset
    build request: (1) the user submits a dataset preparation request with data filters;
    (2) DM selects data from commits that satisfy the data filters; (3 and 4) DM generates
    a version string to represent the training data; and (5–8) DM starts a background
    job to produce the training data.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.11 对应数据集构建请求的八个步骤的高层概述：（1）用户提交具有数据过滤器的数据集准备请求；（2）DM 从满足数据过滤器的提交中选择数据；（3
    和 4）DM 生成表示训练数据的版本字符串；以及（5-8）DM 启动后台作业以生成训练数据。
- en: 'When DM receives a dataset preparation request (figure 2.11, step 1), it carries
    out three acts:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 当 DM 收到数据集准备请求（图 2.11，步骤 1）时，它执行三个动作：
- en: Tries to find the dataset in its storage with the given dataset ID.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试使用给定的数据集 ID 在其存储中查找数据集。
- en: Applies the given data filter to select commits from the dataset.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将给定的数据过滤器应用于从数据集中选择提交。
- en: Creates a `versionedSnapshot` object to track training data in its internal
    storage (`versionHashRegistry)`. The ID of the `versionedSnapshot` object is a
    hash string generated from the selected commits’ ID list.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建 `versionedSnapshot` 对象以跟踪内部存储中的训练数据（`versionHashRegistry`）。`versionedSnapshot`
    对象的 ID 是从所选提交的 ID 列表生成的哈希字符串。
- en: The `versionedSnapshot` object is the training dataset Julia wants; it is a
    group of immutable static files from the selected commits. Julia could use the
    hash string (snapshot ID) returned at step 3 to query the dataset preparation
    status and get the data-downloadable URL when the training dataset is ready. With
    this version string, Julia can always obtain the same training data (`versionedSnapshot`)
    from any time in the future, which is how dataset reproducibility is supported.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '`versionedSnapshot` 对象是 Julia 想要的训练数据集；它是从所选提交中的不可变静态文件组成的。在步骤 3 返回哈希字符串（快照
    ID）后，Julia 可以使用它来查询数据集准备状态，并在训练数据集准备好时获取数据可下载的 URL。使用此版本字符串，Julia 可以始终从将来的任何时间获取相同的训练数据（`versionedSnapshot`），这就是支持数据集可重现性的方式。'
- en: A side benefit of `versionedSnapshot` is that it can be used as a cache across
    different `PrepareTrainingDataset` API calls. If the snapshot ID—a hash string
    of a list of commits—already exists, we return the existing `versionedSnapshot`
    without rebuilding the same data, which can save computation time and network
    bandwidth.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '`versionedSnapshot`的一个副产品是它可以在不同的`PrepareTrainingDataset` API调用之间用作缓存。如果快照ID（一系列提交的哈希字符串）已经存在，我们将返回现有的`versionedSnapshot`而不重建相同的数据，这可以节省计算时间和网络带宽。'
- en: Note In our design, the data filtering happens at the commit level, not at the
    individual example level; for example, having a filter tag `"DataType=Training"`
    in the preparation request indicates that the user only wants data from the commits
    that are labeled `"DataType=Training"`.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 在我们的设计中，数据过滤发生在提交级别，而不是在个别示例级别；例如，在准备请求中使用过滤标记`"DataType=Training"`表明用户只希望从标记为`"DataType=Training"`的提交中获取数据。
- en: After step 3, DM will spawn a background thread to build the training dataset.
    In the background job, DM will download the files of each dataset commit from
    the MinIO server to the local, aggregate and compress them into one file in a
    predefined format, and upload them back to the MinIO server in a different bucket
    (steps 6 and 7). Next, DM will put the data URL of the actual training data in
    the `versionedSnapshot` object and update its status to `"READY"` (step 8). Now
    Julia can find the data URLs from the returned `versionedSnapshot` object and
    start to download the training data.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 第3步之后，DM将生成一个后台线程来构建训练数据集。在后台作业中，DM将从MinIO服务器下载每个数据集提交的文件到本地，将其聚合并压缩成一个预定义格式的文件，并将其上传回MinIO服务器的不同存储桶中（步骤6和7）。接下来，DM将在`versionedSnapshot`对象中放置实际训练数据的数据URL，并将其状态更新为`"READY"`（步骤8）。现在Julia可以从返回的`versionedSnapshot`对象中找到数据URL并开始下载训练数据。
- en: What we haven’t covered is the data schema. In the dataset management service,
    we save the ingested data (`commit`) and the generated training data (`versionedSnapshot`)
    in two different data formats. A data merge operation (figure 2.11, steps 6 and
    7) aggregates the raw ingested data (the selected commits) and converts it into
    training data in an intent classification training data schema. We will discuss
    data schemas in detail in section 2.2.6\. Listing 2.10 highlights the code implemented
    for figure 2.11.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有涉及的是数据模式。在数据集管理服务中，我们将摄取的数据（`commit`）和生成的训练数据（`versionedSnapshot`）保存在两种不同的数据格式中。数据合并操作（图2.11，步骤6和7）将原始摄取的数据（所选提交）聚合并将其转换为意图分类训练数据模式中的训练数据。我们将在2.2.6节详细讨论数据模式。列表2.10突出显示了图2.11中实现的代码。
- en: Listing 2.10 Preparing training data request API
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.10 准备训练数据请求API
- en: '[PRE14]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Creates VersionedSnapshot object to represent the training dataset
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建VersionedSnapshot对象以表示训练数据集
- en: Fetching the training dataset
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 获取训练数据集
- en: Once the DM service receives a training dataset preparation request on the `prepareTrainingDataset`
    API, it will spawn a background job to build the training data and return a `version_hash`
    string for tracking purposes. Julia can use the `FetchTrainingDataset` API and
    the `version_hash` string to query the dataset-building progress and eventually
    get the training dataset. Figure 2.12 shows how dataset fetching requests are
    handled in DM.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦DM服务收到了`prepareTrainingDataset` API上的训练数据准备请求，它将生成一个后台作业来构建训练数据，并返回一个`version_hash`字符串用于跟踪目的。Julia可以使用`FetchTrainingDataset`
    API和`version_hash`字符串来查询数据集构建进度，并最终获取训练数据。图2.12展示了DM中如何处理数据集获取请求。
- en: '![](../Images/02-12.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-12.png)'
- en: 'Figure 2.12 A high-level overview of the three steps to serving a dataset fetching
    request: (1) the user calls the `FetchTrainingDataset` API with a dataset ID and
    a version string; (2) DM will search the `versionHashRegistry` of the dataset
    in its internal storage and return a `versionedSnapshot` object; and (3) the `versionedSnapshot`
    object will have a download URL when the data preparation job is completed.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.12 数据集获取请求服务的三个步骤的高级概述：（1）用户使用数据集ID和版本字符串调用`FetchTrainingDataset` API；（2）DM将在其内部存储中搜索数据集的`versionHashRegistry`并返回一个`versionedSnapshot`对象；（3）当数据准备作业完成时，`versionedSnapshot`对象将具有一个下载URL。
- en: The fetch training dataset is essentially querying the training data preparation
    request status. For each dataset, the DM service creates a `versionedSnapshot`
    object to track each training dataset produced by the `prepareTrainingDataset`
    request.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 获取训练数据集本质上是查询训练数据准备请求的状态。对于每个数据集，DM服务都会创建一个`versionedSnapshot`对象来跟踪`prepareTrainingDataset`请求生成的每个训练数据集。
- en: When a user sends a fetch dataset query, we simply use the hash string in the
    request to search its corresponding `versionedSnapshot` object in the dataset’s
    training snapshots (`versionHashRegistry`) and return it to the user if it exists.
    The `versionedSnapshot` object will keep being updated by the background training
    data process job (figure 2.11, steps 5–8). When the job completes, it will write
    the training data URL to the `versionedSnapshot` object; therefore, the user gets
    the training data at the end. See the code implementation in the following listing.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户发送获取数据集查询时，我们只需在请求中使用哈希字符串来搜索其对应的`versionedSnapshot`对象在数据集的训练快照（`versionHashRegistry`）中是否存在，如果存在则将其返回给用户。`versionedSnapshot`对象将由后台训练数据处理作业（图
    2.11，步骤 5–8）不断更新。当作业完成时，它将训练数据 URL 写入`versionedSnapshot`对象；因此，用户最终获取训练数据。请参见以下清单中的代码实现。
- en: Listing 2.11 Preparing the training data request API
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 2.11 准备训练数据请求 API
- en: '[PRE15]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Searches versionedSnapshot in a dataset’s training snapshots
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在数据集的训练快照中搜索`versionedSnapshot`
- en: ❷ Returns versionedSnapshot; it contains the latest progress of dataset preparation.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 返回`versionedSnapshot`；其中包含了数据集准备的最新进展。
- en: 2.2.5 Internal dataset storage
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.5 内部数据集存储
- en: The internal storage of the sample service is simply a list of in-memory dataset
    objects. Earlier we talked about how a dataset can be both dynamic and static.
    On one hand, a dataset is a logical file group, changing dynamically as it continuously
    absorbs new data from a variety of sources. On the other hand, it’s static and
    reproducible for training.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 示例服务的内部存储仅是一组内存中的数据集对象。之前我们讨论了数据集既可以是动态的又可以是静态的。一方面，数据集是一个逻辑文件组，随着不断地从各种来源吸收新数据而动态变化。另一方面，它是静态的且可重现的用于训练。
- en: 'To showcase this concept, we design each dataset containing a list of commits
    and a list of versioned snapshots. A commit represents the dynamically ingested
    data: data added by a data ingestion call (`CreateDataset` or `UpdateDataset`);
    a commit also has tags and messages for annotation purposes. A versioned snapshot
    represents the static training data, which, produced by the prepare training dataset
    request (`PrepareTrainingDataset`), is converted from a list of selected commits.
    Each snapshot is associated with a version; once the training dataset is built,
    you can use this version string to fetch the corresponding training data (`FetchTrainingDataset`)
    at any time to reuse. Figure 2.13 visualizes the internal storage structure of
    a dataset.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示这个概念，我们设计了每个数据集，其中包含提交列表和版本化快照列表。提交代表动态摄入的数据：通过数据摄入调用（`CreateDataset` 或
    `UpdateDataset`）添加的数据；提交还具有标签和注释目的的消息。版本化快照代表静态训练数据，由准备训练数据集请求（`PrepareTrainingDataset`）产生，从所选提交列表转换而来。每个快照都与一个版本相关联；一旦构建了训练数据集，您可以使用该版本字符串随时获取相应的训练数据（`FetchTrainingDataset`）以供重用。图
    2.13 可视化了数据集的内部存储结构。
- en: '![](../Images/02-13.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-13.png)'
- en: 'Figure 2.13 An internal dataset storage overview. A dataset stores two types
    of data: commits for the ingested raw data and versioned snapshots for the training
    dataset. The dataset metadata and data URLs are stored in the dataset management
    service, and the actual data is stored in the cloud object storage service.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.13 内部数据集存储概述。数据集存储两种类型的数据：用于摄入原始数据的提交和用于训练数据集的版本快照。数据集元数据和数据 URL 存储在数据集管理服务中，实际数据存储在云对象存储服务中。
- en: Note Although the individual training examples of different types of datasets
    can be in different forms, such as images, audios, and text sentences, the dataset’s
    operations (creating, updating, and querying dataset summary) and its dynamic/static
    characters are the same. Because we designed a unified API set across all dataset
    types, we can use one uniformed storage structure to store all different kinds
    of datasets.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 注意虽然不同类型的数据集的各个训练示例可以采用不同的形式，例如图像、音频和文本句子，但数据集的操作（创建、更新和查询数据集摘要）以及其动态/静态特性是相同的。由于我们在所有数据集类型上设计了统一的
    API 集，我们可以使用统一的存储结构来存储所有不同类型的数据集。
- en: In our storage, the actual files (commit data, snapshot data) are stored in
    cloud object storage (such as Amazon S3), and we only keep dataset metadata (see
    explanation later) in our DM system. By offloading file storage work and only
    tracking the file links, we can focus on organizing the datasets and tracking
    their metadata, such as edit history, data statistics, training snapshots, and
    ownership.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的存储中，实际文件（提交数据、快照数据）存储在云对象存储（如Amazon S3）中，我们只在我们的DM系统中保留数据集元数据（稍后解释）。通过卸载文件存储工作并仅跟踪文件链接，我们可以专注于组织数据集并跟踪其元数据，例如编辑历史、数据统计、训练快照和所有权。
- en: Dataset metadata
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集元数据
- en: We define dataset metadata as everything except actual data files, such as the
    dataset ID, data owner, change history (audits), training snapshots, commits,
    data statistics, and so on.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数据集元数据定义为除了实际数据文件以外的所有内容，例如数据集ID、数据所有者、变更历史（审计）、训练快照、提交、数据统计等等。
- en: For demonstration purposes, we store the datasets’ metadata in a memory dictionary
    with the ID as key and put all data files into the MinIO server. But you can extend
    it to use a database or NoSQL database to store the dataset’s metadata.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示目的，我们将数据集的元数据存储在一个内存字典中，以ID作为键，并将所有数据文件放入MinIO服务器。但您可以扩展它以使用数据库或NoSQL数据库来存储数据集的元数据。
- en: So far, we have talked about dataset storage concepts, but how do the actual
    dataset writing and reading work? How do we serialize commits and snapshots for
    different dataset types, such as `GENERIC` and `TEXT_INTENT` types?
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了数据集存储概念，但实际的数据集写入和读取是如何工作的呢？我们如何序列化不同数据集类型（例如`GENERIC`和`TEXT_INTENT`类型）的提交和快照？
- en: 'In the storage backend implementation, we use a simple inheritance concept
    to handle file operations for different dataset types. We define a `DatasetTransformer`
    interface as follows: the `ingest()` function saves input data into internal storage
    as a commit, and the `compress()` function merges data from selected commits into
    a version snapshot (training data).'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在存储后端实现中，我们使用简单的继承概念来处理不同数据集类型的文件操作。我们定义了一个`DatasetTransformer`接口如下：`ingest()`函数将输入数据保存到内部存储作为提交，`compress()`函数将来自选定提交的数据合并为版本快照（训练数据）。
- en: More specifically, for the `"TEXT_INTENT"` type dataset, we have `IntentTextTransformer`
    to apply the strong type of file schema on file conversion. For a `"GENERIC"`
    type dataset, we have `GenericTransformer` to save data in the original format
    without any checks or format conversions. Figure 2.14 illustrates these.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，对于“`TEXT_INTENT`”类型的数据集，我们有`IntentTextTransformer`来应用文件转换的强类型模式。对于“`GENERIC`”类型的数据集，我们有`GenericTransformer`将数据保存在原始格式中，没有任何检查或格式转换。图2.14说明了这些。
- en: '![](../Images/02-14.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-14.png)'
- en: Figure 2.14 Implement `DatasetTransformer` interface to handle different dataset
    types; implement ingest function to save raw input data as commit; and implement
    compress function to aggregate commit data to training data.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.14 实现`DatasetTransformer`接口来处理不同的数据集类型；实现ingest函数将原始输入数据保存为提交；实现compress函数将提交数据聚合为训练数据。
- en: From figure 2.14, we see that the raw intent classification data from data ingestion
    API (section 2.2.3) is saved as a commit by `IntentTextTransformer:Ingest()`;
    the intent classification training data produced by training dataset fetching
    API (section 2.2.4) is saved as a versioned snapshot by `IntentTextTransformer:Compress()`.
    Because they are plain Java code, we leave it for your own discovery; you can
    find the implementation code at our Git repo (org/orca3/miniAutoML/dataManagement/
    transformers/IntentTextTransformer.java).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 从图2.14可以看出，通过数据摄取API（第2.2.3节）保存的原意图分类数据由`IntentTextTransformer:Ingest()`保存为提交；通过训练数据集提取API（第2.2.4节）生成的意图分类训练数据由`IntentTextTransformer:Compress()`保存为版本化的快照。因为它们是纯Java代码，我们留给您自己去发现；您可以在我们的Git存储库中找到实现代码（org/orca3/miniAutoML/dataManagement/transformers/IntentTextTransformer.java）。
- en: 2.2.6 Data schemas
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.6 数据模式
- en: 'So far, we have seen all the APIs, workflows, and internal storage structures.
    Now let’s consider what the data looks like in the DM service. For each kind of
    strongly typed dataset, such as a “`TEXT_INTENT`” dataset, we defined two data
    schemas: one for data ingestion and one for training data fetching (figure 2.15).'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了所有的API、工作流程和内部存储结构。现在让我们来考虑DM服务中的数据是什么样子的。对于每一种强类型数据集，例如“`TEXT_INTENT`”数据集，我们定义了两种数据模式：一种用于数据摄取，一种用于训练数据提取（图2.15）。
- en: '![](../Images/02-15.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-15.png)'
- en: 'Figure 2.15 Each type of dataset has two data schemas: ingestion data schema
    and training data schema. These two schemas will ensure that the data we accept
    and the data we produce follow our data spec.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.15每一种类型的数据集都有两个数据模式：摄取数据模式和训练数据模式。这两个模式将确保我们接受的数据和我们生成的数据都遵循我们的数据规范。
- en: Figure 2.15 shows how the DM service uses two data schemas to implement its
    data contract. Step 1 uses the ingestion data schema to validate the raw input
    data; step 2 uses the training data schema to convert the raw data to the training
    data format; step 3 saves the converted data as a commit; and step 4 merges the
    selected commits into one versioned snapshot when building a training dataset
    but still obeys the training data schema.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.15显示了DM服务如何使用两个数据模式来实现其数据合同。步骤1使用摄取数据模式验证原始输入数据；步骤2使用训练数据模式将原始数据转换为训练数据格式；步骤3将转换后的数据保存为一个提交；步骤4在构建训练数据集时将选定的提交合并为一个版本化的快照，但仍然遵循训练数据模式。
- en: 'These two different data schemas are the data contract that DM service provides
    to our two different users: Jianguo and Julia. No matter how Jianguo collects
    the data, it needs to be converted to the ingestion data format to insert into
    DM. Alternatively, because DM guarantees that the output training data follows
    the training data schema, Julia feels comfortable consuming the dataset without
    worrying about being affected by the data collection changes made by Jianguo.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个不同的数据模式是DM服务提供给我们两个不同用户（Jianguo和Julia）的数据合同。无论Jianguo如何收集数据，它都需要转换为摄入数据格式以插入到DM中。或者，由于DM保证输出的训练数据遵循训练数据模式，Julia可以放心地使用数据集，而不用担心Jianguo所做的数据收集更改会影响到她。
- en: A data ingestion schema
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 一个数据摄入模式
- en: 'We have seen the data schema concept; now let’s look at the ingestion data
    schema we defined for the `TEXT_INTENT` dataset:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了数据模式的概念，现在让我们来看看我们为`TEXT_INTENT`数据集定义的摄入数据模式：
- en: '[PRE16]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'For simplicity, our ingestion data schema requires that all the input data
    for the `TEXT_INTENT` dataset must be in a CSV file format. The first column is
    text utterance, and the remainder of the columns are labels. See a sample CSV
    file as follows:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化起见，我们的数据摄入模式要求`TEXT_INTENT`数据集的所有输入数据必须以CSV文件格式提供。第一列是文本话语，其余列是标签。请参考以下示例CSV文件：
- en: '[PRE17]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Labels
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 标签
- en: A training dataset schema
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据集的模式
- en: 'For `TEXT_INTENT` training data, our schema defines the output data as a zip
    file that contains two files: examples.csv and labels.csv. Labels.csv defines
    a label name to a label ID mapping, and the examples.csv defines training text
    (utterance) to label ID mapping. See the following examples:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`TEXT_INTENT`训练数据，我们的模式将输出数据定义为一个包含两个文件的压缩文件：examples.csv和labels.csv。labels.csv定义了标签名称到标签ID的映射，而examples.csv定义了训练文本（话语）到标签ID的映射。请参考以下示例：
- en: '[PRE18]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Why we use a self-defined data structure
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们使用自定义的数据结构
- en: We build `TEXT_INTENT` with the self-defined data schema instead of using the
    PyTorch or Tensorflow dataset format (like TFRecordDataset) to create abstraction
    from model training frameworks.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用自定义的数据模式来构建`TEXT_INTENT`，而不是使用PyTorch或Tensorflow数据集格式（如TFRecordDataset）来创建与模型训练框架的抽象。
- en: By choosing a framework-specific dataset format, your training code will also
    need to be written in the framework, which is not ideal. Introducing a self-defined
    intermediate dataset format can make the DM framework-neutral, so no framework-specific
    training codes are required.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 选择一个特定于框架的数据集格式，你的训练代码也需要用该框架编写，这并不理想。引入自定义的中间数据集格式可以使DM框架中立，因此不需要特定于框架的训练代码。
- en: The benefit of having two strongly typed data schemas in one dataset
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 一个数据集中有两个强类型的数据模式的好处
- en: By having two strongly typed data schemas in a dataset and letting DM do the
    data transformation from the ingestion data format to the training data format,
    we could parallelize data collection development and training code development.
    For example, when Jianguo wants to add a new feature—“text language”—to the `TEXT_INTENT`
    dataset, he can work with the DM service developers to update the data ingestion
    schema to add a new data field.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在数据集中使用两个强类型的数据模式，并且让DM将数据从摄取的数据格式转换为训练数据格式，我们可以并行开发数据收集和训练代码开发。例如，当Jianguo想要向`TEXT_INTENT`数据集添加一个新特征——“文本语言”时，他可以与DM服务开发人员合作更新数据摄入模式以添加一个新的数据字段。
- en: Julia won’t be affected because the training data schema is not changed. Julia
    may come to us later to update the training data schema when she has the bandwidth
    to consume the new feature in her training code. The key point is that Jianguo
    and Julia don’t have to work synchronously to introduce a new dataset enhancement;
    they can work independently.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: Julia不会受到影响，因为训练数据模式没有改变。当Julia有带宽来消费她训练代码中的新功能时，她可能会后来向我们更新训练数据模式。关键是，Jianguo和Julia不必同步工作来引入新的数据集增强；他们可以独立工作。
- en: Note For simplicity and demo purpose, we choose to use a CSV file to store data.
    The problem with using plain CSV files is their lack of backward compatibility
    support and data-type validation support. In production, we recommend using Parquet,
    Google protobuf, or Avro to define data schemas and store data. They come with
    a set of libraries for data validation, data serialization, and schema backward-compatible
    support.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：为了简单起见和演示目的，我们选择使用CSV文件来存储数据。使用纯CSV文件的问题在于它们缺乏向后兼容性支持和数据类型验证支持。在生产环境中，我们建议使用Parquet、Google
    protobuf或Avro来定义数据模式和存储数据。它们带有一组用于数据验证、数据序列化和模式向后兼容性支持的库。
- en: 'A generic dataset: A dataset with no schema'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 通用数据集：没有模式的数据集
- en: Although we emphasize at multiple places that defining strongly typed dataset
    schemas is fundamental to dataset management service, we will make an exception
    here by adding a free format dataset type—the `GENERIC` dataset. Unlike the strongly
    typed TEXT_ INENT dataset, a `GENERIC`-type dataset has no data schema validation.
    Our service will save any raw input data as is, and when building training data,
    the service simply packs all the raw data together in its original format into
    a training dataset.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在多个地方强调定义强类型数据集模式对数据集管理服务是基础性的，但在这里我们将例外情况添加了一个自由格式的数据集类型——`GENERIC`数据集。与强类型TEXT_
    INENT数据集不同，`GENERIC`类型数据集没有数据模式验证。我们的服务将任何原始输入数据保存原样，并在构建训练数据时，服务只是将所有原始数据按其原始格式打包到训练数据集中。
- en: A `GENERIC` dataset type may sound like a bad idea because we basically pass
    whatever data we receive from upstream data sources to the downstream training
    application, which can break the data parsing logic in the training code easily.
    This is definitely not an option for production, but it provides the agility necessary
    for experimental projects.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`GENERIC`数据集类型听起来可能不是一个好主意，因为我们基本上将来自上游数据源的任何数据传递给下游训练应用程序，这很容易破坏训练代码中的数据解析逻辑。这绝对不是一个生产选项，但它为实验项目提供了所需的灵活性。
- en: Although a strongly typed data schema offers good data type safety protection,
    it comes at the cost of maintaining it. It is quite annoying when you have to
    make frequent schema changes in the DM service to adopt the new data format required
    by a new experimentation.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管强类型数据模式提供了良好的数据类型安全保护，但需要付出维护的代价。当您不得不在DM服务中频繁进行模式更改以采用新的实验所需的新数据格式时，这是相当烦人的。
- en: At the beginning of a deep learning project, a lot of things are uncertain,
    such as which deep learning algorithm works the best, what kind of data we can
    collect, and what data schema we should choose. To move forward with all these
    uncertainties, we need a flexible way to handle arbitrary data to enable model
    training experimentations. This is what `GENERIC` dataset type designs are for.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习项目开始时，有很多事情是不确定的，比如哪种深度学习算法效果最好，我们可以收集到什么样的数据，以及我们应该选择什么样的数据模式。为了解决所有这些不确定性，我们需要一种灵活的方式来处理任意数据，以启用模型训练实验。这就是`GENERIC`数据集类型设计的目的。
- en: Once the business value is proven and the deep learning algorithm is chosen,
    we are now clear about how the training data looks; then it’s time for us to define
    a strongly typed dataset in the dataset management service. In the next section,
    we will discuss how to add a new strongly typed dataset.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦业务价值得到证明，并选择了深度学习算法，我们现在清楚了训练数据的样子；然后是时候在数据集管理服务中定义一个强类型数据集了。在接下来的部分中，我们将讨论如何添加一个新的强类型数据集。
- en: 2.2.7 Adding new dataset type (IMAGE_CLASS)
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.7 添加新的数据集类型（IMAGE_CLASS）
- en: Let’s imagine one day Julia asks us (the platform developers) to promote her
    experimental image classification project to a formal project. Julia and her team
    is developing an image classification model by using a `GENERIC` dataset, and
    because they get good results, they now want to define a strongly typed dataset
    (`IMAGE_CLASS`) to stabilize the data schema for raw data collection and training
    data consumption. This will protect the training code from future dataset updates.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设有一天，朱莉娅（平台开发者之一）要求我们将她的实验性图像分类项目提升为正式项目。朱莉娅和她的团队正在使用 `GENERIC` 数据集开发图像分类模型，并且因为他们取得了良好的结果，现在他们想要定义一个强类型数据集（`IMAGE_CLASS`）来稳定原始数据收集和训练数据消费的数据模式。这将保护训练代码免受未来数据集更新的影响。
- en: 'To add a new dataset type—`IMAGE_CLASS`—we can follow three steps. First, we
    must define the training data format. After discussing with Julia, we decide the
    training data produced by `FetchTrainingDataset` API will be a zip file; it will
    contain these three files:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 要添加一个新的数据集类型——`IMAGE_CLASS`，我们可以按照三个步骤进行。首先，我们必须定义训练数据格式。在与朱莉娅讨论后，我们决定由 `FetchTrainingDataset`
    API 生成的训练数据将是一个 zip 文件；它将包含以下三个文件：
- en: '[PRE19]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The examples.csv and labels.csv files are manifest files that define labels
    for each training image. The actual image files are stored in the examples folder.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: examples.csv 和 labels.csv 文件是定义每个训练图像标签的清单文件。实际图像文件存储在 examples 文件夹中。
- en: 'Second, define the ingestion data format. We need to discuss the ingestion
    data schema with Jianguo, the data engineer who collects images and labels them.
    We agree that the payload data for each `CreateDataset` and `UpdateDataset` request
    is also a zip file; its directory looks as follows: the zip file should be a folder
    with only subdirectories. Each subdirectory under the root folder represents a
    label; the images under it belong to this label. The subdirectory should only
    contain images and not any nested directories:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，定义摄取数据格式。我们需要与收集图像并为其标记标签的数据工程师建国讨论摄取数据架构。我们一致同意，每个 `CreateDataset` 和 `UpdateDataset`
    请求的有效负载数据也是一个 zip 文件；其目录如下所示：zip 文件应该是只包含子目录的文件夹。根文件夹下的每个子目录代表一个标签；其下的图像属于此标签。子目录应只包含图像，而不包含任何嵌套目录：
- en: '[PRE20]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The last step is the code change. After having two data schemas in mind, we
    need to create an `ImageClassTransformer` class that implements the `DatasetTransformer`
    interface to build the data reads and writes logic.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是代码更改。在心中有两种数据模式之后，我们需要创建一个实现了 `DatasetTransformer` 接口的 `ImageClassTransformer`
    类来构建数据读取和写入逻辑。
- en: We first implement the `ImageClassTransformer.ingest()` function. The logic
    needs to use the input data format—defined in step 2—to parse the input data in
    the dataset creation and update requests and then convert the input data to a
    training data format and save it as a commit of the dataset.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先实现 `ImageClassTransformer.ingest()` 函数。逻辑需要使用第 2 步中定义的输入数据格式来解析数据集创建和更新请求中的输入数据，然后将输入数据转换为训练数据格式并将其保存为数据集的提交。
- en: We then implement the `ImageClassTransformer.compress()` function, which first
    selects commits by matching data filters and then merges the matched commits into
    a single training snapshot. As the last step, we register the `ImageClassTransformer
    .ingest()` function to the `DatasetIngestion.ingestion()` function with an IMAGE_
    CLASS type and register `ImageClassTransformer.compress()` to `DatasetCompressor
    .run()` with an `IMAGE_CLASS` type.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们实现 `ImageClassTransformer.compress()` 函数，它首先通过匹配数据过滤器选择提交，然后将匹配的提交合并为单个训练快照。最后一步，我们将
    `ImageClassTransformer.ingest()` 函数注册到 `DatasetIngestion.ingestion()` 函数中，类型为
    `IMAGE_CLASS`，并将 `ImageClassTransformer.compress()` 注册到 `DatasetCompressor.run()`
    函数中，类型为 `IMAGE_CLASS`。
- en: As you can see, with proper dataset structure, we can support new dataset types
    by just adding a few new code snippets. The existing types of datasets and the
    public data ingestion and fetching APIs are not affected.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，通过合适的数据集结构，我们只需添加几个新的代码片段就能支持新的数据集类型。现有的数据集类型和公共数据摄取及获取 API 不会受到影响。
- en: 2.2.8 Service design recap
  id: totrans-298
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.8 服务设计总结
- en: 'Let’s recap how this sample dataset management service addresses the five design
    principles introduced in section 2.1.2:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下这个示例数据集管理服务是如何满足第 2.1.2 节介绍的五项设计原则的：
- en: '*Principle 1*—Support dataset reproducibility. Our sample DM service saves
    all the generated training data as a versioned snapshot with a version hash string
    as key. Users can apply this version string to obtain the training data snapshot
    at any time.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*原则 1*—支持数据集可重现性。我们的示例 DM 服务将所有生成的训练数据保存为带有版本哈希字符串的版本化快照，用户可以随时应用该版本字符串来获取训练数据快照。'
- en: '*Principle 2*—Provide a unified experience across different dataset types.
    The data ingestion API and training data fetching API work the same way for all
    dataset types and sizes.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*原则 2*—为不同的数据集类型提供统一的体验。数据摄取 API 和训练数据获取 API 对所有数据集类型和大小的工作方式相同。'
- en: '*Principle 3*—Adopt strongly typed data schema. Our sample TEXT_INENT type
    and `IMAGE_CLASS` type datasets apply a self-defined data schema to both raw ingestion
    data and training data.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*原则 3*—采用强类型数据架构。我们的示例 TEXT_INENT 类型和 `IMAGE_CLASS` 类型数据集对原始摄取数据和训练数据都应用自定义数据架构。'
- en: '*Principle 4*—Ensure API consistency and handle scaling internally. Although
    we save all datasets’ metadata in memory in our sample code (for simplicity),
    we can easily implement the dataset storage structure in cloud object storage;
    in theory, it has infinite capacity. Also, we require data URLs to send data and
    return data, so no matter how large a dataset is, our API remains consistent.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*原则 4*—确保 API 一致性并在内部处理扩展。尽管我们在示例代码中将所有数据集的元数据保存在内存中（为了简单起见），但我们可以轻松地在云对象存储中实现数据集存储结构；理论上，它具有无限的容量。此外，我们要求数据
    URL 用于发送数据和返回数据，因此无论数据集有多大，我们的 API 都保持一致。'
- en: '*Principle 5*—Guarantee data persistency. Every dataset creation request and
    update request creates a new commit; every training data prepare request creates
    a versioned snapshot. Both commit and snapshot are immutable and persist with
    no data expiration limits.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*原则 5*—保证数据持久性。每个数据集创建请求和更新请求都会创建一个新的提交；每个训练数据准备请求都会创建一个版本化的快照。提交和快照都是不可变的，并且不受数据到期限制的持久存在。'
- en: Note We have trimmed many important features from the sample dataset management
    service to keep it simple. Management APIs, for example, allow you to delete data,
    revert data commits, and view data audit history. Feel free to fork the repo and
    try to implement them.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 我们从示例数据集管理服务中删除了许多重要功能，以保持简单性。管理 API，例如允许您删除数据，还原数据提交以及查看数据审计历史记录。欢迎 fork
    该存储库并尝试实现它们。
- en: 2.3 Open source approaches
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 开源方法
- en: 'If you are interested in employing open source approaches to set up dataset
    management functionality, we select two approaches for you: Delta Lake and Pachyderm.
    Let’s look at them individually.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有兴趣采用开源方法来设置数据集管理功能，我们为您选择了两种方法：Delta Lake 和 Pachyderm。让我们分别来看看它们。
- en: 2.3.1 Delta Lake and Petastorm with Apache Spark family
  id: totrans-308
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.1 Delta Lake 和 Petastorm 与 Apache Spark 家族
- en: In this approach, we propose to save data in a Delta Lake table and use the
    Petastorm library to convert the table data to PyTorch and Tensorflow dataset
    objects. The dataset can be consumed in training code seamlessly.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，我们建议将数据保存在 Delta Lake 表中，并使用 Petastorm 库将表数据转换为 PyTorch 和 Tensorflow
    数据集对象。数据集可以在训练代码中无缝消耗。
- en: Delta Lake
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake
- en: Delta Lake is a storage layer that brings scalable, ACID (atomicity, consistency,
    isolation, durability) transactions to Apache Spark and other cloud object stores
    (e.g., Amazon S3). Delta Lake is developed as open source by Databricks, a respected
    data and AI company.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake 是一个存储层，为 Apache Spark 和其他云对象存储（例如 Amazon S3）带来可扩展的、ACID（原子性、一致性、隔离性、持久性）事务。Delta
    Lake 由 Databricks，一个备受尊敬的数据和人工智能公司，作为开源项目开发。
- en: Cloud storage services, such as Amazon S3, are some of the most scalable and
    cost-effective storage systems in the IT industry. They are ideal places to build
    large data warehouses, but their key-values store design makes it difficult to
    achieve ACID transactions and high performance. The metadata operations such as
    listing objects are expensive, and consistency guarantees are limited.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 云存储服务，例如 Amazon S3，是 IT 行业中最具可扩展性和成本效益的存储系统之一。它们是构建大型数据仓库的理想场所，但其键值存储设计使得难以实现
    ACID 事务和高性能。元数据操作（例如列出对象）昂贵，并且一致性保证有限。
- en: Delta Lake is designed to fill the previously discussed gaps. It works as a
    file system that stores batch and streaming data in object storage (such as Amazon
    S3). In addition, Delta Lake manages metadata, caching, and indexing for its table
    structure and schema enforcement. It provides ACID properties, time travel, and
    significantly faster metadata operations for large tabular datasets. See figure
    2.16 for the Delta Lake concept graph.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake 的设计旨在填补前面讨论的空白。它作为一个文件系统工作，将批处理和流处理数据存储在对象存储中（例如亚马逊S3）。此外，Delta Lake
    管理表结构和模式强制执行的元数据、缓存和索引。它提供了ACID属性、时间旅行和针对大型表格数据集的显著更快的元数据操作。请参见图2.16了解 Delta Lake
    概念图。
- en: '![](../Images/02-16.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-16.png)'
- en: Figure 2.16 Delta Lake data ingestion and processing workflow. Both stream data
    and batch data can be saved as Delta Lake tables, and the Delta Lake tables are
    stored in the cloud object storage, such as Amazon S3.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.16 Delta Lake 数据摄入和处理工作流程。流数据和批数据都可以保存为 Delta Lake 表，并且 Delta Lake 表存储在云对象存储中，例如亚马逊S3。
- en: The Delta Lake table is the core concept of the system. When working with Delta
    Lake, you are usually dealing with Delta Lake tables. They are like SQL tables;
    you can query, insert, update, and merge table content. Schema protection in Delta
    Lake is one of its advantages. It supports schema validation on table writing,
    which prevents data pollution. It also tracks table history, so you can roll back
    a table to any of its past stages (known as time travel).
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake 表是系统的核心概念。在使用 Delta Lake 时，您通常正在处理 Delta Lake 表。它们就像SQL表一样；您可以查询、插入、更新和合并表内容。Delta
    Lake 中的模式保护是其优势之一。它支持在表写入时对模式进行验证，从而防止数据污染。它还跟踪表历史，因此您可以将表回滚到其过去的任何阶段（称为时间旅行）。
- en: 'For building data processing pipelines, Delta Lake recommends naming your tables
    in three categories: bronze, silver, and gold. First, we use bronze tables to
    keep the raw input from different sources (some of which are not so clean). Then
    the data flows constantly from bronze tables to silver tables with data cleaning
    and transformation (ETL). Finally, we perform data filtering and purification
    and save the results to gold tables. Each table is in a machine learningstate;
    they are reproducible and type-safe.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 对于构建数据处理管道，Delta Lake 建议将表命名为三个类别：铜（bronze）、银（silver）和金（gold）。首先，我们使用铜表存储来自不同来源的原始输入（其中一些可能不太干净）。然后，数据不断从铜表流向银表，经过数据清洗和转换（ETL）。最后，我们执行数据过滤和净化，并将结果保存到金表中。每个表都处于机器学习状态；它们是可重现的，并且类型安全。
- en: Why Delta Lake is a good option for deep learning dataset management
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么 Delta Lake 是深度学习数据集管理的良好选择
- en: The following are three features that make Delta Lake a good option for managing
    datasets for deep learning projects.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使 Delta Lake 成为管理深度学习项目数据集的良好选择的三个功能。
- en: First, Delta Lake supports dataset reproducibility. It has a “time travel” feature
    that has the ability to query the data as it existed at a certain point in time
    using data versioning. Imagine you have set up a continuously running ETL pipeline
    to keep your training dataset (gold table) up to date. Because Delta Lake tracks
    table updates as snapshots, every operation is automatically versioned as the
    pipeline writes into the dataset. This means all the training data snapshots are
    kept for free, and you can browse table update history and roll back to past stages
    easily. The following listing provides a few sample commands.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，Delta Lake 支持数据集的可重现性。它具有“时间旅行”功能，可以使用数据版本控制查询数据在特定时间点的状态。想象一下，您已经设置了一个持续运行的ETL管道来保持您的训练数据集（gold
    table）的最新状态。因为 Delta Lake 将表更新跟踪为快照，每个操作都会被自动版本化，当管道写入数据集时。这意味着所有训练数据的快照都是免费的，您可以轻松浏览表更新历史并回滚到过去的阶段。以下列表提供了一些示例命令。
- en: Listing 2.12 Delta Lake time travel commands
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.12 Delta Lake 时间旅行命令
- en: '[PRE21]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ Finds the dataset in Delta Lake
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在 Delta Lake 中查找数据集
- en: ❷ Lists the full history of the data
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 列出数据的完整历史
- en: ❸ Gets the last operation on the dataset
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 获取数据集上的最后一个操作
- en: ❹ Rolls back the dataset by time stamp
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 根据时间戳回滚数据集
- en: ❺ Rolls back dataset by version
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 根据版本回滚数据集
- en: Second, Delta Lake supports continuously streaming data processing. Its tables
    can handle the continuous flow of data from both historical and real-time streaming
    sources seamlessly. For example, your data pipeline or stream data source can
    keep adding data to the Delta Lake table while querying data from the table at
    the same time. This saves you extra steps when writing code to merge the new data
    with existing data.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，Delta Lake 支持持续的流式数据处理。它的表可以无缝处理来自历史和实时流数据源的连续数据流。例如，数据管道或流数据源可以在查询数据表的同时不断向
    Delta Lake 表中添加数据。这样可以节省编写代码将新数据与现有数据合并的额外步骤。
- en: Third, Delta Lake offers schema enforcement and evolvement. It applies schema
    validation on write. It will ensure that new data records match the table’s predefined
    schema; if the new data isn’t compatible with the table’s schema, Delta Lake will
    raise an exception. Having data type validation at writing time is better than
    at reading time because it’s difficult to clean data if it’s polluted.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，Delta Lake 提供模式强制执行和演化功能。它在写入时应用模式验证。它将确保新的数据记录与表的预定义模式匹配；如果新数据与表的模式不兼容，Delta
    Lake 将引发异常。在写入时进行数据类型验证要比在读取时进行更好，因为如果数据被污染，清理数据将变得困难。
- en: Besides strong schema enforcement, Delta Lake also allows you to add new columns
    to existing data tables without causing breaking changes. The dataset schema enforcement
    and adjustment (evolvement) capabilities are critical to deep learning projects.
    These capabilities protect the training data from being polluted by unintended
    data writes and offer safe data updates.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 除了强大的模式强制执行功能之外，Delta Lake 还允许您在现有数据表中添加新列而不会引起破坏性更改。对于深度学习项目来说，数据集模式强制执行和调整（演化）能力至关重要。这些功能可以保护训练数据免受意外数据写入污染，并提供安全的数据更新。
- en: Petastorm
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: Petastorm
- en: Petastorm is an open source data access library developed at Uber ATG (Advanced
    Technologies Group). It enables single-machine or distributed training and evaluation
    of deep learning models directly from datasets in the Apache Parquet format (a
    data file format designed for efficient data storage and retrieval).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: Petastorm 是 Uber ATG（高级技术组）开发的开源数据访问库。它可以直接从 Apache Parquet 格式的数据集中单机或分布式进行深度学习模型的训练和评估。（Apache
    Parquet 是一种为高效数据存储和检索而设计的数据文件格式。）
- en: Petastorm can convert Delta Lake tables to Tensorflow and PyTorch format datasets
    easily, and it also supports distributed training data partitions. With Petastorm,
    the training data from a Delta Lake table can be simply consumed by downstream
    training applications without worrying about the details of data conversion for
    a specific training framework. It also creates good isolation between the dataset
    format and training frameworks (Tensorflow, PyTorch, and PySpark). Figure 2.17
    visualizes the data conversion process.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: Petastorm 可以轻松地将 Delta Lake 表转换为 Tensorflow 和 PyTorch 格式的数据集，并且还支持分布式训练数据分区。使用
    Petastorm，可以简单地从 Delta Lake 表中消耗训练数据，而不必担心特定训练框架的数据转换细节。它还在数据集格式和训练框架（Tensorflow、PyTorch
    和 PySpark）之间创建了良好的隔离。图 2.17 可视化了数据转换过程。
- en: '![](../Images/02-17.png)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-17.png)'
- en: Figure 2.17 Petastorm converts the Delta Lake table to datasets that can be
    read by the PyTorch or Tensorflow framework.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.17 Petastorm 将 Delta Lake 表转换为可以被 PyTorch 或 Tensorflow 框架读取的数据集。
- en: Figure 2.17 depicts the Petastorm data conversion workflow. You can create a
    Petastorm spark converter that reads Delta Lake tables into its cache as Parquet
    files and generates Tensorflow or Pytorch dataset.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.17 描述了 Petastorm 的数据转换工作流。您可以创建一个 Petastorm spark 转换器，将 Delta Lake 表作为 Parquet
    文件读取到其缓存中，并生成 Tensorflow 或 Pytorch 数据集。
- en: 'Example: Preparing training data for a flower image classification'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 例子：为花朵图像分类准备训练数据
- en: Now that we have a general idea of Delta Lake and Petastorm, let’s see a concrete
    model training example. The following code snippets—code listings 2.13 and 2.14—demonstrate
    an end-to-end image classification model training workflow in two steps. First,
    they define an image process ETL pipeline that parses a group of image files into
    the Delta Lake table as an image dataset. Second, they use Petastorm to convert
    the Delta Lake table to a dataset that can be loaded into the PyTorch framework
    directly to start model training.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对 Delta Lake 和 Petastorm 有了一个大致的了解，让我们看一个具体的模型训练示例。接下来的代码片段——代码列表 2.13 和
    2.14——展示了一个端到端的图像分类模型训练工作流程的两个步骤。首先，它们定义了一个图像处理 ETL 管道，将一组图像文件解析为 Delta Lake 表作为图像数据集。然后，它们使用
    Petastorm 将 Delta Lake 表转换为可以直接加载到 PyTorch 框架中进行模型训练的数据集。
- en: Let’s first visit the four-step ETL data processing pipeline in code listing
    2.13\. You can also find the complete code at [http://mng.bz/JVPz](http://mng.bz/JVPz).
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先查看代码清单 2.13 中的四步 ETL 数据处理管道。您还可以在 [http://mng.bz/JVPz](http://mng.bz/JVPz)
    找到完整的代码。
- en: In the beginning step of the pipeline, we load the images from a folder, `flower_`
    `photos`, to spark as binary files. Second, we define the extract functions to
    obtain metadata from each image file, such as label name, file size, and image
    size. Third, we construct the data processing pipeline with the extract functions
    and then pass the image files to the pipeline, which will produce a data frame.
    Each row of the data frame represents an image file and its metadata, including
    file content, label name, image size, and file path. In the last step, we save
    this data frame as a Delta Lake table—`gold_table_training_dataset`. You can also
    see this Delta Lake table’s data schema at the end of the following code listing.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在管道的开始步骤中，我们将图像从文件夹 `flower_photos` 加载到 Spark 中作为二进制文件。其次，我们定义提取函数以从每个图像文件中获取元数据，如标签名称、文件大小和图像大小。第三，我们使用提取函数构建数据处理管道，然后将图像文件传递给管道，管道将生成一个数据框。数据框的每一行代表一个图像文件及其元数据，包括文件内容、标签名称、图像大小和文件路径。在最后一步中，我们将此数据框保存为
    Delta Lake 表—`gold_table_training_dataset`。您还可以在以下代码清单的末尾查看此 Delta Lake 表的数据模式。
- en: Listing 2.13 An ETL to create an image dataset in Delta Lake
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 2.13 Delta Lake 中创建图像数据集的 ETL
- en: '[PRE22]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ Reads images as binaryFile
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将图像作为 binaryFile 读取
- en: ❷ Extracts labels from the image’s subdirectory name
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 从图像的子目录名称提取标签
- en: ❸ Extracts image dimensions
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 提取图像尺寸
- en: ❹ Data schema of the Delta Lake table—gold_table_training_dataset
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ Delta Lake 表—gold_table_training_dataset 的数据模式
- en: Note The raw data used in the demo is the flowers dataset from the TensorFlow
    team. It contains flower photos stored under five subdirectories, one per class.
    The subdirectory name is the label name for the images it contains.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：演示中使用的原始数据是 TensorFlow 团队的 flowers 数据集。它包含存储在五个子目录下的花朵照片，每个子目录对应一个类别。子目录名称是其中包含的图像的标签名称。
- en: 'Now that we have an image dataset built in a Delta Lake table, we can start
    to train a PyTorch model by using this dataset with the help of Petastorm. In
    code listing 2.14, we first read the Delta Lake table `gold_table_training_dataset`
    produced by the ETL pipeline defined in code listing 2.13 and then split the data
    into two data frames: one for training and one for validation. Next, we load these
    two data frames to two Petastorm spark converters; the data will be converted
    to Parquet files inside the converter. At the end, we use the Petastorm API `make_torch_dataloader`
    to read training examples in PyTorch for model training. See the following code
    for the entire three-step process. You can also find the full sample code at:
    [http://mng.bz/wy4B](http://mng.bz/wy4B).'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们在 Delta Lake 表中构建了一个图像数据集，我们可以开始使用 Petastorm 的帮助来训练一个 PyTorch 模型。在代码清单 2.14
    中，我们首先读取由代码清单 2.13 中定义的 ETL 管道生成的 Delta Lake 表 `gold_table_training_dataset`，然后将数据分成两个数据框架：一个用于训练，一个用于验证。接下来，我们将这两个数据框加载到两个
    Petastorm Spark 转换器中；数据将在转换器内转换为 Parquet 文件。最后，我们使用 Petastorm API `make_torch_dataloader`
    从 PyTorch 中读取训练示例以进行模型训练。请参阅以下代码以了解整个三步过程。您还可以在以下链接找到完整的示例代码：[http://mng.bz/wy4B](http://mng.bz/wy4B)。
- en: Listing 2.14 Consuming a Delta Lake image dataset in PyTorch with Petastorm
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 2.14 使用 Petastorm 在 PyTorch 中消耗 Delta Lake 图像数据集
- en: '[PRE23]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '❶ Splits Delta Lake table data into two data frames: training and validation'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将 Delta Lake 表数据分成两个数据框：训练和验证
- en: ❷ Creates the PyTorch data loader from the Petastorm converter for training
    and evaluation
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用 Petastorm 转换器创建 PyTorch 数据加载器进行训练和评估
- en: ❸ Consumes the training data in the training iterations
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在训练迭代中使用训练数据
- en: When to use Delta Lake
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 何时使用 Delta Lake
- en: The common misconception about Delta Lake is that it can only handle structured
    text data, such as sales records and user profiles. But the previous example shows
    it can also deal with unstructured data like images and audio files; you can write
    the file content as a bytes column into a table with other file properties and
    build datasets from them.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 Delta Lake 的一个普遍误解是，它只能处理结构化文本数据，如销售记录和用户配置文件。但前面的示例表明它也可以处理像图像和音频文件这样的非结构化数据；您可以将文件内容作为字节列写入带有其他文件属性的表中，并从中构建数据集。
- en: Delta Lake is a great choice for doing dataset management if you already use
    Apache Spark to build your data pipeline; it supports both structured and unstructured
    data. It’s also cost-effective because Delta Lake keeps data in cloud object storage
    (e.g., Amazon S3, Azure Blob), and Delta Lake’s data schema enforcement and live
    data updated table support mechanism simplify your ETL pipeline development and
    maintenance. Last but not least, the time travel function keeps track of all the
    table updates automatically, so you can feel safe to make data changes and roll
    back to previous versions of the training dataset.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: The limitations of Delta Lake
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: 'The biggest risks of using Delta Lake are lock-in technology and its steep
    learning curve. Delta Lake stores tables in its own mechanism: a combination of
    Parquet-based storage, a transaction log, and indexes, which means it can only
    be written/read by a Delta cluster. You need to use Delta ACID API for data ingestion
    and Delta JDBC to run queries; thus, the data migration cost would be high if
    you decide to move away from Delta Lake in the future. Also, because Delta Lake
    goes with Spark, there is a lot of learning ahead of you if you are new to Spark.'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: Regarding data ingestion performance, Delta Lake stores data to the underlying
    cloud object store, and it’s difficult to achieve low-latency streaming (millisecond
    scale) when using object store operations, such as table creation and saving.
    In addition, Delta Lake needs to update indexes for each ACID transaction; it
    also introduces latency compared with some ETLs performing append-only data writes.
    But in our opinion, data ingestion latency at the second level is not a problem
    for deep learning projects. If you are unfamiliar with Spark and don’t want the
    heavy lifting of setting up Spark and Delta Lake clusters, we have another lightweight
    approach for you—Pachyderm.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.2 Pachyderm with cloud object storage
  id: totrans-360
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we want to propose a lightweight, Kubernetes-based tool—Pachyderm—to
    handle dataset management. We will show you two examples of how to use Pachyderm
    to accomplish image data processing and labeling. But before that, let’s look
    at what Pachyderm is.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: Pachyderm
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: Pachyderm is a tool for building version-controlled, automated, end-to-end data
    pipelines for data science. It runs on Kubernetes and is backed by an object store
    of your choice (e.g., Amazon S3). You can write your own Docker images for data
    scraping, ingestion, cleaning, munging, and wrangling and use the Pachyderm pipeline
    to chain them together. Once you define your pipelines, Pachyderm will handle
    the pipeline scheduling, executing, and scaling.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: Pachyderm offers dataset version control and provenance (data lineage) management.
    It sees every data update (create, write, delete, etc.) as a commit, and it also
    tracks the data source that generates the data update. So you not only can see
    the change history of a dataset, but you can also roll back the dataset to a past
    version and find the data provenance of the change. Figure 2.18 gives a high-level
    view of how Pachyderm works.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: Pachyderm 提供了数据集版本控制和来源追溯（数据血统）管理。它将每个数据更新（创建、写入、删除等）视为一个提交，并且还跟踪生成数据更新的数据源。因此，您不仅可以查看数据集的变更历史，还可以将数据集回滚到过去的版本，并查找更改的数据来源。图
    2.18 展示了 Pachyderm 的工作原理的高级视图。
- en: '![](../Images/02-18.png)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-18.png)'
- en: Figure 2.18 The Pachyderm platform runs with two kinds of objects—a pipeline
    and versioned data. The pipeline is the computational component, and the data
    is the version-control primitive. A data change in the “raw dataset” can trigger
    a pipeline job to process the new data and save the result to the “mature dataset.”
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.18 Pachyderm 平台由两种对象组成——管道和版本控制数据。管道是计算组件，数据是版本控制基元。在“原始数据集”中的数据更改可能会触发管道作业，以处理新数据并将结果保存到“成熟数据集”中。
- en: In Pachyderm, data is version-controlled with a Git style. Each dataset is a
    repository (repo) in Pachyderm, which is the highest-level data object. A repo
    contains commits, files, and branches. Pachyderm only keeps metadata (such as
    audit history and branch) internally and stores the actual files in cloud object
    storage.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Pachyderm 中，数据以 Git 风格进行版本控制。每个数据集在 Pachyderm 中都是一个仓库（repo），它是最高级别的数据对象。一个仓库包含提交、文件和分支。Pachyderm
    仅在内部保留元数据（例如审计历史和分支），并将实际文件存储在云对象存储中。
- en: The Pachyderm pipeline performs various data transformations. The pipelines
    execute a user-defined piece of code—for example, a docker container—to perform
    an operation and process the data. Each of these executions is called a job. Listing
    2.15 shows a simple pipeline definition. This “edges” pipeline watches an “images”
    dataset. When there is a new image added to the images dataset, the pipeline will
    launch a job to run the `"pachyderm/opencv"` docker image to parse the image and
    save its edge picture into the edges dataset.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: Pachyderm 管道执行各种数据转换。管道执行用户定义的代码片段，例如一个 Docker 容器，以执行操作并处理数据。每个执行都称为一个作业。清单
    2.15 显示了一个简单的管道定义。这个“edges”管道监视一个“images”数据集。当在图像数据集中添加了新的图像时，管道将启动一个作业，运行 `"pachyderm/opencv"`
    Docker 镜像解析图像，并将其边缘图片保存到 edges 数据集中。
- en: Listing 2.15 A Pachyderm pipeline definition
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 2.15 Pachyderm 管道定义
- en: '[PRE24]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ❶ A Pachyderm pipeline
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 一个 Pachyderm 管道
- en: ❷ A Pachyderm dataset
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 一个 Pachyderm 数据集
- en: Version and data provenance
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 版本和数据来源
- en: 'In Pachyderm, any changes applied to both the dataset and pipeline are versioned
    automatically, and you can use the Pachyderm command tool `pachctl` to connect
    to the Pachyderm workspace to check file history and even roll back those changes.
    See the following example for using the `pachctl` command to check the edges dataset’s
    change history and the change provenance. First, we run the `pachctl` `list` command
    to list all the commits in the edges dataset. In our example, there are three
    changes (commits) applied to the edges dataset:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Pachyderm 中，对数据集和管道所做的任何更改都会自动进行版本管理，您可以使用 Pachyderm 命令工具 `pachctl` 连接到 Pachyderm
    工作区，查看文件历史记录，甚至还可以回滚这些更改。查看以下示例，了解如何使用 `pachctl` 命令来查看 edges 数据集的变更历史和变更来源。首先，我们运行
    `pachctl` 的 `list` 命令来列出 edges 数据集的所有提交。在我们的示例中，对 edges 数据集进行了三次变更（提交）：
- en: '[PRE25]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: To get the provenance of a data change, we can use pachctl inspect command to
    check on the commit. For example, we can use the following command to check the
    data origin of commit.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取数据更改的来源，我们可以使用 `pachctl inspect` 命令来检查提交情况。例如，我们可以使用以下命令来检查提交的数据来源。
- en: '[PRE26]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'From the following response, we can see the commit `eb58294a976347abaf06e35fe3b0da5b`
    of the edges dataset is computed from the images dataset’s `66f4ff89a017412090dc4a542d9b1142`
    commit:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 从以下回应中，我们可以看到 edges 数据集的提交 `eb58294a976347abaf06e35fe3b0da5b` 是由 images 数据集的提交
    `66f4ff89a017412090dc4a542d9b1142` 计算得出的：
- en: '[PRE27]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: ❶ Data provenance
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 数据来源
- en: The data provenance feature is great for reproducibility and troubleshooting
    datasets, as you can always find the exact data that was used in the past, along
    with the data process code that created it.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 数据来源功能非常适用于数据集的可重现性和故障排除，因为您始终可以找到过去使用的确切数据以及创建它的数据处理代码。
- en: 'Example: Using Pachyderm for labeling and training an image dataset'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：使用 Pachyderm 对图像数据集进行标注和训练
- en: Having seen how Pachyderm works, let’s see a design proposal for using Pachyderm
    to build an automated object detection training pipeline. For object detection
    model training, we first need to prepare the training dataset by labeling the
    target object with a bounding box on each image and then send the dataset—the
    bounding box label file and images—to the training code to start the model training.
    Figure 2.19 shows the process of using Pachyderm to automate this workflow.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 看完了Pachyderm是如何工作的，现在让我们看一个设计提案，使用Pachyderm来构建一个自动化目标检测训练管道。对于目标检测模型训练，我们首先需要通过在每个图像上用一个边界框标记目标对象来准备训练数据集，然后将数据集——边界框标签文件和图像——发送给训练代码开始模型训练。图2.19展示了使用Pachyderm自动化这一工作流程的过程。
- en: '![](../Images/02-19.png)'
  id: totrans-384
  prefs: []
  type: TYPE_IMG
  zh: '![图2.19](../Images/02-19.png)'
- en: Figure 2.19 Automated object detection model training in Pachyderm. The training
    process starts automatically when new images are labeled.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.19在Pachyderm中自动化的目标检测模型训练。当标记了新图像时，训练过程会自动开始。
- en: In this design, we use two pipelines, the labeling pipeline and the training
    pipeline, and two datasets to build this training workflow. In step 1, we upload
    image files to the “raw image dataset.” In step 2, we kick off the labeling pipeline
    to launch a labeling application that opens up a UI for the user to label objects
    by drawing bounding boxes on the images; these images are read from the raw image
    dataset. Once the user finishes the labeling work, the image and the generated
    label data will be saved to the “labeled dataset.” In step 3, we add new training
    data to the labeled dataset, which will trigger the training pipeline to launch
    the training container and start the model training. In step 4, we save the model
    file.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 在此设计中，我们使用了两个流水线，标记流水线和训练流水线，以及两个数据集来构建这个训练工作流程。在第1步，我们将图像文件上传到“原始图像数据集”。在第2步中，我们启动标记流水线，启动一个标记应用程序，为用户打开一个UI界面，通过在图像上绘制边界框来标记对象；这些图像是从原始图像数据集中读取的。一旦用户完成了标记工作，图像和生成的标签数据将被保存到“标记数据集”。在第3步中，我们向已标记的数据集添加新的训练数据，这将触发训练流水线启动训练容器并开始模型训练。在第4步中，我们保存模型文件。
- en: Besides the automation, data including the raw image dataset, the labeled dataset,
    and the model files are all versioned by Pachyderm automatically. Also, by leveraging
    the data provenance feature, we can tell with any given model file which version
    of the labeled dataset is used in its training and from which version of the raw
    image dataset this training data is made.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 除了自动化之外，包括原始图像数据集、标记数据集和模型文件在内的所有数据都会被Pachyderm自动进行版本控制。此外，通过利用数据溯源功能，我们可以确定任何给定模型文件使用的标记数据集的版本，以及用于训练此训练数据的原始图像数据集的版本。
- en: When to use Pachyderm
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 什么时候使用Pachyderm
- en: Pachyderm is a lightweight approach to help you build data engineering pipelines
    easily and offers data versioning support in Git style. It is data scientist–centric
    and easy to use. Pachyderm is Kubernetes based and uses cloud object storage as
    a data store, so it’s cost-effective, simple to set up, and easy to maintain for
    small teams. We would suggest using Pachyderm, and not using Spark, for any data
    science teams that own their infrastructure. Pachyderm works really well with
    unstructured data, like image and audio files.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: Pachyderm是一个轻量级的方法，帮助您轻松构建数据工程流水线，并提供类似Git的数据版本支持。它以数据科学家为中心，易于使用。Pachyderm基于Kubernetes，并使用云对象存储作为数据存储，因此对于小团队来说成本效益高，设置简单，易于维护。我们建议数据科学团队拥有自己基础设施的情况下使用Pachyderm，而不要使用Spark。Pachyderm在处理非结构化数据（如图像和音频文件）方面表现非常出色。
- en: Limitations with Pachyderm
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: Pachyderm的局限性
- en: What is missing in Pachyderm are schema protection and data analysis efficiency.
    Pachyderm sees everything as files; it keeps snapshots for each file version but
    doesn’t care about the file content. There is no data type validation on data
    writing or reading; it completely depends on the pipeline to protect the data
    consistency.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: Pachyderm缺少的是模式保护和数据分析效率。Pachyderm将所有东西都视为文件；它为每个文件版本保留快照，但不关心文件内容。在数据写入或读取时没有数据类型验证；它完全依赖于管道来保护数据一致性。
- en: Lack of schema awareness and protection introduces a lot of risk for any continuous-running
    deep learning training pipeline because any code changes in the upstream data
    processing code might break the downstream data processing or training code. Also,
    without knowing the schema of the data, dataset comparison is hard to implement.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 缺乏模式意识和保护为任何持续运行的深度学习训练流水线引入了很多风险，因为在上游数据处理代码中做任何代码更改可能会破坏下游数据处理或训练代码。此外，没有了解数据的模式，实现数据集比较就变得很困难。
- en: Summary
  id: totrans-393
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: The primary goal of dataset management is to continuously receive fresh data
    from a variety of data sources and deliver datasets to model training while supporting
    training reproducibility (data version tracking).
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集管理的主要目标是持续从各种数据源接收新鲜数据，并在支持训练可重现性（数据版本跟踪）的同时，向模型训练交付数据集。
- en: Having a dataset management component can expedite deep learning project development
    by parallelizing model algorithm development and data engineering development.
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拥有数据集管理组件可以通过将模型算法开发和数据工程开发并行化来加速深度学习项目的开发。
- en: 'The principles to validate the design of a dataset management service are as
    follows: supporting dataset reproducibility; employing strongly typed data schema;
    designing unified API and keeping API behavior consistent across different dataset
    types and sizes; and guaranteeing data persistence.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计数据集管理服务的原则如下：支持数据集可重现性；采用强类型数据模式；设计统一的API，并保持API在不同数据集类型和大小上的一致行为；保证数据持久性。
- en: A dataset management system should at least support (training) dataset versioning,
    which is crucial for model reproducibility and performance troubleshooting.
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集管理系统至少应支持（训练）数据集版本控制，这对模型的可重现性和性能故障排除至关重要。
- en: A dataset is a logic file group for a deep learning task; it’s static from the
    model training perspective and dynamic from the data collection perspective.
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集是深度学习任务的逻辑文件组；从模型训练的角度来看是静态的，从数据收集的角度来看是动态的。
- en: The sample dataset management service is made of three layers—the data ingestion
    layer, internal dataset storage layer, and training dataset fetching layer.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例数据集管理服务由三层组成——数据摄入层、内部数据集存储层和训练数据集获取层。
- en: We define two data schemas for each dataset type in the sample dataset management
    service, one for data ingestion and one for dataset fetching. Each data update
    is stored as a commit, and each training dataset is stored as a versioned snapshot.
    Users can employ a version hash string to fetch the related training data at any
    time (dataset reproducibility).
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在示例数据集管理服务中，我们为每种数据集类型定义了两种数据模式，一种用于数据摄入，一种用于数据集获取。每次数据更新都被存储为一个提交，而每个训练数据集都被存储为一个带版本的快照。用户可以使用版本哈希字符串随时获取相关的训练数据（数据集可重现性）。
- en: The sample dataset management service supports a special dataset type—a `GENERIC`
    dataset. A `GENERIC` dataset has no schema and no data validation, and users can
    upload and download data freely, so it’s good for prototyping new algorithms.
    Once the training code and dataset requirements become mature, the dataset format
    can be promoted to a strongly typed dataset.
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例数据集管理服务支持一种特殊的数据集类型——`GENERIC`数据集。`GENERIC`数据集没有模式和数据验证，并且用户可以自由上传和下载数据，因此非常适合原型化新算法。一旦训练代码和数据集要求变得成熟，数据集格式就可以升级为强类型数据集。
- en: Delta Lake and Petastorm can work together to set up a dataset management service
    for Spark-based, deep learning projects.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Delta Lake和Petastorm可以共同用于为基于Spark的深度学习项目建立数据集管理服务。
- en: Pachyderm is a lightweight, Kubernetes-based data platform that offers data
    versioning support in Git style and allows easy pipeline setup. A pipeline is
    made by docker containers; it can be used to automate data process workflow and
    training workflow for a deep learning project.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pachyderm是一个基于Kubernetes的轻量级数据平台，支持类似Git的数据版本控制，并允许轻松设置流水线。流水线由Docker容器组成；它可以用于自动化数据处理工作流程和深度学习项目的训练工作流程。
