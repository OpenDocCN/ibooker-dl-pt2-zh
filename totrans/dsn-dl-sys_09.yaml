- en: 9 Workflow orchestration
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9 工作流编排
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Defining workflow and workflow orchestration
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义工作流和工作流编排
- en: Why deep learning systems need to support workflows
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么深度学习系统需要支持工作流
- en: Designing a general workflow orchestration system
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计一个通用工作流编排系统
- en: 'Introducing three open source orchestration systems: Airflow, Argo Workflows,
    and Metaflow'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入三个开源编排系统：Airflow、Argo Workflows 和 Metaflow
- en: 'In this chapter, we will discuss the last but critical piece of a deep learning
    system: workflow orchestration—a service that manages, executes, and monitors
    workflow automation. Workflow is an abstract and broad concept; it is essentially
    a sequence of operations that are part of some larger task. If you can devise
    a plan with a set of tasks to complete a work, this plan is a workflow. For example,
    we can define a sequential workflow for training a machine learning (ML) model.
    This workflow can be composed of the following tasks: fetching raw data, rebuilding
    the training dataset, training the model, evaluating the model, and deploying
    the model.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论深度学习系统的最后但至关重要的部分：工作流编排——一个管理、执行和监控工作流自动化的服务。工作流是一个抽象且广泛的概念；它本质上是一系列操作，这些操作是某个更大任务的一部分。如果你可以设计一个带有一组任务的计划来完成一项工作，这个计划就是一个工作流。例如，我们可以为训练机器学习（ML）模型定义一个顺序工作流。这个工作流可以由以下任务组成：获取原始数据、重建训练数据集、训练模型、评估模型和部署模型。
- en: Because a workflow is an execution plan, it can be performed manually. For instance,
    a data scientist can manually complete the tasks of the model training workflow
    we just described. For example, to complete the “fetching raw data” task, the
    data scientist can craft web requests and send them to the dataset management
    (DM) service to fetch a dataset—all with no help from the engineers.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 因为工作流是一个执行计划，它可以手动执行。例如，数据科学家可以手动完成我们刚刚描述的模型训练工作流的任务。例如，要完成“获取原始数据”任务，数据科学家可以制作网络请求并将其发送到数据集管理（DM）服务以获取数据集——所有这些都不需要工程师的帮助。
- en: However, executing a workflow manually is not ideal. We want to automate the
    workflow execution. When there are numerous workflows developed for different
    purposes, we need a dedicated system to handle the complexity of workflow executions.
    We call this kind of system a *workflow orchestration system*.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，手动执行工作流并不理想。我们希望自动化工作流的执行。当针对不同目的开发了大量工作流时，我们需要一个专门的系统来处理工作流执行的复杂性。我们称这种系统为*工作流编排系统*。
- en: A workflow orchestration system is built to manage workflow life cycles, including
    workflow creation, execution, and troubleshooting. It provides not only the pulse
    to keep all the scheduled code running but also a control plane for data scientists
    to manage all the automation in a deep learning system.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流编排系统被建立来管理工作流的生命周期，包括工作流的创建、执行和故障排除。它不仅提供了使所有预定代码保持运行的脉搏，还为数据科学家提供了一个控制平面，用于管理深度学习系统中的所有自动化。
- en: In this chapter, we will discuss workflow orchestration system design and the
    most popular open source orchestration systems used in the deep learning field.
    By reading this chapter, you will not only gain a solid understanding of the system
    requirements and the design options, but you will also learn how to choose the
    right open source orchestration system that works best for your own situation.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论工作流编排系统的设计以及在深度学习领域中使用的最受欢迎的开源编排系统。通过阅读本章，您不仅将对系统要求和设计选项有扎实的理解，还将了解如何选择最适合您自己情况的正确开源编排系统。
- en: 9.1 Introducing workflow orchestration
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 引入工作流编排
- en: Before we dive into the details of designing workflow orchestration systems,
    let’s have a quick discussion on the basic concept of workflow orchestration,
    especially about the special workflow challenges from a deep learning/ML perspective.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨工作流编排系统设计的细节之前，让我们就工作流编排的基本概念进行快速讨论，特别是关于从深度学习/ML 角度出发的特殊工作流挑战。
- en: Note Because the requirements of using workflow orchestration for deep learning
    projects and ML projects are almost identical, we will use the word *deep learning*
    and *machine learning* interchangeably in this chapter.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 由于在深度学习项目和 ML 项目中使用工作流编排的要求几乎相同，因此在本章中我们将深度学习和机器学习这两个词用于交替使用。
- en: 9.1.1 What is workflow?
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.1 什么是工作流？
- en: In general, a workflow is a sequence of operations that are part of some larger
    task. A workflow can be viewed as a directed acyclic graph (DAG) of steps.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，工作流是一系列操作，这些操作是某个较大任务的一部分。工作流可以被视为一种步骤的有向无环图（DAG）。
- en: A step is the smallest resumable unit of computation that describes an action;
    this task could be fetching data or triggering a service, for example. A step
    either succeeds or fails as a whole. In this chapter, we use the word *task* and
    *step* interchangeably.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤是描述一个动作的最小可恢复计算单元；这个任务可以是获取数据或触发服务等。一个步骤要么成功，要么失败。在本章中，我们将 *任务* 和 *步骤* 这两个词互换使用。
- en: A DAG specifies the dependencies between steps and the order in which to execute
    them. Figure 9.1 shows a sample workflow for training natural language processing
    (NLP) models.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: DAG 指定了步骤之间的依赖关系和执行它们的顺序。图 9.1 显示了一个用于训练自然语言处理（NLP）模型的示例工作流。
- en: '![](../Images/09-01.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09-01.png)'
- en: Figure 9.1 A DAG of a sample model training workflow with multiple steps. Both
    ovals and diamonds are steps, but different types. The solid arrows indicate the
    dependencies between steps, and the dotted-line arrows represent the external
    web requests sent from steps.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 展示了一个具有多个步骤的示例模型训练工作流的 DAG。椭圆形和菱形都是步骤，但是不同类型。实线箭头表示步骤之间的依赖关系，虚线箭头表示从步骤发送的外部网络请求。
- en: From the sample DAG in figure 9.1, we see a workflow that consists of many steps.
    Every step depends on another, and the solid arrows show the dependencies between
    steps. These arrows and steps form a workflow DAG with no loops.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 从图 9.1 中的示例 DAG 中，我们看到了一个由许多步骤组成的工作流。每个步骤都依赖于另一个，实线箭头显示了步骤之间的依赖关系。这些箭头和步骤形成了一个没有循环的工作流
    DAG。
- en: If you follow the arrows in the DAG (from left to right) and complete the tasks,
    you can train and release an NLP model to production. For example, when an incoming
    request triggers the workflow, the auth (authorization) step will be executed
    first, and then the dataset-building step and the embedding fetching step will
    both be executed simultaneously. The steps on the other side of the arrows will
    be executed after these two steps have been completed.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你按照 DAG 中的箭头（从左到右）并完成任务，你可以训练并发布一个 NLP 模型到生产环境。例如，当一个传入的请求触发了工作流时，授权（authorization）步骤将首先被执行，然后数据集构建步骤和嵌入获取步骤将同时被执行。在这两个步骤完成之后，箭头的另一侧的步骤将被执行。
- en: Workflows are used everywhere in the IT industry. As long as you can define
    a process as a DAG of single tasks/steps, this process can be considered a workflow.
    Workflows are critical to deep learning model development. In fact, in production
    environments, most of the deep learning model–building activities are presented
    and executed as workflows.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流在 IT 行业中随处可见。只要你能够将一个流程定义为单个任务/步骤的 DAG，这个流程就可以被视为工作流。工作流对于深度学习模型的开发至关重要。事实上，在生产环境中，大多数深度学习模型构建活动都被呈现并执行为工作流。
- en: Note A workflow should not have a loop. To guarantee a workflow can be completed
    under any condition, its execution graph needs to be a DAG, which prevents the
    workflow execution from falling into a dead loop.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：工作流不应该有循环。为了保证工作流在任何情况下都能完成，其执行图必须是一个 DAG，这样可以防止工作流执行陷入死循环。
- en: 9.1.2 What is workflow orchestration?
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.2 什么是工作流编排？
- en: Once we define a workflow, the next step is to run the workflow. Running a workflow
    means executing the workflow steps based on the sequence defined in the workflow’s
    DAG. *Workflow orchestration* is the term we use to describe the execution and
    monitoring of the workflow.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们定义了一个工作流，下一步就是运行工作流。运行工作流意味着根据工作流 DAG 中定义的顺序执行工作流步骤。*工作流编排* 是我们用来描述工作流的执行和监视的术语。
- en: The goal of workflow orchestration is to automate the execution of tasks defined
    in workflows. In practice, the concept of workflow orchestration often extends
    to workflow management as a whole—that is, creating, scheduling, executing, and
    monitoring multiple workflows in an automated manner.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流编排的目标是自动化执行工作流中定义的任务。在实践中，工作流编排的概念通常扩展到整个工作流管理——即以自动化方式创建、调度、执行和监视多个工作流。
- en: 'Why do deep learning systems need workflow orchestration? Ideally, we should
    be able to code an entire deep learning project as one piece. And that’s exactly
    what we do in the prototyping phase of a project, putting all the code in a Jupyter
    notebook. So, why do we need to transform the prototyping code into a workflow
    and run it in a workflow orchestration system? The answer is twofold: automation
    and work sharing. To understand these reasons, let’s look at three sample training
    workflows in figure 9.2.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习系统为什么需要工作流编排？理想情况下，我们应该能够将整个深度学习项目编码为一个整体。这正是我们在项目的原型阶段所做的，将所有代码放在一个Jupyter笔记本中。那么，为什么我们需要将原型代码转换为工作流，并在工作流编排系统中运行它呢？答案有两个方面：自动化和工作共享。为了理解这些原因，让我们看一下图9.2中的三个样本训练工作流。
- en: '![](../Images/09-02.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09-02.png)'
- en: Figure 9.2 Deep learning workflows are composed of many reusable tasks.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2 深度学习工作流由许多可重复使用的任务组成。
- en: A great benefit of using a workflow is that it turns a large chunk of code into
    a group of sharable and reusable components. In figure 9.2, we imagined three
    data scientists working on three model training projects (A, B, and C). Because
    each project’s training logic is different, data scientists developed three different
    workflows (A, B, and C) to automate their model training processes. Although each
    workflow has different DAGs, the steps in each DAG are highly overlapped. The
    total six steps are sharable and reusable. For example, the auth step (step 1)
    is the first step for all three workflows.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 使用工作流的一个巨大好处是它将大量代码转换为一组可共享和可重用的组件。在图9.2中，我们想象三名数据科学家正在进行三个模型训练项目（A、B和C）的工作。由于每个项目的训练逻辑不同，数据科学家开发了三个不同的工作流（A、B和C）来自动化他们的模型训练流程。尽管每个工作流具有不同的DAGs，但每个DAG中的步骤高度重叠。总共的六个步骤是可共享和可重用的。例如，auth步骤（步骤1）是所有三个工作流的第一步。
- en: Having reusable steps can greatly improve data scientists’ productivity. For
    example, to pull data from a DM service (step 2 in figure 9.2), data scientists
    need to learn how the DM web API works. But if someone already built a DM data
    pull method as a step function, scientists can just reuse this step in their workflow
    without learning how to interact with the DM service. If everyone writes their
    project in the form of a workflow, we will have lots of reusable steps, which
    will save lots of duplicate effort at an organizational level!
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 具有可重复使用步骤可以极大地提高数据科学家的生产率。例如，要从DM服务中提取数据（图9.2中的第2步），数据科学家需要了解DM Web API的工作原理。但是如果有人已经将DM数据提取方法构建为一个步骤函数，科学家们就可以在他们的工作流中重复使用这个步骤，而不必学习如何与DM服务交互。如果每个人都以工作流的形式编写他们的项目，我们将拥有大量可重用的步骤，这将在组织级别节省大量重复的工作！
- en: Another reason that a workflow is well adapted to deep learning development
    is that it facilitates collaboration. Model development requires teamwork; a dedicated
    team might work on data while another team works on the training algorithm. By
    defining a complex model-building process in the workflow, we can dispatch a big
    complex project in pieces (or steps) and assign them to different teams while
    still keeping the project organized and the components in proper order. The workflow
    DAG shows the task dependencies clearly for all the project participants to see.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个适应深度学习开发的工作流的原因是它促进了协作。模型开发需要团队合作；一个专门的团队可能负责数据，而另一个团队负责训练算法。通过在工作流中定义复杂的模型构建过程，我们可以将一个大型复杂项目分解为片段（或步骤）并将其分配给不同的团队，同时仍然保持项目有序和组件正确顺序。工作流DAG清楚地显示了所有项目参与者可以看到的任务依赖关系。
- en: In short, a good workflow orchestration system encourages work sharing, facilitates
    team collaboration, and automates complicated development scenarios. All these
    merits make workflow orchestration a crucial component of deep learning project
    development.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，一个好的工作流编排系统鼓励工作共享，促进团队协作，并自动化复杂的开发场景。所有这些优点使工作流编排成为深度学习项目开发的关键组成部分。
- en: 9.1.3 The challenges for using workflow orchestration in deep learning
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.3 深度学习中使用工作流编排的挑战
- en: 'In the previous section, we saw how a workflow system can provide a lot of
    benefits to deep learning project development. But there is one caveat: using
    workflows to prototype deep learning algorithm ideas is cumbersome.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们看到工作流系统可以为深度学习项目开发提供许多好处。但有一个注意事项：使用工作流来原型化深度学习算法的想法是很麻烦的。
- en: To understand how and why it is cumbersome, let’s look at a deep learning development
    process diagram (figure 9.3). This diagram should set the foundation for you to
    understand the challenges that workflow presents in the deep learning context.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解为什么这样做很麻烦，让我们看一下深度学习开发过程的图表（图9.3）。这张图表应该为你理解工作流在深度学习背景下提出的挑战奠定基础。
- en: '![](../Images/09-03.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图9.3](../Images/09-03.png)'
- en: Figure 9.3 A data scientist’s view of deep learning project development
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 深度学习项目开发的数据科学家视角
- en: 'In figure 9.3, we see a typical deep learning project development process from
    a data scientist’s perspective. The process can be divided into two phases: the
    local incubation phase and the production phase.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在图9.3中，我们从数据科学家的角度看到了一个典型的深度学习项目开发过程。该过程可以分为两个阶段：本地孵化阶段和生产阶段。
- en: 'In the local incubation phase, data scientists work on data exploration and
    model training prototyping at their local/dev environment. When the prototyping
    is done and the project looks promising, data scientists start to work on production
    onboarding: moving the prototyping code to the production system.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地孵化阶段，数据科学家在本地/开发环境中进行数据探索和模型训练原型。当原型完成并且项目看起来很有前景时，数据科学家开始进行生产上线：将原型代码移到生产系统。
- en: In the production phase, data scientists convert the prototyping code to a workflow.
    They break the code down into multiple steps and define a workflow DAG and then
    submit the workflow to the workflow orchestration system. After that, the orchestration
    system takes over and runs the workflow based on its schedule.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产阶段，数据科学家将原型代码转换为工作流程。他们将代码分解为多个步骤，并定义一个工作流DAG，然后将工作流提交给工作流编排系统。之后，编排系统接管并根据其时间表运行工作流程。
- en: Gaps between prototyping and production
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在原型和生产之间存在差距。
- en: 'If you ask an engineer who works on workflow orchestration systems how they
    feel about the development process in figure 9.3, the answer most likely is: It’s
    pretty good! But in practice, this process is problematic for data scientists.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你问一个在工作流编排系统上工作的工程师他们对图9.3中的开发过程的感觉，答案很可能是：还不错！但实际上，这个过程对数据科学家来说是有问题的。
- en: 'From the data scientists’ point of view, once an algorithm is tested locally,
    its prototyping code should be shipped to production right away. But in figure
    9.3, we see the prototyping phase and production phase are *not* smoothly connected.
    Shipping incubation code to production is not straightforward; data scientists
    have to do extra work to construct a workflow to run their code in production.
    The gap between prototyping code and production workflow jeopardizes development
    velocity for two reasons:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据科学家的角度来看，一旦算法在本地测试通过，其原型代码应立即投入生产。但是在图9.3中，我们看到原型阶段和生产阶段 *不是* 顺利连接的。将孵化代码部署到生产并不直接；数据科学家必须额外工作来构建一个工作流程来在生产中运行他们的代码。原型代码和生产工作流之间的差距影响了开发速度，原因有两个：
- en: '*Workflow building and debugging aren’t straightforward*—Data scientists normally
    face a huge learning curve when authoring model training workflows in orchestration
    systems. Learning the workflow DAG syntax, workflow libraries, coding paradigms,
    and troubleshooting is a huge burden to data scientists. The workflow troubleshooting
    is the most painful part. The majority of the orchestration system doesn’t support
    local execution, which means data scientists have to test their workflow in the
    remote orchestration system. This is hard because both the workflow environment
    and workflow execution logs are remote, so data scientists cannot easily figure
    out the root cause when a workflow execution goes wrong.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*工作流程的构建和调试并不是直接的* —— 数据科学家在编写模型训练工作流程时，通常会面临巨大的学习曲线。学习工作流DAG语法、工作流程库、编码范例和故障排除对于数据科学家来说是一个巨大的负担。工作流程的故障排除是最痛苦的部分。大多数编排系统不支持本地执行，这意味着数据科学家必须在远程编排系统中测试他们的工作流程。这很困难，因为工作流环境和工作流执行日志都是远程的，所以数据科学家在工作流程执行出错时无法轻易找出根本原因。'
- en: '*Workflow construction happens not once but frequently*—The common misperception
    is that because workflow construction only happens once, it’s fine if it is time-consuming
    and cumbersome. But the fact is, workflow construction happens continuously because
    deep learning development is an iterative process. As figure 9.3 shows, data scientists
    work on prototyping and production experimentation iteratively, so the workflow
    needs to be updated frequently to test new improvements from local to production.
    Therefore, the unpleasant and time-consuming workflow construction happens repeatedly,
    which hinders the development velocity.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*工作流构建并非一次性事件，而是频繁发生*——一种常见的误解是，由于工作流构建只发生一次，所以如果耗时且繁琐也没关系。但事实是，工作流构建是持续不断的，因为深度学习开发是一个迭代过程。正如图9.3所示，数据科学家会迭代地进行原型设计和生产实验，因此工作流需要经常更新，以测试从本地到生产环境的新改进。因此，令人不快且耗时的工作流构建会反复发生，这阻碍了开发速度。'
- en: Smoothing the transition from prototyping to production
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 平滑地从原型设计过渡到生产环境
- en: Although there are gaps, the process in figure 9.3 is good. Data scientists
    start prototyping locally with a straightforward script, and then they keep working
    on it. If the results after each iteration seem promising enough, the “straightforward
    local script” is converted to a workflow and runs in the orchestration system
    in production.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在差异，图9.3中的流程是不错的。数据科学家从一个简单的脚本开始在本地进行原型设计，然后继续完善。如果每次迭代后的结果看起来足够令人满意，那么“简单的本地脚本”将被转换为工作流，并在生产环境中在编排系统中运行。
- en: The key improvement is to make the transition step from prototyping code to
    a production workflow seamless. If an orchestration system is designed for deep
    learning use cases, it should provide tools to help data scientists build workflows
    from their code with minimum effort. For example, Metaflow, an open source library
    that will be discussed in section 9.3.3, allows data scientists to authorize workflow
    by writing Python code with Python annotations. Data scientists can obtain a workflow
    from their prototyping code directly without making any changes. Metaflow also
    provides a unified user experience on model execution between local and cloud
    production environments. This eliminates the friction in workflow testing because
    Metaflow operates workflows the same way in both local and production environments.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 关键的改进是使从原型代码到生产工作流的过渡步骤变得无缝。如果一个编排系统是为深度学习用例设计的，它应该提供工具，帮助数据科学家用最少的工作量从他们的代码构建工作流。例如，Metaflow是一个开源库，将在9.3.3节中讨论，它允许数据科学家通过编写带有Python注解的Python代码来授权工作流。数据科学家可以直接从他们的原型代码中获得工作流，而不需要进行任何更改。Metaflow还在本地和云生产环境之间提供了统一的模型执行用户体验。这消除了工作流测试中的摩擦，因为Metaflow在本地和生产环境中以相同的方式运行工作流。
- en: A deep learning system should be humancentric
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习系统应以人为中心
- en: When we introduce a general-purpose tool—like workflow orchestration—to deep
    learning systems, don’t be satisfied with only enabling the functionality. Try
    to reduce human time in the system. Customization work is always possible to help
    our users be more productive.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们向深度学习系统引入通用工具——如工作流编排时，不要满足于仅仅启用功能。尽量减少系统中人工的时间。总是可以进行定制工作，以帮助我们的用户更加高效。
- en: Metaflow (section 9.3.3) is a good example of what happens when engineers aren't
    satisfied with just building an orchestration system to automate deep learning
    workflows. Instead, they went a step further to optimize the workflow construction
    and management to address the way data scientists work.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Metaflow（9.3.3节）是一个很好的例子，说明当工程师们不满足于仅仅构建一个用于自动化深度学习工作流的编排系统时会发生什么。相反，他们更进一步优化了工作流构建和管理，以解决数据科学家的工作方式。
- en: 9.2 Designing a workflow orchestration system
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2 设计工作流编排系统
- en: In this section, we will approach the design of workflow orchestration systems
    in three steps. First, we use a typical data scientist user scenario to show how
    an orchestration system works from a user perspective. Second, we learn a generic
    orchestration system design. Third, we summarize the key design principles for
    building or evaluating an orchestration system. By reading this section, you will
    understand how orchestration systems work, in general, so you can be confident
    evaluating or working on any orchestration system.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将分三个步骤设计工作流编排系统。首先，我们使用一个典型的数据科学家用户场景，展示编排系统从用户角度的工作方式。第二，我们学习通用编排系统设计。第三，我们总结构建或评估编排系统的关键设计原则。通过阅读本节，您将了解编排系统的一般工作方式，从而可以自信地评估或操作任何编排系统。
- en: 9.2.1 User scenarios
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.1 用户场景
- en: 'Although the process of workflows varies a lot from one scenario to another,
    the user scenarios for data scientists are quite standard. Most workflow usage
    can be divided into two phases: the development phase and the execution phase.
    See figure 9.4 for a data scientist’s (Vena’s) workflow user experience. Let’s
    follow through with Vena’s user scenario in figure 9.4 step by step.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管各工作流场景的过程有很大的差异，但数据科学家的用户场景通常非常标准。大多数工作流使用可以分为两个阶段：开发阶段和执行阶段。请参见图9.4，了解数据科学家（Vena）的工作流用户体验。我们将一步一步地跟随图9.4中
    Vena 的用户场景。
- en: '![](../Images/09-04.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09-04.png)'
- en: Figure 9.4 A general deep learning user scenario of a workflow orchestration
    system
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4：工作流编排系统的通用深度学习用户场景。
- en: Development phase
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 开发阶段
- en: 'During the development phase, data scientists convert their training code into
    a workflow. See Vena’s example as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发阶段，数据科学家将其训练代码转换为工作流。以下是 Vena 的示例：
- en: Vena, a data scientist, prototypes her model training algorithm in a Jupyter
    notebook or pure Python in her local environment. After local testing and evaluation,
    Vena thinks it’s time to deploy the code to production for online experiments
    with real customer data.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据科学家 Vena 在本地环境中使用 Jupyter notebook 或纯 Python 原型开发其模型训练算法。经过本地测试和评估，Vena 认为是时候将代码部署到生产环境中，进行真正的客户数据在线实验了。
- en: Because everything running in production is a workflow, Vena needs to convert
    her prototype code to a workflow. So Vena uses the syntax provided by the orchestration
    system to rebuild her work into a DAG of tasks in a YAML (a text configuration)
    file. For example, data parsing -> data augmentation -> dataset building -> training
    -> [online evaluation, offline evaluation] -> model release.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于生产环境中的所有内容都是工作流，因此 Vena 需要将其原型代码转换为工作流程。所以，Vena 使用编排系统提供的语法，在 YAML（文本配置）文件中将其工作重建为一个任务的
    DAG。例如，数据解析->数据增强->数据集构建->训练->[在线评估、离线评估]->模型发布。
- en: Vena then sets the input/output parameters and actions for each step in the
    DAG. Using the training step as an example, Vena sets the step action as a RESTful
    HTTP request. This step will send a RESTful request to the model training service
    to start a training job. The payload and parameters of this request come from
    the step input parameters.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，Vena 为 DAG 中每个步骤设置输入/输出参数和动作。以训练步骤为例，Vena 将步骤动作设置为 RESTful HTTP 请求。此步将向模型训练服务发送一个
    RESTful 请求来启动训练作业。该请求的有效载荷和参数来自步骤输入参数。
- en: Once the workflow is defined, Vena sets the workflow’s execution schedule in
    the DAG YAML file. For example, Vena can schedule the workflow to run on the first
    day of every month, and she also sets the workflow to be triggered by an external
    event.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦定义好工作流，Vena 就在 DAG YAML 文件中设置工作流的执行计划。例如，Vena 可以将工作流安排在每个月的第一天运行，还可以将工作流设置为由外部事件触发。
- en: Vena runs the workflow local validation and submits the workflow to the orchestration
    service.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Vena 运行工作流本地验证，并将工作流提交给编排服务。
- en: 'To give you an idea of what a workflow means in reality, the following code
    shows a pseudo workflow for Vena (in section 9.3, we will discuss the actual workflow
    systems):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让您了解工作流在现实中的含义，以下代码显示了 Vena 的伪工作流（在第9.3节，我们将讨论实际的工作流系统）：
- en: '[PRE0]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ DAG definition; defines the body of the workflow, including steps and dependencies
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ DAG定义；定义了工作流的主体，包括步骤和依赖项
- en: ❷ Executes a bash command for data augmentation
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 执行数据增强的 bash 命令
- en: ❸ A sequential execution flow
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 顺序执行流
- en: Execution phase
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 执行阶段
- en: 'In the execution phase, the orchestration service executes the model training
    workflow, as illustrated by Vena’s example:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行阶段，编排服务执行模型训练工作流，如 Vena 的示例所示：
- en: Once Vena’s workflow is submitted, the orchestration service saves the workflow
    DAG into a database.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦 Vena 的工作流被提交，编排服务就会将工作流 DAG 保存到数据库中。
- en: The orchestration service’s scheduler component detects Vena’s workflow and
    dispatches the tasks of the workflow to backend workers. The scheduler will make
    sure the tasks are executed in the sequence that is defined in the workflow DAG.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编排服务的调度器组件会检测 Vena 的工作流，并将工作流的任务分派给后端工作者。调度器将确保任务按照工作流 DAG 中定义的顺序执行。
- en: Vena uses the orchestration service’s web UI to check the workflow’s execution
    progress and results in real time.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Vena 使用编排服务的 Web UI 实时检查工作流的执行进度和结果。
- en: If the workflow produces a good model, Vena can promote it to the staging and
    production environments. If not, Vena starts another iteration of prototyping.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果工作流生成了一个好的模型，Vena 可以将其推广到分期和生产环境。如果不是，Vena 就会开始另一个原型的迭代。
- en: A critical indicator of whether an orchestration system is a good fit for deep
    learning is how easy it is to convert the prototyping code into a workflow. In
    figure 9.4, we see that Vena needs to transform her training code into a workflow
    every time she prototypes a new idea. We can imagine how much human time it would
    save if we eased the friction of converting the deep learning code to a workflow.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 判断一个编排系统是否适合深度学习的一个关键指标是将原型代码转换为工作流的难易程度。在图 9.4 中，我们可以看到，每次 Vena 原型化一个新的想法时，她都需要将其训练代码转换为工作流。我们可以想象如果我们减少将深度学习代码转换为工作流的摩擦会节省多少人力时间。
- en: Note A workflow should always be lightweight. The workflow is used to automate
    a process; its goal is to group and connect a series of tasks and execute them
    in a defined sequence. The great benefit of using a workflow is that people can
    share and reuse the tasks, so they can automate their process faster. Therefore,
    the workflow itself shouldn’t do any heavy computation, the real work should be
    done by the tasks of the workflow.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 一个工作流应该总是轻量级的。工作流用于自动化一个过程；其目标是将一系列任务分组并连接起来，并按照定义的顺序执行它们。使用工作流的巨大好处是人们可以共享和重复使用这些任务，因此他们可以更快地自动化他们的流程。因此，工作流本身不应进行任何繁重的计算，真正的工作应由工作流的任务完成。
- en: 9.2.2 A general orchestration system design
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.2 一个通用的编排系统设计
- en: Let’s now turn to a generic workflow orchestration system. To help you understand
    how an orchestration system works and how to research open source orchestration
    systems, we prepared a high-level system design. By zooming out of the detailed
    implementation and only keeping the core components, this design is applicable
    to most orchestration systems, including open source systems, which will be discussed
    in section 9.3\. See figure 9.5 for the design proposal.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们转向一个通用的工作流编排系统。为了帮助您了解编排系统的工作原理以及如何研究开源编排系统，我们准备了一个高级系统设计。通过放大详细的实现并仅保留核心组件，这个设计适用于大多数编排系统，包括将在第
    9.3 节讨论的开源系统。请参见图 9.5 以获取设计方案。
- en: '![](../Images/09-05.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09-05.png)'
- en: Figure 9.5 A design overview for a generic workflow orchestration service
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5 通用工作流编排服务的设计概览
- en: 'A workflow orchestration system generally consists of the following five components:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 一个工作流编排系统通常由以下五个组件组成：
- en: '*Web server*—The web server presents a web user interface and a set of web
    APIs for users to create, inspect, trigger, and debug the behavior of a workflow.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Web 服务器*—Web 服务器提供了一个 Web 用户界面和一组 Web API，供用户创建、检查、触发和调试工作流的行为。'
- en: '*Scheduler and controller*—The scheduler and controller component does two
    things. First, the scheduler watches every active workflow in the system, and
    it schedules the workflow to run when the time is right. Second, the controller
    dispatches the workflow tasks to workers. Although the scheduler and controller
    are two different function units, they usually are implemented together because
    they are all related to workflow execution.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*调度器和控制器*—调度器和控制器组件有两个功能。首先，调度器监视系统中的每个活动工作流，并在合适的时间安排工作流运行。其次，控制器将工作流任务分派给工作者。尽管调度器和控制器是两个不同的功能单元，但它们通常一起实现，因为它们都与工作流执行相关。'
- en: '*Metadata database*—The metadata database stores the workflows’ configuration,
    DAG, editing and execution history, and the tasks’ execution state.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*元数据数据库* — 元数据数据库存储工作流的配置、DAG、编辑和执行历史，以及任务的执行状态。'
- en: '*Worker group*—The worker group provides the compute resource to run workflow
    tasks. The worker abstracts the infrastructure and is agnostic to the task that’s
    running. For example, we might have different types of workers, such as a Kubernetes
    worker and an Amazon Elastic Compute Cloud (EC2) worker, but they can all execute
    the same task, albeit on different infrastructures.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*工作组* — 工作组提供计算资源来运行工作流任务。工作器抽象了基础架构，并且对正在运行的任务不可知。例如，我们可能有不同类型的工作器，例如Kubernetes工作器和Amazon
    Elastic Compute Cloud（EC2）工作器，但它们都可以执行相同的任务，尽管在不同的基础架构上。'
- en: '*Object store*—The object store is shared file storage for all other components;
    it’s normally built on top of cloud object storage, such as Amazon Simple Storage
    Service (S3). One usage of an object store is task output sharing. When a worker
    runs a task, it reads the output value of the previous task from the object store
    as the task input; the worker also saves the task output to the object store for
    its successor tasks.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*对象存储* — 对象存储是所有其他组件的共享文件存储；通常建立在云对象存储之上，例如Amazon Simple Storage Service（S3）。对象存储的一个用途是任务输出共享。当工作器运行任务时，它从对象存储中读取上一个任务的输出值作为任务输入；工作器还将任务输出保存到对象存储中，供后续任务使用。'
- en: Both the object store and the metadata database are accessible to all the components
    of the orchestration system, including the scheduler, web server, and workers’
    components. Having centralized data storage decouples the core components, so
    the web server, scheduler, and workers can work independently.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对象存储和元数据数据库都可以由编排系统的所有组件访问，包括调度程序、Web服务器和工作器组件。具有集中式数据存储可以解耦核心组件，因此Web服务器、调度程序和工作器可以独立工作。
- en: How is a workflow executed?
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流程是如何执行的？
- en: First, Vena defines the DAG for the workflow. Inside the DAG, Vena declares
    a set of tasks and defines the control flow of the task execution sequence. For
    each task, Vena either uses the system’s default operator, such as a Shell command
    operator or Python operator, or builds her own operator to execute tasks.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，Vena为工作流定义了DAG。在DAG内部，Vena声明了一组任务，并定义了任务执行顺序的控制流。对于每个任务，Vena要么使用系统的默认运算符，例如Shell命令运算符或Python运算符，要么构建自己的运算符来执行任务。
- en: Second, Vena submits the workflow—DAG with dependent code—to the web server
    through the web UI or command line. The workflow is saved in the metadata database.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，Vena通过Web UI或命令行将工作流程（具有依赖代码的DAG）提交给Web服务器。工作流程保存在元数据数据库中。
- en: Third, the scheduler periodically (every few seconds or minutes) scans the metadata
    database and detects the new workflow; it then kicks off the workflow at the scheduled
    time. To execute a workflow, the scheduler calls the controller component to dispatch
    the workflow’s tasks to the worker queue based on the task sequence defined in
    DAG.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，调度程序定期（每隔几秒或几分钟）扫描元数据数据库并检测新的工作流程；然后在预定时间启动工作流程。为了执行工作流程，调度程序调用控制器组件根据DAG中定义的任务顺序将工作流程的任务分派到工作器队列中。
- en: Fourth, a worker picks up a task from the shared job queue; it reads the task
    definition from the metadata database and executes the task by running the task’s
    operator. During the execution, the worker saves the task’s output value to the
    object store and reports the task’s execution status back to the metadata database.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 第四，工作人员从共享作业队列中挑选一个任务；它从元数据数据库中读取任务定义，并通过运行任务的运算符执行任务。在执行过程中，工作人员将任务的输出值保存到对象存储中，并将任务的执行状态报告回元数据数据库。
- en: Last but not least, Vena uses the web UI hosted on the web server component
    to monitor the workflow execution. Because both the scheduler/controller components
    and the workers report the status to the metadata database in real time, the web
    UI always displays the latest workflow status.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，Vena使用托管在Web服务器组件上的Web UI来监视工作流程的执行。因为调度程序/控制器组件和工作器实时向元数据数据库报告状态，所以Web
    UI始终显示最新的工作流程状态。
- en: 9.2.3 Workflow orchestration design principles
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.3 工作流程编排设计原则
- en: Because we have seen how a workflow orchestration system works internally and
    externally, now it’s time to examine the design principles that make an orchestration
    system outstanding for deep learning scenarios. We hope you can use the principles
    here as a guide to evolving your system or for evaluating open source approaches.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们已经看到了工作流编排系统在内部和外部的工作方式，现在是时候研究使编排系统在深度学习场景中出色的设计原则了。我们希望您可以将这些原则作为指导，来改进您的系统或评估开源方法。
- en: Note The workflow orchestration system is one of the most complicated components
    in a deep learning system in terms of engineering effort, so don't worry too much
    about making your system match perfectly with these principles in the first few
    versions.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 在深度学习系统中，工作流编排系统是最复杂的组件之一，涉及到大量的工程工作。所以，在最初的几个版本中，不必过于担心使您的系统与这些原则完全匹配。
- en: 'Principle 1: Criticality'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 原则 1：重要性
- en: Workflow orchestration is essentially a job scheduling challenge, so the bottom
    line for any orchestration system is to provide a solid workflow execution experience.
    A valid workflow should always be executed correctly, repeatedly, and on schedule.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流编排本质上是一种作业调度的挑战，所以任何编排系统的底线都是提供一个可靠的工作流执行体验。一个有效的工作流应该总是能够正确、重复地按计划执行。
- en: 'Principle 2: Usability'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 原则 2：可用性
- en: The usability measurement of an orchestration system in a deep learning context
    is whether it optimizes data scientists’ productivity. Most data scientist interactions
    in an orchestration system are workflow creation, testing, and monitoring. So
    a user-friendly orchestration system should let users create, monitor, and troubleshoot
    workflows easily.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习环境中，编排系统的可用性衡量标准是是否优化了数据科学家的工作效率。在一个编排系统中，数据科学家的大部分交互工作都是工作流的创建、测试和监控。因此，一个用户友好的编排系统应该让用户能够轻松地创建、监控和排除故障。
- en: 'Principle 3: Extensibility'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 原则 3：可扩展性
- en: To cater to the wide variety of deep learning infrastructures, people should
    easily define their own task operators and executors without worrying about where
    they are deployed to. The orchestration system should provide the level of abstraction
    that suits your environment, whether if it’s Amazon EC2 or Kubernetes.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了适应各种深度学习基础设施，人们应该能够轻松定义自己的任务操作符和执行器，而不用担心它们部署在哪里。编排系统应该提供适合您环境的抽象级别，无论是 Amazon
    EC2 还是 Kubernetes。
- en: 'Principle 4: Isolation'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 原则 4：隔离性
- en: 'Two types of isolations can occur that are critical: workflow creation isolation
    and workflow execution isolation. Workflow creation isolation means people can
    not interfere with each other when creating workflows. For example, if Vena submits
    an invalid workflow DAG or releases a new version of a common shared library that’s
    referenced in other workflows, the existing workflows shouldn’t be affected.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 可能发生两种关键的隔离：工作流创建隔离和工作流执行隔离。工作流创建隔离意味着在创建工作流时，人们不能相互干扰。例如，如果 Vena 提交了一个无效的工作流有向无环图（DAG），或者发布了一个在其他工作流中被引用的共享库的新版本，那么现有的工作流不应受到影响。
- en: Workflow execution isolation means that each workflow is running in an isolated
    environment. There should be no resource competition between workflows, and failure
    of a workflow won’t affect other workflow’s executions.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流执行隔离意味着每个工作流在一个独立的环境中运行。工作流之间不应有资源竞争，并且一个工作流的失败不会影响其他工作流的执行。
- en: 'Principle 5: Scaling'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 原则 5：扩展性
- en: 'A good orchestration system should address the following two scaling problems:
    handling large numbers of concurrent workflows and handling large expansive workflows.
    Concurrent workflow scaling generally means that given enough compute resources—for
    example, adding more workers to the worker group—the orchestration system can
    cater to an infinite concurrent number of workflow executions. Also, the system
    should always keep the service-level agreement (SLA) for every workflow. For example,
    a workflow should be executed at its scheduled time and no later than 2 seconds,
    regardless of how many other workflows are executing.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的编排系统应该解决以下两个扩展性问题：处理大量同时运行的工作流以及处理大型扩展性工作流。同时运行的工作流扩展性通常指，给定足够的计算资源 —— 例如，向工作组中添加更多的工作节点
    —— 编排系统可以满足无限数量的并发工作流执行。此外，系统应始终为每个工作流保持服务级别协议（SLA）。例如，工作流应在其预定时间执行，且不得晚于2秒，无论有多少其他工作流正在执行。
- en: For single, large workflow scaling, the system should encourage users not to
    worry about performance, so they can focus on readable, straightforward code,
    and easy operations. When the workflow execution hits a limit—for example, the
    training operators take too long to execute—the orchestration system should provide
    some horizontal parallelism operators, such as distributed training operators,
    to address single workflow performance problems.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单一的大型工作流扩展，系统应该鼓励用户不必担心性能，这样他们就可以专注于可读性强、直接明了的代码和简单的操作。当工作流执行达到限制时——例如，训练运算符执行时间过长——编排系统应该提供一些水平并行运算符，例如分布式训练运算符，以解决单个工作流性能问题。
- en: The main scaling idea for deep learning orchestration is that we should solve
    the performance problem at the system level and avoid asking users to write code
    with scalability in mind. This can lead to worse readability, harder debuggability,
    and increased operational burden.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习编排的主要扩展思想是我们应该在系统级别解决性能问题，并避免要求用户考虑可扩展性编写代码。这可能导致可读性下降、调试困难和操作负担增加。
- en: 'Principle 6: Human-centric support for both prototyping and production'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 原则 6：人本支持，既适用于原型设计，也适用于生产环境
- en: The capability of connecting the data scientist’s local prototyping code to
    the production workflow is a requirement specific to deep learning. It’s a key
    indicator that we use to evaluate whether an orchestration system is a good fit
    for deep learning systems.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 连接数据科学家本地原型代码到生产工作流的能力是深度学习特有的要求。这是我们用来评估编排系统是否适合深度学习系统的关键指标。
- en: An orchestration system designed for deep learning will respect that deep learning
    project development is an iterative, ongoing effort from prototyping to production.
    Therefore, it will make a dedicated effort to help data scientists convert their
    local prototype code to production workflow in a seamless fashion.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 一个为深度学习设计的编排系统将尊重深度学习项目开发是从原型到生产的迭代持续努力的事实。因此，它将不遗余力地帮助数据科学家将他们的本地原型代码转换为生产工作流。
- en: 9.3 Touring open source workflow orchestration systems
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3 巡回开源工作流编排系统
- en: 'In this section, we will introduce three battle-tested workflow orchestration
    systems: Airflow, Argo Workflows, and Metaflow. These three open source systems
    are widely adopted in the IT industry and backed by active communities. In addition
    to introducing them generally, we also evaluate these workflow systems from the
    perspective of deep learning project development.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍三种经过实战验证的工作流编排系统：Airflow、Argo Workflows 和 Metaflow。这三个开源系统在IT行业得到了广泛应用，并得到了活跃社区的支持。除了一般介绍外，我们还将从深度学习项目开发的角度评估这些工作流系统。
- en: To make a fair comparison, we implement pseudocode for the same workflow in
    Airflow, Argo Workflows, and Metaflow. Basically, if there is new data, we initially
    transform the data and save it to a new table in the database, and then we notify
    the data science team. Also, we expect the workflow to run daily.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行公正的比较，我们在Airflow、Argo Workflows 和 Metaflow 中为相同工作流实现伪代码。基本上，如果有新数据，我们首先转换数据并将其保存到数据库的新表中，然后通知数据科学团队。此外，我们希望工作流每天运行。
- en: 9.3.1 Airflow
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.1 Airflow
- en: Airflow ([https://airflow.apache.org/docs/apache-airflow/stable/index.html](https://airflow.apache.org/docs/apache-airflow/stable/index.html))
    was created in 2014 at Airbnb and is now a part of the Apache Foundation. Airflow
    is a platform to programmatically author, schedule, and monitor workflows. Airflow
    is not designed for deep learning use cases; it was originally built to orchestrate
    the increasingly complex ETL (extract, transform, load) pipelines (or data pipelines).
    But because of Airflow’s good extensibility, production quality, and GUI support,
    it’s widely used in many other domains, including deep learning. As this book
    is written, Airflow is the most adopted orchestration system.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Airflow（[https://airflow.apache.org/docs/apache-airflow/stable/index.html](https://airflow.apache.org/docs/apache-airflow/stable/index.html)）于2014年在Airbnb创建，现在是Apache基金会的一部分。Airflow是一个平台，用于以编程方式编写、调度和监视工作流。Airflow并不是为深度学习用例设计的；它最初是为了编排越来越复杂的ETL（抽取、转换、加载）管道（或数据管道）而构建的。但由于Airflow具有良好的可扩展性、生产质量和GUI支持，它被广泛应用于许多其他领域，包括深度学习。截至本书撰写时，Airflow是最受欢迎的编排系统。
- en: A typical use case
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 典型用例
- en: Building a workflow in Airflow takes two steps. First, define the workflow DAG
    and tasks. Second, declare the task dependencies in the DAG. An Airflow DAG is
    essentially Python code. See the following listing for how our sample workflow
    is implemented in Airflow.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Airflow 中构建工作流程需要两个步骤。首先，定义工作流 DAG 和任务。其次，在 DAG 中声明任务之间的依赖关系。Airflow DAG 本质上是
    Python 代码。看以下清单，了解我们的示例工作流在 Airflow 中是如何实现的。
- en: Listing 9.1 A sample Airflow workflow definition
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 9.1 一个示例 Airflow 工作流定义
- en: '[PRE1]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Checks whether a new file arrives
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 检查是否有新文件到达
- en: ❷ The actual logic is implemented in the "transform_data" function.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 实际逻辑是在 "transform_data" 函数中实现的。
- en: ❸ The PostgresOperator is a predefined airflow operator for interacting with
    postgres db.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ PostgresOperator 是预定义的 Airflow 运算符，用于与 postgres db 交互。
- en: In code listing 9.1, we see the sample workflow DAG consists of multiple tasks,
    such as `create_table` and `save_into_db`. A task in Airflow is implemented as
    an operator. There are lots of predefined and community-managed operators, such
    as MySqlOperator, SimpleHttpOperator, and Docker operator.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码清单 9.1 中，我们看到示例工作流 DAG 包含多个任务，如 `create_table` 和 `save_into_db`。在 Airflow
    中，任务被实现为运算符。有许多预定义和社区管理的运算符，例如 MySqlOperator、SimpleHttpOperator 和 Docker 运算符。
- en: 'Airflow’s predefined operators help users implement tasks without coding. You
    can also use the PythonOperator to run your customized Python functions. Once
    the workflow DAG is constructed and all the code is deployed to Airflow, we can
    use the UI or the following CLI command to check workflow execution status; see
    some sample shell commands as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Airflow 的预定义运算符帮助用户实现任务而无需编码。您还可以使用 PythonOperator 运行自定义的 Python 函数。一旦工作流 DAG
    被构建并且所有代码被部署到 Airflow，我们可以使用 UI 或以下 CLI 命令来检查工作流执行状态；以下是一些示例 shell 命令：
- en: '[PRE2]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Prints all active DAG
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 打印所有活动的 DAG
- en: ❷ Prints the list of tasks in the "data_process_dag" DAG
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 打印 "data_process_dag" DAG 中任务的列表
- en: ❸ Prints the hierarchy of tasks in the "data_process_dag" DAG
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 打印 "data_process_dag" DAG 中任务的层次结构
- en: If you want to learn more about Airflow, you can check out its architecture
    overview doc and tutorials ([http://mng.bz/Blpw](http://mng.bz/Blpw)).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想了解更多关于 Airflow 的信息，您可以查看其架构概述文档和教程 ([http://mng.bz/Blpw](http://mng.bz/Blpw))。
- en: Key Features
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 关键功能
- en: 'Airflow offers the following key features:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Airflow 提供以下关键功能：
- en: '*DAGs*—Airflow abstracts complex workflow using DAGs, and the workflow DAG
    is implemented through a Python library.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*DAGs* — Airflow 通过 **DAGs** 抽象复杂的工作流程，工作流 DAG 是通过 Python 库实现的。'
- en: '*Programmatic workflow management*—Airflow supports creating tasks on the fly
    and allows the creation of complex dynamic workflows.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*程序化工作流管理* — Airflow 支持动态创建任务，并允许创建复杂的动态工作流。'
- en: '*Great built-in operators to help build automation*—Airflow offers lots of
    predefined operators that help users achieve tasks without coding.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*出色的内置运算符帮助构建自动化* — Airflow 提供了许多预定义的运算符，帮助用户实现任务而无需编码。'
- en: '*Solid task dependency and execution management*—Airflow has an auto-retry
    policy built into every task, and it provides different types of sensors to handle
    run-time dependencies, such as detecting task completion, workflow run status
    change, and file presence.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可靠的任务依赖性和执行管理* — Airflow 在每个任务中都内置了自动重试策略，并提供了不同类型的传感器来处理运行时依赖关系，例如检测任务完成、工作流运行状态变更和文件存在。'
- en: '*Extensibility*—Airflow makes its sensors, hooks, and operators fully extendable,
    which allows it to benefit from a large amount of community-contributed operators.
    Airflow can also be easily integrated into different systems by adding customized
    operators.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可扩展性* — Airflow 使其传感器、钩子和运算符完全可扩展，这使得它能够从大量社区贡献的运算符中受益。Airflow 还可以通过添加定制运算符轻松集成到不同的系统中。'
- en: '*Monitoring and management interface*—Airflow provides a powerful UI so users
    can get a quick overview of workflow/task execution status and history. Users
    can also trigger and clear tasks or workflow runs from the UI.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*监控和管理界面* — Airflow 提供了一个强大的用户界面，用户可以快速了解工作流/任务执行状态和历史。用户还可以从界面触发和清除任务或工作流运行。'
- en: '*Production quality*—Airflow provides many useful tools for maintaining the
    service in production environments, such as task log searching, scaling, alerting,
    and restful APIs.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*生产质量* — Airflow 提供了许多有用的工具，用于在生产环境中维护服务，如任务日志搜索、扩展、报警和 restful API。'
- en: Limitations
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 限制
- en: 'Although Airflow is a great workflow orchestration, we still see several disadvantages
    when using it for deep learning scenarios:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Airflow 是一个出色的工作流编排工具，但在深度学习场景中使用时仍然存在一些缺点：
- en: '*High upfront cost for data scientists to onboard*—Airflow has a steep learning
    curve to achieve tasks that are not supported by the built-in operators. Also,
    there is no easy way to do workflow local testing.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据科学家入门时的高前期成本* — Airflow 对于实现不受内置运算符支持的任务具有陡峭的学习曲线。此外，没有一种简单的方法进行工作流本地测试。'
- en: '*High friction when moving deep learning prototyping code to production*—When
    we apply Airflow to deep learning, data scientists have to convert their local
    model training code into Airflow DAG. This is extra work, and it’s an unpleasant
    experience for data scientists, especially when considering that this is avoidable
    if we build workflow DAG from the model training code directly.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*将深度学习原型代码移至生产环境时的高摩擦力* — 当我们将 Airflow 应用于深度学习时，数据科学家必须将他们的本地模型训练代码转换为 Airflow
    DAG。这是额外的工作，对于数据科学家来说是一种不愉快的体验，特别是考虑到如果我们直接从模型训练代码构建工作流程 DAG，这是可以避免的。'
- en: '*High complexity when operating on Kubernetes* —Deploying and operating Airflow
    on Kubernetes is not straightforward. If you are looking to adopt an orchestration
    system to run on Kubernetes, Argo Workflows is a better choice.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在 Kubernetes 上操作时的高复杂性* — 在 Kubernetes 上部署和操作 Airflow 并不简单。如果您希望采用一个编排系统在
    Kubernetes 上运行，Argo Workflows 是一个更好的选择。'
- en: 9.3.2 Argo Workflows
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.2 Argo Workflows
- en: Argo Workflows is an open source, container-native workflow engine for orchestrating
    parallel workflows/tasks on Kubernetes. Argo Workflows solves the same problem
    that Airflow addresses but in a different way; it takes a Kubernetes-native approach.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Argo Workflows 是一个开源的、容器原生的工作流引擎，用于在 Kubernetes 上编排并行工作流程/任务。Argo Workflows
    解决了与 Airflow 相同的问题，但采用了不同的方式；它采用了 Kubernetes 本地方法。
- en: The biggest difference between Argo Workflows and Airflow is that Argo Workflows
    is built natively on Kubernetes. More specifically, the workflows and tasks in
    Argo Workflows are implemented as Kubernetes custom resource definition (CRD)
    objects, and each task (step) is executed as a Kubernetes pod. See figure 9.6
    for a high-level system overview.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Argo Workflows 和 Airflow 之间最大的区别在于 Argo Workflows 在 Kubernetes 上本地构建。更具体地说，Argo
    Workflows 中的工作流程和任务以 Kubernetes 自定义资源定义（CRD）对象实现，并且每个任务（步骤）都作为 Kubernetes pod
    执行。请参阅图 9.6 以获得高级系统概述。
- en: '![](../Images/09-06.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09-06.png)'
- en: Figure 9.6 The workflow and its steps in Argo Workflows are executed as Kubernetes
    pods.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6 Argo Workflows 中的工作流程及其步骤是作为 Kubernetes pod 执行的。
- en: In figure 9.6, Vena (the data scientist) first defines a workflow and its steps/tasks
    as a Kubernetes CRD object, which is usually presented as a YAML file. Then she
    submits the workflow to Argo Workflows, and its controller creates CRD objects
    inside the Kubernetes cluster. Next, Kubernetes pods are launched dynamically
    to run the workflow steps/tasks in the workflow sequence.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 9.6 中，Vena（数据科学家）首先将工作流程及其步骤/任务定义为 Kubernetes CRD 对象，通常表示为 YAML 文件。然后她将工作流提交到
    Argo Workflows，其控制器在 Kubernetes 集群内创建 CRD 对象。接下来，Kubernetes pod 动态启动以按工作流程顺序运行工作流程步骤/任务。
- en: You may also notice that each step’s execution is completely isolated by container
    and pod; each step uses files to present its input and output values. Argo Workflows
    will magically mount the dependent file into the step’s container.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以注意到，每个步骤的执行完全由容器和 pod 隔离；每个步骤使用文件来表示其输入和输出值。Argo Workflows 会自动将依赖文件挂载到步骤的容器中。
- en: The task isolation created by the Kubernetes pod is a great advantage of Argo
    Workflows. Simplicity is also another reason people choose Argo Workflows. If
    you understand Kubernetes, Argo’s installation and troubleshooting are straightforward.
    We can use either Argo Workflows commands or the standard Kubernetes CLI commands
    to debug the system.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes pod 创建的任务隔离是 Argo Workflows 的一个巨大优势。简单性也是人们选择 Argo Workflows 的另一个原因。如果您了解
    Kubernetes，Argo 的安装和故障排除都很简单。我们可以使用 Argo Workflows 命令或标准的 Kubernetes CLI 命令来调试系统。
- en: A typical use case
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的使用案例
- en: For a better understanding, let’s look at an Argo Workflows example. In this
    section, we use Argo Workflows to automate the same data processing work we saw
    in the previous Airflow section. The workflow includes checking new data first,
    transforming the data, saving it to a new table in the database, and then notifying
    the data scientist team by Slack. See the following code listing for the Argo
    Workflows definition.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解，让我们看一个 Argo Workflows 的例子。在本节中，我们使用 Argo Workflows 来自动化我们在之前 Airflow
    部分看到的相同数据处理工作。工作流程包括首先检查新数据，转换数据，将其保存到数据库中的新表中，然后通过 Slack 通知数据科学家团队。请参阅以下代码清单以查看
    Argo Workflows 的定义。
- en: Listing 9.2 A sample workflow for Argo Workflows with a series of steps
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 代码清单9.2 Argo Workflows的示例工作流程，包含一系列步骤
- en: '[PRE3]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Claims the CRD object type as workflow
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将CRD对象类型声明为工作流程
- en: ❷ Declares the steps of the workflow
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 声明工作流程的步骤
- en: ❸ The step body is defined as another template, similar to a function.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 步骤主体被定义为另一个模板，类似于函数。
- en: ❹ Declares the data-paths artifact is from the new-data-paths artifact generated
    by the check-new-data step
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 声明数据路径工件来自由check-new-data步骤生成的新数据路径工件
- en: ❺ This is how steps pass parameters.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 这就是步骤传递参数的方式。
- en: ❻ The actual step definition, similar to a function implementation
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 实际步骤定义，类似于函数实现
- en: ❼ Declares an output artifact (generates new-data-paths) for this step; the
    artifact is from /tmp/data_paths.txt, which can also be a directory.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 声明此步骤的输出工件（生成新的数据路径）；工件来自/tmp/data_paths.txt，该工件也可以是一个目录。
- en: ❽ Unpacks the data_paths input artifact and puts it at /tmp/raw_data/data_paths.txt
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 解压缩数据_paths输入工件，并将其放置在/tmp/raw_data/data_paths.txt
- en: The most fundamental concepts in Argo Workflows are the workflow and template.
    A workflow object represents a single instance of a workflow; it contains the
    workflow’s definition and execution state. We should treat a workflow as a “live”
    object. A template can be thought of as *functions*; they define instructions
    to be executed. The `entrypoint` field defines what the main function will be,
    meaning the template that will be executed first.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Argo Workflows中最基本的概念是工作流程和模板。工作流程对象代表工作流程的单个实例；它包含工作流程的定义和执行状态。我们应该将工作流程视为一个“活动”对象。模板可以被认为是*函数*；它们定义要执行的指令。`entrypoint`字段定义了主函数是什么，意味着将首先执行的模板。
- en: 'In code listing 9.2, we see a four-step sequential workflow: `check-new-data`
    -> `transform_data` -> `save-into-db` -> `notify-data-science-team`. Each step
    can reference a template, and steps pass parameters via artifacts (files). For
    example, the `check-new-data` references the `data-checker` template, which defines
    the Docker image for checking whether there is new data. The `data-checker` template
    also declares that the step output—the newly arrived data file path—will be saved
    to `/tmp/data_paths.txt` as its output value.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码清单9.2中，我们看到了一个四步顺序工作流程：`check-new-data` -> `transform_data` -> `save-into-db`
    -> `notify-data-science-team`。每个步骤都可以引用一个模板，并且步骤通过工件（文件）传递参数。例如，`check-new-data`引用了`data-checker`模板，该模板定义了用于检查是否有新数据的Docker镜像。`data-checker`模板还声明了步骤输出——新到达的数据文件路径——将被保存到`/tmp/data_paths.txt`作为其输出值。
- en: 'Next, the step `transform_data` binds the output of the `check-new-data` to
    the input of the data-converter template. This is how variables move around between
    steps and templates. Once you submit the workflow—for example, `argo` `submit`
    `-n` `argo` `sample_workflow.yaml`—you can either use the Argo Workflows UI or
    the following commands to review the details of the workflow run:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，步骤`transform_data`将`check-new-data`的输出绑定到data-converter模板的输入。这就是变量在步骤和模板之间移动的方式。一旦您提交了工作流程——例如，`argo`
    `submit` `-n` `argo` `sample_workflow.yaml`——您可以使用Argo Workflows UI或以下命令来查看工作流运行的详细信息：
- en: '[PRE4]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Besides using the `argo` command, we can also use the Kubernetes CLI command
    to check the workflow execution because Argo Workflows runs natively on Kubernetes;
    see the following example:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用`argo`命令之外，我们还可以使用Kubernetes CLI命令来检查工作流的执行，因为Argo Workflows在Kubernetes上原生运行；请参考以下示例：
- en: '[PRE5]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: To learn more about Argo Workflows, you can check out Argo Workflows user guide
    ([http://mng.bz/WAG0](http://mng.bz/WAG0)) and Argo Workflows architecture graph
    ([https://argoproj.github.io/argo-workflows/architecture](https://argoproj.github.io/argo-workflows/architecture)).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于Argo Workflows的信息，您可以查看Argo Workflows用户指南（[http://mng.bz/WAG0](http://mng.bz/WAG0)）和Argo
    Workflows架构图（[https://argoproj.github.io/argo-workflows/architecture](https://argoproj.github.io/argo-workflows/architecture)）。
- en: 'Code Dockerization: Easy production deployment'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 代码Docker化：轻松进行生产部署
- en: Argo Workflows is essentially a Kubernetes pod (Docker images) scheduling system.
    Although it forces people to write their code into a series of Docker images,
    it creates great flexibility and isolation inside the orchestration system. Because
    the code is in Docker form, it can be executed by any worker without worrying
    about configuring the worker environments.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Argo Workflows本质上是一个Kubernetes Pod（Docker镜像）调度系统。尽管它强迫人们将其代码编写成一系列Docker镜像，但它在编排系统内部创建了极大的灵活性和隔离性。因为代码以Docker形式存在，所以可以由任何工作节点执行，而不用担心配置工作节点环境。
- en: Another advantage to Argo Workflows is its low cost of production deployment.
    When you test your code locally in Docker, the Docker image (prototyping code)
    can be used directly in Argo Workflows. Unlike Airflow, Argo Workflows has almost
    no conversion effort from prototyping code to production workflow.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Argo Workflows 的另一个优点是生产部署成本低。当您在 Docker 中本地测试代码时，Docker 镜像（原型代码）可以直接在 Argo
    Workflows 中使用。与 Airflow 不同，Argo Workflows 几乎不需要从原型代码转换为生产工作流程的工作量。
- en: Key Features
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 关键特性
- en: 'Argo Workflows offers the following key features:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: Argo Workflows 提供以下关键特性：
- en: '*Low cost of installation and maintenance*—Argo Workflows runs natively on
    Kubernetes, so you can just use the Kubernetes process to troubleshoot any problems;
    no need to learn other tools. Also, its installation is very straightforward;
    with a few `kubectl` commands, you can get Argo Workflows running in a Kubernetes
    environment.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*安装和维护成本低*—Argo Workflows 在 Kubernetes 上原生运行，因此您可以只使用 Kubernetes 进程来解决任何问题；无需学习其他工具。此外，它的安装非常简单；只需几个
    `kubectl` 命令，您就可以在 Kubernetes 环境中运行 Argo Workflows。'
- en: '*Robust workflow execution*—The Kubernetes pod creates great isolation for
    Argo Workflows’ task execution. Argo Workflows also supports cron workflow and
    task retry.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*稳健的工作流程执行*—Kubernetes pod 为 Argo Workflows 任务执行提供了良好的隔离。Argo Workflows 还支持
    cron 工作流程和任务重试。'
- en: '*Templating and composability*—Argo Workflows templates are like functions.
    When building a workflow, Argo Workflows supports composing different templates
    (step functions). This composability encourages sharing the common work across
    teams, thus greatly improving productivity.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模板化和可组合性*—Argo Workflows 模板就像函数一样。在构建工作流程时，Argo Workflows 支持组合不同的模板（步骤函数）。这种可组合性鼓励团队之间共享通用工作，从而大大提高了生产率。'
- en: '*Fully featured UI*—Argo Workflows offers a convenient UI to manage the entire
    life cycle of a workflow, such as submitting/stopping a workflow, listing all
    workflows, and viewing workflow definitions.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*完整的 UI 功能*—Argo Workflows 提供了一个方便的 UI 来管理工作流程的整个生命周期，例如提交/停止工作流程、列出所有工作流程和查看工作流程定义。'
- en: '*Highly flexible and applicable*—Argo Workflows defines REST APIs to manage
    the system and add new capabilities (plugins), and workflow tasks are defined
    as Docker images. These features make Argo Workflows highly customizable and used
    widely in many domains, such as ML, ETL, batch/data processing, and CI/CD (continuous
    integration and continuous delivery/continuous deployment).'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*高度灵活和适用*—Argo Workflows 定义了 REST API 来管理系统和添加新功能（插件），并且工作流程任务定义为 Docker 镜像。这些特性使得
    Argo Workflows 在许多领域，如 ML、ETL、批处理/数据处理和 CI/CD（持续集成和持续交付/持续部署）中被广泛使用。'
- en: '*Production quality*—Argo Workflows is designed to run in a serious production
    environment. Kubeflow pipeline and Argo CD are great examples of productionizing
    Argo Workflows.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*生产质量*—Argo Workflows 设计用于在严肃的生产环境中运行。Kubeflow pipeline 和 Argo CD 是将 Argo Workflows
    用于生产环境的绝佳示例。'
- en: Limitations
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 限制
- en: 'The downsides of using Argo Workflows in deep learning systems are as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Argo Workflows 在深度学习系统中的缺点如下：
- en: '*Everyone will write and maintain YAML files*—Argo Workflows demands that the
    workflow is defined as a Kubernetes CRD in a YAML file. A short YAML file for
    a single project is manageable, but once the number of workflows starts increasing
    and workflow logic becomes more complex, the YAML file can become long and confusing.
    Argo Workflows offers templates to keep the workflow definition simple, but it’s
    still not very intuitive unless you are accustomed to working with Kubernetes
    YAML configurations.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*每个人都将编写和维护 YAML 文件*—Argo Workflows 要求工作流程定义为 YAML 文件中的 Kubernetes CRD。一个项目的短小
    YAML 文件可以管理，但一旦工作流程数量增加并且工作流程逻辑变得更加复杂，YAML 文件可能变得冗长和混乱。Argo Workflows 提供了模板以保持工作流程定义简单，但除非您习惯使用
    Kubernetes YAML 配置，否则这仍然不太直观。'
- en: '*Must be an expert on Kubernetes*—You will feel like it’s second nature if
    you are an expert on Kubernetes. But a novice user may need to spend quite some
    time learning Kubernetes concepts and practices.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*必须是 Kubernetes 专家*—如果您是 Kubernetes 专家，您会觉得这是司空见惯的。但是初学者可能需要花费相当多的时间学习 Kubernetes
    的概念和实践。'
- en: '*Task execution latency*—In Argo Workflows, for every new task, Argo will launch
    a new Kubernetes pod to execute it. The pod launching can introduce seconds or
    minutes to every single task execution, which limits Argo when supporting time-sensitive
    workflows. For example, Argoflow is not a good fit for real-time model prediction
    workflow, which runs model prediction requests with millisecond SLAs.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*任务执行延迟* —— 在 Argo Workflows 中，对于每个新任务，Argo 将启动一个新的 Kubernetes Pod 来执行它。Pod
    的启动可能会为每个单独的任务执行引入秒数或分钟，这限制了 Argo 在支持时间敏感的工作流时的能力。例如，Argoflow 不适用于实时模型预测工作流，该工作流以毫秒级
    SLA 运行模型预测请求。'
- en: 9.3.3 Metaflow
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.3 Metaflow
- en: Metaflow is a human-friendly Python library that focuses on MLOps. It was originally
    developed at Netflix and open-sourced in 2019\. Metaflow is special in that it
    follows a humancentric design; it’s not only built for automating workflows but
    also aims to reduce the human time (operational cost) spent in deep learning project
    development.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: Metaflow 是一个以人为本的 Python 库，专注于 MLOps。它最初是在 Netflix 开发的，并于 2019 年开源。Metaflow
    的特点在于它遵循以人为本的设计；它不仅用于自动化工作流程，还旨在减少在深度学习项目开发中花费的人工时间（操作成本）。
- en: 'In section 9.1.3, we pointed out that the conversion from prototyping code
    to production workflow generates a lot of friction in ML development. Data scientists
    have to construct and test a new version of the workflow for each model development
    iteration. To bridge the gap between prototyping and production, Metaflow made
    two improvements: first, it simplifies workflow construction, and second, it unifies
    the workflow execution experience between the local and production environments
    (see figure 9.7).'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 9.1.3 节中，我们指出原型代码转换为生产工作流会在 ML 开发中产生很多摩擦。数据科学家必须为每个模型开发迭代构建和测试新版本的工作流。为了弥合原型和生产之间的差距，Metaflow
    进行了两项改进：首先，它简化了工作流程的构建；其次，它统一了本地和生产环境之间的工作流程执行体验（参见图 9.7）。
- en: '![](../Images/09-07.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09-07.png)'
- en: Figure 9.7 Metaflow offers a unified development experience between prototyping
    and production.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7 Metaflow 在原型和生产之间提供了统一的开发体验。
- en: In figure 9.7, we can see that Metaflow treats both prototyping and production
    environments as first-class execution environments. Because the Metaflow library
    offers a set of unified APIs to abstract the actual infrastructure, a workflow
    can be executed in the same way regardless of which environment it runs on. For
    example, a workflow can be run by both a local scheduler and a production scheduler
    without any change. The local scheduler executes workflows locally whereas the
    production scheduler integrates into other production orchestration systems, such
    as AWS Step Functions or Argo Workflows.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 9.7 中，我们可以看到 Metaflow 将原型和生产环境都视为一流的执行环境。由于 Metaflow 库提供了一组统一的 API 来抽象实际的基础设施，一个工作流可以在不同的环境中以相同的方式运行。例如，一个工作流可以在本地调度器和生产调度器上运行而无需任何更改。本地调度器在本地执行工作流，而生产调度器集成到其他生产编排系统中，例如
    AWS Step Functions 或 Argo Workflows。
- en: Metaflow lets users annotate a Python code—a DAG Python class—to define the
    workflow. The Metaflow library then creates/packages the workflow automatically
    from the Python annotations. With Metaflow Python annotation, Vena can build a
    workflow without changing any of her prototyping code.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: Metaflow 允许用户注释 Python 代码 —— 一个 DAG Python 类 —— 来定义工作流程。然后 Metaflow 库会根据 Python
    注释自动创建/打包工作流。使用 Metaflow Python 注释，Vena 可以在不更改任何原型代码的情况下构建工作流程。
- en: Besides seamless workflow creation and testing, Metaflow offers other useful
    features that are key to model reproducibility, such as workflow/steps versioning
    and step input/output saving. To learn more about Metaflow, you can check out
    Metaflow’s official website ([https://docs.metaflow.org/](https://docs.metaflow.org/))
    and a great Metaflow book, *Effective Data Science Infrastructure*, written by
    Ville Tuulos (Manning, 2022; [https://www.manning.com/books/effective-data-science-infrastructure](https://www.manning.com/books/effective-data-science-infrastructure)).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 除了无缝创建和测试工作流之外，Metaflow 还提供其他一些对模型可重复性至关重要的实用功能，如工作流/步骤版本控制和步骤输入/输出保存。要了解更多关于
    Metaflow 的信息，您可以查看 Metaflow 的官方网站（[https://docs.metaflow.org/](https://docs.metaflow.org/)）和一本名为《*Effective
    Data Science Infrastructure*》的精彩 Metaflow 书籍，作者是 Ville Tuulos（Manning，2022；[https://www.manning.com/books/effective-data-science-infrastructure](https://www.manning.com/books/effective-data-science-infrastructure)）。
- en: A typical use case
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 典型用例
- en: Let’s use Metaflow to automate the same data process work we saw in sections
    9.3.1 and 9.3.2\. See the following listing for the pseudocode.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 Metaflow 自动化我们在 9.3.1 和 9.3.2 节中看到的相同的数据处理工作。请参见伪代码清单以下示例。
- en: Listing 9.3 A sample Metaflow workflow
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3 显示了一个 Metaflow 工作流的示例
- en: '[PRE6]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In code listing 9.3, we see Metaflow takes a novel approach to building a workflow
    by using code annotations. By annotating `@step` on the functions and using `self.next`
    function to connect steps, we can easily construct a workflow DAG (figure 9.8)
    from our prototyping code.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码清单 9.3 中，我们看到 Metaflow 通过使用代码注释的新方法构建工作流。通过在函数上注释 `@step` 并使用 `self.next`
    函数来连接步骤，我们可以轻松地从我们的原型代码构建一个工作流 DAG（图 9.8）。
- en: '![](../Images/09-08.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09-08.png)'
- en: Figure 9.8 Workflow DAG constructed from listing 9.3
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8 显示了从图 9.3 构建的工作流 DAG
- en: One of the beauties here is that we don’t have to define the workflow DAG in
    a separate system and repackage code to a different format, such as a Docker image.
    The Metaflow workflow is immersed in our code. Workflow development and prototyping
    code development happen at the same place and can be tested together from the
    very beginning to the end of the entire ML development cycle.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的一大优势在于，我们不需要在一个单独的系统中定义工作流 DAG 并将代码重新打包到不同的格式（比如 Docker 镜像）中。Metaflow 工作流完全融入我们的代码中。工作流开发和原型代码开发发生在同一地方，并且可以从整个
    ML 开发周期的开始到结束一起进行测试。
- en: 'Once the code is ready, we can validate and run the workflow locally. See the
    following sample commands:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 代码准备就绪后，我们可以在本地验证和运行工作流。参见以下示例命令：
- en: '[PRE7]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Once we finish local development and testing, it’s time to push the workflow
    to production, which can be achieved by the following two commands:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们完成了本地开发和测试，就该将工作流推送到生产环境了，可以通过以下两个命令来实现：
- en: '[PRE8]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: These commands will export our data process workflow defined in code listing
    9.3 to AWS Step Functions and Argo Workflows. You can then also search for the
    flow by name within the AWS Step Functions UI or Argo Workflows UI and hence see
    the exported flow.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这些命令将我们在代码清单 9.3 中定义的数据处理工作流导出到 AWS Step Functions 和 Argo Workflows。然后，您还可以在
    AWS Step Functions UI 或 Argo Workflows UI 中按名称搜索流程，从而查看导出的流程。
- en: Note Metaflow offers a unified development experience between local and production
    environments. Thanks to the unified API provided by Metaflow, we have a seamless
    experience when testing our code and workflow locally and in production. Regardless
    of the backend workflow orchestration system used, whether Metaflow local scheduler,
    Argo Workflows, or AWS Step Functions, the Metaflow user experience on the workflow
    development remains the same!
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 Metaflow 在本地和生产环境之间提供了统一的开发体验。由于 Metaflow 提供的统一 API，我们在本地和生产环境中测试代码和工作流时拥有无缝的体验。无论使用哪种后端工作流编排系统，无论是
    Metaflow 本地调度器、Argo Workflows 还是 AWS Step Functions，工作流开发的 Metaflow 用户体验都是相同的！
- en: Key Features
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 关键功能
- en: 'Metaflow offers the following key features:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: Metaflow 提供以下关键功能：
- en: '*Structures code as workflow*—Metaflow lets users create a workflow by annotating
    Python code, which greatly simplifies workflow construction.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*将代码结构化为工作流* — Metaflow 允许用户通过对 Python 代码进行注释来创建工作流，这极大地简化了工作流的构建。'
- en: '*Reproducibility*—Metaflow preserves an immutable snapshot of the data, code,
    and external dependencies required to execute each workflow step. Metaflow also
    records the metadata of each workflow execution.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可重复性* — Metaflow 保留了执行每个工作流步骤所需的数据、代码和外部依赖项的不可变快照。Metaflow 还记录了每个工作流执行的元数据。'
- en: '*Versioning*—Metaflow addresses the version control requirement of an ML project
    by hashing all the code and data in a workflow.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*版本控制* — Metaflow 通过对工作流中的所有代码和数据进行哈希处理来解决 ML 项目的版本控制要求。'
- en: '*Robust workflow execution*—Metadata provides dependency management mechanisms
    at both the workflow level and step level by using the @conda decorator. It also
    offers task retries.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*稳健的工作流执行* — 元数据通过使用 @conda 装饰器在工作流级别和步骤级别提供了依赖管理机制。它还提供了任务重试。'
- en: '*Usability design for ML*—Metaflow treats prototyping and production as equally
    important. It provides a set of unified APIs to abstract the infrastructure, so
    the same code can run in both the prototyping environment and the production environment
    without any changes.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ML 的可用性设计* — Metaflow 将原型设计和生产视为同等重要。它提供了一组统一的 API 来抽象基础设施，因此相同的代码可以在原型环境和生产环境中运行而无需任何更改。'
- en: '*Seamless scalability*—Metaflow integrates with Kubernetes and AWS Batch, which
    allows users to define the required computing resource easily, and can parallel
    the workflow steps over an arbitrarily large number of instances. For example,
    by applying an annotation like `@batch(cpu=1,` `memory=500)` to a step function,
    Metaflow will work with AWS Batch to allocate the required resource to compute
    this step.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*无缝扩展性*—Metaflow集成了Kubernetes和AWS Batch，允许用户轻松定义所需的计算资源，并可以并行执行任意数量的工作流步骤。例如，通过对步骤函数应用像`@batch(cpu=1,`
    `memory=500)`这样的注解，Metaflow将与AWS Batch合作分配所需的资源来计算此步骤。'
- en: Limitations
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 局限性
- en: 'The downsides of using Metaflow in deep learning systems are as follows:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习系统中使用Metaflow的缺点如下：
- en: '*No conditional branching support*—Metaflow step annotation doesn’t support
    conditional branching (only executing a step when a condition is met). This is
    not a red flag, but it’s a nice feature to have.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*没有条件分支支持*—Metaflow步骤注解不支持条件分支（仅在满足条件时执行步骤）。这不是一个红旗，但是这是一个很好的功能。'
- en: '*No job scheduler*—Metaflow itself doesn’t come with a job scheduler, so you
    can’t use a cron workflow. This is not a big problem because Metaflow can integrate
    with other orchestration systems that support job scheduling, such as AWS Step
    Functions and Argo Workflows.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*没有作业调度程序*—Metaflow本身不带有作业调度程序，因此无法使用cron工作流。这并不是一个大问题，因为Metaflow可以与支持作业调度的其他编排系统集成，例如AWS
    Step Functions和Argo Workflows。'
- en: '*Tightly coupled to AWS*—the most important features of Metaflow are tightly
    coupled to AWS—for example, Amazon S3 and AWS Batch. Luckily, Metaflow is an open
    source project, so it’s possible to extend it to non-AWS alternatives.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*与AWS紧密耦合*—Metaflow的最重要特性与AWS紧密耦合，例如，Amazon S3和AWS Batch。幸运的是，Metaflow是一个开源项目，因此可以将其扩展到非AWS替代方案。'
- en: 9.3.4 When to use
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.4 何时使用
- en: If you are looking for an orchestration system to automate workflow execution
    for non-ML projects, both Airflow and Argo Workflows are great choices. They have
    excellent community support and have been used widely in the IT industry. If your
    system runs on Kubernetes and your team feels comfortable working with Docker,
    then Argo Workflows would be a good fit; otherwise, Airflow won’t disappoint you.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在寻找一种用于自动化非ML项目工作流执行的编排系统，Airflow和Argo Workflows都是不错的选择。它们拥有出色的社区支持，并且在IT行业被广泛使用。如果您的系统在Kubernetes上运行，并且您的团队习惯使用Docker，那么Argo
    Workflows将是一个很好的选择；否则，Airflow也不会让您失望。
- en: If you are looking for a system to streamline your ML project development, Metaflow
    is highly recommended. Metaflow is not just an orchestration tool; it’s an MLOps
    tool that focuses on saving data scientists’ time in the ML development cycle.
    Because Metaflow abstracts the backend infrastructure part of a ML project, data
    scientists can focus on model development without worrying about production conversion
    and deployment.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在寻找一个能够简化ML项目开发流程的系统，Metaflow强烈推荐。Metaflow不仅是一个编排工具；它是一个MLOps工具，旨在节省数据科学家在ML开发周期中的时间。由于Metaflow抽象了ML项目的后端基础设施部分，数据科学家可以专注于模型开发，而无需担心生产转换和部署。
- en: Summary
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: A workflow is a sequence of operations that are part of some larger task. A
    workflow can be viewed as a DAG of steps. A step is the smallest resumable unit
    of computation that describes what to do; a step either succeeds or fails as a
    whole. A DAG specifies the dependencies between steps and the order in which to
    execute them.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作流是某个更大任务的操作序列。工作流可以看作是步骤的DAG。步骤是最小的可恢复计算单元，描述了要执行的操作；步骤要么全部成功，要么全部失败。DAG指定了步骤之间的依赖关系和执行顺序。
- en: Workflow orchestration means executing the workflow steps based on the sequence
    defined in the workflow’s DAG.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作流编排意味着根据工作流的有向无环图（DAG）中定义的顺序执行工作流步骤。
- en: Adopting a workflow encourages work sharing, team collaboration, and automation.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 采用工作流鼓励工作共享、团队协作和自动化。
- en: The main challenge of applying a workflow on deep learning projects is to reduce
    workflow construction costs and simplify workflow testing and debugging.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在深度学习项目中应用工作流的主要挑战是降低工作流构建成本，简化工作流测试和调试。
- en: The six recommended design principles for building/evaluating workflow orchestration
    systems are criticality, usability, extensibility, task isolation, scalability,
    and human centricity.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建/评估工作流编排系统的六个推荐设计原则是关键性、可用性、可扩展性、任务隔离性、可扩展性和以人为中心。
- en: When choosing an orchestration system for non-ML projects, both Airflow and
    Argo Workflows are great choices. Argo Workflows is a better option if the project
    runs on Kubernetes and Docker.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在选择非机器学习项目的编排系统时，Airflow 和 Argo Workflows 都是不错的选择。如果项目在 Kubernetes 和 Docker
    上运行，Argo Workflows 是更好的选择。
- en: When selecting an orchestration system for ML projects, Metaflow is so far the
    best option.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在选择机器学习项目的编排系统时，Metaflow 目前是最佳选择。
