- en: 9 Workflow orchestration
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9 工作流编排
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Defining workflow and workflow orchestration
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义工作流和工作流编排
- en: Why deep learning systems need to support workflows
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么深度学习系统需要支持工作流
- en: Designing a general workflow orchestration system
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计一个通用工作流编排系统
- en: 'Introducing three open source orchestration systems: Airflow, Argo Workflows,
    and Metaflow'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入三个开源编排系统：Airflow、Argo Workflows 和 Metaflow
- en: 'In this chapter, we will discuss the last but critical piece of a deep learning
    system: workflow orchestration—a service that manages, executes, and monitors
    workflow automation. Workflow is an abstract and broad concept; it is essentially
    a sequence of operations that are part of some larger task. If you can devise
    a plan with a set of tasks to complete a work, this plan is a workflow. For example,
    we can define a sequential workflow for training a machine learning (ML) model.
    This workflow can be composed of the following tasks: fetching raw data, rebuilding
    the training dataset, training the model, evaluating the model, and deploying
    the model.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论深度学习系统的最后但至关重要的部分：工作流编排——一个管理、执行和监控工作流自动化的服务。工作流是一个抽象且广泛的概念；它本质上是一系列操作，这些操作是某个更大任务的一部分。如果你可以设计一个带有一组任务的计划来完成一项工作，这个计划就是一个工作流。例如，我们可以为训练机器学习（ML）模型定义一个顺序工作流。这个工作流可以由以下任务组成：获取原始数据、重建训练数据集、训练模型、评估模型和部署模型。
- en: Because a workflow is an execution plan, it can be performed manually. For instance,
    a data scientist can manually complete the tasks of the model training workflow
    we just described. For example, to complete the “fetching raw data” task, the
    data scientist can craft web requests and send them to the dataset management
    (DM) service to fetch a dataset—all with no help from the engineers.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 因为工作流是一个执行计划，它可以手动执行。例如，数据科学家可以手动完成我们刚刚描述的模型训练工作流的任务。例如，要完成“获取原始数据”任务，数据科学家可以制作网络请求并将其发送到数据集管理（DM）服务以获取数据集——所有这些都不需要工程师的帮助。
- en: However, executing a workflow manually is not ideal. We want to automate the
    workflow execution. When there are numerous workflows developed for different
    purposes, we need a dedicated system to handle the complexity of workflow executions.
    We call this kind of system a *workflow orchestration system*.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，手动执行工作流并不理想。我们希望自动化工作流的执行。当针对不同目的开发了大量工作流时，我们需要一个专门的系统来处理工作流执行的复杂性。我们称这种系统为*工作流编排系统*。
- en: A workflow orchestration system is built to manage workflow life cycles, including
    workflow creation, execution, and troubleshooting. It provides not only the pulse
    to keep all the scheduled code running but also a control plane for data scientists
    to manage all the automation in a deep learning system.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流编排系统被建立来管理工作流的生命周期，包括工作流的创建、执行和故障排除。它不仅提供了使所有预定代码保持运行的脉搏，还为数据科学家提供了一个控制平面，用于管理深度学习系统中的所有自动化。
- en: In this chapter, we will discuss workflow orchestration system design and the
    most popular open source orchestration systems used in the deep learning field.
    By reading this chapter, you will not only gain a solid understanding of the system
    requirements and the design options, but you will also learn how to choose the
    right open source orchestration system that works best for your own situation.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论工作流编排系统的设计以及在深度学习领域中使用的最受欢迎的开源编排系统。通过阅读本章，您不仅将对系统要求和设计选项有扎实的理解，还将了解如何选择最适合您自己情况的正确开源编排系统。
- en: 9.1 Introducing workflow orchestration
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 引入工作流编排
- en: Before we dive into the details of designing workflow orchestration systems,
    let’s have a quick discussion on the basic concept of workflow orchestration,
    especially about the special workflow challenges from a deep learning/ML perspective.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨工作流编排系统设计的细节之前，让我们就工作流编排的基本概念进行快速讨论，特别是关于从深度学习/ML 角度出发的特殊工作流挑战。
- en: Note Because the requirements of using workflow orchestration for deep learning
    projects and ML projects are almost identical, we will use the word *deep learning*
    and *machine learning* interchangeably in this chapter.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 由于在深度学习项目和 ML 项目中使用工作流编排的要求几乎相同，因此在本章中我们将深度学习和机器学习这两个词用于交替使用。
- en: 9.1.1 What is workflow?
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.1 什么是工作流？
- en: In general, a workflow is a sequence of operations that are part of some larger
    task. A workflow can be viewed as a directed acyclic graph (DAG) of steps.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: A step is the smallest resumable unit of computation that describes an action;
    this task could be fetching data or triggering a service, for example. A step
    either succeeds or fails as a whole. In this chapter, we use the word *task* and
    *step* interchangeably.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: A DAG specifies the dependencies between steps and the order in which to execute
    them. Figure 9.1 shows a sample workflow for training natural language processing
    (NLP) models.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/09-01.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 A DAG of a sample model training workflow with multiple steps. Both
    ovals and diamonds are steps, but different types. The solid arrows indicate the
    dependencies between steps, and the dotted-line arrows represent the external
    web requests sent from steps.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: From the sample DAG in figure 9.1, we see a workflow that consists of many steps.
    Every step depends on another, and the solid arrows show the dependencies between
    steps. These arrows and steps form a workflow DAG with no loops.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: If you follow the arrows in the DAG (from left to right) and complete the tasks,
    you can train and release an NLP model to production. For example, when an incoming
    request triggers the workflow, the auth (authorization) step will be executed
    first, and then the dataset-building step and the embedding fetching step will
    both be executed simultaneously. The steps on the other side of the arrows will
    be executed after these two steps have been completed.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Workflows are used everywhere in the IT industry. As long as you can define
    a process as a DAG of single tasks/steps, this process can be considered a workflow.
    Workflows are critical to deep learning model development. In fact, in production
    environments, most of the deep learning model–building activities are presented
    and executed as workflows.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Note A workflow should not have a loop. To guarantee a workflow can be completed
    under any condition, its execution graph needs to be a DAG, which prevents the
    workflow execution from falling into a dead loop.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 9.1.2 What is workflow orchestration?
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once we define a workflow, the next step is to run the workflow. Running a workflow
    means executing the workflow steps based on the sequence defined in the workflow’s
    DAG. *Workflow orchestration* is the term we use to describe the execution and
    monitoring of the workflow.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: The goal of workflow orchestration is to automate the execution of tasks defined
    in workflows. In practice, the concept of workflow orchestration often extends
    to workflow management as a whole—that is, creating, scheduling, executing, and
    monitoring multiple workflows in an automated manner.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'Why do deep learning systems need workflow orchestration? Ideally, we should
    be able to code an entire deep learning project as one piece. And that’s exactly
    what we do in the prototyping phase of a project, putting all the code in a Jupyter
    notebook. So, why do we need to transform the prototyping code into a workflow
    and run it in a workflow orchestration system? The answer is twofold: automation
    and work sharing. To understand these reasons, let’s look at three sample training
    workflows in figure 9.2.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/09-02.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 Deep learning workflows are composed of many reusable tasks.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: A great benefit of using a workflow is that it turns a large chunk of code into
    a group of sharable and reusable components. In figure 9.2, we imagined three
    data scientists working on three model training projects (A, B, and C). Because
    each project’s training logic is different, data scientists developed three different
    workflows (A, B, and C) to automate their model training processes. Although each
    workflow has different DAGs, the steps in each DAG are highly overlapped. The
    total six steps are sharable and reusable. For example, the auth step (step 1)
    is the first step for all three workflows.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Having reusable steps can greatly improve data scientists’ productivity. For
    example, to pull data from a DM service (step 2 in figure 9.2), data scientists
    need to learn how the DM web API works. But if someone already built a DM data
    pull method as a step function, scientists can just reuse this step in their workflow
    without learning how to interact with the DM service. If everyone writes their
    project in the form of a workflow, we will have lots of reusable steps, which
    will save lots of duplicate effort at an organizational level!
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Another reason that a workflow is well adapted to deep learning development
    is that it facilitates collaboration. Model development requires teamwork; a dedicated
    team might work on data while another team works on the training algorithm. By
    defining a complex model-building process in the workflow, we can dispatch a big
    complex project in pieces (or steps) and assign them to different teams while
    still keeping the project organized and the components in proper order. The workflow
    DAG shows the task dependencies clearly for all the project participants to see.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: In short, a good workflow orchestration system encourages work sharing, facilitates
    team collaboration, and automates complicated development scenarios. All these
    merits make workflow orchestration a crucial component of deep learning project
    development.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 9.1.3 The challenges for using workflow orchestration in deep learning
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the previous section, we saw how a workflow system can provide a lot of
    benefits to deep learning project development. But there is one caveat: using
    workflows to prototype deep learning algorithm ideas is cumbersome.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: To understand how and why it is cumbersome, let’s look at a deep learning development
    process diagram (figure 9.3). This diagram should set the foundation for you to
    understand the challenges that workflow presents in the deep learning context.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解为什么这样做很麻烦，让我们看一下深度学习开发过程的图表（图9.3）。这张图表应该为你理解工作流在深度学习背景下提出的挑战奠定基础。
- en: '![](../Images/09-03.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图9.3](../Images/09-03.png)'
- en: Figure 9.3 A data scientist’s view of deep learning project development
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 深度学习项目开发的数据科学家视角
- en: 'In figure 9.3, we see a typical deep learning project development process from
    a data scientist’s perspective. The process can be divided into two phases: the
    local incubation phase and the production phase.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在图9.3中，我们从数据科学家的角度看到了一个典型的深度学习项目开发过程。该过程可以分为两个阶段：本地孵化阶段和生产阶段。
- en: 'In the local incubation phase, data scientists work on data exploration and
    model training prototyping at their local/dev environment. When the prototyping
    is done and the project looks promising, data scientists start to work on production
    onboarding: moving the prototyping code to the production system.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地孵化阶段，数据科学家在本地/开发环境中进行数据探索和模型训练原型。当原型完成并且项目看起来很有前景时，数据科学家开始进行生产上线：将原型代码移到生产系统。
- en: In the production phase, data scientists convert the prototyping code to a workflow.
    They break the code down into multiple steps and define a workflow DAG and then
    submit the workflow to the workflow orchestration system. After that, the orchestration
    system takes over and runs the workflow based on its schedule.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产阶段，数据科学家将原型代码转换为工作流程。他们将代码分解为多个步骤，并定义一个工作流DAG，然后将工作流提交给工作流编排系统。之后，编排系统接管并根据其时间表运行工作流程。
- en: Gaps between prototyping and production
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在原型和生产之间存在差距。
- en: 'If you ask an engineer who works on workflow orchestration systems how they
    feel about the development process in figure 9.3, the answer most likely is: It’s
    pretty good! But in practice, this process is problematic for data scientists.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你问一个在工作流编排系统上工作的工程师他们对图9.3中的开发过程的感觉，答案很可能是：还不错！但实际上，这个过程对数据科学家来说是有问题的。
- en: 'From the data scientists’ point of view, once an algorithm is tested locally,
    its prototyping code should be shipped to production right away. But in figure
    9.3, we see the prototyping phase and production phase are *not* smoothly connected.
    Shipping incubation code to production is not straightforward; data scientists
    have to do extra work to construct a workflow to run their code in production.
    The gap between prototyping code and production workflow jeopardizes development
    velocity for two reasons:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据科学家的角度来看，一旦算法在本地测试通过，其原型代码应立即投入生产。但是在图9.3中，我们看到原型阶段和生产阶段 *不是* 顺利连接的。将孵化代码部署到生产并不直接；数据科学家必须额外工作来构建一个工作流程来在生产中运行他们的代码。原型代码和生产工作流之间的差距影响了开发速度，原因有两个：
- en: '*Workflow building and debugging aren’t straightforward*—Data scientists normally
    face a huge learning curve when authoring model training workflows in orchestration
    systems. Learning the workflow DAG syntax, workflow libraries, coding paradigms,
    and troubleshooting is a huge burden to data scientists. The workflow troubleshooting
    is the most painful part. The majority of the orchestration system doesn’t support
    local execution, which means data scientists have to test their workflow in the
    remote orchestration system. This is hard because both the workflow environment
    and workflow execution logs are remote, so data scientists cannot easily figure
    out the root cause when a workflow execution goes wrong.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*工作流程的构建和调试并不是直接的* —— 数据科学家在编写模型训练工作流程时，通常会面临巨大的学习曲线。学习工作流DAG语法、工作流程库、编码范例和故障排除对于数据科学家来说是一个巨大的负担。工作流程的故障排除是最痛苦的部分。大多数编排系统不支持本地执行，这意味着数据科学家必须在远程编排系统中测试他们的工作流程。这很困难，因为工作流环境和工作流执行日志都是远程的，所以数据科学家在工作流程执行出错时无法轻易找出根本原因。'
- en: '*Workflow construction happens not once but frequently*—The common misperception
    is that because workflow construction only happens once, it’s fine if it is time-consuming
    and cumbersome. But the fact is, workflow construction happens continuously because
    deep learning development is an iterative process. As figure 9.3 shows, data scientists
    work on prototyping and production experimentation iteratively, so the workflow
    needs to be updated frequently to test new improvements from local to production.
    Therefore, the unpleasant and time-consuming workflow construction happens repeatedly,
    which hinders the development velocity.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*工作流构建并非一次性事件，而是频繁发生*——一种常见的误解是，由于工作流构建只发生一次，所以如果耗时且繁琐也没关系。但事实是，工作流构建是持续不断的，因为深度学习开发是一个迭代过程。正如图9.3所示，数据科学家会迭代地进行原型设计和生产实验，因此工作流需要经常更新，以测试从本地到生产环境的新改进。因此，令人不快且耗时的工作流构建会反复发生，这阻碍了开发速度。'
- en: Smoothing the transition from prototyping to production
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 平滑地从原型设计过渡到生产环境
- en: Although there are gaps, the process in figure 9.3 is good. Data scientists
    start prototyping locally with a straightforward script, and then they keep working
    on it. If the results after each iteration seem promising enough, the “straightforward
    local script” is converted to a workflow and runs in the orchestration system
    in production.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在差异，图9.3中的流程是不错的。数据科学家从一个简单的脚本开始在本地进行原型设计，然后继续完善。如果每次迭代后的结果看起来足够令人满意，那么“简单的本地脚本”将被转换为工作流，并在生产环境中在编排系统中运行。
- en: The key improvement is to make the transition step from prototyping code to
    a production workflow seamless. If an orchestration system is designed for deep
    learning use cases, it should provide tools to help data scientists build workflows
    from their code with minimum effort. For example, Metaflow, an open source library
    that will be discussed in section 9.3.3, allows data scientists to authorize workflow
    by writing Python code with Python annotations. Data scientists can obtain a workflow
    from their prototyping code directly without making any changes. Metaflow also
    provides a unified user experience on model execution between local and cloud
    production environments. This eliminates the friction in workflow testing because
    Metaflow operates workflows the same way in both local and production environments.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 关键的改进是使从原型代码到生产工作流的过渡步骤变得无缝。如果一个编排系统是为深度学习用例设计的，它应该提供工具，帮助数据科学家用最少的工作量从他们的代码构建工作流。例如，Metaflow是一个开源库，将在9.3.3节中讨论，它允许数据科学家通过编写带有Python注解的Python代码来授权工作流。数据科学家可以直接从他们的原型代码中获得工作流，而不需要进行任何更改。Metaflow还在本地和云生产环境之间提供了统一的模型执行用户体验。这消除了工作流测试中的摩擦，因为Metaflow在本地和生产环境中以相同的方式运行工作流。
- en: A deep learning system should be humancentric
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习系统应以人为中心
- en: When we introduce a general-purpose tool—like workflow orchestration—to deep
    learning systems, don’t be satisfied with only enabling the functionality. Try
    to reduce human time in the system. Customization work is always possible to help
    our users be more productive.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们向深度学习系统引入通用工具——如工作流编排时，不要满足于仅仅启用功能。尽量减少系统中人工的时间。总是可以进行定制工作，以帮助我们的用户更加高效。
- en: Metaflow (section 9.3.3) is a good example of what happens when engineers aren't
    satisfied with just building an orchestration system to automate deep learning
    workflows. Instead, they went a step further to optimize the workflow construction
    and management to address the way data scientists work.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Metaflow（9.3.3节）是一个很好的例子，说明当工程师们不满足于仅仅构建一个用于自动化深度学习工作流的编排系统时会发生什么。相反，他们更进一步优化了工作流构建和管理，以解决数据科学家的工作方式。
- en: 9.2 Designing a workflow orchestration system
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2 设计工作流编排系统
- en: In this section, we will approach the design of workflow orchestration systems
    in three steps. First, we use a typical data scientist user scenario to show how
    an orchestration system works from a user perspective. Second, we learn a generic
    orchestration system design. Third, we summarize the key design principles for
    building or evaluating an orchestration system. By reading this section, you will
    understand how orchestration systems work, in general, so you can be confident
    evaluating or working on any orchestration system.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将分三个步骤设计工作流编排系统。首先，我们使用一个典型的数据科学家用户场景，展示编排系统从用户角度的工作方式。第二，我们学习通用编排系统设计。第三，我们总结构建或评估编排系统的关键设计原则。通过阅读本节，您将了解编排系统的一般工作方式，从而可以自信地评估或操作任何编排系统。
- en: 9.2.1 User scenarios
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.1 用户场景
- en: 'Although the process of workflows varies a lot from one scenario to another,
    the user scenarios for data scientists are quite standard. Most workflow usage
    can be divided into two phases: the development phase and the execution phase.
    See figure 9.4 for a data scientist’s (Vena’s) workflow user experience. Let’s
    follow through with Vena’s user scenario in figure 9.4 step by step.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管各工作流场景的过程有很大的差异，但数据科学家的用户场景通常非常标准。大多数工作流使用可以分为两个阶段：开发阶段和执行阶段。请参见图9.4，了解数据科学家（Vena）的工作流用户体验。我们将一步一步地跟随图9.4中
    Vena 的用户场景。
- en: '![](../Images/09-04.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09-04.png)'
- en: Figure 9.4 A general deep learning user scenario of a workflow orchestration
    system
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4：工作流编排系统的通用深度学习用户场景。
- en: Development phase
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 开发阶段
- en: 'During the development phase, data scientists convert their training code into
    a workflow. See Vena’s example as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发阶段，数据科学家将其训练代码转换为工作流。以下是 Vena 的示例：
- en: Vena, a data scientist, prototypes her model training algorithm in a Jupyter
    notebook or pure Python in her local environment. After local testing and evaluation,
    Vena thinks it’s time to deploy the code to production for online experiments
    with real customer data.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据科学家 Vena 在本地环境中使用 Jupyter notebook 或纯 Python 原型开发其模型训练算法。经过本地测试和评估，Vena 认为是时候将代码部署到生产环境中，进行真正的客户数据在线实验了。
- en: Because everything running in production is a workflow, Vena needs to convert
    her prototype code to a workflow. So Vena uses the syntax provided by the orchestration
    system to rebuild her work into a DAG of tasks in a YAML (a text configuration)
    file. For example, data parsing -> data augmentation -> dataset building -> training
    -> [online evaluation, offline evaluation] -> model release.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于生产环境中的所有内容都是工作流，因此 Vena 需要将其原型代码转换为工作流程。所以，Vena 使用编排系统提供的语法，在 YAML（文本配置）文件中将其工作重建为一个任务的
    DAG。例如，数据解析->数据增强->数据集构建->训练->[在线评估、离线评估]->模型发布。
- en: Vena then sets the input/output parameters and actions for each step in the
    DAG. Using the training step as an example, Vena sets the step action as a RESTful
    HTTP request. This step will send a RESTful request to the model training service
    to start a training job. The payload and parameters of this request come from
    the step input parameters.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，Vena 为 DAG 中每个步骤设置输入/输出参数和动作。以训练步骤为例，Vena 将步骤动作设置为 RESTful HTTP 请求。此步将向模型训练服务发送一个
    RESTful 请求来启动训练作业。该请求的有效载荷和参数来自步骤输入参数。
- en: Once the workflow is defined, Vena sets the workflow’s execution schedule in
    the DAG YAML file. For example, Vena can schedule the workflow to run on the first
    day of every month, and she also sets the workflow to be triggered by an external
    event.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦定义好工作流，Vena 就在 DAG YAML 文件中设置工作流的执行计划。例如，Vena 可以将工作流安排在每个月的第一天运行，还可以将工作流设置为由外部事件触发。
- en: Vena runs the workflow local validation and submits the workflow to the orchestration
    service.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Vena 运行工作流本地验证，并将工作流提交给编排服务。
- en: 'To give you an idea of what a workflow means in reality, the following code
    shows a pseudo workflow for Vena (in section 9.3, we will discuss the actual workflow
    systems):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让您了解工作流在现实中的含义，以下代码显示了 Vena 的伪工作流（在第9.3节，我们将讨论实际的工作流系统）：
- en: '[PRE0]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ DAG definition; defines the body of the workflow, including steps and dependencies
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ DAG定义；定义了工作流的主体，包括步骤和依赖项
- en: ❷ Executes a bash command for data augmentation
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 执行数据增强的 bash 命令
- en: ❸ A sequential execution flow
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 顺序执行流
- en: Execution phase
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 执行阶段
- en: 'In the execution phase, the orchestration service executes the model training
    workflow, as illustrated by Vena’s example:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Once Vena’s workflow is submitted, the orchestration service saves the workflow
    DAG into a database.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The orchestration service’s scheduler component detects Vena’s workflow and
    dispatches the tasks of the workflow to backend workers. The scheduler will make
    sure the tasks are executed in the sequence that is defined in the workflow DAG.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Vena uses the orchestration service’s web UI to check the workflow’s execution
    progress and results in real time.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the workflow produces a good model, Vena can promote it to the staging and
    production environments. If not, Vena starts another iteration of prototyping.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A critical indicator of whether an orchestration system is a good fit for deep
    learning is how easy it is to convert the prototyping code into a workflow. In
    figure 9.4, we see that Vena needs to transform her training code into a workflow
    every time she prototypes a new idea. We can imagine how much human time it would
    save if we eased the friction of converting the deep learning code to a workflow.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Note A workflow should always be lightweight. The workflow is used to automate
    a process; its goal is to group and connect a series of tasks and execute them
    in a defined sequence. The great benefit of using a workflow is that people can
    share and reuse the tasks, so they can automate their process faster. Therefore,
    the workflow itself shouldn’t do any heavy computation, the real work should be
    done by the tasks of the workflow.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.2 A general orchestration system design
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s now turn to a generic workflow orchestration system. To help you understand
    how an orchestration system works and how to research open source orchestration
    systems, we prepared a high-level system design. By zooming out of the detailed
    implementation and only keeping the core components, this design is applicable
    to most orchestration systems, including open source systems, which will be discussed
    in section 9.3\. See figure 9.5 for the design proposal.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/09-05.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 A design overview for a generic workflow orchestration service
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: 'A workflow orchestration system generally consists of the following five components:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '*Web server*—The web server presents a web user interface and a set of web
    APIs for users to create, inspect, trigger, and debug the behavior of a workflow.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Scheduler and controller*—The scheduler and controller component does two
    things. First, the scheduler watches every active workflow in the system, and
    it schedules the workflow to run when the time is right. Second, the controller
    dispatches the workflow tasks to workers. Although the scheduler and controller
    are two different function units, they usually are implemented together because
    they are all related to workflow execution.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Metadata database*—The metadata database stores the workflows’ configuration,
    DAG, editing and execution history, and the tasks’ execution state.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Worker group*—The worker group provides the compute resource to run workflow
    tasks. The worker abstracts the infrastructure and is agnostic to the task that’s
    running. For example, we might have different types of workers, such as a Kubernetes
    worker and an Amazon Elastic Compute Cloud (EC2) worker, but they can all execute
    the same task, albeit on different infrastructures.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Object store*—The object store is shared file storage for all other components;
    it’s normally built on top of cloud object storage, such as Amazon Simple Storage
    Service (S3). One usage of an object store is task output sharing. When a worker
    runs a task, it reads the output value of the previous task from the object store
    as the task input; the worker also saves the task output to the object store for
    its successor tasks.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both the object store and the metadata database are accessible to all the components
    of the orchestration system, including the scheduler, web server, and workers’
    components. Having centralized data storage decouples the core components, so
    the web server, scheduler, and workers can work independently.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: How is a workflow executed?
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: First, Vena defines the DAG for the workflow. Inside the DAG, Vena declares
    a set of tasks and defines the control flow of the task execution sequence. For
    each task, Vena either uses the system’s default operator, such as a Shell command
    operator or Python operator, or builds her own operator to execute tasks.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Second, Vena submits the workflow—DAG with dependent code—to the web server
    through the web UI or command line. The workflow is saved in the metadata database.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Third, the scheduler periodically (every few seconds or minutes) scans the metadata
    database and detects the new workflow; it then kicks off the workflow at the scheduled
    time. To execute a workflow, the scheduler calls the controller component to dispatch
    the workflow’s tasks to the worker queue based on the task sequence defined in
    DAG.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Fourth, a worker picks up a task from the shared job queue; it reads the task
    definition from the metadata database and executes the task by running the task’s
    operator. During the execution, the worker saves the task’s output value to the
    object store and reports the task’s execution status back to the metadata database.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least, Vena uses the web UI hosted on the web server component
    to monitor the workflow execution. Because both the scheduler/controller components
    and the workers report the status to the metadata database in real time, the web
    UI always displays the latest workflow status.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.3 Workflow orchestration design principles
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Because we have seen how a workflow orchestration system works internally and
    externally, now it’s time to examine the design principles that make an orchestration
    system outstanding for deep learning scenarios. We hope you can use the principles
    here as a guide to evolving your system or for evaluating open source approaches.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Note The workflow orchestration system is one of the most complicated components
    in a deep learning system in terms of engineering effort, so don't worry too much
    about making your system match perfectly with these principles in the first few
    versions.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: 'Principle 1: Criticality'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Workflow orchestration is essentially a job scheduling challenge, so the bottom
    line for any orchestration system is to provide a solid workflow execution experience.
    A valid workflow should always be executed correctly, repeatedly, and on schedule.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'Principle 2: Usability'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: The usability measurement of an orchestration system in a deep learning context
    is whether it optimizes data scientists’ productivity. Most data scientist interactions
    in an orchestration system are workflow creation, testing, and monitoring. So
    a user-friendly orchestration system should let users create, monitor, and troubleshoot
    workflows easily.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'Principle 3: Extensibility'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: To cater to the wide variety of deep learning infrastructures, people should
    easily define their own task operators and executors without worrying about where
    they are deployed to. The orchestration system should provide the level of abstraction
    that suits your environment, whether if it’s Amazon EC2 or Kubernetes.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 'Principle 4: Isolation'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 'Two types of isolations can occur that are critical: workflow creation isolation
    and workflow execution isolation. Workflow creation isolation means people can
    not interfere with each other when creating workflows. For example, if Vena submits
    an invalid workflow DAG or releases a new version of a common shared library that’s
    referenced in other workflows, the existing workflows shouldn’t be affected.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Workflow execution isolation means that each workflow is running in an isolated
    environment. There should be no resource competition between workflows, and failure
    of a workflow won’t affect other workflow’s executions.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 'Principle 5: Scaling'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 'A good orchestration system should address the following two scaling problems:
    handling large numbers of concurrent workflows and handling large expansive workflows.
    Concurrent workflow scaling generally means that given enough compute resources—for
    example, adding more workers to the worker group—the orchestration system can
    cater to an infinite concurrent number of workflow executions. Also, the system
    should always keep the service-level agreement (SLA) for every workflow. For example,
    a workflow should be executed at its scheduled time and no later than 2 seconds,
    regardless of how many other workflows are executing.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: For single, large workflow scaling, the system should encourage users not to
    worry about performance, so they can focus on readable, straightforward code,
    and easy operations. When the workflow execution hits a limit—for example, the
    training operators take too long to execute—the orchestration system should provide
    some horizontal parallelism operators, such as distributed training operators,
    to address single workflow performance problems.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: The main scaling idea for deep learning orchestration is that we should solve
    the performance problem at the system level and avoid asking users to write code
    with scalability in mind. This can lead to worse readability, harder debuggability,
    and increased operational burden.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'Principle 6: Human-centric support for both prototyping and production'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: The capability of connecting the data scientist’s local prototyping code to
    the production workflow is a requirement specific to deep learning. It’s a key
    indicator that we use to evaluate whether an orchestration system is a good fit
    for deep learning systems.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: An orchestration system designed for deep learning will respect that deep learning
    project development is an iterative, ongoing effort from prototyping to production.
    Therefore, it will make a dedicated effort to help data scientists convert their
    local prototype code to production workflow in a seamless fashion.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 9.3 Touring open source workflow orchestration systems
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will introduce three battle-tested workflow orchestration
    systems: Airflow, Argo Workflows, and Metaflow. These three open source systems
    are widely adopted in the IT industry and backed by active communities. In addition
    to introducing them generally, we also evaluate these workflow systems from the
    perspective of deep learning project development.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: To make a fair comparison, we implement pseudocode for the same workflow in
    Airflow, Argo Workflows, and Metaflow. Basically, if there is new data, we initially
    transform the data and save it to a new table in the database, and then we notify
    the data science team. Also, we expect the workflow to run daily.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.1 Airflow
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Airflow ([https://airflow.apache.org/docs/apache-airflow/stable/index.html](https://airflow.apache.org/docs/apache-airflow/stable/index.html))
    was created in 2014 at Airbnb and is now a part of the Apache Foundation. Airflow
    is a platform to programmatically author, schedule, and monitor workflows. Airflow
    is not designed for deep learning use cases; it was originally built to orchestrate
    the increasingly complex ETL (extract, transform, load) pipelines (or data pipelines).
    But because of Airflow’s good extensibility, production quality, and GUI support,
    it’s widely used in many other domains, including deep learning. As this book
    is written, Airflow is the most adopted orchestration system.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: A typical use case
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Building a workflow in Airflow takes two steps. First, define the workflow DAG
    and tasks. Second, declare the task dependencies in the DAG. An Airflow DAG is
    essentially Python code. See the following listing for how our sample workflow
    is implemented in Airflow.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.1 A sample Airflow workflow definition
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Checks whether a new file arrives
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: ❷ The actual logic is implemented in the "transform_data" function.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: ❸ The PostgresOperator is a predefined airflow operator for interacting with
    postgres db.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: In code listing 9.1, we see the sample workflow DAG consists of multiple tasks,
    such as `create_table` and `save_into_db`. A task in Airflow is implemented as
    an operator. There are lots of predefined and community-managed operators, such
    as MySqlOperator, SimpleHttpOperator, and Docker operator.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 'Airflow’s predefined operators help users implement tasks without coding. You
    can also use the PythonOperator to run your customized Python functions. Once
    the workflow DAG is constructed and all the code is deployed to Airflow, we can
    use the UI or the following CLI command to check workflow execution status; see
    some sample shell commands as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Prints all active DAG
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Prints the list of tasks in the "data_process_dag" DAG
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Prints the hierarchy of tasks in the "data_process_dag" DAG
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: If you want to learn more about Airflow, you can check out its architecture
    overview doc and tutorials ([http://mng.bz/Blpw](http://mng.bz/Blpw)).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Key Features
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 'Airflow offers the following key features:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '*DAGs*—Airflow abstracts complex workflow using DAGs, and the workflow DAG
    is implemented through a Python library.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Programmatic workflow management*—Airflow supports creating tasks on the fly
    and allows the creation of complex dynamic workflows.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Great built-in operators to help build automation*—Airflow offers lots of
    predefined operators that help users achieve tasks without coding.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Solid task dependency and execution management*—Airflow has an auto-retry
    policy built into every task, and it provides different types of sensors to handle
    run-time dependencies, such as detecting task completion, workflow run status
    change, and file presence.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Extensibility*—Airflow makes its sensors, hooks, and operators fully extendable,
    which allows it to benefit from a large amount of community-contributed operators.
    Airflow can also be easily integrated into different systems by adding customized
    operators.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Monitoring and management interface*—Airflow provides a powerful UI so users
    can get a quick overview of workflow/task execution status and history. Users
    can also trigger and clear tasks or workflow runs from the UI.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Production quality*—Airflow provides many useful tools for maintaining the
    service in production environments, such as task log searching, scaling, alerting,
    and restful APIs.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limitations
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'Although Airflow is a great workflow orchestration, we still see several disadvantages
    when using it for deep learning scenarios:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '*High upfront cost for data scientists to onboard*—Airflow has a steep learning
    curve to achieve tasks that are not supported by the built-in operators. Also,
    there is no easy way to do workflow local testing.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*High friction when moving deep learning prototyping code to production*—When
    we apply Airflow to deep learning, data scientists have to convert their local
    model training code into Airflow DAG. This is extra work, and it’s an unpleasant
    experience for data scientists, especially when considering that this is avoidable
    if we build workflow DAG from the model training code directly.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*High complexity when operating on Kubernetes* —Deploying and operating Airflow
    on Kubernetes is not straightforward. If you are looking to adopt an orchestration
    system to run on Kubernetes, Argo Workflows is a better choice.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 9.3.2 Argo Workflows
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Argo Workflows is an open source, container-native workflow engine for orchestrating
    parallel workflows/tasks on Kubernetes. Argo Workflows solves the same problem
    that Airflow addresses but in a different way; it takes a Kubernetes-native approach.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: The biggest difference between Argo Workflows and Airflow is that Argo Workflows
    is built natively on Kubernetes. More specifically, the workflows and tasks in
    Argo Workflows are implemented as Kubernetes custom resource definition (CRD)
    objects, and each task (step) is executed as a Kubernetes pod. See figure 9.6
    for a high-level system overview.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/09-06.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 The workflow and its steps in Argo Workflows are executed as Kubernetes
    pods.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: In figure 9.6, Vena (the data scientist) first defines a workflow and its steps/tasks
    as a Kubernetes CRD object, which is usually presented as a YAML file. Then she
    submits the workflow to Argo Workflows, and its controller creates CRD objects
    inside the Kubernetes cluster. Next, Kubernetes pods are launched dynamically
    to run the workflow steps/tasks in the workflow sequence.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: You may also notice that each step’s execution is completely isolated by container
    and pod; each step uses files to present its input and output values. Argo Workflows
    will magically mount the dependent file into the step’s container.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: The task isolation created by the Kubernetes pod is a great advantage of Argo
    Workflows. Simplicity is also another reason people choose Argo Workflows. If
    you understand Kubernetes, Argo’s installation and troubleshooting are straightforward.
    We can use either Argo Workflows commands or the standard Kubernetes CLI commands
    to debug the system.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: A typical use case
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: For a better understanding, let’s look at an Argo Workflows example. In this
    section, we use Argo Workflows to automate the same data processing work we saw
    in the previous Airflow section. The workflow includes checking new data first,
    transforming the data, saving it to a new table in the database, and then notifying
    the data scientist team by Slack. See the following code listing for the Argo
    Workflows definition.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.2 A sample workflow for Argo Workflows with a series of steps
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Claims the CRD object type as workflow
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Declares the steps of the workflow
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: ❸ The step body is defined as another template, similar to a function.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Declares the data-paths artifact is from the new-data-paths artifact generated
    by the check-new-data step
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: ❺ This is how steps pass parameters.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: ❻ The actual step definition, similar to a function implementation
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Declares an output artifact (generates new-data-paths) for this step; the
    artifact is from /tmp/data_paths.txt, which can also be a directory.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Unpacks the data_paths input artifact and puts it at /tmp/raw_data/data_paths.txt
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: The most fundamental concepts in Argo Workflows are the workflow and template.
    A workflow object represents a single instance of a workflow; it contains the
    workflow’s definition and execution state. We should treat a workflow as a “live”
    object. A template can be thought of as *functions*; they define instructions
    to be executed. The `entrypoint` field defines what the main function will be,
    meaning the template that will be executed first.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'In code listing 9.2, we see a four-step sequential workflow: `check-new-data`
    -> `transform_data` -> `save-into-db` -> `notify-data-science-team`. Each step
    can reference a template, and steps pass parameters via artifacts (files). For
    example, the `check-new-data` references the `data-checker` template, which defines
    the Docker image for checking whether there is new data. The `data-checker` template
    also declares that the step output—the newly arrived data file path—will be saved
    to `/tmp/data_paths.txt` as its output value.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, the step `transform_data` binds the output of the `check-new-data` to
    the input of the data-converter template. This is how variables move around between
    steps and templates. Once you submit the workflow—for example, `argo` `submit`
    `-n` `argo` `sample_workflow.yaml`—you can either use the Argo Workflows UI or
    the following commands to review the details of the workflow run:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Besides using the `argo` command, we can also use the Kubernetes CLI command
    to check the workflow execution because Argo Workflows runs natively on Kubernetes;
    see the following example:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: To learn more about Argo Workflows, you can check out Argo Workflows user guide
    ([http://mng.bz/WAG0](http://mng.bz/WAG0)) and Argo Workflows architecture graph
    ([https://argoproj.github.io/argo-workflows/architecture](https://argoproj.github.io/argo-workflows/architecture)).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'Code Dockerization: Easy production deployment'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Argo Workflows is essentially a Kubernetes pod (Docker images) scheduling system.
    Although it forces people to write their code into a series of Docker images,
    it creates great flexibility and isolation inside the orchestration system. Because
    the code is in Docker form, it can be executed by any worker without worrying
    about configuring the worker environments.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Another advantage to Argo Workflows is its low cost of production deployment.
    When you test your code locally in Docker, the Docker image (prototyping code)
    can be used directly in Argo Workflows. Unlike Airflow, Argo Workflows has almost
    no conversion effort from prototyping code to production workflow.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Key Features
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'Argo Workflows offers the following key features:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '*Low cost of installation and maintenance*—Argo Workflows runs natively on
    Kubernetes, so you can just use the Kubernetes process to troubleshoot any problems;
    no need to learn other tools. Also, its installation is very straightforward;
    with a few `kubectl` commands, you can get Argo Workflows running in a Kubernetes
    environment.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Robust workflow execution*—The Kubernetes pod creates great isolation for
    Argo Workflows’ task execution. Argo Workflows also supports cron workflow and
    task retry.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Templating and composability*—Argo Workflows templates are like functions.
    When building a workflow, Argo Workflows supports composing different templates
    (step functions). This composability encourages sharing the common work across
    teams, thus greatly improving productivity.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Fully featured UI*—Argo Workflows offers a convenient UI to manage the entire
    life cycle of a workflow, such as submitting/stopping a workflow, listing all
    workflows, and viewing workflow definitions.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Highly flexible and applicable*—Argo Workflows defines REST APIs to manage
    the system and add new capabilities (plugins), and workflow tasks are defined
    as Docker images. These features make Argo Workflows highly customizable and used
    widely in many domains, such as ML, ETL, batch/data processing, and CI/CD (continuous
    integration and continuous delivery/continuous deployment).'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Production quality*—Argo Workflows is designed to run in a serious production
    environment. Kubeflow pipeline and Argo CD are great examples of productionizing
    Argo Workflows.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limitations
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'The downsides of using Argo Workflows in deep learning systems are as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '*Everyone will write and maintain YAML files*—Argo Workflows demands that the
    workflow is defined as a Kubernetes CRD in a YAML file. A short YAML file for
    a single project is manageable, but once the number of workflows starts increasing
    and workflow logic becomes more complex, the YAML file can become long and confusing.
    Argo Workflows offers templates to keep the workflow definition simple, but it’s
    still not very intuitive unless you are accustomed to working with Kubernetes
    YAML configurations.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Must be an expert on Kubernetes*—You will feel like it’s second nature if
    you are an expert on Kubernetes. But a novice user may need to spend quite some
    time learning Kubernetes concepts and practices.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Task execution latency*—In Argo Workflows, for every new task, Argo will launch
    a new Kubernetes pod to execute it. The pod launching can introduce seconds or
    minutes to every single task execution, which limits Argo when supporting time-sensitive
    workflows. For example, Argoflow is not a good fit for real-time model prediction
    workflow, which runs model prediction requests with millisecond SLAs.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 9.3.3 Metaflow
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Metaflow is a human-friendly Python library that focuses on MLOps. It was originally
    developed at Netflix and open-sourced in 2019\. Metaflow is special in that it
    follows a humancentric design; it’s not only built for automating workflows but
    also aims to reduce the human time (operational cost) spent in deep learning project
    development.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'In section 9.1.3, we pointed out that the conversion from prototyping code
    to production workflow generates a lot of friction in ML development. Data scientists
    have to construct and test a new version of the workflow for each model development
    iteration. To bridge the gap between prototyping and production, Metaflow made
    two improvements: first, it simplifies workflow construction, and second, it unifies
    the workflow execution experience between the local and production environments
    (see figure 9.7).'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/09-07.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 Metaflow offers a unified development experience between prototyping
    and production.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: In figure 9.7, we can see that Metaflow treats both prototyping and production
    environments as first-class execution environments. Because the Metaflow library
    offers a set of unified APIs to abstract the actual infrastructure, a workflow
    can be executed in the same way regardless of which environment it runs on. For
    example, a workflow can be run by both a local scheduler and a production scheduler
    without any change. The local scheduler executes workflows locally whereas the
    production scheduler integrates into other production orchestration systems, such
    as AWS Step Functions or Argo Workflows.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Metaflow lets users annotate a Python code—a DAG Python class—to define the
    workflow. The Metaflow library then creates/packages the workflow automatically
    from the Python annotations. With Metaflow Python annotation, Vena can build a
    workflow without changing any of her prototyping code.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Besides seamless workflow creation and testing, Metaflow offers other useful
    features that are key to model reproducibility, such as workflow/steps versioning
    and step input/output saving. To learn more about Metaflow, you can check out
    Metaflow’s official website ([https://docs.metaflow.org/](https://docs.metaflow.org/))
    and a great Metaflow book, *Effective Data Science Infrastructure*, written by
    Ville Tuulos (Manning, 2022; [https://www.manning.com/books/effective-data-science-infrastructure](https://www.manning.com/books/effective-data-science-infrastructure)).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: A typical use case
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use Metaflow to automate the same data process work we saw in sections
    9.3.1 and 9.3.2\. See the following listing for the pseudocode.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.3 A sample Metaflow workflow
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In code listing 9.3, we see Metaflow takes a novel approach to building a workflow
    by using code annotations. By annotating `@step` on the functions and using `self.next`
    function to connect steps, we can easily construct a workflow DAG (figure 9.8)
    from our prototyping code.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/09-08.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
- en: Figure 9.8 Workflow DAG constructed from listing 9.3
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: One of the beauties here is that we don’t have to define the workflow DAG in
    a separate system and repackage code to a different format, such as a Docker image.
    The Metaflow workflow is immersed in our code. Workflow development and prototyping
    code development happen at the same place and can be tested together from the
    very beginning to the end of the entire ML development cycle.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the code is ready, we can validate and run the workflow locally. See the
    following sample commands:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Once we finish local development and testing, it’s time to push the workflow
    to production, which can be achieved by the following two commands:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: These commands will export our data process workflow defined in code listing
    9.3 to AWS Step Functions and Argo Workflows. You can then also search for the
    flow by name within the AWS Step Functions UI or Argo Workflows UI and hence see
    the exported flow.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Note Metaflow offers a unified development experience between local and production
    environments. Thanks to the unified API provided by Metaflow, we have a seamless
    experience when testing our code and workflow locally and in production. Regardless
    of the backend workflow orchestration system used, whether Metaflow local scheduler,
    Argo Workflows, or AWS Step Functions, the Metaflow user experience on the workflow
    development remains the same!
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Key Features
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 'Metaflow offers the following key features:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '*Structures code as workflow*—Metaflow lets users create a workflow by annotating
    Python code, which greatly simplifies workflow construction.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Reproducibility*—Metaflow preserves an immutable snapshot of the data, code,
    and external dependencies required to execute each workflow step. Metaflow also
    records the metadata of each workflow execution.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Versioning*—Metaflow addresses the version control requirement of an ML project
    by hashing all the code and data in a workflow.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Robust workflow execution*—Metadata provides dependency management mechanisms
    at both the workflow level and step level by using the @conda decorator. It also
    offers task retries.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Usability design for ML*—Metaflow treats prototyping and production as equally
    important. It provides a set of unified APIs to abstract the infrastructure, so
    the same code can run in both the prototyping environment and the production environment
    without any changes.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Seamless scalability*—Metaflow integrates with Kubernetes and AWS Batch, which
    allows users to define the required computing resource easily, and can parallel
    the workflow steps over an arbitrarily large number of instances. For example,
    by applying an annotation like `@batch(cpu=1,` `memory=500)` to a step function,
    Metaflow will work with AWS Batch to allocate the required resource to compute
    this step.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limitations
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'The downsides of using Metaflow in deep learning systems are as follows:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '*No conditional branching support*—Metaflow step annotation doesn’t support
    conditional branching (only executing a step when a condition is met). This is
    not a red flag, but it’s a nice feature to have.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*No job scheduler*—Metaflow itself doesn’t come with a job scheduler, so you
    can’t use a cron workflow. This is not a big problem because Metaflow can integrate
    with other orchestration systems that support job scheduling, such as AWS Step
    Functions and Argo Workflows.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Tightly coupled to AWS*—the most important features of Metaflow are tightly
    coupled to AWS—for example, Amazon S3 and AWS Batch. Luckily, Metaflow is an open
    source project, so it’s possible to extend it to non-AWS alternatives.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 9.3.4 When to use
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you are looking for an orchestration system to automate workflow execution
    for non-ML projects, both Airflow and Argo Workflows are great choices. They have
    excellent community support and have been used widely in the IT industry. If your
    system runs on Kubernetes and your team feels comfortable working with Docker,
    then Argo Workflows would be a good fit; otherwise, Airflow won’t disappoint you.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: If you are looking for a system to streamline your ML project development, Metaflow
    is highly recommended. Metaflow is not just an orchestration tool; it’s an MLOps
    tool that focuses on saving data scientists’ time in the ML development cycle.
    Because Metaflow abstracts the backend infrastructure part of a ML project, data
    scientists can focus on model development without worrying about production conversion
    and deployment.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A workflow is a sequence of operations that are part of some larger task. A
    workflow can be viewed as a DAG of steps. A step is the smallest resumable unit
    of computation that describes what to do; a step either succeeds or fails as a
    whole. A DAG specifies the dependencies between steps and the order in which to
    execute them.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Workflow orchestration means executing the workflow steps based on the sequence
    defined in the workflow’s DAG.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adopting a workflow encourages work sharing, team collaboration, and automation.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main challenge of applying a workflow on deep learning projects is to reduce
    workflow construction costs and simplify workflow testing and debugging.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The six recommended design principles for building/evaluating workflow orchestration
    systems are criticality, usability, extensibility, task isolation, scalability,
    and human centricity.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When choosing an orchestration system for non-ML projects, both Airflow and
    Argo Workflows are great choices. Argo Workflows is a better option if the project
    runs on Kubernetes and Docker.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When selecting an orchestration system for ML projects, Metaflow is so far the
    best option.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
