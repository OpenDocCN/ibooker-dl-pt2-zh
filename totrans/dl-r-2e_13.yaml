- en: 10 Deep learning for time series
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 10 时间序列的深度学习
- en: '*This chapter covers*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 这一章涵盖了
- en: Examples of machine learning tasks that involve time-series data
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 涉及时间序列数据的机器学习任务的示例
- en: Understanding recurrent neural networks (RNNs)
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解递归神经网络(RNN)
- en: Applying RNNs to a temperature-forecasting example
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用RNNs到温度预测示例
- en: Advanced RNN usage patterns
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级RNN用法模式
- en: 10.1 Different kinds of time-series tasks
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.1 不同类型的时间序列任务
- en: A *time series* can be any data obtained via measurements at regular intervals,
    like the daily price of a stock, the hourly electricity consumption of a city,
    or the weekly sales of a store. Time series are everywhere, whether we’re looking
    at natural phenomena (like seismic activity, the evolution of fish populations
    in a river, or the weather at a location) or human activity patterns (like visitors
    to a website, a country’s GDP, or credit card transactions). Unlike the types
    of data you’ve encountered so far, working with time series involves understanding
    the *dynamics* of a system—its periodic cycles, how it trends over time, its regular
    regime, and its sudden spikes.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*时间序列*可以是通过定期测量获得的任何数据，例如股票的日价格，城市的小时用电量或商店的每周销售额。时间序列无处不在，无论我们是在研究自然现象（如地震活动，河流中鱼类种群的演变或位置的天气）还是人类活动模式（如访问网站的访客，一个国家的国内生产总值或信用卡交易）。与迄今为止遇到的数据类型不同，处理时间序列涉及对系统的*动态*进行理解
    ——其周期性循环，随时间推移的趋势，其正常情况以及突发的波动。'
- en: 'By far, the most common time-series-related task is *forecasting*: predicting
    what will happen next in a series; forecasting electricity consumption a few hours
    in advance so you can anticipate demand; forecasting revenue a few months in advance
    so you can plan your budget; forecasting the weather a few days in advance so
    you can plan your schedule. Forecasting is what this chapter focuses on. But there’s
    actually a wide range of other things you can do with time series:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，与时间序列相关的任务中最常见的是*预测*：预测系列中接下来会发生什么；提前几个小时预测电力消耗，以便您可以预测需求；提前几个月预测收入，以便您可以计划预算；提前几天预测天气，以便您可以安排日程。本章重点关注预测。但实际上，您可以对时间序列进行各种其他操作：
- en: '*Classification*—Assign one or more categorical labels to a time series. For
    instance, given the time series of the activity of a visitor on a website, classify
    whether the visitor is a bot or a human.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分类* - 为时间序列分配一个或多个分类标签。例如，给定网站访客活动的时间序列，分类访客是机器人还是人。'
- en: '*Event detection*—Identify the occurrence of a specific expected event within
    a continuous data stream. A particularly useful application is “hotword detection,”
    where a model monitors an audio stream and detects utterances like “OK, Google”
    or “Hey, Alexa.”'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*事件检测* - 在连续数据流中识别特定预期事件的发生。一个特别有用的应用是“热词检测”，其中模型监视音频流并检测出“OK，Google”或“Hey，Alexa”等话语。'
- en: '*Anomaly detection*—Detect anything unusual happening within a continuous datastream.
    Unusual activity on your corporate network? Might be an attacker. Unusual readings
    on a manufacturing line? Time for a human to go take a look. Anomaly detection
    is typically done via unsupervised learning, because you often don’t know what
    kind of anomaly you’re looking for, so you can’t train on specific anomaly examples.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*异常检测* - 检测连续数据流中发生的任何异常情况。企业网络上的异常活动？可能是攻击者。制造线上的异常读数？该找个人过去看一下了。异常检测通常通过无监督学习来完成，因为您通常不知道要寻找什么类型的异常情况，所以无法针对特定的异常情况进行训练。'
- en: When working with time series, you’ll encounter a wide range of domain-specific
    data-representation techniques. For instance, you have likely already heard about
    the *Fourier transform*, which consists of expressing a series of values in terms
    of a superposition of waves of different frequencies. The Fourier transform can
    be highly valuable when preprocessing any data that is primarily characterized
    by its cycles and oscillations (like sound, the vibrations of the frame of a skyscraper,
    or your brain waves). In the context of deep learning, Fourier analysis (or the
    related Mel-frequency analysis) and other domain-specific representations can
    be useful as a form of feature engineering, a way to prepare data before training
    a model on it, to make the job of the model easier. However, we won’t cover these
    techniques in these pages; we will instead focus on the modeling part.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理时间序列时，你会遇到各种领域特定的数据表示技术。例如，你可能已经听说过*傅立叶变换*，它包括将一系列值表达为不同频率的波的叠加。傅立叶变换在预处理任何主要由其周期和振荡特性（如声音、摩天大楼的振动或你的脑电波）表征的数据时非常有价值。在深度学习的背景下，傅立叶分析（或相关的梅尔频率分析）和其他领域特定的表示形式可以作为特征工程的一种形式，一种在对数据进行训练之前准备数据的方式，以使模型的工作变得更容易。然而，我们在这些页面上不会涵盖这些技术；我们将专注于建模部分。
- en: In this chapter, you’ll learn about recurrent neural networks (RNNs) and how
    to apply them to time-series forecasting.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习循环神经网络（RNNs）以及如何将它们应用于时间序列预测。
- en: 10.2 A temperature-forecasting example
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.2 温度预测示例
- en: 'Throughout this chapter, all of our code examples will target a single problem:
    predicting the temperature 24 hours in the future, given a time series of hourly
    measurements of quantities such as atmospheric pressure and humidity, recorded
    over the recent past by a set of sensors on the roof of a building. As you will
    see, it’s a fairly challenging problem!'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们所有的代码示例都将针对一个单一的问题：预测未来 24 小时的温度，给定建筑物屋顶上一组传感器记录的最近时期的大气压力和湿度等量的每小时测量的时间序列。正如你将看到的，这是一个相当具有挑战性的问题！
- en: We’ll use this temperature-forecasting task to highlight what makes time-series
    data fundamentally different from the kinds of datasets you’ve encountered so
    far. You’ll see that densely connected networks and convolutional networks aren’t
    well-equipped to deal with this kind of dataset, whereas a different kind of machine
    learning technique—recurrent neural networks (RNNs)—really shines on this type
    of problem.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这个温度预测任务来突显时间序列数据与你迄今为止遇到的数据集有着根本不同之处。你将看到，密集连接网络和卷积网络并不适合处理这种类型的数据集，而另一种不同的机器学习技术——循环神经网络（RNNs）在这种类型的问题上表现得非常出色。
- en: 'We’ll work with a weather time-series dataset recorded at the weather station
    at the Max Planck Institute for Biogeochemistry in Jena, Germany.^([1](#endnote1))
    In this dataset, 14 different quantities (such as temperature, pressure, humidity,
    and wind direction) were recorded every 10 minutes over several years. The original
    data goes back to 2003, but the subset of the data we’ll download is limited to
    2009–2016\. Let’s start by downloading and uncompressing the data:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用德国耶拿马克斯·普朗克生物地球化学研究所的气象站记录的天气时间序列数据集。^[1] 在这个数据集中，每隔 10 分钟记录了 14 种不同的数量（如温度、压力、湿度和风向）数年。原始数据追溯到
    2003 年，但我们将下载的数据子集限制在 2009-2016 年。让我们首先下载并解压数据：
- en: url <-
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 网址 <-
- en: '"https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip"'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '"https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip"'
- en: download.file(url, destfile = basename(url))
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: download.file(url, destfile = basename(url))
- en: zip::unzip(zipfile = "jena_climate_2009_2016.csv.zip",
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: zip::unzip(zipfile = "jena_climate_2009_2016.csv.zip",
- en: files = "jena_climate_2009_2016.csv")
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: files = "jena_climate_2009_2016.csv")
- en: Now let’s look at the data. We’ll use readr::read_csv() to read in the data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看看数据。我们将使用 readr::read_csv() 来读取数据。
- en: Listing 10.1 Inspecting the data of the Jena weather dataset
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.1 检查耶拿天气数据集的数据
- en: full_df <- readr::read_csv("jena_climate_2009_2016.csv")➊
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: full_df <- readr::read_csv("jena_climate_2009_2016.csv")➊
- en: ➊ **Note that you can also skip the zip::unzip() call above and pass the zip
    filepath directly to read_csv().**
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **请注意，你也可以跳过上面的 zip::unzip() 调用，并直接将 zip 文件路径传递给 read_csv()。**
- en: 'This outputs a data.frame with 420,451 rows and 15 columns. Each row is a time
    step: a record of a date and 14 weather-related values.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出一个包含 420,451 行和 15 列的数据框。每一行都是一个时间步长：记录了日期和 14 个与天气相关的值。
- en: full_df
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: full_df
- en: '![Image](../images/f0303-01.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0303-01.jpg)'
- en: 'read_csv() parsed all the columns correctly as numeric vectors, except for
    the “Date Time” column, which it parsed as a character vector instead of as a
    date-time vector. We won’t be training on the Date Time column, so this is not
    an issue, but just for completeness, we can convert the character column to an
    R POSIXct format. Note that we pass tz = “Etc/GMT+1” instead of tz = “Europe/Berlin”,
    because the timestamps in the dataset do not adjust for Central European Summer
    Time (also known as Daylight Saving Time), but are instead always at Central European
    Time:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: read_csv() 正确解析了所有列为数值向量，除了“Date Time”列，它解析为字符向量而不是日期时间向量。我们不会在“Date Time”列上进行训练，所以这不是问题，但为了完整起见，我们可以将字符列转换为
    R POSIXct 格式。请注意，我们传递的时区为“Etc/GMT+1”，而不是“Europe/Berlin”，因为数据集中的时间戳不调整为中欧夏令时间（也称为夏时制），而是始终处于中欧时间：
- en: full_df$`Date Time` %<>%
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '`full_df$`Date Time` %<>%`'
- en: as.POSIXct(tz = "Etc/GMT+1", format = "%d.%m.%Y %H:%M:%S")
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`as.POSIXct(tz = "Etc/GMT+1", format = "%d.%m.%Y %H:%M:%S")`'
- en: '**The %<>% assignment pipe**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**%<>% 分配管道**'
- en: 'In the previous example, we use the assignment pipe for the first time. x %<>%
    fn() is shorthand for x <- x %>% fn(). It is useful because it allows you to write
    more readable code and avoid repeating the same variable name multiple times.
    We could also have written this to achieve the same outcome:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们第一次使用了分配管道。x %<>% fn() 是 x <- x %>% fn() 的简写。这是有用的，因为它使您能够编写更易读的代码，避免多次重复使用相同的变量名。我们也可以这样写来达到相同的效果：
- en: full_df$`Date Time` <- full_df$`Date Time` %>%
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '`full_df$`Date Time` <- full_df$`Date Time` %>%`'
- en: as.POSIXct(tz = "Etc/GMT+1", format = "%d.%m.%Y %H:%M:%S")
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`as.POSIXct(tz = "Etc/GMT+1", format = "%d.%m.%Y %H:%M:%S")`'
- en: The assignment pipe is made available by calling library(keras).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用 library(keras)，我们可以使用分配管道。
- en: '[Figure 10.1](#fig10-1) shows the plot of temperature (in degrees Celsius)
    over time. On this plot, you can clearly see the yearly periodicity of temperature—the
    data spans 8 years.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10.1](#fig10-1) 显示了温度（摄氏度）随时间变化的曲线图。在这张图上，你可以清楚地看到温度的年周期性 —— 数据跨越了 8 年。'
- en: Listing 10.2 Plotting the temperature time series
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.2 绘制温度时间序列
- en: plot(`T (degC)` ~ `Date Time`, data = full_df, pch = 20, cex = .3)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`plot(`T (degC)` ~ `Date Time`, data = full_df, pch = 20, cex = .3)`'
- en: '![Image](../images/f0304-01.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0304-01.jpg)'
- en: '**Figure 10.1 Temperature over the full temporal range of the dataset (°C)**'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 10.1 数据集的完整时间范围内的温度（°C）**'
- en: '[Figure 10.2](#fig10-2) shows a more narrow plot of the first 10 days of temperature
    data. Because the data is recorded every 10 minutes, you get 24 × 6 = 144 data
    points per day.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10.2](#fig10-2) 显示了温度数据的前 10 天的更窄的曲线图。因为数据每 10 分钟记录一次，所以每天有 24 × 6 = 144
    个数据点。'
- en: Listing 10.3 Plotting the first 10 days of the temperature time series
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.3 绘制温度时间序列的前 10 天
- en: plot(`T (degC)` ~ `Date Time`, data = full_df[1:1440, ])
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`plot(`T (degC)` ~ `Date Time`, data = full_df[1:1440, ])`'
- en: '![Image](../images/f0305-01.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0305-01.jpg)'
- en: '**Figure 10.2 Temperature over the first 10 days of the dataset (°C)**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 10.2 数据集的前 10 天内的温度（°C）**'
- en: On this plot, you can see daily periodicity, especially for the last four days.
    Also note that this 10-day period is coming from a fairly cold winter month.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这张图上，你可以看到每日周期性，尤其是最后四天。还要注意，这 10 天的周期来自一个相当寒冷的冬季月份。
- en: '**Always look for periodicity in your data**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**始终寻找数据中的周期性**'
- en: Periodicity over multiple time scales is an important and very common property
    of time-series data. Whether you’re looking at the weather, mall parking occupancy,
    traffic to a website, sales of a grocery store, or steps logged in a fitness tracker,
    you’ll see daily cycles and yearly cycles (human-generated data also tends to
    feature weekly cycles). When exploring your data, make sure to look for these
    patterns.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 多个时间尺度上的周期性是时间序列数据的一个重要且非常普遍的特性。无论你是在观察天气、购物中心停车位占用情况、网站流量、杂货店销售情况还是健身追踪器中记录的步数，你都会看到每日周期和年周期（人类生成的数据也倾向于具有每周周期）。在探索数据时，请务必寻找这些模式。
- en: With our dataset, if you were trying to predict average temperature for the
    next month given a few months of past data, the problem would be easy, due to
    the reliable year-scale periodicity of the data. But looking at the data over
    a scale of days, the temperature looks a lot more chaotic. Is this time series
    predictable at a daily scale? Let’s find out.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 通过我们的数据集，如果你试图根据过去几个月的数据预测下个月的平均温度，那问题将会很容易，因为数据具有可靠的年尺度周期性。但是如果以天为单位查看数据，温度看起来就更加混乱。在每天的尺度上，这个时间序列是否可预测？让我们来看看。
- en: In all our experiments, we’ll use the first 50% of the data for training, the
    following 25% for validation, and the last 25% for testing. When working with
    time-series data, it’s important to use validation and test data that is more
    recent than the training data, because you’re trying to predict the future given
    the past, not the reverse, and your validation/test splits should reflect that.
    Some problems happen to be considerably simpler if you reverse the time axis!
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们所有的实验中，我们将使用数据的前50%进行训练，接下来的25%用于验证，最后的25%用于测试。当处理时间序列数据时，使用比训练数据更近期的验证和测试数据很重要，因为你试图根据过去来预测未来，而不是相反，你的验证/测试拆分应该反映这一点。如果你颠倒时间轴，有些问题会变得更简单！
- en: Listing 10.4 Computing the number of samples we’ll use for each data split
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.4 计算我们将用于每个数据拆分的样本数
- en: num_train_samples <- round(nrow(full_df) * .5) num_val_samples <- round(nrow(full_df)
    * 0.25) num_test_samples <- nrow(full_df) - num_train_samples - num_val_samples
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: num_train_samples <- round(nrow(full_df) * .5) num_val_samples <- round(nrow(full_df)
    * 0.25) num_test_samples <- nrow(full_df) - num_train_samples - num_val_samples
- en: train_df <- full_df[seq(num_train_samples), ]➊
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: train_df <- full_df[seq(num_train_samples), ]➊
- en: val_df <- full_df[seq(from = nrow(train_df) + 1, length.out = num_val_samples),
    ]➋
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: val_df <- full_df[seq(from = nrow(train_df) + 1, length.out = num_val_samples),
    ]➋
- en: test_df <- full_df[seq(to = nrow(full_df), length.out = num_test_samples), ]➌
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: test_df <- full_df[seq(to = nrow(full_df), length.out = num_test_samples), ]➌
- en: cat("num_train_samples:", nrow(train_df), "\n")
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: cat("num_train_samples:", nrow(train_df), "\n")
- en: cat("num_val_samples:", nrow(val_df), "\n")
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: cat("num_val_samples:", nrow(val_df), "\n")
- en: cat("num_test_samples:", nrow(test_df), "\n")
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: cat("num_test_samples:", nrow(test_df), "\n")
- en: 'num_train_samples: 210226'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 'num_train_samples: 210226'
- en: 'num_val_samples: 105113'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 'num_val_samples: 105113'
- en: 'num_test_samples: 105112'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 'num_test_samples: 105112'
- en: ➊ **First 50% of rows, 1:210226**
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **前50%的行，1:210226**
- en: ➋ **Next 25% of rows, 210227:315339**
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **接下来的25%的行，210227:315339**
- en: ➌ **Last 25% of rows, 315340:420451**
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **最后25%的行，315340:420451**
- en: 10.2.1 Preparing the data
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.1 准备数据
- en: 'The exact formulation of the problem will be as follows: given data covering
    the previous five days and sampled once per hour, can we predict the temperature
    in 24 hours?'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 问题的确切表述如下：给定覆盖前五天的数据，每小时采样一次，我们能否在24小时内预测温度？
- en: 'First, let’s preprocess the data to a format a neural network can ingest. This
    is easy: the data is already numerical, so you don’t need to do any vectorization.
    But each time series in the data is on a different scale (e.g., atmospheric pressure,
    measured in mbar, is around 1,000, whereas H2OC, measured in millimoles per mole,
    is around 3). We’ll normalize each time series (column) independently so that
    they all take small values on a similar scale. We’re going to use the first 210,226
    time steps as training data, so we’ll compute the mean and standard deviation
    only on this fraction of the data.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们将数据预处理为神经网络可以摄入的格式。这很简单：数据已经是数值型的，所以您不需要进行任何向量化。但是，数据中的每个时间序列都在不同的尺度上（例如，大气压力以mbar为单位，约为1000，而H2OC以毫摩尔/摩尔为单位，约为3）。我们将独立地对每个时间序列（列）进行归一化，以使它们都以类似尺度的小值。我们将使用前
    210,226 个时间步作为训练数据，因此我们只会在数据的这一部分计算均值和标准差。
- en: Listing 10.5 Normalizing the data
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.5 数据归一化
- en: input_data_colnames <- names(full_df) %>% setdiff(c("Date Time"))➊
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: input_data_colnames <- names(full_df) %>% setdiff(c("Date Time"))➊
- en: normalization_values <-
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: normalization_values <-
- en: zip_lists(mean = lapply(train_df[input_data_colnames], mean),
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: zip_lists(mean = lapply(train_df[input_data_colnames], mean),
- en: sd = lapply(train_df[input_data_colnames], sd))➋
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: sd = lapply(train_df[input_data_colnames], sd))➋
- en: str(normalization_values)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: str(normalization_values)
- en: List of 14
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一个包含 14 个元素的列表
- en: $ p (mbar)       :List of 2
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: $ p (mbar)       :List of 2
- en: '..$ mean: num 989'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ..$ 均值：num 989
- en: '..$ sd : num 8.51'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ..$ 标准差：num 8.51
- en: $ T (degC)       :List of 2
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: $ T (degC)       :List of 2
- en: '..$ mean: num 8.83'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ..$ 均值：num 8.83
- en: '..$ sd : num 8.77'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ..$ 标准差：num 8.77
- en: $ Tpot (K)       :List of 2
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: $ Tpot (K)       :List of 2
- en: '..$ mean: num 283'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ..$ 均值：num 283
- en: '..$ sd : num 8.87'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ..$ 标准差：num 8.87
- en: $ Tdew (degC)    :List of 2
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: $ Tdew (degC)    :List of 2
- en: '..$ mean: num 4.31'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ..$ 均值：num 4.31
- en: '..$ sd : num 7.08'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ..$ 标准差：num 7.08
- en: $ rh (%) :List of 2
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: $ rh (%) :List of 2
- en: '..$ mean: num 75.9'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ..$ 均值：num 75.9
- en: '..$ sd : num 16.6'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ..$ 标准差：num 16.6
- en: $ VPmax (mbar)   :List of 2
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: $ VPmax (mbar)   :List of 2
- en: '..$ mean: num 13.1'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ..$ 均值：num 13.1
- en: '..$ sd : num 7.6'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ..$ 标准差：num 7.6
- en: $ VPact (mbar)   :List of 2
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: $ VPact (mbar)   :List of 2
- en: '..$ mean: num 9.19'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ..$ 均值：num 9.19
- en: '..$ sd : num 4.15'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ..$ 标准差：num 4.15
- en: $ VPdef (mbar)    :List of 2
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: $ VPdef (mbar)    :List of 2
- en: '..$ mean: num 3.95'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ..$ 均值：num 3.95
- en: '..$ sd : num 4.77'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ..$ 标准差：num 4.77
- en: '[list output truncated]'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[list output truncated]'
- en: normalize_input_data <- function(df) {
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: normalize_input_data <- function(df) {
- en: normalize <- function(x, center, scale)➌
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: normalize <- function(x, center, scale)➌
- en: (x - center) / scale
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: (x - center) / scale
- en: for(col_nm in input_data_colnames) {
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: for(col_nm in input_data_colnames) {
- en: col_nv <- normalization_values[[col_nm]]
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: col_nv <- normalization_values[[col_nm]]
- en: df[[col_nm]] %<>% normalize(., col_nv$mean, col_nv$sd)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: df[[col_nm]] %<>% normalize(., col_nv$mean, col_nv$sd)
- en: '}'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: df
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: df
- en: '}'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ **Our model input will be all the columns except for the Date Time column.**
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **我们的模型输入将是除了日期时间列之外的所有列。**
- en: ➋ **We compute the normalization values using only the training data.**
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **我们仅使用训练数据来计算归一化值。**
- en: '➌**You can also call scale(col, center = train_col_mean, scale = train_col_sd)
    instead, but for maximum clarity we define a local function: normalize().**'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ➌**你也可以调用scale(col, center = train_col_mean, scale = train_col_sd)，但为了最大的清晰度，我们定义了一个本地函数：normalize()。**
- en: Next, let’s create a TF Dataset object that yields batches of data from the
    past five days along with a target temperature 24 hours in the future. Because
    the samples in the dataset are highly redundant (sample *N* and sample *N* + 1
    will have most of their time steps in common), it would be wasteful to explicitly
    allocate memory for every sample. Instead, we’ll generate the samples on the fly
    while only keeping in memory the original data arrays, and nothing more.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们创建一个TF Dataset对象，从过去五天的数据中产生数据批次，并提供未来24小时的目标温度。由于数据集中的样本高度冗余（样本*N*和样本*N*
    + 1将具有大部分时间步骤相同），为每个样本显式分配内存将是浪费的。相反，我们将在需要时动态生成样本，只保留原始数据数组，什么都不多余。
- en: We could easily write an R function to do this, but there’s a built-in dataset
    utility in Keras that does just that—(timeseries_dataset_from_array())—so we can
    save ourselves some work by using it. You can generally use it for any kind of
    time-series forecasting task.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以很容易地编写一个R函数来做到这一点，但是Keras中有一个内置的数据集实用程序(timeseries_dataset_from_array())可以做到这一点，所以我们可以通过使用它来节省一些工作。你通常可以将其用于任何类型的时间序列预测任务。
- en: '**Understanding timeseries_dataset_from_array()**'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**理解timeseries_dataset_from_array()**'
- en: To understand what timeseries_dataset_from_array() does, let’s look at a simple
    example. The general idea is that you provide an array of time-series data (the
    data argument), and timeseries_dataset_from_array() gives you windows extracted
    from the original time series (we’ll call them “sequences”).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解timeseries_dataset_from_array()的作用，让我们看一个简单的例子。总的思路是，你提供一个时间序列数据的数组（数据参数），timeseries_dataset_from_array()会给你从原始时间序列中提取的窗口（我们称之为“序列”）。
- en: 'For example, if you use data = [0 1 2 3 4 5 6] and sequence_length = 3, then
    timeseries_dataset_from_array() will generate the following samples: [0 1 2],
    [1 2 3] , [2 3 4], [3 4 5], [4 5 6].'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你使用data = [0 1 2 3 4 5 6]和sequence_length = 3，那么timeseries_dataset_from_array()将生成以下样本：[0
    1 2]，[1 2 3]，[2 3 4]，[3 4 5]，[4 5 6]。
- en: You can also pass a targets argument (an array) to timeseries_dataset_ from_array().
    The first entry of the targets array should match the desired target for the first
    sequence that will be generated from the data array. So, if you’re doing time-series
    forecasting, targets should be the same array as data, offset by some amount.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以向timeseries_dataset_from_array()传递一个targets参数（一个数组）。targets数组的第一个条目应该与将从数据数组生成的第一个序列的所需目标相匹配。因此，如果你正在进行时间序列预测，则targets应该是与数据相同的数组，偏移了一些量。
- en: 'For instance, with data = [0 1 2 3 4 5 6 …] and sequence_length = 3, you could
    create a dataset to predict the next step in the series by passing targets = [3
    4 5 6 …]. Let’s try it:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果data = [0 1 2 3 4 5 6 ...]和sequence_length = 3，你可以通过传递targets = [3 4 5
    6 ...]来创建一个数据集，以预测系列中的下一步。让我们试试：
- en: library (keras)
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: library (keras)
- en: int_sequence <- seq(10) ➊
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: int_sequence <- seq(10) ➊
- en: dummy_dataset <- timeseries_dataset_from_array(
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: dummy_dataset <- timeseries_dataset_from_array(
- en: data = head(int_sequence, -3), ➋
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: data = head(int_sequence, -3), ➋
- en: targets = tail(int_sequence, -3), ➌
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: targets = tail(int_sequence, -3), ➌
- en: sequence_length = 3, ➍
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: sequence_length = 3, ➍
- en: batch_size = 2 ➎
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: batch_size = 2 ➎
- en: )
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: library(tfdatasets)
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: library(tfdatasets)
- en: dummy_dataset_iterator <- as_array_iterator(dummy_dataset)
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: dummy_dataset_iterator <- as_array_iterator(dummy_dataset)
- en: repeat {
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: repeat {
- en: batch <- iter_next(dummy_dataset_iterator)
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: batch <- iter_next(dummy_dataset_iterator)
- en: if (is.null(batch)) ➏
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果（is.null(batch)）➏
- en: break
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: break
- en: c(inputs, targets) %<-% batch
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: c(inputs, targets) %<-% batch
- en: for (r in 1:nrow(inputs))
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: for (r in 1:nrow(inputs))
- en: 'cat(sprintf("input: [ %s ] target: %s\n", paste(inputs[r, ], collapse = " "),
    targets[r]))'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 'cat(sprintf("input: [ %s ] target: %s\n", paste(inputs[r, ], collapse = " "),
    targets[r]))'
- en: cat(strrep("-", 27), "\n") ➐
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: cat(strrep("-", 27), "\n") ➐
- en: '}'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ **Generate an array of sorted integers from 1 to 10.**
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **生成一个从1到10排序的整数数组。**
- en: ➋ **The sequences we generate will be sampled from [1 2 3 4 5 6 7] (drop last
    3).**
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **我们生成的序列将从[1 2 3 4 5 6 7]中采样（去除最后3个）。**
- en: ➌ **The target for the sequence that starts at data[N] will be data[N + 4] (tail
    drops first 3).**
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **从数据[N]开始的序列的目标将是数据[N + 4]（尾部去除前3个）。**
- en: ➍ **The sequences will be three steps long.**
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **序列将为三个步长。**
- en: ➎ **The sequences will be batched in batches of size 2.**
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **序列将以大小为 2 的批次分组。**
- en: ➏ **The iterator is exhausted.**
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ➏ **迭代器已经耗尽。**
- en: ➐ **Demark batches.**
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ➐ **标记批次。**
- en: 'This bit of code prints the following results:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码打印以下结果：
- en: 'input: [ 1 2 3 ] target: 4'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 'input: [ 1 2 3 ] target: 4'
- en: 'input: [ 2 3 4 ] target: 5'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 'input: [ 2 3 4 ] target: 5'
- en: '-------------------------------'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '-------------------------------'
- en: 'input: [ 3 4 5 ] target: 6'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 'input: [ 3 4 5 ] target: 6'
- en: 'input: [ 4 5 6 ] target: 7'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 'input: [ 4 5 6 ] target: 7'
- en: '-------------------------------'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '-------------------------------'
- en: 'input: [ 5 6 7 ] target: 8'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 'input: [ 5 6 7 ] target: 8'
- en: '-------------------------------'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '-------------------------------'
- en: 'We’ll use timeseries_dataset_from_array() to instantiate three datasets: one
    for training, one for validation, and one for testing. We’ll use the following
    parameter values:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 timeseries_dataset_from_array() 实例化三个数据集：一个用于训练，一个用于验证，一个用于测试。我们将使用以下参数值：
- en: 'sampling_rate = 6—Observations will be sampled at one data point per hour:
    we will keep only one data point out of 6.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: sampling_rate = 6—观察将每小时采样一次：我们将每 6 个数据点中保留一个数据点。
- en: sequence_length = 120—Observations will go back five days (120 hours).
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: sequence_length = 120—观察将回溯五天（120 小时）。
- en: delay = sampling_rate * (sequence_length + 24 - 1)—The target for a sequence
    will be the temperature 24 hours after the end of the sequence.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: delay = sampling_rate * (sequence_length + 24 - 1)—序列的目标将是序列结束后 24 小时的温度。
- en: Listing 10.6 Instantiating datasets for training, validation, and testing
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Listing 10.6 实例化用于训练、验证和测试的数据集
- en: sampling_rate <- 6
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: sampling_rate <- 6
- en: sequence_length <- 120
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: sequence_length <- 120
- en: delay <- sampling_rate * (sequence_length + 24 - 1)
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: delay <- sampling_rate * (sequence_length + 24 - 1)
- en: batch_size <- 256
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: batch_size <- 256
- en: df_to_inputs_and_targets <- function(df) {
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: df_to_inputs_and_targets <- function(df) {
- en: inputs <- df[input_data_colnames] %>%
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: inputs <- df[input_data_colnames] %>%
- en: normalize_input_data() %>%
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: normalize_input_data() %>%
- en: as.matrix() ➊
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: as.matrix() ➊
- en: targets <- as.array(df$`T (degC)`) ➋
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: targets <- as.array(df$`T (degC)`) ➋
- en: list (
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: list (
- en: head(inputs, -delay), ➌
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: head(inputs, -delay), ➌
- en: tail(targets, -delay) ➍
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: tail(targets, -delay) ➍
- en: )
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: '}'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: make_dataset <- function(df)
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: make_dataset <- function(df)
- en: '{ c(inputs, targets) %<-% df_to_inputs_and_targets(df)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '{ c(inputs, targets) %<-% df_to_inputs_and_targets(df)'
- en: timeseries_dataset_from_array(
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: timeseries_dataset_from_array(
- en: inputs, targets,
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: inputs, targets,
- en: sampling_rate = sampling_rate,
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: sampling_rate = sampling_rate,
- en: sequence_length = sequence_length,
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: sequence_length = sequence_length,
- en: shuffle = TRUE,
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: shuffle = TRUE,
- en: batch_size = batch_size
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: batch_size = batch_size
- en: )
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: '}'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: train_dataset <- make_dataset(train_df)
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: train_dataset <- make_dataset(train_df)
- en: val_dataset <- make_dataset(val_df)
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: val_dataset <- make_dataset(val_df)
- en: test_dataset <- make_dataset(test_df)
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: test_dataset <- make_dataset(test_df)
- en: ➊ **Convert data.frame to a numeric array.**
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **将数据框转换为数值数组。**
- en: ➋ **We don't normalize the targets.**
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **我们不对目标进行标准化。**
- en: ➌ **Drop the last delay samples.**
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **丢弃最后 delay 个样本。**
- en: ➍ **Drop the first delay samples.**
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **丢弃前 delay 个样本。**
- en: Each dataset yields batches as a pair of (samples, targets), where samples is
    a batch of 256 samples, each containing 120 consecutive hours of input data, and
    targets is the corresponding array of 256 target temperatures. Note that the samples
    are randomly shuffled, so two consecutive sequences in a batch (like samples[1,
    ] and samples[2, ]) aren’t necessarily temporally close.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 每个数据集产生一批（samples，targets）作为一对，其中 samples 是一批包含 256 个样本的数据，每个样本包含 120 个连续小时的输入数据，而
    targets 是相应的 256 个目标温度数组。请注意，样本是随机洗牌的，因此批次中的两个连续序列（如 samples[1, ] 和 samples[2,
    ]）不一定在时间上相邻。
- en: Listing 10.7 Inspecting the output of one of our datasets
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: Listing 10.7 检查我们数据集之一的输出
- en: c(samples, targets) %<-% iter_next(as_iterator(train_dataset))
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: c(samples, targets) %<-% iter_next(as_iterator(train_dataset))
- en: 'cat("samples shape: ", format(samples$shape), "\n",'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: cat("样本形状：", format(samples$shape), "\n",
- en: '"targets shape: ", format(targets$shape), "\n", sep = "")'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '"目标形状：", format(targets$shape), "\n", sep = "")'
- en: 'samples shape: (256, 120, 14)'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 'samples shape: (256, 120, 14)'
- en: 'targets shape: (256)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 'targets shape: (256)'
- en: 10.2.2 A common-sense, non–machine learning baseline
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.2 一个常识性的、非机器学习的基准
- en: Before we start using black-box deep learning models to solve the temperature-prediction
    problem, let’s try a simple, common-sense approach. It will serve as a sanity
    check, and it will establish a baseline that we’ll have to beat to demonstrate
    the usefulness of more-advanced machine learning models. Such common-sense baselines
    can be useful when you’re approaching a new problem for which there is no known
    solution (yet). A classic example is that of unbalanced classification tasks,
    where some classes are much more common than others. If your dataset contains
    90% instances of class A and 10% instances of class B, then a common-sense approach
    to the classification task is to always predict “A” when presented with a new
    sample. Such a classifier is 90% accurate overall, and any learning-based approach
    should therefore beat this 90% score to demonstrate usefulness. Sometimes, such
    elementary baselines can prove surprisingly hard to beat.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始使用黑盒深度学习模型来解决温度预测问题之前，让我们尝试一种简单的常识方法。这将作为一个理智的检查，并且它将建立一个我们将不得不击败以证明更高级机器学习模型的有用性的基线。当你面对一个尚无已知解决方案的新问题时，这种常识基线可能会很有用。一个经典的例子是不平衡分类任务，其中一些类别比其他类别更常见。如果你的数据集包含
    90% 的 A 类实例和 10% 的 B 类实例，那么对分类任务的一种常识方法是当提供一个新样本时总是预测为“A”。这样的分类器在整体上准确率为 90%，因此任何基于学习的方法都应该超过这个
    90% 的分数以证明其有用性。有时，这样的基本基线可能会出人意料地难以超越。
- en: 'In this case, the temperature time series can safely be assumed to be continuous
    (the temperatures tomorrow are likely to be close to the temperatures today) as
    well as periodical with a daily period. Thus a common-sense approach is to always
    predict that the temperature 24 hours from now will be equal to the temperature
    right now. Let’s evaluate this approach, using the mean absolute error (MAE) metric,
    defined as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，温度时间序列可以安全地假定是连续的（明天的温度很可能接近今天的温度），并且具有每日周期。因此，一个常识性的方法是始终预测 24 小时后的温度将等于现在的温度。让我们使用平均绝对误差（MAE）指标来评估这种方法，其定义如下：
- en: mean(abs(preds - targets))
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '`mean(abs(preds - targets))`'
- en: Here’s the evaluation code. Rather than evaluating it all eagerly in R using
    for, as_ array_iterator(), and iter_next(), we can just as easily do it with TF
    Dataset transformations. First we call dataset_unbatch() so that each dataset
    element becomes a single case of (samples, target). Next we use dataset_map()
    to calculate the absolute error for each pair of (samples, target), and then dataset_reduce()
    to accumulate the total error and total samples seen.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这是评估代码。我们不再使用 R 中的 for、as_array_iterator() 和 iter_next() 来急切地进行评估，而是可以很容易地使用
    TF Dataset 转换来完成。首先我们调用 dataset_unbatch()，使每个数据集元素成为 (samples, target) 的单个案例。接下来我们使用
    dataset_map() 计算每对 (samples, target) 的绝对误差，然后使用 dataset_reduce() 累积总误差和总样本数。
- en: Recall that functions passed to dataset_map() and dataset_reduce() will be called
    with symoblic tensors. Slicing a tensor with a negative number like samples[-1,
    ] selects the last slice along that axis, as if we had written samples[nrow(samples),
    ].
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，传递给 `dataset_map()` 和 `dataset_reduce()` 的函数将使用符号张量调用。使用负数进行张量切片，如 `samples[-1,
    ]` 会选择沿该轴的最后一个切片，就好像我们写成了 `samples[nrow(samples), ]` 一样。
- en: Listing 10.8 Computing the common-sense baseline MAE
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '`清单 10.8 计算常识基线 MAE`'
- en: evaluate_naive_method <- function(dataset) {
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '`evaluate_naive_method <- function(dataset) {`'
- en: unnormalize_temperature <- function(x) {
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '`unnormalize_temperature <- function(x) {`'
- en: nv <- normalization_values$`T (degC)`
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '`nv <- normalization_values$''T (degC)''`'
- en: (x * nv$sd) + nv$mean
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '`(x * nv$sd) + nv$mean`'
- en: '}'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '`}`'
- en: temp_col_idx <- match("T (degC)", input_data_colnames) ➊
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '`temp_col_idx <- match("T (degC)", input_data_colnames) ➊`'
- en: reduction <-dataset %>%
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '`reduction <- dataset %>%`'
- en: dataset_unbatch() %>%
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '`%>% dataset_unbatch() %>%`'
- en: dataset_map(function(samples, target) {
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '`dataset_map(function(samples, target) {`'
- en: last_temp_in_input <- samples[-1, temp_col_idx] ➋
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '`last_temp_in_input <- samples[-1, temp_col_idx] ➋`'
- en: pred <- unnormalize_temperature(last_temp_in_input) ➌
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '`pred <- unnormalize_temperature(last_temp_in_input) ➌`'
- en: abs(pred - target)
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '`abs(pred - target)`'
- en: '}) %>%'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '`}) %>%`'
- en: dataset_reduce(
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '`dataset_reduce(`'
- en: initial_state = list(total_samples_seen = 0L, total_abs_error = 0),
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '`initial_state = list(total_samples_seen = 0L, total_abs_error = 0),`'
- en: reduce_func = function(state, element) {
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '`reduce_func = function(state, element) {`'
- en: state$total_samples_seen %<>% `+`(1L)
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '`state$total_samples_seen %<>% ''+''(1L)`'
- en: state$total_abs_error %<>% `+`(element)
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '`state$total_abs_error %<>% ''+''(element)`'
- en: state
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '`state`'
- en: '}'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '`}`'
- en: ) %>%
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '`) %>%`'
- en: lapply(as.numeric) ➍
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '`lapply(as.numeric) ➍`'
- en: mae <- with(reduction, total_abs_error / total_samples_seen) ➎
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '`mae <- with(reduction, total_abs_error / total_samples_seen) ➎`'
- en: mae
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '`mae`'
- en: '}'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '`}`'
- en: 'sprintf("Validation MAE: %.2f", evaluate_naive_method(val_dataset))'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '`sprintf("验证 MAE: %.2f", evaluate_naive_method(val_dataset))`'
- en: 'sprintf("Test MAE: %.2f", evaluate_naive_method(test_dataset))'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 'sprintf("测试 MAE: %.2f", evaluate_naive_method(test_dataset))'
- en: '[1] "Validation MAE: 2.43"'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] "验证 MAE: 2.43"'
- en: '[1] "Test MAE: 2.62"'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] "测试 MAE: 2.62"'
- en: ➊ **2, the second column**
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **2，第二列**
- en: ➋ **Slice out the last temperature measurement in the input sequence.**
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **在输入序列中切出最后一个温度测量值。**
- en: ➌ **Recall that we normalized our features, so to retrieve a temperature in
    degrees Celsius, we need to unnormalize it by multiplying it by the standard deviation
    and adding back the mean.**
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **回忆一下，我们对特征进行了归一化，因此要得到以摄氏度为单位的温度，需要将其除以标准差，然后加上均值。**
- en: ➍ **Convert Tensors to R numerics.**
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **将张量转换为 R 数值。**
- en: ➎ **reduction is a named list of two R scalar numbers.**
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **reduction 是一个包含两个 R 标量数字的命名列表。**
- en: This common-sense baseline achieves a validation MAE of 2.44 degrees Celsius
    and a test MAE of 2.62 degrees Celsius. So if you always assume that the temperature
    24 hours in the future will be the same as it is now, you will be off by two and
    a half degrees on average. It’s not too bad, but you probably won’t launch a weather
    forecasting service based on this heuristic. Now the game is to use your knowledge
    of deep learning to do better.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这种常识基线实现了 2.44 摄氏度的验证 MAE 和 2.62 摄氏度的测试 MAE。因此，如果你总是假设未来 24 小时的温度与当前温度相同，平均偏差将达到两个半度。这还算可以，但你可能不会基于这种启发式方法启动气象预报服务。现在的问题是要利用你对深度学习的知识做得更好。
- en: 10.2.3 Let’s try a basic machine learning model
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.3 让我们尝试基本的机器学习模型
- en: In the same way that it’s useful to establish a common-sense baseline before
    trying machine learning approaches, it’s useful to try simple, cheap machine learning
    models (such as small, densely connected networks) before looking into complicated
    and computationally expensive models such as RNNs. This is the best way to make
    sure any further complexity you throw at the problem is legitimate and delivers
    real benefits.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在尝试机器学习方法之前建立一个常识基线一样，可以在进行复杂且计算昂贵的模型（如 RNN）之前，尝试简单、便宜的机器学习模型（例如小型、密集连接网络）是非常有用的。这是确保你对问题投入的任何进一步复杂性都是合法的，并且能够带来真正收益的最佳方法。
- en: Listing 10.9 shows a fully connected model that starts by flattening the data
    and then runs it through two layer_dense()s. Note the lack of an activation function
    on the last layer_dense(), which is typical for a regression problem. We use mean
    squared error (MSE) as the loss, rather than MAE, because unlike MAE, it’s smooth
    around zero, which is a useful property for gradient descent. We will monitor
    MAE by adding it as a metric in compile().
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.9 显示了一个完全连接的模型，它首先将数据展平，然后通过两个 layer_dense() 进行处理。请注意，最后一个 layer_dense()
    没有激活函数，这对于回归问题是典型的。我们使用均方误差（MSE）作为损失，而不是 MAE，因为与 MAE 不同，它在零附近是光滑的，对于梯度下降是一个有用的特性。我们将将
    MAE 作为指标添加到 compile() 中进行监控。
- en: Listing 10.9 Training and evaluating a densely connected model
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.9 训练和评估一个密集连接模型
- en: ncol_input_data <- length(input_data_colnames)
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ncol_input_data <- length(input_data_colnames)
- en: inputs <- layer_input(shape = c(sequence_length, ncol_input_data))
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: inputs <- layer_input(shape = c(sequence_length, ncol_input_data))
- en: outputs <- inputs %>%
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: outputs <- inputs %>%
- en: layer_flatten() %>%
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: layer_flatten() %>%
- en: layer_dense(16, activation = "relu") %>%
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(16, activation = "relu") %>%
- en: layer_dense(1)
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(1)
- en: model <- keras_model(inputs, outputs)
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model(inputs, outputs)
- en: callbacks = list (
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: callbacks = list (
- en: callback_model_checkpoint("jena_dense.keras", save_best_only = TRUE)➊
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: callback_model_checkpoint("jena_dense.keras", save_best_only = TRUE)➊
- en: )
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: model %>%
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: model %>%
- en: compile(optimizer = "rmsprop",
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: compile(optimizer = "rmsprop",
- en: loss = "mse",
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: loss = "mse",
- en: metrics = "mae")
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = "mae")
- en: history <- model %>%
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: history <- model %>%
- en: fit(train_dataset,
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: fit(train_dataset,
- en: epochs = 10,
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 10,
- en: validation_data = val_dataset,
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: validation_data = val_dataset,
- en: callbacks = callbacks)
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: callbacks = callbacks)
- en: model <- load_model_tf("jena_dense.keras")➋
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: model <- load_model_tf("jena_dense.keras")➋
- en: 'sprintf("Test MAE: %.2f", evaluate(model, test_dataset)["mae"])'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 'sprintf("测试 MAE: %.2f", evaluate(model, test_dataset)["mae"])'
- en: '[1] "Test MAE: 2.71"'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] "测试 MAE: 2.71"'
- en: ➊ **We use a callback to save the best-performing model.**
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **我们使用回调保存表现最佳的模型。**
- en: ➋ **Reload the best model, and evaluate it on the test data.**
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **重新加载最佳模型，并在测试数据上进行评估。**
- en: Let’s display the loss curves for validation and training (see [figure 10.3](#fig10-3)).
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们显示验证和训练的损失曲线（参见[图 10.3](#fig10-3)）。
- en: Listing 10.10 Plotting results
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.10 绘制结果
- en: plot(history, metrics = "mae")
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: plot(history, metrics = "mae")
- en: '![Image](../images/f0313-01.jpg)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0313-01.jpg)'
- en: '**Figure 10.3 Training and validation MAE on the Jena temperature-forecasting
    task with a simple, densely connected network**'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 10.3 在 Jena 温度预测任务上使用简单的密集连接网络进行训练和验证的 MAE**'
- en: 'Some of the validation losses are close to the no-learning baseline, but not
    reliably. This goes to show the merit of having this baseline in the first place:
    it turns out to be not easy to outperform. Your common sense contains a lot of
    valuable information to which a machine learning model doesn’t have access.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 一些验证损失接近于无学习基线，但不可靠。这显示了首先拥有这个基线的价值所在：结果证明很难超越它。你的常识包含了很多有价值的信息，机器学习模型无法获取。
- en: You may wonder, if a simple, well-performing model exists to go from the data
    to the targets (the common-sense baseline), why doesn’t the model you’re training
    find it and improve on it? Well, the space of models in which you’re searching
    for a solution—that is, your hypothesis space—is the space of all possible two-layer
    networks with the configuration you defined. The common-sense heuristic is just
    one model among millions that can be represented in this space. It’s like looking
    for a needle in a haystack. Just because a good solution technically exists in
    your hypothesis space doesn’t mean you’ll be able to find it via gradient descent.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，如果存在一个简单且性能良好的模型，可以从数据到目标（常识基线），为什么你正在训练的模型找不到它并改进它呢？嗯，你正在搜索解决方案的模型空间——也就是你定义的所有可能的两层网络的空间。常识启发式仅仅是这个空间中可以表示的数百万模型中的一个。这就像在一堆草堆里寻找一根针。仅仅因为一个好的解决方案在你的假设空间中技术上存在，并不意味着你能够通过梯度下降找到它。
- en: 'That’s a pretty significant limitation of machine learning in general: unless
    the learning algorithm is hardcoded to look for a specific kind of simple model,
    it can sometimes fail to find a simple solution to a simple problem. That’s why
    leveraging good feature engineering and relevant architecture priors is essential:
    you need to precisely tell your model what it should be looking for.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 这在机器学习中是一个非常重要的限制：除非学习算法被硬编码为寻找特定类型的简单模型，否则有时会无法找到简单问题的简单解决方案。这就是为什么利用良好的特征工程和相关的架构先验是必不可少的原因：你需要准确告诉你的模型它应该寻找什么。
- en: 10.2.4 Let’s try a 1D convolutional model
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.4 让我们尝试一个1D卷积模型
- en: Speaking of leveraging the right architecture priors, because our input sequences
    feature daily cycles, perhaps a convolutional model could work. A temporal convnet
    could reuse the same representations across different days, much like a spatial
    convnet can reuse the same representations across different locations in an image.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 谈到利用正确的架构先验，因为我们的输入序列具有日循环，也许一个卷积模型可能有效。时间卷积网络可以在不同的日子重用相同的表示，就像空间卷积网络可以在图像的不同位置重用相同的表示一样。
- en: 'You already know about layer_conv_2d() and layer_separable_conv_2d(), which
    see their inputs through small windows that swipe across 2D grids. There are also
    1D and even 3D versions of these layers: layer_conv_1d(), layer_separable_ conv_1d(),
    and layer_conv_3d().^([2](#endnote2)) The layer_conv_1d() layer relies on 1D windows
    that slide across input sequences, and the layer_conv_3d() layer relies on cubic
    windows that slide across input volumes.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经了解了layer_conv_2d()和layer_separable_conv_2d()，它们通过在2D网格上刷过小窗口查看它们的输入。这些层还有它们的1D甚至3D版本：layer_conv_1d()、layer_separable_
    conv_1d()和layer_conv_3d()。^([2](#endnote2)) layer_conv_1d()层依赖于在输入序列上滑动的1D窗口，而layer_conv_3d()层依赖于在输入体积上滑动的立方窗口。
- en: You can thus build 1D convnets, strictly analogous to 2D convnets. They’re a
    great fit for any sequence data that follows the translation invariance assumption
    (meaning that if you slide a window over the sequence, the content of the window
    should follow the same properties independently of the location of the window).
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您可以构建1D convnets，与2D convnets严格类似。它们非常适合任何遵循平移不变性假设的序列数据（这意味着如果您在序列上滑动一个窗口，窗口的内容应该独立于窗口的位置而遵循相同的属性）。
- en: 'Let’s try one on our temperature-forecasting problem. We’ll pick an initial
    window length of 24, so that we look at 24 hours of data at a time (one cycle).
    As we down-sample the sequences (via layer_max_pooling_1d() layers), we’ll reduce
    the window size accordingly:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在我们的温度预测问题上尝试一个。我们将选择一个初始窗口长度为24，这样我们一次可以查看24小时的数据（一个周期）。当我们通过layer_max_pooling_1d()层对序列进行下采样时，我们将相应地减小窗口大小：
- en: inputs <- layer_input(shape = c(sequence_length, ncol_input_data))
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: inputs <- layer_input(shape = c(sequence_length, ncol_input_data))
- en: outputs <- inputs %>%
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: outputs <- inputs %>%
- en: layer_conv_1d(8, 24, activation = "relu") %>%
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: layer_conv_1d(8, 24, activation = "relu") %>%
- en: layer_max_pooling_1d(2) %>%
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: layer_max_pooling_1d(2) %>%
- en: layer_conv_1d(8, 12, activation = "relu") %>%
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: layer_conv_1d(8, 12, activation = "relu") %>%
- en: layer_max_pooling_1d(2) %>%
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: layer_max_pooling_1d(2) %>%
- en: layer_conv_1d(8, 6, activation = "relu") %>%
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: layer_conv_1d(8, 6, activation = "relu") %>%
- en: layer_global_average_pooling_1d() %>%
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: layer_global_average_pooling_1d() %>%
- en: layer_dense(1)
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(1)
- en: model <- keras_model(inputs, outputs)
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model(inputs, outputs)
- en: callbacks <- list(callback_model_checkpoint("jena_conv.keras", save_best_only
    = TRUE))
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: callbacks <- list(callback_model_checkpoint("jena_conv.keras", save_best_only
    = TRUE))
- en: model %>% compile(optimizer = "rmsprop",
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% compile(optimizer = "rmsprop",
- en: loss = "mse",
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: loss = "mse",
- en: metrics = "mae")
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = "mae")
- en: history <- model %>% fit(
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: history <- model %>% fit(
- en: train_dataset,
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: train_dataset,
- en: epochs = 10,
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 10,
- en: validation_data = val_dataset,
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: validation_data = val_dataset,
- en: callbacks = callbacks
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: callbacks = callbacks
- en: )
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: model <- load_model_tf("jena_conv.keras")
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: model <- load_model_tf("jena_conv.keras")
- en: 'sprintf("Test MAE: %.2f", evaluate(model, test_dataset)["mae"])'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 'sprintf("测试 MAE: %.2f", evaluate(model, test_dataset)["mae"])'
- en: '[1] "Test MAE: 3.20"'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] "测试 MAE: 3.20"'
- en: We get the training and validation curves shown in [figure 10.4](#fig10-4).
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了在 [图 10.4](#fig10-4) 中显示的训练和验证曲线。
- en: plot(history)
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制(history)
- en: '![Image](../images/f0315-01.jpg)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0315-01.jpg)'
- en: '**Figure 10.4 Training and validation MAE on the Jena temperature-forecasting
    task with a 1D convnet**'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 10.4 在 Jena 温度预测任务中使用一维卷积网络的训练和验证 MAE**'
- en: 'As it turns out, this model performs even worse than the densely connected
    one, only achieving a test MAE of 3.2 degrees, far from the common-sense baseline.
    What went wrong here? Two things:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，这个模型的表现甚至比全连接模型还要差，只能达到 3.2 度的测试 MAE，远远低于常识基线。这里出了什么问题呢？有两点：
- en: First, weather data doesn’t quite respect the translation invariance assumption.
    Although the data does feature daily cycles, data from a morning follows different
    properties than data from an evening or from the middle of the night. Weather
    data is translation invariant for only a very specific timescale.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，天气数据并不完全遵循平移不变性的假设。尽管数据具有每日循环，但早晨的数据与晚上或午夜的数据具有不同属性。天气数据只在特定时间尺度上具有平移不变性。
- en: Second, order in our data matters—a lot. The recent past is far more informative
    for predicting the next day’s temperature than data from five days ago. A 1D convnet
    is not able to leverage this fact. In particular, our max-pooling and global average
    pooling layers are largely destroying order information.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，我们的数据顺序非常重要。最近的数据对于预测第二天的温度比五天前的数据要有更多信息。一维卷积网络无法利用这一事实。特别是，我们的最大池化层和全局平均池化层在很大程度上破坏了顺序信息。
- en: 10.2.5 A first recurrent baseline
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.5 首个循环基线
- en: 'Neither the fully connected approach nor the convolutional approach did well,
    but that doesn’t mean machine learning isn’t applicable to this problem. The densely
    connected approach first flattened the time series, which removed the notion of
    time from the input data. The convolutional approach treated every segment of
    the data in the same way, even applying pooling, which destroyed order information.
    Let’s instead look at the data as what it is: a sequence, where causality and
    order matter.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 无论全连接方法还是卷积方法都表现不佳，但这并不意味着机器学习不适用于这个问题。全连接方法首先将时间序列展平，从输入数据中除去了时间的概念。卷积方法将数据的每个片段以相同的方式处理，甚至进行了池化，这破坏了顺序信息。我们应该将数据看作它实际的样子：一个序列，因果和顺序至��重要。
- en: 'There’s a family of neural network architectures designed specifically for
    this use case: recurrent neural networks. Among them, the long short-term memory
    (LSTM) layer has long been very popular. We’ll see in a minute how these models
    work, but let’s start by giving the LSTM layer a try.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 有一类神经网络架构专门设计用于这种情况：循环神经网络。其中，长短期记忆（LSTM）层一直非常受欢迎。我们马上会看到这些模型是如何工作的，但首先让我们尝试一下
    LSTM 层。
- en: '**Listing 10.11 A simple LSTM-based model**'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 10.11 一个简单的基于 LSTM 的模型**'
- en: inputs <- layer_input(shape = c(sequence_length, ncol_input_data))
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: inputs <- layer_input(shape = c(sequence_length, ncol_input_data))
- en: outputs <- inputs %>%
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: outputs <- inputs %>%
- en: layer_lstm(16) %>%
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: layer_lstm(16) %>%
- en: layer_dense(1)
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(1)
- en: model <- keras_model(inputs, outputs)
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model(inputs, outputs)
- en: callbacks <- list(callback_model_checkpoint("jena_lstm.keras", save_best_only
    = TRUE))
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: callbacks <- list(callback_model_checkpoint("jena_lstm.keras", save_best_only
    = TRUE))
- en: model %>% compile(optimizer = "rmsprop",
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% compile(optimizer = "rmsprop",
- en: loss = "mse",
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: loss = "mse",
- en: metrics = "mae")
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = "mae")
- en: history <- model %>% fit(
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: history <- model %>% fit(
- en: train_dataset,
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: train_dataset,
- en: epochs = 10,
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 10,
- en: validation_data = val_dataset,
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: validation_data = val_dataset,
- en: callbacks = callbacks
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: callbacks = callbacks
- en: )
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: model <- load_model_tf("jena_lstm.keras")
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: model <- load_model_tf("jena_lstm.keras")
- en: 'sprintf("Test MAE: %.2f", evaluate(model, test_dataset)["mae"])'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 'sprintf("测试 MAE: %.2f", evaluate(model, test_dataset)["mae"])'
- en: '[1] "Test MAE: 2.52"'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] "测试 MAE: 2.52"'
- en: '[Figure 10.5](#fig10-5) shows the results. Much better! We achieve a test MAE
    of 2.52 degrees. The LSTM-based model can finally beat the common-sense baseline
    (albeit just by a bit, for now), demonstrating the value of machine learning on
    this task.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10.5](#fig10-5)显示了结果。好多了！我们实现了 2.52 度的测试 MAE。基于 LSTM 的模型终于能够击败常识基准（尽管只是一点点），展示了机器学习在此任务上的价值。'
- en: But why did the LSTM model perform markedly better than the densely connected
    one or the convnet? And how can we further refine the model? To answer this, let’s
    take a closer look at recurrent neural networks.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 但为什么 LSTM 模型的表现要比密集连接或卷积网络更好呢？我们如何进一步改进模型？为了回答这个问题，让我们仔细研究一下递归神经网络。
- en: '![Image](../images/f0317-01.jpg)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0317-01.jpg)'
- en: '**Figure 10.5 Training and validation MAE on the Jena temperature-forecasting
    task with an LSTM-based model (note that we omit epoch 1 on this graph, because
    the high training MAE at the first epoch would distort the scale)**'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 10.5 基于 LSTM 模型的 Jena 温度预测任务的训练和验证 MAE（请注意，我们在此图上省略了第一个时期，因为第一个时期的高训练 MAE
    会扭曲比例尺）**'
- en: 10.3 Understanding recurrent neural networks
  id: totrans-338
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.3 理解递归神经网络
- en: 'A major characteristic of all neural networks you’ve seen so far, such as densely
    connected networks and convnets, is that they have no memory. Each input shown
    to them is processed independently, with no state kept between inputs. With such
    networks, to process a sequence or a temporal series of data points, you have
    to show the entire sequence to the network at once: turn it into a single data
    point. For instance, this is what we did in the densely connected network example:
    we flattened our five days of data into a single large vector and processed it
    in one go. Such networks are called *feed-forward networks*.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你见过的所有神经网络（如密集连接网络和卷积网络）的一个主要特征是它们没有记忆。它们分别处理每个输入，不存在输入之间的状态保持。对于这些网络来说，要处理一个序列或时间序列的数据点，你必须一次性将整个序列展示给网络：将其转换为一个单独的数据点。例如，在密集连接网络的示例中，我们将五天的数据压平成一个巨大的向量，并一次性处理了它。这样的网络称为
    *前馈网络*。
- en: In contrast, as you’re reading the present sentence, you’re processing it word
    by word—or rather, eye saccade by eye saccade—while keeping memories of what came
    before; this gives you a fluid representation of the meaning conveyed by this
    sentence. Biological intelligence processes information incrementally while maintaining
    an internal model of what it’s processing, built from past information and constantly
    updated as new information comes in.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，当你阅读当前的句子时，你是逐词处理它，同时记忆之前的内容；这使你对这个句子所传达的含义有流畅的理解。生物智能会增量地处理信息，同时保持一个内部模型，该模型是从过去信息中构建起来的，并随着新信息的到来而不断更新。
- en: 'A *recurrent neural network* (RNN) adopts the same principle, albeit in an
    extremely simplified version: it processes sequences by iterating through the
    sequence elements and maintaining a *state* that contains information relative
    to what it has seen so far. In effect, an RNN is a type of neural network that
    has an internal *loop* (see [figure 10.6](#fig10-6)). The state of the RNN is
    reset between processing two different, independent sequences (such as two samples
    in a batch), so you still consider one sequence to be a single data point: a single
    input to the network. What changes is that this data point is no longer processed
    in a single step; rather, the network internally loops over sequence elements.'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '*递归神经网络*（RNN）采用相同的原则，尽管是在一个极简化的版本：它通过迭代序列元素并保持一个包含迄今为止所见信息的*状态*来处理序列。实际上，RNN
    是一种具有内部*循环*的神经网络（参见[图 10.6](#fig10-6)）。RNN 的状态在处理两个不同的、独立的序列（例如批次中的两个样本）之间重置，所以你仍然将一个序列视为一个单独的数据点：对网络的单个输入。改变的是，这个数据点不再以单个步骤进行处理；相反，网络在序列元素上进行内部循环。'
- en: To make these notions of *loop* and *state* clear, let’s implement the forward
    pass of a toy RNN. This RNN takes as input a sequence of vectors, which we’ll
    encode as a rank 2 tensor of size (timesteps, input_features). It loops over time
    steps, and at each time step, it considers its current state at t and the input
    at t (of shape (input_features)), and combines them to obtain the output at t.
    We’ll then set the state for the next step to be this previous output. For the
    first time step, the previous output isn’t defined; hence, there is no current
    state. So we’ll initialize the state as an all-zero vector called the *initial*
    state of the network. In pseudocode, the following listing shows the RNN.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 为了澄清*循环*和*状态*的概念，让我们实现一个玩具 RNN 的前向传播。这个 RNN 接受一系列向量作为输入，我们将其编码为大小为 (时间步长, 输入特征)
    的二阶张量。它在时间步长上循环，并在每个时间步长上考虑 t 时刻的当前状态和 t 时刻的输入（形状为 (输入特征)），并将它们组合起来得到 t 时刻的输出。然后，我们将下一个步骤的状态设置为上一个输出。对于第一个时间步长，上一个输出未定义；因此，没有当前状态。因此，我们将状态初始化为称为网络的*初始*状态的全零向量。在伪代码中，以下清单显示了
    RNN。
- en: '![Image](../images/f0318-01.jpg)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0318-01.jpg)'
- en: '**Figure 10.6 A recurrent network: A network with a loop**'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 10.6 一个循环网络：具有循环的网络**'
- en: '**Listing 10.12 Pseudocode RNN**'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 10.12 RNN 伪代码**'
- en: state_t <- 0➊
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: state_t <- 0➊
- en: for (input_t in input_sequence) {➋
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: for (input_t in input_sequence) {➋
- en: output_t <- f(input_t,
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: output_t <- f(input_t,
- en: state_t) state_t <- output_t➌
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: state_t) state_t <- output_t➌
- en: '}'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ **The state at t**
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **t 时刻的状态**
- en: ➋ **Iterate over sequence elements.**
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **迭代序列元素。**
- en: ➌ **The previous output becomes the state for the next iteration.**
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **前一个输出成为下一次迭代的状态。**
- en: 'You can even flesh out the function f: the transformation of the input and
    state into an output will be parameterized by two matrices, W and U, and a bias
    vector. It’s similar to the transformation operated by a densely connected layer
    in a feed-forward network.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 甚至可以详细说明函数 f：将输入和状态转换为输出的过程将由两个矩阵 W 和 U，以及一个偏置向量参数化。这类似于前馈网络中密集连接层所进行的转换。
- en: '**Listing 10.13 More-detailed pseudocode for the RNN**'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 10.13 RNN 的更详细的伪代码**'
- en: state_t <- 0
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: state_t <- 0
- en: for (input_t in input_sequence) {
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: for (input_t in input_sequence) {
- en: output_t <- activation(dot(W, input_t) + dot(U,
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: output_t <- activation(dot(W, input_t) + dot(U,
- en: state_t) + b) state_t <- output_t
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: state_t) + b) state_t <- output_t
- en: '}'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: To make these notions absolutely unambiguous, let’s write a naive R implementation
    of the forward pass of the simple RNN.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这些概念绝对清晰，让我们写一个简单的 R 语言实现简单 RNN 的前向传播。
- en: '**Listing 10.14 Base R implementation of a simple RNN**'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 10.14 简单 RNN 的基本 R 实现**'
- en: random_array <- function(dim) array(runif(prod(dim)), dim)
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: random_array <- function(dim) array(runif(prod(dim)), dim)
- en: timesteps <- 100➊
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: timesteps <- 100➊
- en: input_features <- 32➋
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: input_features <- 32➋
- en: output_features <- 64➌
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: output_features <- 64➌
- en: inputs <- random_array(c(timesteps, input_features))➍
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: inputs <- random_array(c(timesteps, input_features))➍
- en: state_t <- array(0, dim = output_features)➎
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: state_t <- array(0, dim = output_features)➎
- en: W <- random_array(c(output_features, input_features))➏
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: W <- random_array(c(output_features, input_features))➏
- en: U <- random_array(c(output_features, output_features))➏
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: U <- random_array(c(output_features, output_features))➏
- en: b <- random_array(c(output_features, 1))➏
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: b <- random_array(c(output_features, 1))➏
- en: successive_outputs <- array(0, dim = c(timesteps, output_features))➏
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: successive_outputs <- array(0, dim = c(timesteps, output_features))➏
- en: for(ts in 1:timesteps) {
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: for(ts in 1:timesteps) {
- en: input_t <- inputs[ts, ]➐
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: input_t <- inputs[ts, ]➐
- en: output_t <- tanh((W %*% input_t) + (U %*% state_t) + b)➑➒
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: output_t <- tanh((W %*% input_t) + (U %*% state_t) + b)➑➒
- en: successive_outputs[ts, ] <- output_t➓
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: successive_outputs[ts, ] <- output_t➓
- en: state_t <- output_t⓫
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: state_t <- output_t⓫
- en: '}'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: final_output_sequence <- successive_outputs⓬
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: final_output_sequence <- successive_outputs⓬
- en: ➊ **Number of time steps in the input sequence**
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **输入序列中的时间步长数**
- en: ➋ **Dimensionality of the input feature space**
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **输入特征空间的维度**
- en: ➌ **Dimensionality of the output feature space**
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **输出特征空间的维度**
- en: '➍ **Input data: random noise for the sake of the example**'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **输入数据：举例时使用的随机噪声**
- en: '➎ **Initial state: an all-zero vector**'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **初始状态：全零向量**
- en: ➏ **Create random weight matrices.**
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: ➏ **创建随机权重矩阵。**
- en: ➐ **input_t is a vector of shape (input_features).**
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: ➐ **input_t 是形状为 (输入特征) 的向量。**
- en: '➑ **W %*% input_t, U %*% input_t, and b all have the same shape: (output_features,
    1).**'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: ➑ **W %*% input_t、U %*% input_t 和 b 都具有相同的形状：(输出特征，1)。**
- en: ➒ **Combine the input with the current state (the previous output) to obtain
    the current output. We use tanh to add nonlinearity (we could use any other activation
    function).**
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: ➒ **将输入与当前状态（上一个输出）组合以获得当前输出。我们使用 tanh 添加非线性（我们可以使用任何其他激活函数）。**
- en: ➓ **Store this output.**
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: ➓ **将此输出存储起来。**
- en: ⓫ **Update the state of the network for the next time step.**
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: ⓫ **更新网络的状态，以便进行下一个时间步骤。**
- en: ⓬ **The final output is a rank 2 tensor of shape (timesteps, output_features).**
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: ⓬ **最终输出是一个形状为(timesteps, output_features)的二阶张量。**
- en: That’s easy enough. In summary, an RNN is a for loop that reuses quantities
    computed during the previous iteration of the loop, nothing more. Of course, you
    could build many different RNNs that fit this definition—this example is one of
    the simplest RNN formulations. RNNs are characterized by their step function,
    such as the following function in this case (see [figure 10.7](#fig10-7)).
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易。总之，RNN是一个for循环，它重复使用在循环的前一次迭代中计算的数量，没有更多的内容。当然，您可以构建许多不同的满足此定义的RNN——这个示例是最简单的RNN公式之一。RNN的特征在于它们的步骤函数，例如本例中的下面的函数（见[图10.7](#fig10-7)）。
- en: output_t <- tanh((W %*% input_t) + (U %*% state_t) + b)
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: output_t <- tanh((W %*% input_t) + (U %*% state_t) + b)
- en: '![Image](../images/f0319-01.jpg)'
  id: totrans-394
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0319-01.jpg)'
- en: '**Figure 10.7 A simple RNN, unrolled over time**'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '**图10.7 一个简单的RNN在时间上展开**'
- en: In this example, the final output is a rank 2 tensor of shape (timesteps, output_
    features), where each time step is the output of the loop at time t. Each time
    step t in the output tensor contains information about time steps 1 to t in the
    input sequence—about the entire past. For this reason, in many cases, you don’t
    need this full sequence of outputs; you just need the last output (output_t at
    the end of the loop), because it already contains information about the entire
    sequence.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，最终输出是一个形状为(timesteps, output_features)的二阶张量，其中每个时间步骤是时间t处循环的输出。输出张量中的每个时间步长t都包含有关输入序列中时间步长1到t的信息——关于整个过去的信息。因此，在许多情况下，您不需要这个完整的输出序列；您只需要最后一个输出（在循环结束时的output_t），因为它已经包含有关整个序列的信息。
- en: 10.3.1 A recurrent layer in Keras
  id: totrans-397
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.1 Keras中的一个递归层
- en: 'The process we just naively implemented in R corresponds to an actual Keras
    layer—the layer_simple_rnn(). There is one minor difference: layer_simple_rnn()
    processes batches of sequences, like all other Keras layers, not a single sequence
    as in the R example. This means it takes inputs of shape (batch_size, timesteps,
    input_features), rather than (timesteps, input_features). When specifying the
    shape argument of the initial input, note that you can set the timesteps entry
    to NA, which enables your network to process sequences of arbitrary length.'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚在R中天真地实现的进程对应于一个实际的Keras层——layer_simple_rnn()。有一个细微的差别：layer_simple_rnn()处理序列批次，像所有其他Keras层一样，而不是在R的例子中一次处理一个序列。这意味着它需要形状为(batch_size,
    timesteps, input_features)的输入，而不是形状为(timesteps, input_features)的输入。在指定初始输入的形状参数时，请注意，您可以将timesteps条目设置为NA，这将使您的网络能够处理任意长度的序列。
- en: '**Listing 10.15 An RNN layer that can process sequences of any length**'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单10.15 可处理任意长度序列的RNN层**'
- en: num_features <- 14
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: num_features <- 14
- en: inputs <- layer_input(shape = c(NA, num_features))
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: inputs <- layer_input(shape = c(NA, num_features))
- en: outputs <- inputs %>% layer_simple_rnn(16)
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: outputs <- inputs %>% layer_simple_rnn(16)
- en: This is especially useful if your model is meant to process sequences of variable
    length. However, if all of your sequences have the same length, I recommend specifying
    a complete input shape, because it enables the model print() method to display
    output length information, which is always nice, and it can unlock some performance
    optimizations (see the sidebar box in section 10.4.1, “RNN runtime performance”).
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的模型用于处理可变长度的序列，那么这是特别有用的。然而，如果您的所有序列都具有相同的长度，我建议指定完整的输入形状，因为它可以让模型的print()方法显示输出长度信息，这总是很好的，并且它可以解锁一些性能优化（请参见第10.4.1节的侧边栏中的内容，“RNN运行时性能”）。
- en: 'All recurrent layers in Keras (layer_simple_rnn(), layer_lstm(), and layer_
    gru()) can be run in two different modes: they can return either full sequences
    of successive outputs for each time step (a rank 3 tensor of shape (batch_size,
    timesteps, output_features)) or return only the last output for each input sequence
    (a rank 2 tensor of shape (batch_size, output_features)). These two modes are
    controlled by the return_sequences argument. Let’s look at an example that uses
    layer_simple_rnn() and returns only the output at the last time step.'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: Keras中的所有循环层（layer_simple_rnn()、layer_lstm()和layer_gru()）都可以以两种不同的模式运行：它们可以为每个时间步长返回连续输出的完整序列（形状为(batch_size,
    timesteps, output_features)的二阶张量），或仅为每个输入序列返回最后一个输出（形状为(batch_size, output_features)的二阶张量）。这两种模式由return_sequences参数控制。让我们看一个使用layer_simple_rnn()并仅返回最后一步输出的例子。
- en: '**Listing 10.16 An RNN layer that returns only its last output step**'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单10.16 只返回最后一步输出的RNN层**'
- en: num_features <- 14
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: num_features <- 14
- en: steps <- 120
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: steps <- 120
- en: inputs <- layer_input(shape = c(steps, num_features))
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: inputs <- layer_input(shape = c(steps, num_features))
- en: outputs <- inputs %>%
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: outputs <- inputs %>%
- en: layer_simple_rnn(16, return_sequences = FALSE)➊
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: layer_simple_rnn(16, return_sequences = FALSE)➊
- en: outputs$shape
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: outputs$shape
- en: TensorShape([None, 16])
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: TensorShape([None, 16])
- en: ➊ **Note that return_sequences = FALSE is the default.**
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **请注意，默认情况下 return_sequences = FALSE。**
- en: The following example returns the full state sequence.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的示例返回完整的状态序列。
- en: '**Listing 10.17 An RNN layer that returns its full output sequence**'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 10.17 返回完整输出序列的 RNN 层**'
- en: num_features <- 14
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: num_features <- 14
- en: steps <- 120
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: steps <- 120
- en: inputs <- layer_input(shape = c(steps, num_features))
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: inputs <- layer_input(shape = c(steps, num_features))
- en: outputs <- inputs %>% layer_simple_rnn(16, return_sequences = TRUE)
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: outputs <- inputs %>% layer_simple_rnn(16, return_sequences = TRUE)
- en: outputs$shape
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: outputs$shape
- en: TensorShape([None, 120, 16])
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: TensorShape([None, 120, 16])
- en: It’s sometimes useful to stack several recurrent layers one after the other
    to increase the representational power of a network. In such a setup, you have
    to get all of the intermediate layers to return a full sequence of outputs.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，堆叠几个递归层一个接一个地可以增加网络的表示能力。在这样的设置中，你必须让所有中间层返回完整的输出序列。
- en: '**Listing 10.18 Stacking RNN layers**'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 10.18 堆叠 RNN 层**'
- en: inputs <- layer_input(shape = c(steps, num_features))
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: inputs <- layer_input(shape = c(steps, num_features))
- en: outputs <- inputs %>%
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: outputs <- inputs %>%
- en: layer_simple_rnn(16, return_sequences = TRUE) %>%
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: layer_simple_rnn(16, return_sequences = TRUE) %>%
- en: layer_simple_rnn(16, return_sequences = TRUE) %>%
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: layer_simple_rnn(16, return_sequences = TRUE) %>%
- en: layer_simple_rnn(16)
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: layer_simple_rnn(16)
- en: 'In practice, you’ll rarely work with layer_simple_rnn(). It’s generally too
    simplistic to be of real use. In particular, layer_simple_rnn() has a major issue:
    although it should theoretically be able to retain at time t information about
    inputs seen many time steps before, such long-term dependencies prove impossible
    to learn in practice. This is due to the *vanishing-gradient problem*, an effect
    that is similar to what is observed with nonrecurrent networks (feed-forward networks)
    that are many layers deep: as you keep adding layers to a network, the network
    eventually becomes untrainable. The theoretical reasons for this effect were studied
    by Hochreiter, Schmidhuber, and Bengio in the early 1990s.^([3](#endnote3))'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，你很少会使用 layer_simple_rnn()。它通常过于简单，无法真正使用。特别是，layer_simple_rnn() 存在一个主要问题：虽然理论上它应该能够在时间
    t 保留关于之前许多时间步的输入的信息，但在实践中，这样的长期依赖关系被证明是不可能学习的。这是由于 *消失梯度问题*，这是一种类似于非递归网络（前馈网络）在许多层深度时观察到的效果：随着你不断向网络添加层，网络最终变得无法训练。这个效果的理论原因是由
    Hochreiter、Schmidhuber 和 Bengio 在 1990 年代初研究的^([3](#endnote3))。
- en: Thankfully, layer_simple_rnn() isn’t the only recurrent layer available in Keras.
    There are two others, layer_lstm() and layer_gru(), which were designed to address
    these issues.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，layer_simple_rnn() 不是 Keras 中唯一可用的递归层。还有另外两个，layer_lstm() 和 layer_gru()，它们设计用来解决这些问题。
- en: Let’s consider layer_lstm(). The underlying long short-term memory (LSTM) algorithm
    was developed by Hochreiter and Schmidhuber in 1997^([4](#endnote4)) ; it was
    the culmination of their research on the vanishing-gradient problem.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来考虑 layer_lstm()。底层长短期记忆（LSTM）算法由 Hochreiter 和 Schmidhuber 在 1997 年开发^([4](#endnote4))；这是他们对消失梯度问题研究的顶点。
- en: 'This layer is a variant of the layer_simple_rnn() you already know about; it
    adds a way to carry information across many time steps. Imagine a conveyor belt
    running parallel to the sequence you’re processing. Information from the sequence
    can jump onto the conveyor belt at any point, be transported to a later time step,
    and jump off, intact, when you need it. This is essentially what LSTM does: it
    saves information for later, thus preventing older signals from gradually vanishing
    during processing. This should remind you of *residual connections*, which you
    learned about in chapter 9: it’s pretty much the same idea.'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 这个层是你已经了解的 layer_simple_rnn() 的一个变体；它添加了一种在许多时间步长之间传递信息的方式。想象一条与你正在处理的序列平行运行的传送带。来自序列的信息可以在任何时间点跳上传送带，被传送到以后的时间步长，然后在你需要时完整地跳下来。这基本上就是
    LSTM 做的事情：它保存信息以备后用，从而在处理过程中防止旧信号逐渐消失。这应该让你想起第 9 章学到的 *残差连接*，基本上是同样的思想。
- en: To understand this process in detail, let’s start from the layer_simple_rnn()
    cell (see [figure 10.8](#fig10-8)). Because you’ll have a lot of weight matrices,
    index the W and U matrices in the cell, with the letter o (Wo and Uo) for *output*.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 要详细理解这个过程，让我们从 layer_simple_rnn() 单元开始（参见 [图 10.8](#fig10-8)）。因为你将会有很多权重矩阵，在单元中，用字母
    o（Wo 和 Uo）表示*output*。
- en: '![Image](../images/f0322-01.jpg)'
  id: totrans-434
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0322-01.jpg)'
- en: '**Figure 10.8 The starting point of an LSTM layer: a SimpleRNN**'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 10.8 LSTM 层的起点：一个 SimpleRNN**'
- en: 'Let’s add to this picture an additional data flow that carries information
    across time steps. Call its values at different time steps c_t, where c stands
    for *carry*. This information will have the following impact on the cell: it will
    be combined with the input connection and the recurrent connection (via a dense
    transformation: a dot product with a weight matrix followed by a bias add and
    the application of an activation function), and it will affect the state being
    sent to the next time step (via an activation function and a multiplication operation).
    Conceptually, the carry dataflow is a way to modulate the next output and the
    next state (see [figure 10.9](#fig10-9)). Simple so far.'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在这个图像中添加一个额外的数据流，它在时间步长之间携带信息。将其称为 c_t 的不同时间步长的值，其中 c 代表*carry*。这个信息将对单元产生以下影响：它将与输入连接和循环连接（通过一个密集转换：一个与权重矩阵的点积，然后加上偏置，并应用激活函数）相结合，并影响发送到下一个时间步的状态（通过激活函数和乘法操作）。从概念上讲，进位数据流是调制下一个输出和下一个状态的一种方式（见
    [图 10.9](#fig10-9)）。到目前为止还是很简单的。
- en: '![Image](../images/f0322-02.jpg)'
  id: totrans-437
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0322-02.jpg)'
- en: '**Figure 10.9 Going from a SimpleRNN to an LSTM: Adding a carry track**'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 10.9 从 SimpleRNN 过渡到 LSTM：添加一个进位轨道**'
- en: 'Now the subtlety—the way the next value of the carry dataflow is computed.
    It involves three distinct transformations. All three have the form of a SimpleRNN
    cell:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看看一个微妙之处——计算进位数据流的下一个值的方式。它涉及三个不同的转换。这三个转换都采用了 SimpleRNN 单元的形式：
- en: y <- activation((state_t %*% U) + (input_t %*% W) + b)
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: y <- activation((state_t %*% U) + (input_t %*% W) + b)
- en: But all three transformations have their own weight matrices, which we’ll index
    with the letters i, f, and k. Here’s what we have so far (it may seem a bit arbitrary,
    but bear with me).
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 但是所有三个转换都有自己的权重矩阵，我们将用字母 i、f 和 k 对它们进行索引。到目前为止我们得到了这样的式子（可能看起来有点随意，但请跟我来）。
- en: '**Listing 10.19 Pseudocode details of the LSTM architecture (1/2)**'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 10.19 LSTM 架构的伪代码细节（1/2）**'
- en: output_t < -
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: output_t < -
- en: activation((state_t %*% Uo) + (input_t %*% Wo) + (c_t %*% Vo) + bo)
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: activation((state_t %*% Uo) + (input_t %*% Wo) + (c_t %*% Vo) + bo)
- en: i_t <- activation((state_t %*% Ui) + (input_t %*% Wi) + bi)
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: i_t <- activation((state_t %*% Ui) + (input_t %*% Wi) + bi)
- en: f_t <- activation((state_t %*% Uf) + (input_t %*% Wf) + bf)
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: f_t <- activation((state_t %*% Uf) + (input_t %*% Wf) + bf)
- en: k_t <- activation((state_t %*% Uk) + (input_t %*% Wk) + bk)
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: k_t <- activation((state_t %*% Uk) + (input_t %*% Wk) + bk)
- en: We obtain the new carry state (the next c_t) by combining i_t, f_t, and k_t.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过组合 i_t、f_t 和 k_t 来获得新的进位状态（下一个 c_t）。
- en: '**Listing 10.20 Pseudocode details of the LSTM architecture (2/2)**'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 10.20 LSTM 架构的伪代码细节（2/2）**'
- en: c_t+1 = i_t * k_t + c_t * f_t
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: c_t+1 = i_t * k_t + c_t * f_t
- en: Add this as shown in [figure 10.10](#fig10-10), and that’s it. Not so complicated—merely
    a tad complex.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 像 [图 10.10](#fig10-10) 中所示添加这个，就这样。并不是那么复杂——只是稍微复杂了一点。
- en: '![Image](../images/f0323-01.jpg)'
  id: totrans-452
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0323-01.jpg)'
- en: '**Figure 10.10 Anatomy of an LSTM**'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 10.10 LSTM 的解剖学**'
- en: If you want to get philosophical, you can interpret what each of these operations
    is meant to do. For instance, you can say that multiplying c_t and f_t is a way
    to deliberately forget irrelevant information in the carry dataflow. Meanwhile,
    i_t and k_t provide information about the present, updating the carry track with
    new information. But at the end of the day, these interpretations don’t mean much,
    because what these operations *actually* do is determined by the contents of the
    weights parameterizing them; and the weights are learned in an end-to-end fashion,
    starting over with each training round, making it impossible to credit this or
    that operation with a specific purpose. The specification of an RNN cell (as just
    described) determines your hypothesis space—the space in which you’ll search for
    a good model configuration during training—but it doesn’t determine what the cell
    does; that is up to the cell weights. The same cell with different weights can
    be doing very different things. So the combination of operations making up an
    RNN cell is better interpreted as a set of *constraints* on your search, not as
    a *design* in an engineering sense.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想深入思考，你可以解释每个操作的意图。例如，你可以说将 c_t 和 f_t 相乘是一种有意忽略 carry 数据流中无关信息的方法。与此同时，i_t
    和 k_t 提供关于当前状态的信息，用新信息更新 carry 跟踪。但归根结底，这些解释并不重要，因为这些操作 *实际上* 做什么取决于参数化它们的权重的内容；而权重是以端对端方式学习的，在每一轮训练开始时重新开始，因此不可能将这种或那种操作归因于特定目的。RNN
    单元的规范（如上所述）确定了你的假设空间——在训练期间搜索良好模型配置的空间——但它并不确定单元所做的事情；这取决于单元权重。具有不同权重的相同单元可能做着非常不同的事情。因此，构成
    RNN 单元的操作组合更好地被解释为对你的搜索的一组 *约束*，而不是以工程意义上的 *设计*。
- en: 'Arguably, the choice of such constraints—the question of how to implement RNN
    cells—is better left to optimization algorithms (like genetic algorithms or reinforcement-learning
    processes) than to human engineers. In the future, that’s how we’ll build our
    models. In summary: you don’t need to understand anything about the specific architecture
    of an LSTM cell; as a human, it shouldn’t be your job to understand it. Just keep
    in mind what the LSTM cell is meant to do: allow past information to be reinjected
    at a later time, thus fighting the vanishing-gradient problem.'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 可以说，这种约束的选择——如何实现 RNN 单元——最好由优化算法（如遗传算法或强化学习过程）来处理，而不是由人类工程师来处理。在未来，这就是我们构建模型的方式。总之：你不需要理解
    LSTM 单元的具体架构；作为人类，你的工作不应该是理解它。只需记住 LSTM 单元的意图：允许过去的信息在以后重新注入，从而解决梯度消失问题。
- en: 10.4 Advanced use of recurrent neural networks
  id: totrans-456
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 递归神经网络的高级用法
- en: So far you’ve learned
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经学会了
- en: What RNNs are and how they work
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNN 是什么以及它们如何工作
- en: What LSTM is, and why it works better on long sequences than a naive RNN
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM 是什么，为什么它在处理长序列时比天真的 RNN 更有效
- en: How to use Keras RNN layers to process sequence dat
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用 Keras RNN 层处理序列数据
- en: Next, we’ll review a number of more advanced features of RNNs, which can help
    you get the most out of your deep learning sequence models. By the end of the
    section, you’ll know most of what there is to know about using recurrent networks
    with Keras.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将回顾一些 RNN 的更高级特性，这些特性可以帮助你充分利用你的深度学习序列模型。通过本节结束，你将了解大部分关于使用 Keras 进行递归网络的知识。
- en: 'We’ll cover the following:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下内容：
- en: '*Recurrent dropout*—This is a variant of dropout, used to fight overfitting
    in recurrent layers.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*递归丢失* —— 这是一种用于对抗递归层中过拟合的 dropout 变体。'
- en: '*Stacking recurrent layers*—This increases the representational power of the
    model (at the cost of higher computational loads).'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*叠加递归层* —— 这增加了模型的表征能力（但增加了计算负载的成本）。'
- en: '*Bidirectional recurrent layers*—These present the same information to a recurrent
    network in different ways, increasing accuracy and mitigating forgetting issues.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*双向递归层* —— 这些以不同方式向递归网络提供相同的信息，增加了准确性并减轻了遗忘问题。'
- en: We’ll use these techniques to refine our temperature-forecasting RNN.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这些技术来完善我们的温度预测 RNN。
- en: 10.4.1 Using recurrent dropout to fight overfitting
  id: totrans-467
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用递归丢失来对抗过拟合
- en: 'Let’s go back to the LSTM-based model we used in section 10.2.5—our first model
    able to beat the common-sense baseline. If you look at the training and validation
    curves ([figure 10.5](#fig10-5)), it’s evident that the model is quickly overfitting,
    despite having only very few units: the training and validation losses start to
    diverge considerably after a few epochs. You’re already familiar with a classic
    technique for fighting this phenomenon: dropout, which randomly zeros out input
    units of a layer to break happenstance correlations in the training data that
    the layer is exposed to. But how to correctly apply dropout in recurrent networks
    isn’t a trivial question.'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到我们在 10.2.5 节中使用的基于 LSTM 的模型——我们的第一个能够击败常识基线的模型。如果你看一下训练和验证曲线（[图 10.5](#fig10-5)），很明显模型很快就开始过拟合了，尽管只有很少的单元：几个周期后，训练和验证损失开始明显发散。你已经熟悉了对抗这种现象的经典技术：辍学，它随机将一层的输入单元归零，以打破训练数据中的偶然相关性。但如何在循环网络中正确应用辍学并不是一个微不足道的问题。
- en: 'It has long been known that applying dropout before a recurrent layer hinders
    learning rather than helping with regularization. In 2016, Yarin Gal, as part
    of his PhD thesis on Bayesian deep learning,^([5](#endnote5)) determined the proper
    way to use dropout with a recurrent network: the same dropout mask (the same pattern
    of dropped units) should be applied at every time step, instead of using a dropout
    mask that varies randomly from time step to time step. What’s more, to regularize
    the representations formed by the recurrent gates of layers such as layer_gru()
    and layer_lstm(), a temporally constant dropout mask should be applied to the
    inner recurrent activations of the layer (a recurrent dropout mask). Using the
    same dropout mask at every time step allows the network to properly propagate
    its learning error through time; a temporally random dropout mask would disrupt
    this error signal and be harmful to the learning process.'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 长久以来人们都知道，在循环层之前应用辍学会阻碍学习，而不是帮助正则化。在 2016 年，Yarin Gal 在他的博士论文中，关于贝叶斯深度学习，^([5](#endnote5))
    确定了在循环网络中正确使用辍学的方法：应该在每个时间步骤上应用相同的辍学掩码（相同的丢弃单元模式），而不是使用随机变化的辍学掩码。此外，为了对循环门（如 layer_gru()
    和 layer_lstm()）形成的表示进行正则化，应该将一个时间恒定的辍学掩码应用于该层的内部循环激活（循环辍学掩码）。在每个时间步上使用相同的辍学掩码允许网络正确传播其学习错误；时间上随机的辍学掩码会干扰这种错误信号，并对学习过程有害。
- en: 'Yarin Gal did his research using Keras and helped build this mechanism directly
    into Keras recurrent layers. Every recurrent layer in Keras has two dropout-related
    arguments: dropout, a float specifying the dropout rate for input units of the
    layer, and recurrent_dropout, specifying the dropout rate of the recurrent units.
    Let’s add recurrent dropout to the layer_lstm() of our first LSTM example and
    see how doing so impacts overfitting.'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: Yarin Gal 在 Keras 中进行了研究，并直接将这种机制集成到了 Keras 的循环层中。Keras 中的每个循环层都有两个与辍学相关的参数：dropout，指定该层输入单元的辍学率，和
    recurrent_dropout，指定循环单元的辍学率。让我们在我们第一个 LSTM 示例的 layer_lstm() 中添加循环辍学，看看这样做如何影响过拟合。
- en: Thanks to dropout, we won’t need to rely as much on network size for regularization,
    so we’ll use an LSTM layer with twice as many units, which should, hopefully,
    be more expressive (without dropout, this network would have started overfitting
    right away—try it). Because networks being regularized with dropout always take
    much longer to fully converge, we’ll train the model for five times as many epochs.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 多亏了辍学，我们不需要过多依赖网络大小来进行正则化，因此我们将使用两倍单元数的 LSTM 层，希望能够更具表现力（如果没有辍学，该网络将立即开始过拟合——可以尝试一下）。由于使用辍学进行正则化的网络总是需要更长时间才能完全收敛，所以我们将对模型进行五倍的训练周期。
- en: '**Listing 10.21 Training and evaluating a dropout-regularized LSTM**'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: '**图示 10.21 训练和评估带有辍学正则化的 LSTM**'
- en: inputs <- layer_input(shape = c(sequence_length, ncol_input_data))
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: inputs <- layer_input(shape = c(sequence_length, ncol_input_data))
- en: outputs <- inputs %>%
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: outputs <- inputs %>%
- en: layer_lstm(32, recurrent_dropout = 0.25) %>%
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: layer_lstm(32, recurrent_dropout = 0.25) %>%
- en: layer_dropout(0.5) %>%➊
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dropout(0.5) %>%➊
- en: layer_dense(1)
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(1)
- en: model <- keras_model(inputs, outputs)
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model(inputs, outputs)
- en: callbacks <- list(callback_model_checkpoint("jena_lstm_dropout.keras", save_best_only
    = TRUE))
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: callbacks <- list(callback_model_checkpoint("jena_lstm_dropout.keras", save_best_only
    = TRUE))
- en: model %>% compile(optimizer = "rmsprop",
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% compile(optimizer = "rmsprop",
- en: loss = "mse",
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: loss = "mse",
- en: metrics = "mae")
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = "mae")
- en: history <- model %>% fit(
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: history <- model %>% fit(
- en: train_dataset,
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: train_dataset,
- en: epochs = 50,
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 50,
- en: validation_data = val_dataset,
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: validation_data = val_dataset,
- en: callbacks = callbacks
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: callbacks = callbacks
- en: )
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: ➊ **To regularize the dense layer, we also add a dropout layer after the LSTM.**
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **为了对稠密层进行正则化，我们在 LSTM 之后添加了一个丢弃层。**
- en: '[Figure 10.11](#fig10-11) shows the results. Success! We’re no longer overfitting
    during the first 15 epochs. We achieve a validation MAE as low as 2.37 degrees
    (2.5% improvement over the no-learning baseline) and a test MAE of 2.45 degrees
    (6.5% improvement over the baseline). Not too bad.'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10.11](#fig10-11) 显示了结果。成功！在前 15 个 epoch 中，我们不再过拟合。我们的验证 MAE 可以降低到 2.37
    度（相比没有学习的基准线改善了 2.5%），测试 MAE 为 2.45 度（相比基准线改善了 6.5%）。还不错。'
- en: '![Image](../images/f0326-01.jpg)'
  id: totrans-491
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0326-01.jpg)'
- en: '**Figure 10.11 Training and validation loss on the Jena temperature-forecasting
    task with a dropout-regularized LSTM**'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 10.11：使用丢弃正则化的 LSTM 在 Jena 温度预测任务上的训练和验证损失**'
- en: '**RNN runtime performance**'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: '**RNN 运行时性能**'
- en: Recurrent models with very few parameters, like the ones in this chapter, tend
    to be significantly faster on a multicore CPU than on GPU, because they involve
    only small matrix multiplications, and the chain of multiplications is not well
    parallelizable due to the presence of a for loop. But larger RNNs can greatly
    benefit from a GPU runtime.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 与第几章中介绍的那种参数极少的循环模型相比，使用多核 CPU 上的 RNN 层通常会快得多，因为它们只涉及小矩阵乘法，并且乘法链由于存在循环而无法很好地并行化。但是，较大的
    RNN 可以极大地受益于 GPU 的运行时。
- en: When using a Keras LSTM or GRU layer on GPU with default arguments, your layer
    will be leveraging a cuDNN kernel, a highly optimized, low-level, NVIDIA-provided
    implementation of the underlying algorithm (I mentioned these in the previous
    chapter).
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 Keras LSTM 或 GRU 层在 GPU 上使用默认参数时，你的层将利用 cuDNN 核心，这是一个高度优化的、低水平的、由 NVIDIA
    提供的底层算法实现（我在上一章中提到过这些）。
- en: 'As usual, cuDNN kernels are a mixed blessing: they’re fast, but inflexible—if
    you try to do anything not supported by the default kernel, you will suffer a
    dramatic slowdown, which more or less forces you to stick to what NVIDIA happens
    to provide.'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，cuDNN 核心是个双刃剑：它们快，但不灵活 —— 如果你尝试做一些默认核心不支持的事情，你将遭受劇烈的减速，这几乎强迫你坚持 NVIDIA
    提供的功能。
- en: For instance, recurrent dropout isn’t supported by the LSTM and GRU cuDNN kernels,
    so adding it to your layers forces the runtime to fall back to the regular TensorFlow
    implementation, which is generally two to five times slower on GPU (even though
    its computational cost is the same).
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，循环丢弃不支持 LSTM 和 GRU 的 cuDNN 核心，因此将其添加到你的层会强制运行时回退到常规的 TensorFlow 实现，其在 GPU
    上通常比传统实现要慢两到五倍（即使它的计算成本是相同的）。
- en: 'As a way to speed up your RNN layer when you can’t use cuDNN, you can try *unrolling*
    it. Unrolling a for loop consists of removing the loop and simply inlining its
    content *N* times. In the case of the for loop of an RNN, unrolling can help TensorFlow
    optimize the underlying computation graph. However, it will also considerably
    increase the memory consumption of your RNN. As such, it’s viable for only relatively
    small sequences (around 100 steps or fewer). Also, note that you can do this only
    if the number of time steps in the data is known in advance by the model (that
    is to say, if you pass a shape without any NA entries to your initial layer_input()).
    It works like this:'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 当无法使用 cuDNN 加速你的 RNN 层时，你可以尝试*展开*它。展开一个循环包括删除该循环，并将其内容简单内联化*N*次。在 RNN 的循环中，展开可以帮助
    TensorFlow 优化底层的计算图。然而，它也会显著增加 RNN 的内存消耗。因此，它只适用于相对较小的序列（约 100 个步长或更少）。另外，请注意，只有当模型预知数据中的时间步数时才能这样做（也就是说，如果你向初始层的
    layer_input() 传递一个没有任何 NA 条目的形状）。它的工作原理如下：
- en: inputs <- layer_input(shape = c(sequence_length, num_features))➊
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: inputs <- layer_input(shape = c(sequence_length, num_features))➊
- en: x <- inputs %>%
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: x <- inputs %>%
- en: layer_lstm(32, recurrent_dropout = 0.2, unroll = TRUE)➋
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: layer_lstm(32, recurrent_dropout = 0.2, unroll = TRUE)➋
- en: ➊ **sequence_length cannot be NA.**
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **sequence_length 不能是 NA。**
- en: ➋ **Pass unroll = TRUE to enable unrolling.**
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **传递 unroll = TRUE 启用展开。**
- en: 10.4.2 Stacking recurrent layers
  id: totrans-504
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4.2 堆叠循环层
- en: 'Because you’re no longer overfitting but seem to have hit a performance bottleneck,
    you should consider increasing the capacity and expressive power of the network.
    Recall the description of the universal machine learning workflow: it’s generally
    a good idea to increase the capacity of your model until overfitting becomes the
    primary obstacle (assuming you’re already taking basic steps to mitigate overfitting,
    such as using dropout). As long as you aren’t overfitting too badly, you’re likely
    under capacity.'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 因为你不再过度拟合，但似乎遇到了性能瓶颈，所以你应该考虑增加网络的容量和表达能力。回想一下通用的机器学习工作流程的描述：增加模型的容量直到过度拟合成为主要障碍是一个好主意（假设您已经采取了基本步骤来减轻过度拟合，比如使用丢弃）。只要你没有过度拟合得太严重，你很可能是在容量不足。
- en: 'Increasing network capacity is typically done by increasing the number of units
    in the layers or adding more layers. Recurrent layer stacking is a classic way
    to build more powerful recurrent networks: for instance, not too long ago the
    Google Translate algorithm was powered by a stack of seven large LSTM layers—that’s
    huge.'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 增加网络容量通常是通过增加层中的单元数或添加更多层来完成的。递归层堆叠是构建更强大的递归网络的经典方法：例如，不久前，Google 翻译算法是由七个大型
    LSTM 层堆叠组成的，这是一个巨大的网络。
- en: To stack recurrent layers on top of each other in Keras, all intermediate layers
    should return their full sequence of outputs (a rank 3 tensor) rather than their
    output at the last time step. As you’ve already learned, this is done by specifying
    return_ sequences = TRUE.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 中堆叠递归层时，所有中间层都应返回它们的完整输出序列（一个三阶张量），而不是它们在最后一个时间步的输出。就像你已经学过的那样，这是通过指定
    return_sequences = TRUE 来完成的。
- en: In the following example, we’ll try a stack of two dropout-regularized recurrent
    layers. For a change, we’ll use Gated Recurrent Unit (GRU) layers instead of LSTM.
    GRU is very similar to LSTM—you can think of it as a slightly simpler, streamlined
    version of the LSTM architecture. It was introduced in 2014 by Cho et al. when
    recurrent networks were just starting to gain interest anew in the then-tiny research
    community.^([6](#endnote6))
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，我们将尝试一组两个带有丢弃正则化的递归层。为了改变一下，我们将使用门控循环单元（GRU）层，而不是 LSTM。GRU 与 LSTM 非常相似
    —— 你可以把它看作是 LSTM 架构的稍微简化、简化版本。它由 Cho 等人在 2014 年提出，当时递归网络才开始重新引起当时小规模研究社区的兴趣^([6](#endnote6))。
- en: '**Listing 10.22 Training and evaluating a dropout-regularized, stacked GRU
    model**'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 10.22 训练和评估一个带有丢弃正则化的堆叠 GRU 模型**'
- en: inputs <- layer_input(shape = c(sequence_length, ncol_input_data))
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: inputs <- layer_input(shape = c(sequence_length, ncol_input_data))
- en: outputs <- inputs %>%
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: outputs <- inputs %>%
- en: layer_gru(32, recurrent_dropout = 0.5, return_sequences = TRUE) %>%
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: layer_gru(32, recurrent_dropout = 0.5, return_sequences = TRUE) %>%
- en: layer_gru(32, recurrent_dropout = 0.5) %>%
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: layer_gru(32, recurrent_dropout = 0.5) %>%
- en: layer_dropout(0.5) %>%
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dropout(0.5) %>%
- en: layer_dense(1)
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(1)
- en: model <- keras_model(inputs, outputs)
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model(inputs, outputs)
- en: callbacks <- list(
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: callbacks <- list(
- en: callback_model_checkpoint("jena_stacked_gru_dropout.keras", save_best_only =
    TRUE)
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: callback_model_checkpoint("jena_stacked_gru_dropout.keras", save_best_only =
    TRUE)
- en: )
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: model %>% compile(optimizer = "rmsprop",
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% compile(optimizer = "rmsprop",
- en: loss = "mse",
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: loss = "mse",
- en: metrics = "mae")
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = "mae")
- en: history <- model %>% fit(
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: history <- model %>% fit(
- en: train_dataset,
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: train_dataset,
- en: epochs = 50,
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 50,
- en: validation_data = val_dataset,
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: validation_data = val_dataset,
- en: callbacks = callbacks
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: callbacks = callbacks
- en: )
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: 'model <- load_model_tf("jena_stacked_gru_dropout.keras") sprintf("Test MAE:
    %.2f", evaluate(model, test_dataset)["mae"])'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: model <- load_model_tf("jena_stacked_gru_dropout.keras") sprintf("测试 MAE：%.2f",
    evaluate(model, test_dataset)["mae"])
- en: '[1] "Test MAE: 2.42"'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] "测试 MAE：2.42"'
- en: '[Figure 10.12](#fig10-12) shows the results. We achieve a test MAE of 2.42
    degrees (a 7.6% improvement over the baseline). You can see that the added layer
    does improve the results a bit, though not dramatically. You may be seeing diminishing
    returns from increasing network capacity at this point.'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10.12](#fig10-12) 显示了结果。我们实现了 2.42 度的测试 MAE（比基线提高了 7.6%）。您可以看到添加的层确实稍微改善了结果，尽管没有戏剧性的改变。您可能会看到在这一点上增加网络容量的回报正在递减。'
- en: '![Image](../images/f0329-01.jpg)'
  id: totrans-532
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0329-01.jpg)'
- en: '**Figure 10.12 Training and validation loss on the Jena temperature-forecasting
    task with a stacked GRU network**'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 10.12 Jena 温度预测任务上堆叠 GRU 网络的训练和验证损失**'
- en: 10.4.3 Using bidirectional RNNs
  id: totrans-534
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4.3 使用双向 RNN
- en: The last technique we’ll look at in this section is the *bidirectional RNN*.
    A bidirectional RNN is a common RNN variant that can offer greater performance
    than a regular RNN on certain tasks. It’s frequently used in natural language
    processing—you could call it the Swiss Army knife of deep learning for natural
    language processing.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 本节我们将要看的最后一种技术是*双向 RNN*。双向 RNN 是常见的 RNN 变体，在某些任务上可以提供比常规 RNN 更好的性能。它经常用于自然语言处理——您可以称其为自然语言处理的瑞士军刀。
- en: 'RNNs are notably order dependent: they process the time steps of their input
    sequences in order, and shuffling or reversing the time steps can completely change
    the representations the RNN extracts from the sequence. This is precisely the
    reason they perform well on problems where order is meaningful, such as the temperature-forecasting
    problem. A bidirectional RNN exploits the order sensitivity of RNNs: it uses two
    regular RNNs, such as the GRU and LSTM layers you’re already familiar with, each
    of which processes the input sequence in one direction (chronologically and antichronologically),
    and then merges their representations. By processing a sequence both ways, a bidirectional
    RNN can catch patterns that may be overlooked by a unidirectional RNN.'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs（循环神经网络）明显依赖于顺序：它们按顺序处理其输入序列的时间步长，对时间步长进行混洗或反转可能会完全改变 RNN 从序列中提取的表示。这恰恰是它们在顺序具有意义的问题上表现良好的原因，比如温度预测问题。双向
    RNN 利用了 RNN 的顺序敏感性：它使用两个常规 RNN，例如你已经熟悉的 GRU 和 LSTM 层，每个都按一定方向（按时间顺序和逆时间顺序）处理输入序列，然后合并它们的表示。通过双向处理序列，双向
    RNN 可以捕捉到单向 RNN 可能忽略的模式。
- en: Remarkably, the fact that the RNN layers in this section have processed sequences
    in chronological order (with older time steps first) may have been an arbitrary
    decision.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 引人注目的是，本节中的 RNN 层以时间顺序（以较早的时间步长优先）处理序列的事实可能是一个任意的决定。
- en: 'At least it’s a decision we’ve made no attempt to question so far. Could the
    RNNs have performed well enough if they processed input sequences in antichronological
    order, for instance (with newer time steps first)? Let’s try this and see what
    happens. All you need to do is modify the TF Dataset so the input sequences are
    reverted along the time dimension. Just transform the dataset with dataset_map()
    like this:'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 至少到目前为止，我们没有试图质疑的决定。例如，如果 RNN 按逆时间顺序处理输入序列（首先是更新的时间步长），它们是否能够表现良好呢？让我们尝试一下，看看会发生什么。您只需要修改
    TF 数据集，使输入序列沿时间维度反转即可。只需像这样用 dataset_map() 转换数据集：
- en: ds %>%
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: ds %>%
- en: dataset_map(function(samples, targets) {
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: dataset_map(function(samples, targets) {
- en: list(samples[, NA:NA:-1, ], targets)
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: list(samples[, NA:NA:-1, ], targets)
- en: '})'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: '})'
- en: Training the same LSTM-based model that you used in the first experiment in
    this section, you get the results shown in [figure 10.13](#fig10-13).
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 训练与本节第一个实验中使用的相同基于 LSTM 的模型，您将获得 [图 10.13](#fig10-13) 中显示的结果。
- en: '![Image](../images/f0330-01.jpg)'
  id: totrans-544
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0330-01.jpg)'
- en: '**Figure 10.13 Training and validation loss on the Jena temperature-forecasting
    task with an LSTM trained on reversed sequences**'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 10.13 使用 LSTM 在 Jena 温度预测任务上的训练和验证损失，训练的是反转序列**'
- en: 'The reversed-order LSTM strongly underperforms even the common-sense baseline,
    indicating that in this case, chronological processing is important to the success
    of the approach. This makes perfect sense: the underlying LSTM layer will typically
    be better at remembering the recent past than the distant past, and naturally
    the more recent weather data points are more predictive than older data points
    for the problem (that’s what makes the common-sense baseline fairly strong). Thus
    the chronological version of the layer is bound to outperform the reversed-order
    version.'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 反转顺序的 LSTM 明显表现不佳，甚至不如常识基线，表明在这种情况下，顺序处理对方法的成功很重要。这是很有道理的：底层 LSTM 层通常更擅长记住最近的过去，而不是遥远的过去，自然地，更近期的天气数据点比较旧的数据点更具有预测性（这就是常识基线相当强的原因）。因此，层的时间顺序版本必定会胜过反转顺序版本。
- en: 'However, this isn’t true for many other problems, including natural language:
    intuitively, the importance of a word in understanding a sentence isn’t usually
    dependent on its position in the sentence. On text data, reversed-order processing
    works just as well as chronological processing—you can read text backward just
    fine (try it!). Although word order does matter in understanding language, *which
    order* you use isn’t crucial. Importantly, an RNN trained on reversed sequences
    will learn different representations than one trained on the original sequences,
    much as you would have different mental models if time flowed backward in the
    real world—if you lived a life where you died on your first day and were born
    on your last day. In machine learning, representations that are *different* yet
    *useful* are always worth exploiting, and the more they differ, the better: they
    offer a new angle from which to look at your data, capturing aspects of the data
    that were missed by other approaches, and thus they can help boost performance
    on a task. This is the intuition behind *ensembling*, a concept we’ll explore
    in chapter 13.'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，对于包括自然语言在内的许多其他问题来说，并不是这样。直观上讲，理解一个句子中的一个词通常并不依赖于它在句子中的位置。在文本数据中，逆序处理和顺序处理一样有效——你可以很好地反向阅读文本（试试看！）。尽管词序在理解语言方面确实很重要，但是使用的顺序并不关键。重要的是，用于反向序列训练的
    RNN 会学习到与用于原始序列训练的 RNN 不同的表示，就像在现实世界中，如果时间倒流，你会有与时间正常流动时完全不同的心理模型——如果你在你生命的最后一天出生并在你的第一天死去的话。在机器学习中，那些*不同*但*有用*的表示总是值得利用的，它们的区别越大，越好：它们提供了一个新的角度来观察数据，捕捉到了其他方法忽略的数据特征，因此它们可以帮助改善任务的性能。这就是集成学习的直觉，我们将在第13章中探讨这个概念。
- en: A bidirectional RNN exploits this idea to improve on the performance of chronological-order
    RNNs. It looks at its input sequence both ways (see [figure 10.14](#fig10-14)),
    obtaining potentially richer representations and capturing patterns that may have
    been missed by the chronological-order version alone.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 双向 RNN 利用这个思想来改进按照时间顺序的 RNN 的性能。它双向查看输入序列（请参见[图 10.14](#fig10-14)），获得可能更丰富的表示，并捕捉到只有按时间顺序的版本可能会错过的模式。
- en: To instantiate a bidirectional RNN in Keras, you use the bidirectional() layer,
    which takes as its first argument a recurrent layer instance. bidirectional()
    creates a second, separate instance of this recurrent layer and uses one instance
    for processing the input sequences in chronological order and the other instance
    for processing the input sequences in reversed order. You can try it on our temperature-forecasting
    task.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 中实例化一个双向 RNN，你可以使用 `bidirectional()` 层，它的第一个参数是一个循环层实例。`bidirectional()`
    创建第二个、单独的循环层实例，并使用一个实例按照时间顺序处理输入序列，使用另一个实例按照反向顺序处理输入序列。你可以在我们的温度预测任务中试一下。
- en: '![Image](../images/f0331-01.jpg)'
  id: totrans-550
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0331-01.jpg)'
- en: '**Figure 10.14 How a bidirectional RNN layer works**'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 10.14 双向 RNN 层的工作原理**'
- en: '**Listing 10.23 Training and evaluating a bidirectional LSTM**'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: '**列表 10.23 训练和评估双向 LSTM**'
- en: inputs <- layer_input(shape = c(sequence_length, ncol_input_data))
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: inputs <- layer_input(shape = c(sequence_length, ncol_input_data))
- en: outputs <- inputs %>%➊
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: outputs <- inputs %>%➊
- en: bidirectional(layer_lstm(units = 16)) %>%
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: bidirectional(layer_lstm(units = 16)) %>%
- en: layer_dense(1)
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(1)
- en: model <- keras_model(inputs, outputs)
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model(inputs, outputs)
- en: model %>% compile(optimizer = "rmsprop",
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% compile(optimizer = "rmsprop",
- en: loss = "mse",
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: loss = "mse",
- en: metrics = "mae")
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = "mae")
- en: history <- model %>%
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: history <- model %>%
- en: fit(train_dataset,
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: fit(train_dataset,
- en: epochs = 10,
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 10,
- en: validation_data = val_dataset)
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: validation_data = val_dataset)
- en: ➊ **Note that layer_lstm() is not composed with inputs directly.**
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **请注意，`layer_lstm()` 不是直接与 `inputs` 组合在一起的。**
- en: 'You’ll find that it doesn’t perform as well as the plain layer_lstm(). It’s
    easy to understand why: all the predictive capacity must come from the chronological
    half of the network, because the antichronological half is known to be severely
    underperforming on this task (again, because the recent past matters much more
    than the distant past, in this case). At the same time, the presence of the antichronological
    half doubles the network’s capacity and causes it to start overfitting much earlier.'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现它的表现不如普通的 `layer_lstm()`。很容易理解为什么：因为反向半部分在这个任务上明显表现不佳（因为在这种情况下，近期的影响要比远期的影响大得多），所以所有的预测能力必须来自于网络的正向半部分。同时，反向半部分的存在使得网络的容量加倍，并导致网络过拟合的时间提前。
- en: However, bidirectional RNNs are a great fit for text data, or any other kind
    of data where order matters, yet where *which order* you use doesn’t matter. In
    fact, for a while in 2016, bidirectional LSTMs were considered the state of the
    art on many natural language processing tasks (before the rise of the Transformer
    architecture, which you will learn about in the next chapter).
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，双向RNN非常适合文本数据，或者任何其他需要考虑顺序的数据，但是使用的顺序却无关紧要的数据。事实上，在2016年的一段时间里，双向LSTM被认为是许多自然语言处理任务的最新技术（在Transformer架构的兴起之前，你将在下一章学到有关该架构的内容）。
- en: 10.4.4 Going even further
  id: totrans-568
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4.4 更进一步
- en: 'There are many other things you could try in order to improve performance on
    the temperature-forecasting problem:'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以尝试许多其他方法来提高温度预测问题的性能：
- en: Adjust the number of units in each recurrent layer in the stacked setup, as
    well as the amount of dropout. The current choices are largely arbitrary and thus
    probably suboptimal.
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整堆叠设置中每个递归层中的单位数量，以及dropout的数量。目前的选择主要是任意的，因此可能是次优的。
- en: Adjust the learning rate used by the RMSprop optimizer, or try a different optimizer.
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整RMSprop优化器使用的学习率，或尝试其他优化器。
- en: Try using a stack of layer_dense() as the regressor on top of the recurrent
    layer, instead of a single layer_dense().
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试将`layer_dense()`堆叠作为递归层之上的回归器，而不是单个`layer_dense()`。
- en: 'Improve the input to the model: try using longer or shorter sequences or a
    different sampling rate, or start doing feature engineering'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改进模型的输入：尝试使用更长或更短的序列或不同的采样率，或开始进行特征工程
- en: As always, deep learning is more an art than a science. We can provide guidelines
    that suggest what is likely to work or not work on a given problem, but, ultimately,
    every dataset is unique; you’ll have to evaluate different strategies empirically.
    There is currently no theory that will tell you in advance precisely what you
    should do to optimally solve a problem. You must iterate.
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，深度学习更像是一门艺术而不是科学。我们可以提供指导方针，建议在特定问题上可能有效或无效的方法，但是，最终，每个数据集都是独一无二的；你必须以经验为依据评估不同的策略。目前没有理论可以事先告诉你如何才能最佳地解决问题。你必须不断迭代。
- en: 'In my experience, improving on the no-learning baseline by about 10% is likely
    the best you can do with this dataset. This isn’t so great, but these results
    make sense: while near-future weather is highly predictable if you have access
    to data from a wide grid of different locations, it’s not very predictable if
    you have measurements only from a single location. The evolution of the weather
    where you are depends on current weather patterns in surrounding locations.'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 依我所见，通过大约10%的改进来超过无学习基线可能是你在这个数据集上能做到的最好的。这并不算太好，但这些结果是有道理的：如果你可以访问来自不同位置广阔网格的数据，那么接近未来的天气是高度可预测的，但如果你只有来自单个位置的测量数据，那么天气的演变就不太可预测。你所处的地方的天气演变取决于周围地区的当前天气模式。
- en: '**Markets and machine learning**'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: '**市场与机器学习**'
- en: Some readers are bound to want to take the techniques I’ve introduced here and
    try them on the problem of forecasting the future price of securities on the stock
    market (or currency exchange rates, and so on). However, markets have very different
    statistical characteristics than natural phenomena such as weather patterns. When
    it comes to markets, past performance is *not* a good predictor of future returns—looking
    in the rear-view mirror is a bad way to drive. Machine learning, on the other
    hand, is applicable to datasets where the past *is* a good predictor of the future,
    like weather, electricity consumption, or foot traffic at a store.
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 一些读者肯定会想要将我在这里介绍的技术应用于预测股票市场上证券的未来价格（或者货币汇率等）。然而，市场具有与天气模式等自然现象非常不同的统计特征。在涉及市场时，过去的表现*不*是未来回报的良好预测指标——通过后视镜看路是开车的不好方式。另一方面，机器学习适用于过去*是*未来的良好预测指标的数据集，比如天气、电力消耗或商店的客流量。
- en: 'Always remember that all trading is fundamentally *information arbitrage*:
    gaining an advantage by leveraging data or insights that other market participants
    are missing. Trying to use well-known machine learning techniques and publicly
    available data to beat the markets is effectively a dead end, because you won’t
    have any information advantage compared to everyone else. You’re likely to waste
    your time and resources with nothing to show for it.'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 请始终记住，所有交易基本上都是*信息套利*：通过利用其他市场参与者所错过的数据或见解来获得优势。试图使用众所周知的机器学习技术和公开可用的数据来击败市场实际上是一条死路，因为您与其他人相比没有任何信息优势。您可能会浪费时间和资源，却一无所获。
- en: Summary
  id: totrans-579
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: As you first learned in chapter 5, when approaching a new problem, it’s good
    to first establish common-sense baselines for your metric of choice. If you don’t
    have a baseline to beat, you can’t tell whether you’re making real progress.
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正如你在第5章中首次学到的，当面对一个新问题时，首先为你选择的指标建立常识基线是很好的。如果你没有一个要超越的基线，那么你无法判断自己是否在取得真正的进步。
- en: Try simple models before expensive ones, to make sure the additional expense
    is justified. Sometimes a simple model will turn out to be your best option.
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在使用昂贵模型之前，请尝试简单模型，以确保额外的开支是合理的。有时，一个简单模型会被证明是你的最佳选择。
- en: When you have data where ordering matters, and in particular for time-series
    data, *recurrent networks* are a great fit and easily outperform models that first
    flatten the temporal data. The two essential RNN layers available in Keras are
    the LSTM layer and the GRU layer.
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当您有数据的顺序很重要，特别是对于时间序列数据时，*循环网络*是一个很好的选择，并且很容易胜过首先展平时间数据的模型。Keras 中提供的两个基本 RNN
    层是 LSTM 层和 GRU 层。
- en: To use dropout with recurrent networks, you should use a time-constant dropout
    mask and recurrent dropout mask. These are built into Keras recurrent layers,
    so all you have to do is use the recurrent_dropout arguments of recurrent layers.
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要在循环网络中使用 dropout，您应该使用时间恒定的 dropout 掩码和循环 dropout 掩码。这些都内置在 Keras 循环层中，所以您只需使用循环层的
    recurrent_dropout 参数即可。
- en: Stacked RNNs provide more representational power than a single RNN layer. They’re
    also much more expensive and thus not always worth it. Although they offer clear
    gains on complex problems (such as machine translation), they may not always be
    relevant to smaller, simpler problems.
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 堆叠的 RNN 提供比单个 RNN 层更多的表示能力。它们也要昂贵得多，因此并不总是值得。尽管它们在复杂问题（如机器翻译）上提供了明显的收益，但它们并不总是与较小、较简单的问题相关。
- en: ^([1](#Rendnote1)) Adam Erickson and Olaf Kolle, [http://www.bgc-jena.mpg.de/wetter](http://www.bgc-jena.mpg.de/wetter).
  id: totrans-585
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ^([1](#Rendnote1)) Adam Erickson 和 Olaf Kolle，[http://www.bgc-jena.mpg.de/wetter](http://www.bgc-jena.mpg.de/wetter)。
- en: ^([2](#Rendnote2)) Note that there isn’t a layer_separable_conv_3d(), not for
    any theoretical reason, but simply because I haven’t implemented it.
  id: totrans-586
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ^([2](#Rendnote2)) 请注意，没有 layer_separable_conv_3d()，这不是出于任何理论原因，而只是因为我还没有实现它。
- en: ^([3](#Rendnote3)) See, for example, Yoshua Bengio, Patrice Simard, and Paolo
    Frasconi, “Learning Long-Term Dependencies with Gradient Descent Is Difficult,”
    *IEEE Transactions on Neural Networks* 5, no. 2 (1994).
  id: totrans-587
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ^([3](#Rendnote3)) 请参阅，例如，Yoshua Bengio，Patrice Simard 和 Paolo Frasconi 的“使用梯度下降学习长期依赖关系”，*IEEE
    Transactions on Neural Networks* 5，第2号（1994）。
- en: ^([4](#Rendnote4)) Sepp Hochreiter and Jürgen Schmidhuber, “Long Short-Term
    Memory,” *Neural Computation* 9, no. 8 (1997).
  id: totrans-588
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ^([4](#Rendnote4)) Sepp Hochreiter 和 Jürgen Schmidhuber，“长短期记忆”，*神经计算* 9，第8号（1997）。
- en: ^([5](#Rendnote5)) See Yarin Gal, “Uncertainty in Deep Learning,” PhD thesis
    (2016), [http://mng.bz/WBq1](http://mng.bz/WBq1).
  id: totrans-589
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ^([5](#Rendnote5)) 请参阅 Yarin Gal 的“深度学习中的不确定性”博士论文（2016），[http://mng.bz/WBq1](http://mng.bz/WBq1)。
- en: '^([6](#Rendnote6)) See Cho et al., “On the Properties of Neural Machine Translation:
    Encoder-Decoder Approaches” (2014), [https://arxiv.org/abs/1409.1259](https://arxiv.org/abs/1409.1259).'
  id: totrans-590
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ^([6](#Rendnote6)) 请参阅 Cho 等人的“关于神经机器翻译的性质：编码器-解码器方法”（2014），[https://arxiv.org/abs/1409.1259](https://arxiv.org/abs/1409.1259)。
