- en: 4 Dipping toes in deep learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 涉足深度学习
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Implementing and training fully connected neural networks using Keras
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Keras 实现和训练全连接神经网络
- en: Implementing and training convolutional neural networks to classify images
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现和训练卷积神经网络以对图像进行分类
- en: Implementing and training a recurrent neural network to solve a time-series
    problem
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现和训练递归神经网络以解决时间序列问题
- en: In chapter 3, you learned about the different model-building APIs provided by
    TensorFlow and their advantages and disadvantages. You also learned about some
    of the options in TensorFlow to retrieve and manipulate data. In this chapter,
    you will learn how to leverage some of that to build deep neural networks and
    use them to solve problems.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 3 章，您了解了 TensorFlow 提供的不同模型构建 API 及其优缺点。您还了解了 TensorFlow 中一些检索和操作数据的选项。在本章中，您将学习如何利用这些知识来构建深度神经网络，并使用它们来解决问题。
- en: '*Deep learning* is a broad term that has many different algorithms under its
    wings. Deep learning algorithms come in many different flavors and colors and
    can be classified by many criteria: the type of data they consume (e.g., structured
    data, images, time-series data), depth (shallow, deep, and very deep), and so
    on. The main types of deep networks we are going to discuss and implement are
    as follows:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*深度学习*是一个广泛的术语，它包含许多不同的算法。深度学习算法有许多不同的类型和颜色，可以根据许多标准进行分类：它们消耗的数据类型（例如，结构化数据、图像、时间序列数据）、深度（浅层、深层和非常深层）等等。我们将要讨论和实现的主要深度网络类型如下：'
- en: Fully connected networks (FCNs)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全连接网络（FCNs）
- en: Convolutional neural networks (CNNs)
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNNs）
- en: Recurrent neural networks (RNNs)
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 递归神经网络（RNNs）
- en: Being able to comfortably implement these neural networks is a key skill to
    be successful in the field, whether you are a graduate student, a data scientist,
    or a research scientist. This knowledge directly extends to becoming skillful
    in implementing more complex deep neural networks that deliver state-of-the-art
    performance in various problem domains.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 能够熟练实现这些神经网络是在该领域取得成功的关键技能，无论你是研究生、数据科学家还是研究科学家。这些知识直接延伸到如何熟练实现更复杂的深度神经网络，这些网络在各种问题领域提供了最先进的性能。
- en: 'In chapter 2, we discussed FCN and various operations in CNNs, such as convolution
    and pooling operations. In this chapter, you will see the FCNs again, as well
    as a holistic implementation of CNNs showing how convolution and pooling operations
    coalesce to form a CNN. Finally, you will learn about a new type of model: RNNs.
    RNNs are typically used to solve time-series problems, where the task is to learn
    patterns in data over time so that, by looking at the past patterns, we can leverage
    them to forecast the future. We will also see how RNNs are used to solve an exciting
    real-world time-series problem.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 2 章中，我们讨论了 FCN 和 CNN 中的各种操作，例如卷积和池化操作。在本章中，您将再次看到 FCNs，以及 CNNs 的整体实现，展示了卷积和池化操作如何合并形成
    CNN。最后，您将了解一个新类型的模型：RNNs。RNNs 通常用于解决时间序列问题，其中的任务是学习数据随时间变化的模式，以便通过查看过去的模式来预测未来。我们还将看到
    RNNs 如何用于解决一个有趣的现实世界时间序列问题。
- en: 4.1 Fully connected networks
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 全连接网络
- en: '*You* *have found some precious photos of your grandmother while going through
    some storage boxes you found in the attic. Unfortunately, they have seen better
    days. Most of the photos are scratched, smudged, and even torn. You know that
    recently deep networks have been used to restore old photos and videos. In the
    hope of restoring these photos, you decide to implement an image restoration model*
    using TensorFlow. You will first develop a model that can restore corrupted images
    of handwritten digits, as this data set is readily available, in order to understand
    the model and the training process. You believe an autoencoder model (a type of
    FCN) would be a great starting point. This autoencoder will have the following
    specifications:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*当您在阁楼找到一些存储盒时，里面有一些珍贵的祖母的照片。不幸的是，它们已经过时了。大多数照片都被划痕、污迹和甚至撕裂了。您知道最近已经使用了深度网络来恢复旧照片和视频。希望能恢复这些照片，您决定使用
    TensorFlow 实现图像恢复模型*。您首先将开发一个可以恢复手写数字损坏图像的模型，因为这个数据集是readily available，以便了解模型和训练过程。您认为自动编码器模型（一种
    FCN）将是一个很好的起点。这个自动编码器将具有以下规格：'
- en: Input layer with 784 nodes
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有 784 个节点的输入层
- en: A hidden layer with 64 nodes, having ReLU activation
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有 64 个节点的隐藏层，采用 ReLU 激活
- en: A hidden layer with 32 nodes, having ReLU activation
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个包含 32 个节点的隐藏层，使用 ReLU 激活函数
- en: A hidden layer with 64 nodes, having ReLU activation
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个包含 64 个节点的隐藏层，使用 ReLU 激活函数
- en: An output layer with 784 nodes with tanh activation
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个包含 784 个节点的输出层，使用 tanh 激活函数
- en: Hyperparameter optimization for deep learning
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的超参数优化
- en: You might have noticed that when defining neural networks, we are choosing structural
    hyperparameters (e.g., number of units in hidden layers) somewhat arbitrarily.
    These values have, in fact, been chosen empirically through a few rounds of trial
    and error.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，在定义神经网络时，我们选择结构超参数（例如，隐藏层中的单元数）有些是凭空选择的。实际上，这些值是通过几轮试错经验选择的。
- en: Typically, in machine learning, these hyperparameters are chosen using a principled
    approach, such as hyperparameter optimization. But hyperparameter optimization
    is an expensive process that needs to evaluate hundreds of models with different
    hyperparameter choices to choose the best set of hyperparameters. This makes it
    very difficult to use for deep learning methods, as these methods usually deal
    with large, complex models and large amounts of data.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在机器学习中，这些超参数是使用基于原则的方法选择的，例如超参数优化。但是，超参数优化是一个昂贵的过程，需要评估具有不同超参数选择的数百个模型，以选择最佳的超参数集。这使得它非常难以用于深度学习方法，因为这些方法通常涉及大型、复杂的模型和大量的数据。
- en: 'Therefore, in deep learning, you will commonly see the following trends, in
    order to limit the time spent on hyperparameter optimization:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在深度学习中，为了限制在超参数优化上花费的时间，你通常会看到以下趋势：
- en: Optimizing a subset of hyperparameters to limit the exploration space (e.g.,
    type of activation instead of number of hidden units, regularization parameters,
    etc.)
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化一部分超参数以限制探索空间（例如，激活类型而不是隐藏单元数量，正则化参数等）。
- en: Using robust optimizers, early stopping, learning rate decay, and so on, which
    are designed to reduce or prevent overfitting
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用健壮的优化器、早停、学习率衰减等方法，旨在减少或预防过拟合
- en: Using model specifications from published models that have delivered state-of-the-art
    performance
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用已发表的模型规范，这些模型提供了最先进的性能
- en: Following rules of thumb such as reducing the output size as you go deeper into
    the network
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 遵循一些经验法则，例如随着网络深入减少输出大小
- en: In this chapter, we will use model architectures chosen empirically. The focus
    of this chapter is to show how a given architecture can be implemented using TensorFlow
    2 and not to find the architectures themselves.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用经验选择的模型架构。本章的重点是展示如何使用 TensorFlow 2 实现给定的架构，而不是找到架构本身。
- en: Let’s examine the data we’ll use to implement the FCN.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下我们将用于实现 FCN 的数据。
- en: 4.1.1 Understanding the data
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.1 理解数据
- en: For this scenario, we will use the MNIST digit data set, a simple data set that
    contains the black-and-white images of hand-written digits and the corresponding
    labels representing the digits. Each image has a single digit and goes from 0-9\.
    Therefore, the data set has 10 different classes. Figure 4.1 shows several samples
    from the data set along with the digit it represents.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种情况，我们将使用 MNIST 数字数据集，这是一个简单的数据集，包含手写数字的黑白图像以及表示数字的对应标签。每个图像都有一个数字，从 0 到
    9。因此，数据集有 10 个不同的类别。图 4.1 显示了数据集中的几个样本及其表示的数字。
- en: '![04-01](../../OEBPS/Images/04-01.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![04-01](../../OEBPS/Images/04-01.png)'
- en: Figure 4.1 Sample digit images. Each image contains a number from 0 to 9.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1 样本数字图像。每个图像包含一个从 0 到 9 的数字。
- en: 'In TensorFlow, you can load the MNIST data set with a single line. Loading
    this data set has become an integral part of various machine learning libraries
    (including TensorFlow) due to its extremely common usage:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中，你可以用一行代码加载 MNIST 数据集。由于其极为常见的用法，加载此数据集已成为各种机器学习库（包括 TensorFlow）的重要组成部分：
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The load_data() method returns two tuples: training data and testing data.
    Here, we will only use the training images (i.e., x_train) data set. As we covered
    earlier, this is an unsupervised task. Because of that, we will not need the labels
    (i.e., y_train) of the images to complete this task.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '`load_data()`方法返回两个元组：训练数据和测试数据。在这里，我们只会使用训练图像（即，x_train）数据集。正如我们之前介绍的，这是一个无监督任务。因此，我们不需要图像的标签（即，y_train）来完成这个任务。'
- en: Better than MNIST?
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 比 MNIST 好吗？
- en: It should be noted that, due to advancements in the field of computer vision
    over the last decade, MNIST is considered too easy, where a test accuracy of more
    than 92% can be achieved with a simple logistic regression model ([http://mng.bz/j2l9](http://mng.bz/j2l9))
    and a 99.84% accuracy with a state-of-the-art model ([http://mng.bz/d2Pv](http://mng.bz/d2Pv)).
    Furthermore, it’s being overused in the computer vision community. Because of
    this, a new data set known as Fashion-MNIST ([https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist))
    has emerged. This is a black-and-white data set containing images belonging to
    10 classes. Instead of digits, it contains images of various fashion categories
    (e.g., T-shirt, sandal, bag, etc.), which poses a much harder problem than recognizing
    digits.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于过去十年计算机视觉领域的进展，MNIST被认为过于简单，简单的逻辑回归模型就可以实现超过92%的测试准确率（[http://mng.bz/j2l9](http://mng.bz/j2l9)），而最先进的模型则可以达到99.84%的准确率（[http://mng.bz/d2Pv](http://mng.bz/d2Pv)）。此外，它在计算机视觉社区中被过度使用。因此，一个名为Fashion-MNIST（[https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)）的新数据集应运而生。这是一个包含属于10个类别的图像的黑白数据集。与数字不同，它包含各种时尚类别的图像（例如T恤、凉鞋、包等），这比识别数字要困难得多。
- en: You can print x_train and y_train to understand those arrays a bit better using
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以打印x_train和y_train来更好地了解这些数组，使用
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This will produce
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'And do the same for y_train:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对y_train执行相同的操作：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This will give
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这将得到
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then we will do some basic data preprocessing. We will normalize all samples
    in the data set by bringing their pixel values from [0, 255] to [-1, 1]. This
    is done by subtracting 128 and dividing the result by 128 element-wise for all
    pixels. This is important because the final layer of the autoencoder has a tanh
    activation, which ranges between (-1, 1). Tanh is a nonlinear activation function
    like the sigmoid function, and for a given input, *x* is computed as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将进行一些基本的数据预处理。我们将通过将它们的像素值从[0, 255]归一化到[-1, 1]来规范化数据集中的所有样本。这是通过减去128并逐元素除以128来完成的。这很重要，因为自动编码器的最后一层具有tanh激活函数，其取值范围为(-1,
    1)。tanh是一个非线性激活函数，类似于sigmoid函数，对于给定的输入*x*，计算如下：
- en: '![04_01a](../../OEBPS/Images/04_01a.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![04_01a](../../OEBPS/Images/04_01a.png)'
- en: 'Therefore, we need to make sure we feed something to the model that is within
    the range of values that can be produced by the final layer. Also, if you look
    at the shape of x_train, you will see that it has a shape of (60000, 28, 28).
    The autoencoder takes a one-dimensional input, so we need to reshape the image
    to a one-dimensional vector of size 784\. Both these transformations can be achieved
    by the following line:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '因此，我们需要确保向模型提供的内容在最终层可以生成的值范围内。另外，如果您查看x_train的形状，您将看到它的形状为(60000, 28, 28)。自动编码器接受一维输入，因此我们需要将图像重塑为大小为784的一维向量。这两种转换可以通过以下行来实现:'
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, reshape([-1, 784]) will unwrap the two-dimensional images (size 28 × 28)
    in the data set to a single dimensional vector (size 784). When reshaping, you
    do not need to provide all the dimensions of the reshaped tensor. If you provide
    the sizes of all dimensions except one, NumPy can still infer the size of the
    missing dimension as it knows the dimensions of the original tensor. The dimension
    that you want NumPy to infer is denoted by -1.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，reshape([-1, 784])将数据集中的二维图像（大小为28×28）展开为一个单一维度的向量（大小为784）。在进行重塑时，您不需要提供重塑张量的所有维度。如果您仅提供除一个维度外的所有维度的大小，NumPy仍然可以推断出缺失维度的大小，因为它知道原始张量的维度。您希望NumPy推断的维度用-1表示。
- en: 'You might be wondering, “These images look crisp and clean. How on earth can
    we train our model to restore corrupted images?” That’s a very easy fix. All we
    need to do is synthesize a corresponding corrupted set of images from the original
    set of images. For that, we will define the generate_masked_inputs(...) function:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 也许你会想：“这些图像看起来清晰干净。我们如何训练模型来恢复损坏的图像？” 这很容易解决。我们只需要从原始图像中合成一组相应的损坏图像集。为此，我们将定义generate_masked_inputs(...)函数：
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This function will set pixels randomly (with 50% probability) to zero. But
    let’s inspect what we are doing in more detail. First, we will give the option
    to set a random seed so that we can deterministically change the generated random
    masks. We are creating a mask of 1s and 0s using the binomial distribution, which
    is the same size as norm_x_train. In simple words, the binomial distribution represents
    the probability of heads (1) or tails (0) if you flip a coin several times. The
    binomial distribution has several important parameters:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数将随机（有50%的概率）将像素设置为零。但让我们更详细地检查我们正在做什么。首先，我们将提供设置随机种子的选项，以便我们可以确定性地改变生成的随机掩码。我们使用二项分布创建一个与norm_x_train大小相同的1和0的掩码。简单来说，二项分布表示如果你多次抛硬币，出现正面（1）或反面（0）的概率。二项分布有几个重要参数：
- en: N—Number of trials
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: N—试验次数
- en: P—Probability of a success (1)
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P—成功的概率（1）
- en: Size—The number of tests (i.e., trial sets)
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Size—测试的数量（即，试验集）
- en: Here, we have x.shape tests and one trial in each with a 50% success probability.
    Then this mask is multiplied element-wise with the original tensor. This will
    result in black pixels randomly distributed over the image (figure 4.2).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有x.shape个测试，在每个测试中有一个50%的成功概率。然后将此掩码与原始张量进行逐元素相乘。这将导致随机分布在图像上的黑色像素（图4.2）。
- en: '![04-02](../../OEBPS/Images/04-02.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![04-02](../../OEBPS/Images/04-02.png)'
- en: Figure 4.2 Some of the synthetically corrupted images
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2 一些合成损坏的图像
- en: Next, let’s discuss the fully connected network we’ll be implementing. It’s
    called an autoencoder model.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论我们将要实现的全连接网络。它被称为自动编码器模型。
- en: 4.1.2 Autoencoder model
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.2 自动编码器模型
- en: Both the autoencoder model and the multilayer perceptron (MLP) model (from chapter
    1) are FCNs. These are called FCNs because all the input nodes are connected to
    all the output nodes, in every layer of the network. Autoencoders operate in a
    similar way to the multilayer perceptron. In other words, the computations (e.g.,
    forward pass) you see in an autoencoder are exactly the same as in an MLP. However,
    the final objectives of the two are different. An MLP is trained to solve a supervised
    task (e.g., classifying flower species), whereas an autoencoder is trained to
    solve an unsupervised task (e.g., reconstructing the original image, given a corrupted/noisy
    image). Let’s now delve into what an autoencoder actually does.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器模型和多层感知机（MLP）模型（来自第1章）都是全连接网络（FCN）。之所以称为FCN，是因为网络中的每一层都将所有输入节点连接到所有输出节点。自动编码器的操作方式与多层感知机类似。换句话说，在自动编码器中看到的计算（例如，正向传播）与MLP中完全相同。然而，两者的最终目标不同。MLP被训练来解决监督任务（例如，分类花的品种），而自动编码器被训练来解决无监督任务（例如，在给定损坏/嘈杂图像的情况下重建原始图像）。现在让我们深入了解自动编码器实际上是做什么的。
- en: Supervised versus unsupervised learning
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习与无监督学习
- en: In supervised learning, a model is trained using a labeled data set. Each input
    (e.g., image/audio/movie review) has a corresponding label (e.g., object class
    for images, sentiment of the review) or continuous value(s) (e.g., bounding boxes
    of an object for images). Some examples of supervised tasks are image classification,
    object detection, speech recognition, and sentiment analysis.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，模型使用带标签的数据集进行训练。每个输入（例如，图像/音频/电影评论）都有一个相应的标签（例如，图像的对象类别、评论的情感）或连续值（例如，图像对象的边界框）。监督任务的一些示例包括图像分类、目标检测、语音识别和情感分析。
- en: In unsupervised learning, the models are trained using unlabeled data (e.g.,
    images/audio/text extracted from websites without any labeling). The training
    process varies significantly depending on the final expected outcome. For example,
    autoencoders are trained to reconstruct images as a pretraining step for an image-based
    supervised learning task. Some examples of unsupervised tasks are image reconstruction,
    image generation using generative adversarial networks, text clustering, and language
    modeling.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督学习中，模型使用未标记的数据进行训练（例如，从网站提取的没有任何标签的图像/音频/文本）。训练过程根据最终预期结果而显著变化。例如，自动编码器被训练为重建图像，作为基于图像的监督学习任务的预训练步骤。无监督任务的一些示例包括图像重构、使用生成对抗网络生成图像、文本聚类和语言建模。
- en: '![04-03](../../OEBPS/Images/04-03.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![04-03](../../OEBPS/Images/04-03.png)'
- en: Figure 4.3 A simple autoencoder with one layer for compression and another layer
    for reconstruction. The black and white rectangles in the input image are the
    pixels present in the image.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3 一个简单的自动编码器，其中一个层用于压缩，另一个层用于重构。输入图像中的黑色和白色矩形是图像中存在的像素。
- en: 'Figure 4.3 depicts a simple autoencoder with two layers. An autoencoder has
    two phases in its functionality:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3描绘了一个具有两层的简单自编码器。自编码器在其功能上有两个阶段：
- en: '*Compression phase*—Compresses a given image (i.e., the corrupted image) to
    a compressed hidden (i.e., latent) representation'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*压缩阶段*—将给定图像（即损坏的图像）压缩为压缩的隐藏（即潜在）表示。'
- en: '*Reconstruction phase*—Reconstructs the original image from the hidden representation'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*重构阶段*—从隐藏表示中重构原始图像。'
- en: In the compression phase, a compressed hidden representation is computed as
    follows
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在压缩阶段，计算压缩的隐藏表示如下所示。
- en: '*h*[1] = *ReLU*(*xW*[1] + *b*[1])'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*h*[1] = *ReLU*(*xW*[1] + *b*[1])'
- en: where *W*[1],*b*[1] are the weights and biases of the first compression layer
    and *h*[1] is the final hidden representation of the layer.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*W*[1]，*b*[1]是第一压缩层的权重和偏置，*h*[1]是层的最终隐藏表示。
- en: 'Similarly, we compute the output of the reconstruction layers:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们计算重构层的输出：
- en: '*ŷ* = *ReLU*(*h*[1] *W*[2] + *b*[2])'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*ŷ* = *ReLU*(*h*[1] *W*[2] + *b*[2])'
- en: This is known as the forward pass, as you are going from the input to the output.
    Then you compute a loss (e.g., mean squared error [MSE]) between the expected
    output (i.e., target) and the prediction. For example, mean squared error for
    a single image is computed as
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为前向传播，因为您从输入到输出。然后，您计算预期输出（即目标）和预测之间的损失（例如，均方误差[MSE]）。例如，单个图像的均方误差计算为
- en: '![04_03a](../../OEBPS/Images/04_03a.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![04_03a](../../OEBPS/Images/04_03a.png)'
- en: where *D* is the dimensionality of the data (784 in our example), *y*[j] is
    the *j*^(th) pixel in our image, and (*ŷ*[j]) is the *j*^(th) pixel of the predicted
    image. We compute this loss for each batch of images and optimize the model parameters
    to minimize the computed loss. This is known as the backward pass.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*D*是数据的维度（在我们的示例中为784），*y*[j]是我们图像中的第*j*个像素，(*ŷ*[j])是预测图像的第*j*个像素。我们为每批图像计算此损失，并优化模型参数以最小化计算的损失。这被称为向后传递。
- en: You can have an arbitrary number of compression and reconstruction layers. In
    our assignment, we need to have two compression layers and two reconstruction
    layers (see the next listing).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以有任意数量的压缩和重构层。在我们的任务中，我们需要两个压缩层和两个重构层（见下一个列表）。
- en: Listing 4.1 The denoising autoencoder model
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.1 去噪自编码器模型。
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Defining four Dense layers, three with ReLU activation and one with tanh activation
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义四个稠密层，其中三个使用ReLU激活，一个使用tanh激活。
- en: ❷ Compiling the model with a loss function and an optimizer
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用损失函数和优化器编译模型。
- en: ❸ Printing the summary
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 打印摘要。
- en: 'Let’s go over what we did in more detail. The first thing you should notice
    is that we used the Keras Sequential API for this task. This makes sense as this
    is a very simple deep learning model. Next, we added four Dense layers. The first
    Dense layer takes an input with 784 features and produces a 64-elements-long vector.
    Then the second layer takes the 64-elements-long vector and produces a 32-elements-long
    vector. The third dense layer takes the 32-elements-long vector and produces a
    64-elements-long vector, passing it on to the final layer, which produces a 784-elements-long
    vector (i.e., size of the input). The first three layers have ReLU activation,
    and the last layer has a tanh activation, as the last layer needs to produce values
    between (-1, 1). Let’s remind ourselves how the ReLU and tanh activations are
    computed:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地讨论我们所做的事情。您应该注意到的第一件事是，我们在这个任务中使用了Keras Sequential API。这是有道理的，因为这是一个非常简单的深度学习模型。接下来，我们添加了四个稠密层。第一个稠密层接受具有784个特征的输入，并产生一个64元素的向量。然后第二层接受64元素的向量并产生一个32元素的向量。第三个稠密层接受32元素的向量并产生一个64元素的向量，将其传递给最终层，该层产生一个784元素的向量（即输入的大小）。前三层使用ReLU激活，最后一层使用tanh激活，因为最后一层需要产生在(-1,
    1)之间的值。让我们再次提醒自己如何计算ReLU和tanh激活：
- en: '*ReLU*(*x*) = max (0, *x*)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*ReLU*(*x*) = max (0, *x*)'
- en: '![04_03b](../../OEBPS/Images/04_03b.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![04_03b](../../OEBPS/Images/04_03b.png)'
- en: 'Finally, we compile the model using the mean squared error as the loss function
    and adam as the optimizer. The model we just described has the specifications
    we defined at the beginning of the section. With the model defined, you can now
    train the model. You will train the model for 10 epochs with batches of size 64:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用均方误差作为损失函数，使用adam作为优化器编译模型。我们刚刚描述的模型具有我们在本节开头定义的规格。有了定义好的模型，现在您可以训练模型了。您将使用64个大小的批次训练模型10个时期：
- en: '[PRE8]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The masked inputs we generated become the input, and the original images will
    be the ground truth. When you train the model, you will see a loss that goes down
    over time:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生成的遮罩输入成为输入，原始图像将成为地面真相。当你训练模型时，你会看到随时间推移损失下降：
- en: '[PRE9]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: It seems the error (i.e., loss value) has gone down from approximately 0.15
    to roughly 0.078\. This is a strong indication that the model is learning to reconstruct
    images. You can get similar results by setting the seed using the fix_random_seed(...)
    function we used in chapter 2 (provided in the notebook). Note that for this task
    we cannot define a metric like accuracy, as it is an unsupervised task.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来误差（即，损失值）从大约 0.15 下降到大约 0.078。这是模型正在学习重建图像的一个强有力的指示。你可以通过设置种子来获得类似的结果，使用我们在第二章中使用的
    fix_random_seed(...) 函数（提供在笔记本中）。请注意，对于这个任务，我们无法定义像准确度这样的指标，因为这是一个无监督的任务。
- en: Denoising autoencoders
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 去噪自动编码器
- en: 'Normally an autoencoder maps a given input to a small latent space and then
    back to the original input space to reconstruct the original images. However,
    here we use the autoencoder for a special purpose: to restore original images
    or denoise original images. Such autoencoders are known as *denoising*. Read more
    about denoising autoencoders at [http://mng.bz/WxyX](http://mng.bz/WxyX).'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，自动编码器将给定的输入映射到一个小的潜在空间，然后再返回到原始输入空间以重建原始图像。然而，在这里，我们将自动编码器用于一个特殊目的：还原原始图像或去噪原始图像。这样的自动编码器被称为
    *去噪*。在[http://mng.bz/WxyX](http://mng.bz/WxyX)上阅读更多关于去噪自动编码器的信息。
- en: 'Let’s now see what the trained model can do! It should now be able to decently
    restore an image of a corrupted digit. And to make things interesting, let’s make
    sure we generate a mask that the training data has not seen:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看训练好的模型能做什么！它现在应该能够还原一个受损数字的图像了。为了让事情变得有趣，让我们确保我们生成的遮罩是训练数据没有见过的：
- en: '[PRE10]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Here, we will be using the first 10 images in the data set to test out the model
    we just trained. However, we are making sure that the random mask is different
    by changing the seed. You can display some information about y_pred using
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用数据集中的前 10 张图像来测试我们刚刚训练的模型。然而，我们通过更改种子确保了随机遮罩不同。你可以使用以下代码显示关于 y_pred
    的一些信息
- en: '[PRE11]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: which will give
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 将会给出
- en: '[PRE12]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Finally, you can visualize what the model does by plotting the images (the code
    provided in the notebook). Figure 4.4 illustrates the corrupted images (top row)
    and the outputs of the model (bottom row). Though you are not yet restoring real-world
    photos of your grandmother, this a great start, as you now know the approach to
    follow.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可以通过绘制图像来可视化模型的作用（在笔记本中提供的代码）。图 4.4 说明了损坏的图像（顶行）和模型的输出（底行）。虽然你还没有恢复你祖母的真实照片，但这是一个很好的开始，因为现在你知道了要遵循的方法。
- en: '![04-04](../../OEBPS/Images/04-04.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![04-04](../../OEBPS/Images/04-04.png)'
- en: Figure 4.4 Images restored by the model. It seems our model is doing a good
    job.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4 模型恢复的图像。看起来我们的模型做得很好。
- en: You might be wondering, “What do autoencoders help you to achieve in general?”
    Autoencoders are a great tool for learning unsupervised features from unlabeled
    data, which is handy when solving more interesting downstream tasks like image
    classification. When autoencoders are trained on an unsupervised task, they learn
    useful features for other tasks (e.g., image classification). Therefore, training
    an autoencoder model to classify images will get you to a well-performing model
    faster and with less labeled data than training a model from scratch. As you are
    probably aware, there’s much more unlabeled data in the world than labeled data,
    as labeling usually requires human intervention, which is time-consuming and expensive.
    Another use of autoencoders is that the hidden representation it produces can
    be used as a low-dimensional proxy to cluster the images.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，“自动编码器通常能帮你实现什么？”自动编码器是从未标记数据中学习无监督特征的好工具，这在解决更有趣的下游任务时非常方便，比如图像分类。当自动编码器在无监督任务上进行训练时，它们学习了其他任务（例如，图像分类）的有用特征。因此，训练一个自动编码器模型来对图像进行分类将比从头开始训练模型更快地获得性能良好的模型，并且所需的标记数据更少。正如你可能知道的，世界上的未标记数据要比标记数据多得多，因为标记通常需要人为干预，这是耗时且昂贵的。自动编码器的另一个用途是它产生的隐藏表示可以用作聚类图像的低维代理。
- en: In this section, you learned about the autoencoder model, which is a type of
    FCN and is used to reconstruct/restore damaged images in an unsupervised manner.
    This is a great way to leverage copious amounts of unlabeled data to pretrain
    models, which becomes useful in more downstream interesting tasks (e.g., image
    classification). You first learned the architecture and then how to implement
    an autoencoder model with the Keras Sequential API. Finally, you trained the model
    on a hand-written image data set (MNIST) to reconstruct the images in the data
    set. During the training process, to ensure the model was learning, you monitored
    the loss to make sure it decreased over time. Finally, you used the model to predict
    restorations of corrupted images and ensured the model was performing well.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你学习了自动编码器模型，它是一种 FCN 类型，用于以无监督的方式重构/恢复损坏的图像。这是一种利用大量未标记数据来预训练模型的好方法，这在更下游的有趣任务（如图像分类）中非常有用。你首先学习了架构，然后学习了如何使用
    Keras Sequential API 实现自动编码器模型。最后，你对手写图像数据集（MNIST）进行了模型训练以重构数据集中的图像。在训练过程中，为了确保模型在学习，你监控了损失以确保随着时间的推移减少。最后，你使用模型预测了损坏图像的恢复，并确保模型表现良好。
- en: 'In the next section, we will discuss a different type of deep learning network
    that has revolutionized the field of computer vision: CNNs.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论一种不同类型的深度学习网络，它彻底改变了计算机视觉领域：CNN。
- en: Exercise 1
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 1
- en: Implement an autoencoder model that takes in a 512-elements-long vector. The
    network has a 32-node layer, a 16-node layer, and finally an output layer. In
    total, there are three layers. All these layers have the sigmoid activation.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 实现一个接受 512 元素长向量的自动编码器模型。网络有一个 32 节点层，一个 16 节点层，最后是一个输出层。总共有三层。所有这些层都具有 sigmoid
    激活。
- en: 4.2 Convolutional neural networks
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 卷积神经网络
- en: You have been working at a startup as a data scientist trying to model traffic
    congestion on the road. One important model in the company’s solution is building
    a model to predict whether a vehicle is present, given a patch or image, as a
    part of a larger plan. You plan to develop a model first on the cifar-10 data
    set and see how well it classifies vehicles. This is a great idea, as it will
    give a rough approximation of the feasibility of the idea while spending minimal
    time and money on labeling custom data. If we can achieve good accuracy on this
    data set, that is a very positive sign. You have learned that CNNs are great for
    computer vision tasks. So, you are planning to implement a CNN.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 你一直在一家初创公司担任数据科学家，试图对道路上的交通拥堵建模。公司解决方案中的一个重要模型是构建一个模型，以预测在给定的图像或图像块中是否存在车辆，作为更大计划的一部分。你计划首先在
    cifar-10 数据集上开发一个模型，并查看它在分类车辆方面的效果如何。这是一个很好的主意，因为它将在最小的时间和金钱上为自定义数据标记提供一个粗略的近似值。如果我们在这个数据集上能够达到较高的准确度，那是一个非常积极的信号。你了解到
    CNN 对于计算机视觉任务非常有效。因此，你计划实现一个 CNN。
- en: 4.2.1 Understanding the data
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.1 理解数据
- en: We will use is the cifar-10 data set. We briefly looked at this data set in
    the previous chapter, and it is a great cornerstone for this task. It has various
    vehicles (e.g., automobile, truck) and other objects (e.g., dog, cat) as classes.
    Figure 4.5 illustrates some of the classes and corresponding samples for them.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的是 cifar-10 数据集。我们在上一章节简要地查看过这个数据集，它是这项任务的重要基石。它包含各种交通工具（如汽车、卡车）和其他物体（如狗、猫）作为类别。图
    4.5 展示了一些类别及其对应的样本。
- en: '![04-05](../../OEBPS/Images/04-05.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![04-05](../../OEBPS/Images/04-05.png)'
- en: Figure 4.5 Sample images from cifar-10 data set along with their labels
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5 cifar-10 数据集的样本图像及其标签
- en: The data set consists of 50,000 training instances and 10,000 testing instances.
    Each instance is a 32 × 32 RGB image. There are 10 different classes of objects
    in this data set.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含 50,000 个训练实例和 10,000 个测试实例。每个实例是一个 32 × 32 的 RGB 图像。这个数据集中有 10 个不同的对象类别。
- en: 'Let’s first load the data by executing the following line:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先通过执行以下行来加载数据：
- en: '[PRE13]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: print(data) will yield
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 执行 print(data) 将产生
- en: '[PRE14]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: If you explore the data a bit, you will realize that
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你稍微探索一下数据，你会意识到
- en: Images are provided with the data type as unsigned eight-bit integers.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像以无符号的八位整数类型提供。
- en: Labels are provided as integer labels (i.e., not one-hot encoded).
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签以整数标签提供（即，未进行 one-hot 编码）。
- en: 'Therefore, we will write a very simple function to convert the images to data
    type float32 (to make the data type consistent with the model parameters) and
    labels to one-hot encoded vectors:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将编写一个非常简单的函数，将图像转换为float32数据类型（使数据类型与模型参数一致），并将标签转换为独热编码向量：
- en: '[PRE15]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Finally, we will create a batched data set by applying this function to all
    the training data:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将通过将此函数应用于所有训练数据来创建一个批处理数据集：
- en: '[PRE16]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We can again look at the data with
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以再次查看数据
- en: '[PRE17]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: which will produce
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生
- en: '[PRE18]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now our data is ready to be fed to a model.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的数据准备好输入模型了。
- en: 4.2.2 Implementing the network
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.2 实现网络
- en: 'To classify these images, we will employ a CNN. CNNs have gained a stellar
    reputation for solving computer vision tasks and are a popular choice for image-related
    tasks for two main reasons:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对这些图像进行分类，我们将采用CNN。CNN以解决计算机视觉任务而闻名，并且是处理图像相关任务的流行选择，原因有两个主要方面：
- en: CNNs process the images while preserving their spatial information (i.e., while
    keeping the height and width dimensions as is), while a fully connected layer
    will need to unwrap the height and width dimensions to a single dimension, losing
    precious locality information.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNN在处理图像时保留它们的空间信息（即保持高度和宽度维度不变），而全连接层则需要将高度和宽度维度展开为一个单一维度，从而丢失宝贵的局部信息。
- en: Unlike a fully connected layer where every input is connected to every output,
    the convolution operation shifts a smaller kernel over the entire image, demanding
    only a handful of parameters in a layer, making CNNs very parameter efficient.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同于全连接层，其中每个输入都连接到每个输出，卷积操作将一个较小的核移动到整个图像上，每层只需少量参数，使得CNN非常高效。
- en: 'A CNN consists of a set of interleaved convolution and pooling layers followed
    by several fully connected layers. This means there are three main types of layers
    in a CNN:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: CNN由一组交错的卷积和池化层以及几个全连接层组成。这意味着CNN中有三种主要类型的层：
- en: Convolution layers
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积层
- en: Pooling layers
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 池化层
- en: Fully connected layers
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全连接层
- en: A convolution layer consists of several filters (i.e., convolution kernels)
    that are convolved over the image to produce a *feature map*. The feature map
    is a representation of how strongly a given filter is present in the image. For
    example, if the filter represents a vertical edge, the feature map represents
    where (and how strongly) in the image vertical edges are present. As another example,
    think of a neural network that is trained to identify faces. A filter might represent
    the shape of an eye and activate the corresponding area of the output highly when
    an eye is present in a given image (figure 4.6). We will discuss the convolution
    operation in more detail later in the chapter.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 一个卷积层由多个滤波器（即卷积核）组成，这些滤波器在图像上进行卷积以生成*特征图*。特征图是一个表示给定滤波器在图像中存在程度的表示。例如，如果滤波器表示垂直边缘，则特征图表示图像中垂直边缘存在的位置（以及强度）。再举一个例子，想象一个训练用于识别人脸的神经网络。一个滤波器可能表示眼睛的形状，并且在给定图像中存在眼睛时会高度激活相应区域的输出（见图4.6）。我们将在本章后面更详细地讨论卷积操作。
- en: '![04-06](../../OEBPS/Images/04-06.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![04-06](../../OEBPS/Images/04-06.png)'
- en: Figure 4.6 The result of a convolution operation at a very abstract level. If
    we have an image of a human face and a convolution kernel that represents the
    shape/color of an eye, then the convolution result can be roughly thought of as
    a heatmap of where that feature (i.e., the eyes) are present in the image.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6 卷积操作的结果，非常抽象。如果我们有一个人脸图像和一个表示眼睛形状/颜色的卷积核，那么卷积结果可以粗略地被认为是该特征（即眼睛）在图像中存在的热图。
- en: Another important characteristic of the convolution layers is that the deeper
    you go in the network (i.e., further away from the input), the more high-level
    features the layers learn. Going back to our face recognition example, the lower
    layers might learn various edges present; the next layer, the shape of an eye,
    ear, and a nose; the next layer, how two eyes are positioned, the alignment of
    the nose and mouth; and so on (figure 4.7).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层的另一个重要特性是，网络越深（即离输入越远），层学习的高级特征就越多。回到我们的人脸识别例子，较低的层可能学习到各种边缘的存在；下一层学习到眼睛、耳朵和鼻子的形状；下一层学习到两只眼睛的位置、鼻子和嘴巴的对齐等（见图4.7）。
- en: '![04-07](../../OEBPS/Images/04-07.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![04-07](../../OEBPS/Images/04-07.png)'
- en: 'Figure 4.7 Features learned by a convolutional neural network. The lower layers
    (closest to the input) are learning edges/lines, whereas the upper layers (furthest
    from input) are learning higher-level features. (Source: [http://mng.bz/8MPg](http://mng.bz/8MPg))'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7 卷积神经网络学习到的特征。较低的层（离输入最近）学习到的是边缘/线条，而较高的层（离输入最远）学习到的是更高级的特征。（来源：[http://mng.bz/8MPg](http://mng.bz/8MPg)）
- en: Next, the pooling layer takes in feature maps generated by a convolution layer
    and reduces their height and width dimensions. Why is it useful to reduce the
    height and width of the feature maps? It helps the model be translation invariant
    during the machine learning task. For instance, if the task is image classification,
    even if the objects appear several pixels offset from what was seen during training,
    the network is still able to identify the object.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，池化层接收卷积层生成的特征图并减少它们的高度和宽度维度。为什么减少特征图的高度和宽度有用？它帮助模型在机器学习任务中具有平移不变性。例如，如果任务是图像分类，即使物体在训练期间看到的几个像素偏移，该网络仍然能够识别出物体。
- en: Finally, to get the final probability distribution, you have several fully connected
    layers. But you might have suspected an issue we face here. A convolution/pooling
    layer produces a three-dimensional output (i.e., height, width, and channel dimensions).
    But a fully connected layer accepts a one-dimensional input. How do we connect
    the three-dimensional output of a convolution/pooling layer to a one-dimensional
    fully connected layer? There’s a simple answer to this problem. You squash all
    three dimensions into a single dimension. In other words, it is analogous to unwrapping
    a two-dimensional RGB image to a one-dimensional vector. This provides the fully
    connected layer with a one-dimensional input. Finally, a softmax activation is
    applied to the outputs of the final fully connected layer (i.e., scores of the
    network) to obtain a valid probability distribution. Figure 4.8 depicts a simple
    CNN.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，为了获得最终的概率分布，你有多个全连接层。但你可能已经怀疑我们在这里面临的问题。卷积/池化层产生三维输出（即高度、宽度和通道维度）。但全连接层接受一维输入。我们如何将卷积/池化层的三维输出连接到一维的全连接层呢？这个问题有一个简单的答案。你将所有三个维度压缩成一个维度。换句话说，这类似于将二维的
    RGB 图像展开成一维向量。这为全连接层提供了一维输入。最后，对最后一个全连接层的输出（即网络的得分）应用 softmax 激活，以获得有效的概率分布。图
    4.8 描述了一个简单的 CNN。
- en: '![04-08](../../OEBPS/Images/04-08.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![04-08](../../OEBPS/Images/04-08.png)'
- en: Figure 4.8 A simple CNN. First, we have an image with height, width, and channel
    dimensions, followed by a convolution and pooling layer. Finally, the last convolution/pooling
    layer output is flattened and fed to a set of fully connected layers.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8 一个简单的 CNN。首先，我们有一个带有高度、宽度和通道维度的图像，然后是一个卷积和池化层。最后，最后一个卷积/池化层的输出被展平，并输入到一组全连接层中。
- en: With a good understanding of what a CNN comprises, we will create the following
    CNN using the Keras Sequential API. However, if you run this code, you will get
    an error. We will investigate and fix this error in the coming sections (see the
    next listing).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 通过深入了解 CNN 的构成，我们将使用 Keras Sequential API 创建以下 CNN。然而，如果你运行此代码，你将会收到一个错误。我们将在接下来的部分调查并修复这个错误（参见下一个列表）。
- en: Listing 4.2 Defining a CNN with the Keras Sequential API
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.2 使用 Keras Sequential API 定义 CNN
- en: '[PRE19]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ Clearing any existing Keras states (e.g., models) to start fresh
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 清除任何现有的 Keras 状态（例如模型）以重新开始
- en: ❷ Defining a convolution layer; it takes parameters like filters, kernel_size,
    strides, activation, and padding.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义卷积层；它接受过滤器、内核大小、步幅、激活和填充等参数。
- en: ❸ Before feeding the data to a fully connected layer, we need to flatten the
    output of the last convolution layer.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在将数据输入全连接层之前，我们需要展平最后一个卷积层的输出。
- en: ❹ Creating an intermediate fully connected layer
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 创建一个中间的全连接层
- en: ❺ Final prediction layer
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 最终预测层
- en: 'You can see that the network consists of three convolution layers and two fully
    connected layers. Keras provides all the layers you need to implement a CNN. As
    you can see, it can be done in a single line of code for our image classification
    network. Let’s explore what is happening in this model in more detail. The first
    layer is specified as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，该网络由三个卷积层和两个全连接层组成。Keras 提供了你实现 CNN 所需的所有层。如你所见，我们的图像分类网络只需一行代码即可完成。让我们更详细地探索一下这个模型中发生了什么。第一层定义如下：
- en: '[PRE20]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Hyperparameters of convolutional neural networks
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络的超参数
- en: In the CNN network from listing 4.2, filters, kernel_size, and strides of the
    Conv2D layers, the number of hidden units in the Dense layers (except the output
    layer) and the activation function are known as the hyperparameters of the model.
    Ideally, these hyperparameters need to be selected using a hyperparameter optimization
    algorithm, which would run hundreds (if not thousands) of models with different
    hyperparameter values and choose the one that maximizes a predefined metric (e.g.,
    model accuracy). However, here we have chosen the values for these hyperparameters
    empirically and will not be using hyperparameter optimization.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'First, the Conv2D layer is the Keras implementation of the 2D convolution operation.
    As you’ll remember from chapter 1, we achieved this using the tf.nn.convolution
    operation. The Conv2D layer executes the same functionality under the hood. However,
    it hides some of the complexities met when using the tf.nn.convolution operation
    directly (e.g., defining the layer parameters explicitly) There are several important
    arguments you need to provide to this layer:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: filters—The number of output channels that will be present in the output.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: kernel_size—The convolution window size on the height and width dimensions,
    in that order.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: strides—Represents how many pixels are skipped on height and with dimensions
    (in that order) every time the convolution window shifts on the input. Having
    a higher value here helps to reduce the size of the convolution output quickly
    as you go deeper.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: activation—The nonlinear activation of the convolution layer.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: padding—Type of padding used for the border while performing the convolution
    operation. Padding borders gives more control over the size of the output.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: input_shape—A three-dimensional tuple representing the input size on (height,
    width, channels) dimensions, in that order. Remember that Keras adds an unspecified
    batch dimension automatically when specifying the shape of the data using this
    argument.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s now go over the convolution function and its parameters in more detail.
    We already know that the convolution operation shifts a convolution window (i.e.,
    a kernel) over the image, while taking the sum of an element-wise product between
    the kernel and the portion of the image that overlaps the kernel at a given time
    (figure 4.9). Mathematically, the convolution operation can be stated as follows
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '![04_08a](../../OEBPS/Images/04_08a.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
- en: where *x* is a *n* × *n* input matrix, *f* is a *m* × *m* filter, and *y* is
    the output.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '![04-09](../../OEBPS/Images/04-09.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 The computations that happen in the convolution operation while shifting
    the window
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from the computations that take place during the convolution operation,
    there are four important hyperparameters that affect the size and values produced
    when using the Conv2D layer in Keras:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Number of filters
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kernel height and width
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kernel stride (height and width)
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Type of padding
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first aspect we will discuss is the number of filters in the layer. Typically,
    a single convolution layer has multiple filters. For example, think of a neural
    network that is trained to identify faces. One of the layers in the network might
    learn to identify the shape of an eye, shape of a nose, and so on. Each of these
    features might be learned by a single filter in the layer.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: The convolution layer takes an image, which is a three-dimensional tensor of
    some height, width, and channels. For example, if the image is an RGB image, there
    will be three channels. If the image is a grayscale image, the number of channels
    will be one. Then, convolving this tensor with n number of filters will result
    in a three-dimensional output of some height, width, and n channels. This is shown
    in figure 4.10\. When used in a CNN, the filters are the parameters of a convolution
    layer. These filters are initialized randomly, and over time they evolve to become
    meaningful features that help solve the task at hand.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: As we have said before, deep neural networks process data in batches. CNNs are
    no exception. You can see that we have set the input_shape parameter to (32, 32,
    3), where an unspecified batch dimension is automatically added, making it (None,
    32, 32, 3). The unspecified dimension is denoted by None, and it means that the
    model can take any arbitrary number of items on that dimension. This means that
    a batch of data can have 3, 4, 100, or any number of images (as the computer memory
    permits) at run time while feeding data to the model. Therefore, the input/output
    of a Conv2D layer is, in fact, a four-dimensional tensor with a batch, height,
    width, and channel dimension. Then the filters will be another four-dimensional
    tensor with a kernel height, width, incoming channel, and outgoing channel dimension.
    Table 4.1 summarizes this information.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Table 4.1 The dimensionality of the input, filters, and the output of a Conv2D
    layer
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Dimensionality** | **Example** |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
- en: '| Input | [batch size, height, width, in channels] | [32, 64, 64, 3] (i.e.,
    a batch of 32, 64 × 64 RGB images) |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
- en: '| Convolution filters | [height, width, in channels, out channels] | [5, 5,
    3, 16] (i.e., 16 convolution filters of size 5 × 5 with 3 incoming channels) |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
- en: '| Output | [batch size, height, width, out channels] | [32, 64, 64, 16] (i.e.,
    a batch of 32, 64 × 64 × 16 tensors) |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
- en: Figure 4.10 depicts how the inputs and outputs look in a convolution layer.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '![04-10](../../OEBPS/Images/04-10.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 A computation of a convolution layer with multiple filters (randomly
    initialized). We left the batch dimension of the tensor representation to avoid
    clutter.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, kernel height and width are the size of the filter on height and width
    dimensions. Figure 4.11 depicts how different kernel sizes lead to different outputs.
    Typically, when implementing CNNs, we keep kernel height and width equal. With
    that, we will refer to both the height and width dimensions of the kernel generally
    as the *kernel size*. We can compute the output size as a function of the kernel
    and input size as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '*size*(*y*) = *size*(*x*) - *size*(*f*) + 1'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: For example, if the image is a 7 × 7 matrix and the filter is a 3 × 3 matrix,
    then the output will be a (7 - 3 + 1, 7 - 3 + 1) = 5 × 5 matrix. Or, if the image
    is a 7 × 7 matrix and the filter is a 5 × 5 matrix, then the output will be a
    3 × 3 matrix.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '![04-11](../../OEBPS/Images/04-11.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 Convolution operation with a kernel size of 2 and kernel size of
    3\. Increasing the kernel size leads to a reduced output size.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: From a modeling perspective, increasing the kernel size (i.e., filter size)
    translates to an increased number of parameters. Typically, you should try to
    reduce the number of parameters in your network and target smaller-sized kernels.
    Having small kernel sizes encourages the model to learn more robust features with
    a small number of parameters, leading to better generalization of the model.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 'The next important parameter is the stride. Just like the kernel size, the
    stride has two components: height and width. Intuitively, the stride defines how
    many pixels/ values you skip while shifting the convolution operation. Figure
    4.12 illustrates the difference between having stride = 1 (i.e., no stride versus
    stride = 2). As before, we can specify the output size as a function of the input
    size, kernel size, and stride:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '![04_11a](../../OEBPS/Images/04_11a.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
- en: '![04-12](../../OEBPS/Images/04-12.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
- en: Figure 4.12 Convolution operation with stride = 1 (i.e., no stride) versus stride
    = 2\. An increased stride leads to a smaller output.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: From a modeling perspective, striding is beneficial, as it helps you to control
    the amount of reduction you need in the output. You might have noticed that, even
    without striding, you still get an automatic dimensionality reduction during convolution.
    However, when using striding, you can control the reduction you want to gain without
    affecting the kernel size.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Finally, padding decides what happens near the borders of the image. As you
    have already seen, when you convolve an image, you don’t get a same-sized output
    as the input. For example, if you have a 4 × 4 matrix and a 2 × 2 kernel, you
    get a 3 × 3 output (i.e., following the equation *size*(*y*) = *size*(*x*) - *size*(*f*
    ) + 1 we saw earlier, where *x* is the input size and *f* is the filter size).
    This automatic dimensionality reduction creates an issue when creating deep models.
    Specifically, it limits the number of layers you can have, as at some point the
    input will become a 1 × 1 pixel due to this automatic dimension reduction. Consequentially,
    this will create a very narrow bottleneck in passing information to the fully
    connected layers that follow, causing massive information loss.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use padding to alleviate this issue. With padding, you create an imaginary
    border of zeros around the image, such that you get the same-sized output as the
    input. More specifically, you append a border that is a *size*(*f* ) - 1-thick
    border of zeros in order to get an output of same size as the input. For example,
    if you have an input of size 4 × 4 and a kernel of size 2 × 2, then you would
    apply a border of size 2 - 1 = 1 vertically and horizontally. This means that
    the kernel is essentially processing a 5 × 5 input (i.e., (4 + 1) × (4 + 1)-sized
    input), resulting in a 4 × 4-sized output. This is called *same padding*. Note
    that it does not always have to be zeros that you are padding. Though currently
    not supported in Keras, there are different padding strategies (some examples
    are available here: [https://www.tensorflow.org/api_docs/python/tf/pad](https://www.tensorflow.org/api_docs/python/tf/pad)),
    such as padding with'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: A constant value
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A reflection of the input
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The nearest value
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you don’t apply padding, that is called *valid padding*. Not applying padding
    leads to the standard convolution operation we discussed earlier. The differences
    in padding are shown in figure 4.13.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '![04-13](../../OEBPS/Images/04-13.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
- en: Figure 4.13 Valid versus same padding. Valid padding leads to a reduced output
    size, whereas same padding results in an output with equal dimensions to the input.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 'With that, we conclude our discussion about various hyperparameters of the
    Conv2D layer. Now let’s circle back to the network we implemented. Unfortunately
    for you, if you try to run the code we discussed, you will be presented with a
    somewhat cryptic error like this:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'What have we done wrong here? It seems TensorFlow is complaining about a negative
    dimension size while trying to compute the output of a convolution layer. Since
    we have learned all about how to compute the size of the output under various
    circumstances (e.g., with stride, with padding, etc.), we will compute the final
    output of the convolution layers. We have the following layers:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We are starting with an input of size 32 × 32 × 3\. Then, after the convolution
    operation, which has 16 filters, a kernel size of 9, and stride 2, we get an output
    of size (height and width)
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Here, we focus only on the height and width dimensions. The next layer has
    32 filters, a kernel size of 7, and no stride:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This layer produces an output of size
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The final convolution layer has 64 filters, a kernel size of 7, and no stride
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: which will produce an output of size
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 6 - 7 + 1= 0
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: We figured it out! With our chosen configuration, our CNN is producing an invalid
    zero-sized output. The term *negative dimension* in the error refers to an output
    with invalid dimensions (i.e., less than one) being produced. The output always
    needs to be greater than or equal to 1.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Let’s correct this network by making sure the outputs will never have negative
    dimensions. Furthermore, we will introduce several interleaved max-pooling layers
    to the CNN, which helps the network to learn translation-invariant features (see
    the next listing).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.3 The corrected CNN model that has positive dimensions
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: ❶ The first convolution layer. The output size reduces from 32 to 16.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: ❷ The first max-pooling layer. The output size reduces from 16 to 8.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: ❸ The second convolution layer. The output size stays the same as there is no
    stride.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: ❹ The second max-pooling layer. The output size reduces from 8 to 4.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Squashing the height, width, and channel dimensions to a single dimension
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: ❻ The two intermediate Dense layers with ReLU activation
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: ❼ The final output layer with softmax activation
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 'Max-pooling is provided by the tensorflow.keras.layers.MaxPool2D layer. The
    hyperparameters of this layer are very similar to tensorflow.keras.layers.Conv2D:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: pool_size—This is analogous to the kernel size parameter of the Conv2D layer.
    It is a tuple representing (window height, window width), in that order.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strides—This is analogous to the strides parameter of the Conv2D layer. It is
    a tuple representing (height stride, width stride), in that order.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Padding—Padding can be same or valid and has the same effect as it has in the
    Conv2D layer.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s analyze the changes we made to our CNN:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: We used padding='same' for all the Conv2D and MaxPool2D layers, meaning that
    there won’t be any automatic reduction of the output size. This eliminates the
    risk of mistakenly going into negative dimensions.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We used stride parameters to control the reduction of the output size as we
    go deeper into the model.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can follow the output sizes in listing 4.1 and make sure that the output
    will never be less than or equal to zero for the input images we have.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: After the Conv2D and MaxPool2D layers, we have to have at least one Dense layer,
    as we are solving an image classification task. To get the final prediction probabilities
    (i.e., the probabilities of a given input belonging to the output classes), a
    Dense layer is essential. But before having a Dense layer, we need to flatten
    our four-dimensional output (i.e., [batch, height, width, channel] shaped) of
    the Conv2D or MaxPool2D layers to a two-dimensional input (i.e., [batch, features]
    shaped) to the Dense layer. That is, except for the batch dimension, everything
    else gets squashed to a single dimension. For this, we use the tensorflow.keras.layers.Flatten
    layer provided by Keras. For example, if the output of our last Conv2D layer was
    [None, 4, 4, 64], then the Flatten layer will flatten this output to a [None,
    1024]-sized tensor. Finally, we add three Dense layers, where the first two dense
    layers have 64 and 32 output nodes and an activation of type ReLU. The final Dense
    layer will have 10 nodes (1 for each class) and a softmax activation.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Performance bottleneck of CNNs
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Typically, in a CNN the very first Dense layer after the convolution/pooling
    layers is considered a *performance bottleneck*. This is because this layer will
    usually contain a large proportion of the parameters of the network. Assume you
    have a CNN where the last pooling layer produces an 8 × 8 × 256 output followed
    by a Dense layer with 1,024 nodes. This Dense layer would contain 16,778,240 (more
    than 16 million) parameters. If you don’t pay attention to the first Dense layer
    of the CNN, you can easily run into out-of-memory errors while running the model.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s time to test our first CNN on the data. But before that we have to compile
    the model with appropriate parameters. Here, we will monitor the training accuracy
    of the mode:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Finally, you can use the training data we created earlier and train the model
    on the data by calling
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'You should get the following output:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: It seems we are getting good at training accuracies (denoted by acc) and creating
    a steady reduction of the training loss (denoted by loss) for the task of identifying
    vehicles (around 72.2% accuracy). But we can go for far better accuracies by employing
    various techniques, as you will see in later chapters. This is very promising
    news for the team, as this means they can continue working on their full solution.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we looked at CNNs. CNNs work extremely well, especially in
    computer vision problems. In this instance, we looked at using a CNN to classify
    images to various classes (e.g., animals, vehicles, etc.) as a feasibility study
    for a model’s ability to detect vehicles. We looked at the technical aspects of
    the CNN in detail, while scrutinizing various operations like convolution and
    pooling, as well as the impact of the parameters associated with these operations
    (e.g., window size, stride, padding). We saw that if we do not pay attention to
    how the output changes while using these parameters, it can lead to errors in
    our code. Next, we went on to fix the error and train the model on the data set.
    Finally, we saw that the model showed promising results, quickly reaching for
    a training accuracy above 70%. Next, we will discuss RNNs, which are heavily invested
    in solving time-series problems.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 2
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following network:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: What is the final output size (ignoring the batch dimension)?
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '4.3 One step at a time: Recurrent neural networks (RNNs)'
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You are working for a machine learning consultant for the National Bureau of
    Meteorology. They have data for CO2 concentration over the last three decades.
    You have been tasked with developing a machine learning model that predicts CO2
    concentration for the next five years. You are planning to implement a simple
    RNN that takes a sequence of CO2 concentrations (in this case, the values from
    the last 12 months) and predicts the next in the sequence.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: It can be clearly seen that what we have in front of us is a time series problem.
    This is quite different from the tasks we have been solving so far. In previous
    tasks, one input did not depend on the previous inputs. In other words, you considered
    each input to be *i.i.d* (independent and identically distributed) inputs. However,
    in this problem, that is not the case. The CO2 concentration today will depend
    on what the CO2 concentration was over the last several months.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Typical feed-forward networks (i.e., fully connected networks, CNNs) cannot
    learn from time series data without special adaptations. However, there is a special
    type of neural network that is designed to learn from time series data. These
    networks are generally known as RNNs. RNNs not only use the current input to make
    a prediction, but also use the *memory* of the network from past time steps, at
    a given time step. Figure 4.14 depicts how a feed-forward network and an RNN differ
    in predicting CO2 concentration over the months. As you can see, if you use a
    feed-forward network, it has to predict the CO2 level for the next month based
    *only* on the previous month, whereas an RNN looks at all the previous months.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '![04-14](../../OEBPS/Images/04-14.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
- en: Figure 4.14 The operational difference between a feed-forward network and an
    RNN in terms of a CO2 concentration-level prediction task
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1 Understanding the data
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The data set is very simple (downloaded from [https://datahub.io/core/co2-ppm/r/co2-mm-gl.csv](https://datahub.io/core/co2-ppm/r/co2-mm-gl.csv)).
    Each datapoint has a date (YYYY-MM-DD format) and a floating-point value representing
    the CO2 concentration in CSV format. The data is provided to us as a CSV file.
    Let’s download the file as follows:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We can easily load this data set using pandas:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now we can see what the data looks like, using the head() operation, which
    will provide the first few entries in the data frame:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: This will give something like figure 4.15.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '![04-15](../../OEBPS/Images/04-15.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
- en: Figure 4.15 Sample data in the data set
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 'In this data set, the only two columns we are interested in are the Date column
    and the Average column. Out of these, the Date column is important for visualization
    purposes only. Let’s set the Date column as the index of the data frame. This
    way, when we plot data, the *x*-axis will be automatically annotated with the
    corresponding date:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We can now visualize the data (figure 4.16) by creating a line plot with
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '![04-16](../../OEBPS/Images/04-16.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
- en: Figure 4.16 CO2 concentration plotted over time
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 'The obvious features of the data are that it has an upward trend and short
    repetitive cycles. Let’s see what sort of improvements we can do to this data.
    The clear upward trend the data is showing poses a problem. This means that the
    data is not distributed in a consistent range. The range increases as we go further
    and further down the timeline. If you feed data as it is to the model, usually
    the model will underperform, because any new data the model has to predict is
    out of the range of the data it saw during training. But if you forget the absolute
    values and think about this data relative to the previous value, you will see
    that it moves between a very small range of values (appx -2.0 to +1.5). In fact,
    we can test this idea easily. We will create a new column called Average Diff,
    which will contain the relative difference between two consecutive time steps:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: If you do a data.head() at this stage, you will see something similar to table
    4.2.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: Table 4.2 Sample data in the data set after introducing the Average diff column
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '| **Date** | **Decimal data** | **Average** | **Trend** | **Average diff**
    |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
- en: '| 1980-01-01 | 1980.042 | 338.45 | 337.83 | 0.00 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
- en: '| 1980-02-01 | 1980.125 | 339.15 | 338.10 | 0.70 |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
- en: '| 1980-031-01 | 1980.208 | 339.48 | 338.13 | 0.33 |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
- en: '| 1980-04-01 | 1980.292 | 339.87 | 338.25 | 0.39 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
- en: '| 1980-05-01 | 1980.375 | 340.30 | 338.78 | 0.43 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
- en: Here, we are subtracting a version of the Average column, where values are shifted
    forward by one time step, from the original average column. Figure 4.17 depicts
    this operation visually.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '![04-17](../../OEBPS/Images/04-17.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
- en: Figure 4.17 Transformations taking place going from the original Average series
    to the Average Diff series
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we can visualize how the values behave (figure 4.18) using the data["Average
    Diff"].plot(figsize=(12,6)) line.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '![04-18](../../OEBPS/Images/04-18.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
- en: Figure 4.18 Relative change of values (i.e., Average[t]-Average[t-1]) of CO2
    concentration over time
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: Can you see the difference? From an ever-increasing data stream, we have gone
    to a stream that changes within a short vertical span. The next step is creating
    batches of data for the model to learn. How do we create batches of data for a
    time series problem? Remember, we cannot just randomly sample data naively, as
    each input depends on its predecessors.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: Let’s assume we want to use 12 past CO2 concentration values (i.e., 12 time
    steps) to predict the current CO2 concentration value. The number of time steps
    is a hyperparameter you must choose carefully. In order to choose this hyperparameter
    confidently, you must have a solid understanding of the data and the memory limitations
    of the model you are using.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: We first randomly choose a position in the sequence and take the 12 values from
    that point on as the inputs and the 13^(th) value as the output we’re interested
    in predicting so that the total sequence length (n_seq) you sample at a time is
    13\. If you do this process 10 times, you will have a batch of data with 10 elements.
    As you can see, this process exploits the randomness while preserving the temporal
    characteristics of the data, and while feeding data to the model. Figure 4.19
    visually describes this process.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '![04-19](../../OEBPS/Images/04-19.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
- en: Figure 4.19 Batching time series data. n_seq represents the number of time steps
    we see at a given time to create a single input and an output.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: To do this in Python, let’s write a function that gives the data at all positions
    as a single data set. In other words, this function returns all possible consecutive
    sequences with 12 elements as x and the corresponding next value for each sequence
    as y. It is possible to perform the shuffling while feeding this data to the model,
    as the next listing shows.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.4 The code for generating time-series data sequences for the model
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: ❶ Extracting a sequence of values n_seq long
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Extracting the next value in the sequence as the output
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Combining everything into an array
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2 Implementing the model
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With a good understanding of the data, we can start implementing the network.
    We will implement a network that has the following:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: A rnn layer with 64 hidden units
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Dense layer with 64 hidden units and a ReLU activation
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Dense layer with a single output and a linear activation
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Note that the hyperparameters of the network (e.g., number of hidden units)
    have been chosen empirically to work well for the given problem. The first layer
    is the most crucial component of the network, as it is the element that makes
    it possible to learn from time series data. The SimpleRNN layer encapsulates the
    functionality shown in figure 4.20.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '![04-20](../../OEBPS/Images/04-20.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
- en: Figure 4.20 The functionality of a SimpleRNN cell. The cell goes from one input
    to another while producing a memory at every time step. The next step consumes
    the current input as well as the memory from the previous time step.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: The computations that happen in an RNN are more sophisticated than in an FCN.
    An RNN goes from one input to the other in the input sequence (i.e., x1, x2, x3)
    in the given order. At each step, the recurrent layer produces an output (i.e.,
    o1, o2, o3) and passes the hidden computation (h0, h1, h2, h3) to the next time
    step. Here, the first hidden state (h0) is typically set to zero.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: At a given time step, the recurrent layer computes a hidden state, just like
    a Dense layer. However, the specific computations involved are bit more complex
    and are out of the scope of this book. The hidden state size is another hyperparameter
    of the recurrent layer. The recurrent layer takes the current input as well as
    the previous hidden state computed by the cell. A larger-sized hidden state helps
    to maintain more memory but increases the memory requirement of the network. As
    the hidden state is dependent on itself from the previous time step, these networks
    are called RNNs.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm used for SimpleRNN
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: The computations mimicked by the SimpleRNN layer are also known as *Elman networks*.
    To learn more about specific computations taking place in recurrent layers, you
    can read the paper “Finding Structure in Time” by J.L. Elman (1990). For a more
    high-level overview of later variations of RNNs and their differences, see [http://mng.bz/xnJg](http://mng.bz/xnJg)
    and [http://mng.bz/Ay2g](http://mng.bz/Ay2g)
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: By default, the SimpleRNN does not expose the hidden state to the developer
    and will be propagated between time steps automatically. For this task, we only
    need the final output produced by each time step, which is the output of that
    layer by default. Therefore, you can simply connect the SimpleRNN in the Sequential
    API to a Dense layer without any additional work.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: Did you notice that we haven’t provided an input_shape to the first layer? This
    is possible, as long as you provide the data in the correct shape during model
    fitting. Keras builds the layers lazily, so until you feed data to your model,
    the model doesn’t need to know the input sizes. But it is always safer to set
    the input_shape argument in the first layer of the model to avoid errors. For
    example, in the model we defined, the first layer (i.e., the SimpleRNN layer)
    can be changed to layers.SimpleRNN(64, input_shape=x), where x is a tuple containing
    the shape of the data accepted by the model.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: 'Another important difference in this model is that it is a regression model,
    not a classification model. In a classification model, there are distinct classes
    (represented by output nodes), and we try to associate a given input with a distinct
    class (or a node). A regression model predicts a continuous value(s) as the output.
    Here, in our regression model, there is no notion of classes in the outputs, but
    a real continuous value representing CO2 concentration. Therefore, we have to
    choose the loss function appropriately. In this case, we will use mean squared
    error (MSE) as the loss. MSE is a very common loss function for regression problems.
    We will compile the rnn with the MSE loss and the adam optimizer:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Let’s cross our fingers and train our model:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'You’ll get the following exception:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: It seems we have done something wrong. The line we just ran resulted in an exception,
    which says something is wrong with the dimensionality of the data given to the
    layer sequential_1 (i.e., the SimpleRNN layer). Specifically, the sequential_1
    layer expects a three-dimensional input but has a two-dimensional input. We need
    to investigate what’s happening here and solve this.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: 'The problem is that the SimpleRNN (or any other sequential layer in tf.keras)
    only accepts data in a very specific format. The data needs to be three-dimensional,
    with the following dimensions, in this order:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: Batch dimension
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Time dimension
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feature dimension
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Even when you have a single element for any of these dimensions, they need to
    be present as a dimension of size 1 in the data. Let’s look at what the dimensionality
    of x is by printing x.shape. You will get x.shape = (429, 12). Now we know what
    went wrong. We tried to pass a two-dimensional data set when we should have passed
    a three-dimensional one. In this case, we need to reshape x into a tensor of shape
    (492, 12, 1). Let’s change our generate_data(...) function to reflect this change
    in the following listing.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.5 The previous generate_data() function with data in the correct shape
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: ❶ Create two lists to hold input sequences and scalar output targets.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Iterate through all the possible starting points in the data for input sequences.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Create the input sequence and the output target at the ith position.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Convert x from a list to an array and make x a 3D tensor to be accepted by
    the RNN.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try training our model now:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'You should see the MSE of the model going down:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: We start with a loss of approximately 0.5 and end up with a loss of roughly
    0.015\. This is a very positive sign, as it indicates the model is learning the
    trends present in the data.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.3 Predicting future CO2 values with the trained model
  id: totrans-344
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Thus far, we have focused on classification tasks. It is much easier to evaluate
    models on classification tasks than regression tasks. In classification tasks
    (assuming a balanced data set), by computing the overall accuracy on the data,
    we can get a decent representative number on how well our model is doing. In regression
    tasks it’s not so simple. We cannot measure an accuracy on regressed values, as
    the predictions are real values, not classes. For example, the magnitude of the
    mean squared loss depends on values we are regressing, which makes them difficult
    to objectively interpret. To address this, we predict the values for the next
    five years and visually inspect what the model is predicting (see the next listing).
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.6 The future CO2 level prediction logic using the trained model
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: ❶ The first data sequence to start predictions from, which is reshaped to the
    correct shape the SimpleRNN accepts
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Save the very last absolute CO2 concentration value to compute the actual
    values from the relative predictions.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Predict for the next 60 months.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Make a prediction using the data sequence.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Modify the history so that the latest prediction is included.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Compute the absolute CO2 concentration.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Update prev_true so that the absolute CO2 concentration can be computed in
    the next time step.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s review what we have done. First, we extracted the last 12 CO2 values
    (from the Average Diff column) from our training data to predict the first future
    CO2 value and reshaped it to the correct shape the model expects the data to be
    in:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Then, we captured the predicted CO2 values in true_vals list. Remember that
    our model only predicts the relative movement of CO2 values with respect to the
    previous CO2 values. Therefore, after the model predicts, to get the absolute
    CO2 value, we need the last CO2 value. prev_true captures this information, which
    initially has the very last value in the Average column of the data:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Now, for the next 60 months (or 5 years), we can recursively predict CO2 values,
    while making the last predicted the next input to the network. To do this we first
    predict a value using the predict(...) method provided in Keras. Then, we need
    to make sure the prediction is also a three-dimensional tensor (though it’s a
    single value). Then we modify the history variable:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'We are taking all but the first value from the history and appending the last
    predicted value to the end. Then we append the absolute predicted CO2 value by
    adding the prev_true value to p_diff:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Finally, we update prev_true to the last absolute CO2 value we predicted:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: By doing this set of operations recursively, we can get the predictions for
    the next 60 months (captured in true_vals variable). If we visualize the predicted
    values, they should look like figure 4.21.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: '![04-21](../../OEBPS/Images/04-21.png)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
- en: Figure 4.21 The CO2 concentration predicted over the next five years. Dashed
    line represents the trend from the current data, and the solid line represents
    the predicted trend.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: Great work! Given the simplicity of the model, predictions look very promising.
    The model has definitely captured the annual trend of the CO2 concentration and
    has learned that the CO2 level is going to keep going up. You can now go to your
    boss and explain factually why we should be worried about climate change and dangerous
    levels of CO2 in the future. We end our discussion about different neural networks
    here.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 3
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: Impressed by your work on predicting the CO2 concentration, your boss has provided
    you the data and asked you to enhance the model to predict both CO2 and temperature
    values. Keeping the other hyperparameters the same, how would you change the model
    for this task? Make sure you specify the input_shape parameter for the first layer.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-371
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fully connected networks (FCNs) are one of the most straightforward neural networks.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FCNs can be implemented using the Keras Dense layer.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolutional neural networks (CNNs) are a popular choice for computer vision
    tasks.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow offers various layers, such as Conv2D, MaxPool2D, and Flatten, that
    help us implement CNNs quickly.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNNs have parameters such as kernel size, stride, and padding that must be set
    carefully. If not, this can lead to incorrectly shaped tensors and runtime errors.
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recurrent neural networks (RNNs) are predominantly used to learn from time-series
    data.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The typical RNN expects the data to be organized into a three-dimensional tensor
    with a batch, time, and feature dimension.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of time steps the RNN looks at is an important hyperparameter that
    should be chosen based on the data.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Answers to exercises
  id: totrans-380
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Exercise 1:** You can do this using the Sequential API, and you will be using
    only the Dense layer.'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 2**'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '**Exercise 3**'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
