["```py\nprompt = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.\"\n# Placeholder definition. The next code blocks show the actual generation\ndef generate(prompt, number_of_tokens):\n  # TODO: pass prompt to language model, and return the text it generates\n  pass\noutput = generate(prompt, 10)\nprint(output)\n```", "```py\nSubject: Apology and Condolences \nDear Sarah, \nI am deeply sorry for the tragic gardening accident that took place in my backyard yesterday. As you may have heard, *...**etc*\n```", "```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n*#* *openchat* *is a 13B LLM*\nmodel_name = \"openchat/openchat\"\n*# If your environment does not have the required resources to run this model*\n*# then try a smaller model like \"gpt2\" or \"**openlm**-research/open_llama_3b\"*\n*# Load a tokenizer*\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n*# Load a language model*\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n```", "```py\nprompt = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.\"\n# Tokenize the input prompt\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n# Generate the text\ngeneration_output = model.generate(\n  input_ids=input_ids, \n  max_new_tokens=256\n)\n# Print the output\nprint(tokenizer.decode(generation_output[0]))\n```", "```py\ntensor([[ 1, 14350, 385, 4876, 27746, 5281, 304, 19235, 363, 278, 25305, 293, 16423, 292, 286, 728, 481, 29889, 12027, 7420, 920, 372, 9559, 29889]])\n```", "```py\nfor id in input_ids[0]:\n   print(tokenizer.decode(id))\n```", "```py\n<s> \nWrite \nan \nemail \napolog \n izing \nto \nSarah \nfor \nthe \ntrag \n ic \ngarden \n ing \nm \n ish \n ap \n. \nExp \n lain \nhow \nit \nhappened \n.\n```", "```py\ntext = \"\"\"\nEnglish and CAPITALIZATION\nߎ堩蟠\nshow_tokens False None elif == >= else: two tabs:\"    \" Three tabs: \"       \"\n12.0*50=600\n\"\"\"\n```", "```py\n[CLS] english and capital ##ization [UNK] [UNK] show _ token ##s false none eli ##f = = > = else : four spaces : \" \" two tab ##s : \" \" 12 . 0 * 50 = 600 [SEP]\n```", "```py\n[CLS] English and CA ##PI ##TA ##L ##I ##Z ##AT ##ION [UNK] [UNK] show _ token ##s F ##als ##e None el ##if = = > = else : Four spaces : \" \" Two ta ##bs : \" \" 12 . 0 * 50 = 600 [SEP]\n```", "```py\nEnglish and CAPITAL IZATION \n� � � � � � \nshow _tokens False None elif == >= else :\nFour spaces : \"     \" Two tabs : \" \t \t \"\n12 . 0 * 50 = 600\n```", "```py\nEnglish and CAPITAL IZATION \n� � � � � \nshow _ tokens False None elif == >= else : \nFour spaces : \"   \" Two tabs : \" \t \t \" \n1 2 . 0 * 5 0 = 6 0 0\n```", "```py\nEnglish and CAP ITAL IZATION \n� � � � � � � \nshow _ tokens False None elif == > = else : \nFour spaces : \"     \" Two t abs : \" \t\t \" \n1 2 . 0 * 5 0 = 6 0 0\n```", "```py\n[CLS] english and capital ##ization [UNK] [UNK] show _ token ##s false none eli ##f = = > = else : four spaces : \" \" two tab ##s : \" \" 12 . 0 * 50 = 600 [SEP]\n```", "```py\n[CLS] English and CA ##PI ##TA ##L ##I ##Z ##AT ##ION [UNK] [UNK] show _ token ##s F ##als ##e None el ##if = = > = else : Four spaces : \" \" Two ta ##bs : \" \" 12 . 0 * 50 = 600 [SEP]\n```", "```py\n English and CAP ITAL IZ ATION � � � � � � show _ t ok ens False None el if == >= else : Two tabs :\" \" Four spaces : \" \" 12 . 0 * 50 = 600 \n```", "```py\nEnglish and CA PI TAL IZ ATION <unk> <unk> show _ to ken s Fal s e None e l if = = > = else : two tab s : \" \" Four spaces : \" \" 12\\. 0 * 50 = 600 </s> \n```", "```py\nEnglish and CAPITAL IZATION � � � � � � show _tokens False None elif == >= else : Four spaces : \" \" Two tabs : \" \" 12 . 0 * 50 = 600\n```", "```py\nEnglish and CAPITAL IZATION � � � � � show _ tokens False None elif == >= else : Four spaces : \" \" Two tabs : \" \" 1 2 . 0 * 5 0 = 6 0 0\n```", "```py\nEnglish and CAP ITAL IZATION � � � � � � � show _ tokens False None elif == > = else : Four spaces : \" \" Two t abs : \" \" 1 2 . 0 * 5 0 = 6 0 0\n```", "```py\n<s> English and C AP IT AL IZ ATION � � � � � � � show _ to kens False None elif == >= else : F our spaces : \" \" Two tabs : \" \" 1 2 . 0 * 5 0 = 6 0 0\n```", "```py\ndef add_numbers(a, b):\n....\"\"\"Add the two numbers `a` and `b`.\"\"\"\n....return a + b\n```", "```py\ndef add_numbers(a, b):\n....\"\"\"Add the two numbers `a` and `b`.\"\"\"\n....return a + b\n```", "```py\nfrom transformers import AutoModel, AutoTokenizer\n# Load a tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-base\")\n# Load a language model\nmodel = AutoModel.from_pretrained(\"microsoft/deberta-v3-xsmall\")\n# Tokenize the sentence\ntokens = tokenizer('Hello world', return_tensors='pt')\n# Process the tokens\noutput = model(**tokens)[0]\n```", "```py\noutput.shape\n```", "```py\ntorch.Size([1, 4, 384])\n```", "```py\nfor token in tokens['input_ids'][0]:\n    print(tokenizer.decode(token))\n```", "```py\n[CLS] \nHello\nworld \n[SEP]\n```", "```py\ntensor([[\n[-3.3060, -0.0507, -0.1098, ..., -0.1704, -0.1618, 0.6932], \n[ 0.8918, 0.0740, -0.1583, ..., 0.1869, 1.4760, 0.0751], \n[ 0.0871, 0.6364, -0.3050, ..., 0.4729, -0.1829, 1.0157], \n[-3.1624, -0.1436, -0.0941, ..., -0.0290, -0.1265, 0.7954]\n]], grad_fn=<NativeLayerNormBackward0>)\n```", "```py\nimport gensim\nimport gensim.downloader as api\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n# Download embeddings (66MB, glove, trained on wikipedia, vector size: 50)\n# Other options include \"word2vec-google-news-300\"\n# More options at https://github.com/RaRe-Technologies/gensim-data\nmodel = api.load(\"glove-wiki-gigaword-50\")\n```", "```py\nmodel.most_similar([model['king']], topn=11)\n```", "```py\n[('king', 1.0000001192092896), \n('prince', 0.8236179351806641), \n('queen', 0.7839043140411377), \n('ii', 0.7746230363845825), \n('emperor', 0.7736247777938843), \n('son', 0.766719400882721), \n('uncle', 0.7627150416374207), \n('kingdom', 0.7542161345481873), \n('throne', 0.7539914846420288), \n('brother', 0.7492411136627197), \n('ruler', 0.7434253692626953)]\n```", "```py\nprint_recommendations(3822)\ntitle Billie Jean \nartist Michael Jackson\nRecommendations:\n```", "```py\nprint_recommendations(842)\n```", "```py\n# Get the playlist dataset file\ndata = request.urlopen('https://storage.googleapis.com/maps-premium/dataset/yes_complete/train.txt')\n# Parse the playlist dataset file. Skip the first two lines as\n# they only contain metadata\nlines = data.read().decode(\"utf-8\").split('\\n')[2:]\n# Remove playlists with only one song\nplaylists = [s.rstrip().split() for s in lines if len(s.split()) > 1]\nprint( 'Playlist #1:\\n ', playlists[0], '\\n')\nprint( 'Playlist #2:\\n ', playlists[1])\nPlaylist #1: ['0', '1', '2', '3', '4', '5', ..., '43'] \nPlaylist #2: ['78', '79', '80', '3', '62', ..., '210']\nLet's train the model:\nmodel = Word2Vec(playlists, vector_size=32, window=20, negative=50, min_count=1, workers=4)\n```", "```py\nsong_id = 2172\n# Ask the model for songs similar to song #2172\nmodel.wv.most_similar(positive=str(song_id))\n```", "```py\n[('2976', 0.9977465271949768), \n('3167', 0.9977430701255798), \n('3094', 0.9975950717926025), \n('2640', 0.9966474175453186), \n('2849', 0.9963167905807495)]\n```", "```py\ntitle Fade To Black \nartist Metallica\n```"]