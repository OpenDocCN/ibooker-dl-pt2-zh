["```py\n>>> from nessvec.indexers import Index  # #1\n>>> index = Index(num_vecs=100_000)  # #2\n>>> index.get_nearest(\"Engineer\").round(2)\nEngineer       0.00\nengineer       0.23\nEngineers      0.27\nEngineering    0.30\nArchitect      0.35\nengineers      0.36\nTechnician     0.36\nProgrammer     0.39\nConsultant     0.39\nScientist      0.39\n```", "```py\n>>> index.get_nearest(\"Programmer\").round(2)\nProgrammer    -0.00\nprogrammer     0.28\nDeveloper      0.33\nProgrammers    0.34\nProgramming    0.37\nEngineer       0.39\nSoftware       0.40\nConsultant     0.42\nprogrammers    0.42\nAnalyst        0.42\ndtype: float64\n>>> index.get_nearest(\"Developer\").round(2)\nDeveloper     -0.00\ndeveloper      0.25\nDevelopers     0.25\nProgrammer     0.33\nSoftware       0.35\ndevelopers     0.37\nDesigner       0.38\nArchitect      0.39\nPublisher      0.39\nDevelopment    0.40\n```", "```py\n>>> \"Chief\" + \"Engineer\"\n'ChiefEngineer'\n>>> \"Chief\" + \" \" + \"Engineer\"\n'Chief Engineer'\n```", "```py\n>>> chief = (index.data[index.vocab[\"Chief\"]]\n...     + index.data[index.vocab[\"Engineer\"]])\n>>> index.get_nearest(chief)\nEngineer     0.110178\nChief        0.128640\nOfficer      0.310105\nCommander    0.315710\nengineer     0.329355\nArchitect    0.350434\nScientist    0.356390\nAssistant    0.356841\nDeputy       0.363417\nEngineers    0.363686\n```", "```py\n>>> answer_vector = wv['woman'] + wv['Europe'] + wv['physics'] +\n...     wv['scientist']\n```", "```py\n>>> answer_vector = wv['woman'] + wv['Europe'] + wv['physics'] +\\\n...     wv['scientist'] - wv['male'] - 2 * wv['man']\n```", "```py\n>>> answer_vector = wv['Louis_Pasteur'] - wv['germs'] + wv['physics']\n```", "```py\nMARIE CURIE : SCIENCE :: ? : MUSIC\n```", "```py\n>>> wv['Marie_Curie'] - wv['science'] + wv['music']\n```", "```py\nTIMBERS : PORTLAND :: ? : SEATTLE\n```", "```py\nWALK : LEGS :: ? : MOUTH\n```", "```py\nANALOGY : WORDS :: ? : NUMBERS\n```", "```py\n>>> from nessvec.examples.ch06.nessvectors import *  # #1\n>>> nessvector('Marie_Curie').round(2)\nplaceness     -0.46\npeopleness     0.35  # #2\nanimalness     0.17\nconceptness   -0.32\nfemaleness     0.26\n```", "```py\nwv['Timbers'] - wv['Portland'] + wv['Seattle'] = ?\n```", "```py\nwv['Seattle_Sounders']\n```", "```py\nwv['Marie_Curie'] - wv['physics'] + wv['classical_music'] = ?\n```", "```py\nPortland Timbers + Seattle - Portland = ?\n```", "```py\n\"San Francisco is to California as what is to Colorado?\"\n```", "```py\nSan Francisco - California + Colorado = Denver\n```", "```py\n>>> import torchtext\n\n>>> dsets = torchtext.datasets.WikiText2()\n>>> num_texts = 10000\n\n>>> filepath = DATA_DIR / f'WikiText2-{num_texts}.txt'\n>>> with open(filepath, 'wt') as fout:\n...     fout.writelines(list(dsets[0])[:num_texts])\n```", "```py\n>>> !tail -n 3 ~/nessvec-data/WikiText2-10000.txt\n\nWhen Marge leaves Dr. Zweig 's office , she says ,\n\" Whenever the wind whistles through the leaves ,\nI 'll think , Lowenstein , Lowenstein â€¦ \" .\nThis is a reference to The Prince of Tides ; the <unk> is Dr. Lowenstein .\n\n= = Reception = =\n```", "```py\n>>> import datasets\n>>> dset = datasets.load_dataset('text', data_files=str(filepath))\n>>> dset\nDatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 10000\n    })\n})\n```", "```py\ndef tokenize_row(row):\n    row['all_tokens'] = row['text'].lower().split()\n    return row\n```", "```py\n>>> dset = dset.map(tokenize_row)\n>>> dset\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'tokens'],\n        num_rows: 10000\n    })\n})\n```", "```py\n>>> vocab = list(set(\n...     [tok for row in dset['train']['tokens'] for tok in row]))\n>>> vocab[:4]\n['cast', 'kaifeng', 'recovered', 'doctorate']\n\n>>> id2tok = dict(enumerate(vocab))\n>>> list(id2tok.items())[:4]\n[(0, 'cast'), (1, 'kaifeng'), (2, 'recovered'), (3, 'doctorate')]\n\n>>> tok2id = {tok: i for (i, tok) in id2tok.items()}\n>>> list(tok2id.items())[:4]\n[('cast', 0), ('kaifeng', 1), ('recovered', 2), ('doctorate', 3)]\n```", "```py\nWINDOW_WIDTH = 10\n\n>>> def windowizer(row, wsize=WINDOW_WIDTH):\n    \"\"\" Compute sentence (str) to sliding-window of skip-gram pairs. \"\"\"\n...    doc = row['tokens']\n...    out = []\n...    for i, wd in enumerate(doc):\n...        target = tok2id[wd]\n...        window = [\n...            i + j for j in range(-wsize, wsize + 1, 1)\n...            if (i + j >= 0) & (i + j < len(doc)) & (j != 0)\n...        ]\n\n...        out += [(target, tok2id[doc[w]]) for w in window]\n...    row['moving_window'] = out\n...    return row\n```", "```py\n>>> dset = dset.map(windowizer)\n>>> dset\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'tokens', 'window'],\n        num_rows: 10000\n    })\n})\n```", "```py\n>>> def skip_grams(tokens, window_width=WINDOW_WIDTH):\n...    pairs = []\n...    for i, wd in enumerate(tokens):\n...        target = tok2id[wd]\n...        window = [\n...            i + j for j in\n...            range(-window_width, window_width + 1, 1)\n...            if (i + j >= 0)\n...            & (i + j < len(tokens))\n...            & (j != 0)\n...        ]\n\n...        pairs.extend([(target, tok2id[tokens[w]]) for w in window])\n    # huggingface datasets are dictionaries for every text element\n...    return pairs\n```", "```py\n>>> from torch.utils.data import Dataset\n\n>>> class Word2VecDataset(Dataset):\n...    def __init__(self, dataset, vocab_size, wsize=WINDOW_WIDTH):\n...        self.dataset = dataset\n...        self.vocab_size = vocab_size\n...        self.data = [i for s in dataset['moving_window'] for i in s]\n...\n...    def __len__(self):\n...        return len(self.data)\n...\n...    def __getitem__(self, idx):\n...        return self.data[idx]\n```", "```py\nfrom torch.utils.data import DataLoader\n\ndataloader = {}\nfor k in dset.keys():\n    dataloader = {\n        k: DataLoader(\n            Word2VecDataset(\n                dset[k],\n                vocab_size=len(vocab)),\n            batch_size=BATCH_SIZE,\n            shuffle=True,\n            num_workers=CPU_CORES - 1)\n    }\n```", "```py\ndef one_hot_encode(input_id, size):\n    vec = torch.zeros(size).float()\n    vec[input_id] = 1.0\n    return vec\n```", "```py\nfrom torch import nn\nEMBED_DIM = 100  # #1\n\nclass Word2Vec(nn.Module):\n    def __init__(self, vocab_size=len(vocab), embedding_size=EMBED_DIM):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embedding_size)  # #2\n        self.expand = nn.Linear(embedding_size, vocab_size, bias=False)\n\n    def forward(self, input):\n        hidden = self.embed(input)  # #3\n        logits = self.expand(hidden)  # #4\n        return logits\n```", "```py\n>>> model = Word2Vec()\n>>> model\n\nWord2Vec(\n  (embed): Embedding(20641, 100)\n  (expand): Linear(in_features=100, out_features=20641, bias=False)\n)\n```", "```py\n>>> import torch\n>>> if torch.cuda.is_available():\n...     device = torch.device('cuda')\n>>> else:\n...     device = torch.device('cpu')\n>>> device\n\ndevice(type='cpu')\n```", "```py\n>>> model.to(device)\n\nWord2Vec(\n  (embed): Embedding(20641, 100)\n  (expand): Linear(in_features=100, out_features=20641, bias=False)\n)\n```", "```py\n>>> from tqdm import tqdm  # noqa\n>>> EPOCHS = 10\n>>> LEARNING_RATE = 5e-4\nEPOCHS = 10\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n```", "```py\nrunning_loss = []\npbar = tqdm(range(EPOCHS * len(dataloader['train'])))\nfor epoch in range(EPOCHS):\n    epoch_loss = 0\n    for sample_num, (center, context) in enumerate(dataloader['train']):\n        if sample_num % len(dataloader['train']) == 2:\n            print(center, context)\n            # center: tensor([ 229,    0, 2379,  ...,  402,  553,  521])\n            # context: tensor([ 112, 1734,  802,  ...,   28,  852,  363])\n        center, context = center.to(device), context.to(device)\n        optimizer.zero_grad()\n        logits = model(input=context)\n        loss = loss_fn(logits, center)\n        if not sample_num % 10000:\n            # print(center, context)\n            pbar.set_description(f'loss[{sample_num}] = {loss.item()}')\n        epoch_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n        pbar.update(1)\n    epoch_loss /= len(dataloader['train'])\n    running_loss.append(epoch_loss)\n\nsave_model(model, loss)\n```", "```py\n>>> from nlpia.data.loaders import get_data\n>>> word_vectors = get_data('word2vec')\n```", "```py\n>>> from gensim.models.keyedvectors import KeyedVectors\n>>> word_vectors = KeyedVectors.load_word2vec_format(\\\n...     '/path/to/GoogleNews-vectors-negative300.bin.gz', binary=True)\n```", "```py\n>>> from gensim.models.keyedvectors import KeyedVectors\n>>> from nlpia.loaders import get_data\n>>> word_vectors = get_data('w2v', limit=200000)  # #1\n```", "```py\n>>> word_vectors.most_similar(positive=['cooking', 'potatoes'], topn=5)\n[('cook', 0.6973530650138855),\n ('oven_roasting', 0.6754530668258667),\n ('Slow_cooker', 0.6742032170295715),\n ('sweet_potatoes', 0.6600279808044434),\n ('stir_fry_vegetables', 0.6548759341239929)]\n>>> word_vectors.most_similar(positive=['germany', 'france'], topn=1)\n[('europe', 0.7222039699554443)]\n```", "```py\n>>> word_vectors.doesnt_match(\"potatoes milk cake computer\".split())\n'computer'\n```", "```py\n>>> word_vectors.most_similar(positive=['king', 'woman'],\n...     negative=['man'], topn=2)\n[('queen', 0.7118192315101624), ('monarch', 0.6189674139022827)]\n```", "```py\n>>> word_vectors.similarity('princess', 'queen')\n0.70705315983704509\n```", "```py\n>>> word_vectors['phone']\narray([-0.01446533, -0.12792969, -0.11572266, -0.22167969, -0.07373047,\n       -0.05981445, -0.10009766, -0.06884766,  0.14941406,  0.10107422,\n       -0.03076172, -0.03271484, -0.03125   , -0.10791016,  0.12158203,\n        0.16015625,  0.19335938,  0.0065918 , -0.15429688,  0.03710938,\n        ...\n```", "```py\n>>> token_list\n[\n  ['to', 'provide', 'early', 'intervention/early', 'childhood', 'special',\n   'education', 'services', 'to', 'eligible', 'children', 'and', 'their',\n   'families'],\n  ['essential', 'job', 'functions'],\n  ['participate', 'as', 'a', 'transdisciplinary', 'team', 'member', 'to',\n   'complete', 'educational', 'assessments', 'for']\n  ...\n]\n```", "```py\n>>> from gensim.models.word2vec import Word2Vec\n```", "```py\n>>> num_features = 300  # #1\n>>> min_word_count = 3  # #2\n>>> num_workers = 2  # #3\n>>> window_size = 6  # #4\n>>> subsampling = 1e-3  # #5\n```", "```py\n>>> model = Word2Vec(\n...     token_list,\n...     workers=num_workers,\n...     size=num_features,\n...     min_count=min_word_count,\n...     window=window_size,\n...     sample=subsampling)\n```", "```py\n>>> model.init_sims(replace=True)\n```", "```py\n>>> model_name = \"my_domain_specific_word2vec_model\"\n>>> model.save(model_name)\n```", "```py\n>>> from gensim.models.word2vec import Word2Vec\n>>> model_name = \"my_domain_specific_word2vec_model\"\n>>> model = Word2Vec.load(model_name)\n>>> model.most_similar('radiology')\n```", "```py\n>>> import spacy\n>>>\n>>> nlp = spacy.load(\"en_core_web_sm\")\n>>> text = \"This is an example sentence.\"\n>>> doc = nlp(text)\n>>>\n>>> for token in doc:\n...    print(token.text, token.vector)\n```", "```py\n['wh', 'whi', 'hi', 'his', 'is', 'isp', 'sp', 'spe', 'pe', 'per', 'er']\n```", "```py\n>>> from nessvec.files import load_fasttext\n>>> df = load_fasttext()  # #1\n>>> df.head().round(2)\n      0     1     2    ...   297   298   299\n,    0.11  0.01  0.00  ...  0.00  0.12 -0.04\nthe  0.09  0.02 -0.06  ...  0.16 -0.03 -0.03\n.    0.00  0.00 -0.02  ...  0.21  0.07 -0.05\nand -0.03  0.01 -0.02  ...  0.10  0.09  0.01\nof  -0.01 -0.03 -0.03  ...  0.12  0.01  0.02\n>>> df.loc['prosocial']  # #2\n0      0.0004\n1     -0.0328\n2     -0.1185\n        ...\n297    0.1010\n298   -0.1323\n299    0.2874\nName: prosocial, Length: 300, dtype: float64\n```", "```py\n>>> from nessvec.indexers import Index\n>>> index = Index()  # #1\n>>> vecs = index.vecs\n>>> vecs.shape\n(3000000, 300)\n```", "```py\n>>> import pandas as pd\n>>> vocab = pd.Series(wv.vocab)\n>>> vocab.iloc[1000000:100006]\nIllington_Fund             Vocab(count:447860, index:2552140)\nIllingworth                 Vocab(count:2905166, index:94834)\nIllingworth_Halifax       Vocab(count:1984281, index:1015719)\nIllini                      Vocab(count:2984391, index:15609)\nIlliniBoard.com           Vocab(count:1481047, index:1518953)\nIllini_Bluffs              Vocab(count:2636947, index:363053)\n```", "```py\n>>> wv['Illini']\narray([ 0.15625   ,  0.18652344,  0.33203125,  0.55859375,  0.03637695,\n       -0.09375   , -0.05029297,  0.16796875, -0.0625    ,  0.09912109,\n       -0.0291748 ,  0.39257812,  0.05395508,  0.35351562, -0.02270508,\n       ...\n       ])\n```", "```py\n>>> import numpy as np\n>>> np.linalg.norm(wv['Illinois'] - wv['Illini'])  # #1\n3.3653798\n>>> cos_similarity = np.dot(wv['Illinois'], wv['Illini']) / (\n...     np.linalg.norm(wv['Illinois']) *\\\n...     np.linalg.norm(wv['Illini']))  # #2\n>>> cos_similarity\n0.5501352\n>>> 1 - cos_similarity # #3\n0.4498648\n```", "```py\n>>> from nlpia.data.loaders import get_data\n>>> cities = get_data('cities')\n>>> cities.head(1).T\ngeonameid                       3039154\nname                          El Tarter\nasciiname                     El Tarter\nalternatenames     Ehl Tarter,Ð­Ð» Ð¢Ð°Ñ€Ñ‚ÐµÑ€\nlatitude                        42.5795\nlongitude                       1.65362\nfeature_class                         P\nfeature_code                        PPL\ncountry_code                         AD\ncc2                                 NaN\nadmin1_code                          02\nadmin2_code                         NaN\nadmin3_code                         NaN\nadmin4_code                         NaN\npopulation                         1052\nelevation                           NaN\ndem                                1721\ntimezone                 Europe/Andorra\nmodification_date            2012-11-03\n```", "```py\n>>> us = cities[(cities.country_code == 'US') &\\\n...     (cities.admin1_code.notnull())].copy()\n>>> states = pd.read_csv(\\\n...     'http://www.fonz.net/blog/wp-content/uploads/2008/04/states.csv')\n>>> states = dict(zip(states.Abbreviation, states.State))\n>>> us['city'] = us.name.copy()\n>>> us['st'] = us.admin1_code.copy()\n>>> us['state'] = us.st.map(states)\n>>> us[us.columns[-3:]].head()\n                     city  st    state\ngeonameid\n4046255       Bay Minette  AL  Alabama\n4046274              Edna  TX    Texas\n4046319    Bayou La Batre  AL  Alabama\n4046332         Henderson  TX    Texas\n4046430           Natalia  TX    Texas\n```", "```py\n>>> vocab = pd.np.concatenate([us.city, us.st, us.state])\n>>> vocab = np.array([word for word in vocab if word in wv.wv])\n>>> vocab[:10]\n```", "```py\n>>> city_plus_state = []\n>>> for c, state, st in zip(us.city, us.state, us.st):\n...     if c not in vocab:\n...         continue\n...     row = []\n...     if state in vocab:\n...         row.extend(wv[c] + wv[state])\n...     else:\n...         row.extend(wv[c] + wv[st])\n...     city_plus_state.append(row)\n>>> us_300D = pd.DataFrame(city_plus_state)\n```", "```py\n>>> word_model.distance('man', 'nurse')\n0.7453\n>>> word_model.distance('woman', 'nurse')\n0.5586\n```", "```py\n>>> from sklearn.decomposition import PCA\n>>> pca = PCA(n_components=2)  # #1\n>>> us_300D = get_data('cities_us_wordvectors')\n>>> us_2D = pca.fit_transform(us_300D.iloc[:, :300])  # #2\n```", "```py\n>>> import seaborn\n>>> from matplotlib import pyplot as plt\n>>> from nlpia.plots import offline_plotly_scatter_bubble\n>>> df = get_data('cities_us_wordvectors_pca2_meta')\n>>> html = offline_plotly_scatter_bubble(\n...     df.sort_values('population', ascending=False)[:350].copy()\\\n...         .sort_values('population'),\n...     filename='plotly_scatter_bubble.html',\n...     x='x', y='y',\n...     size_col='population', text_col='name', category_col='timezone',\n...     xscale=None, yscale=None,  # 'log' or None\n...     layout={}, marker={'sizeref': 3000})\n{'sizemode': 'area', 'sizeref': 3000}\n```", "```py\nfrom nessvec.files import load_glove\n```", "```py\n>>> import requests\n>>> repo = 'https://gitlab.com/tangibleai/nlpia2/-/raw/main'\n>>> name = 'Chapter-06_Reasoning-with-word-embeddings-word-vectors.adoc'\n>>> url = f'{repo}/src/nlpia2/data/{name}'\n>>> adoc_text = requests.get(url)\n```", "```py\n>>> from pathlib import Path\n>>> path = Path.cwd() / name\n>>> with path.open('w') as fout:\n...     fout.write(adoc_text)\n```", "```py\n>>> import subprocess\n>>> subprocess.run(args=[   # #1\n...     'asciidoc3', '-a', '-n', '-a', 'icons', path.name])\n```", "```py\n>>> if os.path.exists(chapt6_html) and os.path.getsize(chapt6_html) > 0:\n...     chapter6_html = open(chapt6_html, 'r').read()\n...     bsoup = BeautifulSoup(chapter6_html, 'html.parser')\n...     text = bsoup.get_text()  # #1\n```", "```py\n>>> import spacy\n>>> nlp = spacy.load('en_core_web_md')\n>>> config = {'punct_chars': None}\n>>> nlp.add_pipe('sentencizer', config=config)\n>>> doc = nlp(text)\n>>> sentences = []\n>>> noun_phrases = []\n>>> for sent in doc.sents:\n...     sent_noun_chunks = list(sent.noun_chunks)\n...     if sent_noun_chunks:\n...         sentences.append(sent)\n...         noun_phrases.append(max(sent_noun_chunks))\n>>> sent_vecs = []\n>>> for sent in sentences:\n...    sent_vecs.append(sent.vector)\n```", "```py\n>>> import numpy as np\n>>> vector = np.array([1, 2, 3, 4])  # #1\n>>> np.sqrt(sum(vector**2))\n5.47...\n>>> np.linalg.norm(vector)  # #2\n5.47...\n```", "```py\n>>> import numpy as np\n>>> for i, sent_vec in enumerate(sent_vecs):\n...     sent_vecs[i] = sent_vec / np.linalg.norm(sent_vec)\n```", "```py\n>>> np_array_sent_vecs_norm = np.array(sent_vecs)\n>>> similarity_matrix = np_array_sent_vecs_norm.dot(\n...     np_array_sent_vecs_norm.T)  # #1\n```", "```py\n>>> import re\n>>> import networkx as nx\n>>> similarity_matrix = np.triu(similarity_matrix, k=1)  # #1\n>>> iterator = np.nditer(similarity_matrix,\n...     flags=['multi_index'], order='C')\n>>> node_labels = dict()\n>>> G = nx.Graph()\n>>> pattern = re.compile(\n...    r'[\\w\\s]*[\\'\\\"]?[\\w\\s]+\\-?[\\w\\s]*[\\'\\\"]?[\\w\\s]*'\n...    )  # #2\n>>> for edge in iterator:\n...     key = 0\n...     value = ''\n...     if edge > 0.95:  # #3\n...         key = iterator.multi_index[0]\n...         value = str(noun_phrases[iterator.multi_index[0]])\n...         if (pattern.fullmatch(value)\n...             and (value.lower().rstrip() != 'figure')):\n...                 node_labels[key] = value\n...         G.add_node(iterator.multi_index[0])\n...         G.add_edge(iterator.multi_index[0],\n...             iterator.multi_index[1], weight=edge)\n```", "```py\n>>> import matplotlib.pyplot as plt\n>>> plt.subplot(1, 1, 1)  # #1\n>>> pos = nx.spring_layout(G, k=0.15, seed=42)  # #2\n>>> nx.draw_networkx(G,\n...    pos=pos,  # #3\n...    with_labels=True,\n...    labels=node_labels,\n...    font_weight='bold')\n>>> plt.show()\n```"]