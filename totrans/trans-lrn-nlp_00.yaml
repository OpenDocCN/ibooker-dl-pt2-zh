- en: front matter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: preface
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Over the past couple of years, it has become increasingly difficult to ignore
    the breakneck speed at which the field of natural language processing (NLP) has
    been progressing. Over this period, you have likely been bombarded with news articles
    about trending NLP models such as ELMo, BERT, and more recently GPT-3\. The excitement
    around this technology is warranted, because these models have enabled NLP applications
    we couldn’t imagine would be practical just three years prior, such as writing
    production code from a mere description of it, or the automatic generation of
    believable poetry and blogging.
  prefs: []
  type: TYPE_NORMAL
- en: A large driver behind this advance has been the focus on increasingly sophisticated
    transfer learning techniques for NLP models. Transfer learning is an increasingly
    popular and exciting paradigm in NLP because it enables you to adapt or transfer
    the knowledge acquired from one scenario to a different scenario, such as a different
    language or task. It is a big step forward for the democratization of NLP and,
    more widely, artificial intelligence (AI), allowing knowledge to be reused in
    new settings at a fraction of the previously required resources.
  prefs: []
  type: TYPE_NORMAL
- en: As a citizen of the West African nation of Ghana, where many budding entrepreneurs
    and inventors do not have access to vast computing resources and where so many
    fundamental NLP problems remain to be solved, this topic is particularly personal
    to me. This paradigm empowers engineers in such settings to build potentially
    life-saving NLP technologies, which would simply not be possible otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: I first encountered these ideas in 2017, while working on open source automatic
    machine learning technologies within the US Defense Advanced Research Projects
    Agency (DARPA) ecosystem. We used transfer learning to reduce the requirement
    for labeled data by training NLP systems on simulated data first and then transferring
    the model to a small set of real labeled data. The breakthrough model ELMo emerged
    shortly after and inspired me to learn more about the topic and explore how I
    could leverage these ideas further in my software projects.
  prefs: []
  type: TYPE_NORMAL
- en: Naturally, I discovered that a comprehensive practical introduction to the topic
    did not exist, due to the sheer novelty of these ideas and the speed at which
    the field is moving. When an opportunity to write a practical introduction to
    the topic presented itself in 2019, I didn’t think twice. You are holding in your
    hands the product of approximately two years of effort toward this purpose. This
    book will quickly bring you up to speed on key recent NLP models in the space
    and provide executable code you will be able to modify and reuse directly in your
    own projects. Although it would be impossible to cover every single architecture
    and use case, we strategically cover architectures and examples that we believe
    will arm you with fundamental skills for further exploration and staying up-to-date
    in this burgeoning field on your own.
  prefs: []
  type: TYPE_NORMAL
- en: You made a good decision when you decided to learn more about this topic. Opportunities
    for novel theories, algorithmic methodologies, and breakthrough applications abound.
    I look forward to hearing about the transformational positive impact you make
    on the society around you with it.
  prefs: []
  type: TYPE_NORMAL
- en: acknowledgments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I am grateful to members of the NLP Ghana open source community, where I have
    had the privilege to learn more about this important topic. The feedback from
    members of the group and users of our tools has served to underscore my understanding
    of how transformational this technology truly is. This has inspired and motivated
    me to push this book across the finish line.
  prefs: []
  type: TYPE_NORMAL
- en: I would like to thank my Manning development editor, Susan Ethridge, for the
    uncountable hours spent reading the manuscript, providing feedback, and guiding
    me through the many challenges. I am thankful for all the time and effort my technical
    development editor, Al Krinker, put in to help me improve the technical dimension
    of my writing.
  prefs: []
  type: TYPE_NORMAL
- en: I am grateful to all members of the editorial board, the marketing professionals,
    and other members of the production team that worked hard to make this book a
    reality. In no particular order, these include Rebecca Rinehart, Bert Bates, Nicole
    Butterfield, Rejhana Markanovic, Aleksandar Dragosavljevic´, Melissa Ice, Branko
    Latincic, Christopher Kaufmann, Candace Gillhoolley, Becky Whitney, Pamela Hunt,
    and Radmila Ercegovac.
  prefs: []
  type: TYPE_NORMAL
- en: The technical peer reviewers provided invaluable feedback at several junctures
    during this project, and the book would not be nearly as good without them. I
    am very grateful for their input. These include Andres Sacco, Angelo Simone Scotto,
    Ariel Gamino, Austin Poor, Clifford Thurber, Diego Casella, Jaume López, Manuel
    R. Ciosici, Marc-Anthony Taylor, Mathijs Affourtit, Matthew Sarmiento, Michael
    Wall, Nikos Kanakaris, Ninoslav Cerkez, Or Golan, Rani Sharim, Sayak Paul, Sebastián
    Palma, Sergio Govoni, Todd Cook, and Vamsi Sistla. I am thankful to the technical
    proofreader, Ariel Gamiño, for catching many typos and other errors during the
    proofreading process. I am grateful to all the excellent comments from book forum
    participants that further helped improve the book.
  prefs: []
  type: TYPE_NORMAL
- en: I am extremely grateful to my wife, Diana, for supporting and encouraging this
    work. I am grateful to my Mom and my siblings—Richard, Gideon, and Gifty—for continuing
    to motivate me.
  prefs: []
  type: TYPE_NORMAL
- en: about this book
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This book is an attempt to produce a comprehensive practical introduction to
    the important topic of transfer learning for NLP. Rather than focusing on theory,
    we stress building intuition via representative code and examples. Our code is
    written to facilitate quickly modifying and repurposing it to solve your own practical
    problems and challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Who should read this book?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To get the most out of this book, you should have some experience with Python,
    as well as some intermediate machine learning skills, such as an understanding
    of basic classification and regression concepts. It would also help to have some
    basic data manipulation and preprocessing skills with libraries such as Pandas
    and NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: That said, I wrote the book in a way that allows you to pick up these skills
    with a bit of extra work. The first three chapters will rapidly bring you up to
    speed on everything you need to know to grasp the transfer learning for NLP concepts
    sufficiently to apply in your own projects. Subsequently, following the included
    curated references on your own will solidify your prerequisite background skills,
    if that is something you feel that you need.
  prefs: []
  type: TYPE_NORMAL
- en: Road map
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The book is divided into three parts. You will get the most out of it by progressing
    through them in the order of appearance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 1 reviews key concepts in machine learning, presents a historical overview
    of advances in machine learning that have enabled the recent progress in transfer
    learning for NLP, and provides the motivation for studying the subject. It also
    walks through a pair of examples that serve to both review your knowledge of more
    traditional NLP methods and get your hands dirty with some key modern transfer
    learning for NLP approaches. A chapter-level breakdown of covered concepts in
    this part of the book follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 1 covers what exactly transfer learning is, both generally in AI and
    in the context of NLP. It also looks at the historical progression of technological
    advances that enabled it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 2 introduces a pair of representative example natural language processing
    (NLP) problems and shows how to obtain and preprocess data for them. It also establishes
    baselines for them using the traditional linear machine learning methods of logistic
    regression and support vector machines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 3 continues baselining the pair of problems from chapter 2 with the
    traditional tree-based machine learning methods—random forests and gradient boosting
    machines. It also baselines them using key modern transfer learning techniques,
    ELMo and BERT.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Part 2 dives deeper into some important transfer learning NLP approaches based
    on shallow neural networks, that is, neural networks with relatively few layers.
    It also begins to explore deep transfer learning in more detail via representative
    techniques, such as ELMo, that employ recurrent neural networks (RNNs) for key
    functions. A chapter-level breakdown of covered concepts in this part of the book
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 4 applies shallow word and sentence embedding techniques, such as word2vec
    and sent2vec, to further explore some of our illustrative examples from part 1
    of the book. It also introduces the important transfer learning concepts of domain
    adaptation and multitask learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 5 introduces a set of deep transfer learning NLP methods that rely on
    RNNs, as well as a fresh pair of illustrative example datasets that will be used
    to study them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 6 discusses the methods introduced in chapter 5 in more detail and applies
    them to the datasets introduced in the same chapter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Part 3 covers arguably the most important subfield in this space, namely, deep
    transfer learning techniques relying on transformer neural networks for key functions,
    such as BERT and GPT. This model architecture class is proving to be the most
    influential on recent applications, partly due to better scalability on parallel
    computing architectures than equivalent prior methods. This part also digs deeper
    into various adaptation strategies for making the transfer learning process more
    efficient. A chapter-level breakdown of covered concepts in this part of the book
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 7 describes the fundamental transformer architecture and uses an important
    variant of it—GPT—for some text generation and a basic chatbot.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 8 covers the important transformer architecture BERT and applies it
    to a number of use cases, including question answering, filling in the blanks,
    and cross-lingual transfer to a low-resource language.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 9 introduces some adaptation strategies meant to make the transfer learning
    process more efficient. This includes the strategies of discriminative fine-tuning
    and gradual unfreezing from the method ULMFiT, as well as knowledge distillation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 10 introduces additional adaptation strategies, including embedding
    factorization and parameter sharing—strategies behind the ALBERT method. The chapter
    also covers adapters and sequential multitask adaptation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 11 concludes the book by reviewing important topics and briefly discussing
    emerging research topics and directions, such as the need to think about and mitigate
    potential negative impacts of the technology. These include biased predictions
    on different parts of the population and the environmental impact of training
    these large models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Software requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kaggle notebooks are the recommended way of executing these methods, because
    they allow you to get moving right away without any setup delays. Moreover, the
    free GPU resources provided by this service at the time of writing expand the
    accessibility of all these methods to people who may not have access to powerful
    GPUs locally, which is consistent with the “democratization of AI” agenda that
    excites so many people about NLP transfer learning. Appendix A provides a Kaggle
    quick start guide and a number of the author’s personal tips on how to maximize
    the platform’s usefulness. However, we anticipate that most readers should find
    it pretty self-explanatory to get started. We have hosted all notebooks publicly
    on Kaggle with all required data attached to enable you to start executing code
    in a few clicks. However, please remember to “copy and edit” (fork) notebooks—instead
    of copying and pasting into a new Kaggle notebook—because this will ensure that
    the resulting libraries in the environment match those that we wrote the code
    for.
  prefs: []
  type: TYPE_NORMAL
- en: About the code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This book contains many examples of source code both in numbered listings and
    in line with normal text. In both cases, source code is formatted in a `fixed-width
    font like this` to separate it from ordinary text. Sometimes code is also `**in
    bold**` to highlight code that has changed from previous steps in the chapter,
    such as when a new feature adds to an existing line of code.
  prefs: []
  type: TYPE_NORMAL
- en: In many cases, the original source code has been reformatted; we’ve added line
    breaks and reworked indentation to accommodate the available page space in the
    book. In rare cases, even this was not enough, and listings include line-continuation
    markers (➥). Additionally, comments in the source code have often been removed
    from the listings when the code is described in the text. Code annotations accompany
    many of the listings, highlighting important concepts.
  prefs: []
  type: TYPE_NORMAL
- en: The code for the examples in this book is available for download from the Manning
    website at [http://www.manning.com/downloads/2116](http://www.manning.com/downloads/2116)and
    from GitHub at [https://github.com/azunre/transfer-learning-for-nlp](https://github.com/azunre/transfer-learning-for-nlp).
  prefs: []
  type: TYPE_NORMAL
- en: liveBook discussion forum
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Purchase of *Transfer Learning for Natural Language Processing* includes free
    access to a private web forum run by Manning Publications where you can make comments
    about the book, ask technical questions, and receive help from the author and
    from other users. To access the forum, go to [https://livebook.manning.com/#!/book/transfer-learning-for-natural-language-processing/discussion](https://livebook.manning.com/#!/book/transfer-learning-for-natural-language-processing/discussion)
    . You can also learn more about Manning’s forums and the rules of conduct at [https://livebook.manning.com/#!/
    discussion](https://livebook.manning.com/#!/discussion).
  prefs: []
  type: TYPE_NORMAL
- en: Manning’s commitment to our readers is to provide a venue where a meaningful
    dialogue between individual readers and between readers and the author can take
    place. It is not a commitment to any specific amount of participation on the part
    of the author, whose contribution to the forum remains voluntary (and unpaid).
    We suggest you try asking the author some challenging questions lest his interest
    stray! The forum and the archives of previous discussions will be accessible from
    the publisher’s website as long as the book is in print.
  prefs: []
  type: TYPE_NORMAL
- en: about the author
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Paul Azunre** holds a PhD in Computer Science from MIT and has served as
    a principal investigator on several DARPA research programs. He founded Algorine
    Inc., a research lab dedicated to advancing AI/ML and identifying scenarios where
    they can have a significant social impact. Paul also co-founded Ghana NLP, an
    open source initiative focused on using NLP and Transfer Learning with Ghanaian
    and other low-resource languages.'
  prefs: []
  type: TYPE_NORMAL
- en: about the cover illustration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The figure on the cover of *Transfer Learning for Natural Language Processing*
    is captioned “Moluquoise,” or Moluccan woman. The illustration is taken from a
    collection of dress costumes from various countries by Jacques Grasset de Saint-Sauveur
    (1757-1810), titled *Costumes civils actuels de tous les peuples connus*, published
    in France in 1788\. Each illustration is finely drawn and colored by hand. The
    rich variety of Grasset de Saint-Sauveur’s collection reminds us vividly of how
    culturally apart the world’s towns and regions were just 200 years ago. Isolated
    from each other, people spoke different dialects and languages. In the streets
    or in the countryside, it was easy to identify where they lived and what their
    trade or station in life was just by their dress.
  prefs: []
  type: TYPE_NORMAL
- en: The way we dress has changed since then and the diversity by region, so rich
    at the time, has faded away. It is now hard to tell apart the inhabitants of different
    continents, let alone different towns, regions, or countries. Perhaps we have
    traded cultural diversity for a more varied personal life—certainly for a more
    varied and fast-paced technological life.
  prefs: []
  type: TYPE_NORMAL
- en: At a time when it is hard to tell one computer book from another, Manning celebrates
    the inventiveness and initiative of the computer business with book covers based
    on the rich diversity of regional life of two centuries ago, brought back to life
    by Grasset de Saint-Sauveur’s pictures.
  prefs: []
  type: TYPE_NORMAL
