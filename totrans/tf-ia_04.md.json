["```py\nimport requests\nimport pandas as pd\nimport tensorflow as tf\n```", "```py\nurl = \"https:/ /archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\nr = requests.get(url)\n\n# Writing data to a file\nwith open('iris.data', 'wb') as f:\n  f.write(r.content)\n```", "```py\niris_df = pd.read_csv('iris.data', header=None)\n```", "```py\n0     1       2       3       4\n0     5.1     3.5     1.4     0.2     Iris-setosa\n1     4.9     3.0     1.4     0.2     Iris-setosa\n2     4.7     3.2     1.3     0.2     Iris-setosa\n```", "```py\niris_df.columns = ['sepal_length', 'sepal_width', 'petal_width', 'petal_length', 'label']\n```", "```py\niris_df[\"label\"] = iris_df[\"label\"].map({'Iris-setosa':0, 'Iris-versicolor':1, 'Iris-virginica':2})\n```", "```py\n      sepal_length   sepal_width   petal_width   petal_length   label\n0     5.1             3.5           1.4           0.2            0\n1     4.9             3.0           1.4           0.2            0\n2     4.7             3.2           1.3           0.2            0\n```", "```py\niris_df = iris_df.sample(frac=1.0, random_state=4321)\nx = iris_df[[\"sepal_length\", \"sepal_width\", \"petal_width\", \"petal_length\"]]\nx = x - x.mean(axis=0)\ny = tf.one_hot(iris_df[\"label\"], depth=3)\n```", "```py\n        sepal_length  sepal_width  petal_width  petal_length\n31      -0.443333        0.346    -2.258667     -0.798667\n23      -0.743333        0.246    -2.058667     -0.698667\n70       0.056667        0.146     1.041333      0.601333\n100      0.456667        0.246     2.241333      1.301333\n44      -0.743333        0.746    -1.858667     -0.798667\n..            ...          ...          ...           ...\n```", "```py\ntf.Tensor(\n    [[1\\. 0\\. 0.]\n     [1\\. 0\\. 0.]\n     [0\\. 1\\. 0.]\n     ...\n     [0\\. 0\\. 1.]\n     [0\\. 0\\. 1.]\n     [0\\. 1\\. 0.]], \nshape=(150, 3), dtype=float32)\n```", "```py\nfrom tensorflow.keras.layers import Dense            ❶\nfrom tensorflow.keras.models import Sequential       ❶\nimport tensorflow.keras.backend as K                 ❶\n\nK.clear_session()                                    ❷\nmodel = Sequential([                                 ❸\n    Dense(32, activation='relu', input_shape=(4,)),  ❸\n    Dense(16, activation='relu'),                    ❸\n    Dense(3, activation='softmax')                   ❸\n])\n```", "```py\n[15, 30, 5]\n```", "```py\n[15/(15+30+5), 30/(15+30+5), 5/(15+30+5)]\n= [0.3, 0.6, 0.1]\n```", "```py\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n```", "```py\nmodel.summary()\n```", "```py\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_3 (Dense)              (None, 32)                160       \n_________________________________________________________________\ndense_4 (Dense)              (None, 16)                528       \n_________________________________________________________________\ndense_5 (Dense)              (None, 3)                 51        \n=================================================================\nTotal params: 739\nTrainable params: 739\nNon-trainable params: 0\n_________________________________________________________________\n```", "```py\nmodel.fit(x, y, batch_size=64, epochs=25)\n```", "```py\nTrain on 150 samples\nEpoch 1/25\n150/150 [==============================] - 0s 2ms/sample - loss: 1.1773 - acc: 0.2667\nEpoch 2/25\n150/150 [==============================] - 0s 148us/sample - loss: 1.1388 - acc: 0.2933\n...\nEpoch 24/25\n150/150 [==============================] - 0s 104us/sample - loss: 0.6254 - acc: 0.7400\nEpoch 25/25\n150/150 [==============================] - 0s 208us/sample - loss: 0.6078 - acc: 0.7400\n```", "```py\ndef fix_random_seed(seed):\n    try:\n        np.random.seed(seed)\n    except NameError:\n        print(\"Warning: Numpy is not imported. Setting the seed for Numpy failed.\")\n    try:\n        tf.random.set_seed(seed)\n    except NameError:\n        print(\"Warning: TensorFlow is not imported. Setting the seed for TensorFlow failed.\")\n    try:\n        random.seed(seed)\n    except NameError:\n        print(\"Warning: random module is not imported. Setting the seed for random failed.\")\n\n# Fixing the random seed\nfix_random_seed(4321)\n```", "```py\nfrom tensorflow.keras.layers import Input, Dense, Concatenate\nfrom tensorflow.keras.models import Model\n```", "```py\ninp1 = Input(shape=(4,))\ninp2 = Input(shape=(2,))\n```", "```py\nout1 = Dense(16, activation='relu')(inp1)\nout2 = Dense(16, activation='relu')(inp2)\n```", "```py\nout = Concatenate(axis=1)([out1,out2])\n```", "```py\nout = Dense(16, activation='relu')(out)\nout = Dense(3, activation='softmax')(out)\n```", "```py\nmodel = Model(inputs=[inp1, inp2], outputs=out)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n```", "```py\nfrom tensorflow.keras.layers import Input, Dense, Concatenate\nfrom tensorflow.keras.models import Model\nimport tensorflow.keras.backend as K\n\nK.clear_session()                                                                ❶\n\ninp1 = Input(shape=(4,))                                                         ❷\ninp2 = Input(shape=(2,))                                                         ❷\n\nout1 = Dense(16, activation='relu')(inp1)                                        ❸\nout2 = Dense(16, activation='relu')(inp2)                                        ❸\n\nout = Concatenate(axis=1)([out1,out2])                                           ❹\n\nout = Dense(16, activation='relu')(out)\nout = Dense(3, activation='softmax')(out) \n\nmodel = Model(inputs=[inp1, inp2], outputs=out)                                  ❺\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])❻\n```", "```py\nmodel.summary()\n```", "```py\nModel: \"model\"\n_____________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to \n=====================================================================================\ninput_1 (InputLayer)            [(None, 4)]          0 \n_____________________________________________________________________________________\ninput_2 (InputLayer)            [(None, 2)]          0 \n_____________________________________________________________________________________\ndense (Dense)                   (None, 16)           80          input_1[0][0]  \n_____________________________________________________________________________________\ndense_1 (Dense)                 (None, 16)           48          input_2[0][0]  \n_____________________________________________________________________________________\nconcatenate (Concatenate)       (None, 32)           0           dense[0][0]        \n                                                                 dense_1[0][0]      \n_____________________________________________________________________________________\ndense_2 (Dense)                 (None, 16)           528         concatenate[0][0]  \n_____________________________________________________________________________________\ndense_3 (Dense)                 (None, 3)            51          dense_2[0][0] \n=====================================================================================\nTotal params: 707\nTrainable params: 707\nNon-trainable params: 0\n_____________________________________________________________________________________\n```", "```py\ntf.keras.utils.plot_model(model)\n```", "```py\ntf.keras.utils.plot_model(model, to_file='model.png’)\n```", "```py\ntf.keras.utils.plot_model(model, show_shapes=True)\n```", "```py\nfrom sklearn.decomposition import PCA\n\npca_model = PCA(n_components=2, random_state=4321)\n\nx_pca = pca_model.fit_transform(x)\n```", "```py\nmodel.fit([x, x_pca], y, batch_size=64, epochs=10)\n```", "```py\nfrom tensorflow.keras import layers\n\nclass MulBiasDense(layers.Layer):\n\n    def __init__(self, units=32, input_dim=32, activation=None):              ❶\n        super(MulBiasDense, self).__init__()                                  ❶\n        self.units = units                                                    ❶\n        self.activation = activation                                          ❶\n\n    def build(self, input_shape):                                             ❷\n        self.w = self.add_weight(shape=(input_shape[-1], self.units),         ❷\n                                 initializer='glorot_uniform', trainable=True)❷\n        self.b = self.add_weight(shape=(self.units,),                         ❷\n                                 initializer='glorot_uniform', trainable=True)❷\n        self.b_mul = self.add_weight(shape=(self.units,),                     ❷\n                                 initializer='glorot_uniform', trainable=True)❷\n\n    def call(self, inputs):                                                   ❸\n        out = (tf.matmul(inputs, self.w) + self.b) * self.b_mul               ❸\n        return layers.Activation(self.activation)(out)                        ❸\n```", "```py\ndef __init__(self, units=32, activation=None):\n    super(MulBiasDense, self).__init__()\n    self.units = units\n    self.activation = activation\n```", "```py\ndef build(self, input_shape):\n    self.w = self.add_weight(shape=(input_shape[-1], self.units),\n                             initializer='glorot_uniform', trainable=True)\n    self.b = self.add_weight(shape=(self.units,),\n                             initializer='glorot_uniform', trainable=True)\n    self.b_mul = self.add_weight(shape=(self.units,),\n                                 initializer='glorot_uniform', trainable=True)\n```", "```py\ndef call(self, inputs):\n    out = (tf.matmul(inputs, self.w) + self.b) * self.b_mul\n    return layers.Activation(self.activation)(out)\n```", "```py\nfrom tensorflow.keras.layers import Input, Dense, Concatenate                    ❶\nfrom tensorflow.keras.models import Model                                        ❶\nimport tensorflow.keras.backend as K                                             ❶\nimport tensorflow as tf                                                          ❶\n\nK.clear_session()                                                                ❷\n\ninp = Input(shape=(4,))                                                          ❸\nout = MulBiasDense(units=32, activation='relu')(inp)                             ❹\nout = MulBiasDense(units=16, activation='relu')(out)                             ❹\nout = Dense(3, activation='softmax')(out)                                        ❺\n\nmodel = Model(inputs=inp, outputs=out)                                           ❻\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])❼\n```", "```py\nimport os # Provides various os related functions\n\ndata_dir = os.path.join('data','flower_images') + os.path.sep\ncsv_ds = tf.data.experimental.CsvDataset(\n    os.path.join(data_dir,'flower_labels.csv') , record_defaults=(\"\",-1), header=True\n)\n```", "```py\nfor item in csv_ds.take(5):\n    print(item)\n```", "```py\n(<tf.Tensor: shape=(), dtype=string, numpy=b'0001.png'>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n(<tf.Tensor: shape=(), dtype=string, numpy=b'0002.png'>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n(<tf.Tensor: shape=(), dtype=string, numpy=b'0003.png'>, <tf.Tensor: shape=(), dtype=int32, numpy=2>)\n(<tf.Tensor: shape=(), dtype=string, numpy=b'0004.png'>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n(<tf.Tensor: shape=(), dtype=string, numpy=b'0005.png'>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n```", "```py\nfname_ds = csv_ds.map(lambda a,b: a)\nlabel_ds = csv_ds.map(lambda a,b: b)\n```", "```py\nlambda x, y : x + y\n```", "```py\nimport tensorflow as tf\n\ndef get_image(file_path):\n\n    # loading the image from disk as a byte string\n    img = tf.io.read_file(data_dir + file_path)\n    # convert the compressed string to a 3D uint8 tensor\n    img = tf.image.decode_png(img, channels=3)\n    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n    img = tf.image.convert_image_dtype(img, tf.float32)\n    # resize the image to the desired size.\n    return tf.image.resize(img, [64, 64])\n```", "```py\nimage_ds = fname_ds.map(get_image)\n```", "```py\nlabel_ds = label_ds.map(lambda x: tf.one_hot(x, depth=10))\n```", "```py\ndata_ds = tf.data.Dataset.zip((image_ds, label_ds))\n```", "```py\nfor item in data_ds:\n    print(item)\n```", "```py\n>>> (<tf.Tensor: shape=(64, 64, 3), dtype=float32, numpy=\narray([[[0.05490196, 0.0872549 , 0.0372549 ],\n        [0.06764706, 0.09705883, 0.04411765],\n        [0.06862745, 0.09901962, 0.04509804],\n        ...,\n        [0.3362745 , 0.25686276, 0.21274512],\n        [0.26568627, 0.18823531, 0.16176471],\n        [0.2627451 , 0.18627453, 0.16960786]]], dtype=float32)>, <tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>)\n```", "```py\ndata_ds = data_ds.shuffle(buffer_size= 20)\n```", "```py\ndata_ds = data_ds.batch(5)\n```", "```py\nfor item in data_ds:\n    print(item)\n    break\n```", "```py\n(\n    <tf.Tensor: shape=(5, 64, 64, 3), dtype=float32, numpy=\n    array(\n        [\n            [\n                [\n                    [0.5852941 , 0.5088236 , 0.39411768],\n                    [0.5852941 , 0.50980395, 0.4009804 ],\n                    [0.5862745 , 0.51176476, 0.40490198],\n                    ...,\n                    [0.82156867, 0.7294118 , 0.62352943],\n                    [0.82745105, 0.74509805, 0.6392157 ],\n                    [0.8284314 , 0.75098044, 0.64509803]\n                ],  \n\n                [\n                    [0.07647059, 0.10784315, 0.05882353],\n                    [0.07843138, 0.11078432, 0.05882353],\n                    [0.11862746, 0.16078432, 0.0892157 ],\n                    ...,\n                    [0.17745098, 0.23529413, 0.12450981],\n                    [0.2019608 , 0.27549022, 0.14509805],\n                    [0.22450982, 0.28921568, 0.16470589]\n                ]\n            ]\n        ], \n        dtype=float32\n    )>, \n    <tf.Tensor: shape=(5, 10), dtype=float32, numpy=\n    array(\n        [\n            [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n            [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n            [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n            [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n            [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]\n        ], \n        dtype=float32\n    )>\n)\n```", "```py\nimport tensorflow as tf\nimport os\n\ndata_dir = os.path.join('data','flower_images', 'flower_images') + os.path.sep \ncsv_ds = tf.data.experimental.CsvDataset(                               ❶\n    os.path.join(data_dir,'flower_labels.csv') , (\"\",-1), header=True   ❶\n)                                                                       ❶\nfname_ds = csv_ds.map(lambda a,b: a)                                    ❷\nlabel_ds = csv_ds.map(lambda a,b: b)                                    ❷\n\ndef get_image(file_path):\n\n    img = tf.io.read_file(data_dir + file_path)\n    # convert the compressed string to a 3D uint8 tensor\n    img = tf.image.decode_png(img, channels=3)\n    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n    img = tf.image.convert_image_dtype(img, tf.float32)\n    # resize the image to the desired size.\n    return tf.image.resize(img, [64, 64])\n\nimage_ds = fname_ds.map(get_image)                                      ❸\nlabel_ds = label_ds.map(lambda x: tf.one_hot(x, depth=10))              ❹\ndata_ds = tf.data.Dataset.zip((image_ds, label_ds))                     ❺\n\ndata_ds = data_ds.shuffle(buffer_size= 20)                              ❻\ndata_ds = data_ds.batch(5)                                              ❻\n```", "```py\nmodel = Sequential([\n    Conv2D(64,(5,5), activation='relu', input_shape=(64,64,3)),\n    Flatten(),\n    Dense(10, activation='softmax')\n])\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n```", "```py\nmodel.fit(data_ds, epochs=10)\n```", "```py\nEpoch 1/10\n42/42 [==============================] - 1s 24ms/step - loss: 3.1604 - acc: 0.2571\nEpoch 2/10\n42/42 [==============================] - 1s 14ms/step - loss: 1.4359 - acc: 0.5190\n...\nEpoch 9/10\n42/42 [==============================] - 1s 14ms/step - loss: 0.0126 - acc: 1.0000\nEpoch 10/10\n42/42 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 1.0000\n```", "```py\ntf.keras.preprocessing.image.ImageDataGenerator\ntf.keras.preprocessing.sequence.TimeSeriesDataGenerator\n```", "```py\ndata_dir = os.path.join('data','flower_images', 'flower_images')\n```", "```py\nimg_gen = ImageDataGenerator()\n```", "```py\nlabels_df = pd.read_csv(os.path.join(data_dir, 'flower_labels.csv'), header=0)\ngen_iter = img_gen.flow_from_dataframe(\n    dataframe=labels_df, \n    directory=data_dir, \n    x_col='file', \n    y_col='label', \n    class_mode='raw', \n    batch_size=5, \n    target_size=(64,64)\n)\n```", "```py\nfor item in gen_iter:\n    print(item)\n    break\n```", "```py\n(\n    array([[[[ 10.,  11.,  11.],\n             [ 51.,  74.,  46.],\n             [ 36.,  56.,  32.],\n             ...,\n             [  4.,   4.,   3.],\n             [ 16.,  25.,  11.],\n             [ 17.,  18.,  13.]],\n            ...\n\n            [[197., 199., 174.],\n             [162., 160., 137.],\n             [227., 222., 207.],\n             ...,\n             [ 57.,  58.,  50.],\n             [ 33.,  34.,  27.],\n             [ 55.,  54.,  43.]]]], dtype=float32\n    ), \n    array([5, 6], dtype=int64)\n)\n```", "```py\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator           ❶\nimport os                                                                     ❶\nimport pandas as pd                                                           ❶\n\ndata_dir = os.path.join('data','flower_images', 'flower_images')              ❷\n\nimg_gen = ImageDataGenerator()                                                ❸\n\nprint(os.path.join(data_dir, 'flower_labels.csv'))\nlabels_df = pd.read_csv(os.path.join(data_dir, 'flower_labels.csv'), header=0)❹\n\ngen_iter = img_gen.flow_from_dataframe(                                       ❺\n    dataframe=labels_df, directory=data_dir, x_col='file', y_col='label',     ❺\n    class_mode='raw', batch_size=2, target_size=(64,64))                      ❺\n```", "```py\npip install tensorflow-datasets\n```", "```py\nimport tensorflow_datasets as tfds\n```", "```py\ntfds.list_builders()\n```", "```py\ndata, info = tfds.load(\"cifar10\", with_info=True)\n```", "```py\nprint(info)\n\n>>> tfds.core.DatasetInfo(\n    name='cifar10',\n    version=3.0.0,\n    description='The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.',\n    homepage='https:/ /www.cs.toronto.edu/~kriz/cifar.xhtml',\n    features=FeaturesDict({\n        'image': Image(shape=(32, 32, 3), dtype=tf.uint8),\n        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n    }),\n    total_num_examples=60000,\n    splits={\n        'test': 10000,\n        'train': 50000,\n    },\n    supervised_keys=('image', 'label'),\n    citation=\"\"\"@TECHREPORT{Krizhevsky09learningmultiple,\n        author = {Alex Krizhevsky},\n        title = {Learning multiple layers of features from tiny images},\n        institution = {},\n        year = {2009}\n    }\"\"\",\n    redistribution_info=,\n)\n```", "```py\nprint(data)\n\n>>> {'test': <DatasetV1Adapter \n        shapes: {image: (32, 32, 3), label: ()}, \n        types: {image: tf.uint8, label: tf.int64}>, \n     'train': <DatasetV1Adapter \n        shapes: {image: (32, 32, 3), label: ()}, \n        types: {image: tf.uint8, label: tf.int64}>\n    }\n```", "```py\ntrain_ds = data[\"train\"]\n```", "```py\ntrain_ds = data[\"train\"].batch(16)\n```", "```py\nfor item in train_ds:\n    print(item)\n    break\n```", "```py\n{\n    'id': <tf.Tensor: shape=(16,), dtype=string, numpy=\n          array(\n              [\n                  b'train_16399', b'train_01680', b'train_47917', b'train_17307',\n                  b'train_27051', b'train_48736', b'train_26263', b'train_01456',\n                  b'train_19135', b'train_31598', b'train_12970', b'train_04223',\n                  b'train_27152', b'train_49635', b'train_04093', b'train_17537'\n              ], \n              dtype=object\n          )>, \n    'image': <tf.Tensor: shape=(16, 32, 32, 3), dtype=uint8, numpy=\n          array(\n              [\n                  [\n                      [\n                          [143,  96,  70],\n                          [141,  96,  72],\n                          [135,  93,  72],\n                          ...,         \n                          [128,  93,  60],\n                          [129,  94,  61],\n                          [123,  91,  58]\n                      ]\n                  ]\n              ], \n              dtype=uint8\n          )>, \n    'label': <tf.Tensor: shape=(16,), dtype=int64, numpy=\n          array(\n              [7, 8, 4, 4, 6, 5, 2, 9, 6, 6, 9, 9, 3, 0, 8, 7], \n              dtype=int64\n          )>\n}\n```", "```py\ndef format_data(x):\n    return (x[\"image\"], tf.one_hot(x[\"label\"], depth=10))\n\ntrain_ds = train_ds.map(format_data)\n```", "```py\nmodel.fit(train_ds, epochs=25)\n```"]