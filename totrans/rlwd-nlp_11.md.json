["```py\nfairseq-train \\\n    data/mt-bin \\\n    --arch lstm \\\n    --share-decoder-input-output-embed \\\n    --optimizer adam \\\n    --lr 1.0e-3 \\\n    --max-tokens 4096 \\\n    --save-dir data/mt-ckpt\n```", "```py\n...\nLSTMModel(\n  (encoder): LSTMEncoder(\n    (embed_tokens): Embedding(16832, 512, padding_idx=1)\n    (lstm): LSTM(512, 512)\n  )\n  (decoder): LSTMDecoder(\n    (embed_tokens): Embedding(11416, 512, padding_idx=1)\n    (layers): ModuleList(\n      (0): LSTMCell(1024, 512)\n    )\n (attention): AttentionLayer(\n (input_proj): Linear(in_features=512, out_features=512, bias=False)\n (output_proj): Linear(in_features=1024, out_features=512, bias=False)\n )\n  )\n)\n...\n```", "```py\n$ fairseq-train \\\n      data/mt-bin \\\n      --arch lstm \\\n      --decoder-attention 0 \\\n      --share-decoder-input-output-embed \\\n      --optimizer adam \\\n      --lr 1.0e-3 \\\n      --max-tokens 4096 \\\n      --save-dir data/mt-ckpt-no-attn\n```", "```py\nLSTMModel(\n  (encoder): LSTMEncoder(\n    (embed_tokens): Embedding(16832, 512, padding_idx=1)\n    (lstm): LSTM(512, 512)\n  )\n  (decoder): LSTMDecoder(\n    (embed_tokens): Embedding(11416, 512, padding_idx=1)\n    (layers): ModuleList(\n      (0): LSTMCell(1024, 512)\n    )\n  )\n)\n```", "```py\n$ fairseq-interactive \\\n    data/mt-bin \\\n    --path data/mt-ckpt/checkpoint_best.pt \\\n    --beam 5 \\\n    --source-lang es \\\n    --target-lang en\n```", "```py\nfairseq-train \\\n  data/mt-bin \\\n  --arch transformer \\\n  --share-decoder-input-output-embed \\\n  --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n  --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n  --dropout 0.3 --weight-decay 0.0 \\\n  --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n  --max-tokens 4096 \\\n  --save-dir data/mt-ckpt-transformer\n```", "```py\nTransformerModel(\n  (encoder): TransformerEncoder(\n    (embed_tokens): Embedding(16832, 512, padding_idx=1)\n    (embed_positions): SinusoidalPositionalEmbedding()\n    (layers): ModuleList(\n      (0): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(                                   ❶\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_\n         affine=True)\n        (fc1): Linear(in_features=512, out_features=2048, bias=True)       ❷\n        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      ...\n      (5): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n  (decoder): TransformerDecoder(\n    (embed_tokens): Embedding(11416, 512, padding_idx=1)\n    (embed_positions): SinusoidalPositionalEmbedding()\n    (layers): ModuleList(\n      (0): TransformerDecoderLayer(\n        (self_attn): MultiheadAttention(                                   ❸\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (encoder_attn): MultiheadAttention(                                ❹\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_\n         affine=True)\n        (fc1): Linear(in_features=512, out_features=2048, bias=True)       ❺\n        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      ...\n      (5): TransformerDecoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (encoder_attn): MultiheadAttention(\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_\n         affine=True)\n        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n)\n```", "```py\n¡ Buenos días !\nS-0     ¡ Buenos días !\nH-0     -0.0753164291381836     Good morning !\nP-0     -0.0532 -0.0063 -0.1782 -0.0635\n¡ Hola !\nS-1     ¡ Hola !\nH-1     -0.17134985327720642    Hi !\nP-1     -0.2101 -0.2405 -0.0635\n¿ Dónde está el baño ?\nS-2     ¿ Dónde está el baño ?\nH-2     -0.2670585513114929     Where &apos;s the toilet ?\nP-2     -0.0163 -0.4116 -0.0853 -0.9763 -0.0530 -0.0598\n¿ Hay habitaciones libres ?\nS-3     ¿ Hay habitaciones libres ?\nH-3     -0.26301929354667664    Are there any rooms available ?\nP-3     -0.1617 -0.0503 -0.2078 -1.2516 -0.0567 -0.0532 -0.0598\n¿ Acepta tarjeta de crédito ?\nS-4     ¿ Acepta tarjeta de crédito ?\nH-4     -0.06886537373065948    Do you accept credit card ?\nP-4     -0.0140 -0.0560 -0.0107 -0.0224 -0.2592 -0.0606 -0.0594\nLa cuenta , por favor .\nS-5     La cuenta , por favor .\nH-5     -0.08584468066692352    The bill , please .\nP-5     -0.2542 -0.0057 -0.1013 -0.0335 -0.0617 -0.0587\nMaria no daba una bofetada a la bruja verde .\nS-6     Maria no daba una bofetada a la bruja verde .\nH-6     -0.3688890039920807     Mary didn &apos;t slapped the green witch .\nP-6     -0.2005 -0.5588 -0.0487 -2.0105 -0.2672 -0.0139 -0.0099 -0.1503 -0.0602\n```", "```py\ndef generate():\n    token = <START>\n    tokens = [<START>]\n    while token != <END>:\n        hidden = model(tokens)\n        probs = softmax(linear(hidden))\n        token = sample(probs)\n        tokens.append(token)\n    return tokens\n```", "```py\nimport torch\nfrom transformers import AutoModelWithLMHead, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('transfo-xl-wt103')\nmodel = AutoModelWithLMHead.from_pretrained('transfo-xl-wt103')\n```", "```py\ngenerated = tokenizer.encode(\"On our way to the beach\")\ncontext = torch.tensor([generated])\npast = None\n```", "```py\nfor i in range(100):\n    output = model(context, mems=past)\n    token = sample_token(output.prediction_scores)\n\n    generated.append(token.item())\n    context = token.view(1, -1)\n    past = output.mems\n```", "```py\nprint(tokenizer.decode(generated))\n```", "```py\nOn our way to the beach, she finds, she finds the men who are in the group to be \" in the group \". This has led to the perception that the \" group \" in the group is \" a group of people in the group with whom we share a deep friendship, and which is a common cause to the contrary. \" <eos> <eos> = = Background = = <eos> <eos> The origins of the concept of \" group \" were in early colonial years with the English Civil War. The term was coined by English abolitionist John\n```", "```py\ntokenizer = AutoTokenizer.from_pretrained('gpt2-large')\nmodel = AutoModelWithLMHead.from_pretrained('gpt2-large')\n```", "```py\ngenerated = tokenizer.encode(\"On our way to the beach\")\ncontext = torch.tensor([generated])\npast = None\n\nfor i in range(100):\n    output = model(context, past_key_values=past)\n    token = sample_token(output.logits)\n\n    generated.append(token.item())\n    context = token.unsqueeze(0)\n    past = output.past_key_values\n\nprint(tokenizer.decode(generated))\n```", "```py\nOn our way to the beach, there was a small island that we visited for the first time. The island was called 'A' and it is a place that was used by the French military during the Napoleonic wars and it is located in the south-central area of the island.\n\nA is an island of only a few hundred meters wide and has no other features to distinguish its nature. On the island there were numerous small beaches on which we could walk. The beach of 'A' was located in the...\n```", "```py\n'Real World Natural Language Processing' is the name of the book. It has all the tools you need to write and program natural language processing programs on your computer. It is an ideal introductory resource for anyone wanting to learn more about natural language processing. You can buy it as a paperback (US$12), as a PDF (US$15) or as an e-book (US$9.99).\n\nThe author's blog has more information and reviews.\n\nThe free 'Real World Natural Language Processing' ebook has all the necessary tools to get started with natural language processing. It includes a number of exercises to help you get your feet wet with writing and programming your own natural language processing programs, and it includes a few example programs. The book's author, Michael Karp has also written an online course about Natural Language Processing.\n\n'Real World Natural Language Processing: Practical Applications' is a free e-book that explains how to use natural language processing to solve problems of everyday life (such as writing an email, creating and\n```", "```py\ntokenizer = AutoTokenizer.from_pretrained('xlm-clm-enfr-1024')\nmodel = AutoModelWithLMHead.from_pretrained('xlm-clm-enfr-1024')\n```", "```py\ngenerated = [0] # start with just <s>\ncontext = torch.tensor([generated])\nlang = 0 # English\n\nfor i in range(100):\n    langs = torch.zeros_like(context).fill_(lang)\n    output = model(context, langs=langs)\n    token = sample_token(output)\n\n    generated.append(token.item())\n    context = torch.tensor([generated])\n\nprint(tokenizer.decode(generated))\n```", "```py\n<s>and its ability to make decisions on the basis of its own. \" </s>The government has taken no decisions on that matter, \" Mr Hockey said. </s>A lot of the information is very sensitive. </s>The new research and information on the Australian economy, which is what we're going to get from people, and the information that we are going to be looking at, we're going to be able to provide and we 'll take it forward. </s>I'm not trying to make sure we're not\n```", "```py\n<s></s>En revanche, les prix des maisons individuelles n' ont guère augmenté ( - 0,1 % ). </s>En mars dernier, le taux de la taxe foncière, en légère augmentation à la hausse par rapport à février 2008\\. </s>\" Je n' ai jamais eu une augmentation \" précise \". </s>\" Je me suis toujours dit que ce n' était pas parce que c' était une blague. </s>En effet, j' étais un gars de la rue \" </s>Les jeunes sont des gens qui avaient beaucoup d' humour... \"\n```", "```py\nfairseq-preprocess --source-lang fr --target-lang en \\\n    --trainpref data/gtc/train.tok \\\n    --validpref data/gtc/dev.tok \\\n    --destdir bin/gtc\n```", "```py\nfairseq-train \\\n    bin/gtc \\\n    --fp16 \\\n --arch transformer \\\n --encoder-layers 6 --decoder-layers 6 \\\n --encoder-embed-dim 1024 --decoder-embed-dim 1024 \\\n --encoder-ffn-embed-dim 4096 --decoder-ffn-embed-dim 4096 \\\n --encoder-attention-heads 16 --decoder-attention-heads 16 \\\n    --share-decoder-input-output-embed \\\n    --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-09 --clip-norm 25.0 \\\n    --lr 1e-4 --lr-scheduler inverse_sqrt --warmup-updates 16000 \\\n    --dropout 0.1 --attention-dropout 0.1 --activation-dropout 0.1 \\\n    --weight-decay 0.00025 \\\n    --criterion label_smoothed_cross_entropy --label-smoothing 0.2 \\\n --max-tokens 4096 \\\n    --save-dir models/gtc01 \\\n    --max-epoch 40\n```", "```py\necho \"tisimptant too spll chck ths dcment.\" \\\n    | python src/tokenize.py \\\n    | fairseq-interactive bin/gtc \\\n    --path models/gtc01/checkpoint_best.pt \\\n    --source-lang fr --target-lang en --beam 10 \\\n    | python src/format_fairseq_output.py\n```", "```py\ntisimplement too spll chck ths dcment.\n```", "```py\nfairseq-preprocess --source-lang en --target-lang fr \\\n    --trainpref data/gtc/train.tok \\\n    --validpref data/gtc/dev.tok \\\n    --destdir bin/gtc-en2fr\n```", "```py\n$ echo 'The quick brown fox jumps over the lazy dog.' | python src/tokenize.py \\ \n    | fairseq-interactive \\\n    bin/gtc-en2fr \\\n    --path models/gtc-en2fr/checkpoint_best.pt \\\n    --source-lang en --target-lang fr \\\n --beam 1 --sampling --sampling-topk 10 \\\n    | python src/format_fairseq_output.py\nThe quink brown fox jumps ove-rthe lazy dog.\n```", "```py\n$ echo \"tisimptant too spll chck ths dcment.\" \\\n    | python src/tokenize.py \\\n    | fairseq-interactive \\\n    bin/gtc-bt512-owt1k-upper \\\n    --path models/bt05/checkpoint_best.pt \\\n    --source-lang fr --target-lang en --beam 10 \\\n    | python src/format_fairseq_output.py\n    It's important to spell check this document.\n```", "```py\n$ echo \"The book wer about NLP.\" |\n    | python src/tokenize.py \\\n    | fairseq-interactive \\\n   ...\nThe book was about NLP.\n\n$ echo \"The books wer about NLP.\" |\n    | python src/tokenize.py \\\n    | fairseq-interactive \\\n   ...\nThe books were about NLP.\n```", "```py\n$ echo \"The book Tom and Jerry put on the yellow desk yesterday wer about NLP.\" |\n    | python src/tokenize.py \\\n    | fairseq-interactive \\\n   ...\nThe book Tom and Jerry put on the yellow desk yesterday was about NLP.\n\n$ echo \"The books Tom and Jerry put on the yellow desk yesterday wer about NLP.\" |\n    | python src/tokenize.py \\\n    | fairseq-interactive \\\n   ...\nThe books Tom and Jerry put on the yellow desk yesterday were about NLP.\n```"]