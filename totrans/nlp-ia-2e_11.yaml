- en: 11 Information extraction and knowledge graphs (grounding)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11 信息提取和知识图谱（基础）
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Extracting named entities from text
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从文本中提取命名实体
- en: Understanding the structure of the sentence using dependency parsing
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用依存解析理解句子的结构
- en: Converting a dependency tree into a knowledge (fact)
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将依存树转换为知识（事实）
- en: Building a knowledge graph from text
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从文本构建知识图谱
- en: In the previous chapter (Chapter 10) you learned how to use large transformers
    to generate smart *sounding* words. But language models on their own are just
    faking it by predicting the next word that will *sound* reasonable to you. Your
    AI can’t reason about the real world until you give them access to facts and knowledge
    about the world. In Chapter 2 you learned how to do exactly this, but you didn’t
    know it then. You were able to tag tokens with their part of speech and their
    logical role in the meaning of a sentence (dependency tree). This old-fashioned
    token tagging algorithm is all you need to give your generative language models
    (AI) knowledge about the real world. The goal of this chapter is to teach your
    bot to understand what it reads. And you’ll put that understanding into a flexible
    data structure designed to store knowledge, known as *knowledge graph*. Then your
    bot can use that knowledge to make decisions and say smart stuff about the world.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章（第10章）中，你学会了如何使用大型变压器生成聪明的*听起来像*单词。但单独的语言模型只是通过预测下一个对你来说*听起来*合理的单词来进行欺骗。直到你为它们提供有关世界的事实和知识之前，你的AI才能推理真实世界。在第2章中，你学会了如何做到这一点，但当时你并不知道。你能够标记代币及其在句子意义中的逻辑角色（依存树）。这种老式的代币标记算法是为了给你的生成式语言模型（AI）提供关于真实世界的知识。本章的目标是教你的机器人理解它所读的内容。然后，你将理解放入一个旨在存储知识的灵活数据结构中，称为*知识图谱*。然后，你的机器人可以利用这些知识做出决策，并就世界发表聪明的言论。
- en: Correctly parsing your text into *entities* and discovering the *relations*
    between them is how you’ll go about extracting facts from the text. A *knowledge
    graph*, also called *knowledge database* (knowledge base) or a *semantic net*,
    is a database that stores knowledge as relationships between concepts. Though
    you can use a relational database to store the relations and concepts, sometimes
    it is more appropriate to use a *graph* data structure. The nodes in the graph
    would be *entities*, while the edges would be relations between these entities.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 正确地将文本解析为*实体*，并发现它们之间的*关系*，这是你从文本中提取事实的方法。 *知识图谱*，也称为*知识数据库*（知识库）或*语义网络*，是一个将知识存储为概念之间关系的数据库。虽然你可以使用关系型数据库存储关系和概念，但有时使用*图*数据结构更为合适。图中的节点将是*实体*，而边将是这些实体之间的关系。
- en: You can see an example of a knowledge graph in Figure [11.1](#figure-knowledge-graph).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在图 [11.1](#figure-knowledge-graph) 中看到一个知识图谱示例。
- en: Figure 11.1 An example of a knowledge graph
  id: totrans-9
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.1 知识图谱示例
- en: '![kg 150 biotech company graphviz](images/kg_150_biotech_company_graphviz.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![kg 150 生物技术公司 Graphviz](images/kg_150_biotech_company_graphviz.png)'
- en: Each fact you extract will create a new connection between the nodes of the
    graph - or possibly, create new nodes. This allows you to ask questions about
    the relationships between things using a query language such as GraphQL, Cypher
    or even SQL.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 每个你提取的事实都会在图的节点之间创建一个新的连接 - 或者，可能会创建新的节点。这使得你可以使用诸如GraphQL、Cypher甚至SQL的查询语言来询问关系。
- en: And your algorithms can then do fact-checking, not only on the text written
    by humans but also text generated by your NLP pipeline or AI. Finally, your AI
    algorithms will be able to do introspection to let you know if what they are telling
    you might actually have some semblance of truth to it.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你的算法可以对文本进行事实检查，不仅限于人类编写的文本，还包括你的NLP管道或AI生成的文本。最后，你的AI算法将能够自省，让你知道它们告诉你的内容是否实际上有些真实的相似之处。
- en: Your AI can use knowledge graphs to fill the *common sense knowledge* gap in
    large language models and perhaps live up to a little bit of the hype around LLMs
    and AI. This is the missing link in the NLP chain that you need to create true
    AI. And you can use a knowledge graph to programmatically generate text that makes
    sense because it is grounded in facts in your database. You can even infer new
    facts or *logical inferences* about the world that aren’t yet included in your
    knowledge base.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你的 AI 可以使用知识图谱来填补大型语言模型中的 *常识知识* 差距，也许有点符合关于 LLMs 和 AI 的炒作。这是你需要创建真正 AI 所需的
    NLP 链中缺失的环节。你可以使用知识图谱来以编程方式生成合理的文本，因为它基于你的数据库中的事实。你甚至可以推断出关于世界的新事实或 *逻辑推理*，这些事实尚未包含在你的知识库中。
- en: You may remember hearing about "inference" when people talk about forward propagation
    or prediction using deep learning models. A deep learning language model uses
    statistics to estimate or guess the next word in the text that you prompt it with.
    And deep learning researchers hope that one day, neural networks will be able
    to match the natural human ability to logically infer things and reason about
    the world. But this isn’t possible, because words don’t contain all the knowledge
    about the world that a machine would need to process to make factually correct
    inferences. So you’re going to use a tried and true logical inference approach
    called "symbolic reasoning."
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 当人们谈论前向传播或使用深度学习模型进行预测时，你可能会听说过“推理”的概念。深度学习语言模型利用统计学来估计或猜测你输入的文本中的下一个词。深度学习研究人员希望有一天，神经网络能够达到与自然人类推理和思考世界的能力相匹配。但这是不可能的，因为单词并不包含机器需要处理的关于世界的全部知识，以做出事实正确的推断。因此，你将使用一种经过验证和可靠的逻辑推理方法，称为“符号推理”。
- en: If you’re familiar with the concept of a compiler then you may want to think
    of the dependency tree as a parse tree or abstract syntax tree (AST). An AST defines
    the logic of a machine language expression or program. You’re going to use the
    natural language dependency tree to extract the logical relations within natural
    language text. And this logic will help you *ground* the statistical deep learning
    models so they can do more than merely make statistical "guesses" about the world
    as they did in previous chapters.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你熟悉编译器的概念，那么你可能会想到依赖树作为解析树或抽象语法树（AST）。AST 定义了机器语言表达式或程序的逻辑。你将使用自然语言依赖树来提取自然语言文本中的逻辑关系。这种逻辑将帮助你
    *基于事实* 地对统计深度学习模型进行推理，使它们能够做更多不仅仅是像之前章节中那样做统计学上的“猜测”世界的工作。
- en: 11.1 Grounding
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.1 推理
- en: Once you have a knowledge graph, your chatbots and AI agents will have a way
    to correctly reason about the world in an explainable way. And if you can extract
    facts from the text your deep learning model generates, you can check to see if
    that text agrees with the knowledge you’ve collected in your knowledge graph.
    This is called *grounding* when you maintain a knowledge graph and then use it
    to double-check the facts and reasoning in the generated text. When you ground
    your language model you attach it to some ground truth facts about the world.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了一个知识图谱，你的聊天机器人和 AI 代理就会有一种以可解释的方式正确推理世界的方法。如果你能从你的深度学习模型生成的文本中提取事实，你可以检查该文本是否与你在知识图谱中收集的知识一致。当你维护一个知识图谱并使用它来对生成的文本中的事实和推理进行双重检查时，这被称为
    *推理*。当你将语言模型与关于世界的一些基本事实联系起来时，你就在为其打下基础。
- en: Grounding can also benefit your NLP pipeline in other ways. Using a knowledge
    graph for the reasoning part of your algorithm can free up your language model
    to do what it does best — generate plausible, grammatical text. So you can fine
    tune your language model to have the tone that you want, without trying to build
    a chameleon that pretends to understand and reason about the world. And your knowledge
    graph can be designed to contain just the facts about a world that you want your
    AI to understand — whether it is facts about the real world that you have in mind
    or some fictional world that you are creating. By separating the reasoning from
    the language you can create an NLP pipeline that both sounds correct and *is*
    correct.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 推理也可以以其他方式使你的 NLP 流程受益。在算法的推理部分使用知识图谱可以释放你的语言模型，让它做它最擅长的事情——生成合理的、符合语法的文本。因此，你可以微调你的语言模型，让它具有你想要的语气，而不是试图构建一个假装理解和推理世界的变色龙。你的知识图谱可以被设计成只包含你希望你的
    AI 理解的世界的事实——无论是你心目中的真实世界的事实还是你正在创建的某个虚构世界的事实。通过将推理与语言分离，你可以创建一个既听起来正确又 *是* 正确的
    NLP 流程。
- en: There are a few other terms that are often used when referring to this grounding
    process. Sometimes it’s referred to as *symbolic reasoning* as opposed to the
    probabilistic reasoning of machine learning models. *First order logic* is one
    system for symbolic reasoning.^([[1](#_footnotedef_1 "View footnote.")]) This
    was the preferred approach to building expert systems and theorem provers before
    the data and processing power was available for machine learning and deep learning.
    It’s also called Good Old Fashioned AI or GOFAI, pronounced "Go Fie". GOFAI is
    back in fashion as researchers attempt to build generally intelligent systems
    that we can rely on to make important decisions.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在提及这个接地过程时，还有一些其他术语经常被使用。有时它被称为*符号推理*，与机器学习模型的概率推理相对。*一阶逻辑*是符号推理的一种系统。这是在数据和处理能力不足以支持机器学习和深度学习之前构建专家系统和定理证明器的首选方法。它也被称为Good
    Old Fashioned AI或GOFAI，发音为“高菲”。随着研究人员试图构建我们可以依赖于做出重要决策的普遍智能系统，GOFAI重新流行起来。
- en: Another advantage of grounding your NLP pipeline is that you can use the facts
    in your knowledge base to *explain* its reasoning. If you ask an ungrounded LLM
    to explain why it said something unreasonable, it will just keep digging a hole
    for itself (and you) by making up more and more nonsense reasons. You saw this
    in the previous chapters when LLMs confidently hallucinated (fabricated) nonexistent
    but plausible references and fictional people to explain where they got their
    nonsense from. The key to creating AI you can trust is to put a floor of reason
    underneath it using a knowledge graph. The first, and perhaps most important algorithm
    in this grounding process is *knowledge extraction*.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 将您的自然语言处理（NLP）流水线接地的另一个优势是，您可以使用知识库中的事实来*解释*其推理过程。如果您要求一个未接地的语言模型解释为什么会说出不合理的话，它只会继续为自己（和您）挖一个越来越深的坑，通过编造越来越多的无稽理由。在前几章中，您已经看到了这一点，当语言模型自信地产生（虚构）不存在但合理的引用和虚构人物来解释他们的胡言乱语的来源时。创造一个您可以信任的人工智能的关键是在其下放置一个理性的地板，使用知识图谱。这个接地过程中的第一个，也许是最重要的算法是*知识提取*。
- en: '11.1.1 Going old-fashioned: information extraction with patterns'
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1.1 传统方法：模式匹配的信息提取
- en: In this chapter, we’ll also get back to methods you see in the very early chapters,
    like regular expressions. Why return to hard-coded (manually composed) regular
    expressions and patterns? Because your statistical or data-driven approach to
    NLP has limits. You want your machine learning pipeline to be able to do some
    basic things, such as answer logical questions or perform actions such as scheduling
    meetings based on NLP instructions. And machine learning falls flat here.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们还将回顾您在前几章中看到的方法，比如正则表达式。为什么要回到硬编码（手动组成的）正则表达式和模式？因为您对自然语言处理的统计或数据驱动方法有限制。您希望您的机器学习流水线能够做一些基本的事情，比如回答逻辑问题或执行根据自然语言指令安排会议。而机器学习在这方面效果不佳。
- en: Plus, as you’ll see here, you can define a compact set of condition checks (a
    regular expression) to extract key bits of information from a natural language
    string. And it can work for a broad range of problems.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，正如您在这里所看到的，您可以定义一组紧凑的条件检查（正则表达式），以从自然语言字符串中提取关键信息。它可以适用于广泛的问题。
- en: Pattern matching (and regular expressions) continues to be the state-of-the-art
    approach for information extraction and related tasks.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 模式匹配（以及正则表达式）仍然是信息提取及相关任务的最先进方法。
- en: Enough of a preamble. Let’s start the journey of knowledge extraction and grounding!
    But we have to cover an important step in processing your documents, to generate
    a proper input to your knowledge extraction pipeline. We need to break our text
    into smaller units.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 先说正事吧。让我们开始知识提取和接地之旅吧！但在处理您的文档之前，我们必须覆盖一个重要步骤，以生成适当的输入到您的知识提取流水线。我们需要将文本分解成较小的单元。
- en: '11.2 First things first: segmenting your text into sentences'
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.2 先解决重要问题：将您的文本分割成句子
- en: Before you can dive into extracting your knowledge from raw text, you need to
    break it down into chunks that your pipeline can work on. Document "chunking"
    is useful for creating semi-structured data about documents that can make it easier
    to search, filter, and sort documents for information retrieval. And for information
    extraction, if you’re extracting relations to build a knowledge base such as NELL
    or Freebase (more about them in a bit), you need to break it into parts that are
    likely to contain a fact or two. When you divide natural language text into meaningful
    pieces, it’s called *segmentation*. The resulting segments can be phrases, sentences,
    quotes, paragraphs, or even entire sections of a long document.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在您开始从原始文本中提取知识之前，您需要将其分解为管道可以处理的块。文档“分块”对于创建关于文档的半结构化数据非常有用，这样可以更容易地搜索、过滤和对信息检索的文档进行排序。而对于信息提取，如果您正在提取关系以构建知识库，如
    NELL 或 Freebase（稍后将更详细介绍），则需要将其分成可能包含一个或两个事实的部分。当您将自然语言文本分解为有意义的片段时，这称为*分割*。生成的片段可以是短语、句子、引用、段落，甚至是长文档的整个部分。
- en: Sentences are the most common chunk for most information extraction problems.
    Sentences are usually punctuated with one of a few symbols (".", "?", "!", or
    a new line). And grammatically correct English language sentences must contain
    a subject (noun) and a verb, which means they’ll usually have at least one fact
    worth extracting. Sentences are often self-contained packets of meaning that don’t
    rely too much on preceding text to convey most of their information.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数信息提取问题来说，句子是最常见的块。句子通常用几个符号（“。”、“？”、“！”或换行符）标点。而语法上正确的英语句子必须包含一个主语（名词）和一个动词，这意味着它们通常至少有一个值得提取的事实。句子通常是自包含的意义包，大部分信息不太依赖于前文来传达。
- en: In addition to facilitating information extraction, you can flag some of those
    statements and sentences as being part of a dialog or being suitable for replies
    in a dialog. Using a sentence segmenter (sentencizer) allows you to train your
    chatbot on longer texts, such as books. Choosing those books appropriately gives
    your chatbot a more literary, intelligent style than if you trained it purely
    on Twitter streams or IRC chats. And these books give your chatbot access to a
    much broader set of training documents to build its common sense knowledge about
    the world.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 除了促进信息提取外，您还可以将其中一些陈述和句子标记为对话的一部分或适合对话中的回复。使用句子分段器（sentencizer）可以让您在更长的文本（如书籍）上训练您的聊天机器人。选择这些书籍可以使您的聊天机器人比纯粹在
    Twitter 流或 IRC 聊天上训练它更具文学性和智能风格。而且这些书籍为您的聊天机器人提供了一个更广泛的训练文档集，以建立关于世界的常识知识。
- en: Sentence segmentation is the first step in your information extraction pipeline.
    It helps isolate facts from each other. Most sentences express a single coherent
    thought. And many of those thoughts are about real things in the real world. And,
    most importantly, all the natural languages have sentences or logically cohesive
    sections of text of some sort. And all languages have a widely shared process
    for generating them (a set of grammar "rules" or habits).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 句子分割是信息提取管道的第一步。它有助于将事实与其他事实隔离开来。大多数句子表达一个单一的连贯思想。而且许多这些思想都是关于现实世界中的真实事物。最重要的是，所有的自然语言都有句子或逻辑上连贯的文本部分。并且所有语言都有一个广泛共享的生成它们的过程（一组语法“规则”或习惯）。
- en: But segmenting text and identifying sentence boundaries is a bit trickier than
    you might think. In English, for example, no single punctuation mark or sequence
    of characters always marks the end of a sentence.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，分割文本并识别句子边界比你想象的要棘手。例如，在英语中，没有单个标点符号或字符序列总是标记句子的结束。
- en: 11.2.1 Why won’t split('.!?') work?
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.1 为什么 split('.!?') 不能起作用？
- en: 'Even a human reader might have trouble finding an appropriate sentence boundary
    within each of the following quotes. Here are some example sentences that most
    humans would be tempted to split into multiple sentences:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 甚至即使是人类读者在以下每个引号内找到适当的句子边界也可能有困难。以下是大多数人类可能会尝试拆分为多个句子的一些示例句子：
- en: '*She yelled "It’s right here!" but I kept looking for a sentence boundary anyway.*'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*她喊道：“就在这里！”但我仍然在寻找一个句子的边界。*'
- en: '*I stared dumbfounded on, as things like "How did I get here?", "Where am I?",
    "Am I alive?" flittered across the screen.*'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*我目瞪口呆地盯着，像“我是怎么到这里的？”、“我在哪里？”、“我还活着吗？”在屏幕上飞来飞去。*'
- en: '*The author wrote "''I don’t think it’s conscious.'' Turing said."*'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*作者写道：“''我不认为它是有意识的。'' 图灵说。”*'
- en: Even a human reader would have trouble finding an appropriate sentence boundary
    within each of these quotes and nested quotes and stories within stories.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: More sentence segmentation "edge cases" such as this are available at tm-town.com.
    ^([[2](#_footnotedef_2 "View footnote.")])
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Technical text is particularly difficult to segment into sentences because engineers,
    scientists, and mathematicians tend to use periods and exclamation points to signify
    a lot of things besides the end of a sentence. When we tried to find the sentence
    boundaries in this book, we had to manually correct several of the extracted sentences.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: 'If only we wrote English like telegrams, with a "STOP" or unique punctuation
    mark at the end of each sentence. But since we don’t, you’ll need some more sophisticated
    NLP than just `split(''.!?'')`. Hopefully, you’re already imagining a solution
    in your head. If so, it’s probably based on one of the two approaches to NLP you’ve
    used throughout this book:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Manually programmed algorithms (regular expressions and pattern-matching)
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statistical models (data-based models or machine learning)
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use the sentence segmentation problem to revisit these two approaches by
    showing you how to use regular expressions as well as more advanced methods to
    find sentence boundaries. And you’ll use the text of this book as a training and
    test set to show you some of the challenges. Fortunately, you haven’t inserted
    any newlines within sentences, to manually "wrap" text like in newspaper column
    layouts. Otherwise, the problem would be even more difficult. In fact, much of
    the source text for this book, in ASCIIdoc format, has been written with "old-school"
    sentence separators (two spaces after the end of every sentence), or with each
    sentence on a separate line. This was so we could use this book as a training
    and test set for your segmenters.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.2 Sentence segmentation with regular expressions
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Regular expressions are just a shorthand way of expressing the tree of “if…​then”
    rules (regular grammar rules) for finding character patterns in strings of characters.
    As we mentioned in Chapters 1 and 2, regular expressions (regular grammars) are
    a particularly succinct way to specify the structure of a finite state machine.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 'Any formal grammar can be used by a machine in two ways:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: To recognize "matches" to that grammar
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To generate a new sequence of symbols
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Not only can you use patterns (regular expressions) for extracting information
    from natural language, but you can also use them to generate strings that match
    that pattern! Check out the `rstr` (short for "random string") package if you
    ever need to generate example strings that match a regular expresssion.^([[3](#_footnotedef_3
    "View footnote.")]) for some of your information extraction patterns here.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: This formal grammar and finite state machine approach to pattern matching has
    some other awesome features. A true finite state machine is guaranteed to eventually
    stop (halt) in a finite number of steps. So if you use a regular expression as
    your pattern matcher you know that you will always receive an answer to your question
    about whether you’ve found a match in your string or not. It will never get caught
    in a perpetual loop…​ as long as you don’t "cheat" and use look-aheads or look-backs
    in your regular expressions. And because a regular expression is deterministic
    it always returns a match or non-match. It will never give you less than 100%
    confidence or probability of there being a match.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这种形式化的语法和有限状态机方法对模式匹配还有其他一些令人惊叹的特点。真正的有限状态机保证在有限步骤内最终停止（停机）。所以如果你使用正则表达式作为你的模式匹配器，你知道你总是会得到关于你是否在你的字符串中找到了一个匹配的答案。它永远不会陷入永久循环……只要你不
    "作弊" 并在你的正则表达式中使用向前看或向后看。而且因为正则表达式是确定性的，它总是返回匹配或非匹配。它永远不会给你不到 100% 的置信度或匹配的概率。
- en: So you’ll stick to regular expressions that don’t require these "look-back"
    or "look-ahead" cheats. You’ll make sure your regular expression matcher processes
    each character and moves ahead to the next character only if it matches — sort
    of like a strict train conductor walking through the seats checking tickets. If
    you don’t have one, the conductor stops and declares that there’s a problem, a
    mismatch, and he refuses to go on, or look ahead or behind you until he resolves
    the problem. There are no "go-backs" or "do-overs" for train passengers, or for
    strict regular expressions.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你会坚持使用不需要这些 "向后看" 或 "向前看" 的正则表达式。你会确保你的正则表达式匹配器处理每个字符，并且仅在它匹配时向前移动到下一个字符 -
    就像一个严格的列车售票员走过座位检查票一样。如果没有，售票员会停下来宣布有问题，不匹配，并且他拒绝继续前进，或者向你前后看直到他解决问题。对于列车乘客或严格的正则表达式来说，没有
    "回头" 或 "重做"。
- en: 'Our regex or FSM has only one purpose in this case: identifying sentence boundaries.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的正则表达式或有限状态机在这种情况下只有一个目的：识别句子边界。
- en: If you do a web search for sentence segmenters,^([[4](#_footnotedef_4 "View
    footnote.")]) you’re likely to be pointed to various regular expressions intended
    to capture the most common sentence boundaries. Here are some of them, combined
    and enhanced to give you a fast, general-purpose sentence segmenter.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你搜索句子分割器，^([[4](#_footnotedef_4 "View footnote.")]) 你可能会被指向旨在捕捉最常见句子边界的各种正则表达式。以下是一些结合和增强以给你一个快速、通用的句子分割器的正则表达式。
- en: The following regex would work with a few "normal" sentences.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 以下正则表达式将适用于一些 "正常" 句子。
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Unfortunately, this `re.split` approach gobbles up (consumes) the sentence-terminating
    token. Notice how the ellipsis and period at the end of "Hello World" are missing
    in the returned list? The splitter only returns the sentence terminator if it
    is the last character in a document or string. A regular expression that assumes
    your sentences will end in white space does do a good job of ignoring the trickery
    of periods within doubly-nested quotes, though:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这种 `re.split` 方法会吞噬掉句子终止符。注意一下 "Hello World" 结尾的省略号和句号在返回列表中消失了吗？分割器只在它是文档或字符串中的最后一个字符时才返回句子终止符。一个假设你的句子将以空白结束的正则表达式确实可以很好地忽略双重嵌套引号中的句号的伎俩，但：
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: See how the returned list contains only one sentence without messing up the
    quote within a quote? Unfortunately, this regex pattern also ignores periods in
    quotes that terminate an actual sentence, so any sentences that end in a quote
    will be joined with the subsequent sentence. This may reduce the accuracy of the
    information extraction steps that follow your sentence segmenter if they rely
    on accurate sentence splits.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 看到返回的列表只包含一个句子而不会弄乱引号内的引用吗？不幸的是，这个正则表达式模式也会忽略引号中终止实际句子的句号，因此任何以引号结尾的句子都将与随后的句子连接起来。如果后续的信息提取步骤依赖于准确的句子分割，这可能会降低其准确性。
- en: 'What about text messages and tweets with abbreviated text, informal punctuation,
    and emojis? Hurried humans squish sentences together, leaving no space surrounding
    periods. The following regex could deal with periods in SMS messages that have
    letters on either side and it would safely skip over numerical values:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 那么文本消息和带有缩写文本、非正式标点和表情符号的 tweets 呢？匆忙的人类会将句子挤在一起，句号周围没有空格。以下正则表达式可以处理具有字母的短信消息中的句号，并且它会安全地跳过数值：
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Even combining these two regexes into a monstrosity such as `r'?<!\d)\.|\.(?!\d|([!.?]+)[\s$]+'`
    is not enough to get all the sentences right. If you parsed the AciiDoc text for
    the manuscript of this chapter, it would make several mistakes.^([[5](#_footnotedef_5
    "View footnote.")]) You’d have to add a lot more "look-ahead" and "look-back"
    to the regex pattern to improve its accuracy as a sentence segmenter. You were
    warned!
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 即使将这两个正则表达式组合成一个像`r'?<!\d)\.|\.(?!\d|([!.?]+)[\s$]+'`这样的怪物也不足以让所有的句子都正确。如果你解析了本章手稿的AciiDoc文本，它会犯几个错误。你需要在正则表达式模式中添加更多的“向前查找”和“向后查找”来提高其作为句子分割器的准确性。你已经被警告了！
- en: If looking for all the edge cases and designing rules around them feel cumbersome,
    that’s because it is. A better approach for sentence segmentation is to use a
    machine learning algorithm trained on a labeled set of sentences. Often a logistic
    regression or a single-layer neural network (perceptron) is enough.^([[6](#_footnotedef_6
    "View footnote.")]) Several packages contain such a statistical model you can
    use to improve your sentence segmenter. SpaCy ^([[7](#_footnotedef_7 "View footnote.")])
    and Punkt (in NLTK) ^([[8](#_footnotedef_8 "View footnote.")]) both have good
    sentence segmenters. You can guess which one we use.^([[9](#_footnotedef_9 "View
    footnote.")])
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果查找所有边缘情况并为其设计规则感觉繁琐，那是因为确实如此。句子分割的更好方法是使用在标记句子集上训练过的机器学习算法。通常情况下，逻辑回归或单层神经网络（感知器）就足够了。有几个包含这样的统计模型，你可以用来改进你的句子分割器。SpaCy和Punkt（在NLTK中）都有很好的句子分割器。你可以猜猜我们使用哪一个。
- en: 'SpaCy has a sentence segmenter built into the default parser pipeline that
    is your best bet for mission-critical applications. It is almost always the most
    accurate, robust, performant option. Here is how you segment text into sentences
    with spaCy:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: SpaCy在默认解析器管道中内置了一个句子分割器，这是你在关键任务应用中的最佳选择。它几乎总是最准确、最健壮、性能最好的选择。以下是如何使用spaCy将文本分割成句子：
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: SpaCy’s accuracy relies on dependency parsing. A dependency parser identifies
    how each word depends on the other words in a sentence diagram, like the one you
    learned about in grammar school (elementary school). Having this dependency structure
    along with the token embeddings helps the spacy sentence segmenter deal with ambiguous
    punctuation and capitalization accurately. But all that sophistication takes processing
    power and time. Speed is not important when you are only processing a few sentences,
    but what if you wanted to parse the AsciiDoc manuscript for Chapter 9 of this
    book?
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: SpaCy的准确性依赖于依赖解析。依赖解析器识别每个词在句子图中如何依赖于其他词，就像你在文法学校（小学）学到的那样。拥有这种依赖结构以及令牌嵌入帮助SpaCy句子分割器准确处理模糊的标点和大写字母。但所有这些复杂性都需要处理能力和时间。当你只处理几个句子时，速度并不重要，但如果你想要解析本书第9章的AsciiDoc手稿呢？
- en: '[PRE4]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Wow, that *is* slow! SpaCy is about 700 times slower than a regular expression.
    If you have millions of documents instead of just this one chapter of text, then
    you will probably need to do something different. For example, on a medical records
    parsing project we needed to switch to a regular expression tokenizer and sentence
    segmenter. The regex parser reduced our processing time from weeks to days, but
    it also reduced the accuracy of the rest of our NLP pipeline.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 哇，这真是太慢了！SpaCy比正则表达式慢了大约700倍。如果你有数百万个文档而不只是这一章的文本，那么你可能需要做一些不同的事情。例如，在一个医疗记录解析项目中，我们需要切换到正则表达式标记器和句子分割器。正则表达式解析器将我们的处理时间从几周缩短到几天，但也降低了我们NLP管道的准确性。
- en: SpaCy has now (as of 2023) caught up with our need for customization. SpaCy
    now allows you to enable or disable any piece of the pipeline you like. And it
    has a statistical sentence segmenter that doesn’t rely on the other elements of
    the spaCy pipeline such as the word embeddings and named entity recognizer. When
    you want to speed up your spaCy NLP pipeline you can remove all the elements you
    do not need and add back just the pipeline elements you want.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: SpaCy现在（截至2023年）已经满足了我们对定制化的需求。SpaCy现在允许你启用或禁用任何你喜欢的管道部分。它还有一个统计句子分段器，不依赖于SpaCy管道的其他元素，比如词嵌入和命名实体识别器。当你想加速你的SpaCy
    NLP管道时，你可以移除所有你不需要的元素，然后只添加你想要的管道元素回来。
- en: First, check out the pipeline attribute of a spacy NLP pipeline to see what
    is there by default. Then use the `exclude` keyword argument to `load` clean out
    the pipeline.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，检查spacy NLP管道的`pipeline`属性，查看默认值中包含什么。然后使用`exclude`关键字参数来`load`清理管道。
- en: '[PRE5]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now that you’ve cleaned your pipes, you can add back the important pieces that
    you need. For this speed run through Chapter 9, your NLP pipeline will only need
    the `senter` pipeline element. The `senter` pipeline is the statistical sentence
    segmenter.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您已经清理了管道，现在可以添加回所需的重要部分。在本章的快速运行中，您的NLP管道只需要`senter`管道元素。`senter`管道是统计句子分割器。
- en: '[PRE6]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: That is a significant time saver — 2.3 vs 11.5 seconds on an 8-core i7 laptop.
    The statistical sentence segmenter is about 5x faster than the full spaCy pipeline.
    The regular expression approach will still be much faster, but the statistical
    sentence segmenter will be more accurate. You can estimate the accuracy of these
    two algorithms by comparing the lists of sentences to see if they produced the
    same splits. This will not tell you which of the two approaches is correctly segmenting
    a particular text line, but at least you will see when the two spaCy pipelines
    agree.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个重要的时间节省器-在8核i7笔记本电脑上为2.3秒与11.5秒。统计句子分割器比完整的spaCy管道快约5倍。正则表达式方法仍将快得多，但统计句子分割器将更准确。您可以通过比较句子列表估算这两种算法的准确性，以查看它们是否产生了相同的分割。这不会告诉你哪种方法正在正确地分段特定的文本行，但至少你会看到两个spaCy管道什么时候达成一致。
- en: '[PRE7]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: So it appears that about 93% of the sentences of this book were segmented the
    same way with the slow and fast pipelines. Look at some example segmentations
    to see which one might be better for your use cases.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，该书约93%的句子通过慢速和快速管道进行分段。请查看一些示例分段，以确定哪种方法适合您的用例。
- en: '[PRE8]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: It looks like that opening sentence with the leading underscore character (\_)
    is bit more difficult for the faster statistical segmenter. So you probably want
    to use the full spacy model whenever you are parsing Markdown or AsciiDoc text
    files. The formatting characters will confuse a statistic segmenter if it has
    not been trained on similar text.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来以前导下划线字符（_）开头的句子对于更快的统计分割器要困难一些。因此，您在解析Markdown或AsciiDoc文本文件时可能需要使用完整的spacy模型。如果统计分割器没有接受过类似文本的训练，那么格式字符会使它混淆。
- en: 11.2.3 Sentence semantics
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.3句子语义
- en: Now that you have your text segmented into sentences containing discrete facts,
    you are ready to start extracting those facts and giving them structure in a knowledge
    graph. To get started, create a heatmap of the BERT embeddings of all the sentences
    of chapter 9.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您的文本已被分割成包含离散事实的句子，您已经准备好开始在知识图谱中将这些事实提取出来并给它们构建结构。要开始，创建第九章所有句子的BERT嵌入热力图。
- en: '[PRE9]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Take a look at this DataFrame. It has columns that contain tags for each line
    of text. You can use the tags to filter out the lines that you don’t want to process.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 看看这个DataFrame，它具有包含每行文本标签的列。您可以使用这些标签来过滤掉不想处理的行。
- en: '[PRE10]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now you can use the 'is_body' tag to process all the sentences within the body
    of the manuscript. These lines should contain mostly complete sentences so that
    you can compare them semantically to each other to see a heatmap of how often
    we say similar things. Now that you understand transformers such as BERT, you
    can use it to give you even more meaningful representations of this text than
    what SpaCy creates.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以使用'is_body'标记来处理手稿正文内的所有句子。这些行应该包含大部分完整的句子，以便您可以语义地将它们与其他语句进行比较，以查看我们有多经常说类似的话的热力图。现在您已经了解了像BERT这样的转换器，可以使用它来为您提供比SpaCy创建的更有意义的文本表示。
- en: '[PRE11]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The MiniLM model is a multipurpose BERT transformer that has been optimized
    and "distilled." It provides high accuracy and speed and should not take long
    to download from Hugging Face. Now you have 689 passages of text (mostly individual
    sentences). The MiniLM language model has embedded them into a 384-dimensional
    vector space. As you learned in Chapter 6, embedding vector semantic similarity
    is computed with the normalized dot product.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: MiniLM模型是一个经过优化和“蒸馏”的通用BERT转换器。它提供高精度和速度，并且从Hugging Face下载不需要很长时间。现在，您有689个文本段落（主要是单个句子）。MiniLM语言模型已将它们嵌入到384维向量空间中。正如您在第6章中了解的那样，嵌入向量语义相似度计算使用归一化点积。
- en: '[PRE12]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now you have a square matrix, one row and one column for each passage of text
    and its BERT embedding vector. And the value in each cell of the matrix contains
    the cosine similarity between that pair of embedding vectors. If you label the
    columns and rows with the first few characters of the text passages, that will
    make it easier to interpret all this data with a heatmap.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As usual, the cosine similarity ranges between zero and one and most values
    are less than .85 (85%) unless they are for sentences that say essentially the
    same thing. So 85% would be a good threshold for identifying redundant statements
    that might be consolidated or reworded to improve the quality of the writing in
    a book such as this. Here’s what the heatmap of these cosine similarity values
    looks like.^([[10](#_footnotedef_10 "View footnote.")])
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![ch9 heatmap](images/ch9-heatmap.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
- en: 'There seems to be only a small square of white-hot similarity about 60% of
    the way through Chapter 9, perhaps near the line that begins "Epoch: 13…​". This
    line corresponds to the output text from a transformer training run, so it is
    not surprising that a natural language model would see these machine-generated
    lines as semantically similar. After all, the BERT language model is just saying
    to you "It’s all just Greek to me." The regular expressions in the scripts for
    tagging lines of the manuscript as natural language or software blocks are not
    working very well.^([[11](#_footnotedef_11 "View footnote.")]) If you improved
    the regular expressions in `nlpia2.text_processing.extractors` you could have
    your heatmap skip over these irrelevant code lines. And AsciiDoc files are structured
    data, so they should be machine-readable without any regular expression guesswork…​
    if only there were an up-to-date Python library for parsing AsciiDoc text.^([[12](#_footnotedef_12
    "View footnote.")])'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Here’s another heatmap of the Chapter 3 text. Do you see anything interesting
    here?
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '![ch3 heatmap](images/ch3-heatmap.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
- en: Notice the giant dark red cross (*gray* cross in print) spanning the entire
    chapter? This means the text in the middle of that cross is very different from
    all the other text in the chapter. Can you guess why? That section contains a
    sentence that starts with "Ernqnov…​", an encrypted line from the "Zen of Python"
    (`import this`). And the tiny white rectangle at that location shows that each
    line of that encrypted poem is very similar to the lines near it.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: A semantic heatmap is one way to find structure in your text data, but if you
    want to create knowledge from text you will need to go further. Your next step
    is to use the vector representations of sentences to create a "graph" of connections
    between entities. Entities in the real world are related by facts. Our mental
    model of the world is a belief network or a *knowledge graph* — a newtwork of
    connections between all the things (entities) you know something about.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 11.3 A knowledge extraction pipeline
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once you have your sentences organized you can start extracting concepts and
    relations from natural language text. For example, imagine a chatbot user says
    "Remind me to read AI Index on Monday."^([[13](#_footnotedef_13 "View footnote.")])
    You’d like that statement to trigger a calendar entry or alarm for the next Monday
    after the current date. Easier said than done.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: 'To trigger correct actions with natural language you need something like an
    NLU pipeline or parser that is a little less fuzzy than a transformer or large
    language model. You need to know that "me" represents a particular kind of named
    entity: a person. Named entities are natural language terms or n-grams that refer
    to a particular thing in the real world, such as a person, place or thing. Sound
    familiar? In English grammar, the part of speech (POS) for a person, place or
    thing is "noun". So you’ll see that the POS tag that spaCy associates with the
    tokens for a named entity is "NOUN".'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: And the chatbot should know that it can expand or *resolve* that word by replacing
    it with that person’s username or other identifying information. You’d also need
    your chatbot to recognize that "aiindex.org" is an abbreviated URL, which is a
    named entity - a name of a specific instance of something, like a website or company.
    And you need to know that a normalized spelling of this particular kind of named
    entity might be " [http://aiindex.org](.html) ", " [https://aiindex.org](.html)
    ", or maybe even " [https://www.aiindex.org](.html) ". Likewise, you need your
    chatbot to recognize that Monday is one of the days of the week (another kind
    of named entity called an "event") and be able to find it on the calendar.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: For the chatbot to respond properly to that simple request, you also need it
    to extract the relation between the named entity "me" and the command "remind."
    You’d even need to recognize the implied subject of the sentence, "you", referring
    to the chatbot, another person named entity. And you need to teach the chatbot
    that reminders happen in the future, so it should find the soonest upcoming Monday
    to create the reminder.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: And that’s just a simple use case. You can construct a graph from scratch using
    your own common sense knowledge or the domain knowledge that you want your AI
    to know about. But if you can extract knowledge from text you can build much larger
    knowledge graphs much quicker. Plus, you will need this algorithm to double-check
    any text generated by your language models.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'Knowledge extraction requires four main steps:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.2 Four stages of knowledge extraction
  id: totrans-104
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![knowledge graph extraction drawio](images/knowledge-graph-extraction_drawio.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
- en: 'Fortunately, the spaCy language models include the building blocks for knowledge
    extraction: named entity recognition, coreference resolution, and relation extraction.
    You only need to know how to combine the results of each of these steps to connect
    the pieces together. Let’s look at each stage separately by looking at an article
    about Timnit Gebru, a thought leader in AI ethics. We’ll continue using the spaCy
    nlp model we initialized in the previous section.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by downloading the Wikipedia article about Timnit Gebru.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Have you heard of Timnit Gebru before? She’s famous among people in your area
    of interest and she’s written several influential papers:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: That’s a pretty interesting research paper title. It certainly seems like something
    her bosses would be interested in publishing. But you aren’t interested in reading
    all of Wikipedia to find interesting tidbits about Stochastic Parrots and AI ethics
    experts such as Timnit Gebru. An information extraction pipeline can automatically
    recognize interesting named entities (people, places, things, and even dates and
    times). And if you want to support her, your NLP pipeline will be able to recognize
    mentions of her hidden behind pronouns in X messages (tweets).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 11.4 Entity Recognition
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step in extracting knowledge about some *thing* is to find the *things*
    that you want to know about. The most important things in natural language text
    are the names of people, places, and things. In linguistics named things are called
    "named entities." These are not just names - they might be things like dates,
    locations, and any piece of information that can be placed into your knowledge
    graph.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: As with sentences, you can go two ways about the task of Named Entity Recognition
    (NER) - using pattern-matching, and using the neural approach.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ll discover that there are cases in which regular expressions are as precise,
    or even more precise, than neural networks. Here are some keystone bits of quantitative
    information that are worth the effort of "hand-crafted" regular expressions:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: GPS locations
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dates
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prices
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Numbers
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s make a quick detour to learn how to extract such numerical data in the
    next section.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '11.4.1 Pattern-based entity recognition: extracting GPS locations'
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: GPS locations are typical of the kinds of numerical data you’ll want to extract
    from text using regular expressions. GPS locations come in pairs of numerical
    values for latitude and longitude. They sometimes also include a third number
    for altitude or height above sea level, but you’ll ignore that for now. Let’s
    just extract decimal latitude/longitude pairs, expressed in degrees. This will
    work for many Google Maps URLs. Though URLs are not technically natural language,
    they are often part of unstructured text data, and you’d like to extract this
    bit of information, so your chatbot can know about places as well as things.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use your decimal number pattern from previous examples, but let’s be more
    restrictive and make sure the value is within the valid range for latitude (+/-
    90 deg) and longitude (+/- 180 deg). You can’t go any farther north than the North
    Pole (+90 deg) or farther south than the South Pole (-90 deg). And if you sail
    from Greenwich England 180 deg east (+180 deg longitude), you’ll reach the date
    line, where you’re also 180 deg west (-180 deg) from Greenwich.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用之前示例中的十进制数模式，但是让我们更为严格，以确保值在有效的纬度（±90度）和经度（±180度）范围内。您不能到达比北极更北的任何地方（+90度），也不能到达南极比更南的任何地方（-90度）。如果你从英国的格林威治出发东行180度（+180度经度），你会到达日期变更线，那儿也是距离格林威治180度西经度（-180度）。
- en: Listing 11.1 Regular expression for GPS coordinates
  id: totrans-124
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列出了GPS坐标的正则表达式11.1
- en: '[PRE17]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Numerical data is pretty easy to extract, especially if the numbers are part
    of a machine-readable string. URLs and other machine-readable strings put numbers
    such as latitude and longitude in a predictable order, format, and units to make
    things easy for us.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 数字数据很容易提取，特别是如果数字是机器可读的格式的一部分。URL和其他机器可读的字符串将纬度和经度等数字以可预测的顺序、格式和单位排列，使我们的工作更易于进行。
- en: However, if we want to extract people’s names, nationalities, places and other
    things that don’t have a standard format, things become much more complicated.
    We can of course account for all the names, locations, and organizations possible.
    But keeping such a collection up to date would be a tremendously laborious task.
    For this, we’ll need the neural approach.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们想提取人名、国籍、地名和其他没有标准格式的内容，情况会变得更加复杂。当然，我们可以考虑所有可能的名称、位置和组织，但是维护这样的集合将是一项巨大的工作。因此，我们需要神经网络的方法。
- en: 11.4.2 Named entity recognition with spaCy
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.4.2 使用spaCy进行命名实体识别。
- en: Because NER is just a foundational task, you can imagine researchers have started
    trying to do it efficiently way before Neural Nets.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 因为命名实体识别（NER）只是一个基础任务，所以在神经网络出现之前，研究人员已经开始尝试高效地完成它。
- en: However, the neural networks gave a huge boost to how fast and accurate NER
    can be performed on a text. Note that recognizing and categorizing named entities
    is not as straightforward as you might think. One of the common challenges of
    NER is *segmentation*, or defining boundaries of the named entity (is "New York"
    one named entity or two separate ones?) Another, even trickier one, is categorizing
    the type of the entity. For example, the name Washington can be used to signify
    a person (such as the writer Washington Irving), a location (Washington DC), an
    organization (Washington Post) and even a sports team (as in "Washington won two
    games in the last season").
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，神经网络大大提高了在文本上执行NER的速度和准确性。请注意，识别和分类命名实体并不像您想象的那样简单。命名实体识别的一个常见挑战是 *分段* ，即定义命名实体的边界（例如“纽约”是一个命名实体还是两个不同的实体？）另一个更加棘手的挑战是分类实体的类型。例如，姓名华盛顿可以用于表示人（如作家华盛顿·欧文）、地点（华盛顿特区）、组织机构（《华盛顿邮报》）甚至运动队（如在“华盛顿在上赛季赢了两场比赛”中）。
- en: So you can see how the *context* of the entity - both the words that came before
    it and after it, potentially much later in the sentence - matters. That’s why
    the popular approaches to NER with neural networks include multi-level CNNs, and
    bi-directional transformers such as BERT, or bi-directional LSTMs. The last one,
    combined with a technique called Conditional Random Weights (CRF) is what spaCy
    uses in its named entity recognition module.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您可以看到实体的 *上下文* - 包括它前面和后面的单词，可能远在句子之后 - 很重要。这就是为什么使用多级CNN和双向转换器（如BERT或双向LSTM）进行NER的流行方法，以及与称为条件随机场（CRF）的技术相结合，是spaCy在命名实体识别模块中使用的方法。
- en: Of course, you don’t have to know how to build neural networks in order to extract
    the named entities from a text. The 'ents' attribute of a `doc` object that gets
    created once you run spaCy on a text contains a list of all those named entities.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，您不必知道如何构建神经网络才能从文本中提取命名实体。在运行spaCy处理文本后创建的 `doc` 对象的 'ents' 属性包含了所有这些命名实体的列表。
- en: '[PRE18]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The challenge of named entity recognition is closely related to a more basic
    problem - part-of-speech (POS) tagging. To recognize named entities in the sentence,
    you need to know which part of speech each word belongs to. In English grammar,
    the *part of speech* (POS) for a person, place or thing is "noun". And your named
    entity will often be a proper noun - a noun that refers to a *particular* person,
    place or thing in the real world. And the part of speech tag for relations is
    a *verb*. The verb tokens will be used to connect the named entities to each other
    as the edges in your knowledge graph.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Part-of-speech tagging is also crucial to the next stage in our pipeline - dependency
    parsing. To determine the relationships between different entities inside the
    sentence, we will need to recognize the verbs in our sentence.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, spaCy already did that for you the moment you fed the text to it.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Can you make sense of this? PUNCT, NOUN and VERB are pretty self-explanatory;
    and you can guess that PROPN stands for Proper Noun. But what about CCONJ? Luckily,
    you can let spaCy explain it to you.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Another tool spaCy gives you is the `tag_` property of each token. While the
    `pos_` tag gives you the part of speech or a particular token, the `tag_` gives
    you more information and details about the token. Let’s see an example:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Wow, this looks much more cryptic. You can vaguely intuit the connection between
    PROPN and NNP, but what is VBZ?
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: That’s for sure much more information, albeit served in a more cryptical form.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Let’s bring all the information about your tokens together in one table.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Now you have a function you can use to extract the tags you are interested in
    for any sentence or text (document). If you coerce a list of dictionaries into
    a DataFrame you will be able to see the sequence of tokens and tags side by side.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: You are already familiar with the 'POS' and 'TAG' labels for tokens. The fourth
    column 'ENT_TYPE', gives you information about the type of the named entity that
    token is a part of. Many named entities span several tokens, such as "Timnit Gebru"
    with spans two tokens. You can see that the small spaCy model didn’t do that well;
    it missed Timnit Gebru as a named entity at the beginning of the text. And when
    spaCy did finally recognize it towards the end of the Wikipedia article, it labeled
    its entity type as "organization."
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: A larger spaCy model should be able to improve your accuracy a little bit, especially
    for words that aren’t very common in the datasets used to train spaCy.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This looks better! "Timnit Gebru" is now correctly classified as a `PERSON`,
    and "Wikimedia" is properly tagged as `ORG` (organization). So this will usually
    be the first algorithm in your knowledge extraction pipeline, the spaCy language
    model that tokenizes your text and tags each token with the linguistic features
    you need for knowledge extraction.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Once you understand how a named entity recognizer works, you can expand the
    kinds of nouns and noun phrases you want to recognize and include them in your
    knowledge graph. This can help generalize your knowledge graph and create a more
    generally intelligent NLP pipeline.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: But you have yet to use the last column in your DataFrame of token tags, `DEP`
    (dependency). The `DEP` tag indicates the token’s role in the dependency tree.
    Before you move on to dependency parsing and relation extraction, you need to
    learn how to deal with step 2 of the knowledge extraction pipeline, coreference
    resolution.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 11.5 Coreference Resolution
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine you’re running NER on a text, and you obtain the list of entities that
    the model has recognized. On closer inspection, you realize over half of them
    are duplicates because they’re referring to the same terms! This is where *Coreference
    resolution* comes in handy because it identifies all the mentions of a noun in
    a sentence. This will consolidate mentions of the same *things* in your knowledge
    graph instead of creating redundant nodes and edges and potentially creating incorrect
    relations.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'Can you see the coreferences to "Timnit Gebru" in this sentence about that
    paper and her bosses:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: As a human, you can understand that "Gebru", "she" and "her" all relate. But
    it’s trickier for a machine to recognize that, especially if "she" is mentioned
    before "Gebru" (a phenomenon called *cataphora*).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'And that’s a relatively simple case! Consider this sentence: "The city councilmen
    refused the demonstrators a permit because they feared violence". Who does "they"
    in the sentence refer to? Our common sense tells us that it refers to the "city
    councilmen" and the answer seems to be easy for us, but this task of identifying
    mentions using common sense is surprisingly difficult for deep learning models.
    This task is called the Winograd schema challenge, or a "common-sense reasoning"
    or "common-sense inference" problem.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how NLP deals with this difficult NLP task. Deep problems call for
    deep learning!
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 11.5.1 Coreference resolution with spaCy
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As of this writing NeuralCoref 4.0 was the fastest and most accurate entity
    resolver available in the open-source community. As the name suggests NeuralCoref
    uses a deep learning neural network (transformer) to resolve coreferences to named
    entities. SpaCy incorporated transformers and NeuralCoref into its "Universe"
    collection of pipelines and models. NeuralCoref uses the original spaCy pipelines
    for `POS` tagging, named entity recognition, and extracting *coreferences* (secondary
    mentions of entities) in the text. It then takes the words surrounding each mention
    of an entity and feeds them into a feed-forward neural network or transformer
    to compute a score estimating whether each pair of mentions refer to the same
    object (entity). Comparing these scores is how the network resolves what each
    mention refers to.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: The `spacy-experimental` package includes coreference resolution algorithms
    within the `CoreferenceResolver` class, but to use NeuralCoref directly you will
    need to install and import the `coreferee` package. The original NeuralCoref is
    no longer actively maintained but spaCy has ported the algorithms to the `coreferee`
    package which works as a custom pipeline within spaCy. You will also need to download
    a transformer-based spaCy language model to use the `coreferee` pipeline.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Like other spacy language models, you must first download "en_core_web_trf"
    before you can `load` and run it. The `trf` suffix indicates that this language
    model is a recent addition to the spaCy toolbox that incorporates a *transformer*
    neural network into the pipeline. This is a very large language model, so you
    probably don’t want to run the `cli.download()` function any more than you need
    to.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: So this pipeline was able to find 2 *coreference chains* that link mentions
    of entities together. These two chains represent two distinct real-world objects,
    "Gebru" and "advice". The "Gebru" token at position 13 is linked to the three
    "she" pronouns at positions 16, 26 and 34\. The "advice" token is linked to the
    word "it" at position 56.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: So now you have consolidated all the mentions of Gebru in this single sentence
    from Wikipedia, and you can use those coreferences to extract important relations
    and facts about her.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 11.5.2 Entity name normalization
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Closely related to coreference resolution is the issue of *normalization* of
    entities. The normalized representation of an entity is usually a string, even
    for numerical information such as dates. For example, the normalized ISO format
    for Timnit Gebru’s date of birth would be "1983-05-13". A normalized representation
    for entities enables your knowledge base to connect all the different things that
    happened in the world on that same date to that same node (entity) in your graph.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: You’d do the same for other named entities. You’d correct the spelling of words
    and attempt to resolve ambiguities for names of objects, animals, people, places,
    and so on. For example, San Francisco may be referred to, in different places
    as "San Fran", "SF", "'Frisco" or "Fog City". Normalization of named entities
    ensures that spelling and naming variations don’t pollute your vocabulary of entity
    names with confounding, redundant names.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: A knowledge graph should normalize each kind of entity the same way, to prevent
    multiple distinct entities of the same type from sharing the same "name." You
    don’t want multiple person name entries in your database referring to the same
    physical person. Even more importantly, the normalization should be applied consistently — both
    when you write new facts to the knowledge base or when you read or query the knowledge
    base.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: If you decide to change the normalization approach after the database has been
    populated, the data for existing entities in the knowledge should be "migrated",
    or altered, to adhere to the new normalization scheme. Schemaless databases (key-value
    stores), like the ones used to store knowledge graphs or knowledge bases, are
    not free from the migration responsibilities of relational databases. After all,
    schemaless databases are interface wrappers for relational databases under the
    hood.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 11.6 Dependency Parsing
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, you learned how to recognize and tag named entities
    in text. Now you’ll learn how to find relationships between these entities. A
    typical sentence may contain several named entities of various types, such as
    geographic entities, organizations, people, political entities, times (including
    dates), artifacts, events, and natural phenomena. And a sentence can contain several
    *relations*, too — facts about the relationship between the named entities in
    the sentence
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'NLP researchers have identified two separate problems or models that can be
    used to identify how the words in a sentence work together to create meaning:
    *dependency parsing* and *constituency parsing*. *Dependency parsing* will give
    your NLP pipelines the ability to diagram sentences like you learned to do in
    grammar school (elementary school). And these tree data structures give your model
    a representation of the logic and grammar of a sentence. This will help your applications
    and bots become a bit smarter about how they interpret sentences and act on them.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '*Constituency parsing* is another technique, and it’s concerned with identifying
    the *constituent subphrases* in a sentence. While dependency parsing deals with
    relationships between words, constituency parsing aims to parse a sentence into
    a series of constituents. These constituents can be, for example, a noun phrase
    ("My new computer") or a verb phrase ("has memory issues"). Its approach is more
    top-down, trying to iteratively break constituents into smaller units and relationships
    between them. Though constituency parsing can capture more syntactic information
    about the sentence, its results are slower to compute and more difficult to interpret.
    So we will focus on dependency parsing for now.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: But wait, you’re probably wondering why understanding relationships between
    entities and sentence diagrams are so important. After all, you’ve probably already
    forgotten how to create them yourself and have probably never used them in real
    life. But that’s only because you’ve internalized this model of the world. We
    need to create that understanding in bots so they can be used to do the same things
    you do without thinking, from simple tasks like grammar checking to complex virtual
    assistants.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Basically, dependency parsing will help your NLP pipelines for all those applications
    mentioned in Chapter 1…​ better. Have you noticed how chatbots like GPT-3 often
    fall on their face when it comes to understanding simple sentences or having a
    substantive conversation? As soon as you start to ask them about the logic or
    reasoning of the words they are "saying" they stumble. Chatbot developers and
    conversation designers get around this limitation by using rule-based chatbots
    for substantive conversations like therapy and teaching. The open-ended neural
    network models like PalM and GPT-3 are only used when the user tries to talk about
    something that hasn’t yet been programmed into it. And the language models are
    trained with the objective of steering the conversation back to something that
    the bot knows about and has rules for.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Dependency parsing, as the name suggests, relies on "dependencies" between the
    words in a sentence to extract information. "Dependencies" between two words could
    refer to their grammatical, phrasal, or any custom relations. But in the context
    of dependency parse trees, we refer to the grammatical relationships between word
    pairs of the sentence, one of them acting as the "head" and the other one acting
    as the "dependent". There exists only one word in a sentence that is not dependent
    on any other word in the parse tree, and this word is called the "root" ("ROOT").
    The root is the starting point for the dependency tree just as the main root of
    a tree in the forest starts the growth of its trunk and branches (relations).
    There are 37 kinds of dependency relations that a word could have, and these relations
    are adapted from the *Universal Stanford Dependencies* system.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'The spaCy package knows how to recognize these relations between words and
    phrases, and even plot the dependency diagrams for you. Let’s try to do dependency
    parsing of a single sentence:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: You can see that the ROOT of the sentence is the verb "fired". This is because
    in our sentence, the word "fired" happens to be the main verb when you organize
    it into a Subject-Verb-Object triple. And the dependency (`DEP`) role that the
    word "Gebru" serves is as the "passive nominal subject" (`nsubjpass`). Is there
    a dependency between them "fired" and "Gebru" that you can use to create a relationship
    or fact in a knowledge graph? The `children` attribute gives you a list of all
    the words that depend on a particular token. These dependencies are the key to
    connecting tokens in a relationship to construct a fact.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: So you will need to include the `children` attribute in your `token_dict` function
    if you want it to show you children of each token in a sentence.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: It may seem weird to you that the token "Gebru" doesn’t have any children (dependents)
    in this sentence. It’s the subject of the sentence, after all. The child-parent
    relationship of natural language grammar rules will be a little confusing at first,
    but you can use `displacy` and your `doc2df` function to help you develop a mental
    model for how words depend on each other.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Redefine the doc2df function to add the `children` attribute as a column so
    you can see if any other words in this sentence have dependents (children).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Looks like the sentence root (labeled `ROOT`) has the most children. "Fired"
    is the most important word in the sentence and all the other words depend on it.
    Every word in a dependency tree is connected to another word elsewhere in the
    sentence. To see this, you need to examine that long list of children in the sentence
    root, 'fired'.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The sentence root branches out to the word "Gebru" and several other words including
    "from." And the word "from" leads to "team", then to "her" and "AI". And "AI"
    leads to "Ethical." You can see that children modify their parents.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: The `ROOT` of the dependency tree is the main verb of a sentence. This is where
    you will usually find tokens with the most children. Verbs become relationships
    in a knowledge graph and children become the objects of that relationship in the
    relationship tripple. The token "Gebru" is a child of the passive verb fired,
    so you know that she was the one being fired, but this sentence does not say who
    is responsible for firing her. Since you do not know the subject of the verb "fired"
    you cannot determine who deserves the "unethically" adverb that describes their
    actions.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Time for dependency diagrams to shine! We’ll use one of spaCy’s sub-libraries
    called `displacy`. It can generate a *scalable vector graphics* SVG string (or
    a complete HTML page), which can be viewed as an image in a browser. This visualization
    can help you find ways to use the tree to create tag patterns for relation extraction.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.2 Visualize a dependency tree
  id: totrans-194
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: When you open the file, you should see something like Figure [11.3](#figure-dependency-diagram).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.3 Dependency diagram for a sentence
  id: totrans-197
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![dependency diagram](images/dependency_diagram.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
- en: Before we explain the connection between dependency parsing and relation extraction,
    let’s briefly dive into another tool at our disposal - constituency parsing.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 11.6.1 Constituency parsing with benepar
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Berkeley Neural Parser and Stanza have been the go-to options for the extraction
    of constituency relations in text. Let’s explore one of them, Berkeley Neural
    Parser.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: This parser cannot be used on its own and requires either spaCy or NLTK to load
    it along with their existing models. You want to use spaCy as your tokenizer and
    dependency tree parse because it is continually improving.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.3 Download the necessary packages
  id: totrans-203
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: After downloading the packages, we can test it out with a sample sentence. But
    we will be adding `benepar` to spaCy’s pipeline first.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Looks quite cryptic, right? In the example above, we generated a parsed string
    for the test sentence. The parse string includes various phrases and the POS tags
    of the tokens in the sentence. Some common tags you may notice in our parse string
    are NP ("Noun Phrase"), VP ("Verb Phrase"), S ("Sentence"), and PP ("Prepositional
    Phrase"). Now you can see how it’s a bit more difficult to extract information
    from the constituency parser’s output. However, it can be useful to identify all
    the phrases in the sentence and use them in sentence simplification and/or summarization.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: You now know how to extract the syntactic structure of sentences. How will it
    help you in your quest for an intelligent chatbot?
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 11.7 From dependency parsing to relation extraction
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ve come to the crucial stage of helping our bot learn from what it reads.
    Take this sentence from Wikipedia:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '*In 1983, Stanislav Petrov, a lieutenant colonel of the Soviet Air Defense
    Forces, saved the world from nuclear war.*'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 'If you were to take notes in a history class after reading or hearing something
    like that, you’d probably paraphrase things and create connections in your brain
    between concepts or words. You might reduce it to a piece of knowledge, that thing
    that you "got out of it." You’d like your bot to do the same thing. You’d like
    it to "take note" of whatever it learns, such as the fact or knowledge that Stanislav
    Petrov was a lieutenant colonel. This could be stored in a data structure something
    like this:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This is an example of two named entity nodes ('Stanislav Petrov' and 'lieutenant
    colonel') and a relation or connection ('is a') between them in a knowledge graph
    or knowledge base. When a relationship like this is stored in a form that complies
    with the RDF standard (resource description format) for knowledge graphs, it’s
    referred to as an RDF triplet. Historically these RDF triplets were stored in
    XML files, but they can be stored in any file format or database that can hold
    a graph of triplets in the form of `(subject, relation, object)`. A collection
    of these triplets will be your knowledge graph!
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go ahead and create some fodder for your knowledge graph using the two
    approaches we know - patterns and machine learning.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 11.7.1 Pattern-based relation extraction
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Remember how you used regular expressions to extract character patterns? Word
    patterns are just like regular expressions but for words instead of characters.
    Instead of character classes, you have word classes. For example, instead of matching
    a lowercase character, you might have a word pattern decision to match all the
    singular nouns ("NN" POS tag).^([[14](#_footnotedef_14 "View footnote.")]) Some
    seed sentences are tagged with some correct relationships (facts) extracted from
    those sentences. A POS pattern can be used to find similar sentences where the
    subject and object words might change or even the relationship words.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest way to extract relations out of the text is to look for all "Subject-Verb-Object"
    triplets using the "nsubj" and "dobj" tags of the ROOT word. But let’s do something
    a bit more complex. What if we want to extract information about meetings between
    historical figures from Wikipedia? You can use the spaCy package in two different
    ways to match these patterns in \(O(1)\) (constant time) no matter how many patterns
    you want to match:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: PhraseMatcher for any word/tag sequence patterns ^([[15](#_footnotedef_15 "View
    footnote.")])
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matcher for POS tag sequence patterns ^([[16](#_footnotedef_16 "View footnote.")])
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start with the latter.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s look at an example sentence and see the POS for every word:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.4 Helper functions for spaCy tagged strings
  id: totrans-223
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now you can see the sequence of POS or TAG features that will make a good pattern.
    If you’re looking for "has-met" relationships between people and organizations,
    you’d probably like to allow patterns such as "PROPN met PROPN", "PROPN met the
    PROPN", "PROPN met with the PROPN", and "PROPN often meets with PROPN". You could
    specify each of those patterns individually, or try to capture them all with some
    * or ? operators on "any word" patterns between your proper nouns:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Patterns in spaCy are a lot like this pseudocode, but much more powerful and
    flexible. SpaCy patterns are very similar to regular expressions for tokens. Like
    regular expressions, you have to be very verbose to explain exactly the word features
    you’d like to match at each position in the token sequence. In a spaCy pattern,
    you use a dictionary of lists to capture all the parts-of-speech and other features
    that you want to match for each token or word.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.5 Example spaCy POS pattern
  id: totrans-228
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: You can then extract the tagged tokens you need from your parsed sentence.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.6 Creating a POS pattern matcher with spaCy
  id: totrans-231
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: A spacy matcher will list the pattern matches as 3-tuples containing match ID
    integers, plus the start and stop token indices (positions) for each match. So
    you extracted a match from the original sentence from which you created the pattern,
    but what about similar sentences from Wikipedia?
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.7 Using a POS pattern matcher
  id: totrans-234
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: You need to add a second pattern to allow for the verb to occur after the subject
    and object nouns.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.8 Combine patterns together to handle more variations
  id: totrans-237
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: So now you have your entities and a relationship. You can even build a pattern
    that is less restrictive about the verb in the middle ("met") and more restrictive
    about the names of the people and groups on either side. Doing so might allow
    you to identify additional verbs that imply that one person or group has met another,
    such as the verb "knows" or even passive phrases such as "had a conversation"
    or "became acquainted with". Then you could use these new verbs to add relationships
    for new proper nouns on either side.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: But you can see how you’re drifting away from the original meaning of your seed
    relationship patterns. This is called semantic drift. To ensure that the new relations
    found in new sentences are truly analogous to the original seed (example) relationships,
    you often need to constrain the subject, relation, and object word meanings to
    be similar to those in the seed sentences. The best way to do this is with some
    vector representation of the meaning of words. Fortunately for you, spaCy tags
    words in a parsed document with not only their POS and dependency tree information
    but also provides the Word2Vec word vector. You can use this vector to prevent
    the connector verb and the proper nouns on either side from drifting too far away
    from the original meaning of your seed pattern.^([[17](#_footnotedef_17 "View
    footnote.")])
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Using semantic vector representations for words and phrases has made automatic
    information extraction accurate enough to build large knowledge bases automatically.
    But human supervision and curation are required to resolve much of the ambiguity
    in natural language text.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 11.7.2 Neural relation extraction
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that you’ve seen the pattern-based method for relation extraction, you
    can imagine that researchers have already tried to do the same with a neural network.
    The neural relation extraction task is traditionally classified into two categories:
    closed and open.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: In *closed* relation extraction, the model extracts relations only from a given
    list of relation types. The advantages of this are that we can minimize the risk
    of getting untrue and bizarre relation labels between entities which makes us
    more confident about using them in real life. But the limitation is that it needs
    human labelers to come up with a list of relevant labels for every category of
    text, which as you can imagine, can get tedious and expensive.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: In *open* relation extraction, the model tries to come up with its own set of
    probable labels for the named entities in the text. This is suitable for processing
    large and generally unknown texts like Wikipedia articles and news entries.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Over the past few years, experiments with Deep Neural Networks have given strong
    results on triplet extraction and subsequently, most of the research on the topic
    now follow neural methods.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, there aren’t as many out-of-the-box solutions for relation extraction
    as there are for the previous stages of the pipeline. What’s more, your relation-extraction
    is usually going to be pretty targeted. In most cases, you wouldn’t want to extract
    ALL possible relations between entities, but only those that are relevant to the
    task you’re trying to perform. For example, you might want to extract interactions
    between drugs from a set of pharmaceutical documents.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: One of the state-of-the-art models that is used nowadays to extract relations
    is LUKE (Language Understanding with Knowledge-based Embeddings). LUKE uses *entity-aware
    attention* - meaning that its training data included information on whether each
    token is an entity or not. It was also trained to be able to "guess" a masked
    entity in a Wikipedia-based dataset (rather than just guessing all masked words,
    like the BERT model was trained).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: SpaCy also includes some infrastructure to create your own relation extraction
    component, but that requires quite a bit of work. We won’t cover it as part of
    this book. Fortunately, authors like Sofie Van Landeghem have created great resources
    ^([[18](#_footnotedef_18 "View footnote.")]) for you to learn from if you want
    to custom-train a relation extractor for your particular needs.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Training your relation extraction model
  id: totrans-250
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When training your relation extractor, you will need labeled data where the
    relations relative to your task are tagged properly in order for the model to
    learn to recognize them. But big datasets are hard to create and label, so it’s
    worth checking if some of the existing datasets used for benchmarking and finetuning
    state-of-the-art models already have the data that you need.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: DocRED and Stanford TACRED together are the de-facto benchmark datasets and
    models for relation extraction methods because of their size and the generality
    of the knowledge graphs
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Stanford’s Text Analysis Conference Relation Extraction Dataset (TACRED) contains
    more than 100,000 example natural language passages paired with their corresponding
    relations and entities. It covers 41 relation types. Over the past few years,
    researchers have improved TACRED’s data quality and reduced ambiguity in the relation
    classes with datasets such as Re-TACRED and DocRED.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: The Document Relation Extraction Dataset (DocRED) expands the breadth of natural
    language text that can be used for relation extraction because it includes relations
    that require parsing of multiple sentences of natural language text. The training
    and validation dataset used to train DocRED is currently (in 2023) the largest
    human-annotated dataset for document-level relation extraction. Most of the human-annotated
    knowledge graph data in DocRED is included in the Wikidata knowledge base. And
    the corresponding natural language text examples can be found in the archived
    version of Wikipedia.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Now you have a better idea of how to take an unstructured text and turn it into
    a collection of facts. Time for the last stage of our pipeline - building a knowledge
    database.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 11.8 Building your knowledge base
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So, you have your relations extracted from your text. You could put them all
    into a big table; and yet, we keep talking about knowledge *graphs*. What really
    makes this particular way of structuring data so powerful?
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go back to Stanislav Petrov, whom we’ve met in the last chapter. What
    if we wanted to answer a question like "What is Stanislav Petrov’s military rank?"
    This is a question that a single relation triple 'Stanislav Petrov', 'is-a', 'lieutenant
    colonel' isn’t enough to answer - because your question-answering machine also
    needs to know that "lieutenant colonel" is a military rank. However, if you organize
    your knowledge as a graph, answering the question becomes possible. Take a look
    at Figure [11.4](#figure-stanislav-knowledge-graph) to understand how it happens.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.4 Stanislav knowledge graph
  id: totrans-259
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![Stanislav Knowledge Graph](images/Stanislav-Knowledge-Graph.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
- en: The red edge and node in this knowledge graph represent a fact that could not
    be directly extracted from the statement about Stanislav. But this fact that "lieutenant
    colonel" is a military rank could be inferred from the fact that the title of
    a person who is a member of a military organization is a military rank. This logical
    operation of deriving facts from a knowledge graph is called knowledge graph *inference*.
    It can also be called querying a knowledge base, analogous to querying a relational
    database. A whole field called Knowledge Base Question Answering is focused on
    finding ways to answer questions like this (they are called "multi-hop questions")
    more efficiently.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: For this particular inference or query about Stanislov’s military ranks, your
    knowledge graph would have to already contain facts about militaries and military
    ranks. It might even help if the knowledge base had facts about the titles of
    people and how people relate to occupations (jobs). Perhaps you can see now how
    a base of knowledge helps a machine understand more about a statement than it
    could without that knowledge. Without this base of knowledge, many of the facts
    in a simple statement like this will be "over the head" of your chatbot. You might
    even say that questions about occupational rank would be "above the pay grade"
    of a bot that only knew how to classify documents according to randomly allocated
    topics. (See Chapter 4 if you’ve forgotten about how random topic allocation can
    be.)
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: It may not be obvious how big a deal this is, but it is a *BIG* deal. If you’ve
    ever interacted with a chatbot that doesn’t understand "which way is up", literally,
    you’d understand. One of the most daunting challenges in AI research is the challenge
    of compiling and efficiently querying a knowledge graph of common sense knowledge.
    We take common-sense knowledge for granted in our everyday conversations.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Humans start acquiring much of their common sense knowledge even before they
    acquire language skills. We don’t spend our childhood writing about how a day
    begins with light and sleep usually follows sunset. And we don’t edit Wikipedia
    articles about how an empty belly should only be filled with food rather than
    dirt or rocks. This makes it hard for machines to find a corpus of common sense
    knowledge to read and learn from. No common-sense knowledge Wikipedia articles
    exist for your bot to do information extraction on. And some of that knowledge
    is instinct, hard-coded into our DNA.^([[19](#_footnotedef_19 "View footnote.")])
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: All kinds of factual relationships exist between things and people, such as
    "kind-of", "is-used-for", "has-a", "is-famous-for", "was-born", and "has-profession."
    NELL, the Carnegie Mellon Never Ending Language Learning bot is focused almost
    entirely on the task of extracting information about the `'kind-of'` relationship.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Most knowledge bases normalize the strings that define these relationships,
    so that "kind of" and "type of" would be assigned a normalized string or ID to
    represent that particular relation. And some knowledge bases also normalize the
    nouns representing the objects in a knowledge base, using coreference resolution
    that we described before. So the bigram "Stanislav Petrov" might be assigned a
    particular ID. Synonyms for "Stanislav Petrov", like "S. Petrov" and "Lt Col Petrov",
    would also be assigned to that same ID, if the NLP pipeline suspected they referred
    to the same person.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: 11.8.1 A large knowledge graph
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you’ve ever heard of a "mind map" they can give a pretty good mental model
    of what knowledge graphs are: connections between concepts in your mind. To give
    you a more concrete mental model of the concept of knowledge graphs you probably
    want to explore the oldest public knowledge graph on the web: NELL (Never Ending
    Language Learning) graph, created by the bot we met in the last section.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: The NLPiA2 Python package has several utilities for making the NELL knowledge
    graph a bit easier to wrap your head around. Later in the chapter, you’ll see
    the details about how these work so you can prettify whatever knowledge graph
    you are working with.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The entity names are very precise and well-defined within a hierarchy, like
    paths for a file or name-spaced variable names in Python. All of the entity and
    value names start with "concept:" so you can strip that from your name strings
    to make the data a bit easier to work with. To simplify things further, you can
    eliminate the namespacing hierarchy and focus on just the last name in the hierarchy.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The `nlpia2.nell` module simplifies the names of things even further. This makes
    it easier to navigate the knowledge graph in a network diagram. Otherwise, the
    names of entities can fill up the width of the plot and crowd each other out.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: NELL scrapes text from Twitter, so the spelling and wording of facts can be
    quite varied. In NELL the names of entities, relations and objects have been normalized
    by lowercasing them and removing all punctuation like apostrophes and hyphens.
    Only proper names are allowed to retain their spaces, to help distinguish between
    names that contain spaces and those that are smashed together. However, in NELL,
    just as in Word2vec token identifiers, proper names are joined with underscore
    ("\_") characters.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: Entity and relation names are like variable names in Python. You want to be
    able to query them like field names in a database, so they should not have ambiguous
    spellings. The original NELL dataset contains one row per triple (fact). Triples
    can be read like a terse, well-defined sentence. Knowledge triples describe a
    single isolated fact about the world. They give you one piece of information about
    an entity (object) in the world.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: As a minimum, a knowledge triple consists of an entity, relation and value.
    The first element of a knowledge triple gives you the name of the entity that
    the fact is about. The second column, "relation," contains the relationship to
    some other quality (adjective) or object (noun) in the world called its value.
    A relation is usually a verb phrase that starts with or implies words like "is"
    or "has." The third column, "value," contains an identifier for some quality of
    that relation. The "value" is the object of the relationship and is a named entity
    just as the subject ("entity") of the triple is.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Because NELL crowdsources the curation of the knowledge base, you also have
    a probability or confidence value that you can use to make inferences on conflicting
    pieces of information. And NELL has 9 more columns of information about the fact.
    It lists all the alternative phrases that were used to reference a particular
    entity, relation or value. NELL also identifies the iteration (loop through Twitter)
    that the fact was created during. The last column provides the source of the data
    - a list of all the texts that created the fact.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: NELL contains facts about more than 800 unique relations and more than 2 million
    entities. Because Twitter is mostly about people, places and businesses, it’s
    a good knowledge base to use to augment a common sense knowledge base. And it
    can be useful for doing fact-checking about famous people or businesses and places
    that are often the targets of misinformation campaigns. There’s even a "latitudelongitude"
    relation that you could use to verify any facts related to the location of things.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Now you have learned how facts can be organized into a knowledge graph. But
    what do we do when we need to use this knowledge - for example, for answering
    questions? That’s what we’ll be dealing with in the last section of this chapter.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: 11.9 Finding answers in a knowledge graph
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that our facts are all organized in a graph database, how do we retrieve
    that knowledge? As with any database, graph databases have special query languages
    to pull information from them. Just as SQL and its different dialects are used
    to query relational databases, a whole family of languages such as SPARQL (SPARQL
    Protocol and RDF Query Language), Cypher, and AQL exist to query graph databases.
    In this book, we’ll focus on SPARQL, as it was adopted as a standard by the open-source
    communities. Other languages, such as Cypher or AQL, are used to query specific
    graph knowledge bases, such as Neo4j and ArangoDB.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: 'As our knowledge base, we’ll use an even bigger knowledge graph than NELL:
    Wikidata, the knowledge database version of Wikipedia. It contains more than 100
    million data items (entities and relations) and is maintained by volunteer editors
    and bots, just like all the other Wikimedia projects.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 'In Wikidata, the relations between entities are called *properties*. There
    are more than 11,000 properties in Wikidata system, and each one has its "P-id",
    a unique identifier that is used to represent that property in queries. Similarly,
    every entity has its own unique "Q-id". You can easily retrieve the Q-id of any
    Wikipedia article by using Wikidata’s REST API:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: You can confirm your findings by heading to ([http://www.wikidata.org/entity/Q59753117](entity.html))
    and finding there more properties of this entity, that link it to different entities.
    As you can see, this is a simple "GET" query that only works if we already have
    the entity’s name and want to find the Q-id (or vice-versa). For more complex
    queries, we will need to use SPARQL. Let’s write your first query then!
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say you want to find out who were Timnit Gebru’s co-authors on her notable
    paper about Stochastic Parrots. If you don’t remember the name of the paper exactly,
    you can actually find it with a simple query. For this, you’ll need a couple of
    property and entity IDs - for simplicity, we just list them in the code.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Important
  id: totrans-290
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Don’t forget to double escape the curly braces in f-strings! And you cannot
    use a backslash as an escape character in f-strings. *WRONG*: f"\{" Instead you
    must double the curly braces. *RIGHT*: f"{{"'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: And if you are familiar with the `jinja2` package, be careful mixing using Python
    f-strings to populate jinja2 templates, you would need four curly braces to create
    a literal double curly brace.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: Cryptic at first sight, what this query means is "Find an entity A such that
    Timnit Gebru has A as notable work, and also A is an instance of an academic article".
    You can see how each relational condition is codified in SPARQL, with operand
    `wd:` preceding entity Q-ids and the operand `wdt:` preceding property P-ids.
    Each relation constraint has a form of "ENTITY has-property ENTITY".
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now use WIKIDATA’s SPARQL API to retrieve the results of our query. For
    this, we will use a dedicated `SPARQLWrapper` package that will simplify the process
    of querying for us. First, let’s set up our wrapper:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Once that’s set, you can execute your query and examine the response:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'This looks right! Now that you’ve got the Q-id of the article - you can retrieve
    its authors by using the ''author'' property of the article:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: And here you have the answer to your question!
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of doing two queries, we could have achieved the same result by nesting
    our queries, within each other, like this:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: SPARQL is a well-developed language whose functionality includes much more than
    just simple queries. Wikidata itself has a pretty good manual on SPARQL.^([[20](#_footnotedef_20
    "View footnote.")]) The deeper you dig into Wikidata using SPARQL the more uses
    you will find for it in your NLP applications. It is one of the only ways you
    can automatically evaluate the quality and correctness of the facts that your
    NLP pipeline asserts to your users.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: 11.9.1 From questions to queries
  id: totrans-304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So, you managed to find the answer to a pretty complex question in a knowledge
    database. That would have been pretty much impossible to do if your database was
    relational, or if all you had was unstructured text.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: However, looking for the answer took us quite a lot of work and two SPARQL queries.
    How do you transform a natural-language question into a query in a structured
    language like SPARQL?
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: You already did this kind of transformation before, back in Chapter 9\. Translating
    human language into machine language is a bit harder than translating between
    human languages, but it’s still the same basic problem for a machine. And now
    you know that transformers are good at transforming (pun intended) one language
    into another. LLMs, being huge transformers, are especially good at it. Sachin
    Charma created a great example of constructing a knowledge graph using another
    graph database, ArangoDB. She used OpenAI’s models to enable natural language
    question answering on the database he created.^([[21](#_footnotedef_21 "View footnote.")])
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: 11.10 Test yourself
  id: totrans-308
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Give an example of a question that’s easier to answer with a graph database
    than with a relational database.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convert a `networkx` directed graph to an edge list in a Pandas DataFrame with
    two columns `source_node` and `target_node`. How long does it take to retrieve
    all the target_node IDs for a single source node? What about all the target_nodes
    for those new source nodes? How would you speed up the Pandas graph query with
    an index?
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a Spacy Matcher that can more of Timnit Gebru’s places of work out of
    the Wikipedia articles about her. How many could you retrieve?
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is there anything a graph database can do that a relational database cannot?
    Can a relational database do anything that a graph database cannot?
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use a Large Language Model to generate a SPARQL wikidata query from natural
    language. Did it work correctly without you editing the code? Will it work for
    a query that requires five relationship (edge) traversals in your knowledge graph?
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Use `extractors.py` and `heatmaps.py` in `nlpia2.text_processing` to create
    a BERT similarity heatmap for sentences extracted from a long document of your
    own (perhaps a sequence of Mastodon microblog posts about NLP). Edit the `heatmaps.py`
    code to improve it so that you can focus on just the lines that are very similar.
    Hint: You can scale the cosine similarity values with a nonlinear function and
    reset the similarity values to zero using a threshold value.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 11.11 Summary
  id: totrans-315
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A knowledge graph can be built to store relationships between entities.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can isolate and extract information from unstructured text using either
    rule-based methods (like regular expressions) or neural-based methods.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part-of-speech tagging and dependency parsing allow you to extract relationships
    between entities mentioned in a sentence.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Languages like SPARQL can help you find the information you need in a knowledge
    graph.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[1]](#_footnoteref_1) Wikipedia article "Symbolic AI" ( [https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence](wiki.html))'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '[[2]](#_footnoteref_2) See the web page titled "Natural Language Processing
    : TM-Town" ( [https://www.tm-town.com/natural-language-processing#golden_rules](www.tm-town.com.html)).'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '[[3]](#_footnoteref_3) "Rstr package on PyPi ( [https://pypi.org/project/rstr/](rstr.html)).'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '[[4]](#_footnoteref_4) See the web page titled "Python sentence segment at
    DuckDuckGo" ( [https://duckduckgo.com/?q=Python+sentence+segment&t=canonical&ia=qa](duckduckgo.com.html)).'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '[[5]](#_footnoteref_5) Manuscript source code on GitLab ( [https://gitlab.com/tangibleai/nlpia2/-/tree/main/src/nlpia2/data/manuscript/adoc](manuscript.html))'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '[[6]](#_footnoteref_6) Each neuron in a single-layer neural net or perceptron,
    is mathematically equivalent to a logistic regression.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '[[7]](#_footnoteref_7) See the web page titled "Facts & Figures : spaCy Usage
    Documentation" ( [https://spacy.io/usage/facts-figures](usage.html)).'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '[[8]](#_footnoteref_8) See the web page titled "nltk.tokenize package — NLTK
    3.3 documentation" ( [http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.punkt](api.html)).'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '[[9]](#_footnoteref_9) SpaCy is far and away the most accurate and efficient
    NLP parser we’ve found, and it is maintained and updated regularly by a brilliant,
    supercooperating team of NLP engineers at Explosion.ai in Europe ( [https://explosion.ai/about](explosion.ai.html)).'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '[[10]](#_footnoteref_10) The `heatmaps.py` module in the nlpia2 package on
    GitLab ( [https://gitlab.com/tangibleai/nlpia2/-/blob/main/src/nlpia2/heatmaps.py](nlpia2.html))'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '[[11]](#_footnoteref_11) The `extractors.extract_lines()` function in the nlpia2
    package on GitLab ( [https://gitlab.com/tangibleai/nlpia2/-/blob/main/src/nlpia2/text_processing/extractors.py#L69](text_processing.html))'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: '[[12]](#_footnoteref_12) The official AsciiDoc parser is Ruby. No Python packages
    exist yet, according to the docs ( [https://gitlab.eclipse.org/eclipse-wg/asciidoc-wg/asciidoc.org/-/blob/main/awesome-asciidoc.adoc#convert](main.html))'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: '[[13]](#_footnoteref_13) Statistics about AI research at the AI Index by Stanford
    University ( [https://AIIndex.org](.html))'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '[[14]](#_footnoteref_14) spaCy uses the "OntoNotes 5" POS tags: ( [https://spacy.io/api/annotation#pos-tagging](api.html))'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '[[15]](#_footnoteref_15) See the web page titled "Code Examples : spaCy Usage
    Documentation" ( [https://spacy.io/usage/examples#phrase-matcher](usage.html)).'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: '[[16]](#_footnoteref_16) See the web page titled "Matcher : spaCy API Documentation"
    ( [https://spacy.io/api/matcher](api.html)).'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '[[17]](#_footnoteref_17) This is the subject of active research: [https://nlp.stanford.edu/pubs/structuredVS.pdf](pubs.html)'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: '[[18]](#_footnoteref_18) "Implementing a custom trainable component for relation
    extraction": ( [https://explosion.ai/blog/relation-extraction](blog.html))'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '[[19]](#_footnoteref_19) There are hard-coded common-sense knowledge bases
    out there for you to build on. Google Scholar is your friend in this knowledge
    graph search.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: '[[20]](#_footnoteref_20) Wikidata SPARQL tutorial: ( [https://www.wikidata.org/wiki/Wikidata:SPARQL_tutorial](wiki.html))'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '[[21]](#_footnoteref_21) How to Build Knowledge Graph Enhanced Chatbot with
    ChatGPT and ArangoDB ( [http://archive.today/fJB7H](archive.today.html)).'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
