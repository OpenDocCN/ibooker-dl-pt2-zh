- en: 1 Introduction to machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: “There are only patterns, patterns on top of patterns, patterns that affect
    other patterns. Patterns hidden by patterns. Patterns within patterns” – Chuck
    Palahniuk
  prefs: []
  type: TYPE_NORMAL
- en: We love patterns. Be it our business or our life, we find patterns and (generally)
    tend to stick to them. We have our preferences of groceries we buy, telecom operators
    and calling packs we use, news articles we follow, movie genres, and audio tracks
    we like – these all are examples of patterns of our preferences. We love patterns,
    and more than patterns we love finding them, arranging them, and maybe getting
    used to them!
  prefs: []
  type: TYPE_NORMAL
- en: There is a cliché going around - “Data is the new electricity”. Data is indeed
    precious; nobody can deny that. But data in its purest form will be of no use.
    We have to clean the data, analyze and visualize it, and then we can develop insights
    from it. Data sciences, machine learning, and artificial intelligence are helping
    us in uncovering these patterns – so that we can take more insightful and balanced
    decisions in our activities and business.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we are going to solve some of such mysteries. We will be studying
    a branch of machine learning referred to as Unsupervised Learning. Unsupervised
    learning solutions are one of the most influential approaches which are changing
    the face of the industry. They are utilized in banking and finance, retail, insurance,
    manufacturing, aviation, medical sciences, telecom, and almost every sector.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the book, we are discussing concepts of unsupervised learning - the
    building blocks of algorithms, their nuts and bolts, background processes, and
    mathematical foundation. The concepts are examined, best practices are studied,
    common errors and pitfalls are analyzed and a case study-based approach complements
    the learning. At the same time, we are developing actual Python code for solving
    such problems. All the codes are accompanied by step-by-step explanations and
    comments.
  prefs: []
  type: TYPE_NORMAL
- en: This book is divided into three parts. The first part explores the basics of
    unsupervised learning and covers easier concepts of k-means clustering, hierarchical
    clustering, principal component analysis, etc. This part gently prepares you for
    the journey ahead. If you are already well-versed in these topics, you can directly
    start with the second part of the book. It is advisable to give the chapters a
    quick read to refresh the concepts.
  prefs: []
  type: TYPE_NORMAL
- en: The second part is at an intermediate level. We start with association rules
    algorithms like apriori, ECLAT, and sequence rule mining. We then increase the
    pace and study more complex algorithms and concepts – spectral clustering, GMM
    clustering, t-SNE, multidimensional scaling (MDS), etc. And then we work on text
    data in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The third and final part is advanced. We are discussing complex topics like
    Restricted Boltzmann Machine, autoencoders, GANs, etc. We also examine end-to-end
    model development including model deployment, best practices, and common pitfalls
    in the last chapter of the book.
  prefs: []
  type: TYPE_NORMAL
- en: By the time you finish this book, you will have a very good understanding of
    unsupervised technique-based machine learning, various algorithms, mathematics
    and statistical foundation on which the algorithm rests, business use cases, Python
    implementation, and best practices followed.
  prefs: []
  type: TYPE_NORMAL
- en: This book is suitable for students and researchers who want to generate an in-depth
    understanding of unsupervised learning algorithms. It is recommended for professionals
    pursuing data science careers who wish to gather the best practices followed and
    solutions to common challenges faced. The content is well suited for managers
    and leaders who intend to have confidence while communicating with teams and clientele.
    Above all, a curious person who intends to get educated on unsupervised learning
    algorithms and develop Python experience to solve the case studies is well suited.
  prefs: []
  type: TYPE_NORMAL
- en: It is advisable that you have a basic understanding of programming in object-oriented
    languages like C++, Java, Objective-C, etc. We are going to use Python throughout
    the book, so if you are experienced with Python it will surely help. A basic understanding
    of mathematics and geometry will help in visualizing the results and some knowledge
    of data-related use cases will help to relate to the business use cases. Most
    important of all, an open mindset to absorb knowledge is necessary throughout
    the chapters in the book.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first chapter is designed to introduce the concepts of machine learning
    to you. In this opening chapter, we are going to cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Data, data types, data management, and quality
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data Analysis, Machine Learning, Artificial Intelligence, and Deep Learning
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Nuts and bolts of Machine Learning
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Different types of machine learning algorithms
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Technical tool kit available
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s first understand the smallest grain we have – “data” as the first topic.
    Welcome to the first chapter and all the very best!
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 Data, data types, data management, and quality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are starting with the protagonist of everything called “*data*”. Data can
    be termed as facts and statistics which are collected for performing any kind
    of analysis or study. But data has its own traits, attributes, quality measures,
    and management principles. It is stored, exported, loaded, transformed, and measured.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to study all of it now- starting with the definition of data. Then
    we will proceed to different types of data, their respective examples and what
    are the attributes of data which make it useful and of good quality.
  prefs: []
  type: TYPE_NORMAL
- en: 1.1.1 What is Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: “DATA” is ubiquitous. You make a phone call using a mobile network – you are
    generating data. You are booking a flight ticket and hotel for an upcoming vacation
    – data is being created. Making a bank transaction, surfing social media and shopping
    websites online, buying an insurance policy, or buying a car – everywhere data
    originates. It is transformed from one form to another, stored, cleaned, managed,
    and analyzed.
  prefs: []
  type: TYPE_NORMAL
- en: Formally put - data is a collection of facts, observations, measures, text,
    numbers, images, and videos. It might be clean or unclean, ordered or unordered,
    having mixed data types, or completely pure and historical or real-time.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.1 How we can transform raw data to become information, knowledge, and
    finally insights that can be used in business to drive decisions and actions
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![01_01](images/01_01.png)'
  prefs: []
  type: TYPE_IMG
- en: Data in itself is not useful till we clean it, arrange it, analyze it, and draw
    insights from it. We can visualize the transition in (Figure 1.1). Raw data is
    converted to information when we can find distinctions in it. When we relate the
    terms and *“connect the dots”,* the same piece of information becomes knowledge.
    Insight is the stage when we can find the major centers and significant points.
    An insight has to be actionable, succinct, and direct. For example, if a customer
    retention team of a telecom operator is told that customers who do not make a
    call for 9 days have 30% more chances of churn than those who do use, it will
    be a useful insight on which they can work and try to resolve. Similarly, if a
    line technician in a manufacturing plant is informed that using Mould ABC results
    in 60% more defects than if used with Mould PQR, they will refrain from using
    this combination. An insight is quite useful for a business team and hence they
    can take corrective measures.
  prefs: []
  type: TYPE_NORMAL
- en: We now know what is data. Let us study various types of data and their attributes
    and go deeper into data.
  prefs: []
  type: TYPE_NORMAL
- en: 1.1.2 Various Types of Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Data is generated across all the transactions we make, be it online mode or
    offline medium, as we discussed at the start of the section. We can broadly classify
    the data as shown in (Figure 1.2) below:'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.2 Data can be divided into quantitative and qualitative categories,
    which are further sub-classified
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![01_02](images/01_02.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Qualitative Data** is the data type that cannot be measured or weighed, for
    example, taste, color, odor, fitness, name, etc. They can only be observed subjectively.
    Formally put, when we categorize something or make a classification for it, the
    data generates is qualitative in nature. For example, colors in a rainbow, cities
    in a country, quality of a product, gender, etc. They are also called *categorical*
    variables. Qualitative data can be further sub-categorized into binary, nominal,
    and ordinal data sets.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Binary data, as the name suggests, has only two classes that are mutually exclusive
    to each other. For example, Yes/No, dry/wet, hard/soft, good/bad, true/false,
    etc.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Nominal data can be described as the type of data which though is categorized
    but does not have any sequence or order in it. For example, distinct languages
    are spoken in a country, colors in a rainbow, types of services available to a
    customer, cities in a country, etc.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ordinal data is similar to nominal data except we can order it in a sequence.
    For example, fast/medium/slow, positive/neutral/negative, etc.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Quantitative data:** All the types of data points which can be measured,
    weighed, scaled, recorded, etc. are quantitative. For example, height, revenue,
    number of customers, demand quantity, area, volume, etc. They are the most common
    form of data and allow mathematical and statistical operations on them too. Quantitative
    data is further sub-categorized as discrete and continuous:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Discrete data are precise, to-the-point, and integers. For example, the number
    of passengers in a plane or the population of a city cannot be in decimals.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Continuous data points can take any value, usually in a range. For example,
    height can take decimal values or the price of a product can need not be an integer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Any data point generally will fall in the above-discussed classes. These classes
    are based on the variable and its type. There is one more logical grouping that
    can be done using source and usage, which makes a lot of sense while solving business
    problems. This grouping allows us to design solutions customized to the data type.
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on the source and usage, we can also think of data in two broad classes:
    unstructured data and structured data.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Structured data**: dataset which can be represented in a row-column structure
    easily is a structured dataset. For example, transactions made by 5 customers
    in a retail store can be stored as shown in the table below (Table 1.1):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Table 1.1 An example of a structured dataset having attributes like amount,
    date, city, items, etc.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Customer ID | Txn Date | Amount ($) | # of items | Payment Mode | City |'
  prefs: []
  type: TYPE_TB
- en: '| 1001 | 01-June-2020 | 100 | 5 | Cash | New Delhi |'
  prefs: []
  type: TYPE_TB
- en: '| 1002 | 02-June-2020 | 101 | 6 | Card | New York |'
  prefs: []
  type: TYPE_TB
- en: '| 1003 | 03-June-2020 | 102 | 7 | Card | London |'
  prefs: []
  type: TYPE_TB
- en: '| 1004 | 04-June-2020 | 103 | 8 | Cash | Dublin |'
  prefs: []
  type: TYPE_TB
- en: '| 1005 | 05-June-2020 | 104 | 9 | Cash | Tokyo |'
  prefs: []
  type: TYPE_TB
- en: In the table above, for each unique customer ID, we have the transaction date,
    the amount spent in $, the number of items purchased, the model of payment, and
    the city in which the transaction has been made. Such data type can be extended
    to employee details, call records, banking transactions, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Most of the data used in analysis and model building is structured. Structured
    data is easier to store, analyze and visualize in the form of graphs and charts.
  prefs: []
  type: TYPE_NORMAL
- en: We have a lot of algorithms and techniques catering to structured data – in
    normal real-world language, we refer to structured data primarily.
  prefs: []
  type: TYPE_NORMAL
- en: '**Unstructured data**: Unstructured data can be text, audio, image, or video.
    The examples of unstructured data and their respective sources are given in (Figure
    1.3) below, where we explain the primary types of unstructured data: text, images,
    audio, and video along with their examples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 1.3 Unstructured data along with its various types and examples. This
    data is usually complex to analyze and generally requires deep learning-based
    algorithms
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![01_03](images/01_03.png)'
  prefs: []
  type: TYPE_IMG
- en: To be noted, our computers and processors understand only binary numbers. So
    these unstructured data points still need to be represented as numbers so that
    we can perform mathematical and statistical calculations on them. For example,
    an image is made up of pixels. If it is a colored image, each pixel will have
    RGB (red, green, blue) values and each of the RGB can take a value from (0-255).
    Hence we will be able to represent an image as a matrix on which further mathematical
    calculations can be made. Similarly, text, audio, and videos can be represented
    too.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Mostly deep learning-based solutions like CNN, and RNN are used for unstructured
    data. We are going to work on text and image data at a later stage in the book
  prefs: []
  type: TYPE_NORMAL
- en: The representation of unstructured data can be understood as shown below in
    an example in (Figure 1.4). We have shown a picture of a vacuum cleaner. A portion
    of the image if represented as a matrix, will look like this. It is only for illustration
    purposes and not the actual values.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.4 An image which is an example of unstructured data can be represented
    as a matrix to analyze
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![01_04](images/01_04.png)'
  prefs: []
  type: TYPE_IMG
- en: Similarly, we can have representations of text or audio, or video data. Owing
    to the size and large number of dimensions, unstructured data is complex to process
    and model, and hence mostly deep learning-based models serve the purpose. These
    deep learning models form the background of artificial intelligence-based solutions.
  prefs: []
  type: TYPE_NORMAL
- en: These are the broad types of data. We can have more categories like ratios or
    scales which can be used to define the relationship of one variable with another.
    All of these data points (whether structured or unstructured) are defined by the
    way they are generated in real life.
  prefs: []
  type: TYPE_NORMAL
- en: These all data points have to be captured, stored, and managed. There are quite
    a few tools available for managing data which we will be discussing in due course.
    But before that let us examine one of the most crucial but often less talked about
    subjects – *data quality*.
  prefs: []
  type: TYPE_NORMAL
- en: 1.1.3 Data quality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: “Garbage in, garbage out” – this principle summarises the importance of good
    quality data. If the data is not clean, usable, correct, and related, we will
    not be able to solve the business problem at hand. But what is the meaning of
    “good quality”? We have shown the major components of data quality in (Figure
    1.5) below, let’s explore them one by one.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.5 Data quality is of paramount importance, attributes of good quality
    data are shown
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![01_05](images/01_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The major attributes of good quality data are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Completeness**: we would expect our dataset to be proper and not missing
    any values. For example, if we are working on sales data for a year, good data
    will have all values for all 12 months. Then it will be a complete data source.
    The completeness of a dataset ensures that we are not missing an important variable
    or data point.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Valid**: validity of data is its conformance to the properties, characteristics,
    and variations that are present and being analyzed in our use case. Validity indicates
    if the observation and measurement we have captured are reliable and valid. For
    example, if the scope of the study is for 2015-2019, then using 2014 data will
    be invalid.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Accurate**: Accuracy is an attribute focussing on the correctness of data.
    If we have inaccurate data, we will generate inaccurate insights and actions will
    be faulty. It is a good practice to start the project by generating KPIs (key
    performance indicators) and comparing them with the numbers reported by the business
    to check the authenticity of the data available to us.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Representative**: It is one of the most important attributes of the data.
    And often most undermined too. Representation of data means that the data in use
    truly captures the business need and is not biased. If the dataset is biased or
    is not representative enough, the model generated will not be able to make predictions
    on the new and unseen data and the entire effort will go down the drain.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Available:** Non-availability of data is a challenge we face quite a lot.
    Data might not be available for the business problem and then we face a dilemma
    to continue the use case. Sometimes we face operational challenges and do not
    have access to the database or face permission issues or data might not be available
    at all for a particular variable since it is not captured. In such cases, we have
    to work with the data available to us and use surrogate variables. For example,
    imagine we are working on a demand generation problem. We want to predict how
    many customers can be expected during the upcoming sale season for a particular
    store. But we do not record the number of customers visiting for a few months.
    We can then use revenue as a surrogate field and synthesize the missing data points
    for us.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Consistent:** Here we check if the data points are consistent across systems
    and interfaces. It should not be the case that one system is reporting a different
    revenue figure while another system is showing a completely different value. When
    faced with such an issue, we generate the respective KPIs as per the data available
    to us and seek guidance from the business team.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Timeliness**: It simply means that we have all the data which is required
    at this point. If the data set is not available now but might become available
    in the future, then it might be prudent that we wait till then.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Integrity**: The data tables and variables we have are interlinked and interrelated
    to each other. For example, an employee’s details can be spread over multiple
    tables which are linked to each other using employee ID. Data integrity addresses
    this requirement and ensures that all such relations between the tables and respective
    entities are consistent.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Good quality of data is of paramount importance. In pragmatic day-to-day business,
    often we do *not* get good-quality data. Due to multiple challenges good clean
    data, which is accessible, consistent, representative, and complete is seldom
    found on the systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Degradation in quality can be due to challenges during data capturing and collection,
    exporting or loading, transformations done, etc. A few of the issues are listed
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: We can get integers as names, or special characters like “#$!&” in a few columns,
    or null values, blanks, or NaN (not a number) as some of the values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Duplicates in the records are also one of the issues we face.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Outliers are one of the nuisances we have to deal with quite a lot. For example,
    let’s say that the average daily transactions are 1000 for an online retailer.
    One fine day, due to a server problem there were no transactions done. It is an
    outlier situation. Or one fine day, the number of transactions was 1,000,000\.
    It is again an example of an outlier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then there are seasonal variations and movements concerning the time of the
    day and days of the week – all of them should be representative enough in the
    dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Inconsistencies in the date format lead to multiple challenges while we try
    to merge multiple data sources. Source 1 might be using DD/MM/YYYY while another
    might be using MM/DD/YYYY. It is to be taken care of during the data loading step
    itself.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All these aberrations and quality issues have to be addressed and cleaned thoroughly.
    We will be solving these data issues throughout the book and sharing the best
    practices to be followed.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The quality of your raw data and the rigor shown during the cleaning process
    - define the impact of your final analysis and the maturity of your solution.
  prefs: []
  type: TYPE_NORMAL
- en: We have now defined the major attributes of data. We will now study the broad
    process and techniques used for data engineering and management.
  prefs: []
  type: TYPE_NORMAL
- en: 1.1.4 Data engineering and management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A strong data engineering process and mature data management practice is a prerequisites
    for a successful machine learning model solution. Refer to Figure 1.6 below where
    the end-to-end journey of data is described – right from the process of data capturing,
    data pipeline, and data loading to the point it is ready for analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.6 Data engineering paves the way for data analysis. It involves data
    loading, transformation, enrichment, cleaning, preparation, etc. which leads to
    the creation of data ready for analysis
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![01_06](images/01_06.png)'
  prefs: []
  type: TYPE_IMG
- en: In the data engineering step, data is cleansed, conformed, reshaped, transformed,
    and ingested. Generally, we have a server where the final data is hosted and is
    ready for access. The most used process is the creation of an ETL (export, transform,
    load) process. Then we make the data ready for analysis. We create new variables,
    treat null values, enrich the data with methods, and then we finally proceed to
    the analysis/model-building stage.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: It will be a good idea to understand the basics of data engineering to complement
    the knowledge about data science as both go hand-in-hand
  prefs: []
  type: TYPE_NORMAL
- en: We have thus studied what is data and what are the qualities of good data to
    use. The data is used for analysis, modeling, visualization, dashboards, and insight
    generation. Many times, we find that terms like data analysis, data science, machine
    learning, data mining, artificial intelligence, business intelligence, big data,
    etc. are used quite interchangeably in the business. It will be a good idea to
    clarify them – which is the topic of the next section. There are plenty of tools
    available for each respective function, which we are discussing. And we will also
    understand the role of software engineering in this entire journey.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Data Analysis, Machine Learning, Artificial Intelligence, and Business Intelligence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data and its importance have opened new avenues and created a lot of job opportunities
    in the market. At the same time, machine learning and artificial intelligence
    are relatively new fields, and as such there is little standardization and differentiation
    in the scope of work. It has resulted in not-so-clear definitions and demarcation
    of these fields. We are examining these fields – where they overlap, where they
    differ, and how one empowers the other. It can be visualized by the means of a
    diagram below in (Figure 1.7):'
  prefs: []
  type: TYPE_NORMAL
- en: Each of the functions empowers each other and complements each other.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.7 How the various fields are interlinked with each other and how they
    are dependent on each other
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![01_07](images/01_07.png)'
  prefs: []
  type: TYPE_IMG
- en: Data mining and data engineering starts all of it by providing the data required
    for analysis. It also exports, transforms, cleans, and loads so that it can be
    consumed by all of the respective functions. Business intelligence and visualizations
    use this data to generate reports and dashboards. Data analytics generates insights
    and trends using data. Data Science stands on the pillars of data analysis, statistics,
    business intelligence, statistics, data visualization, machine learning, and data
    mining. Machine learning creates statistical and mathematical models. And artificial
    intelligence further pushes the capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning uses traditional coding. The coding is done in traditional
    languages and hence all the logics and rules of computer science and software
    engineering are valid in machine learning too. Machine learning helps us in making
    sense of the data that we are otherwise not able to comprehend. And in that aspect,
    it is a fantastic solution. It can relate to historical trends. The most fascinating
    advantage of machine learning is its ability to work on very complex and high-dimensional
    data points like video, audio, image, text, or complex datasets generated by sensors.
    It allows us to think beyond the obvious. Now artificial intelligence can achieve
    the feats which were previously thought not possible at all. Like self-driving
    cars, chatbots conversing like humans, speech-to-text conversion and translation
    to another language of choice, automated grading of essays, photo captioning,
    etc.
  prefs: []
  type: TYPE_NORMAL
- en: We are now clear on how the various fields are different from each other yet
    interlinked and how machine learning is different from traditional software engineering.
    It is the foundation of our topic of discussion, which goes deeper into machine
    learning and its various components, different types of machine learning algorithms,
    and their respective use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 Nuts and Bolts of Machine Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Consider this. If a child has to be taught how to open a door knob, we show
    her the exact steps quite a few times. The child tries to open it but fails. Tries
    again and fails again. But in each subsequent try, the child is improvising the
    approach. And after some time, the child can open the door knob. Or when we try
    to learn to drive, we make mistakes, we learn from them and we improve. *Machine
    Learning* works similarly - wherein the statistical algorithm looks at the historical
    data and finds patterns and insights. The algorithm uncovers relationships and
    anomalies, trends and deviations, similarities and differences – and then shares
    back actionable results with us.
  prefs: []
  type: TYPE_NORMAL
- en: Formally put, machine learning can be called a branch or a study of computer
    algorithms that works on historical data to generate insights and helps in making
    data-driven decisions. The algorithms are based on statistical and mathematical
    foundations and hence have a sound logical explanation. Machine Learning algorithms
    require coding which can be done in any of the languages and tools available viz.
    Python, R, SPSS, SAS, MATLAB, Weka, Julia, Java, etc. It also requires a domain
    understanding of the business.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Languages are only means to an end. All the languages generate similar results
    for a machine learning algorithm even if used across different languages.
  prefs: []
  type: TYPE_NORMAL
- en: So whenever you are doing some online shopping for a dress and the website recommends
    you accessories that go along with it, or you are booking an air ticket and the
    travel operator shows you a customized deal as per your needs and plan – machine
    learning is in the background. It has learned your preferences and compared them
    with your historical trends. It is also looking for similarities you have with
    other customers who behave almost the same. And based on all of that analysis,
    the algorithm is making an intelligent recommendation to you. Quite fascinating,
    right?
  prefs: []
  type: TYPE_NORMAL
- en: Many times we ask this question, why do we require machine learning, and why
    it surpasses human intelligence? The reason is, we humans can analyze only two
    or many be three dimensions simultaneously. But a machine learning algorithm can
    work on 50,60 or maybe 100s of dimensions simultaneously. It can work on any type
    of data, be it structured or unstructured, and can help in the automation of tasks.
    And hence it generates patterns and insights quite difficult for a human mind
    to visualize.
  prefs: []
  type: TYPE_NORMAL
- en: 'Machine Learning like any other project requires a team of experts who are
    working closely with each other and complementing each other’s skill sets. As
    shown in Figure 1.8 below, a machine learning project requires the following roles:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Business stakeholders and Subject Matter Experts (SME):** They define the
    business problem for the project. They own the solution, have a clear understanding
    of the ask, and have a clear measurable goal in sight. They course-correct the
    team in case of confusion and serve as an expert who has a deep understanding
    of the business processes and operations. They are marketing managers, product
    owners, process engineers, quality experts, risk analysts, portfolio leads, etc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: It is imperative that business stakeholders are closely knit into the team from
    day 1.
  prefs: []
  type: TYPE_NORMAL
- en: '**Operations Team:** This team comprises of the scrum master, project manager,
    business analysts, etc. The role of the team can be compared to a typical project
    management team which tracks the progress, maintains the records, reports the
    day-to-day activities, and keep the entire project on track. They create user
    stories and act as a bridge between the business team and the data team.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figure 1.8 Team required for a data science project and the respective interactions
    of them with each other
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![01_08](images/01_08.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Data Team:** The core team which creates the solution, does the coding, and
    generates the output in the form of a model, dashboard, report, and insights is
    the data team. It comprises of three main pillars: The data engineering, UI/Visualization
    team and the Data Science team. Their functions are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data engineering team is responsible for building, maintaining, integrating,
    and ingesting all the data points. They do a periodic data refresh and act as
    a prime custodian of data. They use ETL, SQL, AWS, Kafka, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: UI/ Visualisation team builds dashboards, reports, interactive modules, and
    web applications. They use SQL, and Tableau. Qlik, Power BI, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data Science team is responsible for all the data analysis and model building
    tasks. They discover patterns and insights, test hypotheses and generate the final
    output which is to be finally consumed by all. The final output can be a machine
    learning model which will be used to solve the business problem. In situations
    where a machine learning model is not possible, the team might generate actionable
    insights which can be useful for the business. This team requires SQL, Python,
    R, SAS, SPSS, etc. to complete their job.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have understood the typical team structure for a data science project. We
    will not examine what are the broad steps in a data science project.
  prefs: []
  type: TYPE_NORMAL
- en: A data science project runs like any other project having deadlines, stages,
    testing, phases, etc. The raw material is the data that passes through various
    phases to be cleaned, analyzed and modelled.
  prefs: []
  type: TYPE_NORMAL
- en: We are shown an illustration of a data science project stages in (Figure 1.9)
    below.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.9 Data science project is like any other project, having stages and
    deadlines, dependencies and processes
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![01_09](images/01_09.png)'
  prefs: []
  type: TYPE_IMG
- en: It starts with a business problem definition of the project. The business problem
    has to be concise, clear, measurable, and achievable. The table (Table 1-2) below
    depicts an example of a bad and a good business problem.
  prefs: []
  type: TYPE_NORMAL
- en: Table 1.2 Examples of how to define a business problem to make it clear, concise,
    and measurable
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Examples of an ill-defined business problem | Example of a good business
    problem |'
  prefs: []
  type: TYPE_TB
- en: '| Increase the production | Optimize the various cost heads (A, B, C, and D)
    and identify the most optimal combination to decrease the cost by 1.2% in the
    next 6 months |'
  prefs: []
  type: TYPE_TB
- en: '| Decrease the cost |'
  prefs: []
  type: TYPE_TB
- en: '| Increase the revenue by 80% in 1 month | From the various factors of defects
    in the process (X, Y, Z), identify the most significant factors to reduce the
    defect % by 1.8% in the next 3 months |'
  prefs: []
  type: TYPE_TB
- en: '| Automate the entire process |'
  prefs: []
  type: TYPE_TB
- en: Then we move to the data discovery phase during which we list down all the data
    sources and host them. All the various datasets like customer details, purchase
    histories, social media data, portfolios, etc. are identified and accessed. The
    data tables which are to be used are finalized in this step and most of the time,
    we create a database for us to work, test and learn.
  prefs: []
  type: TYPE_NORMAL
- en: We go ahead with data pre-processing. It involves cleaning data like the removal
    of null values, outliers, duplicates, junk values, etc. The previous step and
    this one can take 60-70% of the project time.
  prefs: []
  type: TYPE_NORMAL
- en: We create a few reports and generate initial insights during the exploratory
    data analysis phase. These insights are discussed with the business stakeholders
    and their guidance is taken for course correction.
  prefs: []
  type: TYPE_NORMAL
- en: The data is now ready for modeling. Quite a few versions of the solution are
    tested. And depending on the requirements we choose the best version. Mostly parameters
    like accuracy and statistical measures like precision, and recall drive the selection
    of the model. We will be exploring the process to choose the best model and terms
    like precision and recall in later chapters of the book.
  prefs: []
  type: TYPE_NORMAL
- en: The final model is chosen and now we are ready for deploying the model in the
    production environment where it will work on unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: These are the broad steps in a machine learning project. Like any other project,
    there is a code repository, best practices, coding standards, common errors, pitfalls,
    etc. which we are discussing throughout the book.
  prefs: []
  type: TYPE_NORMAL
- en: We will now move to one of the important topics which are types of machine learning
    algorithms, which we are discussing now.
  prefs: []
  type: TYPE_NORMAL
- en: 1.4 Types of Machine Learning Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Machine Learning models are impacting decision-making and follow a statistical
    approach to solve a business problem. It works on historical data and finds patterns
    and trends in it. The raw material is the historical data which is analyzed and
    modeled to generate a predictive algorithm. The historical data available and
    the business problem to be solved allow us to classify the machine learning algorithms
    in broadly four classes: **supervised learning, unsupervised learning, semi-supervised
    learning, and reinforcement learning** as depicted in Figure 1.10 below. We are
    examining all of the four types in detail now with a focus on unsupervised learning
    which is the topic of this book.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.10 Machine learning algorithms can be classified as supervised learning
    algorithms, unsupervised learning algorithms, semi-supervised learning algorithms
    and reinforcement learning algorithms
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![01_10](images/01_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 1.4.1 Supervised Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As the name suggests, supervised learning algorithms have a “guidance” or “supervision”
    to direct toward the business goal of predicting for the future.
  prefs: []
  type: TYPE_NORMAL
- en: Formally put, supervised models are statistical models which use both the input
    data and the desired output to predict the future. The output is the value that
    we wish to predict and is referred to as the *target variable* and the data used
    to make that prediction is called *training data*. The target variable is sometimes
    referred to as the *label*. The various attributes or variables present in the
    data are called *independent variables*. Each of the historical data points or
    a *training example* contains these independent variables and the corresponding
    target variable. Supervised learning algorithms make a prediction for unseen future
    data. The accuracy of the solution depends on the training done and patterns learned
    from the labeled historical data. An example to describe the concept is in the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning problems are used in demand prediction, credit card fraud
    detection, customer churn prediction, premium estimation, etc. They are heavily
    used across retail, telecom, banking and finance, aviation, insurance, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning algorithms can be further broken into regression algorithms
    and classification algorithms. We will first work with Regression problems.
  prefs: []
  type: TYPE_NORMAL
- en: Regression algorithms
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Regression algorithms are supervised learning algorithms i.e. they require target
    variables that need to be predicted. These algorithms are used to predict the
    values of a *continuous* variable. For example, revenue, amount of rainfall, number
    of transactions, production yield, and so on. In supervised classification problems,
    we predict a categorical variable like whether it will rain (yes/no), whether
    the credit card transaction is fraudulent or genuine, and so on. This is the main
    difference between classification and regression problems.
  prefs: []
  type: TYPE_NORMAL
- en: Let us understand the regression problem with an example. If we assume that
    the weight of a person is only dependent on height and not on other parameters
    like gender, ethnicity, diet, etc. In such a case, we want to predict the weight
    of a person based on height. The dataset can look like below and the graph plotted
    for the same data will look like as shown below in Figure 1.11.
  prefs: []
  type: TYPE_NORMAL
- en: A regression model will be able to find the inherent patterns in the data and
    fit a mathematical equation describing the relationship. It can then take height
    as an input and predict the weight. Here height is the independent variable and
    weight is the dependent variable or the target variable or the label we want to
    predict.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.11 Data and plot of relationship between height and weight which is
    used for regression problem
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![01_11](images/01_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are quite a few algorithms available for regression problems, the major
    ones are listed below:'
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decision tree
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Random forest
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: K-nearest neighbor
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Boosting algorithms
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Neural network
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can use any of the algorithms to solve this problem. We will explore more
    by using **linear regression** to solve this problem.
  prefs: []
  type: TYPE_NORMAL
- en: The linear regression algorithm models the relationship between dependent and
    target variables by assuming a linear relationship exists between them. The linear
    regression algorithm would result in a mathematical equation for the problem as
    shown in the equation (1-1)
  prefs: []
  type: TYPE_NORMAL
- en: (Equation 1.1)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Weight = β[0] * height + β[1]
  prefs: []
  type: TYPE_NORMAL
- en: Generally put, linear regression is used to fit a mathematical equation depicting
    the relationship between dependent and independent variables as shown in (1-2).
  prefs: []
  type: TYPE_NORMAL
- en: (Equation 1.2)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Y = β0 + β1 x1 + β2x2 + ….+ε
  prefs: []
  type: TYPE_NORMAL
- en: here Y is the target variable which we want to predict
  prefs: []
  type: TYPE_NORMAL
- en: x[1] is the first independent variable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: x[2] is the second independent variable
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: ε is the error term in the equation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: β[0] is the intercept of the equation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A simple visualization for a linear regression problem is shown in (Figure 1.12).
    Here, we have the x and Y variables where x is the independent variable and Y
    is the target variable. The objective of the linear regression problem is to find
    the *line of best fit* which is able to explain the randomness present in the
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.12 Raw data which needs to be modeled is on the left. Using regression,
    a line of best fit is identified
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![01_12](images/01_12.png)'
  prefs: []
  type: TYPE_IMG
- en: This equation is used to make predictions for the unseen data. There are variations
    in linear regression too like simple linear regression, multiple linear regression,
    non-linear regression, etc. Depending on the data at hand, we choose the correct
    algorithm. A complex dataset requires might require a non-linear relationship
    between the various variables.
  prefs: []
  type: TYPE_NORMAL
- en: The next regression algorithm we are discussing is **Tree based solutions**.
    For tree-based algorithms like decision trees, random forests etc. the algorithm
    will start from the top and then like an “if-else” block will split iteratively
    to create nodes and sub-nodes till we reach a terminal node. It can be understood
    better by means of Figure 1.13\. In the decision tree diagram, we start from the
    top with the root node, and then splitting is done till we reach the endpoint
    which is the terminal node.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.13 Decision tree has a root node and after splitting we get a decision
    node and terminal node is the final node which cannot be split further
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![01_13](images/01_13.png)'
  prefs: []
  type: TYPE_IMG
- en: A decision tree is very easy to comprehend and implement, and is fast to train.
    Their usability lies in the fact that they are intuitive enough to understand
    by everyone.
  prefs: []
  type: TYPE_NORMAL
- en: There are other famous regression algorithms like k-nearest neighbor, gradient
    boosting, and deep learning based solutions. Based on the business problem and
    respective accuracies we prefer one regression algorithm over another.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the impact of regression use cases, there are a few business-relevant
    use cases that are implemented in the industry:'
  prefs: []
  type: TYPE_NORMAL
- en: An airport operations team is doing an assessment of the staffing requirement
    and wants to estimate the number of passenger traffic expected. The estimate will
    help the team to prepare a plan for the future. It will result in the optimization
    of the resources required. Regression algorithms can be of help in predicting
    the passengers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A retailer wants to understand what is the expected demand for the upcoming
    sales season so that the inventory can be planned for various goods. It will result
    in cost savings and avoid stock-outs. Regression algorithms can help in such planning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A manufacturing plant wishes to improve the yield from the existing usage of
    various molds and raw materials. The regression solutions can suggest the best
    combination of molds and predict the expected yield too.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A bank offers credit cards to its customers. Consider how the credit limit offered
    to new customers is calculated. Based on the attributes of customers like age,
    occupation, income, and previous transaction history – regression algorithms can
    help in suggesting credit limits at a customer level.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An insurance company wishes to come up with a premium table for its customers
    using historical claims. The risk can be assessed based on the historical data
    around driver details, car information, etc. Regression can surely help with such
    problems.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Regression problems form the basics of supervised learning problems and are
    quite heavily used in the industry. Along with classification algorithms, they
    serve as a go-to solution for most of the predictive problems which we are discussing
    now.
  prefs: []
  type: TYPE_NORMAL
- en: Classification algorithms
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Simply put, classification algorithms are used to predict the values of a categorical
    variable which is the dependent variable. This target variable can be binary (Yes/No,
    good/bad, fraud/genuine, pass/fail, etc.) or multi-class (positive/negative/neutral,
    Yes/No/Don’t know, etc.). Classification algorithms will ascertain whether the
    target event will happen or not by generating a probability score for the target
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: After the model has been trained on historical data, a classification algorithm
    will generate a probability score for the unseen dataset which can be used to
    make the final decision. Depending on the number of classes present in the target
    variable, our business decision will vary.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s have a look at a use case for classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: Consider this. A telecom operator is facing an issue with its decreasing subscriber
    base. The number of existing subscribers is shrinking and the telecom operator
    would like to arrest this churn of subscribers. And for this purpose, a machine
    learning model is envisioned.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the historical data or the training data available for model building
    can look like the table below in (Table 1-3). These data points are only for illustration
    purposes and are not exhaustive. There can be many other significant variables
    available.
  prefs: []
  type: TYPE_NORMAL
- en: Table 1.3 Example of a structured dataset for a telecom operator showing multiple
    data attributes
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| ID | Revenue($) | Duration of service (years) | Avg. Cost | Monthly usage
    (days) | Churned (Y/N) |'
  prefs: []
  type: TYPE_TB
- en: '| 1001 | 100 | 1.1 | 0.10 | 10 | Y |'
  prefs: []
  type: TYPE_TB
- en: '| 1002 | 200 | 4.1 | 0.09 | 25 | N |'
  prefs: []
  type: TYPE_TB
- en: '| 1003 | 300 | 5.2 | 0.05 | 28 | N |'
  prefs: []
  type: TYPE_TB
- en: '| 1004 | 200 | 0.9 | 0.25 | 11 | Y |'
  prefs: []
  type: TYPE_TB
- en: '| 1005 | 100 | 0.5 | 0.45 | 12 | Y |'
  prefs: []
  type: TYPE_TB
- en: 'In the above example, the dataset comprises the past usage data of subscribers.
    The last column (Churned) depicts if that subscriber churned out of the system
    or now. Like subscriber # 1001 churned while 1002 did not. Hence the business
    problem is, we want to build a machine learning model based on this historical
    data and predict if a new unseen customer will churn or not.'
  prefs: []
  type: TYPE_NORMAL
- en: Here, the “churned” status (Yes/No) is the target variable. It is also referred
    as the dependent variable. The other attributes like revenue, duration, average
    cost, monthly usage, etc. are independent variables that are used to create the
    machine learning model. The historical data is called the training data. Post
    the training of the model, the trained supervised learning model will generate
    prediction probabilities for a new customer.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are quite a few algorithms available for classification problems, the
    major ones are listed below:'
  prefs: []
  type: TYPE_NORMAL
- en: Logistic Regression
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decision tree
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Random forest
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: K-nearest neighbor
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Naïve Bayes
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Support Vector Machine
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Boosting algorithms
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Neural network
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will discuss one of the most popular classification algorithms called **logistic
    regression**. Logistic regression uses a logit function to model the classification
    problem. If we are solving for a binary classification problem, it will be binary
    logistic regression else multiple logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to linear regression, logistic regression also fits an equation, albeit
    it uses a sigmoid function to generate the probability score for the event to
    happen or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'A sigmoid function is a mathematical function that has a characteristic “S”
    shaped curve or a sigmoid curve. The mathematical equation of a sigmoid function
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: S(x) = 1/(1 + e^x) which can be rewritten as S(x) = e^x/(e^x + 1)
  prefs: []
  type: TYPE_NORMAL
- en: 'The logistic regression uses the sigmoid function. The equation used in the
    logistic regression problem is:'
  prefs: []
  type: TYPE_NORMAL
- en: log (p/1-p) = β[0] + β[1] x[1]
  prefs: []
  type: TYPE_NORMAL
- en: 'where p: probability for the event to happen'
  prefs: []
  type: TYPE_NORMAL
- en: 'β[0] : intercept term'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'β[1] : coefficient for the independent variable x[1]'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: log(p/1-p) is called the logit and (p/1-p) is the odds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As depicted in Figure 1.14 below, if we try to fit a linear regression equation
    for the probability function, it will not do a good job. We want to obtain the
    probability scores (i.e. a value between 0 and 1). The linear regression will
    not only return values between 0 and 1 but also probability scores that are greater
    than 1 or less than 0\. Hence, we have a sigmoid function of the right which generates
    probability scores for us between 0 and 1 only.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.14 Linear regression model will not be able to do justice (left) hence
    we have logistic regression for classification. Linear regression can generate
    probability scores more than 1 or less than 0 too, which is mathematically incorrect.
    Whereas, the sigmoid function generates probability scores between 0 and 1 only.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![01_14](images/01_14.png)'
  prefs: []
  type: TYPE_IMG
- en: The logistic regression algorithm is one of the most widely used techniques
    for classification problems. It is easy to train and deploy and is often the benchmark
    algorithm whenever we start any supervised classification learning project.
  prefs: []
  type: TYPE_NORMAL
- en: Tree-based algorithms like decision trees, and random forest can also be used
    for classification problems. The other algorithms are also used as per the requirements.
  prefs: []
  type: TYPE_NORMAL
- en: We have studied supervised learning algorithms briefly. We will now discuss
    unsupervised learning algorithms in the next section – the main topic of this
    book and then move to semi-supervised learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 1.4.2 Unsupervised algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Imagine you are given some paper labels as shown in the figure Figure 1.15 below.
    The task is to arrange them using some similarities. Now there are multiple approaches
    to that problem. You can use color, shape, or size. Here we do not have any label
    with us to guide on this arrangement. This is the difference which unsupervised
    algorithm have.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.15 Example of various shapes which can be clubbed together using different
    parameters
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![01_15](images/01_15.png)'
  prefs: []
  type: TYPE_IMG
- en: Formally put, unsupervised learning only takes the input data and then finds
    patterns in them without referencing the target variable. An unsupervised learning
    algorithm hence reacts based on the presence or lack of patterns in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning is hence used for pattern detection, exploring the insights
    in the dataset and understanding the structure of it, segmentation, and anomaly
    detection.
  prefs: []
  type: TYPE_NORMAL
- en: We can understand unsupervised learning algorithms by the means of (Figure 1.16)
    below. The figure on the left shows the raw data points represented in a vector
    space diagram. On the right is the clustering done which will be done using unsupervised
    learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.16 Unsupervised learning algorithm find patterns in the data on the
    left and results in clusters on the right.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![01_16](images/01_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The use cases for unsupervised algorithms are:'
  prefs: []
  type: TYPE_NORMAL
- en: A retail group wants to understand the customers better. The task is to improve
    the customer’s stickiness, revenue, number of visits, basket size, etc. Customer
    segmentation using unsupervised learning can be done here. Depending on the customer’s
    attributes like revenue, number of visits, last visit date, age since joining,
    demographic attributes, etc. the segmentation will result in clusters that can
    be targeted personally. The result will be improved customer experience, increased
    customer lifetime value, etc.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A network provider requires to create an anomaly detection system. The historical
    data will serve as the anomalies data. The unsupervised learning algorithm will
    be able to find patterns and the outliers will be given out by the algorithm.
    The distinguished anomalies will be the ones that need to be addressed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A medical product company wishes to find if there is any underlying patterns
    in the image data of their patients. If there are any patterns and factors, those
    patients can be treated better and maybe they require a different kind of approach.
    Unsupervised learning can help on the images data which will help in addressing
    the patients better.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A digital marketing company wants to understand the “unknowns” in the incoming
    customer data like social media interactions, page clicks, comments, stars etc..
    The understanding will help in improving customer’s recommendations and overall
    purchasing experience.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unsupervised learning algorithms offer flexibility and performance when it comes
    to funding the patterns. They are usable for all kinds of data – structured data
    or text or images or text.
  prefs: []
  type: TYPE_NORMAL
- en: 'The number of unsupervised learning algorithms is lesser than supervised learning.
    The major unsupervised learning algorithms are:'
  prefs: []
  type: TYPE_NORMAL
- en: Clustering algorithms
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: k-means clustering
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hierarchical clustering
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DB Scan clustering
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Spectral clustering
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Principal component analysis
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Singular Value Decomposition
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Association rules
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: t-SNE (t-distributed stochastic neighbor embedding)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Autoencoders
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will be covering all of these algorithms in detail in the coming chapters.
    We will examine the mathematical concepts, the hidden processes, Python implementation,
    and the best practices throughout the book.
  prefs: []
  type: TYPE_NORMAL
- en: Let us understand by means of a case study.
  prefs: []
  type: TYPE_NORMAL
- en: A retailer wants to develop a deeper understanding of its consumer base. And
    then wants to offer personalized recommendations, promotions, discounts, offers,
    etc. The entire customer dataset has to be segmented using attributes like persona,
    previous purchase, response, external data, etc.
  prefs: []
  type: TYPE_NORMAL
- en: For the use case, the steps which are followed in an unsupervised learning project
    are shown in Figure 1.17 below.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1**: We start the project by defining the business problem. We wish
    to understand the customer base better. A customer segmentation approach can be
    a good solution. We want segments which are distinguishable using mathematical
    KPIs (key performance indicators).'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.17 Steps in an unsupervised learning algorithm from data sources to
    the final solution ready for deployment
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![01_17](images/01_17.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Step 2**: This is the data discovery phase. All the various datasets like
    customer details, purchase histories, social media data, portfolios, etc. are
    identified and accessed. The data tables to be used are finalized in this step.
    Then all the data tables are generally loaded to a common database, which we will
    use to analyze, test and learn.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3**: Now we have access to the data. The next step is to clean it and
    make is usable.'
  prefs: []
  type: TYPE_NORMAL
- en: We will treat all the null values, NAN, junk values, duplicates, etc.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 4**: Once the data is clean and ready to be used, we will perform an
    exploratory data analysis of it. Usually, during exploratory analysis, we identify
    patterns, cyclicity, aberrations, max-min range, standard deviation, etc. The
    outputs of EDA stage will be insights and understandings. We will also generate
    few graphs and charts as shown below in Figure 1.18:'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.18 Examples of the graphs and charts from the exploratory data analysis
    of the data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![01_18](images/01_18.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Step 5**: We will begin with the unsupervised approach now. We want to implement
    clustering methods and hence we can try a few clustering methods like k-means,
    hierarchical clustering, etc. The clustering algorithms will result in homogeneous
    segments of customers based on their various attributes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case study, we will be working on last 2-3 years of data which is the
    training data. Since we are using an unsupervised approach there is no target
    variable over here. The algorithm will merge the customer segments which behave
    alike using their transactional patterns, their demographic patterns and their
    purchase preferences. It will look like the figure below in Figure 1.19:'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.19 Output of the clustering algorithm where we can segment customers
    using various attributes
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![01_19](images/01_19.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Step 6**: we will now check how the various algorithms have performed or
    in other words we will compare the accuracy of each algorithm. The final clustering
    algorithm chosen will result in homogeneous segments of customers which can be
    targeted and offered customized offers.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 7**: we will discuss the results with the business stakeholders. Sometimes,
    utilizing the business acumen we merge or break a few segments.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 8**: Deploy the solution in production environment and we are ready
    to work on new unseen datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: These are the broad steps in an unsupervised problem. The algorithm creation
    and selection is a tedious task. We will be studying it in details in the book.
  prefs: []
  type: TYPE_NORMAL
- en: So far we have discussed supervised and unsupervised problem. Next, we move
    to semi-supervised algorithms which lie at juxtaposition of supervised and unsupervised
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 1.4.3 Semi-supervised algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Semi-supervised learning is a middle-path of the two approaches. The primary
    reason for a semi-supervised approach is lack of availability of a complete *labeled*
    dataset for training.
  prefs: []
  type: TYPE_NORMAL
- en: Formally put, the semi-supervised approach uses both supervised and unsupervised
    approaches – supervised to classify the data points and unsupervised to group
    them together.
  prefs: []
  type: TYPE_NORMAL
- en: In semi-supervised learning, we train initially on less number of labeled data
    points available using a supervised algorithm. And then we use it to label or
    *pseudo-label* new data points. The two datasets (labeled and pseudo-labeled)
    are combined together and we use this dataset further for analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Semi-supervised algorithms are used in cases where the dataset is partially
    available like images in the medical industry. If we are creating a cancer detection
    solution by analyzing the images of the patients, we will likely not have enough
    sample sets of training images. Here, the semi-supervised approach can be helpful.
  prefs: []
  type: TYPE_NORMAL
- en: Now we will discuss the last category in machine learning called reinforcement
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: 1.4.4 Reinforcement learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Imagine you are playing a game of chess with a computer. And it goes like this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Round 1: You win after 5 moves'
  prefs: []
  type: TYPE_NORMAL
- en: 'Round 2: You win after 8 moves'
  prefs: []
  type: TYPE_NORMAL
- en: 'Round 3: you win after 14 moves'
  prefs: []
  type: TYPE_NORMAL
- en: 'Round 4: you win after 21 moves'
  prefs: []
  type: TYPE_NORMAL
- en: 'Round 5: computer wins!'
  prefs: []
  type: TYPE_NORMAL
- en: What is happening here is, the algorithm is training itself iteratively depending
    on each interaction and correcting/improving itself.
  prefs: []
  type: TYPE_NORMAL
- en: Formally, reinforcement learning solutions are self-sustained solutions which
    train themselves using a sequence of trial and error. One sequence follows the
    other. The heart of reinforcement learning is reward signals. If the action is
    positive then the reward is positive indicating to continue on it. If the action
    is negative, the reward will penalize the activity. Hence, the solution will always
    correct itself and move ahead thereby improving itself iteratively.
  prefs: []
  type: TYPE_NORMAL
- en: Self-driving cars are the best examples for reinforcement learning algorithms.
    They detect when they have to turn left or right, when to move and when to stop.
    Modern video games also employ reinforcement learning algorithms. Reinforcement
    learning is allowing us to break the barriers of technology and imagine things
    which were earlier thought impossible.
  prefs: []
  type: TYPE_NORMAL
- en: With this, we have covered the different types of machine learning algorithms.
    Together, they are harnessing the true power of data and creating a long-lasting
    impact on our lives.
  prefs: []
  type: TYPE_NORMAL
- en: But the heart of the solutions is the technology, which we have not discussed
    yet. We will now move to the technology stack required to make these solutions
    tick.
  prefs: []
  type: TYPE_NORMAL
- en: 1.5 Technical toolkit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following tools are used for different facets of the project:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Data Engineering: Hadoop, Spark, Scala, Java, C++, SQL, AWS Redshift, Azure'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Data Analysis: SQL, R, Python, Excel'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Machine Learning: SQL, R, Python, Excel, Weka, Julia, MATLAB, SPSS, SAS'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Visualization: Tableau, Power BI, Qlik, COGNOS'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Model deployment: docker, flask, Amazon S3'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Cloud Services: Azure, AWS, GCP'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this book, we are going to work using Python. You are advised to install
    latest version of Python on your system. Python version of (3.5+) is advisable.
    We will be using Jupyter Notebook, hence it is advised to install Anaconda on
    your system.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: All the codes and datasets will be checked-in at the GitHub repository. You
    are expected to replicate them and try to reproduce the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'A most common question is: which is better R or Python? Both are fantastic
    languages. Both are heavily used. But recently after the introduction of TensorFlow,
    Keras libraries on Artificial Intelligence, the balance has slightly tilted in
    the favor of Python.'
  prefs: []
  type: TYPE_NORMAL
- en: With this, we conclude the discussion on technology. Technology along with the
    concepts make machine learning algorithms work for us. We will be exploring all
    of such finer aspects throughout the book.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have completed your very first step in your journey towards
    learning unsupervised machine learning techniques. It is time to wrap up and move
    to the summary.
  prefs: []
  type: TYPE_NORMAL
- en: 1.6 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Machine learning and artificial intelligence are indeed path-breaking. They
    are changing the way we travel, we order food, we plan, we buy, we see a doctor
    or order prescriptions – they are making a “dent” everywhere. Machine learning
    is indeed a powerful capability which is paving the path for the future and is
    proving much better than existing technology stacks when it comes to pattern identification,
    anomaly detection, customizations and automation of tasks. Autonomous driving,
    cancer detection, fraud identification, facial recognition, image captioning,
    and chat-bots are only a few examples where machine learning and artificial intelligence
    are outperforming traditional technologies. And now is the best time to enter
    this field. This sector is attracting investments from almost all the business
    functions. The field has created tons of job opportunities across the spectrum.
    Incredible and impressive indeed!
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, the field lacks trained professionals – data analysts, data
    engineers, visualization experts, data scientists, and data practitioners. They
    all are a rare breed now. The field requires a regular supply of budding talents
    who will become the leaders of tomorrow and will take data-driven-decisions. But
    we only scratched the surface in understanding the power of data – there are still
    miles to be covered.
  prefs: []
  type: TYPE_NORMAL
- en: In this introductory chapter of this book, we introduced concepts of machine
    learning, and data science to you. We compared various processes, what are steps
    in a data science project, and the team required for it. We examined types of
    machine learning algorithms with their respective use cases with an emphasis on
    unsupervised learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapter, we will dive deeper into unsupervised learning concepts
    of clustering. All the mathematical and statistical foundations, pragmatic case
    study, Python implementation are being discussed. The second chapter deals with
    easier clustering algorithms – K-means clustering, hierarchical clustering, and
    DBSCAN. In the later chapters of the book, we will study more complex clustering
    topics like GMM clustering, time series clustering, fuzzy clustering, etc.
  prefs: []
  type: TYPE_NORMAL
- en: You can now proceed to the question section now!
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Q1: Why is machine learning so powerful that it is being used very heavily
    now?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q2: What are the different types of machine learning algorithms and how are
    they different from each other?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q3: What are the steps in a machine learning project?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q4: What is the role of data engineering and why is it important?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q5: What are the various tools available for machine learning?'
  prefs: []
  type: TYPE_NORMAL
