- en: 5 Clustering (advanced)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 聚类 (高级)
- en: “Out of complexity, find simplicity– Einstein”
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: “在复杂中寻找简单- 爱因斯坦”
- en: Sometimes life is very simple, and sometimes we experience quite complex situations.
    We sail through both the situations and change our approach as per the situation.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 有时生活很简单，有时我们会遇到相当复杂的情况。我们在两种情况下都能应对，并根据情况调整我们的方法。
- en: In the Part one of the book we covered easier and simpler topics. It made you
    ready for the journey ahead. We are currently in Part two which is slightly more
    complex than Part one. Part three is more advanced than the first two parts. So,
    the level of difficulty will increase slightly with each and every chapter along
    with the expectations.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的第一部分中，我们涵盖了更简单、更简单的主题。这使您为前进的旅程做好了准备。我们目前处于第二部分，比第一部分稍微复杂一些。第三部分比前两部分更加高级。因此，随着每一章的进行，难度将稍微增加，期望也会增加。
- en: We studied clustering algorithms in part one of the book. We understand that
    clustering is an unsupervised learning technique where we wish to group the data
    points by discovering interesting patterns in the datasets. We went through the
    meaning of clustering solutions, different categories of clustering algorithm
    and a case study at the end. In that chapter, we explored kmeans clustering, hierarchical
    clustering and DBSCAN clustering in depth. We went through the mathematical background,
    process, Python implementation and pros and cons of each. Before starting this
    chapter, it is advisable to refresh chapter two.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本书的第一部分中学习了聚类算法。我们了解到聚类是一种无监督学习技术，我们希望通过发现数据集中的有趣模式来将数据点分组。我们深入研究了聚类解决方案的含义、聚类算法的不同类别以及最后的案例研究。在那一章中，我们深入探讨了
    kmeans 聚类、层次聚类和 DBSCAN 聚类的数学背景、过程、Python 实现以及优缺点。在开始本章之前，建议您复习第二章。
- en: Many times you might encounter datasets which do not conform to a simple shape
    and form. Moreover, we have to find the best fit before making a choice of the
    final algorithm we wish to implement. Here, we might need help of more complex
    clustering algorithms; the topic for the chapter. In this chapter, we are going
    to again study three such complex clustering algorithms – spectral clustering,
    Gaussian Mixture Models (GMM) clustering and fuzzy clustering. As always, Python
    implementation will follow the mathematical and theoretical concepts. This chapter
    is slightly heavy on the mathematical concepts. There is no need for you to be
    a PhD. in mathematics, but it is sometime important to understand how the algorithms
    work in the background. At the same time, you will be surprised to find that Python
    implementation of such algorithms is not tedious. This chapter is not having any
    case study.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会遇到许多次不符合简单形式和形状的数据集。此外，我们在选择最终要实现的算法之前必须找到最佳匹配项。在这里，我们可能需要更复杂的聚类算法的帮助；这就是本章的主题。在本章中，我们将再次研究三种这样的复杂聚类算法
    - 谱聚类、高斯混合模型 (GMM) 聚类和模糊聚类。一如既往，Python 实现将遵循数学和理论概念。本章在数学概念上稍微有些复杂。您不需要成为数学博士，但有时了解算法如何在后台工作是很重要的。与此同时，您会惊讶地发现这些算法的
    Python 实现并不乏味。本章没有任何案例研究。
- en: 'In this fifth chapter of the book, we are going to cover the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的第五章中，我们将涵盖以下主题：
- en: Spectral clustering
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 谱聚类
- en: Fuzzy clustering
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模糊聚类
- en: Gaussian Mixture Models (GMM) clustering
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 高斯混合模型 (GMM) 聚类
- en: Summary
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 总结
- en: Welcome to the fifth chapter and all the very best!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到第五章，祝你一切顺利！
- en: 5.1 Technical toolkit
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 技术工具包
- en: We will continue to use the same version of Python and Jupyter notebook as we
    have used so far. The codes and datasets used in this chapter have been checked-in
    at GitHub ([https://github.com/vverdhan/UnsupervisedLearningWithPython/tree/main/Chapter%205](main.html)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用到目前为止所使用的相同版本的 Python 和 Jupyter 笔记本。本章中使用的代码和数据集已经上传到 GitHub ([https://github.com/vverdhan/UnsupervisedLearningWithPython/tree/main/Chapter%205](main.html))。
- en: We are going to use the regular Python libraries we have used so far – numpy,
    pandas, sklearn, seaborn, matplotlib etc. You would need to install a few Python
    libraries in this chapter which are – skfuzzy and network . Using libraries, we
    can implement the algorithms very quickly. Otherwise, coding these algorithms
    is quite a time-consuming and painstaking task.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用到目前为止所使用的常规 Python 库 - numpy、pandas、sklearn、seaborn、matplotlib 等等。在本章中，您需要安装几个
    Python 库，它们是 - skfuzzy 和 network 。使用库，我们可以非常快速地实现算法。否则，编写这些算法是相当耗时且繁琐的任务。
- en: Let’s get started with a refresh of clustering!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始重新了解一下聚类！
- en: 5.2 Clustering
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 聚类
- en: Recall from chapter 2, clustering is used to group similar objects or data points.
    It is an unsupervised learning technique where we intend to find natural grouping
    in the data as shown in Figure 5-1.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾第二章，聚类用于将相似的对象或数据点分组。它是一种无监督学习技术，我们的目的是在数据中找到自然分组，如图5-1所示。
- en: Figure 5-1 Clustering of objected result into natural grouping.
  id: totrans-18
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5-1 将对象结果聚类为自然分组。
- en: '![05_01](images/05_01.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![05_01](images/05_01.png)'
- en: Here we can observe that on the left side we have ungrouped data and on the
    right side the data points have been grouped into logical groups. We can also
    observe that there can be two methodologies to do the grouping or *clustering*,
    and both result into different clusters. Clustering as a technique is quite heavily
    used in business solutions like customer segmentation, market segmentation etc.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以观察到，在左侧我们有未分组的数据，在右侧数据点已经被分组成逻辑组。我们还可以观察到，可以有两种方法来进行分组或*聚类*，而且两者都会产生不同的聚类。聚类作为一种技术，在业务解决方案中被广泛应用，如客户细分、市场细分等。
- en: We have understood kmeans, hierarchical and DBSCAN clustering in chapter 2\.
    We also covered various distance measurement techniques and indicators to measure
    the performance of clustering algorithms. You are advised to revisit the concepts.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第二章中理解了kmeans、层次和DBSCAN聚类。我们还涵盖了各种距离测量技术和用于衡量聚类算法性能的指标。建议您重新查看这些概念。
- en: In this chapter, we are focussing on advanced clustering methods. We will now
    start with Spectral clustering in the next section.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将专注于高级聚类方法。我们将在下一节开始使用谱聚类。
- en: 5.3 Spectral Clustering
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 谱聚类
- en: Spectral clustering is one of the unique clustering algorithms. There are some
    good quality research is done in this field. Revered researchers like Prof. Andrew
    Yang, Prof. Michael Jordan, Prof. Yair Weiss, Prof. Jianbo Shi, Prof. Jitendra
    Malik to name a few. We are quoting some of the papers in the last section of
    the chapter.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 谱聚类是独特的聚类算法之一。在这个领域进行了一些高质量的研究。像杨安东教授、迈克尔·乔丹教授、雅尔·韦斯教授、施坚博教授、杰特恩德拉·马利克教授等著名的研究人员。我们在本章的最后一节引用了一些论文。
- en: Let’s define spectral clustering first. Spectral clustering works on the affinity
    and not the absolute location of the data points for clustering. And hence, wherever
    the data is in complicated shapes, spectral clustering is the answer. We have
    shown a few examples in the Figure 5-2 where spectral clustering can provide a
    logical solution.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先定义谱聚类。谱聚类基于数据点的相似性而不是绝对位置进行聚类。因此，在数据处于复杂形状的任何地方，谱聚类都是答案。我们在图5-2中展示了一些谱聚类可以提供合理解决方案的示例。
- en: Figure 5-2 Examples of various complex data shapes which can be clustered using
    Spectral Clustering.
  id: totrans-26
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5-2 各种复杂数据形状的示例，可以使用谱聚类进行聚类。
- en: '![05_02](images/05_02.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![05_02](images/05_02.png)'
- en: For Figure 5-2, we could have used other algorithms like k-means clustering
    too. But they might not be able to do justice to such complicated shapes of the
    data. Formally put, algorithms like kmeans clustering utilize compactness of the
    data points. In other words, the closeness of the points to each other and compactness
    towards the cluster center, drive the clustering in kmeans. On the other hand,
    in Spectral clustering *connectivity* is the driving logic. In connectivity, either
    the data points are immediately close to one another or are connected in some
    way. The examples of such connectivity-based clustering have been depicted in
    Figure 5-2.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图5-2，我们也可以使用其他算法，如k均值聚类。但它们可能无法充分反映数据的复杂形状。正式地说，像kmeans聚类这样的算法利用数据点的紧密程度。换句话说，点之间的接近程度和向群集中心的紧密程度驱动kmeans中的聚类。另一方面，在谱聚类中，*连通性*是驱动逻辑。在连通性中，数据点要么彼此紧密相邻，要么以某种方式连接。这种基于连接性的聚类示例已在图5-2中描述。
- en: Look at the Figure 5-3(i) where the data points are in this doughnut pattern.
    There can be data points which can follow this doughnut pattern. It is a complex
    pattern and we have to cluster these data points. Imagine that by using a clustering
    method, the red circles are made a part of the same cluster, which is shown in
    Figure 5-3(ii). After all, they are close to each other. But if we look closely,
    the points are in a circle, are in a pattern and hence the actual cluster should
    be as shown in Figure 5-3(iii).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 观察图5-3(i)，数据点呈现出环形图案。可能有数据点遵循这种环形图案。这是一个复杂的模式，我们需要对这些数据点进行聚类。想象一下，通过使用聚类方法，红色圆圈被划分为同一簇，如图5-3(ii)所示。毕竟，它们彼此靠近。但是如果我们仔细观察，这些点是呈圆形排列的，存在一种模式，因此实际的簇应该如图5-3(iii)所示。
- en: Figure 5-3 (i) We can have such a complex representation of data points which
    need to be clustered. Observe the doughnut shape (ii) A very simple explanation
    can result in red dots being considered as a part of the same cluster but clearly,
    they are not part of the same cluster (iii) We have two circles over here. The
    points in the inner circle belong to one cluster whereas the outer points belong
    to another cluster
  id: totrans-30
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5-3 (i) 我们可以有这样一种复杂的数据点表示需要进行聚类。观察环形（ii）一个非常简单的解释可能导致将红点视为同一簇的一部分，但显然，它们并不属于同一簇（iii）我们这里有两个圆。内圆中的点属于一个簇，而外部的点属于另一个簇。
- en: '![05_03](images/05_03.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![05_03](images/05_03.png)'
- en: The example shown in Figure 5-3 is to depict the advantages with Spectral clustering.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图5-3中显示的示例旨在描述谱聚类的优势。
- en: Like we said earlier, spectral clustering utilizes connectivity approach in
    clustering. In spectral clustering, data points which are immediately next to
    each other are identified in a graph. These data points are sometimes referred
    to as *node*. These data points or nodes are then mapped to a low-dimensional
    space. A low dimensional space is nothing but having During this process, spectral
    clustering uses eigenvalues, affinity matrix, Laplacian matrix and degree matrix
    derived from the data set. The low-dimensional space can then be segregated into
    clusters.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所说的，谱聚类利用连接方法进行聚类。在谱聚类中，立即相邻的数据点在图中被识别出来。这些数据点有时被称为*节点*。然后，这些数据点或节点被映射到低维空间。低维空间实际上就是在这个过程中，谱聚类使用从数据集派生的特征值、亲和力矩阵、拉普拉斯矩阵和度矩阵。然后，低维空间可以被分成多个簇。
- en: Spectral clustering utilizes connectivity approach for clustering wherein It
    relies on graph theory wherein we identify clusters of nodes based on the edges
    connecting them.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 谱聚类利用连接方法进行聚类，它依赖于图论，在这里我们根据连接它们的边来识别节点的簇。
- en: We will study the process in detail. But before examining the process, there
    are a few important mathematical concepts which form the foundation of spectral
    clustering which we will cover now.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将详细研究这个过程。但在检查这个过程之前，有一些重要的数学概念构成了谱聚类的基础，我们现在将进行介绍。
- en: 5.3.1 Building blocks of Spectral Clustering
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.1 谱聚类的构建模块
- en: We know that the goal of clustering is to group data points which are similar
    into one cluster, while the data points which are not similar into another one.
    There are a few mathematical concepts we should be aware. We will start the concept
    of similarity graphs, which is quite an innate representation for data points.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道聚类的目标是将相似的数据点分组到一个簇中，而不相似的数据点分组到另一个簇中。我们应该了解一些数学概念。我们将从相似性图的概念开始，这是数据点的一种相当本质的表示。
- en: Similarity graphs
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 相似性图
- en: A graph is one of the easiest and intuitive method to represent data points.
    In the Figure 5-4(i), we are showing an example of a graph which is nothing but
    a connection between data points represented by the edge. Now two data points
    are connected if the similarity between them is positive or it is above a certain
    threshold which is shown in Figure 5-4(ii). Instead of absolute values for the
    similarity, we can use weights for the similarity. So, in Figure 5-4(ii), as point
    1 and 2 are similar as compared to point 1 and 3, the connection between point
    1 and 2 has a higher weight than point 1 and 3.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图是表示数据点的一种最简单和直观的方法之一。在图5-4(i)中，我们展示了一个图的示例，它只是数据点之间的连接，用边来表示。现在，如果两个数据点之间的相似性是正的，或者它高于某个阈值，那么它们就会连接起来，如图5-4(ii)所示。我们可以使用相似性的权重而不是绝对值。因此，在图5-4(ii)中，由于点1和2相对于点1和3更相似，因此点1和2之间的连接权重高于点1和3之间的连接权重。
- en: Figure 5-4(i) A graph is a simple representation of data points. The points
    or nodes are connected with each other if they are very similar (ii) The weight
    is higher if the similarity between data points is high else for dissimilar data
    points, the weight is less.
  id: totrans-40
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5-4(i) 图是数据点的简单表示。如果它们非常相似，则点或节点彼此连接（ii）如果数据点之间的相似性高，则权重较高，否则对于不相似的数据点，权重较低。
- en: '![05_04](images/05_04.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![05_04](images/05_04.png)'
- en: So, we can conclude that – using Similarity Graphs we wish to cluster the data
    points such that
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们可以得出结论——使用相似性图，我们希望对数据点进行聚类，使得
- en: the edges of the data points have higher value of weight and hence are similar
    to each other and so are in the same cluster.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据点的边具有较高的权重值，因此彼此相似，因此它们在同一簇中。
- en: the edges of the data points have lower values of weight and hence are not similar
    to each other and so they are in different clusters.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据点的边具有较低的权重值，因此彼此不相似，因此它们在不同的簇中。
- en: Apart from similarity graphs, we should also know the concept of **Eigen values
    and Eigen vectors** which we have covered in detail in the previous chapter. You
    are advised to refresh it. We will now move to the concept of Adjacency matrix.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 除了相似性图之外，我们还应该了解**特征值和特征向量**的概念，我们已经在前一章中详细介绍了。建议您进行复习。我们现在将转向邻接矩阵的概念。
- en: Adjacency Matrix
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 邻接矩阵
- en: Have a close look at Figure 5-5\. We can see those various points from 1 to
    5 are connected with each other. And then we are representing the connection in
    a matrix. That matrix is called *Adjacency Matrix*.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细看图5-5。我们可以看到从1到5的各个点彼此连接。然后我们在矩阵中表示连接。该矩阵称为*邻接矩阵*。
- en: Formally put, in adjacency matrix, the rows and columns are the respective nodes.
    The values inside the matrix represent the connection – if the value is 0 that
    means there is no connection and if the value is 1, it means there is a connection.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 正式地说，在邻接矩阵中，行和列分别是相应的节点。矩阵内部的值表示连接——如果值为0，则表示没有连接，如果值为1，则表示存在连接。
- en: Figure 5-5 Adjacency matrix represents the connection between various nodes,
    if the value is 1 that means the corresponding nodes in the row and column are
    connected. If the value is 0, it means they are not connected. For example, there
    is a connection between node 1 and node 5 hence the value is 1 while there is
    no connection between node 1 and node 4 hence the corresponding value is 0.
  id: totrans-49
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5-5 邻接矩阵表示各个节点之间的连接，如果值为1，则表示行和列中的相应节点连接。如果值为0，则表示它们不连接。例如，节点1和节点5之间存在连接，因此该值为1，而节点1和节点4之间没有连接，因此相应的值为0。
- en: '![05_05](images/05_05.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![05_05](images/05_05.png)'
- en: So for adjacency matrix, we are only concerned if there is a connection between
    two data points. If we extend the concept of adjacency matrix, we get degree matrix
    which is our next concept.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 所以对于邻接矩阵，我们只关心两个数据点之间是否存在连接。如果我们扩展邻接矩阵的概念，我们得到度矩阵，这是我们的下一个概念。
- en: Degree Matrix
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 度矩阵
- en: Formally put, a degree matrix is a diagonal matrix, where the degree of a node
    along the diagonal is the number of edges connected to it. If we use the same
    example as above, we can have the degree matrix as shown in Figure 5-6\. Node
    3 and 5 have three connections each and they are getting 3 as the values along
    the diagonal while the other nodes have only two connections each, so they have
    2 as the value along the diagonal.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 正式地说，度矩阵是一个对角矩阵，其中沿对角线的节点的度数是连接到它的边的数量。如果我们使用上述相同的示例，我们可以将度矩阵表示为图5-6所示。节点3和5各自有三个连接，它们在对角线上得到3作为值，而其他节点各自只有两个连接，因此它们在对角线上得到2作为值。
- en: Figure 5-6 While adjacency matrix represents the connection between various
    nodes, degree matrix is for the number of connections to each node. For example,
    node 5 has three connections and hence has 3 in front of it while node 1 has only
    two connections so it has 2.
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5-6 虽然邻接矩阵表示各个节点之间的连接，但度矩阵是每个节点的连接数。例如，节点5有三个连接，因此其前面有3，而节点1只有两个连接，所以它有2。
- en: '![05_06](images/05_06.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![05_06](images/05_06.png)'
- en: You might be wondering why do we use matrix? Matrix provide an elegant representation
    of the data and can clearly depict the relationships between two points.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想为什么我们要使用矩阵？矩阵提供了数据的优雅表示，并且可以清楚地描述两个点之间的关系。
- en: Now we have covered both adjacency matrix and degree matrix, we can move to
    Laplacian matrix.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了邻接矩阵和度矩阵，我们可以转向拉普拉斯矩阵。
- en: Laplacian Matrix
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 拉普拉斯矩阵
- en: There are quite a few variants of Laplacian matrix, but if we take the simplest
    form of Laplacian matrix, it is nothing but a subtraction of adjacency matrix
    from the degree matrix. In other words, L = D – A. We can show it as Figure 5-7.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 拉普拉斯矩阵有很多变体，但如果我们采用最简单的形式，即拉普拉斯矩阵是度矩阵减去邻接矩阵。换句话说，L = D – A。我们可以在图 5-7 中展示它。
- en: Figure 5-7 Laplacian matrix is quite simple to understand. To get a Laplacian
    matrix, we can simply subtract an adjacency matrix from the degree matrix as shown
    in the example above. Here, D represents the degree matrix, A is the adjacency
    matrix and L is the Laplacian matrix.
  id: totrans-60
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5-7 拉普拉斯矩阵相当容易理解。要获得拉普拉斯矩阵，我们只需将邻接矩阵从度矩阵中减去，如上例所示。这里，D 表示度矩阵，A 是邻接矩阵，L 是拉普拉斯矩阵。
- en: '![05_07](images/05_07.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![05_07](images/05_07.png)'
- en: Laplacian matrix is quite an important one and we use the eigen values of L
    to develop spectral clustering. Once we get the eigen values and eigen vectors,
    we can define two other values – spectral gap and Fielder value. The very first
    non-zero eigen value is the *Spectral Gap* which defines the density of the graph.
    The *Fielder value* is the second eigen value which provides us an approximation
    of the minimum cut required to separate the graph into two components. The corresponding
    vector for Fielder value is called the *Fielder vector*.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 拉普拉斯矩阵是非常重要的，我们使用 L 的特征值来开发谱聚类。一旦我们获得了特征值和特征向量，我们可以定义另外两个值 – 谱间隙和 Fielder 值。第一个非零特征值是
    *谱间隙*，它定义了图的密度。*Fielder 值* 是第二个特征值，它提供了将图分割成两个组件所需的最小切割的近似值。Fielder 值的相应向量称为 *Fielder
    矢量*。
- en: Fielder vector has both negative and positive components and their resultant
    sum is zero.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Fielder 矢量具有正负组件，它们的结果总和为零。
- en: We will use this concept once we study the process of Spectral clustering in
    detail in the next section. We will now cover one more concept of Affinity matrix
    before moving to the process of Spectral clustering.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节详细学习谱聚类的过程时使用这个概念。在转向谱聚类的过程之前，我们现在将介绍亲和力矩阵的另一个概念。
- en: Affinity Matrix
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 亲和力矩阵
- en: In the adjacency matrix, if we replace the number of connections with the similarity
    of the weights, we will get affinity matrix. If the points are completely dissimilar,
    the affinity will be 0 else if they are completely similar the affinity will be
    1\. The values in the matrix represent different levels of similarity between
    data points.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在邻接矩阵中，如果我们用权重的相似性替换连接的数量，我们将得到亲和力矩阵。如果点完全不相似，则亲和力为 0，否则如果它们完全相似，则亲和力为 1。矩阵中的值表示数据点之间不同水平的相似性。
- en: '![](images/tgt.png) POP QUIZ – answer these question to check your understanding..
    Answers at the end of the book'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '![](images/tgt.png) 快速测验 – 回答这些问题以检查你的理解。答案在书的末尾'
- en: 1.   The degree matrix is created by counting the number of connections. True
    or False.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 1.   度矩阵是通过计算连接数量创建的。真还是假。
- en: 2.   Laplacian is a transpose of the division of degree and adjacency matrix.
    True or False.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 2.   拉普拉斯矩阵是否是度和邻接矩阵之间的转置的除法。真还是假。
- en: 3.   Write a matrix on a paper and then derive its adjacency and degree matrix.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 3.   在纸上写下一个矩阵，然后推导出它的邻接和度矩阵。
- en: We have now covered all the building blocks for Spectral clustering. We can
    now move to the process of Spectral clustering.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经掌握了谱聚类的所有构建模块。我们现在可以转向谱聚类的过程。
- en: 5.3.2 Process of Spectral Clustering
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.2 谱聚类的过程
- en: 'Now we have covered all the building blocks for Spectral clustering. At a high
    level, the various steps can be noted as:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经掌握了谱聚类的所有构建模块。在高层次上，各个步骤可以总结如下：
- en: We get the dataset and calculate its degree matrix and Adjacency matrix.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们获得数据集并计算其度矩阵和邻接矩阵。
- en: Using them, we get the Laplacian matrix.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用它们，我们得到拉普拉斯矩阵。
- en: Then we calculate the first k eigen vectors of the Laplacian matrix. The k eigenvectors
    are nothing but the ones which correspond to the k smallest eigen values.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们计算拉普拉斯矩阵的前 k 个特征向量。k 个特征向量实际上就是对应于 k 个最小特征值的向量。
- en: The matrix such formed is used to cluster the data points in k-dimensional space.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这样形成的矩阵用于在 k 维空间中对数据点进行聚类。
- en: We will now cover the process of Spectral clustering using an example as shown
    in Figure 5-8\. These are the steps which are generally not followed in real-world
    implementation as we have packages and libraries to achieve it. These steps are
    covered here to give you the idea of how the algorithm can be developed from scratch.
    For the Python implementation, we will use the libraries and packages only. Though
    it is possible to develop an implementation from scratch, it is not time efficient
    to reinvent the wheel.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将介绍使用示例来覆盖谱聚类的过程，如图 5-8 所示。这些步骤通常在实际实现中不会遵循，因为我们有包和库来实现它。这里涵盖了这些步骤，以便让您了解如何从零开始开发算法的想法。对于
    Python 实现，我们将仅使用库和包。虽然可能从头开始开发实现，但重新发明轮子不够高效。
- en: Figure 5-8 Consider the example shown where we have some data points and they
    are connected with each other. We will perform Spectral clustering on this data.
  id: totrans-79
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5-8 考虑所示的示例，其中我们有一些数据点，它们彼此连接。我们将对这些数据执行谱聚类。
- en: '![05_08](images/05_08.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![05_08](images/05_08.png)'
- en: Now when we wish to perform the spectral clustering on this data.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在当我们希望对这些数据进行谱聚类时。
- en: We will leave it upto you to create the adjacency matrix and degree matrix.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将把创建邻接矩阵和度矩阵的工作留给您。
- en: The next step is creating the Laplacian matrix. We are sharing the output Laplacian
    matrix in Figure 5-9.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是创建拉普拉斯矩阵。我们在图 5-9 中分享了输出的拉普拉斯矩阵。
- en: Figure 5-9 Laplacian matrix of the data is shown here. You are advised to create
    the degree and adjacency matrix and check the output.
  id: totrans-84
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5-9 展示了数据的拉普拉斯矩阵。建议您创建度矩阵和邻接矩阵并检查输出。
- en: '![05_09](images/05_09.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![05_09](images/05_09.png)'
- en: Now, the Fielder vector is shown in Figure 5-10 for the above Laplacian matrix.
    We create the Fielder vector as described in the last section. Observe how the
    sum of the matrix is zero.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，Fielder 向量如图 5-10 所示，用于上述拉普拉斯矩阵。我们创建 Fielder 向量，如上一节所述。观察矩阵的和为零。
- en: Figure 5-10 Fielder vector is the output for the Laplacian matrix, observe that
    the sum is zero here.
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5-10 Fielder 向量是拉普拉斯矩阵的输出，在这里观察到矩阵的和为零。
- en: '![05_10](images/05_10.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![05_10](images/05_10.png)'
- en: We can see that there are a few positive values and a few negative values, based
    on which we can create two distinct clusters. This is a very simple example to
    illustrate the process of Spectral Clustering.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以看到有一些正值和一些负值，根据这些值，我们可以创建两个不同的簇。这是一个非常简单的示例，用来说明谱聚类的过程。
- en: Figure 5-11 The two clusters are identified. This is a very simple example to
    illustrate the process of Spectral Clustering.
  id: totrans-90
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5-11 识别了两个簇。这是一个非常简单的示例，用来说明谱聚类的过程。
- en: '![05_11](images/05_11.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![05_11](images/05_11.png)'
- en: The above process is a very simple representation of Spectral Clustering.  Spectral
    clustering is useful for image segmentation, speech analysis, text analytics,
    entity resolution etc. It is quite easy and intuitive method and does not make
    any assumption about the shape of the data. Methods like kmeans assume that the
    points are in a spherical form around the center of the cluster whereas there
    is no such strong assumption in Spectral clustering.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 上述过程是谱聚类的一个非常简单的表示。谱聚类对图像分割、语音分析、文本分析、实体解析等非常有用。这是一种非常简单直观的方法，不对数据的形状做出任何假设。像
    kmeans 这样的方法假设点在聚类中心周围呈球形分布，而在谱聚类中没有这样的强假设。
- en: Another significant difference is that in spectral clustering the data points
    need not have convex boundaries as compared to other methods where compactness
    drives clustering. Spectral clustering is sometimes slow since eigne values, Laplacians
    etc. have to be calculated. With a large dataset the complexity increases and
    hence Spectral clustering can become slow, but it a fast method when we have a
    sparse dataset.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的区别是，在谱聚类中，与其他方法相比，数据点不需要具有凸边界，其中紧凑性驱动聚类。谱聚类有时会很慢，因为需要计算特征值、拉普拉斯等。随着数据集的增大，复杂性增加，因此谱聚类可能会变慢，但是当我们有一个稀疏的数据集时，它是一种快速的方法。
- en: We will now proceed to Python implementation of Spectral Clustering algorithm.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将进行谱聚类算法的 Python 实现。
- en: 5.2.1     Python implementation of Spectral Clustering
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.1     谱聚类的 Python 实现
- en: We have covered so far, the theoretical details of Spectral Clustering, it is
    time to get into the code. For this, we will curate a dataset and run k-means
    algorithm. And then Spectral Clustering to compare the results.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经涵盖了谱聚类的理论细节，现在是时候进入代码了。为此，我们将创建一个数据集并运行 k-means 算法，然后使用谱聚类来比较结果。
- en: 'Step 1: Import all the necessary libraries first. These libraries are standard
    libraries except a few which we will cover. `sklearn` is one of the most famous
    and sought-after libraries and from `sklearn` we are importing `SpectralClustering`,
    `make_blobs` and `make_circles`.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步：首先导入所有必要的库。这些库是标准库，除了我们将要介绍的几个。`sklearn` 是最著名和最受欢迎的库之一，我们从中导入 `SpectralClustering`、`make_blobs`
    和 `make_circles`。
- en: '[PRE0]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Step 2: We will now curate a dataset. We are using `make_circles` method. Here
    we are taking 2000 samples and representing them in a circle. The output is shown
    below.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步：我们现在将创建一个数据集。我们使用的是 `make_circles` 方法。在这里，我们取 2000 个样本，并将它们表示成一个圆。结果如下所示。
- en: '[PRE1]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![05_11a](images/05_11a.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![05_11a](images/05_11a.png)'
- en: 'Step 3: We will now test this dataset with kmeans clustering. The two colors
    are showing two different clusters which are overlapping with each other.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 第三步：我们现在将使用kmeans聚类测试这个数据集。两种颜色显示两个不同的重叠的簇。
- en: '[PRE2]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![05_11b](images/05_11b.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![05_11b](images/05_11b.png)'
- en: 'Step 4: We will now run the same data with Spectral Clustering and we find
    that the two clusters are being handled separately here.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 第四步：我们现在用谱聚类运行相同的数据，发现两个簇在这里被单独处理。
- en: '[PRE3]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![05_11c](images/05_11c.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![05_11c](images/05_11c.png)'
- en: We can observe here that the same dataset is handled differently by the two
    algorithms. Spectral clustering is handling the dataset better as the circles
    which are separate are depicted separately.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到同一数据集在两个算法中的处理方式不同。谱聚类在处理分离的圆圈时表现更好，分离的圆圈被独立的表示出来。
- en: 'Step 5: You are advised to simulate various cases by changing the values in
    the dataset and run the algorithms. Observe the different outputs for comparison.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 第五步：你可以尝试改变数据集中的值并运行算法，模拟不同的情况，进而比较结果。
- en: With this we have finished the first algorithm in this chapter. We will now
    move to Fuzzy Clustering in the next section.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 第一部分的算法已经讲解完成。下一部分我们会讲解模糊聚类。
- en: 5.3 Fuzzy Clustering
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3 模糊聚类
- en: So far, we have covered quite a few clustering algorithms. Did you wonder that
    why a data point should belong to only one cluster? Why can’t a data point belong
    to more than one clusters? Have a look at Figure 5-12.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经涵盖了相当多的聚类算法。你是否想过为什么一个数据点只能属于一个聚类？为什么一个数据点不能属于多个聚类？看一下图5-12。
- en: Figure 5-12 The figure of the left represents all the data points. The red points
    can belong to more than one clusters. In fact, we can allocate more than one cluster
    to each and every point. A probability score can be given for a point to belong
    to a particular cluster.
  id: totrans-113
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5-12 左侧的图表示所有数据点。红点可以属于多个聚类。实际上，我们可以给每个点分配多个聚类。可以给一个点赋予属于特定聚类的概率分数。
- en: '![05_12](images/05_12.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![05_12](images/05_12.png)'
- en: We know that clustering is used to group items in cohesive groups based on the
    similarities between them. The items which are similar are in one cluster, whereas
    the items which are dissimilar are in different clusters. The idea of clustering
    is to ensure the items in same cluster should be as much similar to each other.
    When the items can be only in one cluster, it is called as *hard clustering.*
    K-means clustering is a classic example of hard clustering. But if we reflect
    back on the Figure 5-12, we can observe that an item can belong to more than one
    clusters. It is also called *soft clustering.*
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道聚类是根据对象之间的相似度将其分为同一个组的方法。相似的项在同一簇中，而不同的项在不同的簇中。聚类的思想是确保同一簇中的项应该尽可能相似。当对象只能在一个簇中时，称为*硬聚类*。K-means聚类是硬聚类的经典例子。但是，如果回顾图5-12，我们可以观察到一个对象可以属于多个簇。这也被称为*软聚类*。
- en: It is computationally cheaper to create fuzzy boundaries than create hard clusters.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 创建模糊边界比创建硬聚类更经济。
- en: In fuzzy clustering, an item can be assigned to more than one cluster. The items
    which are closer to the center of a cluster, might be the part of the cluster
    to a higher degree than the items which are near the cluster’s edge. It is referred
    as *membership*. It employs least-square solutions to the most optimal location
    of an item. This optimal location might be the probability space between the two
    or more clusters. We will examine this concept in detail when we study the process
    of fuzzy clustering in detail and now, we will move to types of fuzzy clustering
    algorithms.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在模糊聚类中，一个项目可以被分配给多个聚类。靠近聚类中心的项目可能比靠近聚类边缘的项目更多地属于该聚类。这被称为*成员关系*。它采用最小二乘解决方案找到一个物品的最佳位置。这个最佳位置可能是两个或多个聚类之间的概率空间。我们将在详细介绍模糊聚类过程时详细讨论这个概念，现在我们将转向模糊聚类算法的类型。
- en: 5.3.3 Types of Fuzzy Clustering
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.3 模糊聚类的类型
- en: Fuzzy clustering can be further divided between classical fuzzy algorithms and
    shape-based fuzzy algorithms which we are showing by means of a diagram Figure
    5-13.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 模糊聚类可以进一步分为经典模糊算法和基于形状的模糊算法，我们通过图5-13来展示。
- en: Figure 5-13 Fuzzy algorithms can be divided into Classical Fuzzy algorithm and
    Shape-based fuzzy algorithm.
  id: totrans-120
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5-13 模糊算法可以分为经典模糊算法和基于形状的模糊算法。
- en: '![05_13](images/05_13.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![05_13](images/05_13.png)'
- en: We will cover the Fuzzy c-means algorithm in detail here. Rest of the algorithms
    we will cover in brief.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里详细介绍模糊c均值算法。其他算法我们将简要介绍。
- en: '*Gaustafson-Kessel algorithm* or sometimes called as GK algorithm works by
    associating an item with a cluster and a matrix. GL results in elliptical clusters
    and in order to modify as per varied structures in the datasets GK uses the covariance
    matrix. It allows the algorithm to capture the elliptical properties of the cluster.
    GK can result in narrower clusters and wherever the number of items is higher,
    those areas can be thinner.'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*高斯塔夫松-凯塞尔算法*，有时称为GK算法，通过将一个项目与一个聚类和一个矩阵相关联。通过使用协方差矩阵，GL会生成椭圆形聚类，并根据数据集中的各种结构进行修改。它允许算法捕捉聚类的椭圆形属性。GK可以导致更窄的聚类，并且在项目数较多的地方，这些区域可能更加薄。'
- en: Gath-Geva algorithm is not based on an objective function. The clusters can
    result in any shape since it is a fuzzification of statistical estimators.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Gath-Geva算法不基于目标函数。聚类可以导致任何形状，因为它是统计估计值的模糊化。
- en: The shape based clustering algorithms are self-explanatory as per their names.
    A circular fuzzy clustering algorithm will result in circular shaped clusters
    and so on.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于形状的聚类算法根据其名称自解释。圆形模糊聚类算法将导致圆形的聚类，依此类推。
- en: Fuzzy c-means algorithm or FCM algorithm is the most popular fuzzy clustering
    algorithm. It was initially developed in 1973 by J.C. Dunn and then it has been
    improved multiple times. It is quite similar to k-means clustering. There is a
    concept of membership which we will cover now.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模糊c均值算法或FCM算法是最流行的模糊聚类算法。它最初由J.C. Dunn于1973年开发，之后进行了多次改进。它与k均值聚类非常相似。这里有一个成员关系的概念，我们将在下面介绍。
- en: Refer to Figure 5-14\. In the first figure, we have some items or data points.
    These data points can be a part of a clustering dataset like customer transactions
    etc. In the second figure, we create a cluster for these data points. While this
    cluster is created, membership grades are allocated to each of the data points.
    These membership grades suggest the degree or the level to which a data point
    belong to a cluster. We will shortly examine the mathematical function to calculate
    these values.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 参考图5-14。在第一幅图中，我们有一些项目或数据点。这些数据点可以是聚类数据集的一部分，如客户交易等。在第二幅图中，我们为这些数据点创建了一个聚类。在创建该聚类时，为每个数据点分配成员关系成绩。这些成员关系成绩表明数据点属于聚类的程度或级别。我们将很快介绍计算这些值的数学函数。
- en: We should not be confused between the degree and the probabilities. If we sum
    these degrees, we may not get 1 as these values are normalized between 0 and 1
    for all the items.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不应该混淆程度和概率。如果我们对这些程度进行求和，可能得不到1，因为这些值在0和1之间对所有项目进行了归一化。
- en: In the third figure, we can observe and compare that the point 1, is closer
    to the cluster center and hence belong to the cluster to a higher degree than
    point 2 which is closer to the boundary or the edge of the cluster.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三幅图中，我们可以观察和比较点1靠近聚类中心，因此比点2更多地属于该聚类的程度，而点2靠近聚类的边界或边缘。
- en: Figure 5-14 (i) We have some data points here which can be clustered (ii) The
    data points can be grouped into two clusters. For the first cluster, the cluster
    centroid is represented using a + sign. (iii) We can observe here that point 1
    is much closer to the cluster center as compared to point 2\. So, we can conclude
    that point 1 belongs to this cluster to a higher degree than cluster 2.
  id: totrans-130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5-14 (i) 我们这里有一些数据点，可以被聚类 (ii) 数据点可以被分成两个簇。对于第一个簇，聚类中心用加号表示。(iii) 我们可以观察到这里点
    1 相对于点 2 更接近聚类中心。因此，我们可以得出结论，点 1 属于这个聚类的程度高于聚类 2。
- en: '![05_14](images/05_14.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![05_14](images/05_14.png)'
- en: We will now venture into the technical details of the algorithm. It might be
    a little mathematically heavy though and hence this section can be treated as
    optional.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将深入探讨算法的技术细节。尽管可能有点数学重，但本节可以作为可选内容。
- en: Consider we have a set of *n* items
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一组 *n* 个项目
- en: X = {x[1], x[2], x[3], x[4], x[5]…. x[n]}
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: X = {x[1], x[2], x[3], x[4], x[5]…. x[n]}
- en: We apply FCM algorithm on these items. These n items are clustered into *c*
    fuzzy clusters based on some criteria. Let’s say that we will get from the algorithm,
    a list of c cluster centers as    C = {c[1], c[2], c[3], c[4], c[5]…. c[c]}
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 FCM 算法应用于这些项。这些 n 项根据某些标准被聚类成 *c* 个模糊聚类。假设我们从算法中获得了一个 c 个聚类中心的列表，表示为 C =
    {c[1], c[2], c[3], c[4], c[5]…. c[c]}
- en: The algorithm also returns a partition matrix which can be defined as below.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 算法还返回一个可以定义为下述的划分矩阵。
- en: '![05_00](images/05_00.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![05_00](images/05_00.png)'
- en: Here, each of the element w[i,j] is the degree to which each of the element
    in X belong to cluster c[j]. This is the purpose of partition matrix.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，每个元素 w[i,j] 表示元素 X 中每个元素属于聚类 c[j] 的程度。这是划分矩阵的目的。
- en: Mathematically, we can get w[i,j] as shown in Equation 5-1\. The proof of the
    equation is beyond the scope of the book.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 数学上，我们可以得到 w[i,j] 如方程式 5-1 所示。方程的证明超出了本书的范围。
- en: '![05_14a](images/05_14a.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![05_14a](images/05_14a.png)'
- en: The algorithm generates centroids for the clusters too. The centroid of a cluster
    is the mean of all the points in that cluster and the mean is weighted by their
    respective degrees of belonging to that cluster. wherein If we represent it mathematically,
    we can write it like in Equation 5-2.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法还为聚类生成聚类中心。聚类的中心是该聚类中所有点的平均值，平均值由它们各自的属于该聚类的程度加权得到。如果我们用数学表示，我们可以写成方程式 5-2。
- en: '![05_14b](images/05_14b.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![05_14b](images/05_14b.png)'
- en: In the Equation 5-1 and 5-2 we have a very important term “m”. m is the hyperparameter
    which is used to control the fuzziness of the clusters. The values for m ≥ 1 and
    can be kept as 2 generally.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在方程式 5-1 和 5-2 中，有一个非常重要的术语“m”。m 是用于控制聚类模糊性的超参数。m 的值 ≥ 1，通常可以保持为 2。
- en: Higher the value of m, we will receive fuzzier clusters.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: m 值越高，我们将获得更模糊的聚类。
- en: 'We will now examine step-by-step process in FCM algorithm:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将逐步检查 FCM 算法的过程：
- en: First, we start as we start in k-means clustering. We choose the number of clusters
    we wish to have in the output.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们像 k-means 聚类一样开始。我们选择我们希望在输出中拥有的聚类数量。
- en: Then the coefficients are allocated randomly to each of the data points.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，将系数随机分配给每个数据点。
- en: Now we wish to iterate till the algorithm has converged. Recall how the k-means
    algorithm converges, wherein we initiate the process by randomly allocating the
    number of clusters. And then iteratively we calculate the centroid for each of
    the clusters. This is how kmeans converges. For FCM, we will utilizing the similar
    process albeit with slight differences. We have added a membership value w[i,j]
    and m.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们希望迭代直到算法收敛。回想一下 k-means 算法如何收敛，我们通过随机分配聚类数量来启动该过程。然后，我们迭代地计算每个聚类的中心。这就是
    kmeans 如何收敛的。对于 FCM，我们将使用类似的过程，尽管有细微的差异。我们增加了一个成员值 w[i,j] 和 m。
- en: For FCM, for the algorithm to converge we calculate the centroid for each of
    the cluster as per Equation 5-2.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 FCM，为了使算法收敛，我们根据方程式 5-2 计算每个聚类的中心。
- en: '![05_14c](images/05_14c.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![05_14c](images/05_14c.png)'
- en: For each of the data points, we also calculate its respective coefficient for
    being in that particular cluster. We will use Equation 5-1.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个数据点，我们还计算其在特定聚类中的系数。我们将使用方程式 5-1。
- en: Now we have to iterate until FCM algorithm has converged. The cost function
    which we wish to minimize is given by the ()
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们必须迭代直到 FCM 算法收敛。我们希望最小化的成本函数由 () 给出。
- en: '![05_14d](images/05_14d.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![05_14d](images/05_14d.png)'
- en: Once this function has been minimized, we can conclude that that the FCM algorithm
    has converged. Or in other words, we can stop the process as the algorithm has
    finished processing.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦这个函数被最小化，我们就可以得出结论，即FCM算法已经收敛。换句话说，我们可以停止进程，因为算法已经完成处理。
- en: It will be a good stage now to compare with kmeans algorithm. In kmeans, we
    have a strict objective function which will allow only one cluster membership
    while for FCM clustering, we can get different clustering membership based on
    the probability scores.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是与k均值算法进行比较的好时机了。在k均值中，我们有一个严格的目标函数，它只允许一个聚类成员身份，而对于FCM聚类，我们可以根据概率分数得到不同的聚类成员身份。
- en: FCM is a very useful for the business cases where the boundary between clusters
    is not clear and stringent. Consider in the field of bio-informatics wherein a
    gene can belong to more than one cluster. Or if we have overlapping datasets like
    in fields of the marketing analytics, image segmentation etc. FCM can give comparatively
    more robust results than kmeans.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: FCM在边界不清晰且严格的业务案例中非常有用。考虑在生物信息学领域中，一个基因可以属于多个聚类。或者如果我们有重叠的数据集，比如在营销分析、图像分割等领域。与k均值相比，FCM可以给出相对更稳健的结果。
- en: We will now proceed to Python implementation of FCM clustering in the next section.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将在下一节继续进行FCM聚类的Python实现。
- en: '![](images/tgt.png) POP QUIZ – answer these question to check your understanding..
    Answers at the end of the book'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '![](images/tgt.png) 小测验 - 回答这些问题以检查你的理解。答案在本书末尾。'
- en: 1.   Fuzzy clustering allows us to create overlapping clusters. True or False.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 1.   模糊聚类允许我们创建重叠的聚类。真或假。
- en: 2.   A data point can belong to one and only one cluster. True or False.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 2.   一个数据点只能属于一个聚类。真或假。
- en: 3.   If the value of “m” is lower, we get more clear clusters. True or False.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 3.   如果“m”的值较低，则我们会得到更清晰的聚类。真或假。
- en: 5.3.4 Python implementation of FCM
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.4 FCM的Python实现
- en: We have covered the process of FCM in the last section. We will now work on
    the Python implementation of FCM in this section.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一节已经介绍了FCM的过程。本节我们将着手讨论FCM的Python实现。
- en: 'Step 1: Import the necessary libraries.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤1：导入必要的库。
- en: '[PRE4]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Step 2: We will now declare a color palette, which will be used later for color
    coding the clusters.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤2：我们现在将声明一个颜色调色板，稍后将用于对聚类进行颜色编码。
- en: '[PRE5]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Step 3: We will define the cluster centers.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤3：我们将定义聚类中心。
- en: '[PRE6]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Step 4:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤4：
- en: '[PRE7]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Step 5:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤5：
- en: '[PRE8]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Step 6:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤6：
- en: '[PRE9]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![05_14e](images/05_14e.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![05_14e](images/05_14e.png)'
- en: 'Step 7:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤7：
- en: '[PRE10]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![05_14f](images/05_14f.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![05_14f](images/05_14f.png)'
- en: With this we conclude Fuzzy Clustering and we can move to Gaussian Mixture model
    in the next section.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个我们结束了模糊聚类，我们可以在下一节转到高斯混合模型。
- en: 5.4 Gaussian Mixture Model
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4 高斯混合模型
- en: We would continue our discussion of soft clustering from the last section. Recall
    we introduced Gaussian Mixture Model there. Now we will elaborate on it. We will
    study the concept and have a Python implementation for it.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续我们在上一节中关于软聚类的讨论。回想一下，我们在那里介绍了高斯混合模型。现在我们将对其进行详细说明。我们将研究这个概念，并对其进行Python实现。
- en: First let refresh our understanding of the *Gaussian distribution* or sometimes
    called as *normal distribution*. You might have heard bell-curve, it means the
    same thing.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们重新理解*高斯分布*，有时也被称为*正态分布*。你可能听说过钟形曲线，它指的是同一件事情。
- en: In the Figure 5-15, observe that the distribution where the µ (mean) is 0 and
    σ² (standard deviation) is 1\. It is a perfect normal distribution curve. Compare
    the distribution in different curves here.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在图5-15中，观察µ（平均值）为0，σ²（标准差）为1的分布。这是一个完美的正态分布曲线。比较这里不同曲线上的分布。
- en: (Image source – Wikipedia)
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: （图片来源 - 维基百科）
- en: Figure 5-15 A Gaussian distribution is one of the most famous distributions.
    Observe how the values of mean and standard deviation are changed and their impact
    on the corresponding curve.
  id: totrans-186
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5-15 高斯分布是最著名的分布之一。观察均值和标准差值的变化及其对应曲线的影响。
- en: '![05_15](images/05_15.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![05_15](images/05_15.png)'
- en: The mathematical expression for Gaussian distribution is
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯分布的数学表达式为
- en: '![05_15a](images/05_15a.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![05_15a](images/05_15a.png)'
- en: The equation above is also called the **Probability Density Function (pdf)**.
    In the Figure 5-15, observe that the distribution where the µ is 0 and σ² is 1\.
    It is a perfect normal distribution curve. Compare the distribution in different
    curves in the Figure 5-15 where by changing the values of mean and standard distribution,
    we are getting different graphs.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的方程也被称为**概率密度函数 (pdf)**。在图 5-15 中，观察到当 µ 为 0，σ² 为 1 时的分布。这是一个完美的正态分布曲线。通过改变均值和标准差的值，我们在图
    5-15 中比较不同曲线中的分布，从而得到不同的图形。
- en: You might be wondering why we are using Gaussian distribution here. There is
    a very famous statistical theorem call as the *Central Limit Theorem*. We are
    explaining the theorem briefly here. As per the theorem, the more and more data
    we collect, the distribution tends to become more and more Gaussian. This normal
    distribution can be observed across all walks of like, be it chemistry, physics,
    mathematics, biology or any other branch. That is the beauty of Gaussian distribution.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道为什么我们在这里使用高斯分布。这里有一个非常著名的统计定理叫做*中心极限定理*。我们在这里简要解释该定理。根据该定理，我们收集的数据越多，分布就越趋向于高斯分布。这种正态分布可以在化学、物理、数学、生物学或任何其他学科中观察到。这就是高斯分布的美妙之处。
- en: The plot shown in Figure 5-15 is one-dimensional. We can have multi-dimensional
    Gaussian distribution too. In case of a multi-dimensional Gaussian distribution,
    we will get a 3-D figure shown in Figure 5-16\. Our input was a scalar in one-dimensional.
    Now instead of scalar, out input is now a vector, mean is also a vector and represents
    the center of the data. And hence mean has the same dimensionality as the input
    data. The variance is now covariance matrix ∑. This matrix not only tells us the
    variance in the inputs, it also comments on the relationship between different
    variables. In other words, if the value of x is changed, how the values of y get
    impacted.  Have a look at Figure 5-16 below. We can understand the relationship
    between x and y variable here.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 5-15 中显示的图是一维的。我们也可以有多维高斯分布。在多维高斯分布的情况下，我们将得到图 5-16 中显示的 3D 图。我们的输入是一维的标量。现在，我们的输入不再是标量，而是一个向量，均值也是一个向量，代表数据的中心。因此，均值的维度与输入数据相同。方差现在是协方差矩阵
    ∑。该矩阵不仅告诉我们输入的方差，还评论了不同变量之间的关系。换句话说，如果 x 的值发生变化，y 的值会受到影响。请看下面的图 5-16。我们可以理解这里的
    x 和 y 变量之间的关系。
- en: (Image source – Wikipedia)
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: (图片来源 – 维基百科)
- en: Figure 5-16 3-D representation of a Gaussian Distribution is shown here.
  id: totrans-194
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5-16 展示了高斯分布的三维表示。
- en: '![05_16](images/05_16.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![05_16](images/05_16.png)'
- en: Covariance plays a significant role here. K-means does not consider the covariance
    of a dataset, which is used in GMM model.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 协方差在这里起着重要作用。K-means 不考虑数据集的协方差，而在 GMM 模型中使用。
- en: Let’s examine the process of GMM clustering. Imagine we have a dataset with
    *n* items. When we use GMM clustering, we do not find the clusters using centroid
    method, instead we fit a set of *k* gaussian distributions to the data set at
    hand. In other words, we have *k* clusters. We have to determine the parameters
    for each of these Gaussian distributions which are mean, variance and weight of
    a cluster. Once the parameters for each of the distribution are determined, then
    we can find the respective probability for each of the n items to belong to k
    clusters.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来研究 GMM 聚类的过程。想象我们有一个包含 *n* 个项目的数据集。当我们使用 GMM 聚类时，我们不是使用质心方法来找到聚类，而是对手头的数据集拟合一组
    *k* 个高斯分布。换句话说，我们有 *k* 个聚类。我们必须确定每个高斯分布的参数，包括聚类的均值、方差和权重。一旦确定了每个分布的参数，我们就可以找到每个项目属于
    *k* 个聚类的相应概率。
- en: Mathematically, we can calculate the probability as shown in Equation 5-5\.
    The equation is used to for us to know that a particular point x is a linear combination
    of k Gaussians. The term Φ[j] is used to represent the strength of the Gaussian
    and it can be seen in the second equation that the sum of such strength is equal
    to 1.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，我们可以根据方程 5-5 计算概率。该方程用于告诉我们一个特定点 x 是 k 个高斯分布的线性组合。Φ[j] 术语用于表示高斯的强度，并且可以看到第二个方程中这种强度的总和等于
    1。
- en: '![05_16a](images/05_16a.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![05_16a](images/05_16a.png)'
- en: For spectral clustering, we have to identify the values of Φ, ∑ and µ. As you
    would imagine, getting the values of these parameters can be a tricky business.
    It is indeed a slightly complex called Expectation-Maximization technique or EM
    technique, which we will cover now. This section is quite heavy on mathematical
    concepts and is optional.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 对于谱聚类，我们必须确定 Φ、∑ 和 µ 的值。正如你所想象的，获取这些参数的值可能是一项棘手的任务。确实有一种稍微复杂的叫做期望最大化技术或 EM 技术，我们现在将介绍它。这一部分涉及的数学概念比较深，是可选的。
- en: 5.4.1 Expectation-Maximization (EM) technique
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.1 期望最大化（EM）技术
- en: EM is a statistical and mathematical solution to determine the correct parameters
    for the model. There are quite a few techniques which are popular, perhaps maximum
    likelihood estimation is the most famous of them all. But at the same time, there
    could be a few challenges with maximum likelihood too. The data set might have
    missing values or in other words the dataset is incomplete. Or it is a possibility
    that a point in the dataset is generated by two different Gaussian distributions.
    Hence, it will be very difficult to determine that which distribution generated
    that data point. Here, EM can be helpful.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: EM 是确定模型正确参数的统计和数学解决方案。有很多流行的技术，也许最著名的是最大似然估计。但同时，最大似然估计也可能存在一些挑战。数据集可能有缺失值，或者换句话说，数据集是不完整的。或者数据集中的一个点可能是由两个不同的高斯分布生成的。因此，确定生成数据点的分布将非常困难。在这里，EM
    可以提供帮助。
- en: k-means uses only mean while GMM utilizes both mean and variance of the data.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: k-means 只使用平均值，而 GMM 利用数据的平均值和方差。
- en: The processes which are used to generate a data point are called *latent variables*.
    Since we do not know the exact values of these latent variables, EM firsts estimates
    the optimum values of these latent variables using the current data. Once this
    is done, then the model parameters are estimated. Using these model parameters,
    the latent variables are again determined. And using these new latent variables,
    new model parameters are derived. And the process continues till a good enough
    set of latent values and model parameters are achieved which fit the data well.
    Let’s study in more detail now.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成数据点的过程被称为*潜在变量*。由于我们不知道这些潜在变量的确切值，EM 首先使用当前数据估计这些潜在变量的最佳值。一旦完成了这一步，然后估计模型参数。使用这些模型参数，再次确定潜在变量。并且使用这些新的潜在变量，推导出新的模型参数。这个过程持续进行，直到获得足够好的潜在值和模型参数，使其很好地适应数据。现在让我们更详细地研究一下。
- en: We will take the same example we had in the last section.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用上一节中的相同示例。
- en: Imagine we have a dataset with *n* items. When we use GMM clustering, we do
    not find the clusters using centroid method, instead we fit a set of *k* gaussian
    distributions to the data set at hand. In other words, we have *k* clusters. We
    have to determine the parameters for each of these Gaussian distributions which
    are mean, variance and weight of a cluster. Let’s say that mean is µ[1], µ[2],
    µ[3], µ[4]…. µ[k] and covariance is ∑[1], ∑[2], ∑[3], ∑[4]…. ∑[k]. We can also
    have one more parameter to represent the density or strength of the distribution
    and it can be represented by the symbol Φ.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下我们有一个包含 *n* 个项的数据集。当我们使用 GMM 聚类时，我们不是使用质心方法找到簇，而是将一组 *k* 个高斯分布拟合到手头的数据集。换句话说，我们有
    *k* 个簇。我们必须确定每个高斯分布的参数，即簇的平均值、方差和权重。假设均值为 µ[1]、µ[2]、µ[3]、µ[4]…. µ[k]，协方差为 ∑[1]、∑[2]、∑[3]、∑[4]….
    ∑[k]。我们还可以有一个参数来表示分布的密度或强度，可以用符号 Φ 表示。
- en: Now we will start with the Expectation or the E step. In this step, each data
    point is assigned to a cluster probabilistically. So, for each point we calculate
    its probability to belong to a cluster, if this value is high the point is in
    the correct cluster else the point is in the wrong cluster. In other words, we
    are calculating the probability that each data point is generated by each of the
    k Gaussians.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将从期望步骤或 E 步开始。在这一步中，每个数据点都以概率分配到一个簇。因此，对于每个点，我们计算其属于一个簇的概率，如果这个值很高，则该点在正确的簇中，否则该点在错误的簇中。换句话说，我们正在计算每个数据点由每个
    k 个高斯分布生成的概率。
- en: Since we are calculating probabilities, these are called soft assignments.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们正在计算概率，因此这些被称为软分配。
- en: The probability is calculated using the formula in Equation 5-6\. If we look
    closely, the numerator is the probability and then we are normalizing by the denominator.
    The numerator is the same we have seen in Equation 5-5.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 概率是使用方程式 5-6 计算的。如果我们仔细观察，分子是概率，然后我们通过分母进行归一化。分子与我们在方程式 5-5 中看到的相同。
- en: '![05_16b](images/05_16b.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![05_16b](images/05_16b.png)'
- en: In the Expectation step above, for a data point x[i,j], where i is the row and
    j is the column, we are getting a matrix where rows are represented by the data
    points and columns are their respective Gaussian values.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的期望步骤中，对于数据点 x[i,j]，其中 i 是行，j 是列，我们得到一个矩阵，其中行由数据点表示，列由它们对应的高斯值表示。
- en: Now the expectation step is finished, we will perform the maximization or the
    M step. In this step, we will update the values of µ, ∑ and Φ using the formula
    below in Equation 5-7\. Recall in k-means clustering, we simply take the mean
    of the data points and move ahead. We do something similar here albeit use the
    probability or the expectation we calculated in the last step.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 现在期望步骤已经完成，我们将执行最大化或 M 步骤。在这一步中，我们将使用方程式 5-7 下面的公式更新 µ、∑ 和 Φ 的值。回想一下，在 k-means
    聚类中，我们只是取数据点的平均值并继续前进。我们在这里做了类似的事情，尽管使用了我们在上一步中计算的概率或期望。
- en: The three values can be calculated using the Equation below. Equation 5-7 is
    the calculation of the covariances ∑[j], wherein we calculate the covariances
    of all the points, which is then weighted by the probability of that point being
    generated by Gaussian j. The mathematical proofs are beyond the scope of this
    book.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 三个值可以使用下面的方程式计算。方程式 5-7 是协方差 ∑[j] 的计算，在其中我们计算所有点的协方差，然后以该点由高斯 j 生成的概率加权。数学证明超出了本书的范围。
- en: '![05_16c](images/05_16c.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![05_16c](images/05_16c.png)'
- en: The mean µ[j], is determined by Equation 5-8\. Here, we determine the mean for
    all the points, weighted by the probability of that point being generated by Gaussian
    j.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 均值 µ[j] 由方程式 5-8 确定。在这里，我们确定所有点的均值，以该点由高斯 j 生成的概率加权。
- en: '![05_16d](images/05_16d.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![05_16d](images/05_16d.png)'
- en: Similarly, the density or the strength is calculated by Equation 5-9, where
    we add all the probabilities for each point to be generated by Gaussian j and
    then divide by the total number of points N.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，密度或强度是由方程式 5-9 计算的，其中我们将每个点生成为高斯 j 的概率相加，然后除以总点数 N。
- en: '![05_16e](images/05_16e.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![05_16e](images/05_16e.png)'
- en: Based on these values, new values for ∑, µ and Φ are derived, and the process
    continues till the model converges. We stop when we are able to maximize the log-likelihood
    function.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这些值，推导出新的 ∑、µ 和 Φ 的值，并且该过程继续直到模型收敛。当我们能够最大化对数似然函数时，我们停止。
- en: It is a complex mathematical process. We have covered it to give you the in-depth
    understanding of what happens in the background of the statistical algorithm.
    The Python implementation is much more straight forward than the mathematical
    concept which we will cover now.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个复杂的数学过程。我们已经涵盖了它，以便让您深入了解统计算法背后发生的事情。Python 实现比我们现在将要介绍的数学概念要简单得多。
- en: '![](images/tgt.png) POP QUIZ – answer these question to check your understanding..
    Answers at the end of the book'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '![](images/tgt.png) POP QUIZ – 回答这些问题以检查您的理解。答案在书的末尾。'
- en: 1.   Gaussian distribution has mean equal to 1 and standard deviation equal
    to 0\. True or False.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 1.   高斯分布的均值等于 1，标准差等于 0。是或否。
- en: 2.   GMM models does not consider the covariance of the data. True or False.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 2.   GMM 模型不考虑数据的协方差。是或否。
- en: 5.4.2 Python implementation of GMM
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.2 GMM 的 Python 实现
- en: We will first import the data and then we will compare the results using kmeans
    and GMM.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入数据，然后将使用 kmeans 和 GMM 进行结果比较。
- en: 'Step 1: We will import all the libraries and import the dataset too.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 1：我们将导入所有的库并导入数据集。
- en: '[PRE11]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Step 2: We will now drop any NA from the dataset.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 2：我们现在将从数据集中删除任何 NA。
- en: '[PRE12]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Step 3: We will now fit a `kmeans` algorithm. We are keeping the number of
    clusters as 5\. Please note that we are not saying that they are idle number of
    clusters. These number of clusters are only for illustrative purpose. We are declaring
    a variable kmeans and then using five clusters and then the dataset is fit next.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 3：我们现在将拟合一个 `kmeans` 算法。我们将聚类数保持为 5。请注意，我们并不是说它们是理想的聚类数。这些聚类数仅用于说明目的。我们声明一个变量
    kmeans，然后使用五个聚类，然后是下一个拟合数据集。
- en: '[PRE13]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Step 4: We will now plot the clusters. First, a prediction is made on the dataset
    and then the values are added to the data frame as a new column. The data is then
    plotted with different colors representing different clusters.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 4：我们现在将绘制聚类。首先，在数据集上进行预测，然后将值添加到数据框作为新列。然后，用不同颜色表示不同的聚类绘制数据。
- en: The output is shown in the plot below.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示在下图中。
- en: '[PRE14]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![05_16f](images/05_16f.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![05_16f](images/05_16f.png)'
- en: 'Step 4: We will now fit a GMM model. Note that the code is the same as the
    kmeans algorithm only the algorithm’s name has changed from kmeans to GaussianMixture.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 4：我们现在将拟合一个 GMM 模型。请注意，代码与 kmeans 算法相同，只是算法的名称从 kmeans 更改为 GaussianMixture。
- en: '[PRE15]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Step 5: We will now plot the results. The output is shown below.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 5：我们现在将绘制结果。输出如下所示。
- en: '[PRE16]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![05_16g](images/05_16g.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![05_16g](images/05_16g.png)'
- en: 'Step 6: You are advised to run the code with different values of clusters to
    observe the difference. In the plots below, the left one is kmeans with two clusters
    while the right is GMM with two clusters.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 6：建议您使用不同的聚类值运行代码以观察差异。下面的图中，左边是具有两个聚类的 kmeans，右边是具有两个聚类的 GMM。
- en: '![05_16h](images/05_16h.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![05_16h](images/05_16h.png)'
- en: Gaussian distribution is one of the most widely used data distribution used.
    If we compare kmeans and GMM model, we would understand that kmeans does not consider
    the normal distribution of the data. The relationship of various data points is
    also not considered in kmeans.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯分布是最广泛使用的数据分布之一。如果我们比较 kmeans 和 GMM 模型，我们会发现 kmeans 并不考虑数据的正态分布。kmeans 也不考虑各个数据点之间的关系。
- en: Kmeans is a distance-based algorithm, GMM is a distribution based algorithm.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: Kmeans 是基于距离的算法，GMM 是基于分布的算法。
- en: In short, it is advantageous to use GMM models for creating the clusters particularly
    when we have overlapping datasets. It is a useful technique for financial and
    price modelling, NLP based solutions etc.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，使用 GMM 模型创建聚类尤其有利，特别是当我们有重叠的数据集时。这是一个在金融和价格建模、基于 NLP 的解决方案等方面非常有用的技术。
- en: With this, we have covered all the algorithms in the chapter. We can now move
    to the summary.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个，我们已经涵盖了本章的所有算法。我们现在可以转到总结。
- en: 5.5 Summary
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.5 总结
- en: In this chapter, we have explored three complex clustering algorithms. You might
    have felt the mathematical concepts a bit heavy. They are indeed heavy but give
    a deeper understanding of the process. It is not necessary that these algorithms
    are the best ones for each and every problem. Ideally, in the real-world business
    problem we should first start with classical clustering algorithms – kmeans, hierarchical
    and DBSCAN. If we do not get acceptable results then, we can try the complex algorithms.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了三种复杂的聚类算法。你可能觉得数学概念有点沉重。它们确实很沉重，但能更深入地理解过程。并不是说这些算法对每个问题都是最好的。在现实世界的业务问题中，理想情况下，我们应该首先使用经典的聚类算法
    - kmeans、层次聚类和 DBSCAN。如果我们得不到可接受的结果，那么我们可以尝试复杂的算法。
- en: Many times, a data science problem is equated to the choice of algorithm, which
    it is not. The algorithm is certainly an important ingredient of the entire solution,
    but it is not the only one. In the real-world datasets, there are a lot of variables
    and the amount of data is also quite big. The data has a lot of noise. We have
    to account for all these factors when we shortlist an algorithm. Algorithm maintenance
    and refresh is also one of the major questions we have in mind. All these finer
    points are covered in much detail in the last chapter of the book.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 许多时候，将数据科学问题等同于算法的选择，其实并不是这样。算法当然是整个解决方案的重要组成部分，但不是唯一的部分。在现实世界的数据集中，有很多变量，数据量也相当大。数据有很多噪音。当我们筛选算法时，我们必须考虑所有这些因素。算法的维护和更新也是我们心中的主要问题之一。所有这些细节在书的最后一章中都有详细介绍。
- en: We will cover complex dimensionality reduction techniques in the next chapter.
    You can move to questions now.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一章介绍复杂的降维技术。你现在可以转到问题。
- en: Practical next steps and suggested readings
  id: totrans-251
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实际下一步和建议阅读
- en: In chapter 2 we have done clustering using various techniques. Use the datasets
    from there and perform Spectral clustering, GMM and FCM clustering to compare
    the results.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第二章中，我们使用了各种技术进行聚类。使用那里的数据集，并执行谱聚类、GMM 和 FCM 聚类以比较结果。
- en: There are datasets provided at the end of chapter 2 which can be used for clustering.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二章末尾提供了数据集，可以用于聚类。
- en: Get the credit card dataset for clustering from this Kaggle link ([https://www.kaggle.com/vipulgandhi/spectral-clustering-detailed-explanation](vipulgandhi.html))
    and from the famous IRIS dataset which we have used earlier too.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从此Kaggle链接获取用于聚类的信用卡数据集([https://www.kaggle.com/vipulgandhi/spectral-clustering-detailed-explanation](vipulgandhi.html))，以及我们之前也使用过的著名的IRIS数据集。
- en: There is a great book Computational Network Science by Henry Hexmoor to study
    the mathematical concepts.
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有一本由亨利·赫克斯莫尔（Henry Hexmoor）撰写的《计算网络科学》是研究数学概念的好书。
- en: 'Get Spectral clustering papers from the links below and study them:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从下面的链接获取光谱聚类论文并学习它们：
- en: 'On spectral clustering: analysis and an algorithm [https://proceedings.neurips.cc/paper/2001/file/801272ee79cfde7fa5960571fee36b9b-Paper.pdf](file.html)'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关于光谱聚类：分析与算法[https://proceedings.neurips.cc/paper/2001/file/801272ee79cfde7fa5960571fee36b9b-Paper.pdf](file.html)
- en: Spectral clustering with eigenvalue selection [http://www.eecs.qmul.ac.uk/~sgg/papers/XiangGong-PR08.pdf](papers.html)
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 具有特定特征值选择的光谱聚类[http://www.eecs.qmul.ac.uk/~sgg/papers/XiangGong-PR08.pdf](papers.html)
- en: The mathematics behind spectral clustering and the equivalence to PCA [https://arxiv.org/pdf/2103.00733v1.pdf](pdf.html)
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 光谱聚类背后的数学及其与主成分分析的等价性[https://arxiv.org/pdf/2103.00733v1.pdf](pdf.html)
- en: 'Get GMM papers from the link below and explore them:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从以下链接获取GMM论文并探索它们：
- en: A particular Gaussian Mixture Model for clustering [https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.79.7057&rep=rep1&type=pdf](viewdoc.html)
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用于聚类的特定高斯混合模型[https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.79.7057&rep=rep1&type=pdf](viewdoc.html)
- en: Application of Compound Gaussian Mixture Model in the data stream [https://ieeexplore.ieee.org/document/5620507](document.html)
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在数据流中应用复合高斯混合模型[https://ieeexplore.ieee.org/document/5620507](document.html)
- en: 'Get FCM papers from the link below and study them:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从以下链接获取FCM论文并学习它们：
- en: 'FCM: the fuzzy c-means clustering algorithm [https://www.sciencedirect.com/science/article/pii/0098300484900207](pii.html)'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: FCM：模糊c均值聚类算法[https://www.sciencedirect.com/science/article/pii/0098300484900207](pii.html)
- en: A survey on Fuzzy c-means clustering techniques [https://www.ijedr.org/papers/IJEDR1704186.pdf](papers.html)
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对模糊c均值聚类技术进行调查[https://www.ijedr.org/papers/IJEDR1704186.pdf](papers.html)
- en: Implementation of Fuzzy C-Means and Possibilistic C-Means Clustering Algorithms,
    Cluster Tendency Analysis and Cluster Validation [https://arxiv.org/pdf/1809.08417.pdf](pdf.html)
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模糊c均值和可能性c均值聚类算法的实现，聚类倾向分析和聚类验证[https://arxiv.org/pdf/1809.08417.pdf](pdf.html)
