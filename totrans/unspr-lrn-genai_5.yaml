- en: 5 Clustering (advanced)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: “Out of complexity, find simplicity– Einstein”
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes life is very simple, and sometimes we experience quite complex situations.
    We sail through both the situations and change our approach as per the situation.
  prefs: []
  type: TYPE_NORMAL
- en: In the Part one of the book we covered easier and simpler topics. It made you
    ready for the journey ahead. We are currently in Part two which is slightly more
    complex than Part one. Part three is more advanced than the first two parts. So,
    the level of difficulty will increase slightly with each and every chapter along
    with the expectations.
  prefs: []
  type: TYPE_NORMAL
- en: We studied clustering algorithms in part one of the book. We understand that
    clustering is an unsupervised learning technique where we wish to group the data
    points by discovering interesting patterns in the datasets. We went through the
    meaning of clustering solutions, different categories of clustering algorithm
    and a case study at the end. In that chapter, we explored kmeans clustering, hierarchical
    clustering and DBSCAN clustering in depth. We went through the mathematical background,
    process, Python implementation and pros and cons of each. Before starting this
    chapter, it is advisable to refresh chapter two.
  prefs: []
  type: TYPE_NORMAL
- en: Many times you might encounter datasets which do not conform to a simple shape
    and form. Moreover, we have to find the best fit before making a choice of the
    final algorithm we wish to implement. Here, we might need help of more complex
    clustering algorithms; the topic for the chapter. In this chapter, we are going
    to again study three such complex clustering algorithms – spectral clustering,
    Gaussian Mixture Models (GMM) clustering and fuzzy clustering. As always, Python
    implementation will follow the mathematical and theoretical concepts. This chapter
    is slightly heavy on the mathematical concepts. There is no need for you to be
    a PhD. in mathematics, but it is sometime important to understand how the algorithms
    work in the background. At the same time, you will be surprised to find that Python
    implementation of such algorithms is not tedious. This chapter is not having any
    case study.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this fifth chapter of the book, we are going to cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Spectral clustering
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fuzzy clustering
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gaussian Mixture Models (GMM) clustering
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Welcome to the fifth chapter and all the very best!
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Technical toolkit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will continue to use the same version of Python and Jupyter notebook as we
    have used so far. The codes and datasets used in this chapter have been checked-in
    at GitHub ([https://github.com/vverdhan/UnsupervisedLearningWithPython/tree/main/Chapter%205](main.html)).
  prefs: []
  type: TYPE_NORMAL
- en: We are going to use the regular Python libraries we have used so far – numpy,
    pandas, sklearn, seaborn, matplotlib etc. You would need to install a few Python
    libraries in this chapter which are – skfuzzy and network . Using libraries, we
    can implement the algorithms very quickly. Otherwise, coding these algorithms
    is quite a time-consuming and painstaking task.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started with a refresh of clustering!
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recall from chapter 2, clustering is used to group similar objects or data points.
    It is an unsupervised learning technique where we intend to find natural grouping
    in the data as shown in Figure 5-1.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5-1 Clustering of objected result into natural grouping.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![05_01](images/05_01.png)'
  prefs: []
  type: TYPE_IMG
- en: Here we can observe that on the left side we have ungrouped data and on the
    right side the data points have been grouped into logical groups. We can also
    observe that there can be two methodologies to do the grouping or *clustering*,
    and both result into different clusters. Clustering as a technique is quite heavily
    used in business solutions like customer segmentation, market segmentation etc.
  prefs: []
  type: TYPE_NORMAL
- en: We have understood kmeans, hierarchical and DBSCAN clustering in chapter 2\.
    We also covered various distance measurement techniques and indicators to measure
    the performance of clustering algorithms. You are advised to revisit the concepts.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are focussing on advanced clustering methods. We will now
    start with Spectral clustering in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Spectral Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spectral clustering is one of the unique clustering algorithms. There are some
    good quality research is done in this field. Revered researchers like Prof. Andrew
    Yang, Prof. Michael Jordan, Prof. Yair Weiss, Prof. Jianbo Shi, Prof. Jitendra
    Malik to name a few. We are quoting some of the papers in the last section of
    the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s define spectral clustering first. Spectral clustering works on the affinity
    and not the absolute location of the data points for clustering. And hence, wherever
    the data is in complicated shapes, spectral clustering is the answer. We have
    shown a few examples in the Figure 5-2 where spectral clustering can provide a
    logical solution.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5-2 Examples of various complex data shapes which can be clustered using
    Spectral Clustering.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![05_02](images/05_02.png)'
  prefs: []
  type: TYPE_IMG
- en: For Figure 5-2, we could have used other algorithms like k-means clustering
    too. But they might not be able to do justice to such complicated shapes of the
    data. Formally put, algorithms like kmeans clustering utilize compactness of the
    data points. In other words, the closeness of the points to each other and compactness
    towards the cluster center, drive the clustering in kmeans. On the other hand,
    in Spectral clustering *connectivity* is the driving logic. In connectivity, either
    the data points are immediately close to one another or are connected in some
    way. The examples of such connectivity-based clustering have been depicted in
    Figure 5-2.
  prefs: []
  type: TYPE_NORMAL
- en: Look at the Figure 5-3(i) where the data points are in this doughnut pattern.
    There can be data points which can follow this doughnut pattern. It is a complex
    pattern and we have to cluster these data points. Imagine that by using a clustering
    method, the red circles are made a part of the same cluster, which is shown in
    Figure 5-3(ii). After all, they are close to each other. But if we look closely,
    the points are in a circle, are in a pattern and hence the actual cluster should
    be as shown in Figure 5-3(iii).
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5-3 (i) We can have such a complex representation of data points which
    need to be clustered. Observe the doughnut shape (ii) A very simple explanation
    can result in red dots being considered as a part of the same cluster but clearly,
    they are not part of the same cluster (iii) We have two circles over here. The
    points in the inner circle belong to one cluster whereas the outer points belong
    to another cluster
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![05_03](images/05_03.png)'
  prefs: []
  type: TYPE_IMG
- en: The example shown in Figure 5-3 is to depict the advantages with Spectral clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Like we said earlier, spectral clustering utilizes connectivity approach in
    clustering. In spectral clustering, data points which are immediately next to
    each other are identified in a graph. These data points are sometimes referred
    to as *node*. These data points or nodes are then mapped to a low-dimensional
    space. A low dimensional space is nothing but having During this process, spectral
    clustering uses eigenvalues, affinity matrix, Laplacian matrix and degree matrix
    derived from the data set. The low-dimensional space can then be segregated into
    clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Spectral clustering utilizes connectivity approach for clustering wherein It
    relies on graph theory wherein we identify clusters of nodes based on the edges
    connecting them.
  prefs: []
  type: TYPE_NORMAL
- en: We will study the process in detail. But before examining the process, there
    are a few important mathematical concepts which form the foundation of spectral
    clustering which we will cover now.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1 Building blocks of Spectral Clustering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We know that the goal of clustering is to group data points which are similar
    into one cluster, while the data points which are not similar into another one.
    There are a few mathematical concepts we should be aware. We will start the concept
    of similarity graphs, which is quite an innate representation for data points.
  prefs: []
  type: TYPE_NORMAL
- en: Similarity graphs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A graph is one of the easiest and intuitive method to represent data points.
    In the Figure 5-4(i), we are showing an example of a graph which is nothing but
    a connection between data points represented by the edge. Now two data points
    are connected if the similarity between them is positive or it is above a certain
    threshold which is shown in Figure 5-4(ii). Instead of absolute values for the
    similarity, we can use weights for the similarity. So, in Figure 5-4(ii), as point
    1 and 2 are similar as compared to point 1 and 3, the connection between point
    1 and 2 has a higher weight than point 1 and 3.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5-4(i) A graph is a simple representation of data points. The points
    or nodes are connected with each other if they are very similar (ii) The weight
    is higher if the similarity between data points is high else for dissimilar data
    points, the weight is less.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![05_04](images/05_04.png)'
  prefs: []
  type: TYPE_IMG
- en: So, we can conclude that – using Similarity Graphs we wish to cluster the data
    points such that
  prefs: []
  type: TYPE_NORMAL
- en: the edges of the data points have higher value of weight and hence are similar
    to each other and so are in the same cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the edges of the data points have lower values of weight and hence are not similar
    to each other and so they are in different clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apart from similarity graphs, we should also know the concept of **Eigen values
    and Eigen vectors** which we have covered in detail in the previous chapter. You
    are advised to refresh it. We will now move to the concept of Adjacency matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Adjacency Matrix
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Have a close look at Figure 5-5\. We can see those various points from 1 to
    5 are connected with each other. And then we are representing the connection in
    a matrix. That matrix is called *Adjacency Matrix*.
  prefs: []
  type: TYPE_NORMAL
- en: Formally put, in adjacency matrix, the rows and columns are the respective nodes.
    The values inside the matrix represent the connection – if the value is 0 that
    means there is no connection and if the value is 1, it means there is a connection.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5-5 Adjacency matrix represents the connection between various nodes,
    if the value is 1 that means the corresponding nodes in the row and column are
    connected. If the value is 0, it means they are not connected. For example, there
    is a connection between node 1 and node 5 hence the value is 1 while there is
    no connection between node 1 and node 4 hence the corresponding value is 0.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![05_05](images/05_05.png)'
  prefs: []
  type: TYPE_IMG
- en: So for adjacency matrix, we are only concerned if there is a connection between
    two data points. If we extend the concept of adjacency matrix, we get degree matrix
    which is our next concept.
  prefs: []
  type: TYPE_NORMAL
- en: Degree Matrix
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Formally put, a degree matrix is a diagonal matrix, where the degree of a node
    along the diagonal is the number of edges connected to it. If we use the same
    example as above, we can have the degree matrix as shown in Figure 5-6\. Node
    3 and 5 have three connections each and they are getting 3 as the values along
    the diagonal while the other nodes have only two connections each, so they have
    2 as the value along the diagonal.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5-6 While adjacency matrix represents the connection between various
    nodes, degree matrix is for the number of connections to each node. For example,
    node 5 has three connections and hence has 3 in front of it while node 1 has only
    two connections so it has 2.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![05_06](images/05_06.png)'
  prefs: []
  type: TYPE_IMG
- en: You might be wondering why do we use matrix? Matrix provide an elegant representation
    of the data and can clearly depict the relationships between two points.
  prefs: []
  type: TYPE_NORMAL
- en: Now we have covered both adjacency matrix and degree matrix, we can move to
    Laplacian matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Laplacian Matrix
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are quite a few variants of Laplacian matrix, but if we take the simplest
    form of Laplacian matrix, it is nothing but a subtraction of adjacency matrix
    from the degree matrix. In other words, L = D – A. We can show it as Figure 5-7.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5-7 Laplacian matrix is quite simple to understand. To get a Laplacian
    matrix, we can simply subtract an adjacency matrix from the degree matrix as shown
    in the example above. Here, D represents the degree matrix, A is the adjacency
    matrix and L is the Laplacian matrix.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![05_07](images/05_07.png)'
  prefs: []
  type: TYPE_IMG
- en: Laplacian matrix is quite an important one and we use the eigen values of L
    to develop spectral clustering. Once we get the eigen values and eigen vectors,
    we can define two other values – spectral gap and Fielder value. The very first
    non-zero eigen value is the *Spectral Gap* which defines the density of the graph.
    The *Fielder value* is the second eigen value which provides us an approximation
    of the minimum cut required to separate the graph into two components. The corresponding
    vector for Fielder value is called the *Fielder vector*.
  prefs: []
  type: TYPE_NORMAL
- en: Fielder vector has both negative and positive components and their resultant
    sum is zero.
  prefs: []
  type: TYPE_NORMAL
- en: We will use this concept once we study the process of Spectral clustering in
    detail in the next section. We will now cover one more concept of Affinity matrix
    before moving to the process of Spectral clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Affinity Matrix
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the adjacency matrix, if we replace the number of connections with the similarity
    of the weights, we will get affinity matrix. If the points are completely dissimilar,
    the affinity will be 0 else if they are completely similar the affinity will be
    1\. The values in the matrix represent different levels of similarity between
    data points.
  prefs: []
  type: TYPE_NORMAL
- en: '![](images/tgt.png) POP QUIZ – answer these question to check your understanding..
    Answers at the end of the book'
  prefs: []
  type: TYPE_NORMAL
- en: 1.   The degree matrix is created by counting the number of connections. True
    or False.
  prefs: []
  type: TYPE_NORMAL
- en: 2.   Laplacian is a transpose of the division of degree and adjacency matrix.
    True or False.
  prefs: []
  type: TYPE_NORMAL
- en: 3.   Write a matrix on a paper and then derive its adjacency and degree matrix.
  prefs: []
  type: TYPE_NORMAL
- en: We have now covered all the building blocks for Spectral clustering. We can
    now move to the process of Spectral clustering.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2 Process of Spectral Clustering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now we have covered all the building blocks for Spectral clustering. At a high
    level, the various steps can be noted as:'
  prefs: []
  type: TYPE_NORMAL
- en: We get the dataset and calculate its degree matrix and Adjacency matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using them, we get the Laplacian matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then we calculate the first k eigen vectors of the Laplacian matrix. The k eigenvectors
    are nothing but the ones which correspond to the k smallest eigen values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The matrix such formed is used to cluster the data points in k-dimensional space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will now cover the process of Spectral clustering using an example as shown
    in Figure 5-8\. These are the steps which are generally not followed in real-world
    implementation as we have packages and libraries to achieve it. These steps are
    covered here to give you the idea of how the algorithm can be developed from scratch.
    For the Python implementation, we will use the libraries and packages only. Though
    it is possible to develop an implementation from scratch, it is not time efficient
    to reinvent the wheel.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5-8 Consider the example shown where we have some data points and they
    are connected with each other. We will perform Spectral clustering on this data.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![05_08](images/05_08.png)'
  prefs: []
  type: TYPE_IMG
- en: Now when we wish to perform the spectral clustering on this data.
  prefs: []
  type: TYPE_NORMAL
- en: We will leave it upto you to create the adjacency matrix and degree matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The next step is creating the Laplacian matrix. We are sharing the output Laplacian
    matrix in Figure 5-9.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 5-9 Laplacian matrix of the data is shown here. You are advised to create
    the degree and adjacency matrix and check the output.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![05_09](images/05_09.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, the Fielder vector is shown in Figure 5-10 for the above Laplacian matrix.
    We create the Fielder vector as described in the last section. Observe how the
    sum of the matrix is zero.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 5-10 Fielder vector is the output for the Laplacian matrix, observe that
    the sum is zero here.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![05_10](images/05_10.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that there are a few positive values and a few negative values, based
    on which we can create two distinct clusters. This is a very simple example to
    illustrate the process of Spectral Clustering.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 5-11 The two clusters are identified. This is a very simple example to
    illustrate the process of Spectral Clustering.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![05_11](images/05_11.png)'
  prefs: []
  type: TYPE_IMG
- en: The above process is a very simple representation of Spectral Clustering.  Spectral
    clustering is useful for image segmentation, speech analysis, text analytics,
    entity resolution etc. It is quite easy and intuitive method and does not make
    any assumption about the shape of the data. Methods like kmeans assume that the
    points are in a spherical form around the center of the cluster whereas there
    is no such strong assumption in Spectral clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Another significant difference is that in spectral clustering the data points
    need not have convex boundaries as compared to other methods where compactness
    drives clustering. Spectral clustering is sometimes slow since eigne values, Laplacians
    etc. have to be calculated. With a large dataset the complexity increases and
    hence Spectral clustering can become slow, but it a fast method when we have a
    sparse dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We will now proceed to Python implementation of Spectral Clustering algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1     Python implementation of Spectral Clustering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have covered so far, the theoretical details of Spectral Clustering, it is
    time to get into the code. For this, we will curate a dataset and run k-means
    algorithm. And then Spectral Clustering to compare the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Import all the necessary libraries first. These libraries are standard
    libraries except a few which we will cover. `sklearn` is one of the most famous
    and sought-after libraries and from `sklearn` we are importing `SpectralClustering`,
    `make_blobs` and `make_circles`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 2: We will now curate a dataset. We are using `make_circles` method. Here
    we are taking 2000 samples and representing them in a circle. The output is shown
    below.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![05_11a](images/05_11a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Step 3: We will now test this dataset with kmeans clustering. The two colors
    are showing two different clusters which are overlapping with each other.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![05_11b](images/05_11b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Step 4: We will now run the same data with Spectral Clustering and we find
    that the two clusters are being handled separately here.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![05_11c](images/05_11c.png)'
  prefs: []
  type: TYPE_IMG
- en: We can observe here that the same dataset is handled differently by the two
    algorithms. Spectral clustering is handling the dataset better as the circles
    which are separate are depicted separately.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 5: You are advised to simulate various cases by changing the values in
    the dataset and run the algorithms. Observe the different outputs for comparison.'
  prefs: []
  type: TYPE_NORMAL
- en: With this we have finished the first algorithm in this chapter. We will now
    move to Fuzzy Clustering in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Fuzzy Clustering
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: So far, we have covered quite a few clustering algorithms. Did you wonder that
    why a data point should belong to only one cluster? Why can’t a data point belong
    to more than one clusters? Have a look at Figure 5-12.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5-12 The figure of the left represents all the data points. The red points
    can belong to more than one clusters. In fact, we can allocate more than one cluster
    to each and every point. A probability score can be given for a point to belong
    to a particular cluster.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![05_12](images/05_12.png)'
  prefs: []
  type: TYPE_IMG
- en: We know that clustering is used to group items in cohesive groups based on the
    similarities between them. The items which are similar are in one cluster, whereas
    the items which are dissimilar are in different clusters. The idea of clustering
    is to ensure the items in same cluster should be as much similar to each other.
    When the items can be only in one cluster, it is called as *hard clustering.*
    K-means clustering is a classic example of hard clustering. But if we reflect
    back on the Figure 5-12, we can observe that an item can belong to more than one
    clusters. It is also called *soft clustering.*
  prefs: []
  type: TYPE_NORMAL
- en: It is computationally cheaper to create fuzzy boundaries than create hard clusters.
  prefs: []
  type: TYPE_NORMAL
- en: In fuzzy clustering, an item can be assigned to more than one cluster. The items
    which are closer to the center of a cluster, might be the part of the cluster
    to a higher degree than the items which are near the cluster’s edge. It is referred
    as *membership*. It employs least-square solutions to the most optimal location
    of an item. This optimal location might be the probability space between the two
    or more clusters. We will examine this concept in detail when we study the process
    of fuzzy clustering in detail and now, we will move to types of fuzzy clustering
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.3 Types of Fuzzy Clustering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Fuzzy clustering can be further divided between classical fuzzy algorithms and
    shape-based fuzzy algorithms which we are showing by means of a diagram Figure
    5-13.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5-13 Fuzzy algorithms can be divided into Classical Fuzzy algorithm and
    Shape-based fuzzy algorithm.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![05_13](images/05_13.png)'
  prefs: []
  type: TYPE_IMG
- en: We will cover the Fuzzy c-means algorithm in detail here. Rest of the algorithms
    we will cover in brief.
  prefs: []
  type: TYPE_NORMAL
- en: '*Gaustafson-Kessel algorithm* or sometimes called as GK algorithm works by
    associating an item with a cluster and a matrix. GL results in elliptical clusters
    and in order to modify as per varied structures in the datasets GK uses the covariance
    matrix. It allows the algorithm to capture the elliptical properties of the cluster.
    GK can result in narrower clusters and wherever the number of items is higher,
    those areas can be thinner.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gath-Geva algorithm is not based on an objective function. The clusters can
    result in any shape since it is a fuzzification of statistical estimators.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The shape based clustering algorithms are self-explanatory as per their names.
    A circular fuzzy clustering algorithm will result in circular shaped clusters
    and so on.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fuzzy c-means algorithm or FCM algorithm is the most popular fuzzy clustering
    algorithm. It was initially developed in 1973 by J.C. Dunn and then it has been
    improved multiple times. It is quite similar to k-means clustering. There is a
    concept of membership which we will cover now.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Refer to Figure 5-14\. In the first figure, we have some items or data points.
    These data points can be a part of a clustering dataset like customer transactions
    etc. In the second figure, we create a cluster for these data points. While this
    cluster is created, membership grades are allocated to each of the data points.
    These membership grades suggest the degree or the level to which a data point
    belong to a cluster. We will shortly examine the mathematical function to calculate
    these values.
  prefs: []
  type: TYPE_NORMAL
- en: We should not be confused between the degree and the probabilities. If we sum
    these degrees, we may not get 1 as these values are normalized between 0 and 1
    for all the items.
  prefs: []
  type: TYPE_NORMAL
- en: In the third figure, we can observe and compare that the point 1, is closer
    to the cluster center and hence belong to the cluster to a higher degree than
    point 2 which is closer to the boundary or the edge of the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5-14 (i) We have some data points here which can be clustered (ii) The
    data points can be grouped into two clusters. For the first cluster, the cluster
    centroid is represented using a + sign. (iii) We can observe here that point 1
    is much closer to the cluster center as compared to point 2\. So, we can conclude
    that point 1 belongs to this cluster to a higher degree than cluster 2.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![05_14](images/05_14.png)'
  prefs: []
  type: TYPE_IMG
- en: We will now venture into the technical details of the algorithm. It might be
    a little mathematically heavy though and hence this section can be treated as
    optional.
  prefs: []
  type: TYPE_NORMAL
- en: Consider we have a set of *n* items
  prefs: []
  type: TYPE_NORMAL
- en: X = {x[1], x[2], x[3], x[4], x[5]…. x[n]}
  prefs: []
  type: TYPE_NORMAL
- en: We apply FCM algorithm on these items. These n items are clustered into *c*
    fuzzy clusters based on some criteria. Let’s say that we will get from the algorithm,
    a list of c cluster centers as    C = {c[1], c[2], c[3], c[4], c[5]…. c[c]}
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm also returns a partition matrix which can be defined as below.
  prefs: []
  type: TYPE_NORMAL
- en: '![05_00](images/05_00.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, each of the element w[i,j] is the degree to which each of the element
    in X belong to cluster c[j]. This is the purpose of partition matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Mathematically, we can get w[i,j] as shown in Equation 5-1\. The proof of the
    equation is beyond the scope of the book.
  prefs: []
  type: TYPE_NORMAL
- en: '![05_14a](images/05_14a.png)'
  prefs: []
  type: TYPE_IMG
- en: The algorithm generates centroids for the clusters too. The centroid of a cluster
    is the mean of all the points in that cluster and the mean is weighted by their
    respective degrees of belonging to that cluster. wherein If we represent it mathematically,
    we can write it like in Equation 5-2.
  prefs: []
  type: TYPE_NORMAL
- en: '![05_14b](images/05_14b.png)'
  prefs: []
  type: TYPE_IMG
- en: In the Equation 5-1 and 5-2 we have a very important term “m”. m is the hyperparameter
    which is used to control the fuzziness of the clusters. The values for m ≥ 1 and
    can be kept as 2 generally.
  prefs: []
  type: TYPE_NORMAL
- en: Higher the value of m, we will receive fuzzier clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now examine step-by-step process in FCM algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we start as we start in k-means clustering. We choose the number of clusters
    we wish to have in the output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then the coefficients are allocated randomly to each of the data points.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now we wish to iterate till the algorithm has converged. Recall how the k-means
    algorithm converges, wherein we initiate the process by randomly allocating the
    number of clusters. And then iteratively we calculate the centroid for each of
    the clusters. This is how kmeans converges. For FCM, we will utilizing the similar
    process albeit with slight differences. We have added a membership value w[i,j]
    and m.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For FCM, for the algorithm to converge we calculate the centroid for each of
    the cluster as per Equation 5-2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![05_14c](images/05_14c.png)'
  prefs: []
  type: TYPE_IMG
- en: For each of the data points, we also calculate its respective coefficient for
    being in that particular cluster. We will use Equation 5-1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now we have to iterate until FCM algorithm has converged. The cost function
    which we wish to minimize is given by the ()
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![05_14d](images/05_14d.png)'
  prefs: []
  type: TYPE_IMG
- en: Once this function has been minimized, we can conclude that that the FCM algorithm
    has converged. Or in other words, we can stop the process as the algorithm has
    finished processing.
  prefs: []
  type: TYPE_NORMAL
- en: It will be a good stage now to compare with kmeans algorithm. In kmeans, we
    have a strict objective function which will allow only one cluster membership
    while for FCM clustering, we can get different clustering membership based on
    the probability scores.
  prefs: []
  type: TYPE_NORMAL
- en: FCM is a very useful for the business cases where the boundary between clusters
    is not clear and stringent. Consider in the field of bio-informatics wherein a
    gene can belong to more than one cluster. Or if we have overlapping datasets like
    in fields of the marketing analytics, image segmentation etc. FCM can give comparatively
    more robust results than kmeans.
  prefs: []
  type: TYPE_NORMAL
- en: We will now proceed to Python implementation of FCM clustering in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: '![](images/tgt.png) POP QUIZ – answer these question to check your understanding..
    Answers at the end of the book'
  prefs: []
  type: TYPE_NORMAL
- en: 1.   Fuzzy clustering allows us to create overlapping clusters. True or False.
  prefs: []
  type: TYPE_NORMAL
- en: 2.   A data point can belong to one and only one cluster. True or False.
  prefs: []
  type: TYPE_NORMAL
- en: 3.   If the value of “m” is lower, we get more clear clusters. True or False.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.4 Python implementation of FCM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have covered the process of FCM in the last section. We will now work on
    the Python implementation of FCM in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Import the necessary libraries.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 2: We will now declare a color palette, which will be used later for color
    coding the clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 3: We will define the cluster centers.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 4:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 6:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![05_14e](images/05_14e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Step 7:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![05_14f](images/05_14f.png)'
  prefs: []
  type: TYPE_IMG
- en: With this we conclude Fuzzy Clustering and we can move to Gaussian Mixture model
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Gaussian Mixture Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We would continue our discussion of soft clustering from the last section. Recall
    we introduced Gaussian Mixture Model there. Now we will elaborate on it. We will
    study the concept and have a Python implementation for it.
  prefs: []
  type: TYPE_NORMAL
- en: First let refresh our understanding of the *Gaussian distribution* or sometimes
    called as *normal distribution*. You might have heard bell-curve, it means the
    same thing.
  prefs: []
  type: TYPE_NORMAL
- en: In the Figure 5-15, observe that the distribution where the µ (mean) is 0 and
    σ² (standard deviation) is 1\. It is a perfect normal distribution curve. Compare
    the distribution in different curves here.
  prefs: []
  type: TYPE_NORMAL
- en: (Image source – Wikipedia)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5-15 A Gaussian distribution is one of the most famous distributions.
    Observe how the values of mean and standard deviation are changed and their impact
    on the corresponding curve.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![05_15](images/05_15.png)'
  prefs: []
  type: TYPE_IMG
- en: The mathematical expression for Gaussian distribution is
  prefs: []
  type: TYPE_NORMAL
- en: '![05_15a](images/05_15a.png)'
  prefs: []
  type: TYPE_IMG
- en: The equation above is also called the **Probability Density Function (pdf)**.
    In the Figure 5-15, observe that the distribution where the µ is 0 and σ² is 1\.
    It is a perfect normal distribution curve. Compare the distribution in different
    curves in the Figure 5-15 where by changing the values of mean and standard distribution,
    we are getting different graphs.
  prefs: []
  type: TYPE_NORMAL
- en: You might be wondering why we are using Gaussian distribution here. There is
    a very famous statistical theorem call as the *Central Limit Theorem*. We are
    explaining the theorem briefly here. As per the theorem, the more and more data
    we collect, the distribution tends to become more and more Gaussian. This normal
    distribution can be observed across all walks of like, be it chemistry, physics,
    mathematics, biology or any other branch. That is the beauty of Gaussian distribution.
  prefs: []
  type: TYPE_NORMAL
- en: The plot shown in Figure 5-15 is one-dimensional. We can have multi-dimensional
    Gaussian distribution too. In case of a multi-dimensional Gaussian distribution,
    we will get a 3-D figure shown in Figure 5-16\. Our input was a scalar in one-dimensional.
    Now instead of scalar, out input is now a vector, mean is also a vector and represents
    the center of the data. And hence mean has the same dimensionality as the input
    data. The variance is now covariance matrix ∑. This matrix not only tells us the
    variance in the inputs, it also comments on the relationship between different
    variables. In other words, if the value of x is changed, how the values of y get
    impacted.  Have a look at Figure 5-16 below. We can understand the relationship
    between x and y variable here.
  prefs: []
  type: TYPE_NORMAL
- en: (Image source – Wikipedia)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5-16 3-D representation of a Gaussian Distribution is shown here.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![05_16](images/05_16.png)'
  prefs: []
  type: TYPE_IMG
- en: Covariance plays a significant role here. K-means does not consider the covariance
    of a dataset, which is used in GMM model.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s examine the process of GMM clustering. Imagine we have a dataset with
    *n* items. When we use GMM clustering, we do not find the clusters using centroid
    method, instead we fit a set of *k* gaussian distributions to the data set at
    hand. In other words, we have *k* clusters. We have to determine the parameters
    for each of these Gaussian distributions which are mean, variance and weight of
    a cluster. Once the parameters for each of the distribution are determined, then
    we can find the respective probability for each of the n items to belong to k
    clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Mathematically, we can calculate the probability as shown in Equation 5-5\.
    The equation is used to for us to know that a particular point x is a linear combination
    of k Gaussians. The term Φ[j] is used to represent the strength of the Gaussian
    and it can be seen in the second equation that the sum of such strength is equal
    to 1.
  prefs: []
  type: TYPE_NORMAL
- en: '![05_16a](images/05_16a.png)'
  prefs: []
  type: TYPE_IMG
- en: For spectral clustering, we have to identify the values of Φ, ∑ and µ. As you
    would imagine, getting the values of these parameters can be a tricky business.
    It is indeed a slightly complex called Expectation-Maximization technique or EM
    technique, which we will cover now. This section is quite heavy on mathematical
    concepts and is optional.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.1 Expectation-Maximization (EM) technique
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: EM is a statistical and mathematical solution to determine the correct parameters
    for the model. There are quite a few techniques which are popular, perhaps maximum
    likelihood estimation is the most famous of them all. But at the same time, there
    could be a few challenges with maximum likelihood too. The data set might have
    missing values or in other words the dataset is incomplete. Or it is a possibility
    that a point in the dataset is generated by two different Gaussian distributions.
    Hence, it will be very difficult to determine that which distribution generated
    that data point. Here, EM can be helpful.
  prefs: []
  type: TYPE_NORMAL
- en: k-means uses only mean while GMM utilizes both mean and variance of the data.
  prefs: []
  type: TYPE_NORMAL
- en: The processes which are used to generate a data point are called *latent variables*.
    Since we do not know the exact values of these latent variables, EM firsts estimates
    the optimum values of these latent variables using the current data. Once this
    is done, then the model parameters are estimated. Using these model parameters,
    the latent variables are again determined. And using these new latent variables,
    new model parameters are derived. And the process continues till a good enough
    set of latent values and model parameters are achieved which fit the data well.
    Let’s study in more detail now.
  prefs: []
  type: TYPE_NORMAL
- en: We will take the same example we had in the last section.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine we have a dataset with *n* items. When we use GMM clustering, we do
    not find the clusters using centroid method, instead we fit a set of *k* gaussian
    distributions to the data set at hand. In other words, we have *k* clusters. We
    have to determine the parameters for each of these Gaussian distributions which
    are mean, variance and weight of a cluster. Let’s say that mean is µ[1], µ[2],
    µ[3], µ[4]…. µ[k] and covariance is ∑[1], ∑[2], ∑[3], ∑[4]…. ∑[k]. We can also
    have one more parameter to represent the density or strength of the distribution
    and it can be represented by the symbol Φ.
  prefs: []
  type: TYPE_NORMAL
- en: Now we will start with the Expectation or the E step. In this step, each data
    point is assigned to a cluster probabilistically. So, for each point we calculate
    its probability to belong to a cluster, if this value is high the point is in
    the correct cluster else the point is in the wrong cluster. In other words, we
    are calculating the probability that each data point is generated by each of the
    k Gaussians.
  prefs: []
  type: TYPE_NORMAL
- en: Since we are calculating probabilities, these are called soft assignments.
  prefs: []
  type: TYPE_NORMAL
- en: The probability is calculated using the formula in Equation 5-6\. If we look
    closely, the numerator is the probability and then we are normalizing by the denominator.
    The numerator is the same we have seen in Equation 5-5.
  prefs: []
  type: TYPE_NORMAL
- en: '![05_16b](images/05_16b.png)'
  prefs: []
  type: TYPE_IMG
- en: In the Expectation step above, for a data point x[i,j], where i is the row and
    j is the column, we are getting a matrix where rows are represented by the data
    points and columns are their respective Gaussian values.
  prefs: []
  type: TYPE_NORMAL
- en: Now the expectation step is finished, we will perform the maximization or the
    M step. In this step, we will update the values of µ, ∑ and Φ using the formula
    below in Equation 5-7\. Recall in k-means clustering, we simply take the mean
    of the data points and move ahead. We do something similar here albeit use the
    probability or the expectation we calculated in the last step.
  prefs: []
  type: TYPE_NORMAL
- en: The three values can be calculated using the Equation below. Equation 5-7 is
    the calculation of the covariances ∑[j], wherein we calculate the covariances
    of all the points, which is then weighted by the probability of that point being
    generated by Gaussian j. The mathematical proofs are beyond the scope of this
    book.
  prefs: []
  type: TYPE_NORMAL
- en: '![05_16c](images/05_16c.png)'
  prefs: []
  type: TYPE_IMG
- en: The mean µ[j], is determined by Equation 5-8\. Here, we determine the mean for
    all the points, weighted by the probability of that point being generated by Gaussian
    j.
  prefs: []
  type: TYPE_NORMAL
- en: '![05_16d](images/05_16d.png)'
  prefs: []
  type: TYPE_IMG
- en: Similarly, the density or the strength is calculated by Equation 5-9, where
    we add all the probabilities for each point to be generated by Gaussian j and
    then divide by the total number of points N.
  prefs: []
  type: TYPE_NORMAL
- en: '![05_16e](images/05_16e.png)'
  prefs: []
  type: TYPE_IMG
- en: Based on these values, new values for ∑, µ and Φ are derived, and the process
    continues till the model converges. We stop when we are able to maximize the log-likelihood
    function.
  prefs: []
  type: TYPE_NORMAL
- en: It is a complex mathematical process. We have covered it to give you the in-depth
    understanding of what happens in the background of the statistical algorithm.
    The Python implementation is much more straight forward than the mathematical
    concept which we will cover now.
  prefs: []
  type: TYPE_NORMAL
- en: '![](images/tgt.png) POP QUIZ – answer these question to check your understanding..
    Answers at the end of the book'
  prefs: []
  type: TYPE_NORMAL
- en: 1.   Gaussian distribution has mean equal to 1 and standard deviation equal
    to 0\. True or False.
  prefs: []
  type: TYPE_NORMAL
- en: 2.   GMM models does not consider the covariance of the data. True or False.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.2 Python implementation of GMM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will first import the data and then we will compare the results using kmeans
    and GMM.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: We will import all the libraries and import the dataset too.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 2: We will now drop any NA from the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 3: We will now fit a `kmeans` algorithm. We are keeping the number of
    clusters as 5\. Please note that we are not saying that they are idle number of
    clusters. These number of clusters are only for illustrative purpose. We are declaring
    a variable kmeans and then using five clusters and then the dataset is fit next.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 4: We will now plot the clusters. First, a prediction is made on the dataset
    and then the values are added to the data frame as a new column. The data is then
    plotted with different colors representing different clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: The output is shown in the plot below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![05_16f](images/05_16f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Step 4: We will now fit a GMM model. Note that the code is the same as the
    kmeans algorithm only the algorithm’s name has changed from kmeans to GaussianMixture.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 5: We will now plot the results. The output is shown below.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![05_16g](images/05_16g.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Step 6: You are advised to run the code with different values of clusters to
    observe the difference. In the plots below, the left one is kmeans with two clusters
    while the right is GMM with two clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: '![05_16h](images/05_16h.png)'
  prefs: []
  type: TYPE_IMG
- en: Gaussian distribution is one of the most widely used data distribution used.
    If we compare kmeans and GMM model, we would understand that kmeans does not consider
    the normal distribution of the data. The relationship of various data points is
    also not considered in kmeans.
  prefs: []
  type: TYPE_NORMAL
- en: Kmeans is a distance-based algorithm, GMM is a distribution based algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: In short, it is advantageous to use GMM models for creating the clusters particularly
    when we have overlapping datasets. It is a useful technique for financial and
    price modelling, NLP based solutions etc.
  prefs: []
  type: TYPE_NORMAL
- en: With this, we have covered all the algorithms in the chapter. We can now move
    to the summary.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we have explored three complex clustering algorithms. You might
    have felt the mathematical concepts a bit heavy. They are indeed heavy but give
    a deeper understanding of the process. It is not necessary that these algorithms
    are the best ones for each and every problem. Ideally, in the real-world business
    problem we should first start with classical clustering algorithms – kmeans, hierarchical
    and DBSCAN. If we do not get acceptable results then, we can try the complex algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Many times, a data science problem is equated to the choice of algorithm, which
    it is not. The algorithm is certainly an important ingredient of the entire solution,
    but it is not the only one. In the real-world datasets, there are a lot of variables
    and the amount of data is also quite big. The data has a lot of noise. We have
    to account for all these factors when we shortlist an algorithm. Algorithm maintenance
    and refresh is also one of the major questions we have in mind. All these finer
    points are covered in much detail in the last chapter of the book.
  prefs: []
  type: TYPE_NORMAL
- en: We will cover complex dimensionality reduction techniques in the next chapter.
    You can move to questions now.
  prefs: []
  type: TYPE_NORMAL
- en: Practical next steps and suggested readings
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In chapter 2 we have done clustering using various techniques. Use the datasets
    from there and perform Spectral clustering, GMM and FCM clustering to compare
    the results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are datasets provided at the end of chapter 2 which can be used for clustering.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get the credit card dataset for clustering from this Kaggle link ([https://www.kaggle.com/vipulgandhi/spectral-clustering-detailed-explanation](vipulgandhi.html))
    and from the famous IRIS dataset which we have used earlier too.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There is a great book Computational Network Science by Henry Hexmoor to study
    the mathematical concepts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Get Spectral clustering papers from the links below and study them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On spectral clustering: analysis and an algorithm [https://proceedings.neurips.cc/paper/2001/file/801272ee79cfde7fa5960571fee36b9b-Paper.pdf](file.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Spectral clustering with eigenvalue selection [http://www.eecs.qmul.ac.uk/~sgg/papers/XiangGong-PR08.pdf](papers.html)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The mathematics behind spectral clustering and the equivalence to PCA [https://arxiv.org/pdf/2103.00733v1.pdf](pdf.html)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Get GMM papers from the link below and explore them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A particular Gaussian Mixture Model for clustering [https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.79.7057&rep=rep1&type=pdf](viewdoc.html)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Application of Compound Gaussian Mixture Model in the data stream [https://ieeexplore.ieee.org/document/5620507](document.html)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Get FCM papers from the link below and study them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'FCM: the fuzzy c-means clustering algorithm [https://www.sciencedirect.com/science/article/pii/0098300484900207](pii.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A survey on Fuzzy c-means clustering techniques [https://www.ijedr.org/papers/IJEDR1704186.pdf](papers.html)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implementation of Fuzzy C-Means and Possibilistic C-Means Clustering Algorithms,
    Cluster Tendency Analysis and Cluster Validation [https://arxiv.org/pdf/1809.08417.pdf](pdf.html)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
