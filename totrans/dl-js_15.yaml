- en: Chapter 7\. Visualizing data and models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*This chapter covers*'
  prefs: []
  type: TYPE_NORMAL
- en: How to use tfjs-vis to perform custom data visualization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to peek at the internal workings of models after they are trained and gain
    useful insights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualization is an important skill for machine-learning practitioners because
    it is involved in every phase of the machine-learning workflow. Before we build
    models, we examine our data by visualizing it; during model engineering and training,
    we monitor the training process through visualization; after the model is trained,
    we use visualization to get a sense about how it works.
  prefs: []
  type: TYPE_NORMAL
- en: In [chapter 6](kindle_split_018.html#ch06), you learned the benefits of visualizing
    and understanding data before applying machine learning on it. We described how
    to use Facets, a browser-based tool that helps you get a quick, interactive look
    at your data. In this chapter, we will introduce a new tool, tfjs-vis, which helps
    you visualize your data in custom, programmatic ways. The benefit of doing so,
    versus just looking at the data in its raw format or using off-the-shelf tools
    such as Facets, is the more flexible and versatile visualization paradigm and
    the deeper understanding of data that it leads to.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the visualization of data, we will show how visualization can
    be used on deep-learning models *after* they are trained. We will use the fascinating
    examples of peeking into the “black boxes” of neural networks by visualizing their
    internal activations and computing the patterns that maximally “excite” layers
    of a convnet. This will complete the story of how visualization goes hand-in-hand
    with deep learning in each and every stage of it.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you should know why visualization is an indispensable
    part of any machine-learning workflow. You should also be familiar with the standard
    ways in which data and models are visualized in the framework of TensorFlow.js
    and be able to apply them to your own machine-learning problems.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1\. Data visualization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s start from the visualization of data, because that’s the first thing a
    machine-learning practitioner does when laying hands on a new problem. We assume
    that the visualization task is more advanced than what can be covered by Facets
    (for instance, the data isn’t in a small CSV file). For that, we will first introduce
    a basic charting API that helps you create simple and widely used types of plots,
    including line charts, scatter plots, bar charts, and histograms, in the browser.
    After we’ve covered the basic examples using hand-coded data, we will put things
    together by using an example involving the visualization of an interesting real
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.1\. Visualizing data using tfjs-vis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: tfjs-vis is a visualization library closely integrated with TensorFlow.js. Among
    its many features that this chapter will cover is a lightweight charting API under
    its `tfvis.render.*` namespace.^([[1](#ch07fn1)]) This simple and intuitive API
    allows you to make charts in the browser, with a focus on the types of charts
    most frequently used in machine learning. To help you get started with `tfvis.render`,
    we will give you a tour of the CodePen at [https://codepen.io/tfjs-book/pen/BvzMZr](https://codepen.io/tfjs-book/pen/BvzMZr),
    which showcases how to use `tfvis.render` to create various types of basic data
    plots.
  prefs: []
  type: TYPE_NORMAL
- en: ¹
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'This charting API is built on top of the Vega visualization library: [https://vega.github.io/vega/](https://vega.github.io/vega/).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Basics of tfjs-vis
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'First, note that tfjs-vis is separate from the main TensorFlow.js library.
    You can see this from how the CodePen imports tfjs-vis with a `<script>` tag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This is different from how the main TensorFlow.js library is imported:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The same distinction applies to the npm packages of tfjs-vis and TensorFlow.js
    (`@tensorflow/tfjs-vis` and `@tensorflow/tfjs`, respectively). In a web page or
    JavaScript program that depends on both TensorFlow.js and tfjs-vis, the two dependencies
    must both be imported.
  prefs: []
  type: TYPE_NORMAL
- en: Line charts
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The most commonly used type of chart is perhaps the *line chart* (a curve that
    plots a quantity against an ordered quantity). A line chart has a horizontal axis
    and a vertical axis, which are often referred to as the *x-axis* and *y-axis*,
    respectively. This type of visualization is seen everywhere in life. For example,
    we can plot how the temperature changes over the course of a day with a line chart
    in which the horizontal axis is the time of day and the vertical axis is the reading
    of a thermometer. The horizontal axis of a line chart can also be something other
    than time. For instance, we can use a line chart to show the relation between
    the therapeutic effect of a high-blood-pressure medication (how much it reduces
    blood pressure) and the dose (how much of the medication is used per day). Such
    a plot is referred to as a *dose-response curve*. Another good example of a nontemporal
    line chart is the ROC curve we discussed in [chapter 3](kindle_split_014.html#ch03).
    There, neither the x- nor y-axis has to do with time (they are the false and true
    positive rates of a binary classifier).
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a line chart with `tfvis.render`, use the `linechart()` function.
    As the first example in the CodePen (also [listing 7.1](#ch07ex01)) shows, the
    function takes three arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: The first argument is the HTML element in which the chart will be drawn. An
    empty `<div>` element suffices.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The second argument is the values of the data points in the chart. This is a
    plain old JavaScript object (POJO) with the `value` field pointing to an array.
    The array consists of a number of x-y value pairs, each of which is represented
    by a POJO with fields named `x` and `y`. The `x` and `y` values are, of course,
    the x- and y-coordinates of the data points, respectively.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The third argument, which is optional, contains additional configuration fields
    for the line chart. In this example, we use the `width` field to specify the width
    of the resultant chart (in pixels). You will see more configuration fields in
    the coming examples.^([[2](#ch07fn2)])
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ²
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[https://js.tensorflow.org/api_vis/latest/](https://js.tensorflow.org/api_vis/latest/)
    contains the full documentation of the tfjs-vis API, where you can find information
    about other configuration fields of this function.'
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: Listing 7.1\. Making a simple line chart using `tfvis.render.linechart()`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '***1*** The data series is an array of x-y pairs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***2*** The first argument is an HTML element in which the chart will be drawn.
    Here, ‘plot1’ is the ID of an empty div.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***3*** The second argument is an Object containing the key “value.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***4*** Custom configuration is passed as the third argument. In this case,
    we configure only the width of the plot.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The line chart created by the code in [listing 7.1](#ch07ex01) is shown in the
    left panel of [figure 7.1](#ch07fig01). This is a simple curve with only four
    data points. But the `linechart()` function can support curves with many more
    data points (for example, thousands). However, you will eventually run into the
    browser’s resource restrictions if you try to plot too many data points at once.
    The limit is browser- and platform-dependent and should be discovered empirically.
    In general, it is good practice to limit the size of the data to be rendered in
    interactive visualizations for the sake of a smooth and responsive UI.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.1\. Line charts created using `tfvis.render.linechart()`. Left: a
    single series, made using the code in [listing 7.1](#ch07ex01). Right: two series
    in the same axes, made using the code in [listing 7.2](#ch07ex02).'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](06fig07_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Sometimes you want to plot two curves in the same chart in order to show the
    relation between them (for instance, to contrast them with each other). You can
    make these sorts of plots with `tfvis.render.linechart()`. An example is shown
    in the right panel of [figure 7.1](#ch07fig01) and the code in [listing 7.2](#ch07ex02).
  prefs: []
  type: TYPE_NORMAL
- en: These are known as *multiseries* charts, and each line is called a *series*.
    To create a multiseries chart, you must include an additional field, `series`,
    in the first argument to `linechart()`. The value of the field is an array of
    strings. The strings are the names given to the series and will be rendered as
    a legend in the resulting chart. In the example code, we call our series `'My
    series 1'` and `'My series 2'`.
  prefs: []
  type: TYPE_NORMAL
- en: The `value` field of the first argument also needs to be specified properly
    for a multiseries chart. For our first example, we provided an array of points,
    but for multiseries plots, we must provide an array of arrays. Each element of
    the nested array is the data points of a series and has the same format as the
    values array we saw in [listing 7.1](#ch07ex01) when we plotted a single-series
    chart. Therefore, the length of the nested array must match the length of the
    `series` array, or an error will occur.
  prefs: []
  type: TYPE_NORMAL
- en: The chart created by [listing 7.2](#ch07ex02) is shown in the right panel of
    [figure 7.1](#ch07fig01). As you can see in the chart in electronic versions of
    this book, tfjs-vis has picked two different colors (blue and orange) to render
    the two curves in. This default coloring scheme works well in general because
    blue and orange are easy to tell apart. If there are more series to render, other
    new colors will be selected automatically.
  prefs: []
  type: TYPE_NORMAL
- en: The two series in this example chart are a little special in the sense that
    they have exactly the same set of x-coordinate values (1, 2, 3, and 4). However,
    in general, the x-coordinate values of different series in a multiseries chart
    don’t have to be identical. You are encouraged to try this in exercise 1 at the
    end of this chapter. But, be aware that it is not always a good idea to plot two
    curves in the same chart. For example, if the two curves have very different and
    nonoverlapping y-value ranges, plotting them in the same line chart will make
    the variation in each curve harder to see. In such cases, it is better to plot
    them in separate line charts.
  prefs: []
  type: TYPE_NORMAL
- en: Another thing worth pointing out in [listing 7.2](#ch07ex02) is the custom labels
    for the axes. We use the `xLabel` and `yLabel` fields in the configuration object
    (the third argument passed to `linechart()`) in order to label the x- and y-axis
    as custom strings of our choice. In general, it is good practice to always label
    your axes, as it makes the charts more self-explanatory. tfjs-vis will always
    label your axes as `x` and `y` if you don’t specify `xLabel` and `yLabel`, which
    is what happened in [listing 7.1](#ch07ex01) and the left panel of [figure 7.1](#ch07fig01).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.2\. Making a line chart with two series using `tfvis.render.linechart()`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '***1*** To show multiple series in the same axes, make values an array consisting
    of multiple arrays of x-y pairs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***2*** Series names must be provided when plotting multiple series.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***3*** Overrides the default x- and y-axis labels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scatter plots
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*Scatter* *plots* are another type of chart you can create with `tfvis.render`.
    The most salient difference between a scatter plot and a line chart is the fact
    that a scatter plot doesn’t connect the data points with line segments. This makes
    scatter plots suitable for cases in which the ordering among data points is unimportant.
    For example, a scatter plot may plot the population of a few countries against
    their per-capita GDPs. In such a plot, the primary piece of information is the
    relation between the x- and y-values, not an ordering among the data points.'
  prefs: []
  type: TYPE_NORMAL
- en: In `tfvis.render`, the function that lets you create scatter plots is `scatterplot()`.
    As the example in [listing 7.3](#ch07ex03) shows, `scatterplot()` can render multiple
    series, just like `linechart()`. In fact, the APIs of `scatterplot()` and `linechart()`
    are practically identical, as you can see by comparing [listing 7.2](#ch07ex02)
    with [listing 7.3](#ch07ex03). The scatter plot created by [listing 7.3](#ch07ex03)
    is shown in [figure 7.2](#ch07fig02).
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.2\. A scatter plot that contains two series, made with the code in
    [listing 7.3](#ch07ex03).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](07fig02_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Listing 7.3\. Making a scatter plot using `tfvis.render.scatterplot()`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '***1*** As in linechart(), uses an array of x-y pair arrays to show multiple
    series in a scatter plot'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***2*** Always remember to label your axes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bar charts
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As its name indicates, a *bar chart* uses bars to show the magnitude of quantities.
    Such bars usually start from zero at the bottom so that the ratios between the
    quantities can be read from the relative heights of the bars. Therefore, bar charts
    are a good choice when the ratio between quantities is of importance. For example,
    it is natural to use a bar chart to show the annual revenue of a company over
    a few years. In this case, the relative heights of the bars give the viewer an
    intuitive sense of how the revenue changes from one quarter to another in terms
    of the ratio between them. This makes bar charts distinct from line charts and
    scatter plots, in which the values are not necessarily “anchored” at zero.
  prefs: []
  type: TYPE_NORMAL
- en: To create a bar chart with `tfvis.render`, use `barchart()`. You can find an
    example in [listing 7.4](#ch07ex04). The bar chart created by the code is shown
    in [figure 7.3](#ch07fig03). The API of `barchart()` is similar to those of `linechart()`
    and `scatterplot()`. However, an important difference should be noted. The first
    argument passed to `barchart()` is not an object consisting of a `value` field.
    Instead, it is a simple array of index-value pairs. The horizontal values are
    not specified with a field called `x`, but are instead specified with a field
    called `index`. Similarly, the vertical values are not specified with a field
    called `y`, but are instead associated with a field called `value`. Why this difference?
    It is because the horizontal values of a bar in a bar chart don’t have to have
    a number. Instead, they can be either strings or numbers, as is shown by our example
    in [figure 7.3](#ch07fig03).
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.3\. A bar chart consisting of both string- and numeric-named bars,
    made with the code in [listing 7.4](#ch07ex04)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](07fig03_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Listing 7.4\. Creating a bar chart using `tfvis.render.barchart()`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '***1*** Notice how the index of a bar chart can be numeric or a string. Note
    that the order of the elements matters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Histograms
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The three types of plots described previously let you plot the values of a certain
    quantity. Sometimes, the detailed quantitative values are not as important as
    the *distribution* of the values. For example, consider an economist looking at
    the annual household income data from the result of a national census. To the
    economist, the detailed income values are not the most interesting piece of information.
    They contain too much information (yes, sometimes too much information can be
    a bad thing!). Instead, the economist wants a more succinct summary of the income
    values. They’re interested in how such values are distributed—that is, how many
    of them fall below US$20,000, how many of them are between $20,000 and $40,000,
    or between $40,000 and $60,000, and so forth. *Histograms* are a type of chart
    suited for such a visualization task.
  prefs: []
  type: TYPE_NORMAL
- en: A histogram assigns the values into *bins*. Each bin is simply a continuous
    range for the value, with a lower bound and an upper bound. The bins are chosen
    to be adjacent to each other so as to cover all possible values. In the prior
    example, the economist may use bins such as 0 ~ 20k, 20k ~ 40k, 40k ~ 60k, and
    so forth. Once such a set of `N` bins is chosen, you can write a program to count
    the number of individual data points that fall into each of the bins. Executing
    this program will give you *N* numbers (one for each bin). You can then plot the
    numbers using vertical bars. This gives you a histogram.
  prefs: []
  type: TYPE_NORMAL
- en: '`tfvis.render.histogram()` does all these steps for you. This saves you the
    effort of determining the bounds of the bins and counting the examples by the
    bins. To invoke `histogram()`, simply pass an array of numbers, as shown in the
    following listing. These numbers don’t need to be sorted in any order.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.5\. Visualizing a value distribution using `tfvis.render.histogram()`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '***1*** Uses automatically generated bins'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***2*** Specifies the number of bins explicitly'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In [listing 7.5](#ch07ex05), there are two slightly different `histogram()`
    calls. The first call doesn’t specify any custom options beyond the width of the
    plot. In this case, `histogram()` uses its built-in heuristics to calculate the
    bins. This results in seven bins –4 ~ –2, –2 ~ 0, 0 ~ 2, . . ., 8 ~ 10, as shown
    in the left panel of [figure 7.4](#ch07fig04). When divided among these seven
    bins, the histogram shows the highest value in the bin 4 ~ 6, which contains a
    count of 4 because four of the values in the data array are 5\. Three bins of
    the histogram (–2 ~ 0, 2 ~ 4, and 6 ~ 8) have zero value because none of the elements
    of the data points falls into any of these three bins.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.4\. Histograms of the same data, plotted with the automatically calculated
    bins (left) and an explicitly specified number of bins (right). The code that
    generates these histograms is in [listing 7.5](#ch07ex05).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](07fig04_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Hence, we can argue that the default heuristics end up with too many bins for
    our particular data points. If there are fewer bins, then it will be less likely
    that any of them will end up empty. You can use the configuration field `maxBins`
    to override the default binning heuristics and limit the number of bins. This
    is what’s done by the second `histogram()` call in [listing 7.5](#ch07ex05), the
    result of which is shown on the right in [figure 7.4](#ch07fig04). You can see
    that by limiting the number of bins to three, all the bins become nonempty.
  prefs: []
  type: TYPE_NORMAL
- en: Heatmaps
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A *heatmap* displays a 2D array of numbers as a grid of colored cells. The color
    of each cell reflects the relative magnitude of the elements of the 2D array.
    Traditionally, “cooler” colors such as blue and green are used to represent lower
    values, while “warmer” colors such as orange and red are used to show higher ones.
    This is why these plots are called heatmaps. Perhaps the most frequently encountered
    examples of heatmaps in deep learning are confusion matrices (see the iris-flower
    example in [chapter 3](kindle_split_014.html#ch03)) and attention matrices (see
    the date-conversion example in [chapter 9](kindle_split_021.html#ch09)). tfjs-vis
    provides the function `tfvis.render.heatmap()` to support the rendering of this
    type of visualization.
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 7.6](#ch07ex06) shows how to make a heatmap to visualize a made-up
    confusion matrix involving three classes. The value of the confusion matrix is
    specified in the `values` field of the second input argument. The names of the
    classes, which are used to label the columns and rows of the heatmap, are specified
    as `xTickLabels` and `yTickLabels`. Do not confuse these tick labels with `xLabel`
    and `yLabel` in the third argument, which are for labeling the entire x- and y-axes.
    [Figure 7.5](#ch07fig05) shows the resulting heatmap plot.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.5\. The heatmap rendered by the code in [listing 7.6](#ch07ex06). It
    shows an imaginary confusion matrix involving three classes.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](07fig05_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Listing 7.6\. Visualizing 2D tensors using `tfvis.render.heatmap()`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '***1*** The values passed to heatmap() can be a nested JavaScript array (as
    shown here) or a 2D tf.Tensor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***2*** xTickLabels is used to label the individual columns along the x-axis.
    Don’t confuse it with xLabel. Likewise, yTickLabels is used to label the individual
    rows along the y-axis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***3*** xLabel and yLabel are used to label the entire axes, unlike xTickLabel
    and yTickLabel.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***4*** Apart from the ‘blues’ color map shown here, there are also ‘greyscale’
    and ‘viridian’.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This concludes our quick tour of four major types of charts supported by `tfvis.render`.
    If your future work involves data visualization using tfjs-vis, odds are that
    you will use these charts a lot. [Table 7.1](#ch07table01) provides a brief summary
    of the chart types in order to help you decide which one to use for a given visualization
    task.
  prefs: []
  type: TYPE_NORMAL
- en: Table 7.1\. A summary of the five major types of charts supported by tfjs-vis
    under the `tfvis.render` namespace
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Name of chart | Corresponding function in tfjs-vis | Suitable visualization
    tasks and machine-learning examples |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Line chart | tfvis.render.linechart() | A scalar (y-value) varying with another
    scalar (x-value) that has an intrinsic ordering (time, dose, and so on). Multiple
    series can be plotted in the same axes: for example, metrics from the training
    and validation sets, each of which is plotted against training-epoch number. |'
  prefs: []
  type: TYPE_TB
- en: '| Scatter plot | tfvis.render.scatterplot() | x-y scalar value pairs that do
    not have an intrinsic ordering, such as the relation between two numeric columns
    of a CSV dataset. Multiple series can be plotted in the same axes. |'
  prefs: []
  type: TYPE_TB
- en: '| Bar chart | tfvis.render.barchart() | A set of values belonging to a small
    number of categories, such as accuracies (as percent numbers) achieved by several
    models on the same classification problem. |'
  prefs: []
  type: TYPE_TB
- en: '| Histogram | tfvis.render.histogram() | A set of values of which the distribution
    is of primary interest, such as the distribution of parameter values in the kernel
    of a dense layer. |'
  prefs: []
  type: TYPE_TB
- en: '| Heatmap | tfvis.render.heathmap() | A 2D array of numbers to be visualized
    as a 2D grid of cells, each element of which is color-coded to reflect the magnitude
    of the corresponding value: for example, confusion matrix of a multiclass classifier
    ([section 3.3](kindle_split_014.html#ch03lev1sec3)); attention matrix of a sequence-to-sequence
    model ([section 9.3](kindle_split_021.html#ch09lev1sec3)). |'
  prefs: []
  type: TYPE_TB
- en: '7.1.2\. An integrative case study: Visualizing weather data with tfjs-vis'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The CodePen examples in the previous section used small, hand-coded data. In
    this section, we will show how to use the charting features of tfjs-vis on a much
    larger and more interesting real dataset. This will demonstrate the true power
    of the API and make a case for the value of such data visualization in the browser.
    This example will also highlight some of the nuances and gotchas you may run into
    when using the charting API on real problems.
  prefs: []
  type: TYPE_NORMAL
- en: The data we will use is the Jena-weather-archive dataset. It includes the measurements
    collected with a variety of meteorological instruments at a location in Jena,
    Germany, over a course of eight years (between 2009 and 2017). The dataset, which
    can be downloaded from the Kaggle page (see [www.kaggle.com/pankrzysiu/weather-archive-jena](http://www.kaggle.com/pankrzysiu/weather-archive-jena)),
    comes in a 42MB CSV file. It consists of 15 columns. The first column is a timestamp,
    while the remaining columns are weather data such as temperature (`T deg(C)`),
    air pressure (`p (mbar)`), relative humidity (`rh (%s)`), wind velocity (`wv (m/s)`),
    and so on. If you examine the timestamps, you can see that they have a 10-minute
    spacing, reflecting the fact that the measurements were made every 10 minutes.
    This is a rich dataset to visualize, explore, and try machine learning on. In
    the following sections, we will try making weather forecasts using various machine-learning
    models. In particular, we will predict the temperature of the next day using the
    weather data from the 10 preceding days. But before we embark on this exciting
    weather forecasting task, let’s follow the principle of “always look at your data
    before trying machine learning on it” and see how tfjs-vis can be used to plot
    the data in a clear and intuitive fashion.
  prefs: []
  type: TYPE_NORMAL
- en: 'To download and run the Jena-weather example, use the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Limiting the amount of data for efficient and effective visualization
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The Jena-weather dataset is quite large. At a file size of 42 MB, it is bigger
    than all the CSV or tabular datasets you’ve seen in this book so far. This leads
    to two challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first challenge is for the computer: if you plot all the data from the
    eight years at once, the browser tab will run out of resources, become unresponsive,
    and probably crash. Even if you limit yourself to only 1 of the 14 columns, there
    are still about 420,000 data points to show. This is more than what tfjs-vis (or
    any JavaScript plotting library, for that matter) can safely render at a time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second challenge is for the user: it is hard for a human to look at a large
    amount of data at once and make sense out of it. For instance, how is someone
    supposed to look at all 420,000 data points and extract useful information from
    them? Just like the computer, the human brain has limited information-processing
    bandwidth. The job of a visualization designer is to present the most relevant
    and informative aspects of the data in an efficient way.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We use three tricks to address these challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of plotting the data from the whole eight years at once, we let the
    user choose what time range to plot using an interactive UI. This is the purpose
    of the Time Span drop-down menu in the UI (see the screenshots in [figures 7.6](#ch07fig06)
    and [7.7](#ch07fig07)). The time-span options include Day, Week, 10 Days, Month,
    Year, and Full. The last one corresponds to the whole eight years. For any of
    the other time spans, the UI allows the user to go back and forth in time. This
    is what the left-arrow and right-arrow buttons are for.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Figure 7.6\. Line charts of temperature (`T (degC)`) and air pressure (`p (mbar)`)
    from the Jena-weather-archive dataset, plotted at two different time scales. Top:
    10-day time span. Notice the daily cycle in the temperature curve. Bottom: 1-year
    time span. Notice the annual cycle in the temperature curve and the slight tendency
    for air pressure to be more stable during spring and summer than during other
    seasons.'
  prefs:
  - PREF_IND
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](07fig06_alt.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 7.7\. An example scatter plot from the Jena-weather demo. The plot shows
    the relation between air density (rho, vertical axis) and temperature (T, horizontal)
    over a time period of 10 days, where a negative correlation can be seen.
  prefs:
  - PREF_IND
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](07fig07_alt.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: For any time span longer than a week, we *downsample* the time series before
    plotting them on the screen. For example, consider the time span Month (30 days).
    The full data for this time span contains about 30 * 24 * 6 = 4.32k data points.
    In the code in [listing 7.7](#ch07ex07), you can see that we only plot every sixth
    data point when showing the data from a month. This cuts the number of plotted
    data points down to 0.72k, a significant reduction in the rendering cost. But
    to human eyes, this six-fold reduction in the data-point count barely makes a
    difference.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similar to what we did with the Time Span drop-down menu, we include a drop-down
    menu in the UI so that the user can choose what weather data to plot at any given
    time. Notice the drop-down menus labeled Data Series 1 and Data Series 2\. By
    using them, the user may plot any 1 or any 2 of the 14 columns as line charts
    on the screen, in the same axes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Listing 7.7](#ch07ex07) shows the code responsible for making the charts like
    the ones in [figure 7.6](#ch07fig06). Despite the fact that the code calls `tfvis.render.linechart()`
    just like the CodePen example in the previous section, it is much more abstract
    compared to the code in the previous listings. This is because in our web page,
    we need to defer the decision of what quantities to plot according to the UI state.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.7\. Weather data as a multiseries line chart (in jena-weather/index.js)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '***1*** jenaWeatherData is an object that helps us organize and retrieve the
    weather data from the CSV file. See jena-weather/data.js.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***2*** Specifies the time span for visualization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***3*** Chooses the appropriate stride (downsampling factor)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***4*** Takes advantage of the fact that tfjs-vis’s line chart supports multiple
    series'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***5*** Always label the axes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You are encouraged to explore the data-visualization UI. It contains a lot
    of interesting patterns you can discover about weather. For example, the top panel
    of [figure 7.6](#ch07fig06) shows how the normalized temperature (`T (degC)`)
    and normalized air pressure (`p (mbar)`) vary over a time period of 10 days. In
    the temperature curve, you can see a clear daily cycle: the temperature tends
    to peak around the middle of the day and bottom out shortly after midnight. On
    top of the daily cycle, you can also see a more global trend (a gradual increase)
    over the 10-day period. By contrast, the air-pressure curve doesn’t show a clear
    pattern. The bottom panel of the same figure shows the same measurements over
    the time span of a year. There, you can see the annual cycle of temperature: it
    peaks around August and reaches the bottom around January. The air pressure again
    shows a less clear-cut pattern than temperature at this time scale. The pressure
    can vary in a somewhat chaotic fashion over the entire year, although there appears
    to be a tendency for it to be less variable around summer than in winter. By looking
    at the same measurements at different time scales, we can notice various interesting
    patterns. All these patterns are nearly impossible to notice if we look at just
    the raw data in the numerical CSV format.'
  prefs: []
  type: TYPE_NORMAL
- en: One thing you might have noticed in the charts in [figure 7.6](#ch07fig06) is
    that they show normalized values of temperature and air pressure instead of their
    absolute values, which is due to the fact that the Normalize Data check box in
    the UI was checked when we made these plots. We briefly mentioned normalization
    when discussing the Boston-housing model back in [chapter 2](kindle_split_013.html#ch02).
    The normalization there involved subtracting the mean and dividing the result
    by the standard deviation. We did this in order to improve model training. The
    normalization we performed here is exactly the same. However, it is not just for
    the accuracy of our machine-learning model (to be covered in the next section)
    but is also for visualization. Why? If you try unchecking the Normalize Data check
    box when the chart shows temperature and air pressure, you’ll immediately see
    the reason. The temperature measurement varies in the range between –10 and 40
    (on the Celsius scale), while the air pressure resides in the range between 980
    and 1,000\. When plotted in the same axes without normalization, the two quantities
    with vastly different ranges force the y-axis to expand to a very large range,
    causing both curves to look like basically flat lines with tiny variations. Normalization
    avoids this problem by mapping all measurements to a distribution of zero mean
    and unit standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 7.7](#ch07fig07) shows an example of plotting two weather measurements
    against each other as a scatter plot, a mode you can activate by checking the
    Plot Against Each Other check box and making sure that neither of the Data Series
    drop-down menus is set to None. The code for making such scatter plots is similar
    to the `makeTimeSerieChart()` function in [listing 7.7](#ch07ex07) and is therefore
    omitted here for conciseness. You can study it in the same file (jena-weather/index.js)
    if you are interested in the details.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The example scatter plot shows the relation between the normalized air density
    (y-axis) and normalized temperature (x-axis). Here, you can spot a fairly strong
    negative correlation between the two quantities: the air density gets lower as
    the temperature increases. This example plot uses the 10-day time span, but you
    can verify that the trend largely holds at other time spans as well. This kind
    of correlation between variables is easy to visualize with scatter plots but much
    harder to discover by just looking at the text-format data. This is another example
    of the value afforded by data visualization.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.2\. Visualizing models after training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous sections, we showed how visualization can be useful for data.
    In this section, we will show you how to visualize various aspects of models after
    they are trained in order to gain useful insight. To this end, we will focus primarily
    on convnets that take images as inputs, because they are used widely and produce
    interesting visualization results.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may have heard the remark that deep neural networks are “black boxes.”
    Don’t let this remark mislead you into thinking that it’s hard to get any information
    from the inside of a neural network during its inference or training. To the contrary,
    it is fairly easy to peek at what each layer is doing inside a model written in
    TensorFlow.js.^([[3](#ch07fn3)]) Furthermore, as far as convnets are concerned,
    the internal representations they learn are highly amenable to visualization,
    in large part because they are representations of visual concepts. Since 2013,
    a wide array of techniques has been developed for visualizing and interpreting
    these representations. Since it’s impractical to cover all the interesting techniques,
    we’ll cover three of the most basic and useful ones:'
  prefs: []
  type: TYPE_NORMAL
- en: ³
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What that remark really means is that the large number of mathematical operations
    that occur in a deep neural network, even if they can be accessed, are harder
    to describe in layperson’s terms as compared with certain other types of machine-learning
    algorithms, such as decision trees and logistic regression. For example, with
    a decision tree, you can walk down the branching points one by one and explain
    why a certain branch is chosen by verbalizing the reason as a simple sentence
    like “because factor X is greater than 0.35.” That problem is referred to as *model
    interpretability* and is a different matter from what we are covering in this
    section.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Visualizing the outputs of intermediate layers (intermediate activations)
    of a convnet*—This is useful for understanding how successive convnet layers transform
    their inputs, and for getting a first idea of the visual features learned by individual
    convnet filters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Visualizing convnet filters by finding input images that maximally activate
    them*—This is useful for understanding what visual pattern or concept each filter
    is sensitive to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Visualizing heatmaps of class activation in an input image*—This helps in
    understanding which parts of an input image play the most important role in causing
    the convnet to generate the final classification result, which can also be useful
    for interpreting how a convnet reaches its output and for “debugging” incorrect
    outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code we will use to showcase these techniques is in the visualize-convnet
    example from the tfjs-examples repo. To run the example, use these commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The `yarn visualize` command is different from the `yarn watch` command you’ve
    seen in previous examples. In addition to building and launching the web page,
    it performs some additional steps outside the browser. First, it installs some
    required Python libraries, followed by downloading and converting the VGG16 model
    (a well-known and widely used deep convnet) into TensorFlow.js format. The VGG16
    model has been pretrained on the large-scale ImageNet dataset and is available
    as a Keras application. Once the model conversion is complete, `yarn visualize`
    performs a series of analyses on the converted model in tfjs-node. Why are these
    steps carried out in Node.js instead of the browser? Because VGG16 is a relatively
    large convnet.^([[4](#ch07fn4)]) As a result, several of the steps are computationally
    heavy and run much faster in the less resource-restricted environment in Node.js.
    The computation can be further speeded up if you use tfjs-node-gpu instead of
    the default tfjs-node (this requires a CUDA-enabled GPU with the required driver
    and libraries installed; see [appendix A](kindle_split_027.html#app01)):'
  prefs: []
  type: TYPE_NORMAL
- en: ⁴
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To get an idea of how large VGG16 is, realize that its total weight size is
    528 MB, as compared to the <10MB weight size of MobileNet.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Once the computationally heavy steps are completed in Node.js, they will generate
    a set of image files in the dist/folder. As its last step, `yarn visualize` will
    compile and launch a web server for a set of static web files including those
    images, in addition to opening the index page in your browser.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `yarn visualize` command contains a few additional configurable flags.
    For example, by default, it performs computation and visualization on eight filters
    per convolutional layer of interest. You can change the number of filters by using
    the `--filters` flag: for example, `yarn visualize --filters 32`. Also, the default
    input image used by `yarn visualize` is the cat.jpg image that comes with the
    source code. You can use other image files by using the `--image` flag.^([[5](#ch07fn5)])
    Now let’s look at the visualization results based on the cat.jpg image and 32
    filters.'
  prefs: []
  type: TYPE_NORMAL
- en: ⁵
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Most common image formats, including JPEG and PNG, are supported.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 7.2.1\. Visualizing the internal activations of a convnet
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Here, we compute and display the feature map generated by various convolutional
    layers of the VGG16 model given an input image. These feature maps are called
    *internal* activation because they are not the model’s final output (the model’s
    final output is a length-1,000 vector that represents the probability scores for
    the 1,000 ImageNet classes). Instead, they are the intermediate steps of the model’s
    computation. These internal activations give us a view into how the input is decomposed
    into different features learned by the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall from [chapter 4](kindle_split_015.html#ch04) that the output of a convolutional
    layer has the NHWC shape `[numExamples, height, width, channels]`. Here, we are
    dealing with a single input image, so `numExamples` is 1\. We want to visualize
    the output of each convolutional layer along three remaining dimensions: height,
    width, and channels. The height and width of a convolutional layer’s output are
    determined by its filter size, padding, and strides, as well as the height and
    width of the layer’s input. In general, they get smaller and smaller as you go
    deeper into a convnet. On the other hand, the value of `channels` generally gets
    larger as you go deeper, as the convnet extracts a larger and larger number of
    features through successive layers of representation transformation. These channels
    of convolutional layers cannot be interpreted as different color components. Instead,
    they are the learned feature dimensions. This is why our visualization breaks
    them into separate panels and draws them in grayscale. [Figure 7.8](#ch07fig08)
    shows the activations from five convolutional layers of VGG16 given the cat.jpg
    input image.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.8\. Internal activation from several convolutional layers of VGG16
    performing inference on the cat.jpg image. The original input image is shown on
    the left, together with the top three classes output by the model and their associated
    probability scores. The five layers visualized are the layers named `block1_conv1`,
    `block2_conv1`, `block3_conv2`, `block4_conv2`, and `block5_conv3`. They are ordered
    by their depth in the VGG16 model from top to bottom. That is, `block1_conv1`
    is the closest to the input layer, while `block5_conv1` is the closest to the
    output layer. Note that all internal-activation images are scaled to the same
    size for visualization purposes, even though the activations have smaller sizes
    (lower resolution) in the later layers due to successive convolution and pooling.
    This can be seen in the coarse pixel patterns in the later layers.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](07fig08_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The first thing you may notice in the internal activations is that they look
    increasingly different from the original input as you go deeper in the network.
    The earlier layers (such as `block1_conv1`) appear to encode relatively simple
    visual features such as edges and colors. For example, the arrow labeled “A” points
    at an internal activation that seems to respond to the yellow and pink colors.
    The arrow labeled “B” points at an internal activation that seems to be about
    edges along certain orientations in the input image.
  prefs: []
  type: TYPE_NORMAL
- en: 'But the later layers (such as `block4_conv2` and `block5_conv3`) show activation
    patterns that are more and more removed from simple pixel-level features in the
    input image. For example, the arrow labeled “C” in [figure 7.8](#ch07fig08) points
    at a filter in `block4_ conv2` that seems to encode the cat’s facial features,
    including the ears, eyes, and nose. This is a concrete example of the incremental
    feature extraction that we showed schematically in [figure 4.6](kindle_split_015.html#ch04fig06)
    of [chapter 4](kindle_split_015.html#ch04). However, note that not all filters
    in later layers can be explained verbally in a straightforward way. Another interesting
    observation is that the “sparsity” of the activation maps also increases with
    the depth of the layer: in the first layer shown in [figure 7.8](#ch07fig08),
    all filters are activated (show a nonconstant pixel pattern) by the input image;
    however, in the last layer, some of the layers become blank (constant pixel pattern;
    for example, see the last row in the right panel of [figure 7.8](#ch07fig08)).
    This means the features encoded by those blank filters are absent from this particular
    input image.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You just witnessed an important universal characteristic of the representations
    learned by deep convnets: the features extracted by a layer become increasingly
    more abstract with the depth of the layer. The activations of deeper layers carry
    less and less information about the details in the input, and more and more information
    about the target (in this case, which of the 1,000 ImageNet classes the image
    belongs to). So, a deep neural network effectively acts as an *information distillation
    pipeline*, with raw data going in and being repeatedly transformed so that aspects
    in the input that are irrelevant to the task are filtered out, and aspects that
    are useful for the task are gradually magnified and refined. Even though we showed
    this through a convnet example, this characteristic holds for other deep neural
    networks (such as MLPs) as well.'
  prefs: []
  type: TYPE_NORMAL
- en: The aspects of input images that a convnet finds useful might be different from
    what the human visual system finds useful. The convnet training is driven by data
    and hence is prone to biases in the training data. For instance, the paper by
    Marco Ribeiro and colleagues listed in the “[Materials for further reading and
    exploration](#ch07lev1sec3)” section at the end of the chapter points out a case
    in which the image of a dog got misclassified as a wolf due to the presence of
    snow in the background, presumably because the training images contained instances
    of wolves against snowy backgrounds but no dogs against similar backgrounds.
  prefs: []
  type: TYPE_NORMAL
- en: These are the useful insights we gained by visualizing the internal activation
    patterns of a deep convnet. The following subsection describes how to write code
    in TensorFlow.js to extract these internal activations.
  prefs: []
  type: TYPE_NORMAL
- en: Deep dive into how internal activations are extracted
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The steps for extracting the internal activations are encapsulated in the `writeInternalActivationAndGetOutput()`
    function ([listing 7.8](#ch07ex08)). It takes as its input a TensorFlow.js model
    object that has already been constructed or loaded and the names of the layers
    in question (`layerNames`). The key step is creating a new model object (`compositeModel`)
    with multiple outputs, including the output of the specified layers and the output
    of the original model. `compositeModel` is constructed with the `tf.model()` API,
    as you saw in the Pac-Man and simple-object-detection examples in [chapter 5](kindle_split_016.html#ch05).
    The nice thing about `compositeModel` is that its `predict()` method returns all
    the layers’ activations, along with the model’s final prediction (see the `const`
    named `outputs`). The rest of the code in [listing 7.8](#ch07ex08) (from visualize-convnet/main.js)
    is about the more mundane task of splitting the layers’ outputs into individual
    filters and writing them to files on disk.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.8\. Calculating the internal activation of a convnet in Node.js
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '***1*** Constructs a model that returns all the desired internal activations,
    in addition to the final output of the original model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***2*** outputs is an array of tf.Tensor’s, including the internal activations
    and the final output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***3*** Splits the activation of the convolutional layer by filter'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***4*** Formats activation tensors and writes them to disk'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '7.2.2\. Visualizing what convolutional layers are sensitive to: Maximally-
    y activating images'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another way to illustrate what a convnet learns is by finding the input images
    that its various internal layers are sensitive to. What we mean by a filter being
    sensitive to a certain input image is a maximal activation in the filter’s output
    (averaged across its output height and width dimensions) under the input image.
    By looking at such maximally activating inputs for various layers of the convnet,
    we can infer what each layer is trained to respond to.
  prefs: []
  type: TYPE_NORMAL
- en: 'The way in which we find the maximally activating images is through a trick
    that flips the “normal” neural network training process on its head. Panel A of
    [figure 7.9](#ch07fig09) shows schematically what happens when we train a neural
    network with `tf.Model.fit()`. We freeze the input data and allow the weights
    of the model (such as the kernels and biases of all the trainable layers) to be
    updated from the loss function^([[6](#ch07fn6)]) via backpropagation. However,
    there is no reason why we can’t swap the roles of the input and the weights: we
    can freeze the weights and allow the *input* to be updated through backpropagation.
    In the meantime, we tweak the loss function so that it causes the backpropagation
    to nudge the input in a way that maximizes the output of a certain convolutional
    filter when averaged across its height and width dimensions. This process is schematically
    shown in panel B of [figure 7.9](#ch07fig09) and is called *gradient ascent in
    input space*, as opposed to the *gradient descent in weight space* that underlies
    typical model training. The code that implements gradient descent in input space
    is shown in the next subsection and can be studied by interested readers.'
  prefs: []
  type: TYPE_NORMAL
- en: ⁶
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This diagram can be viewed as a simplified version of [figure 2.9](kindle_split_013.html#ch02fig09),
    which we used to introduce backpropagation back in [chapter 2](kindle_split_013.html#ch02).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Figure 7.9\. A schematic diagram showing the basic idea behind how the maximally
    activating image for a convolutional filter is found through gradient ascent in
    input space (panel B) and how that differs from the normal neural network training
    process based on gradient descent in weight space (panel A). Note that this figure
    differs from some of the model diagrams shown previously in that it breaks the
    weights out from the model. This is for highlighting the two sets of quantities
    that can be updated through backpropagation: the weights and the input.'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](07fig09_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 7.10](#ch07fig10) shows the result of performing the gradient-ascent-in-input-space
    process on four convolutional layers of the VGG16 model (the same model that we
    used to show internal activations). As in the previous illustration, the depth
    of the layers increases from the top to the bottom of the figure. A few interesting
    patterns can be gleaned from these maximally activating input images:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, these are color images instead of the grayscale internal activations
    like the ones in the previous section. This is because they are in the format
    of the convnet’s actual input: an image consisting of three (RGB) channels. Hence,
    they can be displayed in color.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The shallowest layer (`block1_conv1`) is sensitive to simple patterns such as
    global color values and edges with certain orientations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The intermediate-depth layers (such as `block2_conv1`) respond maximally to
    simple textures made from combining different edge patterns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The filters in deeper layers begin to respond to more complex patterns that
    show some resemblance to visual features in natural images (from the ImageNet
    training data, of course), such as grains, holes, colorful stripes, feathers,
    waves, and so forth.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figure 7.10\. Maximally activating input images for four layers of the VGG16
    deep convnet. These images are computed through 80 iterations of gradient ascent
    in input space.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](07fig10_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In general, as the depth of the layer increases, the patterns get more and more
    removed from the pixel level and become more and more large-scale and complex.
    This reflects the layer-by-layer distillation of features by the deep convnet,
    composing patterns of patterns. Looking at the filters of the same layer, even
    though they share similar levels of abstraction, there is considerable variability
    in the detail patterns. This highlights the fact that each layer comes up with
    multiple representations of the same input in mutually complementary ways in order
    to capture the largest possible amount of useful information for solving the task
    that the network is trained to solve.
  prefs: []
  type: TYPE_NORMAL
- en: Deep dive into gradient ascent in input space
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the visualize-convnet example, the core logic for gradient ascent in input
    space is in the `inputGradientAscent()` function in main.js and is shown in [listing
    7.9](#ch07ex09). The code runs in Node.js due to its time- and memory-consuming
    nature.^([[7](#ch07fn7)]) Note that even though the basic idea behind gradient
    ascent in input space is analogous to model training based on gradient descent
    in weight space (see [figure 7.10](#ch07fig10)), we cannot reuse `tf.Model.fit()`
    directly because that function is specialized to freeze the input and update the
    weights. Instead, we need to define a custom function that calculates a “loss”
    given an input image. This is the function defined by the line
  prefs: []
  type: TYPE_NORMAL
- en: ⁷
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For convnets smaller than VGG16 (such as MobileNet and MobileNetV2), it is possible
    to run this algorithm within a reasonable amount of time in the web browser.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Here, `auxModel` is an auxiliary model object created with the familiar `tf.model()`
    function. It has the same input as the original model but outputs the activation
    of a given convolutional layer. We invoke the `apply()` method of the auxiliary
    model in order to obtain the value of the layer’s activation. `apply()` is similar
    to `predict()` in that it executes a model’s forward path. However, `apply()`
    provides finer-grained control, such as setting the `training` option to `true`,
    as is done in the prior line of code. Without setting `training` to `true`, backpropagation
    would not be possible because the forward pass disposes intermediate layer activations
    for memory efficiency by default. The `true` value in the `training` flag lets
    the `apply()` call preserve those internal activations and therefore enable backpropagation.
    The `gather()` call extracts a specific filter’s activation. This is necessary
    because the maximally activating input is calculated on a filter-by-filter basis,
    and the results differ between filters even of the same layer (see the example
    results in [figure 7.10](#ch07fig10)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have the custom loss function, we pass it to `tf.grad()` in order to
    obtain a function that gives us the gradient of the loss with respect to the input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The important thing to realize here is that `tf.grad()` doesn’t give us the
    gradient values directly; instead, it gives us a function (`gradFunction` in the
    prior line) that will return the gradient values when invoked.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have this gradient function, we invoke it in a loop. In each iteration,
    we use the gradient value it returns to update the input image. An important nonobvious
    trick here is to normalize the gradient values before adding them to the input
    image, which ensures that the update in each iteration has a consistent magnitude:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This iterative update to the input image is performed 80 times, giving us the
    results shown in [figure 7.10](#ch07fig10).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.9\. Gradient ascent in input space (in Node.js, from visualize-convnet/main.js)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '***1*** Creates an auxiliary model for which the input is the same as the original
    model, but the output is the convolutional layer of interest'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***2*** This function calculates the value of the convolutional layer’s output
    at the designated filter index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***3*** This function calculates the gradient of the convolutional filter’s
    output with respect to the input image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***4*** Generates a random image as the starting point of the gradient ascent'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***5*** Important trick: scales the gradient with the magnitude (norm) of the
    gradient'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***6*** Performs one step of gradient ascent: updates the image along the direction
    of the gradient'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 7.2.3\. Visual interpretation of a convnet’s classification result
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The last post-training convnet visualization technique we will introduce is
    the *class activation map* (CAM) algorithm. The question that CAM aims to answer
    is “which parts of the input image play the most important roles in causing the
    convnet to output its top classification decision?” For instance, when the cat.jpg
    image was passed to the VGG16 network, we got a top class of “Egyptian cat” with
    a probability score of 0.89\. But by looking at just the image input and the classification
    output, we can’t tell which parts of the image are important for this decision.
    Surely some parts of the image (such as the cat’s head) must have played a greater
    role than other parts (for example, the white background). But is there an objective
    way to quantify this for any input image?
  prefs: []
  type: TYPE_NORMAL
- en: 'The answer is yes! There are multiple ways of doing this, and CAM is one of
    them.^([[8](#ch07fn8)]) Given an input image and a classification result from
    a convnet, CAM gives you a heat map that assigns importance scores to different
    parts of the image. [Figure 7.11](#ch07fig11) shows such CAM-generated heat maps
    overlaid on top of three input images: a cat, an owl, and two elephants. In the
    cat result, we see that the outline of the cat’s head has the highest values in
    the heat map. We can make the post hoc observation that this is because the outline
    reveals the shape of the animal’s head, which is a distinctive feature for a cat.
    The heat map for the owl image also meets our expectation because it highlights
    the head and wing of the animal. The result from the image with two elephants
    is interesting because the image differs from the other two images in that it
    contains two individual animals instead of one. The heat map generated by CAM
    assigns high importance scores to the head regions of both elephants in the image.
    There is a clear tendency for the heat map to focus on the trunks and ears of
    the animals, which may reflect the fact that the length of the trunk and the size
    of the ears are important in telling African elephants (the top class from the
    network) apart from Indian elephants (the third class from the network).'
  prefs: []
  type: TYPE_NORMAL
- en: ⁸
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The CAM algorithm was first described in Bolei Zhou et al., “Learning Deep Features
    for Discriminative Localization,” 2016, [http://cnnlocalization.csail.mit.edu/](http://cnnlocalization.csail.mit.edu/).
    Another well-known method is Local Interpretable Model-Agnostic Explanations (LIME).
    See [http://mng.bz/yzpq](http://mng.bz/yzpq).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Figure 7.11\. Class activation maps (CAMs) for three input images to the VGG16
    deep convnet. The CAM heat maps are overlaid on the original input images.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](07fig11_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Technical side of the CAM algorithm
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'As powerful as the CAM algorithm is, the idea behind it is actually not complicated.
    In a nutshell, each pixel in a CAM map shows how much the probability score of
    the winning class will change if the pixel value is increased by a unit amount.
    To go into the details a little more, the following steps are involved in CAM:'
  prefs: []
  type: TYPE_NORMAL
- en: Find the last (that is, deepest) convolutional layer of the convnet. In VGG16,
    this layer is named `block5_conv3`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the gradient of the network’s output probability for the winning class
    with respect to the output of the convolutional layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The gradient has a shape of `[1, h, w, numFilters]`, where `h`, `w`, and `numFilters`
    are the layer’s output height, width, and filter count, respectively. We then
    average the gradient across the example, height, and width dimensions, which gives
    us a tensor of shape `[numFilters]`. This is an array of importance scores, one
    for each filter of the convolutional layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take the importance-score tensor (of shape `[numFilters]`), and multiply it
    with the actual output value of the convolutional layer (of shape `[1, h, w, numFilters]`),
    with broadcasting (see [appendix B](kindle_split_030.html#app02), section B.2.2).
    This gives us a new tensor of shape `[1, h, w, numFilters]` and is an “importance-scaled”
    version of the layer’s output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, average the importance-scaled layer output across the last (filter)
    dimension and squeeze out the first (example) dimension, which yields a grayscale
    image of shape `[h, w]`. The values in this image are a measure of how important
    each part of the image is for the winning classification result. However, this
    image contains negative values and is of smaller dimensions than the original
    input image (that is, 14 × 14 versus 224 × 224 in our VGG16 example). So, we zero
    out the negative values and up-sample the image before overlaying it on the input
    image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The detailed code is in the function named `gradClassActivationMap()` in visualize-convnet/main.js.
    Although this function runs in Node.js by default, the amount of computation it
    involves is significantly less than the gradient-ascent-in-input-space algorithm
    that we saw in the previous section. So, you should be able to run the CAM algorithm
    using the same code in the browser with acceptable speed.
  prefs: []
  type: TYPE_NORMAL
- en: 'We talked about two things in this chapter: how to visualize data before it
    goes into training a machine-learning model and how to visualize a model after
    it’s trained. We intentionally skipped the important step in between—that is,
    visualization of the model *while* it’s being trained. This will be the focus
    of the next chapter. The reason why we single out the training process is that
    it is related to the concepts and phenomena of underfitting and overfitting, which
    are absolutely critical for any supervised-learning tasks and therefore deserve
    special treatment. Spotting and correcting underfitting and overfitting are made
    significantly easier by visualization. In the next chapter, we’ll revisit the
    tfjs-vis library we introduced in the first part of the chapter and see that it
    can be useful for showing how a model-training process is progressing, in addition
    to its data-visualization power discussed in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: Materials for further reading and exploration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin, “Why Should I Trust
    You? Explaining the Predictions of Any Classifier,” 2016, [https://arxiv.org/pdf/1602.04938.pdf](https://arxiv.org/pdf/1602.04938.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorSpace ([tensorspace.org](http://tensorspace.org)) uses animated 3D graphics
    to visualize the topology and internal activations of convnets in the browser.
    It is built on top of TensorFlow.js, three.js, and tween.js.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The TensorFlow.js tSNE library ([github.com/tensorflow/tfjs-tsne](http://github.com/tensorflow/tfjs-tsne))
    is an efficient implementation of the t-distributed Stochastic Neighbor Embedding
    (tSNE) algorithm based on WebGL. It can help you visualize high-dimensional datasets
    by projecting them to a 2D space while preserving the important structures in
    the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Experiment with the following features of `tfjs.vis.linechart()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modify the code in [listing 7.2](#ch07ex02) and see what happens when the two
    series being plotted have different sets of x-coordinate values. For example,
    try making the x-coordinate values 1, 3, 5, and 7 for the first series and 2,
    4, 6, and 8 for the second series. You can fork and modify the CodePen from [https://codepen.io/tfjs-book/pen/BvzMZr](https://codepen.io/tfjs-book/pen/BvzMZr).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The line charts in the example CodePen are all made with data series without
    duplicate x-coordinate values. Explore how the `linechart()` function handles
    data points with identical x-coordinate values. For example, in a data series,
    include two data points that both have x-value 0 but have different y-values (such
    as –5 and 5).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: In the visualize-convnet example, use the `--image` flag to the `yarn visualize`
    command to specify your own input image. Since we used only animal images in [section
    7.2](#ch07lev1sec2), explore other types of image content, such as people, vehicles,
    household items, and natural scenery. See what useful insights you can gain from
    the internal activations and CAMs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the example in which we calculated the CAM of VGG16, we computed the gradients
    of the probability score for the *winning* class with respect to the last convolutional
    layer’s output. What if instead we compute the gradients for a *nonwinning* class
    (such as that of the lower probability)? We should expect the resulting CAM image
    to *not* highlight key parts that belong to the actual subject of the image. Confirm
    this by modifying the code of the visualize-convnet example and rerunning it.
    Specifically, the class index for which the gradients will be computed is specified
    as an argument to the function `gradClassActivationMap()` in visualize-convnet/cam.js.
    The function is called in visualize-convnet/main.js.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We studied the basic usage of tfjs-vis, a visualization library tightly integrated
    with TensorFlow.js. It can be used to render basic types of charts in the browser.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing data is an indispensable part of machine learning. Efficient and
    effective presentation of data can reveal patterns and provide insights that are
    otherwise hard to obtain, as we showed by using the Jena-weather-archive data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rich patterns and insights can be extracted from trained neural networks. We
    showed the steps and results of
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing the internal-layer activations of a deep convnet.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculating what the layers are maximally responsive to.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Determining which parts of an input image are most relevant to the convnet’s
    classification decision. These help us understand what is learned by the convnet
    and how it operates during inference.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
