["```py\nclass Scalar:\n  def __init__(self, val):\n    self.val = val\n    self.grad = 0\n```", "```py\ndef __repr__(self):\n  return f\"Value: {self.val}, Gradient: {self.grad}\"\n```", "```py\nclass Scalar:\n  def __init__(self, val):\n    self.val = val\n    self.grad = 0.0\n  def __repr__(self):\n    return f\"Value: {self.val}, Gradient: {self.grad}\"\n\nprint(Scalar(3.14))\n```", "```py\nValue: 3.14, Gradient: 0\n```", "```py\ndef __add__(self, other):\n    out = Scalar(self.val + other.val)\n    return out\n\n  def __mul__(self, other):\n    out = Scalar(self.val * other.val)\n    return out\n```", "```py\nclass Scalar:\n  def __init__(self, val):\n    self.val = val\n    self.grad = 0\n\n  def __repr__(self):\n    return f\"Value: {self.val}, Gradient: {self.grad}\"\n\n  def __add__(self, other):\n    out = Scalar(self.val + other.val)\n    return out\n\n  def __mul__(self, other):\n    out = Scalar(self.val * other.val)\n    return out\n\nScalar(3) + Scalar(4) * Scalar(5)\n```", "```py\nValue: 23, Gradient: 0\n```", "```py\nclass Scalar:\n  def __init__(self, val):\n    self.val = val\n    self.grad = 0\n    self.backward = lambda: None      ❶\n  def __repr__(self):\n    return f\"Value: {self.val}, Gradient: {self.grad}\"\n  def __add__(self, other):\n    out = Scalar(self.val + other.val)\n    return out\n  def __mul__(self, other):\n    out = Scalar(self.val * other.val)\n    return out\n```", "```py\nx = Scalar(2.0)\ny = x\n```", "```py\nx.grad = 0.0\n```", "```py\nx = Scalar(2.0)\ny = x\n\nx.grad = 0.0\ny.grad = 1.0\ny.backward()\n```", "```py\nprint(x.grad)\n```", "```py\ndef __add__(self, other):\n  out = Scalar(self.val + other.val)\n\n  def backward():    \n    self.grad += out.grad   \n    other.grad += out.grad    \n    self.backward()              \n    other.backward()        \n  out.backward = backward   \n\n  return out\n```", "```py\nx = Scalar(2.0)\ny = x + x\n```", "```py\nx.grad = 0.0\ny.grad = 1.0\ny.backward()\n```", "```py\nx.grad\n```", "```py\n2.0\n```", "```py\ndef __mul__(self, other):\n  out = Scalar(self.val * other.val)\n\n  def backward():           \n    self.grad += out.grad * other.val   \n    other.grad += out.grad * self.val\n    self.backward()               \n    other.backward()         \n  out.backward = backward    \n\n  return out\n```", "```py\nclass Scalar:\n  def __init__(self, val):\n    self.val = val\n    self.grad = 0\n    self.backward = lambda: None\n  def __repr__(self):\n    return f\"Value: {self.val}, Gradient: {self.grad}\"\n  def __add__(self, other):\n    out = Scalar(self.val + other.val)\n    def backward():\n      self.grad += out.grad\n      other.grad += out.grad\n      self.backward(), other.backward()\n    out.backward = backward\n    return out\n  def __mul__(self, other):\n    out = Scalar(self.val * other.val)\n    def backward():\n      self.grad += out.grad * other.val\n      other.grad += out.grad * self.val\n      self.backward(), other.backward()\n    out.backward = backward\n    return out\n```", "```py\nx = Scalar(3.0)\ny = x * x\n```", "```py\nx.grad = 0.0\ny.grad = 1.0\ny.backward()\n```", "```py\nx.grad\n```", "```py\n6.0.\n```", "```py\nx = Scalar(3.0)\ny = (x * x * x) + (Scalar(4.0) * x) + Scalar(1.0)\n```", "```py\nx.grad = 0.0\ny.grad = 1.0\ny.backward()\n\nx.grad\n```", "```py\nX = pt.linspace(-5, 5, 100)\n```", "```py\ny = 2.0 * X + pt.randn(len(X))\n```", "```py\nw = pt.randn(1, requires_grad = True)\n```", "```py\ndef forward(X):\n  y_pred = X * w\n  mse_loss = pt.mean((y_pred - y) ** 2)\n  return mse_loss\n```", "```py\nLEARNING_RATE = 0.03\n\nfor _ in range(25):\n  mse_loss = forward(X)\n  w.grad = None                ❶\n  mse_loss.backward()\n  w.data -= LEARNING_RATE * w.grad\n\nprint(\"MSE \", mse_loss.data, \" W \", w.data)\n```", "```py\nMSE  tensor(0.7207)  W  tensor([1.9876])\n```", "```py\npt.pow(X.T @ X, -1) * (X.T @ y)\n```", "```py\nw.data -= LEARNING_RATE * w.grad\n```", "```py\nimport torch as pt\npt.manual_seed(0)\n\nX = pt.linspace(-5, 5, 100)\ny = 2 * X + pt.randn(len(X))\n\nw = pt.randn(1, requires_grad = True)\n\ndef forward(X):\n  y_pred = X * w\n  return y_pred\n\ndef loss(y_pred, y):\n  mse_loss = pt.mean((y_pred - y) ** 2)\n  return mse_loss\n\nLEARNING_RATE = 0.03\noptimizer = pt.optim.SGD([w], lr = LEARNING_RATE)   ❶\n\nEPOCHS = 25                                         ❷\nfor _ in range(EPOCHS):\n  y_pred = forward(X)\n  mse = loss(y_pred, y)\n  mse.backward()\n\n  optimizer.step()                                  ❸\n  optimizer.zero_grad()                             ❹\n\nprint(w.item())\n```", "```py\noptimizer = pt.optim.Adam([w], lr = LEARNING_RATE)\n```", "```py\ntorch.optim.Optimizer(params, defaults)\n```", "```py\nimport torch.utils.data.Dataset\n\nclass MapStyleDataset(Dataset):\n    def __getitem__(self, index):    ❶\n      ...\n    def __len__(self):               ❷\n      ...\n```", "```py\nimport torch as pt\nfrom torch.utils.data import TensorDataset\npt.manual_seed(0)\n\nX = pt.linspace(-5, 5, 100)\ny = 2 * X + pt.randn(len(X))\n\ntrain_ds = TensorDataset(y, X)   ❶\n```", "```py\nprint(train_ds[0])\n```", "```py\n(tensor(-11.1258), tensor(-5.))\n```", "```py\nassert\n len(train_ds) == 100\n```", "```py\nfrom torch.utils.data import DataLoader\ntrain_dl = DataLoader(train_ds)\n```", "```py\nlen(next(iter(train_dl))) == 1\n```", "```py\ntrain_dl = DataLoader(train_ds, batch_size = 25)\nlen(next(iter(train_dl)))\n```", "```py\ntrain_dl = DataLoader(train_ds, batch_size = 33)\nfor idx, (y_batch, X_batch) in enumerate(train_dl):\n  print(idx, len(X_batch))\n```", "```py\n0 33\n1 33\n2 33\n3 1\n```", "```py\ntrain_dl = DataLoader(train_ds, batch_size = 33, drop_last=True)\nfor idx, (y_batch, X_batch) in enumerate(train_dl):\n  print(idx, len(X_batch))\n```", "```py\n0 33\n1 33\n2 33\n```", "```py\nimport math\n\ndef lcm(a, b):\n    return a * b / math.gcd(a, b)\n\nlcm(12, 33) / 33\n```", "```py\nimport torch as pt\nfrom torch.utils.data import TensorDataset, DataLoader\npt.manual_seed(0)\n\nX = pt.linspace(-5, 5, 100)\ny = 2 * X + pt.randn(len(X))\n\ntrain_ds = TensorDataset(y, X)                  ❶\ntrain_dl = DataLoader(train_ds, batch_size=1)   ❷\n\nw = pt.empty(1, requires_grad = True)\n\ndef forward(X):\n  y_pred =  X * w\n  return y_pred\n\ndef loss(y_pred, y):\n  mse_loss = pt.mean((y_pred - y) ** 2)\n  return mse_loss\n\nLEARNING_RATE = 0.003\noptimizer = pt.optim.SGD([w], lr = LEARNING_RATE)\n\nEPOCHS = 25\nfor _ in range(EPOCHS):\n  for y_batch, X_batch in train_dl:             ❸\n    y_pred = forward(X_batch)                   ❹\n    mse = loss(y_pred, y_batch)                 ❺\n    mse.backward()\n\n    optimizer.step()\n    optimizer.zero_grad()\n\nprint(w.item())\n```", "```py\ntrain_dl = DataLoader(train_ds, batch_size=25),\n```", "```py\ntrain_dl = DataLoader(train_ds, batch_size=51)\n```"]