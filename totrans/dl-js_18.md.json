["```js\ngit clone https://github.com/tensorflow/tfjs-examples.git\ncd tfjs-examples/lstm-text-generation\nyarn && yarn watch\n```", "```js\nyarn train shakespeare \\\n      --lstmLayerSize 128,128 \\\n      --epochs 120 \\\n      --savePath ./my-shakespeare-model\n```", "```js\nexport function createModel(sampleLen,                    ***1***\n                            charSetSize,                  ***2***\n                            lstmLayerSizes) {             ***3***\n  if (!Array.isArray(lstmLayerSizes)) {\n    lstmLayerSizes = [lstmLayerSizes];\n  }\n\n  const model = tf.sequential();\n  for (let i = 0; i < lstmLayerSizes.length; ++i) {\n    const lstmLayerSize = lstmLayerSizes[i];\n    model.add(tf.layers.lstm({                            ***4***\n      units: lstmLayerSize,\n      returnSequences: i < lstmLayerSizes.length - 1,     ***5***\n      inputShape: i === 0 ?\n          [sampleLen, charSetSize] : undefined            ***6***\n    }));\n  }\n  model.add(\n      tf.layers.dense({\n        units: charSetSize,\n        activation: 'softmax'\n  }));                                                    ***7***\n\n  return model;\n}\n```", "```js\nconst optimizer = tf.train.rmsprop(learningRate);\nmodel.compile({optimizer: optimizer, loss: 'categoricalCrossentropy'});\n```", "```js\nexport function sample(probs, temperature) {\n  return tf.tidy(() => {\n    const logPreds = tf.div(\n        tf.log(probs),                                                     ***1***\n        Math.max(temperature, 1e-6));                                      ***2***\n    const isNormalized = false;\n    return tf.multinomial(logPreds, 1, null, isNormalized).dataSync()[0];  ***3***\n  });\n}\n```", "```js\nconst logPreds = tf.div(tf.log(probs),\n                        Math.max(temperature, 1e-6));\n```", "```js\n[0.1, 0.7, 0.2]\n```", "```js\nlog([0.1, 0.7, 0.2]) / 0.25 = [-9.2103, -1.4267, -6.4378]\n```", "```js\nexp([-9.2103, -1.4267, -6.4378]) / sum(exp([-9.2103, -1.4267, -6.4378]))\n= [0.0004, 0.9930, 0.0066]\n```", "```js\nlog([0.1, 0.7, 0.2]) / 0.75 = [-3.0701, -0.4756, -2.1459]\nexp([-3.0701, -0.4756, -2.1459]) / sum([-3.0701, -0.4756, -2.1459])\n= [0.0591, 0.7919 0.1490]\n```", "```js\ngit clone https://github.com/tensorflow/tfjs-examples.git\ncd tfjs-examples/fashion-mnist-vae\nyarn\nyarn download-data\n```", "```js\nyarn train\n```", "```js\nyarn train --gpu\n```", "```js\nyarn watch\n```", "```js\n    z = zMean + exp(zLogVar * 0.5) * epsilon\n    ```", "```js\n    z = zMean.add(zLogVar.mul(0.5).exp().mul(epsilon));\n    ```", "```js\nclass ZLayer extends tf.layers.Layer {\n  constructor(config) {\n    super(config);\n  }\n  computeOutputShape(inputShape) {\n    tf.util.assert(inputShape.length === 2 && Array.isArray(inputShape[0]),\n        () => `Expected exactly 2 input shapes. ` +\n              `But got: ${inputShape}`);             ***1***\n    return inputShape[0];                            ***2***\n  }\n  call(inputs, kwargs) {\n    const [zMean, zLogVar] = inputs;\n    const batch = zMean.shape[0];\n    const dim = zMean.shape[1];\n\n    const mean = 0;\n    const std = 1.0;\n    const epsilon = tf.randomNormal(                 ***3***\n        [batch, dim], mean, std);                    ***3***\n    return zMean.add(                              ***4***\n        zLogVar.mul(0.5).exp().mul(epsilon));        ***4***\n  }\n  static get ClassName() {                           ***5***\n    return 'ZLayer';\n  }\n}\ntf.serialization.registerClass(ZLayer);              ***6***\n```", "```js\nfunction encoder(opts) {\n  const {originalDim, intermediateDim, latentDim} = opts;\n\n  const inputs = tf.input({shape: [originalDim], name: 'encoder_input'});\n  const x = tf.layers.dense({units: intermediateDim, activation: 'relu'})\n                .apply(inputs);                                              ***1***\n  const zMean = tf.layers.dense({units: latentDim, name: 'z_mean'}).apply(x);***2***\n  const zLogVar = tf.layers.dense({                                          ***2***\n        units: latentDim,                                                    ***2***\n        name: 'z_log_var'                                                    ***2***\n      }).apply(x);                                                         ***2*** ***3***\n  const z =                                                                  ***3***\n      new ZLayer({name: 'z', outputShape: [latentDim]}).apply([zMean,        ***3***\n     zLogVar]);                                                              ***3***\n\n  const enc = tf.model({\n    inputs: inputs,\n    outputs: [zMean, zLogVar, z],\n    name: 'encoder',\n  })\n  return enc;\n}\n```", "```js\nfunction decoder(opts) {\n  const {originalDim, intermediateDim, latentDim} = opts;\n\n  const dec = tf.sequential({name: 'decoder'});   ***1***\n  dec.add(tf.layers.dense({\n    units: intermediateDim,\n    activation: 'relu',\n    inputShape: [latentDim]\n  }));\n  dec.add(tf.layers.dense({\n    units: originalDim,\n    activation: 'sigmoid'                         ***2***\n  }));\n  return dec;\n}\n```", "```js\nfunction vae(encoder, decoder) {\n  const inputs = encoder.inputs;                     ***1***\n  const encoderOutputs = encoder.apply(inputs);\n  const encoded = encoderOutputs[2];                 ***2***\n  const decoderOutput = decoder.apply(encoded);\n  const v = tf.model({                               ***3***\n    inputs: inputs,\n    outputs: [decoderOutput, ...encoderOutputs],     ***4***\n    name: 'vae_mlp',\n  })\n  return v;\n}\n```", "```js\nfunction vaeLoss(inputs, outputs) {\n  const originalDim = inputs.shape[1];\n  const decoderOutput = outputs[0];\n  const zMean = outputs[1];\n  const zLogVar = outputs[2];\n\n  const reconstructionLoss =                                              ***1***\n      tf.losses.meanSquaredError(inputs, decoderOutput).mul(originalDim); ***1***\n\n  let klLoss = zLogVar.add(1).sub(zMean.square()).sub(zLogVar.exp());\n  klLoss = klLoss.sum(-1).mul(-0.5);                                      ***2***\n  return reconstructionLoss.add(klLoss).mean();                           ***3***\n}\n```", "```js\n  for (let i = 0; i < epochs; i++) {\n    console.log(`\\nEpoch #${i} of ${epochs}\\n`)\n    for (let j = 0; j < batches.length; j++) {\n      const currentBatchSize = batches[j].length\n      const batchedImages = batchImages(batches[j]);            ***1***\n      const reshaped =\n          batchedImages.reshape([currentBatchSize, vaeOpts.originalDim]);\n\n      optimizer.minimize(() => {                                ***2***\n        const outputs = vaeModel.apply(reshaped);\n        const loss = vaeLoss(reshaped, outputs, vaeOpts);\n        process.stdout.write('.');                              ***3***\n        if (j % 50 === 0) {\n          console.log('\\nLoss:', loss.dataSync()[0]);\n        }\n        return loss;\n      });\n      tf.dispose([batchedImages, reshaped]);\n    }\n    console.log('');\n    await generate(decoderModel, vaeOpts.latentDim);            ***4***\n  }\n```", "```js\nfunction buildDiscriminator() {\n  const cnn = tf.sequential();\n\n  cnn.add(tf.layers.conv2d({\n    filters: 32,\n    kernelSize: 3,\n    padding: 'same',\n    strides: 2,\n    inputShape: [IMAGE_SIZE, IMAGE_SIZE, 1]                                ***1***\n  }));\n  cnn.add(tf.layers.leakyReLU({alpha: 0.2}));\n  cnn.add(tf.layers.dropout({rate: 0.3}));                                 ***2***\n\n  cnn.add(tf.layers.conv2d(\n      {filters: 64, kernelSize: 3, padding: 'same', strides: 1}));\n  cnn.add(tf.layers.leakyReLU({alpha: 0.2}));\n  cnn.add(tf.layers.dropout({rate: 0.3}));\n  cnn.add(tf.layers.conv2d(\n      {filters: 128, kernelSize: 3, padding: 'same', strides: 2}));\n  cnn.add(tf.layers.leakyReLU({alpha: 0.2}));\n  cnn.add(tf.layers.dropout({rate: 0.3}));\n\n  cnn.add(tf.layers.conv2d(\n      {filters: 256, kernelSize: 3, padding: 'same', strides: 1}));\n  cnn.add(tf.layers.leakyReLU({alpha: 0.2}));\n\n  cnn.add(tf.layers.dropout({rate: 0.3}));\n\n  cnn.add(tf.layers.flatten());\n\n  const image = tf.input({shape: [IMAGE_SIZE, IMAGE_SIZE, 1]});\n  const features = cnn.apply(image);\n\n  const realnessScore =                                                    ***3***\n      tf.layers.dense({units: 1, activation: 'sigmoid'}).apply(features);  ***3***\n  const aux = tf.layers.dense({units: NUM_CLASSES, activation: 'softmax'}) ***4***\n                  .apply(features);                                        ***4***\n  return tf.model({inputs: image, outputs: [realnessScore, aux]});\n}\n```", "```js\nfunction buildGenerator(latentSize) {\n  const cnn = tf.sequential();\n  cnn.add(tf.layers.dense({\n    units: 3 * 3 * 384,                                ***1***\n    inputShape: [latentSize],\n    activation: 'relu'\n  }));\n  cnn.add(tf.layers.reshape({targetShape: [3, 3, 384]}));\n\n  cnn.add(tf.layers.conv2dTranspose({                  ***2***\n    filters: 192,\n    kernelSize: 5,\n    strides: 1,\n    padding: 'valid',\n    activation: 'relu',\n    kernelInitializer: 'glorotNormal'\n  }));\n  cnn.add(tf.layers.batchNormalization());\n\n  cnn.add(tf.layers.conv2dTranspose({                  ***3***\n    filters: 96,\n    kernelSize: 5,\n    strides: 2,\n    padding: 'same',\n    activation: 'relu',\n    kernelInitializer: 'glorotNormal'\n  }));\n  cnn.add(tf.layers.batchNormalization());\n\n  cnn.add(tf.layers.conv2dTranspose({                  ***4***\n    filters: 1,\n    kernelSize: 5,\n    strides: 2,\n    padding: 'same',\n    activation: 'tanh',\n    kernelInitializer: 'glorotNormal'\n  }));\n\n  const latent = tf.input({shape: [latentSize]});      ***5***\n\n  const imageClass = tf.input({shape: [1]});           ***6***\n\n  const classEmbedding = tf.layers.embedding({         ***7***\n    inputDim: NUM_CLASSES,\n    outputDim: latentSize,\n    embeddingsInitializer: 'glorotNormal'\n  }).apply(imageClass);\n\n  const h = tf.layers.multiply().apply(                ***8***\n\n      [latent, classEmbedding]);                       ***8***\n\n  const fakeImage = cnn.apply(h);\n  return tf.model({                                    ***9***\n   inputs: [latent, imageClass],                       ***9***\n   outputs: fakeImage                                  ***9***\n  });                                                  ***9***\n}\n```", "```js\n  discriminator.compile({\n    optimizer: tf.train.adam(args.learningRate, args.adamBeta1),\n    loss: ['binaryCrossentropy', 'sparseCategoricalCrossentropy']\n  });\n```", "```js\ngit clone https://github.com/tensorflow/tfjs-examples.git\ncd tfjs-examples/mnist-acganyarn\n```", "```js\nyarn train\n```", "```js\nyarn train --logDir /tmp/mnist-acgan-logs\n```", "```js\ntensorboard --logdir /tmp/mnist-acgan-logs\n```", "```js\nawait generator.save(saveURL);\n```", "```js\n    const latentVectors = getLatentVectors(10);\n    const sampledLabels = tf.tensor2d(\n        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 1]);\n    const generatedImages =\n        generator.predict([latentVectors, sampledLabels]).add(1).div(2);\n```"]