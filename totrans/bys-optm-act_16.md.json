["```py\ndef forrester_1d(x):\n    y = -((x + 1) ** 2) * torch.sin(2 * x + 2) / 5 + 1\n    return y.squeeze(-1)\n```", "```py\nwith torch.no_grad():\n    predictive_distribution = likelihood(model(xs))\n    predictive_mean = predictive_distribution.mean\n    predictive_upper, predictive_lower =\n    ➥predictive_distribution.confidence_region()\n```", "```py\nplt.figure(figsize=(8, 6))\n\nplt.plot(xs, ys, label=\"objective\", c=\"r\")  ❶\nplt.scatter(                                ❷\n    train_x,                                ❷\n    train_y,                                ❷\n    marker=\"x\",                             ❷\n    c=\"k\",                                  ❷\n    alpha=0.1 if variational else 1,        ❷\n    label=\"observations\",                   ❷\n  )                                         ❷\n```", "```py\nplt.plot(xs, predictive_mean, label=\"mean\")\nplt.fill_between(\n    xs.flatten(),\n    predictive_upper,\n    predictive_lower,\n    alpha=0.3,\n    label=\"95% CI\"\n)\n```", "```py\nif variational:\n  inducing_points =\n  ➥model.variational_strategy.inducing_points.detach().clone()\n  with torch.no_grad():\n      inducing_mean = model(inducing_points).mean\n\n  plt.scatter(\n      inducing_points.squeeze(-1),\n      inducing_mean,\n      marker=\"D\",\n      c=\"orange\",\n      s=100,\n      label=\"inducing pts\"\n  )                         ❶\n```", "```py\ntorch.manual_seed(0)\ntrain_x = torch.rand(size=(1000, 1)) * 10 - 5\ntrain_y = forrester_1d(train_x)\n```", "```py\nxs = torch.linspace(-7.5, 7.5, 151).unsqueeze(1)\nys = forrester_1d(xs)\n```", "```py\nplt.figure(figsize=(8, 6))\nplt.scatter(\n    train_x,\n    train_y,\n    c=\"k\",\n    marker=\"x\",\n    s=10,\n    label=\"observations\"\n)\nplt.legend();\n```", "```py\nclass GPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super().__init__(train_x, train_y, likelihood)\n        self.mean_module = gpytorch.means.\n        ➥ConstantMean()                                  ❶\n        self.covar_module = gpytorch.kernels.\n        ➥ScaleKernel(                                    ❷\n            gpytorch.kernels.RBFKernel()                  ❷\n        )                                                 ❷\n\n    def forward(self, x):                                 ❸\n        mean_x = self.mean_module(x)                      ❸\n        covar_x = self.covar_module(x)                    ❸\n        return gpytorch.distributions.MultivariateNormal  ❸\n        ➥(mean_x, covar_x)                               ❸\n```", "```py\nlikelihood = gpytorch.likelihoods.GaussianLikelihood()\nmodel = GPModel(train_x, train_y, likelihood)\n```", "```py\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)          ❶\nmll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)  ❷\n\nmodel.train()                                                      ❸\nlikelihood.train()                                                 ❸\n\nfor i in tqdm(range(500)):                                         ❹\n    optimizer.zero_grad()                                          ❹\n\n    output = model(train_x)                                        ❹\n    loss = -mll(output, train_y)                                   ❹\n\n    loss.backward()                                                ❹\n    optimizer.step()                                               ❹\n\nmodel.eval()                                                       ❺\nlikelihood.eval()                                                  ❺\n```", "```py\nvisualize_gp_belief(model, likelihood)\n```", "```py\nNumericalWarning: CG terminated in 1000 iterations with average\n  residual norm...\n```", "```py\nfor epoch in range(2):                        ❶\n\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        inputs, labels = data                 ❷\n\n        optimizer.zero_grad()                 ❸\n\n        outputs = net(inputs)                 ❹\n        loss = criterion(outputs, labels)     ❹\n        loss.backward()                       ❹\n        optimizer.step()                      ❹\n```", "```py\nclass ApproximateGPModel(gpytorch.models.ApproximateGP):             ❶\n  def __init__(self, inducing_points):                               ❷\n    variational_distribution =                                       ❸\n    ➥gpytorch.variational.CholeskyVariationalDistribution(          ❸\n        inducing_points.size(0)                                      ❸\n    )                                                                ❸\n    variational_strategy = gpytorch.variational.VariationalStrategy( ❸\n        self,                                                        ❸\n        inducing_points,                                             ❸\n        variational_distribution,                                    ❸\n        learn_inducing_locations=True,                               ❸\n    )                                                                ❸\n    super().__init__(variational_strategy)                           ❸\n\n    ...                                                              ❹\n```", "```py\nclass ApproximateGPModel(gpytorch.models.ApproximateGP):\n    def __init__(self, inducing_points):\n        ...\n\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(\n            gpytorch.kernels.RBFKernel()\n        )\n```", "```py\nmodel = ApproximateGPMod1el(train_x[:50, :])            ❶\nlikelihood = gpytorch.likelihoods.GaussianLikelihood()\n```", "```py\noptimizer = torch.optim.Adam(\n    [\n        {\"params\": model.parameters()},\n        {\"params\": likelihood.parameters()}    ❶\n    ],\n    lr=0.01\n)\n```", "```py\nmll = gpytorch.mlls.VariationalELBO(\n    likelihood,\n    model,\n    num_data=train_y.size(0)    ❶\n)\n```", "```py\ntrain_dataset = torch.utils.data.TensorDataset(train_x, train_y)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=100)\n```", "```py\noutput = model(x_batch)\nloss = -mll(output, y_batch)\n```", "```py\nmodel.train()                                 ❶\nlikelihood.train()                            ❶\n\nfor i in tqdm(range(50)):                     ❷\n    for x_batch, y_batch in train_loader:     ❸\n        optimizer.zero_grad()                 ❹\n\n        output = model(x_batch)               ❹\n        loss = -mll(output, y_batch)          ❹\n\n        loss.backward()                       ❹\n        optimizer.step()                      ❹\n\nmodel.eval()                                  ❺\nlikelihood.eval()                             ❺\n```", "```py\nvisualize_gp_belief(model, likelihood, variational=True)\n```", "```py\nclass NaturalGradientGPModel(gpytorch.models.ApproximateGP):\n  def __init__(self, inducing_points):\n    variational_distribution =                         ❶\n      gpytorch.variational.                            ❶\n      ➥NaturalVariationalDistribution(                ❶\n        inducing_points.size(0)                        ❶\n    )                                                  ❶\n\n    variational_strategy = gpytorch.variational.\n    ➥VariationalStrategy(                             ❷\n        self,                                          ❷\n        inducing_points,                               ❷\n        variational_distribution,                      ❷\n        learn_inducing_locations=True,                 ❷\n    )                                                  ❷\n    super().__init__(variational_strategy)             ❷\n    self.mean_module = gpytorch.means.ConstantMean()   ❷\n    self.covar_module = gpytorch.kernels.ScaleKernel(  ❷\n        gpytorch.kernels.RBFKernel()                   ❷\n    )                                                  ❷\n  def forward(self, x):\n        ...                                            ❸\n```", "```py\nmodel = NaturalGradientGPModel(train_x[:50, :])         ❶\nlikelihood = gpytorch.likelihoods.GaussianLikelihood()\n```", "```py\nngd_optimizer = gpytorch.optim.NGD(                  ❶\n  model.variational_parameters(), num_data=train_y.  ❶\n  ➥size(0), lr=0.1                                  ❶\n)                                                    ❶\n\nhyperparam_optimizer = torch.optim.Adam(             ❷\n  [{\"params\": model.parameters()}, {\"params\":        ❷\n  ➥likelihood.parameters()}],                       ❷\n  lr=0.01                                            ❷\n)                                                    ❷\nmll = gpytorch.mlls.VariationalELBO(\n  likelihood, model, num_data=train_y.size(0)\n)\n```", "```py\noutput = model(x_batch)\nloss = -mll(output, y_batch)\n```", "```py\nmodel.train()                              ❶\nlikelihood.train()                         \n\nfor i in tqdm(range(50)):\n    for x_batch, y_batch in train_loader:\n        ngd_optimizer.zero_grad()          ❷\n        hyperparam_optimizer.zero_grad()   ❷\n\n        output = model(x_batch)\n        loss = -mll(output, y_batch)\n\n        loss.backward()\n\n        ngd_optimizer.step()               ❸\n        hyperparam_optimizer.step()        ❸\n\nmodel.eval()                               ❹\nlikelihood.eval()                          ❹\n```", "```py\n    likelihood = gpytorch.likelihoods.GaussianLikelihood(\n        noise_constraint=gpytorch.constraints.GreaterThan(1e-1)  ❶\n    )\n    ```"]