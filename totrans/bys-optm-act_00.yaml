- en: front matter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: forewords
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the complexity of problems we face in machine learning and related fields
    continues to increase, it is more and more important to optimize our use of resources
    and make informed decisions efficiently. Bayesian optimization, a powerful technique
    for finding the maxima and minima of objective functions that are expensive to
    evaluate, has emerged as a very useful solution to this challenge. One reason
    is that the function can be taken as a black box, which enables researchers and
    practitioners to tackle very complicated functions with Bayesian inference as
    the main method of optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Due to its complexity, Bayesian optimization has been more out of reach for
    beginner ML practitioners than other methods. However, a tool like Bayesian optimization
    must be in the toolkit of any ML practitioner who wants to get the best results.
    To master this topic, one must have a very solid intuition of calculus and probability.
  prefs: []
  type: TYPE_NORMAL
- en: This is where *Bayesian Optimization in Action* comes to the rescue. In this
    book, Quan beautifully and successfully demystifies these complex concepts. Using
    a hands-on approach, clear diagrams, real-world examples, and useful code examples,
    he lifts the veil off the complexities of the topic, both from the theoretical
    and the practical point of view.
  prefs: []
  type: TYPE_NORMAL
- en: Quan uses his extensive experience as a data scientist and educator to give
    the reader a very clear picture of these techniques and how they can be applied
    to solve real-world problems. Starting from the principles of Bayesian inference,
    the book gradually builds up the concepts of Bayesian optimization and the Gaussian
    process model. It teaches state-of-the-art libraries, such as GPyTorch and BoTorch,
    exploring their use in several domains.
  prefs: []
  type: TYPE_NORMAL
- en: This book is an essential read for any data science or ML practitioner who wants
    to harness the true power of Bayesian optimization to solve real-world problems.
    I highly recommend it to anyone looking to master the art of optimization through
    Bayesian inference.
  prefs: []
  type: TYPE_NORMAL
- en: —Luis Serrano, PhD, AI Scientist and Popularizer,
  prefs: []
  type: TYPE_NORMAL
- en: Author of *Grokking Machine Learning*
  prefs: []
  type: TYPE_NORMAL
- en: Engineers and scientists face a common challenge, essential to capturing the
    value of their research and creativity. They need to optimize. A machine learning
    engineer finds the hyperparameters that make models generalize. A group of physicists
    tunes a free electron laser for maximal pulse energy. A software engineer configures
    the garbage collector of the JVM to maximize throughput of a server. A materials
    science engineer selects a microstructure morphology that maximizes the light
    absorption of a solar cell. In each example, there are design decisions that cannot
    be made based on first principles. They are instead made by experimental evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: To evaluate something experimentally, one might execute software, run hardware,
    or build a new object, simultaneously measuring its performance. To find a good
    design, one needs to make multiple evaluations. These evaluations take time, cost
    money, and may incur risk. It is, therefore, imperative that one make as few experimental
    evaluations as are necessary to find an optimal design. That’s what Bayesian optimization
    is all about.
  prefs: []
  type: TYPE_NORMAL
- en: I have used Bayesian optimization, and related precursor methods, in my work
    over the past 20 years. In that time, academic research and reports of industrial
    application have improved the performance and expanded the applicability of Bayesian
    optimization. There now exist high-quality software tools and techniques for building
    project-specific optimizers.
  prefs: []
  type: TYPE_NORMAL
- en: One may think of the current state as analogous to that of prediction with linear
    models. An engineer who wants to build a linear model will find that software
    tools like sklearn enable them to design models for various types (e.g., continuous
    or categorical) and numbers of input and output variables, to perform automated
    variable selection, and to measure the quality of generalization. Similarly, an
    engineer who wants to build a Bayesian optimizer will find that BoTorch, which
    is built on GPyTorch, pyro, and PyTorch, provides tools for optimizing over different
    variable types, maximizing multiple objectives, handling constraints, and more.
  prefs: []
  type: TYPE_NORMAL
- en: 'This book teaches Bayesian optimization, starting from its most basic components—Gaussian
    process regression and numerical optimization of an acquisition function—to the
    latest methods for handling large numbers of evaluations (aka observations) and
    exotic design spaces. Along the way, it covers all the specializations you might
    need for a given project: constraint handling, multiple objectives, parallelized
    evaluation, and evaluation via pairwise comparison. You’ll find enough technical
    depth to make you comfortable with the tools and methods and enough real code
    to enable you to use those tools and methods for real work very quickly.'
  prefs: []
  type: TYPE_NORMAL
- en: For all the success of Bayesian optimization, there is little literature aimed
    at the newcomer. This book fills that niche excellently.
  prefs: []
  type: TYPE_NORMAL
- en: —David Sweet, adjunct professor, Yeshiva University,
  prefs: []
  type: TYPE_NORMAL
- en: author of *Experimentation for Engineers*, Cogneato.xyz
  prefs: []
  type: TYPE_NORMAL
- en: preface
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the fall of 2019, I was a first-year PhD student unsure of what problem to
    work on for my research. I knew I wanted to focus on artificial intelligence (AI)—there
    was something appealing about automating thinking processes using computers—but
    AI is an enormous field, and I was having a hard time narrowing my work down to
    a specific topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'All that uncertainty disappeared when I took a course titled Bayesian Methods
    for Machine Learning. At this point, I had had brief encounters with Bayes theorem
    in my undergrad, but it was during the first lectures of this course that everything
    started to click! Bayes’ theorem offered an intuitive way to think about probability,
    and to me, it is an elegant model of human beliefs: Each of us has a prior belief
    (about anything) that we start with, which is updated as we observe evidence for
    or against that prior, and the result is a posterior belief reflecting both the
    prior and data. The fact that Bayes’ theorem brings this elegant way of maintaining
    beliefs to AI and finds applications across many problems was a strong signal
    to me that Bayesian machine learning is a topic worth pursuing.'
  prefs: []
  type: TYPE_NORMAL
- en: 'By the time we got to the lecture on Bayesian optimization (BayesOpt), my mind
    was made up: the theory was intuitive, the applications were numerous, and there
    was just so much possibility in what could be built. Again, something innate in
    me was (and continues to be) attracted to automating thinking or, more specifically,
    decision-making, and BayesOpt was the perfect attraction. I got myself into the
    research lab of Roman Garnett, the professor teaching the course, and my BayesOpt
    journey began!'
  prefs: []
  type: TYPE_NORMAL
- en: 'Jumping to 2021, I had spent some time researching and implementing BayesOpt
    solutions, and my appreciation for BayesOpt only grew. I would recommend it to
    friends and colleagues to handle difficult optimization problems, promising that
    BayesOpt would work well. There was only one issue: there wasn’t a good resource
    I could point to. Research papers were heavy on math, online tutorials were too
    short to provide substantial insight, and tutorials of BayesOpt software were
    disjointed and didn’t offer a good narrative.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, an idea came to mind, in the form of Toni Morrison’s quote, “If there’s
    a book that you want to read, but it hasn’t been written yet, then you must write
    it.” How very true! The prospect excited me for two reasons: I could write a book
    on something near and dear to my heart, and writing would undoubtedly help me
    gain even deeper insights. I put together a proposal and contacted Manning, the
    publisher of my favorite books with the style I was envisioning.'
  prefs: []
  type: TYPE_NORMAL
- en: In November 2021, my acquisition editor, Andy Waldron, sent me an email, marking
    the very first communication from Manning. In December 2021, I signed my contract
    and began writing, which would later prove to require more time than I initially
    thought (as is the case for every book, I’m sure). In April 2023, I wrote this
    preface as one of the last steps before publication!
  prefs: []
  type: TYPE_NORMAL
- en: acknowledgments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It takes a village to raise a child and no less to write a book. The following
    are only a small part of my own village, who helped me immensely during the writing
    process.
  prefs: []
  type: TYPE_NORMAL
- en: 'My first and foremost thanks are for my parents Bang and Lan, whose constant
    support has allowed me to fearlessly explore the unknown: studying abroad; pursuing
    a PhD; and, of course, writing a book. And I’d like to sincerely thank my sister
    and confidante, Nhu, who is always there to help me through the toughest of times.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Bayesian optimization is a large part of my PhD research, and I’d like to thank
    the people in the program who truly have and continue to make my PhD experience
    invaluable. Special thanks go to my advisor Roman Garnett, who effortlessly convinced
    me to pursue research in Bayesian machine learning. You’re the one who started
    all this. I also thank my friends from the Active Learning lab: Yehu Chen, Shayan
    Monadjemi, and Professor Alvitta Ottley. They say that a PhD has very sparse rewards,
    and working with you all has been what makes up most of those rewards.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, I gratefully acknowledge the amazing team at Manning. I thank my development
    editor, Marina Michaels, who has stewarded this ship, from day one, with the highest
    level of professionalism, care, support, and patience. I completely lucked out
    getting matched with you for our project. Thanks to my acquisition editor, Andy Waldron,
    for having faith in the idea, even though a much better author was already working
    on a book with a similar topic, and Ivan Martinovic´ for helping me with AsciiDoc
    questions and patiently fixing my markup code.
  prefs: []
  type: TYPE_NORMAL
- en: 'I’d like to thank the reviewers who devoted time and energy to significantly
    improve the quality of the writing: Allan Makura, Andrei Paleyes, Carlos Aya-Moreno,
    Claudiu Schiller, Cosimo Attanasi, Denis Lapchev, Gary Bake, George Onofrei, Howard
    Bandy, Ioannis Atsonios, Jesús Antonino Juárez Guerrero, Josh McAdams, Kweku Reginald
    Wade, Kyle Peterson, Lokesh Kumar, Lucian Mircea Sasu, Marc-Anthony Taylor, Marcio
    Nicolau, Max Dehaut, Maxim Volgin, Michele Di Pede, Mirerfan Gheibi, Nick Decroos,
    Nick Vazquez, Or Golan, Peter Henstock, Philip Weiss, Ravi Kiran Bamidi, Richard
    Tobias, Rohit Goswami, Sergio Govoni, Shabie Iqbal, Shreesha Jagadeesh, Simone
    Sguazza, Sriram Macharla, Szymon Harabasz, Thomas Forys, and Vlad Navitski.'
  prefs: []
  type: TYPE_NORMAL
- en: One, inevitably, has blind spots while writing a book, and it’s the reviewers
    who help fill in those blind spots and keep the author focused on what’s truly
    important. Dedicated thanks to Kerry Koitzsch for his insightful feedback and
    James Byleckie for his excellent suggestions on both the code and the writing.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, I thank the teams behind the incredible GPyTorch and BoTorch libraries,
    the main workhorses of the code developed for this book. I have sampled various
    libraries for Gaussian processes and Bayesian optimization, always finding myself
    coming back to GPyTorch and BoTorch. I hope this book can play a part in building
    an already-wonderful community around these libraries.
  prefs: []
  type: TYPE_NORMAL
- en: about this book
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It used to be that to learn about Bayesian optimization, one would need to look
    for online articles and tutorials in the documentation of a relevant library,
    which are scattered and, due to their nature, don’t go deeply into the specifics.
    You could also turn to technical textbooks, but they are usually too dense and
    math heavy, posing a challenge if you are a practitioner who would like to hit
    the ground running right away.
  prefs: []
  type: TYPE_NORMAL
- en: This book fills the gap by offering a blend of hands-on discussions, references
    to more in-depth materials for the interested reader, and ready-to-use code examples.
    It works by first building intuition for the components of Bayesian optimization
    and then implementing them in Python using state-of-the-art software.
  prefs: []
  type: TYPE_NORMAL
- en: The spirit of the book is to provide an accessible introduction to Bayesian
    optimization grounded in high-level intuitions from math and probability. The
    interested reader can, further, find more technical texts that are referenced
    throughout the book for a deeper dive into a topic of interest.
  prefs: []
  type: TYPE_NORMAL
- en: Who should read this book?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data scientists and ML practitioners who are interested in hyperparameter tuning,
    A/B testing, or experimentation and, more generally decision-making, will benefit
    from this book.
  prefs: []
  type: TYPE_NORMAL
- en: Researchers in scientific fields such as chemistry, materials science, and physics
    who face difficult optimization problems will also find this book helpful. While
    most background knowledge necessary to follow the content will be covered, the
    audience should be familiar with common concepts in ML, such as training data,
    predictive models, multivariate normal distributions, and others.
  prefs: []
  type: TYPE_NORMAL
- en: 'How this book is organized: A roadmap'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The book comprises four main parts. Each part contains several chapters that
    cover the corresponding topic:'
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 1 introduces Bayesian optimization using real-world use cases. It also
    includes, without going into technical details, a visual example of how Bayesian
    optimization could accelerate finding the global optimum of an expensive function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Part 1 covers Gaussian processes as the predictive model of the function we
    wish to optimize. The central thesis is that Gaussian processes offer calibrated
    quantification of uncertainty, which is essential in our Bayesian optimization
    framework. This part is composed of two chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 2 shows that Gaussian processes are a natural solution to the problem
    of learning a regression model from some observed data. A Gaussian process defines
    a distribution over functions and can be updated to reflect our belief about the
    function’s value, given some observed data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chapter 3 introduces the two main ways we incorporate prior information into
    a Gaussian process: the mean function and the covariance function. The mean function
    specifies the general trend, while the covariance function specifies the smoothness
    of the function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Part 2 enumerates Bayesian optimization policies, which are decision procedures
    for how function evaluations should be done so that the global optimum may be
    identified as efficiently as possible. While different policies are motivated
    by different objectives, they all aim to balance the tradeoff between exploration
    and exploitation. This part is composed of three chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Chapter 4 discusses a natural way of deciding which function evaluation is
    the most beneficial to make: considering the improvement that would be gained
    from the current best function value. Thanks to the Gaussian process–based belief
    about the function, we may compute these improvement-related quantities in closed
    form and cheaply, enabling two specific Bayesian optimization policies: Probability
    of Improvement and Expected Improvement.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chapter 5 explores the connection between Bayesian optimization and another
    common class of problems, called the *multi-armed bandit*. We learn how to transfer
    multi-armed bandit policies in the Bayesian optimization setting and obtain corresponding
    strategies: Upper Confidence Bound and Thompson sampling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 6 considers a strategy that reduces the most uncertainty in our belief
    about the function’s global optimum. This constitutes entropy-based policies,
    which use a subfield of mathematics called *information theory*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Part 3 presents some of the most common use cases that don’t fit neatly into
    the workflow developed thus far in the book and shows how Bayesian optimization
    may be modified to tackle these optimization tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 7 introduces batch optimization, in which to increase throughput, we
    allow experiments to run in parallel. For example, one may train multiple instances
    of a large neural network simultaneously on a cluster of GPUs. This requires more
    than one recommendation to be returned at the same time by an optimization policy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 8 discusses safety-critical use cases, where we cannot explore the search
    space freely, as some function evaluations may have detrimental effects. This
    motivates the setting in which there are constraints on how the function in question
    should behave and our need to factor in these constraints in the design of optimization
    policies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 9 shows that when we have access to multiple ways of observing the function’s
    values at different levels of cost and accuracy—commonly known as *multifidelity
    Bayesian optimization*—accounting for variable costs can lead to increased optimization
    performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 10 covers pairwise comparisons, which have been shown to reflect one’s
    preference more accurately than number evaluations or ratings, as they are simpler
    and pose a lighter cognitive load on the labeler. Chapter 10 extends Bayesian
    optimization to this setting, first by using a special Gaussian process model
    and then by modifying existing policies to fit into this pairwise comparison workflow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One may aim to optimize multiple, potentially conflicting objectives at the
    same time. Chapter 11 studies this problem of multiobjective optimization and
    shows how Bayesian optimization can be extended to this setting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Part 4 deals with special variants of the Gaussian process models, demonstrating
    their flexibility and effectiveness at modeling and providing uncertainty-calibrated
    predictions, even outside of the Bayesian optimization context:'
  prefs: []
  type: TYPE_NORMAL
- en: In chapter 12, we learn that in some cases, obtaining the closed-form solution
    of a trained Gaussian process is impossible. However, high-fidelity approximations
    may still be made using sophisticated approximate strategies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 13 demonstrates that thanks to the Torch ecosystem, combining PyTorch
    neural networks with GPyTorch Gaussian processes is a seamless process. This allows
    our Gaussian process models to become more flexible and expressive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A beginner will benefit a great deal from the first six chapters. Experienced
    practitioners looking to fit Bayesian optimization into their use case might find
    value in chapters 7 through 11, which can be read independently and in any order.
    Long-time users of Gaussian processes will most likely be interested in the last
    two chapters, where we develop specialized Gaussian processes.
  prefs: []
  type: TYPE_NORMAL
- en: About the code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can get executable snippets of code from the liveBook (online) version of
    this book at [https://livebook.manning.com/book/bayesian-optimization-in-action](https://livebook.manning.com/book/bayesian-optimization-in-action).
    The code is available for download from the Manning website at [https://www.manning.com/books/bayesian-optimization-in-action](https://www.manning.com/books/bayesian-optimization-in-action)
    and GitHub at [https://github.com/KrisNguyen135/bayesian-optimization-in-action](https://github.com/KrisNguyen135/bayesian-optimization-in-action);
    the latter of which will accept issues and pull requests.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will be using Jupyter notebooks to run the code accompanying the book.
    Jupyter notebooks offer a clean way to dynamically work with the code, allowing
    us to explore how each object behaves and interacts with other objects. More information
    on how to get started with Jupyter notebooks can be found on their official website:
    [https://jupyter.org](https://jupyter.org). The ability to dynamically explore
    objects is specifically helpful in our case, as many components of the Bayesian
    optimization workflow are implemented as Python objects by GPyTorch and BoTorch,
    the main libraries we’ll be using.'
  prefs: []
  type: TYPE_NORMAL
- en: GPyTorch and BoTorch are the premiere libraries for Gaussian process modeling
    and Bayesian optimization in Python. There are other choices, such as the scikit-optimize
    extension of scikit-Learn or GPflow and GPflowOpt, which extend the TensorFlow
    framework for Bayesian optimization. However, the combination of GPyTorch and
    BoTorch makes up the most comprehensive and flexible codebase, which includes
    many state-of-the-art algorithms from Bayesian optimization research. I have found
    in my own experience using Bayesian optimization software that GPyTorch and BoTorch
    achieve a good balance between being beginner-friendly and providing state-of-the-art
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'One thing is important to note: it’s exactly because these libraries are being
    actively maintained that the APIs shown in the book might change slightly in newer
    versions, and it’s important that you install the library versions specified in
    the `requirements.txt` file to run the code without errors. You can find more
    instructions on how to create a Python environment using a `requirements.txt`
    file in, for example, the official Python documentation at [https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments).
    With that said, to work with newer versions, you will most likely only need to
    make minor modifications to the code.'
  prefs: []
  type: TYPE_NORMAL
- en: You will notice as you go through the book that the text tends to focus on only
    the key components of the code, leaving out many details, such as library imports
    and bookkeeping code. (Of course, the first time a piece of code is used, it will
    be properly introduced in the text.) Keeping our discussions concise helps us
    stay focused on what’s truly new in each chapter and avoid having to repeat ourselves.
    The code in the Jupyter notebooks, on the other hand, is self-contained, and each
    notebook can be run on its own, without any modification.
  prefs: []
  type: TYPE_NORMAL
- en: liveBook discussion forum
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Purchase of *Bayesian Optimization in Action* includes free access to liveBook,
    Manning’s online reading platform. Using liveBook’s exclusive discussion features,
    you can attach comments to the book globally or to specific sections or paragraphs.
    It’s a snap to make notes for yourself, ask and answer technical questions, and
    receive help from the author and other users. To access the forum, go to [https://livebook.manning.com/book/bayesian-optimization-in-action/discussion](https://livebook.manning.com/book/bayesian-optimization-in-action/discussion).
    You can also learn more about Manning’s forums and the rules of conduct at [https://livebook.manning.com/discussion](https://livebook.manning.com/discussion).
  prefs: []
  type: TYPE_NORMAL
- en: Manning’s commitment to our readers is to provide a venue where a meaningful
    dialogue between individual readers and between readers and the author can take
    place. It is not a commitment to any specific amount of participation on the part
    of the author, whose contribution to the forum remains voluntary (and unpaid).
    We suggest you try asking the author some challenging questions lest their interest
    stray! The forum and the archives of previous discussions will be accessible from
    the publisher’s website for as long as the book is in print.
  prefs: []
  type: TYPE_NORMAL
- en: about the author
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/Nguyen_AuthorPhoto.png)'
  prefs: []
  type: TYPE_IMG
- en: Quan Nguyen is a Python programmer and machine learning enthusiast. He is interested
    in solving decision-making problems that involve uncertainty. Quan has authored
    several books on Python programming and scientific computing. He is currently
    pursuing a PhD in computer science at Washington University in St. Louis, where
    he does research on Bayesian methods in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: About the technical editor
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The technical editor of this book is Kerry Koitzsch. Kerry is an author, and
    software architect with over three decades of diverse experience in the implementation
    of enterprise applications and information architecture solutions. Kerry is the
    author of a book on distributed processing as well as many shorter technical publications
    and holds a patent for innovative OCR technology. He is also a recipient of the
    U.S. Army Achievement Medal.
  prefs: []
  type: TYPE_NORMAL
- en: about the cover illustration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The figure on the cover of *Bayesian Optimization in Action* is captioned “Polonnois,”
    or “Polish Man,” taken from a collection by Jacques Grasset de Saint-Sauveur,
    published in 1797\. Each illustration is finely drawn and colored by hand.
  prefs: []
  type: TYPE_NORMAL
- en: In those days, it was easy to identify where people lived and what their trade
    or station in life was just by their dress. Manning celebrates the inventiveness
    and initiative of the computer business with book covers based on the rich diversity
    of regional culture centuries ago, brought back to life by pictures from collections
    such as this one.
  prefs: []
  type: TYPE_NORMAL
