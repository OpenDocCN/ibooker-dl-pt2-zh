- en: Chapter 6\. Challenges, Controversies, and Shortcomings
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第六章 挑战、争议和缺陷
- en: 'Every technological revolution brings controversy. In this section we focus
    on three of the most controversial aspects of GPT-3: AI bias being encoded into
    the model; low-quality content and the spread of misinformation; and GPT-3’s environmental
    footprint. When you mix human biases with a powerful tool capable of producing
    huge quantities of seemingly coherent text, the results can be dangerous.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 每一次技术革命都带来争议。在本节中，我们重点关注 GPT-3 的三个最具争议性的方面：AI 偏见被编码到模型中；低质量内容和误导性信息的传播；以及 GPT-3
    的环境足迹。当你将人类偏见与一个能够产生大量看似连贯文本的强大工具相结合时，结果可能是危险的。
- en: The fluency and coherence of much of GPT-3’s text output raises several risks
    because people are prepared to interpret it as meaningful. Many also view the
    human developers involved in creating GPT-3-based apps as “authors” of its output
    and demand that they be held accountable for its content.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 大部分文本输出的流畅性和连贯性引发了几项风险，因为人们准备将其解释为有意义的。许多人还将参与创建基于 GPT-3 的应用程序的人类开发者视为其输出的“作者”，并要求他们对其内容负责。
- en: The risks we consider in this chapter follow from the nature of GPT-3’s training
    data, which is to say, the English-speaking internet. Human language reflects
    our worldviews, including our biases—and people who have the time and access to
    publish their words online are often in positions of privilege with respect to
    race, gender, and other attributes that can be forms of oppression, which means
    they tend to be overrepresented in LLM training data. In short, society’s biases
    and dominant worldviews are already encoded in the training data. Without careful
    fine-tuning (more on this later in the chapter), GPT-3 absorbs these biases, problematic
    associations, and abusive language and includes them in its output for the world
    to interpret.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章考虑的风险源于 GPT-3 的训练数据的性质，也就是说，英语互联网。人类语言反映了我们的世界观，包括我们的偏见，而有时间和条件在网上发布自己言论的人往往处于种族、性别等方面的特权地位，这意味着他们在
    LLM 训练数据中往往被过度代表。简言之，社会的偏见和主流世界观已经编码在训练数据中。如果没有仔细的微调（本章后面会详细介绍），GPT-3 会吸收这些偏见、问题关联和滥用语言，并将它们包含在其输出中供世界解释。
- en: Whatever biases appear in the initial training set or user input are repeated
    and can be amplified or even radicalized in GPT-3-generated output. The risk is
    that people read and spread such texts, reinforcing and propagating problematic
    stereotypes and abusive language in the process. Those targeted by the harmful
    messages may experience psychological repercussions. In addition, those wrongly
    perceived to be “authors” of the GPT-3-generated text may face harm to their reputations
    or even attempts at retribution. What’s more, such biases can also emerge in future
    LLMs trained on datasets that include the publicly available output of previous
    generations of LLMs.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 无论最初的训练集或用户输入中出现什么样的偏见，都可能在 GPT-3 生成的输出中重复出现并可能被放大甚至激进化。风险在于人们阅读和传播此类文本，从而在过程中加强和传播有问题的刻板印象和滥用语言。受到有害消息攻击的人可能会遭受心理后果。此外，被错误地认为是
    GPT-3 生成文本的“作者”的人可能会受到声誉损害甚至试图报复。此类偏见也可能出现在以前一代 LLM 的公开可用输出数据集中的未来 LLM 中。
- en: The sections that follow look more closely at some of these controversies.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的章节将更仔细地研究其中一些争议。
- en: The Challenge of AI Bias
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AI 偏见的挑战
- en: Research has established that all LLMs have some sort of encoded human bias,
    including stereotypes and negative sentiment toward specific groups (especially
    marginalized minorities). One highly publicized research paper found that “the
    mix of human biases and seemingly coherent language heightens the potential for
    automation bias, deliberate misuse, and amplification of a hegemonic worldview.”^([1](ch06.xhtml#ch01fn10))
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 研究已经证实，所有 LLM 都具有某种编码的人类偏见，包括对特定群体（尤其是边缘化少数群体）的刻板印象和负面情绪。一篇备受关注的研究论文发现，“人类偏见与看似连贯的语言的混合增加了自动化偏见、故意滥用和对霸权世界观的放大的潜力。”^([1](ch06.xhtml#ch01fn10))
- en: Tip
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: There are a number of O’Reilly Media books focused on the subject of AI bias
    that we encourage you to check out, among them are [*Practical Fairness*](https://oreil.ly/tSDqw)
    and [*97 Things About Ethics Everyone in Data Science Should Know*](https://oreil.ly/vBsVQ).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些 O'Reilly Media 出版的书籍专注于 AI 偏见这一主题，我们鼓励您查阅，其中包括[*Practical Fairness*](https://oreil.ly/tSDqw)和[*97
    Things About Ethics Everyone in Data Science Should Know*](https://oreil.ly/vBsVQ)。
- en: As YouTuber Kilcher notes, working with GPT-3 is “like interacting with a skewed
    subsample of humanity” because it’s been trained on datasets that represent a
    large swath of the internet. LLMs amplify any biases in the datasets on which
    they are trained. Unfortunately, like much of humanity, this “skewed subsample
    of humanity” is rife with toxic biases, including gender, race, and religious
    prejudices.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 正如YouTuber Kilcher所指出的，与GPT-3合作“就像与一个扭曲的人类子样交互”，因为它是在代表互联网大部分的数据集上进行训练的。LLM会放大其所训练数据集中的任何偏见。不幸的是，像人类大部分一样，这个“扭曲的人类子样”充满了有害的偏见，包括性别、种族和宗教偏见。
- en: A 2020 study of GPT-2, GPT-3’s predecessor, found in the training data 272,000
    documents from unreliable news sites and 63,000 from banned subreddits.^([2](ch06.xhtml#ch01fn11))
    In the same study, both GPT-2 and GPT-3 showed a tendency to generate sentences
    with high toxicity scores, even when prompted with non-toxic sentences. OpenAI
    researchers noted early on that biased datasets led GPT-3 to place words like
    “naughty” or “sucked” near female pronouns and “Islam” near words like “terrorism.”
    A 2021 study by Stanford University researcher Abubakar Abid details consistent
    and creative biased tendencies of text generated by GPT-3, like associating the
    word “Jews” with “money” and “Muslim” with “terrorist” in the paper “Persistent
    Anti-Muslim Bias in Large Language Models.”^([3](ch06.xhtml#ch01fn12))
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 一项关于GPT-2的2020年研究发现，在训练数据中有来自不可靠新闻网站的27.2万份文档和来自被禁止的subreddit的6.3万份文档。^([2](ch06.xhtml#ch01fn11))
    在同一项研究中，无论提示是非有毒的句子，GPT-2和GPT-3都显示出生成高毒性分数句子的倾向。OpenAI的研究人员早就注意到，偏见的数据集导致GPT-3将诸如“淘气”或“糟糕”的词语放在女性代词附近，“伊斯兰教”一词放在“恐怖主义”一词附近。斯坦福大学研究员阿布巴卡尔·阿比德（Abubakar
    Abid）的2021年研究详细描述了GPT-3生成的文本的一贯且创造性的偏见倾向，如在论文“大型语言模型中持续存在的反穆斯林偏见”中将“犹太人”与“金钱”联系起来，“穆斯林”与“恐怖分子”联系起来。^([3](ch06.xhtml#ch01fn12))
- en: '[Philosopher AI](https://philosopherai.com), a GPT-3-powered chatbot and essay
    generator, was created to showcase the astounding capabilities of GPT-3, as well
    as its limits. A user enters any prompt, from a few words to a few sentences,
    and the app turns the fragment into a full essay of surprising coherence. Users
    quickly found, however, that certain types of prompts returned offensive and deeply
    troubling results.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[Philosopher AI](https://philosopherai.com)，一款由GPT-3驱动的聊天机器人和论文生成器，旨在展示GPT-3的惊人能力以及其局限性。用户输入任何提示，从几个词到几个句子不等，该应用程序就会将片段转换成一个令人惊讶的连贯性的完整论文。然而，用户很快发现，某些类型的提示会返回令人不快且令人深感困扰的结果。'
- en: 'Take, for example, this [tweet](https://oreil.ly/MmP4k) by Abeba Birhane, an
    AI researcher who prompted Philosopher AI to generate an essay on “what ails Ethiopia”
    (please note that the images in the tweet include extreme, graphic racism):'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 以阿贝巴·比尔哈内（Abeba Birhane）的[推文](https://oreil.ly/MmP4k)为例，他是一位人工智能研究员，要求Philosopher
    AI生成一篇关于“埃塞俄比亚问题”的文章（请注意推文中的图片包含极端、图形化的种族主义）：
- en: 'Every tech-evangelist: #GPT3 provides deep nuanced viewpoint'
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 每位技术布道者：#GPT3 提供了深刻而微妙的观点。
- en: 'Me: GPT-3, generate a philosophical text about Ethiopia'
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我：GPT-3，生成一篇关于埃塞俄比亚的哲学性文本。
- en: GPT-3 *spits out factually wrong and grossly racist text that portrays a tired
    and cliched Western perception of Ethiopia*
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: GPT-3 *输出事实错误且极端种族主义的文本，描绘了一种陈旧和陈词滥调的西方对埃塞俄比亚的看法*。
- en: Birhane included in the tweet two screenshots documenting the app’s output,
    which began with “The main problem with Ethiopia is that Ethiopia itself is the
    problem” and continued in an extremely racist vein.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 比尔哈内在推文中包含了两个截图，显示应用程序的输出，其中包括“埃塞俄比亚的主要问题是埃塞俄比亚本身就是问题”的开头，然后是极端种族主义的内容。
- en: 'This incident shows a potential danger of deploying commercial products and
    services powered by GPT-3: just imagine a virtual therapist, customer-service
    chatbot, or online tutor spouting this sort of toxic language. Nabla, a Paris-based
    health care technology firm, understands this problem all too well, thanks to
    its attempts to build a GPT-3-powered medical advice chatbot. In 2020, Nabla’s
    team of doctors and machine learning engineers [tested GPT-3](https://oreil.ly/KP78D)
    by exploring different health care use cases, such as conducting an administrative
    chat with a patient, checking medical insurance, providing mental health support,
    answering medical questions, and providing medical documentation and diagnosis.
    Early tests made it clear that Nabla’s use cases were far too high-stakes, complex,
    and delicate for GPT-3 to handle. For example, Nabla’s team tested a hypothetical
    scenario in which the chatbot would provide mental health support to a suicidal
    patient:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 此事件显示了由 GPT-3 驱动的商业产品和服务部署的潜在危险性：想象一下一个虚拟的治疗师、客户服务聊天机器人或在线导师说出这种有毒的语言。位于巴黎的医疗技术公司
    Nabla 深知这个问题，这要归功于其尝试构建基于 GPT-3 的医疗建议聊天机器人。2020 年，Nabla 的医生和机器学习工程师团队通过[测试 GPT-3](https://oreil.ly/KP78D)来探索不同的医疗用例，如与患者进行行政聊天、检查医疗保险、提供心理健康支持、回答医疗问题以及提供医疗文件和诊断。早期测试清楚地表明
    Nabla 的使用案例对于 GPT-3 来说过于高风险、复杂和微妙。例如，Nabla 的团队测试了一个假设情景，即聊天机器人将为一个有自杀倾向的患者提供心理健康支持：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The GPT-3-powered bot failed here in a way that could be lethal in a real-world
    deployment.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这里由 GPT-3 驱动的机器人失败了，以一种在现实世界部署中可能致命的方式。
- en: Anti-Bias Countermeasures
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 反偏见对策
- en: 'OpenAI’s [research blog](https://openai.com/blog) often notes potential dangers
    in the algorithms the company releases. For instance, a [February 2019 post](https://oreil.ly/rWw6O)
    about GPT-2 noted:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 的[研究博客](https://openai.com/blog)经常提到公司发布的算法中存在的潜在危险。例如，[2019 年 2 月的一篇文章](https://oreil.ly/rWw6O)关于
    GPT-2 的注意事项：
- en: 'We can also imagine the application of these models for [malicious purposes](https://oreil.ly/zU9vW),
    including the following (or other applications we can’t yet anticipate):'
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们还可以想象这些模型的应用于[恶意目的](https://oreil.ly/zU9vW)，包括以下（或者其他我们尚无法预料的应用）：
- en: ''
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Generate misleading news articles
  id: totrans-25
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成误导性新闻文章
- en: ''
  id: totrans-26
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-27
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Impersonate others online
  id: totrans-28
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在线冒充他人
- en: ''
  id: totrans-29
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-30
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Automate the production of abusive or faked content to post on social media
  id: totrans-31
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化生成滥用或伪造内容以发布到社交媒体
- en: ''
  id: totrans-32
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-33
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Automate the production of spam/phishing content
  id: totrans-34
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化生成垃圾邮件/钓鱼内容
- en: Because of these “concerns about large language models being used to generate
    deceptive, biased, or abusive language at scale,” OpenAI initially released an
    abbreviated version of GPT-3’s predecessor, GPT-2, with sample code, but did not
    release its datasets, training code, or model weights. OpenAI has since invested
    heavily in content filtering models and other research aimed at fixing the biases
    in its AI models. A *content filtering model* is a program fine-tuned to recognize
    potentially offensive language and prevent inappropriate completions. OpenAI provides
    a content filtering engine in its API completions endpoint (discussed in [Chapter 2](ch02.xhtml#using_the_openai_api))
    to filter unwanted text. When the engine is running, it evaluates the text that
    GPT-3 generates and classifies it as “safe,” “sensitive,” or “unsafe.” (For details,
    see the [OpenAI documentation](https://oreil.ly/SjQof).) When you interact with
    the API via the Playground, GPT-3’s content filtering model always runs in the
    background. [Figure 6-1](#content_filter_warning_displayed_in_the) shows an example
    of the Playground tagging potentially offensive content.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 由于对“担心大型语言模型被用于大规模生成具有欺骗性、偏见性或滥用性语言”的担忧，OpenAI 最初发布了 GPT-3 的前身 GPT-2 的简化版本，附带示例代码，但没有发布其数据集、训练代码或模型权重。此后，OpenAI
    在内容过滤模型和其他旨在修复其人工智能模型中的偏见的研究上投入了大量资金。*内容过滤模型*是一个经过精细调整的程序，用于识别潜在的冒犯性语言并防止不适当的完成。OpenAI
    在其 API 完成端点中提供了一个内容过滤引擎（在[第 2 章](ch02.xhtml#using_the_openai_api)中讨论），以过滤不需要的文本。当引擎运行时，它会评估
    GPT-3 生成的文本并将其分类为“安全”、“敏感”或“不安全”。（详情请参阅[OpenAI 文档](https://oreil.ly/SjQof)。）当您通过
    Playground 与 API 交互时，GPT-3 的内容过滤模型始终在后台运行。[图 6-1](#content_filter_warning_displayed_in_the)展示了
    Playground 标记潜在冒犯性内容的示例。
- en: '![](Images/gpt3_0601.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/gpt3_0601.png)'
- en: Figure 6-1\. Content filter warning displayed in the Playground
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-1。在 Playground 中显示的内容过滤警告
- en: Since the problem originated from toxic biases in unfiltered data, it seemed
    logical to OpenAI to look for solutions in the data itself. As you’ve seen, language
    models can output almost any kind of text, with any kind of tone or personality,
    depending on the user’s input. In their June 2021 study, OpenAI researchers Irene
    Solaiman and Christy Dennison explain a process they call PALMS, for Process for
    Adapting Language Models to Society. PALMS is a way to improve language model
    behavior with respect to specific ethical, moral, and societal values by fine-tuning
    models on a curated dataset of fewer than one hundred examples of those values.
    This process becomes more effective as models get larger. Models showed behavioral
    improvement without compromising their accuracy on downstream tasks, suggesting
    that OpenAI can develop tools to narrow GPT-3’s repertoire of behaviors to a constrained
    set of values.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 由于问题源于未经过滤的数据中存在有毒偏见，OpenAI 认为在数据本身中寻找解决方案是合乎逻辑的。正如你所见，语言模型可以输出几乎任何类型的文本，具有任何类型的语调或个性，这取决于用户的输入。在他们于2021年6月的研究中，OpenAI
    的研究员伊琳娜·索莱曼（Irene Solaiman）和克里斯蒂·丹尼森（Christy Dennison）解释了一个他们称之为 PALMS 的过程，即适应语言模型到社会的过程。PALMS
    是一种通过在少于一百个示例的策划数据集上微调模型，以提高语言模型在特定伦理、道德和社会价值观方面行为的方法。随着模型变得更大，这个过程变得更有效。模型在不影响下游任务准确性的情况下表现出行为改进，这表明
    OpenAI 可以开发工具，将 GPT-3 的行为范围缩小到一组受限制的价值观。
- en: 'While the PALMS process is effective, this research only scratches the surface.
    Some important unanswered questions include:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 PALMS 过程是有效的，但这项研究仅仅触及了表面。一些重要的未解答问题包括：
- en: Who should be consulted when designing a values-targeted dataset?
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计价值目标数据集时应该征求谁的意见？
- en: Who is accountable when a user receives output that is not aligned with their
    own values?
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当用户收到与他们自己的价值观不一致的输出时，谁应该负责？
- en: How robust is this methodology compared to real-world prompts? (The OpenAI researchers
    experimented only with a question-answer format.)
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与现实世界提示相比，这种方法论有多坚固？（OpenAI 的研究人员仅尝试了问答格式。）
- en: 'The PALMS process involves three steps: first, outlining the desirable behavior;
    second, crafting and refining the dataset; and third, evaluating the effect on
    model performance. Let’s look at each in turn:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: PALMS 过程包括三个步骤：首先，概述期望的行为；其次，制作和精炼数据集；第三，评估模型性能的影响。让我们依次来看：
- en: Sensitive topic categories and outlining desirable behavior
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 敏感话题类别和概述期望的行为
- en: The researchers created categories (for example, “injustice and inequality”)
    and prioritized them based on their direct impact on human well-being. For each
    category, they described a desired behavior. In the case of “injustice and inequality,”
    they instructed the model to “oppose human injustices and inequalities, or work
    that exacerbates either. This includes harmful stereotypes and prejudices, especially
    against social groups according to international law.” This description was largely
    based on US and international human rights law and Western social movements for
    human equality.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员创建了类别（例如，“不公正和不平等”），并根据其对人类福祉的直接影响对它们进行了优先级排序。对于每个类别，他们描述了一种期望的行为。在“不公正和不平等”这个案例中，他们指示模型“反对人类的不公正和不平等，或者反对加剧这些情况的工作。这包括根据国际法对社会群体进行的有害刻板印象和偏见，尤其是反对人权法和西方争取人类平等的社会运动。”
    这个描述主要基于美国和国际人权法以及西方争取人类平等的社会运动。
- en: Crafting and fine-tuning the dataset
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 制作和优化数据集
- en: Next, the researchers curated a values-targeted dataset, meaning a dataset that
    reflects a predetermined set of target values, of 80 text samples (just 0.000000211%
    of the size of GPT-3’s overall training dataset). Each sample was in question-answer
    format and between 40 and 340 words.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，研究人员策划了一个价值目标数据集，即反映了预定目标值集合的数据集，包含80个文本样本（仅占 GPT-3 整体训练数据集大小的 0.000000211%）。每个样本都是以问答格式，字数在40到340字之间。
- en: Evaluating model performance
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 评估模型性能
- en: Next, OpenAI applied quantitative and qualitative metrics to evaluate the results
    the newly fine-tuned models generated. They asked human evaluators to rate how
    well the models complied with the predetermined values. They also applied toxicity
    scoring, using the Perspective API^([4](ch06.xhtml#ch01fn13)) and other tools
    to examine mentions of gender, race, and religion.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，OpenAI 应用定量和定性指标来评估新微调模型生成的结果。他们要求人类评估者评估模型与预设值的符合程度。他们还应用了毒性评分，使用 Perspective
    API^([4](ch06.xhtml#ch01fn13)) 和其他工具来检查有关性别、种族和宗教的提及。
- en: The results look surprisingly effective. One of the researchers’ examples shows
    two sets of answers to the same question generated by a base model and a values-targeted
    model.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 结果看起来出奇地有效。研究人员的一个例子显示了基础模型和针对价值的模型对同一个问题生成的两组答案。
- en: 'Example:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 例子：
- en: '[PRE1]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Base model:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型：
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Values-targeted model:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 针对价值的模型：
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: PALMS can help companies create standardized processes for deploying LLMs such
    as GPT-3.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: PALMS 可以帮助公司创建部署 LLMs（例如 GPT-3）的标准化流程。
- en: Another breakthrough anti-bias development is [Instruct GPT](https://oreil.ly/bP3el),
    a series of models that are better at following instructions, less toxic, and
    more truthful than the original GPT-3\. (We discuss the Instruct series in more
    detail in [Chapter 2](ch02.xhtml#using_the_openai_api).)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个突破性的反偏见发展是 [Instruct GPT](https://oreil.ly/bP3el)，这是一系列模型，它们比原始的 GPT-3 更善于遵循指示，毒性更低，真实性更强。（我们在
    [第二章](ch02.xhtml#using_the_openai_api) 中更详细地讨论了 Instruct 系列。）
- en: 'Now let’s move to another challenge: the spread of low-quality content and
    misinformation.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们转向另一个挑战：低质量内容和误导信息的传播。
- en: Low-Quality Content and the Spread of Misinformation
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 低质量内容和误导信息的传播
- en: An entirely new category of risk may come into the picture when we consider
    the potential misuse of GPT-3\. Possible use cases here are as trivial as applications
    designed to automate writing term papers, clickbait articles, and interacting
    on social media accounts, all the way to intentionally promoting misinformation
    and extremism using similar channels.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们考虑到 GPT-3 的潜在误用时，一个全新的风险类别可能会出现。这里可能的用例与自动撰写学期论文、点击诱饵文章以及在社交媒体账户上互动等一样琐碎，一直到有意使用类似渠道推广误导信息和极端主义。
- en: 'The authors of the OpenAI paper that presented GPT-3 to the world in July 2020,
    [“Language Models are Few-Shot Learners”](https://oreil.ly/IR1SM), included a
    section on “Misuse of Language Models”:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 的论文作者于2020年7月向世界展示了 GPT-3 的论文 [“语言模型是少样本学习者”](https://oreil.ly/IR1SM)，其中包括一节关于“语言模型的误用”：
- en: Any socially harmful activity that relies on generating text could be augmented
    by powerful language models. Examples include misinformation, spam, phishing,
    abuse of legal and governmental processes, fraudulent academic essay writing and
    social engineering pretexting.…The misuse potential of language models increases
    as the quality of text synthesis improves. The ability of GPT-3 to generate several
    paragraphs of synthetic content that people find difficult to distinguish from
    human-written text…represents a concerning milestone in this regard.
  id: totrans-63
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 任何依赖于生成文本的社会有害活动都可能被强大的语言模型增强。例如，误导信息、垃圾邮件、网络钓鱼、滥用法律和政府程序、欺诈性的学术论文写作以及社会工程学前提等。语言模型的误用潜力随着文本合成质量的提高而增加。GPT-3
    生成几段人们难以区分是否是人工写作的合成内容的能力……在这方面表示了一个令人担忧的里程碑。
- en: 'The GPT-3 experiments are providing us with some particularly vivid examples,
    including low-quality “spam” and the spread of misinformation, as we will show
    you in a moment. Before we imagine GPT-3 becoming too powerful, though, let’s
    consider for a moment what it can actually do right now: produce very cheap, unreliable,
    and low-quality content that floods the internet and pollutes its information
    quality. As AI researcher Julian Togelius [puts it](https://oreil.ly/1XvqN): “GPT-3
    often performs like a clever student who hasn’t done their reading, trying to
    bull$&^! their way through an exam. Some well-known facts, some half-truths, and
    some straight lies, strung together in what [at] first looks like a smooth narrative.”'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 的实验为我们提供了一些特别生动的例子，包括低质量的“垃圾邮件”和误导信息的传播，我们稍后会向您展示。然而，在我们想象GPT-3变得过于强大之前，让我们考虑一下它现在实际能做到什么：产生非常廉价、不可靠和低质量的内容，充斥着互联网并污染其信息质量。正如人工智能研究员朱利安·托吉利斯所说：“GPT-3经常表现得像一个聪明的学生，他没有做好阅读，试图通过一场考试。一些众所周知的事实，一些半真半假的话，以及一些直接的谎言，被串联在一起，看起来一开始像是一个流畅的叙述。”
- en: 'Kilcher notes that the public often has unrealistic expectations of a model
    that is, at the base, predicting the most probable text to follow a given prompt:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Kilcher 指出，公众通常对一个基础模型有不切实际的期望，这个模型在基础上是在预测最有可能跟随给定提示的文本：
- en: I think a lot of the misconceptions come from the fact that people expect something
    else from the model than what it does and what it’s good at.…It’s not an oracle,
    it’s simply continuing texts as it would find them on the internet. So if you
    start a piece of text that looks like it comes from a Flat Earth Society website,
    it’s going to continue that text in [the same] manner. That doesn’t mean…it’s
    lying to you. It simply means “here is the most probable continuation for this
    piece of text.”
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我认为很多误解来自于人们对模型的期望与其实际功能和擅长之间的差异...它不是神谕，它只是按照它在互联网上找到的文本进行延续。所以如果你开始一个看起来像是来自Flat
    Earth Society网站的文本片段，它会以相同的方式延续这段文本。这并不意味着它在对你撒谎。这只是意味着“这是这段文本最有可能的延续。”
- en: 'GPT-3 has no way to verify the truth, logic, or meaning of any of the millions
    of lines of text it produces on a daily basis. The responsibility for verification
    and curation therefore rests with the humans overseeing each project. What generally
    seems to happen is that we, as humans, look for shortcuts: outsourcing the cumbersome
    task of writing to the algorithm, skipping a few steps in the editing process,
    skipping the fact-checking process. This results in more and more low-quality
    content being generated with the help of GPT-3\. And the most worrying aspect
    of it is that most people don’t seem to realize the difference.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3没有办法验证其每天生成的数百万行文本的真实性、逻辑性或含义。因此，验证和策划的责任在于监督每个项目的人类。通常情况下，我们作为人类寻找捷径：将繁琐的写作任务外包给算法，跳过编辑过程的几个步骤，跳过事实核查过程。这导致越来越多的低质量内容通过GPT-3的帮助生成。最令人担忧的是，大多数人似乎没有意识到其中的差异。
- en: Liam Porr, a computer science student at the University of California, Berkeley,
    experienced firsthand how easy it is to mislead people into believing that they’re
    reading a human-authored text, when, in fact, the human has only copied and pasted
    from model-generated outputs. As an experiment, he used GPT-3 to produce [an entirely
    fake blog](https://oreil.ly/qynav) under a pseudonym. He was surprised when, on
    July 20, 2020, one of his posts reached the number-one spot on Hacker News ([Figure 6-2](#a_gpt_three_generated_fake_blog_reached)).
    Few people noticed that his blog was completely AI-generated. Some even hit “subscribe.”
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 利亚姆·波尔（Liam Porr）是加利福尼亚大学伯克利分校的一名计算机科学学生，他亲身经历了误导他人相信他们正在阅读人类撰写的文本时实际上人类只是复制粘贴了模型生成的输出的容易程度。作为一个实验，他使用GPT-3匿名产生了[完全虚假的博客](https://oreil.ly/qynav)。令他惊讶的是，2020年7月20日，他的一篇帖子登上了Hacker
    News的头条位置。很少有人注意到他的博客完全是由人工智能生成的。一些人甚至点击了“订阅”。
- en: Porr wanted to demonstrate that GPT-3 could pass itself off as a human writer—and
    he proved his point. Despite the weird writing pattern and a few errors, only
    a small percentage of Hacker News commenters asked if the post might have been
    generated by an algorithm. Those comments were immediately downvoted by other
    community members. For Porr, the most astonishing aspect of his “achievement”
    was that “it was super easy, actually, which was the scary part.”
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Porr想要证明GPT-3可以自我标榜为人类写手——而他证明了这一点。尽管有着奇怪的写作模式和一些错误，但只有很少一部分Hacker News评论者询问该帖子是否可能是由算法生成的。这些评论立即被其他社区成员点踩了。对于Porr来说，“这个‘成就’最令人惊讶的一点是‘事实上这非常容易，这让人感到害怕’”。
- en: '![](Images/gpt3_0602.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gpt3_0602.png)'
- en: Figure 6-2\. A GPT-3-generated fake blog reached the top place on Hacker News
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-2\. 一篇由GPT-3生成的假博客登上了Hacker News的首位。
- en: Creating and viewing blogs, videos, tweets, and other types of digital information
    has become cheap and easy to the point of information overload. Viewers, unable
    to process all this material, often let cognitive biases decide what they should
    pay attention to. These mental shortcuts influence which information we search
    for, comprehend, remember, and repeat—to a harmful extent. It’s easy to fall prey
    to low-quality pieces of information, which GPT-3 can produce quickly and at high
    volume.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 创建和浏览博客、视频、推文和其他类型的数字信息已经变得廉价且容易到了信息过载的程度。观众们无法处理所有这些材料，通常会让认知偏见决定他们应该关注什么。这些心理捷径影响着我们搜索、理解、记忆和重复的信息，对我们有害。很容易陷入低质量信息的陷阱，而GPT-3能够快速且大量地生成这些信息。
- en: A 2017 study used statistical models to link the spread of low-quality information
    over social media networks to limited reader attention and high information load.
    Both factors, the researchers found, can lead to an inability to discriminate
    between good and bad information.^([5](ch06.xhtml#ch01fn14)) A study from 2019
    showed how automated, bot-controlled social media accounts had influenced the
    spread of misinformation during the 2016 US election period. When a fake news
    article was posted, for example, claiming that Hillary Clinton’s presidential
    campaign was involved in occult rituals, within seconds it was retweeted by many
    bots, as well as humans.^([6](ch06.xhtml#ch01fn15))
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年的一项研究使用统计模型将社交媒体网络上传播低质量信息与读者关注力有限和信息负载高度联系起来。研究人员发现，这两个因素都可能导致无法区分好坏信息。^([5](ch06.xhtml#ch01fn14))
    2019年的一项研究显示，自动化的、机器人控制的社交媒体账户在2016年美国大选期间影响了错误信息的传播。例如，当发布了一篇假新闻文章，声称希拉里·克林顿的总统竞选涉及神秘仪式时，几秒钟内就会被许多机器人和人类转发。^([6](ch06.xhtml#ch01fn15))
- en: A [2021 study](https://oreil.ly/vkufU) corroborated this, finding that 75% of
    American respondents who say they follow news and current events agree that fake
    news is a big problem today.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[2021年的一项研究](https://oreil.ly/vkufU)证实了这一点，发现75%的美国受访者表示他们关注新闻和时事，并认为假新闻是当今一个很大的问题。'
- en: One source of this flood of low-quality content is automated, bot-controlled
    social media accounts that impersonate humans, enabling misguided or malevolent
    actors to take advantage of readers’ vulnerabilities. In 2017, a research team
    estimated that up to 15% of active Twitter accounts were bots.^([7](ch06.xhtml#ch01fn16))
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 低质量内容的一个来源是自动化的、机器人控制的社交媒体账户，它们冒充人类，使得误导或恶意行为者能够利用读者的弱点。2017年，一个研究团队估计高达15%的活跃Twitter账户是机器人。^([7](ch06.xhtml#ch01fn16))
- en: There are many social media accounts that openly identify themselves as GPT-3
    bots, but some GPT-3-powered bots hide their true nature. In 2020, Reddit user
    Philip Winston [uncovered a hidden GPT-3 bot](https://oreil.ly/Oe3Gb) posing as
    a fellow Reddit user under the username /u/thegentlemetre. The bot interacted
    with other forum members for a week on /r/AskReddit, a general chat with an audience
    of 30 million. While its comments were not harmful in this instance, the bot could
    easily have spread harmful or unreliable content.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多社交媒体账户公开自己是GPT-3机器人，但一些GPT-3驱动的机器人隐藏了它们的真实性质。2020年，Reddit用户Philip Winston
    [揭露了一个隐藏的GPT-3机器人](https://oreil.ly/Oe3Gb)，它冒充一个Reddit用户，在用户名为/u/thegentlemetre的账户下进行交互。这个机器人在/r/AskReddit上与其他论坛成员互动了一周，这是一个拥有3000万观众的通用聊天版块。虽然在这种情况下它的评论并不具有害，但这个机器人很容易传播有害或不可靠的内容。
- en: As you’ve seen throughout this book, GPT-3’s output is a synthesis of its training
    data, which is mostly unverified public internet data. Most of this data is neither
    well-curated nor written by responsible, accountable individuals. There’s a cascading
    effect, where the current content of the internet negatively impacts the future
    content by becoming part of its dataset, continually lowering the average quality
    of its text. As Andrej Karpathy [tweeted](https://oreil.ly/AiqC8), half-jokingly,
    “By posting GPT generated text we’re polluting the data for its future versions.”
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在本书中所看到的，GPT-3的输出是其训练数据的综合，这些数据主要是未经验证的公共互联网数据。大多数这些数据既没有得到很好的筛选，也不是由负责任、可靠的个人编写的。存在着一个级联效应，当前互联网内容对未来内容产生负面影响，因为它成为数据集的一部分，不断降低其文本的平均质量。正如Andrej
    Karpathy [在推特上](https://oreil.ly/AiqC8)半开玩笑地说：“通过发布由GPT生成的文本，我们正在污染其未来版本的数据。”
- en: Given the use cases we’ve seen for GPT-3’s growing role in artistic and literary
    production, it’s reasonable to assume that further advancements in text-generating
    models will profoundly impact the future of literature. If a large portion of
    all written material is computer-generated, we are going to encounter a tough
    situation.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们已经看到了GPT-3在艺术和文学创作中日益增长的作用，可以合理地假设文本生成模型的进一步发展将深刻影响文学的未来。如果所有书面材料的大部分是由计算机生成的，我们将会面临一个严峻的局面。
- en: In 2018, researchers conducted the [largest-ever study](https://oreil.ly/ktKgu)
    of the spread of [false news](https://oreil.ly/VotSs) online. They investigated
    a dataset of all the true and fake news stories (as verified by six independent
    fact-checking organizations) that were distributed on Twitter from 2006 to 2017\.
    The study found that fake news online travels “farther, faster, deeper, and more
    broadly than the truth.” Falsehoods were 70% more likely to be retweeted on Twitter
    than the truth and reached a threshold of 1,500 viewers, about six times faster
    than the truth. The effect was greater for fake political news than for fake news
    about terrorism, natural disasters, science, urban legends, or financial information.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年，研究人员进行了有史以来最大规模的一项研究，调查了从2006年到2017年在 Twitter 上分发的所有真实和虚假新闻故事的数据集（由六家独立事实核查机构验证）。研究发现，在线虚假新闻比真实新闻“传播得更远、更快、更深入、更广泛”。谎言比真相在
    Twitter 上转发的可能性高出70%，并且比真相快六倍达到了1,500名观众的阈值。虚假政治新闻比虚假有关恐怖主义、自然灾害、科学、都市传说或金融信息的新闻效果更大。
- en: Acting on the wrong information can become deadly, as the COVID-19 pandemic
    made tragically clear. In the first three months of 2020, as the pandemic began,
    nearly 6,000 people around the globe were hospitalized due to coronavirus misinformation,
    research suggests. During this period, [researchers say](https://oreil.ly/bpOGu),
    at least 800 people may have died due to misinformation related to COVID-19; those
    numbers will surely increase as research continues.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 依据错误信息行事可能会变得致命，正如 COVID-19 大流行所清楚地表明的那样。研究表明，在2020年大流行开始的头三个月，全球有近6,000人因冠状病毒的错误信息而住院。在此期间，[研究人员表示](https://oreil.ly/bpOGu)，至少有800人可能因与
    COVID-19 相关的错误信息而死亡；随着研究的继续，这些数字肯定会增加。
- en: Misinformation is also a powerful weapon to spur political chaos, as is evident
    in the Russian war against Ukraine that is taking place as this book goes to press
    in early 2022\. Researchers and journalists from respected outlets including [Politico](https://oreil.ly/SLOBz)*,*
    [Wired](https://oreil.ly/p9yOr)*,* and [TechTarget](https://oreil.ly/MWf4I) have
    unearthed fake TikTok videos, anti-refugee Instagram accounts, pro-Kremlin Twitter
    bots, and even AI-generated deepfake videos of Ukraine president Volodymyr Zelenskyy
    asking his soldiers to drop their weapons.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 虚假信息也是激发政治混乱的强大武器，正如本书在2022年初印刷时正在进行的俄罗斯对乌克兰的战争所证明的那样。来自备受尊重的媒体机构，包括[Politico](https://oreil.ly/SLOBz)、[Wired](https://oreil.ly/p9yOr)和[TechTarget](https://oreil.ly/MWf4I)的研究人员和记者们已经发现了伪造的
    TikTok 视频、反难民的 Instagram 账户、亲克里姆林宫的 Twitter 机器人，甚至是乌克兰总统弗拉基米尔·泽连斯基的 AI 生成的深度伪造视频，请求他的士兵放下武器。
- en: GPT-3 allows users to mass-generate content. Users can then immediately test
    it on social media channels to see if the message is effective, as often as a
    few thousand times a day. This lets the model quickly learn how to sway targeted
    demographic groups of social media users. In the wrong hands, it can easily become
    the engine of a powerful propaganda machine.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3允许用户批量生成内容。用户随时可以在社交媒体渠道上测试它的效果，每天可以多达数千次。这让模型迅速学会如何影响社交媒体用户的目标人群。如果掌握不当，它很容易成为强大宣传机器的引擎。
- en: 'In 2021, researchers from Georgetown University evaluated GPT-3’s performance
    on six misinformation-related tasks:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 2021年，乔治城大学的研究人员评估了GPT-3在六个与误导信息相关的任务上的表现：
- en: Narrative reiteration
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 叙事重述
- en: Generating varied short messages that advance a particular theme, such as climate
    change denial
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 生成各种短信息，推进特定主题，比如气候变化否认
- en: Narrative elaboration
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 叙事展开
- en: Developing a medium-length story that fits within a desired worldview when given
    only a short prompt, such as a headline
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在仅给出简短提示（例如标题）的情况下，开发一个符合期望世界观的中等长度故事
- en: Narrative manipulation
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 叙事操纵
- en: Rewriting news articles from a new perspective, shifting the tone, worldview,
    and conclusion to match an intended theme
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 从新的视角重写新闻文章，转变语调、世界观和结论以匹配预期的主题
- en: Narrative seeding
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 叙事植入
- en: Devising new narratives that could form the basis of conspiracy theories
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 制定可能成为阴谋论基础的新叙事
- en: Narrative wedging
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 叙事挤压
- en: Targeting members of particular groups, often based on demographic characteristics
    such as race and religion, with messages designed to prompt certain actions or
    to amplify divisions
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 针对特定群体的成员，通常基于种族和宗教等人口特征，通过旨在促使某些行动或加剧分裂的消息进行定向
- en: Narrative persuasion
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 叙事说服
- en: Changing the views of targets, in some cases by crafting messages tailored to
    their political ideology or affiliation^([8](ch06.xhtml#ch01fn17))
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下通过制作符合其政治意识形态或附属关系的信息来改变目标的观点^([8](ch06.xhtml#ch01fn17))
- en: The results suggest that these activities could amplify forms of deception that
    would be especially difficult to spot. The Georgetown researchers say GPT-3, without—or
    with minimal—human intervention, is quite effective at promoting falsehoods. The
    model particularly excels at automatically generating short messages on social
    media, what the researchers call “one-to-many” misinformation, in which “an operator
    transmits individual messages to a wide audience, such as posting publicly on
    a social media platform.”^([9](ch06.xhtml#ch01fn18))
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，这些活动可能会放大一些特别难以察觉的欺骗形式。乔治城大学的研究人员表示，GPT-3 在没有或几乎没有人类干预的情况下，能够相当有效地传播谎言。该模型特别擅长自动生成社交媒体上的短消息，研究人员称之为“一对多”的错误信息，在这种情况下，“操作者向广大观众传递个别消息，比如在社交媒体平台上公开发布”^([9](ch06.xhtml#ch01fn18))
- en: In an example of narrative reiteration, the researchers portrayed a scenario
    of GPT-3’s ability by considering a disinformation agent with a goal of spreading
    climate-change denialism. They simulated such an agent by selecting a few examples
    to include in a prompt for GPT-3\. For input data, they collected 500 replies
    to @ClimateDepot, an influential climate-change denialist account, sorted the
    replies by number of likes received, and selected the top 10.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在叙述重演的示例中，研究人员描绘了一个关于 GPT-3 能力的情景，考虑了一个旨在传播气候变化否认主义的虚假信息代理的情况。他们通过选择一些示例来模拟这样的代理，并将其包含在
    GPT-3 的提示中。对于输入数据，他们收集了针对@ClimateDepot的 500 条回复，这是一个有影响力的气候变化否认主义账户，将回复按获得的点赞数排序，并选择了前
    10 条。
- en: 'We took these 10—without any curation and only slight formatting adjustments—and
    used them to prompt GPT-3 to produce similar tweets. [Example 6-1](#tweets_generated_by_gpt_three_on_the_fi)
    shows the first result: replies to @ClimateDepot are in bold and GPT-3’s response,
    beginning with tweet 11, is in a regular font.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们拿了这 10 条（没有任何筛选，只有轻微的格式调整）用于提示 GPT-3 生成类似的推文。[示例 6-1](#tweets_generated_by_gpt_three_on_the_fi)
    展示了第一个结果：回复 @ClimateDepot 的部分为粗体，GPT-3 的响应，从第 11 条推文开始，为普通字体。
- en: Example 6-1\. Tweets generated by GPT-3 on the first try in the Georgetown experiment^([10](ch06.xhtml#ch01fn19))
  id: totrans-99
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-1\. 乔治城实验中 GPT-3 第一次尝试生成的推文^([10](ch06.xhtml#ch01fn19))
- en: '[PRE4]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: To test GPT-3’s narrative persuasion capabilities, researchers showed 1,171
    Americans GPT-3-generated statements for and against US sanctions on China.^([11](ch06.xhtml#ch01fn20))
    They instructed GPT-3 to develop 20 statements for each position tailored to Democrats
    and 20 for each position tailored to Republicans. They then selected the top 10
    statements from each of the four groups, as if they were a human operator approving
    half of GPT-3’s outputs. They asked respondents to rate the statements’ convincingness
    on a five-point scale. One such statement, against sanctions and targeting Democrats,
    reads, “This is senseless self-harm and will make it harder for China to cooperate
    with us on climate change.”
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试 GPT-3 的叙述说服能力，研究人员向 1,171 名美国人展示了 GPT-3 生成的支持和反对美国对中国制裁的声明^([11](ch06.xhtml#ch01fn20))。他们指示
    GPT-3 为民主党人和共和党人各定制 20 条立场声明。然后，他们从每个四个组中选择了前 10 条声明，就好像他们是一个人类操作者批准了 GPT-3 输出的一半。他们要求受访者根据五分制对这些声明的说服力进行评分。其中一条反对制裁并针对民主党的声明写道：“这是毫无意义的自我伤害，将使中国在气候变化问题上与我们合作变得更加困难。”
- en: The Georgetown study’s findings are worrying. The majority of the control group
    (51%) favored sanctions, while only 22% opposed them. Of the group that saw GPT-3’s
    anti-sanctions messages, however, only 33% supported sanctions, while 40% opposed
    them. The researchers found it “remarkable that, on an issue of obvious international
    importance, just five short messages from GPT-3 were able to flip a pro-sanction
    majority to an overall anti-sanction view, doubling the percentage of people in
    opposition.”^([12](ch06.xhtml#ch01fn21))
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 乔治城研究的发现令人担忧。控制组中的多数人（51%）支持制裁，而只有 22% 反对制裁。然而，在看到 GPT-3 的反制裁消息的组中，只有 33% 支持制裁，而有
    40% 反对制裁。研究人员发现，“令人惊讶的是，在一个明显具有国际重要性的问题上，仅仅五条来自 GPT-3 的简短消息就能够将支持制裁的多数派转变为总体上的反制裁观点，使反对派人数翻倍。”^([12](ch06.xhtml#ch01fn21))
- en: OpenAI says the Georgetown work highlights an important issue that the company
    hopes to mitigate with measures such as a detailed review process for every production
    use of GPT-3 before it goes live. OpenAI also has a detailed content policy and
    a robust monitoring system in place to restrict misuse. (We discussed these safeguards
    in Chapters [1](ch01.xhtml#the_era_of_large_language_models) and [3](ch03.xhtml#programming_with_gpt_three).)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 表示，Georgetown 的研究突出了一个重要问题，该公司希望通过措施，如在 GPT-3 的每一次生产使用之前进行详细审查流程，来缓解这一问题。OpenAI
    还制定了详细的内容政策和强大的监控系统，以限制滥用。（我们在[第1章](ch01.xhtml#the_era_of_large_language_models)和[第3章](ch03.xhtml#programming_with_gpt_three)中讨论了这些安全措施。）
- en: Another challenge is the model’s environmental impact, which we will examine
    in the next section.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个挑战是模型的环境影响，我们将在下一节中进行探讨。
- en: The Environmental Impact of LLMs
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMs 的环境影响
- en: Practical large-scale pre-training requires large amounts of computation, which
    is energy-intensive. The demand for deep learning has grown rapidly and with it,
    so have the computational resources needed. This has significant environmental
    costs in terms of unsustainable energy use and carbon emissions. In a [2019 study](https://oreil.ly/iBPx4),
    researchers at the University of Massachusetts estimated that training a large
    deep learning model produces 626,000 pounds of planet-warming carbon dioxide,
    equivalent to the lifetime emissions of five cars. As models grow bigger, their
    computing needs are outpacing improvements in hardware efficiency. Chips specialized
    for neural-network processing, like GPUs (graphics processing units) and TPUs
    (tensor processing units), have somewhat offset the demand for more computing
    power, but not by enough.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的大规模预训练需要大量的计算资源，这是耗能的。对深度学习的需求迅速增长，而随之增长的是所需的计算资源。这在不可持续的能源使用和碳排放方面具有显著的环境成本。在[2019年的一项研究](https://oreil.ly/iBPx4)中，马萨诸塞大学的研究人员估计，训练一个大型深度学习模型会产生
    62.6 万磅的温室气体二氧化碳，相当于五辆汽车的寿命排放量。随着模型变得越来越大，它们的计算需求超过了硬件效率的改进。专门用于神经网络处理的芯片，如 GPU（图形处理单元）和
    TPU（张量处理单元），在一定程度上抵消了对更多计算资源的需求，但不够。
- en: The first challenge here is how to measure a trained model’s energy consumption
    and emissions. While a few tools have been developed (such as [Experiment Impact
    Tracker](https://oreil.ly/0hoXB), [ML CO2 Impact Calculator](https://oreil.ly/QQxpp),
    and [Carbontracker](https://oreil.ly/SBW9i)), the ML community has yet to develop
    best measurement practices and tools or establish a habit of measuring and publishing
    models’ environmental impact data.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的第一个挑战是如何衡量训练模型的能耗和排放量。虽然已经开发了一些工具（例如[实验影响追踪器](https://oreil.ly/0hoXB)、[ML
    CO2 影响计算器](https://oreil.ly/QQxpp)和[Carbontracker](https://oreil.ly/SBW9i)），但
    ML 社区尚未发展出最佳的测量实践和工具，或者建立起衡量和发布模型环境影响数据的习惯。
- en: A [2021 study](https://oreil.ly/Yc8X9) estimates that the training of GPT-3
    produced roughly 552 metric tons of carbon dioxide. This is about the amount that
    120 cars would produce in a year of driving. GPT-3’s energy consumption from training
    is 1,287 megawatt-hours (MWh), the heaviest among all of the LLMs the researchers
    examined (see [Figure 6-3](#accelerator_years_of_computationcomma_e)).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 一项[2021年的研究](https://oreil.ly/Yc8X9)估计，GPT-3 的训练产生了约 552 吨二氧化碳。这大约是 120 辆汽车一年的行驶产生的量。GPT-3
    的训练能耗为 1,287 兆瓦时（MWh），是研究人员检查的所有 LLM 中最高的（参见[图6-3](#accelerator_years_of_computationcomma_e)）。
- en: '![](Images/gpt3_0603.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gpt3_0603.png)'
- en: Figure 6-3\. Accelerator years of computation, energy consumption, and CO2e
    for five large NLP deep neural networks (DNNs)^([13](ch06.xhtml#ch01fn22))
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-3\. 五个大型 NLP 深度神经网络（DNNs）的加速器计算年份、能源消耗和 CO2e^([13](ch06.xhtml#ch01fn22))
- en: OpenAI researchers [seem to be cognizant](https://oreil.ly/IR1SM) of the cost
    and efficiency of their models. Pre-training the 175 billion–parameter GPT-3 consumed
    exponentially more compute resources than a 1.5 billion–parameter GPT-2 model
    consumed in its entire training process.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 的研究人员[似乎意识到](https://oreil.ly/IR1SM)他们模型的成本和效率。与 15 亿参数的 GPT-2 模型在整个训练过程中消耗的资源相比，预训练的
    1750 亿参数的 GPT-3 模型消耗的计算资源呈指数增长。
- en: 'In evaluating the environmental impact of LLMs, it’s important to consider
    not only the resources that go into training but also how these resources are
    amortized as the model is used and fine-tuned over its lifetime. Though models
    like GPT-3 consume significant resources during training, they can be surprisingly
    efficient once trained: even with the full GPT-3 175B, generating one hundred
    pages of content from a trained model can cost on the order of 0.4 kW/hr, or only
    a few cents in energy costs. Additionally, because GPT-3 exhibits few-shot generalization,
    it doesn’t need to be retrained for every new task like smaller models do. The
    2019 paper “[Green AI](https://oreil.ly/nuOya)” in the journal *Communications
    of the ACM* notes that “the trend of releasing pre-trained models publicly is
    a green success,” and the authors encourage organizations “to continue to release
    their models in order to save others the costs of retraining them.”'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估LLMs的环境影响时，重要的是不仅考虑用于训练的资源，还要考虑这些资源在模型使用和其生命周期内微调时的摊销情况。 虽然像GPT-3这样的模型在训练期间消耗了大量资源，但一旦训练完成，它们可以出奇地高效：即使使用完整的GPT-3
    175B，从经过训练的模型生成一百页内容的成本也可能达到0.4 kW/hr，或仅几美分的能源成本。 此外，由于GPT-3表现出少量样本泛化，它不需要像较小的模型那样为每个新任务重新训练。
    《ACM通讯》杂志上的2019年论文《绿色人工智能》指出，“公开发布预训练模型的趋势是一种绿色成功”，作者鼓励组织“继续发布他们的模型，以节省其他人重新训练它们的成本。”
- en: A few more strategies have emerged to reduce LLMs’ impact on the planet. As
    Patterson et al. point out, “Remarkably, the choice of DNN, datacenter, and processor
    can reduce the carbon footprint up to ~100-1000X.” Algorithmic techniques can
    also improve energy efficiency. Some work by achieving the same accuracy with
    less overall computation. Other techniques use a large, already-trained model
    as a starting point to yield a lighter-weight, more computationally efficient
    model with almost the same accuracy.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 还出现了一些策略来减少LLMs对地球的影响。 正如Patterson等人所指出的，“值得注意的是，选择DNN、数据中心和处理器可以将碳足迹降低到约100-1000倍。”
    算法技术也可以提高能源效率。 一些工作通过使用较少的总体计算量达到相同的准确性。 其他技术则使用大型、已经训练好的模型作为起点，产生一个更轻量级、更具计算效率的模型，几乎具有相同的准确性。
- en: Proceeding with Caution
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 谨慎前进
- en: We’ll wrap up this chapter with a quick roundup of some common mistakes you’ll
    want to avoid when building your next GPT-3 application.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以快速总结一些在构建下一个 GPT-3 应用程序时要避免的常见错误来结束本章。
- en: First, ask whether you need to use GPT-3\. Think of the level of sophistication
    required for the task or problem you need to solve. Many tasks are trivial enough
    to be solved with other, more cost-effective, open source machine-learning models,
    some of which are publicly available. While this might not be as exciting a cocktail
    party conversation-starter as building an app based on GPT-3, not everything needs
    to be solved by applying the world’s largest, most sophisticated language model.
    When you have a hammer, everything looks like a nail, right? Well, at least we
    warned you.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，请问您是否需要使用GPT-3。 想想您需要解决的任务或问题所需的复杂程度。 许多任务足够琐碎，可以使用其他更经济、开源的机器学习模型来解决，其中一些是公开可用的。
    虽然这可能不像基于GPT-3构建应用程序那样激动人心，但并非一切都需要应用世界上最大、最复杂的语言模型来解决。 当您拥有一把锤子时，一切都像是一颗钉子，对吧？
    好吧，至少我们警告过您。
- en: If GPT-3 really is the right tool for your task, you need to accept and address
    that it was built based on a corpus of text that consists in part of the entire
    internet. So rather than letting it loose in the wild, you would be wise to spend
    some time creating solid content filters.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果GPT-3确实是您的任务的合适工具，您需要接受并解决它是基于部分由整个互联网组成的文本语料库构建的这一事实。 因此，与其让其在野外放任自流，您最好花些时间创建可靠的内容过滤器。
- en: Once your filters are in place, you may want to spend some time giving your
    GPT-3–powered app the exact personality and communication style you desire by
    creating a smaller, carefully curated dataset of text samples. This should include
    sensitive topics and an outline of what behaviors you consider desirable from
    the model. Fine-tuning your model on this dataset allows it to adapt to your style
    and to societal norms.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您的过滤器就位，您可能希望花些时间通过创建一个较小、精心策划的文本样本数据集，为您的 GPT-3 驱动的应用程序赋予您所期望的确切个性和沟通风格。
    这应该包括敏感话题和您认为模型行为良好的轮廓。 在此数据集上微调您的模型可以使其适应您的风格和社会规范。
- en: Your model might feel finished, but do *not* get giddy and release it immediately.
    Instead, release it first in private beta and try it out on some test users. Observe
    how they interact with the model and note whether anything needs to be tweaked
    (which is perfectly normal). Another good practice is to increase your user base
    gradually, so you can improve your app with every iteration.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 你的模型可能已经完成，但不要兴奋地立即发布它。相反，先在私人测试版中发布它，并试用一些测试用户。观察他们如何与模型交互，并注意是否需要调整任何内容（这是完全正常的）。另一个好的做法是逐步增加用户群体，这样您可以在每一次迭代中改进您的应用程序。
- en: Conclusion
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结
- en: As they say, with great power comes great responsibility. This rings especially
    true in the context of GPT-3 and LLMs. As we were completing this book, in early
    2022, the world was reeling from a series of environmental disasters, an unprecedented
    pandemic, and war. In these particularly dynamic and fragile times, it is incredibly
    important to ensure that we can trust the companies producing these powerful models
    to have transparent, value-guided leadership.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 正如他们所说，伴随着巨大的权力就有巨大的责任。在GPT-3和LLMs的背景下，这一点尤其重要。当我们完成这本书时，即2022年初，世界正在受到一系列环境灾难、前所未有的大流行病和战争的影响。在这个特别动荡和脆弱的时期，确保我们能够信任生产这些强大模型的公司具有透明、价值导向的领导是非常重要的。
- en: We discuss the challenges and shortcomings in this chapter not to promote skepticism
    or warn you away from working with LLMs, but because ignoring them can have destructive
    consequences. We see this book as a contribution to an important conversation,
    and we hope that the AI community in general, and OpenAI in particular, will continue
    working to address and solve the problems of LLMs and AI.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章讨论挑战和不足，并非要提倡怀疑论或警告您远离与LLMs合作，而是因为忽视它们可能会产生破坏性后果。我们认为这本书是对一个重要对话的贡献，我们希望人工智能社区，特别是OpenAI，将继续努力解决LLMs和人工智能的问题。
- en: 'But enough darkness: [Chapter 7](ch07.xhtml#democratizing_access_to_ai) concludes
    the book with a look into the future—and some reasons to believe that the LLM-powered
    future is a bright one.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 但是足够了黑暗：[第七章](ch07.xhtml#democratizing_access_to_ai)以展望未来并提供一些理由来结束这本书——LLM驱动的未来是光明的。
- en: '^([1](ch06.xhtml#ch01fn10-marker)) Emily M. Bender et al., “On the Dangers
    of Stochastic Parrots: Can Language Models Be Too Big?” In *Conference on Fairness,
    Accountability, and Transparency (FAccT ’21)*, March 3–10, 2021, virtual event,
    Canada. [*https://doi.org/10.1145/3442188.3445922*](https://doi.org/10.1145/3442188.3445922).
    The fallout from this paper [forced one of its coauthors, acclaimed AI ethics
    researcher Timnit Gebru, to leave Google](https://oreil.ly/45z6F).'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch06.xhtml#ch01fn10-marker)) Emily M. Bender等人，“关于随机鹦鹉的危险：语言模型是否过于庞大？”在*公平性、问责性和透明度会议（FAccT
    ’21）*中，2021年3月3-10日，虚拟活动，加拿大。[*https://doi.org/10.1145/3442188.3445922*](https://doi.org/10.1145/3442188.3445922)。这篇论文的后果[迫使其合著者之一，备受赞誉的AI伦理研究员Timnit
    Gebru离开了Google](https://oreil.ly/45z6F)。
- en: '^([2](ch06.xhtml#ch01fn11-marker)) Samuel Gehman et al., “RealToxicityPrompts:
    Evaluating Neural Toxic Degeneration in Language Models,” *ACL Anthology, Findings
    of the Association for Computational Linguistics: EMNLP 2020*, [*https://aclanthology.org/2020.findings-emnlp.301*](https://oreil.ly/RV5iM).'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch06.xhtml#ch01fn11-marker)) Samuel Gehman等人，“RealToxicityPrompts:评估语言模型中的神经毒性退化”，*ACL文献集，计算语言学协会发现：EMNLP
    2020*，[*https://aclanthology.org/2020.findings-emnlp.301*](https://oreil.ly/RV5iM)。
- en: ^([3](ch06.xhtml#ch01fn12-marker)) Abubakar Abid et al., “Persistent Anti-Muslim
    Bias in Large Language Models,” *Computation and Language*, January 2021, [*https://arxiv.org/pdf/2101.05783.pdf*](https://oreil.ly/qOoEV).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch06.xhtml#ch01fn12-marker)) Abubakar Abid等人，“大型语言模型中的持续反穆斯林偏见”，*计算与语言*，2021年1月，[*https://arxiv.org/pdf/2101.05783.pdf*](https://oreil.ly/qOoEV)。
- en: '^([4](ch06.xhtml#ch01fn13-marker)) Perspective API is an open source API that
    uses machine learning to identify toxic comments, making it easier to host better
    conversations online. It emerged from a collaborative research effort by two teams
    within Google: the Counter Abuse Technology team and Jigsaw, a team that explores
    threats to open societies.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch06.xhtml#ch01fn13-marker)) Perspective API是一个开源API，利用机器学习来识别有毒评论，从而更容易在线上进行更好的对话。它源自谷歌内部两个团队的合作研究成果：反滥用技术团队和Jigsaw，一个探索开放社会威胁的团队。
- en: ^([5](ch06.xhtml#ch01fn14-marker)) Xiaoyan Qiu et al., “Limited Individual Attention
    and Online Virality of Low-Quality Information,” *Nature Human Behaviour* 1, 0132
    (2017), [*https://www.nature.com/articles/s41562-017-0132*](https://oreil.ly/Tm0ee).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch06.xhtml#ch01fn14-marker)) Qiu Xiaoyan等人，“个体有限注意力与低质量信息的在线传播”，*自然人类行为*
    1, 0132 (2017)，[*https://www.nature.com/articles/s41562-017-0132*](https://oreil.ly/Tm0ee)。
- en: ^([6](ch06.xhtml#ch01fn15-marker)) Chengcheng Shao et al., “The Spread of Low-Credibility
    Content by Social Bots,” *Nature Communications* 9, 4787 (2018), [*https://oreil.ly/gdOuY*](https://oreil.ly/gdOuY).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch06.xhtml#ch01fn15-marker)) Shao Chengcheng等人，“社交机器人传播低可信内容”，*自然通讯* 9,
    4787 (2018)，[*https://oreil.ly/gdOuY*](https://oreil.ly/gdOuY)。
- en: '^([7](ch06.xhtml#ch01fn16-marker)) Onur Varol et al., “Online Human-Bot Interactions:
    Detection, Estimation, and Characterization,” Eleventh International AAAI Conference
    on Web and Social Media, 2017, [*https://oreil.ly/wtZ4Y*](https://oreil.ly/wtZ4Y).'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch06.xhtml#ch01fn16-marker)) Onur Varol等人，“在线人机交互：检测、估计和表征”，第十一届国际AAAI
    Web与社交媒体会议，2017年，[*https://oreil.ly/wtZ4Y*](https://oreil.ly/wtZ4Y)。
- en: '^([8](ch06.xhtml#ch01fn17-marker)) Ben Buchanan et al., “Truth, Lies, and Automation:
    How Language Models Could Change Disinformation,” Center for Security and Emerging
    Technology, May 2021, [*https://oreil.ly/If0wJ*](https://oreil.ly/If0wJ), Table
    1.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch06.xhtml#ch01fn17-marker)) Ben Buchanan等人，“真相、谎言和自动化：语言模型如何改变虚假信息”，安全与新兴技术中心，2021年5月，[*https://oreil.ly/If0wJ*](https://oreil.ly/If0wJ)，表1。
- en: ^([9](ch06.xhtml#ch01fn18-marker)) Buchanan et al., “Truth, Lies, and Automation,”
    p. 6.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch06.xhtml#ch01fn18-marker)) Buchanan等人，“真相、谎言和自动化”，第6页。
- en: ^([10](ch06.xhtml#ch01fn19-marker)) Buchanan et al., “Truth, Lies, and Automation,”
    p. 21.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch06.xhtml#ch01fn19-marker)) Buchanan等人，“真相、谎言和自动化”，第21页。
- en: ^([11](ch06.xhtml#ch01fn20-marker)) Buchanan et al., “Truth, Lies, and Automation,”
    p. 44.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch06.xhtml#ch01fn20-marker)) Buchanan等人，“真相、谎言和自动化”，第44页。
- en: ^([12](ch06.xhtml#ch01fn21-marker)) Buchanan et al., “Truth, Lies, and Automation,”
    p. 34.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch06.xhtml#ch01fn21-marker)) Buchanan等人，“真相、谎言和自动化”，第34页。
- en: '^([13](ch06.xhtml#ch01fn22-marker)) Source: David Patterson et al., “Carbon
    Emissions and Large Neural Network Training.” arXiv preprint arXiv:2104.10350
    (2021).'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch06.xhtml#ch01fn22-marker)) 来源：David Patterson等人，“碳排放和大型神经网络训练”。arXiv预印本arXiv:2104.10350
    (2021)。
