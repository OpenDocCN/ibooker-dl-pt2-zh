- en: Chapter 6\. Challenges, Controversies, and Shortcomings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Every technological revolution brings controversy. In this section we focus
    on three of the most controversial aspects of GPT-3: AI bias being encoded into
    the model; low-quality content and the spread of misinformation; and GPT-3’s environmental
    footprint. When you mix human biases with a powerful tool capable of producing
    huge quantities of seemingly coherent text, the results can be dangerous.'
  prefs: []
  type: TYPE_NORMAL
- en: The fluency and coherence of much of GPT-3’s text output raises several risks
    because people are prepared to interpret it as meaningful. Many also view the
    human developers involved in creating GPT-3-based apps as “authors” of its output
    and demand that they be held accountable for its content.
  prefs: []
  type: TYPE_NORMAL
- en: The risks we consider in this chapter follow from the nature of GPT-3’s training
    data, which is to say, the English-speaking internet. Human language reflects
    our worldviews, including our biases—and people who have the time and access to
    publish their words online are often in positions of privilege with respect to
    race, gender, and other attributes that can be forms of oppression, which means
    they tend to be overrepresented in LLM training data. In short, society’s biases
    and dominant worldviews are already encoded in the training data. Without careful
    fine-tuning (more on this later in the chapter), GPT-3 absorbs these biases, problematic
    associations, and abusive language and includes them in its output for the world
    to interpret.
  prefs: []
  type: TYPE_NORMAL
- en: Whatever biases appear in the initial training set or user input are repeated
    and can be amplified or even radicalized in GPT-3-generated output. The risk is
    that people read and spread such texts, reinforcing and propagating problematic
    stereotypes and abusive language in the process. Those targeted by the harmful
    messages may experience psychological repercussions. In addition, those wrongly
    perceived to be “authors” of the GPT-3-generated text may face harm to their reputations
    or even attempts at retribution. What’s more, such biases can also emerge in future
    LLMs trained on datasets that include the publicly available output of previous
    generations of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: The sections that follow look more closely at some of these controversies.
  prefs: []
  type: TYPE_NORMAL
- en: The Challenge of AI Bias
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Research has established that all LLMs have some sort of encoded human bias,
    including stereotypes and negative sentiment toward specific groups (especially
    marginalized minorities). One highly publicized research paper found that “the
    mix of human biases and seemingly coherent language heightens the potential for
    automation bias, deliberate misuse, and amplification of a hegemonic worldview.”^([1](ch06.xhtml#ch01fn10))
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There are a number of O’Reilly Media books focused on the subject of AI bias
    that we encourage you to check out, among them are [*Practical Fairness*](https://oreil.ly/tSDqw)
    and [*97 Things About Ethics Everyone in Data Science Should Know*](https://oreil.ly/vBsVQ).
  prefs: []
  type: TYPE_NORMAL
- en: As YouTuber Kilcher notes, working with GPT-3 is “like interacting with a skewed
    subsample of humanity” because it’s been trained on datasets that represent a
    large swath of the internet. LLMs amplify any biases in the datasets on which
    they are trained. Unfortunately, like much of humanity, this “skewed subsample
    of humanity” is rife with toxic biases, including gender, race, and religious
    prejudices.
  prefs: []
  type: TYPE_NORMAL
- en: A 2020 study of GPT-2, GPT-3’s predecessor, found in the training data 272,000
    documents from unreliable news sites and 63,000 from banned subreddits.^([2](ch06.xhtml#ch01fn11))
    In the same study, both GPT-2 and GPT-3 showed a tendency to generate sentences
    with high toxicity scores, even when prompted with non-toxic sentences. OpenAI
    researchers noted early on that biased datasets led GPT-3 to place words like
    “naughty” or “sucked” near female pronouns and “Islam” near words like “terrorism.”
    A 2021 study by Stanford University researcher Abubakar Abid details consistent
    and creative biased tendencies of text generated by GPT-3, like associating the
    word “Jews” with “money” and “Muslim” with “terrorist” in the paper “Persistent
    Anti-Muslim Bias in Large Language Models.”^([3](ch06.xhtml#ch01fn12))
  prefs: []
  type: TYPE_NORMAL
- en: '[Philosopher AI](https://philosopherai.com), a GPT-3-powered chatbot and essay
    generator, was created to showcase the astounding capabilities of GPT-3, as well
    as its limits. A user enters any prompt, from a few words to a few sentences,
    and the app turns the fragment into a full essay of surprising coherence. Users
    quickly found, however, that certain types of prompts returned offensive and deeply
    troubling results.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Take, for example, this [tweet](https://oreil.ly/MmP4k) by Abeba Birhane, an
    AI researcher who prompted Philosopher AI to generate an essay on “what ails Ethiopia”
    (please note that the images in the tweet include extreme, graphic racism):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Every tech-evangelist: #GPT3 provides deep nuanced viewpoint'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Me: GPT-3, generate a philosophical text about Ethiopia'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: GPT-3 *spits out factually wrong and grossly racist text that portrays a tired
    and cliched Western perception of Ethiopia*
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Birhane included in the tweet two screenshots documenting the app’s output,
    which began with “The main problem with Ethiopia is that Ethiopia itself is the
    problem” and continued in an extremely racist vein.
  prefs: []
  type: TYPE_NORMAL
- en: 'This incident shows a potential danger of deploying commercial products and
    services powered by GPT-3: just imagine a virtual therapist, customer-service
    chatbot, or online tutor spouting this sort of toxic language. Nabla, a Paris-based
    health care technology firm, understands this problem all too well, thanks to
    its attempts to build a GPT-3-powered medical advice chatbot. In 2020, Nabla’s
    team of doctors and machine learning engineers [tested GPT-3](https://oreil.ly/KP78D)
    by exploring different health care use cases, such as conducting an administrative
    chat with a patient, checking medical insurance, providing mental health support,
    answering medical questions, and providing medical documentation and diagnosis.
    Early tests made it clear that Nabla’s use cases were far too high-stakes, complex,
    and delicate for GPT-3 to handle. For example, Nabla’s team tested a hypothetical
    scenario in which the chatbot would provide mental health support to a suicidal
    patient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The GPT-3-powered bot failed here in a way that could be lethal in a real-world
    deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Anti-Bias Countermeasures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'OpenAI’s [research blog](https://openai.com/blog) often notes potential dangers
    in the algorithms the company releases. For instance, a [February 2019 post](https://oreil.ly/rWw6O)
    about GPT-2 noted:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also imagine the application of these models for [malicious purposes](https://oreil.ly/zU9vW),
    including the following (or other applications we can’t yet anticipate):'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Generate misleading news articles
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Impersonate others online
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Automate the production of abusive or faked content to post on social media
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Automate the production of spam/phishing content
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Because of these “concerns about large language models being used to generate
    deceptive, biased, or abusive language at scale,” OpenAI initially released an
    abbreviated version of GPT-3’s predecessor, GPT-2, with sample code, but did not
    release its datasets, training code, or model weights. OpenAI has since invested
    heavily in content filtering models and other research aimed at fixing the biases
    in its AI models. A *content filtering model* is a program fine-tuned to recognize
    potentially offensive language and prevent inappropriate completions. OpenAI provides
    a content filtering engine in its API completions endpoint (discussed in [Chapter 2](ch02.xhtml#using_the_openai_api))
    to filter unwanted text. When the engine is running, it evaluates the text that
    GPT-3 generates and classifies it as “safe,” “sensitive,” or “unsafe.” (For details,
    see the [OpenAI documentation](https://oreil.ly/SjQof).) When you interact with
    the API via the Playground, GPT-3’s content filtering model always runs in the
    background. [Figure 6-1](#content_filter_warning_displayed_in_the) shows an example
    of the Playground tagging potentially offensive content.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gpt3_0601.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-1\. Content filter warning displayed in the Playground
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Since the problem originated from toxic biases in unfiltered data, it seemed
    logical to OpenAI to look for solutions in the data itself. As you’ve seen, language
    models can output almost any kind of text, with any kind of tone or personality,
    depending on the user’s input. In their June 2021 study, OpenAI researchers Irene
    Solaiman and Christy Dennison explain a process they call PALMS, for Process for
    Adapting Language Models to Society. PALMS is a way to improve language model
    behavior with respect to specific ethical, moral, and societal values by fine-tuning
    models on a curated dataset of fewer than one hundred examples of those values.
    This process becomes more effective as models get larger. Models showed behavioral
    improvement without compromising their accuracy on downstream tasks, suggesting
    that OpenAI can develop tools to narrow GPT-3’s repertoire of behaviors to a constrained
    set of values.
  prefs: []
  type: TYPE_NORMAL
- en: 'While the PALMS process is effective, this research only scratches the surface.
    Some important unanswered questions include:'
  prefs: []
  type: TYPE_NORMAL
- en: Who should be consulted when designing a values-targeted dataset?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Who is accountable when a user receives output that is not aligned with their
    own values?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How robust is this methodology compared to real-world prompts? (The OpenAI researchers
    experimented only with a question-answer format.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The PALMS process involves three steps: first, outlining the desirable behavior;
    second, crafting and refining the dataset; and third, evaluating the effect on
    model performance. Let’s look at each in turn:'
  prefs: []
  type: TYPE_NORMAL
- en: Sensitive topic categories and outlining desirable behavior
  prefs: []
  type: TYPE_NORMAL
- en: The researchers created categories (for example, “injustice and inequality”)
    and prioritized them based on their direct impact on human well-being. For each
    category, they described a desired behavior. In the case of “injustice and inequality,”
    they instructed the model to “oppose human injustices and inequalities, or work
    that exacerbates either. This includes harmful stereotypes and prejudices, especially
    against social groups according to international law.” This description was largely
    based on US and international human rights law and Western social movements for
    human equality.
  prefs: []
  type: TYPE_NORMAL
- en: Crafting and fine-tuning the dataset
  prefs: []
  type: TYPE_NORMAL
- en: Next, the researchers curated a values-targeted dataset, meaning a dataset that
    reflects a predetermined set of target values, of 80 text samples (just 0.000000211%
    of the size of GPT-3’s overall training dataset). Each sample was in question-answer
    format and between 40 and 340 words.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating model performance
  prefs: []
  type: TYPE_NORMAL
- en: Next, OpenAI applied quantitative and qualitative metrics to evaluate the results
    the newly fine-tuned models generated. They asked human evaluators to rate how
    well the models complied with the predetermined values. They also applied toxicity
    scoring, using the Perspective API^([4](ch06.xhtml#ch01fn13)) and other tools
    to examine mentions of gender, race, and religion.
  prefs: []
  type: TYPE_NORMAL
- en: The results look surprisingly effective. One of the researchers’ examples shows
    two sets of answers to the same question generated by a base model and a values-targeted
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Base model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Values-targeted model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: PALMS can help companies create standardized processes for deploying LLMs such
    as GPT-3.
  prefs: []
  type: TYPE_NORMAL
- en: Another breakthrough anti-bias development is [Instruct GPT](https://oreil.ly/bP3el),
    a series of models that are better at following instructions, less toxic, and
    more truthful than the original GPT-3\. (We discuss the Instruct series in more
    detail in [Chapter 2](ch02.xhtml#using_the_openai_api).)
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s move to another challenge: the spread of low-quality content and
    misinformation.'
  prefs: []
  type: TYPE_NORMAL
- en: Low-Quality Content and the Spread of Misinformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An entirely new category of risk may come into the picture when we consider
    the potential misuse of GPT-3\. Possible use cases here are as trivial as applications
    designed to automate writing term papers, clickbait articles, and interacting
    on social media accounts, all the way to intentionally promoting misinformation
    and extremism using similar channels.
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors of the OpenAI paper that presented GPT-3 to the world in July 2020,
    [“Language Models are Few-Shot Learners”](https://oreil.ly/IR1SM), included a
    section on “Misuse of Language Models”:'
  prefs: []
  type: TYPE_NORMAL
- en: Any socially harmful activity that relies on generating text could be augmented
    by powerful language models. Examples include misinformation, spam, phishing,
    abuse of legal and governmental processes, fraudulent academic essay writing and
    social engineering pretexting.…The misuse potential of language models increases
    as the quality of text synthesis improves. The ability of GPT-3 to generate several
    paragraphs of synthetic content that people find difficult to distinguish from
    human-written text…represents a concerning milestone in this regard.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The GPT-3 experiments are providing us with some particularly vivid examples,
    including low-quality “spam” and the spread of misinformation, as we will show
    you in a moment. Before we imagine GPT-3 becoming too powerful, though, let’s
    consider for a moment what it can actually do right now: produce very cheap, unreliable,
    and low-quality content that floods the internet and pollutes its information
    quality. As AI researcher Julian Togelius [puts it](https://oreil.ly/1XvqN): “GPT-3
    often performs like a clever student who hasn’t done their reading, trying to
    bull$&^! their way through an exam. Some well-known facts, some half-truths, and
    some straight lies, strung together in what [at] first looks like a smooth narrative.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Kilcher notes that the public often has unrealistic expectations of a model
    that is, at the base, predicting the most probable text to follow a given prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: I think a lot of the misconceptions come from the fact that people expect something
    else from the model than what it does and what it’s good at.…It’s not an oracle,
    it’s simply continuing texts as it would find them on the internet. So if you
    start a piece of text that looks like it comes from a Flat Earth Society website,
    it’s going to continue that text in [the same] manner. That doesn’t mean…it’s
    lying to you. It simply means “here is the most probable continuation for this
    piece of text.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'GPT-3 has no way to verify the truth, logic, or meaning of any of the millions
    of lines of text it produces on a daily basis. The responsibility for verification
    and curation therefore rests with the humans overseeing each project. What generally
    seems to happen is that we, as humans, look for shortcuts: outsourcing the cumbersome
    task of writing to the algorithm, skipping a few steps in the editing process,
    skipping the fact-checking process. This results in more and more low-quality
    content being generated with the help of GPT-3\. And the most worrying aspect
    of it is that most people don’t seem to realize the difference.'
  prefs: []
  type: TYPE_NORMAL
- en: Liam Porr, a computer science student at the University of California, Berkeley,
    experienced firsthand how easy it is to mislead people into believing that they’re
    reading a human-authored text, when, in fact, the human has only copied and pasted
    from model-generated outputs. As an experiment, he used GPT-3 to produce [an entirely
    fake blog](https://oreil.ly/qynav) under a pseudonym. He was surprised when, on
    July 20, 2020, one of his posts reached the number-one spot on Hacker News ([Figure 6-2](#a_gpt_three_generated_fake_blog_reached)).
    Few people noticed that his blog was completely AI-generated. Some even hit “subscribe.”
  prefs: []
  type: TYPE_NORMAL
- en: Porr wanted to demonstrate that GPT-3 could pass itself off as a human writer—and
    he proved his point. Despite the weird writing pattern and a few errors, only
    a small percentage of Hacker News commenters asked if the post might have been
    generated by an algorithm. Those comments were immediately downvoted by other
    community members. For Porr, the most astonishing aspect of his “achievement”
    was that “it was super easy, actually, which was the scary part.”
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gpt3_0602.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-2\. A GPT-3-generated fake blog reached the top place on Hacker News
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Creating and viewing blogs, videos, tweets, and other types of digital information
    has become cheap and easy to the point of information overload. Viewers, unable
    to process all this material, often let cognitive biases decide what they should
    pay attention to. These mental shortcuts influence which information we search
    for, comprehend, remember, and repeat—to a harmful extent. It’s easy to fall prey
    to low-quality pieces of information, which GPT-3 can produce quickly and at high
    volume.
  prefs: []
  type: TYPE_NORMAL
- en: A 2017 study used statistical models to link the spread of low-quality information
    over social media networks to limited reader attention and high information load.
    Both factors, the researchers found, can lead to an inability to discriminate
    between good and bad information.^([5](ch06.xhtml#ch01fn14)) A study from 2019
    showed how automated, bot-controlled social media accounts had influenced the
    spread of misinformation during the 2016 US election period. When a fake news
    article was posted, for example, claiming that Hillary Clinton’s presidential
    campaign was involved in occult rituals, within seconds it was retweeted by many
    bots, as well as humans.^([6](ch06.xhtml#ch01fn15))
  prefs: []
  type: TYPE_NORMAL
- en: A [2021 study](https://oreil.ly/vkufU) corroborated this, finding that 75% of
    American respondents who say they follow news and current events agree that fake
    news is a big problem today.
  prefs: []
  type: TYPE_NORMAL
- en: One source of this flood of low-quality content is automated, bot-controlled
    social media accounts that impersonate humans, enabling misguided or malevolent
    actors to take advantage of readers’ vulnerabilities. In 2017, a research team
    estimated that up to 15% of active Twitter accounts were bots.^([7](ch06.xhtml#ch01fn16))
  prefs: []
  type: TYPE_NORMAL
- en: There are many social media accounts that openly identify themselves as GPT-3
    bots, but some GPT-3-powered bots hide their true nature. In 2020, Reddit user
    Philip Winston [uncovered a hidden GPT-3 bot](https://oreil.ly/Oe3Gb) posing as
    a fellow Reddit user under the username /u/thegentlemetre. The bot interacted
    with other forum members for a week on /r/AskReddit, a general chat with an audience
    of 30 million. While its comments were not harmful in this instance, the bot could
    easily have spread harmful or unreliable content.
  prefs: []
  type: TYPE_NORMAL
- en: As you’ve seen throughout this book, GPT-3’s output is a synthesis of its training
    data, which is mostly unverified public internet data. Most of this data is neither
    well-curated nor written by responsible, accountable individuals. There’s a cascading
    effect, where the current content of the internet negatively impacts the future
    content by becoming part of its dataset, continually lowering the average quality
    of its text. As Andrej Karpathy [tweeted](https://oreil.ly/AiqC8), half-jokingly,
    “By posting GPT generated text we’re polluting the data for its future versions.”
  prefs: []
  type: TYPE_NORMAL
- en: Given the use cases we’ve seen for GPT-3’s growing role in artistic and literary
    production, it’s reasonable to assume that further advancements in text-generating
    models will profoundly impact the future of literature. If a large portion of
    all written material is computer-generated, we are going to encounter a tough
    situation.
  prefs: []
  type: TYPE_NORMAL
- en: In 2018, researchers conducted the [largest-ever study](https://oreil.ly/ktKgu)
    of the spread of [false news](https://oreil.ly/VotSs) online. They investigated
    a dataset of all the true and fake news stories (as verified by six independent
    fact-checking organizations) that were distributed on Twitter from 2006 to 2017\.
    The study found that fake news online travels “farther, faster, deeper, and more
    broadly than the truth.” Falsehoods were 70% more likely to be retweeted on Twitter
    than the truth and reached a threshold of 1,500 viewers, about six times faster
    than the truth. The effect was greater for fake political news than for fake news
    about terrorism, natural disasters, science, urban legends, or financial information.
  prefs: []
  type: TYPE_NORMAL
- en: Acting on the wrong information can become deadly, as the COVID-19 pandemic
    made tragically clear. In the first three months of 2020, as the pandemic began,
    nearly 6,000 people around the globe were hospitalized due to coronavirus misinformation,
    research suggests. During this period, [researchers say](https://oreil.ly/bpOGu),
    at least 800 people may have died due to misinformation related to COVID-19; those
    numbers will surely increase as research continues.
  prefs: []
  type: TYPE_NORMAL
- en: Misinformation is also a powerful weapon to spur political chaos, as is evident
    in the Russian war against Ukraine that is taking place as this book goes to press
    in early 2022\. Researchers and journalists from respected outlets including [Politico](https://oreil.ly/SLOBz)*,*
    [Wired](https://oreil.ly/p9yOr)*,* and [TechTarget](https://oreil.ly/MWf4I) have
    unearthed fake TikTok videos, anti-refugee Instagram accounts, pro-Kremlin Twitter
    bots, and even AI-generated deepfake videos of Ukraine president Volodymyr Zelenskyy
    asking his soldiers to drop their weapons.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3 allows users to mass-generate content. Users can then immediately test
    it on social media channels to see if the message is effective, as often as a
    few thousand times a day. This lets the model quickly learn how to sway targeted
    demographic groups of social media users. In the wrong hands, it can easily become
    the engine of a powerful propaganda machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'In 2021, researchers from Georgetown University evaluated GPT-3’s performance
    on six misinformation-related tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Narrative reiteration
  prefs: []
  type: TYPE_NORMAL
- en: Generating varied short messages that advance a particular theme, such as climate
    change denial
  prefs: []
  type: TYPE_NORMAL
- en: Narrative elaboration
  prefs: []
  type: TYPE_NORMAL
- en: Developing a medium-length story that fits within a desired worldview when given
    only a short prompt, such as a headline
  prefs: []
  type: TYPE_NORMAL
- en: Narrative manipulation
  prefs: []
  type: TYPE_NORMAL
- en: Rewriting news articles from a new perspective, shifting the tone, worldview,
    and conclusion to match an intended theme
  prefs: []
  type: TYPE_NORMAL
- en: Narrative seeding
  prefs: []
  type: TYPE_NORMAL
- en: Devising new narratives that could form the basis of conspiracy theories
  prefs: []
  type: TYPE_NORMAL
- en: Narrative wedging
  prefs: []
  type: TYPE_NORMAL
- en: Targeting members of particular groups, often based on demographic characteristics
    such as race and religion, with messages designed to prompt certain actions or
    to amplify divisions
  prefs: []
  type: TYPE_NORMAL
- en: Narrative persuasion
  prefs: []
  type: TYPE_NORMAL
- en: Changing the views of targets, in some cases by crafting messages tailored to
    their political ideology or affiliation^([8](ch06.xhtml#ch01fn17))
  prefs: []
  type: TYPE_NORMAL
- en: The results suggest that these activities could amplify forms of deception that
    would be especially difficult to spot. The Georgetown researchers say GPT-3, without—or
    with minimal—human intervention, is quite effective at promoting falsehoods. The
    model particularly excels at automatically generating short messages on social
    media, what the researchers call “one-to-many” misinformation, in which “an operator
    transmits individual messages to a wide audience, such as posting publicly on
    a social media platform.”^([9](ch06.xhtml#ch01fn18))
  prefs: []
  type: TYPE_NORMAL
- en: In an example of narrative reiteration, the researchers portrayed a scenario
    of GPT-3’s ability by considering a disinformation agent with a goal of spreading
    climate-change denialism. They simulated such an agent by selecting a few examples
    to include in a prompt for GPT-3\. For input data, they collected 500 replies
    to @ClimateDepot, an influential climate-change denialist account, sorted the
    replies by number of likes received, and selected the top 10.
  prefs: []
  type: TYPE_NORMAL
- en: 'We took these 10—without any curation and only slight formatting adjustments—and
    used them to prompt GPT-3 to produce similar tweets. [Example 6-1](#tweets_generated_by_gpt_three_on_the_fi)
    shows the first result: replies to @ClimateDepot are in bold and GPT-3’s response,
    beginning with tweet 11, is in a regular font.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-1\. Tweets generated by GPT-3 on the first try in the Georgetown experiment^([10](ch06.xhtml#ch01fn19))
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: To test GPT-3’s narrative persuasion capabilities, researchers showed 1,171
    Americans GPT-3-generated statements for and against US sanctions on China.^([11](ch06.xhtml#ch01fn20))
    They instructed GPT-3 to develop 20 statements for each position tailored to Democrats
    and 20 for each position tailored to Republicans. They then selected the top 10
    statements from each of the four groups, as if they were a human operator approving
    half of GPT-3’s outputs. They asked respondents to rate the statements’ convincingness
    on a five-point scale. One such statement, against sanctions and targeting Democrats,
    reads, “This is senseless self-harm and will make it harder for China to cooperate
    with us on climate change.”
  prefs: []
  type: TYPE_NORMAL
- en: The Georgetown study’s findings are worrying. The majority of the control group
    (51%) favored sanctions, while only 22% opposed them. Of the group that saw GPT-3’s
    anti-sanctions messages, however, only 33% supported sanctions, while 40% opposed
    them. The researchers found it “remarkable that, on an issue of obvious international
    importance, just five short messages from GPT-3 were able to flip a pro-sanction
    majority to an overall anti-sanction view, doubling the percentage of people in
    opposition.”^([12](ch06.xhtml#ch01fn21))
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI says the Georgetown work highlights an important issue that the company
    hopes to mitigate with measures such as a detailed review process for every production
    use of GPT-3 before it goes live. OpenAI also has a detailed content policy and
    a robust monitoring system in place to restrict misuse. (We discussed these safeguards
    in Chapters [1](ch01.xhtml#the_era_of_large_language_models) and [3](ch03.xhtml#programming_with_gpt_three).)
  prefs: []
  type: TYPE_NORMAL
- en: Another challenge is the model’s environmental impact, which we will examine
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: The Environmental Impact of LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Practical large-scale pre-training requires large amounts of computation, which
    is energy-intensive. The demand for deep learning has grown rapidly and with it,
    so have the computational resources needed. This has significant environmental
    costs in terms of unsustainable energy use and carbon emissions. In a [2019 study](https://oreil.ly/iBPx4),
    researchers at the University of Massachusetts estimated that training a large
    deep learning model produces 626,000 pounds of planet-warming carbon dioxide,
    equivalent to the lifetime emissions of five cars. As models grow bigger, their
    computing needs are outpacing improvements in hardware efficiency. Chips specialized
    for neural-network processing, like GPUs (graphics processing units) and TPUs
    (tensor processing units), have somewhat offset the demand for more computing
    power, but not by enough.
  prefs: []
  type: TYPE_NORMAL
- en: The first challenge here is how to measure a trained model’s energy consumption
    and emissions. While a few tools have been developed (such as [Experiment Impact
    Tracker](https://oreil.ly/0hoXB), [ML CO2 Impact Calculator](https://oreil.ly/QQxpp),
    and [Carbontracker](https://oreil.ly/SBW9i)), the ML community has yet to develop
    best measurement practices and tools or establish a habit of measuring and publishing
    models’ environmental impact data.
  prefs: []
  type: TYPE_NORMAL
- en: A [2021 study](https://oreil.ly/Yc8X9) estimates that the training of GPT-3
    produced roughly 552 metric tons of carbon dioxide. This is about the amount that
    120 cars would produce in a year of driving. GPT-3’s energy consumption from training
    is 1,287 megawatt-hours (MWh), the heaviest among all of the LLMs the researchers
    examined (see [Figure 6-3](#accelerator_years_of_computationcomma_e)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gpt3_0603.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-3\. Accelerator years of computation, energy consumption, and CO2e
    for five large NLP deep neural networks (DNNs)^([13](ch06.xhtml#ch01fn22))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: OpenAI researchers [seem to be cognizant](https://oreil.ly/IR1SM) of the cost
    and efficiency of their models. Pre-training the 175 billion–parameter GPT-3 consumed
    exponentially more compute resources than a 1.5 billion–parameter GPT-2 model
    consumed in its entire training process.
  prefs: []
  type: TYPE_NORMAL
- en: 'In evaluating the environmental impact of LLMs, it’s important to consider
    not only the resources that go into training but also how these resources are
    amortized as the model is used and fine-tuned over its lifetime. Though models
    like GPT-3 consume significant resources during training, they can be surprisingly
    efficient once trained: even with the full GPT-3 175B, generating one hundred
    pages of content from a trained model can cost on the order of 0.4 kW/hr, or only
    a few cents in energy costs. Additionally, because GPT-3 exhibits few-shot generalization,
    it doesn’t need to be retrained for every new task like smaller models do. The
    2019 paper “[Green AI](https://oreil.ly/nuOya)” in the journal *Communications
    of the ACM* notes that “the trend of releasing pre-trained models publicly is
    a green success,” and the authors encourage organizations “to continue to release
    their models in order to save others the costs of retraining them.”'
  prefs: []
  type: TYPE_NORMAL
- en: A few more strategies have emerged to reduce LLMs’ impact on the planet. As
    Patterson et al. point out, “Remarkably, the choice of DNN, datacenter, and processor
    can reduce the carbon footprint up to ~100-1000X.” Algorithmic techniques can
    also improve energy efficiency. Some work by achieving the same accuracy with
    less overall computation. Other techniques use a large, already-trained model
    as a starting point to yield a lighter-weight, more computationally efficient
    model with almost the same accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Proceeding with Caution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ll wrap up this chapter with a quick roundup of some common mistakes you’ll
    want to avoid when building your next GPT-3 application.
  prefs: []
  type: TYPE_NORMAL
- en: First, ask whether you need to use GPT-3\. Think of the level of sophistication
    required for the task or problem you need to solve. Many tasks are trivial enough
    to be solved with other, more cost-effective, open source machine-learning models,
    some of which are publicly available. While this might not be as exciting a cocktail
    party conversation-starter as building an app based on GPT-3, not everything needs
    to be solved by applying the world’s largest, most sophisticated language model.
    When you have a hammer, everything looks like a nail, right? Well, at least we
    warned you.
  prefs: []
  type: TYPE_NORMAL
- en: If GPT-3 really is the right tool for your task, you need to accept and address
    that it was built based on a corpus of text that consists in part of the entire
    internet. So rather than letting it loose in the wild, you would be wise to spend
    some time creating solid content filters.
  prefs: []
  type: TYPE_NORMAL
- en: Once your filters are in place, you may want to spend some time giving your
    GPT-3–powered app the exact personality and communication style you desire by
    creating a smaller, carefully curated dataset of text samples. This should include
    sensitive topics and an outline of what behaviors you consider desirable from
    the model. Fine-tuning your model on this dataset allows it to adapt to your style
    and to societal norms.
  prefs: []
  type: TYPE_NORMAL
- en: Your model might feel finished, but do *not* get giddy and release it immediately.
    Instead, release it first in private beta and try it out on some test users. Observe
    how they interact with the model and note whether anything needs to be tweaked
    (which is perfectly normal). Another good practice is to increase your user base
    gradually, so you can improve your app with every iteration.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As they say, with great power comes great responsibility. This rings especially
    true in the context of GPT-3 and LLMs. As we were completing this book, in early
    2022, the world was reeling from a series of environmental disasters, an unprecedented
    pandemic, and war. In these particularly dynamic and fragile times, it is incredibly
    important to ensure that we can trust the companies producing these powerful models
    to have transparent, value-guided leadership.
  prefs: []
  type: TYPE_NORMAL
- en: We discuss the challenges and shortcomings in this chapter not to promote skepticism
    or warn you away from working with LLMs, but because ignoring them can have destructive
    consequences. We see this book as a contribution to an important conversation,
    and we hope that the AI community in general, and OpenAI in particular, will continue
    working to address and solve the problems of LLMs and AI.
  prefs: []
  type: TYPE_NORMAL
- en: 'But enough darkness: [Chapter 7](ch07.xhtml#democratizing_access_to_ai) concludes
    the book with a look into the future—and some reasons to believe that the LLM-powered
    future is a bright one.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch06.xhtml#ch01fn10-marker)) Emily M. Bender et al., “On the Dangers
    of Stochastic Parrots: Can Language Models Be Too Big?” In *Conference on Fairness,
    Accountability, and Transparency (FAccT ’21)*, March 3–10, 2021, virtual event,
    Canada. [*https://doi.org/10.1145/3442188.3445922*](https://doi.org/10.1145/3442188.3445922).
    The fallout from this paper [forced one of its coauthors, acclaimed AI ethics
    researcher Timnit Gebru, to leave Google](https://oreil.ly/45z6F).'
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](ch06.xhtml#ch01fn11-marker)) Samuel Gehman et al., “RealToxicityPrompts:
    Evaluating Neural Toxic Degeneration in Language Models,” *ACL Anthology, Findings
    of the Association for Computational Linguistics: EMNLP 2020*, [*https://aclanthology.org/2020.findings-emnlp.301*](https://oreil.ly/RV5iM).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch06.xhtml#ch01fn12-marker)) Abubakar Abid et al., “Persistent Anti-Muslim
    Bias in Large Language Models,” *Computation and Language*, January 2021, [*https://arxiv.org/pdf/2101.05783.pdf*](https://oreil.ly/qOoEV).
  prefs: []
  type: TYPE_NORMAL
- en: '^([4](ch06.xhtml#ch01fn13-marker)) Perspective API is an open source API that
    uses machine learning to identify toxic comments, making it easier to host better
    conversations online. It emerged from a collaborative research effort by two teams
    within Google: the Counter Abuse Technology team and Jigsaw, a team that explores
    threats to open societies.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch06.xhtml#ch01fn14-marker)) Xiaoyan Qiu et al., “Limited Individual Attention
    and Online Virality of Low-Quality Information,” *Nature Human Behaviour* 1, 0132
    (2017), [*https://www.nature.com/articles/s41562-017-0132*](https://oreil.ly/Tm0ee).
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch06.xhtml#ch01fn15-marker)) Chengcheng Shao et al., “The Spread of Low-Credibility
    Content by Social Bots,” *Nature Communications* 9, 4787 (2018), [*https://oreil.ly/gdOuY*](https://oreil.ly/gdOuY).
  prefs: []
  type: TYPE_NORMAL
- en: '^([7](ch06.xhtml#ch01fn16-marker)) Onur Varol et al., “Online Human-Bot Interactions:
    Detection, Estimation, and Characterization,” Eleventh International AAAI Conference
    on Web and Social Media, 2017, [*https://oreil.ly/wtZ4Y*](https://oreil.ly/wtZ4Y).'
  prefs: []
  type: TYPE_NORMAL
- en: '^([8](ch06.xhtml#ch01fn17-marker)) Ben Buchanan et al., “Truth, Lies, and Automation:
    How Language Models Could Change Disinformation,” Center for Security and Emerging
    Technology, May 2021, [*https://oreil.ly/If0wJ*](https://oreil.ly/If0wJ), Table
    1.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch06.xhtml#ch01fn18-marker)) Buchanan et al., “Truth, Lies, and Automation,”
    p. 6.
  prefs: []
  type: TYPE_NORMAL
- en: ^([10](ch06.xhtml#ch01fn19-marker)) Buchanan et al., “Truth, Lies, and Automation,”
    p. 21.
  prefs: []
  type: TYPE_NORMAL
- en: ^([11](ch06.xhtml#ch01fn20-marker)) Buchanan et al., “Truth, Lies, and Automation,”
    p. 44.
  prefs: []
  type: TYPE_NORMAL
- en: ^([12](ch06.xhtml#ch01fn21-marker)) Buchanan et al., “Truth, Lies, and Automation,”
    p. 34.
  prefs: []
  type: TYPE_NORMAL
- en: '^([13](ch06.xhtml#ch01fn22-marker)) Source: David Patterson et al., “Carbon
    Emissions and Large Neural Network Training.” arXiv preprint arXiv:2104.10350
    (2021).'
  prefs: []
  type: TYPE_NORMAL
