- en: 1 What is deep learning?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 深度学习是什么？
- en: '*This chapter covers*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: High-level definitions of fundamental concepts
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础概念的高级定义
- en: Time line of the development of machine learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习发展时间线
- en: Key factors behind deep learning’s rising popularity and future potential
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习崛起及未来潜力背后的关键因素
- en: 'In the past few years, artificial intelligence (AI) has been a subject of intense
    media hype. Machine learning, deep learning, and AI come up in countless articles,
    often outside of technology-minded publications. We’re promised a future of intelligent
    chatbots, self-driving cars, and virtual assistants—a future sometimes painted
    in a grim light and other times as utopian, where human jobs will be scarce and
    most economic activity will be handled by robots or AI agents. For a future or
    current practitioner of machine learning, it’s important to be able to recognize
    the signal amid the noise, so that you can tell world-changing developments from
    overhyped press releases. Our future is at stake, and it’s a future in which you
    have an active role to play: after reading this book, you’ll be one of those who
    develop those AI systems. So let’s tackle these questions: What has deep learning
    achieved so far? How significant is it? Where are we headed next? Should you believe
    the hype?'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，人工智能（AI）一直是媒体炒作的对象。机器学习、深度学习和人工智能在无数文章中被提及，常常出现在非技术类刊物中。我们被承诺一个智能聊天机器人、自动驾驶汽车和虚拟助手的未来——有时候是被描绘成灰暗的，有时候是乌托邦的，一个未来，人类的工作将会稀缺，大部分经济活动将由机器人或AI代理处理。对于一个未来或现在的机器学习从业者来说，能够识别噪音中的信号非常重要，这样你就可以从炒作的新闻稿中找出改变世界的发展。我们的未来岌岌可危，而且这是一个你要积极参与的未来：在阅读完本书后，你将成为那些开发这些AI系统的人之一。因此，让我们来解决这些问题：到目前为止，深度学习已经取得了什么成就？它有多重要？我们接下来将走向何方？你应该相信这种炒作吗？
- en: This chapter provides essential context around artificial intelligence, machine
    learning, and deep learning.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章提供了围绕人工智能、机器学习和深度学习的基本背景。
- en: 1.1 Artificial intelligence, machine learning, and deep learning
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 人工智能、机器学习和深度学习
- en: First, we need to define clearly what we’re talking about when we mention AI.
    What are artificial intelligence, machine learning, and deep learning (see [figure
    1.1](#fig1-1))? How do they relate to each other?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，当我们提到人工智能时，我们需要清楚地定义我们谈论的是什么。人工智能、机器学习和深度学习是什么（见[图 1.1](#fig1-1)）？它们之间有何关联？
- en: '![Image](../images/f0002-01.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0002-01.jpg)'
- en: '**Figure 1.1 Artificial intelligence, machine learning, and deep learning**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 1.1 人工智能、机器学习和深度学习**'
- en: 1.1.1 Artificial intelligence
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1.1 人工智能
- en: Artificial intelligence was born in the 1950s, when a handful of pioneers from
    the nascent field of computer science started asking whether computers could be
    made to “think”—a question whose ramifications we’re still exploring today.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能诞生于20世纪50年代，当时来自计算机科学新兴领域的一小撮先驱开始思考计算机是否能够“思考”——这个问题的影响我们今天仍在探索。
- en: 'Although many of the underlying ideas had been brewing in the years and even
    decades prior, “artificial intelligence” finally crystallized as a field of research
    in 1956, when John McCarthy, then a young assistant professor of mathematics at
    Dartmouth College, organized a summer workshop under the following proposal:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管许多潜在的想法在之前的年份甚至几十年里一直在酝酿，但是“人工智能”最终在1956年成为一个研究领域，当时达特茅斯学院数学系的年轻助理教授约翰·麦卡锡（John
    McCarthy）根据以下提议组织了一个暑期研讨会：
- en: '*The study is to proceed on the basis of the conjecture that every aspect of
    learning or any other feature of intelligence can in principle be so precisely
    described that a machine can be made to simulate it. An attempt will be made to
    find how to make machines use language, form abstractions and concepts, solve
    kinds of problems now reserved for humans, and improve themselves. We think that
    a significant advance can be made in one or more of these problems if a carefully
    selected group of scientists work on it together for a summer.*'
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*这项研究的基础假设是，学习的每个方面或智能的任何其他特征原则上都可以被如此精确地描述，以至于可以制造出一台机器来模拟它。我们将尝试找出如何让机器使用语言、形成抽象和概念、解决目前仅保留给人类的各种问题，并改进自己。我们认为，如果一组精心挑选的科学家们一起为此进行一个夏天的工作，就可以在这些问题中的一个或多个方面取得重大进展。*'
- en: At the end of the summer, the workshop concluded without having fully solved
    the riddle it set out to investigate. Nevertheless, it was attended by many people
    who would move on to become pioneers in the field, and it set in motion an intellectual
    revolution that is still ongoing to this day.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 夏末，研讨会在没有完全解决其旨在调查的谜团的情况下结束了。尽管如此，参加的人很多，其中许多人后来成为该领域的先驱，并引发了一场至今仍在进行的知识革命。
- en: Concisely, AI can be described as *the effort to automate intellectual tasks
    normally performed by humans*. As such, AI is a general field that encompasses
    machine learning and deep learning, but that also includes many more approaches
    that may not involve any learning. Consider that until the 1980s, most AI textbooks
    didn’t mention “learning” at all! Early chess programs, for instance, involved
    only hardcoded rules crafted by programmers and didn’t qualify as machine learning.
    In fact, for a fairly long time, most experts believed that human-level artificial
    intelligence could be achieved by having programmers handcraft a sufficiently
    large set of explicit rules for manipulating knowledge stored in explicit databases.
    This approach is known as *symbolic AI*. It was the dominant paradigm in AI from
    the 1950s to the late 1980s, and it reached its peak popularity during the *expert
    systems* boom of the 1980s.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 简言之，AI可以被描述为*试图自动化人类通常执行的智力任务*。因此，AI是一个通用领域，包括机器学习和深度学习，但也包括许多不涉及任何学习的其他方法。考虑到直到20世纪80年代，大多数AI教科书根本没有提到“学习”！例如，早期的国际象棋程序仅涉及程序员制定的硬编码规则，并且不符合机器学习的条件。事实上，相当长的一段时间，大多数专家都相信，通过让程序员手工制作足够大的一组明确规则来操作明确数据库中存储的知识，可以实现人类级别的人工智能。这种方法被称为*符号AI*。它是20世纪50年代到20世纪80年代末的AI的主导范式，并且在20世纪80年代的*专家系统*繁荣时期达到了最高的流行度。
- en: 'Although symbolic AI proved suitable to solve well-defined, logical problems,
    such as playing chess, it turned out to be intractable to figure out explicit
    rules for solving more complex, fuzzy problems, such as image classification,
    speech recognition, or natural language translation. A new approach arose to take
    symbolic AI’s place: *machine learning*.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管符号AI被证明适合解决定义良好的逻辑问题，例如下棋，但发现为解决更复杂、模糊的问题（例如图像分类、语音识别或自然语言翻译）制定明确规则是不可行的。一个新的方法出现取代了符号AI的位置：*机器学习*。
- en: 1.1.2 Machine learning
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1.2 机器学习
- en: 'In Victorian England, Lady Ada Lovelace was a friend and collaborator of Charles
    Babbage, the inventor of the *Analytical Engine*: the first-known general-purpose
    mechanical computer. Although visionary and far ahead of its time, the Analytical
    Engine wasn’t meant as a general-purpose computer when it was designed in the
    1830s and 1840s, because the concept of general-purpose computation had yet to
    be invented. It was merely meant as a way to use mechanical operations to automate
    certain computations from the field of mathematical analysis—hence the name Analytical
    Engine. As such, it was the intellectual descendant of earlier attempts at encoding
    mathematical operations in gear form, such as the Pascaline, or Leibniz’s stepped
    reckoner, a refined version of the Pascaline. Designed by Blaise Pascal in 1642
    (at age 19!), the Pascaline was the world’s first mechanical calculator—it could
    add, subtract, multiply, or even divide digits.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在维多利亚时代的英国，阿达·洛夫莱斯（Lady Ada Lovelace）是查尔斯·巴贝奇（Charles Babbage）的朋友和合作者，他是*分析引擎*的发明者：第一个已知的通用机械计算机。尽管分析引擎具有远见和超前的时间，但在19世纪30年代和40年代设计时，分析引擎并不是作为一台通用计算机存在的，因为通用计算的概念尚未发明。它仅仅被设计为使用机械操作来自动执行来自数学分析领域的某些计算——因此被称为分析引擎。因此，它是早期尝试用齿轮形式编码数学运算的思想的继承者，例如帕斯卡林或莱布尼茨的阶梯计算器，帕斯卡林的改进版本。由布莱斯·帕斯卡（Blaise
    Pascal）于1642年设计（当时年仅19岁！），帕斯卡林是世界上第一台机械计算器——它可以进行加法、减法、乘法，甚至是除法。
- en: 'In 1843, Ada Lovelace remarked on the invention of the Analytical Engine:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 1843年，阿达·洛夫莱斯（Ada Lovelace）评论了分析引擎的发明：
- en: '*The Analytical Engine has no pretensions whatever to originate anything. It
    can do whatever we know how to order it to perform…. Its province is to assist
    us in making available what we’re already acquainted with.*'
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*分析引擎完全没有假装要创造任何东西。它只能执行我们知道如何指示它执行的任务……它的职责是帮助我们利用我们已经了解的东西。*'
- en: Even with 179 years of historical perspective, Lady Lovelace’s observation remains
    arresting. Could a general-purpose computer “originate” anything, or would it
    always be bound to dully execute processes we humans fully understand? Could it
    ever be capable of any original thought? Could it learn from experience? Could
    it show creativity?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 即使有着179年的历史背景，洛夫莱斯夫人的这一观察依然令人震惊。一个通用计算机能否“创造”出任何东西，还是它只会执行我们人类完全理解的过程？它能有任何原创思考吗？它能从经验中学习吗？它能展现创造力吗？
- en: Her remark was later quoted by AI pioneer Alan Turing as “Lady Lovelace’s objection”
    in his landmark 1950 paper “Computing Machinery and Intelligence,”^([1](#Rendnote1))
    which introduced the *Turing test* as well as key concepts that would come to
    shape AI.^([2](#Rendnote2)) Turing was of the opinion—highly provocative at the
    time—that computers could in principle be made to emulate all aspects of human
    intelligence.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 后来AI先驱艾伦·图灵在他1950年的里程碑论文“计算机器和智能”中引用了洛夫莱斯夫人的这一评论作为“洛夫莱斯夫人的反驳”，并介绍了图灵测试以及将会塑造AI的关键概念。图灵当时具有高度挑衅性的观点是，计算机原则上可以模拟人类智能的方方面面。
- en: 'The usual way to make a computer do useful work is to have a human programmer
    write down *rules*—a computer program—to be followed to turn input data into appropriate
    answers, just like Lady Lovelace writing down step-by-step instructions for the
    Analytical Engine to perform. Machine learning turns this around: the machine
    looks at the input data and the corresponding answers, and figures out what the
    rules should be (see [figure 1.2](#fig1-2)). A machine learning system is *trained*
    rather than explicitly programmed. It’s presented with many examples relevant
    to a task, and it finds statistical structure in these examples that eventually
    allows the system to come up with rules for automating the task. For instance,
    if you wished to automate the task of tagging your vacation pictures, you could
    present a machine learning system with many examples of pictures already tagged
    by humans, and the system would learn statistical rules for associating specific
    pictures to specific tags.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让计算机做有用的工作的通常方式是由人类程序员编写*规则*，即一份计算机程序，用于将输入数据转化为适当的答案，就像洛夫莱斯夫人编写分析机执行的步骤指令一样。而机器学习则颠倒了这种思路：机器查看输入数据和相应的答案，并找出规则应该是什么（见[图1.2](#fig1-2)）。机器学习系统是被*训练*而不是明确地进行编程。它被呈现给了任务相关的许多示例，并且在这些示例中找到了统计结构，最终允许系统制定自动化任务的规则。例如，如果你想自动化处理你的度假照片的标记任务，你可以向机器学习系统提供许多由人类已经标记过的照片示例，而系统会学习用于将特定照片与特定标签相关联的统计规则。
- en: '![Image](../images/f0004-01.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0004-01.jpg)'
- en: '**Figure 1.2 Machine learning: A new programming paradigm**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**图1.2 机器学习：一种新的编程范式**'
- en: Although machine learning started to flourish only in the 1990s, it has quickly
    become the most popular and most successful subfield of AI, a trend driven by
    the availability of faster hardware and larger datasets. Machine learning is related
    to mathematical statistics, but it differs from statistics in several important
    ways, in the same sense that medicine is related to chemistry but cannot be reduced
    to chemistry, because medicine deals with its own distinct systems with their
    own distinct properties. Unlike statistics, machine learning tends to deal with
    large, complex datasets (such as a dataset of millions of images, each consisting
    of tens of thousands of pixels) for which classical statistical analysis such
    as Bayesian analysis would be impractical. As a result, machine learning, and
    especially deep learning, exhibits comparatively little mathematical theory—maybe
    too little—and is fundamentally an engineering discipline. Unlike theoretical
    physics or mathematics, machine learning is a very hands-on field driven by empirical
    findings and deeply reliant on advances in software and hardware.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管机器学习在1990年代才开始蓬勃发展，但它迅速成为人工智能最受欢迎和最成功的子领域，这一趋势是由更快的硬件速度和更大的数据集所推动的。机器学习与数理统计有关，但在几个重要方面与统计学不同，就像医学与化学有关但不能完全归纳为化学一样，因为医学处理了具有自身特性的独立系统。与统计学不同，机器学习倾向于处理大型复杂数据集（例如由数百万个由数万个像素组成的图像组成的数据集），对于这些数据集，诸如贝叶斯分析之类的经典统计分析是不实际的。因此，机器学习，尤其是深度学习，在数学理论方面相对较少——也许太少了，并且从根本上是一门工程学科。与理论物理或数学不同，机器学习是一个非常实际的领域，它是由实证结果推动并深度依赖于软件和硬件的进步。
- en: 1.1.3 Learning rules and representations from data
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1.3 从数据中学习规则和表示
- en: 'To define *deep learning* and understand the difference between deep learning
    and other machine learning approaches, first we need some idea of what machine
    learning algorithms do. We just stated that machine learning discovers rules for
    executing a data processing task, given examples of what’s expected. So, to do
    machine learning, we need the following three things:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 要定义*深度学习*并理解深度学习与其他机器学习方法之间的区别，首先我们需要对机器学习算法做些了解。我们刚才说过，机器学习发现了执行数据处理任务的规则，给出了预期的示例。所以，要进行机器学习，我们需要以下三件事：
- en: '*Input data points*—For instance, if the task is speech recognition, these
    data points could be sound files of people speaking. If the task is image tagging,
    they could be pictures.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输入数据点*——例如，如果任务是语音识别，这些数据点可能是人们说话的声音文件。如果任务是图像标记，它们可以是图片。'
- en: '*Examples of the expected output*—In a speech-recognition task, these could
    be human-generated transcripts of sound files. In an image task, expected outputs
    could be tags such as “dog,” “cat,” and so on.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预期输出的示例*——在语音识别任务中，这些可以是由人类生成的声音文件的文本转录。在图像任务中，预期的输出可以是诸如“狗”、“猫”等标签。'
- en: '*A way to measure whether the algorithm is doing a good job*—This is necessary
    to determine the distance between the algorithm’s current output and its expected
    output. The measurement is used as a feedback signal to adjust the way the algorithm
    works. This adjustment step is what we call *learning*.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*评估算法工作是否良好的方法*——这是确定算法当前输出与预期输出之间距离的必要条件。该度量用作调整算法工作方式的反馈信号。这一调整步骤就是我们所谓的*学习*。'
- en: 'A machine learning model transforms its input data into meaningful outputs,
    a process that is “learned” from exposure to known examples of inputs and outputs.
    Therefore, the central problem in machine learning and deep learning is to *meaningfully
    transform data*: in other words, to learn useful *representations* of the input
    data at hand— representations that get us closer to the expected output.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型将其输入数据转换为有意义的输出，这个过程是从已知输入和输出的示例中“学习”的。因此，机器学习和深度学习的核心问题是*有意义地转换数据*：换句话说，学习有用的*表示*手头的输入数据——这些表示使我们更接近预期的输出。
- en: 'Before we go any further: what’s a representation? At its core, it’s a different
    way to look at data—to represent or encode data. For instance, a color image can
    be encoded in the RGB format (red-green-blue) or in the HSV format (hue-saturation-value):
    these are two different representations of the same data. Some tasks that may
    be difficult with one representation can become easy with another. For example,
    the task “select all red pixels in the image” is simpler in the RGB format, whereas
    “make the image less saturated” is simpler in the HSV format. Machine learning
    models are all about finding appropriate representations for their input data—transformations
    of the data that make it more amenable to the task at hand.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进一步之前：什么是表示？在其核心，它是查看数据的不同方式——表示或编码数据。例如，彩色图像可以用RGB格式（红-绿-蓝）或HSV格式（色调-饱和度-亮度）进行编码：这是同一数据的两种不同表示。对于某些使用一种表示可能很困难的任务，使用另一种表示可能变得更容易。例如，任务“选择图像中的所有红色像素”在RGB格式中更简单，而“使图像的饱和度降低”在HSV格式中更简单。机器学习模型的全部意义在于为其输入数据找到适当的表示——将数据转换为更适合手头任务的形式。
- en: Let’s make this concrete. Consider an *x*-axis, a *y*-axis, and some points
    represented by their coordinates in the (*x, y*) system, as shown in [figure 1.3](#fig1-3).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们具体一点。考虑一个*x*轴，一个*y*轴，以及一些通过它们在(*x, y*)系统中的坐标表示的点，如[图 1.3](#fig1-3)所示。
- en: '![Image](../images/f0005-01.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0005-01.jpg)'
- en: '**Figure 1.3 Some sample data**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 1.3 一些样本数据**'
- en: 'As you can see, we have a few white points and a few black points. Let’s say
    we want to develop an algorithm that can take the coordinates (*x, y*) of a point
    and output whether that point is likely to be black or white. In this case, we
    have the following data:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，我们有一些白点和一些黑点。假设我们想要开发一个算法，该算法可以接受点的坐标(*x, y*)并输出该点可能是黑色还是白色。在这种情况下，我们有以下数据：
- en: The inputs are the coordinates of our points.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入是我们点的坐标。
- en: The expected outputs are the colors of our points.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预期的输出是我们点的颜色。
- en: A way to measure whether our algorithm is doing a good job could be, for instance,
    the percentage of points that are being correctly classified.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估我们的算法是否良好的一种方法可以是，例如，被正确分类的点的百分比。
- en: What we need here is a new representation of our data that cleanly separates
    the white points from the black points. One transformation we could use, among
    many other possibilities, would be a coordinate change, illustrated in [figure
    1.4](#fig1-4).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要的是一种能清晰地将白点与黑点分开的新数据表示。我们可以使用的一种转换，除了许多其他可能性外，是一个坐标变换，如[图1.4](#fig1-4)所示。
- en: 'In this new coordinate system, the coordinates of our points can be said to
    be a new representation of our data. And it’s a good one! With this representation,
    the black/white classification problem can be expressed as a simple rule: “Black
    points are such that *x* > 0,” or “White points are such that *x* < 0.” This new
    representation, combined with this simple rule, neatly solves the classification
    problem.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个新的坐标系统中，我们点的坐标可以说是我们数据的新表示。而且这个表示是很好的！有了这个表示，黑/白分类问题可以表达为一个简单的规则：“黑点是满足*x*
    > 0的点”，或者“白点是满足*x* < 0的点”。这个新的表示，加上这个简单的规则，很好地解决了分类问题。
- en: '![Image](../images/f0006-01.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0006-01.jpg)'
- en: '**Figure 1.4 Coordinate change**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**图1.4 坐标变换**'
- en: 'In this case, we defined the coordinate change by hand: we used our human intelligence
    to come up with our own appropriate representation of the data. This is fine for
    such an extremely simple problem, but could you do the same if the task was to
    classify images of handwritten digits? Could you write down explicit, computer-executable
    image transformations that would illuminate the difference between a 6 and an
    8, between a 1 and a 7, across all kinds of different handwriting?'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们手动定义了坐标变换：我们利用我们的人类智慧来得出我们自己的适当数据表示。对于这样一个极其简单的问题来说，这是可以的，但如果任务是对手写数字的图像进行分类，你能做到同样的吗？你能否写出明确的、可由计算机执行的图像转换，以揭示6和8之间、1和7之间的差异，以及各种不同手写的区别？
- en: This is possible to an extent. Rules based on representations of digits, such
    as “number of closed loops” or vertical and horizontal pixel histograms, can do
    a decent job of telling apart handwritten digits. But finding such useful representations
    by hand is hard work, and, as you can imagine, the resulting rule-based system
    is brittle—a nightmare to maintain. Every time you come across a new example of
    handwriting that breaks your carefully thought-out rules, you will have to add
    new data transformations and new rules, while taking into account their interaction
    with every previous rule.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在一定程度上是可能的。基于数字表示的规则，比如“封闭环数量”或者垂直和水平像素直方图，可以很好地区分手写数字。但是手动找到这样有用的表示是很费力的工作，而且，你可以想象到，由此产生的基于规则的系统是脆弱的——维护起来是一场噩梦。每当你遇到一个打破你精心考虑的规则的新的手写样本时，你就不得不添加新的数据转换和新的规则，同时考虑它们与每个以前的规则的相互作用。
- en: You’re probably thinking, if this process is so painful, could we automate it?
    What if we tried systematically searching for different sets of automatically
    generated representations of the data and rules based on them, identifying good
    ones by using as feedback the percentage of digits being correctly classified
    in some development data-set? We would then be doing machine learning. *Learning*,
    in the context of machine learning, describes an automatic search process for
    data transformations that produce useful representations of some data, guided
    by some feedback signal—representations that are amenable to simpler rules solving
    the task at hand.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，如果这个过程如此痛苦，我们能不能自动化它呢？如果我们尝试系统地搜索不同的自动生成的数据表示及基于它们的规则集合，并通过使用一些开发数据集中正确分类的数字的百分比作为反馈来确定好的表示，那我们就是在做机器学习了。*学习*，在机器学习的背景下，描述的是一种自动搜索数据转换的过程，产生对一些数据有用的表示，受到某种反馈信号的指导——这些表示适合于解决手头任务的简单规则。
- en: These transformations can be coordinate changes (like in our 2-D coordinates
    classification example), or taking a histogram of pixels and counting loops (like
    in our digits classification example), but they could also be linear projections,
    translations, nonlinear operations (such as “select all points such that *x* >
    0”), and so on. Machine learning algorithms aren’t usually creative in finding
    these transformations; they’re merely searching through a predefined set of operations,
    called a *hypothesis space*. For instance, the space of all possible coordinate
    changes would be our hypothesis space in the 2-D coordinates classification example.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这些转换可以是坐标变换（如我们的2-D坐标分类示例中的转换），也可以是像素直方图和计数循环（如我们的数字分类示例中的转换），还可以是线性投影、平移、非线性操作（如“选择所有满足
    *x* > 0 的点”）等等。机器学习算法通常不会自行发现这些转换的创造性方式；它们只是在预定义的一组操作中进行搜索，称为*假设空间*。例如，在2-D坐标分类示例中，所有可能的坐标变换构成了我们的假设空间。
- en: 'So that’s what machine learning is, concisely: searching for useful representations
    and rules over some input data, within a predefined space of possibilities, using
    guidance from a feedback signal. This simple idea allows for solving a remarkably
    broad range of intellectual tasks, from speech recognition to autonomous driving.
    Now that you understand what we mean by *learning*, let’s take a look at what
    makes *deep learning* special.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是机器学习的精简定义：在一定的可能性空间内，通过接收反馈信号的指导，对一些输入数据搜索有用的表示和规则。这个简单的想法能够解决一系列智能任务，从语音识别到自动驾驶。现在您已经理解了我们所说的*学习*的含义，让我们来看看*深度学习*有何特殊之处。
- en: 1.1.4 The “deep” in “deep learning”
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1.4 “深度学习”中的“深度”
- en: 'Deep learning is a specific subfield of machine learning: a new take on learning
    representations from data that emphasizes learning successive layers of increasingly
    meaningful representations. The “deep” in “deep learning” isn’t a reference to
    any kind of deeper understanding achieved by the approach; rather, it stands for
    this idea of successive layers of representations. How many layers contribute
    to a model of the data is called the *depth* of the model. Other appropriate names
    for the field could have been *layered representations learning* or *hierarchical
    representations learning*. Modern deep learning often involves tens or even hundreds
    of successive layers of representations, and they’re all learned automatically
    from exposure to training data. Meanwhile, other approaches to machine learning
    tend to focus on learning only one or two layers of representations of the data
    (say, taking a pixel histogram and then applying a classification rule); hence,
    they’re sometimes called *shallow learning*.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是机器学习的一个特定子领域：一种从数据中学习表示的新方法，强调学习越来越有意义的连续层次的表示。所谓“深度”并不是指这种方法达到了任何更深的理解，而是指连续层级表示的思想。对数据模型有多少层贡献被称为模型的*深度*。该领域其他适当的名称可以是*分层表示学习*或*层次表示学习*。现代深度学习通常涉及到连续的几十甚至上百层的表示，它们都是从训练数据中自动学习得来的。与此同时，其他机器学习方法往往只专注于学习一两层的数据表示（例如，计算像素直方图然后应用分类规则）；因此，它们有时被称为*浅层学习*。
- en: In deep learning, these layered representations are learned via models called
    *neural networks*, structured in literal layers stacked on top of each other.
    The term “neural network” refers to neurobiology, but although some of the central
    concepts in deep learning were developed in part by drawing inspiration from our
    understanding of the brain (in particular, the visual cortex), deep learning models
    are not models of the brain. There’s no evidence that the brain implements anything
    like the learning mechanisms used in modern deep learning models. You may come
    across pop-science articles proclaiming that deep learning works like the brain
    or was modeled after the brain, but that isn’t the case. It would be confusing
    and counterproductive for newcomers to the field to think of deep learning as
    being in any way related to neurobiology; you don’t need that shroud of “just
    like our minds” mystique and mystery, and you may as well forget anything you
    may have read about hypothetical links between deep learning and biology. For
    our purposes, deep learning is a mathematical framework for learning representations
    from data.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，这些分层表示是通过称为*神经网络*的模型学习的，这些模型结构化地堆叠在一起。术语“神经网络”是指神经生物学，但尽管深度学习中的一些核心概念部分是通过从我们对大脑的理解中汲取灵感（特别是视觉皮层）而开发出来的，但深度学习模型并不是大脑的模型。没有证据表明大脑实现了任何类似于现代深度学习模型中使用的学习机制。你可能会遇到一些流行科学文章宣称深度学习就像大脑工作或者是根据大脑建模的，但事实并非如此。对于初学者来说，将深度学习与神经生物学有任何联系是令人困惑和适得其反的；你不需要对“就像我们的思维一样”这种神秘感和神秘感的掩饰，你也可以忘记任何关于深度学习和生物学之间的假设联系的东西。对于我们的目的来说，深度学习是从数据中学习表示的数学框架。
- en: What do the representations learned by a deep learning algorithm look like?
    Let’s examine how a network several layers deep (see [figure 1.5](#fig1-5)) transforms
    an image of a digit to recognize what digit it is.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '深度学习算法学到的表示是什么样子的？让我们看看一个多层次网络（参见[图1.5](#fig1-5)）如何将一个数字图像转换为识别它是什么数字的表示。  '
- en: As you can see in [figure 1.6](#fig1-6), the network transforms the digit image
    into representations that are increasingly different from the original image and
    increasingly informative about the final result. You can think of a deep network
    as a multistage *information-distillation* process, where information goes through
    successive filters and comes out increasingly *purified* (that is, useful with
    regard to some task).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在[图1.6](#fig1-6)中所看到的，该网络将数字图像转换为越来越不同于原始图像且越来越有关最终结果的表示。你可以把深度网络想象成一个多阶段的*信息精炼*过程，在这个过程中信息经过连续的筛选，并且逐渐*纯化*（即，关于某个任务而言有用）。
- en: '![Image](../images/f0008-01.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0008-01.jpg)'
- en: '**Figure 1.5 A deep neural network for digit classification**'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**图1.5 用于数字分类的深度神经网络**'
- en: '![Image](../images/f0008-02.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0008-02.jpg)'
- en: '**Figure 1.6 Data representations learned by a digit-classification model**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**图1.6 由数字分类模型学到的数据表示**'
- en: 'So that’s what deep learning is, technically: a multistage way to learn data
    representations. It’s a simple idea—but, as it turns out, very simple mechanisms,
    sufficiently scaled, can end up looking like magic.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 所以从技术上讲，深度学习是什么？是一种学习数据表示的多阶段方法。这是一个简单的想法——但事实证明，非常简单的机制，足够大规模地扩展，最终看起来就像魔术一样。
- en: 1.1.5 Understanding how deep learning works, in three figures
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1.5 用三个图来理解深度学习是如何工作的
- en: At this point, you know that machine learning is about mapping inputs (such
    as images) to targets (such as the label “cat”), which is done by observing many
    examples of input and targets. You also know that deep neural networks do this
    input-to-target mapping via a deep sequence of simple data transformations (layers)
    and that these data transformations are learned by exposure to examples. Now let’s
    look at how this learning happens, concretely.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经知道机器学习是关于将输入（比如图像）映射到目标（比如标签“猫”），这是通过观察大量输入和目标示例来完成的。你也知道深度神经网络通过一系列简单的数据转换（层）的深度序列来进行这种输入到目标的映射，并且这些数据转换是通过接触示例来学习的。现在让我们具体看看这个学习是如何发生的。
- en: 'The specification of what a layer does to its input data is stored in the layer’s
    *weights*, which in essence are a bunch of numbers. In technical terms, we’d say
    that the transformation implemented by a layer is *parameterized* by its weights
    (see [figure 1.7](#fig1-7)). (Weights are also sometimes called the *parameters*
    of a layer.) In this context, *learning* means finding a set of values for the
    weights of all layers in a network, such that the network will correctly map example
    inputs to their associated targets. But here’s the thing: a deep neural network
    can contain tens of millions of parameters. Finding the correct values for all
    of them may seem like a daunting task, especially given that modifying the value
    of one parameter will affect the behavior of all the others!'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一个层对其输入数据执行的操作规范存储在层的*权重*中，本质上是一堆数字。在技术术语中，我们会说层实现的转换由其权重*参数化*（参见[图 1.7](#fig1-7)）。
    （权重有时也被称为层的*参数*。）在这个上下文中，*学习*意味着找到网络中所有层的权重的一组值，使得网络能够正确地将示例输入映射到其关联的目标。但是有个问题：一个深度神经网络可能包含数千万个参数。找到所有这些参数的正确值可能看起来是一项艰巨的任务，特别是考虑到修改一个参数的值将影响所有其他参数的行为！
- en: '![Image](../images/f0009-01.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0009-01.jpg)'
- en: '**Figure 1.7 A neural network is parameterized by its weights.**'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 1.7 神经网络由其权重参数化。**'
- en: To control something, first you need to be able to observe it. To control the
    output of a neural network, you need to be able to measure how far this output
    is from what you expected. This is the job of the *loss function* of the network,
    also sometimes called the *objective function* or *cost function*. The loss function
    takes the predictions of the network and the true target (what you wanted the
    network to output) and computes a distance score, capturing how well the network
    has done on this specific example (see [figure 1.8](#fig1-8)).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 要控制某物，首先你需要能够观察它。要控制神经网络的输出，你需要能够测量这个输出与你期望的有多远。这是网络的*损失函数*的工作，有时也称为*目标函数*或*成本函数*。损失函数接受网络的预测和真实目标（你希望网络输出的内容）并计算一个距离分数，捕捉网络在这个特定示例上表现如何（参见[图
    1.8](#fig1-8)）。
- en: '![Image](../images/f0009-02.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0009-02.jpg)'
- en: '**Figure 1.8 A loss function measures the quality of the network’s output.**'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 1.8 损失函数衡量网络输出的质量。**'
- en: 'The fundamental trick in deep learning is to use this score as a feedback signal
    to adjust the value of the weights a little, in a direction that will lower the
    loss score for the current example (see [figure 1.9](#fig1-9)). This adjustment
    is the job of the *optimizer*, which implements what’s called the *backpropagation*
    algorithm: the central algorithm in deep learning. The next chapter explains in
    more detail how backpropagation works.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的基本技巧是将此分数用作反馈信号，稍微调整权重的值，使得当前示例的损失分数降低（参见[图 1.9](#fig1-9)）。这种调整是*优化器*的工作，它实现了所谓的*反向传播*算法：深度学习中的核心算法。下一章将更详细地解释反向传播的工作原理。
- en: '![Image](../images/f0010-01.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0010-01.jpg)'
- en: '**Figure 1.9 The loss score is used as a feedback signal to adjust the weights.**'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 1.9 损失分数被用作反馈信号来调整权重。**'
- en: 'Initially, the weights of the network are assigned random values, so the network
    merely implements a series of random transformations. Naturally, its output is
    far from what it should ideally be, and the loss score is accordingly very high.
    But with every example the network processes, the weights are adjusted a little
    in the correct direction, and the loss score decreases. This is the *training
    loop*, which, repeated a sufficient number of times (typically tens of iterations
    over thousands of examples), yields weight values that minimize the loss function.
    A network with a minimal loss is one for which the outputs are as close as they
    can be to the targets: a trained network. Once again, it’s a simple mechanism
    that, once scaled, ends up looking like magic.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，网络的权重被赋予随机值，因此网络只是实现一系列随机变换。自然地，它的输出远离理想状态，相应地损失分数也很高。但是随着网络处理每个示例，权重稍微朝正确的方向调整，损失分数减少。这是*训练循环*，重复足够多次（通常在数千个示例上进行数十次迭代），产生使损失函数最小化的权重值。具有最小损失的网络是输出尽可能接近目标的网络：一个经过训练的网络。再一次，这是一个简单的机制，一旦扩展，就会看起来像魔法。
- en: 1.1.6 What deep learning has achieved so far
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1.6 到目前为止深度学习取得了什么成就
- en: 'Although deep learning is a fairly old subfield of machine learning, it rose
    to prominence only in the early 2010s. In the few years since, it has achieved
    nothing short of a revolution in the field, producing remarkable results on perceptual
    tasks and even natural language processing tasks—problems involving skills that
    seem natural and intuitive to humans but have long been elusive for machines.
    In particular, deep learning has enabled the following breakthroughs, all in historically
    difficult areas of machine learning:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习是机器学习的一个相当古老的子领域，但它直到2010年代初才声名鹊起。在此后的几年里，它在领域中取得了革命性的成就，在感知任务甚至自然语言处理任务上取得了显著成果——这些问题涉及到对人类来说似乎自然而直观但长期以来一直难以实现的技能。特别是，深度学习使以下历史上困难的机器学习领域取得了突破：
- en: Near-human-level image classification
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接近人类水平的图像分类
- en: Near-human-level speech transcription
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接近人类水平的语音转录
- en: Near-human-level handwriting transcription
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接近人类水平的手写转录
- en: Dramatically improved machine translation
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显著改进的机器翻译
- en: Dramatically improved text-to-speech conversion
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显著改进的文本转语音转换
- en: Digital assistants such as Google Assistant and Amazon Alexa
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数字助手，如谷歌助手和亚马逊Alexa
- en: Near-human-level autonomous driving
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接近人类水平的自动驾驶
- en: Improved ad targeting, as used by Google, Baidu, or Bing
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改进的广告定位，如谷歌、百度或必应所使用的
- en: Improved search results on the web
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改进的网络搜索结果
- en: Ability to answer natural language questions
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回答自然语言问题的能力
- en: Superhuman Go playing
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超人类的围棋对弈
- en: We’re still exploring the full extent of what deep learning can do. We’ve started
    applying it with great success to a wide variety of problems that were thought
    to be impossible to solve just a few years ago—automatically transcribing the
    tens of thousands of ancient manuscripts held in the Vatican’s Apostolic Archive,
    detecting and classifying plant diseases in fields using a simple smartphone,
    assisting oncologists or radiologists with interpreting medical imaging data,
    predicting natural disasters such as floods, hurricanes, or even earthquakes,
    and so on. With every milestone, we’re getting closer to an age where deep learning
    assists us in every activity and every field of human endeavor—science, medicine,
    manufacturing, energy, transportation, software development, agriculture, and
    even artistic creation.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍在探索深度学习的全部潜力。我们已经开始将其成功应用于许多曾被认为几年前无法解决的各种问题——自动转录梵蒂冈宗座档案馆中保存的数万份古老手稿，使用简单的智能手机在田间检测和分类植物疾病，协助肿瘤学家或放射科医生解释医学成像数据，预测洪水、飓风甚至地震等自然灾害，等等。随着每一个里程碑的实现，我们越来越接近一个深度学习在人类努力的每个活动和领域中都能协助我们的时代——科学、医学、制造业、能源、交通运输、软件开发、农业，甚至艺术创作。
- en: 1.1.7 Don’t believe the short-term hype
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1.7 不要相信短期炒作
- en: Although deep learning has led to remarkable achievements in recent years, expectations
    for what the field will be able to achieve in the next decade tend to run much
    higher than what will likely be possible. Although some world-changing applications,
    like autonomous cars, are already within reach, many more are likely to remain
    elusive for a long time, such as believable dialogue systems, human-level machine
    translation across arbitrary languages, and human-level natural language understanding.
    In particular, talk of human-level general intelligence shouldn’t be taken too
    seriously. The risk with high expectations for the short term is that, as technology
    fails to deliver, research investment will dry up, slowing progress for a long
    time.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习在近年来取得了显著成就，但对该领域未来十年能够取得的成就的期望往往高于可能实现的水平。尽管一些像自动驾驶汽车这样的改变世界的应用已经可以实现，但更多的应用可能会在很长一段时间内仍然难以实现，比如可信的对话系统，跨任意语言的人类级机器翻译以及人类级自然语言理解。特别是，对于短期内达到人类级通用智能的讨论不应该太认真。对短期内的高期望的风险在于，随着技术的失灵，研究投资将枯竭，长时间内的进展将放缓。
- en: 'This has happened before. Twice in the past, AI went through a cycle of intense
    optimism followed by disappointment and skepticism, with a dearth of funding as
    a result. It started with symbolic AI in the 1960s. In those early days, projections
    about AI were flying high. One of the best-known pioneers and proponents of the
    symbolic AI approach was Marvin Minsky, who claimed in 1967, “Within a generation…
    the problem of creating ‘artificial intelligence’ will substantially be solved.”
    Three years later, in 1970, he made a more precisely quantified prediction: “In
    from three to eight years we will have a machine with the general intelligence
    of an average human being.” In 2022, such an achievement still appears to be far
    in the future—so far that we have no way to predict how long it will take—but
    in the 1960s and early 1970s, several experts believed it to be right around the
    corner (as do many people today). A few years later, as these high expectations
    failed to materialize, researchers and government funds turned away from the field,
    marking the start of the first *AI winter* (a reference to a nuclear winter, because
    this was shortly after the height of the Cold War).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这已经发生过。在过去两次，AI经历了密集的乐观期，随之而来的是失望和怀疑，资金匮乏也是其结果。它始于20世纪60年代的符号AI。在那些早期，关于AI的预测风靡一时。符号AI方法中最知名的先驱和支持者之一是马文·明斯基，他在1967年声称，“一代人内…创造‘人工智能’的问题将被很大程度上解决。”三年后，即1970年，他对此作了更为明确的预测：“在三到八年内，我们将拥有一台具有平均人类智能的机器。”在2022年，这样的成就似乎远在未来—而且我们无法预测需要多长时间—但在20世纪60年代和70年代初，一些专家相信这个成就就在不远的将来（今天也有很多人持相同看法）。几年后，随着这些极高的期望未能实现，研究人员和政府资金开始远离这一领域，标志着第一个*AI寒冬*的开始（这是对核寒冬的一种参考，因为这是冷战达到高潮后不久）。
- en: It wouldn’t be the last one. In the 1980s, a new take on symbolic AI, *expert
    systems*, started gathering steam among large companies. A few initial success
    stories triggered a wave of investment, with corporations around the world starting
    their own in-house AI departments to develop expert systems. Around 1985, companies
    were spending over $1 billion each year on the technology; but by the early 1990s,
    these systems had proven expensive to maintain, difficult to scale, and limited
    in scope, and interest died down. Thus began the second AI winter.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这不会是最后一个。在20世纪80年代，对符号AI的新看法，*专家系统*，开始在大公司中蓬勃发展。一些最初的成功案例引发了一波投资热潮，全球各大公司开始成立自己的内部AI部门开发专家系统。大约在1985年，公司每年在这项技术上的支出超过10亿美元；但到了20世纪90年代初，这些系统已被证明难以维护、难以扩展并且范围有限，导致兴趣消减。于是第二次AI寒冬开始了。
- en: We may be currently witnessing the third cycle of AI hype and disappointment,
    and we’re still in the phase of intense optimism. It’s best to moderate our expectations
    for the short term and make sure people less familiar with the technical side
    of the field have a clear idea of what deep learning can and can’t deliver.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能目前正见证着AI炒作和失望的第三个周期，而我们仍处于密集乐观的阶段。最好是在短期内控制我们的期望，并确保对这个领域技术方面不太熟悉的人清楚地知道深度学习能做什么，以及不能做什么。
- en: 1.1.8 The promise of AI
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1.8 AI的承诺
- en: Although we may have unrealistic short-term expectations for AI, the long-term
    picture is looking bright. We’re only getting started in applying deep learning
    to many important problems for which it could prove transformative, from medical
    diagnoses to digital assistants. AI research has been moving forward amazingly
    quickly in the past 10 years, in large part due to a level of funding never before
    seen in the short history of AI, but so far relatively little of this progress
    has made its way into the products and processes that form our world. Most of
    the research findings of deep learning aren’t yet applied, or at least are not
    applied to the full range of problems they could solve across all industries.
    Your doctor doesn’t yet use AI, and neither does your accountant. You probably
    don’t use AI technologies very often in your day-to-day life. Of course, you can
    ask your smartphone simple questions and get reasonable answers, you can get fairly
    useful product recommendations on Amazon.com, and you can search for “birthday”
    on Google Photos and instantly find those pictures of your daughter’s birthday
    party from last month. That’s a far cry from where such technologies used to stand.
    But such tools are still only accessories to our daily lives. AI has yet to transition
    to being central to the way we work, think, and live.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们可能对人工智能抱有不切实际的短期期望，但长期前景看起来光明。我们只是刚刚开始将深度学习应用于许多重要问题，它可能会产生变革性的影响，从医学诊断到数字助理。过去10年来，人工智能研究取得了惊人的进展，这在很大程度上是由于人工智能短暂历史上前所未有的资金水平，但到目前为止，这种进展相对较少地融入到构成我们世界的产品和流程中。深度学习的大部分研究成果尚未应用，或者至少尚未应用于所有行业中它们可以解决的全部问题。你的医生尚未使用人工智能，你的会计师也没有。你在日常生活中可能很少使用人工智能技术。当然，你可以向你的智能手机提出简单的问题，并得到合理的答案，你可以在Amazon.com上获得相当有用的产品推荐，你可以在Google照片上搜索“生日”，并立即找到上个月你女儿生日聚会的照片。这与这些技术过去的水平相距甚远。但这样的工具仍然只是我们日常生活的附件。人工智能尚未过渡到成为我们工作、思考和生活方式的核心。
- en: 'Right now, it may seem hard to believe that AI could have a large impact on
    our world, because it isn’t yet widely deployed—much as, back in 1995, it would
    have been difficult to believe in the future impact of the internet. Back then,
    most people didn’t see how the internet was relevant to them and how it was going
    to change their lives. The same is true for deep learning and AI today. But make
    no mistake: AI is coming. In a not-so-distant future, AI will be your assistant,
    even your friend; it will answer your questions, help educate your kids, and watch
    over your health. It will deliver your groceries to your door and drive you from
    point A to point B. It will be your interface to an increasingly complex and information-intensive
    world. And, even more important, AI will help humanity as a whole move forward,
    by assisting human scientists in new breakthrough discoveries across all scientific
    fields, from genomics to mathematics.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，人工智能可能会对我们的世界产生重大影响似乎难以置信，因为它尚未被广泛应用——就像在1995年，很难相信互联网未来的影响一样。那时，大多数人看不到互联网与他们有何关系，也看不到它将如何改变他们的生活。对于深度学习和人工智能今天也是如此。但不要误解：人工智能即将到来。在不久的将来，人工智能将成为你的助手，甚至是你的朋友；它将回答你的问题，帮助教育你的孩子，并监督你的健康。它将把你的杂货送到家门口，并把你从A点开到B点。它将成为你接入日益复杂和信息密集的世界的接口。更重要的是，人工智能将帮助整个人类向前迈进，通过协助人类科学家在各个科学领域取得新的突破性发现，从基因组学到数学。
- en: On the way, we may face a few setbacks and maybe even a new AI winter—in much
    the same way the internet industry was overhyped in 1998–1999 and suffered from
    a crash that dried up investment throughout the early 2000s. But we’ll get there
    eventually. AI will end up being applied to nearly every process that makes up
    our society and our daily lives, much like the internet is today.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，我们可能会遇到一些挫折，甚至可能出现新的人工智能寒冬——就像互联网行业在1998年至1999年被过度炒作，并遭受了在2000年初导致投资枯竭的崩溃一样。但我们最终会成功。人工智能最终将被应用到几乎构成我们社会和日常生活的每一个过程中，就像互联网今天一样。
- en: Don’t believe the short-term hype, but do believe in the long-term vision. It
    may take a while for AI to be deployed to its true potential—a potential the full
    extent of which no one has yet dared to dream—but AI is coming, and it will transform
    our world in a fantastic way.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 不要相信短期的炒作，但要相信长期的愿景。人工智能可能需要一段时间才能发挥其真正的潜力——一个尚未有人敢于梦想其全部潜力的潜力——但人工智能即将到来，它将以一种奇妙的方式改变我们的世界。
- en: '1.2 Before deep learning: A brief history of machine learning'
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 在深度学习之前：机器学习的简要历史
- en: Deep learning has reached a level of public attention and industry investment
    never before seen in the history of AI, but it isn’t the first successful form
    of machine learning. It’s safe to say that most of the machine learning algorithms
    used in the industry today aren’t deep learning algorithms. Deep learning isn’t
    always the right tool for the job—sometimes there isn’t enough data for deep learning
    to be applicable, and sometimes the problem is better solved by a different algorithm.
    If deep learning is your first contact with machine learning, you may find yourself
    in a situation where all you have is the deep learning hammer, and every machine
    learning problem starts to look like a nail. The only way not to fall into this
    trap is to be familiar with other approaches and practice them when appropriate.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习已经达到了公众关注和工业投资的水平，这在人工智能历史上从未有过，但它并不是机器学习的第一种成功形式。可以说，今天工业界使用的大多数机器学习算法都不是深度学习算法。深度学习并不总是解决问题的正确工具——有时候没有足够的数据来应用深度学习，有时候用不同的算法解决问题更好。如果深度学习是你第一次接触机器学习，你可能会发现自己处于这样一种情况：你手头只有深度学习的“锤子”，每个机器学习问题开始看起来都像是一根钉子。避免落入这种陷阱的唯一方法是熟悉其他方法，并在适当时练习它们。
- en: A detailed discussion of classical machine learning approaches is outside of
    the scope of this book, but I’ll briefly go over them and describe the historical
    context in which they were developed. This will allow us to place deep learning
    in the broader context of machine learning and better understand where deep learning
    comes from and why it matters.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 对古典机器学习方法的详细讨论超出了本书的范围，但我将简要介绍它们，并描述它们被发展的历史背景。这将使我们能够将深度学习置于机器学习的更广泛背景中，并更好地理解深度学习的起源和意义。
- en: 1.2.1 Probabilistic modeling
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.1 概率建模
- en: '*Probabilistic modeling* is the application of the principles of statistics
    to data analysis. It is one of the earliest forms of machine learning, and it’s
    still widely used to this day. One of the best-known algorithms in this category
    is the naive Bayes algorithm.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '*概率建模*是将统计学原理应用于数据分析的过程。这是最早的机器学习形式之一，至今仍然被广泛使用。这个类别中最著名的算法之一是朴素贝叶斯算法。'
- en: Naive Bayes is a type of machine learning classifier based on applying Bayes’
    theorem while assuming that the features in the input data are all independent
    (a strong, or “naive” assumption, which is where the name comes from). This form
    of data analysis predates computers and was applied by hand decades before its
    first computer implementation (most likely dating back to the 1950s). Bayes’ theorem
    and the foundations of statistics date back to the 18th century, and these are
    all you need to start using naive Bayes classifiers.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯是一种基于贝叶斯定理的机器学习分类器，假设输入数据中的特征都是独立的（这是一个强大或“朴素”的假设，这也是名称的由来）。这种形式的数据分析早在计算机出现之前就存在了，几十年前就是通过手工应用（很可能可以追溯到20世纪50年代）。贝叶斯定理和统计学的基础可以追溯到18世纪，这些是你开始使用朴素贝叶斯分类器所需要的全部。
- en: A closely related model is *logistic regression* (logreg for short), which is
    sometimes considered to be the “Hello World” of modern machine learning. Don’t
    be misled by its name—logreg is a classification algorithm rather than a regression
    algorithm. Much like naive Bayes, logreg predates computing by a long time, yet
    it’s still useful to this day, thanks to its simple and versatile nature. It’s
    often the first thing a data scientist will try on a dataset to get a feel for
    the classification task at hand.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 一个密切相关的模型是*逻辑回归*（简称logreg），有时被认为是现代机器学习的“Hello World”。不要被它的名字所误导——logreg是一个分类算法，而不是回归算法。与朴素贝叶斯类似，logreg在计算机出现很久之前就存在了，但由于其简单且多才多艺的性质，直到今天仍然很有用。它通常是数据科学家在处理数据集时尝试的第一件事，以了解手头的分类任务的感觉。
- en: 1.2.2 Early neural networks
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.2 早期神经网络
- en: Early iterations of neural networks have been completely supplanted by the modern
    variants covered in these pages, but it’s helpful to be aware of how deep learning
    originated. Although the core ideas of neural networks were investigated in toy
    forms as early as the 1950s, the approach took decades to get started. For a long
    time, the missing piece was an efficient way to train large neural networks. This
    changed in the mid-1980s, when multiple people independently rediscovered the
    backpropagation algorithm—a way to train chains of parametric operations using
    gradient-descent optimization (we’ll precisely define these concepts later in
    the book)—and started applying it to neural networks.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的神经网络已经被这些页面中涵盖的现代变体完全取代，但了解深度学习的起源仍然很有帮助。虽然神经网络的核心理念早在 20 世纪 50 年代就以玩具形式进行了研究，但这种方法花了几十年才开始起步。长时间以来，缺失的部分是训练大型神经网络的有效方法。这种情况在
    20 世纪 80 年代中期发生了变化，当时有多人独立重新发现了反向传播算法——一种使用梯度下降优化来训练参数化操作链的方法（我们将在本书后面精确定义这些概念），并开始将其应用于神经网络。
- en: The first successful practical application of neural networks came in 1989 from
    Bell Labs, when Yann LeCun combined the earlier ideas of convolutional neural
    networks and backpropagation and applied them to the problem of classifying handwritten
    digits. The resulting network, dubbed *LeNet*, was used by the United States Postal
    Service in the 1990s to automate the reading of ZIP codes on mail envelopes.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 1989 年，贝尔实验室的 Yann LeCun 将卷积神经网络和反向传播的早期理念结合起来，并将它们应用于手写数字分类问题，从而取得了神经网络的第一个成功的实际应用。得到的网络被称为
    *LeNet*，它在 1990 年代被美国邮政服务用于自动读取邮寄信封上的邮政编码。
- en: 1.2.3 Kernel methods
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**1.2.3 核方法**'
- en: 'As neural networks started to gain some respect among researchers in the 1990s,
    thanks to this first success, a new approach to machine learning rose to fame
    and quickly sent neural networks back to oblivion: kernel methods. *Kernel methods*
    are a group of classification algorithms, the best known of which is the *Support
    Vector Machine* (SVM). The modern formulation of an SVM was developed by Vladimir
    Vapnik and Corinna Cortes in the early 1990s at Bell Labs and published in 1995,^([3](#Rendnote3))
    although an older linear formulation was published by Vapnik and Alexey Chervonenkis
    as early as 1963.^([4](#Rendnote4))'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 随着神经网络在 1990 年代开始在研究人员中获得一些尊重，得益于这一第一个成功，一种新的机器学习方法崭露头角，并迅速将神经网络送回了遗忘：核方法。*核方法*是一组分类算法，其中最著名的是
    *支持向量机*（SVM）。SVM 的现代形式是由贝尔实验室的 Vladimir Vapnik 和 Corinna Cortes 在 20 世纪 90 年代初开发的，并于
    1995 年发表^([3](#Rendnote3))，尽管早在 1963 年，Vapnik 和 Alexey Chervonenkis 就已经发表了一个较早的线性形式^([4](#Rendnote4))。
- en: 'SVM is a classification algorithm that works by finding “decision boundaries”
    separating two classes (see [figure 1.10](#fig1-10)). SVMs proceed to find these
    boundaries in the following two steps:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: SVM 是一种分类算法，它通过找到分隔两个类别的“决策边界”（参见[图 1.10](#fig1-10)）来工作。SVM 在以下两个步骤中找到这些边界：
- en: '**1** The data is mapped to a new high-dimensional representation where the
    decision boundary can be expressed as a hyperplane (if the data was two-dimensional,
    as in [figure 1.10](#fig1-10), a hyperplane would be a straight line).'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**1** 数据被映射到一个新的高维表示，其中决策边界可以表示为一个超平面（如果数据是二维的，如[图 1.10](#fig1-10)，超平面将是一条直线）。'
- en: '**2** A good decision boundary (a separation hyperplane) is computed by trying
    to maximize the distance between the hyper-plane and the closest data points from
    each class, a step called *maximizing the margin*. This allows the boundary to
    generalize well to new samples outside of the training dataset.'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**2** 通过试图最大化超平面与每个类别最近数据点之间的距离来计算一个良好的决策边界（一个分隔超平面），这一步骤被称为 *最大化间隔*。这使得边界能够很好地泛化到训练数据集之外的新样本。'
- en: '![Image](../images/f0014-01.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0014-01.jpg)'
- en: '**Figure 1.10 A decision boundary**'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 1.10 决策边界**'
- en: 'The technique of mapping data to a high-dimensional representation where a
    classification problem becomes simpler may look good on paper, but in practice,
    it’s often computationally intractable. That’s where the *kernel trick* comes
    in (the key idea that kernel methods are named after). Here’s the gist of it:
    to find good decision hyperplanes in the new representation space, you don’t have
    to explicitly compute the coordinates of your points in the new space; you just
    need to compute the distance between pairs of points in that space, which can
    be done efficiently using a kernel function. A *kernel function* is a computationally
    tractable operation that maps any two points in your initial space to the distance
    between these points in your target representation space, completely bypassing
    the explicit computation of the new representation. Kernel functions are typically
    crafted by hand rather than learned from data—in the case of an SVM, only the
    separation hyperplane is learned.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据映射到高维表示中，以使分类问题变得更简单的技术在纸上看起来很好，但在实践中通常是计算上难以处理的。这就是*核技巧*的作用（核方法的关键思想）。它的要点是：为了在新的表示空间中找到好的决策超平面，你不需要显式计算点在新空间中的坐标；你只需要计算该空间中一对点之间的距离，这可以通过使用核函数以高效的方式完成。*核函数*是一种计算可行的操作，将你的初始空间中的任何两个点映射到这些点在目标表示空间中的距离，完全绕过新表示的显式计算。核函数通常是手工制作而非从数据中学习的，在SVM的情况下，只需要学习分离超平面。
- en: At the time they were developed, SVMs exhibited state-of-the-art performance
    on simple classification problems and were one of the few machine learning methods
    backed by extensive theory and amenable to serious mathematical analysis, making
    them well understood and easily interpretable. Because of these useful properties,
    SVMs became extremely popular in the field for a long time.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 当它们被开发出来时，SVM在简单分类问题上表现出了最先进的性能，并且是少数几种机器学习方法之一，支持广泛的理论并易于进行深入的数学分析，使它们受到长期以来极大的关注。
- en: But SVMs proved hard to scale to large datasets and didn’t provide good results
    for perceptual problems such as image classification. Because an SVM is a shallow
    method, applying an SVM to perceptual problems requires first extracting useful
    representations manually (a step called *feature engineering*), which is difficult
    and brittle. For instance, if you want to use an SVM to classify handwritten digits,
    you can’t start from the raw pixels; you should first find by hand useful representations
    that make the problem more tractable, like the pixel histograms mentioned earlier.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，SVM难以扩展到大型数据集，并且在诸如图像分类之类的感知问题上没有提供良好的结果。因为SVM是一种浅层方法，将SVM应用于感知问题需要先手动提取有用的表示（称为*特征工程*步骤），这很困难且易于出错。例如，如果要使用SVM对手写数字进行分类，则不能从原始像素开始；你应该首先手动查找有用的表示，使问题更易于处理，例如前面提到的像素直方图。
- en: 1.2.4 Decision trees, random forests, and gradient-boosting machines
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.4 决策树、随机森林和梯度提升机。
- en: '*Decision trees* are flowchart-like structures that let you classify input
    data points or predict output values given inputs (see [figure 1.11](#fig1-11)).
    They’re easy to visualize and interpret. Decision trees learned from data began
    to receive significant research interest in the 2000s, and by 2010, they were
    often preferred to kernel methods.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '*决策树*是类似流程图的结构，可以让你分类输入的数据点或者根据输入预测输出值（见[图1.11](#fig1-11)）。它们易于可视化和解释。从数据中学习的决策树在2000年代开始受到重视，并且到2010年，它们通常被更喜欢使用于核方法。'
- en: '![Image](../images/f0015-01.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0015-01.jpg)'
- en: '**Figure 1.11 A decision tree: The parameters that are learned are the questions
    about the data. A question could be, for instance, “Is coefficient 2 in the data
    greater than 3.5?”**'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**图1.11 决策树：学习的参数是关于数据的问题。一个问题可以是，“数据中的系数2是否大于3.5？”**'
- en: In particular, the *random forest* algorithm introduced a robust, practical
    take on decision tree learning that involves building a large number of specialized
    decision trees and then ensembling their outputs. Random forests are applicable
    to a wide range of problems—you could say that they’re almost always the second-best
    algorithm for any shallow machine learning task. When the popular machine learning
    competition website Kaggle ([http://kaggle.com](http://www.kaggle.com)) got started
    in 2010, random forests quickly became a favorite on the platform—until 2014,
    when *gradient-boosting machines* took over. A gradient-boosting machine, much
    like a random forest, is a machine learning technique based on ensembling weak
    prediction models, generally decision trees. It uses *gradient boosting*, a way
    to improve any machine learning model by iteratively training new models that
    specialize in addressing the weak points of the previous models. Applied to decision
    trees, the use of the gradient-boosting technique results in models that strictly
    outperform random forests most of the time, while having similar properties. It
    may be one of the best, if not *the* best, algorithm for dealing with nonperceptual
    data today. Alongside deep learning, it’s one of the most commonly used techniques
    in Kaggle competitions.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，*随机森林*算法引入了一种稳健的、实用的决策树学习方法，涉及构建大量专门的决策树，然后对它们的输出进行集成。随机森林适用于各种问题——你可以说它们几乎总是浅层机器学习任务的第二好算法。当流行的机器学习竞赛网站Kaggle（[http://kaggle.com](http://www.kaggle.com)）在2010年启动时，随机森林很快成为该平台上的宠儿——直到2014年，*梯度提升机*取代了它。梯度提升机与随机森林类似，是一种基于弱预测模型集成的机器学习技术，通常是决策树。它使用*梯度提升*，一种通过迭代训练新模型来改善任何机器学习模型的方法，这些新模型专门用于解决前一模型的弱点。应用于决策树时，梯度提升技术的使用导致模型大部分时间都严格优于随机森林，同时具有类似的性质。它可能是今天处理非感知数据的最佳算法之一，如果不是*最佳*。与深度学习一样，它是Kaggle竞赛中最常用的技术之一。
- en: 1.2.5 Back to neural networks
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.5 回到神经网络
- en: 'Around 2010, although neural networks were almost completely shunned by the
    scientific community at large, a number of people still working on neural networks
    started to make important breakthroughs: the groups of Geoffrey Hinton at the
    University of Toronto, Yoshua Bengio at the University of Montreal, Yann LeCun
    at New York University, and IDSIA in Switzerland.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 大约在2010年左右，尽管神经网络几乎被整个科学界抛弃，但仍有一些人在继续研究神经网络，并取得了重要的突破：多伦多大学的Geoffrey Hinton小组、蒙特利尔大学的Yoshua
    Bengio小组、纽约大学的Yann LeCun小组以及瑞士的IDSIA。
- en: In 2011, Dan Cireşan from IDSIA began to win academic image-classification competitions
    with GPU-trained deep neural networks—the first practical success of modern deep
    learning. But the watershed moment came in 2012, with the entry of Hinton’s group
    in the yearly large-scale image-classification challenge, ImageNet (ImageNet Large
    Scale Visual Recognition Challenge, or ILSVRC for short). The ImageNet challenge
    was notoriously difficult at the time, consisting of classifying high-resolution
    color images into 1,000 different categories after training on 1.4 million images.
    In 2011, the top-five accuracy of the winning model, based on classical approaches
    to computer vision, was only 74.3%.^([5](#Rendnote5)) Then, in 2012, a team led
    by Alex Krizhevsky and advised by Geoffrey Hinton was able to achieve a top-five
    accuracy of 83.6%—a significant breakthrough. The competition has been dominated
    by deep convolutional neural networks every year since. By 2015, the winner reached
    an accuracy of 96.4%, and the classification task on ImageNet was considered to
    be a completely solved problem.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 2011年，来自IDSIA的Dan Cireşan开始用GPU训练的深度神经网络赢得学术图像分类比赛，这是现代深度学习的第一个实际成功案例。但真正的转折点是在2012年，Hinton的团队参加了每年一次的大规模图像分类挑战赛ImageNet（简称ImageNet
    Large Scale Visual Recognition Challenge，或简称ILSVRC）。当时，ImageNet挑战赛以其难度大而臭名昭著，需要在训练了140万张图像后，将高分辨率彩色图像分类为1000个不同的类别。2011年，基于经典计算机视觉方法的获胜模型的前五准确率仅为74.3%。^([5](#Rendnote5))
    然后，在2012年，由Alex Krizhevsky领导的团队，在Geoffrey Hinton的指导下，取得了83.6%的前五准确率，这是一个重大突破。从那时起，每年的比赛都被深度卷积神经网络所主导。到2015年，获胜者的准确率达到了96.4%，ImageNet上的分类任务被认为是一个完全解决的问题。
- en: Since 2012, deep convolutional neural networks (*convnets*) have become the
    go-to algorithm for all computer vision tasks; more generally, they work on all
    perceptual tasks. At any major computer vision conference after 2015, it was nearly
    impossible to find presentations that didn’t involve convnets in some form. At
    the same time, deep learning has also found applications in many other types of
    problems, such as natural language processing. It has completely replaced SVMs
    and decision trees in a wide range of applications. For instance, for several
    years, the European Organization for Nuclear Research, CERN, used decision tree–based
    methods for analyzing particle data from the ATLAS detector at the Large Hadron
    Collider (LHC), but CERN eventually switched to Keras-based deep neural networks
    due to their higher performance and ease of training on large datasets.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 自2012年以来，深度卷积神经网络（*卷积神经网络*）已成为所有计算机视觉任务的首选算法；更一般地说，它们适用于所有感知任务。在2015年之后的任何重要计算机视觉会议上，几乎不可能找到不涉及卷积神经网络的演示。与此同时，深度学习在许多其他类型的问题中也找到了应用，比如自然语言处理。它已完全取代了在广泛范围内的应用中的支持向量机和决策树。例如，几年来，欧洲核子研究组织（CERN）一直使用基于决策树的方法来分析大型强子对撞机（LHC）上的ATLAS探测器的粒子数据，但最终CERN转而使用基于Keras的深度神经网络，因为它们在性能和大型数据集的训练方面更为出色。
- en: 1.2.6 What makes deep learning different?
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.6 深度学习的特点在哪里？
- en: 'The primary reason deep learning took off so quickly is that it offered better
    performance for many problems. But that’s not the only reason. Deep learning also
    makes problem-solving much easier, because it completely automates what used to
    be the most crucial step in a machine learning workflow: feature engineering.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习迅速发展的主要原因是它在许多问题上提供了更好的性能。但这并不是唯一的原因。深度学习还使问题解决变得更容易，因为它完全自动化了机器学习工作流程中曾经最关键的步骤：特征工程。
- en: 'Previous machine learning techniques—shallow learning—involved transforming
    the input data into only one or two successive representation spaces, usually
    via simple transformations such as high-dimensional nonlinear projections (SVMs)
    or decision trees. But the refined representations required by complex problems
    generally can’t be attained by such techniques. As such, humans had to go to great
    lengths to make the initial input data more amenable to processing by these methods:
    they had to manually engineer good layers of representations for their data. This
    is called *feature engineering*. Deep learning, on the other hand, completely
    automates this step: with deep learning, you learn all features in one pass rather
    than having to engineer them yourself. This has greatly simplified machine learning
    workflows, often replacing sophisticated multistage pipelines with a single, simple,
    end-to-end deep learning model.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 以前的机器学习技术——浅层学习——涉及将输入数据转换成仅有一个或两个连续的表示空间，通常是通过简单的转换，比如高维非线性投影（支持向量机）或决策树。但是，复杂问题所需的精细表示通常无法通过这些技术实现。因此，人们不得不极力使初始输入数据更易于这些方法处理：他们不得不手工构建数据的良好表示层。这就是所谓的*特征工程*。另一方面，深度学习完全自动化了这一步骤：通过深度学习，你可以一次学习所有特征，而不是自己进行特征工程。这大大简化了机器学习工作流程，通常用单个简单的端到端深度学习模型替代了复杂的多阶段管道。
- en: 'You may ask, if the crux of the issue is to have multiple successive layers
    of representations, could shallow methods be applied repeatedly to emulate the
    effects of deep learning? In practice, successive applications of shallow-learning
    methods produce fast-diminishing returns, because the optimal first representation
    layer in a three-layer model isn’t the optimal first layer in a one-layer or two-layer
    model. What is transformative about deep learning is that it allows a model to
    learn all layers of representation *jointly*, at the same time, rather than in
    succession (*greedily*, as it’s called). With joint feature learning, whenever
    the model adjusts one of its internal features, all other features that depend
    on it automatically adapt to the change, without requiring human intervention.
    Everything is supervised by a single feedback signal: every change in the model
    serves the end goal. This is much more powerful than greedily stacking shallow
    models, because it allows for complex, abstract representations to be learned
    by breaking them down into long series of intermediate spaces (layers); each space
    is only a simple transformation away from the previous one.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问，如果问题的关键是拥有多个连续的表示层，那么浅层方法能否被重复应用以模拟深度学习的效果？实际上，浅层学习方法的连续应用会产生快速减小的回报，因为三层模型中的最优第一表示层并不是单层或双层模型中的最优第一层。深度学习的变革性在于它允许模型同时学习所有层的表示，而不是连续地（贪婪地，正如它被称为的那样）学习。通过联合特征学习，每当模型调整其内部特征时，所有依赖于它的其他特征都会自动适应变化，而无需人为干预。一切都由单一的反馈信号监督：模型中的每一次变化都服务于最终目标。这比贪婪地堆叠浅层模型要强大得多，因为它允许将复杂的、抽象的表示分解成一系列长期的中间空间（层）；每个空间与前一个空间之间只相差一个简单的转换。
- en: 'These are the two essential characteristics of how deep learning learns from
    data: the *incremental, layer-by-layer way in which increasingly complex representations
    are developed*, and the fact that *these intermediate incremental representations
    are learned jointly*, each layer being updated to follow both the representational
    needs of the layer above and the needs of the layer below. Together, these two
    properties have made deep learning vastly more successful than previous approaches
    to machine learning.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是深度学习从数据中学习的两个基本特征：逐渐增加的、逐层的方式发展越来越复杂的表示，以及这些中间增量表示是共同学习的，每一层都更新以同时遵循上一层的表示需求和下一层的需求。这两个属性的结合使得深度学习比以前的机器学习方法成功得多。
- en: 1.2.7 The modern machine learning landscape
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.7 现代机器学习格局
- en: A great way to get a sense of the current landscape of machine learning algorithms
    and tools is to look at machine learning competitions on Kaggle. Due to its highly
    competitive environment (some contests have thousands of entrants and million-dollar
    prizes) and to the wide variety of machine learning problems covered, Kaggle offers
    a realistic way to assess what works and what doesn’t. So, what kind of algorithm
    is reliably winning competitions? What tools do top entrants use?
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解当前机器学习算法和工具的现状，一个很好的方法是看一看 Kaggle 上的机器学习竞赛。由于其高度竞争的环境（一些比赛有数千名参赛者和百万美元的奖金）以及涵盖的机器学习问题的广泛多样性，Kaggle
    提供了一种实际的评估什么有效、什么无效的方式。那么，哪种算法可靠地赢得了竞赛？顶级参赛者使用什么工具？
- en: In early 2019, Kaggle ran a survey asking teams that ended in the top five of
    any competition since 2017 which primary software tool they had used in the competition
    (see [figure 1.12](#fig1-12)). It turns out that top teams tend to use either
    deep learning methods (most often via the Keras library) or gradient-boosted trees
    (most often via the LightGBM or XGBoost libraries).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在2019年初，Kaggle进行了一项调查，询问了自2017年以来任何竞赛中进入前五名的团队使用的主要软件工具（见[图1.12](#fig1-12)）。结果表明，顶级团队倾向于使用深度学习方法（通常通过Keras库）或梯度提升树（通常通过LightGBM或XGBoost库）。
- en: '![Image](../images/f0018-01.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0018-01.jpg)'
- en: '**Figure 1.12 Machine learning tools used by top teams on Kaggle**'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 1.12 Kaggle 顶级团队使用的机器学习工具**'
- en: It’s not just competition champions, either. Kaggle also runs a yearly survey
    among machine learning and data science professionals worldwide. With tens of
    thousands of respondents, this survey is one of the most reliable sources about
    the state of the industry. [Figure 1.13](#fig1-13) shows the percentage of usage
    of different machine learning software frameworks.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅仅是竞赛冠军。Kaggle还每年对全球机器学习和数据科学专业人士进行调查。这项调查有数万名受访者，是关于行业状况的最可靠来源之一。[图1.13](#fig1-13)显示了不同机器学习软件框架的使用百分比。
- en: 'From 2016 to 2020, the entire machine learning and data science industry has
    been dominated by these two approaches: deep learning and gradient-boosted trees.
    Specifically, gradient-boosted trees are used for problems where structured data
    is available, whereas deep learning is used for perceptual problems such as image
    classification.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 从2016年到2020年，整个机器学习和数据科学行业都被这两种方法主导：深度学习和梯度提升树。具体来说，梯度提升树用于有结构化数据可用的问题，而深度学习用于图像分类等感知问题。
- en: '![Image](../images/f0019-01.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0019-01.jpg)'
- en: '**Figure 1.13 Tool usage across the machine learning and data science industry
    (Source: [http://www.kaggle.com/kaggle-survey-2020](http://www.kaggle.com/kaggle-survey-2020))**'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**图1.13 工具在机器学习和数据科学行业中的使用情况（来源：[http://www.kaggle.com/kaggle-survey-2020](http://www.kaggle.com/kaggle-survey-2020)）**'
- en: 'Users of gradient-boosted trees tend to use Scikit-Learn, XGBoost, or LightGBM.
    Meanwhile, most practitioners of deep learning use Keras, often in combination
    with its parent framework, TensorFlow. The common point of these tools is they’re
    all available as R or Python libraries: R and Python are by far the most widely
    used language for machine learning and data science.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升树的用户倾向于使用Scikit-Learn、XGBoost或LightGBM。与此同时，大多数深度学习从业者使用Keras，通常与其母框架TensorFlow结合使用。这些工具的共同点是它们都作为R或Python库提供：R和Python是迄今为止最广泛使用的机器学习和数据科学语言。
- en: 'You should be the most familiar with the following two techniques to be successful
    in applied machine learning today: gradient-boosted trees, for shallow-learning
    problems; and deep learning, for perceptual problems. In technical terms, this
    means you’ll need to be familiar with XGBoost and Keras—the libraries that currently
    dominate Kaggle competitions. With this book in hand, you’re already one big step
    closer.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 要在今天的应用机器学习中取得成功，你应该最熟悉以下两种技术：梯度提升树，用于浅层学习问题；深度学习，用于感知问题。从技术上讲，这意味着你需要熟悉XGBoost和Keras——目前在Kaggle竞赛中占据主导地位的库。有了这本书，你已经离成功更近了一大步。
- en: 1.3 Why deep learning? Why now?
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.3 为什么深度学习？为什么现在？
- en: 'The two key ideas of deep learning for computer vision—convolutional neural
    networks and backpropagation—were already well understood by 1990\. The long short-term
    memory (LSTM) algorithm, which is fundamental to deep learning for time series,
    was developed in 1997 and has barely changed since. Why did deep learning take
    off only after 2012? What changed in these two decades? In general, the following
    three technical forces are driving advances in machine learning:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在计算机视觉领域的两个关键思想——卷积神经网络和反向传播——在1990年已经被充分理解。长短期记忆（LSTM）算法，这对于时间序列的深度学习至关重要，于1997年开发，并且自那时以来几乎没有改变。为什么深度学习直到2012年之后才起飞？这两个十年发生了什么变化？总的来说，以下三个技术力量推动了机器学习的进步：
- en: Hardware
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬件
- en: Datasets and benchmarks
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集和基准
- en: Algorithmic advances
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法进步
- en: Because the field is guided by experimental findings rather than by theory,
    algorithmic advances become possible only when appropriate data and hardware are
    available to try new ideas (or to scale up old ideas, as is often the case). Machine
    learning isn’t mathematics or physics, where major advances can be done with a
    pen and a piece of paper. It’s an engineering science.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 因为该领域是由实验发现而不是理论来指导的，所以只有在适当的数据和硬件可用于尝试新的想法（或者扩展旧的想法，通常情况下是这样）时，算法的进步才成为可能。机器学习不是数学或物理学，主要进步不能只靠一支笔和一张纸完成。这是一门工程科学。
- en: 'The real bottlenecks throughout the 1990s and 2000s were data and hardware.
    But here’s what happened during that time: the internet took off and high-performance
    graphics chips were developed for the needs of the gaming market.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 1990年代和2000年代的真正瓶颈是数据和硬件。但在此期间发生了以下事情：互联网蓬勃发展，高性能图形芯片为游戏市场的需求而开发。
- en: 1.3.1 Hardware
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.1 硬件
- en: Between 1990 and 2010, off-the-shelf CPUs became faster by a factor of approximately
    5,000\. As a result, nowadays it’s possible to run small deep learning models
    on your laptop, whereas this would have been intractable 25 years ago.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在1990年至2010年之间，现成的CPU速度提高了约5000倍。因此，现在可以在笔记本电脑上运行小型深度学习模型，而在25年前这是不可行的。
- en: But typical deep learning models used in computer vision or speech recognition
    require orders of magnitude more computational power than your laptop can deliver.
    Throughout the 2000s, companies like NVIDIA and AMD invested billions of dollars
    in developing fast, massively parallel chips (graphical processing units, or GPUs)
    to power the graphics of increasingly photorealistic video games—cheap, single-purpose
    supercomputers designed to render complex 3-D scenes on your screen in real time.
    This investment came to benefit the scientific community when, in 2007, NVIDIA
    launched CUDA ([https://developer.nvidia.com/about-cuda](https://www.developer.nvidia.com/about-cuda)),
    a programming interface for its line of GPUs. A small number of GPUs started replacing
    massive clusters of CPUs in various highly parallelizable applications, beginning
    with physics modeling. Deep neural networks, consisting mostly of many small matrix
    multiplications, are also highly parallelizable, and around 2011, some researchers
    began to write CUDA implementations of neural nets—Dan Cireşan^([6](#Rendnote6))
    and Alex Krizhevsky^([7](#Rendnote7)) were among the first.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，在计算机视觉或语音识别中使用的典型深度学习模型需要比您的笔记本电脑提供的计算能力高出几个数量级。在2000年代，像NVIDIA和AMD这样的公司投资了数十亿美元来开发快速、高度并行的芯片（图形处理单元，或GPU），以推动越来越逼真的视频游戏的图形——便宜的、单一用途的超级计算机，设计用于实时在屏幕上渲染复杂的三维场景。当时，NVIDIA于2007年推出了CUDA（[https://developer.nvidia.com/about-cuda](https://www.developer.nvidia.com/about-cuda)），这是其GPU系列的编程接口。一小部分GPU开始取代各种高度可并行化应用程序中的大型CPU集群，从物理建模开始。由于深度神经网络主要由许多小矩阵乘法组成，因此也具有高度可并行化性，并且大约在2011年左右，一些研究人员开始编写神经网络的CUDA实现——Dan
    Cireşan^([6](#Rendnote6))和Alex Krizhevsky^([7](#Rendnote7))是最早的。
- en: What happened is that the gaming market subsidized supercomputing for the next
    generation of artificial intelligence applications. Sometimes, big things begin
    as games. The NVIDIA Titan RTX, a GPU that cost $2,500 at the end of 2019, can
    deliver a peak of 16 teraflops in single precision (16 trillion float32 operations
    per second). That’s about 500 times more computing power than the world’s fastest
    supercomputer from 1990, the Intel Touchstone Delta. On a Titan RTX, it takes
    only a few hours to train an ImageNet model of the sort that would have won the
    ILSVRC competition around 2012 or 2013\. Meanwhile, large companies train deep
    learning models on clusters of hundreds of GPUs.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 发生的事情是，游戏市场为下一代人工智能应用程序提供了超级计算的资助。有时，大事物起源于游戏。NVIDIA Titan RTX是一款于2019年底售价为2500美元的GPU，可在单精度下提供16
    teraflops的峰值计算能力（每秒16万亿次float32运算）。这大约是1990年世界上最快的超级计算机英特尔Touchstone Delta的500倍。在Titan
    RTX上，只需几个小时即可训练出2012年或2013年左右赢得ILSVRC竞赛的ImageNet模型。与此同时，大公司使用数百个GPU的集群来训练深度学习模型。
- en: 'What’s more, the deep learning industry has been moving beyond GPUs and is
    investing in increasingly specialized, efficient chips for deep learning. In 2016,
    at its annual I/O convention, Google revealed its Tensor Processing Unit (TPU)
    project: a new chip design developed from the ground up to run deep neural networks
    significantly faster and far more energy efficiently than top-of-the-line GPUs.
    In 2020, the third iteration of the TPU card represents 420 teraflops of computing
    power. That’s 10,000 times more than the Intel Touchstone Delta from 1990.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 更重要的是，深度学习行业已经超越了GPU，并且正在投资于越来越专门化、高效的深度学习芯片。2016年，在其年度I/O大会上，Google公布了其张量处理单元（TPU）项目：一种新的芯片设计，从头开始开发，以比顶级GPU更快、更节能地运行深度神经网络。2020年，第三代TPU卡代表着420
    teraflops的计算能力。这比1990年的英特尔Touchstone Delta高出10,000倍。
- en: These TPU cards are designed to be assembled into large-scale configurations,
    called “pods.” One pod (1024 TPU cards) peaks at 100 petaflops. For scale, that’s
    about 10% of the peak computing power of the current largest supercomputer, the
    IBM Summit at Oak Ridge National Lab, which consists of 27,000 NVIDIA GPUs and
    peaks at around 1.1 exaflops.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这些TPU卡设计成可组装成大型配置，称为“pods”。一个pod（1024个TPU卡）峰值达到100 petaflops。就规模而言，这大约是当前最大超级计算机IBM
    Summit在奥克岭国家实验室的峰值计算能力的10％，它由27,000个NVIDIA GPU组成，峰值约为1.1 exaflops。
- en: 1.3.2 Data
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.2 数据
- en: 'AI is sometimes heralded as the new industrial revolution. If deep learning
    is the steam engine of this revolution, then data is its coal: the raw material
    that powers our intelligent machines, without which nothing would be possible.
    When it comes to data, in addition to the exponential progress in storage hardware
    over the past 20 years (following Moore’s law), the game changer has been the
    rise of the internet, making it feasible to collect and distribute very large
    datasets for machine learning. Today, large companies work with image datasets,
    video datasets, and natural language datasets that couldn’t have been collected
    without the internet. User-generated image tags on Flickr, for instance, have
    been a treasure trove of data for computer vision. So are YouTube videos. And
    Wikipedia is a key dataset for natural language processing.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能有时被誉为新的工业革命。如果深度学习是这场革命的蒸汽机，那么数据就是其煤炭：为我们的智能机器提供动力的原材料，没有它，一切都不可能。在数据方面，除了过去20年存储硬件的指数级进步（遵循摩尔定律）外，互联网的崛起是游戏规则的改变者，使得收集和分发非常大的机器学习数据集成为可能。今天，大公司使用图像数据集、视频数据集和自然语言数据集，这些数据集如果没有互联网就无法收集。例如，Flickr上用户生成的图像标签一直是计算机视觉的宝藏。YouTube视频也是如此。而维基百科是自然语言处理的关键数据集。
- en: If there’s one dataset that has been a catalyst for the rise of deep learning,
    it’s the ImageNet dataset, consisting of 1.4 million images that have been hand
    annotated with 1,000 image categories (one category per image). But what makes
    ImageNet special isn’t just its large size but also the yearly competition associated
    with it.^([8](#Rendnote8))
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果说有一个数据集推动了深度学习的崛起，那就是ImageNet数据集，包括140万张图像，已手动注释为1000个图像类别（每个图像一个类别）。但是，使ImageNet特殊的不仅仅是其规模，还有与之相关的每年一度的竞赛^([8](#Rendnote8))。
- en: As Kaggle has been demonstrating since 2010, public competitions are an excellent
    way to motivate researchers and engineers to push the envelope. Having common
    benchmarks that researchers compete to beat has greatly helped the rise of deep
    learning by highlighting its success against classical machine learning approaches.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 自2010年以来，Kaggle一直在展示，公开竞赛是激励研究人员和工程师突破瓶颈的极佳途径。拥有研究人员竞争超越的共同基准极大地推动了深度学习的崛起，突显了其对传统机器学习方法的成功。
- en: 1.3.3 Algorithms
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.3 算法
- en: In addition to hardware and data, until the late 2000s, we were missing a reliable
    way to train very deep neural networks. As a result, neural networks were still
    fairly shallow, using only one or two layers of representations; thus, they weren’t
    able to shine against more-refined shallow methods such as SVMs and random forests.
    The key issue was that of *gradient propagation* through deep stacks of layers.
    The feedback signal used to train neural networks would fade away as the number
    of layers increased.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 除了硬件和数据之外，直到2000年代后期，我们缺乏可靠的方法来训练非常深的神经网络。因此，神经网络仍然相对浅层，只使用一两层表示；因此，它们无法与更精细的浅层方法（如SVM和随机森林）相抗衡。关键问题是通过深层堆栈的层进行*梯度传播*。用于训练神经网络的反馈信号随着层数的增加而逐渐消失。
- en: 'This changed around 2009–2010 with the advent of the following simple but important
    algorithmic improvements that allowed for better gradient propagation:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 2009年至2010年间，随着以下简单但重要的算法改进的出现，使得更好的梯度传播成为可能：
- en: Better *activation functions* for neural layers
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好的神经层激活函数
- en: Better *weight-initialization schemes*, starting with layer-wise pretraining,
    which was then quickly abandoned
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好的*权重初始化方案*，从逐层预训练开始，然后迅速被抛弃
- en: Better *optimization schemes*, such as RMSprop and Adam
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好的*优化方案*，例如RMSprop和Adam
- en: Only when these improvements began to allow for training models with 10 or more
    layers did deep learning start to shine. Finally, in 2014, 2015, and 2016, even
    more advanced ways to improve gradient propagation were discovered, such as batch
    normalization, residual connections, and depthwise separable convolutions.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 只有当这些改进开始允许训练具有10个或更多层的模型时，深度学习才开始展现其优势。最终，在2014年、2015年和2016年，发现了更先进的改进梯度传播的方式，如批归一化、残差连接和深度可分离卷积。
- en: Today, we can train models that are arbitrarily deep from scratch. This has
    unlocked the use of extremely large models, which hold considerable representational
    power—that is to say, which encode very rich hypothesis spaces. This extreme scalability
    is one of the defining characteristics of modern deep learning. Large-scale model
    architectures, which feature tens of layers and tens of millions of parameters,
    have brought about critical advances both in computer vision (for instance, architectures
    such as ResNet, Inception, or Xception) and natural language processing (for instance,
    large Transformer-based architectures such as BERT, GPT-3, or XLNet).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，我们可以从头开始训练任意深度的模型。这解锁了使用极大模型的可能性，这些模型具有相当大的表征能力——也就是说，它们编码了非常丰富的假设空间。这种极端的可扩展性是现代深度学习的一个显著特征。大规模模型架构，具有数十层和数千万参数的特征，为计算机视觉（例如，ResNet、Inception或Xception等架构）和自然语言处理（例如，基于大型Transformer的架构，如BERT、GPT-3或XLNet）带来了关键进展。
- en: 1.3.4 A new wave of investment
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.4 新一轮投资浪潮
- en: As deep learning became the new state of the art for computer vision in 2012–2013,
    and eventually for all perceptual tasks, industry leaders took note. What followed
    was a gradual wave of industry investment far beyond anything previously seen
    in the history of AI (see [figure 1.14](#fig1-14)).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习成为2012年至2013年计算机视觉的新技术代表，最终也成为所有感知任务的新技术代表，行业领袖们开始关注。随之而来的是一波渐进的行业投资浪潮，远远超出了人工智能历史上以往的任何投资规模（参见[图1.14](#fig1-14)）。
- en: In 2011, right before deep learning took the spotlight, the total venture capital
    investment in AI worldwide was less than a billion dollars, which went almost
    entirely to practical applications of shallow machine learning approaches. In
    2015, it had risen to over $5 billion, and in 2017, to a staggering $16 billion.
    Hundreds of startups launched in these few years, trying to capitalize on the
    deep learning hype. Meanwhile, large tech companies such as Google, Amazon, and
    Microsoft have invested in internal research departments in amounts that would
    most likely dwarf the flow of venture-capital money.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习受到关注之前的2011年，全球人工智能的风险投资总额不到10亿美元，几乎完全投向了浅层机器学习方法的实际应用。到了2015年，这一数字已经上升到了50多亿美元，而在2017年，更是激增到了惊人的160亿美元。在这几年中，数百家初创公司纷纷涌现，试图利用深度学习的热潮。与此同时，谷歌、亚马逊和微软等大型科技公司投资于内部研究部门的金额很可能超过了风险投资资金的流动。
- en: '![Image](../images/f0023-01.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0023-01.jpg)'
- en: '**Figure 1.14 OECD estimate of total investments in AI startups (Source: [http://mng.bz/zGN6](http://mng.bz/zGN6))**'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**图1.14 经济合作与发展组织(OECD)对人工智能初创公司总投资的估计（来源：[http://mng.bz/zGN6](http://mng.bz/zGN6)）**'
- en: Machine learning—in particular, deep learning—has become central to the product
    strategy of these tech giants. In late 2015, Google CEO Sundar Pichai stated,
    “Machine learning is a core, transformative way by which we’re rethinking how
    we’re doing everything. We’re thoughtfully applying it across all our products,
    be it search, ads, YouTube, or Play. And we’re in early days, but you’ll see us—in
    a systematic way— apply machine learning in all these areas.”^([9](#Rendnote9))
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习——特别是深度学习——已经成为这些科技巨头产品战略的核心。2015年底，谷歌CEO桑达尔·皮查伊表示：“机器学习是我们重新思考我们如何做一切的核心、变革性方式。我们正在审慎地将其应用于我们所有的产品，无论是搜索、广告、YouTube还是Play。我们还处于早期阶段，但你会看到我们——以系统的方式——在所有这些领域应用机器学习。”^([9](#Rendnote9))
- en: As a result of this wave of investment, the number of people working on deep
    learning went from a few hundred to tens of thousands in less than 10 years, and
    research progress has reached a frenetic pace.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这一波投资浪潮，从不到10年的时间里，从事深度学习研究的人数从几百人增加到了数万人，研究进展达到了狂热的速度。
- en: 1.3.5 The democratization of deep learning
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.5 深度学习的民主化
- en: One of the key factors driving this inflow of new faces in deep learning has
    been the democratization of the toolsets used in the field. In the early days,
    doing deep learning required significant C++ and CUDA expertise, which few people
    possessed.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 推动深度学习领域涌现新面孔的关键因素之一是该领域使用的工具集的民主化。在早期，进行深度学习需要具有重要的C++和CUDA专业知识，而这方面的人才寥寥无几。
- en: Nowadays, basic R or Python scripting skills suffice to do advanced deep learning
    research. This has been driven most notably by the development of the TensorFlow
    library—a symbolic tensor-manipulation frameworks that supports autodifferentiation,
    greatly simplifying the implementation of new models—and by the rise of user-friendly
    libraries such as Keras, which makes deep learning as easy as manipulating LEGO
    bricks. After its release in early 2015, Keras quickly became the go-to deep learning
    solution for large numbers of new startups, graduate students, and researchers
    pivoting into the field.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，基本的 R 或 Python 脚本技能就足以进行高级的深度学习研究。这主要是由 TensorFlow 库的发展推动的——一种支持自动微分的符号张量操作框架，极大地简化了新模型的实现——以及用户友好型库（例如
    Keras），它使得深度学习就像操纵乐高积木一样容易。在 2015 年初发布后，Keras 很快成为了大量新创企业、研究生和转向该领域的研究人员的首选深度学习解决方案。
- en: 1.3.6 Will it last?
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.6 它会持续下去吗？
- en: Is there anything special about deep neural networks that makes them the “right”
    approach for companies to be investing in and for researchers to flock to? Or
    is deep learning just a fad that may not last? Will we still be using deep neural
    networks in 20 years?
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络有什么特别之处，使得它们成为公司投资和研究人员涌入的“正确”方法？或者深度学习只是一个可能不会持续的潮流？20年后我们还会使用深度神经网络吗？
- en: 'Deep learning has several properties that justify its status as an AI revolution,
    and it’s here to stay. We may not be using neural networks two decades from now,
    but whatever we use will directly inherit from modern deep learning and its core
    concepts. These important properties can be broadly sorted into the following
    three categories:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习具有几个属性，证明了它作为人工智能革命的地位，并且它将持续存在。也许 20 年后我们不会再使用神经网络，但我们使用的任何东西都将直接继承现代深度学习及其核心概念。这些重要属性可以大致分为以下三个类别：
- en: '*Simplicity*—Deep learning removes the need for feature engineering, replacing
    complex, brittle, engineering-heavy pipelines with simple, end-to-end trainable
    models that are typically built using only five or six different tensor operations.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*简单性*—深度学习消除了对特征工程的需求，用简单的、端到端可训练的模型取代了复杂、脆弱且工程量大的流水线，这些模型通常只使用五六种不同的张量操作构建而成。'
- en: '*Scalability*—Deep learning is highly amenable to parallelization on GPUs or
    TPUs, so it can take full advantage of Moore’s law. In addition, deep learning
    models are trained by iterating over small batches of data, allowing them to be
    trained on datasets of arbitrary size. (The only bottleneck is the amount of parallel
    computational power available, which, thanks to Moore’s law, is a fast-moving
    barrier.)'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可伸缩性*—深度学习非常适合在 GPU 或 TPU 上进行并行化处理，因此它可以充分利用摩尔定律。此外，深度学习模型是通过迭代处理小批量数据进行训练的，这使得它们可以在任意大小的数据集上进行训练。（唯一的瓶颈是可用的并行计算能力量，感谢摩尔定律，这是一个不断发展的障碍。）'
- en: '*Versatility and reusability*—Unlike many prior machine learning approaches,
    deep learning models can be trained on additional data without restarting from
    scratch, making them viable for continuous online learning—an important property
    for very large production models. Furthermore, trained deep learning models are
    repurposable and thus reusable: for instance, it’s possible to take a deep learning
    model trained for image classification and drop it into a video-processing pipeline.
    This allows us to reinvest previous work into increasingly complex and powerful
    models. This also makes deep learning applicable to fairly small datasets.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*多功能性和可重用性*—与许多先前的机器学习方法不同，深度学习模型可以在不从头开始重新启动的情况下训练额外的数据，这使得它们对连续在线学习是可行的——这对于非常大的生产模型是一种重要的性质。此外，经过训练的深度学习模型是可重用的：例如，可以将经过训练用于图像分类的深度学习模型应用到视频处理流水线中。这使得我们可以将之前的工作再投入到日益复杂和强大的模型中。这也使得深度学习适用于相当小的数据集。'
- en: 'Deep learning has been in the spotlight for only a few years, and we may not
    yet have established the full scope of what it can do. With every passing year,
    we learn about new use cases and engineering improvements that lift previous limitations.
    Following a scientific revolution, progress generally follows a sigmoid curve:
    it starts with a period of fast progress, which gradually stabilizes as researchers
    hit hard limitations, and then further improvements become incremental.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习仅在几年内成为热点，我们可能尚未确定其能够完成的全部范围。随着每一年的过去，我们了解到新的用例和工程改进，这些改进解决了以前的限制。在一场科学革命之后，进展通常遵循一个Sigmoid曲线：它开始于快速进展的阶段，随着研究人员遇到严重限制，逐渐稳定，然后进一步的改进变得渐进性。
- en: When I was writing the first edition of this book, in 2016, I predicted that
    deep learning was still in the first half of that sigmoid, with much more transformative
    progress to come in the following few years. This has proven true in practice—2017
    and 2018 have seen the rise of Transformer-based deep learning models for natural
    language processing, which have been a revolution in the field, while deep learning
    also kept delivering steady progress in computer vision and speech recognition.
    Today, in 2022, deep learning seems to have entered the second half of that sigmoid.
    We should still expect significant progress in the years to come, but we’re probably
    out of the initial phase of explosive progress.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 当我在2016年撰写本书的第一版时，我预测深度学习仍处于那个Sigmoid函数的前半部分，在接下来的几年里将会有更多变革性的进展。这在实践中得到了证实——2017年和2018年见证了基于Transformer的深度学习模型在自然语言处理领域的崛起，这在该领域引起了一场革命，同时深度学习在计算机视觉和语音识别领域也持续稳步取得进展。如今，2022年，深度学习似乎已经进入了那个Sigmoid函数的后半部分。我们仍然应该期待未来几年的重大进展，但我们可能已经走出了最初阶段的爆炸性进展。
- en: Today, I’m extremely excited about the deployment of deep learning technology
    to every problem it can solve—the list is endless. Deep learning is still a revolution
    in the making, and it will take many years to realize its full potential.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，我对深度学习技术在解决所有可能的问题上的部署感到非常兴奋——列表是无穷无尽的。深度学习仍然是一场正在进行中的革命，要实现其全部潜力还需要很多年。
- en: '^([1](#endnote1)) A.M. Turing, “Computing Machinery and Intelligence,” *Mind*
    59, no. 236 (1950): 433–460.'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '^([1](#endnote1)) A.M. 图灵，“计算机机器和智能”，*心灵* 59, no. 236 (1950): 433–460。'
- en: ^([2](#endnote2)) Although the Turing test has sometimes been interpreted as
    a literal test—a goal the field of AI should set out to reach—Turing merely meant
    it as a conceptual device in a philosophical discussion about the nature of cognition.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ^([2](#endnote2)) 尽管图灵测试有时被解释为一个字面上的测试——AI领域应该设定的目标——但图灵只是将其作为一个关于认知本质的哲学讨论中的概念设备。
- en: '^([3](#endnote3)) Vladimir Vapnik and Corinna Cortes, “Support-Vector Networks,”
    *Machine Learning* 20, no. 3 (1995): 273–297.'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '^([3](#endnote3)) 弗拉迪米尔·瓦普尼克和科琳娜·科特斯，“支持向量网络”，*机器学习* 20, no. 3 (1995): 273–297。'
- en: ^([4](#endnote4)) Vladimir Vapnik and Alexey Chervonenkis, “A Note on One Class
    of Perceptrons,” *Automation and Remote Control* 25 (1964).
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ^([4](#endnote4)) 弗拉迪米尔·瓦普尼克和亚历克谢·切尔沃能基斯，“关于一类感知器的注记”，*自动化与遥控* 25 (1964)。
- en: ^([5](#endnote5)) “Top-five accuracy” measures how often the model selects the
    correct answer as part of its top five guesses (out of 1,000 possible answers,
    in the case of ImageNet).
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ^([5](#endnote5)) “前五准确率”衡量模型在其前五个猜测中选择正确答案的频率（在ImageNet的情况下，有1000个可能的答案）。
- en: ^([6](#endnote6)) See “Flexible, High Performance Convolutional Neural Networks
    for Image Classification,” *Proceedings of the 22nd International Joint Conference
    on Artificial Intelligence* (2011), [http://mng.bz/nN0K.](http://mng.bz/nN0K)
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ^([6](#endnote6)) 请参阅“灵活、高性能的卷积神经网络用于图像分类”，*第22届国际人工智能联合会议论文集* (2011)，[http://mng.bz/nN0K.](http://mng.bz/nN0K)
- en: ^([7](#endnote7)) See “ImageNet Classification with Deep Convolutional Neural
    Networks,” *Advances in Neural Information Processing Systems* 25 (2012), [http://mng.bz/2286.](http://mng.bz/2286)
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ^([7](#endnote7)) 请参阅“用深度卷积神经网络进行ImageNet分类”，*神经信息处理系统的进展* 25 (2012)，[http://mng.bz/2286.](http://mng.bz/2286)
- en: ^([8](#endnote8)) The ImageNet Large Scale Visual Recognition Challenge (ILSVRC),
    [http://www.image-net.org/challenges/LSVRC.](http://www.image-net.org/challenges/LSVRC)
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ^([8](#endnote8)) ImageNet大规模视觉识别挑战（ILSVRC），[http://www.image-net.org/challenges/LSVRC.](http://www.image-net.org/challenges/LSVRC)
- en: ^([9](#endnote9)) Sundar Pichai, Alphabet earnings call, Oct. 22, 2015.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ^([9](#endnote9)) Sundar Pichai，Alphabet盈利电话会议，2015年10月22日。
