- en: 5 Sequential labeling and language modeling
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 顺序标注和语言建模
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Solving part-of-speech (POS) tagging and named entity recognition (NER) using
    sequential labeling
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用顺序标注解决词性标注（POS）和命名实体识别（NER）
- en: Making RNNs more powerful—multilayer and bidirectional recurrent neural networks
    (RNNs)
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使 RNNs 更强大——多层和双向循环神经网络（RNNs）
- en: Capturing statistical properties of language using language models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用语言模型捕捉语言的统计特性
- en: Using language models to evaluate and generate natural language text
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用语言模型评估和生成自然语言文本
- en: In this chapter, we are going to discuss sequential labeling—an important NLP
    framework where systems tag individual words with corresponding labels. Many NLP
    applications, such as part-of-speech tagging and named entity recognition, can
    be framed as sequential-labeling tasks. In the second half of the chapter, I’ll
    introduce the concept of language models, one of the most fundamental yet exciting
    topics in NLP. I’ll talk about why they are important and how to use them to evaluate
    and even generate some natural language text.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论顺序标注——一个重要的自然语言处理框架，系统会为每个单词打上相应的标签。许多自然语言处理应用，如词性标注和命名实体识别，可以被构建为顺序标注任务。在本章的后半部分，我将介绍语言模型的概念，这是自然语言处理中最基本但也最令人兴奋的主题之一。我将谈论它们为何重要以及如何使用它们来评估甚至生成一些自然语言文本。
- en: 5.1 Introducing sequential labeling
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 介绍顺序标注
- en: In the previous chapter, we discussed sentence classification, where the task
    is to assign some label to a given sentence. Spam filtering, sentiment analysis,
    and language detection are some concrete examples of sentence classification.
    Although many real-world NLP problems can be formulated as a sentence-classification
    task, this method can also be quite limited, because the model, by definition,
    allows us to assign only a single label to the whole sentence. But what if you
    wanted something more granular? For example, what if you wanted to do something
    with individual words, not just with the sentence? The most typical scenario you
    encounter is when you want to extract something from the sentence, which cannot
    be easily solved by sentence classification. This is where sequential labeling
    comes into play.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了句子分类，任务是为给定的句子分配一些标签。垃圾邮件过滤、情感分析和语言检测是句子分类的一些具体例子。尽管许多现实世界的自然语言处理问题可以被规约为句子分类任务，但这种方法也可能相当有限，因为根据定义，该模型只允许我们为整个句子分配一个单一的标签。但如果你想要更细粒度的东西呢？例如，如果你想要对单个单词做一些操作，而不仅仅是句子呢？你遇到的最典型的场景是当你想要从句子中提取一些东西时，这并不能很容易地通过句子分类来解决。这就是顺序标注发挥作用的地方。
- en: 5.1.1 What is sequential labeling?
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.1 顺序标注是什么？
- en: '*Sequential* *labeling* is an NLP task where, given a sequence such as a sentence,
    the NLP system assigns a label to each element (e.g., word) of the input sequence.
    This contrasts with sentence classification, where a label is assigned just to
    the input *sentence*. Figure 5.1 illustrates this contrast.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*顺序标注* 是一种自然语言处理任务，给定一个序列，比如一个句子，NLP 系统会为输入序列的每个元素（比如单词）分配一个标签。这与句子分类形成对比，句子分类仅为输入*句子*分配一个标签。图
    5.1 展示了这种对比。'
- en: '![CH05_F01_Hagiwara](../Images/CH05_F01_Hagiwara.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F01_Hagiwara](../Images/CH05_F01_Hagiwara.png)'
- en: Figure 5.1 Sentence classification vs. sequential labeling
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 句子分类与顺序标注
- en: But why is this even a good idea? When do we need a label per word? A typical
    scenario where sequential labeling comes in handy is when you want to analyze
    a sentence and produce some linguistic information per word. For example, part-of-speech
    (POS) tagging, which I mentioned in chapter 1, produces a POS tag such as nouns,
    verbs, and prepositions for each word in the input sentence and is a perfect match
    for sequential labeling. See figure 5.2 for an illustration.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 但为什么这是个好主意呢？我们什么时候需要每个词都有一个标签？顺序标注非常方便的一个典型场景是当你想要分析一个句子并为每个词生成一些语言学信息。例如，词性标注（POS）就是一个很好的例子，如我在第一章中提到的，它为输入句子中的每个单词生成一个词性标签，比如名词、动词和介词，非常适合顺序标注。请参见图
    5.2 进行说明。
- en: '![CH05_F02_Hagiwara](../Images/CH05_F02_Hagiwara.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F02_Hagiwara](../Images/CH05_F02_Hagiwara.png)'
- en: Figure 5.2 Part-of-speech (POS) tagging using sequential labeling
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 使用顺序标注进行词性标注（POS）
- en: POS tagging is one of the most fundamental, important NLP tasks. Many English
    words (and words in many other languages as well) are ambiguous, meaning that
    they have multiple possible interpretations. For example, the word “book” can
    be used to describe a physical or electronic object consisting of pages (“I read
    a book”) or an action for reserving something (“I need to book a flight”). Downstream
    NLP tasks, such as parsing and classification, benefit greatly by knowing what
    each appearance of “book” actually means to process the input sentence. If you
    were to build a speech synthesis system, you must know the POS of certain words
    to pronounce them correctly—“lead” as a noun (a kind of metal) rhymes with “bed,”
    whereas “lead” as a verb (to direct, guide) rhymes with “bead.” POS tagging is
    an important first step toward solving this ambiguity.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 词性标注是最基础、最重要的自然语言处理任务之一。许多英语单词（以及许多其他语言的单词）都是有歧义的，意味着它们有多种可能的解释。例如，单词“book”可以用来描述由页面组成的物理或电子对象（“我读了一本书”），也可以用来描述预订某物的行为（“我需要预订一次航班”）。下游的自然语言处理任务，比如解析和分类，在知道每个“book”的出现实际上意味着什么以便处理输入句子时受益匪浅。如果你要构建一个语音合成系统，你必须知道某些单词的词性才能正确地发音——名词“lead”（一种金属）与“bed”押韵，而动词“lead”（指导，引导）与“bead”押韵。词性标注是解决这种歧义的重要第一步。
- en: Another scenario is when you want to extract some pieces of information from
    a sentence. For example, if you want to extract subsequences (phrases) such as
    noun phrases and verb phrases, this is also a sequential-labeling task. How can
    you achieve extraction using labeling? The idea is to mark the beginning and the
    end (or the beginning and the continuation, depending on how you represent it)
    of the desired piece of information using labeling. An example of this is *named
    entity recognition* (NER), which is a task to identify mentions to real-world
    entities, such as proper nouns and numerical expressions, from a sentence (illustrated
    in figure 5.3.).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个场景是当你想要从一个句子中提取一些信息片段时。例如，如果你想要提取名词短语和动词短语等子序列（短语），这也是一个序列标记任务。你如何使用标记来实现提取？这个想法是使用标记来标记所需信息片段的开始和结束（或开始和继续，取决于你如何表示它）。*命名实体识别*（NER）就是一个例子，它是从一个句子中识别真实世界实体的任务，比如专有名词和数字表达式（在图
    5.3 中说明）。
- en: '![CH05_F03_Hagiwara](../Images/CH05_F03_Hagiwara.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F03_Hagiwara](../Images/CH05_F03_Hagiwara.png)'
- en: Figure 5.3 Named entity recognition (NER) using sequential labeling
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 使用序列标记的命名实体识别（NER）
- en: Notice that all the words that are not part of any named entities are tagged
    as O (for “Outside”). For now, you can ignore some cryptic labels in figure 5.3
    such as B-GPE and I-MONEY. I’ll talk more about how to formulate NER as a sequential-labeling
    problem in section 5.4.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，所有不属于任何命名实体的单词都被标记为 O（代表“外部”）。目前，你可以忽略图 5.3 中一些神秘的标签，比如 B-GPE 和 I-MONEY。在第
    5.4 节中，我会更多地讨论如何将命名实体识别问题制定为一个序列标记问题。
- en: 5.1.2 Using RNNs to encode sequences
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.2 使用 RNNs 编码序列
- en: 'In sentence classification, we used recurrent neural networks (RNNs) to convert
    an input of variable length into a fixed-length vector. The fixed-length vector,
    which is converted to a set of “scores” by a linear layer, captures the information
    about the input sentence that is necessary for deriving the sentence label. As
    a reminder, what this RNN does can be represented by the following pseudocode
    and the diagram shown in figure 5.4:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在句子分类中，我们使用递归神经网络（RNNs）将可变长度的输入转换为固定长度的向量。这个固定长度的向量通过一个线性层转换为一组“分数”，捕获了关于输入句子的信息，这对于推导句子标签是必要的。作为提醒，这个
    RNN 的作用可以用以下伪代码和图 5.4 中显示的图表来表示：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![CH05_F04_Hagiwara](../Images/CH05_F04_Hagiwara.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F04_Hagiwara](../Images/CH05_F04_Hagiwara.png)'
- en: Figure 5.4 Recurrent neural network (RNN) for sentence classification
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4 句子分类的递归神经网络（RNN）
- en: 'What kind of neural network could be used for sequential tagging? We seem to
    need some information for every input word in the sentence, not just at the end.
    If you look at the pseudocode for rnn_vec() carefully, you can notice that we
    already have information for every word in the input, which is captured by state.
    The function just happens to return only the final value of state, but there is
    no reason we can’t store intermediate values of state and return them as a list
    instead, as in the following function:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 什么样的神经网络可以用于序列标记？我们似乎需要句子中每个输入单词的一些信息，而不仅仅是在末尾。如果您仔细查看rnn_vec()的伪代码，您会注意到我们已经有了输入中每个单词的信息，这些信息由状态捕获。该函数恰好只返回状态的最终值，但我们没有理由不能存储状态的中间值并将它们作为列表返回，就像以下函数一样：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If you apply this function to the “time flies” example shown in figure 5.2
    and unroll it—that is, write it without using a loop—it will look like the following:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将此函数应用于图5.2中显示的“time flies”示例并展开它——也就是说，不使用循环写出它——它将如下所示：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note that v() here is a function that returns the embedding for the given word.
    This can be visualized as shown in figure 5.5\. Notice that for each input word
    word, the network produces the corresponding state that captures some information
    about word. The length of the list states is the same as that of words. The final
    value of states, that is, states[-1], is identical to the return value of rnn_vec()
    from earlier.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的v()是一个函数，它返回给定单词的嵌入。这可以通过图5.5中所示的方式进行可视化。请注意，对于每个输入单词word，网络都会产生捕获有关该单词的一些信息的相应状态。状态列表states的长度与words的长度相同。状态的最终值，即states[-1]，与先前的rnn_vec()的返回值相同。
- en: '![CH05_F05_Hagiwara](../Images/CH05_F05_Hagiwara.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F05_Hagiwara](../Images/CH05_F05_Hagiwara.png)'
- en: Figure 5.5 Recurrent neural network (RNN) for sequential labeling
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5 用于序列标记的递归神经网络（RNN）
- en: If you think of this RNN as a black box, it takes a sequence of something (e.g.,
    word embeddings) and converts it to a sequence of vectors that encode some information
    about individual words in the input, so this architecture is called a *Seq2Seq*
    (for “sequence-to-sequence”) encoder in AllenNLP.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将这个循环神经网络视为一个黑匣子，它接受一系列东西（例如，词嵌入）并将其转换为编码有关输入中各个单词信息的向量序列，因此该架构在AllenNLP中被称为*Seq2Seq*（代表“序列到序列”）编码器。
- en: The final step is to apply a linear layer to each state of this RNN to derive
    a set of scores that correspond to how likely each label is. If this is a part-of-speech
    tagger, we
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是将这个RNN的每个状态应用于一个线性层，以得到对每个标签的可能性的一组分数。如果这是一个词性标注器，我们
- en: need one score for the label NOUN, another for VERB, and so on for each and
    every word. This conversion is illustrated in figure 5.6\. Note that the same
    linear layer (with the same set of parameters) is applied to every state.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对于标签NOUN，需要一个分数，对于VERB，需要另一个分数，以此类推，适用于每个单词。此转换如图5.6所示。请注意，相同的线性层（具有相同的参数集）应用于每个状态。
- en: '![CH05_F06_Hagiwara](../Images/CH05_F06_Hagiwara.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F06_Hagiwara](../Images/CH05_F06_Hagiwara.png)'
- en: Figure 5.6 Applying a linear layer to RNN
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6 将线性层应用于RNN
- en: To sum up, we can use almost the same structure for sequential labeling as the
    one we used for sentence classification. The only difference is the former produces
    a hidden state per each word, not just per sentence. To derive scores used for
    determining labels, a linear layer is applied to every hidden state.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们可以使用几乎与我们用于句子分类相同的结构进行序列标记，唯一的区别是前者为每个单词生成一个隐藏状态，而不仅仅是每个句子。要生成用于确定标签的分数，必须将线性层应用于每个隐藏状态。
- en: 5.1.3 Implementing a Seq2Seq encoder in AllenNLP
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.3 在AllenNLP中实现Seq2Seq编码器
- en: 'AllenNLP implements an abstract class called Seq2SeqEncoder for abstracting
    all Seq2Seq encoders that take in a sequence of vectors and return another sequence
    of modified vectors. In theory, you can inherit the class and implement your own
    Seq2Seq encoder. In practice, however, you most likely will use one of the off-the-shelf
    implementations that PyTorch/AllenNLP provide, such as LSTM and GRU. Remember,
    when we built the encoder for the sentiment analyzer, we used PyTorch’s built-in
    torch.nn.LSTM and wrapped it with PytorchSeq2VecWrapper, as shown next, which
    makes it compatible with AllenNLP’s abstraction:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: AllenNLP实现了一个称为Seq2SeqEncoder的抽象类，用于抽象化所有接受向量序列并返回另一个修改后向量序列的Seq2Seq编码器。理论上，您可以继承该类并实现自己的Seq2Seq编码器。然而，在实践中，您很可能会使用PyTorch/AllenNLP提供的现成实现之一，例如LSTM和GRU。请记住，当我们为情感分析器构建编码器时，我们使用了PyTorch的内置torch.nn.LSTM，并将其包装为PytorchSeq2VecWrapper，如下所示，这使其与AllenNLP的抽象兼容：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'AllenNLP also implements PytorchSeq2SeqWrapper, which takes one of PyTorch’s
    built-in RNN implementations and makes it compliant with AllenNLP’s Seq2SeqEncoder,
    so there’s very little change you need to initialize a Seq2Seq encoder, as shown
    here:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: AllenNLP还实现了PytorchSeq2SeqWrapper，它使用PyTorch的内置RNN实现之一，并使其符合AllenNLP的Seq2SeqEncoder，因此你需要做的很少，只需要像这样初始化一个Seq2Seq编码器：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: That’s it! There are a couple more things to note, but there’s surprisingly
    few changes you need to make to the sentence classification code to make it work
    for sequential labeling. This is thanks to the powerful abstraction of AllenNLP—most
    of the time you need to worry only about how individual components interact with
    each other, not about how these components work internally.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！还有一些需要注意的地方，但是你会惊奇地发现，为了使其用于顺序标记，你需要进行的更改很少，这得益于AllenNLP的强大抽象——大部分时间你只需要关心各个组件如何相互作用，而不需要关心这些组件工作的实现方式。
- en: 5.2 Building a part-of-speech tagger
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 构建一个词性标注器
- en: 'In this section, we are going to build our first sequential-labeling application—a
    part-of-speech (POS) tagger. You can see the entire code for this section on the
    Google Colab notebook ([http://realworldnlpbook.com/ch5.html#pos-nb](http://realworldnlpbook.com/ch5.html#pos-nb)).
    We assume that you have already imported all necessary dependencies as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将构建我们的第一个顺序标记应用程序—一个词性（POS）标注器。您可以在Google Colab笔记本上查看此部分的全部代码([http://realworldnlpbook.com/ch5.html#pos-nb](http://realworldnlpbook.com/ch5.html#pos-nb))。我们假设您已经导入了所有必要的依赖项，如下所示：
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 5.2.1 Reading a dataset
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.1 读取数据集
- en: As we saw in chapter 1, a *part of speech* (POS) is a category of words that
    share similar grammatical properties. Part-of-speech tagging is the process of
    tagging each word in a sentence with a corresponding part-of-speech tag. A training
    set for POS tagging follows a tagset, which is a set of predefined POS tags for
    the language.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在第1章中所看到的那样，*词性*（POS）是一组共享相似语法属性的词汇类别。词性标注是将句子中的每个单词与相应的词性标记进行标记化的过程。用于POS标注的训练集遵循一组预定义的语言POS标签集。
- en: To train a POS tagger, we need a dataset where every word in every sentence
    is tagged with its corresponding POS tag. In this experiment, we are going to
    use the English Universal Dependencies (UD) dataset. Universal Dependencies is
    a language-independent dependency grammar framework developed by a group of researchers.
    UD also defines a tagset called the *Universal part-of-speech tagset* ([http://realworldnlpbook.com/ch1.html#universal-pos](http://realworldnlpbook.com/ch1.html#universal-pos)).
    The use of UD and the Universal POS tagset has been very popular in the NLP community,
    especially for language-independent tasks and models such as POS tagging and parsing.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练一个词性标注器，我们需要一个数据集，其中每个句子中的每个单词都标有相应的词性标记。在这个实验中，我们将使用英语Universal Dependencies
    (UD)数据集。Universal Dependencies是一个由一群研究者开发的语言无关的依存语法框架。UD还定义了一个标签集，称为*通用词性标记集*
    ([http://realworldnlpbook.com/ch1.html#universal-pos](http://realworldnlpbook.com/ch1.html#universal-pos))。UD和Universal
    POS标记集的使用在NLP社区中非常流行，尤其是在诸如词性标注和解析等语言无关任务和模型中。
- en: We are going to use one subcorpus of UD called *A Gold Standard Universal Dependencies
    Corpus for English*, which is built on top of the English Web Treebank (EWT) ([http://realworldnlpbook.com/ch5.html#ewt](http://realworldnlpbook.com/ch5.html#ewt))
    and can be used under a Creative Commons license. You can download the entire
    dataset from the dataset page ([http://realworldnlpbook.com/ch5.html#ewt-data](http://realworldnlpbook.com/ch5.html#ewt-data)),
    if needed.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用UD的一个子语料库，名为*"A Gold Standard Universal Dependencies Corpus for English"，该语料库建立在英语Web
    Treebank (EWT)之上([http://realworldnlpbook.com/ch5.html#ewt](http://realworldnlpbook.com/ch5.html#ewt))，并可在创作共用许可下使用。如需要，您可以从数据集页面([http://realworldnlpbook.com/ch5.html#ewt-data](http://realworldnlpbook.com/ch5.html#ewt-data))下载整个数据集。
- en: 'Universal Dependencies datasets are distributed in a format called the *CoNLL-U
    format* ([http://universaldependencies.org/docs/format.html](http://universaldependencies.org/docs/format.html)).
    The AllenNLP models package already implements a dataset reader called UniversalDependenciesDatasetReader
    that reads datasets in this format and returns a collection of instances that
    include information like word forms, POS tags, and dependency relationship, so
    all you need to do is initialize and use it as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Universal Dependencies 数据集以一种称为*CoNLL-U 格式*（[http://universaldependencies.org/docs/format.html](http://universaldependencies.org/docs/format.html)）的格式分发。AllenNLP
    模型包已经实现了一个名为 UniversalDependenciesDatasetReader 的数据集读取器，它以这种格式读取数据集，并返回包含词形、词性标签和依赖关系等信息的实例集合，因此你只需初始化并使用它，如下所示：
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Also, don’t forget to initialize data loaders and a Vocabulary instance, too,
    as shown next:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 还有，不要忘记初始化数据加载器和一个词汇表实例，如下所示：
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 5.2.2 Defining the model and the loss
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.2 定义模型和损失
- en: 'The next step for building a POS tagger is to define the model. In the previous
    section, we already saw that you can initialize a Seq2Seq encoder with very little
    modification using AllenNLP’s built-in PytorchSeq2VecWrapper. Let’s define other
    components (word embeddings and LSTM) and some variables necessary for the model
    as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 构建词性标注器的下一步是定义模型。在前面的部分中，我们已经看到你可以使用 AllenNLP 内置的 PytorchSeq2VecWrapper 很少修改就初始化一个
    Seq2Seq 编码器。让我们按照以下方式定义其他组件（词嵌入和 LSTM）以及模型所需的一些变量：
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now we are ready to define the body of the POS tagger model, as shown here.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备定义词性标注器模型的主体，如下所示。
- en: Listing 5.1 POS tagger model
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 5.1 词性标注器模型
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ We use accuracy to evaluate the POS tagger.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们使用准确度来评估词性标注器。
- en: ❷ We need **args to capture unnecessary instance fields that AllenNLP automatically
    destructures.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们需要**args来捕获 AllenNLP 自动解构的不必要的实例字段。
- en: ❸ The Seq2Seq encoder is trained using a sequence cross-entropy loss.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用序列交叉熵损失训练 Seq2Seq 编码器。
- en: Notice that the code shown in listing 5.1 is very similar to the code for LstmClassifier
    (listing 4.1), which we used for building a sentiment analyzer. In fact, except
    for some naming differences, only one fundamental difference exists—the type of
    loss function.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，清单 5.1 中显示的代码与我们用于构建情感分析器的 LstmClassifier 代码（清单 4.1）非常相似。实际上，除了一些命名差异之外，只存在一个基本差异——损失函数的类型。
- en: Recall that we used a loss function called *cross entropy* for sentence classification
    tasks, which basically measures how far apart two distributions are. If the model
    produces a high probability for the true label, the loss will be low. Otherwise,
    it will be high. But this assumed that there is only one label per sentence. How
    can we measure how far the prediction is from the true label when there is one
    label per word?
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，我们在句子分类任务中使用了一种称为*交叉熵*的损失函数，它基本上衡量了两个分布之间的距离。如果模型产生了真实标签的高概率，损失将很低。否则，它将很高。但是这假设每个句子只有一个标签。当每个词只有一个标签时，我们如何衡量预测与真实标签的差距？
- en: 'The answer is: still use the cross entropy, but average it over all the elements
    in the input sequence. For POS tagging, you compute the cross entropy per word
    as if it were an individual classification task, sum it over all the words in
    the input sentence, and divide by the length of the sentence. This will give you
    a number reflecting how well your model is predicting the POS tags for the input
    sentence on average. See figure 5.7 for an illustration.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是：仍然使用交叉熵，但是将其平均化到输入序列中的所有元素上。对于词性标注，你计算每个词的交叉熵，就像它是一个单独的分类任务一样，将其求和到输入句子中的所有词上，并除以句子的长度。这将给你一个反映你的模型平均预测输入句子的词性标签的好坏程度的数字。查看图
    5.7 进行说明。
- en: '![CH05_F07_Hagiwara](../Images/CH05_F07_Hagiwara.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F07_Hagiwara](../Images/CH05_F07_Hagiwara.png)'
- en: Figure 5.7 Computing loss for sequence
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7 计算序列的损失
- en: As for the evaluation metric, POS taggers are usually evaluated using accuracy,
    which we are going to use here. Average human performance on POS tagging is around
    97%, whereas the state-of-the-art POS taggers slightly outperform this ([http://realworldnlp
    book.com/ch5.html#pos-sota](http://realworldnlpbook.com/ch5.html#pos-sota)). You
    need to note that accuracy is not without a problem, however—assume there is a
    relatively rare POS tag (e.g., SCONJ, which means subordinating conjugation),
    which accounts for only 2% of total tokens, and a POS tagger messes it up every
    time it appears. If the tagger gets the rest of the tokens all correct, it still
    achieves 98% accuracy.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 关于评估指标，POS 标注器通常使用准确率进行评估，我们将在这里使用。POS 标注的平均人类表现约为 97%，而最先进的 POS 标注器略高于此（[http://realworldnlp
    book.com/ch5.html#pos-sota](http://realworldnlpbook.com/ch5.html#pos-sota)）。然而，需要注意准确率并非没有问题——假设存在一个相对罕见的
    POS 标签（例如 SCONJ，表示从属连接），它仅占总标记数的 2%，而一个 POS 标注器每次出现都会搞砸它。如果标注器将其余标记都正确识别，则仍可达到
    98% 的准确率。
- en: 5.2.3 Building the training pipeline
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.3 构建训练流水线
- en: Now we are ready to move on to building the training pipeline. As with the previous
    tasks, training pipelines in AllenNLP look very similar to each other. See the
    next listing for the training code.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备好开始构建训练流水线了。与之前的任务一样，AllenNLP 中的训练流水线看起来非常相似。请查看下一个清单以查看训练代码。
- en: Listing 5.2 Training pipeline for POS tagger
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 5.2 POS 标注器的训练流水线
- en: '[PRE10]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'When you run this code, AllenNLP alternates between two phases: 1) training
    the model using the train set, and 2) evaluating it using the validation set for
    each epoch, while monitoring the loss and accuracy on both sets. Validation set
    accuracy plateaus around 88% after several epochs. After the training is over,
    you can run the model for an unseen instance as shown next:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当运行此代码时，AllenNLP 会在两个阶段之间交替进行：1）使用训练集训练模型，2）使用验证集评估每个时代，同时监控两个集合上的损失和准确率。经过几个时代后，验证集的准确率会在约
    88% 左右稳定。训练结束后，您可以运行下面显示的模型以查看一个未见过的实例：
- en: '[PRE11]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This code uses UniversalPOSPredictor, a predictor that I wrote for this particular
    POS tagger. Although its details are not important, you can look at its code if
    you are interested ([http://realworldnlpbook.com/ch5#upos-predictor](http://realworldnlpbook.com/ch5#upos-predictor)).
    If successful, this will show a list of POS tags: [''DET'', ''NOUN'', ''VERB'',
    ''DET'', ''NOUN'', ''PUNCT''], which is indeed a correct POS tag sequence for
    the input sentence.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码使用了 UniversalPOSPredictor，这是我为这个特定的 POS 标注器编写的一个预测器。虽然其细节并不重要，但如果您感兴趣，可以查看它的代码（[http://realworldnlpbook.com/ch5#upos-predictor](http://realworldnlpbook.com/ch5#upos-predictor)）。如果成功，这将显示一个
    POS 标签列表：['DET', 'NOUN', 'VERB', 'DET', 'NOUN', 'PUNCT']，这确实是输入句子的正确 POS 标签序列。
- en: 5.3 Multilayer and bidirectional RNNs
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 多层和双向 RNN
- en: As we’ve seen so far, RNNs are a powerful tool for building NLP applications.
    In this section, I talk about their structural variants—multilayer and bidirectional
    RNNs—which are even more powerful components for building highly accurate NLP
    applications.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们迄今所见，RNN 是构建 NLP 应用程序的强大工具。在本节中，我将讨论它们的结构变体——多层和双向 RNN，这些是构建高度准确的 NLP 应用程序的更强大组件。
- en: 5.3.1 Multilayer RNNs
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.1 多层 RNN
- en: If you look at an RNN as a black box, it is a neural network structure that
    converts a sequence of vectors (word embeddings) into another sequence of vectors
    (hidden states). The input and output sequences are of the same length, usually
    the number of input tokens. This means that you can repeat this “encoding” process
    multiple times by stacking RNNs on top of each other. The output (hidden states)
    of one RNN becomes the input of another RNN that is just above the previous one.
    A substructure (such as a single RNN) of a bigger neural network is called a *layer*,
    because you can stack them together like layers. The structure of a two-layered
    RNN is shown in figure 5.8.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将 RNN 视为黑盒子，则它是一个将一系列向量（单词嵌入）转换为另一系列向量（隐藏状态）的神经网络结构。输入和输出序列的长度相同，通常是输入标记的数量。这意味着您可以通过将
    RNN 堆叠在一起多次重复这个“编码”过程。一个 RNN 的输出（隐藏状态）成为上面的另一个 RNN 的输入，这个 RNN 刚好位于前一个 RNN 的上面。较大神经网络的子结构（例如单个
    RNN）称为*层*，因为您可以像层一样将它们堆叠在一起。两层 RNN 的结构如图 5.8 所示。
- en: '![CH05_F08_Hagiwara](../Images/CH05_F08_Hagiwara.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F08_Hagiwara](../Images/CH05_F08_Hagiwara.png)'
- en: Figure 5.8 Two-layered RNN
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8 两层 RNN
- en: Why is this a good idea? If you think of a layer of RNN as a machine that takes
    in something concrete (e.g., word embeddings) and extracts some abstract concepts
    (e.g., scores for POS tags), you can expect that, by repeating this process, RNNs
    are able to extract increasingly more abstract concepts as the number of layers
    increases. Although not fully theoretically proven, many real-world NLP applications
    use multilayer RNNs. For example, Google’s Neural Machine Translation (NMT) system
    uses a stacked RNN consisting of eight layers for both the encoder and the decoder
    ([http://realworldnlpbook.com/ch5.html#nmt-paper](http://realworldnlpbook.com/ch5.html#nmt-paper)).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这是一个好主意呢？如果你将 RNN 的一层看作是一个接受具体输入（例如，单词嵌入）并提取一些抽象概念（例如，POS 标签的得分）的机器，你可以期望，通过重复这个过程，RNN
    能够随着层数的增加提取越来越抽象的概念。尽管没有完全经过理论证明，但许多真实世界的 NLP 应用都使用了多层 RNNs。例如，谷歌的神经机器翻译（NMT）系统使用了一个包括八层编码器和解码器的堆叠
    RNN（[http://realworldnlpbook.com/ch5.html#nmt-paper](http://realworldnlpbook.com/ch5.html#nmt-paper)）。
- en: 'To use multilayer RNNs in your NLP application, all you need to do is change
    how the encoder is initialized. Specifically, you need to specify only the number
    of layers using the num_layers parameter, as shown in the next code snippet, and
    AllenNLP makes sure that the rest of the training pipeline works as-is:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 要在你的 NLP 应用中使用多层 RNNs，你需要做的只是改变编码器的初始化方式。具体来说，你只需要使用 num_layers 参数指定层数，就像下一个代码片段中所示的那样，而
    AllenNLP 会确保训练管道的其余部分按原样工作：
- en: '[PRE12]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: If you change this line and rerun the POS tagger training pipeline, you will
    notice that accuracy on the validation set is almost unchanged or slightly lower
    than the previous model with a single-layer RNN. This is not surprising—information
    required for POS tagging is mostly superficial, such as the identity of the word
    being tagged and neighboring words. Very rarely does it require deep understanding
    of the input sentence. On the other hand, adding layers to an RNN is not without
    additional cost. It slows down the training and inference and increases the number
    of parameters, which makes it susceptible to overfitting. For this small experiment,
    adding layers to the RNN seems to do more harm than good. When you change the
    structure of the network, always remember to verify its effect on a validation
    set.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你更改了这一行并重新运行 POS 标记器训练管道，你会注意到在验证集上的准确率几乎没有变化，或者略低于前一个单层 RNN 模型。这并不奇怪——进行
    POS 标记所需的信息大多是表面的，比如被标记的单词的身份和相邻单词。很少情况下需要深入理解输入句子。另一方面，向 RNN 添加层并非没有额外的成本。它会减慢训练和推断的速度，并增加参数的数量，从而使其容易过拟合。对于这个小实验来说，向
    RNN 添加层似乎弊大于利。当你改变网络的结构时，一定要记得验证其对验证集的影响。
- en: 5.3.2 Bidirectional RNNs
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.2 双向 RNNs
- en: So far, we’ve been feeding words to RNNs as they come in—from the beginning
    of the sentence to the end. This means that when an RNN is processing a word,
    it can leverage only the information it has encountered so far, which is the word’s
    left context. True, you can get a lot of information from a word’s left context.
    For example, if a word is preceded by a modal verb (e.g., “can”), it is a strong
    signal that the next word is a verb. However, the right context holds a lot of
    information as well. For example, if you know that the next word is a determiner
    (e.g., “a”), it is a strong signal that “book” on its left is a verb, not a noun.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直将单词逐个输入 RNN——从句子的开头到结尾。这意味着当 RNN 处理一个单词时，它只能利用到目前为止遇到的信息，也就是单词的左侧上下文。当然，你可以从单词的左侧上下文中获得很多信息。例如，如果一个单词前面是情态动词（例如，“can”），那么下一个单词是动词的信号就很强烈。然而，右侧上下文也包含了很多信息。例如，如果你知道下一个单词是限定词（例如，“a”），那么左侧的“book”是一个动词，而不是名词的信号就很强烈。
- en: Bidirectional RNNs (or simply biRNNs) solve this problem by combining two RNNs
    with opposite directions. A forward RNN is a forward-facing RNN that we’ve been
    using so far in this book—it scans the input sentence left to right and uses the
    input word and all the information on its left to update the state. A backward
    RNN, on the other hand, scans the input sentence right to left. It uses the input
    word and all the information on its right to update the state. This is equivalent
    to flipping the order of the input sentence and feeding it to a forward RNN. The
    final hidden states produced by biRNNs are concatenations of hidden states from
    the forward and backward RNNs. See figure 5.9 for an illustration.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 双向 RNN（或简称双向RNN）通过组合两个方向相反的RNN来解决这个问题。前向RNN是我们在本书中一直使用的正向RNN，它从左到右扫描输入句子，并使用输入词和在其左侧所有信息来更新状态。而反向RNN则按相反的方向扫描输入句子。它使用输入词和在其右侧所有信息来更新状态。这相当于翻转输入句子的顺序并将其馈送给前向RNN。双向RNN产生的最终隐藏状态是来自前向和后向RNN的隐藏状态的连接。详见图
    5.9。
- en: '![CH05_F09_Hagiwara](../Images/CH05_F09_Hagiwara.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F09_Hagiwara](../Images/CH05_F09_Hagiwara.png)'
- en: Figure 5.9 Bidirectional RNN
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.9 双向 RNN
- en: Let’s use a concrete example to illustrate this. Assume the input sentence is
    “time flies like an arrow” and you’d like to know the POS tag for the word “like”
    in the middle of this sentence. The forward RNN processes “time” and “flies,”
    and by the time it reaches “like,” its internal state (A in figure 5.9) encodes
    all the information about “time flies like.” Similarly, the backward RNN processes
    “arrow” and “an,” and by the time it reaches “like,” the internal state (B in
    figure 5.9) has encoded all the information about “like an arrow.” The internal
    state from the biRNN for “like” is the concatenation of these two states (A +
    B). You literally stitch together two vectors—no mathematical operations involved.
    As a result, the internal state for “like” encodes all the information from the
    entire sentence. This is a great improvement over just knowing half the sentence!
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用具体的例子来说明。假设输入句子是“time flies like an arrow”，你想知道这个句子中间的单词“like”的词性标注。前向RNN处理“time”和“flies”，到达“like”时，它的内部状态（图5.9中的A）编码了关于“time
    flies like”所有的信息。同样地，反向RNN处理“arrow”和“an”，到达“like”时，它的内部状态（图5.9中的B）编码了关于“like an
    arrow”的所有信息。双向RNN的“like”的内部状态是这两个状态（A + B）的连接。您只需将两个向量连接在一起——不需要进行数学运算。因此，“like”的内部状态编码了整个句子的信息。这比只知道句子的一半要好得多！
- en: 'Implementing a biRNN is similarly easy—you just need to add the bidirectional=True
    flag when initializing the RNN as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 实现双向 RNN 同样容易——您只需要在初始化 RNN 时添加 bidirectional=True 标志，如下所示：
- en: '[PRE13]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: If you train the POS tagger with this change, the validation set accuracy will
    jump from ~88% to 91%. This implies that incorporating the information on both
    sides of the word is effective for POS tagging.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用这个变化来训练 POS 标注器，验证集的准确率将从 ~88% 跳升到91%。这意味着将词的两侧信息结合起来对于 POS 标注是有效的。
- en: Note that you can combine the two techniques introduced in this section by stacking
    bidirectional RNNs. The output from one layer of biRNN (concatenation of a forward
    and a backward layer) becomes the input to another layer of biRNN (see figure
    5.10). You can implement this by specifying both flags—num_layers and bidirectional—when
    initializing the RNN in PyTorch/AllenNLP.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您可以通过堆叠双向 RNN 来结合本节介绍的两种技术。双向 RNN 的一层输出（由前向和后向层连接）成为另一层双向 RNN 的输入（见图5.10）。您可以在初始化
    PyTorch/AllenNLP 中的 RNN 时指定 num_layers 和 bidirectional 两个标志来实现此目的。
- en: '![CH05_F10_Hagiwara](../Images/CH05_F10_Hagiwara.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F10_Hagiwara](../Images/CH05_F10_Hagiwara.png)'
- en: Figure 5.10 Two-layered bidirectional RNN
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.10 两层双向 RNN
- en: 5.4 Named entity recognition
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4 命名实体识别
- en: Sequential labeling can be applied to many information extraction tasks, not
    just to part-of-speech tagging. In this section, I’ll introduce the task of named
    entity recognition (NER) and demonstrate how to build an NER tagger using sequential
    labeling. The code for this section can be viewed and executed via the Google
    Colab platform ([http://realworldnlpbook.com/ch5#ner-nb](http://realworldnlpbook.com/ch5#ner-nb)).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 序列标注可以应用于许多信息提取任务，不仅仅是词性标注。在本节中，我将介绍命名实体识别（NER）的任务，并演示如何使用序列标注构建一个 NER 标注器。此部分的代码可以通过
    Google Colab 平台查看和执行（[http://realworldnlpbook.com/ch5#ner-nb](http://realworldnlpbook.com/ch5#ner-nb)）。
- en: 5.4.1 What is named entity recognition?
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.1 什么是命名实体识别？
- en: 'As mentioned earlier, named entities are mentions of real-world entities such
    as proper nouns. Common named entities that are usually covered by NER systems
    include the following:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面提到的，命名实体是对现实世界实体的提及，如专有名词。通常由NER系统覆盖的常见命名实体包括以下内容：
- en: 'Personal name (PER): Alan Turing, Lady Gaga, Elon Musk'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 个人姓名（PER）：艾伦·图灵、Lady Gaga、埃隆·马斯克
- en: 'Organization (ORG): Google, United Nations, Giants'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组织（ORG）：谷歌、联合国、巨人
- en: 'Location (LOC): Mount Rainer, Bali Island, Nile'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置（LOC）：雷尼尔山、巴厘岛、尼罗河
- en: 'Geopolitical entity (GPE): UK, San Francisco, Southeast Asia'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 地理政治实体（GPE）：英国、旧金山、东南亚
- en: However, different NER systems deal with different sets of named entities. The
    concept of named entities is a bit overloaded in NLP to mean any mentions that
    are of interest to the application’s user. For example, in the medical domain,
    you may want to extract mentions to names of drugs and chemical compounds. In
    the financial domain, companies, products, and stock symbols may be of interest.
    In many domains, numerical and temporal expressions are also considered.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，不同的NER系统处理不同的命名实体集合。在NLP中，命名实体的概念有点过载，意味着任何对应用程序用户感兴趣的提及。例如，在医学领域，你可能想提取药品和化学化合物的名称提及。在金融领域，公司、产品和股票符号可能是感兴趣的。在许多领域，数字和时间表达式也被视为命名实体。
- en: 'Identifying named entities is in itself important, because named entities (who,
    what, where, when, and so on) are often what most people are interested in. But
    NER is also an important first step for many other NLP applications. One such
    task is *relation extraction*: extracting all relations between named entities
    from the given document. For example, given a press release document, you may
    want to extract an event described in the release, such as which company purchased
    which other company for what price. This often assumes that all the parties are
    already identified via NER. Another task that is closely related to NER is *entity
    linking*, where mentions of named entities are linked to some knowledge base,
    such as Wikipedia. When Wikipedia is used as a knowledge base, entity linking
    is also called *Wikification*.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 识别命名实体本身就很重要，因为命名实体（谁、什么、在哪里、什么时候等）通常是大多数人感兴趣的。但是NER也是许多其他自然语言处理应用的重要第一步。一个这样的任务是*关系抽取*：从给定的文档中提取所有命名实体之间的关系。例如，给定一个新闻稿件，你可能想提取出其中描述的事件，比如哪家公司以什么价格收购了哪家其他公司。这通常假设所有各方都已通过NER识别。与NER密切相关的另一个任务是*实体链接*，其中命名实体的提及与某些知识库（如维基百科）相关联。当维基百科被用作知识库时，实体链接也称为*维基化*。
- en: But you may be wondering, what’s so difficult about simply extracting named
    entities? If they are just proper nouns, can you simply compile a dictionary of,
    say, all the celebrities (or all the countries, or whatever you are interested
    in) and use it? The idea is, whenever the system encounters a noun, it would run
    the name through this dictionary and tag the mention if it appears in it. Such
    dictionaries are called *gazetteers*, and many NER systems do use them as a component.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 但是你可能会想，仅仅提取命名实体有什么难的？如果它们只是专有名词，你可以简单地编制一个字典，比如所有的名人（或所有的国家，或你感兴趣的任何东西），然后使用它吗？这个想法是，每当系统遇到一个名词，它就会通过这个字典，并标记出现在其中的提及。这样的字典称为*地名词典*，许多NER系统确实使用它们作为一个组件。
- en: 'However, relying solely on such dictionaries has one major issue—*ambiguity*.
    Earlier we saw that a single word type could have multiple parts of speech (e.g.,
    “book” as a noun and a verb), and named entities are no exception. For example,
    “Georgia” can be the name of a country, a US state, towns and communities across
    the United States (Georgia, Indiana; Georgia, Nebraska), a film, a number of songs,
    ships, and a personal name. Simple words like “book” could also be named entities,
    including: Book (a community in Louisiana), Book/Books (a surname), The Books
    (an American band), and so on. Simply matching mentions against dictionaries would
    tell you nothing about their identities if they are ambiguous.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，仅仅依靠这样的字典有一个主要问题——*歧义性*。前面我们看到一个单词类型可能有多个词性（例如，“book”既是名词又是动词），命名实体也不例外。例如，“Georgia”可以是一个国家的名字，也可以是美国的一个州，跨越美国的城镇和社区（乔治亚州，印第安纳州；乔治亚州，内布拉斯加州），一部电影，几首歌曲，船只和一个人名。像“book”这样的简单单词也可能是命名实体，包括：Book（路易斯安那州的一个社区），Book/Books（一个姓氏），The
    Books（一个美国乐队）等。如果它们是模糊的，简单地将提及与字典进行匹配将告诉你它们的身份。
- en: Fortunately, sentences often offer clues that can be used to *disambiguate*
    the mentions. For example, if the sentence reads “I live in Georgia,” it’s usually
    a strong signal that “Georgia” is a name of a place, not a film or a person’s
    name. NER systems use a combination of signals about the mentions themselves (e.g.,
    whether they are in a predefined dictionary) and about their context (whether
    they are preceded or followed by certain words) to determine their tags.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，句子通常提供了可以用于*消歧*提及的线索。 例如，如果句子中读到“我住在乔治亚州”，通常是“乔治亚州”是地点名称，而不是电影或人名的强烈信号。
    NER 系统使用关于提及本身的信号（例如，它们是否在预定义字典中）以及关于它们上下文的信号（它们是否由某些词先导或跟随）的组合来确定它们的标记。
- en: 5.4.2 Tagging spans
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.2 标记跨度
- en: Unlike POS tagging, where each word is assigned a POS tag, mentions to named
    entities can span over more than one word, for example, “the United States” and
    “World Trade Organization.” A *span* in NLP is simply a range over one or more
    contiguous words. How can we use the same sequential tagging framework to model
    spans?
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 与词性标注不同，指向命名实体的提及可以跨越多个词，例如，“美国”和“世界贸易组织”。 在 NLP 中，*跨度* 只是一个或多个连续词的范围。 我们如何使用相同的序列标记框架来建模跨度？
- en: 'A common practice in NLP is to use some form of encoding to convert spans into
    per-word tags. The most common encoding scheme used in NER is called *IOB2 tagging*.
    It represents spans by a combination of the positional tag and the category tag.
    Three types of positional tags follow:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: NLP 中的一个常见做法是使用某种形式的编码将跨度转换为每个词的标记。 NER 中最常用的编码方案称为*IOB2标记*。 它通过位置标记和类别标记的组合来表示跨度。
    以下是三种类型的位置标记：
- en: 'B (Beginning): assigned to the first (or the only) token of a span'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: B（Beginning）：分配给跨度的第一个（或唯一的）标记
- en: 'I (Inside): assigned to all but the first token of a span'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我（Inside）：分配给跨度的所有标记的第一个标记之外的所有标记
- en: 'O (Outside): assigned to all words outside of any spans'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: O（Outside）：分配给任何跨度之外的所有单词
- en: Now, let’s take a look at the NER example we saw earlier and is shown in figure
    5.11\. The token “Apple” is the first (and the only) token of ORG (for “organization”),
    and it is assigned a B-ORG tag. Similarly, “UK”, the first and the only token
    of GPE (for “geopolitical entity”), is assigned B-GPE. For “$1” and “billion,”
    the first and the second tokens of a monetary expression (MONEY), B-MONEY and
    I-MONEY are assigned, respectively. All the other tokens are given O.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下之前看到的 NER 示例，并显示在图 5.11 中。 标记“Apple”是 ORG（表示“组织”）的第一个（也是唯一的）标记，并分配了一个
    B-ORG 标记。 类似地，“UK”是 GPE（表示“地缘政治实体”）的第一个和唯一的标记，并分配了 B-GPE。 对于“$1”和“billion”，表示货币表达式（MONEY）的第一个和第二个标记，分别分配了
    B-MONEY 和 I-MONEY。 所有其他标记都被赋予 O。
- en: '![CH05_F11_Hagiwara](../Images/CH05_F11_Hagiwara.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F11_Hagiwara](../Images/CH05_F11_Hagiwara.png)'
- en: Figure 5.11 Named entity recognition (NER) using sequential labeling
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11 命名实体识别（NER）使用序列标记
- en: 'The rest of the pipeline for solving NER is very similar to that of part-of-speech
    tagging: both are concerned with assigning an appropriate tag for each word and
    can be solved by RNNs. In the next section, we are going to build a simple NER
    system using neural networks.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 解决 NER 的其余管道与解决词性标注非常相似：两者都涉及为每个词分配适当的标记，并且可以通过 RNN 解决。 在接下来的部分中，我们将使用神经网络构建一个简单的
    NER 系统。
- en: 5.4.3 Implementing a named entity recognizer
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.3 实现命名实体识别器
- en: 'To build an NER system, we use the Annotated Corpus for Named Entity Recognition
    prepared by Abhinav Walia published on Kaggle ([http://realworldnlpbook.com/ch5.html#ner-data](http://realworldnlpbook.com/ch5.html#ner-data)).
    In what follows, I’m going to assume that you downloaded and expanded the dataset
    under data/entity-annotated-corpus. Alternatively, you can use the copy of the
    dataset I uploaded to S3 ([http://realworldnlpbook.com/ch5.html#ner-data-s3](http://realworldnlpbook.com/ch5.html#ner-data-s3)),
    which is what the following code does. I wrote a dataset reader for this dataset
    ([http://realworldnlpbook.com/ch5.html#ner-reader](http://realworldnlpbook.com/ch5.html#ner-reader)),
    so you can simply import (or copy and paste) it and use it:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建一个 NER 系统，我们使用由 Abhinav Walia 准备的命名实体识别注释语料库，该语料库已在 Kaggle 上发布（[http://realworldnlpbook.com/ch5.html#ner-data](http://realworldnlpbook.com/ch5.html#ner-data)）。
    在接下来的内容中，我假设您已经下载并展开了数据集，并将其放置在 data/entity-annotated-corpus 下。 或者，您可以使用我上传到 S3
    的数据集的副本（[http://realworldnlpbook.com/ch5.html#ner-data-s3](http://realworldnlpbook.com/ch5.html#ner-data-s3)），这就是以下代码所做的事情。
    我为这个数据集编写了一个数据集读取器（[http://realworldnlpbook.com/ch5.html#ner-reader](http://realworldnlpbook.com/ch5.html#ner-reader)），所以您只需导入（或复制粘贴）它并使用它：
- en: '[PRE14]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Because the dataset is not separated into train, validation, and test sets,
    the dataset reader will separate it into train and validation splits for you.
    All you need to do is specify which split you want when you initialize data loaders,
    as shown here:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据集没有分为训练集、验证集和测试集，数据集读取器将为您分离为训练集和验证集。您所需要做的就是在初始化数据加载器时指定您想要的分割方式，如下所示：
- en: '[PRE15]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The RNN-based sequential tagging model and the rest of the training pipeline
    look almost the same as the previous example (POS tagger). The only difference
    is how we evaluate our NER model. Because most of the tags for a typical NER dataset
    are simply “O,” using tag accuracy is misleading—a stupid system that tags everything
    “O” would achieve very high accuracy. Instead, NER is usually evaluated as an
    information extraction task, where the goal is to extract named entities from
    texts, not just to tag them. We’d like to evaluate NER systems based on the “cleanness”
    of retrieved named entities (how many of them are actual entities) and their “completeness”
    (how many of actual entities the system was able to retrieve). Does any of this
    sound familiar to you? Yes, these are the definition of recall and precision we
    talked about in section 4.3\. Because there are usually multiple types of named
    entities, these metrics (precision, recall, and F1-measure) are computed per entity
    type.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 基于RNN的顺序标注模型和训练流程的其余部分与以前的示例（词性标注器）几乎相同。唯一的区别在于我们如何评估我们的NER模型。因为典型的NER数据集中大多数标签只是“O”，使用标签准确度很容易误导
    —— 一个将所有东西标记为“O”的愚蠢系统可以获得非常高的准确度。相反，NER通常被评估为一项信息提取任务，其目标是从文本中提取命名实体，而不仅仅是标记它们。我们希望基于检索到的命名实体的“干净程度”（有多少是实际实体）和“完整程度”（系统能够检索到多少实际实体）来评估NER系统。这些听起来熟悉吗？是的，这些就是我们在第4.3节中讨论过的召回率和精确度的定义。由于命名实体通常有多种类型，因此这些指标（精确度、召回率和F1度量）是按实体类型计算的。
- en: NOTE If these metrics are computed while ignoring entity types, it’s called
    a *micro average*. For example, the micro-averaged precision is the total number
    of true positives of all types divided by the total number of retrieved named
    entities regardless of the type. On the other hand, if these metrics are computed
    per entity type and are then averaged, it’s called a *macro average*. For example,
    if the precision for PER and GPE is 80% and 90%, respectively, its macro average
    is 85%. What AllenNLP computes in the following is the micro average.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果在计算这些指标时忽略实体类型，它被称为*总体平均*。例如，总体平均的精确度是所有类型的真阳性总数除以检索到的命名实体总数，而不管类型如何。另一方面，如果按实体类型计算这些指标，然后对它们进行平均，它被称为*宏平均*。例如，如果PER和GPE的精确度分别为80%和90%，则它的宏平均为85%。接下来，AllenNLP所计算的是总体平均。
- en: 'AllenNLP implements SpanBasedF1Measure, which computes per-type metrics (precision,
    recall, and F1-measure) as well as the average. You can define the metric in __init__()
    of your model as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: AllenNLP实现了SpanBasedF1Measure，它计算每个类型的指标（精确度、召回率和F1度量），以及平均值。你可以在你的模型的__init__()方法中定义这个指标，如下所示：
- en: '[PRE16]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'And use it to get metrics during training and validation, as shown next:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 并使用它在训练和验证过程中获得指标，如下所示：
- en: '[PRE17]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: If you run this training pipeline, you get an accuracy around 0.97, and precision,
    recall, F1-measure will all hover around 0.83\. You can also use the predict()
    method to obtain named entity tags for an unseen sentence as
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行这个训练流程，你会得到大约0.97的准确度，而精确度、召回率和F1度量将都在0.83左右。你还可以使用`predict()`方法来获得未见过的句子的命名实体标签，如下所示：
- en: '[PRE18]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'which produces the following:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 它将生成如下结果：
- en: '[PRE19]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This is not perfect—the NER tagger got the first named entity (“Apple”) correct
    but missed two others (“UK” and “$1 billion”). If you look at the training data,
    the mention “UK” never appears, and no monetary values are tagged. It is not surprising
    that the system is struggling to tag entities that it has never seen before. In
    NLP (and also machine learning in general), the characteristic of the test instances
    needs to match that of the train data for the model to be fully effective.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不完美 —— NER标注器正确获取了第一个命名实体（“Apple”），但错过了另外两个（“UK”和“10亿美元”）。如果你查看训练数据，你会发现提及“UK”的情况从未出现过，而且没有标注货币值。系统难以标记它从未见过的实体是毫不奇怪的。在自然语言处理（以及机器学习一般）中，测试实例的特征需要与训练数据匹配，才能使模型完全有效。
- en: 5.5 Modeling a language
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.5 语言建模
- en: In this section, I’ll switch gears a little bit and introduce *language models*,
    which is one of the most important concepts in NLP. We’ll discuss what they are,
    why they are important, and how to train them using the neural network components
    we’ve introduced so far.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我将稍微转换一下方向，介绍*语言模型*，这是自然语言处理中最重要的概念之一。我们将讨论它们是什么，它们为什么重要，以及如何使用我们迄今介绍的神经网络组件来训练它们。
- en: 5.5.1 What is a language model?
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5.1 什么是语言模型？
- en: 'Imagine you are asked to predict what word comes next given a partial sentence:
    “My trip to the beach was ruined by bad ___.” What words could come next? Many
    things could ruin a trip to a beach, but most likely it’s bad weather. Maybe it’s
    bad-mannered people at the beach, or maybe it’s bad food that the person had eaten
    before the trip, but most would agree that “weather” is a likely word that comes
    after this partial sentence. Few other nouns (*people*, *food*, *dogs*) and words
    of other parts of speech (*be*, *the*, *run*, *green*) are as appropriate as “weather”
    in this context.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你被要求预测接下来的单词是什么，给出一个部分句子：“My trip to the beach was ruined by bad ___。”接下来可能是什么词？许多事情都可能毁了一次海滩之行，但最有可能的是天气不好。也许是海滩上的没礼貌的人，或者可能是这个人在旅行前吃的不好的食物，但大多数人会同意在这个部分句子之后跟着“weather”是一个可能性很高的词。在这种情况下，很少有其他名词（*people*，*food*，*dogs*）和其他词性的词（*be*，*the*，*run*，*green*）与“weather”一样合适。
- en: What you just did is to assign some belief (or probability) to an English sentence.
    You just compared several alternatives and judged how likely they are as English
    sentences. Most people would agree that the probability of “My trip to the beach
    was ruined by bad weather” is a lot higher than “My trip to the beach was ruined
    by bad dogs.”
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 刚才你所做的是为一个英文句子分配一些信念（或概率）。你刚刚比较了几个替代方案，并判断它们作为英文句子的可能性有多大。大多数人都会同意，“My trip
    to the beach was ruined by bad weather”的概率远远高于“My trip to the beach was ruined
    by bad dogs”。
- en: Formally, a *language model* is a statistical model that gives a probability
    to a piece of text. An English language model would assign higher probabilities
    to sentences that look like English. For example, an English language model would
    give a higher probability to “My trip to the beach was ruined by bad weather”
    than it does to “My trip to the beach was ruined by bad dogs” or even “by weather
    was trip my bad beach the ruined to.” The more grammatical and the more “sense”
    the sentence makes, the higher the probability is.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，*语言模型*是一种给出文本片段概率的统计模型。一个英文语言模型会为看起来像英文的句子分配较高的概率。例如，一个英文语言模型会给“My trip
    to the beach was ruined by bad weather”比给“My trip to the beach was ruined by bad
    dogs”或甚至“by weather was trip my bad beach the ruined to.”更高的概率。句子的语法越好，越有“意义”，概率就越高。
- en: 5.5.2 Why are language models useful?
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5.2 语言模型有什么用处？
- en: You may be wondering what use such a statistical model has. Although predicting
    the next word might come in handy when you are answering fill-in-the-blank questions
    for an exam, what particular roles do language models play in NLP?
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道这样一个统计模型有什么用。虽然在回答填空题时预测下一个单词可能会派上用场，但语言模型在自然语言处理中扮演着什么特殊的角色呢？
- en: The answer is, it is essential for any systems that generate natural language.
    For example, machine translation systems, which generate a sentence in a language
    given a sentence in another language, would benefit greatly from high-quality
    language models. Why? Let’s say we’d like to translate a Spanish sentence “Está
    lloviendo fuerte” into English (“It is raining hard”). The last word “fuerte”
    has several English equivalents—*strong*, *sharp*, *loud*, *heavy*, and so on.
    How would you determine which English equivalent is the most appropriate in this
    context? There could be many approaches to solve this problem, but one of the
    simplest is to use an English language model and rerank several different translation
    candidates. Assuming you’ve finished translating up to “It is raining,” you would
    simply replace the word “fuerte” with all the equivalents you can find in a Spanish-English
    dictionary, which generates “It is raining strong,” “It is raining sharp,” “It
    is raining loud,” “It is raining hard.” Then all you need to do is ask the language
    model which one of these candidates has the highest probability.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: NOTE In fact, neural machine translation models can be thought of as a variation
    of a language model that generates sentences in the target language conditioned
    on its input (sentences in the source language). Such a language model is a called
    a *conditional language model* as opposed to an *unconditional language model*,
    which we discuss here. We’ll discuss machine translation models in chapter 6.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: A similar situation arises in speech recognition, too, which is another task
    that generates text given spoken audio input. For example, if somebody uttered
    “You’re right,” how would a speech recognition system know it’s actually “you’re
    right?” Because “you’re” and “your” can have the same pronunciation, and so can
    “right” and “write” and even “Wright” and “rite,” the system output could be any
    one of “You’re write,” “You’re Wright,” “You’re rite,” “Your right,” “Your write,”
    “Your Wright,” and so on. Again, the simplest approach to resolving this ambiguity
    is to use a language model. An English language model would properly rerank these
    candidates and determine “you’re right” is the most likely transcription.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: In fact, humans do this type of disambiguation all the time, though unconsciously.
    When you are having a conversation with somebody else at a large party, the actual
    audio signal you receive is often very noisy. Most people can still understand
    each other without any issues because people’s language models help them “correct”
    what you hear and interpolate any missing parts. You’ll notice this most if you
    try to converse in a less proficient, second language—you’d have a lot harder
    time understanding the other person in a noisy environment, because your language
    model is not as good as your first language’s.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 5.5.3 Training an RNN language model
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At this point, you may be wondering what the connection is between predicting
    the next word and assigning a probability to a sentence. These two are actually
    equivalent. Instead of explaining the theory behind it, which requires you to
    understand some math (especially probability theory), I’ll attempt an intuitive
    example next without going into mathematical details.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine you want to estimate the chance of tomorrow’s weather being rainy and
    the ground wet. Let’s simplify this and assume there are only two types of weather,
    sunny and rainy. There are only two outcomes for the ground: dry or wet. This
    is equivalent to estimating the probably of a sequence: [rain, wet].'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Further assume that there’s a 50-50 chance of rain on a given day. After raining,
    the ground is wet with a 90% chance. Then, what is the probability of the rain
    and the ground being wet? It’s simply 50% times 90%, which is 45%, or 0.45\. If
    we know the probability of one event happening after another, you can simply multiply
    two probabilities to get the total probability for the sequence. This is called
    the *chain rule* in probability theory.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, if you can correctly estimate the probability of one word occurring
    after a partial sentence, you can simply multiply it with the probability of the
    partial sentence. Starting from the first word, you can keep doing this until
    you reach the end of the sentence. For example, if you’d like to compute the probability
    for “The trip to the beach was . . . ,” you can multiply the following:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: The probability of “The” occurring at the beginning of a sentence
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The probability of “trip” occurring after “The”
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The probability of “to” occurring after “The trip”
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The probability of “the” occurring after “The trip to”
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And so on
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This means that to build a language model, you need a model that predicts the
    probability (or, more precisely, the probability distribution) of the next word
    given the context. You may have noticed that this sounds a little familiar. Indeed,
    what’s done here is very similar to the sequential-labeling models that we’ve
    been talking about in this chapter. For example, a part-of-speech (POS) tagging
    model predicts the probability distribution over the possible POS tags given the
    context. A named entity recognition (NER) model does it for the possible named
    entity tags. The difference is that a language model does it for the possible
    next words, given what the model has encountered so far. Hopefully it’s starting
    to make some sense why I talk about language models in this chapter!
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: In summary, to build a language model, you tweak an RNN-based sequence-labeling
    model a little bit so that it gives the estimates for the next word, instead of
    POS or NER tags. In chapter 3, I talked about the Skip-gram model, which predicts
    the words in a context given the target word. Notice the similarity here—both
    models predict the probability over possible words. The input to the Skip-gram
    model is just a single word, whereas the input to the language model is the partial
    sequence. You can use a similar mechanism for converting one vector to another
    using a linear layer, then converting it to a probability distribution using softmax,
    as we discussed in chapter 3\. The architecture is shown in figure 5.12.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '![CH05_F12_Hagiwara](../Images/CH05_F12_Hagiwara.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
- en: Figure 5.12 Architecture of RNN-based language model
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: The way RNN-based language models are trained is similar to other sequential-labeling
    models. The loss function we use is the sequential cross-entropy loss, which measures
    how “off” the predicted words are from actual words. The cross-entropy loss is
    computed per word and averaged over all words in the sentence.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 5.6 Text generation using RNNs
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We saw that language models give probabilities to natural language sentences.
    But the more fun part is you can generate natural language sentences from scratch
    using a language model! In the final section of this chapter, we are going to
    build a language model. You can use the trained model to evaluate and generate
    English sentences. You can find the entire script for this subsection on a Google
    Colab notebook ([http://realworldnlpbook.com/ch5.html#lm-nb](http://realworldnlpbook.com/ch5.html#lm-nb)).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 5.6.1 Feeding characters to an RNN
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the first half of this section, we are going to build an English language
    model and train it using a generic English corpus. Before we start, we note that
    the RNN language model we build in this chapter operates on *characters*, not
    on words or tokens. All the RNN models we’ve seen so far operate on words, which
    means the input to the RNN was always sequences of words. On the other hand, the
    RNN we are going to use in this section takes sequences of characters as the input.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: In theory, RNNs can operate on sequences of anything, be it tokens or characters
    or something completely different (e.g., waveform for speech recognition), as
    long as they are something that can be turned into vectors. In building language
    models, we often feed characters, even including whitespace and punctuations as
    the input, treating them as words of length 1\. The rest of the model works exactly
    the same—individual characters are first embedded (converted to vectors) and then
    fed into the RNN, which is in turn trained so that it can best predict the distribution
    over the characters that are likely to come next.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: You have a couple of considerations when you are deciding whether you should
    feed words or characters to an RNN. Using characters will definitely make the
    RNN less efficient, meaning that it would need more computation to “figure out”
    the same concept. For example, a word-based RNN can receive the word “dog” at
    a timestep and update its internal states, whereas a character-based RNN would
    not able to do it until it receives three elements *d*, *o*, and *g*, and probably
    “_” (whitespace). A character-based RNN needs to “learn” that a sequence of these
    three characters means something special (the concept of “dog”).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, by feeding characters to RNNs, you can bypass many issues
    arising from dealing with tokens. One such issue is related to out-of-vocabulary
    (or OOV) words. When training a word-based RNN, you usually fix the entire set
    of vocabulary, often by enumerating all words that appeared in the train set.
    But whenever it encounters an OOV word in the test set, it doesn’t know what to
    do with it. Oftentimes, it assigns a special token <UNK> to all OOV words and
    treats them in the same way, which is not ideal. A character-based RNN, on the
    contrary, can still operate on individual characters, so it may be able to figure
    out what “doggy” means, for example, based on the rules it has learned by observing
    “dog” in the train set, even though it has never seen the exact word “doggy” before.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 5.6.2 Evaluating text using a language model
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s start building a character-based language model. The first step is to
    read a plain text dataset file and generate instances for training the model.
    I’m going to show how to construct an instance without using a dataset reader
    for a demonstration purpose. Suppose you have a Python string object text that
    you’d like to turn into an instance for training a language model. First you need
    to segment it into characters using CharacterTokenizer as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Note that tokens here is a list of Token objects. Each Token object contains
    a single character, instead of a single word. Then you insert the <START> and
    <END> symbols at the beginning and at the end of the list as shown next:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Inserting special symbols like these at the beginning and end of each sentence
    is a common practice in NLP. With these symbols, models can distinguish between
    occurrences of a token in the middle of a sentence versus at the beginning/end
    of a sentence. For example, a period is a lot more likely to occur at the end
    of a sentence (“. <END>”) than the beginning (“<START> .”), to which a language
    model can give two very different probabilities, which is impossible to do without
    the use of these symbols.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, you can construct an instance by specifying individual text fields.
    Notice that the “output” of a language model is identical to the input, simply
    shifted by one token, as shown here:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Here token_indexers specifies how individual tokens are mapped into IDs. We
    simply use SingleIdTokenIndexer we’ve been using so far as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Figure 5.13 shows an instance created from this process.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.13 显示了从该过程创建的实例。
- en: '![CH05_F13_Hagiwara](../Images/CH05_F13_Hagiwara.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F13_Hagiwara](../Images/CH05_F13_Hagiwara.png)'
- en: Figure 5.13 Instance for training a language model
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.13 用于训练语言模型的实例
- en: 'The rest of the training pipeline, as well as the model, is very similar to
    that for sequential labeling mentioned earlier in this chapter. See the Colab
    notebook for more details. As shown in the next code snippet, after the model
    is fully trained, you can construct instances from new texts, turn them into instances,
    and compute the loss, which basically measures how successful the model was in
    predicting what comes next:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 训练流程的其余部分以及模型与本章前面提到的顺序标记模型非常相似。有关更多详细信息，请参见 Colab 笔记本。如下面的代码片段所示，在模型完全训练后，你可以从新的文本中构建实例、将它们转化为实例，并计算损失，该损失基本上衡量了模型在预测下一个字符方面的成功程度：
- en: '[PRE24]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The loss here is the cross-entropy loss between the predicted and the expected
    characters. The more “unexpected” the characters there are, the higher the values
    will be, so you can use these values to measure how natural the input is as English
    text. As expected, natural sentences (such as the first one) are given scores
    that are lower than unnatural sentences (such as the last one).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的损失是预测字符与期望字符之间的交叉熵损失。出现较多“不符合预期”的字符，损失值就会越高，因此你可以使用这些值来衡量输入作为英文文本的自然程度。正如预期的那样，自然句子（如第一个句子）得分低于非自然句子（如最后一个句子）。
- en: NOTE If you calculate 2 to the power of the cross entropy, the value is called
    *perplexity*. Given a fixed natural language text, perplexity becomes lower because
    the language model is better at predicting what comes next, so it is commonly
    used for evaluating the quality of language models in the literature.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果你计算交叉熵的 2 的幂，那么这个值就被称为*困惑度*。对于给定的固定自然语言文本，困惑度会降低，因为语言模型在预测下一个字符方面表现更好，所以它通常用于评估文献中的语言模型的质量。
- en: 5.6.3 Generating text using a language model
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6.3 使用语言模型生成文本
- en: The most interesting aspect of (fully trained) language models is that they
    can predict possible characters that may appear next given some context. Specifically,
    they can give you a probability distribution over possible characters that may
    come next, from which you choose to determine the next character. For example,
    if the model has generated “t” and “h,” and the LM is trained on generic English
    text, it would probably assign a high probability on the letter “e,” generating
    common English words including *the*, *they*, *them*, and so on. If you start
    this process from the <START> token and keep doing this until you reach the end
    of the sentence (i.e., by generating <END>), you can generate an English sentence
    from scratch. By the way, this is another reason why tokens such as <START> and
    <END> are useful—you need something to feed to the RNN to kick off the generation,
    and you also need to know when the sentence stops.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: (完全训练好的) 语言模型最有趣的方面在于，它们可以根据给定的一些上下文来预测可能出现的下一个字符。具体而言，它们可以给出可能的下一个字符的概率分布，然后根据该分布选择确定下一个字符。例如，如果模型生成了“t”和“h”，并且
    LM 是基于通用英文文本训练的，它可能会对字母“e”分配较高的概率，生成常见的英文单词，包括 *the*、*they*、*them* 等。如果你从 <START>
    标记开始这个过程，并一直进行下去直到达到句子的结尾（即生成 <END>），你就可以从头开始生成一句英文句子。顺便说一句，这也是为什么像 <START> 和
    <END> 这样的标记很有用——你需要将某些内容输入 RNN 以开始生成，并且你还需要知道句子何时结束。
- en: 'Let’s look at this process in a Python-like pseudocode next:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在下面的类似 Python 代码的伪代码中看一下这个过程：
- en: '[PRE25]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This loop looks very similar to the one for updating RNNs with one key difference:
    here, we are not receiving any input but instead are generating characters and
    feeding them as the input. In other words, the RNN operates on the sequence of
    characters that the RNN itself generated so far. Such models that operate on past
    sequences they produced are called *autoregressive models*. See figure 5.14 for
    an illustration of this.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这个循环看起来与更新 RNNs 的循环非常相似，但有一个关键区别：在这里，我们不接收任何输入，而是生成字符并将它们作为输入。换句话说，RNN 的操作对象是
    RNN 自己迄今为止生成的字符序列。这种在其自身生成的过去序列上操作的模型称为 *自回归模型*。有关此过程的示例，请参见图 5.14。
- en: '![CH05_F14_Hagiwara](../Images/CH05_F14_Hagiwara.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F14_Hagiwara](../Images/CH05_F14_Hagiwara.png)'
- en: Figure 5.14 Generating text using an RNN
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.14 使用 RNN 生成文本
- en: 'In the previous code snippet, init_state() and update() functions are the ones
    that initialize and update the hidden states of the RNN, as we’ve seen earlier.
    In generating text, we assume that the model and its parameters are already trained
    on a large amount of natural language text. softmax() is a function to run Softmax
    on the given vector, and linear() is the linear layer to expand/shrink the size
    of the vector. The function sample() returns a character according to the given
    probability distribution. For example, if the distribution is “a”: 0.6, “b”: 0.3,
    “c”: 0.1, it will choose “a” 60% of the time, “b” 30% of the time, and “c” 10%
    of the time. This ensures that the generated string is different every time while
    every string is likely to look like a real English sentence.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个代码段中，init_state()和update()函数是初始化和更新RNN隐藏状态的函数，正如我们之前所见。 在生成文本时，我们假设模型及其参数已经训练好了大量的自然语言文本。softmax()函数是在给定向量上运行Softmax的函数，而linear()是扩展/缩小向量大小的线性层。sample()函数根据给定的概率分布返回一个字符。例如，如果分布是“a”：0.6，“b”：0.3，“c”：0.1，则会在60％的时间内选择“a”，30％的时间选择“b”，10％的时间选择“c”。这确保生成的字符串每次都不同，同时每个字符串看起来都像是英语句子。
- en: NOTE You can use PyTorch’s torch.multinomial() for sampling an element from
    a probability distribution.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，您可以使用PyTorch的torch.multinomial()从概率分布中进行抽样。
- en: 'If you train this language model using the English sentences from Tatoeba and
    generate sentences according to this algorithm, the system will produce something
    similar to the following cherry-picked examples:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用Tatoeba中的英语句子进行训练，并按照这个算法生成句子，系统将会创建类似于以下举出的例子：
- en: '[PRE26]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This is not a bad start! If you look at these sentences, there are many words
    and phrases that make sense as valid English (*You can say that*, *That’s a problem*,
    *to go there*, *see him go*, etc.). Even when the system generates peculiar words
    (*despoit*, *studented*, *redusention*, *distaples*), they look almost like real
    English words because they all basically follow morphological and phonological
    rules of English. This means that the language model was successful in learning
    the basic building blocks of English, such as how to arrange letters (orthography),
    how to form words (morphology), and how to form basic sentence structures (syntax).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是个坏开端！如果你看看这些句子，有很多词语和短语看起来是合理的英语句子（“You can say that”、“That's a problem”、“to
    go there”、“see him go”等）。即使系统生成了奇怪的单词（“despoit”、“studented”、“redusention”、“distaples”），它们看起来几乎像真正的英语单词，因为它们基本上遵循英语的形态和音韵规则。这意味着语言模型成功地学习了英语的基本语言要素，如字母排列（拼写）、词形变化（形态学）以及基本句子结构（语法）。
- en: However, if you look at sentences as a whole, few of them make any sense (e.g.,
    *What you see him go as famous to eat!*). This means the language model we trained
    falls short of modeling semantic consistency of sentences. This is potentially
    because our model is not powerful enough (our LSTM-RNN needs to compress everything
    about the sentence into a 256-dimensional vector) or the training dataset is too
    small (just 10,000 sentences), or both. But you can easily imagine that if we
    keep increasing the model capacity as well as the size of the train set, the model
    gets incredibly good at producing realistic natural language text. In February
    2019, OpenAI announced that it developed a huge language model based on the Transformer
    model (which we’ll cover in chapter 8) trained on 40 GB of internet text. The
    model shows that it can produce realistic-looking text that shows near-perfect
    grammar and long-term topical consistency given a prompt. In fact, the model was
    so good that OpenAI decided not to release the large model they had trained due
    to their concerns about malicious use of the technology. But it is important to
    keep in mind that, no matter how intelligent the output looks, their model is
    trained on the same principle as our toy example in this chapter—just trying to
    predict the next character!
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sequential-labeling models tag each word in the input with a label, which can
    be achieved by recurrent neural networks (RNNs).
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part-of-speech (POS) tagging and named entity recognition (NER) are two instances
    of sequential-labeling tasks.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multilayer RNNs stack multiple layers of RNNs, whereas bidirectional RNNs combine
    forward and backward RNNs to encode the entire sentence.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Language models assign probabilities to natural language text, which is achieved
    by predicting the next word.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use a trained language model to assess how “natural” a natural language
    sentence is or even to generate realistic-looking text from scratch.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
