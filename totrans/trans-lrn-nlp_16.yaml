- en: appendix B Introduction to fundamental deep learning tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This appendix covers
  prefs: []
  type: TYPE_NORMAL
- en: Introducing five fundamental algorithmic and software tools used in the book
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overviewing stochastic gradient descent, the algorithm used to train neural
    networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started with TensorFlow for neural network modeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started with PyTorch for neural network modeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overviewing higher-level neural network modeling frameworks Keras, fast.ai,
    and Hugging Face transformers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this appendix, we attempt to provide a brief primer on a few fundamental
    tools and concepts that are used throughout the book. The brief introductions
    to these tools are not absolutely necessary to work through and fully benefit
    from the book. However, going through them can help one get oriented, and they
    are probably most useful to a new entrant into the field of deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we first introduce the reader to the fundamental algorithm behind
    the deep learning revolution we are presently experiencing. This is, of course,
    the *stochastic gradient-descent algorithm*, which is used to train neural networks.
    We follow that up with introductions to two fundamental neural network modeling
    frameworks, PyTorch and TensorFlow. We then introduce three tools built on top
    of these modeling frameworks to provide a higher-level interface to them: Keras,
    fast.ai, and Hugging Face transformers. These tools all complement each other,
    and you are likely to use them all at some point in your career. Our exposition
    of the concepts is not exhaustive; it offers a “bird’s-eye view” into why the
    various tools are needed and how they compare to and complement each other. We
    touch on introductory concepts and cite curated references to facilitate a deeper
    dive. If you feel your experience with these tools is minimal, you may benefit
    from diving deeper before beginning to work through this book.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with the algorithmic engine behind the deep learning revolution,
    the stochastic gradient-descent algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: B.1 Stochastic gradient descent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A neural network has a set of parameters, called *weights*, which determine
    how it is going to transform input data into output data. Determining which set
    of weights allows the network to most closely approximate a set of training data
    is called *training* the network. Stochastic gradient descent is the method used
    to achieve this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s denote the weights by `W`, the input data as `x`, and output data as
    `y`. Let’s also denote the output data predicted by the neural network for the
    input `x` as `y_pred`. The loss function, which measures how close `y` is to `y_pred`,
    is denoted as the function `f`. Note that it is a function of `x`, `y`, and `W`.
    The stochastic gradient-descent algorithm is formulated as a procedure to find
    the minimum of `f`, that is, where the predictions are as close as possible to
    the training data. If the gradient for `f`, denoted by `f’`, exists—if it is a
    *differentiable* function—we know `f’=0` at such a point. The algorithm attempts
    to find such points using the following sequence of steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Draw a random input-output batch `x-y` of data from the training set. This randomness
    is the reason the algorithm is qualified as *stochastic*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass inputs through the network with the current values of `W` to obtain `y_pred`*.*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the corresponding loss function value `f`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute corresponding gradient `f’` of the loss function with respect to W.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Change `W` a bit in the opposite direction of the gradient to lower `f`. The
    size of the step is determined by picking the *learning rate* of the algorithm,
    a very important hyperparameter for convergence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This process is illustrated for the overly simple case of a single weight, with
    the minimum found at step 2 of the algorithm in figure B.1\. This figure is inspired
    by figure 2.11 of François Chollet’s excellent book, *Deep Learning with Python*
    (Manning Publications, 2018), which you should also check out for a very intuitive
    explanation of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '![APPB_01](../Images/APPB_01.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure B.1 Stochastic gradient descent illustrated in the overly simple case
    of a single weight. At each step, compute the gradient with respect to W, and
    take a step of a prespecified size, as determined by the learning rate, in the
    opposite direction of the gradient of the loss function. In this hypothetical
    scenario, the minimum is found at step 2.
  prefs: []
  type: TYPE_NORMAL
- en: A number of variants of this algorithm exist, including Adam, RMSprop, and Adagrad.
    These tend to focus on avoiding local minima and being adaptive in various ways
    (such as in learning rate) to converge faster. The concept of *momentum*—which
    manifests as an extra additive term in the `W` update at each step—is used by
    several such variants to avoid local minima. Some of the most popular variants,
    with brief descriptions, follow.
  prefs: []
  type: TYPE_NORMAL
- en: '*Adagrad* adapts the learning rate in response to how often a parameter is
    encountered. Infrequent parameters are updated with larger steps to achieve a
    balance. This technique was used to train the GloVe static word embedding, described
    in chapter 4 of this book. It was needed in this case to appropriately handle
    rare words in language.'
  prefs: []
  type: TYPE_NORMAL
- en: '*RMSprop* was developed to address the observation that Adagrad’s learning
    rate often decreased too quickly. We can partially alleviate the issue by scaling
    updates by an exponentially decaying average of squared gradients.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Adam*, which stands for *Adaptive Moment Estimation*, also varies the learning
    rate for different parameters. It shares a similarity to RMSprop in that it uses
    the decaying squared gradient average to perform updates. Both the first and second
    moments of the decaying squared gradient average are estimated, updated, and then
    used to update the parameters at every step. This is a popular algorithm to try
    first for many problems.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Nadam,* short for *Nesterov-accelerated Adam*, employs an innovation called
    the *Nesterov-accelerated gradient* to improve Adam convergence further.'
  prefs: []
  type: TYPE_NORMAL
- en: Because the exposition here is meant to be only a brief introduction, and not
    a detailed treatment, we do not explore these variants further. This subject has
    been covered in detail by many excellent references,[¹](#pgfId-1036974),[²](#pgfId-1036977)
    and we encourage you to dive deeper to gain a better understanding. Even though
    you can use modern frameworks without a deep understanding of these variants,
    understanding them better can certainly help you tune hyperparameters and ultimately
    deploy better models.
  prefs: []
  type: TYPE_NORMAL
- en: B.2 TensorFlow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As described in the previous section, knowing the gradient of the loss function
    with respect to the neural network weights is critical to training the network.
    Because modern neural networks are huge, reaching into billions of parameters,
    this gradient function would be impossible to compute by hand. Instead, using
    a fundamental neural network modeling tool such as TensorFlow, the gradient is
    found automatically by applying the chain rule of taking derivatives to the functions
    composing the neural network model. This process is known as *automatic differentiation*.
  prefs: []
  type: TYPE_NORMAL
- en: The basic data structure within Tensorflow is a *tensor*, on which operations
    are performed by building a *graph*. In versions 1.x of the framework, the graph
    is put together with various `tf.*` API calls, and a `Session` object is used
    to compile and execute it to yield numerical values. An illustrative example of
    using this API to define a graph, and execute both it and its gradient, is shown
    in listing B.1\. Specifically, we are interested in computing the matrix product
    `z = x*y`, where `x` is a simple column vector and `y` is a simple row vector.
    We are also interested in computing its gradient, with respect to both `x` and
    `y`, automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Listing B.1 Computing the matrix product z = x*y and its gradient with TensorFlow
    1
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Always imports TensorFlow first
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Eager execution was introduced as the nondefault earlier than 2.0, so here
    we ensure it is off.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Defines the placeholders for vector variables to assign values to later
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Defines the vector derivative graph of the product, with respect to both x
    and y. Parameter grad_ys multiplies the output and can be used to take chain derivatives,
    so we set it to identity matrix for no effect.
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Executes the graph using the Session object
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Runs the function, specifying values for placeholders
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Runs the gradient, specifying values for placeholders
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Displays the results
  prefs: []
  type: TYPE_NORMAL
- en: Executing this code yields the following output. You should be able to verify
    by hand that these values are correct, using your basic linear algebra knowledge,
    which is a prerequisite for this book. We have also included a Kaggle kernel notebook
    executing these commands in the book’s companion repository:[³](#pgfId-1037017)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Version 2.0 and later of the framework made the more “Pythonic” *eager execution*
    mode the default, which made the framework more accessible. It also now included
    Keras, making it easier to use a wide variety of higher-level functions. An illustrative
    example of using this API to define and execute the same graph from listing B.1
    is shown in the next listing. The greater accessibility is immediately evident,
    with eager mode enabling execution right away instead of via a `Session` object
    on a graph.
  prefs: []
  type: TYPE_NORMAL
- en: Listing B.2 Computing the matrix product z = x*y and its gradient with TensorFlow
    2
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Column vector
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Row vector
  prefs: []
  type: TYPE_NORMAL
- en: ❸ This is how you would compute automatic derivatives with respect to x. The
    word “Tape” here means all states are “recorded” and can be played back to retrieve
    the info we need.
  prefs: []
  type: TYPE_NORMAL
- en: ❹ This is how you would compute automatic derivatives with respect to y. Parameter
    output_gradients multiplies the output and can be used to take chain derivatives,
    so we set it to identity matrix for no effect.
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Displays the results
  prefs: []
  type: TYPE_NORMAL
- en: Executing this code should yield the same output values as before.
  prefs: []
  type: TYPE_NORMAL
- en: The framework is organized hierarchically, with both high-level and low-level
    APIs, as shown in figure B.2.
  prefs: []
  type: TYPE_NORMAL
- en: '![APPB_02](../Images/APPB_02.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure B.2 Illustration of the hierarchical organization of the TensorFlow framework
  prefs: []
  type: TYPE_NORMAL
- en: This figure is heavily influenced by figure 1 of the official TensorFlow documentation.[⁴](#pgfId-1037068)
    If you are a beginner at using the framework, skimming through this reference
    in more detail can be helpful. The TensorFlow version of Keras, to be discussed
    further in the last section of this appendix, is also shown in the diagram.
  prefs: []
  type: TYPE_NORMAL
- en: The better way to pick up the ins and outs of the various features of TensorFlow
    is to get your hands dirty experimenting with relevant Kaggle kernel/notebook
    tutorials, as described in appendix A. In particular, just going to kaggle.com
    and searching for “TensorFlow tutorial” will bring up a myriad of wonderful tutorials,
    and you can choose something that works best for your learning style and experience
    level. The tutorial at [https://www.kaggle.com/akashkr/tensorflow-tutorial](https://www.kaggle.com/akashkr/tensorflow-tutorial)
    seems to be a good one for beginners.
  prefs: []
  type: TYPE_NORMAL
- en: B.3 PyTorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This framework was released by Facebook after TensorFlow (in 2016 versus 2015).
    However, it quickly became preferred by many researchers, as evidenced by declining
    relative popularity of TensorFlow to PyTorch in terms of academic paper citations.[⁵](#pgfId-1037076)
    This increased popularity has widely been attributed to the framework’s ability
    to modify various PyTorch model objects programmatically at runtime, making for
    easier code optimization during the research process. Indeed, the introduction
    of eager mode in TensorFlow 2.0 is widely believed to have been influenced by
    PyTorch’s success. Although the differences between the platforms became much
    smaller after the release of TensorFlow 2.0, the popular wisdom is that PyTorch
    is preferred by researchers whereas TensorFlow is preferred for deployment in
    production.
  prefs: []
  type: TYPE_NORMAL
- en: As an illustration, we perform the same sequence of operations from listings
    B.1\. and B.2—vector multiplication and its derivative, which is core to neural
    network models—in PyTorch and show the corresponding code in the next listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing B.3 Computing the matrix product z = x*y and its gradient in PyTorch
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Always import PyTorch first.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Imports the grad function for automatic differentiation
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Column vector
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Row vector
  prefs: []
  type: TYPE_NORMAL
- en: ❺ This ensures that gradients can be computed with respect to x.
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Computes the product
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Computes automatic derivatives with respect to x. retain_graph ensures that
    we can keep taking derivatives; otherwise, the “Tape” is discarded and can’t be
    played back.
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Computes automatic derivatives with respect to y. The parameter grad_outputs
    multiplies the output and can be used to take chain derivatives, so we set it
    to identity matrix for no effect.
  prefs: []
  type: TYPE_NORMAL
- en: ❾ Displays the results
  prefs: []
  type: TYPE_NORMAL
- en: Executing this code should yield the same result as in the previous section.
    We have also included a Kaggle kernel notebook executing these commands in the
    book’s companion repository.[⁶](#pgfId-1037115)
  prefs: []
  type: TYPE_NORMAL
- en: As before, we recommend working through some Kaggle kernels to pick up the ins
    and outs of PyTorch, if you are feel that you might need more experience. The
    tutorial at [https://www.kaggle.com/kanncaa1/pytorch-tutorial-for-deep-learning-lovers](https://www.kaggle.com/kanncaa1/pytorch-tutorial-for-deep-learning-lovers)
    seems to be a good one for beginners.
  prefs: []
  type: TYPE_NORMAL
- en: B.4 Keras, fast.ai, and Transformers by Hugging Face
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned earlier in the appendix, the Keras library is a higher-level neural
    network modeling framework, which is also now included in TensorFlow versions
    2.0 and later. By using it, you can specify neural network architectures in both
    TensorFlow and PyTorch from a single API—just change the backend as needed! It
    comes prepackaged with TensorFlow, as we illustrated in figure B.2\. Its API is
    relatively simple compared to both TensorFlow and PyTorch, which has made it very
    popular. Many excellent resources exist for learning it, perhaps the best one
    being the author’s own book.[⁷](#pgfId-1037123) This is also an excellent reference
    for learning TensorFlow, and about neural networks in general, and we highly recommend
    it if you feel you need to brush up on these topics. You can also work through
    some Kaggle kernels to pick up the basics, with the tutorial at [https://www.kaggle.com/prashant111/keras-basics-for-beginners](https://www.kaggle.com/prashant111/keras-basics-for-beginners)
    being just one good example.
  prefs: []
  type: TYPE_NORMAL
- en: Another popular higher-level modeling API in the field is fast.ai. This library
    was developed as a companion to the massive open online course (MOOC) of the same
    name and implements state-of-the-art methods in a way that makes them extremely
    easy to use. One of its motivations was democratizing these tools for the developing
    world. A popular feature of this library is its learning rate determination utility,
    which we use in chapter 9 of this book. The framework is used for both NLP and
    computer vision and runs on PyTorch. Naturally, the best reference for learning
    the library is the fast.ai MOOC[⁸](#pgfId-1037128) itself. The free course covers
    the basics of neural networks and deep learning and is another wonderful resource
    that we highly recommend. The library achieves simplicity by defining its own
    set of data structures that handle a lot of boilerplate stuff for the user. On
    the other hand, this might make it harder to customize for nonstandard use cases.
    In this author’s experience, it is a wonderful tool to have in one’s arsenal.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, transformers by Hugging Face is a higher-level modeling framework specifically
    for transformer-based models. These have become arguably the most important architecture
    for modern NLP. You will learn exactly why that is the case throughout the book.
    The library might be the most popular library in the space today, due to the sheer
    ease of using it to deploy these models. Before this library existed, deploying
    transformer models in Keras, TensorFlow, and/or PyTorch was quite tedious. The
    library simplified the process into a few lines of Python code in some cases,
    which led to an explosion in its popularity, and it is considered indispensable
    for the modern NLP practitioner. Due to the transparency and simplicity of the
    API, you can probably get away with just reading through the book and working
    through relevant examples, even without any prior experience with it. For further
    reference, check out the intro notebooks from the authors on GitHub^([9](#pgfId-1037132))
    and their official quick start[^(10)](#pgfId-1037135) documentation.
  prefs: []
  type: TYPE_NORMAL
- en: 1. F. Chollet, Deep Learning with Python (Manning Publications, 2018).
  prefs: []
  type: TYPE_NORMAL
- en: 2. S. Ruder, “An Overview of Gradient Descent Optimization Algorithms,” arXiv
    (2016).
  prefs: []
  type: TYPE_NORMAL
- en: 3. [https://github.com/azunre/transfer-learning-for-nlp](https://github.com/azunre/transfer-learning-for-nlp)
  prefs: []
  type: TYPE_NORMAL
- en: 4. [https://developers.google.com/machine-learning/crash-course/first-steps-with-tensorflow/toolkit](https://developers.google.com/machine-learning/crash-course/first-steps-with-tensorflow/toolkit)
  prefs: []
  type: TYPE_NORMAL
- en: 5. [https://en.wikipedia.org/wiki/TensorFlow](https://en.wikipedia.org/wiki/TensorFlow)
  prefs: []
  type: TYPE_NORMAL
- en: 6. [https://github.com/azunre/transfer-learning-for-nlp](https://github.com/azunre/transfer-learning-for-nlp)
  prefs: []
  type: TYPE_NORMAL
- en: 7. F. Chollet, Deep Learning with Python (Manning Publications, 2018).
  prefs: []
  type: TYPE_NORMAL
- en: 8. [https://www.fast.ai/](https://www.fast.ai/)
  prefs: []
  type: TYPE_NORMAL
- en: 9. [https://github.com/huggingface/transformers/tree/master/notebooks](https://github.com/huggingface/transformers/tree/master/notebooks)
  prefs: []
  type: TYPE_NORMAL
- en: 10. [https://huggingface.co/transformers/quicktour.html](https://huggingface.co/transformers/quicktour.html)
  prefs: []
  type: TYPE_NORMAL
