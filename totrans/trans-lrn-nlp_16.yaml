- en: appendix B Introduction to fundamental deep learning tools
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录B 初级深度学习工具介绍
- en: This appendix covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本附录涵盖
- en: Introducing five fundamental algorithmic and software tools used in the book
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍本书中使用的五种基本算法和软件工具
- en: Overviewing stochastic gradient descent, the algorithm used to train neural
    networks
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对训练神经网络使用的算法——随机梯度下降进行概述
- en: Getting started with TensorFlow for neural network modeling
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以TensorFlow开始进行神经网络建模
- en: Getting started with PyTorch for neural network modeling
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以PyTorch开始进行神经网络建模
- en: Overviewing higher-level neural network modeling frameworks Keras, fast.ai,
    and Hugging Face transformers
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对较高级别的神经网络建模框架Keras、fast.ai和Hugging Face transformers进行概述
- en: In this appendix, we attempt to provide a brief primer on a few fundamental
    tools and concepts that are used throughout the book. The brief introductions
    to these tools are not absolutely necessary to work through and fully benefit
    from the book. However, going through them can help one get oriented, and they
    are probably most useful to a new entrant into the field of deep learning.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本附录中，我们试图对本书中使用的一些基本工具和概念提供一个简要的入门。对这些工具的简要介绍并不绝对必要，以便理解并充分从本书中受益。不过，阅读它们可以帮助一个新的深度学习领域的人快速融入，并且对他们可能是最有用的。
- en: 'Specifically, we first introduce the reader to the fundamental algorithm behind
    the deep learning revolution we are presently experiencing. This is, of course,
    the *stochastic gradient-descent algorithm*, which is used to train neural networks.
    We follow that up with introductions to two fundamental neural network modeling
    frameworks, PyTorch and TensorFlow. We then introduce three tools built on top
    of these modeling frameworks to provide a higher-level interface to them: Keras,
    fast.ai, and Hugging Face transformers. These tools all complement each other,
    and you are likely to use them all at some point in your career. Our exposition
    of the concepts is not exhaustive; it offers a “bird’s-eye view” into why the
    various tools are needed and how they compare to and complement each other. We
    touch on introductory concepts and cite curated references to facilitate a deeper
    dive. If you feel your experience with these tools is minimal, you may benefit
    from diving deeper before beginning to work through this book.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们首先向读者介绍了我们目前经历的深度学习革命背后的基本算法。当然，这就是用于训练神经网络的*随机梯度下降算法*。接着我们介绍了两种基本的神经网络建模框架，PyTorch和TensorFlow。然后我们介绍了这两个建模框架之上构建的三种工具，以提供一个更高级别的接口：Keras、fast.ai和Hugging
    Face transformers。这些工具相互补充，你可能在职业生涯的某个时候都会用到它们。我们对概念的阐述并不是穷尽的；它提供了一个“鸟瞰”为什么这些工具是需要的以及它们如何相互比较和相互补充。我们涉及了介绍性的概念，并引用了精心筛选的参考资料，以便深入研究。如果你觉得自己对这些工具的经验很少，你可能需要在开始阅读本书之前深入研究一下它们。
- en: Let’s start with the algorithmic engine behind the deep learning revolution,
    the stochastic gradient-descent algorithm.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从深度学习革命背后的算法引擎开始，即随机梯度下降算法。
- en: B.1 Stochastic gradient descent
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.1 随机梯度下降
- en: A neural network has a set of parameters, called *weights*, which determine
    how it is going to transform input data into output data. Determining which set
    of weights allows the network to most closely approximate a set of training data
    is called *training* the network. Stochastic gradient descent is the method used
    to achieve this.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络有一组参数，称为*权重*，确定它将如何将输入数据转换为输出数据。确定哪组权重允许网络最接近地逼近一组训练数据称为*训练*网络。随机梯度下降是实现这一目标的方法。
- en: 'Let’s denote the weights by `W`, the input data as `x`, and output data as
    `y`. Let’s also denote the output data predicted by the neural network for the
    input `x` as `y_pred`. The loss function, which measures how close `y` is to `y_pred`,
    is denoted as the function `f`. Note that it is a function of `x`, `y`, and `W`.
    The stochastic gradient-descent algorithm is formulated as a procedure to find
    the minimum of `f`, that is, where the predictions are as close as possible to
    the training data. If the gradient for `f`, denoted by `f’`, exists—if it is a
    *differentiable* function—we know `f’=0` at such a point. The algorithm attempts
    to find such points using the following sequence of steps:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Draw a random input-output batch `x-y` of data from the training set. This randomness
    is the reason the algorithm is qualified as *stochastic*.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass inputs through the network with the current values of `W` to obtain `y_pred`*.*
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the corresponding loss function value `f`.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute corresponding gradient `f’` of the loss function with respect to W.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Change `W` a bit in the opposite direction of the gradient to lower `f`. The
    size of the step is determined by picking the *learning rate* of the algorithm,
    a very important hyperparameter for convergence.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This process is illustrated for the overly simple case of a single weight, with
    the minimum found at step 2 of the algorithm in figure B.1\. This figure is inspired
    by figure 2.11 of François Chollet’s excellent book, *Deep Learning with Python*
    (Manning Publications, 2018), which you should also check out for a very intuitive
    explanation of the algorithm.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '![APPB_01](../Images/APPB_01.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
- en: Figure B.1 Stochastic gradient descent illustrated in the overly simple case
    of a single weight. At each step, compute the gradient with respect to W, and
    take a step of a prespecified size, as determined by the learning rate, in the
    opposite direction of the gradient of the loss function. In this hypothetical
    scenario, the minimum is found at step 2.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: A number of variants of this algorithm exist, including Adam, RMSprop, and Adagrad.
    These tend to focus on avoiding local minima and being adaptive in various ways
    (such as in learning rate) to converge faster. The concept of *momentum*—which
    manifests as an extra additive term in the `W` update at each step—is used by
    several such variants to avoid local minima. Some of the most popular variants,
    with brief descriptions, follow.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '*Adagrad* adapts the learning rate in response to how often a parameter is
    encountered. Infrequent parameters are updated with larger steps to achieve a
    balance. This technique was used to train the GloVe static word embedding, described
    in chapter 4 of this book. It was needed in this case to appropriately handle
    rare words in language.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '*RMSprop* was developed to address the observation that Adagrad’s learning
    rate often decreased too quickly. We can partially alleviate the issue by scaling
    updates by an exponentially decaying average of squared gradients.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*RMSprop*是为了解决Adagrad的学习速率经常过快下降的问题而开发的。我们可以通过将更新缩放为平方梯度的指数衰减平均值来部分缓解这个问题。'
- en: '*Adam*, which stands for *Adaptive Moment Estimation*, also varies the learning
    rate for different parameters. It shares a similarity to RMSprop in that it uses
    the decaying squared gradient average to perform updates. Both the first and second
    moments of the decaying squared gradient average are estimated, updated, and then
    used to update the parameters at every step. This is a popular algorithm to try
    first for many problems.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*Adam*表示*自适应矩估计*，也针对不同参数变化学习率。它与RMSprop具有相似之处，因为它使用衰减平方梯度平均值来执行更新。衰减平方梯度平均值的第一和二个时刻被估计，更新，然后用于在每个步骤中更新参数。这是尝试解决许多问题的流行算法。'
- en: '*Nadam,* short for *Nesterov-accelerated Adam*, employs an innovation called
    the *Nesterov-accelerated gradient* to improve Adam convergence further.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*Nadam*是*Nesterov加速Adam*的缩写，采用称为*Nesterov加速梯度*的创新来进一步改善Adam的收敛性。'
- en: Because the exposition here is meant to be only a brief introduction, and not
    a detailed treatment, we do not explore these variants further. This subject has
    been covered in detail by many excellent references,[¹](#pgfId-1036974),[²](#pgfId-1036977)
    and we encourage you to dive deeper to gain a better understanding. Even though
    you can use modern frameworks without a deep understanding of these variants,
    understanding them better can certainly help you tune hyperparameters and ultimately
    deploy better models.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这里的介绍只是一个简短的介绍，而不是详细的处理，所以我们不会更深入地探讨这些变体。这个主题已经被许多优秀的参考文献详细覆盖了[¹](#pgfId-1036974)，[²](#pgfId-1036977)，我们鼓励您深入研究以获得更好的理解。即使您可以在不深入了解这些变体的情况下使用现代框架，更好地了解它们可以帮助您调整超参数，并最终部署更好的模型。
- en: B.2 TensorFlow
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.2 TensorFlow
- en: As described in the previous section, knowing the gradient of the loss function
    with respect to the neural network weights is critical to training the network.
    Because modern neural networks are huge, reaching into billions of parameters,
    this gradient function would be impossible to compute by hand. Instead, using
    a fundamental neural network modeling tool such as TensorFlow, the gradient is
    found automatically by applying the chain rule of taking derivatives to the functions
    composing the neural network model. This process is known as *automatic differentiation*.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一节所述，了解损失函数相对于神经网络权重的梯度对于训练网络至关重要。由于现代神经网络是巨大的，达到数十亿个参数，因此手动计算此梯度函数将是不可能的。相反，使用TensorFlow等基本神经网络建模工具，通过应用取导数的链式法则自动找到梯度来计算它。这个过程被称为*自动微分*。
- en: The basic data structure within Tensorflow is a *tensor*, on which operations
    are performed by building a *graph*. In versions 1.x of the framework, the graph
    is put together with various `tf.*` API calls, and a `Session` object is used
    to compile and execute it to yield numerical values. An illustrative example of
    using this API to define a graph, and execute both it and its gradient, is shown
    in listing B.1\. Specifically, we are interested in computing the matrix product
    `z = x*y`, where `x` is a simple column vector and `y` is a simple row vector.
    We are also interested in computing its gradient, with respect to both `x` and
    `y`, automatically.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Tensorflow中的基本数据结构是*张量*，通过构建一个*计算图*来对其进行操作。在框架的1.x版本中，通过多种API调用`tf.*`来构建图，并使用`Session`对象编译和执行它以产生数值。示例B.1中演示了使用此API定义图形并执行其梯度计算的说明性示例。具体来说，我们需要计算矩阵乘积`z
    = x*y`，其中`x`是简单的列向量，而`y`是简单的行向量。我们还希望自动计算它相对于`x`和`y`的梯度。
- en: Listing B.1 Computing the matrix product z = x*y and its gradient with TensorFlow
    1
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 示例B.1使用TensorFlow 1计算矩阵乘积z = x * y及其梯度
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Always imports TensorFlow first
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ❶总是先导入TensorFlow
- en: ❷ Eager execution was introduced as the nondefault earlier than 2.0, so here
    we ensure it is off.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ❷Eager执行在2.0之前作为非默认值引入，因此在此确保其关闭。
- en: ❸ Defines the placeholders for vector variables to assign values to later
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ❸定义留给后面分配值的向量变量占位符
- en: ❹ Defines the vector derivative graph of the product, with respect to both x
    and y. Parameter grad_ys multiplies the output and can be used to take chain derivatives,
    so we set it to identity matrix for no effect.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Executes the graph using the Session object
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Runs the function, specifying values for placeholders
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Runs the gradient, specifying values for placeholders
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Displays the results
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Executing this code yields the following output. You should be able to verify
    by hand that these values are correct, using your basic linear algebra knowledge,
    which is a prerequisite for this book. We have also included a Kaggle kernel notebook
    executing these commands in the book’s companion repository:[³](#pgfId-1037017)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Version 2.0 and later of the framework made the more “Pythonic” *eager execution*
    mode the default, which made the framework more accessible. It also now included
    Keras, making it easier to use a wide variety of higher-level functions. An illustrative
    example of using this API to define and execute the same graph from listing B.1
    is shown in the next listing. The greater accessibility is immediately evident,
    with eager mode enabling execution right away instead of via a `Session` object
    on a graph.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Listing B.2 Computing the matrix product z = x*y and its gradient with TensorFlow
    2
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Column vector
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Row vector
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: ❸ This is how you would compute automatic derivatives with respect to x. The
    word “Tape” here means all states are “recorded” and can be played back to retrieve
    the info we need.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: ❹ This is how you would compute automatic derivatives with respect to y. Parameter
    output_gradients multiplies the output and can be used to take chain derivatives,
    so we set it to identity matrix for no effect.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Displays the results
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Executing this code should yield the same output values as before.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: The framework is organized hierarchically, with both high-level and low-level
    APIs, as shown in figure B.2.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '![APPB_02](../Images/APPB_02.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
- en: Figure B.2 Illustration of the hierarchical organization of the TensorFlow framework
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: This figure is heavily influenced by figure 1 of the official TensorFlow documentation.[⁴](#pgfId-1037068)
    If you are a beginner at using the framework, skimming through this reference
    in more detail can be helpful. The TensorFlow version of Keras, to be discussed
    further in the last section of this appendix, is also shown in the diagram.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: The better way to pick up the ins and outs of the various features of TensorFlow
    is to get your hands dirty experimenting with relevant Kaggle kernel/notebook
    tutorials, as described in appendix A. In particular, just going to kaggle.com
    and searching for “TensorFlow tutorial” will bring up a myriad of wonderful tutorials,
    and you can choose something that works best for your learning style and experience
    level. The tutorial at [https://www.kaggle.com/akashkr/tensorflow-tutorial](https://www.kaggle.com/akashkr/tensorflow-tutorial)
    seems to be a good one for beginners.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: B.3 PyTorch
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This framework was released by Facebook after TensorFlow (in 2016 versus 2015).
    However, it quickly became preferred by many researchers, as evidenced by declining
    relative popularity of TensorFlow to PyTorch in terms of academic paper citations.[⁵](#pgfId-1037076)
    This increased popularity has widely been attributed to the framework’s ability
    to modify various PyTorch model objects programmatically at runtime, making for
    easier code optimization during the research process. Indeed, the introduction
    of eager mode in TensorFlow 2.0 is widely believed to have been influenced by
    PyTorch’s success. Although the differences between the platforms became much
    smaller after the release of TensorFlow 2.0, the popular wisdom is that PyTorch
    is preferred by researchers whereas TensorFlow is preferred for deployment in
    production.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: As an illustration, we perform the same sequence of operations from listings
    B.1\. and B.2—vector multiplication and its derivative, which is core to neural
    network models—in PyTorch and show the corresponding code in the next listing.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Listing B.3 Computing the matrix product z = x*y and its gradient in PyTorch
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Always import PyTorch first.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Imports the grad function for automatic differentiation
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Column vector
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Row vector
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: ❺ This ensures that gradients can be computed with respect to x.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Computes the product
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Computes automatic derivatives with respect to x. retain_graph ensures that
    we can keep taking derivatives; otherwise, the “Tape” is discarded and can’t be
    played back.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Computes automatic derivatives with respect to y. The parameter grad_outputs
    multiplies the output and can be used to take chain derivatives, so we set it
    to identity matrix for no effect.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: ❾ Displays the results
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Executing this code should yield the same result as in the previous section.
    We have also included a Kaggle kernel notebook executing these commands in the
    book’s companion repository.[⁶](#pgfId-1037115)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: As before, we recommend working through some Kaggle kernels to pick up the ins
    and outs of PyTorch, if you are feel that you might need more experience. The
    tutorial at [https://www.kaggle.com/kanncaa1/pytorch-tutorial-for-deep-learning-lovers](https://www.kaggle.com/kanncaa1/pytorch-tutorial-for-deep-learning-lovers)
    seems to be a good one for beginners.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: B.4 Keras, fast.ai, and Transformers by Hugging Face
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned earlier in the appendix, the Keras library is a higher-level neural
    network modeling framework, which is also now included in TensorFlow versions
    2.0 and later. By using it, you can specify neural network architectures in both
    TensorFlow and PyTorch from a single API—just change the backend as needed! It
    comes prepackaged with TensorFlow, as we illustrated in figure B.2\. Its API is
    relatively simple compared to both TensorFlow and PyTorch, which has made it very
    popular. Many excellent resources exist for learning it, perhaps the best one
    being the author’s own book.[⁷](#pgfId-1037123) This is also an excellent reference
    for learning TensorFlow, and about neural networks in general, and we highly recommend
    it if you feel you need to brush up on these topics. You can also work through
    some Kaggle kernels to pick up the basics, with the tutorial at [https://www.kaggle.com/prashant111/keras-basics-for-beginners](https://www.kaggle.com/prashant111/keras-basics-for-beginners)
    being just one good example.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Another popular higher-level modeling API in the field is fast.ai. This library
    was developed as a companion to the massive open online course (MOOC) of the same
    name and implements state-of-the-art methods in a way that makes them extremely
    easy to use. One of its motivations was democratizing these tools for the developing
    world. A popular feature of this library is its learning rate determination utility,
    which we use in chapter 9 of this book. The framework is used for both NLP and
    computer vision and runs on PyTorch. Naturally, the best reference for learning
    the library is the fast.ai MOOC[⁸](#pgfId-1037128) itself. The free course covers
    the basics of neural networks and deep learning and is another wonderful resource
    that we highly recommend. The library achieves simplicity by defining its own
    set of data structures that handle a lot of boilerplate stuff for the user. On
    the other hand, this might make it harder to customize for nonstandard use cases.
    In this author’s experience, it is a wonderful tool to have in one’s arsenal.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Finally, transformers by Hugging Face is a higher-level modeling framework specifically
    for transformer-based models. These have become arguably the most important architecture
    for modern NLP. You will learn exactly why that is the case throughout the book.
    The library might be the most popular library in the space today, due to the sheer
    ease of using it to deploy these models. Before this library existed, deploying
    transformer models in Keras, TensorFlow, and/or PyTorch was quite tedious. The
    library simplified the process into a few lines of Python code in some cases,
    which led to an explosion in its popularity, and it is considered indispensable
    for the modern NLP practitioner. Due to the transparency and simplicity of the
    API, you can probably get away with just reading through the book and working
    through relevant examples, even without any prior experience with it. For further
    reference, check out the intro notebooks from the authors on GitHub^([9](#pgfId-1037132))
    and their official quick start[^(10)](#pgfId-1037135) documentation.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Hugging Face 的 Transformers 是一个专门针对基于 Transformer 模型的高级建模框架。这些模型已经成为现代自然语言处理中可能是最重要的架构。你将在整本书中准确了解其中的原因。这个库可能是当今领域中最受欢迎的库，因为使用它部署这些模型非常简单。在这个库存在之前，使用
    Keras、TensorFlow 和/或 PyTorch 部署 Transformer 模型相当繁琐。该库在某些情况下简化了这个过程，只需几行 Python
    代码，导致其受欢迎程度激增，并被认为是现代自然语言处理从业者不可或缺的工具。由于 API 的透明度和简单性，你可能只需阅读本书并通过相关示例进行工作，甚至无需任何先前的使用经验即可。如需进一步参考，请查看作者在
    GitHub 上的入门笔记[^(9)](#pgfId-1037132)以及官方快速入门文档[^(10)](#pgfId-1037135)。
- en: 1. F. Chollet, Deep Learning with Python (Manning Publications, 2018).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 1. F. Chollet，《Deep Learning with Python》（Manning Publications，2018）。
- en: 2. S. Ruder, “An Overview of Gradient Descent Optimization Algorithms,” arXiv
    (2016).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 2. S. Ruder，“梯度下降优化算法概述”，arXiv（2016）。
- en: 3. [https://github.com/azunre/transfer-learning-for-nlp](https://github.com/azunre/transfer-learning-for-nlp)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 3. [https://github.com/azunre/transfer-learning-for-nlp](https://github.com/azunre/transfer-learning-for-nlp)
- en: 4. [https://developers.google.com/machine-learning/crash-course/first-steps-with-tensorflow/toolkit](https://developers.google.com/machine-learning/crash-course/first-steps-with-tensorflow/toolkit)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 4. [https://developers.google.com/machine-learning/crash-course/first-steps-with-tensorflow/toolkit](https://developers.google.com/machine-learning/crash-course/first-steps-with-tensorflow/toolkit)
- en: 5. [https://en.wikipedia.org/wiki/TensorFlow](https://en.wikipedia.org/wiki/TensorFlow)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 5. [https://en.wikipedia.org/wiki/TensorFlow](https://en.wikipedia.org/wiki/TensorFlow)
- en: 6. [https://github.com/azunre/transfer-learning-for-nlp](https://github.com/azunre/transfer-learning-for-nlp)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 6. [https://github.com/azunre/transfer-learning-for-nlp](https://github.com/azunre/transfer-learning-for-nlp)
- en: 7. F. Chollet, Deep Learning with Python (Manning Publications, 2018).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 7. F. Chollet，《Deep Learning with Python》（Manning Publications，2018）。
- en: 8. [https://www.fast.ai/](https://www.fast.ai/)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 8. [https://www.fast.ai/](https://www.fast.ai/)
- en: 9. [https://github.com/huggingface/transformers/tree/master/notebooks](https://github.com/huggingface/transformers/tree/master/notebooks)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 9. [https://github.com/huggingface/transformers/tree/master/notebooks](https://github.com/huggingface/transformers/tree/master/notebooks)
- en: 10. [https://huggingface.co/transformers/quicktour.html](https://huggingface.co/transformers/quicktour.html)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 10. [https://huggingface.co/transformers/quicktour.html](https://huggingface.co/transformers/quicktour.html)
