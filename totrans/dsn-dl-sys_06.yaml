- en: 6 Model serving design
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 模型服务设计
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Defining model serving
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义模型服务
- en: Common model serving challenges and approaches
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常见模型服务挑战和方法
- en: Designing model serving systems for different user scenarios
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计不同用户场景下的模型服务系统
- en: '*Model serving* is the process of executing a model with user input data. Among
    all the activities in a deep learning system, model serving is the closest to
    the end customers. After all the hard work of dataset preparation, training algorithm
    development, hyperparameter tuning, and testing results in models is completed,
    these models are presented to customers by model serving services.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*模型服务*是使用用户输入数据执行模型的过程。在深度学习系统中的所有活动中，模型服务是最接近最终客户的活动。在完成了数据集准备、训练算法开发、超参数调整和测试结果生成模型的所有辛勤工作之后，这些模型由模型服务服务呈现给客户。'
- en: Take speech translation as an example. After training a sequence-to-sequence
    model for voice translation, the team is ready to present it to the world. For
    people to use this model remotely, the model is usually hosted in a web service
    and exposed by a web API. Then we (the customers) can send our voice audio file
    over the web API and get back a translated voice audio file. All the model loading
    and execution happens at the web service backend. Everything included in this
    user workflow—service, model files, and model execution—is called *model serving*.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 以语音翻译为例。在为语音翻译训练了一个序列到序列模型之后，团队准备向世界展示它。为了让人们远程使用这个模型，通常会将模型托管在 Web 服务中，并通过
    Web API 公开。然后我们（客户）可以通过 Web API 发送我们的语音音频文件，并获得一个翻译后的语音音频文件。所有模型加载和执行都发生在 Web
    服务后端。包括在这个用户工作流程中的一切——服务、模型文件和模型执行——都被称为*模型服务*。
- en: Building model serving applications is another special deep learning domain
    for which software engineers are particularly well suited. Model serving uses
    request latency, scalability, availability, and operability—all areas that engineers
    know inside and out. With some introduction to the concepts of deep learning model
    serving, developers who have some experience with distributed computing can play
    a significant role in building the model serving element.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 构建模型服务应用程序是另一个特殊的深度学习领域，软件工程师特别适合这个领域。模型服务使用请求延迟、可伸缩性、可用性和可操作性——所有这些都是工程师内外熟知的领域。通过一些深度学习模型服务概念的介绍，有一些分布式计算经验的开发人员可以在构建模型服务元素方面发挥重要作用。
- en: Serving models in production can be challenging because models are trained by
    various frameworks and algorithms, so the methods and libraries to execute the
    model vary. Also, the terminology used in the model serving field is confusing,
    with too many terms, like *model prediction* and *model inference*, that sound
    different but mean the same thing in the serving context. Furthermore, there are
    many model serving options from which to choose. On the one hand, we have black-box
    solutions like TensorFlow Serving, TorchServe, and NVIDIA Triton Inference Server.
    On the other, we have customized approaches like building your own predictor service
    or embedding models directly into your applications. These approaches all seem
    very similar and capable, so it is hard to select one over another. Therefore,
    if you are new to this domain, you can quickly become lost.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型在生产中提供服务可能很具有挑战性，因为模型是由各种框架和算法训练的，因此执行模型的方法和库各不相同。此外，模型服务领域使用的术语令人困惑，有太多不同的术语，如*模型预测*和*模型推理*，听起来不同但在服务上下文中意思相同。此外，有许多模型服务选项可供选择。一方面，我们有像
    TensorFlow Serving、TorchServe 和 NVIDIA Triton 推理服务器等黑盒解决方案。另一方面，我们有像构建自己的预测服务或直接将模型嵌入应用程序中这样的定制方法。这些方法看起来都非常相似且功能强大，因此很难选择其中一个。因此，如果您对这个领域还不熟悉，您可能会很快迷失方向。
- en: 'Our goal here is to help you find your way. We hope to empower you to design
    and build the model serving solution that best fits your situation. To achieve
    this goal, we have lots of content to cover, from the conceptual understanding
    of model serving and service-design considerations to concrete examples and model
    deployment workflow. To avoid exhausting you with a super-long chapter, we divide
    this content into two chapters: chapter 6 focuses on concepts, definitions, and
    design, and chapter 7 puts those concepts into practice, including building a
    sample prediction service and addressing open source tools as well as deploying
    and monitoring model production.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是帮助你找到自己的方向。我们希望能赋予你设计和构建最适合你情况的模型服务解决方案的能力。为了实现这个目标，我们有很多内容需要介绍，包括模型服务的概念理解、服务设计考虑因素、具体示例和模型部署工作流程。为了避免让你阅读超长的一章内容，我们将这部分内容分成了两章：第6章重点关注概念、定义和设计，第7章将这些概念付诸实践，包括构建一个样本预测服务，介绍开源工具以及部署和监控模型生产。
- en: In this chapter, we start by clarifying the terminology and providing our own
    definitions of the elements used in model serving. We also describe the main challenges
    facing us in the model serving field. Then we will move to the design aspect,
    explaining the three common strategies of model serving and designing a model
    serving system that fits different use cases.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先澄清术语，并为模型服务中使用的元素提供我们自己的定义。我们还描述了我们在模型服务领域面临的主要挑战。然后我们将转向设计方面，解释模型服务的三种常见策略，并设计一个适合不同用例的模型服务系统。
- en: By reading this chapter, you will not only gain a solid understanding of how
    model serving works, but you will also know the common design patterns that can
    address most of the model serving use cases. With the concepts and terminology
    clear in your mind, you should be comfortable joining any model serving–related
    discussion or reading articles and papers on the topic. And, of course, this chapter
    builds the foundation so you can follow the practical work addressed in the next
    chapter.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 通过阅读本章，你不仅会对模型服务的工作原理有扎实的理解，还会了解到可以应对大多数模型服务用例的常见设计模式。随着概念和术语在你脑海中变得清晰，你应该可以自如地参与任何与模型服务相关的讨论，或者阅读关于这个主题的文章和论文。当然，本章也为你在下一章介绍的实际工作奠定了基础。
- en: 6.1 Explaining model serving
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 解释模型服务
- en: In the engineering of model serving, the terminology is a major problem. For
    example, *model*, *model architecture*, *inference graph*, *prediction*, and *inference*
    are all terms people use without clearly defining them, so they can have the same
    meaning or refer to different concepts depending on the context (model serving
    or model training). When we work with data scientists to build model serving solutions,
    the confusion around model serving terms causes a lot of miscommunication. In
    this section, we will explain the core concepts of model serving and interpret
    commonly used terminologies from an engineering perspective to help you avoid
    falling into the terminology trap.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型服务的工程中，术语是一个主要问题。例如，*模型*、*模型架构*、*推理图*、*预测*和*推理*等术语被人们使用时没有清晰地定义它们，因此它们可以具有相同的含义，也可以根据上下文（模型服务或模型训练）而指代不同的概念。当我们与数据科学家合作构建模型服务解决方案时，模型服务术语的混淆会导致很多交流不畅。在本节中，我们将从工程角度解释模型服务的核心概念，并对常用术语进行解释，以帮助你避免陷入术语陷阱。
- en: 6.1.1 What is a machine learning model?
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.1 什么是机器学习模型？
- en: There are multiple definitions of machine learning models in academia, from
    distilled representations of learnings of datasets to mathematical representations
    for recognizing certain patterns or making decisions based on previously unseen
    information. Nevertheless, as model serving developers, we can understand a model
    simply as a collection of files that are produced during training.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在学术界对机器学习模型有多种定义，从对数据集学习的精简表达到基于以前未见过的信息识别特定模式或做出决策的数学表达。然而，作为模型服务开发人员，我们可以简单地将模型理解为在训练过程中产生的一组文件的集合。
- en: The idea of a model is simple, but many people misunderstand that models are
    just static files. Although models are saved as files, they aren’t static; they’re
    essentially executable programs.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的概念很简单，但很多人误解模型只是静态文件。虽然模型被保存为文件，但它们并不是静态的，它们实质上是可执行的程序。
- en: Let’s take apart that statement and determine what it means. A model consists
    of a machine learning algorithm, model data, and a model executor. A *model executor*
    is a wrapper code of the machine learning algorithm; it takes user input and runs
    the algorithm to compute and return prediction results. A *machine learning algorithm*
    refers to the algorithm used in model training, sometimes also called *model architecture*.
    Using speech translation as an example again, if the translation model is trained
    by a sequence-to-sequence network as its training algorithm, the machine learning
    algorithm in the model is the same sequence-to-sequence network. *Model data*
    is the data required to run the machine learning algorithm, such as the neural
    network’s learned parameters (weights and biases), embeddings, and label classes.
    Figure 6.1 illustrates a generic model structure.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解这个语句并确定其含义。一个模型由机器学习算法、模型数据和模型执行器组成。*模型执行器*是机器学习算法的封装代码；它接收用户输入并运行算法来计算和返回预测结果。*机器学习算法*是指模型训练中使用的算法，有时也称为*模型架构*。再以语音翻译为例，如果翻译模型是由序列到序列网络作为其训练算法，则模型中的机器学习算法就是相同的序列到序列网络。*模型数据*是运行机器学习算法所需的数据，例如神经网络的学习参数（权重和偏差）、嵌入和标签类别等。图6.1展示了一个通用的模型结构。
- en: '![](../Images/06-01.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06-01.png)'
- en: Figure 6.1 A model is composed of a machine learning algorithm, model executor,
    and model data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 一个模型由机器学习算法、模型执行器和模型数据组成。
- en: Note We often refer to machine learning algorithms as *model algorithms* in
    this chapter for simplicity.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 在本章中，我们经常简称机器学习算法为*模型算法*。
- en: The most important takeaway in this section is that the output of a model training
    execution—or simply, a model—isn’t just a set of static data. In contrast, deep
    learning models are executable programs that include a machine learning algorithm
    and its dependent data, so the models can make predictions based on input data
    at run time.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中最重要的要点是，模型训练执行的输出，也就是模型，并不仅仅只是一组静态数据。相反，深度学习模型是可执行程序，包括机器学习算法及其依赖的数据，因此模型可以根据运行时的输入数据进行预测。
- en: Note Models are not only weights and biases. Sometimes data scientists save
    a neural network’s trained parameters—weights and biases—to a file and name it
    “model file.” This confuses people into thinking a model is just a data file that
    contains only weights and biases. Weights and biases are the model *data*, but
    we also need the algorithm and the wrapper code to run the prediction.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 模型不仅仅包括权重和偏差。有时数据科学家将神经网络的训练参数（权重和偏差）保存到一个文件中，并命名为“模型文件”。这会让人们误以为模型只是一个只包含权重和偏差的数据文件。权重和偏差是模型的*数据*，但我们还需要算法和封装代码来运行预测。
- en: 6.1.2 Model prediction and inference
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.2 模型预测和推断
- en: Academics may consider model inference and prediction to be two separate concepts.
    A model inference can refer to learning about how data is generated and understanding
    its causes and effects, whereas a model prediction might refer to predicting future
    events.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 学术界可能认为模型推断和预测是两个不同的概念。模型推断可以指学习数据是如何生成的、理解其原因和影响，而模型预测则可能指对未来事件的预测。
- en: A sample model prediction scenario might include using sales records to train
    a model to predict which individuals are likely to respond to the next marketing
    campaign. And a sample model inference scenario would include using sales records
    to train a model to understand the sales effect from the product price and customer
    income. The predictive accuracy on previously unseen data for model inference
    is not very important because the main focus is on learning the data generation
    process. Model training is designed to fit the full dataset.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一个样本模型预测的场景可能包括使用销售记录来训练一个模型，以预测哪些个体可能会对下一次营销活动做出回应。而一个样本模型推断的场景将包括使用销售记录来训练一个模型，从产品价格和客户收入的角度理解销售效果。对于模型推断来说，之前未见过的数据上的预测准确性并不是非常重要，因为主要关注的是学习数据生成过程。模型训练的目的是拟合整个数据集。
- en: 'From an engineering perspective, model prediction and model inference mean
    the same. Although models can be built and used for different purposes, both model
    prediction and model inference in the context of model serving refer to the same
    action: executing the model with given data points to obtain a set of output values.
    Figure 6.2 illustrates the model serving workflow for the prediction model and
    the inference model; as you can see, there is no difference between them.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/06-02.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 Model prediction and model inference are the same in model serving
    engineering.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: To simplify the text in the illustrations of this chapter, starting from figure
    6.2, we use the word *model* to represent model data, model executor, and machine
    learning (model) algorithm. This is not only to keep the text short but also to
    emphasize that the machine learning model is an executable program.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.3 What is model serving?
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Model* *serving* simply means executing a model with input data to make predictions,
    which includes fetching the expected model, setting up the model’s execution environment,
    executing the model to make a prediction with given data points, and returning
    the prediction result. The most used method for model serving is to host models
    in a web service and expose the model’s predict function through a web API.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we build an object detection model to detect sharks in seacoast images;
    we can build a web service to host this model and expose a shark detection web
    API. This web API can then be used by beach hotels anywhere in the world to detect
    sharks with their own coast images. Conventionally, we call the model serving
    web service the prediction service.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical model prediction workflow in a prediction service has four steps:
    receiving a user request; loading the model from an artifact store to memory or
    GPU; executing the model’s algorithm; and, finally, returning the prediction results.
    Figure 6.3 shows this workflow.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/06-03.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 A typical model prediction workflow in a prediction service
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides the four-step prediction workflow, figure 6.3 also mentions three main
    components of the model serving: the prediction service (A), the model artifactory
    store (B), and the prediction web API (C). The model artifactory store (component
    B) holds all the models produced by the model training. The web API (component
    C) receives prediction requests. The prediction service (component A) responds
    to the prediction request, loads the model from the artifactory store, runs the
    model, and returns the prediction result.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Although the four steps of the prediction workflow are generally applicable
    to all kinds of models, the actual implementation of the steps depends on the
    business needs, model training algorithm, and model training framework. We will
    discuss design options for prediction services in section 6.3, and we will present
    two sample prediction services in chapter 7.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Model serving runs machine learning algorithms in a special mode
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 模型服务以特殊模式运行机器学习算法。
- en: 'Model training and model serving execute the same machine learning algorithm
    but in two different models: learning mode and evaluation mode.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练和模型服务使用相同的机器学习算法，但是有两种不同的模式：学习模式和评估模式。
- en: In the learning mode, we run the algorithm in an *open loop*, meaning in each
    training iteration, we first run the neural network (algorithm) with an input
    data sample to calculate prediction results. Based on the difference between the
    prediction results and the expected results, the network’s parameters (weights
    and bias) are updated to fit the dataset closer.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习模式中，我们以*开环*的方式运行算法，这意味着在每个训练迭代中，我们首先对神经网络（算法）运行一个输入数据样本来计算预测结果。根据预测结果与预期结果之间的差异，网络的参数（权重和偏差）会被更新以更接近数据集。
- en: In the evaluation model, the neural network (algorithm) is run in a closed loop,
    which means that the network’s parameters will not be updated. The neural network
    is run solely to obtain the prediction results. So from a code implementation
    perspective, model serving is essentially running the machine learning algorithm
    (neural network) in the evaluation mode.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估模式中，神经网络（算法）在闭环中运行，这意味着网络的参数不会被更新。神经网络仅用于获取预测结果。因此从代码实现的角度来看，模型服务本质上是以评估模式运行机器学习算法（神经网络）。
- en: 6.1.4 Model serving challenges
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.4 模型服务的挑战
- en: Building a web service to serve models cost-effectively is a lot more complicated
    than running models locally on our laptops. Following are the six common challenges
    for serving models in web services.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个成本效益高的网络服务以服务模型比在我们的笔记本电脑上本地运行模型要复杂得多。以下是为网络服务提供模型所面临的六个常见挑战。
- en: The model prediction API differs per model algorithm. Different deep learning
    algorithms (such as recurrent neural networks and convolutional neural networks
    [CNN]) require different input data formats, and their output format can also
    vary. When designing the web prediction API, it’s quite challenging to design
    a unified web API that meets the input data requirements for every model algorithm.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 模型预测 API 根据模型算法而异。不同的深度学习算法（如循环神经网络和卷积神经网络 [CNN]）需要不同的输入数据格式，其输出格式也可能不同。在设计
    Web 预测 API 时，设计一个满足每种模型算法输入数据要求的统一 Web API 是非常具有挑战性的。
- en: Model executing environments are different per training framework. Models can
    be trained in different frameworks, such as TensorFlow and PyTorch. And each training
    framework has its special setup and configuration to execute its models. The prediction
    service should encapsulate the model execution environment setup at its backend,
    so customers can focus on using the model prediction API, not the framework with
    which this model is trained.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 模型执行环境因训练框架而异。模型可以在不同的框架中进行训练，例如 TensorFlow 和 PyTorch。而每个训练框架都有其特殊的设置和配置来执行其模型。预测服务应该在其后端封装模型执行环境的设置，这样客户就可以专注于使用模型预测
    API，而不是该模型所训练的框架。
- en: There are too many model serving tools, libraries, and systems from which to
    choose. If we decide to use existing open source approaches to model serving,
    the immediate question becomes which approach we should choose. There are 20+
    different options, such as TorchServe, TensorFlow Serving, NVIDIA Triton Inference
    Server, Seldon Core, and KFServing. How do we know which approach works best for
    our situation?
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 有太多的模型服务工具、库和系统可供选择。如果我们决定使用现有的开源模型服务方法，那么立即出现的问题就是我们应该选择哪种方法。有 20 多种不同的选择，比如
    TorchServe、TensorFlow Serving、NVIDIA Triton Inference Server、Seldon Core 和 KFServing。我们如何知道哪种方法最适合我们的情况？
- en: There is no universal, most cost-effective model serving design; we need to
    tailor a model serving approach that fits our own use case. Unlike model training
    and hyperparameter tuning service, which both have a one-fits-all approach—prediction
    service design heavily depends on concrete user scenarios. For example, designing
    a prediction service that supports just one model, such as a flower recognition
    model, is a lot different than designing a prediction service that supports 10
    different types of models, such as PDF scanning, text intent classification, and
    image classification.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 没有通用的、最具成本效益的模型服务设计；我们需要量身定制适合我们自己用例的模型服务方法。与模型训练和超参数调整服务不同，它们都有一种适用于所有情况的方法——预测服务的设计严重依赖于具体的用户场景。例如，设计一个仅支持一个模型的预测服务，比如花卉识别模型，与设计一个支持10种不同类型模型的预测服务，比如PDF扫描、文本意图分类和图像分类，是完全不同的。
- en: Reduce model prediction latency while maintaining resource saturation. From
    a cost-efficiency perspective, we want our compute resources to be fully saturated
    with model prediction workloads. In addition, we would like to provide our customers
    with a real-time model prediction experience, so we don’t want the prediction
    latency to drop because of the rigid infrastructure budget. To accomplish this,
    we need to reduce the time cost at every step of the prediction workflow innovatively,
    such as loading the model faster or preheating the model before serving.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在保持资源饱和度的同时减少模型预测延迟。从成本效益的角度来看，我们希望我们的计算资源完全饱和于模型预测工作负载。此外，我们希望为客户提供实时的模型预测体验，因此我们不希望由于严格的基础设施预算而导致预测延迟下降。为了实现这一目标，我们需要创新地减少预测工作流的每个步骤的时间成本，比如更快地加载模型或在提供服务之前预热模型。
- en: Model deployment and post-deployment model monitoring are things we should consider
    on day one. Model deployment—progressing a model from training to production—is
    critical for successful model development. We want to advance the model to production
    quickly, and we want to have multiple versions of the model in production, so
    we can evaluate different training algorithms quickly and choose the best model.
    Post-deployment model monitoring can help detect model performance regression;
    it’s a crucial protection mechanism for models in fraud detection and loan approval,
    for instance.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 模型部署和部署后模型监控是我们在第一天就应该考虑的事情。模型部署——将模型从训练推进到生产——对于成功的模型开发至关重要。我们希望快速将模型推进到生产环境，并且我们希望在生产环境中有多个模型版本，这样我们可以快速评估不同的训练算法并选择最佳模型。部署后的模型监控可以帮助检测模型性能退化；这是欺诈检测和贷款批准等模型的关键保护机制。
- en: The good news is that these six challenges are all engineering problems, so
    you will be able to handle them! We will discuss how to address them here and
    in the next chapter.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，这些六个挑战都是工程问题，所以你能够处理它们！我们将在这里和下一章讨论如何解决它们。
- en: 6.1.5 Model serving terminology
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.5 模型服务术语
- en: As we proceed through the chapter, we’d like to refresh your memory of the model
    serving terms. Many terms have various definitions in academia but are interchangeable
    in practice when talking about model serving. The following definitions should
    help you and your colleagues avoid confusion when they are mentioned.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们继续阅读本章，我们希望提醒你模型服务术语。许多术语在学术界有不同的定义，但在实践中讨论模型服务时是可以互换的。以下定义应该帮助你和你的同事在提到它们时避免混淆。
- en: '*Model serving, model scoring**, model inference*, and *model prediction* are
    interchangeable terminologies in the deep learning context. They all refer to
    executing a model with given data points. In this book, we will use *model serving*.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型服务*、*模型评分*、*模型推断*和*模型预测*在深度学习的上下文中是可以互换的术语。它们都指的是使用给定数据点执行模型。在本书中，我们将使用*模型服务*。'
- en: '*Prediction service**, scoring service**, inference service*, and *model serving
    service* are interchangeable; they refer to the web service that allows remote
    model execution. In this book, we use the prediction service.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预测服务*、*评分服务*、*推断服务*和*模型服务*是可以互换的；它们指的是允许远程执行模型的网络服务。在本书中，我们使用预测服务。'
- en: '*Predict* and *inference* are interchangeable in the model serving context;
    they are the entry function related to running the model algorithm. In this book,
    we use *predict*.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在模型服务的上下文中，*预测*和*推断*是可以互换的；它们是与运行模型算法相关的入口函数。在本书中，我们使用*预测*。
- en: '*Prediction request*, *scoring request**,* and *inference request* are interchangeable;
    they refer to the web API request that executes a model to make a prediction.
    In this book, we use *prediction request*.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预测请求*、*评分请求* 和 *推断请求* 是可以互换的；它们指的是执行模型以进行预测的 Web API 请求。在本书中，我们使用 *预测请求*。'
- en: '*Machine learning algorithm*, *training algorithm*, and *model algorithm* are
    interchangeable, as we state in section 6.1.3; the algorithm that runs in model
    training and serving is the same machine learning algorithm (same neural network)
    but in a different execution mode.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*机器学习算法*、*训练算法* 和 *模型算法* 是可以互换的，正如我们在第 6.1.3 节中所述；在模型训练和服务中运行的算法是相同的机器学习算法（相同的神经网络），但处于不同的执行模式。'
- en: '*Model deployment* and *model release* are interchangeable; they indicate the
    process of deploying/copying a trained model (files) to the production environment
    where the business is running, so the customer can benefit from this new model.
    Typically, this refers to loading the model files into the prediction service.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型部署* 和 *模型发布* 是可以互换的；它们指的是将经过训练的模型（文件）部署/复制到业务运行的生产环境中，以便客户可以从这个新模型中受益。通常，这指的是将模型文件加载到预测服务中。'
- en: 6.2 Common model serving strategies
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 常见的模型服务策略
- en: 'Before we review the concrete model serving use cases and prediction service
    designs in section 6.3, let’s first check out the three common model serving strategies:
    direct model embedding, model service, and model server. No matter what you need
    to do for your specific use cases, you can usually take one of the following three
    approaches to build your prediction service.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们审查第 6.3 节中的具体模型服务用例和预测服务设计之前，让我们先了解三种常见的模型服务策略：直接模型嵌入、模型服务和模型服务器。无论你的具体用例需要做什么，通常可以采用以下三种方法之一来构建你的预测服务。
- en: 6.2.1 Direct model embedding
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.1 直接模型嵌入
- en: Direct model embedding means loading the model and running model prediction
    inside the user application’s process. For example, a flower identity–check mobile
    app can load an image classification model directly in its local process and predict
    plant identity from the given photos. The entire model loading and serving happen
    within the model app locally (on the phone), without talking to other processes
    or remote servers.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 直接模型嵌入意味着在用户应用程序的进程内加载模型并运行模型预测。例如，一个花卉识别的手机应用可以直接在其本地进程中加载图像分类模型，并从给定的照片中预测植物身份。整个模型加载和服务都发生在本地模型应用程序内（在手机上），而不需要与其他进程或远程服务器进行通信。
- en: Most user applications, like mobile apps, are written in strongly typed languages,
    such as Go, Java, and C#, but most deep learning modeling code is written in Python.
    It is therefore difficult to embed model code into application code, and even
    if you do, the process can take a while. To facilitate model prediction across
    non-Python processes, deep learning frameworks such as PyTorch and TensorFlow
    provide C++ libraries. Additionally, TensorFlow offers Java ([https://github.com/tensorflow/java](https://github.com/tensorflow/java))
    and JavaScript ([https://github.com/tensorflow/tfjs](https://github.com/tensorflow/tfjs))
    libraries for loading and executing TensorFlow models directly from Java or JavaScript
    applications.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数用户应用程序，如手机应用程序，都是用 Go、Java 和 C# 等强类型语言编写的，但大多数深度学习建模代码是用 Python 编写的。因此，将模型代码嵌入应用程序代码是很困难的，即使你这样做了，这个过程也可能需要一段时间。为了在非
    Python 进程中促进模型预测，PyTorch 和 TensorFlow 等深度学习框架提供了 C++ 库。此外，TensorFlow 还提供了 Java
    ([https://github.com/tensorflow/java](https://github.com/tensorflow/java)) 和 JavaScript
    ([https://github.com/tensorflow/tfjs](https://github.com/tensorflow/tfjs)) 库，用于直接从
    Java 或 JavaScript 应用程序加载和执行 TensorFlow 模型。
- en: Another disadvantage of direct embedding is resource consumption. If the model
    runs on client devices, users without high-end devices may not have a good experience.
    Running big deep learning models requires a lot of computation, and this can cause
    slower apps.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 直接嵌入的另一个缺点是资源消耗。如果模型在客户端设备上运行，没有高端设备的用户可能会有不好的体验。运行大型深度学习模型需要大量的计算，这可能导致应用程序变慢。
- en: Lastly, direct embedding involves mixing model serving code with application
    business logic, which poses a challenge for backward compatibility. Therefore,
    because it is rarely used, we only describe it briefly.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，直接嵌入涉及将模型服务代码与应用程序业务逻辑混合在一起，这对向后兼容性构成了挑战。因此，因为它很少被使用，我们只简要描述它。
- en: 6.2.2 Model service
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.2 模型服务
- en: '*Model* *service* refers to running model serving on the server side. For each
    model, each version of a model, or each type of model, we build a dedicated web
    service for it. This web service exposes the model prediction API over HTTP or
    gRPC interfaces.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*模型服务* 指的是在服务器端运行模型服务。对于每个模型、每个模型的版本或每种类型的模型，我们都为其构建一个专用的 Web 服务。这个 Web 服务通过
    HTTP 或 gRPC 接口公开模型预测 API。'
- en: The model service manages the full life cycle of model serving, including fetching
    the model file from the model artifact store, loading the model, executing the
    model algorithm for a customer request, and unloading the model to reclaim the
    server resources. Using the documents classification use case as an example, to
    automatically sort documents in images and PDF by their content, we can train
    a CNN model for OCR (optical character recognition) to extract text from document
    images or PDF. To serve this model in a model service approach, we build a web
    service exclusively for this CNN model, and the web API is only designed for this
    CNN model’s prediction function. Sometimes we build a dedicated web service for
    each major model version update.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 模型服务管理模型服务的全部生命周期，包括从模型制品存储库中获取模型文件、加载模型、为客户请求执行模型算法以及卸载模型以回收服务器资源。以文档分类用例为例，为了自动按照内容对图像和
    PDF 中的文档进行分类，我们可以训练一个用于光学字符识别（OCR）的 CNN 模型来提取文档图像或 PDF 中的文本。为了在模型服务方法中为这个模型提供服务，我们为这个
    CNN 模型专门构建一个 Web 服务，并且 Web API 仅设计用于这个 CNN 模型的预测函数。有时我们为每个主要模型版本更新构建一个专用的 Web
    服务。
- en: The common pattern of model service is to build the model execution logic into
    a Docker image and use gRPC or HTTP interface to expose the model’s predict function.
    For service setup, we can host multiple service instances and employ a load balancer
    to distribute customers’ prediction requests to these instances.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 模型服务的常见模式是将模型执行逻辑构建到 Docker 镜像中，并使用 gRPC 或 HTTP 接口公开模型的预测函数。对于服务设置，我们可以托管多个服务实例，并使用负载均衡器将客户的预测请求分发到这些实例。
- en: The biggest advantage of the model service approach is simplicity. We can easily
    convert a model’s training container to a model serving container because, essentially,
    a model prediction execution entails running the trained model neural network.
    The model training code can turn into a prediction web service quickly by adding
    an HTTP or gRPC interface and setting the neural network to evaluation mode. We
    will see a model service’s design and use case in sections 6.3.1 and 6.3.2 and
    a concrete code example in chapter 7.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 模型服务方法的最大优势是简单性。我们可以很容易地将模型的训练容器转换为模型服务容器，因为本质上，模型预测执行涉及运行经过训练的模型神经网络。模型训练代码可以通过添加
    HTTP 或 gRPC 接口并设置神经网络为评估模式快速转换为预测 Web 服务。我们将在第 6.3.1 和 6.3.2 节中看到模型服务的设计和用例，并在第
    7 章中看到一个具体的代码示例。
- en: Because model service is specific to the model algorithm, we need to build separate
    services for different model types or versions. If you have several different
    models to serve, this one service-per-model approach can spawn many services,
    and the maintenance work for these services—such as patching, deploying, and monitoring—can
    be exhausting. If you are facing this situation, the model server approach is
    the right choice.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 因为模型服务针对模型算法具体化，所以我们需要为不同的模型类型或版本构建单独的服务。如果您有多个不同的模型需要提供服务，这种一模型一服务的方法可能会产生许多服务，并且维护这些服务的工作——如打补丁、部署和监控——可能会很辛苦。如果您面临这种情况，模型服务器方法是正确的选择。
- en: 6.2.3 Model server
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.3 模型服务器
- en: The model server approach is designed to handle multiple types of models in
    a black-box manner. Regardless of the model algorithm and model version, the model
    server can operate these models with a unified web prediction API. The model server
    is the next stage; we no longer need to make code changes or deploy new services
    with a new type of model or new version of the model. This saves a lot of duplicate
    development and maintenance work from the model service approach.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 模型服务器方法旨在以黑盒方式处理多种类型的模型。无论模型算法和模型版本如何，模型服务器都可以使用统一的 Web 预测 API 操作这些模型。模型服务器是下一阶段；我们不再需要为新型模型或模型新版本进行代码更改或部署新服务。这从模型服务方法中节省了许多重复的开发和维护工作。
- en: Yet, the model server approach is a lot more complicated to implement and manage
    than the model service approach. Handling model serving for various types of models
    in one service and one unified API is complicated. The model algorithms and model
    data are different; their predict functions are also different. For example, an
    image classification model can be trained with a CNN network, whereas a text classification
    model can be trained with a long short-term memory (LSTM) network. Their input
    data is different (text vs. image), and their algorithms are different (CNN vs.
    LSTM). Their model data also varies; text classification models require embedding
    files to encode input text whereas CNN models don’t require embedding files. These
    differences present many challenges to finding a low-maintenance, low-cost, and
    unified serving approach.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Although building a model server approach is difficult, it’s definitely possible.
    Many open source model serving libraries and services—such as TensorFlow Serving,
    TorchServe, and NVIDIA Triton Inference Server—offer model server solutions. We
    simply need to build customized integration logic to incorporate these tools into
    our existing systems to solve business needs—for example, integrating TorchServe
    into our model storage, monitoring, and alerting system.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: From a model deployment perspective, the model server is a black-box approach.
    As long as we save the model file following the model server standards, the model
    prediction should function when we upload the model-to-model server through its
    management API. The complexity of model serving implementation and maintenance
    can be greatly reduced. We will see a model server design and use case in section
    6.3.3 and a code example with TorchServe in chapter 7.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Note Should we always consider a model server approach? Not always. If we don’t
    think of service development cost and maintenance cost, the model server approach
    is the most powerful because it’s designed to cover all types of models. But if
    we care about model serving cost efficiency—and we should!—then the ideal approach
    depends on the use cases. In the next section, we will discuss the common model
    serving use cases and the applied design.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Designing a prediction service
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A common mistake in software system design is aiming to build an omnipotent
    system without considering the concrete user scenario. Overdesign will redirect
    our focus from the immediate customer needs to the features that might be useful
    in the future. As a result, the system either takes an unnecessarily long time
    to build or is difficult to use. This is especially true for model serving.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning is an expensive business, both in terms of human and computational
    resources. We should build only the necessities to move models into production
    as quickly as possible and minimize the operation costs. To do so, we need to
    begin with user scenarios.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will present three typical model serving scenarios, from
    simple to complex. For each use case, we explain the scenario and illustrate a
    suitable high-level design. By reading the following three subsections sequentially,
    you will see how the prediction service’s design evolves when use cases become
    more and more complicated.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Note The goal of prediction service design is not to build a powerful system
    that works for various models but to build a system that suits the circumstances
    in a cost-efficient manner.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.1 Single model application
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Imagine building a mobile app that can swap people’s faces between two pictures.
    The consumer expects the app UI to upload photos, select sources and target pictures,
    and execute a deepfake model ([https://arxiv.org/abs/1909.11573](https://arxiv.org/abs/1909.11573))
    for swapping faces between the two selected images. For an application like this
    that only needs to work with one model, the serving approach can be either model
    service (6.2.2) or direct model embedding (6.2.1).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Model service approach
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: 'From the discussion in section 6.2.2, the model service approach involves building
    a web service for each model. So we can build the face-swap model app with the
    following three components: a front UI app (component A) that runs on our phone;
    an application backend to handle user operation (component B); and a backend service,
    or *predictor* (component C), to host a deepfake model and expose a web API to
    execute the model for each face-swap request.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: When a user uploads a source image and a target image and clicks the face-swap
    button on the mobile app, the mobile backend application will receive the request
    and call the predictor’s web API for face-swapping. Then the predictor preprocesses
    the user request data (the images), executes the model algorithm, and postprocesses
    the model output (the images) to the application backend. Ultimately, the mobile
    app will display the source and target images with swapped faces. Figure 6.4 illustrates
    a general design that suits the face-swap use case.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/06-04.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 A single model predictor design in a client/server setup
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: If we zoom into the predictor (component C), we see that the model serving logic
    works the same as the general model prediction workflow that we introduced in
    figure 6.3\. The predictor (model serving service) loads the model file from the
    model artifactory and runs the model to respond to the request received by the
    web interface.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: The design in figure 6.4 generally works for any application that has a web
    backend and only one model. The key component in this design is the predictor;
    it is a web service and often runs as a Docker container. We can implement this
    approach quickly because this predictor container can be easily converted from
    the model training container that builds the model. The two main work items that
    transform a training container to a predictor container are the web predict API
    and the evaluation mode in the training neural network. We will present a concrete
    predictor container example in section 7.1.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4中的设计通常适用于具有Web后端和只有一个模型的任何应用程序。该设计的关键组件是预测器；它是一个Web服务，通常作为Docker容器运行。我们可以快速实现这个方法，因为预测器容器可以从构建模型的训练容器中轻松转换。将训练容器转换为预测器容器的两个主要工作项是Web预测API和训练神经网络中的评估模式。我们将在第7.1节中介绍一个具体的预测器容器示例。
- en: Direct model embedding approach
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 直接模型嵌入方法
- en: Another design approach for building a single model application is combining
    the model execution code with the application’s user logic code. There is no server
    backend, so everything happens locally on the user’s computer or phone. Using
    the face swap app as an example, the deepfake model file is in the application’s
    deployment package, and when the application starts, the model is loaded into
    the application’s process space. Figure 6.5 illustrates this concept.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 构建单一模型应用的另一种设计方法是将模型执行代码与应用的用户逻辑代码结合起来。没有后端服务器，所有操作都在用户的计算机或手机上本地完成。以换脸应用为例，深度伪造模型文件在应用部署包中，当应用启动时，模型被加载到应用的进程空间中。图6.5展示了这个概念。
- en: '![](../Images/06-05.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06-05.png)'
- en: Figure 6.5 In the direct model embedding design, the model is executed in the
    same process as the application logic.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 在直接模型嵌入设计中，模型在应用逻辑的同一进程中执行。
- en: Model serving doesn’t have to run in a separate service. In figure 6.5, we see
    that the model serving code (the single model box) and the data transformation
    code can run with the user logic code in the same application. Nowadays, many
    deep learning frameworks provide libraries to run models in non-Python applications.
    For example, TensorFlow offers Java, C++, and JavaScript SDK to load and execute
    models. With SDK’s help, we can train and execute models directly in Java/C++/
    JavaScript applications.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 模型服务不一定要在独立的服务中运行。在图6.5中，我们可以看到模型服务代码（单一模型框）和数据转换代码可以与用户逻辑代码在同一个应用中运行。现在，很多深度学习框架都提供了在非Python应用中运行模型的库。例如，TensorFlow提供了Java、C++和JavaScript的SDK来加载和执行模型。借助SDK的帮助，我们可以直接在Java/C++/JavaScript应用中训练和执行模型。
- en: Note Why should we consider direct model embedding? By using model embedding,
    we can directly integrate model serving logic with application logic and run them
    together in the same process space. This provides two advantages over the predictor
    service approach in figure 6.4\. First, it reduces one network hop; there is no
    web request to the predictor, and model execution happens locally. Second, it
    improves service debuggability because we can run the application as one piece
    locally.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 注意为什么应该考虑直接模型嵌入？通过使用模型嵌入，我们可以直接将模型服务逻辑与应用逻辑集成并在同一个进程空间中运行它们。这相对于图6.4中的预测器服务方法有两个优势。首先，它减少了一次网络跳转；没有对预测器的Web请求，模型执行在本地进行。其次，它提高了服务的调试能力，因为我们可以将应用作为一个整体在本地运行。
- en: Why is the model service approach more popular?
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么模型服务方法更受欢迎？
- en: 'Although the direct model embedding approach looks simple and saves one network
    hop, it is still not a popular choice for building model serving. Here are the
    four reasons:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然直接模型嵌入方法看起来简单并且可以节省一次网络跳转，但它仍然不是构建模型服务的常见选择。以下是四个原因：
- en: The model algorithm has to be reimplemented in a different language. A model’s
    algorithm and execution code is usually written in Python. If we choose a model
    service approach, implementing the model serving as a web service (predictor in
    figure 6.4), we can reuse most of the training code and build it quickly. But
    if we choose to embed model serving in a non-Python application, we must reimplement
    model loading, model execution, and data process logic in the application’s language
    (such as Java or C++). This work is nontrivial, and not many developers have the
    depth of knowledge to rewrite the training algorithms.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ownership boundary is blurred. When embedding a model into an application,
    the business logic code can mingle with the serving code. When the codebase becomes
    complicated, it’s difficult to draw a boundary between the serving code (owned
    by the data scientist) and other application code (owned by the developer). When
    data scientists and developers are from two different teams but work on the same
    code repo, the shipping velocity will drop significantly because the cross-team
    code review and deployment takes longer than usual.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance problems can occur on the client’s devices. Usually, apps are run
    on the customer’s mobiles, tablets, or lower-end laptops. On these devices, capturing
    features from raw user data and then preprocessing model input data and running
    model prediction can lead to performance problems such as CPU usage spikes, app
    slowdown, and high memory usage.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A memory leak can occur easily. For example, when executing a TensorFlow model
    in Java, the algorithm execution and input/output parameter objects are all created
    in the native space. These objects won’t be recycled by Java GC (Garbage Collection)
    automatically; we have to manually depose them. It’s very easy to overlook recycling
    the native resources claimed by the model, and because the native objects’ memory
    allocations are not tracked in Java heap, their memory usage is difficult to observe
    and measure. So the memory leak can happen and is hard to fix.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note To troubleshoot native memory leaks, Jemalloc ([https://github.com/jemalloc/jemalloc/wiki/Background](https://github.com/jemalloc/jemalloc/wiki/Background))
    is a very handy tool. You can check out my blog post “Fix Memory Issues in Your
    Java Apps” ([http://mng.bz/lJ8o](http://mng.bz/lJ8o)) for further details.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: For the previously listed reasons, we highly recommend you adopt the model service
    approach for single model application use cases.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.2 Multitenant application
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will use a chatbot application as an example to explain multitenant use cases.
    First, let’s set the context. A *tenant* is a company or organization (such as
    a school or a retail store) that uses the chatbot application to communicate with
    its customers. The tenants use the same software/service—the chatbot application—but
    have separate accounts with their data segregated. A *chat user* is the customer
    of a tenant and uses the chatbot to do business with the tenant.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: By design, the chatbot application relies on an intent classification model
    to identify the user’s intention from his conversation, and then the chatbot redirects
    the user request to the correct service department of the tenant. Currently, this
    chatbot is taking a single model application approach, meaning it’s using a single
    intent classification model for every user and tenant.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Now, because of customer feedback from tenants on the low prediction accuracy
    of the single intent classification model, we decide to let tenants use our training
    algorithm to build their own model with their own dataset. This way, the model
    can fit better with each tenant’s business situation. For model serving, we will
    let tenants use their own model for intent classification prediction requests.
    When a chatbot user now speaks to the chatbot application, the application will
    find the tenant’s specific model to answer the user’s question. The chatbot is
    changed to a multitenant application.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: In this chatbot multitenant use case, although the models belong to different
    tenants and are trained with different datasets, they are the same type of model.
    Because these models are trained with the same algorithm, their model’s algorithm
    and predict function are all the same. We can extend the model service design
    in figure 6.4 to support multitenancy by adding a model cache. By caching model
    graphs and their dependent data in memory, we can perform multitenancy model serving
    in one service. Figure 6.6 illustrates this concept.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Compared with the model service design in figure 6.4, the design in figure 6.6
    adds a model cache (component A) and a model file server (component B). Because
    we want to support multiple models in one service, we need a model cache in memory
    to host and execute different models. The model file server stores the model files
    that can be loaded into the prediction service’s model cache. The model server
    can also be shared among prediction service instances.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: To build a good model cache, we need to consider model cache management and
    memory resource management. For the model cache, we need to assign a unique model
    ID as a cache key to identify each model in the cache. For example, we can use
    the model training run ID as the model ID; the benefit is, for each model in the
    cache, we can trace which training run produced it. Another more flexible way
    of constructing the model ID is combining the model name (a customized string)
    and the model version. No matter which model ID style we choose, the ID has to
    be unique, and it must be provided in the prediction request.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/06-06.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 A prediction service with model caching for multitenant applications
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: For memory resource management, because each server has limited memory and GPU
    resources, we can’t load all the required models into memory. So we need to build
    model-swapping logic to the model cache. When the resource capacity is reached—for
    instance, the process is about to run out of memory—some models need to be evicted
    from the model cache to free some resources for new model prediction requests.
    Methods like LRU (least recently used) algorithm and model partition across different
    instances can help reduce the cache missing rate (the request model is not in
    the cache) and make model swapping less disruptive. The sample intent classification
    prediction service we build in section 7.1 demonstrates the model caching concept;
    you can explore the details there.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Can we extend the model caching design to multiple model types?
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: We don’t recommend extending the model caching design to multiple model types.
    The input/output data format and data process logic of various model types, such
    as the image classification model and intent classification model, are very different,
    so it’s hard to host and execute different types of models in the same model cache.
    To do that, we would need to build separate web interfaces and separate data preprocess
    and postprocess code for each type of model. At this point, you will find it’s
    easier to build separate prediction services for each model type—with each service
    having its own type of web interface and data process logic and managing the model
    cache for its own model type. For example, we can build an image classification
    prediction service and an intent classification prediction service for these two
    different model types separately.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: This one service per model type approach works well when you only have a few
    model types. But if you have 20+ types of models, then it can’t scale. Building
    and maintaining web services—such as setting up a CI/CD pipeline, networking,
    and deployment—is costly. Also, the work of monitoring a service is nontrivial;
    we need to build monitoring and alerting mechanisms to ensure the service is running
    24/7\. Consider the costs of onboarding and maintenance work if we follow this
    design to support 100+ model types for the entire company. To scale up and serve
    lots of different model types in one system, we need to take the model server
    approach (section 6.2.3), which we will discuss further in the next section.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.3 Supporting multiple applications in one system
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You have successfully built multiple model serving services to support different
    applications, such as multitenant chatbot, face-swapping, flower recognition,
    and PDF document scanning. Now, you are given two more tasks: (1) building the
    model serving support for a new application that uses a voice recognition model
    and (2) reducing model serving costs for all applications.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: So far, all the model serving implementations have been built with the model
    service approach. From previous discussions in sections 6.3.1 and 6.3.2, we know
    this approach can’t scale when we have more and more model types. When many products
    and applications have model serving requirements, it’s better to build just one
    centralized prediction service to address all the serving needs. We name this
    type of prediction service a *prediction platform*. It takes the model server
    approach (section 6.2.3) and handles all kinds of model serving in one place.
    This is the most cost-efficient approach for multiple application situations because
    the model onboarding and maintenance cost is limited to one system, which is much
    less than one prediction service per application approach (section 6.2.2).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: To build such an omnipotent model serving system, we need to consider lots of
    elements, such as model file format, model libraries, model training frameworks,
    model caching, model versioning, model flow execution, model data processing,
    model management, and a unified prediction API that suits all model types. Figure
    6.7 illustrates the design and workflow of the prediction platform.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: The prediction platform design in figure 6.7 is much more complicated than the
    model service approach in figure 6.6\. This is because we need to combine multiple
    components and services to support arbitrary models. Let’s look at each component
    of the system and then the model prediction workflow.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/06-07.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 A general prediction service (platform) design that works with arbitrary
    model types
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Unified web API
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: To support arbitrary models, we expect the public prediction APIs to be generic.
    No matter which model is called, the API’s spec—for instance, its payload schema
    of the prediction request and response—should be generic enough to satisfy the
    model’s algorithm requirement. One example of this kind of unified API is the
    KFServing predict protocol ([http://mng.bz/BlB2](http://mng.bz/BlB2)), which aims
    to standardize the prediction protocol that works for any models and various prediction
    backends.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 'The web APIs are also expected to be simple, so we can reduce the customer
    onboarding and maintenance effort. The prediction APIs can be categorized into
    three buckets: model prediction requests API, model metadata fetching API, and
    model deployment API. The model metadata fetching API and deployment API are very
    useful because they are agnostic about the model they are serving. We need these
    methods to check the model metadata, such as the model version and algorithm info,
    and to check the model deployment status.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Routing component
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Normally, each type of serving backend can only handle a few types of models.
    To support arbitrary models, we need to have different kinds of serving backends,
    such as TensorFlow Serving backend for TensorFlow models and TorchServe backend
    for PyTorch models. When receiving a model prediction request, the system needs
    to know which backend can handle it. This is done with the routing component.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: The routing component responds to route the prediction request to the correct
    backend inference server. For a given request, the routing component first fetches
    the model’s metadata; the metadata includes the model algorithm name and version,
    the model version, and the training framework. Then, by matching the model metadata
    with the routing config, it determines to which inference backend it should route
    the prediction request.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Graph execution component
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'The graph execution component handles the type of prediction that needs to
    execute a series of model predictions. For example, to automate the mortgage approval
    process, we have to run a loan approval prediction request following three models
    in a sequence: a PDF scanning model to parse the text from the PDF loan application,
    a named entity recognition model to recognize the keywords, and a loan-scoring
    model to score the loan application. To support such requirements, we can define
    a directed acyclic graph (DAG) to describe the model execution chain and build
    a graph execution engine to execute in one go.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Inference server
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: The inference (model) server does the actual work to compute model prediction
    by managing model caching and model prediction execution. It’s similar to the
    prediction service shown in figure 6.6 but more sophisticated because it needs
    to support arbitrary model algorithms. Besides the predict API, the inference
    server should also offer model management API to register new models and remove
    models programmatically.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Building an inference server is much more complicated than building a predictor
    service; not many engineers want to try that. But luckily, there are multiple
    black-box, open source approaches that work out of the box, such as TensorFlow
    Serving, TorchServe, and NVIDIA Triton Inference Server. In practice, we often
    reuse these existing open source inference servers and integrate them into our
    own routing component and graph execution component. We will discuss more on the
    open source model server tools in chapter 7.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Applications
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: In figure 6.7, we see applications A, B, and C are sharing the same model serving
    backend. The model serving for different applications occurs at the same place.
    Compared with the model service design in figure 6.6, the prediction platform
    is more scalable and more cost-efficient because there is almost no onboarding
    cost to add new application D.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we want to onboard new application D—a voice-to-text scripting
    application—we just need to upload the voice scripting model to the model file
    server and then let the application use the unified prediction web API of the
    prediction platform. There is no code change on the prediction platform side for
    supporting a new application.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Model prediction workflow
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: After explaining each key component, let’s look at a typical model prediction
    workflow (figure 6.7). First, we publish our model files to the model file server
    and update the config in the routing component, so the routing component knows
    to which inference server it should route the prediction request for this type
    of model. Second, applications send prediction requests to the prediction system’s
    web APIs, and then the request is routed by the routing component to the correct
    inference server. Third, the inference server will load the model from the model
    file server, convert the request payload to model input, run the model algorithm,
    and return the prediction result with a postprocess.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Note Prediction platform design is not always the best serving approach! In
    theory, the design in figure 6.7 can work for any model, but it does come with
    some extra cost. Its setup, maintenance, and debugging are way more complicated
    than the model service approach. This design is overkill for scenarios introduced
    in sections 6.3.1 and 6.3.2\. Because each design has its merits, we recommend
    not adhering to one serving approach but choosing the serving method based on
    your actual user scenarios.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.4 Common prediction service requirements
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Although we state that designing prediction services should start from concrete
    use cases, different situations lead to different designs. Three common requirements
    exist among all model serving designs:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '*Model deployment safety*—No matter what model rollout strategy and version
    strategy we choose, we must have a way to roll back a model to the previous state
    or version.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Latency*—Web request latency is a crucial factor in the success of many online
    businesses. Once we build the model serving support, the next step is to try our
    best to reduce the average prediction response time.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Monitoring and alerting*—Model serving is the most critical service in a deep
    learning system; if it goes down, the business stops. Remember, actual businesses
    run on top of the model prediction in realtime. Customers are affected immediately
    if the service is down or serving latency increases. Prediction service should
    be the most-equipped service among other deep learning services in monitoring
    and alerting.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we have reviewed concepts, definitions, and abstract high-level
    system designs of model serving. We hope you gain a clear picture of what model
    serving is and what to consider when designing model serving systems. In the next
    chapter, we will demo two sample prediction services and discuss the commonly
    used prediction open source tools. These examples will show you how the design
    concepts in this chapter are applied in real life.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A model can be several files; it is composed of three elements: machine learning
    algorithm, model executor (wrapper), and model data.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model prediction and model inference have the same meaning in the model serving
    context.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Direct model embedding, model service, and model server are the three common
    types of model serving strategies.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model service approach involves building a prediction service for each model,
    each version of a model, or each type of model.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model server approach consists of building only one prediction service,
    but it can run models trained with different algorithms and frameworks and can
    run different versions of each model.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When designing a model serving system, the first component to understand is
    the use case, so we can decide which serving approach is most appropriate.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cost efficiency is the primary goal for designing model serving systems; the
    cost includes service deployment, maintenance, monitoring, infrastructure, and
    service development.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For single model applications, we recommend the model service approach.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For multitenant applications, we recommend the model service approach with an
    in-memory model cache.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For supporting multiple applications with different types of models, the model
    server and prediction platform are the most suitable approaches. They include
    a unified prediction API, a routing component, a graph execution component, and
    multiple model server backends.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
