- en: 6 Model serving design
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 模型服务设计
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Defining model serving
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义模型服务
- en: Common model serving challenges and approaches
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常见模型服务挑战和方法
- en: Designing model serving systems for different user scenarios
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计不同用户场景下的模型服务系统
- en: '*Model serving* is the process of executing a model with user input data. Among
    all the activities in a deep learning system, model serving is the closest to
    the end customers. After all the hard work of dataset preparation, training algorithm
    development, hyperparameter tuning, and testing results in models is completed,
    these models are presented to customers by model serving services.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*模型服务*是使用用户输入数据执行模型的过程。在深度学习系统中的所有活动中，模型服务是最接近最终客户的活动。在完成了数据集准备、训练算法开发、超参数调整和测试结果生成模型的所有辛勤工作之后，这些模型由模型服务服务呈现给客户。'
- en: Take speech translation as an example. After training a sequence-to-sequence
    model for voice translation, the team is ready to present it to the world. For
    people to use this model remotely, the model is usually hosted in a web service
    and exposed by a web API. Then we (the customers) can send our voice audio file
    over the web API and get back a translated voice audio file. All the model loading
    and execution happens at the web service backend. Everything included in this
    user workflow—service, model files, and model execution—is called *model serving*.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 以语音翻译为例。在为语音翻译训练了一个序列到序列模型之后，团队准备向世界展示它。为了让人们远程使用这个模型，通常会将模型托管在 Web 服务中，并通过
    Web API 公开。然后我们（客户）可以通过 Web API 发送我们的语音音频文件，并获得一个翻译后的语音音频文件。所有模型加载和执行都发生在 Web
    服务后端。包括在这个用户工作流程中的一切——服务、模型文件和模型执行——都被称为*模型服务*。
- en: Building model serving applications is another special deep learning domain
    for which software engineers are particularly well suited. Model serving uses
    request latency, scalability, availability, and operability—all areas that engineers
    know inside and out. With some introduction to the concepts of deep learning model
    serving, developers who have some experience with distributed computing can play
    a significant role in building the model serving element.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 构建模型服务应用程序是另一个特殊的深度学习领域，软件工程师特别适合这个领域。模型服务使用请求延迟、可伸缩性、可用性和可操作性——所有这些都是工程师内外熟知的领域。通过一些深度学习模型服务概念的介绍，有一些分布式计算经验的开发人员可以在构建模型服务元素方面发挥重要作用。
- en: Serving models in production can be challenging because models are trained by
    various frameworks and algorithms, so the methods and libraries to execute the
    model vary. Also, the terminology used in the model serving field is confusing,
    with too many terms, like *model prediction* and *model inference*, that sound
    different but mean the same thing in the serving context. Furthermore, there are
    many model serving options from which to choose. On the one hand, we have black-box
    solutions like TensorFlow Serving, TorchServe, and NVIDIA Triton Inference Server.
    On the other, we have customized approaches like building your own predictor service
    or embedding models directly into your applications. These approaches all seem
    very similar and capable, so it is hard to select one over another. Therefore,
    if you are new to this domain, you can quickly become lost.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型在生产中提供服务可能很具有挑战性，因为模型是由各种框架和算法训练的，因此执行模型的方法和库各不相同。此外，模型服务领域使用的术语令人困惑，有太多不同的术语，如*模型预测*和*模型推理*，听起来不同但在服务上下文中意思相同。此外，有许多模型服务选项可供选择。一方面，我们有像
    TensorFlow Serving、TorchServe 和 NVIDIA Triton 推理服务器等黑盒解决方案。另一方面，我们有像构建自己的预测服务或直接将模型嵌入应用程序中这样的定制方法。这些方法看起来都非常相似且功能强大，因此很难选择其中一个。因此，如果您对这个领域还不熟悉，您可能会很快迷失方向。
- en: 'Our goal here is to help you find your way. We hope to empower you to design
    and build the model serving solution that best fits your situation. To achieve
    this goal, we have lots of content to cover, from the conceptual understanding
    of model serving and service-design considerations to concrete examples and model
    deployment workflow. To avoid exhausting you with a super-long chapter, we divide
    this content into two chapters: chapter 6 focuses on concepts, definitions, and
    design, and chapter 7 puts those concepts into practice, including building a
    sample prediction service and addressing open source tools as well as deploying
    and monitoring model production.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是帮助你找到自己的方向。我们希望能赋予你设计和构建最适合你情况的模型服务解决方案的能力。为了实现这个目标，我们有很多内容需要介绍，包括模型服务的概念理解、服务设计考虑因素、具体示例和模型部署工作流程。为了避免让你阅读超长的一章内容，我们将这部分内容分成了两章：第6章重点关注概念、定义和设计，第7章将这些概念付诸实践，包括构建一个样本预测服务，介绍开源工具以及部署和监控模型生产。
- en: In this chapter, we start by clarifying the terminology and providing our own
    definitions of the elements used in model serving. We also describe the main challenges
    facing us in the model serving field. Then we will move to the design aspect,
    explaining the three common strategies of model serving and designing a model
    serving system that fits different use cases.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先澄清术语，并为模型服务中使用的元素提供我们自己的定义。我们还描述了我们在模型服务领域面临的主要挑战。然后我们将转向设计方面，解释模型服务的三种常见策略，并设计一个适合不同用例的模型服务系统。
- en: By reading this chapter, you will not only gain a solid understanding of how
    model serving works, but you will also know the common design patterns that can
    address most of the model serving use cases. With the concepts and terminology
    clear in your mind, you should be comfortable joining any model serving–related
    discussion or reading articles and papers on the topic. And, of course, this chapter
    builds the foundation so you can follow the practical work addressed in the next
    chapter.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 通过阅读本章，你不仅会对模型服务的工作原理有扎实的理解，还会了解到可以应对大多数模型服务用例的常见设计模式。随着概念和术语在你脑海中变得清晰，你应该可以自如地参与任何与模型服务相关的讨论，或者阅读关于这个主题的文章和论文。当然，本章也为你在下一章介绍的实际工作奠定了基础。
- en: 6.1 Explaining model serving
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 解释模型服务
- en: In the engineering of model serving, the terminology is a major problem. For
    example, *model*, *model architecture*, *inference graph*, *prediction*, and *inference*
    are all terms people use without clearly defining them, so they can have the same
    meaning or refer to different concepts depending on the context (model serving
    or model training). When we work with data scientists to build model serving solutions,
    the confusion around model serving terms causes a lot of miscommunication. In
    this section, we will explain the core concepts of model serving and interpret
    commonly used terminologies from an engineering perspective to help you avoid
    falling into the terminology trap.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型服务的工程中，术语是一个主要问题。例如，*模型*、*模型架构*、*推理图*、*预测*和*推理*等术语被人们使用时没有清晰地定义它们，因此它们可以具有相同的含义，也可以根据上下文（模型服务或模型训练）而指代不同的概念。当我们与数据科学家合作构建模型服务解决方案时，模型服务术语的混淆会导致很多交流不畅。在本节中，我们将从工程角度解释模型服务的核心概念，并对常用术语进行解释，以帮助你避免陷入术语陷阱。
- en: 6.1.1 What is a machine learning model?
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.1 什么是机器学习模型？
- en: There are multiple definitions of machine learning models in academia, from
    distilled representations of learnings of datasets to mathematical representations
    for recognizing certain patterns or making decisions based on previously unseen
    information. Nevertheless, as model serving developers, we can understand a model
    simply as a collection of files that are produced during training.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在学术界对机器学习模型有多种定义，从对数据集学习的精简表达到基于以前未见过的信息识别特定模式或做出决策的数学表达。然而，作为模型服务开发人员，我们可以简单地将模型理解为在训练过程中产生的一组文件的集合。
- en: The idea of a model is simple, but many people misunderstand that models are
    just static files. Although models are saved as files, they aren’t static; they’re
    essentially executable programs.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的概念很简单，但很多人误解模型只是静态文件。虽然模型被保存为文件，但它们并不是静态的，它们实质上是可执行的程序。
- en: Let’s take apart that statement and determine what it means. A model consists
    of a machine learning algorithm, model data, and a model executor. A *model executor*
    is a wrapper code of the machine learning algorithm; it takes user input and runs
    the algorithm to compute and return prediction results. A *machine learning algorithm*
    refers to the algorithm used in model training, sometimes also called *model architecture*.
    Using speech translation as an example again, if the translation model is trained
    by a sequence-to-sequence network as its training algorithm, the machine learning
    algorithm in the model is the same sequence-to-sequence network. *Model data*
    is the data required to run the machine learning algorithm, such as the neural
    network’s learned parameters (weights and biases), embeddings, and label classes.
    Figure 6.1 illustrates a generic model structure.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解这个语句并确定其含义。一个模型由机器学习算法、模型数据和模型执行器组成。*模型执行器*是机器学习算法的封装代码；它接收用户输入并运行算法来计算和返回预测结果。*机器学习算法*是指模型训练中使用的算法，有时也称为*模型架构*。再以语音翻译为例，如果翻译模型是由序列到序列网络作为其训练算法，则模型中的机器学习算法就是相同的序列到序列网络。*模型数据*是运行机器学习算法所需的数据，例如神经网络的学习参数（权重和偏差）、嵌入和标签类别等。图6.1展示了一个通用的模型结构。
- en: '![](../Images/06-01.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06-01.png)'
- en: Figure 6.1 A model is composed of a machine learning algorithm, model executor,
    and model data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 一个模型由机器学习算法、模型执行器和模型数据组成。
- en: Note We often refer to machine learning algorithms as *model algorithms* in
    this chapter for simplicity.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 在本章中，我们经常简称机器学习算法为*模型算法*。
- en: The most important takeaway in this section is that the output of a model training
    execution—or simply, a model—isn’t just a set of static data. In contrast, deep
    learning models are executable programs that include a machine learning algorithm
    and its dependent data, so the models can make predictions based on input data
    at run time.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中最重要的要点是，模型训练执行的输出，也就是模型，并不仅仅只是一组静态数据。相反，深度学习模型是可执行程序，包括机器学习算法及其依赖的数据，因此模型可以根据运行时的输入数据进行预测。
- en: Note Models are not only weights and biases. Sometimes data scientists save
    a neural network’s trained parameters—weights and biases—to a file and name it
    “model file.” This confuses people into thinking a model is just a data file that
    contains only weights and biases. Weights and biases are the model *data*, but
    we also need the algorithm and the wrapper code to run the prediction.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 模型不仅仅包括权重和偏差。有时数据科学家将神经网络的训练参数（权重和偏差）保存到一个文件中，并命名为“模型文件”。这会让人们误以为模型只是一个只包含权重和偏差的数据文件。权重和偏差是模型的*数据*，但我们还需要算法和封装代码来运行预测。
- en: 6.1.2 Model prediction and inference
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.2 模型预测和推断
- en: Academics may consider model inference and prediction to be two separate concepts.
    A model inference can refer to learning about how data is generated and understanding
    its causes and effects, whereas a model prediction might refer to predicting future
    events.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 学术界可能认为模型推断和预测是两个不同的概念。模型推断可以指学习数据是如何生成的、理解其原因和影响，而模型预测则可能指对未来事件的预测。
- en: A sample model prediction scenario might include using sales records to train
    a model to predict which individuals are likely to respond to the next marketing
    campaign. And a sample model inference scenario would include using sales records
    to train a model to understand the sales effect from the product price and customer
    income. The predictive accuracy on previously unseen data for model inference
    is not very important because the main focus is on learning the data generation
    process. Model training is designed to fit the full dataset.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一个样本模型预测的场景可能包括使用销售记录来训练一个模型，以预测哪些个体可能会对下一次营销活动做出回应。而一个样本模型推断的场景将包括使用销售记录来训练一个模型，从产品价格和客户收入的角度理解销售效果。对于模型推断来说，之前未见过的数据上的预测准确性并不是非常重要，因为主要关注的是学习数据生成过程。模型训练的目的是拟合整个数据集。
- en: 'From an engineering perspective, model prediction and model inference mean
    the same. Although models can be built and used for different purposes, both model
    prediction and model inference in the context of model serving refer to the same
    action: executing the model with given data points to obtain a set of output values.
    Figure 6.2 illustrates the model serving workflow for the prediction model and
    the inference model; as you can see, there is no difference between them.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 从工程的角度来看，模型预测和模型推断意味着相同。虽然模型可以被建立和用于不同的目的，但是在模型服务的上下文中，模型预测和模型推断指的是同样的行为：使用给定的数据点执行模型以获得一组输出值。图
    6.2 展示了预测模型和推断模型的模型服务工作流程；正如你所见，它们之间没有区别。
- en: '![](../Images/06-02.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06-02.png)'
- en: Figure 6.2 Model prediction and model inference are the same in model serving
    engineering.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2 模型预测和模型推断在模型服务工程中是相同的。
- en: To simplify the text in the illustrations of this chapter, starting from figure
    6.2, we use the word *model* to represent model data, model executor, and machine
    learning (model) algorithm. This is not only to keep the text short but also to
    emphasize that the machine learning model is an executable program.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化本章中插图中的文本，从图 6.2 开始，我们使用 *模型* 一词来表示模型数据、模型执行者和机器学习 (模型) 算法。这不仅是为了保持文本简洁，也强调了机器学习模型是可执行程序。
- en: 6.1.3 What is model serving?
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.3 什么是模型服务？
- en: '*Model* *serving* simply means executing a model with input data to make predictions,
    which includes fetching the expected model, setting up the model’s execution environment,
    executing the model to make a prediction with given data points, and returning
    the prediction result. The most used method for model serving is to host models
    in a web service and expose the model’s predict function through a web API.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*模型服务* 意味着简单地使用输入数据执行模型进行预测，这包括获取预期的模型、设置模型的执行环境、使用给定的数据点执行模型进行预测，并返回预测结果。模型服务最常用的方法是在
    Web 服务中托管模型，并通过 Web API 公开模型的预测功能。'
- en: Suppose we build an object detection model to detect sharks in seacoast images;
    we can build a web service to host this model and expose a shark detection web
    API. This web API can then be used by beach hotels anywhere in the world to detect
    sharks with their own coast images. Conventionally, we call the model serving
    web service the prediction service.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们构建了一个目标检测模型，用于检测海岸图片中的鲨鱼；我们可以构建一个网络服务来托管这个模型，并公开一个鲨鱼检测的 Web API。然后，世界上任何海滨酒店都可以使用这个
    Web API 来检测他们自己海岸图片中的鲨鱼。在传统上，我们称模型服务的网络服务为预测服务。
- en: 'A typical model prediction workflow in a prediction service has four steps:
    receiving a user request; loading the model from an artifact store to memory or
    GPU; executing the model’s algorithm; and, finally, returning the prediction results.
    Figure 6.3 shows this workflow.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 预测服务中的典型模型预测工作流程有四个步骤：接收用户请求；从工件存储加载模型到内存或 GPU；执行模型的算法；最后返回预测结果。图 6.3 展示了这个工作流程。
- en: '![](../Images/06-03.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06-03.png)'
- en: Figure 6.3 A typical model prediction workflow in a prediction service
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3 预测服务中的典型模型预测工作流程
- en: 'Besides the four-step prediction workflow, figure 6.3 also mentions three main
    components of the model serving: the prediction service (A), the model artifactory
    store (B), and the prediction web API (C). The model artifactory store (component
    B) holds all the models produced by the model training. The web API (component
    C) receives prediction requests. The prediction service (component A) responds
    to the prediction request, loads the model from the artifactory store, runs the
    model, and returns the prediction result.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 除了四步预测工作流程之外，图 6.3 还提到了模型服务的三个主要组件：预测服务 (A)、模型工件存储 (B) 和预测 Web API (C)。模型工件存储
    (组件 B) 包含模型训练生成的所有模型。Web API (组件 C) 接收预测请求。预测服务 (组件 A) 响应预测请求，从工件存储加载模型，运行模型，并返回预测结果。
- en: Although the four steps of the prediction workflow are generally applicable
    to all kinds of models, the actual implementation of the steps depends on the
    business needs, model training algorithm, and model training framework. We will
    discuss design options for prediction services in section 6.3, and we will present
    two sample prediction services in chapter 7.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然预测工作流程的四个步骤通常适用于所有类型的模型，但步骤的实际实现取决于业务需求、模型训练算法和模型训练框架。我们将在第 6.3 节讨论预测服务的设计选项，并在第
    7 章中介绍两个示例预测服务。
- en: Model serving runs machine learning algorithms in a special mode
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 模型服务以特殊模式运行机器学习算法。
- en: 'Model training and model serving execute the same machine learning algorithm
    but in two different models: learning mode and evaluation mode.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练和模型服务使用相同的机器学习算法，但是有两种不同的模式：学习模式和评估模式。
- en: In the learning mode, we run the algorithm in an *open loop*, meaning in each
    training iteration, we first run the neural network (algorithm) with an input
    data sample to calculate prediction results. Based on the difference between the
    prediction results and the expected results, the network’s parameters (weights
    and bias) are updated to fit the dataset closer.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习模式中，我们以*开环*的方式运行算法，这意味着在每个训练迭代中，我们首先对神经网络（算法）运行一个输入数据样本来计算预测结果。根据预测结果与预期结果之间的差异，网络的参数（权重和偏差）会被更新以更接近数据集。
- en: In the evaluation model, the neural network (algorithm) is run in a closed loop,
    which means that the network’s parameters will not be updated. The neural network
    is run solely to obtain the prediction results. So from a code implementation
    perspective, model serving is essentially running the machine learning algorithm
    (neural network) in the evaluation mode.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估模式中，神经网络（算法）在闭环中运行，这意味着网络的参数不会被更新。神经网络仅用于获取预测结果。因此从代码实现的角度来看，模型服务本质上是以评估模式运行机器学习算法（神经网络）。
- en: 6.1.4 Model serving challenges
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.4 模型服务的挑战
- en: Building a web service to serve models cost-effectively is a lot more complicated
    than running models locally on our laptops. Following are the six common challenges
    for serving models in web services.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个成本效益高的网络服务以服务模型比在我们的笔记本电脑上本地运行模型要复杂得多。以下是为网络服务提供模型所面临的六个常见挑战。
- en: The model prediction API differs per model algorithm. Different deep learning
    algorithms (such as recurrent neural networks and convolutional neural networks
    [CNN]) require different input data formats, and their output format can also
    vary. When designing the web prediction API, it’s quite challenging to design
    a unified web API that meets the input data requirements for every model algorithm.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 模型预测 API 根据模型算法而异。不同的深度学习算法（如循环神经网络和卷积神经网络 [CNN]）需要不同的输入数据格式，其输出格式也可能不同。在设计
    Web 预测 API 时，设计一个满足每种模型算法输入数据要求的统一 Web API 是非常具有挑战性的。
- en: Model executing environments are different per training framework. Models can
    be trained in different frameworks, such as TensorFlow and PyTorch. And each training
    framework has its special setup and configuration to execute its models. The prediction
    service should encapsulate the model execution environment setup at its backend,
    so customers can focus on using the model prediction API, not the framework with
    which this model is trained.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 模型执行环境因训练框架而异。模型可以在不同的框架中进行训练，例如 TensorFlow 和 PyTorch。而每个训练框架都有其特殊的设置和配置来执行其模型。预测服务应该在其后端封装模型执行环境的设置，这样客户就可以专注于使用模型预测
    API，而不是该模型所训练的框架。
- en: There are too many model serving tools, libraries, and systems from which to
    choose. If we decide to use existing open source approaches to model serving,
    the immediate question becomes which approach we should choose. There are 20+
    different options, such as TorchServe, TensorFlow Serving, NVIDIA Triton Inference
    Server, Seldon Core, and KFServing. How do we know which approach works best for
    our situation?
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 有太多的模型服务工具、库和系统可供选择。如果我们决定使用现有的开源模型服务方法，那么立即出现的问题就是我们应该选择哪种方法。有 20 多种不同的选择，比如
    TorchServe、TensorFlow Serving、NVIDIA Triton Inference Server、Seldon Core 和 KFServing。我们如何知道哪种方法最适合我们的情况？
- en: There is no universal, most cost-effective model serving design; we need to
    tailor a model serving approach that fits our own use case. Unlike model training
    and hyperparameter tuning service, which both have a one-fits-all approach—prediction
    service design heavily depends on concrete user scenarios. For example, designing
    a prediction service that supports just one model, such as a flower recognition
    model, is a lot different than designing a prediction service that supports 10
    different types of models, such as PDF scanning, text intent classification, and
    image classification.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 没有通用的、最具成本效益的模型服务设计；我们需要量身定制适合我们自己用例的模型服务方法。与模型训练和超参数调整服务不同，它们都有一种适用于所有情况的方法——预测服务的设计严重依赖于具体的用户场景。例如，设计一个仅支持一个模型的预测服务，比如花卉识别模型，与设计一个支持10种不同类型模型的预测服务，比如PDF扫描、文本意图分类和图像分类，是完全不同的。
- en: Reduce model prediction latency while maintaining resource saturation. From
    a cost-efficiency perspective, we want our compute resources to be fully saturated
    with model prediction workloads. In addition, we would like to provide our customers
    with a real-time model prediction experience, so we don’t want the prediction
    latency to drop because of the rigid infrastructure budget. To accomplish this,
    we need to reduce the time cost at every step of the prediction workflow innovatively,
    such as loading the model faster or preheating the model before serving.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在保持资源饱和度的同时减少模型预测延迟。从成本效益的角度来看，我们希望我们的计算资源完全饱和于模型预测工作负载。此外，我们希望为客户提供实时的模型预测体验，因此我们不希望由于严格的基础设施预算而导致预测延迟下降。为了实现这一目标，我们需要创新地减少预测工作流的每个步骤的时间成本，比如更快地加载模型或在提供服务之前预热模型。
- en: Model deployment and post-deployment model monitoring are things we should consider
    on day one. Model deployment—progressing a model from training to production—is
    critical for successful model development. We want to advance the model to production
    quickly, and we want to have multiple versions of the model in production, so
    we can evaluate different training algorithms quickly and choose the best model.
    Post-deployment model monitoring can help detect model performance regression;
    it’s a crucial protection mechanism for models in fraud detection and loan approval,
    for instance.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 模型部署和部署后模型监控是我们在第一天就应该考虑的事情。模型部署——将模型从训练推进到生产——对于成功的模型开发至关重要。我们希望快速将模型推进到生产环境，并且我们希望在生产环境中有多个模型版本，这样我们可以快速评估不同的训练算法并选择最佳模型。部署后的模型监控可以帮助检测模型性能退化；这是欺诈检测和贷款批准等模型的关键保护机制。
- en: The good news is that these six challenges are all engineering problems, so
    you will be able to handle them! We will discuss how to address them here and
    in the next chapter.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，这些六个挑战都是工程问题，所以你能够处理它们！我们将在这里和下一章讨论如何解决它们。
- en: 6.1.5 Model serving terminology
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.5 模型服务术语
- en: As we proceed through the chapter, we’d like to refresh your memory of the model
    serving terms. Many terms have various definitions in academia but are interchangeable
    in practice when talking about model serving. The following definitions should
    help you and your colleagues avoid confusion when they are mentioned.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们继续阅读本章，我们希望提醒你模型服务术语。许多术语在学术界有不同的定义，但在实践中讨论模型服务时是可以互换的。以下定义应该帮助你和你的同事在提到它们时避免混淆。
- en: '*Model serving, model scoring**, model inference*, and *model prediction* are
    interchangeable terminologies in the deep learning context. They all refer to
    executing a model with given data points. In this book, we will use *model serving*.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型服务*、*模型评分*、*模型推断*和*模型预测*在深度学习的上下文中是可以互换的术语。它们都指的是使用给定数据点执行模型。在本书中，我们将使用*模型服务*。'
- en: '*Prediction service**, scoring service**, inference service*, and *model serving
    service* are interchangeable; they refer to the web service that allows remote
    model execution. In this book, we use the prediction service.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预测服务*、*评分服务*、*推断服务*和*模型服务*是可以互换的；它们指的是允许远程执行模型的网络服务。在本书中，我们使用预测服务。'
- en: '*Predict* and *inference* are interchangeable in the model serving context;
    they are the entry function related to running the model algorithm. In this book,
    we use *predict*.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在模型服务的上下文中，*预测*和*推断*是可以互换的；它们是与运行模型算法相关的入口函数。在本书中，我们使用*预测*。
- en: '*Prediction request*, *scoring request**,* and *inference request* are interchangeable;
    they refer to the web API request that executes a model to make a prediction.
    In this book, we use *prediction request*.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预测请求*、*评分请求* 和 *推断请求* 是可以互换的；它们指的是执行模型以进行预测的 Web API 请求。在本书中，我们使用 *预测请求*。'
- en: '*Machine learning algorithm*, *training algorithm*, and *model algorithm* are
    interchangeable, as we state in section 6.1.3; the algorithm that runs in model
    training and serving is the same machine learning algorithm (same neural network)
    but in a different execution mode.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*机器学习算法*、*训练算法* 和 *模型算法* 是可以互换的，正如我们在第 6.1.3 节中所述；在模型训练和服务中运行的算法是相同的机器学习算法（相同的神经网络），但处于不同的执行模式。'
- en: '*Model deployment* and *model release* are interchangeable; they indicate the
    process of deploying/copying a trained model (files) to the production environment
    where the business is running, so the customer can benefit from this new model.
    Typically, this refers to loading the model files into the prediction service.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型部署* 和 *模型发布* 是可以互换的；它们指的是将经过训练的模型（文件）部署/复制到业务运行的生产环境中，以便客户可以从这个新模型中受益。通常，这指的是将模型文件加载到预测服务中。'
- en: 6.2 Common model serving strategies
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 常见的模型服务策略
- en: 'Before we review the concrete model serving use cases and prediction service
    designs in section 6.3, let’s first check out the three common model serving strategies:
    direct model embedding, model service, and model server. No matter what you need
    to do for your specific use cases, you can usually take one of the following three
    approaches to build your prediction service.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们审查第 6.3 节中的具体模型服务用例和预测服务设计之前，让我们先了解三种常见的模型服务策略：直接模型嵌入、模型服务和模型服务器。无论你的具体用例需要做什么，通常可以采用以下三种方法之一来构建你的预测服务。
- en: 6.2.1 Direct model embedding
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.1 直接模型嵌入
- en: Direct model embedding means loading the model and running model prediction
    inside the user application’s process. For example, a flower identity–check mobile
    app can load an image classification model directly in its local process and predict
    plant identity from the given photos. The entire model loading and serving happen
    within the model app locally (on the phone), without talking to other processes
    or remote servers.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 直接模型嵌入意味着在用户应用程序的进程内加载模型并运行模型预测。例如，一个花卉识别的手机应用可以直接在其本地进程中加载图像分类模型，并从给定的照片中预测植物身份。整个模型加载和服务都发生在本地模型应用程序内（在手机上），而不需要与其他进程或远程服务器进行通信。
- en: Most user applications, like mobile apps, are written in strongly typed languages,
    such as Go, Java, and C#, but most deep learning modeling code is written in Python.
    It is therefore difficult to embed model code into application code, and even
    if you do, the process can take a while. To facilitate model prediction across
    non-Python processes, deep learning frameworks such as PyTorch and TensorFlow
    provide C++ libraries. Additionally, TensorFlow offers Java ([https://github.com/tensorflow/java](https://github.com/tensorflow/java))
    and JavaScript ([https://github.com/tensorflow/tfjs](https://github.com/tensorflow/tfjs))
    libraries for loading and executing TensorFlow models directly from Java or JavaScript
    applications.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数用户应用程序，如手机应用程序，都是用 Go、Java 和 C# 等强类型语言编写的，但大多数深度学习建模代码是用 Python 编写的。因此，将模型代码嵌入应用程序代码是很困难的，即使你这样做了，这个过程也可能需要一段时间。为了在非
    Python 进程中促进模型预测，PyTorch 和 TensorFlow 等深度学习框架提供了 C++ 库。此外，TensorFlow 还提供了 Java
    ([https://github.com/tensorflow/java](https://github.com/tensorflow/java)) 和 JavaScript
    ([https://github.com/tensorflow/tfjs](https://github.com/tensorflow/tfjs)) 库，用于直接从
    Java 或 JavaScript 应用程序加载和执行 TensorFlow 模型。
- en: Another disadvantage of direct embedding is resource consumption. If the model
    runs on client devices, users without high-end devices may not have a good experience.
    Running big deep learning models requires a lot of computation, and this can cause
    slower apps.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 直接嵌入的另一个缺点是资源消耗。如果模型在客户端设备上运行，没有高端设备的用户可能会有不好的体验。运行大型深度学习模型需要大量的计算，这可能导致应用程序变慢。
- en: Lastly, direct embedding involves mixing model serving code with application
    business logic, which poses a challenge for backward compatibility. Therefore,
    because it is rarely used, we only describe it briefly.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，直接嵌入涉及将模型服务代码与应用程序业务逻辑混合在一起，这对向后兼容性构成了挑战。因此，因为它很少被使用，我们只简要描述它。
- en: 6.2.2 Model service
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.2 模型服务
- en: '*Model* *service* refers to running model serving on the server side. For each
    model, each version of a model, or each type of model, we build a dedicated web
    service for it. This web service exposes the model prediction API over HTTP or
    gRPC interfaces.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*模型服务* 指的是在服务器端运行模型服务。对于每个模型、每个模型的版本或每种类型的模型，我们都为其构建一个专用的 Web 服务。这个 Web 服务通过
    HTTP 或 gRPC 接口公开模型预测 API。'
- en: The model service manages the full life cycle of model serving, including fetching
    the model file from the model artifact store, loading the model, executing the
    model algorithm for a customer request, and unloading the model to reclaim the
    server resources. Using the documents classification use case as an example, to
    automatically sort documents in images and PDF by their content, we can train
    a CNN model for OCR (optical character recognition) to extract text from document
    images or PDF. To serve this model in a model service approach, we build a web
    service exclusively for this CNN model, and the web API is only designed for this
    CNN model’s prediction function. Sometimes we build a dedicated web service for
    each major model version update.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 模型服务管理模型服务的全部生命周期，包括从模型制品存储库中获取模型文件、加载模型、为客户请求执行模型算法以及卸载模型以回收服务器资源。以文档分类用例为例，为了自动按照内容对图像和
    PDF 中的文档进行分类，我们可以训练一个用于光学字符识别（OCR）的 CNN 模型来提取文档图像或 PDF 中的文本。为了在模型服务方法中为这个模型提供服务，我们为这个
    CNN 模型专门构建一个 Web 服务，并且 Web API 仅设计用于这个 CNN 模型的预测函数。有时我们为每个主要模型版本更新构建一个专用的 Web
    服务。
- en: The common pattern of model service is to build the model execution logic into
    a Docker image and use gRPC or HTTP interface to expose the model’s predict function.
    For service setup, we can host multiple service instances and employ a load balancer
    to distribute customers’ prediction requests to these instances.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 模型服务的常见模式是将模型执行逻辑构建到 Docker 镜像中，并使用 gRPC 或 HTTP 接口公开模型的预测函数。对于服务设置，我们可以托管多个服务实例，并使用负载均衡器将客户的预测请求分发到这些实例。
- en: The biggest advantage of the model service approach is simplicity. We can easily
    convert a model’s training container to a model serving container because, essentially,
    a model prediction execution entails running the trained model neural network.
    The model training code can turn into a prediction web service quickly by adding
    an HTTP or gRPC interface and setting the neural network to evaluation mode. We
    will see a model service’s design and use case in sections 6.3.1 and 6.3.2 and
    a concrete code example in chapter 7.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 模型服务方法的最大优势是简单性。我们可以很容易地将模型的训练容器转换为模型服务容器，因为本质上，模型预测执行涉及运行经过训练的模型神经网络。模型训练代码可以通过添加
    HTTP 或 gRPC 接口并设置神经网络为评估模式快速转换为预测 Web 服务。我们将在第 6.3.1 和 6.3.2 节中看到模型服务的设计和用例，并在第
    7 章中看到一个具体的代码示例。
- en: Because model service is specific to the model algorithm, we need to build separate
    services for different model types or versions. If you have several different
    models to serve, this one service-per-model approach can spawn many services,
    and the maintenance work for these services—such as patching, deploying, and monitoring—can
    be exhausting. If you are facing this situation, the model server approach is
    the right choice.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 因为模型服务针对模型算法具体化，所以我们需要为不同的模型类型或版本构建单独的服务。如果您有多个不同的模型需要提供服务，这种一模型一服务的方法可能会产生许多服务，并且维护这些服务的工作——如打补丁、部署和监控——可能会很辛苦。如果您面临这种情况，模型服务器方法是正确的选择。
- en: 6.2.3 Model server
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.3 模型服务器
- en: The model server approach is designed to handle multiple types of models in
    a black-box manner. Regardless of the model algorithm and model version, the model
    server can operate these models with a unified web prediction API. The model server
    is the next stage; we no longer need to make code changes or deploy new services
    with a new type of model or new version of the model. This saves a lot of duplicate
    development and maintenance work from the model service approach.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 模型服务器方法旨在以黑盒方式处理多种类型的模型。无论模型算法和模型版本如何，模型服务器都可以使用统一的 Web 预测 API 操作这些模型。模型服务器是下一阶段；我们不再需要为新型模型或模型新版本进行代码更改或部署新服务。这从模型服务方法中节省了许多重复的开发和维护工作。
- en: Yet, the model server approach is a lot more complicated to implement and manage
    than the model service approach. Handling model serving for various types of models
    in one service and one unified API is complicated. The model algorithms and model
    data are different; their predict functions are also different. For example, an
    image classification model can be trained with a CNN network, whereas a text classification
    model can be trained with a long short-term memory (LSTM) network. Their input
    data is different (text vs. image), and their algorithms are different (CNN vs.
    LSTM). Their model data also varies; text classification models require embedding
    files to encode input text whereas CNN models don’t require embedding files. These
    differences present many challenges to finding a low-maintenance, low-cost, and
    unified serving approach.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，模型服务器方法比模型服务方法更加复杂，实现和管理起来更加困难。在一个服务和一个统一的 API 中处理各种类型模型的模型服务是复杂的。模型算法和模型数据是不同的；它们的预测函数也是不同的。例如，图像分类模型可以用
    CNN 网络训练，而文本分类模型可以用长短期记忆（LSTM）网络训练。它们的输入数据不同（文本 vs 图像），它们的算法也不同（CNN vs LSTM）。它们的模型数据也不同；文本分类模型需要嵌入文件来编码输入文本，而
    CNN 模型不需要嵌入文件。这些差异给找到一个低维护、低成本和统一的服务方法带来了许多挑战。
- en: Although building a model server approach is difficult, it’s definitely possible.
    Many open source model serving libraries and services—such as TensorFlow Serving,
    TorchServe, and NVIDIA Triton Inference Server—offer model server solutions. We
    simply need to build customized integration logic to incorporate these tools into
    our existing systems to solve business needs—for example, integrating TorchServe
    into our model storage, monitoring, and alerting system.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然构建模型服务器方法很困难，但绝对可行。许多开源模型服务库和服务，如 TensorFlow Serving、TorchServe 和 NVIDIA Triton
    推理服务器，提供了模型服务器解决方案。我们只需要构建定制的集成逻辑，将这些工具整合到我们现有系统中以解决业务需求，例如将 TorchServe 集成到我们的模型存储、监控和警报系统中。
- en: From a model deployment perspective, the model server is a black-box approach.
    As long as we save the model file following the model server standards, the model
    prediction should function when we upload the model-to-model server through its
    management API. The complexity of model serving implementation and maintenance
    can be greatly reduced. We will see a model server design and use case in section
    6.3.3 and a code example with TorchServe in chapter 7.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 从模型部署的角度来看，模型服务器采用黑盒方法。只要我们按照模型服务器的标准保存模型文件，当我们通过其管理 API 将模型上传到模型服务器时，模型预测应该正常工作。模型服务实现和维护的复杂性可以大大降低。我们将在第
    6.3.3 节看到模型服务器的设计和用例，并在第 7 章看到使用 TorchServe 的代码示例。
- en: Note Should we always consider a model server approach? Not always. If we don’t
    think of service development cost and maintenance cost, the model server approach
    is the most powerful because it’s designed to cover all types of models. But if
    we care about model serving cost efficiency—and we should!—then the ideal approach
    depends on the use cases. In the next section, we will discuss the common model
    serving use cases and the applied design.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们是否总是应该考虑模型服务器方法？并不总是。如果我们不考虑服务开发成本和维护成本，模型服务器方法是最强大的，因为它设计用来覆盖所有类型的模型。但如果我们关心模型服务的成本效益——而我们应该关心！——那么理想的方法取决于用例。在下一节中，我们将讨论常见的模型服务用例和应用设计。
- en: 6.3 Designing a prediction service
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 设计预测服务
- en: A common mistake in software system design is aiming to build an omnipotent
    system without considering the concrete user scenario. Overdesign will redirect
    our focus from the immediate customer needs to the features that might be useful
    in the future. As a result, the system either takes an unnecessarily long time
    to build or is difficult to use. This is especially true for model serving.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件系统设计中一个常见的错误是试图构建一个无所不能的系统，而不考虑具体的用户场景。过度设计会将我们的注意力从即时的客户需求转移到未来可能有用的功能上。结果，系统要么需要花费不必要的时间来构建，要么难以使用。这对于模型服务尤其如此。
- en: Deep learning is an expensive business, both in terms of human and computational
    resources. We should build only the necessities to move models into production
    as quickly as possible and minimize the operation costs. To do so, we need to
    begin with user scenarios.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是一项昂贵的业务，无论是人力资源还是计算资源。我们应该只构建必需品，尽快将模型投入生产，并尽量减少操作成本。为此，我们需要从用户场景开始。
- en: In this section, we will present three typical model serving scenarios, from
    simple to complex. For each use case, we explain the scenario and illustrate a
    suitable high-level design. By reading the following three subsections sequentially,
    you will see how the prediction service’s design evolves when use cases become
    more and more complicated.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍三种典型的模型服务场景，从简单到复杂。对于每个用例，我们解释场景并说明一个适合的高级设计。通过按顺序阅读以下三个小节，您将看到当用例变得越来越复杂时，预测服务的设计如何演变。
- en: Note The goal of prediction service design is not to build a powerful system
    that works for various models but to build a system that suits the circumstances
    in a cost-efficient manner.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 预测服务设计的目标不是构建适用于各种模型的强大系统，而是以成本效益的方式构建适合环境的系统。
- en: 6.3.1 Single model application
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.1 单模型应用
- en: Imagine building a mobile app that can swap people’s faces between two pictures.
    The consumer expects the app UI to upload photos, select sources and target pictures,
    and execute a deepfake model ([https://arxiv.org/abs/1909.11573](https://arxiv.org/abs/1909.11573))
    for swapping faces between the two selected images. For an application like this
    that only needs to work with one model, the serving approach can be either model
    service (6.2.2) or direct model embedding (6.2.1).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下构建一个可以在两张图片之间交换人脸的移动应用程序。消费者期望应用程序 UI 能够上传照片，选择源图片和目标图片，并执行一个 deepfake 模型（[https://arxiv.org/abs/1909.11573](https://arxiv.org/abs/1909.11573)）来交换所选图片之间的人脸。对于只需要与一个模型一起工作的应用程序，服务方法可以是模型服务（6.2.2）或直接模型嵌入（6.2.1）。
- en: Model service approach
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 模型服务方法
- en: 'From the discussion in section 6.2.2, the model service approach involves building
    a web service for each model. So we can build the face-swap model app with the
    following three components: a front UI app (component A) that runs on our phone;
    an application backend to handle user operation (component B); and a backend service,
    or *predictor* (component C), to host a deepfake model and expose a web API to
    execute the model for each face-swap request.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 从 6.2.2 节的讨论中可以看出，模型服务方法包括为每个模型构建一个 Web 服务。因此，我们可以使用以下三个组件构建换脸模型应用程序：在手机上运行的前端
    UI 应用程序（组件 A）；用于处理用户操作的应用后端（组件 B）；以及用于托管 deepfake 模型并公开 Web API 以执行每个换脸请求的后端服务，或*预测器*（组件
    C）。
- en: When a user uploads a source image and a target image and clicks the face-swap
    button on the mobile app, the mobile backend application will receive the request
    and call the predictor’s web API for face-swapping. Then the predictor preprocesses
    the user request data (the images), executes the model algorithm, and postprocesses
    the model output (the images) to the application backend. Ultimately, the mobile
    app will display the source and target images with swapped faces. Figure 6.4 illustrates
    a general design that suits the face-swap use case.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户在移动应用程序上上传源图片和目标图片，并点击换脸按钮时，移动后端应用程序将接收请求并调用预测器的 Web API 进行换脸。然后，预测器对用户请求数据（图片）进行预处理，执行模型算法，并对模型输出（图片）进行后处理，然后将其发送到应用后端。最终，移动应用程序将显示源图片和目标图片，并交换它们的人脸。图
    6.4 描述了适用于换脸用例的一般设计。
- en: '![](../Images/06-04.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06-04.png)'
- en: Figure 6.4 A single model predictor design in a client/server setup
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 在客户端/服务器设置中的单模型预测器设计
- en: If we zoom into the predictor (component C), we see that the model serving logic
    works the same as the general model prediction workflow that we introduced in
    figure 6.3\. The predictor (model serving service) loads the model file from the
    model artifactory and runs the model to respond to the request received by the
    web interface.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们放大预测器（组件 C），我们会发现模型服务逻辑与我们在图 6.3 中介绍的一般模型预测工作流程相同。预测器（模型服务服务）从模型仓库中加载模型文件并运行模型以响应通过
    Web 接口收到的请求。
- en: The design in figure 6.4 generally works for any application that has a web
    backend and only one model. The key component in this design is the predictor;
    it is a web service and often runs as a Docker container. We can implement this
    approach quickly because this predictor container can be easily converted from
    the model training container that builds the model. The two main work items that
    transform a training container to a predictor container are the web predict API
    and the evaluation mode in the training neural network. We will present a concrete
    predictor container example in section 7.1.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4中的设计通常适用于具有Web后端和只有一个模型的任何应用程序。该设计的关键组件是预测器；它是一个Web服务，通常作为Docker容器运行。我们可以快速实现这个方法，因为预测器容器可以从构建模型的训练容器中轻松转换。将训练容器转换为预测器容器的两个主要工作项是Web预测API和训练神经网络中的评估模式。我们将在第7.1节中介绍一个具体的预测器容器示例。
- en: Direct model embedding approach
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 直接模型嵌入方法
- en: Another design approach for building a single model application is combining
    the model execution code with the application’s user logic code. There is no server
    backend, so everything happens locally on the user’s computer or phone. Using
    the face swap app as an example, the deepfake model file is in the application’s
    deployment package, and when the application starts, the model is loaded into
    the application’s process space. Figure 6.5 illustrates this concept.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 构建单一模型应用的另一种设计方法是将模型执行代码与应用的用户逻辑代码结合起来。没有后端服务器，所有操作都在用户的计算机或手机上本地完成。以换脸应用为例，深度伪造模型文件在应用部署包中，当应用启动时，模型被加载到应用的进程空间中。图6.5展示了这个概念。
- en: '![](../Images/06-05.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06-05.png)'
- en: Figure 6.5 In the direct model embedding design, the model is executed in the
    same process as the application logic.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 在直接模型嵌入设计中，模型在应用逻辑的同一进程中执行。
- en: Model serving doesn’t have to run in a separate service. In figure 6.5, we see
    that the model serving code (the single model box) and the data transformation
    code can run with the user logic code in the same application. Nowadays, many
    deep learning frameworks provide libraries to run models in non-Python applications.
    For example, TensorFlow offers Java, C++, and JavaScript SDK to load and execute
    models. With SDK’s help, we can train and execute models directly in Java/C++/
    JavaScript applications.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 模型服务不一定要在独立的服务中运行。在图6.5中，我们可以看到模型服务代码（单一模型框）和数据转换代码可以与用户逻辑代码在同一个应用中运行。现在，很多深度学习框架都提供了在非Python应用中运行模型的库。例如，TensorFlow提供了Java、C++和JavaScript的SDK来加载和执行模型。借助SDK的帮助，我们可以直接在Java/C++/JavaScript应用中训练和执行模型。
- en: Note Why should we consider direct model embedding? By using model embedding,
    we can directly integrate model serving logic with application logic and run them
    together in the same process space. This provides two advantages over the predictor
    service approach in figure 6.4\. First, it reduces one network hop; there is no
    web request to the predictor, and model execution happens locally. Second, it
    improves service debuggability because we can run the application as one piece
    locally.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 注意为什么应该考虑直接模型嵌入？通过使用模型嵌入，我们可以直接将模型服务逻辑与应用逻辑集成并在同一个进程空间中运行它们。这相对于图6.4中的预测器服务方法有两个优势。首先，它减少了一次网络跳转；没有对预测器的Web请求，模型执行在本地进行。其次，它提高了服务的调试能力，因为我们可以将应用作为一个整体在本地运行。
- en: Why is the model service approach more popular?
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么模型服务方法更受欢迎？
- en: 'Although the direct model embedding approach looks simple and saves one network
    hop, it is still not a popular choice for building model serving. Here are the
    four reasons:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然直接模型嵌入方法看起来简单并且可以节省一次网络跳转，但它仍然不是构建模型服务的常见选择。以下是四个原因：
- en: The model algorithm has to be reimplemented in a different language. A model’s
    algorithm and execution code is usually written in Python. If we choose a model
    service approach, implementing the model serving as a web service (predictor in
    figure 6.4), we can reuse most of the training code and build it quickly. But
    if we choose to embed model serving in a non-Python application, we must reimplement
    model loading, model execution, and data process logic in the application’s language
    (such as Java or C++). This work is nontrivial, and not many developers have the
    depth of knowledge to rewrite the training algorithms.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型算法必须在不同的语言中重新实现。模型的算法和执行代码通常是用Python编写的。如果我们选择模型服务方法，将模型服务实现为Web服务（图6.4中的预测器），我们可以重用大部分训练代码并快速构建它。但是，如果我们选择将模型服务嵌入非Python应用程序中，我们必须在应用程序的语言中（如Java或C
    ++）重新实现模型加载、模型执行和数据处理逻辑。这项工作并不简单，而且没有多少开发人员具备重写训练算法的深度知识。
- en: The ownership boundary is blurred. When embedding a model into an application,
    the business logic code can mingle with the serving code. When the codebase becomes
    complicated, it’s difficult to draw a boundary between the serving code (owned
    by the data scientist) and other application code (owned by the developer). When
    data scientists and developers are from two different teams but work on the same
    code repo, the shipping velocity will drop significantly because the cross-team
    code review and deployment takes longer than usual.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有权边界变得模糊。将模型嵌入应用程序时，业务逻辑代码可能会与服务代码混合在一起。当代码库变得复杂时，很难在服务代码（由数据科学家拥有）和其他应用程序代码（由开发人员拥有）之间划定界限。当数据科学家和开发人员来自两个不同的团队，但在同一个代码仓库上工作时，交叉团队的代码审查和部署时间会比平常长得多。
- en: Performance problems can occur on the client’s devices. Usually, apps are run
    on the customer’s mobiles, tablets, or lower-end laptops. On these devices, capturing
    features from raw user data and then preprocessing model input data and running
    model prediction can lead to performance problems such as CPU usage spikes, app
    slowdown, and high memory usage.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户设备可能出现性能问题。通常，应用程序在客户的手机、平板电脑或低端笔记本电脑上运行。在这些设备上，从原始用户数据中捕获特征，然后预处理模型输入数据并运行模型预测可能会导致性能问题，如CPU使用率飙升、应用程序减速和内存使用量高。
- en: A memory leak can occur easily. For example, when executing a TensorFlow model
    in Java, the algorithm execution and input/output parameter objects are all created
    in the native space. These objects won’t be recycled by Java GC (Garbage Collection)
    automatically; we have to manually depose them. It’s very easy to overlook recycling
    the native resources claimed by the model, and because the native objects’ memory
    allocations are not tracked in Java heap, their memory usage is difficult to observe
    and measure. So the memory leak can happen and is hard to fix.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存泄漏很容易发生。例如，在Java中执行TensorFlow模型时，算法执行和输入/输出参数对象都是在本地空间中创建的。这些对象不会被Java GC（垃圾收集）自动回收；我们必须手动释放它们。很容易忽视模型所声明的本地资源的回收，并且由于Java堆中不跟踪本地对象的内存分配，它们的内存使用量很难观察和测量。所以内存泄漏可能会发生，并且很难修复。
- en: Note To troubleshoot native memory leaks, Jemalloc ([https://github.com/jemalloc/jemalloc/wiki/Background](https://github.com/jemalloc/jemalloc/wiki/Background))
    is a very handy tool. You can check out my blog post “Fix Memory Issues in Your
    Java Apps” ([http://mng.bz/lJ8o](http://mng.bz/lJ8o)) for further details.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：为了排除本地内存泄漏，Jemalloc ([https://github.com/jemalloc/jemalloc/wiki/Background](https://github.com/jemalloc/jemalloc/wiki/Background))
    是一个非常方便的工具。您可以查看我的博客文章“在您的Java应用程序中修复内存问题” ([http://mng.bz/lJ8o](http://mng.bz/lJ8o))
    获取更多详情。
- en: For the previously listed reasons, we highly recommend you adopt the model service
    approach for single model application use cases.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 出于前述原因，我们强烈建议您采用模型服务方法来处理单一模型应用用例。
- en: 6.3.2 Multitenant application
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.2 多租户应用程序
- en: We will use a chatbot application as an example to explain multitenant use cases.
    First, let’s set the context. A *tenant* is a company or organization (such as
    a school or a retail store) that uses the chatbot application to communicate with
    its customers. The tenants use the same software/service—the chatbot application—but
    have separate accounts with their data segregated. A *chat user* is the customer
    of a tenant and uses the chatbot to do business with the tenant.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以聊天机器人应用程序作为示例来解释多租户用例。首先，让我们设定一下背景。*租户*是一家公司或组织（例如学校或零售店），他们使用聊天机器人应用程序与其客户进行沟通。租户使用相同的软件/服务-聊天机器人应用程序，但具有单独的带有其数据隔离的账户。*聊天用户*是租户的客户，使用聊天机器人与租户进行业务交流。
- en: By design, the chatbot application relies on an intent classification model
    to identify the user’s intention from his conversation, and then the chatbot redirects
    the user request to the correct service department of the tenant. Currently, this
    chatbot is taking a single model application approach, meaning it’s using a single
    intent classification model for every user and tenant.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 按设计，聊天机器人应用程序依靠意图分类模型从对话中识别用户的意图，然后将用户请求重定向到租户的正确服务部门。目前，该聊天机器人采用单一模型应用的方法，这意味着它为每个用户和租户使用单一的意图分类模型。
- en: Now, because of customer feedback from tenants on the low prediction accuracy
    of the single intent classification model, we decide to let tenants use our training
    algorithm to build their own model with their own dataset. This way, the model
    can fit better with each tenant’s business situation. For model serving, we will
    let tenants use their own model for intent classification prediction requests.
    When a chatbot user now speaks to the chatbot application, the application will
    find the tenant’s specific model to answer the user’s question. The chatbot is
    changed to a multitenant application.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，由于租户反馈单一意图分类模型预测准确度低，我们决定让租户使用我们的训练算法，使用他们自己的数据集构建自己的模型。这样，模型可以更好地适应每个租户的业务情况。对于模型服务，我们将让租户使用自己的模型进行意图分类预测请求。当一个聊天机器人用户与聊天机器人应用程序交互时，应用程序将找到租户的特定模型来回答用户的问题。聊天机器人被改为多租户应用程序。
- en: In this chatbot multitenant use case, although the models belong to different
    tenants and are trained with different datasets, they are the same type of model.
    Because these models are trained with the same algorithm, their model’s algorithm
    and predict function are all the same. We can extend the model service design
    in figure 6.4 to support multitenancy by adding a model cache. By caching model
    graphs and their dependent data in memory, we can perform multitenancy model serving
    in one service. Figure 6.6 illustrates this concept.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个聊天机器人多租户使用案例中，虽然这些模型属于不同的租户并使用不同的数据集进行训练，但它们属于相同类型的模型。因为这些模型使用相同的算法进行训练，它们的模型算法和预测函数都是相同的。我们可以通过添加模型缓存来扩展图6.4中的模型服务设计，以支持多租户。通过将模型图和其相关数据缓存在内存中，我们可以在一个服务中执行多租户模型服务。图6.6说明了这个概念。
- en: Compared with the model service design in figure 6.4, the design in figure 6.6
    adds a model cache (component A) and a model file server (component B). Because
    we want to support multiple models in one service, we need a model cache in memory
    to host and execute different models. The model file server stores the model files
    that can be loaded into the prediction service’s model cache. The model server
    can also be shared among prediction service instances.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 与图6.4中的模型服务设计相比，图6.6中的设计增加了一个模型缓存（组件A）和一个模型文件服务器（组件B）。因为我们希望在一个服务中支持多个模型，所以我们需要一个内存中的模型缓存来托管和执行不同的模型。模型文件服务器存储可以加载到预测服务模型缓存中的模型文件。模型服务器也可以在预测服务实例之间共享。
- en: To build a good model cache, we need to consider model cache management and
    memory resource management. For the model cache, we need to assign a unique model
    ID as a cache key to identify each model in the cache. For example, we can use
    the model training run ID as the model ID; the benefit is, for each model in the
    cache, we can trace which training run produced it. Another more flexible way
    of constructing the model ID is combining the model name (a customized string)
    and the model version. No matter which model ID style we choose, the ID has to
    be unique, and it must be provided in the prediction request.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建一个良好的模型缓存，我们需要考虑模型缓存管理和内存资源管理。对于模型缓存，我们需要分配一个唯一的模型 ID 作为缓存键，以识别缓存中的每个模型。例如，我们可以使用模型训练运行
    ID 作为模型 ID；好处是，对于缓存中的每个模型，我们都可以追踪到是哪个训练运行生成了它。另一种更灵活的构建模型 ID 的方式是结合模型名称（自定义字符串）和模型版本。无论我们选择哪种模型
    ID 样式，ID 必须是唯一的，并且必须在预测请求中提供。
- en: '![](../Images/06-06.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06-06.png)'
- en: Figure 6.6 A prediction service with model caching for multitenant applications
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6 面向多租户应用的模型缓存预测服务
- en: For memory resource management, because each server has limited memory and GPU
    resources, we can’t load all the required models into memory. So we need to build
    model-swapping logic to the model cache. When the resource capacity is reached—for
    instance, the process is about to run out of memory—some models need to be evicted
    from the model cache to free some resources for new model prediction requests.
    Methods like LRU (least recently used) algorithm and model partition across different
    instances can help reduce the cache missing rate (the request model is not in
    the cache) and make model swapping less disruptive. The sample intent classification
    prediction service we build in section 7.1 demonstrates the model caching concept;
    you can explore the details there.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 对于内存资源管理，因为每台服务器的内存和 GPU 资源都是有限的，我们无法将所有所需的模型都加载到内存中。因此，我们需要构建模型交换逻辑到模型缓存中。当资源容量达到时——例如，进程即将耗尽内存时——需要从模型缓存中将一些模型驱逐出去，为新的模型预测请求释放一些资源。像
    LRU（最近最少使用）算法和模型在不同实例之间的分区可以帮助减少缓存未命中率（请求的模型不在缓存中），并使模型交换更少地造成中断。我们在第 7.1 节中构建的样本意图分类预测服务演示了模型缓存的概念；你可以在那里探索详细信息。
- en: Can we extend the model caching design to multiple model types?
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将模型缓存设计扩展到多个模型类型吗？
- en: We don’t recommend extending the model caching design to multiple model types.
    The input/output data format and data process logic of various model types, such
    as the image classification model and intent classification model, are very different,
    so it’s hard to host and execute different types of models in the same model cache.
    To do that, we would need to build separate web interfaces and separate data preprocess
    and postprocess code for each type of model. At this point, you will find it’s
    easier to build separate prediction services for each model type—with each service
    having its own type of web interface and data process logic and managing the model
    cache for its own model type. For example, we can build an image classification
    prediction service and an intent classification prediction service for these two
    different model types separately.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不建议将模型缓存设计扩展到多个模型类型。各种模型类型的输入/输出数据格式和数据处理逻辑，如图像分类模型和意图分类模型，都非常不同，因此很难在同一个模型缓存中托管和执行不同类型的模型。为此，我们需要为每种模型类型构建单独的
    Web 接口以及单独的数据预处理和后处理代码。在这一点上，你会发现为每种模型类型构建单独的预测服务更容易一些——每个服务都有自己的 Web 接口类型和数据处理逻辑，并管理其自己模型类型的模型缓存。例如，我们可以为这两种不同类型的模型分别构建图像分类预测服务和意图分类预测服务。
- en: This one service per model type approach works well when you only have a few
    model types. But if you have 20+ types of models, then it can’t scale. Building
    and maintaining web services—such as setting up a CI/CD pipeline, networking,
    and deployment—is costly. Also, the work of monitoring a service is nontrivial;
    we need to build monitoring and alerting mechanisms to ensure the service is running
    24/7\. Consider the costs of onboarding and maintenance work if we follow this
    design to support 100+ model types for the entire company. To scale up and serve
    lots of different model types in one system, we need to take the model server
    approach (section 6.2.3), which we will discuss further in the next section.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 当你只有少量模型类型时，每种模型类型一个服务的方法效果很好。但如果你有20多种模型类型，那么它就无法扩展。构建和维护 Web 服务，比如设置 CI/CD
    管道、网络和部署，成本很高。此外，监控服务的工作也不容易；我们需要建立监控和报警机制，以确保服务 24/7 运行。如果我们按照这种设计支持整个公司的100+模型类型，考虑到入职和维护工作的成本。为了扩展规模并在一个系统中提供多种不同的模型类型，我们需要采取模型服务器方法（第6.2.3节），我们将在下一节进一步讨论。
- en: 6.3.3 Supporting multiple applications in one system
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.3 在一个系统中支持多个应用程序
- en: 'You have successfully built multiple model serving services to support different
    applications, such as multitenant chatbot, face-swapping, flower recognition,
    and PDF document scanning. Now, you are given two more tasks: (1) building the
    model serving support for a new application that uses a voice recognition model
    and (2) reducing model serving costs for all applications.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经成功地构建了多个模型服务以支持不同的应用程序，比如多租户聊天机器人、换脸、花卉识别和 PDF 文档扫描。现在，你又有两个任务：（1）为使用语音识别模型的新应用程序构建模型服务支持；（2）减少所有应用程序的模型服务成本。
- en: So far, all the model serving implementations have been built with the model
    service approach. From previous discussions in sections 6.3.1 and 6.3.2, we know
    this approach can’t scale when we have more and more model types. When many products
    and applications have model serving requirements, it’s better to build just one
    centralized prediction service to address all the serving needs. We name this
    type of prediction service a *prediction platform*. It takes the model server
    approach (section 6.2.3) and handles all kinds of model serving in one place.
    This is the most cost-efficient approach for multiple application situations because
    the model onboarding and maintenance cost is limited to one system, which is much
    less than one prediction service per application approach (section 6.2.2).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，所有的模型服务实现都是采用模型服务方法构建的。从前面第6.3.1节和第6.3.2节的讨论中，我们知道当我们有越来越多的模型类型时，这种方法无法扩展。当许多产品和应用程序都有模型服务需求时，最好只构建一个集中的预测服务来解决所有的服务需求。我们将这种类型的预测服务称为*预测平台*。它采用了模型服务器方法（第6.2.3节），并在一个地方处理所有类型的模型服务。对于多个应用程序情况来说，这是最具成本效益的方法，因为模型入职和维护成本仅限于一个系统，远远低于每个应用程序一种预测服务方法（第6.2.2节）。
- en: To build such an omnipotent model serving system, we need to consider lots of
    elements, such as model file format, model libraries, model training frameworks,
    model caching, model versioning, model flow execution, model data processing,
    model management, and a unified prediction API that suits all model types. Figure
    6.7 illustrates the design and workflow of the prediction platform.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建这样一个全能的模型服务系统，我们需要考虑很多因素，比如模型文件格式、模型库、模型训练框架、模型缓存、模型版本控制、模型流执行、模型数据处理、模型管理，以及适合所有模型类型的统一预测
    API。图6.7展示了预测平台的设计和工作流程。
- en: The prediction platform design in figure 6.7 is much more complicated than the
    model service approach in figure 6.6\. This is because we need to combine multiple
    components and services to support arbitrary models. Let’s look at each component
    of the system and then the model prediction workflow.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7中的预测平台设计比图6.6中的模型服务方法复杂得多。这是因为我们需要组合多个组件和服务来支持任意模型。让我们来看看系统的每个组件，然后是模型预测工作流程。
- en: '![](../Images/06-07.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06-07.png)'
- en: Figure 6.7 A general prediction service (platform) design that works with arbitrary
    model types
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 通用的预测服务（平台）设计，适用于任意模型类型
- en: Unified web API
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 统一的 Web API
- en: To support arbitrary models, we expect the public prediction APIs to be generic.
    No matter which model is called, the API’s spec—for instance, its payload schema
    of the prediction request and response—should be generic enough to satisfy the
    model’s algorithm requirement. One example of this kind of unified API is the
    KFServing predict protocol ([http://mng.bz/BlB2](http://mng.bz/BlB2)), which aims
    to standardize the prediction protocol that works for any models and various prediction
    backends.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持任意模型，我们希望公共预测 API 是通用的。无论调用哪个模型，API 的规范——例如预测请求和响应的有效载荷模式——都应该足够通用，以满足模型的算法需求。这种统一
    API 的一个示例是 KFServing 的预测协议（[http://mng.bz/BlB2](http://mng.bz/BlB2)），该协议旨在为任何模型和各种预测后端标准化预测协议。
- en: 'The web APIs are also expected to be simple, so we can reduce the customer
    onboarding and maintenance effort. The prediction APIs can be categorized into
    three buckets: model prediction requests API, model metadata fetching API, and
    model deployment API. The model metadata fetching API and deployment API are very
    useful because they are agnostic about the model they are serving. We need these
    methods to check the model metadata, such as the model version and algorithm info,
    and to check the model deployment status.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Web API 也应该简单易懂，这样我们就能减少客户的接入和维护工作量。预测 API 可以分为三种类型：模型预测请求 API、模型元数据获取 API 和模型部署
    API。模型元数据获取 API 和部署 API 非常有用，因为它们对于它们所提供的模型是无所不知的。我们需要这些方法来检查模型元数据，例如模型版本和算法信息，以及检查模型部署状态。
- en: Routing component
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 路由组件
- en: Normally, each type of serving backend can only handle a few types of models.
    To support arbitrary models, we need to have different kinds of serving backends,
    such as TensorFlow Serving backend for TensorFlow models and TorchServe backend
    for PyTorch models. When receiving a model prediction request, the system needs
    to know which backend can handle it. This is done with the routing component.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，每种类型的服务后端只能处理几种类型的模型。为了支持任意模型，我们需要有不同种类的服务后端，例如 TensorFlow Serving 后端用于 TensorFlow
    模型，TorchServe 后端用于 PyTorch 模型。当接收到模型预测请求时，系统需要知道哪个后端可以处理它。这是通过路由组件来完成的。
- en: The routing component responds to route the prediction request to the correct
    backend inference server. For a given request, the routing component first fetches
    the model’s metadata; the metadata includes the model algorithm name and version,
    the model version, and the training framework. Then, by matching the model metadata
    with the routing config, it determines to which inference backend it should route
    the prediction request.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 路由组件负责将预测请求路由到正确的后端推理服务器。对于给定的请求，路由组件首先获取模型的元数据；元数据包括模型算法名称和版本、模型版本和训练框架。然后，通过将模型元数据与路由配置进行匹配，确定应该将预测请求路由到哪个推理后端。
- en: Graph execution component
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图执行组件
- en: 'The graph execution component handles the type of prediction that needs to
    execute a series of model predictions. For example, to automate the mortgage approval
    process, we have to run a loan approval prediction request following three models
    in a sequence: a PDF scanning model to parse the text from the PDF loan application,
    a named entity recognition model to recognize the keywords, and a loan-scoring
    model to score the loan application. To support such requirements, we can define
    a directed acyclic graph (DAG) to describe the model execution chain and build
    a graph execution engine to execute in one go.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图执行组件处理需要执行一系列模型预测的预测类型。例如，为了自动化抵押贷款批准流程，我们必须按照以下三个模型的顺序运行贷款批准预测请求：一个 PDF 扫描模型来解析贷款申请的文本，一个命名实体识别模型来识别关键词，以及一个贷款评分模型来评分贷款申请。为了支持这种需求，我们可以定义一个有向无环图（DAG）来描述模型执行链，并构建一个图执行引擎以一次性执行。
- en: Inference server
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 推理服务器
- en: The inference (model) server does the actual work to compute model prediction
    by managing model caching and model prediction execution. It’s similar to the
    prediction service shown in figure 6.6 but more sophisticated because it needs
    to support arbitrary model algorithms. Besides the predict API, the inference
    server should also offer model management API to register new models and remove
    models programmatically.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 推理（模型）服务器通过管理模型缓存和模型预测执行来执行实际的模型预测工作。它类似于图6.6中显示的预测服务，但更加复杂，因为它需要支持任意模型算法。除了预测
    API 之外，推理服务器还应该提供模型管理 API，以实现注册新模型和通过编程方式删除模型的功能。
- en: Building an inference server is much more complicated than building a predictor
    service; not many engineers want to try that. But luckily, there are multiple
    black-box, open source approaches that work out of the box, such as TensorFlow
    Serving, TorchServe, and NVIDIA Triton Inference Server. In practice, we often
    reuse these existing open source inference servers and integrate them into our
    own routing component and graph execution component. We will discuss more on the
    open source model server tools in chapter 7.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 构建推理服务器比构建预测器服务复杂得多；很少有工程师愿意尝试。但幸运的是，有多个黑盒开源方法可以直接使用，例如 TensorFlow Serving、TorchServe
    和 NVIDIA Triton Inference Server。在实践中，我们经常重用这些现有的开源推理服务器，并将它们集成到我们自己的路由组件和图执行组件中。我们将在第7章中更多地讨论开源模型服务器工具。
- en: Applications
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 应用场景
- en: In figure 6.7, we see applications A, B, and C are sharing the same model serving
    backend. The model serving for different applications occurs at the same place.
    Compared with the model service design in figure 6.6, the prediction platform
    is more scalable and more cost-efficient because there is almost no onboarding
    cost to add new application D.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在图6.7中，我们看到应用程序A、B和C共享同一模型服务后端。不同应用程序的模型服务发生在同一地方。与图6.6中的模型服务设计相比，预测平台更具可扩展性和更具成本效益，因为添加新应用程序D几乎没有任何入职成本。
- en: For example, if we want to onboard new application D—a voice-to-text scripting
    application—we just need to upload the voice scripting model to the model file
    server and then let the application use the unified prediction web API of the
    prediction platform. There is no code change on the prediction platform side for
    supporting a new application.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们想要引入新的应用程序D——一个语音转文本脚本应用程序——我们只需将语音脚本模型上传到模型文件服务器，然后让该应用程序使用预测平台的统一预测
    web API。对于支持新应用程序，预测平台端不需要进行任何代码更改。
- en: Model prediction workflow
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 模型预测工作流程
- en: After explaining each key component, let’s look at a typical model prediction
    workflow (figure 6.7). First, we publish our model files to the model file server
    and update the config in the routing component, so the routing component knows
    to which inference server it should route the prediction request for this type
    of model. Second, applications send prediction requests to the prediction system’s
    web APIs, and then the request is routed by the routing component to the correct
    inference server. Third, the inference server will load the model from the model
    file server, convert the request payload to model input, run the model algorithm,
    and return the prediction result with a postprocess.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在解释每个关键组件之后，让我们看一个典型的模型预测工作流程（图6.7）。首先，我们将我们的模型文件发布到模型文件服务器，并更新路由组件中的配置，使路由组件知道应该将这种类型的模型的预测请求路由到哪个推理服务器。其次，应用程序向预测系统的
    web API 发送预测请求，然后路由组件将请求路由到正确的推理服务器。第三，推理服务器将从模型文件服务器加载模型，将请求载荷转换为模型输入，运行模型算法，并以后处理方式返回预测结果。
- en: Note Prediction platform design is not always the best serving approach! In
    theory, the design in figure 6.7 can work for any model, but it does come with
    some extra cost. Its setup, maintenance, and debugging are way more complicated
    than the model service approach. This design is overkill for scenarios introduced
    in sections 6.3.1 and 6.3.2\. Because each design has its merits, we recommend
    not adhering to one serving approach but choosing the serving method based on
    your actual user scenarios.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 预测平台设计并不总是最佳的服务方法！理论上，图6.7中的设计可以适用于任何模型，但它确实带来了一些额外的成本。它的设置、维护和调试比模型服务方法要复杂得多。这种设计对于第6.3.1节和第6.3.2节中介绍的情景来说是过度的。因为每种设计都有其优点，我们建议不要坚持一个服务方法，而是根据实际用户场景选择服务方法。
- en: 6.3.4 Common prediction service requirements
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.4 常见的预测服务需求
- en: 'Although we state that designing prediction services should start from concrete
    use cases, different situations lead to different designs. Three common requirements
    exist among all model serving designs:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们声明设计预测服务应该从具体的用例开始，但不同的情况会导致不同的设计。所有模型服务设计中存在三个共同的要求：
- en: '*Model deployment safety*—No matter what model rollout strategy and version
    strategy we choose, we must have a way to roll back a model to the previous state
    or version.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型部署安全性*——无论我们选择什么样的模型部署策略和版本策略，我们都必须有一种方法将模型回滚到先前的状态或版本。'
- en: '*Latency*—Web request latency is a crucial factor in the success of many online
    businesses. Once we build the model serving support, the next step is to try our
    best to reduce the average prediction response time.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*延迟* — 网络请求延迟是许多在线业务成功的关键因素。一旦我们建立了模型服务支持，下一步就是尽力减少平均预测响应时间。'
- en: '*Monitoring and alerting*—Model serving is the most critical service in a deep
    learning system; if it goes down, the business stops. Remember, actual businesses
    run on top of the model prediction in realtime. Customers are affected immediately
    if the service is down or serving latency increases. Prediction service should
    be the most-equipped service among other deep learning services in monitoring
    and alerting.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*监控和警报* — 模型服务是深度学习系统中最关键的服务；如果它停止运行，业务也会停止。请记住，实际业务是实时运行在模型预测之上的。如果服务停止或服务延迟增加，客户会立即受到影响。在监控和警报方面，预测服务应该是其他深度学习服务中配备最齐全的服务。'
- en: In this chapter, we have reviewed concepts, definitions, and abstract high-level
    system designs of model serving. We hope you gain a clear picture of what model
    serving is and what to consider when designing model serving systems. In the next
    chapter, we will demo two sample prediction services and discuss the commonly
    used prediction open source tools. These examples will show you how the design
    concepts in this chapter are applied in real life.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们回顾了模型服务的概念、定义和抽象的高级系统设计。我们希望您能清楚地了解什么是模型服务以及在设计模型服务系统时要考虑什么。在下一章中，我们将演示两个样本预测服务，并讨论常用的预测开源工具。这些示例将展示本章中的设计概念如何应用于实际生活中。
- en: Summary
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'A model can be several files; it is composed of three elements: machine learning
    algorithm, model executor (wrapper), and model data.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个模型可以是几个文件；它由三个元素组成：机器学习算法、模型执行器（包装器）和模型数据。
- en: Model prediction and model inference have the same meaning in the model serving
    context.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型预测和模型推断在模型服务环境中具有相同的含义。
- en: Direct model embedding, model service, and model server are the three common
    types of model serving strategies.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接模型嵌入、模型服务和模型服务器是模型服务策略的三种常见类型。
- en: The model service approach involves building a prediction service for each model,
    each version of a model, or each type of model.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型服务方法涉及为每个模型、每个模型版本或每种类型的模型构建一个预测服务。
- en: The model server approach consists of building only one prediction service,
    but it can run models trained with different algorithms and frameworks and can
    run different versions of each model.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型服务器方法包括仅构建一个预测服务，但它可以运行使用不同算法和框架训练的模型，并且可以运行每个模型的不同版本。
- en: When designing a model serving system, the first component to understand is
    the use case, so we can decide which serving approach is most appropriate.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在设计模型服务系统时，首先要了解的是使用情况，这样我们就可以决定哪种服务方法最合适。
- en: Cost efficiency is the primary goal for designing model serving systems; the
    cost includes service deployment, maintenance, monitoring, infrastructure, and
    service development.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成本效益是设计模型服务系统的主要目标；成本包括服务部署、维护、监控、基础设施和服务开发。
- en: For single model applications, we recommend the model service approach.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于单一模型应用程序，我们建议采用模型服务方法。
- en: For multitenant applications, we recommend the model service approach with an
    in-memory model cache.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于多租户应用程序，我们建议采用带有内存模型缓存的模型服务方法。
- en: For supporting multiple applications with different types of models, the model
    server and prediction platform are the most suitable approaches. They include
    a unified prediction API, a routing component, a graph execution component, and
    multiple model server backends.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于支持具有不同类型模型的多个应用程序，模型服务器和预测平台是最合适的方法。它们包括统一的预测 API、路由组件、图执行组件和多个模型服务器后端。
