- en: 5 Exploring the search space with bandit-style policies
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 使用老虎机风格策略探索搜索空间
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: The multi-armed bandit problem and how it’s related to BayesOpt
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多臂老虎机问题及其与 BayesOpt 的关系
- en: The Upper Confidence Bound policy in BayesOpt
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 BayesOpt 中的上限置信度策略
- en: The Thompson sampling policy in BayesOpt
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 BayesOpt 中的 Thompson 抽样策略
- en: Which slot machine should you play at a casino to maximize your winnings? How
    can you develop a strategy to intelligently try out multiple slot machines and
    narrow down the most profitable machine? What does this problem have to do with
    BayesOpt? These are the questions this chapter will help us answer.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在赌场应该玩哪台老虎机以最大化你的收益？你如何制定一个策略，智能地尝试多台老虎机并缩小最有利可图的机器？这个问题与 BayesOpt 有什么关系？这些是本章将帮助我们回答的问题。
- en: Chapter 4 was our introduction to BayesOpt policies, which decide how the search
    space should be explored and inspected. The exploration strategy of a BayesOpt
    policy should guide us toward the optimum of the objective function we’d like
    to optimize. The two particular policies we learned about were Probability of
    Improvement (PoI) and Expected Improvement (EI), which use the idea that we’d
    like to improve from the best objective value we have seen so far. This improvement-based
    mindset is only a heuristic and, therefore, doesn’t constitute the only approach
    to BayesOpt.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 第四章是我们对 BayesOpt 策略的介绍，它决定了如何探索和检查搜索空间。BayesOpt 策略的探索策略应该指导我们朝着我们想要优化的目标函数的最优解前进。我们学到的两个特定策略是改进概率（PoI）和期望改进（EI），它们利用了我们希望从到目前为止看到的最佳目标值中改进的想法。这种基于改进的思维方式只是一种启发式方法，因此并不构成
    BayesOpt 的唯一方法。
- en: In this chapter, we learn about two more BayesOpt policies that are directly
    taken from the heavily related decision-making problem called the *multi-armed
    bandit* (MAB). Posed as a question of which is the most profitable slot machine
    to play at a casino, the MAB problem sets the stage for many decision-making-under-uncertainty
    problems. MAB has enjoyed a long history and extensive research, and many good
    solutions have been developed for this problem. As we learn in this chapter, MAB
    and BayesOpt are very similar problems—both deal with decision-making under uncertainty
    for optimization—so the expectation is that solutions to the MAB problem will
    work well on BayesOpt too.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了另外两个直接来自于与决策制定问题密切相关的 *多臂老虎机*（MAB）的 BayesOpt 策略。作为在赌场玩哪台老虎机最有利可图的问题，MAB
    问题为许多决策不确定性问题设置了舞台。MAB 有着悠久的历史和广泛的研究，为这个问题开发了许多良好的解决方案。正如我们在本章中所学到的，MAB 和 BayesOpt
    是非常相似的问题——都处理决策不确定性的优化——因此预期是 MAB 问题的解决方案也将在 BayesOpt 上表现良好。
- en: First, we briefly talk about the MAB problem and how it is related to BayesOpt.
    This discussion provides some background on the problem and puts into perspective
    how BayesOpt is related to other problems in AI. Then, we learn about the two
    most popular policies in MAB and apply them to BayesOpt. The first is the *Upper
    Confidence Bound* policy, which uses the *optimism in the face of uncertainty*
    principle to reason about its decisions. The second policy is called *Thompson
    sampling*, which is a randomized solution that actively uses the probabilistic
    nature of our predictive models. We then implement and run these policies on our
    running examples and analyze their performance.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们简要讨论 MAB 问题及其与 BayesOpt 的关系。这个讨论提供了一些关于问题的背景，并将 BayesOpt 与 AI 中的其他问题联系起来。然后，我们学习了
    MAB 中两个最流行的策略，并将它们应用到 BayesOpt 中。第一个是 *上限置信度* 策略，它使用 *面对不确定性的乐观主义* 原则来推断其决策。第二个策略被称为
    *Thompson 抽样*，它是一种主动利用我们预测模型的概率性质的随机化解决方案。然后，我们在我们的运行示例上实现并运行这些策略，并分析它们的性能。
- en: By the end of this chapter, we gain an understanding of the MAB problem and
    its relationship with BayesOpt. More importantly, we add two more items to our
    portfolio of BayesOpt policies, exposing ourselves to more ways of exploring the
    search space in a black box optimization problem.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本章末尾，我们对 MAB 问题及其与 BayesOpt 的关系有了了解。更重要的是，我们将 BayesOpt 策略的投资组合增加了两项，使我们暴露于在黑盒优化问题中探索搜索空间的更多方式。
- en: 5.1 Introduction to the MAB problem
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 MAB 问题简介
- en: In this section, we learn about the MAB problem on a high level. We start by
    discussing its problem statement and setting in the first subsection.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们从高层次学习 MAB 问题。我们首先讨论它的问题陈述和设置在第一小节。
- en: Important In a MAB problem, we need to choose an action to take at each step
    over a long horizon. Each action yields a reward according to an unknown reward
    rate, and our goal is to maximize the total reward we receive at the end of the
    long horizon.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示 在MAB问题中，我们需要在一个长时间段内每一步选择一个要采取的动作。每个动作根据未知的奖励率产生奖励，我们的目标是在长时间段结束时最大化我们获得的总奖励。
- en: We also explore its connections to BayesOpt, as well as other problems in AI
    and ML. This provides us with context, relating MAB to the rest of the text.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还探讨了它与BayesOpt的关系，以及AI和ML中的其他问题。这为我们提供了背景，将MAB与文本的其余部分联系起来。
- en: 5.1.1 Finding the best slot machine at a casino
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.1 寻找赌场最好的老虎机
- en: While *multi-armed bandit* may evoke a mysterious image in your mind, the term
    actually refers to a gambler’s problem of choosing which slot machines to play
    at a casino to obtain the largest amount of reward. Imagine you’re at a casino
    and there’s a slot machine with an “arm” you can pull.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然*多臂老虎机*可能在你的脑海中勾起神秘的形象，但这个术语实际上指的是一个赌徒在赌场选择玩哪些老虎机以获取最大奖励的问题。想象一下，你在一家赌场，有一台可以拉动“手臂”的老虎机。
- en: Upon pulling the arm of the slot machine, you may receive coins as a reward;
    however, there’s randomness in this process. Specifically, programmed within the
    inner-workings of this slot machine is a reward probability *p*. Each time the
    arm of the machine is pulled, the machine returns coins according to that probability.
    This slot machine is visualized in figure 5.1\. If *p* = 0.5, then the machine
    rewards its players half of the time. If *p* = 0.01, then roughly only 1 in 100
    pulls will result in coins being returned. Since this probability is programmed
    inside the machine—and therefore hidden away from us—there’s no way for us to
    know for sure what that probability is.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 拉动老虎机的手臂时，你可能会得到硬币作为奖励；但是，这个过程中存在随机性。具体来说，在这台老虎机的内部机制中编程了一个奖励概率*p*。每次拉动机器的手臂时，机器都会根据该概率返回硬币。这台老虎机在图5.1中可视化。如果*p*=0.5，则该机器向其玩家奖励一半的时间。如果*p*=0.01，则大约只有100次拉动中的1次会导致返回硬币。由于这个概率被编程在机器内部——因此隐藏在我们看不见的地方——我们无法确定这个概率是多少。
- en: '![](../../OEBPS/Images/05-01.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/05-01.png)'
- en: Figure 5.1 A slot machine with an arm that can be pulled. The machine may return
    coins according to its reward probability when its arm is pulled.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 一个带有可以拉动手臂的老虎机。当拉动手臂时，根据其奖励概率，该机器可能返回硬币。
- en: In this hypothetical setting, the casino programs its slot machines so that
    the rate at which these machines reward players with coins is lower than that
    of the machines being played and receiving coins. In other words, even if there
    are occasional winners who receive rewards from these slot machines, on average,
    it’s the casino that makes a profit.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个假设的情景中，赌场调整其老虎机的程序，以便这些机器向玩家奖励硬币的速度低于玩家所玩的机器和收到硬币的机器。换句话说，即使偶尔会有赢家从这些老虎机中获得奖励，平均而言，赚取利润的是赌场。
- en: Definition A particularly unsuccessful player who is losing coins faster than
    winning them might begrudgingly call the slot machine they have been playing “a
    bandit,” thinking the machine is stealing their money. Since the machine has an
    arm that may be pulled, it can be called a *one-armed bandit*.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 特别不成功的玩家，他们比赢得硬币的速度更快地失去硬币，可能会不情愿地称呼他们一直在玩的老虎机为“强盗”，认为机器在偷他们的钱。由于这台机器有一个可以拉动的手臂，因此可以称为*单臂强盗*。
- en: Now, imagine there’s not just one slot machine but a *row* of machines we may
    choose to play, each with its own reward probability *p*, as visualized in figure
    5.2.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，想象一下，不只是一个老虎机，而是一排我们可以选择玩的老虎机，每个都有自己的奖励概率*p*，如图5.2所示。
- en: '![](../../OEBPS/Images/05-02.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/05-02.png)'
- en: Figure 5.2 A slot machine with an arm that can be pulled. The machine may return
    coins according to its reward probability when its arm is pulled.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2 一个带有可以拉动手臂的老虎机。当拉动手臂时，根据其奖励概率，该机器可能返回硬币。
- en: With this row of slot machines, a strategic player may turn this setting into
    a decision-making challenge and aim to try out the slot machines in some intelligent
    manner so they can most quickly identify the machine that has the highest reward
    probability. Their intent here is to maximize the amount of reward they receive
    given that they can only pull the arms of these machines a specific number of
    times.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这一排老虎机，一个有战略眼光的玩家可能会将这种设置转化为一个决策挑战，并试图以某种智能的方式尝试这些老虎机，以便尽快确定哪台老虎机具有最高的奖励概率。他们的目的是在只能拉动这些机器的手臂特定次数的情况下最大化他们获得的奖励量。
- en: Definition This decision-making problem is called the *multi-armed bandit* (or
    MAB), as there are multiple arms we may pull. The goal is to design a policy of
    deciding which machine’s arm we should pull next to maximize the total reward
    we obtain at the end.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 这个决策问题被称为*多臂老虎机*（或 MAB），因为我们可以拉动多个手臂。目标是设计一个策略，决定接下来应该拉动哪个机器的手臂，以最大化我们最终获得的总奖励。
- en: 'We see that MAB possesses many characteristics of an optimization-under-uncertainty
    problem, like BayesOpt:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现 MAB 具有许多不确定性优化问题的特征，例如 BayesOpt：
- en: '*We can take specific actions.* Each action corresponds to pulling the arm
    of a specific slot machine.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*我们可以采取具体的行动*。每个行动对应于拉动特定老虎机的手臂。'
- en: '*We have a limited budget.* We can only pull these arms for a specific number
    of times until we have to stop.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*我们有限的预算*。我们只能在特定次数内拉动这些手臂，直到我们不得不停止。'
- en: '*There’s uncertainty in the outcomes of the actions that may be taken.* We
    don’t know what the reward probability of each slot machine is, and we can’t even
    estimate it until we have pulled its arm multiple times. Further, upon pulling
    an arm, we can’t know for sure whether we will receive coins, as there’s randomness
    in the reward.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在可能采取的行动的结果中存在不确定性*。我们不知道每台老虎机的奖励概率是多少，甚至在我们多次拉动其手臂之前我们也无法估计。此外，拉动手臂后，我们不能确定是否会收到硬币，因为奖励中存在随机性。'
- en: '*We’d like to optimize for an objective.* We aim to maximize the *cumulative
    reward*, which is the total number of coins we receive while pulling the arms
    of these machines until we stop.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*我们想要为一个目标进行优化*。我们的目标是最大化*累积奖励*，即我们在拉动这些机器的手臂直到停止时收到的硬币总数。'
- en: Perhaps most importantly, in MAB, we face the same exploration–exploitation
    tradeoff we discussed in section 4.1.2\. In particular, each time we decide to
    pull the arm of a machine, we need to choose between the machine that has been
    giving us a good success rate so far (exploitation) and others with a reward probability
    we don’t know much about (exploration).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 或许最重要的是，在 MAB 中，我们面临与我们在 4.1.2 节中讨论的探索-利用权衡相同的问题。特别是，每次我们决定拉动一台机器的手臂时，我们需要在迄今为止给我们较好成功率的机器（利用）和其他奖励概率我们了解不多的机器（探索）之间做出选择。
- en: 'The problem we face is a tradeoff because by exploring, we run the risk of
    wasting our pulls on machines that have low reward probability, but excessive
    exploitation means we might completely miss out on a machine that has a higher
    reward rate than what we currently observe. Figure 5.3 shows an example of this
    where the following is true:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们面临的问题是一个权衡，因为通过探索，我们可能会冒着将我们的拉动浪费在奖励概率低的机器上的风险，但过度利用意味着我们可能会完全错过一个奖励率比我们目前观察到的更高的机器。图
    5.3 展示了一个例子，其中以下情况为真：
- en: We have collected 70 coins over 100 pulls of the first machine. That is, the
    first machine offers the highest empirical success rate so far, at 70%.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在第一台机器上拉了 100 次手臂，收集了 70 枚硬币。也就是说，迄今为止第一台机器提供的经验成功率最高，为 70%。
- en: We have more data from the second machine, so we have the least uncertainty
    about its reward rate, which is around 50%.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从第二台机器收集了更多数据，因此我们对其奖励率的不确定性最小，大约为 50%。
- en: Although the empirical success rate of the third machine is the lowest (0%),
    we may need to try pulling its arm more times to be more certain of its reward
    rate.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尽管第三台机器的经验成功率最低（0%），但我们可能需要尝试更多次拉动它的手臂，以更确定其奖励率。
- en: '![](../../OEBPS/Images/05-03.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/05-03.png)'
- en: Figure 5.3 An example of an MAB dataset showing the exploration–exploitation
    dilemma. An MAB policy must choose between a machine with a consistently high
    success rate and one with a reward rate that is uncertain.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 一个展示探索-利用困境的 MAB 数据集示例。MAB 策略必须在一个成功率持续高的机器和一个奖励率不确定的机器之间做出选择。
- en: 'It is the job of a MAB policy, similar to that of a BayesOpt policy, to look
    at the data on past rewards and decide which arm we should pull next, while balancing
    this tradeoff between exploration and exploitation. The MAB problem models a wide
    range of applications you might see in the real world:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 与贝叶斯优化策略类似，多臂老虎机策略的工作是查看过去奖励的数据，并决定我们接下来应该拉动哪个臂，同时平衡探索和开发之间的权衡。多臂老虎机问题模拟了你可能在现实世界中看到的广泛应用的一系列应用场景：
- en: In product recommendation, an engine needs to pick from a number of products
    in a store to suggest one to a user. Each product can be seen as the arm of a
    slot machine, and pulling the arm means the engine picks that product to present
    to the user. If the user clicks on the advertisement for the product, we may see
    it as receiving a reward, as the user’s click is something we want to achieve.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在产品推荐中，引擎需要从商店中的许多产品中选择一个向用户推荐。每个产品都可以看作是一个老虎机的臂，拉动臂意味着引擎选择该产品向用户展示。如果用户点击了该产品的广告，我们可以视为收到了奖励，因为用户的点击是我们想要实现的目标。
- en: Many resource management problems may be framed as MAB, where we need to consider
    how to best allocate different resources to different organizations to best optimize
    some high-level objective (e.g., profit or productivity) without knowing in advance
    how well each organization will operate. Portfolio management can also be framed
    as an MAB problem in the same way.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多资源管理问题可以被构建为多臂老虎机问题，其中我们需要考虑如何最好地将不同资源分配给不同的组织，以最佳地优化一些高级目标（例如，利润或生产力），而不知道每个组织的运作效果。投资组合管理也可以以同样的方式构建为多臂老虎机问题。
- en: MAB also has seen application in the design of clinical trials, where each patient
    needs to be assigned to a specific treatment. We wish to optimize the treatment
    outcome across all patients but need to contend with limited resources and determining
    how likely each patient is to benefit from a given treatment.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多臂老虎机问题也在临床试验设计中得到应用，其中每个患者需要被分配到特定的治疗中。我们希望优化所有患者的治疗结果，但需要处理有限的资源以及确定每个患者从给定治疗中受益的可能性。
- en: In each of these applications, we can take a set of actions—that is, a set of
    arms we can pull—to optimize an objective under uncertainty.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些应用中，我们可以采取一组行动——也就是说，一组可以拉动的臂——以在不确定性下优化一个目标。
- en: 5.1.2 From MAB to BayesOpt
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.2 从多臂老虎机到贝叶斯优化
- en: We have seen that MAB and BayesOpt have many shared characteristics. In both
    problems, we need to reason about what decision we should take so that we can
    maximize a quantity we care about. Further, the outcome of each action is not
    deterministic. This means we don’t know whether an action will yield a good result
    until we actually take it.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到多臂老虎机和贝叶斯优化具有许多共同的特征。在这两个问题中，我们需要思考我们应该采取什么决策，以便我们可以最大化我们关心的数量。此外，每个行动的结果都不是确定的。这意味着我们不知道一个行动是否会产生好的结果，直到我们真正采取行动为止。
- en: However, these two problems are not equivalent. In MAB, we aim to maximize the
    *cumulative reward* over time—that is, the total number of coins one receives.
    With BayesOpt, we seek to simply find an input of a function that leads to a high
    value; so as long as among the dataset we collect there is a good objective value,
    we will succeed at optimization. This single-valued objective is sometimes called
    the *simple reward*. This difference means we need to make sure we are frequently
    receiving rewards in MAB to maintain a good cumulative reward, while in BayesOpt,
    we could afford to be more explorative to potentially find a good objective value.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，这两个问题并不相等。在多臂老虎机问题中，我们的目标是随着时间的推移最大化*累积奖励*——也就是说，接收到的硬币总数。而在贝叶斯优化中，我们只是寻找一个导致高价值的函数输入；只要我们收集的数据集中有一个良好的目标值，我们就会成功优化。这种单值目标有时被称为*简单奖励*。这种差异意味着我们需要确保在多臂老虎机问题中频繁地获得奖励，以维持良好的累积奖励，而在贝叶斯优化中，我们可以更加勇于探索，以潜在地找到一个良好的目标值。
- en: Definition The term *simple reward* doesn’t mean the objective is easier or
    simpler to optimize, but rather that the objective is a single number as opposed
    to a sum that is the cumulative reward.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 定义术语*简单奖励*并不意味着目标更容易或更简单地优化，而是目标是一个单一的数字，而不是累积奖励的总和。
- en: Further, there are only a finite number of actions to take in MAB (a finite
    number of arms we could pull). In BayesOpt, as we are attempting to optimize an
    objective function in a continuous domain, there are infinitely many actions.
    Since we assume with a GP that function values at nearby points are similar to
    one another, we can think of this as actions close to each other yielding similar
    reward rates. This is illustrated in figure 5.4, where each infinitesimal point
    is a slot machine whose arm we could pull (that is, query the objective function
    value at), and machines that are close to each other have similar colors.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在多臂老虎机中只有有限数量的动作可供选择（我们可以拉动的手臂数量有限）。在BayesOpt中，由于我们试图优化连续域中的目标函数，有无限多个动作。由于我们假设使用GP时接近点的函数值彼此相似，我们可以将这看作是彼此接近的动作产生类似的奖励率。这在图5.4中有所说明，其中每个微不足道的点都是我们可以拉动其手臂的老虎机（即，查询目标函数值），彼此接近的机器具有相似的颜色。
- en: '![](../../OEBPS/Images/05-04.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/05-04.png)'
- en: Figure 5.4 BayesOpt is similar to an MBA problem with infinitely many actions.
    Each infinitesimal point is a slot machine whose arm we could pull. Further, machines
    that are close to each other are correlated in the sense that they have similar
    reward rates.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4 BayesOpt类似于具有无限多个动作的MBA问题。每个微不足道的点都是一个老虎机，我们可以拉动其手臂。此外，彼此接近的机器在某种意义上是相关的，因为它们具有类似的奖励率。
- en: Most formalizations of the MAB problem consider the binary setting where a slot
    machine, when pulled, either returns a coin or doesn’t return anything. The function
    value we may observe in BayesOpt, on the other hand, could take on any real value.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数多臂老虎机问题的形式化考虑了二进制设置，即拉动老虎机时，要么返回一个硬币，要么什么也不返回。另一方面，在BayesOpt中我们可能观察到的函数值可以取任意实数值。
- en: The main differences between MAB and BayesOpt are summarized in table 5.1\.
    Although these are fundamental differences, the tradeoff between exploration and
    exploitation in decision-making is present in both problems, so it’s reasonable
    to aim to repurpose MAB policies to BayesOpt.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 多臂老虎机和BayesOpt之间的主要区别在表5.1中总结。虽然这些是根本性的差异，但在决策中探索和利用之间的权衡在两个问题中都存在，因此将多臂老虎机策略重新用于BayesOpt是合理的。
- en: Table 5.1 The differences between the MAB and BayesOpt
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.1 多臂老虎机和BayesOpt之间的区别
- en: '| Criterion | MAB | BayesOpt |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 标准 | 多臂老虎机 | BayesOpt |'
- en: '| --- | --- | --- |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Objective to be maximized | Cumulative reward | Simple reward |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 要最大化的目标 | 累积奖励 | 简单奖励 |'
- en: '| Type of observation/reward | Binary | Real-valued |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 观察/奖励类型 | 二进制 | 实值 |'
- en: '| Number of actions | Finite | Infinite |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 动作数量 | 有限 | 无限 |'
- en: '| Correlation between actions | No | Yes for similar actions |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 动作之间的相关性 | 否 | 是对于相似的动作 |'
- en: In the rest of this chapter, we learn about two such policies, the motivations
    behind them, and how to implement them with BoTorch. We start with the Upper Confidence
    Bound policy in the next section.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，我们将学习两种这样的策略，它们背后的动机以及如何使用BoTorch实现它们。我们将在下一节从上置信界限策略开始。
- en: 5.2 Being optimistic under uncertainty with the Upper Confidence Bound policy
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 在上置信界限策略下面对不确定性的乐观主义
- en: How should we account for the infinitely many possibilities for the value we
    may observe when we evaluate the objective function at a particular location?
    Further, how should we reason about these possibilities in a simple, efficient
    way that is conducive to decision-making? In this section, we explore the *Upper
    Confidence Bound* (UCB) policy from MAB, which results in a BayesOpt policy of
    the same name.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该如何考虑在特定位置评估目标函数时可能观察到的无限多种可能性的值？此外，我们应该如何以简单、高效的方式推理这些可能性，以促进决策？在本节中，我们探讨了多臂老虎机中的*上置信界限*（UCB）策略，该策略导致了同名的BayesOpt策略。
- en: UCB operates under the *optimism in the face of uncertainty* principle. In MAB,
    the idea is to use an upper bound of our estimate for each slot machine’s reward
    rate as a substitute for the true, unknown reward rate. That is, we optimistically
    estimate the reward rate of each machine using an upper bound of what we believe
    it to be and, finally, choose the one with the highest upper bound.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: UCB遵循*面对不确定性的乐观主义*原则。在多臂老虎机中，思想是使用每个老虎机奖励率的估计上界作为真实的未知奖励率的替代品。也就是说，我们乐观地估计每台机器的奖励率，使用我们认为的奖励率的上界，最后选择具有最高上界的机器。
- en: We first discuss this principle and how it aids our decision-making reasoning.
    Afterwards, we learn how to implement the UCB policy using BoTorch and analyze
    its behavior.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先讨论这个原则以及它如何辅助我们的决策推理。然后，我们学习如何使用BoTorch实现UCB策略并分析其行为。
- en: 5.2.1 Optimism under uncertainty
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.1 不确定情况下的乐观主义
- en: Let’s consider a simple example to make this idea concrete. Say you wake up
    one day and observe that even though it’s sunny outside, there are dark clouds
    on the horizon. You check the weather app on your phone to see if it will stay
    sunny throughout the day and whether you should bring an umbrella to work in case
    it rains later. Unfortunately, the app can’t tell you with absolute certainty
    whether it will stay sunny. Instead, you only see an estimate that the probability
    of sunny weather is between 30% and 60%.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举个简单的例子来具体说明这个想法。假设你某一天醒来发现外面虽然是晴天，但地平线上有乌云。你查看手机上的天气应用程序，看看今天是否会一直晴朗，以及是否应该带伞上班以防下雨。不幸的是，该应用程序无法确切地告诉你天气是否会晴朗。相反，你只能看到晴天的概率估计在30%到60%之间。
- en: You think to yourself that if the probability that it will stay sunny is less
    than 50%, then you will bring an umbrella. However, you don’t have a single-valued
    estimate here, but instead, a range between 30% and 60%. How, then, should you
    decide whether an umbrella is necessary?
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 你心里想，如果晴天的概率低于50%，那么你会带伞。然而，这里并不是一个单一值的估计，而是一个介于30%到60%之间的范围。那么，你应该如何决定是否需要带伞呢？
- en: A pessimist might say that since the probability of sunny weather could be as
    low as 30%, you should be on the safe side and prepare for the worst. Someone
    who considers the average case might look further into the app to see what the
    mean estimate is for the probability it will stay sunny and make their decision
    accordingly. From the perspective of an optimist, on the other hand, a 60% chance
    is reason enough to think that it will stay sunny, so this person wouldn’t bother
    bringing an umbrella with them to work. These ways of thinking are shown in figure
    5.5.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 悲观主义者可能会说，因为晴天的概率可能只有30%，所以你应该采取保险措施，做最坏的打算。考虑平均情况的人可能会进一步研究应用程序，看看晴天的平均估计概率是多少，然后做出相应的决定。而从乐观主义者的角度来看，60%的机会足以让人相信天气会晴朗，所以这个人不会打算带伞去上班。这些思考方式如图5.5所示。
- en: 'In figure 5.5, the third person’s reasoning corresponds to the idea behind
    the UCB policy: being optimistic about the outcome of an unknown event and making
    decisions based on this belief. In the MAB problem, the UCB would construct an
    upper bound of the reward rate of each slot machine and proceed to pick the machine
    with the highest bound.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在图5.5中，第三个人的推理对应于UCB策略背后的思想：对未知事件的结果持乐观态度，并根据这种信念做出决策。在多臂老虎机问题中，UCB会构建每个老虎机奖励率的上限，并选择具有最高上限的老虎机。
- en: '![](../../OEBPS/Images/05-05.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/05-05.png)'
- en: Figure 5.5 Different ways of thinking about the future and making decisions.
    The last person corresponds to the UCB policy, which reasons about an unknown
    quantity in an optimistic manner.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5 关于未来思考和做出决策的不同方式。最后一个人对应于UCB策略，以乐观的方式推理一个未知量。
- en: Note The way this policy is realized in BayesOpt is particularly simple since
    with a GP as our predictive model of the objective function, we already have an
    upper bound of the reward rate of each action. That is, we have an upper bound
    of the objective value at any given input location.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 注：在BayesOpt中实现这一策略的方式特别简单，因为我们使用高斯过程作为目标函数的预测模型，我们已经有了每个动作的奖励率的上限。也就是说，我们已经有了在任何给定输入位置的目标值的上限。
- en: Specifically, we know that the objective value at a given location follows a
    normal distribution, and a commonly used measure to quantify uncertainty of a
    normal distribution is the 95% CI, which contains 95% of the probability mass
    of the distribution. With a GP, we visualize this 95% CI across the input space
    as the thick line in figure 5.6\. The upper bound of this 95% CI, which is the
    upper boundary of the shaded region highlighted in figure 5.6, is exactly what
    the UCB policy uses as the acquisition scores for the data points in the search
    space, and the point that gives the highest score is the one at which we will
    evaluate the objective function next, which, in this case, is the location indicated
    by the dotted line around –1.3.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们知道给定位置的目标值遵循正态分布，而量化正态分布不确定性的常用度量是 95% CI，其中包含分布的 95% 的概率质量。使用 GP，我们将该
    95% CI 在输入空间中可视化为图 5.6 中粗线。该 95% CI 的上限，也就是图 5.6 中突出显示的阴影区域的上边界，正是 UCB 策略将用作搜索空间中数据点的获取分数的上边界，并且给出最高分数的点是我们将评估下一个目标函数的位置，这在本例中是由虚线围绕的位置约为
    –1.3。
- en: '![](../../OEBPS/Images/05-06.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/05-06.png)'
- en: Figure 5.6 The UCB of a GP that corresponds to the 95% CI. This bound can be
    used as the acquisition score of the UCB policy.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6 对应于 95% CI 的 GP 的 UCB。此边界可用作 UCB 策略的获取分数。
- en: Definition *Acquisition scores* quantify how valuable a data point is in guiding
    us towards the optimum of the objective function. We first learned about acquisition
    scores in section 4.1.1.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*获取分数* 的定义量化了数据点在引导我们朝着目标函数的最优值的过程中的价值。我们首次在 4.1.1 节了解到了获取分数。'
- en: You might think that this form of decision-making might not be appropriate,
    especially in high-stakes situations where the cost of a bad decision is high.
    In our umbrella example, by optimistically underestimating the probability of
    rain, you might run the risk of getting caught in the rain without an umbrella.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能认为这种决策方式可能不合适，特别是在决策成本高昂的高风险情况下。在我们的伞例子中，通过乐观地低估下雨的概率，您可能会冒着没有伞就被雨淋的风险。
- en: However, it is a particularly computationally efficient way of reasoning, as
    we simply need to extract an upper bound of our estimate of a quantity of interest
    and use that bound for decision-making. Moreover, as we see in the next subsection,
    by choosing which CI we’d like to use (as opposed to sticking with the 95% CI),
    we have complete control over how optimistic the UCB is. This control also allows
    the policy to balance exploration and exploitation, which is the core question
    that needs to be addressed by any BayesOpt policy.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这是一种特别高效的推理方式，因为我们只需要提取我们对感兴趣数量的估计的一个上界，并将该边界用于决策。此外，正如我们在下一小节中看到的那样，通过选择我们想要使用的
    CI（而不是坚持使用 95% CI），我们完全控制 UCB 有多乐观。这种控制还允许策略平衡探索和利用，这是任何 BayesOpt 策略需要解决的核心问题。
- en: 5.2.2 Balancing exploration and exploitation
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.2 平衡探索和利用
- en: In this subsection, we talk further about how the UCB policy may be adjusted
    by us, the BayesOpt users. This offers a level of control that balances between
    regions with high uncertainty (exploration) and regions with high predictive mean
    (exploitation). This discussion aims to provide a deeper understanding of UCB
    before moving ahead with BoTorch implementation in the next subsection.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们进一步讨论了我们，BayesOpt 用户，如何调整 UCB 策略。这提供了一种在高不确定性区域（探索）和高预测均值区域（利用）之间平衡的控制水平。这次讨论旨在在下一小节
    BoTorch 实现之前更深入地理解 UCB。
- en: Remember that with a normal distribution, going two standard deviations away
    from the mean (that is, mean *μ* plus/minus 2 times standard deviation σ) gives
    us the 95% CI. The upper bound of this interval (*μ* + 2σ) is the acquisition
    score of UCB we saw in the previous subsection.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，对于正态分布，从平均值（即，均值 *μ* 加/减 2 倍标准差 σ）偏离两个标准偏差会给我们带来 95% CI。这个区间的上限（*μ* + 2σ）就是我们在上一小节中看到的
    UCB 的获取分数。
- en: 'However, the 95% CI is not the only CI of a normal distribution. By setting
    the multiplier, denoted as β, for the standard deviation σ in the formula *μ*
    + βσ, we obtain other CIs. For example, the following is true in a one-dimensional
    normal distribution, as shown in figure 5.7:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 95% 置信区间（CI）不是正态分布的唯一 CI。通过在公式 *μ* + βσ 中设置标准差 σ 的乘法器（表示为 β），我们可以获得其他 CI。例如，在一维正态分布中，如图
    5.7 所示，以下结论成立：
- en: 'Going one standard deviation above the mean (*μ* + σ)—that is, setting β =
    1—gives the 68% CI: 68% of the probability mass of the normal distribution lies
    between *μ* – σ and *μ* + σ.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 离均值上一个标准差（*μ* + σ）——即设置β = 1——给出了68%的置信区间：正态分布的68%的概率质量位于*μ* - σ和*μ* + σ之间。
- en: Similarly, three standard deviations away from the mean (β = 3) gives us the
    99.7% CI.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同样，距离均值三个标准差（β = 3）给出了99.7%的置信区间。
- en: '![](../../OEBPS/Images/05-07.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/05-07.png)'
- en: Figure 5.7 Different CIs of standard normal distributions. Going away from the
    mean to one, two, and three standard deviations, we obtain the 68%, 95%, and 99.7%
    CIs. The upper bounds of these intervals are used by the UCB policy.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7展示了标准正态分布的不同置信区间。从均值偏离一个、两个和三个标准差，我们得到了68%、95%和99.7%的置信区间。UCB策略使用这些间隔的上界。
- en: In fact, any value of β would give us a unique CI for the normal distribution.
    Since the UCB only dictates that we should use an upper bound from our predictive
    model to make decisions, any value of the form *μ* + βσ can serve as the upper
    bound used in UCB. By setting this parameter β, we can control for the behavior
    of the UCB policy.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，任何β的值都可以给我们一个唯一的正态分布置信区间。由于UCB只指示我们应该使用来自预测模型的上界来做决策，所以形如*μ* + βσ的任何值都可以作为UCB中使用的上界。通过设置此参数β，我们可以控制UCB策略的行为。
- en: Figure 5.8 shows three different upper bounds corresponding to β = 1, 2, 3,
    while the mean function, in fact, corresponds to setting β = 0\. We see that although
    the upper bounds are of roughly the same shape, these bounds go up and down at
    different rates. Further, since the data point maximizing this bound is the point
    the UCB selects to query for optimization, different bounds, that is, different
    values of β will induce different optimization behaviors.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8展示了对应于β = 1、2、3的三个不同的上界，而事实上，均值函数对应于设置β = 0\. 我们可以看到，尽管上界的形状大致相同，但这些上界以不同的速率上下波动。此外，由于最大化此上界的数据点是UCB选择进行优化查询的点，不同的上界，即不同的β值，将导致不同的优化行为。
- en: '![](../../OEBPS/Images/05-08.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/05-08.png)'
- en: Figure 5.8 Different upper bounds of a GP, corresponding to different CIs and
    β values. The larger β is, the more explorative the UCB policy becomes.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8展示了GP的不同上界，对应不同的置信区间和β值。β越大，UCB策略就越趋向于探索性。
- en: Important The smaller β is, the more exploitative UCB becomes. Conversely, the
    larger β is, the more explorative UCB becomes.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，β越小，UCB就越趋向于剥削性。相反，β越大，UCB就越趋向于探索性。
- en: 'We can see that the value of β controls the behavior of UCB by inspecting the
    formula for the acquisition score: *μ* + βσ. When β is small, the mean *μ* contributes
    the most to the acquisition score. So, the data point with the highest predictive
    mean will maximize this score. This selection corresponds to pure exploitation,
    as we are simply picking out the point with the largest predicted value. When
    β is large, on the other hand, the standard deviation σ, which quantifies our
    uncertainty, becomes more important in the UCB score, emphasizing the need for
    exploration.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过检查获取分数的公式*μ* + βσ来看到β值控制UCB行为的方式。当β值很小时，平均值*μ*对获取分数做出的贡献最大。因此，具有最高预测均值的数据点将最大化此分数。此选择对应纯粹的剥削性，因为我们只是选择具有最大预测值的点。另一方面，当β值很大时，标准差σ，即量化我们的不确定性，在UCB分数中变得更加重要，强调了探索的需求。
- en: We see this distinction in figure 5.8 where around 0 is where we achieve the
    highest predictive mean, indicating it is the region of exploitation. As β increases,
    the point at which the different upper bounds peak gradually moves to the left
    where there is more uncertainty in our prediction.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从图5.8中看出这个差异，其中0附近是我们实现最高预测均值的地方，表明这是开发区域。随着β的增加，不同上界峰值所在的点逐渐向左移动，在这里我们的预测更加不确定。
- en: Note Interestingly, in the limit where β → ∞, the point that maximizes UCB’s
    acquisition score is where the standard deviation σ is maximized—that is, where
    our uncertainty is at its highest. This behavior corresponds to pure exploration,
    as we are selecting data points with high uncertainty.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 注意有趣的是，在β → ∞的极限情况下，最大化UCB获取分数的点是使标准差σ最大化的点，即我们的不确定性最大化的点。这种行为对应纯粹的探索性，因为我们选择具有高度不确定性的数据点。
- en: 'Finally, UCB correctly assigns higher scores to data points that offer better
    balance between exploration and exploitation:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，UCB正确地为提供在探索和开发之间提供更好平衡的数据点分配更高的分数：
- en: If two data points have the same predictive mean but different predictive standard
    deviations, then the one with the higher uncertainty will have a higher score.
    The policy, therefore, rewards exploration.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果两个数据点具有相同的预测均值但不同的预测标准差，则具有较高不确定性的数据点将具有更高的分数。因此，该策略奖励探索。
- en: If two data points have the same predictive standard deviations but different
    predictive means, then the one with the higher mean will have a higher score.
    The policy, therefore, rewards exploitation.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果两个数据点具有相同的预测标准差但不同的预测均值，则具有较高均值的数据点将具有更高的分数。因此，该策略奖励利用。
- en: Remember that one of the policies discussed in chapter 4, EI, also has this
    property, which is a desideratum of any BayesOpt policy.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在第 4 章讨论的策略中，EI 也具有这种特性，这是任何贝叶斯优化策略的期望。
- en: All in all, this parameter, β, controls UCB and how the policy explores and
    exploits the search space. This means that by setting the value of this parameter,
    we have direct control over its behavior. Unfortunately, beyond the fact that
    the value for β corresponds to the level of exploration, there’s no straightforward,
    principled way of setting this parameter, and some values might work on some problems
    but not on others. Exercise 1 of this chapter further discusses a more-involved
    method of setting β that might generally work sufficiently well.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，这个参数 β 控制 UCB 以及策略如何探索和利用搜索空间。这意味着通过设置该参数的值，我们可以直接控制其行为。不幸的是，除了 β 的值对应于探索程度这一事实之外，没有一种直观的、原则性的方法来设置该参数，某些值可能在某些问题上有效，但在其他问题上却无效。本章的练习
    1 进一步讨论了一种更为复杂的设置 β 的方法，该方法可能通常工作得足够好。
- en: Note The documentation of BoTorch usually shows UCB with β = 0.1, while many
    research papers that use UCB for BayesOpt opt for β = 3\. So, 0.1 could be your
    go-to value when using this policy if you prefer exploitation, and 3 should be
    the default if you lean toward exploration.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 BoTorch 的文档通常显示 UCB 的 β = 0.1，而许多使用 UCB 进行贝叶斯优化的研究论文选择 β = 3。所以，如果你更偏向利用，0.1
    可以是你使用该策略的首选值，如果你更倾向于探索，3 应该是默认值。
- en: 5.2.3 Implementation with BoTorch
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.3 在 BoTorch 中实现
- en: Having thoroughly discussed the motivation and math behind UCB, we now learn
    how to implement the policy using BoTorch. The code we look at here is included
    in CH05/01 - BayesOpt loop.ipynb. Remember from section 4.2.2 that although we
    could manually implement the PoI policy ourselves, declaring the BoTorch policy
    object, using it with the GP model, and optimizing the acquisition score using
    BoTorch’s helper function `optimize_acqf()` make it easier to implement our BayesOpt
    loop.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在充分讨论了 UCB 的动机和数学之后，我们现在学习如何使用 BoTorch 实现该策略。我们在这里看到的代码包含在 CH05/01 - BayesOpt
    loop.ipynb 中。请记住，虽然我们可以手动实现 PoI 策略，但声明 BoTorch 策略对象，将其与 GP 模型一起使用，并使用 BoTorch
    的辅助函数 `optimize_acqf()` 优化获取分数，这样可以更轻松地实现我们的贝叶斯优化循环。
- en: For this reason, we do the same thing here and use the built-in UCB policy class,
    even though we could simply compute the quantity μ + βσ ourselves. This can be
    done with
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 出于这个原因，我们在这里做同样的事情，并使用内置的 UCB 策略类，尽管我们可以简单地自己计算 μ + βσ 的数量。这可以通过以下方式完成
- en: '[PRE0]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, the UCB class implementation in BoTorch takes in the GP model as its first
    input and a positive value for its second input `beta`. As you might expect, this
    second input denotes the value for the UCB parameter β in the score formula μ
    + βσ, which trades off between exploration and exploitation. Here, we set it to
    1 for now.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，BoTorch 中的 UCB 类实现将 GP 模型作为其第一个输入，并将正值作为其第二个输入 `beta`。正如你所料，这第二个输入表示 UCB
    参数 β 在评分公式 μ + βσ 中的值，该公式在探索和利用之间进行权衡。在这里，我们暂时将其设置为 1。
- en: Note Believe it or not, this is all we need to change from our BayesOpt code
    from the previous chapter to run the UCB policy on the objective functions we
    have. This demonstrates the benefit of BoTorch’s modularity, which allows us to
    plug and play any policy we’d like to use into our BayesOpt pipeline.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 信不信由你，这就是我们需要从上一章的贝叶斯优化代码中更改的全部内容，以在我们拥有的目标函数上运行 UCB 策略。这展示了 BoTorch 模块化的好处，它允许我们将任何我们想要使用的策略插入我们的贝叶斯优化流程中。
- en: Running our UCB policy with β = 1 on our familiar Forrester objective function
    generates figure 5.9, which shows optimization progress throughout 10 function
    evaluations. We see that like what happened to the PoI policy, UCB with β = 1
    fails to sufficiently explore the search space and is stuck at a local optimum
    in the case of the Forrester function. This means the value for β is too small.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/05-09.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 Progress made by the UCB policy with the tradeoff parameter β = 1\.
    The value of the parameter is not sufficiently large to encourage exploration,
    causing progress to be stuck at a local optimum.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Note We learn about the Probability of Improvement policy in section 4.2.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us try again, this time setting this tradeoff parameter to a larger value:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The progress of this version of the UCB policy is shown in figure 5.10\. This
    time, UCB is able to find the global optimum thanks to the higher level of exploration
    induced by the larger value of β. However, if β is so large that UCB only spends
    its budget exploring the search space, then our optimization performance might
    also suffer. (We see an example of this in this chapter’s exercises later on.)
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/05-10.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
- en: Figure 5.10 Progress made by the UCB policy with the tradeoff parameter β =
    2\. The policy successfully finds the global optimum.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the importance of using a good value for this tradeoff parameter while
    using the UCB policy is clear. However, again, it’s difficult to say what value
    will work well for a given objective function. Exercise 1 of this chapter explores
    a certain strategy to adjust the value of this parameter as we progress through
    the search.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: This marks the end of our discussion on UCB. We have seen that by adopting the
    *optimism in the face of uncertainty* mindset from the MAB problem, we obtain
    a BayesOpt policy whose explorative behavior can be directly controlled and tuned
    with a tradeoff parameter. In the next section, we move on to the second policy
    taken from MAB, which has an entirely different motivation and strategy.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Smart sampling with the Thompson sampling policy
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we learn about another heuristic in MAB that directly translates
    to a widely used BayesOpt policy called *Thompson sampling* (TS). As we will see,
    this policy uses an entirely different motivation from that of UCB and, therefore,
    induces different optimization behavior. Similar to section 5.2 with UCB, we learn
    the general idea behind this BayesOpt policy first and then move on to its code
    implementation later.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1 One sample to represent the unknown
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With UCB, we make decisions based on an optimistic estimate of the unknown quantity
    we care about. This offers a simple way to reason about the consequences of the
    actions we take and the rewards we receive that trades off exploration and exploitation.
    What about TS?
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Definition The idea of Thompson sampling is to first maintain a probabilistic
    belief about the quantity we care about and then *sample* from that belief and
    treat that sample as a replacement of the true unknown quantity in question. This
    sampled replacement is then used to pick out the best decision we should make.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Let’s come back to our weather forecast example to see how this works. Again,
    we are interested in the problem of deciding whether we should bring an umbrella
    to work in which we are given an estimate of the probability that the weather
    will stay sunny throughout the day. Remember that the UCB relies on an upper bound
    of this estimate to inform its decisions, but what will the TS policy do?
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'The TS first draws a sample from the probability distribution we use to model
    the unknown quantity—whether it will stay sunny, in this case—and make decisions
    based on this sample. Let’s say that instead of a range like we saw in the previous
    section, the weather app on our phone now announces there’s a 66% (roughly two
    out of three) chance the weather will say sunny. This means that when following
    the TS policy, we first flip a coin with a two-thirds bias for heads:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: If the coin lands on heads (with a 66% chance), then we treat it as if the weather
    will stay clear throughout the day and conclude we won’t need an umbrella.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the coin lands on tails (with 34% chance), then we treat it as if it will
    rain and conclude we should bring an umbrella to work.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This process of TS is visualized in figure 5.11 as a decision tree, where at
    the beginning, we flip a biased coin to obtain a sample from the probability distribution
    of sunny weather. Based on whether the coin lands on heads (representing a sample
    of sunny weather) or tails (a sample of rain), we decide whether or not we should
    bring an umbrella to work.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/05-11.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
- en: Figure 5.11 The TS policy as a decision tree. We flip a biased coin to obtain
    a sample from the probability distribution of sunny weather and decide whether
    to bring an umbrella based on this sample.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: While this might seem like an arbitrary method of decision-making at first,
    TS is particularly effective in both MAB and BayesOpt. First of all, given a probability
    distribution representing a quantity of interest, a sample of that distribution
    is a possible realization of that quantity, so that sample may be used as a representation
    of the distribution. In optimization under uncertainty problems, TS offers the
    same benefit as UCB in that sampling from a probability distribution is often
    easy to do. Just as UCB’s optimistic estimates of the reward rates may be generated
    in an efficient manner, sampling may be done just as efficiently.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider how TS works in BayesOpt. From a GP trained on the current observed
    data, we draw one sample. Remember from chapter 3 that a sample from a GP is a
    function representing a particular realization of the objective function according
    to our GP belief. However, unlike the true objective function, which is unknown
    in regions without observed data, a sample drawn from a GP is entirely known.
    This means we can find the location at which this sample is maximized.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.12 shows the GP we trained on the Forrester function and three samples
    drawn from it as dashed lines. Further, along each of the samples, we use a diamond
    to indicate the location at which the sample is maximized. As we can see, one
    sample is maximized around –3.2, another around –1.2, and the third at 5.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/05-12.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
- en: Figure 5.12 Samples drawn from a GP and data points that maximize the corresponding
    samples. Whichever sample is drawn, TS selects the data point that maximizes that
    sample as the next point at which to evaluate the objective function.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: When using the TS policy, it’s entirely up to chance to determine which of these
    three samples (or a completely different sample) we draw from the GP. However,
    whichever sample we draw, the data point that maximizes the sample is the one
    we will query next. That is, if we draw the sample maximized at –3.2, then –3.2
    is where we will evaluate the objective function next. If we draw the sample maximized
    at 5, then we will query the point *x* = 5 next.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Definition The acquisition scores that TS computes are the values of a random
    sample drawn from the GP. The data point that maximizes this sample is the one
    we will query next.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike other policies we have seen so far, TS is a *randomized* policy, which
    means that when faced with the same training data and GP, it’s not guaranteed
    that the policy will make the same decision (unless we computationally set the
    random seed). However, this randomness is in no way a disadvantage. We have said
    that drawing a sample from a GP is easy, so computing the TS acquisition score
    may be done efficiently. Further, maximization of a random sample inherently balances
    between exploration and exploitation, which, as we know, is the main concern in
    BayesOpt:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: If a data point has a high predictive mean, then the value of a random sample
    at that data point is likely to be high, making it more likely to be the one maximizing
    the sample.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a data point has a high predictive standard deviation (that is, uncertainty),
    then a random sample at that data point will also have a higher variability and,
    thus, a higher chance of having a high value. This higher variability, therefore,
    also makes the data point more likely to be chosen as the next point to query.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TS is likely to exploit with a random sample that is maximized in a region with
    a high predictive mean, but the policy is just as likely to explore with another
    sample given the same situation. We see this in figure 5.12, where one sample
    is maximized around –1.2, which has a relatively high predictive mean. If this
    is the sample we draw, then by evaluating the objective at –1.2, we will be exploiting
    the function. However, if we draw either of the other two samples, then we will
    be exploring, as the regions where the samples are maximized have high uncertainty.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: This is quite an elegant tradeoff scheme. By using the randomness of the samples,
    TS directly uses the probabilistic nature of our predictive model, the GP, to
    explore and exploit the search space. The randomized nature of TS means that at
    any given iteration of the BayesOpt loop, the policy might not make the best decision
    to trade off exploration and exploitation, but over time, in the aggregation of
    its decisions, the policy will be able to explore the space sufficiently and narrow
    down high-performing regions.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2 Implementation with BoTorch
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s now move on to implementing this TS policy in Python. Once again, the
    code is included in the CH05/01 - BayesOpt loop.ipynb notebook. Remember that
    for the BayesOpt policies we have seen, implementation comes down to declaring
    a BoTorch policy object and specifying the relevant information. Then, to find
    the data point we should query next, we optimize the acquisition score, as computed
    by the policy. This, however, is *not* the case with TS.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: The main challenge of implementing TS as a generic PyTorch module that takes
    in a data point and scores it according to some criterion is that sampling from
    a GP can only be done with a finite number of points. In the past, we have represented
    a sample from a GP using a sample of a high-dimensional MVN distribution of a
    dense grid over the search space. When samples of this dense Gaussian are plotted,
    they appear to be actual functions with smooth curves, but in reality, they are
    defined over a grid.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: All of this is to say that it is computationally impossible to draw a sample
    as a function from a GP since that would require an infinite number of bits. A
    typical solution is to draw the corresponding MVN over a large number of points
    that span the input space so that all regions in the search space are represented.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'Important This is exactly what we do to implement TS: generate a large number
    of points throughout the search space and draw a sample from the MVN distribution
    corresponding to predictions from a GP over these points.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: The procedure for TS is summarized in figure 5.13, where we use a *Sobol sequence*
    as the points that span the search space. (We discuss why a Sobol sequence is
    preferred over other sampling strategies shortly.) We then draw a sample from
    the GP on these points and pick out the point that yields the highest value from
    the sample. This sample is then used to represent a sample from the GP itself,
    and we evaluate the objective function at the location where the sampled value
    is maximized next.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/05-13.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
- en: Figure 5.13 Flowchart of the implementation of TS in BoTorch. We use a Sobol
    sequence to populate the search space, draw a sample from the GP on the sequence,
    and choose the point in the sequence that maximizes the sample to evaluate the
    objective function at.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Definition A *Sobol sequence* is an infinite list of points in a region in Euclidean
    space that aims to cover the region evenly.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Let’s first discuss why we need to use a Sobol sequence to generate points that
    span our search space. A simpler solution is to use a dense grid. However, generating
    a dense grid quickly becomes intractable as the number of dimensions of our search
    space grows, so this strategy is out of the question. Another potential solution
    is to uniformly sample from that space, but statistical theory shows that uniform
    sampling is, in fact, not the best way to generate points that cover a space evenly,
    and a Sobol sequence does a better job.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.14 shows the comparison between a Sobol sequence of 100 points and
    the same number of points uniformly sampled within the two-dimensional unit square.
    We see that the Sobol sequence covers the square more evenly, which is what we’d
    like to achieve with TS. This contrast is even more glaring in higher dimensions,
    giving us more reason to prefer Sobol sequences than uniformly sampled data points.
    We don’t discuss Sobol sequences in detail here; what is important for us to know
    is that a Sobol sequence is the go-to method of sample generation if our goal
    is to cover a space evenly.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/05-14.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
- en: Figure 5.14 Points from a Sobol sequence vs. uniformly sampled points in the
    two-dimensional unit square. The Sobol sequence covers the square more evenly
    and, therefore, should be used by TS.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: 'PyTorch provides an implementation of Sobol sequences, which can be used as
    follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ The number of dimensions of the space
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: ❷ The number of points to be generated
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Here, `sobol` is an instance of the `SobolEngine` class, which implements the
    sampling logic of Sobol sequences in the unit cube, and `candidate_x` is a PyTorch
    tensor of the shape `(num_candidates,` `dim)` that contains the generated points
    with the correct dimensionality.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Note It’s important to remember that `SobolEngine` generates points that cover
    the unit cube. To get `candidate_x` to cover the space we want, we need to resize
    this tensor accordingly.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: How many points the Sobol sequence should contain (that is, the value of `num_
    candidates`) is up to us (the users) to decide; the preceding example shows that
    we are using 1,000\. In a typical case, you would want a value large enough so
    that the search space could be sufficiently covered. However, a value that is
    too large will make sampling from the posterior GP numerically unstable.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Sobol 序列应该包含多少个点（即 `num_ candidates` 的值）由我们（用户）决定；前面的例子展示了我们使用的是 1,000。在典型情况下，您会想要一个足够大的值，以便搜索空间被足够覆盖。然而，一个值太大会使从后验
    GP 中采样数值上不稳定。
- en: Numerical instability when drawing GP samples
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制 GP 样本时的数值不稳定问题
- en: 'The numerical instability when drawing GP samples could, at times, lead to
    the following warning when we run TS:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在绘制 GP 样本时的数值不稳定性可能会导致我们在运行 TS 时出现以下警告：
- en: '[PRE3]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This warning indicates that the code has encountered a numerical problem due
    to the fact that the covariance matrix of the GP is not positive definite (`p.d.`)
    when it should be. However, this code also applies an automatic fix where we add
    a “jitter” of 1e–6 to this covariance matrix, making the matrix positive definite,
    so we, the users, don’t have to do anything further.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这个警告表明，代码遇到了数值问题，因为 GP 的协方差矩阵不是正定 (`p.d.`) 的。然而，这个代码也应用了自动修复，其中我们向这个协方差矩阵中添加了
    1e–6 的“抖动”，使矩阵成为正定的，所以我们用户不需要再做任何事情。
- en: 'Just as we did in section 4.2.3, we use the `warnings` module to disable this
    warning to make the output of our code cleaner as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在 4.2.3 节中所做的一样，我们使用 `warnings` 模块来禁用此警告，使我们的代码输出更干净，如下所示：
- en: '[PRE4]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Code for TS
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ TS 代码
- en: You could play around with multiple thousands of points to find the number that
    works best for your use case, objective function, and trained GP. However, you
    should always use at least 1,000 points.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以玩耍数千个点来找到最适合您的用例、目标函数和训练 GP 的数量。然而，您应该至少使用 1,000 个点。
- en: Next, we move on to the second component of TS, which is the process of sampling
    an MVN from our posterior GP and maximizing it. First, the sampler object that
    implements the sampling may be declared as
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们转向 TS 的第二个组成部分，即从我们的后验 GP 中采样 MVN 并最大化它的过程。首先，实现取样的采样器对象可以被声明为：
- en: '[PRE5]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `MaxPosteriorSampling` class in BoTorch implements the logic of TS: sampling
    from the posterior of a GP and maximizing that sample. Here, `model` refers to
    the GP trained on observed data. It is important to set `replacement` to `False`,
    making sure we are sampling without replacement (sampling with replacement isn’t
    appropriate for TS). Finally, to obtain the data point that gives the highest
    sampled value among `candidate_x`, we pass it to the sampler object:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: BoTorch 中的 `MaxPosteriorSampling` 类实现了 TS 的逻辑：从 GP 后验中采样并最大化该样本。这里，`model` 指的是所观察数据上训练的
    GP。重要的是将 `replacement` 设为 `False`，确保我们是无替换采样（替换采样不适用于 TS）。最后，为了获得在 `candidate_x`
    中最大样本值的数据点，我们将其传递给采样器对象：
- en: '[PRE6]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The returned value is, indeed, the point that maximizes the sample, which is
    the point we query next. And with that, our implementation of the TS policy is
    complete. We may plug this into the code we have been using so far for the BayesOpt
    loop on the Forrester function with the following:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的值确实是最大化样本的点，这是我们下一个查询的点。有了这一点，我们的 TS 策略实现就完成了。我们可以将这个代码插入到迄今为止我们用于 Forrester
    函数的贝叶斯优化循环中，并使用以下代码：
- en: '[PRE7]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Generates points from a Sobol engine
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从 Sobol 引擎生成点
- en: ❷ Resizes the generated points to be between −5 and 5, which is our search space
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 调整生成的点的大小为 −5 到 5，即我们的搜索空间。
- en: ❸ Generates the TS candidate to be queried next
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 生成下一个要查询的 TS 候选
- en: ❹ Visualizes our current progress without the acquisition function
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 在没有采集函数的情况下可视化我们的当前进度
- en: Note that the overall structure of our BayesOpt loop remains the same. What’s
    different is that instead of a BoTorch policy object, we now have a Sobol sequence
    to generate the set of points that cover our search space, which are then fed
    into a `MaxPosteriorSampling` object that implements the TS policy. The variable
    `next_x`, like before, contains the data point we will query next.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们的 BayesOpt 循环的总体结构仍然相同。不同的是，我们现在有一个 Sobol 序列来生成涵盖我们搜索空间的点集，然后将其馈送到实现 TS
    策略的 `MaxPosteriorSampling` 对象中，而不是 BoTorch 策略对象。变量 `next_x`，就像之前一样，包含我们将查询的数据点。
- en: Note Since we don’t have a BoTorch policy object when visualizing our progress
    using the `visualize_gp_belief_and_policy()` helper function, we don’t specify
    the `policy` argument anymore. This function, therefore, will only show the trained
    GP at each iteration, without the acquisition scores.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：由于在使用 `visualize_gp_belief_and_policy()` 辅助函数可视化过程中，我们没有 BoTorch 策略对象，因此不再指定
    `policy` 参数。因此，该函数仅显示每个迭代中的训练好的 GP，而没有获取分数。
- en: '![](../../OEBPS/Images/05-15.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/05-15.png)'
- en: Figure 5.15 Progress made by the TS policy. The policy explores the search space
    for some iterations and then gradually zeros in on the global optimum.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.15 TS 策略的进展。该策略探索搜索空间一段时间后逐渐将关注点锁定在全局最优解上。
- en: Optimization progress of TS is shown in figure 5.15, where we can observe that
    the policy successfully zeros in on the global optimum throughout the procedure—but
    not without spending a couple of queries exploring the space. This showcases the
    ability to trade off exploration and exploitation of TS in BayesOpt.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.15 显示了 TS 的优化进展，我们可以观察到该策略成功地将关注点锁定在全局最优解上但不是不花费探索空间查询次数。这展示了 TS 在 BayesOpt
    中协调探索和开发的能力。
- en: Our discussion on BayesOpt policies inspired by policies under the MAB setting
    is thus concluded. We have seen that each of the two policies we learned, UCB
    and TS, uses natural heuristics to reason about unknown quantities in an efficient
    manner and make decisions accordingly. One challenge in BayesOpt, the balance
    between exploration and exploitation, is also addressed by both policies, allowing
    the policies to have good optimization performance. In the next and final chapter
    of the second part of the book, we learn about yet another commonly used heuristic
    to decision-making in BayesOpt, this time by using information theory.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了基于 MAB 设置的策略所启发的 BayesOpt 策略。我们已经看到，我们学习的两个策略，UCB 和 TS，每个都使用自然启发式，以高效的方式推理未知量并相应地做出决策。BayesOpt
    中的一个挑战，即探索和开发之间的平衡问题，也由这两种策略解决，使这些策略具有良好的优化性能。在本书第二部分的下一章节中，我们将学习另一种常用的启发式决策方法，即使用信息论。
- en: 5.4 Exercises
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4 练习
- en: 'There are two exercises in this chapter:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 本章有两个练习：
- en: The first exercise explores a potential method to set the tradeoff parameter
    for the UCB policy that considers how far along we are in optimization.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一个练习探索了为 UCB 策略设置权衡参数的潜在方法，该方法考虑了我们在优化过程中的进展情况。
- en: The second exercise applies the two policies we have learned in this chapter
    to the hyperparameter tuning problem seen in previous chapters.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二个练习将本章中学习到的两个策略应用于先前章节中看到的超参数调整问题。
- en: '5.4.1 Exercise 1: Setting an exploration schedule for the UCB'
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '5.4.1 练习 1: 为 UCB 设置探索计划'
- en: This exercise, implemented in CH05/02 - Exercise 1.ipynb, discusses a strategy
    of adaptively setting the value of the tradeoff parameter β of the UCB policy.
    As mentioned in the section on UCB, the performance of the policy heavily depends
    on this parameter, but it’s not obvious how we should set its value. A value might
    work well on some objective functions but poorly on others.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 此练习在 CH05/02 - Exercise 1.ipynb 中实现，讨论了一种自适应设置 UCB 策略的权衡参数 β 的方法。正如 UCB 部分中提到的那样，策略的表现严重依赖于此参数，但我们不清楚该如何设置其值。一个值在某些目标函数上可能效果很好，但在其他目标函数上效果很差。
- en: BayesOpt practitioners have noticed that as we collect more and more data, UCB
    can become too exploitative. This is because as the size of our training dataset
    increases, we gain more knowledge about the objective function, and our uncertainty
    about the predictions made by the GP decreases. This means the CI produced by
    the GP will become tighter, moving the upper bound that makes up the acquisition
    score used by UCB closer to the mean prediction.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: BayesOpt 从业者已经注意到，随着我们收集越来越多的数据，UCB 可能会过于开发。这是因为随着训练数据集的大小增加，我们对目标函数的了解更多，GP
    预测的不确定性也会减少。这意味着 GP 产生的 CI 将变得更紧，将 UCB 使用的获取分数的上限移动到了平均预测附近。
- en: 'However, if the UCB acquisition score is similar to the mean prediction, then
    the policy is exploitative, as it will only query data points with high predictive
    means. This phenomenon shows that the more data we have observed, the more explorative
    we should be with UCB. Here, a natural way of gradually encouraging more exploration
    from UCB is to slowly increase the value of the tradeoff parameter β, which is
    what we learn to do in this exercise, following these steps:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果 UCB 收获分数与平均预测相似，则该策略是开发性的，因为它只查询具有高预测平均值的数据点。这种现象表明，我们观察到的数据越多，我们对 UCB
    的探索应该越多。这里，一种渐进的鼓励UCB更多探索的自然方式是慢慢增加权衡参数 β 的值，这是我们在这个练习中学习的，按照以下步骤进行：
- en: Recreate the BayesOpt loop in CH04/02 - Exercise 1.ipynb, which uses the one-dimensional
    Forrester function as the optimization objective.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新创建 CH04/02 - Exercise 1.ipynb 中的BayesOpt循环，将一维Forrester函数用作优化目标。
- en: We aim to slowly increase the value of the tradeoff parameter β by multiplying
    it with a constant at each iteration of the loop. That is, at the end of each
    iteration, we need to update the parameter with `beta` `*=` `multiplier`.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们旨在通过在循环的每个迭代中将其乘以一个常量来逐渐增加权衡参数 β 的值。也就是说，在每次迭代结束时，我们需要使用 `beta *= multiplier`
    更新参数。
- en: Say we want β to start out with a value of 1 and to have a value of 10 at the
    end of the search (the tenth iteration). What is the value that the multiplier
    for β needs to have?
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假设我们希望β的值从1开始，并在搜索结束时（第十次迭代）达到10。乘数β的值是多少？
- en: 'Implement this scheduling logic, and observe the resulting optimization performance:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现这个调度逻辑，并观察所得的优化性能：
- en: Specifically, even though this version of UCB starts out with β = 1, does it
    get stuck at a local optimum like the version for which the parameter is fixed
    at 1?
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特别是，尽管这个版本的UCB从β=1开始，但它是否会像参数固定在1的版本一样陷入局部最优？
- en: '5.4.2 Exercise 2: BayesOpt for hyperparameter tuning'
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.2 练习2：BayesOpt 用于超参数调整
- en: 'This exercise, implemented in CH05/03 - Exercise 2.ipynb, applies BayesOpt
    to an objective function that simulates the accuracy surface of a support-vector
    machine model in a hyperparameter tuning task. The *x*-axis denotes the value
    of the penalty parameter *C*, while the *y*-axis denotes the value for the RBF
    kernel parameter *γ*. See the exercises in chapters 3 and 4 for more detail. Follow
    these steps:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习在 CH05/03 - Exercise 2.ipynb 中实现，将BayesOpt应用于超参数调整任务中支持向量机模型的准确率表面。*x*-轴表示罚项参数*C*的值，*y*-轴表示RBF核参数*γ*的值。有关更多详细信息，请参见第3章和第4章的练习。按照以下步骤进行：
- en: Recreate the BayesOpt loop in CH04/03 - Exercise 2.ipynb, including the outer
    loop that implements repeated experiments.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新创建 CH04/03 - Exercise 2.ipynb 中的BayesOpt循环，包括实施重复实验的外层循环。
- en: 'Run the UCB policy, setting the value of the tradeoff parameter to β ∈ { 1,
    3, 10, 30 }, and observe the values'' aggregated performance:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 UCB 策略，并将权衡参数的值设置为 β ∈ { 1, 3, 10, 30 }，观察结果的总体表现：
- en: Which value leads to over-exploitation, and which leads to over-exploration?
    Which value works best?
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪个值导致了过度开发，哪个导致了过度探索？哪个值效果最好？
- en: 'Run the adaptive version of UCB (see exercise 1):'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 UCB 的自适应版本（参见练习1）：
- en: The tradeoff parameter should start at 3 and end at 10.
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 权衡参数应该从3开始，并在10结束。
- en: Observe that changing the end value from 10 to 30 doesn’t affect optimization
    performance too much. We, therefore, say that this strategy is robust against
    the value of this end value, which is a desideratum.
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意，将结束值从10改为30不会对优化性能产生太大影响。因此，我们认为这种策略对于该结束值的值是健壮的，这是一个期望。
- en: Compare the performance of this adaptive version and that of other versions
    with fixed β.
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较此自适应版本与其他具有固定 β 的版本的性能。
- en: Run the TS policy, and observe its aggregated performance.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 TS 策略，并观察其总体表现。
- en: Summary
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: The MAB problem consists of a set of actions that may be performed (arms of
    slot machines that may be pulled), each of which returns a reward according to
    its specific reward rate. The goal is to maximize the sum of the reward (cumulative
    reward) we receive given a number of iterations.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MAB问题由可以执行的一组操作(可以拉动的老虎机的臂)组成，每个操作根据其特定的奖励率返回奖励。目标是在给定一定数量的迭代之后最大化我们接收到的奖励总和（累积奖励）。
- en: An MAB policy selects which action to take next based on past data. A good policy
    needs to balance under-explored actions and high-performing actions.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MAB策略根据过去的数据选择下一步要采取的行动。好的策略需要在未被探索的行动和高性能行动之间保持平衡。
- en: Unlike the MAB problem, where there are a finite set of sections, there are
    infinitely many actions that we can take in BayesOpt.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与 MAB 问题不同，在 BayesOpt 中我们可以采取无限多的行动，而不是有限个部分。
- en: The goal of BayesOpt is to maximize the largest observed reward, which is often
    called the *simple reward*.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BayesOpt 的目标是最大化观察到的最大回报，这通常被称为*简单回报*。
- en: 'The rewards in BayesOpt are correlated: similar actions yield similar rewards.
    This is not necessarily true with the MAB.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BayesOpt 的回报是相关的：相似的行动会产生相似的回报。这在 MAB 中不一定成立。
- en: The UCB policy uses an optimistic estimate of the quantity of interest to make
    decisions. This *optimism in the face of uncertainty* heuristic can balance exploration
    and exploitation, with a tradeoff parameter that we, the users, can set.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UCB 策略使用对感兴趣数量的乐观估计来做决策。这种*在面对不确定性的乐观主义*启发式方法可以平衡探索和利用，其中的权衡参数由我们用户设定。
- en: The smaller the tradeoff parameter of the UCB policy is, the more exploitative
    the policy becomes, tending to stay within regions known to give high rewards.
    The larger the tradeoff parameter is, the more explorative the policy becomes,
    tending to query regions far away from observed data.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UCB 策略的权衡参数越小，策略就越倾向于停留在已知有高回报的区域，变得更加自利。权衡参数越大，策略就越倾向于查询远离观测数据的区域，变得更加寻求探索。
- en: The TS policy draws a sample from a probabilistic model of the quantity of interest
    and uses this sample to make decisions.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TS 策略从感兴趣的数量的概率模型中抽取样本并使用这个样本来做决策。
- en: 'The random nature of TS allows the policy to explore and exploit the search
    space appropriately: both regions of high uncertainty and those of high predictive
    mean are likely to be chosen by TS.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TS 的随机性质使得策略可以适当地探索和利用搜索空间：TS 有可能选择高不确定性和高预测平均值的区域。
- en: 'For computational reasons, more care is needed when implementing TS: we first
    generate a set of points to cover the search space evenly, and then a sample from
    the GP posterior is drawn for these points.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 出于计算原因，在实现 TS 时需要更加谨慎。我们首先生成一组点以均匀覆盖搜索空间，然后为这些点从 GP 后验中抽取样本。
- en: To cover a space evenly, we can use a Sobol sequence to generate the points
    within the unit cube and scale them to the targeted space.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了均匀地覆盖空间，我们可以使用 Sobol 序列生成单位立方体内的点，并将它们缩放到目标空间。
