- en: Appendix. Solutions to the exercises
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录。 练习的解决方案
- en: 'A.1 Chapter 2: Gaussian processes as distributions over functions'
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '第2章A.1: 高斯过程作为函数分布'
- en: 'In this exercise, we train a GP on a real-world dataset we saw in chapter 1\.
    The solution is included in the CH02/02 - Exercise.ipynb notebook. Complete the
    following steps:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们对我们在第1章看到的真实数据集进行了GP训练。 解决方案包含在CH02/02 - Exercise.ipynb笔记本中。 完成以下步骤：
- en: Create the four-dimensional dataset.
  id: totrans-3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建四维数据集。
- en: 'We first import the necessary libraries: PyTorch for array/tensor manipulation,
    GPyTorch for GP modeling, and Matplotlib for visualization:'
  id: totrans-4
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，我们导入必要的库：PyTorch用于数组/张量操作，GPyTorch用于GP建模，Matplotlib用于可视化：
- en: '[PRE0]'
  id: totrans-5
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We then store the numbers in the table in two PyTorch tensors, `train_x` and
    `train_y`, which, respectively, contain the features and labels of our dataset:'
  id: totrans-6
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，我们将表中的数字存储在两个PyTorch张量中，`train_x` 和 `train_y`，分别包含数据集的特征和标签：
- en: '[PRE1]'
  id: totrans-7
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Normalize the fifth column by subtracting the mean from all values and dividing
    the results by their standard deviation.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过从所有值中减去均值并将结果除以它们的标准差来标准化第五列。
- en: 'We normalize the labels as follows:'
  id: totrans-9
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们按如下方式标准化标签：
- en: '[PRE2]'
  id: totrans-10
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'When printed out, `train_y` should contain the following values: `tensor([-0.4183,`
    `1.4974,` `-0.5583,` `-0.5207])`.'
  id: totrans-11
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 打印出时，`train_y` 应该包含以下值：`tensor([-0.4183,` `1.4974,` `-0.5583,` `-0.5207])`。
- en: Treat the first four columns as features and the fifth as labels. Train a GP
    on this data.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将前四列视为特征，第五列为标签。 在这个数据上训练一个GP。
- en: 'We reimplement our GP model class as follows:'
  id: totrans-13
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们如下重新实现我们的GP模型类：
- en: '[PRE3]'
  id: totrans-14
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We then initialize an object of this class with our training data:'
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，我们用我们的训练数据初始化这个类的对象：
- en: '[PRE4]'
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Create a test dataset containing compositions with zero percent germanium and
    manganese.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含百分之零锗和锰的组合的测试数据集。
- en: 'To assemble our test dataset, we first create a mesh grid for the first and
    second columns that spans over the unit square:'
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要组装我们的测试数据集，我们首先为第一列和第二列创建一个跨越单位正方形的网格：
- en: '[PRE5]'
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'These first two columns are stored in `grid_x1` and `grid_x2`. We then append
    two additional, all-zero columns to `grid_x1` and `grid_x2`, completing the test
    set with four columns:'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这前两列存储在 `grid_x1` 和 `grid_x2` 中。 然后，我们在 `grid_x1` 和 `grid_x2` 中附加两个额外的全零列，完成了具有四列的测试集：
- en: '[PRE6]'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ First column
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❶ 第一列
- en: ❷ Second column
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❷ 第二列
- en: ❸ Third column, containing all zeros
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❸ 第三列，包含全零
- en: ❹ Forth column, containing all zeros
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❹ 第四列，包含全零
- en: Predict the mixing temperature on this test set.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测这个测试集的混合温度。
- en: 'To make predictions on this test set, we simply pass `xs` through our GP model
    under the `torch.no_grad()` context:'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要在这个测试集上进行预测，我们只需在`torch.no_grad()`上下文中通过我们的GP模型传递 `xs`：
- en: '[PRE7]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Visualize the predictions.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化预测。
- en: 'To visualize these predictions, we first create a figure with two panels (that
    is, two Matplotlib subplots):'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要可视化这些预测，我们首先创建一个具有两个面板（即，两个Matplotlib子图）的图：
- en: '[PRE8]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We then use `plt.imshow()` to visualize the mean and standard deviation vectors
    as heat maps, making sure to reshape the two vectors into square matrices:'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，我们使用`plt.imshow()`将均值和标准差向量可视化为热图，确保将这两个向量重塑为方阵：
- en: '[PRE9]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Heat map for the predictive mean
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❶ 预测均值的热图
- en: ❷ Heat map for the predictive standard deviation
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❷ 预测标准差的热图
- en: This will create plots similar to those in figure A.1.
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将创建类似于图A.1中的图。
- en: '![](../../OEBPS/Images/A-01.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/A-01.png)'
- en: Figure A.1 Predictions made by a GP on a 2-dimensional space
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图A.1 GP在二维空间上的预测
- en: Note If you are using a different GP implementation, it’s entirely possible
    to produce heat maps that are slightly different from those in figure A.1\. As
    long as the general trend of the heat maps is the same, your solution is correct.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 如果您使用不同的GP实现，则完全有可能生成与图A.1中略有不同的热图。 只要热图的一般趋势相同，您的解决方案就是正确的。
- en: 'A.2 Chapter 3: Incorporating prior knowledge with the mean and covariance functions'
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '第3章A.2: 使用均值和协方差函数结合先验知识'
- en: 'This exercise provides practice for implementing a GP model with automatic
    relevance determination (ARD). The solution is included in CH03/03 - Exercise.ipynb.
    Complete the following steps:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 该练习提供了使用自动相关性确定（ARD）的GP模型的实践。 解决方案包含在CH03/03 - Exercise.ipynb中。 完成以下步骤：
- en: Implement the two-dimensional function in Python using PyTorch.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用PyTorch在Python中实现二维函数。
- en: 'We first import the necessary libraries—PyTorch for array/tensor manipulation,
    GPyTorch for GP modeling, and Matplotlib for visualization:'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，我们导入必要的库——PyTorch用于数组/张量操作，GPyTorch用于GP建模，Matplotlib用于可视化：
- en: '[PRE10]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We then implement the objective function using the given formula:'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Visualize the function over the domain [`0,` `2`]².
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To visualize the function, we need to create a mesh grid the domain. We store
    this grid in `xs`:'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ One-dimensional grid
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❷ Two-dimensional grid
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We then can obtain the function values over this grid by passing `xs` to `f()`.
    The results are stored in `ys`:'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We use `plt.imshow()` to visualize `ys` as a heat map:'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Randomly draw 100 data points from the domain [`0,` `2`]². This will be used
    as our training data.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To randomly sample 100 points within the domain, we use `torch.rand()` to sample
    from the unit square, and then we multiply the result by 2 to scale it to our
    domain:'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The function values of these points can be obtained by calling `f(train_x)`:'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Implement a GP model with a constant mean function and a Matérn 5/2 kernel
    with an output scale implemented as a `gpytorch.kernels.ScaleKernel` object. We
    implement our GP model as specified:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Set to None to disable ARD and 2 to enable ARD.
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Don’t specify the `ard_num_dims` parameter when initializing the kernel object
    or set the parameter to `None`.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is done in the previous code.
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Train the hyperparameters of the GP model using gradient descent, and inspect
    the length scale after training.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We initialize our GP and train it using gradient descent for 500 iterations
    as follows:'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ Enables the training model
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❷ Gradient descent to optimize the GP’s hyperparameters
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❸ Enables the prediction model
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After these 500 iterations, we inspect the length scale by printing out the
    following value:'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In other words, the optimized length scale is equal to roughly 1.15.
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Redefine the GP model class, this time setting `ard_num_dims` `=` `2`.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Setting `ard_num_dims=2` in the `GPModel` class and rerunning all the code
    cells, we obtain the following values for the length scales:'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Here, the length scale of the first dimension is large (roughly 1.70), while
    the length scale of the second dimension is small (roughly 0.87). This corresponds
    to the fact that the objective function varies more along the second dimension.
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A.3 Chapter 4: Refining the best result with improvement-based policies'
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two exercises in this chapter:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: The first covers a way of refining the Probability of Improvement (PoI) policy,
    allowing it to better explore the search space.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The second applies the two BayesOpt policies we have learned to a simulated
    real-world task of hyperparameter tuning.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A.3.1 Exercise 1: Encouraging exploration with Probability of Improvement'
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This exercise, implemented in the CH04/02 - Exercise 1.ipynb notebook, walks
    us through how to modify PoI to encourage exploration. Complete the following
    steps:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Recreate the BayesOpt loop in the CH04/01 - BayesOpt loop notebook, which uses
    the one-dimensional Forrester function as the optimization objective.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Before the `for` loop that implements BayesOpt, declare a variable named `epsilon`:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Inside the `for` loop, initialize the PoI policy as before, but this time,
    specify that the incumbent threshold, set by the `best_f` argument, is the incumbent
    value *plus* the value stored in `epsilon`:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Rerun the notebook, and observe whether this modification leads to better optimization
    performance than the original PoI policy by encouraging more exploration, as shown
    in figure A.2.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/A-02.png)'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure A.2 Optimization progress of the modified PoI at the last iteration.
    The policy has found the optimum.
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Here, the modified PoI has found the optimum.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How much more explorative PoI becomes heavily depends on the minimum improvement
    threshold stored in `epsilon`. Setting this variable to 0.001 doesn’t sufficient
    encourage exploration, and the policy once again gets stuck. Setting this variable
    to 0.5 works well.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Implement a relative minimum improvement threshold with a 110% improvement
    requirement:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ Omitted
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❷ Relative improvement
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A.3.2 Exercise 2: BayesOpt for hyperparameter tuning'
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This exercise, implemented in CH04/03 - Exercise 2.ipynb, applies BayesOpt
    to an objective function that simulates the accuracy surface of a support-vector
    machine model in a hyperparameter tuning task. Complete the following steps:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Recreate the BayesOpt loop in CH04/01 - BayesOpt loop.ipynb. Our objective function
    is implemented as
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Declare the corresponding test data with `xs` for a two-dimensional grid representing
    the domain and `ys` for the function values of `xs`:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Modify the helper function that visualizes optimization progress. We declare
    this function as `visualize_progress_and_policy()`, which only needs to take in
    a policy object and `next_x` as the next point to query. First, the function computes
    the acquisition scores for the test data `xs`:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: ❶ To be continued
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we declare the two Matplotlib subplots and, for the first, plot the ground
    truth stored in `ys`:'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: ❶ Heat map showing the ground truth
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❷ Scattered points showing labeled data
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, we plot another heat map in the second subplot, showing the acquisition
    scores:'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: ❶ Heat map showing the acquisition scores
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We optionally show `next_x`:'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Copy the GP class from the exercise in chapter 3, which implements a Matérn
    2.5 kernel with ARD. Further modify this class to make it integratable with BoTorch:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: ❶ BoTorch-related modifications
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❷ Matérn 2.5 kernel with ARD
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❸ Omitted
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Reuse the helper function `fit_gp_model()` and the `for` loop that implements
    BayesOpt. We copy `fit_gp_model()` and declare the initial dataset:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We then declare the BayesOpt loop:'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: ❶ Placeholder for policy initialization
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❷ Making the search more exhaustive
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❸ Calling the new visualization helper function
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Run the PoI policy on this objective function. Observe that the policy once
    again gets stuck at a local optimum. Replace the line that initializes the BayesOpt
    policy with
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Running the entire notebook shows that the policy once again gets stuck at a
    local optimum, as shown in figure A.3.
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/A-03.png)'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure A.3 Optimization progress of PoI at the last iteration. The policy is
    stuck at a local optimum.
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Run the modified version of PoI, where the minimum improvement threshold is
    set at 0.1\. Replace the line that initializes the BayesOpt policy with
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: This policy is more explorative and outperforms regular PoI. Figure A.4 shows
    the progress of this policy at iteration 17, where it first achieves an accuracy
    of at least 90%.
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/A-04.png)'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure A.4 Optimization progress of the modified PoI at iteration 17, where
    the policy first achieves an accuracy of at least 90%
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Here, C = 1.6770 and *γ* = 1.9039 are the parameters giving this accuracy.
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Run the Expected Improvement (EI) policy on this objective function. Replace
    the line that initializes the BayesOpt policy with
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This policy performs well on our objective function, finding an accuracy of
    at least 90% at iteration 15, as shown in figure A.5.
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/A-05.png)'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure A.5 Optimization progress of EI at iteration 4, where the policy first
    achieves an accuracy of at least 90%
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Here, *C* = 1.6331 and *γ* = 1.8749 are the parameters giving this accuracy.
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Implement repeated experiments, and visualize the average incumbent values
    and error bars across 10 experiments. We first put the code that implements our
    BayesOpt loop in an outer loop that iterates over multiple experiments. We store
    the best-seen value at each step across the experiments in `incumbents`:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: ❶ Uniformly samples a data point in the search space as the starting point
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❷ Keeps track of the best-seen value
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❸ The mitted code is the same as before.
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❹ Saves results to a file so that we can visualize them later
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We then implement a helper function that plots the average incumbent values
    and error bars. This function reads in a PyTorch tensor saved at `path`, which
    should be the saved version of `incumbents` in the previous step:'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: ❶ Helper subfunction to compute the error bars
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❷ Loads saved optimization results
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❸ Computes the mean results and error bars
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❹ Visualizes the mean results and error bars
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We then can run the preceding policies we have in the previous code and compare
    their performance:'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: This generates figure A.6, which shows the optimization performance of PoI,
    the modified version of PoI, and EI.
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/A-06.png)'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure A.6 Optimization progress of various policies, aggregated across 10 repeated
    experiments
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We see that figure A.6 gives us more insight than inspecting the policies in
    a single run. Here, not only does PoI perform worse than the other two policies,
    but its performance is also less robust, as seen from the large error bars. The
    modified PoI and EI perform comparably, and it’s hard to tell if one is better
    than the other, as their error bars overlap.
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A.4 Chapter 5: Exploring the search space with bandit-style policies'
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two exercises in this chapter:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: The first exercise explores a potential method to set the tradeoff parameter
    for the UCB policy that considers how far along we are in optimization.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The second exercise applies the two policies we have learned in this chapter
    to the hyperparameter tuning problem seen in previous chapters.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A.4.1 Exercise 1: Setting an exploration schedule for Upper Confidence Bound'
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This exercise, implemented in CH05/02 - Exercise 1.ipynb, discusses a strategy
    of adaptively setting the value of the tradeoff parameter β of the UCB policy.
    Complete the following steps:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Recreate the BayesOpt loop in CH04/02 - Exercise 1.ipynb, which uses the one-dimensional
    Forrester function as the optimization objective.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Since there are 10 iterations in the BayesOpt loop, β is multiplied by the
    multiplier *m* 10 times to go from 1 to 10\. That is, 1 × *m*10 = 10\. Solving
    this equation gives the code for the multiplier:'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Implement this scheduling logic, and observe the resulting optimization performance.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We modify the BayesOpt loop as follows:'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: ❶ Obtains the trained GP
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❷ Finds the point that maximizes the acquisition score, queries the objective
    function, and updates the training data
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This code produces figure A.7.
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/A-07.png)'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure A.7 Progress made by the adaptive version of the UCB policy. The policy
    is able to escape the local optimum and get closer to the global optimum.
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We see that the policy inspects a local optimum at the fifth iteration but ultimately
    is able to escape and get closer to the global optimum at the end.
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A.4.2 Exercise 2: BayesOpt for hyperparameter tuning'
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This exercise, implemented in CH05/03 - Exercise 2.ipynb, applies BayesOpt
    to an objective function that simulates the accuracy surface of a support-vector
    machine model in a hyperparameter tuning task. Complete the following steps:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Recreate the BayesOpt loop in CH04/03 - Exercise 2.ipynb, including the outer
    loop that implements repeated experiments.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the UCB policy, setting the value of the tradeoff parameter to β ∈ { 1,
    3, 10, 30 }, and observe the values’ aggregated performance.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The value of the tradeoff parameter can be set when the policy object is initialized:'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Figure A.8 shows the optimization performance of the four versions of UCB. We
    see that when β *=* 1, the policy is too exploitative and achieves the worst performance.
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/A-08.png)'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure A.8 Progress made by the various UCB policies
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As the value of the tradeoff parameter increases, performance increases, but
    when β = 30, over-exploration causes UCB to be slower at locating an accuracy
    of 90%. Overall, β = 10 achieves the best performance.
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Run the adaptive version of UCB (see Exercise 1).
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We modify the BayesOpt loop as follows:'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: ❶ Randomly generates the initial training data
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❷ Records the incumbent value and retrains the model
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❸ Finds the point maximizing the acquisition score, queries the objective function,
    and updates the training data
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure A.9 shows the optimization performance of the two adaptive versions against
    the best performing fixed value β = 10.
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/A-09.png)'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure A.9 Progress made by two adaptive versions of the UCB policy. The policy
    is robust against the end value of the tradeoff parameter.
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: These versions are comparable, and changing the end value from 10 to 30 doesn’t
    affect optimization performance much.
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Run the Thompson sampling (TS) policy, and observe its aggregated performance.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We implement TS as follows:'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: ❶ Randomly generates the initial training data
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❷ Records the incumbent value and retrains the model
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❸ Finds the point maximizing the acquisition score, queries the objective function,
    and updates the training data
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure A.10 shows the optimization performance of TS. We see that the policy
    makes significant progress at the beginning and is comparable to EI from chapter
    6 and slightly worse than the best version of UCB.
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/A-10.png)'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure A.10 Progress made by TS. The policy is comparable to EI and slightly
    worse than the best version of UCB.
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A.5 Chapter 6: Using information theory with entropy-based policies'
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two exercises in this chapter:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: The first exercise covers a variant of binary search in which prior information
    can be taken into account when making decisions.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The second walks us through the process of implementing Max-value Entropy Search
    (MES) in the hyperparameter tuning problem seen in previous chapters.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A.5.1 Exercise 1: Incorporating prior knowledge into entropy search'
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This exercise, implemented in CH06/02 - Exercise 1.ipynb, shows us an instance
    of using different priors when finding the information-theoretically optimal decision
    and will ultimately help us further appreciate the elegance and flexibility of
    entropy search as a generic decision-making-under-uncertainty procedure:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Prove that *Pr*(*X* = 1) + *Pr*(*X* = 2) + ... + *Pr*(*X* = 10) = 1.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can do this by simply adding the probabilities together:'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1 / 2 + 1 / 4 + ... + 1 / 2⁹ + 1 / 2⁹ = 1 / 2 + 1 / 4 + ... + 1 / 2⁸ + 1 / 2⁸
    = ... = 1 / 2 + 1 / 2 = 1.
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Calculate the entropy of this prior distribution.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Remember the formula for the entropy is –Σ*[i]**p[i]* log *p[i]*. We can write
    a Python function that computes this sum:'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: ❶ Get the current probability.
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❷ Sum over the terms.
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This function takes `first` and `last` as parameters, which correspond to the
    smallest and biggest numbers *X* could be (which start out as 1 and 10), respectively.
    We then iterate through the numbers between `first` and `last` and add up the
    –*p[i]* log *p[i]* terms. Here, `marginal_probability()` is a helper function
    that computes *Pr*(*X* = *n*), which we implement as
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: ❶ An edge case when the floor is the highest possible floor
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Running `compute_entropy(1,` `10)` gives us 1.99609375\. This is the entropy
    of the prior distribution for *X*.
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Given the prior distribution defined between 1 and 10, what is the probability
    the phone will break when dropped from the second floor? What is this probability
    for the fifth floor? How about the first floor?
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The probability that the phone will break when dropped from the second floor
    is exactly *Pr*(*X* = 1), which is 0.5.
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The probability that the phone will break from the fifth floor is the probability
    that *X* ≤ 4, which is *Pr*(*X* = 1) + *Pr*(*X* = 2) + *Pr*(*X* = 3) + *Pr*(*X*
    = 4) = 15/16 = 0.9375.
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'These two calculations could be implemented as a function:'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: ❶ Sum over the probabilities for X less than the threshold.
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Since our prior knowledge dictates that the phone won’t break if dropped from
    the first floor, this probability is 0.
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Compute the entropy of the fictitious posterior distribution in the two cases
    where we conduct a trial on the fifth floor.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Using the `compute_entropy()` function we implemented, we can compute the entropy
    in two cases. If the phone breaks, we set `first` `=` `1` and `last` `=` `4`;
    otherwise, we set `first` `=` `5` and `last` `=` `10`:'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Given the prior distribution, compute the expected posterior entropy after you
    conduct a trial on the fifth floor.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We have already done the necessary calculations for this expected posterior
    entropy computation. First, the probability that the phone will break from the
    fifth floor is 0.9375, in which case the posterior entropy is 1.75\. Second, the
    probability that the phone won’t break from the fifth floor is 1 – 0.9375 = 0.0625,
    in which case the posterior entropy is 1.9375.
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Taking the average of the two cases gives (0.9375) 1.75 + (0.0625) 1.9375 =
    1.76171875\. This is the expected posterior entropy after you conduct a trial
    on the fifth floor.
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Compute this expected posterior entropy for other floors.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can implement a function that does the calculation we just went over:'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: ❶ The probability that the phone will break from a given floor
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❷ Takes the average of the two cases
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Using this function, we can plot out the expected posterior entropy for numbers
    between 1 and 10.
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This plot is shown in figure A.11, which tells us that the information-theoretically
    optimal location for our first trial is the second floor, since 2 gives us the
    lowest expected posterior entropy (and, therefore, uncertainty).
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/A-11.png)'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure A.11 The expected posterior entropy as a function of the location to
    conduct the trial
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We see that this is not the same as the decision binary search suggests, 5\.
    This is a direct effect of the domain knowledge we are encoding using the prior
    distribution for *X*: since there’s a high chance that *X* = 2 (50%), it’s actually
    better to simply try out that number first in case we can immediately find our
    answer if the phone breaks.'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Interestingly, dropping the phone from the first floor gives us no reduction
    in entropy. This is because we know for sure that the phone won’t break from this
    floor, so our knowledge of the world won’t change after conducting this trial.
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A.5.2 Exercise 2: BayesOpt for hyperparameter tuning'
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This exercise, implemented in the CH06/03 - Exercise 2.ipynb notebook, applies
    BayesOpt to an objective function that simulates the accuracy surface of a support-vector
    machine model in a hyperparameter tuning task:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Recreate the BayesOpt loop in CH04/03 - Exercise 2.ipynb, including the outer
    loop that implements repeated experiments.
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the MES policy.
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Since our objective function is two-dimensional, we should set the size of
    the Sobol sequence used by MES as 2,000:'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: ❶ Randomly generates the initial training data
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❷ Records the incumbent value and retrains the model
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❸ Finds the point maximizing the acquisition score, queries the objective function,
    and updates the training data
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure A.12 shows the optimization performance of MES. We see that the policy
    is competitive against all the BayesOpt policies we have learned thus far.
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/A-12.png)'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure A.12 Progress made by MES. The policy performs the best of the four policies
    shown.
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A.6 Chapter 7: Maximizing throughput with batch optimization'
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two exercises in this chapter:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: The first covers the implementation of TS under the batch setting.
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The second shows us how to run BayesOpt policies on a four-dimensional aerostructural
    optimization problem.
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A.6.1 Exercise 1: Extending TS to the batch setting via resampling'
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Remember that TS in the sequential setting, which we learned in section 5.3,
    draws one sample from the current GP belief about the objective function and queries
    the data point that maximizes that sample. In the batch setting, we simply repeat
    this process of sampling from the GP and maximizing the sample multiple times
    to assemble a batch of queries of the desired size. The code for this exercise
    can be found in the CH07/02 - Exercise 1.ipynb notebook:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Recreate the batch BayesOpt loop in CH05/01 - BayesOpt loop.ipynb notebook.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement TS with a Sobol sampler, as described in section 5.3.
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We implement the policy as follows, where we use a 2,000-element Sobol sequence
    and specify the number of samples as the batch size:'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: ❶ Specifies the length of the Sobol sequence
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❷ Randomly picks the initial training data
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❸ Retrains the GP
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❹ Initializes the Sobol sequence
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❺ Draws multiple samples from the GP
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❻ Queries the objective function and updates the training data
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Run this TS policy on the hyperparameter tuning objective function, and observe
    its performance.
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After running batch TS, we can plot the progress made by the policy against
    other policies we have learned, as shown in figure A.13\. Here, TS is able to
    make significant progress after only the first batch of queries.
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/A-13.png)'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure A.13 Progress made by batch TS in the hyperparameter tuning example.
    The policy makes significant progress after only the first batch of queries.
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A.6.2 Exercise 2: Optimizing airplane designs'
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This exercise provides an objective function that simulates this process of
    benchmarking the performance of an airplane design. The code is provided in the
    CH07/04 - Exercise 2.ipynb notebook. Complete the following steps:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: Implement the objective function that simulates the performance benchmarking.
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The code for the objective function is already provided, so we simply copy
    and paste it in our program:'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Implement a GP model with a constant mean function and a Matérn 2.5 kernel with
    an output scale implemented as a `gpytorch.kernels.ScaleKernel` object.
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The class implementation of this GP model is mostly the same as before, except
    that we need to specify the correct number of dimensions in the ARD kernel:'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: ❶ A Matérn 2.5 kernel for four dimensions
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Implement a helper function that trains the GP on a given training dataset.
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can simply copy the same helper function `fit_gp_model()` from other notebooks
    in this chapter, namely 02 - Exercise 1.ipynb, as we don’t need to modify anything
    in this helper function.
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Define the settings of our optimization problem.
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We first define the bounds of our search space:'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We then specify how many queries can be made, the batch size, and the number
    of experiments to repeat for each policy:'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Run each batch BayesOpt policy we learn in this chapter on the objective function
    implemented previously.
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We first use this code to implement the optimization loop and the outer loop
    that repeats the experiments for each policy. Specifically, for each individual
    experiment, we randomly sample one data point inside the search space and then
    run each BayesOpt policy until we run out of queries. We see how each policy is
    defined in the next step:'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: ❶ Randomly initializes the training data
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❷ Keeps track of optimization progress and updates the predictive model
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❸ Defines the policy and finds the next batch to query
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❹ Queries the points recommended by the policy and updates the training data
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For the PoI policy, we use the following code:'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'For the EI policy, we use the following code:'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'For the UCB policy, we use the following code:'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'These three policies can then be optimized using the helper function `optimize_
    acqf()` as follows:'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Otherwise, for either TS or MES, we first need to define the Sobol sequence:'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: ❶ Specifies the number of dimensions to be 4
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For the TS policy, we use the following code:'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'For the MES, we use the following code, which uses the helper function `optimize_acqf_cyclic()`
    to implement cyclic optimization. Note that we are specifying that cyclic optimization
    should only take a maximum of 5 iterations:'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: ❶ Specifies the maximum number of iterations in cyclic optimization
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Plot the optimization progress of the BayesOpt policies we have run and observe
    their performance.
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure A.14 shows the optimization results obtained by the policies we’ve implemented.
    We see that most policies are comparable, except for TS; batch PoI has a slight
    edge.
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/A-14.png)'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure A.14 Progress made by various BayesOpt policies in the airplane design
    optimization example. Most policies are comparable, except for TS.
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A.7 Chapter 8: Satisfying extra constraints with constrained optimization'
  id: totrans-328
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two exercises in this chapter:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: The first verifies that the result we obtain from BoTorch’s implementation of
    the constrained EI policy is the same as the product of the regular EI score and
    the probability of feasibility.
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The second shows us how to run constrained BayesOpt on a four-dimensional aerostructural
    optimization problem.
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A.7.1 Exercise 1: Manual computation of constrained EI'
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The acquisition score of the constrained EI policy is the product of the EI
    score and the probability of feasibility. Although the `ConstrainedExpectedImprovement`
    class from BoTorch provides an implementation of the constrained EI score, we
    can, in fact, perform the computation manually. In this exercise, we explore this
    manual computation and verify our result against that of the `ConstrainedExpectedImprovement`
    class. The solution of this exercise is in the CH08/02 - Exercise 1.ipynb notebook
    amd can be explained as follows:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: Recreate the constrained BayesOpt problem used in CH08/01 - Constrained optimization.ipynb,
    including the objective function, the cost function, the GP implementation, and
    the helper function that trains a GP on some training data `fit_gp_model()`.
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a PyTorch tensor that is a dense grid between -5 and 5\. This tensor
    will act as our test set. We use `torch.linspace()` to create a dense grid:'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Create a toy training dataset by randomly sampling three data points from our
    search space (between –5 and 5), and evaluate the objective and cost functions
    at these points. We use `torch.rand()` to randomly sample between 0 and 1 and
    scale the samples to our search space:'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: ❶ Fixes the seed for reproducibility
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❷ Samples between 0 and 1 and then scales the samples to our search space
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Train a GP on the data from the objective function and another GP on the data
    from the cost function using the helper function `fit_gp_model()`.
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: ❶ Trains a GP on the objective function’s data
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❷ Trains a GP on the cost function’s data
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Use the GP trained on the data from the cost function to compute the probability
    of feasibility for each point in the test set.
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We first compute the predictive distribution of the cost GP on our test set:'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'We then initialize a normal distribution object, with the mean and standard
    deviation corresponding to the means and standard deviations of `cost_pred_dist`:'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Finally, we call the `cdf()` method on this object to compute the probability
    of feasibility. The argument this method takes is the upper bound of our cost
    constraint, which is 0:'
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Initialize a regular EI policy, with the `model` argument being the GP trained
    on the data from the objective function and the `best_f` argument being the current
    feasible incumbent.
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We compute the current feasible incumbent with `train_utility[train_cost` `<=`
    `0].max()`:'
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'We then compute the EI score by calling the EI policy object on `xs[:,` `None,`
    `None]`, which is the test dense grid reshaped to make sure it’s of appropriate
    shape:'
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Initialize a constrained EI policy, and compute the constrained EI score for
    each point in the test set:'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'We also compute the constrained EI score with the reshaped test set:'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Compute the product of the EI scores and the probabilities of feasibility,
    and verify that this manual computation leads to the same results as those from
    BoTorch’s implementation. Run an assertion to make sure all corresponding terms
    match up:'
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Plot the EI scores and the constrained EI scores in a graph, and visually verify
    that the former is always greater than or equal to the latter. Prove this is the
    case.
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We plot out the scores we have computed thus far as follows:'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: This code generates figure A.15, which shows that the EI score is, indeed, always
    at least the constrained EI score.
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/A-15.png)'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure A.15 The acquisition score of EI (solid line) and constrained EI (dashed
    line). The former is always greater than or equal to the latter.
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We can mathematically prove this by noting that the constrained EI score is
    equal to the regular EI score multiplied by the probability of feasibility. This
    probability of feasibility is always a maximum of 1, so the EI score is always
    greater than or equal to the constrained EI score.
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A.7.2 Exercise 2: Constrained optimization of airplane design'
  id: totrans-370
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we tackle a constrained optimization problem using the airplane-utility
    objective function in exercise 2 of chapter 7\. This process allows us to run
    constrained BayesOpt on a higher-dimensional problem in which it’s not obvious
    where the feasibility optimal solution is. The solution to this exercise is included
    in the CH08/03 - Exercise 2.ipynb notebook:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: Recreate the BayesOpt problem used in the CH07/04 - Exercise 2.ipynb notebook,
    including the airplane-utility objective function named `flight_utility()`, the
    bounds of our search space (the four-dimensional unit hypercube), the GP implementation,
    and the helper function that trains a GP on some training data `fit_gp_model()`.
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Implement the following cost function, which simulates the cost of making the
    airplane design specified by a four-dimensional input:'
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Our goal is to maximize the objective function `flight_utility()`, while following
    the constraint that the cost, as computed by `flight_cost()`, is less than or
    equal to 0.
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To this end, we set the number of queries a BayesOpt policy can make in each
    experiment as 50, and designate that each policy needs to run 10 repeated experiments:'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: The default value quantifying optimization progress if no feasible solution
    is found should be set to –2.
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Run the constrained EI policy as well as the regular EI policy on this problem;
    visualize and compare their average progress (along with error bars).
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We implement the constrained EI policy in the same way as we did in chapter
    8, where we set `best_f` to be either the current feasible incumbent if a feasible
    solution has been found or the default value –2 otherwise. Our model list contains
    the objective GP, which has an index of 0, and the cost GP, which has an index
    of 1:'
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE78]'
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: ❶ Finds the appropriate value of the current incumbent
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❷ The list of GP models
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❸ Index of the objective model
  id: totrans-385
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❹ Index of the constraint model and the lower and upper bounds
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We implement the regular EI policy as follows:'
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: Figure A.16 shows the optimization results obtained by the two preceding policies
    we implemented. We see that constrained EI completely dominates the regular EI
    policy by accounting for the cost constraint we impose.
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/A-16.png)'
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure A.16 Progress made by various Bayesian optimization policies in the constrained
    airplane design optimization example. Compared to the regular EI, the constrained
    variant finds a more feasible solution on average.
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A.8 Chapter 9: Balancing utility and cost with multifidelity optimization'
  id: totrans-392
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two exercises in this chapter:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 1 walks through the process of measuring and visualizing the average
    performance of an optimization policy across multiple experiments.
  id: totrans-394
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Exercise 2 applies the optimization policies we know to a two-dimensional problem
    with three functions we can query from.
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A.8.1 Exercise 1: Visualizing average performance in multifidelity optimization'
  id: totrans-396
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we run the optimization loop multiple times and learn how
    to take the average performance to obtain a more holistic comparison:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: Copy the problem setup and the multifidelity optimization loop from the CH09/03
    - Measuring performance.ipynb notebook, and add another variable denoting the
    number of experiments we want to run (10, by default).
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To facilitate repeated experiments, add an outer loop to the optimization loop
    code. This should be a `for` loop with 10 iterations, where a different random
    observation is generated each time:'
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: ❶ Repeats the experiment 10 times
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❷ Generates a random initial training set specific to the current iteration
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❸ The inner loop that runs optimization until we exhaust our budget
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Make each of the variables `recommendations` and `spent_budget` a list of lists,
    where each inner-list keeps track of optimization performance of an individual
    experiment. We add to the code for the nested loop in the previous step as follows:'
  id: totrans-404
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  id: totrans-405
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: ❶ Each variable is a (currently empty) list of lists.
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❷ Appends an empty list to each list of lists for the next experiment
  id: totrans-407
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❸ Adds the optimization progress statistics to the newest list in each variable
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Run the multifidelity MES policy and its single-fidelity version on our optimization
    problem.
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We first make the regular grid and the currently empty interpolated recommended
    values that we will fill in later:'
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'We then iterate through each list in `recommendations` (renamed to `incumbents`
    in our code) and `spend_budget`, compute the linear interpolation, and then fill
    in the values in `interp_incumbents`:'
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE83]'
  id: totrans-413
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: Use the linearly interpolated values to plot the average performance and error
    bars of the two policies we ran and compare their performance. The comparison
    is visualized in figure A.17, where we see that the multifidelity MES policy greatly
    outperforms its single-fidelity competitor.
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/A-17.png)'
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure A.17 Average optimization progress of the single- and multifidelity MES
    policies on the Forrester function across 10 experiments. The multifidelity policy
    greatly outperforms the single-fidelity one.
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Plot the linearly interpolated curves representing individual runs’ optimization
    progress, along with the average performance and error bars. The comparison is
    visualized in figure A.18\. Indeed, our optimization progress in each run, as
    measured by the maximum posterior mean recommendation, is not monotonically increasing.
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/A-18.png)'
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure A.18 Linearly interpolated curves representing individual runs' optimization
    progress across 10 experiments. Our optimization progress in each run, as measured
    by the maximum posterior mean recommendation, is not monotonically increasing.
  id: totrans-419
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A.8.2 Exercise 2: Multifidelity optimization with multiple low-fidelity approximations'
  id: totrans-420
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This exercise shows us that our multifidelity Max-value Entropy Search policy
    can balance between multiple low-fidelity functions. The solution, found in the
    CH09/05 - Exercise 2.ipynb notebook, can be explained as follows:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: Implement the objective function. The code for this step has already been provided
    in the instructions.
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define the bounds of our search space as the unit square:'
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  id: totrans-424
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Declare the `fidelities` variable that stores the correlation values of the
    different functions we can query:'
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Set the fixed cost of the linear cost model to 0.2 and the weight to 1:'
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Set the limit of our budget in each experiment to 10 and the number of repeated
    experiments to 10 as well:'
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE87]'
  id: totrans-430
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Set the number of candidates drawn from the Sobol sequence to 5,000, and use
    100 restarts and 500 raw samples when using helper functions to optimize the acquisition
    score of a given policy:'
  id: totrans-431
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  id: totrans-432
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Redefine the helper function `get_final_recommendation` that finds the posterior
    mean maximizer:'
  id: totrans-433
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  id: totrans-434
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: ❶ The necessary changes
  id: totrans-435
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Run the multifidelity MES policy and its single-fidelity version on our optimization
    problem, and plot the average optimization progress and error bars of each policy,
    using the method described in exercise 1\. Figure A.19 shows the comparison between
    the two policies.
  id: totrans-436
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/A-19.png)'
  id: totrans-437
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure A.19 Average optimization progress of the single- and multifidelity MES
    policy on the Branin function across 10 experiments. The multifidelity policy,
    once again, outperforms the single-fidelity one.
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A.9 Chapter 11: Optimizing multiple objectives at the same time'
  id: totrans-439
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we apply the multiobjective optimization techniques we have
    learned to the problem of optimizing the aerostructural design of an airplane.
    This exercise allows us to observe the performance of the Expected Hypervolume
    Improvement (EHVI) policy in a multidimensional problem:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: 'We copy the code for the objective functions as follows:'
  id: totrans-441
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  id: totrans-442
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: ❶ The first objective function
  id: totrans-443
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❷ The second objective function, negated from the code in exercise 2 of chapter
    8
  id: totrans-444
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We implement the helper function as follows:'
  id: totrans-445
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  id: totrans-446
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'We declare the bounds of the search space:'
  id: totrans-447
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  id: totrans-448
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'We declare the reference point:'
  id: totrans-449
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: The class implementation and helper function can be implemented using the same
    code as, for example, in CH08/03 - Exercise 2.ipynb.
  id: totrans-451
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We set the experimental settings:'
  id: totrans-452
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: We implement the two BayesOpt policies in the same way as in CH11/02 - Multi-objective
    BayesOpt loop.ipynb. Figure A.20 shows the performance of the two policies aggregated
    across 10 experiments. EHVI, once again, outperforms the alternating EI policy.
  id: totrans-454
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/A-20.png)'
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure A.20 Average hypervolume and error bars as a function of the number of
    queries made by two Bayesian optimization policies. EHVI consistently outperforms
    the alternating EI policy.
  id: totrans-456
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A.10 Chapter 12: Scaling Gaussian processes to large data sets'
  id: totrans-457
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This exercise demonstrates the improvement in efficiency when going from a regular
    GP model to a VGP one on a real-life dataset of housing prices in California.
    Our goal is to observe the computational benefits of a VGP—this time, in a real-world
    setting.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: 'Complete the following steps:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the Pandas library to read in the dataset:'
  id: totrans-460
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  id: totrans-461
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: Once read in, the Pandas dataframe should look similar to the output in figure
    A.21.
  id: totrans-462
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/A-21.png)'
  id: totrans-463
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure A.21 The housing price dataset shown as a Pandas dataframe. This is the
    training set for this exercise.
  id: totrans-464
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We create the scatter plot as follows:'
  id: totrans-465
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  id: totrans-466
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: The visualization should look similar to figure A.22.
  id: totrans-467
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/A-22.png)'
  id: totrans-468
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure A.22 The housing price dataset shown as a scatter
  id: totrans-469
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To extract our training features, we use the `torch.from_numpy()` method to
    convert a NumPy array to a PyTorch tensor:'
  id: totrans-470
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  id: totrans-471
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'We similarly do this for the log of the house prices, which are our training
    labels:'
  id: totrans-472
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  id: totrans-473
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'We normalize the training labels `train_y` as follows:'
  id: totrans-474
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  id: totrans-475
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'We implement the GP model as follows:'
  id: totrans-476
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  id: totrans-477
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: ❶ The constant mean function
  id: totrans-478
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❷ An ARD Matern 5/2 kernel with an output scale
  id: totrans-479
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Make a likelihood whose noise is constrained to be at least 0.1, using the
    following code:'
  id: totrans-480
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  id: totrans-481
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: ❶ The constraint forces the noise to be at least 0.1.
  id: totrans-482
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We train the previously implemented GP model using gradient descent as follows:'
  id: totrans-483
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  id: totrans-484
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: ❶ The gradient descent optimizer Adam
  id: totrans-485
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❷ The (negative) marginal log likelihood loss function
  id: totrans-486
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❸ Enable training mode
  id: totrans-487
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The total training time was 24 seconds on a MacBook.
  id: totrans-488
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We implement the VGP model as follows:'
  id: totrans-489
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  id: totrans-490
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: ❶ Variational parameters
  id: totrans-491
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❷ The same as the GP
  id: totrans-492
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This VGP is trained as follows:'
  id: totrans-493
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  id: totrans-494
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: ❶ Randomly picks 100 points as the initial inducing points
  id: totrans-495
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❷ Prepares the mini batches
  id: totrans-496
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❸ Natural gradient descent for variational parameters
  id: totrans-497
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ❹ Adam for the other parameters
  id: totrans-498
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: On the same MacBook, training took 6 seconds—a 400% improvement in speed.
  id: totrans-499
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The solution is included in the CH12/02 - Exercise.ipynb notebook.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
