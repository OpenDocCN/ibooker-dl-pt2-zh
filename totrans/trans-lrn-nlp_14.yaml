- en: 11 Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing important concepts covered by this book
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summarizing related important emerging concepts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Considering limitations and environmental and ethical considerations around
    transfer learning methods for NLP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Envisioning the future of transfer learning in NLP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keeping up with latest developments in the field
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ve covered a great deal of material in the preceding chapters—we hope they
    were informative and engaging. This concluding chapter attempts to provide a meaningful
    summary of everything we did and looks to the future of the field and emerging
    research trends. Due to the prolific output and the quick-moving nature of the
    field, we certainly have not covered every single influential architecture or
    promising research direction. To mitigate this, we include a brief discussion
    of various research trends we did not get a chance to cover in this book, making
    connections to, and framing in the context of, covered material as much as possible.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we also try to provide a broader context by touching on emerging
    questions that have not traditionally been given much attention, such as ethical
    considerations and the environmental impact of the various models. These are closely
    tied to the awareness of the limitations of these models, which we try to highlight
    as much as possible in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Crucially, we discuss various tips for staying up-to-date in a rapidly moving
    field such as this one. It is important to stress that having mastered the content
    of the book, you are only now beginning your journey in the field. The tools and
    skills presented will change over time, and each unique application of them may
    require creativity on your part or new techniques yet to be developed. Retaining
    a competitive edge in a rapidly moving field such as this is truly a journey,
    not a destination. We encourage readers to keep an inquisitive attitude toward
    ongoing research and to continue to contribute in some capacity to its development.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin this final chapter by overviewing the key concepts.
  prefs: []
  type: TYPE_NORMAL
- en: 11.1 Overview of key concepts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Transfer learning aims to leverage prior knowledge from different settings—be
    it a different task, language, or domain—to help solve a problem at hand. It is
    inspired by the way in which humans learn, because we typically do not learn things
    from scratch for any given problem, but rather build on prior knowledge that may
    be related. Allowing a practitioner without substantial computing resources to
    achieve state-of-the-art performance is considered an important step toward democratizing
    access to the fruits of the ongoing technological revolution. As a more concrete
    motivation, consider the representative costs of training various sizes of the
    BERT model, as shown in figure 11.1.[¹](#pgfId-1247306)
  prefs: []
  type: TYPE_NORMAL
- en: '![11_01](../Images/11_01.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 Training costs for various sizes of BERT. Two representative costs
    are shown—for a single run and for an entire training process that includes hyperparameter
    tuning. The largest size of 1.5 billion parameters costs $80k for single run,
    and $1.6 million when all optimization steps are accounted for!
  prefs: []
  type: TYPE_NORMAL
- en: As can be seen in the figure, the largest-sized BERT training cost can reach
    into millions of dollars. Transfer learning literally enables you to reuse this
    precious knowledge on your personal computing projects within a few hours and,
    at worst, a few dollars for fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning was popularized in computer vision before it recently came
    to be used heavily by the natural language processing (NLP) community. Whereas
    computer vision deals with teaching computers how to understand and act on images
    and videos, NLP considers how to do the same with human speech, be it text or
    speech audio. In this book, we focused on text. Some NLP tasks of particular interest
    to us include document classification, machine translation, and question answering.
  prefs: []
  type: TYPE_NORMAL
- en: Although historically such tasks were initially solved by attempting to specify
    fixed rules for every scenario—a paradigm now known as symbolic AI—machine learning
    has now become the dominant trend. Instead of explicitly programming a computer
    for every possible scenario, the computer is *trained* to associate input to output
    signals by seeing many examples of such corresponding input-output pairs. The
    methods used to *learn* appropriate input-output associations have traditionally
    included decision trees, random forests, kernel methods such as SVM, and neural
    networks. Neural networks have recently become the clear favorite for learning
    such representations for perceptual problems, that is, computer vision and NLP.
    As such, this is the most important class of methods we explored in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before diving into modern transfer learning method for NLP, we conducted a
    review experiment with some traditional machine learning methods. Specifically,
    we employed the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support vector machines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient-boosting machines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'to look at two important problems: email spam detection and Internet Movie
    Database (IMDB) movie review classification. To turn text into numbers for this
    experiment, we used the bag-of-words model. This model simply counts the frequency
    of word tokens contained in each email and thereby represents it as a vector of
    such frequency counts.'
  prefs: []
  type: TYPE_NORMAL
- en: Modern NLP methodologies have focused on vectorizing sections of text—words,
    subwords, sentences, and so on—via techniques such as word2vec and sent2vec. The
    resulting numerical vectors are then further processed as features of a traditional
    machine learning approach, such as being used for classification with random forests.
  prefs: []
  type: TYPE_NORMAL
- en: 'As was outlined in the first chapter of this book, this important subarea of
    NLP research has a rich history originating with the *term-vector model of information
    retrieval* in the 1960s. This culminated with pretrained shallow neural-network-based
    techniques such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: fastText
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GloVe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: word2vec, which came in several variants in the mid-2010s including Continuous
    Bag of Words (CBOW) and Skip-Gram
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both CBOW and Skip-Gram are extracted from shallow neural networks that were
    trained for various goals. Skip-Gram attempts to predict words neighboring any
    target word in a sliding window, whereas CBOW attempts to predict the target word
    given the neighbors. GloVe, which stands for “Global Vectors,” attempts to extend
    word2vec by incorporating global information into the embeddings. It optimizes
    the embeddings such that the cosine product between words reflects the number
    of times they co-occur, with the goal of making the resulting vectors more interpretable.
    The fastText technique attempts to enhance word2vec by repeating the Skip-Gram
    methods on character n-grams (versus word n-grams), thereby able to handle previously
    unseen words. Each of these variants of pretrained embeddings has its strengths
    and weaknesses. As a numerical demonstration of this class of methods, we used
    fastText word embeddings to revisit the IMDB movie classification example, where
    the bag-of-words method was replaced with fastText for converting the text into
    numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Several techniques were inspired by word2vec to try to embed larger section
    of text into vector spaces in such a way that sections of text with similar meanings
    would be closer to each other in the induced vector space. This enables arithmetic
    to be performed on these sections of text to make inference with regards to analogies,
    combined meanings, and so forth. Such methods include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Paragraph vectors, or *doc2vec*, which exploits the concatenation (versus averaging)
    of words from pretrained word embeddings in summarizing them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Sent2vec*, which extends the classic Continuous Bag of Words (CBOW) of word2vec—where
    a shallow network is trained to predict a word in a sliding window from its context—to
    sentences by optimizing word and word n-gram embeddings for an accurate averaged
    representation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a numerical demonstration of this class of methods, we used an implementation
    of sent2vec that builds on fastText, instead of bag-of-words, to perform the IMDB
    movie classification experiment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Several authors[²](#pgfId-1247340), [³](#pgfId-1247343), [⁴](#pgfId-1247346)
    have suggested various classification systems for categorizing transfer learning
    methods into groups. Roughly speaking, categorization is based on whether transfer
    occurs among different languages, tasks, or data domains. Each of these types
    of categorization is usually correspondingly referred to as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Cross-lingual learning*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Multitask learning*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Domain adaptation*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We performed a multitask transfer learning experiments using the familiar tasks
    of IMDB classification and email spam detection to illustrate the concept. To
    illustrate domain adaptation via example, we used an autoencoder to adapt a model
    trained for IMDB movie review classification to the domain of Amazon book reviews.
    This exercise also allowed us to illustrate an instance of zero-shot transfer
    learning, where no fine-tuning in the Amazon book review domain was necessary
    to begin providing valuable results.
  prefs: []
  type: TYPE_NORMAL
- en: Advances in sequence-to-sequence modeling brought forth a revolution for tasks
    such as machine translation. The encoder and decoder in this setup were initially
    recurrent neural networks (RNNs). Due to problems with long input sequences, the
    technique known as attention was developed to allow output to focus only on relevant
    sections of the input. Although initially this was combined with RNNs, it evolved
    into the technique of self-attention being used to build out both the encoder
    and decoder. Self-attention differs from the original attention formulation in
    that associations are sought between parts of a sequence and other parts of the
    same sequence, rather than between parts of two distinct input and output sequences.
    The architecture in which self-attention replaced attention came to be known as
    *the transformer*, and it scales better than earlier RNN-based sequence-to-sequence
    models on parallel computing architecture. This improved scalability drove its
    wide adoption over the competing architectures. We used a pretrained translation
    transformer model between English and the Ghanaian language Twi to probe the efficacy
    and other characteristics of this important architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Early explorations of transfer learning for NLP focused on analogies to computer
    vision, where it has been used successfully for a while. One such model—SIMOn—employed
    character-level convolutional neural networks (CNNs) combined with bidirectional
    LSTMs for structural semantic text classification. SIMOn stands for *Semantic
    Inference for the Modeling of Ontologies*. It was developed during DARPA’s Data-Driven
    Discovery of Models (D3M)[⁵](#pgfId-1247361) program,[⁶](#pgfId-1247364) which
    was an attempt to automate some typical tasks faced by data scientists. It demonstrated
    NLP transfer learning methods directly analogous to those that have been used
    in computer vision. The features learned by this model were shown to also be useful
    for unsupervised learning tasks and to work well on social media language data,
    which can be somewhat idiosyncratic and very different from the kind of language
    on Wikipedia and other large book-based datasets. Column-type classification was
    used as an illustrative example for this modeling framework.
  prefs: []
  type: TYPE_NORMAL
- en: 'By way of reminder, the heuristics for fine-tuning in computer vision go roughly
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A threshold is moved away from the output (and toward the input) as more data
    becomes available in the target domain. Parameters between the threshold and output
    are *unfrozen* and trained while the rest are kept the same. This is motivated
    by the fact that an increased amount of data can be used to train more parameters
    effectively than could be done otherwise.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, movement of the threshold must happen away from the output and
    toward the input, because this allows us to retain parameters encoding general
    features that are close to the input, while retraining layers closer to the output
    that encode features specific to the source domain.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moreover, when the source and the target are highly dissimilar, some of the
    more specific parameters/layers can be fully discarded.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One major weakness of the early embedding methods such as word2vec was disambiguation—distinguishing
    between various uses of a word that may have different meanings depending on context.
    These words are technically referred to as homographs, for example, duck (posture)
    versus duck (bird) and fair (a gathering) versus fair (just). *Embeddings from
    Language Models*—abbreviated as ELMo after the popular *Sesame Street* character—is
    one of the earliest attempts to develop contextualized embeddings of words, using
    bidirectional *long short-term memory networks* (bi-LSTMs). ELMo is arguably the
    most popular early pretrained language model associated with the ongoing NLP transfer
    learning revolution. It shares a lot of architectural similarities with SIMOn,
    also being composed of character-level CNNs followed by bi-LSTMs. The embedding
    of a word in this model depends on its context, which ELMo achieves by being trained
    to predict the next word in a sequence of words. Huge datasets, such as Wikipedia
    and various datasets of books, were used to train this model. We applied ELMo
    to an illustrative example problem, namely fake news detection, as a practical
    demonstration.
  prefs: []
  type: TYPE_NORMAL
- en: '*Universal Language Model Fine-Tuning* (ULMFiT) took this a step further by
    formalizing a method to fine-tune any neural network-based language model for
    any particular task. This framework introduces and demonstrates some key techniques
    and concepts enabling adapting a pretrained language model for new settings more
    effectively. These include discriminative fine-tuning and gradual unfreezing.
    Discriminative fine-tuning stipulates that because the different layers of a language
    model contain different type of information, they should be tuned at different
    rates. Gradual unfreezing describes a procedure for fine-tuning progressively
    more parameters in a gradual manner with the aim of reducing the risks of overfitting.
    The ULMFiT framework also includes innovations in varying the learning rate in
    a unique way during the adaptation process. We illustrated these concepts numerically
    using the fast.ai library.'
  prefs: []
  type: TYPE_NORMAL
- en: The OpenAI *Generative Pretrained Transformer* (GPT) modified the encoder-decoder
    architecture of the transformer to achieve a fine-tunable language model for NLP.
    It discarded the encoders, retaining the decoders and their self-attention sublayers.
    It is trained with a *causal modeling objective*—to predict the next word in a
    sequence. It is particularly suited for text generation. We demonstrated how you
    can quickly use the pretrained GPT-2 model for text generation using the transformers
    library by Hugging Face, which we introduced earlier in the book.
  prefs: []
  type: TYPE_NORMAL
- en: Bidirectional Encoder Representations from Transformers (BERT) did arguably
    the opposite, modifying the transformer architecture by preserving the encoders
    and discarding the decoders, also relying on *masking* of words, which would then
    need to be predicted accurately as the training metric. More specifically, it
    is trained with the *masked modeling objective*—to fill in the blanks. Additionally,
    it is trained with the *next sentence prediction* task—to determine whether a
    given sentence is a plausible following sentence after a target sentence. Although
    not suited for text generation, this model performs very well on other general
    language tasks such as classification and question answering. We applied it to
    the two important applications of question answering and document classification.
    The document classification use case was spam detection. We also demonstrated
    its application to filling in the blanks and detecting whether one sentence is
    a plausible next sentence for another one.
  prefs: []
  type: TYPE_NORMAL
- en: The model mBERT, which stands for “multilingual BERT,” is effectively BERT pretrained
    on over 100 languages simultaneously. Naturally, this model is particularly well-suited
    for cross-lingual transfer learning. We showed how the multilingual pretrained
    weights checkpoint could facilitate creating BERT embeddings for languages that
    were not even originally included in the multilingual training corpus. Both BERT
    and mBERT were created at Google.
  prefs: []
  type: TYPE_NORMAL
- en: 'In all of these language-model-based methods—ELMo, ULMFiT, GPT, and BERT—it
    was shown that embeddings generated could be fine-tuned for specific downstream
    NLP tasks with relatively few labeled data points. This explains the focus the
    NLP community has paid to language models: it validates the conjecture that the
    hypothesis set induced by them would be generally useful.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We also covered some adaptation strategies for the deep NLP transfer learning
    modeling architectures that were covered. In other words, given a pretrained architecture
    such as ELMo, BERT, or GPT, how can transfer learning be carried out more efficiently?
    We focused on *parameter efficiency* for this purpose, where the goal is to yield
    a model with the fewest parameters possible while suffering minimal reduction
    in performance. The purpose of this is to make the model smaller and easier to
    store, which would make it easier to deploy on smartphone devices, for instance.
    Alternatively, smart adaptation strategies may be required just to get to an acceptable
    level of performance in some difficult transfer cases. The adaptation strategies
    we covered follow:'
  prefs: []
  type: TYPE_NORMAL
- en: The first adaptation strategies we explored were the aforementioned ULMFiT techniques
    of *gradual unfreezing* and *discriminative fine-tuning* with the fast.ai library.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We then explored the model compression method known as *knowledge distillation*,
    due to its recent prominence in the NLP field. This process essentially attempts
    to mimic the output from the larger *teacher* model by the significantly smaller
    *student* model. In particular, we use an implementation of the method DistilBERT
    in the transformers library to demonstrate that the size of a BERT model can be
    more than halved by this approach.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next adaptation strategy we touched on revolves around two ideas aimed at
    creating transformer-based language models that scale more favorably with a bigger
    vocabulary and longer input length. The first involves clever factorization, or
    splitting up of a larger matrix of weights into two smaller matrices, allowing
    you to increase the dimensions of one without affecting the dimensions of the
    other. The second idea involves sharing parameters across all layers. These two
    strategies are the bedrock of the method known as ALBERT (A Lite BERT). We used
    the implementation in the transformers library to get some hands-on experience
    with the method.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consequently, we built on the idea of multitask learning, where a model is trained
    to perform a variety of tasks at once and yields a more generalizable model. When
    faced with a transfer scenario where we have insufficient training data to fine-tune
    on a given task, why not fine-tune on multiple tasks? Discussing this idea provided
    a great opportunity to introduce the *General Language Understanding Evaluation*
    (GLUE) dataset, a collection of data for several tasks representative of human
    language reasoning, such as detecting similarity between sentences, similarity
    between questions, paraphrasing, sentiment analysis, and question answering. We
    showed how to quickly leverage the transformers library for multitask fine-tuning
    using this dataset. This exercise also demonstrated how to similarly fine-tune
    a model from the BERT family on a custom dataset from one of these important classes
    of problems.
  prefs: []
  type: TYPE_NORMAL
- en: We also built on the idea of domain adaptation, particularly the fact that the
    similarity of the source and target domains plays a crucial role in the effectiveness
    of transfer learning. Greater similarity implies an easier transfer learning process
    in general. When the source and target are too dissimilar, it may be impossible
    to carry out the process in a single step. In those circumstances, the idea of
    *sequential adaptation* may be used to break the overall desired transfer into
    simpler, more manageable steps. By way of example, we sequentially adapted a “fill-in-the-blanks”
    objective pretrained BERT to a low-resource sentence similarity-detection scenario,
    by first adapting to a data-rich question-similarity scenario. Data for both scenarios
    in the experiment came from the GLUE dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The final adaptation strategy we explored was the use of so-called *adaptation
    modules* or *adapters*. These are newly introduced modules of only a few parameters
    between layers of a pretrained neural network. Fine-tuning this modified model
    for new tasks requires training only these few additional parameters. The weights
    of the original network are kept the same. Virtually no loss in performance, compared
    to fine-tuning the entire model, is often observed when adding just 3-4% additional
    parameters per task. These adapters are also modular and easily shared between
    researchers. We used the AdapterHub framework to load some of these adapters and
    show how they can be used to adapt a general BERT model to one expected to do
    well on a sentiment classification task.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2 Other emerging research trends
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Throughout the book, we have tried to emphasize that in a fast-moving field
    such as transfer learning for NLP, it would be impossible for a single book like
    this one to fully cover every architecture or innovation. Instead, we have taken
    the approach of focusing on the architectures and techniques we think are fundamental.
    Future innovations are likely to be in some sense derived from such architectures
    and techniques, so readers can possibly pick them up by doing a little bit of
    their own legwork. To facilitate this even further, we focus this section on a
    brief discussion of various research trends we did not get a chance to cover in
    this book but which have become somewhat influential in the field. We frame these
    in the context of what we did cover as much as possible to ease your picking up
    of those topics if needed.
  prefs: []
  type: TYPE_NORMAL
- en: We begin the exercise by overviewing RoBERTa[⁷](#pgfId-1247413)—Robustly Optimized
    BERT Approach—which employs some optimization tricks to improve BERT efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.1 RoBERTa
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The study in question attempted to replicate BERT while paying extra attention
    to various training hyperparameters and settings and their effect on the outcome.
    In general, it was observed that the performance of the original BERT could be
    improved significantly via careful design choices. One such choice is removing
    the next-sentence prediction (NSP) task while maintaining the masked language
    modeling (MLM)—fill-in-the-blanks—task. In other words, they found NSP degraded
    performance on downstream tasks and showed that removing it was beneficial. Additional
    design choices included large learning rates and mini-batches during training.
    It is implemented in the transformers library by Hugging Face that we introduced
    earlier in the book.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we look at one of the largest language model developed yet—GPT-3[⁸](#pgfId-1247425)—which
    has gathered much buzz lately, culminating in it winning the Best Paper Award
    at the NeurIPS 2020 virtual research conference (December 2020).
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.2 GPT-3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You may recall from our coverage of it that the GPT model has undergone several
    iterations—GPT, GPT-2, and, more recently, GPT-3\. At the time of writing, GPT-3
    happens to be one of the largest pretrained language models at 175 billion parameters.
    Its predecessor GPT-2 stands at 1.5 billion parameters and was also considered
    the largest when it was released just a year prior. Prior to the release of GPT-3
    in June 2020, the largest model was Microsoft’s Turing NLG, which stands at 17
    billion parameters and was released in February 2020\. The sheer speed of progress
    on some of these metrics has been mind-blowing, and these records tend to be broken
    very quickly. For comparison, this explosion in parameters is illustrated in figure
    11.2.
  prefs: []
  type: TYPE_NORMAL
- en: '![11_02](../Images/11_02.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 The growth in the number of parameters for the largest models over
    time. As can be seen in the diagram, the explosion in the size of the models appears
    to be accelerating, with one of the most recent advances—GPT-3—representing a
    growth factor of 10×.
  prefs: []
  type: TYPE_NORMAL
- en: As can be seen in the figure, GPT-3 represents an increase of more than 10x
    over the previously largest Turing NLG, a leap eclipsing prior progresses. Indeed,
    an architecture called the Switch Transformer,[⁹](#pgfId-1247441) which leverages
    sparsity by assigning separate transformer block sections to different inputs,
    claimed to have achieved a trillion parameter size in January 2021\. We did not
    include this architecture in figure 11.2 because it is still undergoing peer review
    at the time of writing. It is abundantly clear, however, that this trend of growing
    model sizes appears to be accelerating.
  prefs: []
  type: TYPE_NORMAL
- en: In the GPT-3 paper, the authors show that this huge model can perform a broad
    range of tasks with very few examples. For instance, it can be primed to translate
    one language to another by seeing only a few example translations or to detect
    spam email by seeing only a few example spam emails. In fact, some unexpected
    applications, such as writing code from a description of it, have been widely
    reported. At the moment, the model has not been released to the public by the
    OpenAI authors, only to a few early adopters by invitation and via a paid API.
    The stated reason by OpenAI for restricting access is monitoring use and thereby
    limiting any potential harmful applications of this technology. An early adopter
    of GPT-3 is an app called Shortly, which provides access to GPT-3 for creative
    writing purposes and which anyone can try for a small fee.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, a recent smaller but powerful open source alternative to GPT-3
    is already available in the transformers library: GPT-Neo from EleutherAI. This
    organization aims to build a model equivalent to the full-size GPT-3 and make
    it available to the public under an open license.[^(10)](#pgfId-1247447) Different
    models of varying sizes are available in their repository,[^(11)](#pgfId-1247451)
    where you can also test-run the models using the Hugging Face-hosted inference
    API in the browser. We also provide a companion Kaggle notebook[^(12)](#pgfId-1247455)
    showing it in action for the exercise we performed in chapter 7\. Upon inspection,
    you should find its performance better but, naturally, at a significantly higher
    cost. (The weights are more than 10 GB in size for the largest model!)'
  prefs: []
  type: TYPE_NORMAL
- en: An important thing to note about the GPT-3 work is that the authors themselves
    recognized in the paper that the benefits from making language models bigger has
    approached a limit. And even at that limit, performance on certain types of tasks,
    such as text generation concerning an understanding of common-sense physics, remains
    poor. Thus, although it does represent an important technological breakthrough,
    innovation in modeling approaches (versus simply scaling up models) has to be
    the path forward.
  prefs: []
  type: TYPE_NORMAL
- en: Next we look at a set of methods aimed at improving the performance of transformer-based
    models on longer input sequences. This is important because vanilla transformer
    models scale quadratically in run time and memory usage with input length.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.3 XLNet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: XLNet,[^(13)](#pgfId-1247463) which builds on a similar earlier model Transformer-XL,[^(14)](#pgfId-1247468)
    was designed to work better for longer input sequences. One of its critical component
    ideas is causal language modeling (CLM), which we discussed when covering GPT
    and which involves the classical language modeling task of predicting the next
    word in a sequence. Recall that future tokens are masked in this approach. The
    authors of the XLNet paper refer to this equivalently as autoregressive language
    modeling. The other critical component of XLNet is performing CLM over all possible
    permutations of the words in the input sequence. This idea is sometimes referred
    to as permutation language modeling (PLM). By combining PLM and CLM, bidirectionality
    is achieved because all tokens get to be included as past tokens in some permutation.
    Both XLNet and Transformer-XL have no sequence length limit and are implemented
    in the transformers library by Hugging Face.
  prefs: []
  type: TYPE_NORMAL
- en: With that view of XLNet in mind, let’s move on to consider BigBird,[^(15)](#pgfId-1247475)
    an innovation introducing the idea of a *sparse attention mechanism* for greater
    computational efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.4 BigBird
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: BigBird reduces the quadratic dependency of traditional transformer-based models
    to linear by introducing a sparse attention mechanism that is shown to approximate
    and maintain the properties of the original full attention. Instead of applying
    the full attention to the entire input sequence at once, sparse attention looks
    at the sequence token by token, allowing it to be more intelligent and drop some
    connections. Sequences with a length up to eight times as long as what could be
    handled with traditional transformer-based models can be handled on similar hardware.
    It is implemented in the transformers library by Hugging Face.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we touch on Longformer,[^(16)](#pgfId-1247486) another innovation on the
    traditional full self-attention of the transformer that scales better with input
    length.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.5 Longformer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Longformer is yet another attempt at battling the quadratic scaling of the traditional
    transformer attention. The innovation here is to combine a local windowed attention
    with a global task-oriented attention. The local attention is used for contextual
    representation whereas the global attention is used to build a full-sequence representation
    that is used in prediction. The scaling achieved is linear in the input sequence
    length, similar to BigBird. Longformer is implemented in the transformers library
    by Hugging Face.
  prefs: []
  type: TYPE_NORMAL
- en: We introduce the Reformer[^(17)](#pgfId-1247496) next, which is another approach
    to alleviating the quadratic scaling of the original self-attention.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.6 Reformer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Reformer introduces two techniques for combating the quadratic scaling in
    computing time and memory with the input length of the original transformer. Replacing
    the original full self-attention with one that employs locally sensitive hashing
    reduces redundant computations and time complexity from quadratic to O(L*log*L)
    (where L is the input sequence length). A technique known as *reversible layers*
    allows storing activations only once. In practice, this means that instead of
    storing activations *N* times for a model with *N* layers, only a small fraction
    of the memory is used. Depending on the value of *N*, the memory savings can be
    huge. Reformer is implemented in the transformers library by Hugging Face.
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, making transformer-based models work better with longer input lengths
    has emerged as a meta research trend. We have likely not included all the great
    research on the subject here, and you will likely find much more if you do some
    of your own digging.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will touch on recently reemerging sequence-to-sequence modeling approaches.
    These attempt to cast the various problems we have encountered in this book into
    a text-to-text modeling framework.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.7 T5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You may recall in our discussion in this book that sequence-to-sequence models
    have played an important role in NLP. First appearing in the context of recurrent
    neural network (RNN) models, they were also explored notably by the translation
    application area in the context of the original transformer architecture. T5,
    the “Text-to-Text Transfer Transformer,”[^(18)](#pgfId-1247518) is an attempt
    to cast a broad range of NLP problems into a unifying sequence-to-sequence framework.
    It allows for the application of the same model, objective, training procedure,
    and decoding process for every task. Handled problem classes vary from summarization
    to sentiment analysis and question answering, among many others. Translation for
    the language pairs between English and each of Romanian, German, and French is
    included in training. Some representative data transformations, which make it
    possible to train a single model on diverse tasks, are shown in figure 11.3 for
    translation and summarization (it is inspired by figure 1 from the T5 paper).
  prefs: []
  type: TYPE_NORMAL
- en: '![11_03](../Images/11_03.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 T5 is a sequence-to-sequence model that employs a number of transformations
    to enable a single model, decoding procedure, and objective to be trained on a
    variety of tasks simultaneously. It can be thought of as an interesting variation
    of multitask learning.
  prefs: []
  type: TYPE_NORMAL
- en: As can be seen in the figure, task data is transformed by prepending a standard
    task descriptor to the original text data. Training data included the datasets
    GLUE and SuperGLUE, CNN/Daily Mail dataset for abstractive summarization, and
    so on. The goal was to handle the included diverse set of natural language understanding
    tasks without modifying the model. In that sense, it can be thought of as an interesting
    variant or iteration of the multitask learning idea that we have been bringing
    up throughout the book. The inclusion of such a variety of tasks for simultaneous
    learning is likely to enable parameter sharing and better generalizability of
    the resulting model. Crucially, the model is initially trained on a dataset the
    authors call the Colossal Clean Crawled Corpus (C4) using a masked language modeling
    or autoencoding objective, before fine-tuning on the aforementioned variety of
    tasks. Basically, 15% of all tokens are dropped, and the result is fed to the
    input, while the uncorrupted input is fed to the output for prediction. Note that
    the C4 corpus is essentially the internet for the target language (English) with
    pornographic material, code, and other “garbage data” filtered out. The model
    architecture used for training is similar to the transformer architecture we used
    in chapter 7 for translation. State-of-the-art results were achieved by the resulting
    model on many of the included tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the original T5 model, a multilingual version, unsurprisingly
    called mT5, was also developed[^(19)](#pgfId-1247530) by training on 101 languages
    simultaneously. Both T5 and mT5 are implemented in the transformers library by
    Hugging Face.
  prefs: []
  type: TYPE_NORMAL
- en: In what follows, we briefly introduce BART, which is similar to T5 in that it
    is a transformer-based sequence-to-sequence modeling framework.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.8 BART
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: BART,[^(20)](#pgfId-1247537) which stands for Bidirectional and Autoregressive
    Transformers, can be thought of as T5 minus the single unifying transformation
    to enable an unmodified model to be applied to a variety of tasks. Instead, a
    standard transformer encoder-decoder architecture is first pretrained to reproduce
    corrupted input via a variety of noising approaches. This ranges from masked language
    modeling, as in BERT and T5, to permutation language modeling as in XLNet, among
    others. The model is then modified for a variety of tasks, such as SQuAD, summarization,
    and so on, and fine-tuned separately for each such task similar to what we did
    with the traditional BERT. This model performs particularly well in language-generation
    tasks, such as summarization, translation, and dialogue. A multilingual version—mBART[^(21)](#pgfId-1247543)—obtained
    by training on 25 languages simultaneously, has also been developed. Both BART
    and mBART are also implemented in the transformers library by Hugging Face.
  prefs: []
  type: TYPE_NORMAL
- en: In the following subsection, we look at a recent cross-lingual model that goes
    beyond merely training on multiple languages simultaneously, by modeling cross-lingual
    transfer explicitly via a modified language modeling objective when parallel data
    is available.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.9 XLM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: XLM,[^(22)](#pgfId-1247551) which the authors use to mean a “cross-lingual language
    model,” is a modeling framework that combines both monolingual and parallel data
    cross-lingual learning approaches. Monolingual embeddings learned on different
    languages can be *aligned* using a small vocabulary of words with known numerical
    representation. If parallel data is available, the authors introduce an approach
    they call *translation language modeling* (TLM) and exploit it for cross-lingual
    learning simultaneously. Essentially, this applies masked language modeling to
    a concatenated sequence of parallel data in both languages, with words dropped
    and asked to be predicted in both parts of the concatenated sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Significant improvements were observed in cross-lingual learning tasks. It also
    spurred a number of similar models, notably XLM-R,[^(23)](#pgfId-1247559) which
    combines the ideas of XLM with those of RoBERTa for improved performance. Both
    XLM and XLM-R are implemented in the transformers library by Hugging Face.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we briefly touch on a model specializing in an important class of problem
    data we encountered in the book—tabular data.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.10 TAPAS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In chapters 5 and 6, we discussed the method SIMOn and its handling of tabular
    data type classification—an important class of problem data typically encountered
    by data scientists. TAPAS[^(24)](#pgfId-1247570) is an attempt to extend the modeling
    benefits of transformer-based models to this important class of problems, by modeling
    and specializing for question answering in tabular data explicitly. TAPAS stands
    for Table Parser. In chapter 8, we discussed applying BERT to the task of question
    answering. The output of the resulting specialized model was the beginning and
    end positions of a potential answer to the question of interest in the input context
    paragraph. In addition to this, TAPAS learns to detect which cell in a table might
    contain the context paragraph from which the answer can be extracted with similar
    start and end indices. Like most of the other models discussed in this section,
    this model is implemented in the transformers library by Hugging Face.
  prefs: []
  type: TYPE_NORMAL
- en: This brings us to the end of this overview tour of recent work that we have
    not had an opportunity to analyze in detail in this book. Most of these model
    architectures can be used via code very similar to what we have used to work with
    BERT and DistilBERT in the transformers library.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we try to make an educated guess on where we can expect
    the field to move next—what sorts of topics might remain and/or become popular
    given current and emerging research trends.
  prefs: []
  type: TYPE_NORMAL
- en: 11.3 Future of transfer learning in NLP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we attempt to extrapolate the trends described in the previous
    two sections by anticipating what the immediate future of the field is likely
    to look like.
  prefs: []
  type: TYPE_NORMAL
- en: Critical analysis of the past two sections reveals two arguably orthogonal meta-trends—a
    push to make the models as large as possible, as well as a push to develop more
    efficient versions of larger models.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3, one of the largest leaps in the number of parameters we have observed
    yet—10 times—initially raised some concerns among researchers that top research
    companies would begin to focus on size over clever modeling. However, as we discussed
    in the previous section, the limitations of scaling up models quickly became apparent,
    with the authors of the GPT-3 paper admitting to limits that have likely been
    reached. Given that GPT-3 is currently available only via a limited paid API,
    we can expect the other players in the space to attempt to build even larger models
    soon as they have a monetary incentive to do so (we already mentioned the trillion-parameter
    Switch Transformer undergoing peer review). This race will likely culminate in
    a similar model from Google and/or Facebook being released, which will likely
    push GPT-3 to be fully open sourced (a similar scenario historically played out
    with GPT-2). Beyond that, we expect more resources to begin to be dedicated to
    more efficient methods of achieving similar performance.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the interesting problems remaining in the immediate future of NLP transfer
    learning likely lie in the movement some have termed *TinyML*. This can be defined
    as a general goal to shrink the size of models so that they can fit on smaller
    hardware. We saw an instance of this when we demonstrated that the size of BERT
    can be roughly halved with little loss in performance via an approach such as
    DistilBERT in chapter 9\. Another approach that accomplished this for us was ALBERT
    in chapter 10, achieving a 90% reduction in model size. A large fraction of the
    world’s population now owns a smartphone, which can run these smaller versions
    of cutting-edge models. The opportunities this opens for a field such as the Internet
    of Things (IoT), where devices form smart networks with each node executing complex
    functions independently, cannot be overstated. Although many phone apps featuring
    translation and other tools today probably feature a server backend where the
    actual translation and other computation happens, the ability to run such algorithms
    locally on smartphone devices without an internet connection is becoming a more
    feasible and prevalent paradigm. We expect the drive to make models such as BERT
    and its derivatives smaller and more parameter-efficient to continue at a fever
    pitch over the next couple years.
  prefs: []
  type: TYPE_NORMAL
- en: Another trend you may have picked up from the previous section is the increasing
    focus on cross-lingual models. In fact, the past year has seen an increase in
    global investment in methods for so-called “low-resource” languages. We alluded
    to this via an example in chapter 7, when we used a transformer architecture to
    translate a low-resource West African language, Twi, into English. Many popular
    economic models project that an increasingly important class of consumers is emerging
    in the African market, which is likely at least one driver behind sudden interest
    and investment in this space. For many low-resource languages, the initial barrier
    to the applicability of all the methods that we discussed tends to be data availability.
    Therefore, we can expect appropriate multilingual data development to receive
    much attention over the upcoming year or so, followed by intense research into
    language-specific methodology enhancements in the subsequent years. Notable places
    to keep an eye on for these developments, specifically as they pertain to African
    languages, include NLP Ghana,[^(25)](#pgfId-1247586) Masakhane,[^(26)](#pgfId-1247589)
    EthioNLP,[^(27)](#pgfId-1247592) Zindi Africa,[^(28)](#pgfId-1247595) AfricaNLP,[^(29)](#pgfId-1247598)
    and Black in AI.[^(30)](#pgfId-1247601)
  prefs: []
  type: TYPE_NORMAL
- en: 'Speech is another NLP research frontier that is poised for a watershed moment.
    Until recently, automatic speech recognition models, which transcribe speech into
    text, required many hours of parallel speech-text data to achieve good results.
    A recent architecture, Wav2Vec2[^(31)](#pgfId-1247605) from Facebook, demonstrated
    that pretraining on speech in many languages simultaneously dramatically reduced
    how much parallel data is required. This is similar to the functionality of mBERT
    we explored in this book for text. The Wav2Vec2 model is available in the transformers
    library and can be fine-tuned on new languages with just a few hours of annotated
    speech data. We expect this to spur the development of speech-recognition tools
    for many languages for the first time over the next year. Moreover, we anticipate
    that something similar is on the horizon for the reverse direction: text-to-speech,
    that is, the generation of speech from text.'
  prefs: []
  type: TYPE_NORMAL
- en: In chapter 1, we described how transfer learning in NLP was inspired by its
    advances in computer vision. Interestingly enough, recent advances in NLP transfer
    learning seem to be inspiring further advances in computer vision. One specific
    example is DALL-E,[^(32)](#pgfId-1247609) a version of GPT-3 trained on text description-image
    pairs, which has learned to generate images from text prompts. A wider trend is
    to build contextual scene-based object embeddings,[^(33)](#pgfId-1247612) which
    try to predict missing objects from other observable objects in the scene, analogously
    to the fill-in-the-blanks objective for words utilized by BERT and similar masked
    language models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another research question that seems to be gaining more attention recently
    is this: What are the environmental and ethical impacts of these models? At the
    outset of the recent research spike, researchers seemed content with releasing
    models that improved technical metrics only, but over time, the field has come
    to value detailed explorations of any potential ethical impacts. Of related increased
    interest are questions around explainability: Can we actually explain how a model
    came to its decisions, so we know for sure that it is not discriminating? We dive
    into these ethical questions further in the next section.'
  prefs: []
  type: TYPE_NORMAL
- en: 11.4 Ethical and environmental considerations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may recall that when we looked at the problem of fake news detection in
    chapters 5 and 6, we raised the point that what can be called fake is debatable.
    If care is not taken with the quality of the data labels, the biases ingrained
    in whoever prepared the labels for the training data will likely just transfer
    to the classification system. This was our first encounter with the important
    need to be fully aware of any potential limitations of these models before deploying
    them in circumstances which can significantly impact human lives.
  prefs: []
  type: TYPE_NORMAL
- en: When we fine-tuned mBERT on the JW300 dataset prepared by Jehovah’s Witnesses
    in chapter 8, we found that it filled in the blanks in a biased way. When we tried
    to predict a basic noun “school,” it would offer words such as Eden as plausible
    completions. This indicated strong religious bias, and it was the second time
    during our journey that we were alerted of the fact that blindly applying these
    models to some data can have biased and unforeseen outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will discuss this on a broader level, considering potential
    ethical and environmental considerations that should probably be kept at the back
    of the mind of any practitioner working on deploying these models. This is a topic
    that is receiving increasing attention recently, but it is far from new in the
    machine learning field in general.
  prefs: []
  type: TYPE_NORMAL
- en: Early high-profile machine learning studies of bias predictably happened in
    computer vision. The landmark work, “Gender Shades,”[^(34)](#pgfId-1247623) studied
    the accuracy of commercial gender-classification systems along racial and gender
    dimensions. It found that these systems underperformed for dark-skinned women
    as compared to lighter-skinned males by up to 35 absolute percentage points. This
    has an immense practical impact on minority communities, which may be policed
    by some of these automatic computer vision systems in some regions. An incorrect
    classification or detection can mean wrongful arrest, which even if cleared can
    mean job loss in the most vulnerable communities. There have been multiple widespread
    reports of this happening to real people. A cynical power imbalance was uncovered
    behind systems parading as “objective” and “scientific,” where the richer communities
    where these systems were developed and their economic benefits were mostly reaped
    did not suffer the harm inflicted on the poorer communities. The impact of this
    work and related studies was significant, with the US Congress recently taking
    up related mitigating regulation arguably as a direct consequence. Companies such
    as IBM and Amazon have also been forced to review how they share these technologies
    with law enforcement authorities, with IBM completely discontinuing the service.
  prefs: []
  type: TYPE_NORMAL
- en: Concerns about bias of pretrained NLP language models has also been high recently.
    In fact, the GPT-3 paper[^(35)](#pgfId-1247627) includes a dedicated section for
    a study along several dimensions, namely race, sex, and religion. It has become
    more and more common to see academic articles do this recently, which is quite
    encouraging to see. The GPT-3 study in particular probes the association the model
    learned from the training data for the various dimensions of interest. For instance,
    they discovered that occupations usually associated with a higher level of education
    were more closely associated with male pronouns when filling in the blanks. Similarly,
    prompts implying professional competence were more likely to be completed by male
    pronouns and specifiers. This is likely gender bias directly learned by the model
    from the internet, which we probably can’t expect to be an unbiased source. On
    the other hand, positive descriptors were assigned to nouns primed with “Asian”
    and “White” at a significantly higher rate than for “Black” personas. Again, a
    racial bias was clearly learned by the model from the internet, and blind application
    of the model would simply propagate this bias. On the religious dimension, the
    word “Islam” was associated with the word “terrorism” among the most likely completions.
    As a direct real-world impact of this bias, consider the case of the Palestinian
    man whose benign “good morning” Facebook post was incorrectly translated as “attack
    them” and led to significant unfair consequences.[^(36)](#pgfId-1247630)
  prefs: []
  type: TYPE_NORMAL
- en: Another way pretrained NLP language models could inadvertently affect poor communities
    is via climate change. In fact, these models have recently been shown to have
    quite the carbon footprint.[^(37)](#pgfId-1247634), [^(38)](#pgfId-1247637) Although
    the carbon footprint of training a single BERT model once was found to be equivalent
    to a single average roundtrip flight between New York and San Francisco, during
    fine-tuning and hyperparameter optimization, the model is actually trained many
    times. If the model was to be deployed via *neural architecture search*, where
    various architecture hyperparameters are exhaustively varied and the best-performing
    model selected, the researchers found a single model deployment to cost the footprint
    of up to five average cars over their entire lifetime. Again, this is serious
    and particularly egregious, because the effects of climate change, directly linked
    to these carbon footprints, hit hardest in the poorest communities, which do not
    experience the direct benefits of these models. It is clear that these costs need
    to be taken into account when evaluating these models. This realization is arguably
    one of the forces driving the field toward more parameter-efficient models.
  prefs: []
  type: TYPE_NORMAL
- en: 'A lingering criticism of pretrained language models, and deep learning in general,
    is that the models tend to be not very *explainable*—it is hard to explain how
    a model arrived at its predictions for any particular scenario. This is related
    to the earlier bias discussion we had in this section—having the model explain
    how it arrived at associations related to education, for instance, could help
    detect if such a decision was made based on the race or sex variable. Most notable
    recent approaches at achieving this, such as bertviz,[^(39)](#pgfId-1247642) try
    to build on the attention visualization we explored in chapter 7\. This still
    does not address the lack of training data transparency question that remains:
    the training of language models takes place at such a large data scale that it
    is virtually impossible for researchers to ensure that it is unbiased. Therefore,
    we expect to see an investment of time and effort into the development of methods
    that could perform comparably from significantly smaller, well-curated datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: With our brief discussion of some ethical issues that should be kept in mind
    complete, we provide some tips in the next section on how to stay up-to-date in
    this fast-moving field.
  prefs: []
  type: TYPE_NORMAL
- en: 11.5 Staying up-to-date
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we have stressed throughout this chapter, the state of transfer learning
    methods in NLP updates very quickly. The material covered in this book should
    be viewed only as a platform from which to continue keeping yourself updated on
    latest developments. In this section, we present a few basic tips on how one might
    achieve this. To summarize, participating in various relevant competitions on
    the Kaggle and/or Zindi platforms can be a good way to work on realistic, yet
    sufficiently clean, trending relevant data and problems. Keeping track of latest
    papers on *arXiv* is a must, and although news and social media coverage can be
    sensationalist and otherwise unreliable, it can still help one spot impactful
    papers early on.
  prefs: []
  type: TYPE_NORMAL
- en: 11.5.1 Kaggle and Zindi competitions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Throughout the book, we have encouraged you to use Kaggle to run the various
    code presented. Although the free GPU compute provided by the platform and ease
    of setup were immediately stated benefits, the biggest benefit may not have been
    explicitly stated until now. Arguably the most powerful aspect of the Kaggle platform
    is access to the numerous constantly ongoing and archived-for-posterity competitions
    on the platform.
  prefs: []
  type: TYPE_NORMAL
- en: Top firms facing all sorts of technical challenges use the platform to stimulate
    research and development of solutions to said problems by offering cash prizes,
    sometimes into the thousands of dollars for top-place finishes. This means that
    by tracking these competitions, you are kept updated on what the most pressing
    problems in industry are, while having access to the representative data for immediate
    testing and experimentation. You could browse current and past competitions by
    topic to find data to test any ideas you might have—all you need to do is attach
    the dataset to the notebooks we have been using in this book, change some paths,
    and you likely will be ready to produce some preliminary insights. Winning the
    competitions is, of course, great, if you can do so, but the learning value you
    will get from experimenting, failing, and trying again is what is truly invaluable.
    Indeed, in my experience, a solution to a contest problem that may be considered
    mediocre by leaderboard placement may be the one that leads to a real-world impact,
    if it is easier to deploy and scale in practice, for instance. We provide some
    specific tips for using Kaggle in appendix A to help beginners get started with
    it.
  prefs: []
  type: TYPE_NORMAL
- en: We have also highlighted the recent increase in focus on low-resource languages
    in NLP. It is thus crucial to mention the Zindi Africa platform, which provides
    many of the same functionalities as Kaggle but focuses on African languages and
    problems. If you are a researcher who wants to see how your methods might perform
    on some of these types of languages, this platform would be a good place to find
    related contests and data for experimentation.
  prefs: []
  type: TYPE_NORMAL
- en: 11.5.2 arXiv
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Machine learning, and by extension NLP, is arguably the most open field of research
    today. With a few exceptions, results are typically published on the open platform
    *arXiv* immediately when they become available. This allows research teams to
    claim priority to any discovery, while going through refinements and paper publication
    formalities. This means that the most cutting-edge research is already available
    to you if you are able to find it. *arXiv* is archived by Google Scholar, so setting
    alerts there for keywords that are important to you can help you detect relevant
    papers early on.
  prefs: []
  type: TYPE_NORMAL
- en: The volume of paper uploads to the *arXiv* platform is huge, and it is hard
    to find the most important papers that might be relevant to you. To address this
    issue, I recommend following the authors of your favorite papers on social media—Twitter
    seems to be the platform preferred by researchers in this space. Keeping an eye
    on media coverage can also be helpful, as long as you treat all claims with a
    grain of salt. We say a few more words about that next.
  prefs: []
  type: TYPE_NORMAL
- en: 11.5.3 News and social media (Twitter)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In general, it is good to treat news and social media coverage of scientific
    topics as potentially sensationalist and otherwise technically unreliable. This
    makes sense if one thinks about the incentives a media outlet might have related
    to covering the technology and the fact that often journalists may not have a
    technical background on the subject. However, vetted news can be a good indicator
    of excitement in the community about a particular paper or topic, and that is
    always a good thing to be aware of.
  prefs: []
  type: TYPE_NORMAL
- en: If you use a platform such as Google News, you can set alerts for topics such
    as “language models” in your feed. You will probably get a lot of hits, and not
    all of them might be worth your attention. I usually really dig into a paper deeply
    only after I see it come up in venues that I consider consistently “reliable”
    over a period of time, which gives me some confidence that the claims have withstood
    at least a short period of open review. The case of GPT-3 comes up when thinking
    about a recent example—this was one whose impact was immediately evident by following
    this heuristic on Google News.
  prefs: []
  type: TYPE_NORMAL
- en: With regard to social media, Twitter appears to be the platform of choice for
    machine learning research scientists. In fact, many are very open about their
    work and will gladly engage with you directly on the platform if you just ask
    them a question. This is one of the things I love most about working in this field.
    Please feel free to reach out to me at @pazunre. Your favorite author or scientist
    probably shares their latest favorite papers on their feed, and by just following
    them you could have these delivered to you directly. Some popular accounts you
    may find interesting in this space include @fchollet, @seb_ruder, and @huggingface.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond competitions, reading papers on *arXiv*, and tracking news and social
    media, nothing is really a good substitute for working on real-world practical
    challenges with these tools. For many people, this might just mean holding a job
    in machine learning and/or NLP and working on a practical application of them
    daily. Practical experience is what is valued most by the majority of potential
    employers in this field. If you have yet to gain such practical experience in
    this area and are looking to break in, open source projects can be a great way
    to do that—check out TensorFlow, PyTorch, Hugging Face, NLP Ghana, Masakhane,
    and so on. The list is endless, and there is no shortage of interesting problems
    to solve and contribute to, while also potentially benefitting everyone.
  prefs: []
  type: TYPE_NORMAL
- en: I hope these tips will help guide you into your machine learning and NLP future,
    where you are empowered to make a significant positive impact on your society.
    It has been a privilege to share a part of your journey with you.
  prefs: []
  type: TYPE_NORMAL
- en: 11.6 Final words
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is it! You have done it—you have read all of this book. I had an incredible
    time writing it, interacting with many researchers in the process to discuss ideas
    and working through the many challenges. I sincerely hope that you enjoyed the
    journey as much as I did. As you go forth and change the world with these tools,
    remember to be kind to those around you, do no harm to the ecosystem, and stay
    vigilant about the potential misuses of your tech. From my brief time interacting
    with some of the brilliant minds working in this field, I sincerely believe that
    most are excited about making these technological breakthroughs a source for good.
    Thus, I watch the research news with excitement day after day, impatient to see
    what our collective human mind will come up with next. I can only hope that you
    share some of this excitement.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You are only at the beginning of your journey in this rapidly evolving field;
    retaining a competitive edge is a journey and not a destination.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The skills you have picked up by working through this book have put you in a
    good position to enable you to stay up-to-date with continued effort.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some key fundamental pretrained transfer-learning-enabled language modeling
    architectures we covered include the Transformer, BERT, mBERT, ELMo, and GPT.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The desire to make these larger models smaller and more efficient led to the
    development of architectures/techniques such as ALBERT, DistilBERT, and ULMFiT,
    which we covered as well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Emerging architectures that are the descendants of the aforementioned models,
    which are not covered by the book in detail but which you should be aware of,
    include BART, T5, Longformer, Reformer, XLNet, and many more.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is important to be aware of potential ethical and environmental impacts of
    these models when deploying them in practice.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recent concerns about ethical and environmental impacts, as well as the desire
    to put model capabilities on smartphones and IoT, will likely continue to fuel
    the development of increasingly more efficient transformer architectures in the
    near future.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '1. Sharir O. et al., “The Cost of Training NLP Models: A Concise Overview,”
    arXiv (2020).'
  prefs: []
  type: TYPE_NORMAL
- en: 2. S.J. Pan and Q. Yang, “A Survey on Transfer Learning,” IEEE Transactions
    on Knowledge and Data Engineering (2009).
  prefs: []
  type: TYPE_NORMAL
- en: 3. S. Ruder, “Neural Transfer Learning for Natural Language Processing,” National
    University of Ireland, Galway, Ireland (2019).
  prefs: []
  type: TYPE_NORMAL
- en: 4. D. Wang and T. F. Zheng, “Transfer Learning for Speech and Language Processing,”
    Proceedings of 2015 Asia-Pacific Signal and Information Processing Association
    Annual Summit and Conference (APSIPA).
  prefs: []
  type: TYPE_NORMAL
- en: 5. Lipmann Richard et al., “An Overview of the DARPA Data-Driven Discovery of
    Models (D3M) Program,” Proceedings of the 29th Conference on Neural Information
    Processing Systems (NeurIPS) (2016).
  prefs: []
  type: TYPE_NORMAL
- en: 6. [https://datadrivendiscovery.org/](https://datadrivendiscovery.org/)
  prefs: []
  type: TYPE_NORMAL
- en: '7. Yinhan Liu et al., “RoBERTa: A Robustly Optimized BERT Pretraining Approach,”
    arXiv (2019).'
  prefs: []
  type: TYPE_NORMAL
- en: 8. Tom B. Brown et al., “Language Models Are Few-Shot Learners,” NeurIPS (2020).
  prefs: []
  type: TYPE_NORMAL
- en: '9. W. Fedus et al., “Switch Transformers: Scaling to Trillion Parameter Models
    with Simple and Efficient Sparsity,”arXiv (2021).'
  prefs: []
  type: TYPE_NORMAL
- en: 10. [https://www.eleuther.ai/projects/gpt-neo/](https://www.eleuther.ai/projects/gpt-neo/)
  prefs: []
  type: TYPE_NORMAL
- en: 11. [https://huggingface.co/EleutherAI](https://huggingface.co/EleutherAI)
  prefs: []
  type: TYPE_NORMAL
- en: 12. [https://www.kaggle.com/azunre/tlfornlp-chapter7-gpt-neo](https://www.kaggle.com/azunre/tlfornlp-chapter7-gpt-neo)
  prefs: []
  type: TYPE_NORMAL
- en: '13. Z. Yang et al., “XLNet: Generalized Autoregressive Pretraining for Language
    Understanding,” NeurIPS (2019).'
  prefs: []
  type: TYPE_NORMAL
- en: '14. Z. Dai et al., “Transformer-XL: Attentive Language Models beyond a Fixed-Length
    Context,” ACL (2019).'
  prefs: []
  type: TYPE_NORMAL
- en: '15. M. Zaheer et al., “BigBird: Transformers for Longer Sequences,” arXiv (2020).'
  prefs: []
  type: TYPE_NORMAL
- en: '16. I. Beltagy et al., “Longformer: The Long-Document Transformer,” arXiv (2020).'
  prefs: []
  type: TYPE_NORMAL
- en: '17. N. Kitaev et al., “Reformer: The Efficient Transformer,” arXiv (2020).'
  prefs: []
  type: TYPE_NORMAL
- en: 18. C. Raffel et al., “Exploring the Limits of Transfer Learning with a Unified
    Text-to-Text Transformer,” arXiv (2020).
  prefs: []
  type: TYPE_NORMAL
- en: '19. L. Xue et al., “mT5: A Massively Multilingual Pre-Trained Text-to-Text
    Transformer,” arXiv (2019).'
  prefs: []
  type: TYPE_NORMAL
- en: '20. M. Lewis et al., “BART: Denoising Sequence-to-Sequence Pre-training for
    Natural Language Generation, Translation, and Comprehension,” arXiv (2020).'
  prefs: []
  type: TYPE_NORMAL
- en: 21. Y. Liu et al., “Multilingual Denoising Pre-Training for Neural Machine Translation,”
    arXiv (2020).
  prefs: []
  type: TYPE_NORMAL
- en: 22. G. Lample and A. Conneau, “Cross-Lingual Language Model Pretraining,” arXiv
    (2019).
  prefs: []
  type: TYPE_NORMAL
- en: 23. A. Conneau et al., “Unsupervised Cross-Lingual Representation Learning at
    Scale,” arXiv (2019).
  prefs: []
  type: TYPE_NORMAL
- en: '24. J. Herzig et al., “TaPas: Weakly Supervised Table Parsing via Pre-Training,”
    arXiv (2020).'
  prefs: []
  type: TYPE_NORMAL
- en: 25. [https://ghananlp.org/](https://ghananlp.org/)
  prefs: []
  type: TYPE_NORMAL
- en: 26. [https://www.masakhane.io/](https://www.masakhane.io/)
  prefs: []
  type: TYPE_NORMAL
- en: 27. [https://ethionlp.github.io/](https://ethionlp.github.io/)
  prefs: []
  type: TYPE_NORMAL
- en: 28. [https://zindi.africa/](https://zindi.africa/)
  prefs: []
  type: TYPE_NORMAL
- en: 29. [https://www.k4all.org/project/language-dataset-fellowship/](https://www.k4all.org/project/language-dataset-fellowship/)
  prefs: []
  type: TYPE_NORMAL
- en: 30. [https://blackinai.github.io/](https://blackinai.github.io/)
  prefs: []
  type: TYPE_NORMAL
- en: 31. [https://huggingface.co/facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53)
  prefs: []
  type: TYPE_NORMAL
- en: 32. [https://openai.com/blog/dall-e/](https://openai.com/blog/dall-e/)
  prefs: []
  type: TYPE_NORMAL
- en: '33. A. Dosovitskiy et al, “An Image Is Worth 16x16 Words: Transformers for
    Image Recognition at Scale,” arXiv (2020).'
  prefs: []
  type: TYPE_NORMAL
- en: '34. J. Builamwini and T. Gebru, “Gender Shades: Intersectional Accuracy Disparities
    in Commercial Gender Classification,” Journal of Machine Learning Research 81
    (2018).'
  prefs: []
  type: TYPE_NORMAL
- en: 35. Tom B. Brown et al., “Language Models Are Few-Shot Learners,” NeurIPS (2020).
  prefs: []
  type: TYPE_NORMAL
- en: 36. [http://mng.bz/w0V2](http://mng.bz/w0V2)
  prefs: []
  type: TYPE_NORMAL
- en: 37. E. Strubell et al., “Energy and Policy Considerations for Deep Learning
    in NLP,” ACL (2019).
  prefs: []
  type: TYPE_NORMAL
- en: '38. E. Bender et al., “On the Dangers of Stochastic Parrots: Can Language Models
    Be Too Big?” FAccT (2021).'
  prefs: []
  type: TYPE_NORMAL
- en: 39. Jesse Vig, “A Multiscale Visualization of Attention in the Transformer Model,”
    ACL (2019).
  prefs: []
  type: TYPE_NORMAL
