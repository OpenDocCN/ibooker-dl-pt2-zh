["```py\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.applications import InceptionResNetV2\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, Dense, Dropout\n\nK.clear_session()\n\ndef get_inception_resnet_v2_pretrained():\n    model = Sequential([                                       ❶\n        Input(shape=(224,224,3)),                              ❷\n        InceptionResNetV2(include_top=False, pooling='avg'),   ❸\n        Dropout(0.4),                                          ❹\n        Dense(200, activation='softmax')                       ❺\n    ])\n    loss = tf.keras.losses.CategoricalCrossentropy()\n    adam = tf.keras.optimizers.Adam(learning_rate=0.0001)\n    model.compile(loss=loss, optimizer=adam, metrics=['accuracy'])\n    return model \n\nmodel = get_inception_resnet_v2_pretrained()\nmodel.summary()\n```", "```py\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninception_resnet_v2 (Model)  (None, 1536)              54336736  \n_________________________________________________________________\ndropout (Dropout)            (None, 1536)              0         \n_________________________________________________________________\ndense (Dense)                (None, 200)               307400    \n=================================================================\nTotal params: 54,644,136\nTrainable params: 54,583,592\nNon-trainable params: 60,544\n_________________________________________________________________\n```", "```py\nK.clear_session()\n\nmodel = load_model(os.path.join('models','inception_resnet_v2.h5'))\n\ndef unwrap_model(model):\n    inception = model.get_layer('inception_resnet_v2')\n    inp = inception.input\n    out = model.get_layer('dropout')(inception.output)\n    out = model.get_layer('dense')(out)\n    return Model(inp, out)   \n\nunwrapped_model = unwrap_model(model)\n\nunwrapped_model.summary()\n```", "```py\nModel: \"model\"\n___________________________________________________________________________\n➥ ________________\nLayer (type)                    Output Shape         Param #     Connected \n➥ to                     \n===========================================================================\n➥ ================\ninput_2 (InputLayer)            [(None, None, None,  0                                            \n___________________________________________________________________________\n➥ ________________\nconv2d (Conv2D)                 (None, None, None, 3 864         \n➥ input_2[0][0]                    \n___________________________________________________________________________\n➥ ________________\nbatch_normalization (BatchNorma (None, None, None, 3 96          \n➥ conv2d[0][0]                     \n___________________________________________________________________________\n➥ ________________\nactivation (Activation)         (None, None, None, 3 0           \n➥ batch_normalization[0][0]        \n___________________________________________________________________________\n➥ ________________\n\n...\n___________________________________________________________________________\n➥ ________________\nconv_7b (Conv2D)                (None, None, None, 1 3194880     \n➥ block8_10[0][0]                  \n___________________________________________________________________________\n➥ ________________\nconv_7b_bn (BatchNormalization) (None, None, None, 1 4608        \n➥ conv_7b[0][0]                    \n___________________________________________________________________________\n➥ ________________\nconv_7b_ac (Activation)         (None, None, None, 1 0           \n➥ conv_7b_bn[0][0]                 \n___________________________________________________________________________\n➥ ________________\nglobal_average_pooling2d (Globa (None, 1536)         0           \n➥ conv_7b_ac[0][0]                 \n___________________________________________________________________________\n➥ ________________\ndropout (Dropout)               (None, 1536)         0           \n➥ global_average_pooling2d[0][0]   \n___________________________________________________________________________\n➥ ________________\ndense (Dense)                   (None, 200)          307400      \n➥ dropout[1][0]                    \n===========================================================================\n➥ ================\nTotal params: 54,644,136\nTrainable params: 54,583,592\nNon-trainable params: 60,544\n___________________________________________________________________________\n➥ ________________\n```", "```py\nlast_conv_layer = 'conv_7b' # This is the name of the last conv layer of the model\n\ngrad_model = Model(\n    inputs=unwrapped_model.inputs, \n    outputs=[\n        unwrapped_model.get_layer(last_conv_layer).output,\n        unwrapped_model.output\n    ]    \n)\n```", "```py\nimg_path = 'data/tiny-imagenet-200/val/images/val_434.JPEG'\n\nval_df = pd.read_csv(                                                          ❶\n    os.path.join('data','tiny-imagenet-200', 'val', 'val_annotations.txt'),\n    sep='\\t', index_col=0, header=None\n)\n\nwith open(os.path.join('data','class_indices'),'rb') as f:                     ❷\n    class_indices = pickle.load(f)\nwords = pd.read_csv(                                                           ❸\n    os.path.join('data','tiny-imagenet-200', 'words.txt'), \n    sep='\\t', index_col=0, header=None\n)\n\ndef get_image_class_label(img_path, val_df, class_indices, words):\n    \"\"\" Returns the normalized input, class (int) and the label name for a given image\"\"\"\n\n    img = np.expand_dims(                                                      ❹\n        np.asarray(\n            Image.open(img_path).resize((224,224)                              ❺\n    )\n\n    img /= 127.5                                                               ❻\n    img -= 1                                                                   ❻\n\n    if img.ndim == 3:\n        img = np.repeat(np.expand_dims(img, axis=-1), 3, axis=-1)              ❼\n\n    _, img_name = os.path.split(img_path)\n\n    wnid = val_df.loc[img_name,1]                                              ❽\n    cls = class_indices[wnid]                                                  ❾\n    label = words.loc[wnid, 1]                                                 ❿\n    return img, cls, label\n\n# Test the function with a test image\nimg, cls, label = get_image_class_label(img_path, val_df, class_indices, words)⓫\n```", "```py\n# Define a sample probe set to get Grad-CAM\nimage_fnames = [\n    os.path.join('data','tiny-imagenet-200', 'val','images',f) \\\n    for f in [\n        'val_9917.JPEG', 'val_9816.JPEG', 'val_9800.JPEG', 'val_9673.JPEG', \n➥ 'val_9470.JPEG',\n        'val_4.JPEG', 'val_127.JPEG', 'val_120.JPEG', 'val_256.JPEG', \n➥ 'val_692.JPEG'\n    ]\n]\n\ngrad_info = {}\nfor fname in image_fnames:                                                      ❶\n    img, cls, label = get_image_class_label(fname, val_df, class_indices, words)❷\n\n    with tf.GradientTape() as tape:                                             ❸\n        conv_output, preds = grad_model(img)                                    ❸\n        loss = preds[:, cls]                                                    ❹\n\n    grads = tape.gradient(loss, conv_output)                                    ❺\n\n    weights = tf.reduce_mean(grads, axis=(1, 2), keepdims=True)                 ❻\n    grads *= weights                                                            ❻\n\n    grads = tf.reduce_sum(grads, axis=(0,3))                                    ❼\n    grads = tf.nn.relu(grads)                                                   ❼\n\n    grads /= tf.reduce_max(grads)                                               ❽\n    grads = tf.cast(grads*255.0, 'uint8')                                       ❽\n\n    grad_info[fname] = {'image': img, 'class': cls, 'label':label, 'gradcam': \n➥ grads}                                                                       ❾\n```", "```py\ninp = layers.Input(shape=(512, 512, 3))\n# Defining the pretrained resnet 50 as the encoder\nencoder = tf.keras.applications.ResNet50 (\n    include_top=False, input_tensor=inp,pooling=None\n)\n```", "```py\ndef upsample_conv(inp, copy_and_crop, filters):\n    \"\"\" Up sampling layer of the U-net \"\"\"\n\n    # 2x2 transpose convolution layer\n    conv1_out = layers.Conv2DTranspose(\n        filters, (2,2), (2,2), activation='relu'\n    )(inp)\n    # Size of the crop length for one side\n    crop_side = int((copy_and_crop.shape[1]-conv1_out.shape[1])/2)\n\n    # Crop if crop side is > 0\n    if crop_side > 0:\n        cropped_copy = layers.Cropping2D(crop_side)(copy_and_crop)\n    else:\n        cropped_copy = copy_and_crop\n\n    # Concat the cropped encoder output and the decoder output\n    concat_out = layers.Concatenate(axis=-1)([conv1_out, cropped_copy])\n\n    # 3x3 convolution layer\n    conv2_out = layers.Conv2D(\n        filters, (3,3), activation='relu', padding='valid'\n    )(concat_out)\n\n    # 3x3 Convolution layer\n    out = layers.Conv2D(\n        filters, (3,3), activation='relu', padding='valid'\n    )(conv2_out)\n\n    return out\n```", "```py\nconv1_out = layers.Conv2DTranspose(\n                    filters=filters, kernel_size=(2,2), \n                    strides=(2,2), activation='relu'\n    )(inp)\n```", "```py\ncrop_side = int((copy_and_crop.shape[1]-conv1_out.shape[1])/2)\nif crop_side > 0:\n        cropped_copy = layers.Cropping2D(crop_side)(copy_and_crop)\n    else:\n        cropped_copy = copy_and_crop\n```", "```py\ndef decoder(inp, encoder):\n    \"\"\" Define the decoder of the U-net model \"\"\"\n\n    up_1 = upsample_conv(inp, encoder.get_layer(\"conv3_block4_out\").output, \n➥ 512) # 32x32\n\n    up_2 = upsample_conv(up_1, \n➥ encoder.get_layer(\"conv2_block3_out\").output, 256) # 64x64\n\n    up_3 = upsample_conv(up_2, encoder.get_layer(\"conv1_relu\").output, 64) \n➥ # 128 x 128    \n\n    return up_3\n```", "```py\nencoder.get_layer(\"conv3_block4_out\").output\n```", "```py\ndef unet_pretrained_encoder():\n    \"\"\" Define a pretrained encoder based on the Resnet50 model \"\"\"\n\n    # Defining an input layer of size 384x384x3\n    inp = layers.Input(shape=(512, 512, 3))\n    # Defining the pretrained resnet 50 as the encoder\n    encoder = tf.keras.applications.ResNet50 (\n        include_top=False, input_tensor=inp,pooling=None\n    )\n\n    # Encoder output # 8x8\n    decoder_out = decoder(encoder.get_layer(\"conv4_block6_out\").output, encoder)\n\n    # Final output of the model (note no activation)\n    final_out = layers.Conv2D(num_classes, (1,1))(decoder_out)    \n    # Final model\n    model = models.Model(encoder.input, final_out)\n    return model\n```"]