- en: '3 AI mastery: Essential techniques, Part 2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An introduction to data mining
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An overview of the artificial neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A description of deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An introduction to Bayesian networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An overview of unsupervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI expert Arthur Samuel, introduced in chapter 1 for the success of his 1959
    checkers program, defined machine learning as the field of study that gives computers
    the ability to learn without being explicitly programmed. “Without being explicitly
    programmed” can be misleading, as learning is achieved with techniques such as
    data mining and neural networks, which rely on algorithms explicitly programmed
    by engineers.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will explore data mining, a technique used to extract valuable
    information, patterns, and associations from data. I briefly mention Bayesian
    networks, a method that encodes probabilistic relationships between variables
    of interest. I then introduce artificial neural networks and deep learning, powerful
    pattern recognition algorithms that have achieved impressive results in computer
    vision, natural language, and audio processing. We finish this chapter with unsupervised
    learning, a set of algorithms that can analyze unlabeled datasets to discover
    similarities and differences. I’ll provide enough detail to allow you to understand
    what these machine learning techniques entail and how they’re applied, but we
    won’t get caught up in the theory.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Data mining
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine a program that helps a grocery store ensure that its shelves are consistently
    stocked with fresh produce precisely when needed, eliminating the problems of
    overstocking or running out of popular items. The program also excels at optimizing
    store layouts, strategically placing complementary items to boost sales, fine-tuning
    prices for maximum profit, and crafting personalized promotions and discounts
    based on individual customers’ past purchases to enhance customer loyalty, increase
    sales, and optimize profits. This example perfectly illustrates one of the numerous
    benefits data mining techniques can bring to the retail industry. Data mining
    is an artificial intelligence approach encompassing a range of techniques and
    algorithms to discover hidden patterns, relationships, and valuable insights from
    vast and complex data sources. Its applications are vast and continually evolving
    as organizations increasingly recognize the immense value of extracting actionable
    insights from the ever-expanding volumes of data at their disposal. Indeed, the
    amount of data available has increased exponentially over recent decades due to
    the near-universal adoption of the internet, the popularization of e-commerce,
    the use of barcodes on most commercial products, the popularity of social media,
    and ubiquitous web tracking. Exacerbated by low-cost data storage that promotes
    accumulation, the proliferation of data has created the need for automated techniques
    to extract knowledge and insight from it. It is obviously impossible for individuals
    to process or analyze even a minuscule fraction of what’s available.
  prefs: []
  type: TYPE_NORMAL
- en: Much like gold mining, which extracts gold from piles of rock and sand, data
    mining is carried out to uncover meaningful correlations, patterns, anomalies,
    or rules hidden within extensive data sets. Formally, data mining refers to a
    collection of algorithms used for tasks such as classification, prediction, clustering,
    and market basket analysis. These algorithms make use of statistical, probabilistic,
    and mathematical techniques to identify data patterns, with some of their implementations
    having names like SLIQ [1] CART [2], C4.5 [3], and CHAID [4].
  prefs: []
  type: TYPE_NORMAL
- en: Data mining algorithms are applied across various industries. For instance,
    the Walt Disney MyMagic+ project utilizes data mining to enhance the efficiency
    of its operations and infrastructure. One notable application is its use in minimizing
    wait times for attractions and restaurants.
  prefs: []
  type: TYPE_NORMAL
- en: The food industry employs data mining for demand forecasting and competitive
    pricing of products. For instance, franchise companies like McDonald’s utilize
    data mining to identify optimal locations for new stores. Local governments apply
    data mining to predict traffic volumes, especially during peak hours, while utility
    companies utilize data mining to forecast electricity demand and maintain a reliable
    supply.
  prefs: []
  type: TYPE_NORMAL
- en: A typical data mining task involves classification, which is the process of
    categorizing labeled data into meaningful groups. The knowledge derived from analyzing
    the data is often represented in a decision tree. A decision tree is a flowchart
    used to associate input data with the appropriate category through a series of
    questions or tests represented by the nodes in the tree. Each node evaluates a
    specific attribute of the data, and each distinct attribute value corresponds
    to a branch emanating from that node. An output node, also known as a leaf or
    terminal node, signifies a category or decision. The nodes situated between the
    input nodes and the terminal nodes are commonly referred to as test nodes.
  prefs: []
  type: TYPE_NORMAL
- en: The structure of a decision tree is inferred from the data. Mathematical formulas
    are employed to assess the potential contribution of each node in reaching a decision
    efficiently, and the most discriminative nodes are strategically positioned at
    the beginning of the tree. For instance, if you wish to determine whether an animal
    is a bird, the initial question you might consider is whether it has feathers
    or perhaps whether it can fly. On the other hand, asking whether it resides in
    a forest would not lead you to a decision as swiftly.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Decision trees for fraud prevention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Banks bear a substantial responsibility for effectively managing the risks associated
    with credit and payment transactions. Given the substantial sums of money involved,
    any security breach in banking operations can have a severe detrimental effect
    on an institution’s reputation. When a credit card is used at a merchant’s location,
    a highly efficient AI-powered system must swiftly provide authorization decisions,
    typically within milliseconds. To identify potential fraudulent activities, this
    system needs to assess numerous parameters linked to the card, all while processing
    10s of thousands of transactions per second without causing any delays. Consider
    the sheer volume of queries that inundate the system on Black Friday, the day
    following Thanksgiving, which traditionally marks the commencement of the Christmas
    shopping season in the United States.
  prefs: []
  type: TYPE_NORMAL
- en: To build a decision tree that a card-issuing bank could use for real-time fraud
    prevention, we might analyze 18 months of their credit card transactions. Each
    transaction record will contain many attributes such as purchase amount, purchase
    time and date, international or domestic merchant, merchant category code (indicating
    merchant business type), geographic area, and whether the card was present during
    the transaction. Each fraudulent transaction would have to have been labeled as
    such by a human.
  prefs: []
  type: TYPE_NORMAL
- en: The decision tree will be constructed by weighing each attribute’s potential
    to help the risk system decide in real time whether to accept or decline a credit
    card transaction. The space of attributes will be recursively partitioned based
    on importance, and the attributes most useful for forming an assessment will be
    placed earliest in the decision tree. In the fraud-prevention example, the data
    may show that fraud is significantly more common in international transactions
    than domestic ones, and therefore, this question should be asked first. Thus,
    the node associated with this question will be the first in the tree.
  prefs: []
  type: TYPE_NORMAL
- en: After creating this initial node, we create two branches, one for domestic and
    one for international. We then repeat the procedure to find the most discriminative
    question to ask about the transactions associated with each branch. For domestic
    transactions, perhaps the data shows that the likelihood of fraud is significantly
    higher for transactions done online or over the phone than transactions done by
    physically swiping a card.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the most significant question we could ask next might be whether
    the card was present for the transaction, and the node following the domestic
    branch in the decision tree might address this question. We would create a “card
    present” branch and a “card not present” branch in the domestic path, and we would
    repeat this process until the available attributes are all represented in an efficient
    decision tree. Figure 3.1 illustrates the first few nodes of the decision tree
    we’re discussing.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 The first few levels of a decision tree. In a real-world application,
    a full tree may contain thousands of nodes.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We don’t always rely solely on the raw attributes available from the transaction
    records for a business application. We might also try to enrich the data with
    attributes gleaned from further analysis. For our credit card authorization problem,
    we might realize the importance of questions about the number of transactions
    performed in the last 15 minutes or the last hour, or maybe research identifies
    suspicious purchasing patterns involving restaurants and gas stations. The logic
    represented in a decision tree is sometimes translated into a set of `if-then-else`
    rules, making it easier to understand, especially if the tree is very large.
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, a data mining project generally follows an iterative process:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the application domain and the goals of the data mining project
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gathering the data, which often involves a costly labeling step
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Integrating the data gathered from various sources
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cleaning the data to remove inconsistencies
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Performing analysis to identify new attributes that enrich the data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dividing the data into at least two sets, one for training and one for testing
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Selecting suitable data mining algorithms
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Building the system using the designated training data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pruning the decision tree to keep the model sufficiently general
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Testing the model using the designated testing and evaluating its performance
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Testing the scalability and resilience of the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeating steps 2 to 11 until you achieve the desired performance
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploying the model and integrating the system into operations
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 3.2 shows a process diagram outlining the creation and deployment of
    a data mining model.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 An example of a flowchart that depicts the various steps to design
    and deploy a data-mining model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Although decision-tree algorithms are the most popular, other data-mining techniques
    are also used. For example, association analysis is often used for market-basket
    studies, which attempt to identify sets of products that tend to be purchased
    together. This straightforward approach is based on addition, intersection, and
    union. For example, suppose we have one million receipts, of which 20,000 include
    the purchase of bread and cheese and 5,000 include bread, cheese, and olives.
    We could infer from this data that a customer who buys both bread and cheese has
    a 25% likelihood of also purchasing olives. Information about customer buying
    habits gleaned from association analysis can be used to develop cross-selling
    strategies, provide relevant coupons, and even optimize how products are displayed
    on store shelves. Information obtained through related approaches can be used
    to predict the effects of rebate offers or to develop strategies to improve customer
    retention.
  prefs: []
  type: TYPE_NORMAL
- en: The value of the predictions and guidance provided by data mining heavily depends
    on the quality of the input, and the adage “garbage in, garbage out” (GIGO) is
    very apt. Unreliable data leads to unreliable models, and the inconsistencies
    that arise when compiling data in different formats from multiple sources present
    significant problems. It can be almost impossible for software or even people
    to recognize when data has been labeled incorrectly, and it can be a challenge
    to mitigate the biases and other subjective effects that individual people can
    have when data is first recorded. Practical challenges arise, too, when applying
    data mining in real situations. For example, the structure and logic of a decision
    tree can’t be updated incrementally in light of new information, so decision trees
    aren’t effective for adapting to changes in data and behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Artificial neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine we want to teach a computer how to recognize handwritten numbers from
    0 to 9\. At the beginning, we show our program lots of pictures of handwritten
    numbers (training examples) to train the program. Our program relies on different
    layers to learn. The first layer, for instance, looks at the picture and tries
    to recognize things like edges and curves. The next layer will use the results
    of the first layer to look for shapes, like loops and lines. For instance, the
    number 8 has a shape that resembles two circles stacked on top of each other.
    We can continue this process by going deeper into more layers. After passing through
    all these layers, our program guesses which number is in the picture. We compare
    the guess made by our program to the correct number in the picture. If the program
    guessed wrong—for example, it guessed 1 when the number was actually a 7—we tell
    the program that it made an error. The program will then adjust its parameters
    (numerical values) that serve as the secret sauce behind the program’s classification.
    These numbers reflect the importance of certain shapes, inputs, or features and
    how strongly they influence the program’s response. During training, the program
    fine-tunes these parameters (referred to as weights) through a process known as
    backpropagation, which learns to recognize patterns and make correct predictions
    by discovering the optimal combination of weights that minimizes errors. These
    weights are how our program learns to assign significance to different pieces
    of information. The more pictures we provide, the better our program will learn
    how to recognize numbers. In this example, we described the method of training
    a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Artificial neural networks are conceptualized as algorithmic models of brain
    cells that transform input data into output data. The introduction of this concept
    is attributed to McCulloch and Pitts in 1943 when they demonstrated that Boolean
    operations could be executed using “neural” elements that mimic living neurons.
    Since 1950, this field has witnessed significant advancements.
  prefs: []
  type: TYPE_NORMAL
- en: In 1958, Frank Rosenblatt introduced the Perceptron, which marked a pivotal
    moment in the field of artificial neural networks as it was able to learn and
    adapt its decision-making based on training data. Even though the Perceptron was
    a simple program with many limitations, it laid the foundation for further advancements
    in neural networks. In 1960, Bernard Widrow and Marcian Hoff developed the first
    neural network systems applied to real-world problems. They designed ADALINE (Adaptive
    Linear Neuron), which identified binary patterns, allowing it to predict the next
    bit when reading streaming bits from a phone line. MADALINE (Multiple ADALINE)
    was developed to eliminate echoes on phone lines.
  prefs: []
  type: TYPE_NORMAL
- en: In his 1974 PhD thesis, reprinted in 1994 [5], Paul Werbos proposed the development
    of reinforcement learning systems by using neural networks to approximate dynamic
    programming. Dynamic programming is an optimization approach that transforms a
    complex problem into a sequence of simpler problems. In 1986, Rumelhart, Hinton,
    and Williams rediscovered the backpropagation technique and made this fundamental
    technique broadly known with the publication of the backpropagation training algorithm
    [6].
  prefs: []
  type: TYPE_NORMAL
- en: The backpropagation algorithm structure comprises an input layer, one or more
    hidden layers, and an output layer. Each node, or artificial neuron, connects
    to another and has a weight and threshold. If the output of any node is more than
    the specified threshold value, that node is activated, transmitting data to the
    next layer of the network. Otherwise, no data is passed to the next layer of the
    network. Figure 3.3 depicts a backpropagation algorithm structure with three layers
    (input, hidden, and output).
  prefs: []
  type: TYPE_NORMAL
- en: '*Input layer*—This layer receives the input data fed into the network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hidden layer*—Neural networks encode the information learned from the training
    data using the value of the weights for the connections between the layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Output layer*—The output layer collects the predictions made in the hidden
    layers and computes the model’s prediction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![figure](../Images/3-3.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 3.3 A backpropagation model with three layers**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The input values in figure 3.3 are 0.5, 0.6, and 0.2\. Each node in the input
    and hidden layers is connected to all the nodes in the next layer, and there are
    no connections between the nodes within a particular layer. Each connection between
    nodes has a weighting factor associated with it. Initially, the nodes are connected
    with random weights. The training consists of modifying the values of these weights
    by iteratively processing a set of training examples and comparing its prediction
    to each example’s correct label. When the results are different, the weights are
    adjusted. These weight modifications are made backward (i.e., from the output
    layer through each hidden layer down to the input layer), hence the name backpropagation
    algorithm. Although it is not guaranteed, the weights will eventually converge,
    and the learning process ends. These modified weights, which are numbers between
    0 and 1 or –1 and 1, represent what the neural network learns.
  prefs: []
  type: TYPE_NORMAL
- en: The input to individual neural network nodes must be numeric and fall in the
    closed interval range of [0,1] or [–1,1], which requires normalizing the inputs
    to values between 0 and 1 or –1 and 1 for each attribute from the training examples.
    Discrete-valued attributes may be encoded such that there is one input unit per
    domain value. To illustrate, suppose we have the attribute marital status with
    the values single, married, widowed, and divorced. One possible method is to represent
    the four values as single = (1, 0, 0, 0), married = (0, 1, 0, 0), widowed = (0,
    0, 1, 0), and divorced = (0, 0, 0, 1).
  prefs: []
  type: TYPE_NORMAL
- en: For numerical data, we can apply the simple formula
  prefs: []
  type: TYPE_NORMAL
- en: Normalized Value = (Value – MIN) / (MAX – MIN)
  prefs: []
  type: TYPE_NORMAL
- en: where MIN represents the smallest value in the dataset and MAX represents the
    highest value in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider the numbers 2, 4, 5, 6, 20, 56, and 62\. The minimum value
    is 2, the maximum value is 62, and the range is 60\. Following the normalization
    procedure and rounding to the nearest hundredth yields the results in table 3.1\.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3.1 Normalization procedure
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Original number | Normalized value |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 2  | (2 – 2)/60 = 0  |'
  prefs: []
  type: TYPE_TB
- en: '| 4  | (4 – 2)/60 = 0.03  |'
  prefs: []
  type: TYPE_TB
- en: '| 5  | (5 – 2)/60 = 0.05  |'
  prefs: []
  type: TYPE_TB
- en: '| 6  | (6 – 2)/60 = 0.07  |'
  prefs: []
  type: TYPE_TB
- en: '| 20  | (20 – 2)/60 = 0.3  |'
  prefs: []
  type: TYPE_TB
- en: '| 56  | (56 – 2)/60 = 0.9  |'
  prefs: []
  type: TYPE_TB
- en: '| 62  | (62 – 2)/60 = 1  |'
  prefs: []
  type: TYPE_TB
- en: The backpropagation algorithm remains a widely used method for training supervised
    artificial neural networks. Initially, the neural network’s connections are established
    with randomly generated weights, typically between 0 and 1, connecting different
    nodes. The training process is iterative and involves presenting training examples
    to the network.
  prefs: []
  type: TYPE_NORMAL
- en: During each iteration, a labeled example is fed into the network’s input layer.
    The algorithm then computes the network’s output through a process known as forward
    propagation, which includes calculations through the hidden layers to produce
    the final output. Subsequently, the algorithm compares this output to the expected
    results or target values. When the computed output values differ from the expected
    results, the backpropagation algorithm comes into play. It applies an error-correction
    procedure by tracing back through the hidden layers toward the input layer, adjusting
    the network’s weights to minimize the error. This iterative process continues
    until the network’s performance improves and the desired accuracy is achieved.
  prefs: []
  type: TYPE_NORMAL
- en: 'While success is not assured, this process is reiterated through numerous cycles
    until one of two conditions is met: either the weights converge, allowing the
    neural network to correctly evaluate all test samples, or the neural network’s
    error falls within an acceptable threshold. Essentially, what a neural network
    “learns” boils down to a collection of numeric values between 0 and 1 (the adjusted
    weights). These adjusted weights encapsulate the essence of what an artificial
    neural network represents.'
  prefs: []
  type: TYPE_NORMAL
- en: While neural networks trained using the backpropagation algorithm have demonstrated
    their utility for various problems, they come with several inherent limitations.
    First, these networks tend to operate as enigmatic black boxes because the inner
    workings of the trained model, including the critical features it identifies,
    can be challenging to examine. This opacity stems from the complex relationships
    between the hidden layers, which consist primarily of numerical weights (typically
    between 0 and 1). Consequently, neural network models struggle to explain their
    decisions, which can be a significant drawback. In contexts where accountability
    and transparency are paramount, this limitation becomes particularly concerning.
  prefs: []
  type: TYPE_NORMAL
- en: Second, the design of a neural network is not a straightforward task. It involves
    making crucial decisions, such as determining the optimal number of hidden layers,
    configuring the connections between nodes within these layers, setting the learning
    rate for weight adjustments, selecting appropriate training data, and establishing
    robust testing and validation procedures. These design choices significantly affect
    the network’s overall performance and effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, while neural networks demand substantial computational resources, there
    is no guarantee that the training process will yield a highly effective model.
    Achieving convergence to an optimal solution is not assured, and issues like over-
    or underfitting can arise during training.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Deep learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning, often hailed as the poster child of artificial intelligence,
    has become nearly synonymous with AI itself. Anthony W. Kosner, in a 2015 *Forbes*
    article titled “Deep Learning and Machine Intelligence Will Eat The World” [7],
    and Apurv Mishra, in a 2017 statement published in *Scientific American* [8],
    asserted that deep learning had propelled AI to a point where it could match or
    even surpass human experts in fields like medicine when it came to interpreting
    visual data. In 2018, CNN reported that deep neural networks developed by industry
    giants Alibaba (BABA) and Microsoft (MSFT) had surpassed human performance on
    a Stanford University reading comprehension test [9].
  prefs: []
  type: TYPE_NORMAL
- en: However, despite its impressive moniker, it’s worth noting that many aspects
    of what we label as “deep learning” may already be familiar to us. What distinguishes
    a neural network as “deep,” as opposed to traditional backpropagation, lies primarily
    in the number of hidden layers and, occasionally, in how nodes are interconnected.
    The primary advantage of incorporating more hidden layers lies in their proven
    ability to construct a hierarchy of complex concepts from simpler ones, making
    them highly effective at discerning various characteristics. Figure 3.4 offers
    a visual representation of a typical deep neural network architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 A deep neural network architecture
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The roots of deep learning algorithms can be traced back to the work of Ivakhnenko
    and Lapa in the mid-1960s. In their 1965 report “Cybernetic Predicting Devices”
    [10], they explore approaches to pattern recognition utilizing artificial neural
    networks with hierarchical layers of nodes between the input and output layers.
    They argue that deep networks hold computational advantages over classical networks,
    particularly when dealing with problems exhibiting nonlinear characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: In contemporary deep learning, the fundamental approach remains anchored in
    the concept of the backpropagation algorithm. Data, regardless of its type, is
    typically transformed into numerical vectors within the range of 0 to 1\. The
    core of deep learning still revolves around the adjustment of weights, represented
    by hundreds of thousands of values ranging between 0 and 1\. While deep learning
    primarily operates in a supervised manner, where training data consists of numerous
    samples meticulously labeled, it’s worth noting that the field has expanded to
    encompass various learning paradigms beyond traditional supervised learning, including
    unsupervised learning, reinforcement learning, and semi-supervised learning. This
    broader spectrum of techniques empowers deep learning to tackle a wide array of
    tasks across different domains.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate, let’s embark on a straightforward project: creating a deep learning
    system capable of classifying color photographs of cats and dogs. When designing
    such a model, we may explore two prevailing architectures: convolutional neural
    networks [11] or vision transformers [12], which currently dominate the landscape
    of solutions for computer vision tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to the insights of English physician Thomas Young in 1802, we understand
    how to translate color images into a series of numbers ranging from 0 to 1\. Young’s
    work elucidated the RGB (red, green, blue) color system, which enables us to represent
    the colors in an image as combinations of these three primary colors. Consequently,
    each pixel in our images can be described by a set of three numbers, representing
    its color in the RGB spectrum. In deep learning, images are treated as vectors
    of numbers, where each number corresponds to the RGB value for a specific pixel
    within the image.
  prefs: []
  type: TYPE_NORMAL
- en: The training process follows a similar pattern of weight adjustments, which
    is a hallmark of neural networks. With sufficient high-quality training data and
    a well-designed network, our deep neural network should be capable of accurately
    identifying most of the images earmarked for testing. It’s important to note that
    we maintain separate sets for training and testing to evaluate the efficacy of
    the training process. This division allows us to validate the model’s performance
    on previously unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, the process of designing a deep learning project can be outlined
    with the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Defining the objective *—Clearly articulate the neural network’s intended
    purpose and the problem it seeks to address.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Data gathering *—Accumulate a substantial volume of high-quality data relevant
    to the project’s objectives.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Data labeling *—Assign appropriate labels to the data. This may entail manual
    labeling or other labeling methods, depending on the project’s requirements.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Data partitioning *—Segregate the labeled data into distinct sets, typically
    at least two: one for training the model and another for unbiased testing.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Data preprocessing *—Prepare the data for input into the neural network by
    normalizing and converting it into vectorized form.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Performance evaluation *—Develop a robust evaluation framework to assess the
    network’s performance, selecting pertinent metrics tailored to the specific task.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Model refinement *—Continuously improve the model by fine-tuning parameters
    and adjusting architecture to mitigate errors and prevent overfitting.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 3.4.1 The benefits of deep learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Enumerating the advantages of deep learning is a multifaceted endeavor owing
    to its remarkable adaptability. Its core strength lies in its ability to extract
    patterns and glean insights from vast, complex datasets. This ability holds significant
    value across a wide spectrum of industries. It finds utility in healthcare, facilitating
    disease diagnosis, and in finance, where it enhances risk assessment and fraud
    detection. In natural language processing, deep learning models enable machines
    to understand and generate human language, leading to breakthroughs in virtual
    assistants and language translation.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning also excels in image recognition, where it powers technologies
    like facial recognition and autonomous vehicles. These models have the capacity
    to automate tasks, resulting in heightened productivity and substantial cost savings.
    Moreover, deep learning enables businesses to provide highly personalized experiences
    to customers, tailoring recommendations, content, and services to individual preferences.
  prefs: []
  type: TYPE_NORMAL
- en: Another notable strength lies in predictive analytics, where deep learning models
    analyze historical and real-time data to predict sales, anticipate market demand,
    and make data-driven decisions. Similarly, deep learning’s impressive computer
    vision capabilities automate tasks such as image recognition, object detection,
    and quality control in manufacturing, thereby enhancing operational efficiency
    and product quality.
  prefs: []
  type: TYPE_NORMAL
- en: By overcoming data challenges once deemed insurmountable, deep learning stands
    as one of the most transformative technologies, driving innovation across various
    sectors.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.2 Limitations of deep learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s begin by considering the insights of Francois Chollet, the creator of
    the Keras deep-learning library and a key contributor to the TensorFlow machine-learning
    framework. Chollet points out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[The] deep learning model is “just” a chain of simple, continuous geometric
    transformations mapping one vector space into another. All it can do is map one
    data manifold X into another manifold Y, assuming the existence of a learnable
    continuous transform from X to Y, and the availability of a dense sampling of
    X: Y to use as training data. Most of the programs that one may wish to learn
    cannot be expressed as a continuous geometric morphing of a data manifold.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For a deeper exploration of deep learning, I recommend reading Chollet’s excellent
    book, *Deep Learning with Python* [13], from which this quote is taken.
  prefs: []
  type: TYPE_NORMAL
- en: The coupling of the terms *deep* and *learning* with computers might lead one
    to believe that computers are truly learning in a profound sense. However, as
    we’ve explored in this book, deep-learning algorithms are essentially mathematical
    formulas. Using formulas alone, we cannot define or create true intelligence.
    This is why, when *PYMNTS* magazine asked for my views on deep learning in October
    2017 [14], my response was, “Before you use the word ‘deep,’ show me the learning.”
    We will delve into this topic further in chapter 6.
  prefs: []
  type: TYPE_NORMAL
- en: In many business applications I’ve encountered, applying deep learning networks
    can be challenging due to the substantial requirement for labeled data. Numerous
    businesses not only lack the necessary amount of data but also labeled data. Even
    when the data is available, designing and training a deep network can be time-consuming,
    and such a network often underperforms when applied to data that deviates from
    its training data.
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider AlphaGo, which required millions of training scenarios
    and years of meticulous engineering by a team of experts. While it excelled in
    the game of Go, adapting it for other purposes would necessitate extensive work
    by a different team of data scientists and engineers. Expensive retraining is
    required every time you make changes to the objectives of a deep learning project.
  prefs: []
  type: TYPE_NORMAL
- en: To be fair, deep learning has benefited in recent years from advances in computational
    speed, thanks to specialized hardware designed for it. In a few specific domains,
    there is an abundance of labeled data available for training. For instance, the
    ImageNet project provides access to over 14 million images annotated by humans,
    making it a valuable resource for object recognition testing. However, for most
    problems, the challenges associated with deep learning persist.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, deep neural networks are susceptible to hacking through adversarial
    examples. By making subtle pixel alterations, one can disrupt the training of
    a deep-learning surveillance system designed to detect intruders. Even though
    creating adversarial attacks might not be straightforward, the question remains:
    should we trust a technique with such a vulnerability when it comes to mission-critical
    tasks?'
  prefs: []
  type: TYPE_NORMAL
- en: Even in fields where deep-learning algorithms demonstrate efficiency, a critical
    aspect they often lack is trustworthiness. How can one trust a system that cannot
    provide justifications for its conclusions? Deep-learning algorithms, like the
    traditional neural networks, are black boxes. Consider a scenario where a military
    operator must justify a life-and-death decision made by such a system. How can
    we permit the use of such systems in applications with significant societal implications,
    such as criminal justice or lending?
  prefs: []
  type: TYPE_NORMAL
- en: These concerns have motivated the European Commission to introduce a regulatory
    framework designed to ensure that all AI programs used in Europe can be relied
    upon to safeguard the safety and fundamental rights of individuals and businesses.
    Margrethe Vestager, Executive Vice President of the European Commission for a
    Europe Fit for the Digital Age, emphasized the importance of trust in AI, stating,
    “On Artificial Intelligence, trust is a must, not a nice to have. With these landmark
    rules, the EU is spearheading the development of new global norms to make sure
    AI can be trusted” [15].
  prefs: []
  type: TYPE_NORMAL
- en: 'This regulatory framework categorizes AI systems based on their level of risk
    and imposes corresponding restrictions. These range from an outright ban on programs
    that could pose threats to safety or livelihoods to stringent checks on those
    used in critical infrastructures, such as transportation, education, recruitment,
    credit scoring, law enforcement, criminal justice, or elections. AI systems used
    in Europe must adhere to the following requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: Appropriate datasets must be used to minimize the risk of bias and discriminatory
    outcomes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Full traceability must be ensured to allow for oversight.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detailed documentation must be readily available to explain the system’s functioning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clear and adequate information must be provided to individuals deciding whether
    to use such a system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Effective human oversight and monitoring mechanisms must be in place.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The technology must be purposeful, robust, secure, accurate, and unbiased.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s important to note that while deep learning is a prevalent AI approach,
    not all deep-learning solutions inherently fail to meet these requirements. Nevertheless,
    addressing the issues related to trust and transparency remains a significant
    challenge in the field.
  prefs: []
  type: TYPE_NORMAL
- en: As for why this field consistently garners hype, one obvious explanation is
    the use of terms like “neurons” and the presentation of neural networks as being
    biologically inspired with structures similar to that of humans. In chapter 6,
    we will elaborate on why this claim is ludicrous.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning, a machine learning approach dating back to the early
    days of cybernetics, involves learning behavior through trial and error. Its core
    principle revolves around using a system of rewards and punishments as a reinforcement
    signal. Arthur Samuel pioneered the use of reinforcement learning in his 1956
    checkers program. This program played against itself, making random moves and
    assessing the outcomes using the checkers rules to determine the best strategies.
    Reinforcement learning can be particularly beneficial in environments where you
    can clearly define rewards for positive actions, although such clarity is not
    commonly found in real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Bayesian networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In contrast to the previous AI techniques I’ve employed in the business world,
    I have limited experience with Bayesian networks. Nevertheless, I’ll provide a
    brief overview since they offer a well-established approach that can be valuable
    in projects involving probabilistic descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian networks serve as maps illustrating the interplay between various events,
    enabling us to comprehend how changes in one event can have cascading effects
    on others. In more technical terms, Bayesian networks belong to the category of
    probabilistic models employing directed acyclic graphs to depict relationships
    between variables and their probabilistic dependencies. Their roots trace back
    to Thomas Bayes and his posthumously published manuscripts in 1763\. Bayes’ theorem
    introduced the concept of conditional probability. The theorem provides a formula
    for adjusting beliefs when presented with new evidence, elucidating the likelihood
    of an event based on information concerning conditions related to that event.
    One way to express Bayes’ theorem is that the probability of event B happening,
    given that event A has occurred, multiplied by the probability of event A occurring,
    is equivalent to the probability of event A occurring, given that event B has
    occurred, multiplied by the probability of event B happening.
  prefs: []
  type: TYPE_NORMAL
- en: A Bayesian network encapsulates probabilistic relationships among variables
    of interest. It comprises a structure resembling a directed acyclic graph or belief
    network, with nodes interconnected by edges. Each node represents a variable,
    and the directed edges symbolize the conditional dependence between these variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, consider a Bayesian network for weather prediction, where a node
    representing clouds is linked to a node representing rain with an arrow pointing
    from clouds to rain. If the clouds node is set to 40%, signifying a 40% likelihood
    of cloudy conditions, the value of the rain variable would be contingent on the
    cloud’s variable. Perhaps, if it’s cloudy, there’s a 30% chance of rain, whereas
    if it’s not cloudy, there’s only a 5% chance. The model can handle these probabilities
    and dependencies by computing the likelihood of various scenarios: cloudy and
    not raining, cloudy and raining, not cloudy and raining, or not cloudy and not
    raining. In more complex Bayesian networks with hundreds of nodes and dependencies,
    they can be used to infer the overall significance of a particular variable in
    determining outcomes.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.5 is a suite of Bayesian networks developed by researchers at the St.
    Petersburg Coastal and Marine Science Center [16] to (a) generate scenarios of
    total water level, (b) forecast storm effects, and (c) predict magnitudes of beach
    recovery. The Bayesian networks incorporate topographic, bathymetric, and shoreline
    data available from the historical and post-Hurricane Sandy research programs
    at Fire Island. The Bayesian networks generate predictions in the form of probability
    of coastal change.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-5.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 An example of a Bayesian network
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Bayesian networks made their debut in the late 1970s, serving as a means to
    model distributed processing in reading comprehension. This modeling approach
    aimed to amalgamate semantical expectations with perceptual evidence, forging
    coherent interpretations [17]. Often hailed as the progenitor of Bayesian networks,
    Judea Pearl elucidates in *The Book of Why* [18] how one should design software
    for reasoning under uncertainty:'
  prefs: []
  type: TYPE_NORMAL
- en: 'I was firmly convinced that any artificial intelligence system should emulate
    what we comprehend about human neural information processing. Machine reasoning
    under uncertainty, I believed, should be built upon a similar message-passing
    architecture. The question that vexed me was: What should these messages be? It
    took several months, but I eventually discerned that the messages took two forms—conditional
    probabilities in one direction and likelihood ratios in the other.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Pearl envisioned a hierarchical network, with each node transmitting its belief
    to neighboring nodes. The recipient node processed the incoming information in
    one of two ways: updating its beliefs using conditional probabilities when the
    message flowed from parent to child or adjusting beliefs by multiplication with
    a likelihood ratio when the message moved from child to parent.'
  prefs: []
  type: TYPE_NORMAL
- en: Despite their merits, Bayesian networks have limitations, akin to other machine
    learning methods. They operate on precise probabilities, which may not align with
    real-world scenarios characterized by imprecise knowledge necessitating confidence
    intervals, such as between 56% and 62%. Another substantial constraint emerges
    when attempting to create Bayesian network structures in complex domains. While
    experts can define simple Bayesian networks, in many fields, constructing such
    networks proves a formidable task for human operators. The ideal scenario involves
    computers autonomously learning network structures, parameters, and conditional
    probabilities from data, yet this remains a significant challenge in most practical
    applications. The absence of a universally effective method for constructing Bayesian
    networks from data implies that their creation often demands extensive time and
    human involvement.
  prefs: []
  type: TYPE_NORMAL
- en: 3.6 Unsupervised learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unsupervised learning is an approach within the field of artificial intelligence
    where algorithms are employed to discern patterns and structures within data without
    relying on labeled outputs. The primary objective is to group data based on inherent
    similarities, differences, or concealed structures. For instance, unsupervised
    learning techniques like clustering prove invaluable in the retail sector, as
    they enable the grouping of customers with similar purchasing behaviors. This
    categorization can unveil distinct customer segments, such as budget-conscious
    shoppers, luxury enthusiasts, and occasional buyers, empowering retailers to fine-tune
    marketing strategies, provide tailored product recommendations, and optimize inventory
    management more effectively to meet the diverse needs of their customer base.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning excels at extracting meaningful insights and knowledge
    from unorganized or unlabeled data, aiding organizations in making informed decisions.
    To illustrate the potency of unsupervised learning, consider its role in combatting
    money laundering—a critical societal issue that demands the application of this
    technique.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the implementation of stringent legislation, criminals and terrorists
    have proven to be resourceful and persistent in their illicit endeavors. According
    to data from the United Nations Office on Drugs and Crime, the estimated global
    amount of money laundered in a single year range from 2% to 5% of the world’s
    gross domestic product, which translates to a staggering $800 billion to $2 trillion
    in current US dollars as of 2022 [19].
  prefs: []
  type: TYPE_NORMAL
- en: Money laundering operations are primarily designed to conceal unlawfully acquired
    funds and obscure their origins. Typically, this is achieved through a series
    of complex financial transactions intentionally structured to be challenging to
    trace. Money launderers often employ ingenious tactics, frequently commingling
    illegal transactions with those of legitimate enterprises. They further complicate
    matters by concealing ownership structures, such as the creation of trusts and
    offshore companies in jurisdictions with lax regulatory oversight. Additionally,
    they use modern tools and technologies, including cryptocurrencies, currency exchanges,
    international money transfers, and even cash smuggling, to facilitate their activities.
    The proliferation of internet technology has only exacerbated these challenges,
    as online auctions, gambling, banking, and peer-to-peer payment apps provide additional
    avenues for them to safeguard their anonymity.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following money laundering case [20] study portrays a scenario that developed
    over several years and involved numerous individuals:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Tom works as a government employee. He was able to support his family until
    he became an addict to gambling. In his role, he had the power to propose and
    approve projects for private companies competing in the public sector. His gambling
    habit pushed to corruption, and large businessmen were willing to pay him well
    for the guarantee of government business, and Tom became rich very quickly through
    his corrupt activities. Tom’s friend Gina, who owned an exchange and tourism company,
    was willing to help him launder the bribes that he was receiving. She used her
    employees as “straw men” to create a number of different bank accounts through
    which funds could be laundered—more than US$4,000,000 was laundered in total through
    such accounts. However, the cash payments and subsequent transfer offshore risked
    attracting attention, and so Tom developed a more sophisticated laundering method—a
    fruit-delivery company. This company, which was owned by Gina’s husband, laundered
    US$2,700,000 in three months, disguising the transactions by creating false invoices
    which were settled by the businessmen on Tom’s instructions. The total amount
    of money Tom laundered was estimated to be in the range of US$1,000,000,000\.
    It is worth noting that the disclosures by the institutions took place because
    of the simple initial laundering scheme, whereas the later scheme involving an
    established company appeared to have had little risk of disclosure.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Supervised learning methods prove inadequate when combating money laundering
    for several compelling reasons. One significant challenge arises from the difficulty,
    and sometimes impossibility, of creating effective training and testing datasets.
    Money laundering schemes are not isolated, discrete events. Criminals operate
    meticulously, using complex strategies to conceal their activities. These elaborate
    scenarios cannot be accurately represented through labeled samples typically used
    in supervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, supervised techniques lack adaptability, whereas criminal organizations
    are highly agile, constantly inventing complex tactics to elude detection. They
    operate under the assumption that their financial transactions are under scrutiny,
    meticulously crafting each transaction to mimic legitimate activities. For instance,
    businesses like restaurants and nightclubs, known for dealing primarily in cash,
    can deposit large sums without arousing suspicion from financial institutions.
    This makes them well-suited to money laundering since they can handle substantial
    cash volumes without triggering red flags.
  prefs: []
  type: TYPE_NORMAL
- en: As a result of supervised learning’s ineffectiveness in the fight against money
    laundering, current approaches rely heavily on predefined rules set by anti-money
    laundering experts to flag suspicious transactions. However, as previously discussed,
    business rules have significant limitations in dynamic environments. Consequently,
    current anti-money laundering solutions suffer from a high rate of false positives.
    Some estimates even suggest that as much as 90% of compliance analysts’ time is
    spent investigating the numerous false alarms generated by such systems.
  prefs: []
  type: TYPE_NORMAL
- en: Effectively combating money laundering necessitates the application of unsupervised
    learning techniques. One of the primary unsupervised learning methods is clustering,
    which endeavors to categorize unlabeled data into groups or clusters. These clusters
    are defined by the similarity among examples within each group, enabling the identification
    of anomalies. For instance, a collection of wire transfers might be grouped based
    on factors like frequency, dollar amount, and the type of beneficiary. Analysis
    could reveal connections between transfers originating from related brokerage
    houses, industrial firms, or money transmitters. These clustered transfers might
    also exhibit common financial traits, such as shared accounts or types of accounts
    and involvement with the same financial organizations and individuals. When investigators
    detect irregular patterns within the activities of a manufacturing firm or an
    insurance company, it prompts a closer examination to determine whether these
    businesses might be fronts for money laundering.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning, while a powerful tool in machine learning, comes with
    its own limitations. The most critical limitation is the evaluation of unsupervised
    models. Unlike supervised learning, where we can rely on labeled output for performance
    assessment, unsupervised learning lacks such clear metrics or guidance for evaluating
    the quality of learned representations. Additionally, unsupervised learning models
    can be quite sensitive to the choice of hyperparameters and initializations, making
    it challenging to determine the optimal settings for a particular problem.
  prefs: []
  type: TYPE_NORMAL
- en: 3.7 So, what is artificial intelligence?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After exploring AI and its diverse techniques, it’s natural to inquire about
    the contemporary landscape of AI. Remarkably, I still adhere to the same definition
    I articulated in my 1988 PhD thesis [21]:'
  prefs: []
  type: TYPE_NORMAL
- en: Artificial Intelligence is essentially a set of human-coded mathematical algorithms,
    primarily rooted in probabilities and statistics. These algorithms serve the purpose
    of scrutinizing data and deriving insightful patterns and interconnections among
    attributes and concepts.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data mining stands out as a prominent AI technique for analyzing vast datasets
    and uncovering patterns. Its popularity spans various domains due to its ability
    to provide human-readable explanations of the knowledge it extracts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Artificial neural networks represent another essential AI technique capable
    of learning from extensive datasets. These networks consist of interconnected
    layers of nodes, and during training, the weights connecting these nodes are adjusted
    to encode information about patterns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning, an extension of neural networks, introduces networks with significantly
    more layers than earlier models. However, it’s important to note that the increased
    number of layers doesn’t necessarily equate to higher intelligence. Instead, it
    allows deep learning networks to capture complex hierarchical patterns and representations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayesian networks, with origins dating back over 260 years, are a type of statistical
    model. They prove particularly valuable for unsupervised learning techniques like
    clustering, making them a practical choice for handling unlabeled data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised learning is a powerful AI technique where algorithms are utilized
    to identify patterns and structures within data without the need for labeled outputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
