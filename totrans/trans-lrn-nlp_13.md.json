["```py\nimport pandas as pd\n\ntrain_df = pd.DataFrame(data=data)\ntrain_df.to_csv(\"albert_dataset.csv\")\n```", "```py\nfrom transformers import AlbertTokenizer                          ❶\ntokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")     ❷\n```", "```py\nfrom transformers import AlbertForMaskedLM                    ❶\n\nmodel = AlbertForMaskedLM.from_pretrained(\"albert-base-v2\")   ❷\n\nprint(\"Number of parameters in ALBERT model:\")\nprint(model.num_parameters())\n```", "```py\nfrom transformers import LineByLineTextDataset\n\ndataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=\"albert_dataset.csv\",\n    block_size=128)                    ❶\n```", "```py\nfrom transformers import DataCollatorForLanguageModeling\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=True, mlm_probability=0.15)       ❶\n```", "```py\nfrom transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"albert\",\n    overwrite_output_dir=True,\n    num_train_epochs=10,\n    per_gpu_train_batch_size=16,\n    save_total_limit=1,\n)\n```", "```py\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=dataset,\n    prediction_loss_only=True,\n)\n```", "```py\nimport time\nstart = time.time()\ntrainer.train()\nend = time.time()\nprint(\"Number of seconds for training:\")\nprint((end-start))\n```", "```py\ntrainer.save_model(\"albert_fine-tuned\")\n```", "```py\nfrom transformers import pipeline\n\nfill_mask = pipeline(                                      ❶\n    \"fill-mask\",\n    model=\"albert_fine-tuned\",\n    tokenizer=tokenizer\n)\n\nprint(fill_mask(\"The author fails to [MASK] the plot.\"))   ❷\n```", "```py\n[{'sequence': '[CLS] the author fails to describe the plot.[SEP]', 'score': 0.07632581889629364, 'token': 4996}, {'sequence': '[CLS] the author fails to appreciate the plot.[SEP]', 'score': 0.03849967569112778, 'token': 8831}, {'sequence': '[CLS] the author fails to anticipate the plot.[SEP]', 'score': 0.03471902385354042, 'token': 27967}, {'sequence': '[CLS] the author fails to demonstrate the plot.[SEP]', 'score': 0.03338927403092384, 'token': 10847}, {'sequence': '[CLS] the author fails to identify the plot.[SEP]', 'score': 0.032832834869623184, 'token': 5808}]\n```", "```py\nfor param in model.albert.parameters():\n    param.requires_grad = False\n```", "```py\n!git clone --branch v3.0.1 https:/ /github.com/huggingface/transformers  ❶\n!cd transformers\n!pip install -r transformers/examples/requirements.txt                  ❷\n!pip install transformers==3.0.1                                        ❸\n```", "```py\n!mkdir GLUE\n!python transformers/utils/download_glue_data.py --data_dir GLUE --tasks all❶\n```", "```py\n!ls GLUE/STS-B \n```", "```py\nLICENSE.txt  dev.tsv  original    readme.txt  test.tsv  train.tsv\n```", "```py\n!head GLUE/STS-B/train.tsv\n```", "```py\nindex genre   filename year old_index source1 source2 sentence1 sentence2 score\n\n0    main-captions    MSRvid    2012test    0001    none    none    A plane is taking off.    An air plane -is taking off.    5.000\n\n1    main-captions    MSRvid    2012test    0004    none    none    A man is playing a large flute.    A man is playing a flute.    3.800\n\n2    main-captions    MSRvid    2012test    0005    none    none    A man is spreading shreddedcheese on a pizza.    A man is spreading shredded cheese on an uncooked pizza.    3.800\n\n3    main-captions    MSRvid    2012test    0006    none    none    Three men are playing chess.    Two men are playing chess.    2.600\n\n4    main-captions    MSRvid    2012test    0009    none    none    A man is playing the cello.A man seated is playing the cello.    4.250\n\n5    main-captions    MSRvid    2012test    0011    none    none    Some men are fighting.    Two men are fighting.    4.250\n\n6    main-captions    MSRvid    2012test    0012    none    none    A man is smoking.    A man is skating.    0.500\n\n7    main-captions    MSRvid    2012test    0013    none    none    The man is playing the piano.    The man is playing the guitar.    1.600\n\n8    main-captions    MSRvid    2012test    0014    none    none    A man is playing on a guitar and singing.    A woman is playing an acoustic guitar and singing.    2.200\n```", "```py\n%%time                                                            ❶\n!python transformers/examples/text-classification/run_glue.py --model_name_or_path bert-base-cased --task_name STS-B --do_train --do_eval --data_dir GLUE/STS-B/ --max_seq_length 256 --per_gpu_train_batch_size 32 --learning_rate 2e-5 --num_train_epochs 3.0 --output_dir /tmp/STS-B/\n```", "```py\n!cat /tmp/STS-B/eval_results_sts-b.txt\n```", "```py\neval_loss = 0.493795601730334\neval_pearson = 0.8897041761974835\neval_spearmanr = 0.8877572577691144\neval_corr = 0.888730716983299\n```", "```py\n!python transformers/examples/text-classification/run_glue.py --model_name_or_path bert-base-cased --task_name QQP --do_train --do_eval --data_dir GLUE/QQP/ --max_seq_length 256 --per_gpu_train_batch_size 32 --learning_rate 2e-5 --num_train_epochs 1 --output_dir /tmp/QQP/\n```", "```py\n!cat /tmp/QQP/eval_results_qqp.txt\n```", "```py\neval_loss = 0.24864352908579548\neval_acc = 0.8936433341578036\neval_f1 = 0.8581700639883898\neval_acc_and_f1 = 0.8759066990730967\nepoch = 1.0\n```", "```py\nfrom transformers import BertForSequenceClassification, BertConfig     ❶\n\nqqp_model = BertForSequenceClassification.from_pretrained(\"/tmp/QQP\")  ❷\n```", "```py\nshared_encoder = getattr(qqp_model, \"bert\")                  ❶\n\nconfiguration = BertConfig()\nconfiguration.vocab_size = qqp_model.config.vocab_size       ❷\nconfiguration.num_labels = 1                                 ❸\n\nstsb_model = BertForSequenceClassification(configuration)    ❹\n\nsetattr(stsb_model, \"bert\", shared_encoder)                  ❺\n```", "```py\nstsb_model.save_pretrained(\"/tmp/STSB_pre\")\n```", "```py\n!cp /tmp/QQP/vocab.txt /tmp/STSB_pre \n```", "```py\n!python transformers/examples/text-classification/run_glue.py --model_name_or_path /tmp/STSB_pre --task_name STS-B --do_train --do_eval --data_dir GLUE/STS-B/ --max_seq_length 256 --per_gpu_train_batch_size 32 --learning_rate 2e-5 --num_train_epochs 3 --output_dir /tmp/STS-B/\n```", "```py\n!cat /tmp/STS-B/eval_results_sts-b.txt\n```", "```py\neval_loss = 0.49737201514158474\neval_pearson = 0.8931606380447263\neval_spearmanr = 0.8934618150816026\neval_corr = 0.8933112265631644\nepoch = 3.0\n```", "```py\npip install adapter-transformers\n```", "```py\nfrom transformers import BertForSequenceClassification, BertTokenizer\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\") ❶\nmodel.load_adapter(\"sentiment/sst-2@ukp\")                                  ❷\n```", "```py\nimport torch\n\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")            ❶\ntokensA = tokenizer.tokenize(\"That was an amazing contribution, good!\")   ❷\ninput_tensorA = torch.tensor([tokenizer.convert_tokens_to_ids(tokensA)])\ntokensB = tokenizer.tokenize(\"That is bad for the environment.\")          ❸\ninput_tensorB = torch.tensor([tokenizer.convert_tokens_to_ids(tokensB)])\noutputsA = model(input_tensorA,adapter_names=['sst-2'])                   ❹\noutputsB = model(input_tensorB,adapter_names=['sst-2'])                   ❺\nprint(\"The prediction for sentence A - That was an amazing contribution, good! - is:\")\nprint(torch.nn.functional.softmax(outputsA[0][0]))                        ❻\nprint(\"The prediction for sentence B - That is very bad for the environment. - is:\")\nprint(torch.nn.functional.softmax(outputsB[0][0]))                        ❼\n```", "```py\nThe prediction for sentence A - That was an amazing contribution, good! - is:\ntensor([0.0010, 0.9990], grad_fn=<SoftmaxBackward>)\nThe prediction for sentence B - That is very bad for the environment. - is:\ntensor([0.8156, 0.1844], grad_fn=<SoftmaxBackward>)\n```"]