- en: front matter
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: preface
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A useful piece of feedback that I got from a reviewer of this book was that
    it became a “cheat code” for them to scale the steep MLOps learning curve. I hope
    that the content of this book will help you become a better informed practitioner
    of machine learning engineering and data science, as well as a more productive
    contributor to your projects, your team, and your organization.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'In 2021, major technology companies are vocal about their efforts to “democratize”
    artificial intelligence (AI) by making technologies like deep learning more accessible
    to a broader population of scientists and engineers. Regrettably, the democratization
    approach taken by the corporations focuses too much on core technologies and not
    enough on the practice of delivering AI systems to end users. As a result, machine
    learning (ML) engineers and data scientists are well prepared to create experimental,
    proof-of-concept AI prototypes but fall short in successfully delivering these
    prototypes to production. This is evident from a wide spectrum of issues: from
    unacceptably high failure rates of AI projects to ethical controversies about
    AI systems that make it to end users. I believe that, to become successful, the
    effort to democratize AI must progress beyond the myopic focus on core, enabling
    technologies like Keras, PyTorch, and TensorFlow. *MLOps* emerged as a unifying
    term for the practice of taking experimental ML code and running it effectively
    in production. Serverless ML is the leading cloud-native software development
    model for ML and MLOps, abstracting away infrastructure and improving productivity
    of the practitioners.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: I also encourage you to make use of the Jupyter notebooks that accompany this
    book. The DC taxi fare project used in the notebook code is designed to give you
    the practice you need to grow as a practitioner. Happy reading and happy coding!
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: acknowledgments
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I am forever grateful to my daughter, Sophia. You are my eternal source of happiness
    and inspiration. My wife, Alla, was boundlessly patient with me while I wrote
    my first book. You were always there to support me and to cheer me along. To my
    father, Mikhael, I wouldn’t be who I am without you.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: 'I also want to thank the people at Manning who made this book possible: Marina
    Michaels, my development editor; Frances Buontempo, my technical development editor;
    Karsten Strøbaek, my technical proofreader; Deirdre Hiam, my project editor; Michele
    Mitchell, my copyeditor; and Keri Hales, my proofreader.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 'Many thanks go to the technical peer reviewers: Conor Redmond, Daniela Zapata,
    Dianshuang Wu, Dimitris Papadopoulos, Dinesh Ghanta, Dr. Irfan Ullah, Girish Ahankari,
    Jeff Hajewski, Jesús A. Juárez-Guerrero, Trichy Venkataraman Krishnamurthy, Lucian-Paul
    Torje, Manish Jain, Mario Solomou, Mathijs Affourtit, Michael Jensen, Michael
    Wright, Pethuru Raj Chelliah, Philip Kirkbride, Rahul Jain, Richard Vaughan, Sayak
    Paul, Sergio Govoni, Srinivas Aluvala, Tiklu Ganguly, and Todd Cook. Your suggestions
    helped make this a better book.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: about this book
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Thank you for purchasing *MLOps Engineering at Scale*.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Who should read this book
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To get the most value from this book, you’ll want to have existing skills in
    data analysis with Python and SQL, as well as have some experience with machine
    learning. I expect that if you are reading this book, you are interested in developing
    your expertise as a machine learning engineer, and you are planning to deploy
    your machine learning—based prototypes to production.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: This book is for information technology professionals or those in academia who
    have had some exposure to machine learning and are working on or are interested
    in launching a machine learning system in production. There is a refresher on
    machine learning prerequisites for this book in appendix A. Keep in mind that
    if you are brand new to machine learning you may find that studying both machine
    learning and cloud-based infrastructure for machine learning at the same time
    can be overwhelming.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: If you are a software or a data engineer, and you are planning on starting a
    machine learning project, this book can help you gain a deeper understanding of
    the machine learning project life cycle. You will see that although the practice
    of machine learning depends on traditional information technologies (i.e., computing,
    storage, and networking), it is different from the traditional information technology
    in practice. The former is significantly more experimental and more iterative
    than you may have experienced as a software or a data professional, and you should
    be prepared for the outcomes to be less known in advance. When working with data,
    the machine learning practice is more like the scientific process, including forming
    hypotheses about data, testing alternative models to answer questions about the
    hypothesis, and ranking and choosing the best performing models to launch atop
    your machine learning platform.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: If you are a machine learning engineer or practitioner, or a data scientist,
    keep in mind that this book is not about making you a better researcher. The book
    is not written to educate you about the frontiers of science in machine learning.
    This book also will not attempt to reteach you the machine learning basics, although
    you may find the material in appendix A, targeted at information technology professionals,
    a useful reference. Instead, you should expect to use this book to become a more
    valuable collaborator on your machine learning team. The book will help you do
    more with what you already know about data science and machine learning so that
    you can deliver ready-to-use contributions to your project or your organization.
    For example, you will learn how to implement your insights about improving machine
    learning model accuracy and turn them into production-ready capabilities.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'How this book is organized: A road map'
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This book is composed of three parts. In part 1, I chart out the landscape of
    what it takes to put a machine learning system in production, describe an engineering
    gap between experimental machine learning code and production machine learning
    systems, and explain how serverless machine learning can help bridge the gap.
    By the end of part 1, I’ll have taught you how to use serverless features of a
    public cloud (Amazon Web Services) to get started with a real-world machine learning
    use case, prepare a working machine learning data set for the use case, and ensure
    that you are prepared to apply machine learning to the use case.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 1 presents a broad view on the field on machine learning systems engineering
    and what it takes to put the systems in production.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 2 introduces you to the taxi trips data set for the Washington, DC,
    municipality and teaches you how to start using the data set for machine learning
    in the Amazon Web Services (AWS) public cloud.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 3 applies the AWS Athena interactive query service to dig deeper into
    the data set, uncover data quality issues, and then address them through a rigorous
    and principled data quality assurance process.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 4 demonstrates how to use statistical measures to summarize data set
    samples and to quantify their similarity to the entire data set. The chapter also
    covers how to pick the right size for your test, training, and validation data
    sets and use distributed processing in the cloud to prepare the data set samples
    for machine learning.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In part 2, I teach you to use the PyTorch deep learning framework to develop
    models for a structured data set, explain how to distribute and scale up machine
    learning model training in the cloud, and show how to deploy trained machine learning
    models to scale with user demand. In the process, you’ll learn to evaluate and
    assess the performance of alternative machine learning model implementations and
    how to pick the right one for the use case.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 5 covers the PyTorch fundamentals by introducing the core tensor application
    programming interface (API) and helping you gain a level of fluency with using
    the API.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 6 focuses on the deep learning aspects of PyTorch, including support
    for automatic differentiation, alternative gradient descent algorithms, and supporting
    utilities.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 7 explains how to scale up your PyTorch programs by teaching about the
    graphical processing unit (GPU) features and how to take advantage of them to
    accelerate your deep learning code.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 8 teaches about data parallel approaches for distributed PyTorch training
    and covers, in-depth, the distinction between traditional, parameter, server-based
    approaches and the ring-based distributed training (e.g., Horovod).
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In part 3, I introduce you to the battle-tested techniques of machine learning
    practitioners and cover feature engineering, hyperparameter tuning, and machine
    learning pipeline assembly. By the conclusion of this book, you will have set
    up a machine learning platform that ingests raw data, prepares it for machine
    learning, applies feature engineering, and trains high-performance, hyperparameter-tuned
    machine learning models.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 9 explores the use cases around feature selection and feature engineering,
    using case studies to build intuition about the features that can be selected
    or engineered for the DC taxi data set.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 10 teaches how to eliminate boilerplate engineering code in your DC
    taxi PyTorch model implementation by adopting a framework called PyTorch Lightning.
    Also, the chapter navigates through the steps required to train, validate, and
    test your enhanced deep learning model.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 11 integrates your deep learning model with an open-source hyperparameter
    optimization framework called Optuna, helping you train multiple models based
    on alternative hyperparameter values, and then ranking the trained models according
    to their loss and metric performance.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 12 packages your deep learning model implementation into a Docker container
    in order to run it through the various stages of the entire machine learning pipeline,
    starting from the development data set all the way to a trained model ready for
    production deployment.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: About the code
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can access the code for this book from my Github repository: [github.com/osipov/smlbook](http://github.com/osipov/smlbook).
    The code in this repository is packaged as Jupyter notebooks and is designed to
    be used in a Linux-based Jupyter notebook environment. This means that you have
    options when it comes to how you can execute the code. If you have your own, local
    Jupyter environment, for example, with the Jupyter native client (JupyterApp:
    [https://github.com/jupyterlab/jupyterlab_app](https://github.com/jupyterlab/jupyterlab_app))
    or a Conda distribution ([https://jupyter.org/install](https://jupyter.org/install)),
    that’s great! If you do not use a local Jupyter distribution, you can run the
    code from the notebooks using a cloud-based service such as Google Colab or Binder.
    My Github repository README.md file includes badges and hyperlinks to help you
    launch chapter-specific notebooks in Google Colab.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: I strongly urge you to use a local Jupyter installation as opposed to a cloud
    service, especially if you are worried about the security of your AWS account
    credentials. Some steps of the code will require you to use your AWS credentials
    for tasks like creating storage buckets, launching AWS Glue extract-transform-load
    (ETL) jobs, and more. The code for chapter 12 must be executed on node with Docker
    installed, so I recommend planning to use a local Jupyter installation on a laptop
    or a desktop where you have sufficient capacity to install Docker. You can find
    out more about Docker installation requirements in appendix B.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: liveBook discussion forum
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Purchase of *MLOps for Engineering at Scale* includes free access to liveBook,
    Manning’s online reading platform. Using liveBook’s exclusive discussion features,
    you can attach comments to the book globally or to specific sections or paragraphs.
    It’s a snap to make notes for yourself, ask and answer technical questions, and
    receive help from the author and other users.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: To access the forum, go to [https://livebook.manning.com/#!/book/mlops-engineering-at-scale/discussion](https://livebook.manning.com/#!/book/mlops-engineering-at-scale/discussion).
    Be sure to join the forum and say hi! You can also learn more about Manning’s
    forums and the rules of conduct at [https://livebook.manning.com/#!/discussion](https://livebook.manning.com/#!/discussion).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Manning’s commitment to our readers is to provide a venue where a meaningful
    dialogue between individual readers and between readers and the author can take
    place. It is not a commitment to any specific amount of participation on the part
    of the author, whose contribution to the forum remains voluntary (and unpaid).
    We suggest you try asking the author some challenging questions lest his interest
    stray! The forum and the archives of previous discussions will be accessible from
    the publisher’s website as long as the book is in print.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: about the author
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| ![250Osipov](Images/Osipov.png)  | Carl Osipov has been working in the information
    technology industry since 2001, with a focus on projects in big data analytics
    and machine learning in multi-core, distributed systems, such as service-oriented
    architecture and cloud computing platforms. While at IBM, Carl helped IBM Software
    Group to shape its strategy around the use of Docker and other container-based
    technologies for serverless cloud computing using IBM Cloud and Amazon Web Services.
    At Google, Carl learned from the world’s foremost experts in machine learning
    and helped manage the company’s efforts to democratize artificial intelligence
    with Google Cloud and TensorFlow. Carl is an author of over 20 articles in professional,
    trade, and academic journals; an inventor with six patents at USPTO; and the holder
    of three corporate technology awards from IBM. |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| ![250Osipov](Images/Osipov.png)  | Carl Osipov自2001年以来一直在信息技术行业工作，专注于大数据分析和多核分布式系统中的机器学习项目，比如面向服务的体系结构和云计算平台。在IBM期间，卡尔帮助IBM软件集团塑造了其围绕使用Docker和其他基于容器的技术进行无服务器云计算的策略，使用了IBM
    Cloud和Amazon Web Services。在Google，卡尔向世界顶尖的机器学习专家学习，并帮助管理公司努力通过Google Cloud和TensorFlow实现人工智能的大众化。卡尔是《专业，贸易和学术期刊》上的20多篇文章的作者；美国专利与商标局的六项专利的发明人；以及IBM获得的三项企业技术奖的获得者。
    |'
- en: about the cover illustration
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于封面插图
- en: The figure on the cover of *MLOps Engineering at Scale* is captioned “Femme
    du Thibet,” or a woman of Tibet. The illustration is taken from a collection of
    dress costumes from various countries by Jacques Grasset de Saint-Sauveur (1757—1810),
    titled *Costumes de Différents Pays*, published in France in 1797\. Each illustration
    is finely drawn and colored by hand. The rich variety of Grasset de Saint-Sauveur’s
    collection reminds us vividly of how culturally apart the world’s towns and regions
    were just 200 years ago. Isolated from each other, people spoke different dialects
    and languages. In the streets or in the countryside, it was easy to identify where
    they lived and what their trade or station in life was just by their dress.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*MLOps规模工程*封面上的图画标题为“Femme du Thibet”，即一位来自西藏的妇女。这幅插图取自雅克·格拉塞·德·圣索维尔（1757-1810）的《不同国家的服装》系列，该系列于1797年在法国出版。每幅插图都是精细绘制和手工上色的。格拉塞·德·圣索维尔的收藏丰富多样，生动地提醒了我们200年前世界各地城镇和地区在文化上的巨大差异。人们相互隔离，说着不同的方言和语言。在街上或在乡间地区，可以通过服装轻松地辨别他们居住的地方，以及他们的行业或社会地位。'
- en: The way we dress has changed since then and the diversity by region, so rich
    at the time, has faded away. It is now hard to tell apart the inhabitants of different
    continents, let alone different towns, regions, or countries. Perhaps we have
    traded cultural diversity for a more varied personal life—certainly for a more
    varied and fast-paced technological life.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 自那时起，我们的着装方式已经发生了变化，当时地区的多样性已经消失。现在很难区分不同大陆的居民，更不用说不同的城镇，地区或国家。也许我们已经以文化多样性换取了更加多样化的个人生活——当然也换取了更加多样化和快节奏的技术生活。
- en: At a time when it is hard to tell one computer book from another, Manning celebrates
    the inventiveness and initiative of the computer business with book covers based
    on the rich diversity of regional life of two centuries ago, brought back to life
    by Grasset de Saint-Sauveur’s pictures.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在难以辨别一本电脑书与另一本之际，Manning通过以格拉塞·德·圣索维尔的图片为基础的书籍封面，庆祝计算机商业的独创性和主动性，并将两个世纪前地区生活丰富多样性的丰盛多样性重新带回生活。
