["```py\nimport gpytorch\n\nrbf_kernel = gpytorch.kernels.RBFKernel()\n```", "```py\n>>> rbf_kernel(torch.tensor([0.]), torch.tensor([0.1])).evaluate().item()\n0.9896470904350281\n```", "```py\n>>> rbf_kernel(torch.tensor([0.]), torch.tensor([10.])).evaluate().item()\n0.0\n```", "```py\nrbf_kernel = gpytorch.kernels.RBFKernel()\nrbf_kernel.lengthscale = 100               ❶\n```", "```py\nimport torch\nfrom torchvision import datasets, transforms\n\ntransform = transforms.Compose([                               ❶\n    transforms.ToTensor(),                                     ❶\n    transforms.Normalize((0.1307,), (0.3081,))                 ❶\n])                                                             ❶\n\ndataset = datasets.MNIST(                                      ❷\n    \"../data\", train=True, download=True, transform=transform  ❷\n)                                                              ❷\n\ntrain_x = dataset.data.view(-1, 28 * 28)                       ❸\n```", "```py\nplt.figure(figsize=(8, 8))\n\nplt.imshow(train_x[0, :].view(28, 28));    ❶\n```", "```py\n>>> train_x[0, :]\ntensor([ 0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,\n        18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n       253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253, 253,\n       253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253, 253,\n       198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253, 205,\n        11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,  90,\n[output truncated]\n```", "```py\nind1 = 304    ❶\nind2 = 786    ❷\nind3 = 4      ❸\n```", "```py\n>>> rbf_kernel(train_x[[ind1, ind2, ind3], :]).evaluate()\ntensor([[1.0000e+00, 4.9937e-25, 0.0000e+00],\n        [4.9937e-25, 1.0000e+00, 0.0000e+00],\n        [0.0000e+00, 0.0000e+00, 1.0000e+00]], ...)\n```", "```py\nimport torch\nfrom torchvision import datasets, transforms\n```", "```py\ntransform = transforms.Compose([\n    transforms.ToTensor(),                       ❶\n    transforms.Normalize((0.1307,), (0.3081,))   ❷\n])\n```", "```py\ndataset1 = datasets.MNIST(                                       ❶\n    \"../data\", train=True, download=True, transform=transform    ❶\n)                                                                ❶\n\ndataset2 = datasets.MNIST(                                       ❷\n    \"../data\", train=False, download=True, transform=transform   ❷\n)                                                                ❷\n```", "```py\ntrain_x = dataset1.data[:1000, ...].view(1000, -1)\n➥.to(torch.float)                   ❶\ntrain_y = dataset1.targets[:1000]    ❶\n\ntest_x = dataset2.data[:500, ...].view(500, -1)\n➥.to(torch.float)                   ❷\ntest_y = dataset2.targets[:500]      ❷\n```", "```py\nclass GPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super().__init__(train_x, train_y, likelihood)\n        self.mean_module = gpytorch.means.ConstantMean()                  ❶\n        self.covar_module = gpytorch.kernels.ScaleKernel(                 ❷\n            gpytorch.kernels.RBFKernel()                                  ❷\n        )                                                                 ❷\n\n    def forward(self, x):                                                 ❸\n        mean_x = self.mean_module(x)                                      ❸\n        covar_x = self.covar_module(x)                                    ❸\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x) ❸\n```", "```py\nlikelihood = gpytorch.likelihoods.GaussianLikelihood()     ❶\nmodel = GPModel(train_x, train_y, likelihood)              ❶\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)  ❷\nmll = gpytorch.mlls.ExactMarginalLogLikelihood             ❷\n➥(likelihood, model)                                      ❷\n\nmodel.train()                                              ❸\nlikelihood.train()                                         ❸\n\nfor i in tqdm(range(500)):                                 ❹\n    optimizer.zero_grad()                                  ❹\n\n    output = model(train_x)                                ❹\n    loss = -mll(output, train_y)                           ❹\n\n    loss.backward()                                        ❹\n    optimizer.step()                                       ❹\n\nmodel.eval()                                               ❺\nlikelihood.eval()                                          ❺\n```", "```py\nwith torch.no_grad():\n    mean_preds = model(test_x).mean\n\nprint(torch.mean(torch.abs(mean_preds - test_y)))\n\nOutput: 2.7021167278289795\n```", "```py\ndata_dim = train_x.size(-1)                            ❶\n\nclass LargeFeatureExtractor(torch.nn.Sequential):\n    def __init__(self, data_dim):\n        super(LargeFeatureExtractor, self).__init__()\n\n        self.add_module('linear1', torch.nn.Linear\n        ➥(data_dim, 1000))                            ❷\n        self.add_module('relu1', torch.nn.ReLU())      ❷\n        self.add_module('linear2', torch.nn.Linear\n        ➥(1000, 500))                                 ❸\n        self.add_module('relu2', torch.nn.ReLU())      ❸\n\n        self.add_module('linear3', torch.nn.Linear\n        ➥(500, 50))                                   ❹\n        self.add_module('relu3', torch.nn.ReLU())      ❹\n\n        self.add_module('linear4', torch.nn.Linear\n        ➥(50, 2))                                     ❺\n\nfeature_extractor = LargeFeatureExtractor(data_dim)    ❻\n```", "```py\nclass GPRegressionModel(gpytorch.models.ExactGP):\n  def __init__(self, train_x, train_y, likelihood):\n      super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n\n      self.mean_module = gpytorch.means.ConstantMean()\n\n      self.covar_module = gpytorch.kernels\n      ➥.GridInterpolationKernel(                    ❶\n          gpytorch.kernels.ScaleKernel(              ❶\n              gpytorch.kernels.RBFKernel             ❶\n              ➥(ard_num_dims=2)                     ❶\n          ),                                         ❶\n          num_dims=2,                                ❶\n          grid_size=100                              ❶\n      )                                              ❶\n\n      self.feature_extractor = feature_extractor     ❷\n\n      self.scale_to_bounds = gpytorch.utils.grid\n      ➥.ScaleToBounds(-1., 1.)                      ❸\n```", "```py\nclass GPRegressionModel(gpytorch.models.ExactGP):\n  def forward(self, x):\n    projected_x = self.feature_extractor(x)            ❶\n    projected_x = self.scale_to_bounds(projected_x)    ❶\n\n    mean_x = self.mean_module(projected_x)             ❷\n    covar_x = self.covar_module(projected_x)           ❷\n    return gpytorch.distributions.MultivariateNormal   ❷\n    ➥(mean_x, covar_x)                                ❷\n```", "```py\nlikelihood = gpytorch.likelihoods.GaussianLikelihood()             ❶\nmodel = GPRegressionModel(train_x, train_y, likelihood)            ❶\nmll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)  ❶\n\noptimizer = torch.optim.Adam([\n    {'params': model.feature_extractor.parameters()},              ❷\n    {'params': model.covar_module.parameters()},                   ❷\n    {'params': model.mean_module.parameters()},                    ❷\n    {'params': model.likelihood.parameters()},                     ❷\n], lr=0.01)\n```", "```py\nmodel.train()                      ❶\nlikelihood.train()                 ❶\n\nfor i in tqdm(range(500)):         ❷\n    optimizer.zero_grad()          ❷\n\n    output = model(train_x)        ❷\n    loss = -mll(output, train_y)   ❷\n\n    loss.backward()                ❷\n    optimizer.step()               ❷\n\nmodel.eval()                       ❸\nlikelihood.eval()                  ❸\n```", "```py\nwith torch.no_grad():\n    extracted_features = model.feature_extractor(train_x)\n    extracted_features = model.scale_to_bounds(extracted_features)\n```", "```py\nfor label in range(10):\n    mask = train_y == label           ❶\n\n    plt.scatter(                      ❷\n        extracted_features[mask, 0],  ❷\n        extracted_features[mask, 1],  ❷\n        c=train_y[mask],              ❷\n        vmin=0,                       ❷\n        vmax=9,                       ❷\n        label=label,                  ❷\n    )                                 ❷\n```", "```py\nwith torch.no_grad():\n    mean_preds = model(test_x).mean\n\nprint(torch.mean(torch.abs(mean_preds - test_y)))\n\nOutput: 0.8524129986763\n```"]