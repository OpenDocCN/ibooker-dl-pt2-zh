["```py\ndocker build -t orca3/services:latest -f services.dockerfile .\ndocker run --name training-service -v \n  ➥ /var/run/docker.sock:/var/run/docker.sock \n  ➥ --network orca3 --rm -d -p \"${TS_PORT}\":51001\n  ➥ orca3/services:latest training-service.jar\n```", "```py\ngrpcurl -plaintext \n -d \"{\n  \"metadata\": \n    { \"algorithm\":\"intent-classification\",    ❶\n      \"dataset_id\":\"1\",\n      \"name\":\"test1\",\n      \"train_data_version_hash\":\"hashBA==\",   ❷\n      \"parameters\":                           ❸\n        {\"LR\":\"4\",\"EPOCHS\":\"15\",\n         \"BATCH_SIZE\":\"64\",\n         \"FC_SIZE\":\"128\"}}\n}\" \n\"${TS_SERVER}\":\"${TS_PORT}\" \ntraining.TrainingService/Train\n```", "```py\ngrpcurl -plaintext \\\n -d \"{\"job_id\\\": \"$job_id\"}\" \\     ❶\n\"${TS_SERVER}\":\"${TS_PORT}\" \ntraining.TrainingService/GetTrainingStatus\n```", "```py\nservice TrainingService {\n rpc Train(TrainRequest) returns (TrainResponse);\n rpc GetTrainingStatus(GetTrainingStatusRequest) \n   returns (GetTrainingStatusResponse);\n}\n\nmessage TrainingJobMetadata {           ❶\n string algorithm = 1;                  ❶\n string dataset_id = 2;                 ❶\n string name = 3;                       ❶\n string train_data_version_hash = 4;    ❶\n map<string, string> parameters = 5;    ❶\n}                                       ❶\n\nmessage GetTrainingStatusResponse {\n TrainingStatus status = 1;\n int32 job_id = 2;\n string message = 3;\n TrainingJobMetadata metadata = 4;\n int32 positionInQueue = 5;\n}\n```", "```py\npublic void train(                                      ❶\n  TrainRequest request, \n  StreamObserver<TrainResponse> responseObserver) {\n\n   int jobId = store.offer(request);                    ❷\n   responseObserver.onNext(TrainResponse\n     .newBuilder().setJobId(jobId).build());            ❸\n   responseObserver.onCompleted();\n}\n\npublic class MemoryStore {\n   // jobs are waiting to pick up\n   public SortedMap<Integer, TrainingJobMetadata>       ❹\n     jobQueue = new TreeMap<>();                        ❹\n   // jobs’ docker container is in launching state  \n   public Map<Integer, ExecutedTrainingJob>             ❹\n     launchingList = new HashMap<>();                   ❹\n   // jobs’ docker container is in running state\n   public Map<Integer, ExecutedTrainingJob>             ❹\n     runningList = new HashMap<>();                     ❹\n   // jobs’ docker container is completed\n   public Map<Integer, ExecutedTrainingJob>             ❹\n     finalizedJobs = new HashMap<>();                   ❹\n   // .. .. ..\n\n   public int offer(TrainRequest request) {\n       int jobId = jobIdSeed.incrementAndGet();         ❺\n       jobQueue.put(jobId, request.getMetadata());      ❻\n       return jobId;\n   }\n}\n```", "```py\npublic boolean hasCapacity() {                           ❶\n  return store.launchingList.size()\n    + store.runningList.size() == 0;\n}\n\npublic String launch(                                    ❷\n  int jobId, TrainingJobMetadata metadata, \n  VersionedSnapshot versionedSnapshot) {\n\n    Map<String, String> envs = new HashMap<>();          ❸\n    .. .. ..                                             ❸\n    envs.put(\"TRAINING_DATA_PATH\",                       ❸\n    versionedSnapshot.getRoot());                        ❸\n    envs.putAll(metadata.getParametersMap());            ❸\n    List<String> envStrings = envs.entrySet()            ❸\n           .stream()                                     ❸\n           .map(kvp -> String.format(\"%s=%s\", \n             kvp.getKey(), kvp.getValue()))\n           .collect(Collectors.toList());\n\n   String containerId = dockerClient                     ❹\n    .createContainerCmd(metadata.getAlgorithm())         ❺\n           .. .. ..\n           .withCmd(\"server\", \"/data\")\n           .withEnv(envStrings)                          ❻\n           .withHostConfig(HostConfig.newHostConfig()\n             .withNetworkMode(config.network))\n           .exec().getId();\n\n   dockerClient.startContainerCmd(containerId).exec();   ❼\n   jobIdTracker.put(jobId, containerId);\n   return containerId;\n}\n```", "```py\npublic void updateContainerStatus() {\n  Set<Integer> launchingJobs = store.launchingList.keySet();\n  Set<Integer> runningJobs = store.runningList.keySet();\n\n  for (Integer jobId : launchingJobs) {               ❶\n\n    String containerId = jobIdTracker.get(jobId);\n    InspectContainerResponse.ContainerState state =   ❷\n        dockerClient.inspectContainerCmd(             ❷\n          containerId).exec().getState();             ❷\n    String containerStatus = state.getStatus();\n\n    // move the launching job to the running \n    // queue if the container starts to run. \n       .. .. ..\n   }\n\n   for (Integer jobId : runningJobs) {                ❸\n      // move the running job to the finalized \n      // queue if it completes (success or fail).\n       .. .. ..\n   }\n}\n```", "```py\npublic void getTrainingStatus(GetTrainingStatusRequest request) {\n  int jobId = request.getJobId();\n  .. .. ..  \n  if (store.finalizedJobs.containsKey(jobId)) {           ❶\n    job = store.finalizedJobs.get(jobId);\n    status = job.isSuccess() ? TrainingStatus.succeed \n        : TrainingStatus.failure;\n\n  } else if (store.launchingList.containsKey(jobId)) {    ❷\n    job = store.launchingList.get(jobId);\n    status = TrainingStatus.launch;\n\n  } else if (store.runningList.containsKey(jobId)) {      ❸\n    job = store.runningList.get(jobId);\n    status = TrainingStatus.running;\n  } else {                                                ❹\n    TrainingJobMetadata metadata = store.jobQueue.get(jobId);\n    status = TrainingStatus.waiting;\n       .. .. ..\n   }\n   .. .. ..\n}\n```", "```py\n# 1\\. read all the input parameters from \n# environment variables, these environment \n# variables are set by training service - docker job tracker.\nEPOCHS = int_or_default(os.getenv('EPOCHS'), 20)\n.. .. ..\nTRAINING_DATA_PATH = os.getenv('TRAINING_DATA_PATH')\n\n# 2\\. download training data from dataset management\nclient.fget_object(TRAINING_DATA_BUCKET, \n  TRAINING_DATA_PATH + \"/examples.csv\", \"examples.csv\")\nclient.fget_object(TRAINING_DATA_BUCKET, \n  TRAINING_DATA_PATH + \"/labels.csv\", \"labels.csv\")\n\n# 3\\. prepare dataset\n.. .. ..\ntrain_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n                             shuffle=True, collate_fn=collate_batch)\nvalid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n                             shuffle=True, collate_fn=collate_batch)\ntest_dataloader = DataLoader(split_test_, batch_size=BATCH_SIZE,\n                            shuffle=True, collate_fn=collate_batch)\n\n# 4\\. start model training\nfor epoch in range(1, EPOCHS + 1):\n   epoch_start_time = time.time()\n   train(train_dataloader)\n   .. .. ..\n\nprint('Checking the results of test dataset.')\naccu_test = evaluate(test_dataloader)\nprint('test accuracy {:8.3f}'.format(accu_test))\n\n# 5\\. save model and upload to metadata store.\n.. .. ..\nclient.fput_object(config.MODEL_BUCKET, \n  config.MODEL_OBJECT_NAME, model_local_path)\nartifact = orca3_utils.create_artifact(config.MODEL_BUCKET, \n  config.MODEL_OBJECT_NAME)\n.. .. ..\n```", "```py\n$ kubectl get crd                                       ❶\n\nNAME                              CREATED AT\n...\npytorchjobs.kubeflow.org        2021-09-15T18:33:58Z    ❷\n...\n```", "```py\nkind: PyTorchJob                 ❶\nmetadata:\n  name: pytorch-demo             ❷\nspec:\n  pytorchReplicaSpecs:           ❸\n    Master:\n      replicas: 1                ❹\n      restartPolicy: OnFailure\n      containers:\n          .. .. ..\n    Worker:\n      replicas: 1                ❺\n      .. .. ..\n        spec:\n          containers:            ❻\n            - name: pytorch\n              .. .. ..\n              env:               ❼\n                - name: credentials\n                  value: \"/etc/secrets/user-gcp-sa.json\"\n              command:           ❽\n                - \"python3\"\n                - “-m”\n                - \"/opt/pytorch-mnist/mnist.py\"\n                - \"--epochs=20\"\n                - “--batch_size=32”\n```", "```py\nkubectl get -o yaml pytorchjobs pytorch-demo -n kubeflow\n```"]