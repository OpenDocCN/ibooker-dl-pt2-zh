- en: 4 Distributed training
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 分布式训练
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容包括
- en: Understanding data parallelism, model parallelism, and pipeline parallelism
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解数据并行、模型并行和流水线并行
- en: Using a sample training service that supports data parallel training in Kubernetes
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用支持 Kubernetes 中数据并行训练的示例训练服务
- en: Training large models with multiple GPUs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用多个 GPU 训练大型模型
- en: 'One obvious trend in the deep learning research field is to improve model performance
    with larger datasets and bigger models with increasingly more complex architecture.
    But more data and bulkier models have consequences: they slow down the model training
    process as well as the model development process. As is often the case in computing,
    performance is pitted against speed. For example, it can cost several months to
    train a BERT (Bidirectional Encoder Representations from Transformers) natural
    language processing model with a single GPU.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习研究领域明显的一个趋势是通过更大的数据集和更大的模型以及越来越复杂的架构来提高模型性能。但更多的数据和更庞大的模型也会带来一些后果：它们会减慢模型训练过程以及模型开发过程。在计算中，性能常常与速度相抵触。例如，使用单个
    GPU 训练一个 BERT（双向编码器表示转换器）自然语言处理模型可能需要几个月的时间。
- en: To address the problem of ever-growing datasets and model parameter size, researchers
    have created various distributed training strategies. And major training frameworks,
    such as TensorFlow and PyTorch, provide SDKs that implement these training strategies.
    With the help of these training SDKs, data scientists can write training code
    that runs across multiple devices (CPU or GPU) and in parallel.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决数据集和模型参数规模不断增长的问题，研究人员创造了各种分布式训练策略。而主要的训练框架，如 TensorFlow 和 PyTorch，提供了实现这些训练策略的
    SDK。借助这些训练 SDK，数据科学家可以编写跨多个设备（CPU 或 GPU）并行运行的训练代码。
- en: In this chapter, we will explore how to support distributed training from a
    software engineer’s perspective. More specifically, we will see how to write a
    training service to execute different distributed training codes (developed by
    data scientists) in a group of machines.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将从软件工程师的角度探讨如何支持分布式训练。具体来说，我们将看到如何编写一个训练服务来在一组机器上执行不同的分布式训练代码（由数据科学家开发）。
- en: After reading this chapter, you will have a holistic view of how distributed
    training can work from the perspectives of both a data scientist and a developer.
    You will know several distributed training strategies and distributed training
    code patterns, as well as how a training service facilitates different distributed
    training codes.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读完本章后，你将全面了解分布式训练如何从数据科学家和开发者的角度进行工作。你将了解到几种分布式训练策略和分布式训练代码模式，以及训练服务如何促进不同的分布式训练代码。
- en: 4.1 Types of distributed training methods
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 分布式训练方法的类型
- en: 'There are three major types of distributed training methods: model parallelism,
    data parallelism, and pipeline parallelism. *Model parallelism* is a strategy
    to split a neural network into several sequential subnetworks and run each subnetwork
    on different devices (GPU or CPU). In this way, we can train a large model with
    a group of GPUs.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 有三种主要类型的分布式训练方法：模型并行、数据并行和流水线并行。*模型并行* 是一种将神经网络拆分为多个顺序子网络并在不同设备上运行每个子网络的策略。通过这种方式，我们可以使用一组
    GPU 训练大型模型。
- en: '*Pipeline parallelism* is an advanced version of model parallelism. A major
    problem with model parallelism is that only one GPU is active during training;
    the others are idle. By dividing each training example batch into small microbatches,
    pipeline parallelism overlaps computations between layers to maximize GPU performance.
    This allows different GPUs to work on various microbatches at the same time. The
    GPUs’ training throughput and device utilization improve, resulting in a much
    faster model training speed than model parallelism.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*流水线并行* 是模型并行的高级版本。模型并行的一个主要问题是在训练过程中只有一个 GPU 处于活动状态；其他 GPU 处于空闲状态。通过将每个训练示例批次划分为小的微批次，流水线并行可以在层之间重叠计算，以最大化
    GPU 性能。这允许不同的 GPU 同时处理各种微批次。GPU 的训练吞吐量和设备利用率得到提高，从而比模型并行更快地进行模型训练。'
- en: '*Data parallelism* partitions the dataset into smaller pieces and lets each
    device train these subdatasets separately. Because each device now trains a smaller
    dataset, the training speed improves.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*数据并行* 将数据集分成较小的部分，并让每个设备单独训练这些子数据集。因为现在每个设备训练的是较小的数据集，所以训练速度得到了提高。'
- en: Converting a single-device training code to model parallelism or pipeline parallelism
    training requires lots of code changes, including splitting the neural network
    into multiple subnetworks, running subnetworks on different GPUs, and copying
    the subnetworks’ compute output on different GPUs. The sheer quantity, as well
    as the complexity of these changes, makes them problematic and hard to debug.
    Each model algorithm might have a dramatically different model architecture, so
    no standardized method exists for splitting a model for model parallelism or pipeline
    parallelism. Data scientists must build the code case by case.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 将单设备训练代码转换为模型并行化或流水线并行化训练需要进行大量的代码改动，包括将神经网络分割为多个子网络，在不同的GPU上运行子网络，并将子网络的计算输出复制到不同的GPU上。这些改动的数量之多以及复杂性使得它们难以处理和调试。每个模型算法可能具有截然不同的模型架构，因此没有标准化的方法可以用于模型并行化或流水线并行化的模型分割。数据科学家必须逐个案例构建代码。
- en: On the contrary, data parallelism requires only minimal code changes on a single-device
    training code. And there are standardized patterns for converting a nondistributed
    training code to data parallelism without changing the model algorithm or architecture.
    Also, data parallelism code is relatively easy to both understand and debug. These
    merits make data parallelism our primary choice for distributed training.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，数据并行化仅需要在单设备训练代码上进行最少的代码改动。而且，有标准化的模式可以将非分布式训练代码转换为数据并行化，而无需更改模型算法或架构。此外，数据并行化代码相对易于理解和调试。这些优点使得数据并行化成为我们分布式训练的首选。
- en: Although data parallelism has a lot of advantages, model parallelism and pipeline
    parallelism have their own strengths and uses as well. When you have large models
    that can’t fit into one GPU, for instance, they are the best-distributed solution.
    We will talk about them more in section 4.4.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管数据并行化有很多优点，模型并行化和流水线并行化也各自具有自己的优势和用途。例如，当您有无法适应一个GPU的大型模型时，它们是最佳的分布式解决方案。我们将在第4.4节中更详细地讨论它们。
- en: 4.2 Data parallelism
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 数据并行化
- en: In this section, we will look at data parallelism theories and their parallel
    execution challenges, along with sample training codes in PyTorch, TensorFlow,
    and Horovod.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将研究数据并行化理论及其并行执行的挑战，以及PyTorch、TensorFlow和Horovod中的示例训练代码。
- en: 4.2.1 Understanding data parallelism
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.1 理解数据并行化
- en: Data parallelism involves a group of training devices working together on a
    large dataset. By having each device process a subset of the dataset, we can greatly
    reduce the training time.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并行化涉及到一组训练设备在一个大数据集上一起工作。通过让每个设备处理数据集的一个子集，我们可以大大减少训练时间。
- en: Synchronous data parallelism is the most adopted data parallelism method. It
    replicates the model network to every device in the training group, whether it
    is a GPU or CPU. The dataset is split into minibatches, and these batches are
    distributed across all devices (again, either CPU or GPU). The training steps
    occur simultaneously, using a different minibatch on each of the devices; therefore,
    the devices act as their own data partition. When calculating gradients to update
    the neural network, the algorithm calculates the final gradients by aggregating
    them from each device. Then it dispatches the aggregated gradients back to each
    device to update their local neural network. Although the training dataset on
    each device is different, the neural networks local to these devices are the same
    because they are updated by the same gradients in each training iteration. As
    a result, this process is called synchronous data parallelism.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 同步数据并行化是最常采用的数据并行化方法。它将模型网络复制到训练组中的每个设备上，无论是GPU还是CPU。数据集被分割成小批量，并在所有设备上（再次是CPU或GPU）上分发这些批量。训练步骤同时进行，每个设备上使用不同的小批量；因此，设备充当自己的数据分区。在计算梯度以更新神经网络时，算法通过从每个设备聚合梯度来计算最终梯度。然后，它将聚合梯度分发回每个设备，以更新其本地神经网络。虽然每个设备上的训练数据集是不同的，但这些设备上的本地神经网络是相同的，因为它们在每个训练迭代中都是由相同的梯度更新的。因此，这个过程被称为同步数据并行化。
- en: You can visualize this process in figure 4.1\. The figure compares the process
    of deep learning training on a single GPU, in graph (a) on the left, with the
    setup for synchronous data parallel training using three GPUs, in graph (b) on
    the right.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在图4.1中可视化这个过程。该图比较了在单个GPU上进行深度学习训练的过程（左侧的图(a)）与使用三个GPU进行同步数据并行训练的设置（右侧的图(b)）。
- en: '![](../Images/04-01.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-01.png)'
- en: Figure 4.1 A synchronous data parallelism concept graph. (a) Deep learning training
    on a single GPU. (b) Synchronous data parallel training with three GPUs.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1 同步数据并行概念图。（a）在单GPU上进行深度学习训练。（b）使用三个GPU进行同步数据并行训练。
- en: By comparing graphs (a) and (b), you can see that synchronous data parallelism
    introduces two extra steps compared with single-device training. The first extra
    step is to divide one training batch into three minibatches, so each device can
    work on its own minibatch. The second step is to synchronize the gradients aggregated
    from all the machines, so they are all operating with the same gradients when
    updating their local model.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 通过比较图（a）和（b），您会发现与单设备训练相比，同步数据并行引入了两个额外步骤。第一个额外步骤是将一个训练批次分成三个小批次，这样每个设备就可以处理自己的小批次。第二步是同步来自所有机器的聚合梯度，以便它们在更新本地模型时都使用相同的梯度。
- en: Note To aggregate gradients computed by different workers, you can use the algorithm
    all-reduce. This is a popular algorithm that independently combines arrays of
    data from all processes into a single array. In “Writing Distributed Applications
    with PyTorch” ([https://pytorch.org/tutorials/intermediate/dist_tuto.html](https://pytorch.org/tutorials/intermediate/dist_tuto.html)),
    you can find an example of how PyTorch supports the all-reduce algorithm.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：为了聚合不同工作者计算的梯度，您可以使用算法all-reduce。这是一种流行的算法，它独立地将所有进程的数据数组组合成一个单一数组。在“使用PyTorch编写分布式应用程序”（[https://pytorch.org/tutorials/intermediate/dist_tuto.html](https://pytorch.org/tutorials/intermediate/dist_tuto.html)）中，您可以找到PyTorch支持all-reduce算法的示例。
- en: From an implementation perspective, data parallelism requires minimal changes
    in a single-device model training process. Its main overhead is the added step
    of synchronizing the gradient aggregation.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 从实现的角度来看，数据并行只需要对单设备模型训练过程进行最少的更改。其主要开销是添加了梯度聚合的步骤。
- en: 'Model parameter updates: Synchronous vs. Asynchronous'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 模型参数更新：同步vs.异步
- en: 'There are two schools of thought on aggregating gradients across workers in
    data parallelism: synchronous updates and asynchronous updates. Let’s review how
    each of these works, along with their advantages and drawbacks, so you can choose
    for yourself:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于在数据并行中跨工作者聚合梯度有两种思路：同步更新和异步更新。让我们分别看看它们是如何工作的，以及它们的优点和缺点，这样您就可以自行选择：
- en: '*Synchronous model update*—As demonstrated in figure 4.1, a synchronous model
    update pauses the training iteration at the gradient sync step until all devices
    receive the aggregated gradients. Then it proceeds to the next step, updating
    the model parameters. In this way, all devices get the same gradient updates at
    the same time, thus ensuring that the model of each worker is on the same page
    in every training iteration. The problem with synchronous model updates is obvious:
    the training iteration is blocked while the gradients are being synchronized among
    the workers, so none of the workers can begin processing the next minibatch of
    data. If there are some slow machines or network problems, the entire distributed
    working group is stuck, and the faster workers are idle.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*同步模型更新*—如图4.1所示，同步模型更新在梯度同步步骤暂停训练迭代，直到所有设备接收到聚合梯度。然后它继续下一步，更新模型参数。通过这种方式，所有设备在同一时间获得相同的梯度更新，从而确保每个工作者的模型在每个训练迭代中都是一致的。同步模型更新的问题是显而易见的：当梯度在工作者之间同步时，训练迭代被阻塞，因此没有一个工作者可以开始处理下一个数据小批次。如果存在一些慢机器或网络问题，则整个分布式工作组都会受阻，而较快的工作者则处于空闲状态。'
- en: '*Asynchronous model update*—In contrast, the asynchronous model update approach
    does not force each training device or worker to wait to receive gradients from
    the other devices. Instead, whenever a device has finished computing the gradients,
    it immediately updates its local model without checking other devices. Every device
    works independently, and although its gradients still need to be copied to every
    other device, synchronization of these updates is not necessary. The asynchronous
    method may seem very appealing; it’s simple and can run more training steps per
    minute than the synchronous method. A downside to the asynchronous method is that
    it takes a longer time to train and produces less-accurate models than the synchronous
    model update method.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*异步模型更新* — 与之相反，异步模型更新方法不强制每个训练设备或工作器等待接收来自其他设备的梯度。相反，每当一个设备完成梯度计算后，它立即更新本地模型而无需检查其他设备。每个设备都独立工作，尽管它的梯度仍然需要复制到每个其他设备，但这些更新的同步不是必要的。异步方法可能看起来很吸引人；它简单，并且可以比同步方法每分钟运行更多的训练步骤。异步方法的缺点是训练时间较长，并且产生的模型比同步模型更新方法产生的模型不准确。'
- en: When we use the asynchronous method, gradients are calculated independently
    on different devices. Some machines run faster while others run slower; consequently,
    these gradients can be produced from different training iterations of each device.
    So there is no guarantee that the aggregated gradients will point in the optimal
    direction. For example, say the gradients from a slow machine are calculated from
    training iteration 5, while other, faster machines have already moved to training
    iteration 20\. When we aggregate the gradients from all the workers, the gradients
    from the lower iteration are applied to those from the higher iteration; this
    degrades the gradient quality.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用异步方法时，梯度在不同设备上独立计算。一些机器运行速度更快，而其他机器运行速度较慢；因此，这些梯度可以来自每个设备的不同训练迭代。因此，无法保证聚合的梯度将指向最佳方向。例如，假设来自慢速机器的梯度是从训练迭代
    5 计算的，而其他更快的机器已经移动到训练迭代 20。当我们聚合所有工作器的梯度时，低迭代的梯度会应用于高迭代的梯度；这会降低梯度质量。
- en: In addition, the asynchronous method often converges slowly and has a higher
    accuracy loss than the synchronous method. Thus, most data parallelism libraries
    today are doing synchronous model updates. In this chapter, when we mention data
    parallelism and its code implementation, we mean synchronous data parallelism.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，异步方法通常收敛速度较慢，并且比同步方法有更高的准确度损失。因此，今天大多数数据并行库都在执行同步模型更新。在本章中，当我们提到数据并行和其代码实现时，我们指的是同步数据并行。
- en: Memory constraint for dataset and model
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集和模型的内存约束
- en: In deep learning, datasets and models consume the most memory of the compute
    instance during training. The training process will be terminated by an out-of-memory
    (OOM) error if the training data or neural network (model) exceeds the memory
    limits of the local device. Data parallelism is designed to improve training speed
    but not to solve memory constraint problems.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，数据集和模型在训练过程中消耗计算实例的大部分内存。如果训练数据或神经网络（模型）超出了本地设备的内存限制，训练过程将被终止，出现内存不足（OOM）错误。数据并行旨在提高训练速度，但不能解决内存约束问题。
- en: For OOM caused by loading a dataset, we can reduce the batch size of the training
    data, so the training process loads a smaller amount of data into local memory
    in each training loop. In the data parallelism context, we need to make sure the
    minibatch training data can fit into the memory of every worker device.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对于由加载数据集引起的 OOM，我们可以减少训练数据的批量大小，因此训练过程在每个训练循环中加载较小量的数据到本地内存中。在数据并行背景下，我们需要确保小批量训练数据可以适合每个工作器设备的内存。
- en: For OOM caused by the model size, we need to adopt model parallelism or pipeline
    parallelism (see section 4.4). Data parallelism simply won’t work when the size
    of a neural network (model) exceeds the memory limits of a single device.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对于由模型大小引起的 OOM，我们需要采用模型并行或管道并行（见 4.4 节）。当神经网络（模型）的大小超过单个设备的内存限制时，数据并行简单地无法工作。
- en: 4.2.2 Multiworker training challenges
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.2 多工作器训练挑战
- en: Fault tolerance and bandwidth saturation are the two challenges we, as software
    developers, need to address when executing data parallelism code in the training
    service. Meeting these two challenges is critical to reducing operating costs
    and improving training performance for data parallelism–distributed training.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 容错性和带宽饱和是我们作为软件开发者在执行训练服务中的数据并行代码时需要解决的两个挑战。 解决这两个挑战对于降低运营成本和改善数据并行性训练的性能至关重要。
- en: Fault tolerance
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 容错性
- en: We don’t want the entire distributed training group to fail just because one
    of the workers fails unexpectedly. This not only causes a problem for service
    availability but also increases our training cost because all other workers’ efforts
    are wasted if one fails.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不希望整个分布式训练组因为一个工作节点意外失败而全部失败。 这不仅会导致服务可用性问题，还会增加我们的训练成本，因为如果一个节点失败，所有其他节点的工作都会被浪费。
- en: To improve fault tolerance, we can preserve the training state (i.e., the model
    parameters) of each training step in a remote filesystem for each worker. Then,
    if one worker fails or takes too long to complete one training iteration, we can
    restart that worker and load its most recent previous state.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高容错性，我们可以在每个工作节点的远程文件系统中保留每个训练步骤（即模型参数）的训练状态。 然后，如果一个工作节点失败或花费太长时间来完成一个训练迭代，我们可以重新启动该工作节点并加载其最近的先前状态。
- en: Both TensorFlow and PyTorch frameworks have features to back up and restore.
    As training service developers, we can set up the remote disk or backup storage
    system and pass the access configuration to the training container. Then, during
    the training, the training code can use the external filesystem to backup or restore
    states.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow和PyTorch框架都具有备份和恢复功能。 作为训练服务开发者，我们可以设置远程磁盘或备份存储系统，并将访问配置传递给训练容器。 然后，在训练过程中，训练代码可以使用外部文件系统来备份或恢复状态。
- en: Bandwidth saturation
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 带宽饱和
- en: Adding more GPUs and more machines to the distributed training group doesn’t
    always improve performance. Whether we use synchronous or asynchronous model updates,
    the algorithm must communicate the gradients or model parameters between the training
    workers at the end of each training iteration. The time spent on moving data in
    and out of GPU RAM and across the network will eventually outweigh the speedup
    obtained by splitting the training workload.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 向分布式训练组添加更多的GPU和更多的机器并不总是会提高性能。 无论我们使用同步还是异步模型更新，算法都必须在每个训练迭代结束时在训练节点之间通信梯度或模型参数。
    在GPU RAM和网络之间移动数据所花费的时间最终将超过通过分割训练工作负载获得的加速。
- en: Therefore, a cap exists for how many parallel instances can occur before data
    parallelism reaches its peak performance. This cap is determined by the number
    of model parameters and the density of the model (how many nonzero values are
    in model weights). If it’s a large, dense model with lots of parameters and gradients
    to transfer, its saturation is greater than either a smaller model or a large,
    sparse model.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在数据并行达到最佳性能之前，可以并行发生多少个实例存在上限。 这一限制由模型参数的数量和模型的密度（模型权重中的非零值有多少）确定。 如果是一个大型、密集的模型，有大量的参数和梯度需要传输，那么它的饱和度就比较大，大于一个较小的模型或一个大型的稀疏模型。
- en: There are some recommended parallel instance numbers, such as a 6× speedup on
    8 GPUs for neural machine translations and a 32× speedup on 50 GPUs for ImageNet
    models. But we need to determine the sweet spot with our own experiments because
    both GPU and model architectures evolve rapidly, and standard recommendations
    will quickly become outdated. As platform developers, besides choosing the perfect
    number of parallel workers, we have three additional methods for mitigating bandwidth
    saturation.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些推荐的并行实例数，例如，对于神经机器翻译，在8个GPU上可以实现6倍的加速，对于ImageNet模型，在50个GPU上可以实现32倍的加速。 但是，我们需要通过我们自己的实验来确定最佳实验点，因为GPU和模型架构都在快速发展，标准推荐很快就会过时。
    作为平台开发者，除了选择最佳的并行工作节点数量外，我们还有三种额外的方法来减轻带宽饱和。
- en: First, we can group the parallel workers (i.e., the containers or pods) into
    fewer machines to reduce the network hops. For example, in Kubernetes, you can
    set `nodeSelector` with affinity and anti-affinity rules ([http://mng.bz/qo76](http://mng.bz/qo76))
    to provision training instances (Kubernetes pods) on a few selected servers that
    have a better network and more computational power.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以将并行工作者（即容器或Pod）分组到更少的机器中，以减少网络跳数。例如，在Kubernetes中，您可以设置具有亲和性和反亲和性规则的`nodeSelector`（[http://mng.bz/qo76](http://mng.bz/qo76)），以在一些选择的具有更好网络和更多计算能力的服务器上提供训练实例（Kubernetes
    Pod）。
- en: A second option is to always upgrade the training image to use the latest version
    of the training framework. Popular frameworks such as PyTorch, TensorFlow, and
    others are constantly evolving to reduce the data volume transferred within the
    network for distributed training. Pay attention to the release note and take advantage
    of these improvements.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个选择是始终将训练映像升级为使用训练框架的最新版本。诸如PyTorch、TensorFlow等流行框架不断发展，以减少网络中传输的数据量以进行分布式训练。注意发布说明并利用这些改进。
- en: Finally, don’t underestimate the gains that can result from doing small tweaks
    when initializing the distributed group. Consider using PyTorch, for example.
    The PyTorch data parallel library partitions the neural network parameter gradients
    into buckets and then sends the buckets around to the workers during the gradient
    synchronization step. The bucket size determines how much data is transferred
    between different devices at one time. So by choosing the right bucket size, we
    can determine a sweet spot between device saturation and network saturation, thus
    reaching the best training speed. The bucket size can be configured in the constructor
    of the PyTorch distributed data parallel (DDP) component ([http://mng.bz/7ZB7](http://mng.bz/7ZB7)).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，不要低估初始化分布式组时进行微小调整可能带来的收益。例如，考虑使用PyTorch。PyTorch数据并行库将神经网络参数梯度分区为桶，然后在梯度同步步骤期间将桶发送到工作进程中。桶的大小决定了一次在不同设备之间传输多少数据。因此，通过选择合适的桶大小，我们可以确定设备饱和和网络饱和之间的最佳训练速度的甜蜜点。桶的大小可以在PyTorch分布式数据并行（DDP）组件的构造函数中配置（[http://mng.bz/7ZB7](http://mng.bz/7ZB7)）。
- en: 4.2.3 Writing distributed training (data parallelism) code for different training
    frameworks
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.3为不同的训练框架编写分布式训练（数据并行性）代码
- en: 'In this section, you will see some training code snippets for data parallel
    distributed training in three training frameworks: TensorFlow, PyTorch, and Horovod.
    Don’t worry if the code samples here are difficult to parse. The purpose is to
    experience how data scientists handle distributed training on their side. This
    will give you a sense of how training services enable distributed training.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将看到一些用于数据并行分布式训练的训练代码片段，涵盖了三个训练框架：TensorFlow、PyTorch和Horovod。如果这里的代码示例难以解析，不用担心。目的是体验数据科学家如何处理分布式训练。这将让您了解训练服务如何实现分布式训练。
- en: PyTorch
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch
- en: The PyTorch framework has a DDP library that implements data parallelism at
    the module level. The DDP wraps the model object so that it can run the object
    across multiple machines seamlessly. Its training processes can be placed on the
    same machine or across machines.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch框架具有DDP库，该库在模块级别实现数据并行性。DDP包装模型对象，使其可以在多台机器上无缝运行。其训练进程可以放置在同一台机器上，也可以分布在多台机器上。
- en: To convert a single device/process training code to a data parallel–distributed
    training code, we need to make the following two modifications. First, we must
    initialize the training group by allowing each training process to register itself
    with the master process. One of the processes claims to be the master while the
    others claim to be the workers. Each training process will be pending at this
    registration stage until all workers join the distributed group.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 要将单设备/进程训练代码转换为数据并行-分布式训练代码，我们需要进行以下两个修改。首先，我们必须通过允许每个训练进程向主进程注册自己来初始化训练组。其中一个进程声称自己是主进程，而其他进程声称自己是工作进程。每个训练进程将在此注册阶段等待，直到所有工作进程加入分布式组。
- en: 'To register a process, we need to know the total number of training processes
    (`world_size`), a unique ID for this process (`rank`), and the master process’s
    address (define `MASTER_ADDR` and `MASTER_PORT` in environment variables). See
    the code sample as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 要注册一个进程，我们需要知道总的训练进程数（`world_size`），该进程的唯一ID（`rank`）以及主进程的地址（在环境变量中定义`MASTER_ADDR`和`MASTER_PORT`）。如下所示查看代码示例：
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Second, we use the DDP class to wrap the model object. The PyTorch DDP class
    will handle the distributed data communication, gradient aggregation, and local
    model parameter updates:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们使用 DDP 类来包装模型对象。PyTorch DDP 类将处理分布式数据通信、梯度聚合和本地模型参数更新：
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ The DDP wrapper takes care of the distributed training execution.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ DDP 包装器负责分布式训练的执行。
- en: For advanced use cases, the PyTorch library provides the API so you can implement
    your own gradient synchronization function at a lower level. You can check the
    details at the official tutorial, “Writing Distributed Applications with Pytorch”
    ([http://mng.bz/m27W](http://mng.bz/m27W)).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对于高级用例，PyTorch 库提供了 API，因此您可以在较低的级别实现自己的梯度同步函数。您可以在官方教程“使用 Pytorch 编写分布式应用程序”([http://mng.bz/m27W](http://mng.bz/m27W))
    中查看详细信息。
- en: TensorFlow/Keras
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow/Keras
- en: 'TensorFlow supports distributed training in a very similar way to PyTorch;
    it first defines a distributed training strategy (such as `MultiWorkerMirroredStrategy`)
    and then initializes the model with this strategy. To let the strategy identify
    the workers in the distributed group, we need to define a `TF_CONFIG` environment
    variable in each training process. `TF_CONFIG` contains a worker’s unique ID and
    the addresses of all other workers in the group. See the code as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 支持与 PyTorch 非常相似的分布式训练方式；它首先定义分布式训练策略（例如 `MultiWorkerMirroredStrategy`），然后使用该策略初始化模型。为了让策略识别分布式组中的工作节点，我们需要在每个训练进程中定义
    `TF_CONFIG` 环境变量。`TF_CONFIG` 包含一个工作节点的唯一 ID 和组中所有其他工作节点的地址。请参阅以下代码：
- en: '[PRE2]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Horovod
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Horovod
- en: 'Horovod is a single-purpose distributed framework. Compared to TensorFlow and
    PyTorch, which can be used across a range of tasks, such as data processing, model
    training, and model serving, Horovod can only focus on one task: making distributed
    deep learning training fast and easy to use.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Horovod 是一个单一目的的分布式框架。与可以用于一系列任务的 TensorFlow 和 PyTorch 相比，例如数据处理、模型训练和模型服务，Horovod
    只能专注于一个任务：使分布式深度学习训练变得快速且易于使用。
- en: Horovod’s greatest advantage is that it works with different training frameworks,
    such as TensorFlow, Keras, PyTorch, and Apache MXNet. Therefore, we can configure
    our training cluster in one manner (the Horovod way) to run distributed training
    for PyTorch, TensorFlow, and other frameworks. Here, we only list two code snippets
    for using Horovod with TensorFlow and PyTorch, but you can check examples of other
    frameworks on Horovod’s website.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Horovod 最大的优势在于它可以与不同的训练框架一起使用，例如 TensorFlow、Keras、PyTorch 和 Apache MXNet。因此，我们可以以一种方式（Horovod
    方式）配置我们的训练集群，以运行 PyTorch、TensorFlow 和其他框架的分布式训练。这里，我们只列出了两个代码片段，用于使用 TensorFlow
    和 PyTorch 与 Horovod，但您可以在 Horovod 的网站上查看其他框架的示例。
- en: 'Let’s look at the TensorFlow example. To set up data parallelism–distributed
    training, first we initialize the Horovod training group, which will find other
    Horovod nodes in your cluster automatically. Next, we broadcast the rank 0’s (master
    worker’s) initial variable states to all other processes. This will ensure the
    consistent initialization of all workers. Then we wrap the gradient tape with
    distributed gradient tape, which will average gradients on all workers. The remaining
    code is simply normal TensorFlow training code. As such, please see the code that
    follows ([https://github.com/horovod/horovod/blob/master/examples](https://github.com/horovod/horovod/blob/master/examples)):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 TensorFlow 的示例。为了设置数据并行ism-分布式训练，首先我们初始化 Horovod 训练组，它将自动找到集群中的其他 Horovod
    节点。接下来，我们将 rank 0（主工作节点）的初始变量状态广播到所有其他进程。这将确保所有工作节点的一致初始化。然后我们将梯度磁带包装在分布式梯度磁带中，这将对所有工作节点上的梯度进行平均。其余的代码只是普通的
    TensorFlow 训练代码。因此，请参阅以下代码([https://github.com/horovod/horovod/blob/master/examples](https://github.com/horovod/horovod/blob/master/examples))：
- en: '[PRE3]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Initializes Horovod
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 初始化 Horovod
- en: ❷ Adjusts the number of steps based on the number of GPUs
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 根据 GPU 数量调整步数
- en: 'The following code is an example of using Horovod with PyTorch. Some PyTorch
    Horovod APIs are different than TensorFlow—for example, `hvd.DistributedOptimizer`
    versus `hvd.DistributedGradientTape`. But these APIs are from the same Horovod
    SDK and share the same interworker mechanism under the hood. Let’s look at the
    PyTorch code snippet:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码是使用 PyTorch 与 Horovod 的示例。一些 PyTorch Horovod API 与 TensorFlow 不同，例如 `hvd.DistributedOptimizer`
    与 `hvd.DistributedGradientTape`。但是这些 API 来自相同的 Horovod SDK 并在幕后共享相同的工作节点机制。让我们看看
    PyTorch 代码片段：
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Although the model is defined in two different frameworks—TensorFlow 2 and PyTorch—we
    can see from these two code snippets that they use the same Horovod SDK to run
    distributed training. The benefit here is that we can use a standard method (the
    Horovod way) to set up the distributed worker group in our training cluster, and
    it can still function for the training code written in different training frameworks.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管模型在两个不同的框架中定义——TensorFlow 2 和 PyTorch——但从这两个代码片段我们可以看出它们都使用同样的 Horovod SDK
    来运行分布式训练。这里的好处在于我们可以使用一个标准方法（即 Horovod 方式）在我们的训练集群中设置分布式工作组，并且它仍然可以用于不同训练框架中编写的训练代码。
- en: Two takeaways on training code
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 关于训练代码的两个要点
- en: 'It’s fine if you are confused when reading those training code snippets. As
    a training service developer, you don’t need to write these pieces of code. We
    want to emphasize two points from this discussion:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在阅读这些训练代码片段时感到困惑，那没关系。作为训练服务开发人员，你不需要编写这些代码片段。我们想要从这次讨论中强调两点：
- en: Although the code samples in this section implement distributed training in
    different frameworks with different APIs, the code follows the same data parallelism
    paradigm described in section 4.2.1\. That is, the code always (1) sets up the
    communication group for each parallel training process and (2) configures the
    model object to aggregate gradients across all workers. So, as developers, we
    can use a unified method to set up and manage distributed training processes for
    different training frameworks.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管本节中的代码示例使用不同框架和不同 API 实现分布式训练，但代码遵循第 4.2.1 节中描述的相同数据并行范式。也就是说，代码始终（1）为每个并行训练进程设置通信组，（2）配置模型对象以在所有工作节点上聚合梯度。因此，作为开发人员，我们可以使用统一的方法为不同的训练框架设置和管理分布式训练进程。
- en: The work of extending model training code from single-device training to data
    parallelism–distributed training is relatively trivial. Nowadays, the distributed
    training frameworks/SDKs are so powerful that we don’t need to implement every
    detail of data parallelism, such as the gradient synchronization that synchronizes
    the gradients across the network. The training frameworks and SDKs handle these
    processes so they run seamlessly. The distributed data parallel training code
    is almost identical to the single-device training code, except when configuring
    training groups.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将模型训练代码从单设备训练扩展到数据并行分布式训练的工作相对较为琐碎。如今，分布式训练框架/SDK 非常强大，我们不需要实现数据并行的每一个细节，比如同步梯度，它会在整个网络中同步梯度。训练框架和
    SDK 处理这些过程，使它们运行无缝。分布式数据并行训练代码几乎与单设备训练代码相同，除了配置训练组时。
- en: 4.2.4 Engineering effort in data parallel–distributed training
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.4 数据并行分布式训练中的工程努力
- en: So what does the work look like for enabling data parallel–distributed training
    in production? First, it requires a joint engineering effort between data scientists
    and service developers. For their part, data scientists need to upgrade the single-device
    training code to run distributedly, using code like the snippets in the previous
    section. Meanwhile, service developers must enhance the training service to automatically
    set up distributed worker groups that allow distributed training to happen.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 那么在生产环境中启用数据并行分布式训练的工作是什么样子的呢？首先，它需要数据科学家和服务开发人员的联合工程努力。对于数据科学家来说，他们需要升级单设备训练代码以分布式运行，使用类似前一节中的代码片段。与此同时，服务开发人员必须增强训练服务，以自动设置分布式工作组，以允许进行分布式训练。
- en: To make the training service user friendly, the service should incorporate the
    setup details for different distributed training frameworks. Therefore, data scientists
    have to define only the number of parallel instances they need for the training.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使训练服务用户友好，服务应该整合不同分布式训练框架的设置细节。因此，数据科学家只需定义他们在训练中所需的并行实例数。
- en: 'Let’s use TensorFlow distributed training as an example. From our discussion
    in section 4.2.3, the TensorFlow training code on each device must have `tf_config`
    (see the following example) as an environment variable. So the underlying TensorFlow-distributed
    library in the training process knows how to communicate with other training processes:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以 TensorFlow 分布式训练为例。从我们在第 4.2.3 节中的讨论中可以看出，每个设备上的 TensorFlow 训练代码必须将`tf_config`（见下面的示例）设置为环境变量。这样，在训练过程中底层的
    TensorFlow 分布式库就知道如何与其他训练进程通信了：
- en: '[PRE5]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: From a usability perspective, we can’t expect data scientists to figure out
    the setup value—server IP address and task indexes—for every distributed training
    process, especially if the entire training group is provisioned dynamically. A
    training service should automatically create the group of compute resources for
    a distributed training request, initialize the distributed training libraries
    with the correct IP addresses, and kick off the training process.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 从可用性的角度来看，我们不能指望数据科学家为每个分布式训练进程找出设置值——服务器 IP 地址和任务索引，特别是如果整个训练组都是动态分配的。一个训练服务应该自动为分布式训练请求创建一组计算资源，用正确的
    IP 地址初始化分布式训练库，并启动训练进程。
- en: Figure 4.2 is a conceptual diagram of a training service that supports distributed
    training. From the diagram, you can see that Alex, a data scientist, sends a training
    request to kick off a distributed training run. The service (built by Tang, the
    service developer) then spawns two worker machines and executes the training code
    distributedly. Besides preparing the training code, Alex can specify configurations
    for the training run, such as the number of parallel workers and the type of distributed
    training framework (TensorFlow, PyTorch, or Horovod).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 是支持分布式训练的训练服务的概念图。从图中可以看到，数据科学家 Alex 发送了一个训练请求来启动分布式训练任务。然后服务（由服务开发者 Tang
    构建）生成两个工作机器并分布式执行训练代码。除了准备训练代码外，Alex 还可以为训练任务指定配置，例如并行工作进程的数量和分布式训练框架的类型（TensorFlow、PyTorch
    或 Horovod）。
- en: '![](../Images/04-02.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-02.png)'
- en: Figure 4.2 An overview of a distributed training system
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 分布式训练系统概览
- en: Let’s take a slow walk through this diagram to better understand how the system
    is set up and who does what job. We see that Tang, as the engineer, needs to make
    three enhancements—numbered 1, 2, and 3 in figure 4.2—to change the training service
    from a single-device trainer (as we saw in chapter 3) to a data parallel–distributed
    trainer.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们缓慢地浏览一下这张图，以更好地了解系统是如何设置的，以及谁做了什么工作。我们看到，作为工程师的 Tang 需要进行三项增强——在图 4.2 中编号为
    1、2 和 3——来将训练服务从单设备训练器（正如我们在第三章中所见）转变为数据并行的分布式训练器。
- en: The first step is to update the training service to build a distributed training
    group on demand (at the runtime). When the service receives the request for distributed
    training, it allocates multiple workers from the training cluster for the training
    job and distributes the training code to each worker.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是更新训练服务，以在运行时按需构建分布式训练组。当服务收到分布式训练请求时，它从训练集群中为训练任务分配多个工作进程，并将训练代码分发给每个工作进程。
- en: The second step is to programmatically initialize each training process with
    the correct server IP, port number, and training process ID. This ensures that
    the distributed libraries (collectively known as the framework, such as TensorFlow)
    have enough information to set up interworker communication for the training group.
    As we saw in the previous section, the setup configuration varies for each distributed
    training framework. The training service should know how to set up interworker
    communication for various frameworks, so data scientists can focus only on the
    algorithm development and not worry about the infrastructure underneath.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步是以编程方式为每个训练进程初始化正确的服务器 IP、端口号和训练进程 ID。这确保了分布式库（通常称为框架，比如 TensorFlow）具有足够的信息来为训练组建立内部工作进程之间的通信。正如我们在前一节中看到的，对于每个分布式训练框架，设置配置都有所不同。训练服务应该知道如何为各种框架建立内部工作进程之间的通信，这样数据科学家就可以专注于算法开发，而不用担心底层基础架构。
- en: The third step is to provide remote storage to back up and restore each worker’s
    training state. In distributed training, if a single worker fails, the entire
    training group fails, and a great deal of computation is wasted. Thus, giving
    distributed training groups the capability to recover from a hardware failure
    or network problem is crucial. By providing remote storage and a backup API, the
    distributed training processes can save their training state (neural network)
    after each training iteration. When a training process fails in the middle of
    the training and can restore its previous state and start over, the entire training
    group continues.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 第三步是为每个工作节点提供远程存储，以备份和恢复其训练状态。在分布式训练中，如果一个工作节点失败，整个训练组失败，并且将浪费大量计算资源。因此，在发生硬件故障或网络问题时，使分布式训练组能够恢复是至关重要的。通过提供远程存储和备份API，分布式训练进程可以在每个训练迭代后保存其训练状态（神经网络）。当训练过程在训练中间发生故障时，可以恢复其先前的状态并重新开始，整个训练组也将继续进行。
- en: 'Note If you want to learn more about data parallelism, you can start with the
    following two articles: a blog post from O’Reilly, “Distributed TensorFlow: Reduce
    both experimentation time and training time for neural networks by using many
    GPU servers,” by Jim Dowling ([www.oreilly.com/content/distributed-tensorflow/](http://www.oreilly.com/content/distributed-tensorflow/)),
    and a paper from Google Brain, “Revisiting Distributed Synchronous SGD,” by Chen
    et al.([https://arxiv.org/pdf/1604.00981.pdf](https://arxiv.org/pdf/1604.00981.pdf)).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '注意：如果你想了解更多关于数据并行的知识，你可以从以下两篇文章开始：来自O''Reilly的博客文章“Distributed TensorFlow:
    Reduce both experimentation time and training time for neural networks by using
    many GPU servers” by Jim Dowling([www.oreilly.com/content/distributed-tensorflow/](http://www.oreilly.com/content/distributed-tensorflow/))，以及来自Google
    Brain的一篇论文“Revisiting Distributed Synchronous SGD” by Chen et al.([https://arxiv.org/pdf/1604.00981.pdf](https://arxiv.org/pdf/1604.00981.pdf))。'
- en: 4.3 A sample service supporting data parallel–distributed training
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 支持数据并行-分布式训练的示例服务
- en: In this section, we will extend the sample service introduced in the previous
    chapter (section 3.3) to support data parallelism–distributed training.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将扩展上一章（第3.3节）介绍的示例服务，以支持数据并行-分布式训练。
- en: 4.3.1 Service overview
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.1 服务概述
- en: Compared to the single-device training discussed in section 3.3, the user workflow
    remains the same. Alex, the data scientist, first builds the model training code
    and sends a training request to the training service. Then, the service runs the
    actual training and produces the model at the end.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 与第3.3节讨论的单设备训练相比，用户工作流程保持不变。数据科学家Alex首先构建模型训练代码，并向训练服务发送训练请求。然后，服务运行实际训练，并在最后生成模型。
- en: However, there are some crucial differences. First, Alex upgrades the intent
    classification training code to enable it for both a single device and multiple
    devices. Second, Tang, the service developer, modifies the training service API
    to offer a new parameter, `PARALLEL_INSTANCES`. This parameter allows Alex to
    define the size of the worker group for his distributed training run.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，有一些关键区别。首先，Alex升级了意图分类训练代码，使其能够适用于单设备和多设备。其次，服务开发者Tang修改了训练服务API，提供了一个新的参数`PARALLEL_INSTANCES`。该参数允许Alex为分布式训练设置工作组的大小。
- en: To manage a cluster of servers properly, we need help from Kubernetes. Kubernetes
    can save us a lot of effort on worker resource allocation and interworker communication.
    So we introduce a new component—the *Kubernetes job tracker*—to manage training
    jobs in Kubernetes. You can see the updated service design graph and user workflow
    in figure 4.3.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 要正确管理服务器集群，我们需要Kubernetes的帮助。Kubernetes可以在工作资源分配和工作节点通信方面为我们节省大量努力。因此，我们引入了一个新组件——*Kubernetes作业追踪器*，用于在Kubernetes中管理训练作业。你可以在图4.3中看到更新后的服务设计图和用户工作流程。
- en: '![](../Images/04-03.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-03.png)'
- en: Figure 4.3 (a) The previous training service design introduced in figure 3.5;
    (b) the updated service design with distributed training support in Kubernetes
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3(a)：之前在图3.5中介绍的训练服务设计；(b) 在Kubernetes上具备分布式训练支持的更新服务设计
- en: Figure 4.3 (a) repeats the training service’s system diagram we discussed in
    section 3.3, which uses a Docker job tracker to run the training jobs in the Docker
    engine. Figure 4.3 (b) visualizes the updated training service that now supports
    distributed training—including both Kubernetes and Docker engine backends. The
    Kubernetes job tracker is added to run training jobs in the Kubernetes cluster
    for distributed training jobs. This component executes training jobs by launching
    Kubernetes pods and monitors and updates the job-execution status in the memory
    store.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3 (a) 重复了我们在第 3.3 节讨论的训练服务的系统图，该系统使用 Docker 作业跟踪器在 Docker 引擎中运行训练作业。图 4.3
    (b) 可视化了现在支持分布式训练的更新后的训练服务，包括 Kubernetes 和 Docker 引擎后端。Kubernetes 作业跟踪器被添加以在 Kubernetes
    集群中运行分布式训练作业。该组件通过启动 Kubernetes pod 来执行训练作业，并监视和更新内存存储中的作业执行状态。
- en: We also made some changes to the intent classification PyTorch training code
    so it can run distributedly. We’ll review this shortly, in section 4.3.5.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还对意图分类 PyTorch 训练代码进行了一些更改，以便可以分布式运行。我们将在稍后的第 4.3.5 节中简要回顾这一点。
- en: 'One great timesaver is that we don’t need to change the service API interface
    that we’ve already created (section 3.3.3). Our users can simply work the same
    API to train models in both Docker engines and Kubernetes clusters. This follows
    training service principle number one, which we introduced in chapter 3 (section
    3.1.2): using unified APIs and keeping them agnostic on the backend implementation.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很好的时间节省者是，我们不需要更改已经创建的服务 API 接口（第 3.3.3 节）。我们的用户可以简单地使用相同的 API 在 Docker 引擎和
    Kubernetes 集群中训练模型。这符合我们在第 3 章中介绍的训练服务原则之一（第 3.1.2 节）：使用统一的 API 并使其在后端实现上保持不可知性。
- en: 4.3.2 Playing with the service
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.2 与服务交互
- en: 'First, let’s run the training service with the Kubernetes backend; see the
    commands as follows (`scripts/ts-001-start-server-kube.sh`):'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们使用 Kubernetes 后端运行训练服务；请参见以下命令（`scripts/ts-001-start-server-kube.sh`）：
- en: '[PRE6]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Local Kubernetes config
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 本地 Kubernetes 配置
- en: Note This section contains only the main steps and key commands necessary to
    run the sample service. As a result, the concept can be demonstrated clearly without
    lengthy pages of code and execution output. Please follow the instructions in
    the “Distributed trainer training demo” ([github.com/orca3/MiniAutoML/blob/main/training-service/distributed_trainer_demo.md](http://github.com/orca3/MiniAutoML/blob/main/training-service/distributed_trainer_demo.md))
    document at the orca3/MiniAutoML git repository if you want to run the lab in
    this section.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 本节仅包含运行示例服务所需的主要步骤和关键命令。因此，可以清晰地演示概念，而不需要冗长的代码页和执行输出。如果您想在本节运行实验，请按照 orca3/MiniAutoML
    存储库中的 “Distributed trainer training demo”（[github.com/orca3/MiniAutoML/blob/main/training-service/distributed_trainer_demo.md](http://github.com/orca3/MiniAutoML/blob/main/training-service/distributed_trainer_demo.md)）文档中的说明进行操作。
- en: 'Once the training service container is running, we can submit a training gRPC
    request. Although the service is now running on the Kubernetes backend, the training
    API is still the same. Compared to the training request we sent to the Docker
    backend demo (see section 3.3.1), only one more parameter—`PARALLEL_INSTANCES=3`—is
    added in the request payload. This tells the training service to create a distributed
    training group with three workers to train the model. If we set this parameter
    to 1, it will be a single-device training request. See the following code snippet
    to submit a distributed training request with three parallel instances (`scripts/ts-004-start-parallel-run.sh`
    `1`):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练服务容器正在运行，我们就可以提交一个训练 gRPC 请求。尽管服务现在在 Kubernetes 后端运行，但训练 API 仍然保持不变。与我们发送给
    Docker 后端演示的训练请求（请参见第 3.3.1 节）相比，请求有效负载中仅添加了一个额外参数 — `PARALLEL_INSTANCES=3`。这告诉训练服务创建一个包含三个工作节点的分布式训练组来训练模型。如果我们将此参数设置为
    1，则是单设备训练请求。查看以下代码片段以提交一个包含三个并行实例的分布式训练请求（`scripts/ts-004-start-parallel-run.sh`
    `1`）：
- en: '[PRE7]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Requires a training group with three workers
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 需要一个包含三个工作节点的训练组
- en: 'To check the progress of the training execution, we can use the `GetTrainingStatus`
    API:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查训练执行的进度，我们可以使用 `GetTrainingStatus` API：
- en: '[PRE8]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Provides job ID to query status
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 提供作业 ID 以查询状态
- en: 'Besides querying the training service API to obtain job-execution status, we
    can also check the training progress in Kubernetes. By using the Kubernetes command
    `kubectl` `get` `all`, we see three worker pods are created in the local Kubernetes
    environment. One is the master worker, and the other two are normal workers. A
    Kubernetes service object `intent-classification-1-master-service` is also created
    for the master worker/pod, which enables the network connectivity between master
    pods and worker pods. See the code snippet as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 除了查询训练服务 API 以获取作业执行状态外，我们还可以在 Kubernetes 中检查训练进度。使用 Kubernetes 命令`kubectl`
    `get` `all`，我们可以看到在本地 Kubernetes 环境中创建了三个工作 Pod。其中一个是主工作 Pod，另外两个是普通工作 Pod。还创建了一个
    Kubernetes 服务对象 `intent-classification-1-master-service` 用于主工作 Pod，它使主 Pod 和工作
    Pod 之间具有网络连接性。代码段如下：
- en: '[PRE9]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ One of the worker pods
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 工作 Pod 的其中之一
- en: ❷ Mastering the training pod
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 控制训练 Pod
- en: ❸ The Kubernetes service for training pods communication
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 用于训练 Pod 通信的 Kubernetes 服务
- en: 4.3.3 Launching training jobs
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.3 启动训练作业
- en: Now, let’s look at the workflow for launching training jobs with the Kubernetes
    backend. When receiving a training request, the request will be added to the job
    queue. Meanwhile, the Kubernetes job tracker monitors the job queue. When the
    tracker finds jobs waiting and the system has available capacity, it will start
    to process these jobs.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看使用 Kubernetes 后端启动训练作业的工作流程。当接收到训练请求时，将该请求添加到作业队列中。同时，Kubernetes 作业跟踪器会监视作业队列。当跟踪器发现等待的作业并且系统具有可用的容量时，它将开始处理这些作业。
- en: To launch a PyTorch-distributed training job, the tracker first creates the
    required numbers of Kubernetes pods. Each pod hosts one training process. The
    tracker also passes separate parameters to each pod, and it then moves the job
    from the job queue to the launching list (figure 4.4).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动一个 PyTorch 分布式训练作业，跟踪器首先创建所需数量的 Kubernetes Pod。每个 Pod 托管一个训练进程。跟踪器还向每个 Pod
    传递独立的参数，然后将作业从作业队列移动到启动列表（图4.4）。
- en: '![](../Images/04-04.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-04.png)'
- en: 'Figure 4.4 The workflow for launching a training job in Kubernetes: step 1,
    detects the waiting job in the job queue; step 2, creates Kubernetes pods to run
    training; and step 3, moves the job from the job queue to the launching list.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4 显示了在 Kubernetes 中启动训练作业的工作流程：第1步，在作业队列中检测等待的作业；第2步，创建 Kubernetes Pod 来运行训练；第3步，将作业从作业队列移动到启动列表。
- en: In figure 4.4, the Kubernetes job tracker can handle both single-device training
    and distributed training. It creates one Kubernetes pod for single-device training
    and multiple pods for distributed training.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在图4.4中，Kubernetes 作业跟踪器可以处理单设备训练和分布式训练。它为单设备训练创建一个 Kubernetes Pod，并为分布式训练创建多个
    Pod。
- en: A Kubernetes job tracker runs one training pod similarly to a Docker job tracker.
    It wraps up all the user-defined parameters in the environment variables and passes
    them to the Kubernetes pod.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 Kubernetes 作业跟踪器类似于一个 Docker 作业跟踪器，运行一个训练 Pod。它将所有用户定义的参数封装在环境变量中，并将它们传递到
    Kubernetes Pod。
- en: 'To set up PyTorch distributed training with multiple pods, the service handles
    two more functions. First, it creates a Kubernetes service object to talk to the
    master pod. From the PyTorch distributed training algorithm section (4.2.3), we
    know that each PyTorch training process needs the IP address of the master process
    (pod) to initialize the distributed training group. For example, each PyTorch
    code needs to have the following code snippet before the training logic starts:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用多个 Pod 设置 PyTorch 分布式训练，该服务处理两个功能。首先，它创建一个 Kubernetes 服务对象来与主 Pod 通信。从PyTorch分布式训练算法部分（4.2.3）我们知道，每个PyTorch训练过程都需要主过程（Pod）的IP地址来初始化分布式训练组。例如，在训练逻辑开始之前，每个PyTorch代码需要具有以下代码片段：
- en: '[PRE10]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Joins the current process to a distributed group by seeking the master pod
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加入到分布式组中的当前进程，通过寻找主 Pod
- en: But in Kubernetes, a pod is an ephemeral resource, so we can’t rely on the pod
    IP address to locate a pod. Instead, we use the Kubernetes domain name service
    (DNS) as a permanent address to locate pods. Even if the pod is destroyed and
    recreated in a different node and the IP is different, we can always use the same
    DNS to reach it. So, to enable the training group’s initialization, we first create
    a Kubernetes service for the master pod and then pass the DNS to all worker pods
    as the master pod address.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 但是在 Kubernetes 中，pod 是一个短暂的资源，所以我们不能依赖 pod 的 IP 地址来定位一个 pod。相反，我们使用 Kubernetes
    域名服务 (DNS) 作为永久地址来定位 pod。即使 pod 在不同节点被销毁和重建，IP 不同，我们仍然可以使用相同的 DNS 来达到它。所以，为了启用训练组的初始化，我们首先为主
    pod 创建一个 Kubernetes 服务，然后将 DNS 传递给所有工作节点作为主 pod 地址。
- en: 'Second, it passes four environment variables to each pod. The four variables
    required by each training pod are `WORLD_SIZE`, `RANK`, `MASTER_ADDR`, and `MASTER_PORT`:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，它向每个 pod 传递了四个环境变量。每个训练 pod 需要的四个变量是 `WORLD_SIZE`、`RANK`、`MASTER_ADDR` 和
    `MASTER_PORT`：
- en: '`WORLD_SIZE` means the total number of pods of the training group, including
    master and workers.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WORLD_SIZE` 表示训练组中包括主节点和工作节点在内的总 pod 数。'
- en: '`RANK` is the unique ID of one training process; the master process’s rank
    must be 0.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RANK` 是一个训练过程的唯一 ID；主进程的 rank 必须为 0。'
- en: '`MASTER_ADDR` and `MASTER_PORT` define the host address and port number of
    the master process, so each worker can use them to reach the master pod.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MASTER_ADDR` 和 `MASTER_PORT` 定义了主进程的主机地址和端口号，因此每个工作节点可以用它们来达到主 pod。'
- en: 'For example, when running distributed training with three instances, we create
    three pods (one master, two workers) with the following environment variables
    for each pod:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 举例来说，当使用三个实例进行分布式训练时，我们为每个 pod 创建三个环境变量（一个主节点，两个工作节点）：
- en: '[PRE11]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In light of all the explanations, let’s take a look at how the actual code is
    implemented. The following listing highlights how launching distributed training
    in Kubernetes is implemented.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 综合以上各种解释，让我们一起来看看实际代码是如何实现的。以下列表突出了在 Kubernetes 中如何实现启动分布式训练。
- en: Listing 4.1 Launching a distributed training job
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.1 启动分布式训练作业
- en: '[PRE12]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '❶ World size >1: indicates it’s a distributed training'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '❶ World size >1: 表示这是一个分布式训练'
- en: ❷ Creates a Kubernetes service and points to the master pod
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建一个 Kubernetes 服务，并指向主 pod
- en: ❸ Sets the distributed training–related config as the environment variable
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 设置与分布式训练相关的配置作为环境变量
- en: ❹ Defines pod configuration; passes in training parameters as environment variables
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 定义 pod 配置；将训练参数作为环境变量传递
- en: ❺ Creates actual training pods
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 创建实际的训练 pod
- en: '**RANK** values do not neccesarily map to pods one to one'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '**RANK** 值不一定一一对应于 pod'
- en: '`RANK` is a tricky variable in distributed training. Please be aware that `RANK`
    is the unique ID of a training process, not a pod. A pod can run multiple training
    processes if it has multiple GPUs. In the example here, because we run one training
    process per pod, we assign a different `RANK` value to each pod.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`RANK` 是分布式训练中的一个棘手变量。请注意，`RANK` 是训练过程的唯一 ID，而不是一个 pod。如果一个 pod 拥有多个 GPU，则可以运行多个训练过程。在这个示例中，因为我们每个
    pod 运行一个训练过程，所以我们为每个 pod 分配一个不同的 `RANK` 值。'
- en: When we run multiple training processes in one pod, then we need to assign multiple
    `RANK` values to a pod. For example, when we run two processes in a pod, this
    pod needs two `RANK` values, one for each process.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在一个 pod 中运行多个训练过程时，我们需要为一个 pod 分配多个 `RANK` 值。例如，当我们在一个 pod 中运行两个进程时，这个 pod
    需要两个 `RANK` 值，一个用于每个进程。
- en: You may notice that the Kubernetes pods and services created in this sample
    are customized for PyTorch distributed training library. In fact, the sample service
    is not limited to PyTorch. To support training code written in other frameworks,
    such as TensorFlow 2, we can extend the Kubernetes job tracker to support the
    settings for TensorFlow distributed training.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能注意到，此示例中创建的 Kubernetes pod 和服务是针对 PyTorch 分布式训练库进行定制的。实际上，该示例服务并不局限于 PyTorch。为了支持使用其他框架编写的训练代码，例如
    TensorFlow 2，我们可以扩展 Kubernetes 作业跟踪器以支持 TensorFlow 分布式训练的设置。
- en: For example, we can collect all the IPs or DNSs of the worker pods, put them
    together, and broadcast them back to each worker pod. During the broadcasting,
    we set worker group information to the `TF_CONFIG` environment variable in every
    pod to start the distributed training group. The `TF_CONFIG` environment variable
    is a special requirement for the TensorFlow distributed library.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.4 Updating and fetching the job status
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After creating training pods, the Kubernetes job tracker will continue querying
    the pod execution status and move the job to other job lists when its status changes.
    For example, if the pod is created successfully and starts running, the tracker
    moves the job from the launching list to the running list. If the pod execution
    is completed, the tracker moves the job from the running list to the finalized
    jobs list. Figure 4.5 depicts this process.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/04-05.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.5 Track the Kubernetes training job status: step 1, obtains the jobs
    in the running list; step 2, queries the pod execution status of each of the jobs
    running in the Kubernetes cluster; and step 3, moves the job to the finalized
    job list if the pod execution is complete (success or failure).'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: When a user submits a job status query, the training service will search the
    job ID in all four job queues in the memory store and return the job object. Interestingly,
    although there are multiple training pods, we only need to check the status of
    the master pod to track the distributed training progress. This is because, for
    synchronous data parallel training, all workers have to sync with each other in
    every training cycle, so the master pod can represent the other worker pods.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: The code for querying and updating job execution status is very similar to the
    Docker job tracker that we see in section 3.3.5\. The only difference is that
    we query the Kubernetes cluster instead of the Docker engine to obtain the training
    status. We leave the code for you to explore; you can find it in the `updateContainerStatus`
    method of the `KubectlTracker` class.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.5 Converting the training code to run distributedly
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We made two changes to our intent classification training code (introduced in
    the previous chapter, section 3.3.6) to support both distributed mode and single-device
    mode.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'First change: Initialize the training group'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: We use the `WORLD_SIZE` environment variable to check whether the training code
    should run in distributed training. If the world size equals 1, then we use the
    same single-device training code that we saw in section 3.3.6.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'But if the value is greater than 1, we initialize the training process to join
    the distributed group. Please also notice that a unique `RANK` value for each
    pod is passed from the training service (Kubernetes job tracker), which is needed
    for distributed group initialization. After self-registering to the distributed
    group, we declare the model and data sampler to be distributed as well. See the
    following code for the changes:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Second change: Only upload the final model from the master pod'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'In the second change, we only allow the master pod (rank = 0) to upload the
    final model. This is to prevent each worker from uploading the same models multiple
    times:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Rank 0 is the master pod.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.6 Improvements
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If we continue the path to making this sample service production ready, we can
    follow the thoughts in section 4.2.2 to work on improving fault tolerance and
    reducing network bandwidth saturation. We can also extend the Kubernetes job tracker
    to support TensorFlow and Horovod distributed training. From a training service
    perspective, they are not very different because the configuration that the training
    service passes to the training code is very generic; this information is needed
    for all frameworks but with different names. As long as the protocol between the
    training service and the training code is clear and stable, we can still treat
    the training code as a black box, even in the distributed setting.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Training large models that can’t load on one GPU
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The neural network size (defined by the number of parameters) is growing rapidly
    in the research field, and we cannot ignore this trend. Using the ImageNet challenge
    as an example, the winner in 2014 (GoogleNet) had 4 million parameters; the winner
    in 2017 (Squeeze-and-Excitation Networks) had 145.8 million parameters; and the
    current leading approaches have more than 1 billion parameters.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Although our neural network size grew nearly 300×, GPU memory has only increased
    4×. You will see cases more often in the future in which we can’t train a model
    because it can’t be loaded onto one GPU.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will discuss common strategies for training large models.
    Unlike the data parallelism strategy described in section 4.2, the method introduced
    here requires effortful work on training code.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Note Although the methods introduced in this section are normally implemented
    by data scientists, we hope you can still follow them. Understanding the strategies
    behind these training techniques is very helpful for designing communication protocols
    between training services and training codes. It also provides insight into troubleshooting
    or fine-tuning the training performance in training service. To keep it simple,
    we will only describe algorithms at the concept level and focus on the necessary
    work from an engineering perspective.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '4.4.1 Traditional methods: Memory saving'
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s say your data science team wants to train a model that can load to the
    largest GPU in your training cluster; for example, they want to train a 24 GB
    BERT model in a 10 GB memory GPU. There are several memory-saving techniques the
    team can use to train the model in this situation, including gradient accumulation
    and memory swap. This work is generally implemented by data scientists. As a platform
    developer, you just need to be aware of these options. We’ll describe them briefly,
    so you will know when to suggest each of their use.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: note There are several other memory-saving methods, such as OpenAI’s gradient
    checkpointing ([https://github.com/cybertronai/gradient-checkpointing](https://github.com/cybertronai/gradient-checkpointing))
    and NVIDIA’s vDNN ([https://arxiv.org/abs/1602.08124](https://arxiv.org/abs/1602.08124)),
    but because this book is not about deep learning algorithms, we will leave them
    for independent study.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Gradient accumulation
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: In deep learning training, the dataset is split into batches. In each training
    step, for loss calculation, gradient computation, and model parameter updating,
    we take the whole batch of examples (training data) into memory and handle the
    computations all at once.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: We can mitigate the memory pressure by reducing the batch size—for example,
    training 16 examples in a batch rather than 32 examples in a batch. But reducing
    batch size can cause the model to converge a lot more slowly. And this is where
    gradient accumulation can be helpful.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Gradient accumulation cuts batch examples into configurable numbers of minibatches
    and then calculates the loss and gradients after each minibatch. But instead of
    updating the model parameters, it waits and accumulates the gradients over all
    the minibatches. And then, ultimately, it updates the model parameters based on
    the cumulative gradient.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at an example to see how this speeds up the process. Imagine that,
    because of GPU memory constraints, we can’t run training with a batch size of
    32\. With gradient accumulation, we can split each batch into four minibatches,
    each with a size of 8\. Because we accumulate the gradients for all four minibatches
    and only update the model after all four are complete, the process is almost equal
    to training with a batch size of 32\. The difference is that we only compute 8
    examples at a time in GPU instead of 32, so the cost is 4× *slower* than it would
    be with a batch of 32.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Memory swap (GPU and CPU)
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'The memory swap method is very simple: it copies activations between CPU and
    GPU, back and forth. If you are unaccustomed to deep learning terms, think of
    *activation* as the computation output from each node of the neural network. The
    idea is to only keep the necessary data for the current computation step in GPU
    and swap the compute result out to CPU memory for future steps.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Building on this idea, a new relay-style execution technique called L2L (layer
    to layer) keeps only the executing layers and transit buffers on the GPU. The
    whole model and the optimizer—which holds the state—are stored in the CPU space.
    L2L can greatly increase the GPU throughput and allow us to develop large models
    on affordable devices. If you are interested in this method, you can check out
    the paper “Training Large Neural Networks with Constant Memory Using a New Execution
    Algorithm,” by Pudipeddi et al. ([https://arxiv.org/abs/2002.05645](https://arxiv.org/abs/2002.05645)),
    which also has a PyTorch implementation in GitHub.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'Both gradient accumulation and memory swap are effective ways to train a large
    model on a smaller GPU. But, like most things, they come with a cost: they tend
    to slow down the training. Because of this drawback, we normally use them only
    for prototyping ideas.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'To obtain workable training speeds, we really need to train models distributedly
    on multiple GPUs. So, in the next section, we will introduce a more production-like
    approach: pipeline parallelism. It can train a large model distributedly with
    impressive training speed.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.2 Pipeline model parallelism
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In section 4.2, we discussed the most commonly used distributed training method:
    data parallelism. This approach keeps a copy of the whole model on each device
    and partitions data into multiple devices. Then it aggregates the gradients and
    updates the model in each training step. The whole approach of data parallelism
    works well, as long as the entire model can be loaded into one GPU. As we see
    in this section, however, we are not always able to do this. And that is where
    pipeline parallelism can be useful. In this section, we will learn about pipeline
    parallelism, a training method that trains large models distributedly on multiple
    GPUs.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: To understand pipeline parallelism, let’s first take a brief look at model parallelism.
    This little detour will make the jump to pipeline parallelism easier.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Model parallelism
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: The idea of model parallelism is to split a neural network into smaller subnets
    and run each subnet on different GPUs. Figure 4.6 illustrates the model parallelism
    approach.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/04-06.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 Split a four-layer, fully connected deep learning network into four
    subgroups; each group has one layer, and each subgroup runs on one GPU.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.6 visualizes the model parallel process. It first converts a neural
    network (four layers) into four sub–neural networks (single layer) and then assigns
    each single-layer network a dedicated GPU. By doing so, we run a model distributedly
    on four GPUs.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: The concept of model parallelism is straightforward, but the actual implementation
    can be tricky; it depends on the architecture of the network. To give you an idea,
    the following listing is a piece of dummy PyTorch code that makes a network run
    on two GPUs.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.2 A sample model parallelism implementation in PyTorch
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As you can see in listing 4.2, two subnetworks are initialized and assigned
    to two GPUs in the `__init__` function, and then they are connected in the `forward`
    function. Because of the variety of structures of deep learning networks, no general
    method (paradigm) exists to split the network. We must implement model parallelism
    case by case.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Another problem with model parallelism is its severe underutilization of GPU
    resources. Because all the devices in the training group have sequential dependency,
    only one device can work at a time, which wastes a lot of GPU cycles. Figure 4.7
    visualizes the GPU utilization situation for model parallel training with three
    GPUs.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/04-07.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 Model parallel training can have severely low GPU usage. In this
    approach, the network is split into three subnets and runs on three GPUs. Because
    of the sequential dependency among the three GPUs, each GPU is idle 66% of the
    training time.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Let’s walk through this figure to see why GPU usage is so low. On the left,
    in figure 4.7 (a), we see the model parallel design. We split a model network
    into three subnetworks and let each subnetwork run on a different GPU. In each
    training iteration, when running the forward pass, we first compute subnet 1 and
    then subnet 2 and subnet 3; when running the backward pass, the gradient update
    happens reversely.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'In figure 4.7 (b), on the right, you can see the resource utilization of the
    three GPUs during the training. The time axis is divided into two parts: the forward
    pass and the backward pass. The forward pass means the computation of the model
    inference, from GPU 1 to GPU 2 and GPU3, whereas the backward pass means backpropagation
    for the model weights update, from GPU 3 to GPU 2 and GPU 1.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: If you look vertically at the time bar, regardless of whether it’s a forward
    pass or a backward pass, you see only one GPU active at a time. This is because
    of the sequential dependency between each subnet. For instance, in the forward
    pass, subnet 2 needs to wait for subnet 1’s output to fulfill its own forward
    calculation, so GPU 2 will be idle in the forward pass until the calculation on
    GPU 1 completes.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: No matter how many GPUs you add, only one GPU can work at one time, which is
    a huge waste. This is when pipeline parallelism comes in handy. Pipeline parallelism
    makes model training more efficient by eliminating that waste and fully saturating
    the GPUs. Let’s see how it works.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline parallelism
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline parallelism is essentially an improved version of model parallelism.
    In addition to partitioning a network to different GPUs, it also divides each
    training example batch into small minibatches and overlaps computations of these
    minibatches between layers. By doing so, it keeps all GPUs busy most of the time,
    thus improving GPU utilization.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two major implementations of this approach: PipeDream (Microsoft)
    and GPipe (Google). We use GPipe as the demo example here because it optimizes
    the gradients’ update in each training step and has better training throughput.
    You can find further details about GPipe from “GPipe: Easy scaling with micro-batch
    pipeline parallelism,” by Huang et al. ([https://arxiv.org/abs/1811.06965](https://arxiv.org/abs/1811.06965)).
    Let’s look, in figure 4.8, at how GPipe works at a high level.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/04-08.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.8 (a) An example neural network with sequential layers is partitioned
    across four accelerators. F[k] is the composite forward computation function of
    the *k*th cell. Bk is the backpropagation function, which depends on both B[k+1],
    from the upper layer, and F[k]. (b) The naive model parallelism strategy leads
    to severe under utilization due to the sequential dependency of the network. (c)
    Pipeline parallelism divides the input minibatch into smaller microbatches, enabling
    different accelerators to work on different microbatches simultaneously. Gradients
    are applied synchronously at the end. (Source: figure 2, “GPipe: Easy Scaling
    with Micro-Batch Pipeline Parallelism,” Huang et al., 2019, arXiv:1811.06965)'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.8 (a) depicts a neural network made of four subnetworks; each subnetwork
    is loaded on one GPU. F means forward pass, B means backward pass, and F[k] and
    B[k] run on GPUk. The training sequence is first, forward pass, F[0] -> F[1] ->
    F[2] -> F[3], and second, backward pass, F[3] -> (B[3], F[2]) -> (B[2], F[2])
    -> (B[1], F[1]) -> B[0].
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.8 (b) displays the training flow for naive model parallelism. We can
    see that the GPU is seriously underutilized; only one GPU is activated in the
    forward- and backward pass; thus, each GPU is idle 75% of the time.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.8 (c) shows the GPipe improvements in the sequence of training operations.
    GPipe first divides every training example batch into four equal microbatches,
    which are pipelined through the four GPUs. F[(0,2)] in the graph means forward
    pass computation at GPU 0 with minibatch 2\. During the backward pass, gradients
    for each microbatch are computed based on the same model parameters used for the
    forward pass. The key is that it doesn’t update model parameters immediately;
    instead, it accumulates all the gradients for each microbatch. At the end of each
    training batch, we use the accumulated gradients from all four microbatches to
    update the model parameters across all four GPUs.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 'By comparing figure 4.8 (b) and (c), we see the GPU utilization increase greatly;
    now each GPU is idle 47% of the time. Let’s see a code example using PyTorch GPipe
    implementation to train a transformer model on two GPUs (see following listing).
    To demo the idea clearly, we keep only the pipeline-related code and partition
    them into four parts. You can check out the tutorial “PyTorch: Training transformer
    models using pipeline parallelism,” by Pritam Damania, for the full code ([http://mng.bz/5mD8](http://mng.bz/5mD8)).'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.3 Training transformer models using pipeline parallelism
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As we can see from listing 4.3, pipeline parallelism code is much more complicated
    than distributed data parallelism. Besides setting up the communication group,
    we also need to consider how to divide our model network and transfer gradients
    and activation (model the subnetwork’s forward output) in interworker communication.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.3 How software engineers can support pipeline parallelism
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You may have noticed that all the methods we talk about in this section are
    techniques for writing training code. Because data scientists normally write the
    training code, you may be wondering what we, as software developers, can do to
    support pipeline parallel training.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: First, we can work on building the training service to automate the pipeline
    training execution and improve resource utilization (for example, always keeping
    the GPU busy). This automation includes matters like allocating worker resources,
    enabling interworker communication, and distributing the pipeline training code
    with corresponding initialized parameters to each worker (such as worker IP address,
    process ID, GPU ID, and worker group size).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Second, we can alert the data scientist team about the new distributed training
    options. Sometimes the data scientist team isn’t aware of the new engineering
    methods that can improve the model training experience, so communication is key
    here. We can collaborate with members of the team and lead the conversation about
    experimenting with the pipeline parallelism method.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Third, we can work on improving the availability of model training. In section
    4.2.4, we discussed that distributed training is fragile; it requires every worker
    to perform consistently. If one worker fails, the entire training group fails,
    which is a huge waste of time and budget. The effort spent on training-process
    monitoring, failover, and failure recovery would be much appreciated by data scientists.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Data parallelism or pipeline parallelism?
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we know that there are two major strategies for distributed training: data
    parallelism and pipeline parallelism. You might understand these concepts, but
    you might still be uncertain about when to use them.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: We would suggest always starting with model training on a single machine. If
    you have a large dataset and the training takes a long time, then consider distributed
    training. We always prefer data parallelism over pipeline parallelism merely because
    data parallelism is simpler to implement and we can obtain results quicker. If
    the model is so big that it can’t load on one GPU, then pipeline parallelism is
    the right choice.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Distributed training has two schools of thought: data parallelism and model
    parallelism. Pipeline parallelism is an improved version of model parallelism.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a model can be loaded into one GPU, data parallelism is the primary method
    to implement distributed training; it’s simple to use and provides great speed
    improvements.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Kubernetes to manage the computing cluster can greatly reduce the complexity
    of compute resource management.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although each training framework (TensorFlow, PyTorch) offers different configurations
    and APIs to write distributed training code, their code pattern and execution
    workflow are very similar. Thus, a training service can support the various distributed
    training codes with a unified approach.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After encapsulating the setup configuration of various training frameworks,
    a training service can still treat training code as a black box, even in the distributed
    training setting.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To obtain data parallelism training progress/status, you only need to check
    the master worker because all workers are always in sync with each other. Also,
    to avoid saving duplicated models from all workers when their training jobs complete,
    you can set the training code to persist model and checkpoint files only when
    the code is executed by the master worker.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Horovod is a great distributed training framework. It offers a unified method
    to run distributed training for code written in various frameworks: PyTorch, TensorFlow,
    MXNet, and PySpark. If a training code uses Horovod to implement distributed training,
    a training service can use a single method (the Horovod method) to execute it,
    regardless of the training frame with which it’s written.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Availability, resilience, and failure recovery are important engineering concerns
    for distributed training.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are two strategies for training a model that does not fit into one GPU:
    the memory-saving method and the model parallelism method.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The memory-saving method loads only a portion of the model or a small data batch
    to GPU at a time—for instance, gradient accumulation and memory swap. These methods
    are easy to implement but slow down the model training process.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model parallelism method divides a large model into a group of sub–neural
    networks and distributes them onto multiple GPUs. The downside of this approach
    is low GPU utilization. To overcome that, pipeline model parallelism was invented.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
