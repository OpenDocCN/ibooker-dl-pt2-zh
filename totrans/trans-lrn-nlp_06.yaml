- en: 4 Shallow transfer learning for NLP
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 自然语言处理的浅层迁移学习
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包括
- en: Using pretrained word embeddings in a semisupervised fashion to transfer pretrained
    knowledge to a problem
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以半监督的方式使用预训练的词嵌入将预训练知识转移到问题中
- en: Using pretrained embeddings of larger sections of text in a semisupervised fashion
    to transfer pretrained knowledge to a problem
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以半监督的方式使用预训练的较大文本部分的嵌入来将预训练知识转移到问题中
- en: Using multitask learning to develop better-performing models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用多任务学习来开发性能更好的模型
- en: Modifying target domain data to reuse knowledge from a resource-rich source
    domain
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修改目标域数据以重用来自资源丰富的源域的知识
- en: In this chapter, we will cover some prominent shallow transfer learning approaches
    and concepts. This allows us to explore some major themes in transfer learning,
    while doing so within the context of relatively simple models in the class of
    eventual interest—shallow neural networks. Several authors have suggested various
    classification systems for categorizing transfer learning methods into groups.[¹](#pgfId-1092394),[²](#pgfId-1092397),[³](#pgfId-1092400)
    Roughly speaking, categorization is based on whether transfer occurs between different
    languages, tasks, or data domains. Each of these types of categorization is usually
    correspondingly referred to as *cross-lingual learning*, *multitask learning*,
    and *domain adaptation*, as visualized in figure 4.1.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖一些重要的浅层迁移学习方法和概念。这使我们能够探索迁移学习中的一些主要主题，同时在感兴趣的最终类别——浅层神经网络类别的背景下进行。几位作者已经提出了将迁移学习方法分类到不同组别中的各种分类系统。[¹](#pgfId-1092394),[²](#pgfId-1092397),[³](#pgfId-1092400)
    大致来说，分类是基于迁移是否发生在不同的语言、任务或数据域之间。每种类型的分类通常相应地被称为 *跨语言学习*、*多任务学习* 和 *领域自适应*，如图4.1所示。
- en: '![04_01](../Images/04_01.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![04_01](../Images/04_01.png)'
- en: Figure 4.1 Visualizing the categorization of transfer learning into multitask
    learning, domain adaptation, and cross-lingual learning
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1 将迁移学习划分为多任务学习、领域自适应和跨语言学习的可视化分类
- en: The methods we will look at here will involve components that are neural networks
    in one way or another, but unlike those discussed in chapter 3, these neural networks
    do not have many layers. This is the reason the label “shallow” is appropriate
    to describe this collection of methods. Just as in the previous chapter, we present
    these methods in the context of specific hands-on examples to facilitate the advancement
    of your practical NLP skills. Cross-lingual learning will be addressed in later
    chapters of the book, given that modern neural machine translation methods are
    deep in general. We will explore the other two types of transfer learning briefly
    in this chapter.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里看到的方法涉及到某种程度上是神经网络的组件，但不像第3章中讨论的那样，这些神经网络没有很多层。这就是为什么标签“浅层”适合描述这些方法集合的原因。与上一章一样，我们将这些方法放在特定的实际例子的背景下，以促进您的实际自然语言处理技能的提升。跨语言学习将在本书的后续章节中讨论，因为现代神经机器翻译方法通常是深层的。我们将在本章中简要探讨另外两种迁移学习。
- en: We begin by exploring a common form of semisupervised learning that employs
    pretrained word embeddings, such as word2vec, in the context of applying it to
    one of the examples from the previous two chapters of the book. Recall from chapter
    1 that these methods differ from those in chapter 3 in that they produce a single
    vector per word, regardless of context.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先探讨了一种常见的半监督学习形式，它使用了预训练的词嵌入，如word2vec，将其应用于本书前两章中的一个示例。请回忆第1章，这些方法与第3章中的方法不同，因为它们产生每个单词一个向量，而不考虑上下文。
- en: We revisit the IMDB movie review sentiment classification. Recall that this
    example is concerned with classifying movie reviews from IMDB into positive or
    negative based on the sentiments expressed. It is a prototypical sentiment analysis
    example that has been used widely in the literature to study many algorithms.
    We combine feature vectors generated by pretrained word embeddings for each review
    with some traditional machine learning classification methods, namely random forests
    and logistic regression. We then demonstrate that using higher-level embeddings,
    which vectorize bigger sections of text—at the sentence, paragraph, and document
    level—can lead to improved performance. The general idea of vectorizing text and
    then applying a traditional machine learning classification method to the resulting
    vectors is visualized in figure 4.2.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重新访问了IMDB电影评论情感分类。回想一下，此示例涉及将IMDB的电影评论根据表达的情感分为积极或消极。这是一个典型的情感分析示例，在文献中被广泛使用来研究许多算法。我们将由预训练的单词嵌入生成的特征向量与一些传统的机器学习分类方法相结合，即随机森林和逻辑回归。然后，我们演示了使用更高级别的嵌入，即将更大的文本部分——句子、段落和文档级别——向量化，可以提高性能。将文本向量化，然后将传统的机器学习分类方法应用于生成的向量的一般思想在图4.2中可视化。
- en: '![04_02](../Images/04_02.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![04_02](../Images/04_02.png)'
- en: Figure 4.2 Sequence of typical steps for semisupervised learning with word,
    sentence, or document embeddings
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2 使用单词、句子或文档嵌入进行半监督学习的典型步骤序列
- en: Subsequently, we cover multitask learning and learn how to train a single system
    simultaneously to perform multiple tasks—in our case, represented by both of our
    running examples from the previous chapter, the email spam classification and
    the IMDB movie review sentiment analysis. You can gain several potential benefits
    from multitask learning. By training a single machine learning model for multiple
    tasks, a shared representation is learned on a larger and more varied collection
    of data from the combined data pool, which can lead to performance improvements.
    Moreover, it has been widely observed that this shared representation has a better
    ability to generalize to tasks beyond those that were trained on, and this improvement
    can be achieved without any increase in model size. We explore some of these benefits
    in the context of our running examples. Specifically, we focus on shallow neural
    multitask learning, where a single additional dense layer, as well as a classification
    layer, is trained for each specific task in the setup. Different tasks also share
    a layer between them, a setup typically referred to as *hard-parameter sharing*.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，我们将涵盖多任务学习，并学习如何同时训练单个系统来执行多个任务——在我们的案例中，分别由上一章的两个示例代表，即电子邮件垃圾分类和IMDB电影评论情感分析。你可以从多任务学习中获得几个潜在的好处。通过为多个任务训练单个机器学习模型，可以在更大更多样的来自合并数据池的数据上学习共享表示，这可能导致性能提升。此外，广泛观察到，这种共享表示具有更好的泛化能力，可以推广到未经训练的任务，而且可以在不增加模型大小的情况下实现此改进。我们在我们的示例中探索了其中一些好处。具体地，我们专注于浅层神经多任务学习，其中为设置中的每个特定任务训练了一个额外的密集层以及分类层。不同的任务还共享它们之间的一层，这种设置通常被称为*硬参数共享*。
- en: Finally, we introduce a popular dataset as another running example for the remainder
    of the chapter. This is the Multi-Domain Sentiment Dataset, which describes [Amazon.com](http://Amazon.com)
    product reviews for a set of different products. We use this dataset to explore
    domain adaptation. Assume that we are given one *source* domain, which can be
    defined as a particular distribution of data for a specific task, and a classifier
    that has been trained to perform well on data in that domain for that task. The
    goal of domain adaptation is to modify, or adapt, data in a different *target*
    domain in such a way that the pretrained knowledge from the source domain can
    aid learning in the target domain. We apply a simple *autoencoding* approach to
    “project” samples in the target domain into the source domain feature space.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们引入了一个流行的数据集作为本章的另一个运行示例。这就是多领域情感数据集，描述了[Amazon.com](http://Amazon.com)的一组不同产品的产品评论。我们使用此数据集来探索领域自适应。假设我们有一个*源*领域，它可以被定义为特定任务的特定数据分布，并且已经训练好在该领域中的数据上表现良好的分类器。领域自适应的目标是修改或适应不同*目标*领域的数据，以使源领域的预训练知识可以帮助在目标领域中学习。我们应用了一种简单的*自动编码*方法来将目标领域中的样本“投影”到源领域特征空间中。
- en: An autoencoder is a system that learns to reconstruct inputs with very high
    accuracy, typically by encoding them into an efficient latent representation and
    learning to decode that representation efficiently. They have traditionally been
    heavily used in model reduction applications, because the latent representation
    is often of a smaller dimension than the original space from which the encoding
    happens, and the said dimension value can also be picked to strike the right balance
    of computational efficiency and accuracy.[⁴](#pgfId-1092426) In the extreme scenario,
    improvements can be obtained with no labeled data in the target domain being used
    for training. This is typically referred to as *zero-shot domain adaptation*,
    where learning happens with no labeled data in the target domain. We demonstrate
    an instance of it in our experiments.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是一个系统，它通过将输入编码成一个有效的潜在表示，然后学习有效解码该表示，从而学习以非常高的准确度重构输入。它们传统上在模型减少应用中被广泛使用，因为潜在表示通常比编码发生的原始空间的维度要小，所选维度值也可以为计算效率和准确度的正确平衡而选择。[⁴](#pgfId-1092426)
    在极端情况下，在目标域中使用无标签数据进行训练可以获得改进，这通常称为 *零样本域适应*，其中学习发生在目标域中没有标记的数据。在我们的实验中，我们演示了一个例子。
- en: 4.1 Semisupervised learning with pretrained word embeddings
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1带有预训练单词嵌入的半监督学习
- en: The concept of word embeddings is central to the field of NLP. It is a name
    given to a collection of techniques that produce a set of vectors of real numbers
    for each word that needs to be analyzed. A major consideration in word embedding
    design is the dimension of the vector generated. Bigger vectors generally can
    achieve better representation capability of words within a language and thereby
    better performance on many tasks, while naturally being more expensive computationally.
    Picking the optimal dimension requires striking a balance between these competing
    factors and has often been done empirically, although some recent approaches argue
    for a more thorough theoretical optimization approach.[⁵](#pgfId-1092436)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 单词嵌入的概念是自然语言处理领域的核心。它是给需要分析的每个单词产生一组实数向量的技术集合的名称。在单词嵌入设计中一个重要的考虑因素是生成向量的维度。更高维度的向量通常可以更好地代表语言中的单词，在许多任务上表现更好，但计算成本也自然更高。选择最优维度需要在这些竞争因素之间取得平衡，通常是经验性的，尽管一些最近的方法提出了更彻底的理论优化方法。[⁵](#pgfId-1092436)
- en: As was outlined in the first chapter of this book, this important subarea of
    NLP research has a rich history, originating with the *term-vector model of information
    retrieval* in the 1960s. This culminated with pretrained, shallow neural-network-based
    techniques such as fastText, GloVe, and word2vec, which came in several variants
    in the mid-2010s, including Continuous Bag of Words (CBOW) and Skip-Gram. Both
    CBOW and Skip-Gram are extracted from shallow neural networks that were trained
    for various goals. Skip-Gram attempts to predict words neighboring any target
    word in a sliding window, whereas CBOW attempts to predict the target word given
    the neighbors. GloVe—which stands for global vectors—attempts to extend word2vec
    by incorporating global information into the embeddings. It optimizes the embeddings
    such that the cosine product between words reflects the number of times they cooccur,
    with the goal of making the resulting vectors more interpretable. The technique
    fastText attempts to enhance word2vec by repeating the Skip-Gram methods on character
    n-grams (versus word n-grams), thereby being able to handle previously unseen
    words. Each of these variants of pretrained embeddings has its strengths and weaknesses,
    and these are summarized in table 4.1.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如本书第一章所述，这个重要的NLP研究子领域有着丰富的历史，起源于60年代的术语向量模型的信息检索。这一领域的顶峰是在2010年代中期，出现了预训练的浅层神经网络技术，例如fastText、GloVe和word2vec，它们有多个变体，包括连续词袋（CBOW）和Skip-Gram。CBOW和Skip-Gram都是从受过不同目标训练的浅层神经网络中提取的。Skip-Gram尝试预测滑动窗口中任何目标单词周围的单词，而CBOW尝试预测给定邻居的目标单词。GloVe，即全局向量，尝试扩展word2vec通过将全局信息纳入嵌入中。它通过优化嵌入，使得单词之间的余弦积反映它们共现的次数，其目标是使得结果向量更加可解释。技术fastText尝试通过在字符n-gram（而不是单词n-gram）上重复Skip-Gram方法，从而能够处理以前看不见的单词。每个预训练嵌入的变体都有其优点和缺点，并在表4.1中总结。
- en: Table 4.1 Comparison of strengths and weaknesses of various popular word-embedding
    methods
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.1 比较各种流行单词嵌入方法的优缺点
- en: '| Word-embedding method | Strengths | Weaknesses |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 词嵌入方法 | 优势 | 劣势 |'
- en: '| Skip-Gram word2vec | Works well with a small training dataset and rare words
    | Slower training, plus lower accuracy for frequent words |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| Skip-Gram word2vec | 适用于小型训练数据集和罕见词 | 训练速度慢，且对常见词准确性较低 |'
- en: '| CBOW word2vec | Several times faster in training and better accuracy for
    frequent words | Doesn’t work as well with little training data and rare words
    |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| CBOW word2vec | 训练速度几倍快于，并对常见词提供更好的准确性 | 在处理少量训练数据和罕见词方面效果不佳 |'
- en: '| GloVe | Vectors have more interpretability than other methods | Higher memory
    requirement during training to store co-occurrences of words |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| GloVe | 向量比其他方法更容易解释 | 训练期间需要更高的内存存储词语共现情况 |'
- en: '| fastText | Can handle out-of-vocabulary words | Higher computing cost; larger
    and more complex model |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| fastText | 能够处理词汇外的词 | 计算成本更高；模型更大更复杂 |'
- en: To reiterate, fastText is known for its ability to handle out-of-vocabulary
    words, which comes from it having been designed to embed subword character n-grams,
    or *subwords* (versus entire words, as is the case with word2vec). This enables
    it to build up embeddings for previously unseen words by aggregating composing
    character n-gram embeddings. That comes at the expense of a larger pretrained
    embedding and a higher computing resource requirement and cost. For these reasons,
    we will use the fastText software framework in this section to load embeddings
    in the word2vec input format, without the subword information. This allows us
    to keep the computing cost lower, making the exercise easier for the reader, while
    also showcasing how out-of-vocabulary issues would be handled and providing a
    solid experience platform from which the reader can venture into subword embeddings.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 需要强调的是，fastText以处理词汇外的词而闻名，这源自它的设计初衷即嵌入子词字符n-gram或*子词*（与word2vec的整个词相对应）。这使得它能够通过聚合组成的字符n-gram嵌入来为以前未见过的词构建嵌入。这一优点是以更大的预先训练嵌入和更高的计算资源需求和成本为代价的。因此，在本节中，我们将使用fastText软件框架以word2vec输入格式加载嵌入，而没有子词信息。这可以降低计算成本，使读者更容易进行练习，同时展示如何处理词汇外问题，并提供一个坚实的体验平台，让读者可以进入子词嵌入的领域。
- en: Let’s begin the computing experiment! The first thing we need to do is obtain
    the appropriate pretrained word-embedding file. Because we will be using the fastText
    framework, we could obtain these pretrained files from the authors’ official website,[⁶](#pgfId-1092480)
    which hosts appropriate embedding files in a number of formats. Note that these
    files are extremely large, because they attempt to capture vectorization information
    about all possible words within a language. For instance, the .vec format embedding
    for the English language, which was trained on Wikipedia 2017 and provides vectorization
    information without handling subwords and out-of-vocabulary words, is about 6
    GB. The corresponding .bin format embedding, which contains the famous fastText
    subword information and can handle out-of-vocabulary words, is about 25% larger,
    at approximately 7.5 GB. We also note that Wikipedia embeddings are provided in
    up to 294 languages, even including traditionally unaddressed African languages
    such as Twi, Ewe, and Hausa. It has been shown, however, that for many of the
    included low-resource languages, the quality of these embeddings is not very good.[⁷](#pgfId-1092483)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始计算实验！我们需要做的第一件事是获得适当的预训练词嵌入文件。因为我们将使用fastText框架，我们可以从作者的官方网站[⁶](#pgfId-1092480)获取这些预训练文件，该网站提供多种格式的嵌入文件。请注意，这些文件非常庞大，因为它们试图捕获语言中所有可能单词的向量化信息。例如，针对英语语言的.wec格式嵌入，是在维基百科2017年数据集上训练的，提供了在不处理子词和词汇外词的情况下的向量化信息，大约为6GB。相应的.bin格式嵌入，包含了着名的fastText子词信息，能够处理词汇外词，大约大25%，约为7.5GB。我们还注意到，维基百科嵌入提供了高达294种语言，甚至包括传统上未解决的非洲语言，例如特威语、埃维语和豪萨语。但已经表明，对于许多包括低资源语言，这些嵌入的质量并不是很好。[⁷](#pgfId-1092483)
- en: Due to the size of these embeddings, it is a lot easier to execute this example
    using the recommended cloud-based notebooks we have hosted on Kaggle (versus running
    them locally) because the embedding files have already been openly hosted in the
    cloud environment by other users.[⁸](#pgfId-1092488) As such, we can simply attach
    them to a running notebook without having to obtain and run the files locally.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些嵌入的大小，建议使用我们在Kaggle上托管的推荐云笔记本来执行此示例（而不是在本地运行），因为其他用户已经将嵌入文件在云环境中公开托管。因此，我们可以简单地将它们附加到正在运行的笔记本上，而无需获取并在本地运行文件。
- en: 'Once the embedding is available, we can load it using the following code snippet,
    making sure to time the loading function call:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦嵌入可用，我们可以使用以下代码段加载它，确保计时加载函数调用：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Loads a pretrained fastText embedding in “word2vec” format (without subword
    information)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载以“word2vec”格式（不含子词信息）预训练的fastText嵌入。
- en: Loading the embedding takes more than 10 minutes the first time on the Kaggle
    environment we used for execution. In practice, in such a situation, it is not
    uncommon to load the embedding once into memory and then serve access to it using
    an approach such as Flask for as long as it is needed. This can also be achieved
    using the Jupyter notebook that comes along with this chapter of the book.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们用于执行的Kaggle环境中，第一次加载嵌入需要超过10分钟。实际上，在这种情况下，通常将嵌入加载到内存中一次，然后使用诸如Flask之类的方法提供对它的访问，只要需要。这也可以通过本书本章附带的Jupyter笔记本来实现。
- en: Having obtained and loaded the pretrained embedding, let’s look back at the
    IMDB movie review classification example, which we will analyze in this section.
    In particular, we pick up right after listing 2.10 in the preprocessing stage
    of the pipeline, which generates a NumPy array `raw_data` that contains word-level
    tokenized representations of movie reviews, with stop words and punctuation removed.
    For the reader’s convenience, we show listing 2.10 again next.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 获得并加载了预训练的嵌入后，让我们回顾一下IMDB电影评论分类示例，在本节中我们将对其进行分析。特别是，在管道的预处理阶段，我们直接从2.10清单开始，生成了一个包含电影评论的单词级标记表示的NumPy数组`raw_data`，其中删除了停用词和标点符号。为了读者的方便，我们接下来再次展示2.10清单。
- en: Listing 2.10 (Duplicated from chapter 2) Loading the IMDB data into a NumPy
    array
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 2.10清单（从第2章复制）将IMDB数据加载到NumPy数组中。
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Goes through every file in the current folder
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 遍历当前文件夹中的每个文件。
- en: ❷ Applies tokenization and stop word analysis routines
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 应用标记化和停用词分析例程。
- en: ❸ Tracks corresponding sentiment labels
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 跟踪相应的情感标签。
- en: ❹ Converts to a NumPy array
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 转换为NumPy数组。
- en: ❺ Calls the function above on the data
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 在数据上调用上述函数。
- en: If you have already worked through chapter 2, you may recall that after listing
    2.10, we proceeded to generate a simple bag-of-words representation for the output
    NumPy array, which simply counts occurrence frequencies of possible word tokens
    in each review. We then used the resulting vectors as numerical features for further
    machine learning tasks. Here, instead of the bag-of-words representation, we extract
    corresponding vectors from the pretrained embedding instead.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经完成了第2章，您可能还记得在2.10清单之后，我们继续为输出NumPy数组生成了一个简单的词袋表示，该表示只是计算了每个评论中可能单词标记的出现频率。然后，我们使用生成的向量作为进一步机器学习任务的数值特征。在这里，我们不使用词袋表示，而是从预训练的嵌入中提取相应的向量。
- en: Because our embedding of choice does not handle out-of-vocabulary words out
    of the box, the next thing we do is develop a methodology for addressing this
    situation. The simplest thing to do, quite naturally, is to simply skip any such
    words. Because the fastText framework errors out when such a word is encountered,
    we will use a *try and except* block to catch these errors without interrupting
    execution. Assume that you are given a pretrained input embedding that serves
    as a dictionary, with words as keys and corresponding vectors as values, and an
    input list of words in a review. The next listing shows a function that produces
    a two-dimensional NumPy array with rows representing embedding vectors for each
    word in the review.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们选择的嵌入框架不能直接处理词汇表外的单词，所以我们要做的下一步是开发一种解决这种情况的方法。最简单的方法自然是简单地跳过任何这样的单词。因为当遇到这样的单词时，fastText框架会报错，我们将使用一个*try
    and except*块来捕获这些错误而不中断执行。假设您有一个预训练的输入嵌入，用作字典，其中单词作为键，相应的向量作为值，并且有一个单词列表在评论中。接下来的清单显示了一个函数，该函数生成一个二维NumPy数组，其中每行代表评论中每个单词的嵌入向量。
- en: Listing 4.1 Producing a 2-D Numpy array of movie review word embedding vectors
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.1 生成电影评论单词嵌入向量的 2-D Numpy 数组
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Loops through every word
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 循环遍历每个单词
- en: ❷ Extracts the corresponding embedding vector, and enforces “row shape”
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 提取相应的嵌入向量，并强制“行形状”
- en: ❸ Handles the edge case of the first vector and an empty-out array
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 处理第一个向量和一个空数组的边缘情况
- en: ❹ Concatenates the row embedding vector to the output NumPy array
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将行嵌入向量连接到输出 NumPy 数组
- en: ❺ Skips the execution on the current word, and continues the execution from
    the next word when out-of-vocabulary errors occur
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 在发生词汇表外错误时跳过当前单词的执行，并从下一个单词继续执行
- en: The function in this listing can now be used to analyze the entire dataset as
    captured by the variable `raw_data`. However, before doing so we must decide how
    we will combine or *aggregate* the embedding vectors for individual words in a
    review into a single vector representing the entire review. It has been found
    in practice that the heuristic of simply averaging the words works as a strong
    baseline. Because the embeddings were trained in a way that ensures that similar
    words are closer to each other in the resulting vector space, it makes intuitive
    sense that their average would represent the average meaning of the collection.
    The averaging baseline for summarization/ aggregation is often recommended as
    a first attempt at embedding bigger sections of text from word embeddings. This
    is also the approach we use in this section, as demonstrated by the code in listing
    4.2\. Effectively, this code calls the function from listing 4.1 repeatedly on
    every review in the corpus, averages the output, and concatenates the resulting
    vectors into a single two-dimensional NumPy array. The rows of this resulting
    array correspond to aggregated-by-averaging embedding vectors for each review.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 此列表中的函数现在可以用来分析由变量`raw_data`捕获的整个数据集。但在此之前，我们必须决定如何将评论中单词的嵌入向量组合或*聚合*成代表整个评论的单个向量。实践中发现，简单地对单词进行平均通常可以作为一个强有力的基准。由于嵌入是以一种确保相似单词在生成的向量空间中彼此更接近的方式进行训练的，因此它们的平均值代表了该集合的平均含义在直觉上是有意义的。摘要/聚合的平均基准经常被推荐作为从单词嵌入中嵌入更大文本部分的第一次尝试。这也是我们在本节中使用的方法，正如列表
    4.2 中的代码所示。实际上，该代码在语料库中的每个评论上重复调用列表 4.1 中的函数，对输出进行平均，并将结果向量连接成一个单一的二维 NumPy 数组。该结果数组的行对应于每个评论的通过平均聚合的嵌入向量。
- en: Listing 4.2 Loading IMDB data into a NumPy array
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.2 将 IMDB 数据加载到 NumPy 数组中
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Loops through every IMDB review
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 循环遍历每个 IMDB 评论
- en: ❷ Extracts the embedding vectors for every word in the review, making sure to
    handle out-of-vocabulary words
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 提取评论中每个单词的嵌入向量，确保处理词汇表外的单词
- en: ❸ Averages the word vectors in each review
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 对每个评论中的单词向量进行平均
- en: ❹ Concatenates the average row vector to output the NumPy array
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将平均行向量连接到输出 NumPy 数组
- en: ❺ Out-of-vocabulary edge case handling
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 词汇表外边缘情况处理
- en: 'We can now assemble embedding vectors for the whole dataset using the next
    function call:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用下一个函数调用为整个数据集组装嵌入向量：
- en: '[PRE4]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: These can now be used as feature vectors for the same logistic regression and
    random forest codes as were used in listings 2.11 and 3.1, respectively. Using
    these codes to train and evaluate these models, we found the corresponding accuracy
    scores to be 77% and 66%, respectively, when the hyperparameters `maxtokens` and
    `maxtokenlen` are set to 200 and 100, respectively, and the value of `Nsamp`—the
    number of samples from each class—is equal to 1,000\. These are only slightly
    lower than the corresponding values obtained from the bag-of-words baseline that
    was initially developed in the previous chapters (corresponding to accuracy scores
    of 79% and 67%, respectively). We hypothesize that this slight reduction is likely
    due to the aggregation of individual word vectors by the naive averaging approach
    that was described. In the next section, we attempt to perform more intelligent
    aggregation using embedding methods that were designed to embed at a higher text
    level.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这些可以作为特征向量用于相同的逻辑回归和随机森林代码，就像列表2.11和3.1中分别使用的那样。 使用这些代码来训练和评估这些模型时，当超参数`maxtokens`和`maxtokenlen`分别设置为200和100时，我们发现对应的准确率分别为77%和66%，而`Nsamp`—每个类的样本数—等于1,000。
    这些只比在前几章最初开发的基于词袋的基线稍低一些（分别对应准确率为79%和67%）。 我们假设这种轻微的降低可能是由于聚合个别单词向量的天真平均方法造成的。
    在下一节中，我们尝试使用专门设计用于在更高文本级别嵌入的嵌入方法来执行更智能的聚合。
- en: 4.2 Semisupervised learning with higher-level representations
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 使用更高级别的表示进行半监督学习
- en: Several techniques, inspired by word2vec, try to embed larger sections of text
    into vector spaces in such a way that sentences with similar meanings would be
    closer to each other in the induced vector space. This enables us to perform arithmetic
    on sentences to make inferences about analogies, combined meanings, and so on.
    One prominent approach is paragraph vectors, or *doc2vec*, which exploits the
    concatenation (versus averaging) of words from pretrained word embeddings in summarizing
    them. Another is sent2vec, which extends the classic Continuous Bag-of-Words (CBOW)
    of word2vec—where a shallow network is trained to predict a word in a sliding
    window from its context—to sentences by optimizing word and word n-gram embeddings
    for an accurate averaged representation. In this section, we use a pretrained
    sent2vec model as an illustrative representative method and apply it to the IMDB
    movie classification example.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 受word2vec启发，有几种技术试图以这样一种方式将文本的较大部分嵌入向量空间，以使具有相似含义的句子在诱导的向量空间中更接近彼此。 这使我们能够对句子执行算术运算，以推断类比、合并含义等等。
    一个著名的方法是段落向量，或者*doc2vec*，它利用了从预训练词嵌入中汇总单词时的连接（而不是平均）来总结它们。 另一个是sent2vec，它通过优化单词和单词n-gram嵌入以获得准确的平均表示，将word2vec的经典连续词袋（CBOW）—在滑动窗口中训练浅层网络以预测上下文中的单词—扩展到句子。
    在本节中，我们使用一个预训练的sent2vec模型作为一个说明性的代表方法，并将其应用于IMDB电影分类示例。
- en: 'You can find a few open source implementations of sent2vec online. We are employing
    a heavily used implementation that builds on fastText.[⁹](#pgfId-1092609) To install
    that implementation directly from its hosted URL, execute the following command:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在网上找到几个sent2vec的开源实现。 我们正在使用一个基于fastText构建的使用频繁的实现。 要直接从托管的URL安装该实现，请执行以下命令：
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Quite naturally, just as in the case of the pretrained word embeddings, the
    next step is to obtain the pretrained sent2vec sentence embedding to be loaded
    by the particular implementation/framework we have installed. These are hosted
    by the authors of the framework on their GitHub page and on Kaggle by other users.[^(10)](#pgfId-1092617)
    For simplicity, we choose the smallest 600-dimensional embedding `wiki_unigrams.bin`,
    approximately 5 GB in size, which captures just the unigram information on Wikipedia.
    Note that significantly larger models are available pretrained on book corpora
    and Twitter, and also include bigram information.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 很自然地，就像在预训练词嵌入的情况下一样，下一步是获取预训练的sent2vec句子嵌入，以供我们已安装的特定实现/框架加载。 这些由框架的作者在他们的GitHub页面上托管，并由其他用户在Kaggle上托管。
    为简单起见，我们选择了最小的600维嵌入`wiki_unigrams.bin`，大约5 GB大小，仅捕获了维基百科上的单字信息。 请注意，预训练模型的大小明显更大，在书籍语料库和Twitter上预训练，还包括双字信息。
- en: 'Having obtained the pretrained embedding, we load it using the following code
    snippet, making sure to time the loading process as before:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在获得预训练嵌入后，我们使用以下代码片段加载它，确保像以前一样计时加载过程。
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Loads sent2vec embedding
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载 sent2vec 嵌入
- en: It’s worth mentioning that we found the load time during the first execution
    to be less than 10 seconds—a notable improvement over the fastText word embedding
    loading time of over 10 minutes. This increased speed is attributed to the significantly
    more efficient implementation of the current package versus the gensim implementation
    that we used in the previous section. It is not uncommon to try different packages
    to find the most efficient one for your application in practice.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，我们发现首次执行时的加载时间少于 10 秒——相比于 fastText 单词嵌入的加载时间超过 10 分钟，这是一个显着的改进。这种增加的速度归因于当前包的实现比我们在上一节中使用的
    gensim 实现要高效得多。在实践中，尝试不同的包以找到最有效的包对于你的应用程序并不罕见。
- en: Next, we define a function to generate vectors for a collection of reviews.
    It is essentially a simpler form of the function presented in listing 4.2 for
    pretrained word embeddings. It is simpler because we do not need to worry about
    out-of-vocabulary words. This function is shown in the following listing.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个函数来生成一系列评论的向量。它本质上是列表 4.2 中呈现的预训练单词嵌入函数的简化形式。它更简单，因为我们不需要担心词汇表外的单词。该函数如下列表所示。
- en: Listing 4.3 Loading IMDB data into a NumPy array
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.3 将 IMDB 数据加载到 NumPy 数组中
- en: '[PRE7]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Loops through every IMDB review
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 遍历每个 IMDB 评论
- en: ❷ Extracts the embedding vectors for every review
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 提取每个评论的嵌入向量
- en: ❸ Edge case handling
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 处理边缘情况
- en: 'We can now use this function to extract sent2vec embedding vectors for each
    review as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用此函数提取每个评论的 sent2vec 嵌入向量，如下所示：
- en: '[PRE8]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We can also split this into training and test datasets and train logistic regression
    and random forest classifiers on top of the embedding vectors as before, using
    code analogous to what is shown in listings 2.11 and 3.1, respectively. This yields
    accuracy scores of 82% and 68% for the logistic regression and random forest classifiers,
    respectively (at the same hyperparameter values as in the previous section). This
    value for the logistic regression classifier combined with sent2vec is an improvement
    on the corresponding values of 79% and 67%, respectively, for the bag-of-words
    baseline, as well as an improvement over the averaged word embedding approach
    from the previous section.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以像以前一样将此分为训练集和测试集，并在嵌入向量的基础上训练 logistic 回归和随机森林分类器，使用类似于列表 2.11 和 3.1 中所示的代码。在
    logistic 回归和随机森林分类器的情况下，准确率分别为 82% 和 68%（与上一节中相同的超参数值）。与上一节中基于词袋的基线的对应值为 79% 和
    67% 相比，对于 logistic 回归分类器与 sent2vec 结合起来的这个值是一个改进，同时也是对于上一节中的平均词嵌入方法的改进。
- en: 4.3 Multitask learning
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 多任务学习
- en: Traditionally, machine learning algorithms have been trained to perform a single
    task at a time, with the data collected and trained on being independent for each
    separate task. This is somewhat antithetical to the way humans and other animals
    learn, where training for multiple tasks occurs simultaneously, and information
    from training on one task may inform and accelerate the learning of other tasks.
    This additional information may improve performance not just on the current tasks
    being trained on but also on future tasks, and sometimes even in cases where no
    labeled data is available on such future tasks. This scenario of transfer learning
    with no labeled data in the target domain is often referred to as zero-shot transfer
    learning.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，机器学习算法一次只能训练执行一个任务，所收集和训练的数据对于每个单独的任务是独立的。这在某种程度上与人类和其他动物学习的方式相矛盾，人类和其他动物的学习方式是同时进行多个任务的训练，从而一个任务的训练信息可能会影响和加速其他任务的学习。这些额外的信息不仅可能提高当前正在训练的任务的性能，还可能提高未来任务的性能，有时甚至在没有关于这些未来任务的标记数据的情况下也可能如此。在目标域中没有标记数据的迁移学习场景通常被称为零样本迁移学习。
- en: In machine learning, multitask learning has historically appeared in a number
    of settings, from *multiobjective optimization* to *l2* and other forms of *regularization*
    (which can itself be framed as a form of multiobjective optimization). Figure
    4.3 shows the form of neural multitask learning we will employ, where some layers/parameters
    are shared between all tasks, that is, *hard parameter sharing*.[^(11)](#pgfId-1092675)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，多任务学习在许多场景中历史上出现过，从*多目标优化*到*l2*和其他形式的*正则化*（本身可以被构造为一种多目标优化形式）。图 4.3 展示了我们将要使用的神经多任务学习的形式，其中一些层/参数在所有任务之间共享，即*硬参数共享*。[^(11)](#pgfId-1092675)
- en: '![04_03](../Images/04_03.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![04_03](../Images/04_03.png)'
- en: Figure 4.3 The general form of neural multitask learning we will employ—hard
    parameter sharing (in this case, three tasks)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3 我们将使用的神经多任务学习的一般形式——硬参数共享（在本例中有三个任务）
- en: In the other prominent type of neural multitask learning, *soft parameter sharing*,
    all tasks have their own layers/parameters that are not shared. Instead, they
    are encouraged to be similar via various constraints imposed on the task-specific
    layers across the various tasks. We do not address this type of multitask learning
    further, but it is good to be aware of its existence for your own future potential
    literature explorations.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一种突出的神经多任务学习类型中，*软参数共享*，所有任务都有自己的层/参数，不进行共享。相反，通过对各个任务的特定层施加的各种约束，鼓励它们相似。我们不再进一步讨论这种类型的多任务学习，但了解它的存在对于您自己未来的潜在文献调研是很有好处的。
- en: Let’s proceed to our illustrative example for this section, by setting it up
    and baselining it in the next subsection.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续进行本节的实例说明，通过在下一个小节中设置和基线化它。
- en: 4.3.1 Problem setup and a shallow neural single-task baseline
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.1 问题设置和浅层神经单任务基线
- en: Consider figure 4.3 again, but with only two tasks—the first task being IMDB
    movie review classification from the previous two sections, and the second task
    being email spam classification from the previous chapter. The resulting setup
    represents the specific example we will address in this section. This setup is
    shown in figure 4.4 to facilitate conceptualizing.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 再次考虑图4.3，但只有两个任务——第一个任务是前两节中的IMDB电影评论分类，第二个任务是前一章中的电子邮件垃圾邮件分类。所得到的设置代表了我们将在本节中解决的具体示例。为了促进概念化，这个设置在图4.4中显示。
- en: '![04_04](../Images/04_04.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![04_04](../Images/04_04.png)'
- en: Figure 4.4 The specific form of neural multitask hard parameter sharing we will
    employ, with two specific tasks shown—IMDB reviews and email spam classification
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4 我们将使用的神经多任务硬参数共享的具体形式，显示了两个特定任务——IMDB评论和电子邮件垃圾邮件分类
- en: Before proceeding, we must decide how the inputs to the resulting neural network
    will be converted into numbers for analysis. One popular choice is to encode the
    input at the character level using one-hot encoding, where each character is replaced
    by a sparse vector of a dimension equal to the total number of possible characters.
    This vector contains 1 in the column corresponding to the character and 0 otherwise.
    An illustration of this method, which aims to help you concisely visualize the
    process of one-hot encoding, is shown in figure 4.5.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，我们必须决定如何将输入转换为用于分析的数字。一种流行的选择是使用字符级别的独热编码对输入进行编码，其中每个字符被维度等于可能字符总数的稀疏向量替换。这个向量在与字符对应的列中包含1，其他位置为0。图4.5显示了这种方法的插图，旨在帮助您简洁地可视化独热编码过程。
- en: '![04_05](../Images/04_05.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![04_05](../Images/04_05.png)'
- en: Figure 4.5 A visualization of the process of one-hot encoding characters into
    row-vector representations. The process replaces every character in the vocabulary
    with a sparse vector with a dimension equal to the size of the vocabulary. 1 is
    placed in the column corresponding to the vocabulary character index.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5 将字符进行独热编码以行向量表示的过程的可视化。该过程将词汇表中的每个字符替换为与词汇表大小相等的稀疏向量。1被放置在与词汇表字符索引相对应的列中。
- en: 'This method can be expensive from a memory perspective, given the significant
    inherent increase in dimension, and as such, it is common to perform the one-hot
    encoding “on the fly” via specialized neural network layers. Here, we take an
    even more straightforward approach: we pass each review through the sent2vec embedding
    function and use the embedding vectors as input features to the setup shown in
    figure 4.4.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 从内存角度来看，这种方法可能会很昂贵，因为维度显著增加，并且因此，通过专门的神经网络层“即时”执行一键编码是常见的。在这里，我们采用了更简单的方法：我们将每个评论通过sent2vec嵌入函数，并将嵌入向量作为输入特征传递给图4.4所示的设置。
- en: Before proceeding to the exact two-task setup shown in figure 4.4, we perform
    another baseline. We use the IMDB movie classification task as the only one present,
    to see how the task-specific shallow neural classifier compares with the model
    from the previous section. The code corresponding to this shallow neural baseline
    is shown in the next listing.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续进行图4.4所示的准确的双任务设置之前，我们进行了另一个基准测试。我们将仅使用IMDB电影分类任务，以查看任务特定的浅层神经分类器与上一节中的模型相比如何。与这个浅层神经基线相关的代码将显示在下一个列表中。
- en: Listing 4.4 Shallow single-task Keras neural network
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 4.4 浅层单任务 Keras 神经网络
- en: '[PRE9]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ The input must match the dimension of the sent2vec vectors.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 输入必须匹配 sent2vec 向量的维度。
- en: ❷ A dense neural layer trained on top of the sent2vec vectors
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在 sent2vec 向量之上训练的密集神经层
- en: ❸ Applies dropout to reduce overfitting
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 应用 dropout 减少过拟合
- en: ❹ The output indicates a single binary classifier—is review “positive” or “negative”?
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 输出指示一个单一的二元分类器——评论是“积极”还是“消极”？
- en: We found that the performance of this classifier was about 82% at the hyperparameter
    values specified in the previous section. This is higher than the baseline of
    bag-of-words combined with logistic regression and approximately equal to sent2vec
    combined with logistic regression from the previous section.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，在上一节中指定的超参数值下，该分类器的性能约为 82%。这高于基于词袋结合逻辑回归的基线，大约等于上一节中的 sent2vec 结合逻辑回归。
- en: 4.3.2 Dual-task experiment
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.2 双任务实验
- en: 'We now introduce another task: the email spam classification problem from the
    previous chapter. We will not repeat the preprocessing steps and associated code
    here for this auxiliary task; see chapter 2 for these details. Assuming the availability
    of the sent2vec vectors `train`_`x2` corresponding to the emails in the data sample,
    listing 4.5 shows how one can create a multiple-output shallow neural model to
    train it simultaneously for email spam classification and the classification of
    IMDB movie reviews via hard-parameter sharing.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们介绍另一个任务：上一章的电子邮件垃圾分类问题。我们不会在这里重复预处理步骤和相关代码，这是一个辅助任务；有关这些详细信息，请参阅第2章。假设数据样本中的邮件对应的
    sent2vec 向量 `train`_`x2` 可用，清单 4.5 显示了如何创建一个多输出的浅层神经模型，同时对其进行训练，用于电子邮件垃圾分类和 IMDB
    电影评论的分类，通过硬参数共享。
- en: Listing 4.5 Shallow dual-task hard parameter sharing Keras neural network
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 4.5 浅层双任务硬参数共享 Keras 神经网络
- en: '[PRE10]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Concatenates the sent2vec vectors for the different tasks
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将不同任务的 sent2vec 向量连接起来
- en: ❷ Shared dense neural layer
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 共享的密集神经层
- en: ❸ Two task-specific outputs, each being a binary classifier
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 两个任务特定的输出，每个都是二元分类器
- en: 'Having defined the hard-parameter sharing setup for the two-task multitask
    scenario involving IMDB movie reviews and email spam classification, we can compile
    and train the resulting model via the following:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 已经为涉及 IMDB 电影评论和电子邮件垃圾分类的两任务多任务场景定义了硬参数共享设置，我们可以通过以下方式编译和训练生成的模型：
- en: '[PRE11]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Specifies two loss functions (both binary_crossentropy in our case)
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 指定两个损失函数（在我们的情况下都是 binary_crossentropy）
- en: ❷ Specifies the training and validation data for each input
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 指定每个输入的训练和验证数据
- en: For this experiment, we set the hyperparameters `maxtokens` and `maxtokenlen`
    both to 100 and the value of `Nsamp`—the number of samples from each class—to
    1,000 (as in the previous section).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个实验，我们将超参数 `maxtokens` 和 `maxtokenlen` 都设置为 100，`Nsamp`（每个类别的样本数）的值设置为 1,000（与上一节相同）。
- en: 'We found, upon training the multitask system, that the IMDB classification
    performance dropped slightly, from approximately 82% in the single-task shallow
    setup in listing 4.4 to about 80%. The email classification accuracy similarly
    dropped from 98.7% to 98.2%. Given the drop in performance, one may rightfully
    ask: What was the point of all this?'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，在训练多任务系统时，IMDB 分类性能略有下降，从清单 4.4 中单任务浅层设置的约 82% 下降到约 80%。电子邮件分类的准确性同样从 98.7%
    下降到 98.2%。鉴于性能下降，人们可能会合理地问：这一切的意义何在？
- en: First of all, observe that the trained model can be deployed independently for
    each task, by simply replacing the omitted task input with zeros to respect the
    expected overall input dimension and ignoring the corresponding output. Moreover,
    we expect the shared pretrained layer `dense1` of the multitask setup to be more
    readily generalizable to arbitrary new tasks than that from listing 4.4\. This
    is because it has been trained to be predictive on a more varied and general set
    of data and tasks.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要注意的是，训练好的模型可以独立地用于每个任务，只需将省略的任务输入替换为零以尊重预期的整体输入维度，并忽略相应的输出。此外，我们期望多任务设置中的共享预训练层
    `dense1` 比清单 4.4 中的更容易泛化到任意新任务。这是因为它已经在更多种类和更一般的数据和任务上进行了训练以进行预测。
- en: To make this more concrete, consider replacing either or both task-specific
    layers with new ones, initializing the shared layer `dense1` to pretrained weights
    from the previous experiment, and fine-tuning the resulting model on the new task
    dataset. Having seen a broader range of task data, potentially similar to the
    newly added tasks, these shared weights are more likely to contain useful information
    for the downstream tasks being considered.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更具体地说明这一点，考虑将任务特定层中的一个或两个替换为新的层，将共享层`dense1`初始化为前一个实验的预训练权重，并在新的任务数据集上对结果模型进行微调。通过观察更广泛范围的任务数据，可能与新添加的任务类似，这些共享权重更有可能包含有用信息，可用于考虑的下游任务。
- en: We will return to the idea of multitask learning later in the book, which will
    give us the opportunity to investigate and think about these phenomena further.
    The experiment in this section has hopefully armed you with the required foundation
    for further exploration.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本书的后面回到多任务学习的概念，这将为我们提供进一步研究和思考这些现象的机会。本节的实验希望为您提供了进一步探索所需的基础。
- en: 4.4 Domain adaptation
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.4 域自适应
- en: In this section, we briefly explore the idea of *domain adaptation*, one of
    the oldest and most prominent ideas in transfer learning. An implicit assumption
    made often by machine learning practitioners is that the data during the inference
    phase will come from the same distribution as the data that was used for training.
    This is, of course, rarely true in practice.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们简要探讨了*域自适应*的概念，这是转移学习中最古老和最显着的想法之一。机器学习实践者经常做出的一个隐含假设是，推断阶段的数据将来自用于训练的相同分布。当然，实践中很少有这种情况发生。
- en: Enter domain adaptation to attempt to address this issue. Let’s define domain
    as a particular distribution of data for a specific task. Assume that we are given
    a *source* domain and an algorithm that has been trained to perform well on data
    in that domain. The goal of domain adaptation is to modify, or adapt, data in
    a different *target* domain in such a way that the pretrained knowledge from the
    source domain can be applicable to faster learning and/or direct inference in
    the target domain. A variety of approaches have been explored, ranging from multitask
    learning (as introduced in the previous section)—where learning on different data
    distributions occurs simultaneously—to coordinate transformations—which enable
    more effective learning on a single combined feature space[^(12)](#pgfId-1092782)—to
    methods that exploit measures of similarity between the source and target domains
    to help us select which data should be used for training.[^(13)](#pgfId-1092785)
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 进入域自适应以尝试解决这个问题。让我们将域定义为针对特定任务的数据的特定分布。假设我们有一个*源*域和一个经过训练以在该域中表现良好的算法。域自适应的目标是修改或调整不同*目标*域中的数据，以便来自源域的预训练知识可以适用于更快的学习和/或直接推断目标域中的情况。已经探索了各种方法，从多任务学习（如前一节介绍的）—在不同数据分布上同时进行学习—到协同变换—这些方法能够在单个组合特征空间上进行更有效的学习—再到利用源域和目标域之间相似性度量的方法，帮助我们选择哪些数据应该用于训练。
- en: We apply a simple *autoencoding* approach to “project” samples in the target
    domain into the source domain feature space. An autoencoder is a system that can
    learn to reconstruct inputs with high accuracy, typically by encoding them into
    an efficient latent representation and learning to decode the said representation.
    A technical way of describing the process of reconstructing input is “learning
    the identity function.” Autoencoders have traditionally been heavily used in model
    dimensionality-reduction applications, because the latent representation is often
    of a smaller dimension than the original space from which the encoding happens,
    and the said dimension value can also be picked to strike the right balance between
    computational efficiency and accuracy.[^(14)](#pgfId-1092790) In the extreme favorable
    scenario, you can obtain improvements with no labeled data in the target domain,
    which is typically referred to as *zero-shot domain adaptation*.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用简单的*自编码*方法将目标域中的样本“投射”到源域特征空间中。自编码器是一种可以学习以高准确度重构输入的系统，通常是通过将它们编码成高效的潜在表示来学习解码所述表示。描述重构输入过程的技术方法是“学习身份函数”。自编码器在模型维度缩减应用中传统上被大量使用，因为潜在表示通常比编码发生的原始空间的维度小，并且所述维度值也可以被选择为在计算效率和准确性之间达到正确平衡。在极端有利的情况下，您可以在目标域中不需要标记数据的情况下获得改进，这通常被称为*零样本域自适应*。
- en: The idea of zero-shot transfer learning arises in many contexts. You can think
    of it as a sort of “holy grail” of transfer learning, because obtaining labeled
    data in the target domain can be an expensive exercise. Here, we explore whether
    a classifier trained to predict the polarity of IMDB movie reviews can be used
    to predict the polarity of reviews in some other domain. For example, can a classifier
    trained on IMDB review data predict the polarity of book reviews or DVD reviews
    obtained from a completely different data source?
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 零样本迁移学习的概念在许多情境中都出现过。您可以将其视为一种转移学习的“圣杯”，因为在目标域中获取标记数据可能是一项昂贵的任务。在这里，我们探讨了一个分类器是否可以用来预测IMDB电影评论的极性，以预测来自完全不同数据源的书评或DVD评论的极性，例如，是否可以使用在IMDB评论数据上训练的分类器来预测书评或DVD评论的极性？
- en: A natural alternative source of reviews, given its ubiquity in the contemporary
    world, is Amazon.com. Few sources of data today contain as much variety with regard
    to categories of products and sheer volume of data as this e-commerce website,
    which many Americans have come to rely on for basic daily-need purchases, at the
    expense of business for more traditional brick-and-mortar stores. A rich repository
    of reviews is available. Indeed, one of the most prominently and heavily explored
    datasets in the space of NLP domain adaptation happens to be a collection of reviews
    for different product categories on Amazon—the Multi-Domain Sentiment Dataset.[^(15)](#pgfId-1092797)
    This dataset represents 25 categories, from which we picked the product category
    of book reviews, feeling that it was sufficiently different from IMDB reviews
    to present a challenging test case.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在当今世界，一个自然的备选评论来源是亚马逊。鉴于这家电子商务网站在产品类别和数据量方面的多样性，以及它被许多美国人视为基本日常需求购买的主要来源，相比于传统的实体店而言，它具有更多的商业额。这里有一个丰富的评论库。事实上，自然语言处理领域中最显著和深度探索的数据集之一就是亚马逊不同产品类别的评论集合——多领域情感数据集。这个数据集包含25个类别，我们从中选择了图书评论的产品类别，认为它与IMDB评论足够不同，可以提供一个具有挑战性的测试案例。
- en: The data in this dataset is contained in a markup language format, where tags
    are used to define various elements, and is organized by category and polarity
    into separate files. It suffices to note for our purposes that reviews are contained
    within appropriately named `<review_text>...</review_text>` tags. Given this information,
    the code in the next listing can be used to load positive and negative book reviews
    and prepare them for analysis.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集中的数据以标记语言格式存储，其中标签用于定义各种元素，并且按类别和极性组织到单独的文件中。对于我们的目的来说，值得注意的是评论包含在适当命名的`<review_text>...</review_text>`标签内。在获得这些信息后，下一个清单中的代码可以用于加载积极和消极的图书评论，并为其准备分析。
- en: Listing 4.6 Loading reviews from the Multi-Domain Sentiment Dataset
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载来自多领域情感数据集的评论时，清单4.6
- en: '[PRE12]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Locates the first line of review, and combines all subsequent characters until
    the end tag into the review text
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定位评论的第一行，并将所有后续字符组合到结束标记中，形成评论文本
- en: ❷ Reads lines from the source text file, both positive and negative reviews,
    by leveraging defined function
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 通过利用定义的函数，从源文本文件中读取正面和负面评论。
- en: ❸ Creates labels for the positive and negative classes
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 为正面和负面类别创建标签。
- en: ❹ Appends, shuffles, and extracts the corresponding sent2vec vectors
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 追加、洗牌并提取相应的sent2vec向量。
- en: 'Having loaded the book review text and prepared it for further processing,
    we now test the trained IMDB classifier from the previous section directly on
    the target data to see how accurate it is without any processing, using the following
    code:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载了书评文本并准备进行进一步处理之后，我们现在直接在目标数据上测试之前部分训练的IMDB分类器，看看它在没有任何处理的情况下的准确性，使用以下代码：
- en: '[PRE13]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This yields an accuracy of about 74%. Although this is a decrease from the performance
    of the same classifier of 82% on the IMDB data, it is still sufficiently high
    to demonstrate an instance of zero-shot knowledge transfer from the movie review
    task to the book review task. Let’s attempt to improve this number via zero-shot
    domain adaptation with an autoencoder.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了约74%的准确性。虽然这与IMDB数据上相同分类器的82%的性能相比有所减少，但仍足够高来证明从电影评论任务到书评任务的零-shot知识转移的一个实例。让我们尝试通过自编码器进行零-shot域适应来提高这个数字。
- en: Note Zero-shot domain transfer is more likely to be successful the more “similar”
    the source and target domains. Similarity can be measured by techniques such as
    cosine similarity applied to the sent2vec vectors from the two domains. A suggested
    take-home exercise is to explore the MDSD cosine similarities across some domains,
    along with the efficacy of the zero-shot transfer experiment described here between
    them. The library scikit-learn has a simple method for computing cosine similarity.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，零-shot域转移越是“相似”的源和目标域，成功的可能性就越大。相似性可以通过应用于两个域的sent2vec向量的余弦相似度等技术来衡量。建议的课后练习是探索MDSD余弦相似度在一些领域之间的应用，以及在此处描述的零-shot转移实验之间的有效性。scikit-learn库有一个简单的方法来计算余弦相似度。
- en: We train an autoencoder to reconstruct the IMDB data. The autoencoder takes
    the form of a shallow neural network that is similar to the multitask layers we
    used in the previous section. The Keras Python code is shown in listing 4.7\.
    A major difference from the previous neural networks is that because this is a
    regression problem, there is no activation in the output layer. The encoding dimension
    `encoding_dim` was empirically tuned for the right balance of accuracy and computing
    cost.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练一个自编码器来重构IMDB数据。自编码器采用了一个类似于我们在上一部分中使用的多任务层的浅层神经网络。Keras Python代码在4.7节中显示。与以前的神经网络的一个主要区别是，因为这是一个回归问题，输出层没有激活。编码维度`encoding_dim`是通过经验调整以获得正确的准确性和计算成本的平衡。
- en: Listing 4.7 Keras shallow neural autoencoder
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 4.7 Keras浅层神经自编码器列表
- en: '[PRE14]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ The input size must be the same as the dimension of the sent2vec vectors.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 输入大小必须与sent2vec向量的维度相同。
- en: ❷ Encodes into the space of the specified latent dimension, encoding_dim
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 编码到指定的潜在维度空间，编码维度为encoding_dim。
- en: ❸ Decodes from the space of the specified latent dimension back into the sent2vec
    space
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 从指定的潜在维度空间解码回到sent2vec空间。
- en: 'We train the autoencoder for 50 epochs, which takes only a few seconds, by
    setting both the input and output to the IMDB sent2vec vectors from the previous
    chapter, as shown via the following compilation and training code:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练自编码器50个epochs，只需要几秒钟，通过将输入和输出都设置为前一章节中的IMDB sent2vec向量，如下所示通过编译和训练代码：
- en: '[PRE15]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We use mean squared error (mse) as a loss function for this regression problem
    and mean absolute error (mae) as an additional metric. A minimum validation mae
    value of approximately 0.06 was achieved.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这个回归问题中使用均方误差（mse）作为损失函数，和平均绝对误差（mae）作为额外的度量。最小验证mae值约为0.06。
- en: 'We next project the book reviews into the IMDB feature space using the autoencoder
    trained to reconstruct the features just described. This just means we preprocess
    the book review feature vectors using the autoencoder. We then repeat the accuracy
    evaluation experiment of the IMDB classifier on these preprocessed vectors as
    input as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用训练好的自编码器将书评投影到IMDB特征空间中，该自编码器是经过训练来重构刚刚描述的特征。这意味着我们使用自编码器对书评特征向量进行预处理。然后我们将IMDB分类器的准确性评估实验重复在这些预处理的向量上作为输入，如下所示：
- en: '[PRE16]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: An accuracy of approximately 75% is now observed, demonstrating an improvement
    of about 1% and an instance of zero-shot domain adaptation. Repeating this several
    times, we find the improvement to consistently stay around 0.5-1%, giving us confidence
    that the autoencoding domain adaptation did indeed lead to some positive transfer.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在观察到的准确率约为75%，表明改进约为1%，并且实现了零-shot领域适应的一个实例。重复多次后，我们发现改进始终保持在0.5-1%左右，这让我们确信自动编码领域适应的确导致了一些积极的迁移。
- en: Summary
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: Pretrained word embeddings, as well as embeddings at higher levels of text—such
    as sentences—are ubiquitous in NLP and can be used to convert text into numbers/vectors.
    This simplifies further processing to extract meaning from them.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预训练的词嵌入，以及文本更高层次的嵌入——如句子，已经在自然语言处理中变得无处不在，并且可以用来将文本转换为数字/向量。这简化了进一步从中提取含义的处理过程。
- en: Such an extraction represents a form of semisupervised shallow transfer learning,
    which has been widely used in practice with immense success.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种提取代表了一种半监督浅层迁移学习，它已经在实践中被广泛使用，并取得了巨大成功。
- en: Techniques like hard-parameter sharing and soft-parameter sharing allow us to
    create multitask learning systems, which have potential benefits, including simplified
    engineering design, improved generalization, and reduced overfitting.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像硬参数共享和软参数共享这样的技术使我们能够创建多任务学习系统，其中包括简化的工程设计、改进的泛化和减少的过拟合风险。
- en: Sometimes it may be possible to achieve zero-shot transfer learning with no
    labeled data in the target domain, which is an ideal scenario because labeled
    data can be expensive to collect.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有时可能会在目标领域没有标记数据的情况下实现零-shot迁移学习，这是一种理想的情况，因为标记数据收集可能很昂贵。
- en: It is sometimes possible to modify or adapt data in a target domain to be more
    similar to the data in the source domain, for instance, via projection methods
    such as autoencoders, and this can improve performance.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有时可能会修改或调整目标领域的数据，使其更类似于源领域的数据，例如，通过自编码器等投影方法，这可以提高性能。
- en: 1. S. J. Pan and Q. Yang, “A Survey on Transfer Learning,” IEEE Transactions
    on Knowledge and Data Engineering (2009).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 1. S. J. Pan and Q. Yang, “迁移学习综述”，IEEE Knowledge and Data Engineering Transactions（2009）。
- en: 2. S. Ruder, “Neural Transfer Learning for Natural Language Processing,” National
    University of Ireland, Galway, Ireland (2019).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 2. S. Ruder, “自然语言处理的神经迁移学习”，爱尔兰国立大学，高威（2019）。
- en: 3. D. Wang and T. F. Zheng, “Transfer Learning for Speech and Language Processing,”
    Proceedings of 2015 Asia-Pacific Signal and Information Processing Association
    Annual Summit and Conference (APSIPA).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 3. D. Wang and T. F. Zheng, “语音和语言处理的迁移学习”，2015年亚太信号和信息处理协会年度峰会和会议（APSIPA）。
- en: '4. Jing Wang, Haibo Hea Danil, and V.Prokhorov, “A Folded Neural Network Autoencoder
    for Dimensionality Reduction,” Procedia Computer Science 13 (2012): 120-27.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '4. Jing Wang, Haibo Hea Danil和V.Prokhorov, “用于降维的折叠神经网络自动编码器”，计算机科学会议文献13 (2012):
    120-27.'
- en: 5. Z. Yin and Y. Shen, “On the Dimensionality of Word Embedding,” 32nd Conference
    on Neural Information Processing Systems (NeurIPS 2018), Montreal, Canada.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 5. Z. Yin and Y. Shen, “关于词嵌入的维度性”，32届神经信息处理系统会议(NeurIPS 2018)，加拿大蒙特利尔。
- en: 6. [https://fasttext.cc/docs/en/english-vectors.html](https://fasttext.cc/docs/en/english-vectors.html)
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 6. [https://fasttext.cc/docs/en/english-vectors.html](https://fasttext.cc/docs/en/english-vectors.html)
- en: 7. J. Alabi et al., “Massive vs. Curated Word Embeddings for Low-Resourced Languages.
    The Case of Yorùbá and Twi,” The International Conference on Language Resources
    and Evaluation (LREC 2020), Marseille, France.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 7. J. Alabi等，“大规模词嵌入与策划词嵌入对低资源语言的影响。约鲁巴语和特威语的情况”，语言资源和评估国际会议（LREC 2020），法国马赛。
- en: 8. [https://www.kaggle.com/yangjia1991/jigsaw](https://www.kaggle.com/yangjia1991/jigsaw)
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 8. [https://www.kaggle.com/yangjia1991/jigsaw](https://www.kaggle.com/yangjia1991/jigsaw)
- en: 9. [https://github.com/epfml/sent2vec](https://github.com/epfml/sent2vec)
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 9. [https://github.com/epfml/sent2vec](https://github.com/epfml/sent2vec)
- en: 10. [https://www.kaggle.com/maxjeblick/sent2vec](https://www.kaggle.com/maxjeblick/sent2vec)
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 10. [https://www.kaggle.com/maxjeblick/sent2vec](https://www.kaggle.com/maxjeblick/sent2vec)
- en: 11. S. Ruder, “Neural Transfer Learning for Natural Language Processing,” National
    University of Ireland, Galway, Ireland (2019).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 11. S. Ruder, “自然语言处理的神经迁移学习”，爱尔兰国立大学，高威（2019）。
- en: 12. Hal Daumé III, “Frustratingly Easy Domain Adaptation,” Proceedings of the
    45th Annual Meeting of the Association of Computational Linguistics (2007), Prague,
    Czech Republic.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 12. Hal Daumé III，“令人沮丧地简单领域自适应”，计算语言学协会第45届年会论文集（2007），捷克布拉格。
- en: 13. S. Ruder and B. Plank, “Learning to Select Data for Transfer Learning with
    Bayesian Optimization,” Proceedings of the 2017 Conference on Empirical Methods
    in Natural Language Processing (2017), Copenhagen, Denmark.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 13. S. Ruder 和 B. Plank，“使用贝叶斯优化学习选择数据的迁移学习方法”，2017年自然语言处理实证方法会议论文集（2017），丹麦哥本哈根。
- en: '14. Jing Wang, Haibo Hea Danil, and V.Prokhorov, “A Folded Neural Network Autoencoder
    for Dimensionality Reduction,” Procedia Computer Science 13 (2012): 120-127.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 14. Jing Wang、Haibo Hea Danil 和 V.Prokhorov，“用于降维的折叠神经网络自动编码器”，Procedia Computer
    Science 13（2012）：120-127。
- en: 15. [https://www.cs.jhu.edu/~mdredze/datasets/sentiment/](https://www.cs.jhu.edu/~mdredze/datasets/sentiment/)
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 15. [https://www.cs.jhu.edu/~mdredze/datasets/sentiment/](https://www.cs.jhu.edu/~mdredze/datasets/sentiment/)
