- en: 4 Shallow transfer learning for NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Using pretrained word embeddings in a semisupervised fashion to transfer pretrained
    knowledge to a problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using pretrained embeddings of larger sections of text in a semisupervised fashion
    to transfer pretrained knowledge to a problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using multitask learning to develop better-performing models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modifying target domain data to reuse knowledge from a resource-rich source
    domain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will cover some prominent shallow transfer learning approaches
    and concepts. This allows us to explore some major themes in transfer learning,
    while doing so within the context of relatively simple models in the class of
    eventual interest—shallow neural networks. Several authors have suggested various
    classification systems for categorizing transfer learning methods into groups.[¹](#pgfId-1092394),[²](#pgfId-1092397),[³](#pgfId-1092400)
    Roughly speaking, categorization is based on whether transfer occurs between different
    languages, tasks, or data domains. Each of these types of categorization is usually
    correspondingly referred to as *cross-lingual learning*, *multitask learning*,
    and *domain adaptation*, as visualized in figure 4.1.
  prefs: []
  type: TYPE_NORMAL
- en: '![04_01](../Images/04_01.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 Visualizing the categorization of transfer learning into multitask
    learning, domain adaptation, and cross-lingual learning
  prefs: []
  type: TYPE_NORMAL
- en: The methods we will look at here will involve components that are neural networks
    in one way or another, but unlike those discussed in chapter 3, these neural networks
    do not have many layers. This is the reason the label “shallow” is appropriate
    to describe this collection of methods. Just as in the previous chapter, we present
    these methods in the context of specific hands-on examples to facilitate the advancement
    of your practical NLP skills. Cross-lingual learning will be addressed in later
    chapters of the book, given that modern neural machine translation methods are
    deep in general. We will explore the other two types of transfer learning briefly
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We begin by exploring a common form of semisupervised learning that employs
    pretrained word embeddings, such as word2vec, in the context of applying it to
    one of the examples from the previous two chapters of the book. Recall from chapter
    1 that these methods differ from those in chapter 3 in that they produce a single
    vector per word, regardless of context.
  prefs: []
  type: TYPE_NORMAL
- en: We revisit the IMDB movie review sentiment classification. Recall that this
    example is concerned with classifying movie reviews from IMDB into positive or
    negative based on the sentiments expressed. It is a prototypical sentiment analysis
    example that has been used widely in the literature to study many algorithms.
    We combine feature vectors generated by pretrained word embeddings for each review
    with some traditional machine learning classification methods, namely random forests
    and logistic regression. We then demonstrate that using higher-level embeddings,
    which vectorize bigger sections of text—at the sentence, paragraph, and document
    level—can lead to improved performance. The general idea of vectorizing text and
    then applying a traditional machine learning classification method to the resulting
    vectors is visualized in figure 4.2.
  prefs: []
  type: TYPE_NORMAL
- en: '![04_02](../Images/04_02.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 Sequence of typical steps for semisupervised learning with word,
    sentence, or document embeddings
  prefs: []
  type: TYPE_NORMAL
- en: Subsequently, we cover multitask learning and learn how to train a single system
    simultaneously to perform multiple tasks—in our case, represented by both of our
    running examples from the previous chapter, the email spam classification and
    the IMDB movie review sentiment analysis. You can gain several potential benefits
    from multitask learning. By training a single machine learning model for multiple
    tasks, a shared representation is learned on a larger and more varied collection
    of data from the combined data pool, which can lead to performance improvements.
    Moreover, it has been widely observed that this shared representation has a better
    ability to generalize to tasks beyond those that were trained on, and this improvement
    can be achieved without any increase in model size. We explore some of these benefits
    in the context of our running examples. Specifically, we focus on shallow neural
    multitask learning, where a single additional dense layer, as well as a classification
    layer, is trained for each specific task in the setup. Different tasks also share
    a layer between them, a setup typically referred to as *hard-parameter sharing*.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we introduce a popular dataset as another running example for the remainder
    of the chapter. This is the Multi-Domain Sentiment Dataset, which describes [Amazon.com](http://Amazon.com)
    product reviews for a set of different products. We use this dataset to explore
    domain adaptation. Assume that we are given one *source* domain, which can be
    defined as a particular distribution of data for a specific task, and a classifier
    that has been trained to perform well on data in that domain for that task. The
    goal of domain adaptation is to modify, or adapt, data in a different *target*
    domain in such a way that the pretrained knowledge from the source domain can
    aid learning in the target domain. We apply a simple *autoencoding* approach to
    “project” samples in the target domain into the source domain feature space.
  prefs: []
  type: TYPE_NORMAL
- en: An autoencoder is a system that learns to reconstruct inputs with very high
    accuracy, typically by encoding them into an efficient latent representation and
    learning to decode that representation efficiently. They have traditionally been
    heavily used in model reduction applications, because the latent representation
    is often of a smaller dimension than the original space from which the encoding
    happens, and the said dimension value can also be picked to strike the right balance
    of computational efficiency and accuracy.[⁴](#pgfId-1092426) In the extreme scenario,
    improvements can be obtained with no labeled data in the target domain being used
    for training. This is typically referred to as *zero-shot domain adaptation*,
    where learning happens with no labeled data in the target domain. We demonstrate
    an instance of it in our experiments.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Semisupervised learning with pretrained word embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The concept of word embeddings is central to the field of NLP. It is a name
    given to a collection of techniques that produce a set of vectors of real numbers
    for each word that needs to be analyzed. A major consideration in word embedding
    design is the dimension of the vector generated. Bigger vectors generally can
    achieve better representation capability of words within a language and thereby
    better performance on many tasks, while naturally being more expensive computationally.
    Picking the optimal dimension requires striking a balance between these competing
    factors and has often been done empirically, although some recent approaches argue
    for a more thorough theoretical optimization approach.[⁵](#pgfId-1092436)
  prefs: []
  type: TYPE_NORMAL
- en: As was outlined in the first chapter of this book, this important subarea of
    NLP research has a rich history, originating with the *term-vector model of information
    retrieval* in the 1960s. This culminated with pretrained, shallow neural-network-based
    techniques such as fastText, GloVe, and word2vec, which came in several variants
    in the mid-2010s, including Continuous Bag of Words (CBOW) and Skip-Gram. Both
    CBOW and Skip-Gram are extracted from shallow neural networks that were trained
    for various goals. Skip-Gram attempts to predict words neighboring any target
    word in a sliding window, whereas CBOW attempts to predict the target word given
    the neighbors. GloVe—which stands for global vectors—attempts to extend word2vec
    by incorporating global information into the embeddings. It optimizes the embeddings
    such that the cosine product between words reflects the number of times they cooccur,
    with the goal of making the resulting vectors more interpretable. The technique
    fastText attempts to enhance word2vec by repeating the Skip-Gram methods on character
    n-grams (versus word n-grams), thereby being able to handle previously unseen
    words. Each of these variants of pretrained embeddings has its strengths and weaknesses,
    and these are summarized in table 4.1.
  prefs: []
  type: TYPE_NORMAL
- en: Table 4.1 Comparison of strengths and weaknesses of various popular word-embedding
    methods
  prefs: []
  type: TYPE_NORMAL
- en: '| Word-embedding method | Strengths | Weaknesses |'
  prefs: []
  type: TYPE_TB
- en: '| Skip-Gram word2vec | Works well with a small training dataset and rare words
    | Slower training, plus lower accuracy for frequent words |'
  prefs: []
  type: TYPE_TB
- en: '| CBOW word2vec | Several times faster in training and better accuracy for
    frequent words | Doesn’t work as well with little training data and rare words
    |'
  prefs: []
  type: TYPE_TB
- en: '| GloVe | Vectors have more interpretability than other methods | Higher memory
    requirement during training to store co-occurrences of words |'
  prefs: []
  type: TYPE_TB
- en: '| fastText | Can handle out-of-vocabulary words | Higher computing cost; larger
    and more complex model |'
  prefs: []
  type: TYPE_TB
- en: To reiterate, fastText is known for its ability to handle out-of-vocabulary
    words, which comes from it having been designed to embed subword character n-grams,
    or *subwords* (versus entire words, as is the case with word2vec). This enables
    it to build up embeddings for previously unseen words by aggregating composing
    character n-gram embeddings. That comes at the expense of a larger pretrained
    embedding and a higher computing resource requirement and cost. For these reasons,
    we will use the fastText software framework in this section to load embeddings
    in the word2vec input format, without the subword information. This allows us
    to keep the computing cost lower, making the exercise easier for the reader, while
    also showcasing how out-of-vocabulary issues would be handled and providing a
    solid experience platform from which the reader can venture into subword embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin the computing experiment! The first thing we need to do is obtain
    the appropriate pretrained word-embedding file. Because we will be using the fastText
    framework, we could obtain these pretrained files from the authors’ official website,[⁶](#pgfId-1092480)
    which hosts appropriate embedding files in a number of formats. Note that these
    files are extremely large, because they attempt to capture vectorization information
    about all possible words within a language. For instance, the .vec format embedding
    for the English language, which was trained on Wikipedia 2017 and provides vectorization
    information without handling subwords and out-of-vocabulary words, is about 6
    GB. The corresponding .bin format embedding, which contains the famous fastText
    subword information and can handle out-of-vocabulary words, is about 25% larger,
    at approximately 7.5 GB. We also note that Wikipedia embeddings are provided in
    up to 294 languages, even including traditionally unaddressed African languages
    such as Twi, Ewe, and Hausa. It has been shown, however, that for many of the
    included low-resource languages, the quality of these embeddings is not very good.[⁷](#pgfId-1092483)
  prefs: []
  type: TYPE_NORMAL
- en: Due to the size of these embeddings, it is a lot easier to execute this example
    using the recommended cloud-based notebooks we have hosted on Kaggle (versus running
    them locally) because the embedding files have already been openly hosted in the
    cloud environment by other users.[⁸](#pgfId-1092488) As such, we can simply attach
    them to a running notebook without having to obtain and run the files locally.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the embedding is available, we can load it using the following code snippet,
    making sure to time the loading function call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Loads a pretrained fastText embedding in “word2vec” format (without subword
    information)
  prefs: []
  type: TYPE_NORMAL
- en: Loading the embedding takes more than 10 minutes the first time on the Kaggle
    environment we used for execution. In practice, in such a situation, it is not
    uncommon to load the embedding once into memory and then serve access to it using
    an approach such as Flask for as long as it is needed. This can also be achieved
    using the Jupyter notebook that comes along with this chapter of the book.
  prefs: []
  type: TYPE_NORMAL
- en: Having obtained and loaded the pretrained embedding, let’s look back at the
    IMDB movie review classification example, which we will analyze in this section.
    In particular, we pick up right after listing 2.10 in the preprocessing stage
    of the pipeline, which generates a NumPy array `raw_data` that contains word-level
    tokenized representations of movie reviews, with stop words and punctuation removed.
    For the reader’s convenience, we show listing 2.10 again next.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.10 (Duplicated from chapter 2) Loading the IMDB data into a NumPy
    array
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Goes through every file in the current folder
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Applies tokenization and stop word analysis routines
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Tracks corresponding sentiment labels
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Converts to a NumPy array
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Calls the function above on the data
  prefs: []
  type: TYPE_NORMAL
- en: If you have already worked through chapter 2, you may recall that after listing
    2.10, we proceeded to generate a simple bag-of-words representation for the output
    NumPy array, which simply counts occurrence frequencies of possible word tokens
    in each review. We then used the resulting vectors as numerical features for further
    machine learning tasks. Here, instead of the bag-of-words representation, we extract
    corresponding vectors from the pretrained embedding instead.
  prefs: []
  type: TYPE_NORMAL
- en: Because our embedding of choice does not handle out-of-vocabulary words out
    of the box, the next thing we do is develop a methodology for addressing this
    situation. The simplest thing to do, quite naturally, is to simply skip any such
    words. Because the fastText framework errors out when such a word is encountered,
    we will use a *try and except* block to catch these errors without interrupting
    execution. Assume that you are given a pretrained input embedding that serves
    as a dictionary, with words as keys and corresponding vectors as values, and an
    input list of words in a review. The next listing shows a function that produces
    a two-dimensional NumPy array with rows representing embedding vectors for each
    word in the review.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.1 Producing a 2-D Numpy array of movie review word embedding vectors
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Loops through every word
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Extracts the corresponding embedding vector, and enforces “row shape”
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Handles the edge case of the first vector and an empty-out array
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Concatenates the row embedding vector to the output NumPy array
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Skips the execution on the current word, and continues the execution from
    the next word when out-of-vocabulary errors occur
  prefs: []
  type: TYPE_NORMAL
- en: The function in this listing can now be used to analyze the entire dataset as
    captured by the variable `raw_data`. However, before doing so we must decide how
    we will combine or *aggregate* the embedding vectors for individual words in a
    review into a single vector representing the entire review. It has been found
    in practice that the heuristic of simply averaging the words works as a strong
    baseline. Because the embeddings were trained in a way that ensures that similar
    words are closer to each other in the resulting vector space, it makes intuitive
    sense that their average would represent the average meaning of the collection.
    The averaging baseline for summarization/ aggregation is often recommended as
    a first attempt at embedding bigger sections of text from word embeddings. This
    is also the approach we use in this section, as demonstrated by the code in listing
    4.2\. Effectively, this code calls the function from listing 4.1 repeatedly on
    every review in the corpus, averages the output, and concatenates the resulting
    vectors into a single two-dimensional NumPy array. The rows of this resulting
    array correspond to aggregated-by-averaging embedding vectors for each review.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.2 Loading IMDB data into a NumPy array
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Loops through every IMDB review
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Extracts the embedding vectors for every word in the review, making sure to
    handle out-of-vocabulary words
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Averages the word vectors in each review
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Concatenates the average row vector to output the NumPy array
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Out-of-vocabulary edge case handling
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now assemble embedding vectors for the whole dataset using the next
    function call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: These can now be used as feature vectors for the same logistic regression and
    random forest codes as were used in listings 2.11 and 3.1, respectively. Using
    these codes to train and evaluate these models, we found the corresponding accuracy
    scores to be 77% and 66%, respectively, when the hyperparameters `maxtokens` and
    `maxtokenlen` are set to 200 and 100, respectively, and the value of `Nsamp`—the
    number of samples from each class—is equal to 1,000\. These are only slightly
    lower than the corresponding values obtained from the bag-of-words baseline that
    was initially developed in the previous chapters (corresponding to accuracy scores
    of 79% and 67%, respectively). We hypothesize that this slight reduction is likely
    due to the aggregation of individual word vectors by the naive averaging approach
    that was described. In the next section, we attempt to perform more intelligent
    aggregation using embedding methods that were designed to embed at a higher text
    level.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Semisupervised learning with higher-level representations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Several techniques, inspired by word2vec, try to embed larger sections of text
    into vector spaces in such a way that sentences with similar meanings would be
    closer to each other in the induced vector space. This enables us to perform arithmetic
    on sentences to make inferences about analogies, combined meanings, and so on.
    One prominent approach is paragraph vectors, or *doc2vec*, which exploits the
    concatenation (versus averaging) of words from pretrained word embeddings in summarizing
    them. Another is sent2vec, which extends the classic Continuous Bag-of-Words (CBOW)
    of word2vec—where a shallow network is trained to predict a word in a sliding
    window from its context—to sentences by optimizing word and word n-gram embeddings
    for an accurate averaged representation. In this section, we use a pretrained
    sent2vec model as an illustrative representative method and apply it to the IMDB
    movie classification example.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find a few open source implementations of sent2vec online. We are employing
    a heavily used implementation that builds on fastText.[⁹](#pgfId-1092609) To install
    that implementation directly from its hosted URL, execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Quite naturally, just as in the case of the pretrained word embeddings, the
    next step is to obtain the pretrained sent2vec sentence embedding to be loaded
    by the particular implementation/framework we have installed. These are hosted
    by the authors of the framework on their GitHub page and on Kaggle by other users.[^(10)](#pgfId-1092617)
    For simplicity, we choose the smallest 600-dimensional embedding `wiki_unigrams.bin`,
    approximately 5 GB in size, which captures just the unigram information on Wikipedia.
    Note that significantly larger models are available pretrained on book corpora
    and Twitter, and also include bigram information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having obtained the pretrained embedding, we load it using the following code
    snippet, making sure to time the loading process as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Loads sent2vec embedding
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth mentioning that we found the load time during the first execution
    to be less than 10 seconds—a notable improvement over the fastText word embedding
    loading time of over 10 minutes. This increased speed is attributed to the significantly
    more efficient implementation of the current package versus the gensim implementation
    that we used in the previous section. It is not uncommon to try different packages
    to find the most efficient one for your application in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we define a function to generate vectors for a collection of reviews.
    It is essentially a simpler form of the function presented in listing 4.2 for
    pretrained word embeddings. It is simpler because we do not need to worry about
    out-of-vocabulary words. This function is shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.3 Loading IMDB data into a NumPy array
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Loops through every IMDB review
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Extracts the embedding vectors for every review
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Edge case handling
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now use this function to extract sent2vec embedding vectors for each
    review as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We can also split this into training and test datasets and train logistic regression
    and random forest classifiers on top of the embedding vectors as before, using
    code analogous to what is shown in listings 2.11 and 3.1, respectively. This yields
    accuracy scores of 82% and 68% for the logistic regression and random forest classifiers,
    respectively (at the same hyperparameter values as in the previous section). This
    value for the logistic regression classifier combined with sent2vec is an improvement
    on the corresponding values of 79% and 67%, respectively, for the bag-of-words
    baseline, as well as an improvement over the averaged word embedding approach
    from the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Multitask learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Traditionally, machine learning algorithms have been trained to perform a single
    task at a time, with the data collected and trained on being independent for each
    separate task. This is somewhat antithetical to the way humans and other animals
    learn, where training for multiple tasks occurs simultaneously, and information
    from training on one task may inform and accelerate the learning of other tasks.
    This additional information may improve performance not just on the current tasks
    being trained on but also on future tasks, and sometimes even in cases where no
    labeled data is available on such future tasks. This scenario of transfer learning
    with no labeled data in the target domain is often referred to as zero-shot transfer
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: In machine learning, multitask learning has historically appeared in a number
    of settings, from *multiobjective optimization* to *l2* and other forms of *regularization*
    (which can itself be framed as a form of multiobjective optimization). Figure
    4.3 shows the form of neural multitask learning we will employ, where some layers/parameters
    are shared between all tasks, that is, *hard parameter sharing*.[^(11)](#pgfId-1092675)
  prefs: []
  type: TYPE_NORMAL
- en: '![04_03](../Images/04_03.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 The general form of neural multitask learning we will employ—hard
    parameter sharing (in this case, three tasks)
  prefs: []
  type: TYPE_NORMAL
- en: In the other prominent type of neural multitask learning, *soft parameter sharing*,
    all tasks have their own layers/parameters that are not shared. Instead, they
    are encouraged to be similar via various constraints imposed on the task-specific
    layers across the various tasks. We do not address this type of multitask learning
    further, but it is good to be aware of its existence for your own future potential
    literature explorations.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s proceed to our illustrative example for this section, by setting it up
    and baselining it in the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1 Problem setup and a shallow neural single-task baseline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Consider figure 4.3 again, but with only two tasks—the first task being IMDB
    movie review classification from the previous two sections, and the second task
    being email spam classification from the previous chapter. The resulting setup
    represents the specific example we will address in this section. This setup is
    shown in figure 4.4 to facilitate conceptualizing.
  prefs: []
  type: TYPE_NORMAL
- en: '![04_04](../Images/04_04.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 The specific form of neural multitask hard parameter sharing we will
    employ, with two specific tasks shown—IMDB reviews and email spam classification
  prefs: []
  type: TYPE_NORMAL
- en: Before proceeding, we must decide how the inputs to the resulting neural network
    will be converted into numbers for analysis. One popular choice is to encode the
    input at the character level using one-hot encoding, where each character is replaced
    by a sparse vector of a dimension equal to the total number of possible characters.
    This vector contains 1 in the column corresponding to the character and 0 otherwise.
    An illustration of this method, which aims to help you concisely visualize the
    process of one-hot encoding, is shown in figure 4.5.
  prefs: []
  type: TYPE_NORMAL
- en: '![04_05](../Images/04_05.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 A visualization of the process of one-hot encoding characters into
    row-vector representations. The process replaces every character in the vocabulary
    with a sparse vector with a dimension equal to the size of the vocabulary. 1 is
    placed in the column corresponding to the vocabulary character index.
  prefs: []
  type: TYPE_NORMAL
- en: 'This method can be expensive from a memory perspective, given the significant
    inherent increase in dimension, and as such, it is common to perform the one-hot
    encoding “on the fly” via specialized neural network layers. Here, we take an
    even more straightforward approach: we pass each review through the sent2vec embedding
    function and use the embedding vectors as input features to the setup shown in
    figure 4.4.'
  prefs: []
  type: TYPE_NORMAL
- en: Before proceeding to the exact two-task setup shown in figure 4.4, we perform
    another baseline. We use the IMDB movie classification task as the only one present,
    to see how the task-specific shallow neural classifier compares with the model
    from the previous section. The code corresponding to this shallow neural baseline
    is shown in the next listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.4 Shallow single-task Keras neural network
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: ❶ The input must match the dimension of the sent2vec vectors.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ A dense neural layer trained on top of the sent2vec vectors
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Applies dropout to reduce overfitting
  prefs: []
  type: TYPE_NORMAL
- en: ❹ The output indicates a single binary classifier—is review “positive” or “negative”?
  prefs: []
  type: TYPE_NORMAL
- en: We found that the performance of this classifier was about 82% at the hyperparameter
    values specified in the previous section. This is higher than the baseline of
    bag-of-words combined with logistic regression and approximately equal to sent2vec
    combined with logistic regression from the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2 Dual-task experiment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We now introduce another task: the email spam classification problem from the
    previous chapter. We will not repeat the preprocessing steps and associated code
    here for this auxiliary task; see chapter 2 for these details. Assuming the availability
    of the sent2vec vectors `train`_`x2` corresponding to the emails in the data sample,
    listing 4.5 shows how one can create a multiple-output shallow neural model to
    train it simultaneously for email spam classification and the classification of
    IMDB movie reviews via hard-parameter sharing.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.5 Shallow dual-task hard parameter sharing Keras neural network
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Concatenates the sent2vec vectors for the different tasks
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Shared dense neural layer
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Two task-specific outputs, each being a binary classifier
  prefs: []
  type: TYPE_NORMAL
- en: 'Having defined the hard-parameter sharing setup for the two-task multitask
    scenario involving IMDB movie reviews and email spam classification, we can compile
    and train the resulting model via the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Specifies two loss functions (both binary_crossentropy in our case)
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Specifies the training and validation data for each input
  prefs: []
  type: TYPE_NORMAL
- en: For this experiment, we set the hyperparameters `maxtokens` and `maxtokenlen`
    both to 100 and the value of `Nsamp`—the number of samples from each class—to
    1,000 (as in the previous section).
  prefs: []
  type: TYPE_NORMAL
- en: 'We found, upon training the multitask system, that the IMDB classification
    performance dropped slightly, from approximately 82% in the single-task shallow
    setup in listing 4.4 to about 80%. The email classification accuracy similarly
    dropped from 98.7% to 98.2%. Given the drop in performance, one may rightfully
    ask: What was the point of all this?'
  prefs: []
  type: TYPE_NORMAL
- en: First of all, observe that the trained model can be deployed independently for
    each task, by simply replacing the omitted task input with zeros to respect the
    expected overall input dimension and ignoring the corresponding output. Moreover,
    we expect the shared pretrained layer `dense1` of the multitask setup to be more
    readily generalizable to arbitrary new tasks than that from listing 4.4\. This
    is because it has been trained to be predictive on a more varied and general set
    of data and tasks.
  prefs: []
  type: TYPE_NORMAL
- en: To make this more concrete, consider replacing either or both task-specific
    layers with new ones, initializing the shared layer `dense1` to pretrained weights
    from the previous experiment, and fine-tuning the resulting model on the new task
    dataset. Having seen a broader range of task data, potentially similar to the
    newly added tasks, these shared weights are more likely to contain useful information
    for the downstream tasks being considered.
  prefs: []
  type: TYPE_NORMAL
- en: We will return to the idea of multitask learning later in the book, which will
    give us the opportunity to investigate and think about these phenomena further.
    The experiment in this section has hopefully armed you with the required foundation
    for further exploration.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Domain adaptation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we briefly explore the idea of *domain adaptation*, one of
    the oldest and most prominent ideas in transfer learning. An implicit assumption
    made often by machine learning practitioners is that the data during the inference
    phase will come from the same distribution as the data that was used for training.
    This is, of course, rarely true in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Enter domain adaptation to attempt to address this issue. Let’s define domain
    as a particular distribution of data for a specific task. Assume that we are given
    a *source* domain and an algorithm that has been trained to perform well on data
    in that domain. The goal of domain adaptation is to modify, or adapt, data in
    a different *target* domain in such a way that the pretrained knowledge from the
    source domain can be applicable to faster learning and/or direct inference in
    the target domain. A variety of approaches have been explored, ranging from multitask
    learning (as introduced in the previous section)—where learning on different data
    distributions occurs simultaneously—to coordinate transformations—which enable
    more effective learning on a single combined feature space[^(12)](#pgfId-1092782)—to
    methods that exploit measures of similarity between the source and target domains
    to help us select which data should be used for training.[^(13)](#pgfId-1092785)
  prefs: []
  type: TYPE_NORMAL
- en: We apply a simple *autoencoding* approach to “project” samples in the target
    domain into the source domain feature space. An autoencoder is a system that can
    learn to reconstruct inputs with high accuracy, typically by encoding them into
    an efficient latent representation and learning to decode the said representation.
    A technical way of describing the process of reconstructing input is “learning
    the identity function.” Autoencoders have traditionally been heavily used in model
    dimensionality-reduction applications, because the latent representation is often
    of a smaller dimension than the original space from which the encoding happens,
    and the said dimension value can also be picked to strike the right balance between
    computational efficiency and accuracy.[^(14)](#pgfId-1092790) In the extreme favorable
    scenario, you can obtain improvements with no labeled data in the target domain,
    which is typically referred to as *zero-shot domain adaptation*.
  prefs: []
  type: TYPE_NORMAL
- en: The idea of zero-shot transfer learning arises in many contexts. You can think
    of it as a sort of “holy grail” of transfer learning, because obtaining labeled
    data in the target domain can be an expensive exercise. Here, we explore whether
    a classifier trained to predict the polarity of IMDB movie reviews can be used
    to predict the polarity of reviews in some other domain. For example, can a classifier
    trained on IMDB review data predict the polarity of book reviews or DVD reviews
    obtained from a completely different data source?
  prefs: []
  type: TYPE_NORMAL
- en: A natural alternative source of reviews, given its ubiquity in the contemporary
    world, is Amazon.com. Few sources of data today contain as much variety with regard
    to categories of products and sheer volume of data as this e-commerce website,
    which many Americans have come to rely on for basic daily-need purchases, at the
    expense of business for more traditional brick-and-mortar stores. A rich repository
    of reviews is available. Indeed, one of the most prominently and heavily explored
    datasets in the space of NLP domain adaptation happens to be a collection of reviews
    for different product categories on Amazon—the Multi-Domain Sentiment Dataset.[^(15)](#pgfId-1092797)
    This dataset represents 25 categories, from which we picked the product category
    of book reviews, feeling that it was sufficiently different from IMDB reviews
    to present a challenging test case.
  prefs: []
  type: TYPE_NORMAL
- en: The data in this dataset is contained in a markup language format, where tags
    are used to define various elements, and is organized by category and polarity
    into separate files. It suffices to note for our purposes that reviews are contained
    within appropriately named `<review_text>...</review_text>` tags. Given this information,
    the code in the next listing can be used to load positive and negative book reviews
    and prepare them for analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.6 Loading reviews from the Multi-Domain Sentiment Dataset
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Locates the first line of review, and combines all subsequent characters until
    the end tag into the review text
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Reads lines from the source text file, both positive and negative reviews,
    by leveraging defined function
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Creates labels for the positive and negative classes
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Appends, shuffles, and extracts the corresponding sent2vec vectors
  prefs: []
  type: TYPE_NORMAL
- en: 'Having loaded the book review text and prepared it for further processing,
    we now test the trained IMDB classifier from the previous section directly on
    the target data to see how accurate it is without any processing, using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This yields an accuracy of about 74%. Although this is a decrease from the performance
    of the same classifier of 82% on the IMDB data, it is still sufficiently high
    to demonstrate an instance of zero-shot knowledge transfer from the movie review
    task to the book review task. Let’s attempt to improve this number via zero-shot
    domain adaptation with an autoencoder.
  prefs: []
  type: TYPE_NORMAL
- en: Note Zero-shot domain transfer is more likely to be successful the more “similar”
    the source and target domains. Similarity can be measured by techniques such as
    cosine similarity applied to the sent2vec vectors from the two domains. A suggested
    take-home exercise is to explore the MDSD cosine similarities across some domains,
    along with the efficacy of the zero-shot transfer experiment described here between
    them. The library scikit-learn has a simple method for computing cosine similarity.
  prefs: []
  type: TYPE_NORMAL
- en: We train an autoencoder to reconstruct the IMDB data. The autoencoder takes
    the form of a shallow neural network that is similar to the multitask layers we
    used in the previous section. The Keras Python code is shown in listing 4.7\.
    A major difference from the previous neural networks is that because this is a
    regression problem, there is no activation in the output layer. The encoding dimension
    `encoding_dim` was empirically tuned for the right balance of accuracy and computing
    cost.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.7 Keras shallow neural autoencoder
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: ❶ The input size must be the same as the dimension of the sent2vec vectors.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Encodes into the space of the specified latent dimension, encoding_dim
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Decodes from the space of the specified latent dimension back into the sent2vec
    space
  prefs: []
  type: TYPE_NORMAL
- en: 'We train the autoencoder for 50 epochs, which takes only a few seconds, by
    setting both the input and output to the IMDB sent2vec vectors from the previous
    chapter, as shown via the following compilation and training code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We use mean squared error (mse) as a loss function for this regression problem
    and mean absolute error (mae) as an additional metric. A minimum validation mae
    value of approximately 0.06 was achieved.
  prefs: []
  type: TYPE_NORMAL
- en: 'We next project the book reviews into the IMDB feature space using the autoencoder
    trained to reconstruct the features just described. This just means we preprocess
    the book review feature vectors using the autoencoder. We then repeat the accuracy
    evaluation experiment of the IMDB classifier on these preprocessed vectors as
    input as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: An accuracy of approximately 75% is now observed, demonstrating an improvement
    of about 1% and an instance of zero-shot domain adaptation. Repeating this several
    times, we find the improvement to consistently stay around 0.5-1%, giving us confidence
    that the autoencoding domain adaptation did indeed lead to some positive transfer.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pretrained word embeddings, as well as embeddings at higher levels of text—such
    as sentences—are ubiquitous in NLP and can be used to convert text into numbers/vectors.
    This simplifies further processing to extract meaning from them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Such an extraction represents a form of semisupervised shallow transfer learning,
    which has been widely used in practice with immense success.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Techniques like hard-parameter sharing and soft-parameter sharing allow us to
    create multitask learning systems, which have potential benefits, including simplified
    engineering design, improved generalization, and reduced overfitting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sometimes it may be possible to achieve zero-shot transfer learning with no
    labeled data in the target domain, which is an ideal scenario because labeled
    data can be expensive to collect.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is sometimes possible to modify or adapt data in a target domain to be more
    similar to the data in the source domain, for instance, via projection methods
    such as autoencoders, and this can improve performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1. S. J. Pan and Q. Yang, “A Survey on Transfer Learning,” IEEE Transactions
    on Knowledge and Data Engineering (2009).
  prefs: []
  type: TYPE_NORMAL
- en: 2. S. Ruder, “Neural Transfer Learning for Natural Language Processing,” National
    University of Ireland, Galway, Ireland (2019).
  prefs: []
  type: TYPE_NORMAL
- en: 3. D. Wang and T. F. Zheng, “Transfer Learning for Speech and Language Processing,”
    Proceedings of 2015 Asia-Pacific Signal and Information Processing Association
    Annual Summit and Conference (APSIPA).
  prefs: []
  type: TYPE_NORMAL
- en: '4. Jing Wang, Haibo Hea Danil, and V.Prokhorov, “A Folded Neural Network Autoencoder
    for Dimensionality Reduction,” Procedia Computer Science 13 (2012): 120-27.'
  prefs: []
  type: TYPE_NORMAL
- en: 5. Z. Yin and Y. Shen, “On the Dimensionality of Word Embedding,” 32nd Conference
    on Neural Information Processing Systems (NeurIPS 2018), Montreal, Canada.
  prefs: []
  type: TYPE_NORMAL
- en: 6. [https://fasttext.cc/docs/en/english-vectors.html](https://fasttext.cc/docs/en/english-vectors.html)
  prefs: []
  type: TYPE_NORMAL
- en: 7. J. Alabi et al., “Massive vs. Curated Word Embeddings for Low-Resourced Languages.
    The Case of Yorùbá and Twi,” The International Conference on Language Resources
    and Evaluation (LREC 2020), Marseille, France.
  prefs: []
  type: TYPE_NORMAL
- en: 8. [https://www.kaggle.com/yangjia1991/jigsaw](https://www.kaggle.com/yangjia1991/jigsaw)
  prefs: []
  type: TYPE_NORMAL
- en: 9. [https://github.com/epfml/sent2vec](https://github.com/epfml/sent2vec)
  prefs: []
  type: TYPE_NORMAL
- en: 10. [https://www.kaggle.com/maxjeblick/sent2vec](https://www.kaggle.com/maxjeblick/sent2vec)
  prefs: []
  type: TYPE_NORMAL
- en: 11. S. Ruder, “Neural Transfer Learning for Natural Language Processing,” National
    University of Ireland, Galway, Ireland (2019).
  prefs: []
  type: TYPE_NORMAL
- en: 12. Hal Daumé III, “Frustratingly Easy Domain Adaptation,” Proceedings of the
    45th Annual Meeting of the Association of Computational Linguistics (2007), Prague,
    Czech Republic.
  prefs: []
  type: TYPE_NORMAL
- en: 13. S. Ruder and B. Plank, “Learning to Select Data for Transfer Learning with
    Bayesian Optimization,” Proceedings of the 2017 Conference on Empirical Methods
    in Natural Language Processing (2017), Copenhagen, Denmark.
  prefs: []
  type: TYPE_NORMAL
- en: '14. Jing Wang, Haibo Hea Danil, and V.Prokhorov, “A Folded Neural Network Autoencoder
    for Dimensionality Reduction,” Procedia Computer Science 13 (2012): 120-127.'
  prefs: []
  type: TYPE_NORMAL
- en: 15. [https://www.cs.jhu.edu/~mdredze/datasets/sentiment/](https://www.cs.jhu.edu/~mdredze/datasets/sentiment/)
  prefs: []
  type: TYPE_NORMAL
