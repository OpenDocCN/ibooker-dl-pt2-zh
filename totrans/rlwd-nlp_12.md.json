["```py\nimport torch\nfrom torch import nn, optim\nfrom transformers import AutoTokenizer, AutoModel, AdamW, get_cosine_schedule_with_warmup\n```", "```py\nBERT_MODEL = 'bert-base-cased'\n```", "```py\ntokenizer = AutoTokenizer.from_pretrained(BERT_MODEL)\n```", "```py\n>>> token_ids = tokenizer.encode('The best movie ever!')\n\n[101, 1109, 1436, 2523, 1518, 106, 102]\n\n>>> tokenizer.decode(token_ids)\n\n'[CLS] The best movie ever! [SEP]'\n```", "```py\n>>> result = tokenizer(\n>>>    ['The best movie ever!', 'Aweful movie'],\n>>>    max_length=10,\n>>>    pad_to_max_length=True,\n>>>    truncation=True,\n>>>    return_tensors='pt')\n```", "```py\n>>> result['input_ids']\n\ntensor([[ 101, 1109, 1436, 2523, 1518,  106,  102,    0,    0,    0],\n        [ 101,  138, 7921, 2365, 2523,  102,    0,    0,    0,    0]])\n\n>>> result['token_type_ids']\n\ntensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n\n>>> result['attention_mask']\n\ntensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])\n```", "```py\ntrain_data = read_dataset('train.txt', batch_size=32, tokenizer=tokenizer, max_length=128)\ndev_data = read_dataset('dev.txt', batch_size=32, tokenizer=tokenizer, max_length=128)\n```", "```py\nclass BertClassifier(nn.Module):\n    def __init__(self, model_name, num_labels):\n        super(BertClassifier, self).__init__()\n        self.bert_model = AutoModel.from_pretrained(model_name)                ❶\n\n        self.linear = nn.Linear(self.bert_model.config.hidden_size, num_labels)❷\n\n        self.loss_function = nn.CrossEntropyLoss()\n\n    def forward(self, input_ids, attention_mask, token_type_ids, label=None):\n        bert_out = self.bert_model(                                            ❸\n          input_ids=input_ids,\n          attention_mask=attention_mask,\n          token_type_ids=token_type_ids)\n\n        logits = self.linear(bert_out.pooler_output)                           ❹\n\n        loss = None\n        if label is not None:\n            loss = self.loss_function(logits, label)                           ❺\n\n        return loss, logits\n```", "```py\n>>> model(**train_data[0])\n\n(tensor(1.8050, grad_fn=<NllLossBackward>),\n tensor([[-0.5088,  0.0806, -0.2924, -0.6536, -0.2627],\n         [-0.3816,  0.3512, -0.1223, -0.5136, -0.4421],\n         ...\n         [-0.4220,  0.3026, -0.1723, -0.4913, -0.4106],\n         [-0.3354,  0.3871, -0.0787, -0.4673, -0.4169]],\n        grad_fn=<AddmmBackward>))\n```", "```py\nMAX_EPOCHS = 100\nmodel = Model()\n\nfor epoch in range(MAX_EPOCHS):\n    for batch in train_set:\n        loss, prediction = model.forward(**batch)\n        new_model = optimizer(model, loss)\n        model = new_model\n```", "```py\noptimizer = AdamW(model.parameters(), lr=1e-5)\nscheduler = get_cosine_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=100,\n    num_training_steps=1000)\n```", "```py\nfor epoch in range(epochs):\n    print(f'epoch = {epoch}')\n\n    model.train()                                    ❶\n\n    losses = []\n    total_instances = 0\n    correct_instances = 0\n    for batch in train_data:\n        batch_size = batch['input_ids'].size(0)\n        move_to(batch, device)                       ❷\n\n        optimizer.zero_grad()                        ❸\n\n        loss, logits = model(**batch)                ❹\n        loss.backward()                              ❺\n        optimizer.step()\n        scheduler.step()\n\n        losses.append(loss)\n\n        total_instances += batch_size\n        correct_instances += torch.sum(torch.argmax(logits, dim=-1)    == batch['label']).item()                    ❻\n\n    avr_loss = sum(losses) / len(losses)\n    accuracy = correct_instances / total_instances\n    print(f'train loss = {avr_loss}, accuracy = {accuracy}')\n```", "```py\nepoch = 0\ntrain loss = 1.5403757095336914, accuracy = 0.31624531835205993\ndev loss = 1.7507736682891846, accuracy = 0.2652134423251589\nepoch = 1\n...\nepoch = 8\ntrain loss = 0.4508829712867737, accuracy = 0.8470271535580525\ndev loss = 1.687158465385437, accuracy = 0.48319709355131696\nepoch = 9\n...\n```", "```py\npip install allennlp==2.5.0\npip install allennlp-models==2.5.0\n```", "```py\nfrom allennlp.data.tokenizers import PretrainedTransformerTokenizer\nfrom allennlp_models.pair_classification.dataset_readers import SnliReader\n\nBERT_MODEL = 'bert-base-cased'\ntokenizer = PretrainedTransformerTokenizer(model_name=BERT_MODEL, add_special_tokens=False)\n\nreader = SnliReader(tokenizer=tokenizer)\ndataset_url = 'https://realworldnlpbook.s3.amazonaws.com/data/snli/snli_1.0_dev.jsonl'\nfor instance in reader.read():\n    print(instance)\n```", "```py\nInstance with fields:\n         tokens: TextField of length 29 with text:\n                [[CLS], Two, women, are, em, ##bracing, while, holding, to, go, packages,\n ., [SEP], The, sisters, are, hugging, goodbye, while, holding, to, go, \n packages, after, just, eating, lunch, ., [SEP]]\n                and TokenIndexers : {'tokens': 'SingleIdTokenIndexer'}\n         label: LabelField with label: neutral in namespace: 'labels'.'\n\nInstance with fields:\n         tokens: TextField of length 20 with text:\n                [[CLS], Two, women, are, em, ##bracing, while, holding, to, go, packages,\n ., [SEP], Two, woman, are, holding, packages, ., [SEP]]\n                and TokenIndexers : {'tokens': 'SingleIdTokenIndexer'}\n         label: LabelField with label: entailment in namespace: 'labels'.'\n\nInstance with fields:\n         tokens: TextField of length 23 with text:\n                [[CLS], Two, women, are, em, ##bracing, while, holding, to, go, packages,\n ., [SEP], The, men, are, fighting, outside, a, del, ##i, ., [SEP]]\n                and TokenIndexers : {'tokens': 'SingleIdTokenIndexer'}\n         label: LabelField with label: contradiction in namespace: 'labels'.'\n...\n```", "```py\nlocal bert_model = \"bert-base-cased\";\n```", "```py\n\"dataset_reader\": {\n    \"type\": \"snli\",\n    \"tokenizer\": {\n        \"type\": \"pretrained_transformer\",\n        \"model_name\": bert_model,\n        \"add_special_tokens\": false\n    },\n    \"token_indexers\": {\n        \"bert\": {\n            \"type\": \"pretrained_transformer\",\n            \"model_name\": bert_model,\n        }\n    }\n},\n```", "```py\n\"train_data_path\": \"https://realworldnlpbook.s3.amazonaws.com/data/snli/snli_1.0_train.jsonl\",\n\"validation_data_path\": \"https://realworldnlpbook.s3.amazonaws.com/data/snli/snli_1.0_dev.jsonl\",\n```", "```py\n\"model\": {\n    \"type\": \"basic_classifier\",\n\n    \"text_field_embedder\": {\n        \"token_embedders\": {\n            \"bert\": {\n                \"type\": \"pretrained_transformer\",\n                \"model_name\": bert_model\n            }\n        }\n    },\n    \"seq2vec_encoder\": {\n        \"type\": \"bert_pooler\",\n        \"pretrained_model\": bert_model\n    }\n},\n```", "```py\nlocal bert_model = \"bert-base-cased\";\n\n{\n    \"dataset_reader\": {\n        \"type\": \"snli\",\n        \"tokenizer\": {\n            \"type\": \"pretrained_transformer\",\n            \"model_name\": bert_model,\n            \"add_special_tokens\": false\n        },\n        \"token_indexers\": {\n            \"bert\": {\n                \"type\": \"pretrained_transformer\",\n                \"model_name\": bert_model,\n            }\n        }\n    },\n    \"train_data_path\": \"https://realworldnlpbook.s3.amazonaws.com/data/snli/snli_1.0_train.jsonl\",\n    \"validation_data_path\": \"https://realworldnlpbook.s3.amazonaws.com/data/snli/snli_1.0_dev.jsonl\",\n\n    \"model\": {\n        \"type\": \"basic_classifier\",\n\n        \"text_field_embedder\": {\n            \"token_embedders\": {\n                \"bert\": {\n                    \"type\": \"pretrained_transformer\",\n                    \"model_name\": bert_model\n                }\n            }\n        },\n        \"seq2vec_encoder\": {\n            \"type\": \"bert_pooler\",\n            \"pretrained_model\": bert_model,\n        }\n    },\n    \"data_loader\": {\n        \"batch_sampler\": {\n            \"type\": \"bucket\",\n            \"sorting_keys\": [\"tokens\"],\n            \"padding_noise\": 0.1,\n            \"batch_size\" : 32\n        }\n    },\n    \"trainer\": {\n        \"optimizer\": {\n            \"type\": \"huggingface_adamw\",\n            \"lr\": 5.0e-6\n        },\n        \"validation_metric\": \"+accuracy\",\n        \"num_epochs\": 30,\n        \"patience\": 10,\n        \"cuda_device\": 0\n    }\n}\n```", "```py\nallennlp train examples/nli/snli_transformers.jsonnet --serialization-dir models/snli\n```", "```py\n...\nallennlp.training.trainer - Epoch 4/29\nallennlp.training.trainer - Worker 0 memory usage MB: 6644.208\nallennlp.training.trainer - GPU 0 memory usage MB: 8708\nallennlp.training.trainer - Training\nallennlp.training.trainer - Validating\nallennlp.training.tensorboard_writer -                        Training |  Validation\nallennlp.training.tensorboard_writer - accuracy           |     0.933  |     0.908\nallennlp.training.tensorboard_writer - gpu_0_memory_MB    |  8708.000  |       N/A\nallennlp.training.tensorboard_writer - loss               |     0.190  |     0.293\nallennlp.training.tensorboard_writer - reg_loss           |     0.000  |     0.000\nallennlp.training.tensorboard_writer - worker_0_memory_MB |  6644.208  |       N/A\nallennlp.training.checkpointer - Best validation performance so far. Copying weights to 'models/snli/best.th'.\nallennlp.training.trainer - Epoch duration: 0:21:39.687226\nallennlp.training.trainer - Estimated training time remaining: 9:04:56\n...\n```"]