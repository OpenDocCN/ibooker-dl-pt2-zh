["```py\ntrain_data_loader = DataLoader(train_dataset,\n                               batch_sampler=BucketBatchSampler(\n                                   train_dataset,\n                                   batch_size=32,\n                                   sorting_keys=[\"tokens\"]))\n```", "```py\n    def forward(self,\n                words: Dict[str, torch.Tensor],\n                pos_tags: torch.Tensor = None,\n                **args) -> Dict[str, torch.Tensor]:\n        mask = get_text_field_mask(words)\n\n        embeddings = self.embedder(words)\n        encoder_out = self.encoder(embeddings, mask)\n        tag_logits = self.linear(encoder_out)\n\n        output = {\"tag_logits\": tag_logits}\n        if pos_tags is not None:\n            self.accuracy(tag_logits, pos_tags, mask)\n            output[\"loss\"] = sequence_cross_entropy_with_logits(\n                tag_logits, pos_tags, mask)\n\n        return output\n```", "```py\ntensor([[ True,  True,  True,  True,  True,  True,  True,  True, False],\n        [ True,  True,  True,  True,  True,  True,  True,  True,  True],\n        [ True,  True,  True,  True,  True,  True,  True,  True, False],\n        [ True,  True,  True,  True,  True,  True,  True,  True,  True],\n        [ True,  True,  True,  True,  True,  True,  True,  True, False],\n        [ True,  True,  True,  True,  True,  True,  True,  True, False],\n        [ True,  True,  True,  True,  True,  True,  True,  True, False],\n        [ True,  True,  True,  True,  True,  True,  True,  True,  True],\n        [ True,  True,  True,  True,  True,  True,  True,  True, False],\n        [ True,  True,  True,  True,  True,  True,  True,  True,  True],\n        [ True,  True,  True,  True,  True,  True,  True,  True,  True],\n```", "```py\nYou can say that you don't know it, and why decided of yourself.\nPike of your value is to talk of hubies.\nThe meeting despoit from a police?\nThat's a problem, but us?\nThe sky as going to send nire into better.\nWe'll be look of the best ever studented.\nThere's you seen anything every's redusention day.\nHow a fail is to go there.\nIt sad not distaples with money.\nWhat you see him go as famous to eat!\n```", "```py\n    \"trainer\": {\n        \"optimizer\": {\n            \"type\": \"huggingface_adamw\",\n            \"lr\": 1.0e-5\n        },\n        \"num_epochs\": 20,\n        \"patience\": 10,\n        \"cuda_device\": 0\n    }\n```", "```py\n>>> import torch\n>>> import torch.nn as nn\n\n>>> input = torch.randn(3)\n>>> input\ntensor([-0.5565,  1.5350, -1.3066])\n\n>>> target = torch.empty(3).random_(2)\n>>> target\ntensor([0., 0., 1.])\n\n>>> loss = nn.BCEWithLogitsLoss(reduction='none')\n>>> loss(input, target)\ntensor([0.4531, 1.7302, 1.5462])\n\n>>> loss = nn.BCEWithLogitsLoss(reduction='none', pos_weight=torch.tensor(2.))\n>>> loss(input, target)\ntensor([0.4531, 1.7302, 3.0923])\n```", "```py\npip install allennlp\npip install allennlp_optuna\n```", "```py\necho 'allennlp_optuna' >> .allennlp_plugins\n```", "```py\nlocal embedding_dim = std.parseJson(std.extVar('embedding_dim'));\nlocal hidden_dim = std.parseJson(std.extVar('hidden_dim'));\nlocal lr = std.parseJson(std.extVar('lr'));\n```", "```py\n[\n    {\n        \"type\": \"int\",\n        \"attributes\": {\n            \"name\": \"embedding_dim\",\n            \"low\": 64,\n            \"high\": 256\n        }\n    },\n    {\n        \"type\": \"int\",\n        \"attributes\": {\n            \"name\": \"hidden_dim\",\n            \"low\": 64,\n            \"high\": 256\n        }\n    },\n    {\n        \"type\": \"float\",\n        \"attributes\": {\n            \"name\": \"lr\",\n            \"low\": 1e-4,\n            \"high\": 1e-1,\n            \"log\": true\n        }\n    }\n]\n```", "```py\nallennlp tune \\\n    examples/tuning/sst_classifier.jsonnet \\\n    examples/tuning/hparams.json \\\n    --include-package examples \\\n    --serialization-dir result \\\n    --study-name sst-lstm \\\n    --n-trials 20 \\\n    --metrics best_validation_accuracy \\\n    --direction maximize\n```", "```py\nTrial 19 finished with value: 0.3469573115349682 and parameters: {'embedding_dim': 120, 'hidden_dim': 82, 'lr': 0.00011044322486693224}. Best is trial 14 with value: 0.3869209809264305.\n```", "```py\noptuna dashboard --study-name sst-lstm --storage sqlite:///allennlp_optuna.db\n```"]