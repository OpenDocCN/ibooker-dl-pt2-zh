- en: Chapter 6\. Working with data
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第6章。处理数据
- en: '*This chapter covers*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖内容*'
- en: How to use the `tf.data` API to train models using large datasets
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用`tf.data` API来使用大型数据集训练模型
- en: Exploring your data to find and fix potential issues
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索您的数据以查找和解决潜在问题
- en: How to use data augmentation to create new “pseudo-examples” to improve model
    quality
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用数据增强来创建新的“伪样本”以提高模型质量
- en: The wide availability of large volumes of data is a major factor leading to
    today’s machine-learning revolution. Without easy access to large amounts of high-quality
    data, the dramatic rise in machine learning would not have happened. Datasets
    are now available all over the internet—freely shared on sites like Kaggle and
    OpenML, among others—as are benchmarks for state-of-the-art performance. Entire
    branches of machine learning have been propelled forward by the availability of
    “challenge” datasets, setting a bar and a common benchmark for the community.^([[1](#ch06fn1)])
    If machine learning is our generation’s Space Race, then data is clearly our rocket
    fuel;^([[2](#ch06fn2)]) it’s potent, it’s valuable, it’s volatile, and it’s absolutely
    critical to a working machine-learning system. Not to mention that polluted data,
    like tainted fuel, can quickly lead to systemic failure. This chapter is about
    data. We will cover best practices for organizing data, how to detect and clean
    out issues, and how to use it efficiently.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 大量高质量数据的广泛可用是导致当今机器学习革命的主要因素。如果没有轻松获取大量高质量数据，机器学习的急剧上升就不会发生。现在数据集可以在互联网上随处可得——在Kaggle和OpenML等网站上免费共享——同样可以找到最先进性能的基准。整个机器学习领域都是通过可用的“挑战”数据集前进的，这些数据集设定了一个标准和一个共同的基准用于社区。如果说机器学习是我们这一代的太空竞赛，那么数据显然就是我们的火箭燃料；它是强大的，它是有价值的，它是不稳定的，它对于一个正常工作的机器学习系统绝对至关重要。更不用说污染的数据，就像污染的燃料一样，很快就会导致系统性失败。这一章是关于数据的。我们将介绍组织数据的最佳实践，如何检测和清除问题，以及如何高效使用数据。
- en: ¹
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See how ImageNet propelled the field of object recognition or what the Netflix
    challenge did for collaborative filtering.
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 看看ImageNet如何推动目标识别领域，或者Netflix挑战对协同过滤做出了什么贡献。
- en: ²
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ²
- en: ''
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Credit for the analogy to Edd Dumbill, “Big Data Is Rocket Fuel,” *Big Data*,
    vol. 1, no. 2, pp. 71–72.
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 感谢Edd Dumbill将这个类比归功于“大数据是火箭燃料”，*大数据*，卷1，号2，第71-72页。
- en: “But haven’t we been working with data all along?” you might protest. It’s true—in
    previous chapters we worked with all sorts of data sources. We’ve trained image
    models using both synthetic and webcam-image datasets. We’ve used transfer learning
    to build a spoken-word recognizer from a dataset of audio samples, and we accessed
    tabular datasets to predict prices. So what’s left to discuss? Aren’t we already
    proficient in handling data?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: “但我们不是一直在处理数据吗？”你可能会抗议。没错，在之前的章节中，我们处理过各种数据源。我们使用合成和网络摄像头图像数据集来训练图像模型。我们使用迁移学习从音频样本数据集构建了一个口语识别器，并访问了表格数据集以预测价格。那么还有什么需要讨论的呢？我们不是已经能够熟练处理数据了吗？
- en: Recall in our previous examples the patterns of our data usage. We’ve typically
    needed to first download our data from a remote source. Then we (usually) applied
    some transformation to get our data into the correct format—for instance, by converting
    strings into one-hot vocabulary vectors or by normalizing the means and variances
    of tabular sources. We have then always needed to batch our data and convert it
    into a standard block of numbers represented as a tensor before connecting it
    to our model. All this before we even ran our first training step.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的例子中回顾我们对数据使用的模式。我们通常需要首先从远程源下载数据。然后我们（通常）对数据应用一些转换，将数据转换为正确的格式，例如将字符串转换为独热词汇向量，或者规范化表格源的均值和方差。然后我们总是需要对数据进行分批处理，并将其转换为表示为张量的标准数字块，然后再连接到我们的模型。这一切都是在我们运行第一个训练步骤之前完成的。
- en: 'This download-transform-batch pattern is very common, and TensorFlow.js comes
    packaged with tooling to make these types of manipulations easier, more modular,
    and less error prone. This chapter will introduce the tools in the `tf.data` namespace:
    most importantly, `tf.data.Dataset`, which can be used to lazily stream data.
    The lazy-streaming approach allows for downloading, transforming, and accessing
    data on an as-needed basis rather than downloading the data source in its entirety
    and holding it in memory as it is accessed. Lazy streaming makes it much easier
    to work with data sources that are too large to fit in a single browser tab or
    even too large within the RAM of a single machine.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 下载 - 转换 - 批处理的模式非常常见，TensorFlow.js 提供了工具，使这些类型的操作更加简单、模块化和不容易出错。本章将介绍 `tf.data`
    命名空间中的工具，最重要的是 `tf.data.Dataset`，它可以用于惰性地流式传输数据。惰性流式传输的方法允许按需下载、转换和访问数据，而不是将数据源完全下载并在访问时将其保存在内存中。惰性流式传输使得更容易处理数据源，这些数据源太大，无法放入单个浏览器标签页甚至单台机器的
    RAM 中。
- en: We will first introduce the `tf.data.Dataset` API and show how to configure
    it and connect it to a model. We will then introduce some theory and tooling to
    help you review and explore your data and resolve problems you might discover.
    The chapter wraps up by introducing data augmentation, a method for expanding
    a dataset to improve model quality by creating synthetic pseudo-examples.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先介绍 `tf.data.Dataset` API，并演示如何配置它并与模型连接起来。然后，我们将介绍一些理论和工具，帮助你审查和探索数据，并解决可能发现的问题。本章还介绍了数据增强，这是一种通过创建合成伪示例来扩展数据集，提高模型质量的方法。
- en: 6.1\. Using tf.data to manage data
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1\. 使用 tf.data 管理数据
- en: How would you train a spam filter if your email database were hundreds of gigabytes
    and required special credentials to access? How can you construct an image classifier
    if your database of training images is too large to fit on a single machine?
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的电子邮件数据库有数百 GB 的容量，并且需要特殊凭据才能访问，你该如何训练垃圾邮件过滤器？如果训练图像数据库的规模太大，无法放入单台机器上，你该如何构建图像分类器？
- en: Accessing and manipulating large volumes of data is a key skill for the machine-learning
    engineer, but so far, we have been dealing with applications in which the data
    could conceivably fit within the memory available to our application. Many applications
    require working with large, cumbersome, and possibly privacy-sensitive data sources
    that this technique is not suitable for. Large applications require technology
    for accessing data from a remote source, piece by piece, on demand.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 访问和操作大量数据是机器学习工程师的关键技能，但到目前为止，我们所处理的应用程序都可以在可用内存中容纳得下数据。许多应用程序需要处理大型、笨重且可能涉及隐私的数据源，此技术就不适用了。大型应用程序需要一种技术，能够按需逐块从远程数据源中访问数据。
- en: TensorFlow.js comes packaged with an integrated library designed just for this
    sort of data management. It is built to enable users to ingest, preprocess, and
    route data in a concise and readable way, inspired by the `tf.data` API in the
    Python version of TensorFlow. Assuming your code imports TensorFlow.js using an
    import statement like
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow.js 附带了一个集成库，专门用于这种类型的数据管理。它的构建目标是以简洁易读的方式，让用户能够快速地摄取、预处理和路由数据，灵感来自于
    TensorFlow Python 版本中的 `tf.data` API。假设你的代码使用类似于 import 语句来导入 TensorFlow.js，像这样：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: this functionality will be available under the `tf.data` namespace.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这个功能在 `tf.data` 命名空间下可用。
- en: 6.1.1\. The tf.data.Dataset object
  id: totrans-22
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.1\. tf.data.Dataset 对象
- en: Most interaction with `tfjs-data` comes through a single object type called
    `Dataset`. The `tf.data.Dataset` object provides a simple, configurable, and performant
    way to iterate over and process large (possibly unlimited) lists of data elements.^([[3](#ch06fn3)])
    In the coarsest abstraction, you can imagine a dataset as an iterable collection
    of arbitrary elements, not unlike the `Stream` in Node.js. Whenever the next element
    is requested from the dataset, the internal implementation will download it, access
    it, or execute a function to create it, as needed. This abstraction makes it easy
    for the model to train on more data than can conceivably be held in memory at
    once. It also makes it convenient to share and organize datasets as first-class
    objects when there is more than one dataset to keep track of. `Dataset` provides
    a memory benefit by streaming only the required bits of data, rather than accessing
    the whole thing monolithically. The `Dataset` API also provides performance optimizations
    over the naive implementation by prefetching values that are about to be needed.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 与`tfjs-data`的大多数交互都通过一种称为`Dataset`的单一对象类型进行。`tf.data.Dataset`对象提供了一种简单、可配置和高性能的方式来迭代和处理大型（可能是无限的）数据元素列表^([[3](#ch06fn3)])。在最粗略的抽象中，你可以将数据集想象成任意元素的可迭代集合，不太不同于Node.js中的`Stream`。每当需要从数据集中请求下一个元素时，内部实现将根据需要下载、访问或执行函数来创建它。这种抽象使得模型能够在内存中一次性存储的数据量比可以想象的要多。它还使得在有多个要跟踪的数据集时，将数据集作为一流对象进行共享和组织变得更加方便。`Dataset`通过仅流式传输所需的数据位而不是一次性访问整个数据来提供内存优势。`Dataset`
    API还通过预取即将需要的值来提供性能优化。
- en: ³
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³
- en: ''
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In this chapter, we will use the term *elements* frequently to refer to the
    items in the `Dataset`. In most cases, *element* is synonymous with *example*
    or *datapoint*—that is, in the training dataset, each element is an (*x, y*) pair.
    When reading from a CSV source, each element is a row of the file. `Dataset` is
    flexible enough to handle heterogeneous types of elements, but this is not recommended.
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在本章中，我们将经常使用术语*元素*来指代`Dataset`中的项。在大多数情况下，*元素*与*示例*或*数据点*是同义词——也就是说，在训练数据集中，每个元素都是一个(*x,
    y*)对。当从CSV源中读取数据时，每个元素都是文件的一行。`Dataset`足够灵活，可以处理异构类型的元素，但不建议这样做。
- en: 6.1.2\. Creating a tf.data.Dataset
  id: totrans-27
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.2\. 创建`tf.data.Dataset`
- en: As of TensorFlow.js version 1.2.7, there are three ways to connect up `tf.data
    .Dataset` to some data provider. We will go through each in some detail, but [table
    6.1](#ch06table01) contains a brief summary.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 截至TensorFlow.js版本1.2.7，有三种方法可以将`tf.data.Dataset`连接到某个数据提供程序。我们将对每种方法进行详细介绍，但[table
    6.1](#ch06table01)中包含了简要摘要。
- en: Table 6.1\. Creating a `tf.data.Dataset` object from a data source
  id: totrans-29
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 6.1\. 从数据源创建一个`tf.data.Dataset`对象
- en: '| How to get a new tf.data.Dataset | API | How to use it to build a dataset
    |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 如何获得新的tf.data.Dataset | API | 如何使用它构建数据集 |'
- en: '| --- | --- | --- |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| From a JavaScript array of elements; also works for typed arrays like Float32Array
    | tf.data.array(items) | const dataset = tf.data.array([1,2,3,4,5]); See [listing
    6.1](#ch06ex01) for more. |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 从JavaScript数组中获取元素；也适用于像Float32Array这样的类型化数组 | tf.data.array(items) | const
    dataset = tf.data.array([1,2,3,4,5]); 有关更多信息，请参见[listing 6.1](#ch06ex01)。 |'
- en: '| From a (possibly remote) CSV file, where each row is an element | tf.data.csv(
    source,'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '| 从（可能是远程）CSV文件中获取，其中每一行都是一个元素 | tf.data.csv( source,'
- en: csvConfig) | const dataset = tf.data.csv("https://path/to/my.csv"); See [listing
    6.2](#ch06ex02) for more. The only required parameter is the URL from which to
    read the data. Additionally, csvConfig accepts an object with keys to help guide
    the parsing of the CSV file. For instance,
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: csvConfig) | const dataset = tf.data.csv("https://path/to/my.csv"); 有关更多信息，请参见[listing
    6.2](#ch06ex02)。唯一必需的参数是从中读取数据的URL。此外，csvConfig接受一个带有键的对象来帮助指导CSV文件的解析。例如，
- en: columnNames—A string[] can be provided to set the names of the columns manually
    if they don’t exist in a header or need to be overridden.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: columnNames—可以提供一个字符串数组来手动设置列的名称，如果它们在标题中不存在或需要被覆盖。
- en: delimiter—A single character string can be used to override the default comma
    delimiter.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: delimiter—可以使用单字符字符串来覆盖默认的逗号分隔符。
- en: columnConfigs—A map of string columnName to columnConfig objects can be provided
    to guide the parsing and return type of the dataset. The columnConfig will inform
    the parser of the element’s type (string or int), or if the column is to be considered
    as the dataset label.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: columnConfigs—可以提供一个从字符串列名到columnConfig对象的映射，以指导数据集的解析和返回类型。columnConfig将通知解析器元素的类型（字符串或整数），或者如果列应被视为数据集标签。
- en: configuredColumnsOnly—Whether to return data for each column in the CSV or only
    those columns included in the columnConfigs object.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: configuredColumnsOnly—是否仅返回CSV中包含的列或仅返回列配置对象中包含的列的数据。
- en: More detail is available in the API docs at js.[tensorflow.org](http://tensorflow.org).
    |
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 更多详细信息请查阅[js.tensorflow.org](http://tensorflow.org)上的API文档。 |
- en: '| From a generic generator function that yields elements | tf.data.generator(
    generatorFunction) | function* countDownFrom10() { for (let i=10; i>0; i--) {'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '| 从生成元素的通用生成函数 | tf.data.generator(generatorFunction) | function* countDownFrom10()
    { for (let i=10; i>0; i--) {'
- en: yield(i);
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: yield(i);
- en: '}'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: const dataset =
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: const dataset =
- en: tf.data.generator(countDownFrom10); See [listing 6.3](#ch06ex03) for more. Note
    that the argument passed to tf.data.generator() when called with no arguments
    returns a Generator object. |
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: tf.data.generator(countDownFrom10); 详见[清单6.3](#ch06ex03)。请注意，在没有参数的情况下调用tf.data.generator()时传递给tf.data.generator()的参数将返回一个Generator对象。
    |
- en: Creating a tf.data.Dataset from an array
  id: totrans-46
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 从数组创建 tf.data.Dataset
- en: The simplest way to create a new `tf.data.Dataset` is to build one from a JavaScript
    array of elements. Given an array already in memory, you can create a dataset
    backed by the array using the `tf.data.array()` function. Of course, it won’t
    bring any training speed or memory-usage benefit over using the array directly,
    but accessing an array via a dataset offers other important benefits. For instance,
    using datasets makes it easier to set up preprocessing and makes our training
    and evaluation easier through the simple `model.fitDataset()` and `model.evaluateDataset()`
    APIs, as we will see in [section 6.2](#ch06lev1sec2). In contrast to `model.fit(x,
    y)`, `model.fitDataset(myDataset)` does not immediately move all of the data into
    GPU memory, meaning that it is possible to work with datasets larger than the
    GPU can hold. Realize that the memory limit of the V8 JavaScript engine (1.4 GB
    on 64-bit systems) is usually larger than TensorFlow.js can hold in WebGL memory
    at a time. Using the `tf.data` API is also good software engineering practice,
    as it makes it easy to swap in another type of data in a modular fashion without
    changing much code. Without the dataset abstraction, it is easy to let the details
    of the implementation of the dataset source leak into its usage in the training
    of the model, an entanglement that will need to be unwound as soon as a different
    implementation is used.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 创建新的`tf.data.Dataset`最简单的方法是从一个JavaScript数组中构建。假设已经存在一个内存中的数组，您可以使用`tf.data.array()`函数创建一个由该数组支持的数据集。当然，这不会比直接使用数组带来任何训练速度或内存使用上的好处，但通过数据集访问数组提供了其他重要的好处。例如，使用数据集更容易设置预处理，并通过简单的`model.fitDataset()`和`model.evaluateDataset()`API使我们的训练和评估更加简单，就像我们将在[第6.2节](#ch06lev1sec2)中看到的那样。与`model.fit(x,
    y)`相比，`model.fitDataset(myDataset)`不会立即将所有数据移入GPU内存，这意味着可以处理比GPU能够容纳的更大的数据集。请注意，V8
    JavaScript引擎的内存限制（64位系统上为1.4 GB）通常比TensorFlow.js一次可以在WebGL内存中容纳的内存要大。使用`tf.data`
    API也是良好的软件工程实践，因为它使得以模块化的方式轻松地切换到另一种类型的数据而无需改变太多代码。如果没有数据集抽象，很容易让数据集源的实现细节泄漏到模型训练中的使用中，这种纠缠将需要在使用不同实现时解开。
- en: To build a dataset from an existing array, use `tf.data.array(itemsAsArray)`,
    as shown in the following listing.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 要从现有数组构建数据集，请使用`tf.data.array(itemsAsArray)`，如下面的示例所示。
- en: Listing 6.1\. Building a `tf.data.Dataset` from an array
  id: totrans-49
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单6.1\. 从数组构建`tf.data.Dataset`
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '***1*** Creates the tfjs-data dataset backed by an array. Note that this does
    not clone the array or its elements.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 创建由数组支持的tfjs-data数据集。请注意，这不会克隆数组或其元素。'
- en: '***2*** Uses the forEachAsync() method to iterate on all values provided by
    the dataset. Note that forEachAsync() is an async function, and hence you should
    use await with it.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 使用`forEachAsync()`方法迭代数据集提供的所有值。请注意，`forEachAsync()`是一个异步函数，因此应该在其前面使用await。'
- en: We iterate over the elements of the dataset using the `forEachAsync()` function,
    which yields each element in turn. See more details about the `Dataset.forEachAsync`
    function in [section 6.1.3](#ch06lev2sec3).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`forEachAsync()`函数迭代数据集的元素，该函数依次生成每个元素。有关`Dataset.forEachAsync`函数的更多详细信息，请参见[第6.1.3节](#ch06lev2sec3)。
- en: Elements of datasets may contain JavaScript primitives^([[4](#ch06fn4)]) (such
    as numbers and strings) as well as tuples, arrays, and nested objects of such
    structures, in addition to tensors. In this tiny example, the three elements of
    the dataset all have the same structure. They are all objects with the same keys
    and the same type of values at those keys. `tf.data.Dataset` can in general support
    a mixture of types of elements, but the common use case is that dataset elements
    are meaningful semantic units of the same type. Typically, they should represent
    examples of the same kind of thing. Thus, except in very unusual use cases, each
    element should have the same type and structure.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的元素可能包含 JavaScript 基元^([[4](#ch06fn4)])（如数字和字符串），以及元组、数组和这些结构的嵌套对象，除了张量。在这个小例子中，数据集的三个元素都具有相同的结构。它们都是具有相同键和相同类型值的对象。`tf.data.Dataset`通常支持各种类型的元素，但常见的用例是数据集元素是具有相同类型的有意义的语义单位。通常，它们应该表示同一类型的示例。因此，除非在非常不寻常的用例中，每个元素都应具有相同的类型和结构。
- en: ⁴
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁴
- en: ''
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you are familiar with the Python TensorFlow implementation of `tf.data`,
    you may be surprised that `tf.data.Dataset` can contain JavaScript primitives
    in addition to tensors.
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果您熟悉 Python TensorFlow 中 `tf.data` 的实现，您可能会对 `tf.data.Dataset` 可以包含 JavaScript
    基元以及张量感到惊讶。
- en: Creating a tf.data.Dataset from a CSV file
  id: totrans-58
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 从 CSV 文件创建 tf.data.Dataset
- en: A very common type of dataset element is a key-value object representing one
    row of a table, such as one row of a CSV file. The next listing shows a very simple
    program that will connect to and list out the Boston-housing dataset, the one
    we first used in [chapter 2](kindle_split_013.html#ch02).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一种非常常见的数据集元素类型是表示表的一行的键值对象，例如 CSV 文件的一行。下一个列表显示了一个非常简单的程序，该程序将连接并列出波士顿房屋数据集，我们首先在[chapter
    2](kindle_split_013.html#ch02)中使用过的数据集。
- en: Listing 6.2\. Building a `tf.data.Dataset` from a CSV file
  id: totrans-60
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.2\. 从 CSV 文件构建 `tf.data.Dataset`
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '***1*** Creates the tfjs-data dataset backed by a remote CSV file'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 创建由远程 CSV 文件支持的 tfjs-data 数据集'
- en: '***2*** Uses the forEachAsync() method to iterate on all values provided by
    the dataset. Note that forEachAsync() is an async function.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 使用 forEachAsync() 方法在数据集提供的所有值上进行迭代。请注意，forEachAsync() 是一个异步函数。'
- en: 'Instead of `tf.data.array()`, here we use `tf.data.csv`() and point to a URL
    of a CSV file. This will create a dataset backed by the CSV file, and iterating
    over the dataset will iterate over the CSV rows. In Node.js, we can connect to
    a local CSV file by using a URL handle with the file:// prefix, like the following:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们使用 `tf.data.csv()` 而不是 `tf.data.array()`，并指向 CSV 文件的 URL。这将创建一个由 CSV 文件支持的数据集，并且在数据集上进行迭代将遍历
    CSV 行。在 Node.js 中，我们可以通过使用以 file:// 为前缀的 URL 句柄连接到本地 CSV 文件，如下所示：
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: When iterating, we see that each CSV row is transformed into a JavaScript object.
    The elements returned from the dataset are objects with one property for each
    column of the CSV, and the properties are named according to the column names
    in the CSV file. This is convenient for interacting with the elements in that
    it is no longer necessary to remember the order of the fields. [Section 6.3.1](#ch06lev2sec5)
    will go into more detail describing how to work with CSVs and will go through
    an example.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在迭代时，我们看到每个 CSV 行都被转换为 JavaScript 对象。从数据集返回的元素是具有 CSV 的每列的一个属性的对象，并且属性根据 CSV
    文件中的列名命名。这对于与元素交互非常方便，因为不再需要记住字段的顺序。[Section 6.3.1](#ch06lev2sec5)将详细描述如何处理 CSV
    并通过一个例子进行说明。
- en: Creating a tf.data.Dataset from a generator function
  id: totrans-67
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 从生成器函数创建 tf.data.Dataset
- en: The third and most flexible way to create a `tf.data.Dataset` is to build one
    from a generator function. This is done using the `tf.data.generator()` method.
    `tf.data.generator()` takes a JavaScript *generator function* (or `function*`)^([[5](#ch06fn5)])
    as its argument. If you are not familiar with generator functions, which are relatively
    new to JavaScript, you may wish to take a moment to read their documentation.
    The purpose of a generator function is to “yield” a sequence of values as they
    are needed, either forever or until the sequence is exhausted. The values that
    are yielded from the generator function flow through to become the values of the
    dataset. A very simple generator function might, for instance, yield random numbers
    or extract snapshots of data from a piece of attached hardware. A sophisticated
    generator may be integrated with a video game, yielding screen captures, scores,
    and control input-output. In the following listing, the very simple generator
    function yields samples of dice rolls.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 创建`tf.data.Dataset`的第三种最灵活的方式是使用生成器函数构建。这是使用`tf.data.generator()`方法完成的。`tf.data.generator()`接受一个JavaScript
    *生成器函数*（或`function*`）^([[5](#ch06fn5)])作为参数。如果您对生成器函数不熟悉，它们是相对较新的JavaScript功能，您可能希望花一点时间阅读它们的文档。生成器函数的目的是在需要时“产出”一系列值，可以是永远或直到序列用尽为止。从生成器函数产生的值将流经并成为数据集的值。一个非常简单的生成器函数可以产生随机数，或者从连接的硬件中提取数据的快照。一个复杂的生成器函数可以与视频游戏集成，产生屏幕截图、得分和控制输入输出。在下面的示例中，非常简单的生成器函数产生骰子掷得的样本。
- en: ⁵
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁵
- en: ''
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Learn more about ECMAscript generator functions at [http://mng.bz/Q0rj](http://mng.bz/Q0rj).
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 了解有关 ECMAscript 生成器函数的更多信息，请访问[http://mng.bz/Q0rj](http://mng.bz/Q0rj)。
- en: Listing 6.3\. Building a `tf.data.Dataset` for random dice rolls
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单6.3。构建用于随机掷骰子的`tf.data.Dataset`
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '***1*** numPlaysSoFar is closed over by rollTwoDice(), which allows us to calculate
    how many times the function is executed by the dataset.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** numPlaysSoFar被rollTwoDice()闭合，这使我们可以计算数据集执行该函数的次数。'
- en: '***2*** Defines a generator function (using function* syntax) that will yield
    the result of calling rollTwoDice() an unlimited number of times'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2***定义了一个生成器函数（使用function*语法），可以无限次调用rollTwoDice()并产生结果。'
- en: '***3*** The dataset is created here.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3***数据集在此处创建。'
- en: '***4*** Takes a sample of exactly one element of the dataset. The take() method
    will be described in [section 6.1.4](#ch06lev2sec4).'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4***获取数据集中仅一个元素的样本。 `take()`方法将在[第6.1.4节](#ch06lev2sec4)中描述。'
- en: A couple of interesting notes regarding the game-simulation dataset created
    in [listing 6.3](#ch06ex03). First, note that the dataset created here, `myGeneratorDataset`,
    is infinite. Since the generator function never returns, we could conceivably
    take samples from the dataset forever. If we were to execute `forEachAsync()`
    or `toArray()` (see [section 6.1.3](#ch06lev2sec3)) on this dataset, it would
    never end and would probably crash our server or browser, so watch out for that.
    In order to work with such objects, we need to create some other dataset that
    is a limited sample of the unlimited one using `take(n)`. More on this in a moment.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 关于在[清单6.3](#ch06ex03)中创建的游戏模拟数据集，有一些有趣的要点。首先，请注意，这里创建的数据集`myGeneratorDataset`是无限的。由于生成器函数永远不会返回，我们可以从数据集中无限次取样。如果我们在此数据集上执行`forEachAsync()`或`toArray()`（参见[第6.1.3节](#ch06lev2sec3)），它将永远不会结束，并且可能会使我们的服务器或浏览器崩溃，所以要小心。为了使用这样的对象，我们需要创建一些其他数据集，它是无限数据集的有限样本，使用`take(n)`。稍后会详细介绍。
- en: Second, note that the dataset closes over a local variable. This is helpful
    for logging and debugging to determine how many times the generator function has
    been executed.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，请注意，数据集会关闭局部变量。这对于记录和调试，以确定生成器函数执行了多少次非常有帮助。
- en: Third, note that the data does not exist until it is requested. In this case,
    we only ever access exactly one sample of the dataset, and this would be reflected
    in the value of `numPlaysSoFar`.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请注意数据直到被请求才存在。在这种情况下，我们只能访问数据集的一项样本，并且这会反映在`numPlaysSoFar`的值中。
- en: Generator datasets are powerful and tremendously flexible and allow developers
    to connect models to all sorts of data-providing APIs, such as data from a database
    query, from data downloaded piecemeal over the network, or from a piece of connected
    hardware. More details about the `tf.data.generator()` API are provided in [info
    box 6.1](#ch06sb01).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器数据集非常强大且非常灵活，允许开发人员将模型连接到各种提供数据的 API，例如来自数据库查询、通过网络逐段下载的数据，或者来自一些连接的硬件。有关`tf.data.generator()`
    API的更多详细信息请参见[信息框6.1](#ch06sb01)。
- en: '|  |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**tf.data.generator() argument specification**'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**tf.data.generator()参数规范**'
- en: 'The `tf.data.generator()` API is flexible and powerful, allowing the user to
    hook the model up to many sorts of data providers. The argument passed to `tf.data.generator()`
    must meet the following specifications:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.data.generator()`API是灵活且强大的，允许用户将模型连接到许多类型的数据提供者。传递给`tf.data.generator()`的参数必须符合以下规范：'
- en: It must be callable with zero arguments.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它必须可调用且不带参数。
- en: 'When called with zero arguments, it must return an object that conforms to
    the iterator and iterable protocol. This means that the returned object must have
    a method `next()`. When `next()` is called with no arguments, it should return
    a JavaScript object `{value: ELEMENT, done: false}` in order to pass forward the
    value `ELEMENT`. When there are no more values to return, it should return `{value:
    undefined, done: true}`.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '在不带参数的情况下调用时，它必须返回符合迭代器和可迭代协议的对象。这意味着返回的对象必须具有一个`next()`方法。当调用`next()`而没有参数时，它应该返回JavaScript对象`{value:
    ELEMENT, done: false}`，以便将值`ELEMENT`传递给下一步。当没有更多值可返回时，它应该返回`{value: undefined,
    done: true}`。'
- en: JavaScript’s generator functions return `Generator` objects, which meet this
    spec and are thus the easiest way to use `tf.data.generator()`. The function may
    close over local variables, access local hardware, connect to network resources,
    and so on.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: JavaScript 的生成器函数返回`Generator`对象，符合此规范，因此是使用`tf.data.generator()`的最简单方法。该函数可能闭包于局部变量，访问本地硬件，连接到网络资源等。
- en: '[Table 6.1](#ch06table01) contains the following code illustrating how to use
    `tf.data.generator()`:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[表6.1](#ch06table01)包含以下代码，说明如何使用`tf.data.generator()`：'
- en: '[PRE5]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'If you wish to avoid using generator functions for some reason and would rather
    implement the iterable protocol directly, you can also write the previous code
    in the following, equivalent way:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果出于某种原因希望避免使用生成器函数，而更愿意直接实现可迭代协议，还可以以以下等效方式编写前面的代码：
- en: '[PRE6]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '|  |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 6.1.3\. Accessing the data in your dataset
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.3\. 访问数据集中的数据
- en: Once you have your data as a dataset, inevitably you are going to want to access
    the data in it. Data structures you can create but never read from are not really
    useful. There are two APIs to access the data from a dataset, but `tf.data` users
    should only need to use these infrequently. More typically, higher-level APIs
    will access the data within a dataset for you. For instance, when training a model,
    we use the `model.fitDataset()` API, described in [section 6.2](#ch06lev1sec2),
    which accesses the data in the dataset for us, and we, the users, never need to
    access the data directly. Nevertheless, when debugging, testing, and coming to
    understand how the `Dataset` object works, it’s important to know how to peek
    into the contents.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦将数据作为数据集，您必然会想要访问其中的数据。创建但从不读取数据结构实际上并不实用。有两种API可以访问数据集中的数据，但`tf.data`用户应该只需要偶尔使用这些API。更典型的情况是，高级API将为您访问数据集中的数据。例如，在训练模型时，我们使用`model.fitDataset()`API，如[第6.2节](#ch06lev1sec2)所述，它会为我们访问数据集中的数据，而我们，用户，从不需要直接访问数据。然而，在调试、测试和理解`Dataset`对象工作原理时，了解如何查看内容很重要。
- en: The first way to access data from a dataset is to stream it all out into an
    array using `Dataset.toArray()`. This function does exactly what it sounds like.
    It iterates through the entire dataset, pushing all the elements into an array
    and returning that array to the user. The user should use caution when executing
    this function to not inadvertently produce an array that is too large for the
    JavaScript runtime. This mistake is easy to make if, for instance, the dataset
    is connected to a large remote data source or is an unlimited dataset reading
    from a sensor.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据集中访问数据的第一种方法是使用`Dataset.toArray()`将其全部流出到数组中。这个函数的作用正是它的名字听起来的样子。它遍历整个数据集，将所有元素推入数组中，并将该数组返回给用户。用户在执行此函数时应小心，以免无意中生成一个对JavaScript运行时来说太大的数组。如果，例如，数据集连接到一个大型远程数据源或是从传感器读取的无限数据集，则很容易犯这个错误。
- en: The second way to access data from a dataset is to execute a function on each
    example of the dataset using `dataset.forEachAsync(f)`. The argument provided
    to `forEachAsync()` will apply to each element in turn in a way similar to the
    `forEach()` construct in JavaScript arrays and sets—that is, the native `Array.forEach()`
    and `Set.forEach()`.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据集中访问数据的第二种方法是使用`dataset.forEachAsync(f)`在数据集的每个示例上执行函数。提供给`forEachAsync()`的参数将逐个应用于每个元素，类似于JavaScript数组和集合中的`forEach()`构造——即本地的`Array.forEach()`和`Set.forEach()`。
- en: It is important to note that `Dataset.forEachAsync()` and `Dataset.toArray()`
    are both async functions. This is in contrast to `Array.forEach()`, which is synchronous,
    so it might be easy to make a mistake here. `Dataset.toArray()` returns a promise
    and will in general require `await` or `.then()` if synchronous behavior is required.
    Take care that if `await` is forgotten, the promise might not resolve in the order
    you expect, and bugs will arise. A typical bug is for the dataset to appear empty
    because the contents are iterated over before the promise resolves.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，`Dataset.forEachAsync()` 和 `Dataset.toArray()` 都是异步函数。这与 `Array.forEach()`
    相反，后者是同步的，因此在这里可能很容易犯错。`Dataset.toArray()` 返回一个 Promise，并且通常需要使用 `await` 或 `.then()`
    来获取同步行为。要小心，如果忘记使用 `await`，则 Promise 可能不会按照您期望的顺序解析，从而导致错误。一个典型的错误是数据集看起来为空，因为在
    Promise 解析之前已经迭代了其内容。
- en: The reason why `Dataset.forEachAsync()` is asynchronous while `Array.forEach()`
    is not is that the data being accessed by the dataset might, in general, need
    to be created, calculated, or fetched from a remote source. Asynchronicity here
    allows us to make efficient use of the available computation while we wait. These
    methods are summarized in [table 6.2](#ch06table02).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dataset.forEachAsync()` 是异步的而 `Array.forEach()` 不是的原因是，数据集正在访问的数据通常需要创建、计算或从远程源获取。异步性使我们能够在等待期间有效地利用可用的计算资源。这些方法在[table
    6.2](#ch06table02)中进行了总结。'
- en: Table 6.2\. Methods that iterate over a dataset
  id: totrans-99
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 6.2\. 迭代数据集的方法
- en: '| Instance method of the tf.data.Dataset object | What it does | Example |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| `tf.data.Dataset` 对象的实例方法 | 功能 | 示例 |'
- en: '| --- | --- | --- |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| .toArray() | Asynchronously iterates over the entire dataset and pushes each
    element into an array, which is returned | const a = tf.data.array([1, 2, 3, 4,
    5, 6]); const arr = await a.toArray();'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '| `.toArray()` | 异步地迭代整个数据集，并将每个元素推送到一个数组中，然后返回该数组 | const a = tf.data.array([1,
    2, 3, 4, 5, 6]); const arr = await a.toArray();'
- en: console.log(arr);
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: console.log(arr);
- en: // 1,2,3,4,5,6 |
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: // 1,2,3,4,5,6 |
- en: '| .forEachAsync(f) | Asynchronously iterates over all the elements of the dataset
    and executes f on each | const a = tf.data.array([1, 2, 3]); await a.forEachAsync(e
    => console.log("hi " + e));'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '| `.forEachAsync(f)` | 异步地迭代数据集的所有元素，并对每个元素执行 f | const a = tf.data.array([1,
    2, 3]); await a.forEachAsync(e => console.log("hi " + e));'
- en: // hi 1
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: // hi 1
- en: // hi 2
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: // hi 2
- en: // hi 3 |
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: // hi 3 |
- en: 6.1.4\. Manipulating tfjs-data datasets
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.4\. 操作 tfjs-data 数据集
- en: It certainly is very nice when we can use data directly as it has been provided,
    without any cleanup or processing. But in the experience of the authors, this
    *almost never* happens outside of examples constructed for educational or benchmarking
    purposes. In the more common case, the data must be transformed in some way before
    it can be analyzed or used in a machine-learning task. For instance, often the
    source contains extra elements that must be filtered; or data at certain keys
    needs to be parsed, deserialized, or renamed; or the data was stored in sorted
    order and thus needs to be randomly shuffled before using it to train or evaluate
    a model. Perhaps the dataset must be split into nonoverlapping sets for training
    and testing. Preprocessing is nearly inevitable. If you come across a dataset
    that is clean and ready-to-use out of the box, chances are that someone already
    did the cleanup and preprocessing for you!
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们可以直接使用数据而不需要任何清理或处理时，这当然是非常好的。但在作者的经验中，除了用于教育或基准测试目的构建的示例外，这几乎*从未*发生。在更常见的情况下，数据必须在某种程度上进行转换，然后才能进行分析或用于机器学习任务。例如，源代码通常包含必须进行过滤的额外元素；或者需要解析、反序列化或重命名某些键的数据；或者数据已按排序顺序存储，因此在使用它来训练或评估模型之前，需要对其进行随机洗牌。也许数据集必须分割成用于训练和测试的非重叠集。预处理几乎是不可避免的。如果你遇到了一个干净且可直接使用的数据集，很有可能是有人已经为你清理和预处理了！
- en: '`tf.data.Dataset` provides a chainable API of methods to perform these sorts
    of operations, described in [table 6.3](#ch06table03). Each of these methods returns
    a new `Dataset` object, but don’t be misled into thinking that all the elements
    of the dataset are copied or that all the elements are iterated over for each
    method call! The `tf.data .Dataset` API only loads and transforms elements in
    a lazy fashion. A dataset that was created by chaining together several of these
    methods can be thought of as a small program that will execute only once elements
    are requested from the end of the chain. It is only at that point that the `Dataset`
    instance crawls back up the chain of operations, possibly all the way to requesting
    data from the remote source.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.data.Dataset` 提供了可链式 API 的方法来执行这些操作，如[表 6.3](#ch06table03)中所述。每一个这些方法都返回一个新的
    `Dataset` 对象，但不要被误导以为所有数据集元素都被复制或每个方法调用都迭代所有数据集元素！`tf.data.Dataset` API 只会懒惰地加载和转换元素。通过将这些方法串联在一起创建的数据集可以看作是一个小程序，它只会在从链的末端请求元素时执行。只有在这个时候，`Dataset`
    实例才会爬回操作链，可能一直爬回到请求来自远程源的数据。'
- en: Table 6.3\. Chainable methods on the `tf.data.Dataset` object
  id: totrans-112
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 6.3\. `tf.data.Dataset` 对象上的可链式方法
- en: '| Instance method of the tf.data.Dataset object | What it does | Example |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| tf.data.Dataset 对象的实例方法 | 它的作用 | 示例 |'
- en: '| --- | --- | --- |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| .filter(predicate) | Returns a dataset containing only elements for which
    the predicate evaluates to true | myDataset.filter(x => x < 10); Returns a dataset
    containing only values from myDataset that are less than 10. |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| .filter(谓词) | 返回一个仅包含谓词为真的元素的数据集 | myDataset.filter(x => x < 10); 返回一个数据集，仅包含
    myDataset 中小于 10 的值。|'
- en: '| .map(transform) | Applies the provided function to every element in the dataset
    and returns a new dataset of the mapped elements | myDataset.map(x => x * x);
    Returns a dataset of the squared values of the original dataset. |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| .map(转换) | 将提供的函数应用于数据集中的每个元素，并返回一个新数据集，其中包含映射后的元素 | myDataset.map(x => x
    * x); 返回一个数据集，包含原始数据集的平方值。|'
- en: '| .mapAsync( asyncTransform) | Like map, but the provided function must be
    asynchronous | myDataset.mapAsync(fetchAsync); Assuming fetchAsync is an asynchronous
    function that yields the data fetched from a provided URL, will return a new dataset
    containing the data at each URL. |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| .mapAsync(异步转换) | 类似 map，但提供的函数必须是异步的 | myDataset.mapAsync(fetchAsync); 假设
    fetchAsync 是一个异步函数，可以从提供的 URL 获取数据，将返回一个包含每个 URL 数据的新数据集。|'
- en: '| .batch( batchSize,'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '| .batch(批次大小，'
- en: smallLastBatch?) | Bundles sequential spans of elements into single-element
    groups and converts primitive elements into tensors | const a = tf.data.array(
    [1, 2, 3, 4, 5, 6, 7, 8])
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: smallLastBatch?) | 将连续的元素跨度捆绑成单一元素组，并将原始元素转换为张量 | const a = tf.data.array([1,
    2, 3, 4, 5, 6, 7, 8])
- en: .batch(4);
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: .batch(4);
- en: await a.forEach(e => e.print());
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: await a.forEach(e => e.print());
- en: '// Prints:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: // 输出：
- en: // Tensor [1, 2, 3, 4]
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: // 张量 [1, 2, 3, 4]
- en: // Tensor [5, 6, 7, 8] |
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: // 张量 [5, 6, 7, 8] |
- en: '| .concatenate( dataset) | Concatenates the elements from two datasets together
    to form a new dataset | myDataset1.concatenate(myDataset2) Returns a dataset that
    will iterate over all the values of myDataset1 first, and then over all the values
    of myDataset2. |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| .concatenate(数据集) | 将两个数据集的元素连接在一起形成一个新的数据集 | myDataset1.concatenate(myDataset2)
    返回一个数据集，首先迭代 myDataset1 中的所有值，然后迭代 myDataset2 中的所有值。|'
- en: '| .repeat(count) | Returns a dataset that will iterate over the original dataset
    multiple (possibly unlimited) times | myDataset.repeat(NUM_EPOCHS) Returns a dataset
    that will iterate over all the values of myDataset NUM_EPOCHS times. If NUM_EPOCHS
    is negative or undefined, the result will iterate an unlimited number of times.
    |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| .repeat(次数) | 返回一个将多次（可能无限次）迭代原始数据集的 dataset | myDataset.repeat(NUM_EPOCHS)
    返回一个 dataset，将迭代 myDataset 中所有的值 NUM_EPOCHS 次。如果 NUM_EPOCHS 为负数或未定义，则结果将无限次迭代。|'
- en: '| .take(count) | Returns a dataset containing only the first count examples
    | myDataset.take(10); Returns a dataset containing only the first 10 elements
    of myDataset. If myDataset contains fewer than 10 elements, then there is no change.
    |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| .take(数量) | 返回一个仅包含前数量个示例的数据集 | myDataset.take(10); 返回一个仅包含 myDataset 的前
    10 个元素的数据集。如果 myDataset 中的元素少于 10 个，则没有变化。|'
- en: '| .skip(count) | Returns a dataset that skips the first count examples | myDataset.skip(10);
    Returns a dataset that contains all the elements of myDataset except the first
    10\. If myDataset contains 10 or fewer elements, this returns an empty dataset.
    |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| .skip(count) | 返回一个跳过前 count 个示例的数据集 | myDataset.skip(10); 返回一个包含 myDataset
    中除了前 10 个元素之外所有元素的数据集。如果 myDataset 包含 10 个或更少的元素，则返回一个空数据集。 |'
- en: '| .shuffle( bufferSize,'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '| .shuffle( bufferSize,'
- en: seed?
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 种子？
- en: ') | Produces a dataset that shuffles the elements of the original dataset Be
    aware: this shuffling is done by selecting randomly within a window of size bufferSize;
    thus, the ordering beyond the size of the window is preserved. | const a = tf.data.array(
    [1, 2, 3, 4, 5, 6]).shuffle(3);'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ) | 生成原始数据集元素的随机洗牌数据集 注意：此洗牌是通过在大小为 bufferSize 的窗口内随机选择来完成的；因此，超出窗口大小的排序被保留。|
    const a = tf.data.array( [1, 2, 3, 4, 5, 6]).shuffle(3);
- en: await a.forEach(e => console.log(e));
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: await a.forEach(e => console.log(e));
- en: // prints, e.g., 2, 4, 1, 3, 6, 5 Prints the values 1 through 6 in a randomly
    shuffled order. The shuffle is partial, in that not all orders are possible since
    the window is smaller than the total data size. For example, it is not possible
    that the last element, 6, will now be the first in the new order, since the 6
    would need to move back more than bufferSize (3) spaces. |
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: // 输出，例如，2, 4, 1, 3, 6, 5 以随机洗牌顺序输出 1 到 6 的值。洗牌是部分的，因为不是所有的顺序都是可能的，因为窗口比总数据量小。例如，最后一个元素
    6 现在成为新顺序中的第一个元素是不可能的，因为 6 需要向后移动超过 bufferSize（3）个空间。|
- en: These operations can be chained together to create simple but powerful processing
    pipelines. For instance, to split a dataset randomly into training and testing
    datasets, you can follow the recipe in the following listing (see tfjs-examples/iris-fitDataset/data.js).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这些操作可以链接在一起创建简单但强大的处理管道。例如，要将数据集随机分割为训练和测试数据集，可以按照以下列表中的步骤操作（参见 tfjs-examples/iris-fitDataset/data.js）。
- en: Listing 6.4\. Creating a train/test split using `tf.data.Dataset`
  id: totrans-135
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.4\. 使用 `tf.data.Dataset` 创建训练/测试分割
- en: '[PRE7]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '***1*** We use the same shuffle seed for the training and testing data; otherwise
    they will be shuffled independently, and some samples will be in both training
    and testing.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 我们在训练和测试数据中使用相同的洗牌种子；否则它们将独立洗牌，并且一些样本将同时出现在训练和测试中。'
- en: '***2*** Takes the first N samples for the training data'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 获取训练数据的前 N 个样本'
- en: '***3*** Skips the first N samples for the testing data'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 跳过测试数据的前 N 个样本'
- en: There are some important considerations to attend to in this listing. We would
    like to randomly assign samples into the training and testing splits, and thus
    we shuffle the data first. We take the first `N` samples for the training data.
    For the testing data, we skip those samples, taking the rest. It is very important
    that the data is shuffled *the same way* when we are taking the samples, so we
    don’t end up with the same example in both sets; thus we use the same random seed
    for both when sampling both pipelines.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个列表中有一些重要的考虑事项需要注意。我们希望将样本随机分配到训练和测试集中，因此我们首先对数据进行洗牌。我们取前 `N` 个样本作为训练数据。对于测试数据，我们跳过这些样本，取剩下的样本。当我们取样本时，数据以
    *相同的方式* 进行洗牌非常重要，这样我们就不会在两个集合中都有相同的示例；因此当同时采样两个管道时，我们使用相同的随机种子。
- en: It’s also important to notice that we apply the `map()` function *after* the
    skip operation. It would also be possible to call `.map(preprocessFn)` *before*
    the skip, but then the `preprocessFn` would be executed even for examples we discard—a
    waste of computation. This behavior can be verified with the following listing.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，我们在跳过操作之后应用 `map()` 函数。也可以在跳过之前调用 `.map(preprocessFn)`，但是那样 `preprocessFn`
    就会对我们丢弃的示例执行——这是一种计算浪费。可以使用以下列表来验证这种行为。
- en: Listing 6.5\. Illustrating `Dataset.forEach skip()` and `map()` interactions
  id: totrans-142
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.5\. 说明 `Dataset.forEach skip()` 和 `map()` 交互
- en: '[PRE8]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '***1*** Skips then maps'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 先跳过再映射'
- en: '***2*** Maps then skips'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 先映射再跳过'
- en: Another common use for `dataset.map()` is to normalize our input data. We can
    imagine a scenario in which we wish to normalize our input to be zero mean, but
    we have an unlimited number of input samples. In order to subtract the mean, we
    would need to first calculate the mean of the distribution, but calculating the
    mean of an unlimited set is not tractable. We could also consider taking a representative
    sample and calculating the mean of that sample, but we could be making a mistake
    if we don’t know what the right sample size is. Consider a distribution in which
    nearly all values are 0, but every ten-millionth example has a value of 1e9\.
    This distribution has a mean value of 100, but if you calculate the mean on the
    first 1 million examples, you will be quite off.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`dataset.map()` 的另一个常见用法是对输入数据进行归一化。我们可以想象一种情况，我们希望将输入归一化为零均值，但我们有无限数量的输入样本。为了减去均值，我们需要先计算分布的均值，但是计算无限集合的均值是不可行的。我们还可以考虑取一个代表性的样本，并计算该样本的均值，但如果我们不知道正确的样本大小，就可能犯错误。考虑一个分布，几乎所有的值都是
    0，但每一千万个示例的值为 1e9。这个分布的均值是 100，但如果你在前 100 万个示例上计算均值，你就会相当偏离。'
- en: We can perform a streaming normalization using the dataset API in the following
    way ([listing 6.6](#ch06ex06)). In this listing, we will keep a running tally
    of how many samples we’ve seen and what the sum of those samples has been. In
    this way, we can perform a streaming normalization. This listing operates on scalars
    (not tensors), but a version designed for tensors would have a similar structure.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用数据集 API 进行流式归一化，方法如下（[示例 6.6](#ch06ex06)）。在此示例中，我们将跟踪我们已经看到的样本数量以及这些样本的总和。通过这种方式，我们可以进行流式归一化。此示例操作标量（不是张量），但是针对张量设计的版本具有类似的结构。
- en: Listing 6.6\. Streaming normalization using `tf.data.map()`
  id: totrans-148
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6.6\. 使用 `tf.data.map()` 进行流式归一化
- en: '[PRE9]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '***1*** Returns a unary function, which will return its input minus the mean
    of all its input so far'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 返回一个一元函数，它将返回输入减去迄今为止其所有输入的均值。'
- en: Note that we generate a new mapping function, which closes over its own copy
    of the sample counter and accumulator. This is to allow for multiple datasets
    to be normalized independently. Otherwise, both datasets would use the same variables
    to count invocations and sums. This solution is not without its own limitations,
    especially with the possibility of numeric overflow in `sumSoFar` or `samplesSoFar`,
    so some care is warranted.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们生成了一个新的映射函数，它在自己的样本计数器和累加器上关闭。这是为了允许多个数据集独立归一化。否则，两个数据集将使用相同的变量来计数调用和求和。这种解决方案并不是没有限制，特别是在
    `sumSoFar` 或 `samplesSoFar` 中可能发生数值溢出的情况下，需要谨慎处理。
- en: 6.2\. Training models with model.fitDataset
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2\. 使用 model.fitDataset 训练模型
- en: The streaming dataset API is nice, and we’ve seen that it allows us to do some
    elegant data manipulation, but the main purpose of the `tf.data` API is to simplify
    connecting data to our model for training and evaluation. How is `tf.data` going
    to help us here?
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 流式数据集 API 不错，我们已经看到它可以让我们进行一些优雅的数据操作，但是 `tf.data` API 的主要目的是简化将数据连接到模型进行训练和评估的过程。那么
    `tf.data` 如何帮助我们呢？
- en: Ever since [chapter 2](kindle_split_013.html#ch02), whenever we’ve wanted to
    train a model, we’ve used the `model.fit()` API. Recall that `model.fit()` takes
    at least two mandatory arguments—`xs` and `ys`. As a reminder, the `xs` variable
    must be a tensor that represents a collection of input examples. The `ys` variable
    must be bound to a tensor that represents a corresponding collection of output
    targets. For example, in the previous chapter’s [listing 5.11](kindle_split_016.html#ch05ex11),
    we trained and fine-tuned on our synthetic object-detection model with calls like
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 自[第 2 章](kindle_split_013.html#ch02)以来，每当我们想要训练一个模型时，我们都会使用 `model.fit()` API。回想一下，`model.fit()`至少需要两个必要参数
    - `xs` 和 `ys`。作为提醒，`xs` 变量必须是一个表示一系列输入示例的张量。`ys` 变量必须绑定到表示相应输出目标集合的张量。例如，在上一章的
    [示例 5.11](kindle_split_016.html#ch05ex11) 中，我们使用类似的调用对我们的合成目标检测模型进行训练和微调。
- en: '[PRE10]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: where `images` was, by default, a rank-4 tensor of shape `[2000, 224, 224, 3]`,
    representing a collection of 2,000 images. The `modelFitArgs` configuration object
    specified the batch size for the optimizer, which was by default 128\. Stepping
    back, we see that TensorFlow.js was given an in-memory^([[6](#ch06fn6)]) collection
    of 2,000 examples, representing the entirety of the data, and then looped through
    that data 128 examples at a time to complete each epoch.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 其中`images`默认情况下是一个形状为`[2000, 224, 224, 3]`的秩为4的张量，表示一组2000张图像。`modelFitArgs`配置对象指定了优化器的批处理大小，默认为128。回顾一下，我们可以看到TensorFlow.js被提供了一个内存中^([[6](#ch06fn6)])的包含2000个示例的集合，表示整个数据集，然后循环遍历该数据，每次以128个示例为一批完成每个时期的训练。
- en: ⁶
  id: totrans-157
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁶
- en: ''
  id: totrans-158
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In *GPU* memory, which is usually more limited than the system RAM!
  id: totrans-159
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在*GPU*内存中，通常比系统RAM有限！
- en: What if this wasn’t enough data, and we wanted to train with a much larger dataset?
    In this situation, we are faced with a pair of less than ideal options. Option
    1 is to load a much larger array and see if it works. At some point, however,
    TensorFlow.js is going to run out of memory and emit a helpful error indicating
    that it was unable to allocate the storage for the training data. Option 2 is
    for us to instead upload our data to the GPU in separate chunks and call `model.fit()`
    on each chunk. We would need to perform our own orchestration of `model.fit()`,
    training our model on pieces of our training data iteratively whenever it is ready.
    If we wanted to perform more than one epoch, we would need to go back and re-download
    our chunks again in some (presumably shuffled) order. Not only is this orchestration
    cumbersome and error prone, but it also interferes with TensorFlow’s own reporting
    of the epoch counter and reported metrics, which we will be forced to stitch back
    together ourselves.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些数据不足，我们想要用更大的数据集进行训练怎么办？在这种情况下，我们面临着一对不太理想的选择。选项1是加载一个更大的数组，然后看看它是否起作用。然而，到某个时候，TensorFlow.js会耗尽内存，并发出一条有用的错误消息，指示它无法为训练数据分配存储空间。选项2是我们将数据上传到GPU中的不同块中，并在每个块上调用`model.fit()`。我们需要执行自己的`model.fit()`协调，每当准备好时，就对我们的训练数据的部分进行迭代训练我们的模型。如果我们想要执行多个时期，我们需要回到一开始，以某种（可能是打乱的）顺序重新下载我们的块。这种协调不仅繁琐且容易出错，而且还干扰了TensorFlow自己的时期计数器和报告的指标，我们将被迫自己将它们拼接在一起。
- en: 'Tensorflow.js provides us a much more convenient tool for this task using the
    `model.fitDataset()` API:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Tensorflow.js为我们提供了一个更方便的工具，使用`model.fitDataset()`API：
- en: '[PRE11]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`model.fitDataset()` accepts a dataset as its first argument, but the dataset
    must meet a certain pattern to work. Specifically, the dataset must yield objects
    with two properties. The first property is named `xs` and has a value of type
    `Tensor`, representing the features for a batch of examples; this is similar to
    the `xs` argument to `model.fit()`, but the dataset yields elements one batch
    at a time rather than the whole array at once. The second required property is
    named `ys` and contains the corresponding target tensor.^([[7](#ch06fn7)]) Compared
    to `model.fit()`, `model.fitDataset()` provides a number of advantages. Foremost,
    we don’t need to write code to manage and orchestrate the downloading of pieces
    of our dataset—this is handled for us in an efficient, as-needed streaming manner.
    Caching structures built into the dataset allow for prefetching data that is anticipated
    to be needed, making efficient use of our computational resources. This API call
    is also more powerful, allowing us to train on much larger datasets than can fit
    on our GPU. In fact, the size of the dataset we can train on is now limited only
    by how much time we have because we can continue to train for as long as we are
    able to get new training examples. This behavior is illustrated in the data-generator
    example in the tfjs-examples repository.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`model.fitDataset()` 的第一个参数接受一个数据集，但数据集必须符合特定的模式才能工作。具体来说，数据集必须产生具有两个属性的对象。第一个属性名为`xs`，其值为`Tensor`类型，表示一批示例的特征；这类似于`model.fit()`中的`xs`参数，但数据集一次产生一个批次的元素，而不是一次产生整个数组。第二个必需的属性名为`ys`，包含相应的目标张量。^([[7](#ch06fn7)])
    与`model.fit()`相比，`model.fitDataset()`提供了许多优点。首先，我们不需要编写代码来管理和协调数据集的下载——这在需要时以一种高效、按需流式处理的方式为我们处理。内置的缓存结构允许预取预期需要的数据，有效利用我们的计算资源。这个API调用也更强大，允许我们在比GPU容量更大得多的数据集上进行训练。事实上，我们可以训练的数据集大小现在只受到我们拥有多少时间的限制，因为只要我们能够获得新的训练样本，我们就可以继续训练。这种行为在tfjs-examples存储库中的数据生成器示例中有所体现。'
- en: ⁷
  id: totrans-164
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁷
- en: ''
  id: totrans-165
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For models with multiple inputs, an array of tensors is expected instead of
    the individual feature tensors. The pattern is similar for models fitting multiple
    targets.
  id: totrans-166
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 对于具有多个输入的模型，期望的是张量数组而不是单独的特征张量。对于拟合多个目标的模型，模式类似。
- en: 'In this example, we will train a model to learn how to estimate the likelihood
    of winning a simple game of chance. As usual, you can use the following commands
    to check out and run the demo:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，我们将训练一个模型来学习如何估计获胜的可能性，一个简单的游戏机会。和往常一样，您可以使用以下命令来查看和运行演示：
- en: '[PRE12]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The game used here is a simplified card game, somewhat like poker. Both players
    are given `N` cards, where `N` is a positive integer, and each card is represented
    by a random integer between 1 and 13\. The rules of the game are as follows:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 此处使用的游戏是一个简化的纸牌游戏，有点像扑克牌。每个玩家都会被分发 `N` 张牌，其中 `N` 是一个正整数，并且每张牌由 1 到 13 之间的随机整数表示。游戏规则如下：
- en: The player with the largest group of same-valued cards wins. For example, if
    player 1 has three of a kind, and player 2 has only a pair, player 1 wins.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拥有相同数值牌最多的玩家获胜。例如，如果玩家 1 有三张相同的牌，而玩家 2 只有一对，玩家 1 获胜。
- en: If both players have the same-sized maximal group, then the player with the
    group with the largest face value wins. For example, a pair of 5s beats a pair
    of 4s.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果两名玩家拥有相同大小的最大组，则拥有最大面值组的玩家获胜。例如，一对 5 比一对 4 更大。
- en: If neither player even has a pair, the player with the highest single card wins.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果两名玩家甚至都没有一对牌，那么拥有最高单张牌的玩家获胜。
- en: Ties are settled randomly, 50/50.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平局将随机解决，50/50。
- en: It should be easy to convince yourself that each player has an equal chance
    of winning. Thus, if we know nothing about our cards, we should only be able to
    guess whether we will win or not half of the time. We will build and train a model
    that takes as input player 1’s cards and predicts whether that player will win.
    In the screenshot in [figure 6.1](#ch06fig01), you should see that we were able
    to achieve approximately 75% accuracy on this problem after training on about
    250,000 examples (50 epochs * 50 batches per epoch * 100 samples per batch). Five
    cards per hand were used in this simulation, but similar accuracies are achieved
    for other counts. Higher accuracies are achievable by running with larger batches
    and for more epochs, but even at 75%, our intelligent player has a significant
    advantage over the naive player at estimating the likelihood that they will win.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 要说服自己，每个玩家都有平等的获胜机会应该很容易。因此，如果我们对自己的牌一无所知，我们应该只能猜测我们是否会赢得比赛的时间一半。我们将构建和训练一个模型，该模型以玩家
    1 的牌作为输入，并预测该玩家是否会获胜。在[图 6.1](#ch06fig01)的屏幕截图中，您应该看到我们在大约 250,000 个示例（50 个周期
    * 每周期 50 个批次 * 每个批次 100 个样本）训练后，在这个问题上达到了约 75% 的准确率。在这个模拟中，每手使用五张牌，但对于其他数量也可以达到类似的准确率。通过使用更大的批次和更多的周期，可以实现更高的准确率，但是即使在
    75% 的情况下，我们的智能玩家也比天真的玩家在估计他们将获胜的可能性时具有显著的优势。
- en: Figure 6.1\. The UI of the data-generator example. A description of the rules
    of the game and a button to run simulations are at top-left. Below that are the
    generated features and the data pipeline. The Dataset-to-Array button runs the
    chained dataset operations that will simulate the game, generate features, batch
    samples together, take `N` such batches, convert them to an array, and print the
    array out. At top-right, there are affordances to train a model using this data
    pipeline. When the user clicks the Train-Model-Using-Fit-Dataset button, the `model.fitDataset()`
    operation takes over and pulls samples from the pipeline. Loss and accuracy curves
    are printed below this. At bottom-right, the user may enter values for player
    1’s hand and press a button to make predictions from the model. Larger predictions
    indicate that the model believes the hand is more likely to win. Values are drawn
    with replacement, so five of a kind can happen.
  id: totrans-175
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.1\. 数据生成器示例的用户界面。游戏规则的描述和运行模拟的按钮位于左上方。在此下方是生成的特征和数据管道。Dataset-to-Array 按钮运行链接数据集操作，模拟游戏，生成特征，将样本批次化，取
    `N` 个这样的批次，将它们转换为数组，并将数组打印出来。右上角有用于使用此数据管道训练模型的功能。当用户点击 Train-Model-Using-Fit-Dataset
    按钮时，`model.fitDataset()` 操作接管并从管道中提取样本。下方打印了损失和准确率曲线。在右下方，用户可以输入玩家 1 的手牌值，并按下按钮从模型中进行预测。更大的预测表明模型认为该手牌更有可能获胜。值是有替换地抽取的，因此可能会出现五张相同的牌。
- en: '![](06fig01a_alt.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](06fig01a_alt.jpg)'
- en: If we were to perform this operation using `model.fit()`, we would need to create
    and store a tensor of 250,000 examples just to represent the input features. The
    data in this example are pretty small—only tens of floats per instance—but for
    our object-detection task in the previous chapter, 250,000 examples would have
    required 150 GB of GPU memory,^([[8](#ch06fn8)]) far beyond what is available
    in most browsers in 2019.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用 `model.fit()` 执行此操作，那么我们需要创建和存储一个包含 250,000 个示例的张量，以表示输入特征。这个例子中的数据相当小——每个实例只有几十个浮点数——但是对于上一章中的目标检测任务，250,000
    个示例将需要 150 GB 的 GPU 内存，^([[8](#ch06fn8)]) 远远超出了2019年大多数浏览器的可用范围。
- en: ⁸
  id: totrans-178
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁸
- en: ''
  id: totrans-179
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: numExamples × width × height × colorDepth × sizeOfInt32 = 250,000 × 224 × 224
    × 3 × 4 bytes .
  id: totrans-180
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: numExamples × width × height × colorDepth × sizeOfInt32 = 250,000 × 224 × 224
    × 3 × 4 bytes 。
- en: Let’s take a dive into relevant portions of this example. First, let’s look
    at how we generate our dataset. The code in the following listing (simplified
    from tfjs-examples/data-generator/index.js) is similar to the dice-rolling generator
    dataset in [listing 6.3](#ch06ex03), with a bit more complexity since we are storing
    more information.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解此示例的相关部分。首先，让我们看一下如何生成我们的数据集。下面清单中的代码（从 tfjs-examples/data-generator/index.js
    简化而来）与 [清单 6.3](#ch06ex03) 中的掷骰子生成器数据集类似，但更复杂一些，因为我们存储了更多信息。
- en: Listing 6.7\. Building a `tf.data.Dataset` for our card game
  id: totrans-182
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 6.7\. 为我们的卡片游戏构建 `tf.data.Dataset`
- en: '[PRE13]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '***1*** The game library provides randomHand() and compareHands(), functions
    to generate a hand from a simplified poker-like card game and to compare two such
    hands to tell which player has won.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 游戏库提供了 `randomHand()` 和 `compareHands()` 函数，用于从简化的类似扑克牌的卡片游戏生成手牌，以及比较两个这样的手牌以确定哪位玩家赢了。'
- en: '***2*** Simulates two players in a simple, poker-like card game'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 模拟简单的类似扑克牌的卡片游戏中的两名玩家'
- en: '***3*** Calculates the winner of the game'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 计算游戏的赢家'
- en: '***4*** Returns the two hands and who won'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 返回两名玩家的手牌以及谁赢了'
- en: Once we have our basic generator dataset connected up to the game logic, we
    want to format the data in a way that makes sense for our learning task. Specifically,
    our task is to attempt to predict the `player1Win` bit from the `player1Hand`.
    In order to do so, we are going to need to make our dataset return elements of
    the form `[batchOf-Features, batchOfTargets]`, where the features are calculated
    from player 1’s hand. The following code is simplified from tfjs-examples/data-generator/index.js.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们将基本生成器数据集连接到游戏逻辑，我们希望以对我们的学习任务有意义的方式格式化数据。具体来说，我们的任务是尝试从 `player1Hand` 预测
    `player1Win` 位。为了做到这一点，我们需要使我们的数据集返回形式为 `[batchOf-Features, batchOfTargets]` 的元素，其中特征是从玩家
    1 的手中计算出来的。下面的代码简化自 tfjs-examples/data-generator/index.js。
- en: Listing 6.8\. Building a dataset of player features
  id: totrans-189
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 6.8\. 构建玩家特征的数据集
- en: '[PRE14]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '***1*** Takes the state of one complete game and returns a feature representation
    of player 1’s hand and the win status'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 获取一局完整游戏的状态，并返回玩家 1 手牌的特征表示和获胜状态'
- en: '***2*** handOneHot has the shape [numCards, max_value_card]. This operation
    sums the number of each type of card, resulting in a tensor with the shape [max_value_card].'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** `handOneHot` 的形状为 `[numCards, max_value_card]`。此操作对每种类型的卡片进行求和，结果是形状为
    `[max_value_card]` 的张量。'
- en: '***3*** Converts each element from the game output object format to an array
    of two tensors: one for the features and one for the target'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 将游戏输出对象格式的每个元素转换为两个张量的数组：一个用于特征，一个用于目标'
- en: '***4*** Groups together BATCH_SIZE consecutive elements into a single element.
    This would also convert the data from JavaScript arrays to tensors if they weren''t
    already tensors.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 将 BATCH_SIZE 个连续元素分组成单个元素。如果它们尚未是张量，则还会将数据从 JavaScript 数组转换为张量。'
- en: Now that we have a dataset in the proper form, we can connect it to our model
    using `model.fitDataset()`, as shown in the following listing (simplified from
    tfjs-examples/data-generator/index.js).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个符合规范的数据集，我们可以使用 `model.fitDataset()` 将其连接到我们的模型上，如下清单所示（简化自 tfjs-examples/data-generator/index.js）。
- en: Listing 6.9\. Building and training a model on the dataset
  id: totrans-196
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 6.9\. 构建并训练数据集的模型
- en: '[PRE15]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '***1*** This call launches the training.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 此调用启动训练。'
- en: '***2*** How many batches constitutes an epoch. Since our dataset is unlimited,
    this needs to be defined to tell TensorFlow.js when to execute the epoch-end callback.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 一个 epoch 包含多少批次。由于我们的数据集是无限的，因此需要定义此参数，以告知 TensorFlow.js 何时执行 epoch
    结束回调。'
- en: '***3*** We are using the training data as validation data. Normally this is
    bad, since we will get a biased impression of how well we are doing. In this case,
    it is not a problem since the data used for training and the data used for validation
    are guaranteed to be independent by virtue of the generator.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 我们将训练数据用作验证数据。通常这是不好的，因为我们会对我们的表现有偏见。但在这种情况下，由于训练数据和验证数据是由生成器保证独立的，所以这不是问题。'
- en: '***4*** We need to tell TensorFlow.js how many samples to take from the validation
    dataset to constitute one evaluation.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 我们需要告诉 TensorFlow.js 从验证数据集中取多少样本来构成一个评估。'
- en: '***5*** model.fitDataset() creates history that is compatible with tfvis, just
    like model.fit().'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** model.fitDataset() 创建与 tfvis 兼容的历史记录，就像 model.fit() 一样。'
- en: 'As we see in the previous listing, fitting a model to a dataset is just as
    simple as fitting a model to a pair of x, y tensors. As long as our dataset yields
    tensor values in the right format, everything just works, we get the benefit of
    streaming data from a possibly remote source, and we don’t need to manage the
    orchestration on our own. Beyond passing in a dataset instead of a tensor pair,
    there are a few differences in the configuration object that merit discussion:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的清单中看到的，将模型拟合到数据集与将模型拟合到一对 x、y 张量一样简单。只要我们的数据集以正确的格式产生张量值，一切都能正常工作，我们能从可能是远程来源的流数据中获益，而且我们不需要自己管理编排。除了传入数据集而不是张量对之外，在配置对象中还有一些差异值得讨论：
- en: '`batchesPerEpoch`—As we saw in [listing 6.9](#ch06ex09), the configuration
    for `model.fitDataset()` takes an optional field for specifying the number of
    batches that constitute an epoch. When we handed the entirety of the data to `model.fit()`,
    it was easy to calculate how many examples there are in the whole dataset. It’s
    just `data.shape[0]`! When using `fitDataset()`, we can tell TensorFlow.js when
    an epoch ends in one of two ways. The first way is to use this configuration field,
    and `fitDataset()` will execute `onEpochEnd` and `onEpochStart` callbacks after
    that many batches. The second way is to have the dataset itself end as a signal
    that the dataset is exhausted. In [listing 6.7](#ch06ex07), we could change'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batchesPerEpoch`—正如我们在 [清单 6.9](#ch06ex09) 中看到的，`model.fitDataset()` 的配置接受一个可选字段来指定构成一个周期的批次数。当我们把整个数据交给
    `model.fit()` 时，计算整个数据集中有多少示例很容易。它就是 `data.shape[0]`！当使用 `fitDataset()` 时，我们可以告诉
    TensorFlow.js 一个周期何时结束有两种方法。第一种方法是使用这个配置字段，`fitDataset()` 将在那么多批次之后执行 `onEpochEnd`
    和 `onEpochStart` 回调。第二种方法是让数据集本身结束作为信号，表明数据集已经耗尽。在 [清单 6.7](#ch06ex07) 中，我们可以改变'
- en: '[PRE16]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: to
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 到
- en: '[PRE17]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: to mimic this behavior.
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模仿这种行为。
- en: '`validationData`—When using `fitDataset()`, the `validationData` may be a dataset
    also. But it doesn’t have to be. You can continue to use tensors for `validationData`
    if you want to. The validation dataset needs to meet the same specification with
    respect to the format of returned elements as the training dataset does.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`validationData`—当使用 `fitDataset()` 时，`validationData` 也可以是一个数据集。但不是必须的。如果你想要的话，你可以继续使用张量作为
    `validationData`。验证数据集需要符合返回元素格式的相同规范，就像训练数据集一样。'
- en: '`validationBatches`—If your validation data comes from a dataset, you need
    to tell TensorFlow.js how many samples to take from the dataset to constitute
    a complete evaluation. If no value is specified, then TensorFlow.js will continue
    to draw from the dataset until it returns a done signal. Because the code in [listing
    6.7](#ch06ex07) uses a never-ending generator to generate the dataset, this would
    never happen, and the program would hang.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`validationBatches`—如果你的验证数据来自一个数据集，你需要告诉 TensorFlow.js 从数据集中取多少样本来构成一个完整的评估。如果没有指定值，那么
    TensorFlow.js 将继续从数据集中提取，直到返回一个完成信号。因为 [清单 6.7](#ch06ex07) 中的代码使用一个永不结束的生成器来生成数据集，这永远不会发生，程序会挂起。'
- en: The rest of the configuration is identical to that of the `model.fit()` API,
    so no changes are necessary.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 其余配置与 `model.fit()` API 完全相同，因此不需要进行任何更改。
- en: 6.3\. Common patterns for accessing data
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3\. 访问数据的常见模式
- en: All developers need some solutions for connecting their data to their model.
    These connections range from common stock connections, to well-known experimental
    datasets like MNIST, to completely custom connections, to proprietary data formats
    within an enterprise. In this section, we will review how `tf.data` can help to
    make these connections simple and maintainable.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 所有开发人员都需要一些解决方案，将其数据连接到其模型中。这些连接范围从常见的股票连接，到众所周知的实验数据集，如 MNIST，到完全自定义连接，到企业内的专有数据格式。在本节中，我们将回顾
    `tf.data` 如何帮助简化和可维护这些连接。
- en: 6.3.1\. Working with CSV format data
  id: totrans-214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.1\. 处理 CSV 格式数据
- en: 'Beyond working with common stock datasets, the most common way to access data
    involves loading prepared data stored in some file format. Data files are often
    stored in CSV (comma separated value) format^([[9](#ch06fn9)]) due to its simplicity,
    human readability, and broad support. Other formats have other advantages in storage
    efficiency and access speed, but CSV might be considered the *lingua franca* of
    datasets. In the JavaScript community, we typically want to be able to conveniently
    stream data from some HTTP endpoint. This is why TensorFlow.js provides native
    support for streaming and manipulating data from CSV files. In [section 6.1.2](#ch06lev2sec2),
    we briefly described how to construct a `tf.data.Dataset` backed by a CSV file.
    In this section, we will dive deeper into the CSV API to show how `tf.data` makes
    working with these data sources very easy. We will describe an example application
    that connects to remote CSV datasets, prints their schema, counts the elements
    of the dataset, and offers the user an affordance to select and print the individual
    examples. Check out the example using the familiar commands:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用常见的股票数据集之外，访问数据的最常见方式涉及加载存储在某种文件格式中的准备好的数据。由于其简单性、易读性和广泛支持，数据文件通常存储在 CSV（逗号分隔值）格式^([[9](#ch06fn9)])
    中。其他格式在存储效率和访问速度方面具有其他优势，但 CSV 可能被认为是数据集的*共通语言*。在 JavaScript 社区中，我们通常希望能够方便地从某个
    HTTP 终端流式传输数据。这就是为什么 TensorFlow.js 提供了对从 CSV 文件中流式传输和操作数据的本机支持。在 [section 6.1.2](#ch06lev2sec2)
    中，我们简要描述了如何构建由 CSV 文件支持的 `tf.data.Dataset`。在本节中，我们将深入介绍 CSV API，以展示 `tf.data`
    如何使与这些数据源的工作变得非常容易。我们将描述一个示例应用程序，该应用程序连接到远程 CSV 数据集，打印其模式，计算数据集的元素数量，并为用户提供选择和打印单个示例的便利。查看使用熟悉的命令的示例：
- en: ⁹
  id: totrans-216
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁹
- en: ''
  id: totrans-217
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As of January 2019, the data science and machine-learning challenge site [kaggle.com/datasets](http://kaggle.com/datasets)
    boasts 13,971 public datasets, of which over two-thirds are hosted in the CSV
    format.
  id: totrans-218
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 截至 2019 年 1 月，数据科学和机器学习挑战网站 [kaggle.com/datasets](http://kaggle.com/datasets)
    拥有 13,971 个公共数据集，其中超过三分之二以 CSV 格式托管。
- en: '[PRE18]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This should pop open a site that instructs us to enter the URL of a hosted
    CSV file or to use one of the suggested four URLs by clicking, for example, Boston
    Housing CSV. See [figure 6.2](#ch06fig02) for an illustration. Underneath the
    URL entry input box, buttons are provided to perform three actions: 1) count the
    rows in the dataset, 2) retrieve the column names of the CSV, if they exist, and
    3) access and print a specified sample row of the dataset. Let’s go through how
    these work and how the `tf.data` API makes them very easy.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这将弹出一个网站，指导我们输入托管的 CSV 文件的 URL，或通过点击建议的四个 URL 之一，例如波士顿房屋 CSV。请参见 [figure 6.2](#ch06fig02)
    进行说明。在 URL 输入框下方，提供了三个按钮来执行三个操作：1) 计算数据集中的行数，2) 检索 CSV 的列名（如果存在），以及 3) 访问并打印数据集的指定样本行。让我们看看这些是如何工作的，以及
    `tf.data` API 如何使它们变得非常容易。
- en: Figure 6.2\. Web UI for our tfjs-data CSV example. Click one of the preset CSV
    buttons at the top or enter a path to your own hosted CSV, if you have one. Be
    sure to enable CORS access for your CSV if you go with your own hosted file.
  id: totrans-221
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.2\. 我们的 tfjs-data CSV 示例的 Web UI。点击顶部的预设 CSV 按钮之一或输入您自己托管的 CSV 的路径（如果有）。如果您选择自己托管的文件，请确保为您的
    CSV 启用 CORS 访问。
- en: '![](06fig01_alt.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](06fig01_alt.jpg)'
- en: We saw earlier that creating a tfjs-data dataset from a remote CSV is very simple
    using a command like
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前看到，使用类似以下命令从远程 CSV 创建一个 tfjs-data 数据集非常简单：
- en: '[PRE19]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: where `url` is either a string identifier using the http://, https://, or file://
    protocol, or a `RequestInfo`. This call does *not* actually issue any requests
    to the URL to check whether, for example, the file exists or is accessible, because
    of the lazy iteration. In [listing 6.10](#ch06ex10), the CSV is first fetched
    at the asynchronous `myData.forEach()` call. The function we call in the `forEach()`
    will simply stringify and print elements in the dataset, but we could imagine
    doing other things with this iterator, such as generating UI elements for every
    element in the set or computing statistics for a report.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 `url` 可以是使用 http://、https:// 或 file:// 协议的字符串标识符，或者是 `RequestInfo`。此调用实际上并不会向
    URL 发出任何请求，以检查文件是否存在或是否可访问，这是由于惰性迭代。在 [列表 6.10](#ch06ex10) 中，CSV 首先在异步 `myData.forEach()`
    调用处获取。我们在 `forEach()` 中调用的函数将简单地将数据集中的元素转换为字符串并打印，但我们可以想象对此迭代器执行其他操作，比如为集合中的每个元素生成
    UI 元素或计算报告的统计信息。
- en: Listing 6.10\. Printing the first 10 records in a remote CSV file
  id: totrans-226
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.10\. 打印远程 CSV 文件中的前 10 条记录
- en: '[PRE20]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '***1*** Creates the tfjs-data dataset by providing the URL to tf.data.csv()'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 通过提供 URL 到 `tf.data.csv()` 创建 tfjs-data 数据集。'
- en: '***2*** Creates a dataset consisting of the first 10 rows of the CSV dataset.
    Then, uses the forEach() method to iterate over all values provided by the dataset.
    Note that forEach() is an async function.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 创建由 CSV 数据集的前 10 行组成的数据集。然后，使用 forEach() 方法迭代数据集提供的所有值。请注意，forEach()
    是一个异步函数。'
- en: 'CSV datasets often use the first row as a metadata header containing the names
    associated with each column. By default, `tf.data.csv()` assumes this to be the
    case, but it can be controlled using the `csvConfig` object passed in as the second
    argument. If column names are not provided by the CSV file itself, they can be
    provided manually in the constructor like so:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: CSV 数据集通常使用第一行作为包含每列名称的元数据标题。默认情况下，`tf.data.csv()` 假定是这种情况，但可以使用作为第二个参数传递的 `csvConfig`
    对象进行控制。如果 CSV 文件本身未提供列名，则可以像这样手动提供：
- en: '[PRE21]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: If you provide a manual `columnNames` configuration to the CSV dataset, it will
    take precedence over the header row read from the data file. By default, the dataset
    will assume the first line is a header row. If the first row is not a header,
    the absence must be configured and `columnNames` provided manually.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 如果为 CSV 数据集提供了手动的 `columnNames` 配置，则它将优先于从数据文件读取的标题行。默认情况下，数据集将假定第一行是标题行。如果第一行不是标题行，则必须配置缺少的部分并手动提供
    `columnNames`。
- en: Once the `CSVDataset` object exists, it is possible to query it for the column
    names using `dataset.columnNames()`, which returns an ordered string list of the
    column names. The `columnNames()` method is specific to the `CSVDataset` subclass
    and is not generally available from datasets built from other sources. The Get
    Column Names button in the example is connected to a handler that uses this API.
    Requesting the column names results in the `Dataset` object making a fetch call
    to the provided URL to access and parse the first row; thus the async call in
    the following listing (condensed from tfjs-examples/csv-data/index.js).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 `CSVDataset` 对象存在，就可以使用 `dataset.columnNames()` 查询其列名，该方法返回列名的有序字符串列表。`columnNames()`
    方法是特定于 `CSVDataset` 子类的，不是通常从其他来源构建的数据集中可用的。示例中的 Get Column Names 按钮连接到使用此 API
    的处理程序。请求列名将导致 `Dataset` 对象向提供的 URL 发送获取调用以访问和解析第一行；因此，下面列表中的异步调用（从 tfjs-examples/csv-data/index.js
    简化而来）。
- en: Listing 6.11\. Accessing column names from a CSV
  id: totrans-234
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.11\. 从 CSV 中访问列名
- en: '[PRE22]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '***1*** Contacts the remote CSV to collect and parse the column headers'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 联系远程 CSV 文件以收集和解析列标题。'
- en: Now that we have the column names, let’s get a row from our dataset. In [listing
    6.12](#ch06ex12), we show how the web app prints out a single selected row of
    the CSV file, where the user selects which row via an input element. In order
    to fulfill this request, we will first use the `Dataset.skip()` method to create
    a new dataset the same as the original one, but skipping the first `n - 1` elements.
    We will then use the `Dataset.take()` method to create a dataset that ends after
    one element. Finally, we will use `Dataset.toArray()` to extract the data into
    a standard JavaScript array. If everything goes right, our request will produce
    an array that contains exactly one element at the specified position. This sequence
    is put together in the following listing (condensed from tfjs-examples/csv-data/index.js).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了列名，让我们从数据集中获取一行。在[清单 6.12](#ch06ex12)中，我们展示了Web应用程序如何打印CSV文件的单个选择的行，用户通过输入元素选择行。为了满足这个请求，我们首先使用`Dataset.skip()`方法创建一个新的数据集，与原始数据集相同，但跳过前`n
    - 1`个元素。然后，我们将使用`Dataset.take()`方法创建一个在一个元素后结束的数据集。最后，我们将使用`Dataset.toArray()`将数据提取到标准的JavaScript数组中。如果一切顺利，我们的请求将产生一个包含指定位置的一个元素的数组。该序列在下面的清单中组合在一起（从tfjs-examples/csv-data/index.js中精简）。
- en: Listing 6.12\. Accessing a selected row from a remote CSV
  id: totrans-238
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 6.12\. 访问远程CSV中的选择行
- en: '[PRE23]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '***1*** sampleIndex is a number returned by a UI element.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** sampleIndex是由UI元素返回的一个数字。'
- en: '***2*** Creates the dataset myData, configured to read from url, but does not
    actually connect yet'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 创建名为myData的数据集，配置为从url读取，但实际上还没有连接'
- en: '***3*** Creates a new dataset but skips over the first sampleIndex values'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 创建一个新的数据集，但跳过前sampleIndex个值'
- en: '***4*** Creates a new dataset, but only keeps the first 1 element'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 创建一个新的数据集，但只保留第一个元素。'
- en: '***5*** This is the call that actually causes the Dataset object to contact
    the URL and perform the fetch. Note that the return type is an array of objects,
    in this case, containing exactly one object, with keys corresponding to the header
    names and values associated with those columns.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 这是实际上导致Dataset对象联系URL并执行获取的调用。请注意，返回类型是一个对象数组，本例中仅包含一个对象，键对应于标题名称和与那些列关联的值。'
- en: 'We can now take the output of the row, which—as you can see from the output
    of the `console.log` in [listing 6.12](#ch06ex12) (repeated in a comment)—comes
    in the form of an object mapping the column name to the value, and styles it for
    insertion into our document. Something to watch out for: if we ask for a row that
    doesn’t exist, perhaps the 400th element of a 300-element dataset, we will end
    up with an empty array.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以取得行的输出，正如你从[清单 6.12](#ch06ex12)中的`console.log`输出中所看到的（在评论中重复），它以将列名映射到值的对象形式呈现，并对其进行样式化以插入到我们的文档中。需要注意的是：如果我们请求一个不存在的行，例如300个元素数据集的第400个元素，我们将得到一个空数组。
- en: It’s pretty common when connecting to remote datasets to make a mistake and
    use a bad URL or improper credentials. In these circumstances, it’s best to catch
    the error and provide the user with a reasonable error message. Since the `Dataset`
    object does not actually contact the remote resource until the data is needed,
    it’s important to take care to write the error handling in the right place. The
    following listing shows a short snippet of how error handling is done in our CSV
    example web app (condensed from tfjs-examples/csv-data/index.js). For more details
    about how to connect to CSV files guarded by authentication, see [info box 6.2](#ch06sb02).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在连接远程数据集时，常常会犯错，使用错误的URL或不当的凭证。在这些情况下，最好捕获错误并向用户提供一个合理的错误消息。由于`Dataset`对象实际上直到需要数据时才联系远程资源，因此重要的是要注意将错误处理写在正确的位置。下面的清单显示了我们CSV示例Web应用程序中如何处理错误的一个简短片段（从tfjs-examples/csv-data/index.js中精简）。有关如何连接受身份验证保护的CSV文件的更多详细信息，请参见[信息框
    6.2](#ch06sb02)。
- en: Listing 6.13\. Handling errors arising from failed connections
  id: totrans-247
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 6.13\. 处理由于连接失败引起的错误
- en: '[PRE24]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '***1*** Wrapping this line in a try block wouldn’t help because the bad URL
    is not fetched here.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 将这行代码放在try块中也没有帮助，因为坏的URL在这里并没有被获取。'
- en: '***2*** The error from a bad connection will be thrown at this step.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 从坏连接引起的错误将在此步骤抛出。'
- en: 'In [section 6.2](#ch06lev1sec2), we learned how to use `model.fitDataset()`.
    We saw that the method requires a dataset that yields elements in a very particular
    form. Recall that the form is an object with two properties `{xs, ys}`, where
    `xs` is a tensor representing a batch of the input, and `ys` is a tensor representing
    a batch of the associated target. By default, the CSV dataset will return elements
    as JavaScript objects, but we can configure the dataset to instead return elements
    closer to what we need for training. For this, we will need to use the `csvConfig.columnConfigs`
    field of `tf.data.csv`(). Consider a CSV file about golf with three columns: “club,”
    “strength,” and “distance.” If we wished to predict distance from club and strength,
    we could apply a map function on the raw output to arrange the fields into `xs`
    and `ys`; or, more easily, we could configure the CSV reader to do this for us.
    [Table 6.4](#ch06table04) shows how to configure the CSV dataset to separate the
    feature and label properties, and perform batching so that the output is suitable
    for entry into `model.fitDataset()`.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [6.2 节](#ch06lev1sec2) 中，我们学习了如何使用 `model.fitDataset()`。我们看到该方法需要一个以非常特定形式产生元素的数据集。回想一下，形式是一个具有两个属性
    `{xs, ys}` 的对象，其中 `xs` 是表示输入批次的张量，`ys` 是表示相关目标批次的张量。默认情况下，CSV 数据集将返回 JavaScript
    对象形式的元素，但我们可以配置数据集以返回更接近我们训练所需的元素。为此，我们将需要使用 `tf.data.csv()` 的 `csvConfig.columnConfigs`
    字段。考虑一个关于高尔夫的 CSV 文件，其中有三列：“club”、“strength” 和 “distance”。如果我们希望从俱乐部和力量预测距离，我们可以在原始输出上应用映射函数，将字段排列成
    `xs` 和 `ys`；或者，更容易的是，我们可以配置 CSV 读取器来为我们执行此操作。[表 6.4](#ch06table04) 显示了如何配置 CSV
    数据集以分离特征和标签属性，并执行批处理，以便输出适合输入到 `model.fitDataset()` 中。
- en: Table 6.4\. Configuring a CSV dataset to work with `model.fitDataset()`
  id: totrans-252
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 6.4\. 配置 CSV 数据集以与 `model.fitDataset()` 一起使用
- en: '| How the dataset is built and configured | Code for building the dataset |
    Result of dataset.take(1).toArray()[0] (the first element returned from the dataset)
    |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 数据集的构建和配置方式 | 构建数据集的代码 | dataset.take(1).toArray()[0] 的结果（从数据集返回的第一个元素） |'
- en: '| --- | --- | --- |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Raw CSV default | dataset = tf.data.csv(csvURL) | {club: 1, strength: 45,
    distance: 200} |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 默认的原始 CSV | dataset = tf.data.csv(csvURL) | {club: 1, strength: 45, distance:
    200} |'
- en: '| CSV with label configured in columnConfigs | columnConfigs = {distance: {isLabel:
    true}};'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '| 在 columnConfigs 中配置标签的 CSV | columnConfigs = {distance: {isLabel: true}};'
- en: dataset = tf.data.csv(csvURL,
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: dataset = tf.data.csv(csvURL,
- en: '{columnConfigs}); | {xs: {club: 1, strength: 45}, ys: {distance: 200}} |'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '{columnConfigs}); | {xs: {club: 1, strength: 45}, ys: {distance: 200}} |'
- en: '| CSV with columnConfigs and then batched | columnConfigs = {distance: {isLabel:
    true}};'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '| 在 columnConfigs 中配置后进行批处理的 CSV | columnConfigs = {distance: {isLabel: true}};'
- en: dataset = tf.data
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: dataset = tf.data
- en: .csv(csvURL,
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: .csv(csvURL,
- en: '{columnConfigs})'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '{columnConfigs})'
- en: '.batch(128); | [xs: {club: Tensor, strength: Tensor},'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '.batch(128); | [xs: {club: Tensor, strength: Tensor},'
- en: 'ys: {distance:Tensor}] Each of these three tensors has shape = **[128]**. |'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 'ys: {distance:Tensor}] 这三个张量中的每一个的形状都是 **[128]**。 |'
- en: '| CSV with columnConfigs and then batched and mapped from object to array |
    columnConfigs = {distance: {isLabel: true}};'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '| 在 columnConfigs 中配置后进行批处理和从对象映射到数组的 CSV | columnConfigs = {distance: {isLabel:
    true}}。'
- en: dataset = tf.data
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: dataset = tf.data
- en: .csv(csvURL,
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: .csv(csvURL,
- en: '{columnConfigs})'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '{columnConfigs})'
- en: .map(({xs, ys}) =>
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: .map(({xs, ys}) =>
- en: '{'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: return
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: return
- en: '{xs:Object.values(xs),'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '{xs:Object.values(xs),'
- en: ys:Object.values(ys)};
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: ys:Object.values(ys)};
- en: '})'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '})'
- en: '.batch(128); | {xs: Tensor, ys: Tensor} Note that the mapping function returned
    items of the form {xs: [number, number], ys: [number]}. The batch operation automatically
    converts numeric arrays to tensors. Thus, the first tensor (xs) has shape = [128,2].
    The second tensor (ys) has shape = [128, 1]. |'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '.batch(128); | {xs: Tensor, ys: Tensor} 请注意，映射函数返回的项目形式为 {xs: [number, number],
    ys: [number]}。批处理操作会自动将数值数组转换为张量。因此，第一个张量 (xs) 的形状为 [128,2]。第二个张量 (ys) 的形状为 [128,
    1]。 |'
- en: '|  |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Fetching CSV data guarded by authentication**'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '**通过认证获取 CSV 数据**'
- en: 'In the previous examples, we have connected to data available from remote files
    by simply providing a URL. This works well both in Node.js and from the browser
    and is very easy, but sometimes our data is protected, and we need to provide
    `Request` parameters. The `tf.data.csv()` API allows us to provide `RequestInfo`
    in place of a raw string URL, as shown in the following code. Other than the additional
    authorization parameter, there is no change in the dataset:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的示例中，我们通过简单提供 URL 来连接到远程文件中的数据。这在 Node.js 和浏览器中都很好用，而且非常容易，但有时我们的数据是受保护的，我们需要提供
    `Request` 参数。`tf.data.csv()` API 允许我们在原始字符串 URL 的位置提供 `RequestInfo`，如下面的代码所示。除了额外的授权参数之外，数据集没有任何变化：
- en: '[PRE25]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '|  |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 6.3.2\. Accessing video data using tf.data.webcam()
  id: totrans-281
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.2\. 使用 tf.data.webcam() 访问视频数据
- en: One of the most exciting applications for TensorFlow.js projects is to train
    and apply machine-learning models to the sensors directly available on mobile
    devices. Motion recognition using the mobile’s onboard accelerometer? Sound or
    speech understanding using the onboard microphone? Visual assistance using the
    onboard camera? There are so many good ideas out there, and we’ve just begun.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow.js 项目最令人兴奋的应用之一是直接训练和应用机器学习模型到移动设备上直接可用的传感器。使用移动设备上的内置加速计进行运动识别？使用内置麦克风进行声音或语音理解？使用内置摄像头进行视觉辅助？有很多好主意，而我们才刚刚开始。
- en: In [chapter 5](kindle_split_016.html#ch05), we explored working with the webcam
    and microphone in the context of transfer learning. We saw how to use the camera
    to control a game of Pac-Man, and we used the microphone to fine-tune a speech-understanding
    system. While not every modality is available as a convenient API call, `tf.data`
    does have a simple and easy API for working with the webcam. Let’s explore how
    that works and how to use it to predict from trained models.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 5 章](kindle_split_016.html#ch05) 中，我们探讨了在转移学习的背景下使用网络摄像头和麦克风的工作。我们看到了如何使用摄像头来控制一款“吃豆人”游戏，并使用麦克风来微调语音理解系统。虽然并非每种模态都可以通过方便的
    API 调用来使用，但 `tf.data` 对于使用网络摄像头有一个简单易用的 API。让我们来看看它是如何工作以及如何使用它来预测训练好的模型。
- en: With the `tf.data` API, it is very simple to create a dataset iterator yielding
    a stream of images from the webcam. [Listing 6.14](#ch06ex14) shows a basic example
    from the documentation. The first thing we notice is the call to the `tf.data.webcam`().
    This constructor returns a webcam iterator by taking an optional HTML element
    as its input argument. The constructor works only in the browser environment.
    If the API is called in the Node.js environment, or if there is no available webcam,
    then the constructor will throw an exception indicating the source of the error.
    Furthermore, the browser will request permission from the user before opening
    the webcam. The constructor will throw an exception if the permission is denied.
    Responsible development should cover these cases with user-friendly messages.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `tf.data` API，从网络摄像头获取图像流并创建数据集迭代器非常简单。[列表 6.14](#ch06ex14) 展示了来自文档的一个基本示例。我们注意到的第一件事是调用
    `tf.data.webcam`()。这个构造函数通过接受一个可选的 HTML 元素作为其输入参数来返回一个网络摄像头迭代器。该构造函数仅在浏览器环境中有效。如果在
    Node.js 环境中调用 API，或者没有可用的网络摄像头，则构造函数将抛出一个指示错误来源的异常。此外，浏览器会在打开网络摄像头之前向用户请求权限。如果权限被拒绝，构造函数将抛出一个异常。负责任的开发应该用用户友好的消息来处理这些情况。
- en: Listing 6.14\. Creating a dataset using `tf.data.webcam()` and an HTML element
  id: totrans-285
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.14\. 使用 `tf.data.webcam()` 和 HTML 元素创建数据集
- en: '[PRE26]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '***1*** Element shows webcam video and determines tensor size'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 元素显示网络摄像头视频并确定张量大小'
- en: '***2*** Constructor for the video Dataset object. The element will display
    content from the webcam. The element also configures the size of the created tensors.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 视频数据集对象的构造函数。该元素将显示来自网络摄像头的内容。该元素还配置了创建的张量的大小。'
- en: '***3*** Takes one frame from the video stream and offers the value as a tensor'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 从视频流中获取一帧并将其作为张量提供'
- en: '***4*** Stops the video stream and pauses the webcam iterator'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 停止视频流并暂停网络摄像头迭代器'
- en: When creating a webcam iterator, it is important that the iterator knows the
    shape of the tensors to be produced. There are two ways to control this. The first
    way, shown in [listing 6.14](#ch06ex14), uses the shape of the provided HTML element.
    If the shape needs to be different, or perhaps the video isn’t to be shown at
    all, the desired shape can be provided via a configuration object, as shown in
    [listing 6.15](#ch06ex15). Note that the provided HTML element argument is undefined,
    meaning that the API will create a hidden element in the DOM to act as a handle
    to the video.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 创建网络摄像头迭代器时，重要的是迭代器知道要生成的张量的形状。有两种方法可以控制这个。第一种方法，如 [列表 6.14](#ch06ex14) 所示，使用提供的
    HTML 元素的形状。如果形状需要不同，或者视频根本不需要显示，可以通过一个配置对象提供所需的形状，如 [列表 6.15](#ch06ex15) 所示。请注意，提供的
    HTML 元素参数是未定义的，这意味着 API 将创建一个隐藏的元素在 DOM 中作为视频的句柄。
- en: Listing 6.15\. Creating a basic webcam dataset using a configuration object
  id: totrans-292
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.15\. 使用配置对象创建基本网络摄像头数据集
- en: '[PRE27]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '***1*** Building a webcam dataset iterator using a configuration object instead
    of an HTML element. Here, we also specify which camera to use on a device featuring
    multiple cameras. ‘user’ refers to the camera facing the user; as an alternative
    to ‘user,’ ‘environment’ refers to the rear camera.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 使用配置对象构建网络摄像头数据集迭代器，而不是使用 HTML 元素。在这里，我们还指定了设备上要使用的摄像头，对于具有多个摄像头的设备，“user”
    指的是面向用户的摄像头；作为“user”的替代，“environment” 指的是后置摄像头。'
- en: It is also possible to use the configuration object to crop and resize portions
    of the video stream. Using the HTML element and the configuration object in tandem,
    the API allows the caller to specify a location to crop from and a desired output
    size. The output tensor will be interpolated to the desired size. See the next
    listing for an example of selecting a rectangular portion of a square video and
    then reducing the size to fit a small model.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以使用配置对象裁剪和调整视频流的部分。使用 HTML 元素和配置对象并行，API 允许调用者指定要从中裁剪的位置和期望的输出大小。输出张量将插值到所需的大小。请参阅下一个清单，以查看选择方形视频的矩形部分并缩小尺寸以适应小模型的示例。
- en: Listing 6.16\. Cropping and resizing data from a webcam
  id: totrans-296
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 6.16。从网络摄像头裁剪和调整数据
- en: '[PRE28]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '***1*** Without the explicit configuration, the videoElement would control
    the output size, 300 × 300 here.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 没有显式配置，videoElement 将控制输出大小，这里是 300 × 300。'
- en: '***2*** The user requests a 150 × 100 extraction from the video.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 用户请求从视频中提取 150 × 100 的内容。'
- en: '***3*** The extracted data will be from the center of the original video.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 提取的数据将来自原始视频的中心。'
- en: '***4*** Data captured from this webcam iterator depends on both the HTML element
    and the webcamConfig.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 从此网络摄像头迭代器中捕获的数据取决于 HTML 元素和 webcamConfig。'
- en: It is important to point out some obvious differences between this type of dataset
    and the datasets we’ve been working with so far. For example, the values yielded
    from the webcam depend on when you extract them. Contrast this with the CSV dataset,
    which will yield the rows in order no matter how fast or slowly they are drawn.
    Furthermore, samples can be drawn from the webcam for as long as the user desires
    more. The API callers must explicitly tell the stream to end when they are done
    with it.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 强调一些这种类型的数据集与我们迄今为止使用的数据集之间的一些明显区别是很重要的。例如，从网络摄像头中产生的值取决于您何时提取它们。与以 CSV 数据集不同，不管它们是以多快或慢的速度绘制的，它们都会按顺序产生行。此外，可以根据用户需要从网络摄像头中提取样本。API
    调用者在完成后必须明确告知流结束。
- en: Data is accessed from the webcam iterator using the `capture()` method, which
    returns a tensor representing the most recent frame. API users should use this
    tensor for their machine-learning work, but must remember to dispose of it to
    prevent a memory leak. Because of the intricacies involved in asynchronous processing
    of the webcam data, it is better to apply necessary preprocessing functions directly
    to the captured frame rather than use the deferred `map()` functionality provided
    by `tf.data`.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `capture()` 方法从网络摄像头迭代器中访问数据，该方法返回表示最新帧的张量。API 用户应该将这个张量用于他们的机器学习工作，但必须记住使其释放，以防止内存泄漏。由于涉及网络摄像头数据的异步处理的复杂性，最好直接将必要的预处理函数应用于捕获的帧，而不是使用由
    `tf.data` 提供的延迟 `map()` 功能。
- en: That is to say, rather than processing data using `data.map()`,
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，不要使用 `data.map()` 处理数据，
- en: '[PRE29]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'apply the function directly to the image:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 将函数直接应用于图像：
- en: '[PRE30]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The `forEach()` and `toArray()` methods should not be used on a webcam iterator.
    For processing long sequences of frames from the device, users of the `tf.data.webcam()`
    API should define their own loop using, for example, `tf.nextFrame()` and call
    `capture()` at a reasonable frame rate. The reason is that if you were to call
    `forEach()` on your webcam, then the framework would draw frames as fast as the
    browser’s JavaScript engine can possibly request them from the device. This will
    typically create tensors faster than the frame rate of the device, resulting in
    duplicate frames and wasted computation. For similar reasons, a webcam iterator
    should *not* be passed as an argument to the `model.fit()` method.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 不应在网络摄像头迭代器上使用 `forEach()` 和 `toArray()` 方法。为了从设备中处理长序列的帧，`tf.data.webcam()`
    API 的用户应该自己定义循环，例如使用 `tf.nextFrame()` 并以合理的帧率调用 `capture()`。原因是，如果您在网络摄像头上调用 `forEach()`，那么框架会以浏览器的
    JavaScript 引擎可能要求它们从设备获取的速度绘制帧。这通常会比设备的帧速率更快地创建张量，导致重复的帧和浪费的计算。出于类似的原因，不应将网络摄像头迭代器作为
    `model.fit()` 方法的参数传递。
- en: '[Listing 6.17](#ch06ex17) shows the abbreviated prediction loop from the webcam-transfer-learning
    (Pac-Man) example we saw in [chapter 5](kindle_split_016.html#ch05). Note that
    the outer loop will continue for as long as `isPredicting` is true, which is controlled
    by a UI element. Internally, the rate of the loop is moderated by a call to `tf.nextFrame()`,
    which is pinned to the UI’s refresh rate. The following code is from tfjs-examples/webcam-transfer-learning/index.js.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 6.17](#ch06ex17) 展示了来自我们在[第 5 章](kindle_split_016.html#ch05)中看到的网络摄像头迁移学习（Pac-Man）示例中的简化预测循环。请注意，外部循环将持续到
    `isPredicting` 为 true 为止，这由 UI 元素控制。内部上循环的速率由调用 `tf.nextFrame()` 控制，该调用与 UI 的刷新率固定。以下代码来自
    tfjs-examples/webcam-transfer-learning/index.js。'
- en: Listing 6.17\. Using `tf.data.webcam()` in a prediction loop
  id: totrans-310
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6.17\. 在预测循环中使用 `tf.data.webcam()`
- en: '[PRE31]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '***1*** Captures a frame from the webcam and normalizes it between –1 and 1\.
    Returns a batched image (1-element batch) of shape [1, w, h, c].'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 从网络摄像头捕获一帧图像，并将其标准化为 -1 到 1 之间。返回形状为 [1, w, h, c] 的批处理图像（1 个元素的批处理）。'
- en: '***2*** webcam here refers to an iterator returned from tfd.webcam; see init()
    in [listing 6.18](#ch06ex18).'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 这里的网络摄像头指的是从 tfd.webcam 返回的迭代器；请参见 [示例 6.18](#ch06ex18) 中的 init()。'
- en: '***3*** Draws the next frame from the webcam iterator'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 从网络摄像头迭代器绘制下一帧'
- en: '***4*** Waits until the next animation frame before performing another prediction'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 在执行下一个预测之前等待下一个动画帧'
- en: 'One final note: when using the webcam, it is often a good idea to draw, process,
    and discard an image before making predictions on the feed. There are two good
    reasons for this. First, passing the image through the model ensures that the
    relevant model weights have been loaded to the GPU, preventing any stuttering
    slowness on startup. Second, this gives the webcam hardware time to warm up and
    begin sending actual frames. Depending on the hardware, sometimes webcams will
    send blank frames while the device is powering up. See the next listing for a
    snippet showing how this is done in the webcam-transfer-learning example (from
    webcam-transfer-learning/index.js).'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 最后需要注意的是：在使用网络摄像头时，通常最好在进行预测之前绘制、处理并丢弃一张图像。这样做有两个好处。首先，通过模型传递图像可以确保相关的模型权重已经加载到
    GPU 上，防止启动时出现任何卡顿或缓慢。其次，这会给网络摄像头硬件时间来热身并开始发送实际的帧。根据硬件不同，有时在设备启动时，网络摄像头会发送空白帧。在下一节中，您将看到一个片段，展示了如何在网络摄像头迁移学习示例（来自
    webcam-transfer-learning/index.js）中完成这个操作。
- en: Listing 6.18\. Creating a video dataset from `tf.data.webcam()`
  id: totrans-317
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6.18\. 从 `tf.data.webcam()` 创建视频数据集
- en: '[PRE32]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '***1*** Constructor for the video dataset object. The ‘webcam’ element is a
    video element in the HTML document.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 视频数据集对象的构造函数。‘webcam’ 元素是 HTML 文档中的视频元素。'
- en: '***2*** Makes a prediction on the first frame returned from the webcam to make
    sure the model is completely loaded on the hardware'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 对从网络摄像头返回的第一帧进行预测，以确保模型完全加载到硬件上'
- en: '***3*** The value returned from webcam.capture() is a tensor. It must be disposed
    to prevent a leak.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 从 `webcam.capture()` 返回的值是一个张量。必须将其销毁以防止内存泄漏。'
- en: 6.3.3\. Accessing audio data using tf.data.microphone()
  id: totrans-322
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.3\. 使用 tf.data.microphone() 访问音频数据
- en: Along with image data, `tf.data` also includes specialized handling to collect
    audio data from the device microphone. Similar to the webcam API, the microphone
    API creates a lazy iterator allowing the caller to request frames as needed, packaged
    neatly as tensors suitable for consumption directly into a model. The typical
    use case here is to collect frames to be used for prediction. While it’s technically
    possible to produce a training stream using this API, zipping it together with
    the labels would be challenging.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 除了图像数据外，`tf.data` 还包括专门处理从设备麦克风收集音频数据的功能。与网络摄像头 API 类似，麦克风 API 创建了一个惰性迭代器，允许调用者根据需要请求帧，这些帧被整齐地打包成适合直接输入模型的张量。这里的典型用例是收集用于预测的帧。虽然技术上可以使用此
    API 生成训练数据流，但与标签一起压缩它将是具有挑战性的。
- en: '[Listing 6.19](#ch06ex19) shows an example of how to collect one second of
    audio data using the `tf.data.microphone()` API. Note that executing this code
    will trigger the browser to request that the user grant access to the microphone.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 6.19](#ch06ex19) 展示了使用 `tf.data.microphone()` API 收集一秒钟音频数据的示例。请注意，执行此代码将触发浏览器请求用户授权访问麦克风。'
- en: Listing 6.19\. Collecting one second of audio data using the `tf.data.microphone()`
    API
  id: totrans-325
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6.19\. 使用 `tf.data.microphone()` API 收集一秒钟的音频数据
- en: '[PRE33]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '***1*** The microphone configuration allows the user to control some common
    audio parameters. We spell out some of these in the main text.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 麦克风配置允许用户控制一些常见的音频参数。我们在正文中详细说明了其中的一些。'
- en: '***2*** Executes the capture of audio from the microphone'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 执行从麦克风捕获音频。'
- en: '***3*** The audio spectrum data is returned as a tensor of shape [43, 232,
    1].'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 音频频谱数据以形状 [43, 232, 1] 的张量返回。'
- en: '***4*** In addition to the spectrogram data, it is also possible to retrieve
    the waveform data directly. The shape of this data will be [fftSize * numFramesPerSpectrogram,
    1] = [44032, 1].'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 除了频谱图数据之外，还可以直接检索波形数据。此数据的形状将为 [fftSize * numFramesPerSpectrogram,
    1] = [44032, 1]。'
- en: '***5*** Users should call stop() to end the audio stream and turn off the microphone.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 用户应该调用 stop() 来结束音频流并关闭麦克风。'
- en: 'The microphone includes a number of configurable parameters to give users fine
    control over how the fast Fourier transform (FFT) is applied to the audio data.
    Users may want more or fewer frames of frequency-domain audio data per spectrogram,
    or they may be interested in only a certain frequency range of the audio spectrum,
    such as those frequencies necessary for audible speech. The fields in [listing
    6.19](#ch06ex19) have the following meaning:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 麦克风包括一些可配置参数，以使用户对如何将快速傅里叶变换（FFT）应用于音频数据有精细控制。用户可能希望每个频域音频数据的频谱图有更多或更少的帧，或者他们可能只对音频频谱的某个特定频率范围感兴趣，例如对可听到的语音所必需的那些频率。[列表
    6.19](#ch06ex19) 中的字段具有以下含义：
- en: '`sampleRateHz`: `44100`'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sampleRateHz`: `44100`'
- en: The sampling rate of the microphone waveform. This must be exactly 44,100 or
    48,000 and must match the rate specified by the device itself. It will throw an
    error if the specified value doesn’t match the value made available by the device.
  id: totrans-334
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 麦克风波形的采样率。这必须是精确的 44,100 或 48,000，并且必须与设备本身指定的速率匹配。如果指定的值与设备提供的值不匹配，将会抛出错误。
- en: '`fftSize: 1024`'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fftSize: 1024`'
- en: Controls the number of samples used to compute each nonoverlapping “frame” of
    audio. Each frame undergoes an FFT, and larger frames give more frequency sensitivity
    but have less time resolution, as time information *within the frame* is lost.
  id: totrans-336
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制用于计算每个不重叠的音频“帧”的样本数。每个帧都经过 FFT 处理，较大的帧具有更高的频率敏感性，但时间分辨率较低，因为帧内的时间信息 *丢失*了。
- en: Must be a power of 2 between 16 and 8,192, inclusive. Here, `1024` means that
    energy within a frequency band is calculated over a span of about 1,024 samples.
  id: totrans-337
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 必须是介于 16 和 8,192 之间的 2 的幂次方，包括 16 和 8,192。在这里，`1024` 意味着在约 1,024 个样本的范围内计算频率带内的能量。
- en: Note that the highest measurable frequency is equal to half the sample rate,
    or approximately 22 kHz.
  id: totrans-338
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请注意，最高可测频率等于采样率的一半，即约为 22 kHz。
- en: '`columnTruncateLength: 232`'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`columnTruncateLength: 232`'
- en: Controls how much frequency information is retained. By default, each audio
    frame contains `fftSize` points, or 1,024 in our case, covering the entire spectrum
    from 0 to maximum (22 kHz). However, we are typically interested in only the lower
    frequencies. Human speech is generally only up to 5 kHz, and thus we only keep
    the part of the data representing zero to 5 kHz.
  id: totrans-340
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制保留多少频率信息。默认情况下，每个音频帧包含 `fftSize` 点，或者在我们的情况下是 1,024，覆盖从 0 到最大值（22 kHz）的整个频谱。然而，我们通常只关心较低的频率。人类语音通常只有高达
    5 kHz，因此我们只保留表示零到 5 kHz 的数据部分。
- en: Here, 232 = (5 kHz/22 kHz) * 1024.
  id: totrans-341
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这里，232 = (5 kHz/22 kHz) * 1024。
- en: '`numFramesPerSpectrogram: 43`'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numFramesPerSpectrogram: 43`'
- en: The FFT is calculated on a series of nonoverlapping windows (or frames) of the
    audio sample to create a spectrogram. This parameter controls how many are included
    in each returned spectrogram. The returned spectrogram will be of shape `[numFramesPerSpectrogram,
    fftSize, 1]`, or `[43, 232, 1]` in our case.
  id: totrans-343
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: FFT 是在音频样本的一系列不重叠窗口（或帧）上计算的，以创建频谱图。此参数控制每个返回的频谱图中包含多少个窗口。返回的频谱图将具有形状`[numFramesPerSpectrogram,
    fftSize, 1]`，在我们的情况下为`[43, 232, 1]`。
- en: The duration of each frame is equal to the `sampleRate`/`fftSize`. In our case,
    44 kHz * 1,024 is about 0.023 seconds.
  id: totrans-344
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个帧的持续时间等于 `sampleRate`/`fftSize`。在我们的情况下，44 kHz * 1,024 约为 0.023 秒。
- en: There is no delay between frames, so the entire spectrogram duration is about
    43 * 0.023 = 0.98, or just about 1 second.
  id: totrans-345
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 帧之间没有延迟，因此整个频谱图的持续时间约为 43 * 0.023 = 0.98，或者约为 1 秒。
- en: '`smoothingTimeConstant: 0`'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`smoothingTimeConstant: 0`'
- en: How much to blend the previous frame’s data with this frame. It must be between
    0 and 1.
  id: totrans-347
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要将前一帧的数据与本帧混合多少。它必须介于 0 和 1 之间。
- en: '`includeSpectogram: True`'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`includeSpectogram: True`'
- en: If true, the spectrogram will be calculated and made available as a tensor.
    Set this to false if the application does not actually need to calculate the spectrogram.
    This can happen only if the waveform is needed.
  id: totrans-349
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果为真，则会计算并提供声谱图作为张量。如果应用程序实际上不需要计算声谱图，则将其设置为 false。这只有在需要波形时才会发生。
- en: '`includeWaveform: True`'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`includeWaveform: True`'
- en: If true, the waveform is kept and made available as a tensor. This can be set
    to false if the caller will not need the waveform. Note that at least one of `includeSpectrogram`
    and `includeWaveform` must be true. It is an error if they are both false. Here
    we have set them both to true to show that this is a valid option, but in a typical
    application, only one of the two will be necessary.
  id: totrans-351
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果为真，则保留波形并将其作为张量提供。如果调用者不需要波形，则可以将其设置为 false。请注意，`includeSpectrogram` 和 `includeWaveform`
    中至少一个必须为 true。如果它们都为 false，则会报错。在这里，我们将它们都设置为 true，以显示这是一个有效的选项，但在典型应用中，两者中只需要一个。
- en: Similar to the video stream, the audio stream sometimes takes some time to start,
    and data from the device might be nonsense to begin with. Zeros and infinities
    are commonly encountered, but actual values and durations are platform dependent.
    The best solution is to “warm up” the microphone for a short amount of time by
    throwing away the first few samples until the data no longer is corrupted. Typically,
    200 ms of data is enough to begin getting clean samples.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 与视频流类似，音频流有时需要一些时间才能启动，设备的数据可能一开始就是无意义的。常见的是遇到零和无穷大，但实际值和持续时间是依赖于平台的。最佳解决方案是通过丢弃前几个样本来“预热”麦克风一小段时间，直到数据不再损坏为止。通常，200
    毫秒的数据足以开始获得干净的样本。
- en: '6.4\. Your data is likely flawed: Dealing with problems in your data'
  id: totrans-353
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4\. 您的数据可能存在缺陷：处理数据中的问题
- en: It’s nearly a guarantee that there are problems with your raw data. If you’re
    using your own data source, and you haven’t spent several hours with an expert
    combing through the individual features, their distributions, and their correlations,
    then there is a very high chance that there are flaws that will weaken or break
    your machine-learning model. We, the authors of this book, can say this with confidence
    because of our experience with mentoring the construction of many machine-learning
    systems in many domains and building some ourselves. The most common symptom is
    that some model is not converging, or is converging to an accuracy well below
    what is expected. Another related but even more nefarious and difficult-to-debug
    pattern is when the model converges and performs well on the validation and testing
    data but then fails to meet expectations in production. Sometimes there is a genuine
    modeling issue, or a bad hyperparameter, or just bad luck, but, by far, the most
    common root cause for these bugs is that there is a flaw in the data.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎可以保证您的原始数据存在问题。如果您使用自己的数据源，并且您还没有花费数小时与专家一起研究各个特征、它们的分布和它们的相关性，那么很有可能存在会削弱或破坏您的机器学习模型的缺陷。我们，本书的作者，可以自信地说这一点，因为我们在指导多个领域的许多机器学习系统的构建以及构建一些自己的系统方面具有丰富的经验。最常见的症状是某些模型没有收敛，或者收敛到远低于预期精度的水平。另一个相关但更为阴险和难以调试的模式是模型在验证和测试数据上收敛并表现良好，但在生产中未能达到预期。有时确实存在建模问题、糟糕的超参数或只是倒霉，但到目前为止，这些错误的最常见根本原因是数据存在缺陷。
- en: Behind the scenes, all the datasets we’ve used (such as MNIST, iris-flowers,
    and speech-commands) went through manual inspection, pruning of bad examples,
    formatting into a standard and suitable format, and other data science operations
    that we didn’t talk about. Data issues can arise in many forms, including missing
    fields, correlated samples, and skewed distributions. There is such a richness
    and diversity of complexity in working with data, someone could write a book on
    it. In fact, please see *Data Wrangling with JavaScript* by Ashley Davis for a
    fuller exposition!^([[10](#ch06fn10)])
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，我们使用的所有数据集（如 MNIST、鸢尾花和语音命令）都经过了手动检查、剪裁错误示例、格式化为标准和合适的格式以及其他我们没有讨论的数据科学操作。数据问题可以以多种形式出现，包括缺失字段、相关样本和偏斜分布。在处理数据时存在如此丰富和多样的复杂性，以至于有人可以写一本书来讲述。事实上，请参阅
    Ashley Davis 的*用 JavaScript 进行数据整理*，以获取更详尽的阐述！^([[10](#ch06fn10)])
- en: ^(10)
  id: totrans-356
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(10)
- en: ''
  id: totrans-357
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Available from Manning Publications, [www.manning.com/books/data-wrangling-with-javascript](http://www.manning.com/books/data-wrangling-with-javascript).
  id: totrans-358
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 由 Manning Publications 出版，[www.manning.com/books/data-wrangling-with-javascript](http://www.manning.com/books/data-wrangling-with-javascript)。
- en: Data scientists and data managers have become full-time professional roles in
    many companies. The tools these professionals use and best practices they follow
    are diverse and often depend on the specific domain under scrutiny. In this section,
    we will touch on the basics and point to a few tools to help you avoid the heartbreak
    of long model debugging sessions only to find out that it was the data itself
    that was flawed. For a more thorough treatment of data science, we will offer
    references where you can learn more.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家和数据管理人员在许多公司已经成为全职专业角色。这些专业人员使用的工具和他们遵循的最佳实践是多样的，并且通常取决于正在审查的具体领域。在本节中，我们将介绍基础知识，并指出一些工具，帮助你避免长时间模型调试会话的痛苦，只是发现数据本身存在缺陷。对于更全面的数据科学处理，我们将提供你可以进一步学习的参考资料。
- en: 6.4.1\. Theory of data
  id: totrans-360
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.1\. 数据理论
- en: In order to know how to detect and fix *bad* data, we must first know what *good*
    data looks like. Much of the theory underpinning the field of machine learning
    rests on the premise that our data comes from a *probability distribution*. In
    this formulation, our training data consists of a collection of independent *samples*.
    Each sample is described as an (*x*, *y*) pair, where *y* is the part of the sample
    we wish to predict from the *x* part. Continuing this premise, our inference data
    consists of a collection of samples *from the exact same distribution as our training
    data*. The only important difference between the training data and the inference
    data is that at inference time, we do not get to see *y*. We are supposed to estimate
    the *y* part of the sample from the *x* part using the statistical relationships
    learned from the training data.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 为了知道如何检测和修复*不好*的数据，我们必须首先知道*好*的数据是什么样子。机器学习领域的许多理论基础在于我们的数据来自*概率分布*的假设。在这种表述中，我们的训练数据包含一系列独立的*样本*。每个样本都描述为一个(*x*,
    *y*)对，其中*y*是我们希望从*x*部分预测出的部分。继续这个假设，我们的推断数据包含一系列来自*与我们的训练数据完全相同分布的样本*。训练数据和推断数据之间唯一重要的区别是在推断时，我们无法看到*y*。我们应该使用从训练数据中学到的统计关系来估计样本的*y*部分。
- en: There are a number of ways that our real-life data can fail to live up to this
    platonic ideal. If, for instance, our training data and inference data are samples
    from *different* distributions, we say there is dataset *skew*. As a simple example,
    if you are estimating road traffic based on features like weather and time of
    day, and all of your training data comes from Mondays and Tuesdays while your
    test data comes from Saturdays and Sundays, you can expect that the model accuracy
    will be less than optimal. The distribution of auto traffic on weekdays is not
    the same as the distribution of traffic on weekends. As another example, imagine
    we are building a face-recognition system, and we train the system to recognize
    faces based on a collection of labeled data from our home country. We should not
    be surprised to find that the system struggles and fails when used in locations
    with different demographics. Most data-skew issues you’ll encounter in real machine-learning
    settings will be more subtle than these two examples.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的真实数据有许多方面可能与这种理想情况不符。例如，如果我们的训练数据和推断数据来自*不同*的分布，我们称之为数据集*偏斜*。举个简单例子，如果你正在根据天气和时间等特征来估计道路交通情况，而你所有的训练数据都来自周一和周二，而你的测试数据来自周六和周日，你可以预期模型的准确性将不如最佳。工作日的汽车交通分布与周末的不同。另一个例子，想象一下我们正在构建一个人脸识别系统，我们训练系统来识别基于我们本国的一组标记数据。我们不应该感到惊讶的是，当我们在具有不同人口统计数据的地点使用时，系统会遇到困难和失败。你在真实机器学习环境中遇到的大多数数据偏差问题会比这两个例子更微妙。
- en: Another way that skew can sneak into a dataset is if there was some shift during
    data collection. If, for instance, we are taking audio samples to learn speech
    signals, and then halfway through the construction of our training set, our microphone
    breaks, so we purchase an upgrade, we can expect that the second half of our training
    set will have a different noise and audio distribution than our first half. Presumably,
    at inference time, we will be testing using only the new microphone, so skew exists
    between the training and test set as well.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中偏斜的另一种可能性是在数据收集过程中出现了某种变化。例如，如果我们正在采集音频样本来学习语音信号，然后在构建训练集的过程中，我们的麦克风损坏了一半，所以我们购买了升级版，我们可以预期我们训练集的后半部分会与前半部分具有不同的噪声和音频分布。据推测，在推断时，我们将仅使用新麦克风进行测试，因此训练集和测试集之间也存在偏差。
- en: At some level, dataset skew is unavoidable. For many applications, our training
    data necessarily comes from the past, and the data we pass to our application
    necessarily comes from right now. The underlying distribution producing these
    samples is bound to change as cultures, interests, fashions, and other confounding
    factors change with the times. In such a situation, all we can do is understand
    the skew and minimize the impact. For this reason, many machine-learning models
    in production settings are constantly retrained using the freshest available training
    data in an attempt to keep up with continually shifting distributions.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 在某个层面上，数据集的偏斜是无法避免的。对于许多应用程序，我们的训练数据必然来自过去，我们传递给应用程序的数据必然来自现在。产生这些样本的潜在分布必然会随着时间的变化而改变，如文化、兴趣、时尚和其他混淆因素的变化。在这种情况下，我们所能做的就是理解偏斜并尽量减少其影响。因此，生产环境中的许多机器学习模型都会不断使用最新可用的训练数据进行重新训练，以尝试跟上不断变化的分布。
- en: Another way our data samples can fail to live up to the ideal is by failing
    to be independent. Our ideal states that the samples are *independent and identically
    distributed* (IID). But in some datasets, one sample gives clues to the likely
    value of the next. Samples from these datasets are not independent. The most common
    way that sample-to-sample dependence creeps into a dataset is by the phenomenon
    of sorting. For access speed and all sorts of other good reasons, we have been
    trained as computer scientists to organize our data. In fact, database systems
    often organize our data for us without us even trying. As a result, when you stream
    your data from some source, you have to be very careful that the results do not
    have some pattern in their order.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据样本无法达到理想状态的另一种方式是缺乏独立性。我们的理想状态是样本是*独立同分布*（IID）的。但在某些数据集中，一个样本会提供下一个样本的可能值的线索。这些数据集的样本不是独立的。样本与样本的依赖关系最常见的方式是通过排序现象引入到数据集中。为了访问速度和其他各种很好的理由，我们作为计算机科学家已经接受了对数据进行组织的训练。事实上，数据库系统通常会在我们不经意的情况下为我们组织数据。因此，当你从某个源流式传输数据时，必须非常谨慎，确保结果的顺序没有某种模式。
- en: Consider the following hypothetical. We wish to build an estimate of the cost
    of housing in California for an application in real estate. We get a CSV dataset
    of housing prices^([[11](#ch06fn11)]) from around the state, along with relevant
    features, such as the number of rooms, the age of the development, and so on.
    We might be tempted to simply begin training a function from features to price
    right away since we have the data, and we know how to do it. But knowing that
    data often has flaws, we decide to take a look first. We begin by plotting some
    features versus their index in the array, using datasets and Plotly.js. See the
    plots in [figure 6.3](#ch06fig03) for an illustration^([[12](#ch06fn12)]) and
    the following listing (summarized from [https://codepen.io/tfjs-book/pen/MLQOem](https://codepen.io/tfjs-book/pen/MLQOem))
    for how the illustrations were made.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下假设。我们希望为房地产应用程序建立一个对加利福尼亚州住房成本的估计。我们获得了一个来自全州各地的住房价格的CSV数据集^([[11](#ch06fn11)])，以及相关特征，如房间数、开发年龄等。我们可能会考虑立即开始从特征到价格的训练函数，因为我们有数据，而且我们知道如何操作。但是知道数据经常有缺陷，我们决定先看看数据。我们首先使用数据集和Plotly.js绘制一些特征与数组中的索引的图。请参考[图6.3](#ch06fig03)中的插图^([[12](#ch06fn12)])和下面的清单（摘自[https://codepen.io/tfjs-book/pen/MLQOem](https://codepen.io/tfjs-book/pen/MLQOem)）了解如何制作这些插图。
- en: ^(11)
  id: totrans-367
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(11)
- en: ''
  id: totrans-368
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A description of the California housing dataset used here is available from
    the Machine Learning Crash Course at [http://mng.bz/Xpm6](http://mng.bz/Xpm6).
  id: totrans-369
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 关于在这里使用的加利福尼亚住房数据集的描述可在机器学习速成课程的网址[http://mng.bz/Xpm6](http://mng.bz/Xpm6)中找到。
- en: ^(12)
  id: totrans-370
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(12)
- en: ''
  id: totrans-371
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The plots in [figure 6.3](#ch06fig03) were made using the CodePen at [https://codepen.io/tfjs-book/pen/MLQOem](https://codepen.io/tfjs-book/pen/MLQOem).
  id: totrans-372
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 图6.3中的插图是使用CodePen制作的，网址是[https://codepen.io/tfjs-book/pen/MLQOem](https://codepen.io/tfjs-book/pen/MLQOem)。
- en: Figure 6.3\. Plots of four dataset features vs. the sample index. Ideally, in
    a clean IID dataset, we would expect the sample index to give us no information
    about the feature value. We see that for some features, the distribution of y
    values clearly depends on x. Most egregiously, the “longitude” feature seems to
    be sorted by the sample index.
  id: totrans-373
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.3. 四个数据集特征与样本索引的图。理想情况下，在一个干净的IID数据集中，我们预期样本索引对特征值没有任何信息。我们可以看到，对于某些特征，y值的分布显然取决于x。尤其令人震惊的是，“经度”特征似乎是按照样本索引排序的。
- en: '![](06fig02_alt.jpg)'
  id: totrans-374
  prefs: []
  type: TYPE_IMG
  zh: '![](06fig02_alt.jpg)'
- en: Listing 6.20\. Building a plot of a feature vs. index using tfjs-data
  id: totrans-375
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.20\. 使用 tfjs-data 构建特征与索引的绘图
- en: '[PRE34]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '***1*** Takes the first 1,000 samples and collects their values and their indices.
    Don’t forget await, or your plot will (probably) be empty!'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 获取前 1,000 个样本并收集它们的值和它们的索引。别忘了等待，否则你的图表可能会是空的！'
- en: 'Imagine we were to construct a train-test split with this dataset where we
    took the first 500 samples for training and the remainder for testing. What would
    happen? It appears from this analysis that we would be training with data from
    one geographic area and testing with data from another. The Longitude panel in
    [figure 6.3](#ch06fig03) shows the crux of the problem: the first samples are
    from a higher longitude (more westerly) than any of the others. There is still
    probably plenty of signal in the features, and the model would “work” somewhat,
    but it would not be as accurate or high-quality as if our data were truly IID.
    If we didn’t know better, we might spend days or weeks playing with different
    models and hyperparameters before we figured out what was wrong and looked at
    our data!'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，如果我们使用这个数据集构建了一个训练-测试分割，其中我们取前 500 个样本进行训练，剩余的用于测试。会发生什么？从这个分析中可以看出，我们将用来自一个地理区域的数据进行训练，而用来自另一个地理区域的数据进行测试。图
    6.3 中的经度面板显示了问题的关键所在：第一个样本来自一个经度更高（更向西）的地方。特征中可能仍然有大量信号，模型会“工作”一定程度，但准确性或质量不如如果我们的数据真正是
    IID。如果我们不知道更好的话，我们可能会花费几天甚至几周时间尝试不同的模型和超参数，直到找出问题所在并查看我们的数据！
- en: What can we do to clean this up? Fixing this particular issue is pretty simple.
    In order to remove the relationship between the data and the index, we can just
    shuffle our data into a random order. However, there is something we must watch
    out for here. TensorFlow.js datasets have a built-in shuffle routine, but it is
    a *streaming window* shuffle routine. This means that samples are randomly shuffled
    within a window of fixed size but no further. This is out of necessity because
    TensorFlow.js datasets stream data, and they may stream an unlimited number of
    samples. In order to completely shuffle a never-ending data source, you first
    need to wait until it is done.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以做些什么来清理这个问题？修复这个特定问题非常简单。为了消除数据和索引之间的关系，我们可以将数据随机洗牌成随机顺序。然而，这里有一些需要注意的地方。TensorFlow.js
    数据集有一个内置的洗牌例程，但它是一个*流式窗口*洗牌例程。这意味着样本在固定大小的窗口内随机洗牌，但没有更进一步。这是因为 TensorFlow.js 数据集流式传输数据，它们可能传输无限数量的样本。为了完全打乱一个永无止境的数据源，你首先需要等待直到它完成。
- en: So, can we make do with this streaming window shuffle for our longitude feature?
    Certainly if we know the size of the datasets (17,000 in this case), we can specify
    the window to be larger than the entire dataset, and we are all set. In the limit
    of very large window sizes, windowed shuffling and our normal exhaustive shuffling
    are identical. If we don’t know how large our dataset is, or the size is prohibitively
    large (that is, we can’t hold the whole thing at once in a memory cache), we may
    have to make do with less.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们能否使用这个经度特征的流式窗口洗牌？当然，如果我们知道数据集的大小（在这种情况下是 17,000），我们可以指定窗口大小大于整个数据集，然后一切都搞定了。在非常大的窗口大小极限下，窗口化洗牌和我们的常规穷举洗牌是相同的。如果我们不知道我们的数据集有多大，或者大小是不可行的大（即，我们无法一次性在内存缓存中保存整个数据集），我们可能不得不凑合一下。
- en: '[Figure 6.4](#ch06fig04), created with [https://codepen.io/tfjs-book/pen/JxpMrj](https://codepen.io/tfjs-book/pen/JxpMrj),
    illustrates what happens when we shuffle our data with four different window sizes
    using `tf.data` `.Dataset`’s `shuffle()` method:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6.4](#ch06fig04)，使用 [https://codepen.io/tfjs-book/pen/JxpMrj](https://codepen.io/tfjs-book/pen/JxpMrj)
    创建，说明了使用 `tf.data` 的 `.Dataset` 的 `shuffle()` 方法进行四种不同窗口大小的数据洗牌时会发生什么：'
- en: '[PRE35]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Figure 6.4\. Four plots of longitude vs. the sample index for four shuffled
    datasets. The shuffle window size is different for each, increasing from 10 to
    6,000 samples. We see that even at a window size of 250, there is still a strong
    relationship between the index and the feature value. There are more large values
    near the beginning. It isn’t until we are using a shuffle window size almost as
    large as the dataset that the data’s IID nature is nearly restored.
  id: totrans-383
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.4\. 四个不同的洗牌数据集的经度与样本索引的四个图。每个洗牌窗口大小不同，从 10 增加到 6,000 个样本。我们可以看到，即使在窗口大小为
    250 时，索引和特征值之间仍然存在强烈的关系。在开始附近有更多的大值。直到我们使用的洗牌窗口大小几乎与数据集一样大时，数据的 IID 特性才几乎恢复。
- en: '![](06fig03_alt.jpg)'
  id: totrans-384
  prefs: []
  type: TYPE_IMG
  zh: '![](06fig03_alt.jpg)'
- en: We see that the structural relationship between the index and the feature value
    remains clear even for relatively large window sizes. It isn’t until the window
    size is 6,000 that it looks to the naked eye like the data can now be treated
    as IID. So, is 6,000 the right window size? Was there a number between 250 and
    6,000 that would have worked? Is 6,000 still not enough to catch distributional
    issues we aren’t seeing in these illustrations? The right approach here is to
    shuffle the entire dataset by using a `windowSize` >= the number of samples in
    the dataset. For datasets where this is not possible due to memory limitations,
    time constraints, or possibly unlimited datasets, you must put on your data scientist
    hat and examine the distribution to determine an appropriate window size.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，即使对于相对较大的窗口大小，索引与特征值之间的结构关系仍然很明显。直到窗口大小达到6,000时，我们才能用肉眼看到数据现在可以视为IID。那么，6,000是正确的窗口大小吗？在250和6,000之间是否有一个数字可以起作用？6,000仍然不足以捕捉我们在这些示例中没有看到的分布问题吗？在这里的正确方法是使用一个`windowSize`
    >= 数据集中的样本数量来对整个数据集进行洗牌。对于由于内存限制、时间限制或可能是无限数据集而无法进行此操作的数据集，您必须戴上数据科学家的帽子并检查分布以确定适当的窗口大小。
- en: 6.4.2\. Detecting and cleaning problems with data
  id: totrans-386
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.2\. 检测和清理数据问题
- en: 'In the previous section, we went through how to detect and fix one type of
    data problem: sample-to-sample dependence. Of course, this is just one of the
    many types of problems that can arise in data. A full treatment of all the types
    of things that can go wrong is far beyond the scope of this book, because there
    are as many things that can go wrong with data as there are things that can go
    wrong with code. Let’s go through a few here, though, so you will recognize the
    problems when you see them and know what terms to search for to find more information.'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们已经讨论了如何检测和修复一种类型的数据问题：样本间的依赖关系。当然，这只是数据可能出现的许多问题类型之一。由于出现数据问题的种类和代码出错的种类一样多，因此对所有可能出错的事情进行全面处理远远超出了本书的范围。不过，我们还是要在这里介绍一些问题，这样当你遇到问题时就能识别出它们，并且知道要搜索哪些术语以获取更多信息。
- en: Outliers
  id: totrans-388
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 离群值
- en: Outliers are samples in our dataset that are very unusual and somehow do not
    belong to the underlying distribution. For instance, if we were working with a
    dataset of health statistics, we might expect the typical adult’s weight to be
    between roughly 40 and 130 kilograms. If, in our dataset, 99.9% of our samples
    were in this range, but every so often we encountered a nonsensical sample report
    of 145,000 kg, or 0 kg, or worse, NaN,^([[13](#ch06fn13)]) we would consider these
    samples as outliers. A quick online search reveals that there are many opinions
    about the right way to deal with outliers. Ideally, we would have very few outliers
    in our training data, and we would know how to find them. If we could write a
    program to reject outliers, we could remove them from our dataset and go on training
    without them. Of course, we would want to also trigger that same logic at inference
    time; otherwise we would introduce skew. In this case, we could use the same logic
    to inform the user that their sample constitutes an outlier to the system, and
    that they must try something different.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 离群值是我们数据集中非常不寻常的样本，而且在某种程度上不符合基础分布。例如，如果我们正在处理健康统计数据集，我们可能会期望典型成年人的体重在大约40至130公斤之间。如果我们的数据集中有99.9%的样本在这个范围内，但偶尔我们遇到了一些荒谬的样本报告，如145,000公斤，或者0公斤，或者更糟糕的NaN，我们会将这些样本视为离群值。快速的在线搜索显示，关于处理离群值的正确方式有很多不同的观点。理想情况下，我们的训练数据中应该只有很少的离群值，并且我们应该知道如何找到它们。如果我们能编写一个程序来拒绝离群值，我们就可以将它们从我们的数据集中移除，然后继续训练而不受它们的影响。当然，我们也希望在推断时触发相同的逻辑；否则，我们会引入偏差。在这种情况下，我们可以使用相同的逻辑通知用户，他们的样本构成了系统的离群值，并且他们必须尝试其他方法。
- en: ^(13)
  id: totrans-390
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(13)
- en: ''
  id: totrans-391
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Ingesting a value of NaN in our input features would propagate that NaN throughout
    our model.
  id: totrans-392
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在我们的输入特征中摄取NaN值将在我们的模型中传播该NaN值。
- en: Another common way to deal with outliers at the feature level is to clamp values
    by providing a reasonable minimum and maximum. In our case, we might replace weight
    with
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 在特征级别处理离群值的另一种常见方法是通过提供合理的最小值和最大值来夹紧数值。在我们的案例中，我们可能会用以下方式替换体重
- en: '[PRE36]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'In such circumstances, it is also a good idea to add a new feature, indicating
    that the outlier value has been replaced. This way, an original value of 40 kg
    can be distinguished from a value of –5 kg that was clamped to 40 kg, giving the
    network the opportunity to learn the relationship between the outlier status and
    the target, if such a relationship exists:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，还有一个好主意是添加一个新的特征，指示异常值已被替换。这样，原始值40公斤就可以与被夹为40公斤的值-5公斤区分开来，给网络提供了学习异常状态与目标之间关系的机会，如果这样的关系存在的话：
- en: '[PRE37]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Missing data
  id: totrans-397
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 缺失数据
- en: Frequently, we are confronted with situations in which some samples are missing
    some features. This can happen for any number of reasons. Sometimes the data comes
    from hand-entered forms, and some fields are just skipped. Sometimes sensors were
    broken or down at the time of data collection. For some samples, perhaps some
    features just don’t make sense. For example, what is the most recent sale price
    of a home that has never been sold? Or what is the telephone number of a person
    without a telephone?
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 经常，我们面临一些样本缺少某些特征的情况。这可能发生在任何数量的原因。有时数据来自手工录入的表单，有些字段被跳过了。有时传感器在数据收集时损坏或失效。对于一些样本，也许一些特征根本没有意义。例如，从未出售过的房屋的最近销售价格是多少？或者没有电话的人的电话号码是多少？
- en: As with outliers, there are many ways to address the problem of missing data,
    and data scientists have different opinions about which techniques are appropriate
    in which situations. Which technique is best depends on a few considerations,
    including whether the likelihood of the feature to be missing depends on the value
    of the feature itself, or whether the “missingness” can be predicted from other
    features in the sample. [Info box 6.3](#ch06sb03) outlines a glossary of categories
    of missing data.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 与异常值一样，有许多方法可以解决缺失数据的问题，数据科学家对于在哪些情况下使用哪些技术有不同的意见。哪种技术最好取决于一些考虑因素，包括特征缺失的可能性是否取决于特征本身的值，或者“缺失”是否可以从样本中的其他特征预测出来。[信息框
    6.3](#ch06sb03)概述了缺失数据类别的术语表。
- en: '|  |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Categories of missing data**'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺失数据的类别**'
- en: 'Missing at random (MAR):'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 随机缺失（MAR）：
- en: The likelihood of the feature to be missing does not depend on the hidden missing
    value, but it may depend on some other observed value.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征缺失的可能性并不取决于隐藏的缺失值，但它可能取决于其他一些观察到的值。
- en: 'Example: If we had an automated visual system recording automobile traffic,
    it might record, among other things, license plate numbers and time of day. Sometimes,
    if it’s dark, we are unable to read the license plate. The plate’s presence does
    not depend on the license plate value, but it may depend on the (observed) time-of-day
    feature.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例：如果我们有一个自动化的视觉系统记录汽车交通，它可能记录，除其他外，车牌号码和时间。有时，如果天黑了，我们就无法读取车牌。车牌的存在与车牌值无关，但可能取决于（观察到的）时间特征。
- en: Missing completely at random (MCAR)
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 完全随机缺失（MCAR）
- en: The likelihood of the feature to be missing does not depend on the hidden missing
    value or any of the observed values.
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征缺失的可能性不取决于隐藏的缺失值或任何观察到的值。
- en: 'Example: Cosmic rays interfere with our equipment and sometimes corrupt values
    from our dataset. The likelihood of corruption does not depend on the value stored
    or on other values in the dataset.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例：宇宙射线干扰我们的设备，有时会破坏我们数据集中的值。破坏的可能性不取决于存储的值或数据集中的其他值。
- en: Missing not at random (MNAR)
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 非随机缺失（MNAR）
- en: The likelihood of the feature to be missing depends on the hidden value, given
    the observed data.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征缺失的可能性取决于给定观察数据的隐藏值。
- en: 'Example: A personal weather station keeps track of all sorts of statistics,
    like air pressure, rainfall, and solar radiation. However, when it snows, the
    solar radiation meter does not take a signal.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例：个人气象站跟踪各种统计数据，如气压、降雨量和太阳辐射。然而，下雪时，太阳辐射计不会发出信号。
- en: '|  |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: When data is missing from our training set, we have to apply some corrections
    to be able to turn the data into a fixed-shape tensor, which requires a value
    in every cell. There are four important techniques for dealing with the missing
    data.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据在我们的训练集中缺失时，我们必须应用一些修正才能将数据转换为固定形状的张量，这需要每个单元格中都有一个值。处理缺失数据的四种重要技术。
- en: The simplest technique, if the training data is plentiful and the missing fields
    are rare, is to discard training samples that have missing data. However, be aware
    that this can introduce a bias in your trained model. To see this plainly, imagine
    a problem in which there is missing data much more commonly from the positive
    class than the negative class. You would end up learning an incorrect likelihood
    of the classes. Only if your missing data is MCAR are you completely safe to discard
    samples.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的技术是，如果训练数据丰富且缺失字段很少，就丢弃具有缺失数据的训练样本。但是，请注意，这可能会在您的训练模型中引入偏差。为了明显地看到这一点，请想象一个问题，其中从正类缺失数据的情况远比从负类缺失数据的情况要常见得多。您最终会学习到错误的类别可能性。只有当您的缺失数据是MCAR时，您才完全可以放心地丢弃样本。
- en: Listing 6.21\. Handling missing features by removing the data
  id: totrans-414
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第 6.21 列表。通过删除数据处理缺失的特征
- en: '[PRE38]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '***1*** Keeps only those elements whose value of ‘featureName’ is truthy: that
    is, not 0, null, undefined, NaN, or an empty string'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 仅保留值为''truthy''的元素：即不为0、null、未定义、NaN或空字符串时'
- en: Another technique for dealing with missing data is to fill the missing data
    in with some value, also known as *imputation*. Common imputation techniques include
    replacing missing numeric feature values with the mean, median, or mode value
    of that feature. Missing categorical features may be replaced with the most common
    value for that feature (also mode). More sophisticated techniques involve building
    predictors for the missing features from the available features and using those.
    In fact, using neural networks is one of the “sophisticated techniques” for the
    imputation of missing data. The downside of using imputation is that the learner
    is not aware that the feature was missing. If there is information in the missingness
    about the target variable, it will be lost in imputation.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 处理缺失数据的另一种技术是用某个值填充缺失数据，也称为*插补*。常见的插补技术包括用该特征的均值、中位数或众数替换缺失的数值特征值。缺失的分类特征可以用该特征的最常见值（也是众数）替换。更复杂的技术包括从可用特征构建缺失特征的预测器，并使用它们。事实上，使用神经网络是处理缺失数据的“复杂技术”之一。使用插补的缺点是学习器不知道特征缺失了。如果缺失信息与目标变量有关，则在插补中将丢失该信息。
- en: Listing 6.22\. Handling missing features with imputation
  id: totrans-418
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第 6.22 列表。使用插补处理缺失的特征
- en: '[PRE39]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '***1*** Function to calculate the value to use for imputation. Remember to
    only include valid values when computing the mean.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 用于计算用于插补的值的函数。在计算均值时，请记住只包括有效值。'
- en: '***2*** Both undefined and null values are considered to be missing here. Some
    datasets might use sentinel values like –1 or 0 to indicate missingness. Be sure
    to look at your data!'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 这里将未定义和null值都视为缺失。一些数据集可能使用哨兵值，如 -1 或 0 来表示缺失。请务必查看您的数据！'
- en: '***3*** Note that this will return NaN when all the data is missing.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 请注意，当所有数据都丢失时，这将返回NaN。'
- en: '***4*** Function to conditionally update a row if the value at featureName
    is missing'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 如果featureName处的值缺失，则有条件地更新行的函数'
- en: '***5*** Uses the tf.data.Dataset map() method to map the replacement over all
    the elements'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 使用tf.data.Dataset map()方法将替换映射到所有元素上'
- en: Sometimes missing values are replaced with a *sentinel value*. For instance,
    a missing body weight value might be replaced with a –1, indicating that no weight
    was taken. If this appears to be the case with your data, take care to handle
    the sentinel value *before* clamping it as an outlier (for example, based on our
    prior example, replacing this –1 with 40 kg).
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 有时缺失值会被替换为*哨兵值*。例如，缺失的体重值可能会被替换为 -1，表示未称重。如果您的数据看起来是这种情况，请注意在将其视为异常值之前*先*处理哨兵值（例如，根据我们先前的示例，用40
    kg替换这个 -1）。
- en: Conceivably, if there is a relationship between the missingness of the feature
    and the target to be predicted, the model may be able to use the sentinel value.
    In practice, the model will spend some of its computational resources learning
    to distinguish when the feature is used as a value and when it is used as an indicator.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 可以想象，如果缺失特征与要预测的目标之间存在关系，模型可能能够使用哨兵值。实际上，模型将花费部分计算资源来学习区分特征何时用作值，何时用作指示器。
- en: Perhaps the most robust way to manage missing data is to both use imputation
    to fill in a value and add a second indicator feature to communicate to the model
    when that feature was missing. In this case, we would replace the missing body
    weight with a guess and also add a new feature `weight_missing`, which is 1 when
    weight was missing and 0 when it was provided. This allows the model to leverage
    the missingness, if valuable, and also to not conflate it with the actual value
    of the weight.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 管理缺失数据可能最稳健的方法是既使用插补来填补一个值，又添加一个第二个指示特征，以便在该特征缺失时通知模型。在这种情况下，我们将缺失的体重替换为一个猜测值，同时添加一个新特征`weight_missing`，当体重缺失时为1，提供时为0。这使得模型可以利用缺失情况，如果有价值的话，并且不会将其与体重的实际值混淆。
- en: Listing 6.23\. Adding a feature to indicate missingness
  id: totrans-428
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.23\. 添加一个特征来指示缺失
- en: '[PRE40]'
  id: totrans-429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '***1*** Function to add a new feature to each row, which is 1 if the feature
    is missing and 0 otherwise'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 将一个新特征添加到每一行的函数，如果特征缺失则为1，否则为0'
- en: '***2*** Uses the tf.data.Dataset map() method to map the additional feature
    into each row'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 使用 tf.data.Dataset map() 方法将附加特征映射到每一行'
- en: Skew
  id: totrans-432
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 偏斜
- en: Earlier in this chapter, we described the concept of skew, a difference in distribution
    from one dataset to another. It is one of the major problems machine-learning
    practitioners face when deploying trained models to production. Detecting skew
    involves modeling the distributions of the datasets and comparing them to see
    if they match. A simple way to quickly look at the statistics of your dataset
    is to use a tool like Facets ([https://pair-code.github.io/facets/](https://pair-code.github.io/facets/)).
    See [figure 6.5](#ch06fig05) for a screenshot. Facets will analyze and summarize
    your datasets to allow you to look at per-feature distributions, which will help
    you to quickly suss out problems with different distributions between your datasets.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的前面，我们描述了偏斜的概念，即一个数据集与另一个数据集之间的分布差异。当将训练好的模型部署到生产中时，这是机器学习实践者面临的主要问题之一。检测偏斜涉及对数据集的分布进行建模并比较它们是否匹配。快速查看数据集统计信息的简单方法是使用像
    Facets（[https://pair-code.github.io/facets/](https://pair-code.github.io/facets/)）这样的工具。参见[图
    6.5](#ch06fig05)以查看截图。Facets 将分析和总结您的数据集，以便您查看每个特征的分布，这将帮助您快速发现数据集之间的不同分布问题。
- en: Figure 6.5\. A screenshot of Facets showing per-feature value distributions
    for the training and test split of the UC Irvine Census Income datasets (see [http://archive.ics.uci.edu/ml/datasets/Census+Income](http://archive.ics.uci.edu/ml/datasets/Census+Income)).
    This dataset is the default loaded at [https://pair-code.github.io/facets/](https://pair-code.github.io/facets/),
    but you can navigate to the site and upload your own CSVs to compare. This view
    is known as Facets Overview.
  id: totrans-434
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.5\. Facets 的截图显示了 UC Irvine Census Income 数据集的训练集和测试集的每个特征值分布情况（参见[http://archive.ics.uci.edu/ml/datasets/Census+Income](http://archive.ics.uci.edu/ml/datasets/Census+Income)）。该数据集是默认加载在[https://pair-code.github.io/facets/](https://pair-code.github.io/facets/)上的，但您可以导航到该网站并上传自己的
    CSV 文件进行比较。这个视图被称为 Facets 概览。
- en: '![](06fig04_alt.jpg)'
  id: totrans-435
  prefs: []
  type: TYPE_IMG
  zh: '![](06fig04_alt.jpg)'
- en: A simple, rudimentary skew-detection algorithm may calculate the mean, median,
    and variance of each feature and check whether any differences across datasets
    are within acceptable bounds. More sophisticated methods may attempt to predict,
    given samples, which dataset they are from. Ideally, this should not be possible
    since they are from the same distribution. If it is possible to predict whether
    a data point is from training or testing, this is a sign of skew.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单、基础的偏斜检测算法可能会计算每个特征的均值、中位数和方差，并检查数据集之间的任何差异是否在可接受范围内。更复杂的方法可能会尝试根据样本预测它们来自哪个数据集。理想情况下，这应该是不可能的，因为它们来自相同的分布。如果能够预测数据点是来自训练还是测试，这是偏斜的迹象。
- en: Bad strings
  id: totrans-437
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 错误字符串
- en: Very commonly, categorical data is provided as string-valued features. For instance,
    when users access your web page, you might keep logs of which browser was used
    with values like `FIREFOX`, `SAFARI`, and `CHROME`. Typically, before ingesting
    these values into a deep-learning model, the values are converted into integers
    (either through a known vocabulary or by hashing), which are then mapped into
    an *n*-dimensional vector space (See [section 9.2.3](kindle_split_021.html#ch09lev2sec5)
    on word embeddings). A common problem is where the strings from one dataset have
    different formatting from the strings in a different dataset. For instance, the
    training data might have `FIREFOX`, while at service time, the model receives
    `FIREFOX\n`, with the newline character included, or `"FIREFOX"`, with quotes.
    This is a particularly insidious form of skew and should be handled as such.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 非常常见的情况是，分类数据是以字符串形式提供的特征。例如，当用户访问您的网页时，您可能会记录使用的浏览器，值为`FIREFOX`、`SAFARI`和`CHROME`。通常，在将这些值送入深度学习模型之前，这些值会被转换为整数（通过已知词汇表或哈希），然后映射到一个*n*维向量空间（见[9.2.3节](kindle_split_021.html#ch09lev2sec5)关于词嵌入）。一个常见的问题是，一个数据集中的字符串与另一个数据集中字符串的格式不同。例如，训练数据可能有`FIREFOX`，而在服务时间，模型收到`FIREFOX\n`，包括换行符，或者`"FIREFOX"`，包括引号。这是一个特别阴险的形式的偏差，应该像处理偏差一样处理。
- en: Other things to watch out for in your data
  id: totrans-439
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 其他需要注意的数据内容。
- en: 'In addition to the problems called out in the previous sections, here are a
    few more things to be aware of when feeding your data to a machine-learning system:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前面提到的问题之外，在将数据提供给机器学习系统时，还有一些需要注意的事项：
- en: '*Overly unbalanced data*—If there are some features that take the same value
    for nearly every sample in your dataset, you may consider getting rid of them.
    It is very easy to overfit with this type of signal, and deep-learning methods
    do not handle very sparse data well.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*过度不平衡的数据*——如果有一些特征在你的数据集中几乎每个样本都取相同的值，你可以考虑将它们去掉。这种类型的信号很容易过拟合，而深度学习方法对非常稀疏的数据不能很好地处理。'
- en: '*Numeric/categorical distinction*—Some datasets will use integers to represent
    elements of an enumerated set, and this can cause problems when the rank order
    of these integers is meaningless. For instance, if we have an enumerated set of
    music genres, like `ROCK`, `CLASSICAL`, and so on, and a vocabulary that mapped
    these values to integers, it is important that we handle the values like enumerated
    values when we pass them into the model. This means encoding the values using
    one-hot or embedding (see [chapter 9](kindle_split_021.html#ch09)). Otherwise,
    these numbers will be interpreted as floating-point values, suggesting spurious
    relationships between terms based on the numeric distance between their encodings.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数值/分类区别*——一些数据集将使用整数表示列举集合的元素，而当这些整数的等级顺序没有意义时，这可能会导致问题。例如，如果我们有一个音乐类型的列举集合，比如`ROCK`，`CLASSICAL`等，和一个将这些值映射到整数的词汇表，很重要的是，当我们把这些值传递到模型中时，我们要像处理列举值一样处理这些值。这意味着使用one-hot或嵌入（见[第9章](kindle_split_021.html#ch09)）对值进行编码。否则，这些数字将被解释为浮点数值，根据它们的编码之间的数字距离，可能会提示虚假的术语之间的关系。'
- en: '*Massive scale differences*—This was mentioned earlier, but it bears repeating
    in this section on what can go wrong with data. Watch out for numeric features
    that have large-scale differences. They can lead to instability in training. In
    general, it’s best to z-normalize (normalize the mean and standard deviation of)
    your data before training. Just be sure to use the same preprocessing at serving
    time as you did during training. You can see an example of this in the tensorflow/tfjs-examples
    iris example, as we explored in [chapter 3](kindle_split_014.html#ch03).'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*规模巨大的差异*——这是之前提到过的，但在这一节中重申，可以出现数据错误的情况。要注意具有大量差异的数值特征。它们可能导致训练不稳定。通常情况下，在训练之前最好对数据进行z标准化（规范化均值和标准差）。只需要确保在服务时间中使用与训练期间相同的预处理即可。你可以在tensorflow/tfjs-examples
    iris example中看到一个例子，我们在[第3章](kindle_split_014.html#ch03)中探讨过。'
- en: '*Bias, security, and privac**y*—Obviously, there is much more to responsible
    machine-learning development than can be covered in a book chapter. It is critical,
    if you are developing machine-learning solutions, that you spend the time to familiarize
    yourself with at least the basics of the best practices for managing bias, security,
    and privacy. A good place to get started is the page on responsible AI practices
    at [https://ai.google/education/responsible-ai-practices](https://ai.google/education/responsible-ai-practices).
    Following these practices is just the right thing to do to be a good person and
    a responsible engineer—obviously important goals in and of themselves. In addition,
    paying careful attention to these issues is a wise choice from a purely selfish
    perspective, as even small failures of bias, security, or privacy can lead to
    embarrassing systemic failures that quickly lead customers to look elsewhere for
    more reliable solutions.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*偏见、安全和隐私*——显然，在负责任的机器学习开发中，有很多内容远远超出了一本书章的范围。如果你正在开发机器学习解决方案，花时间了解至少管理偏见、安全和隐私的基本最佳实践至关重要。一个好的起点是谷歌在
    [https://ai.google/education/responsible-ai-practices](https://ai.google/education/responsible-ai-practices)
    上的负责任 AI 实践页面。遵循这些实践只是做一个好人和一个负责任的工程师的正确选择——显然是重要的目标本身。此外，仔细关注这些问题也是一个明智的选择，因为即使是偏见、安全或隐私的小失误也可能导致令人尴尬的系统性故障，迅速导致客户寻找更可靠的解决方案。'
- en: In general, you should aim to spend time convincing yourself that your data
    is as you expect it to be. There are many tools to help you do this, from notebooks
    like Observable, Jupyter, Kaggle Kernel, and Colab, to graphical UI tools like
    Facets. See [figure 6.6](#ch06fig06) for another way to explore your data in Facets.
    Here, we use Facets’ plotting feature, known as Facets Dive, to view points from
    the State Universities of New York (SUNY) dataset. Facets Dive allows the user
    to select columns from the data and visually express each field in a custom way.
    Here, we’ve used the drop-down menus to use the Longitude1 field as the x-position
    of the point, the Latitude1 field as the y-position of the point, the City string
    field as the name of the point, and the Undergraduate Enrollment as the color
    of the point. We expect the latitude and longitude, plotted on the 2D plane, to
    reveal a map of New York state, and indeed that’s what we see. The correctness
    of the map can be verified by comparing it to SUNY’s web page at [www.suny.edu/attend/visit-us/campus-map/](http://www.suny.edu/attend/visit-us/campus-map/).
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，你应该花时间确信你的数据符合你的预期。有许多工具可以帮助你做到这一点，从像 Observable、Jupyter、Kaggle Kernel
    和 Colab 这样的笔记本，到图形界面工具如 Facets。在 Facets 中，看 [图 6.6](#ch06fig06) ，还有另一种探索数据的方式。在这里，我们使用
    Facets 的绘图功能，也就是 Facets Dive，来查看纽约州立大学（SUNY）数据集中的点。Facets Dive 允许用户从数据中选择列，并以自定义的方式直观地表达每个字段。在这里，我们使用下拉菜单将
    Longitude1 字段用作点的 x 位置，将 Latitude1 字段用作点的 y 位置，将 City 字符串字段用作点的名称，并将 Undergraduate
    Enrollment 用作点的颜色。我们期望在二维平面上绘制的纬度和经度能揭示出纽约州的地图，而确实是我们所看到的。地图的正确性可以通过将其与 SUNY 的网页
    [www.suny.edu/attend/visit-us/campus-map/](http://www.suny.edu/attend/visit-us/campus-map/)
    进行比较来验证。
- en: Figure 6.6\. Another screenshot of Facets, this time exploring the State of
    New York, Campuses dataset from the data-csv example. Here, we see the Facets
    Dive view that allows you to explore the relationships between different features
    of a dataset. Each point shown is a data point from the dataset, and here we have
    it configured so that the point’s x-position is set to the Latitude1 feature,
    the y-position is the Longitude1 feature, the color is related to the Undergraduate
    Enrollment feature, and the words on the front are set to the City feature, which
    contains, for each data point, the name of the city the university campus is in.
    We can see from this visualization a rough outline of the state of New York, with
    Buffalo in the west and New York in the southeast. Apparently, the city of Selden
    contains one of the largest campuses by undergraduate enrollment.
  id: totrans-446
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.6\. Facets 的另一张截图，这次探索的是数据-csv 示例中的纽约州校园数据集。在这里，我们看到 Facets Dive 视图，它允许您探索数据集的不同特征之间的关系。每个显示的点都是数据集中的一个数据点，这里我们将点的
    x 位置设置为 Latitude1 特征，y 位置是 Longitude1 特征，颜色与 Undergraduate Enrollment 特征相关，前面的字是设置为
    City 特征的，其中包含每个数据点所在的城市的名称。从这个可视化中，我们可以看到纽约州的大致轮廓，西边是布法罗，东南边是纽约。显然，Selden 市包含了一个根据本科生注册人数计算的最大校园之一。
- en: '![](06fig05_alt.jpg)'
  id: totrans-447
  prefs: []
  type: TYPE_IMG
  zh: '![](06fig05_alt.jpg)'
- en: 6.5\. Data augmentation
  id: totrans-448
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5\. 数据增强
- en: So, we’ve collected our data, we’ve connected it to a `tf.data.Dataset` for
    easy manipulation, and we’ve scrutinized it and cleaned it of problems. What else
    can we do to help our model succeed?
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们已经收集了我们的数据，我们将其连接到一个`tf.data.Dataset`以便进行简单操作，我们仔细检查并清理了其中的问题。还有什么其他方法可以帮助我们的模型成功呢？
- en: Sometimes, the data you have isn’t enough, and you wish to expand the dataset
    programmatically, creating new examples by making small changes to existing data.
    For instance, recall the MNIST hand-written digit-classification problem from
    [chapter 4](kindle_split_015.html#ch04). MNIST contains 60,000 training images
    of 10 hand-written digits, or 6,000 per digit. Is that enough to learn all the
    types of flexibility we want for our digit classifier? What happens if someone
    draws a digit too large or small? Or rotated slightly? Or skewed? Or with a thicker
    or thinner pen? Will our model still understand?
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，你拥有的数据不足，希望通过程序化地扩展数据集，通过对现有数据进行微小更改来创建新的示例。例如，回顾一下[第 4 章](kindle_split_015.html#ch04)中的
    MNIST 手写数字分类问题。MNIST 包含 60,000 个训练图像，共 10 个手写数字，每个数字 6,000 个。这足以学习我们想要为我们的数字分类器提供的所有类型的灵活性吗？如果有人画了一个太大或太小的数字会发生什么？或者稍微旋转了一下？或者有偏斜？或者用笔写得粗或细了？我们的模型还会理解吗？
- en: If we take an MNIST sample digit and alter the image by moving the digit one
    pixel to the left, the semantic label of the digit doesn’t change. The 9 shifted
    to the left is still a 9, but we have a new training example. This type of programmatically
    generated example, created from mutating an actual example, is known as a *pseudo-example*,
    and the process of adding pseudo-examples to the data is known as *data augmentation*.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们拿一个 MNIST 样本数字，并将图像向左移动一个像素，那么数字的语义标签不会改变。向左移动的 9 仍然是一个 9，但我们有了一个新的训练示例。这种从实际示例变异生成的程序化示例称为*伪示例*，将伪示例添加到数据的过程称为*数据增强*。
- en: Data augmentation takes the approach of generating more training data from existing
    training samples. In the case of image data, various transformations such as rotating,
    cropping, and scaling often yield believable-looking images. The purpose is to
    increase the diversity of the training data in order to benefit the generalization
    power of the trained model (in other words, to mitigate overfitting), which is
    especially useful when the size of the training dataset is small.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强采取的方法是从现有的训练样本中生成更多的训练数据。对于图像数据，各种变换如旋转、裁剪和缩放通常会产生看起来可信的图像。其目的是增加训练数据的多样性，以增加训练模型的泛化能力（换句话说，减轻过拟合），这在训练数据集大小较小时特别有用。
- en: '[Figure 6.7](#ch06fig07) shows data augmentation applied to an input example
    consisting of an image of a cat, from a dataset of labeled images. The data is
    augmented by applying rotations and skew in such a way that the label of the example,
    that is, “CAT” does not change, but the input example changes significantly.'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6.7](#ch06fig07)展示了应用于由猫图像组成的输入示例的数据增强，来自带标签的图像数据集。通过应用旋转和偏斜等方式增强数据，使得示例的标签即“CAT”不变，但输入示例发生了显著变化。'
- en: Figure 6.7\. Generation of cat pictures via random data augmentation. A single
    labeled example can yield a whole family of training samples by providing random
    rotations, reflections, translations, and skews. Meow.
  id: totrans-454
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.7。通过随机数据增强生成猫图片。单个标记示例可以通过提供随机旋转、反射、平移和偏斜来产生整个训练样本族。喵。
- en: '![](06fig06_alt.jpg)'
  id: totrans-455
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.6](06fig06_alt.jpg)'
- en: If you train a new network using this data-augmentation configuration, the network
    will never see the same input twice. But the inputs it sees are still heavily
    intercorrelated because they come from a small number of original images—you can’t
    produce new information, you can only remix existing information. As such, this
    may not be enough to completely get rid of overfitting. Another risk of using
    data augmentation is that the training data is now less likely to match the distribution
    of the inference data, introducing skew. Whether the benefits of the additional
    training pseudo-examples outweigh the costs of skew is application-dependent,
    and it’s something you may just need to test and experiment with.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用这个数据增强配置来训练一个新的网络，那么这个网络永远不会看到相同的输入两次。但它所看到的输入仍然存在很强的相互关联性，因为它们来自少量原始图像——你无法产生新的信息，只能重新混合现有信息。因此，这可能不足以完全消除过拟合。使用数据增强的另一个风险是，训练数据现在不太可能与推断数据的分布匹配，引入了偏差。额外训练伪例的好处是否超过偏差成本取决于应用程序，并且这可能只是需要测试和实验的内容。
- en: '[Listing 6.24](#ch06ex24) shows how you can include data augmentation as a
    `dataset.map()` function, injecting allowable transformations into your dataset.
    Note that augmentation should be applied per example. It’s also important to see
    that augmentation should *not* be applied to the validation or testing set. If
    we test on augmented data, then we will have a biased measure of the power of
    our model because the augmentations will not be applied at inference time.'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码清单6.24](#ch06ex24)展示了如何将数据增强作为`dataset.map()`函数包括在内，将允许的变换注入到数据集中。请注意，增强应逐个样本应用。还需要注意的是，不应将增强应用于验证或测试集。如果在增强数据上进行测试，则在推断时不会应用增强，因此会对模型的能力产生偏见。'
- en: Listing 6.24\. Training a model on a dataset with data augmentation
  id: totrans-458
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**6**\. 在使用数据增强对数据集进行训练的示例中的代码清单。'
- en: '[PRE41]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '***1*** The augmentation function takes a sample in {image, label} format and
    returns a new, perturbed sample in the same format.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 数据增强函数以{图像，标签}格式的样本作为输入，并返回相同格式下的新样本。'
- en: '***2*** Assume that randomRotate, randomSkew, and randomMirror are defined
    elsewhere by some library. The amount to rotate, skew, and so on is generated
    randomly for each call. The augmentation should depend only on the features, not
    the label of the sample.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 假设随机旋转、随机扭曲和随机镜像在其他地方由某个库定义。旋转、扭曲等的数量在每次调用时随机生成。数据增强只应依赖于特征，而不是样本的标签。'
- en: '***3*** This function returns two tf.data.Datasets, each with element type
    {image, label}.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 该函数返回两个tf.data.Datasets，每个元素类型为{image, label}。'
- en: '***4*** The augmentation is applied to the individual elements before batching.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 增强应用于单个元素的批处理之前。'
- en: '***5*** We fit the model on the augmented dataset.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 我们在增强的数据集上拟合模型。'
- en: '***6*** IMPORTANT! Do not apply augmentation to the validation set. Repeat
    is called on the validationData here since the data won’t loop automatically.
    Only 10 batches are taken per validation measurement, as configured.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 重要！不要对验证集应用数据增强。在这里对验证数据进行重复操作，因为数据不会自动循环。每次验证测量只会取10批数据，根据配置。'
- en: Hopefully, this chapter convinced you of the importance of understanding your
    data before throwing machine-learning models at it. We talked about out-of-the-box
    tools such as Facets, which you can use to examine your datasets and thereby deepen
    your understanding of them. However, when you need a more flexible and customized
    visualization of your data, it becomes necessary to write some code to do that
    job. In the next chapter, we will teach you the basics of tfjs-vis, a visualization
    module maintained by the authors of TensorFlow.js that can support such data-visualization
    use cases.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 希望本章能让你深刻理解在将机器学习模型应用于数据之前了解数据的重要性。我们介绍了Facets等开箱即用的工具，您可以使用它们来检查数据集并深入了解它们。然而，当您需要更灵活和自定义的数据可视化时，需要编写一些代码来完成这项工作。在下一章中，我们将教您tfjs-vis的基础知识，这是由TensorFlow.js的作者维护的可支持此类数据可视化用例的可视化模块。
- en: Exercises
  id: totrans-467
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习
- en: Extend the simple-object-detection example from [chapter 5](kindle_split_016.html#ch05)
    to use `tf.data .generator()` and `model.fitDataset()` instead of generating the
    full dataset up front. What advantages are there to this structure? Does performance
    meaningfully improve if the model is provided a much larger dataset of images
    to train from?
  id: totrans-468
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将第5章的简单物体检测示例扩展，使用`tf.data .generator()`和`model.fitDataset()`代替提前生成完整数据集。此结构有何优势？如果为模型提供更大的图像数据集进行训练，性能会有明显提升吗？
- en: Add data augmentation to the MNIST example by adding small shifts, scales, and
    rotations to the examples. Does this help in performance? Does it make sense to
    validate and test on the data stream with augmentation, or is it more proper to
    test only on “real” natural examples?
  id: totrans-469
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过对示例中的MNIST示例添加小偏移、缩放和旋转来添加数据增强。这是否有助于性能？在使用数据流进行验证和测试时，是否对数据流进行增强更合适，还是只在“真实”的自然样本上测试？
- en: Try plotting some of the features from some of the datasets we’ve used in other
    chapters using the techniques in [section 6.4.1](#ch06lev2sec8). Does the data
    meet the expectations of independence? Are there outliers? What about missing
    values?
  id: totrans-470
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用[第6.4.1节](#ch06lev2sec8)中的技术绘制其他章节中使用的一些数据集的一些特征。数据符合独立性的预期吗？有异常值吗？还有缺失值吗？
- en: Load some of the CSV datasets we’ve discussed here into the Facets tool. What
    features look like they could cause problems? Any surprises?
  id: totrans-471
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将我们在这里讨论过的一些CSV数据集导入Facets工具中。哪些特征看起来可能会引起问题？有什么意外情况吗？
- en: Consider some of the datasets we’ve used in earlier chapters. What sorts of
    data augmentation techniques would work for those?
  id: totrans-472
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑一些我们在早期章节中使用的数据集。哪些数据增强技术适用于那些数据？
- en: Summary
  id: totrans-473
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 概要
- en: Data is a critical force powering the deep-learning revolution. Without access
    to large, well-organized datasets, most deep-learning applications could not happen.
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据是推动深度学习革命的关键力量。没有大型、组织良好的数据集，大多数深度学习应用都无法实现。
- en: TensorFlow.js comes packaged with the `tf.data` API to make it easy to stream
    large datasets, transform data in various ways, and connect them to models for
    training and prediction.
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow.js内置了`tf.data` API，使得流式传输大型数据集、以各种方式转换数据，并将其连接到模型进行训练和预测变得简单。
- en: 'There are several ways to build a `tf.data.Dataset` object: from a JavaScript
    array, from a CSV file, or from a data-generating function. Building a dataset
    that streams from a remote CSV file can be done in one line of JavaScript.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有几种方法可以构建`tf.data.Dataset`对象：从JavaScript数组、从CSV文件，或者从数据生成函数。从远程CSV文件流式传输数据集可以在一行JavaScript代码中完成。
- en: '`tf.data.Dataset` objects have a chainable API that makes it easy and convenient
    to shuffle, filter, batch, map, and perform other operations commonly needed in
    a machine-learning application.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tf.data.Dataset`对象具有可链接的API，使得在机器学习应用中常见的洗牌、过滤、批处理、映射和执行其他操作变得简单而方便。'
- en: '`tf.data.Dataset` accesses data in a lazy streaming fashion. This makes working
    with large remote datasets simple and efficient but comes at the cost of working
    with asynchronous operations.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tf.data.Dataset`以延迟流式传输的方式访问数据。这使得处理大型远程数据集变得简单高效，但付出的代价是处理异步操作。'
- en: '`tf.Model` objects can be trained directly from a `tf.data.Dataset` using their
    `fitDataset()` method.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tf.Model`对象可以直接使用其`fitDataset()`方法从`tf.data.Dataset`进行训练。'
- en: Auditing and cleaning data requires time and care, but it is a required step
    for any machine-learning system you intend to put to practical use. Detecting
    and managing problems like skew, missing data, and outliers at the data-processing
    stage will end up saving debugging time during the modeling stage.
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 审计和清理数据需要时间和关注，但这是任何你打算投入实际应用的机器学习系统都必不可少的一步。在数据处理阶段检测和管理偏差、缺失数据和异常值等问题，最终会节省建模阶段的调试时间。
- en: Data augmentation can be used to expand the dataset to include programmatically
    generated pseudo-examples. This can help the model to cover known invariances
    that were underrepresented in the original dataset.
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据增强可以用于扩展数据集，包括程序生成的伪示例。这可以帮助模型覆盖原始数据集中未充分表示的已知不变性。
