- en: 3 Customizing a Gaussian process with the mean and covariance functions
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 使用均值和协方差函数自定义高斯过程
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖内容
- en: Controlling the expected behavior of a GP using mean functions
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用均值函数控制 GP 的预期行为
- en: Controlling the smoothness of a GP using covariance functions
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用协方差函数控制 GP 的平滑度
- en: Learning the optimal hyperparameters of a GP using gradient descent
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用梯度下降学习 GP 的最优超参数
- en: In chapter 2, we saw that the mean and covariance functions are the two core
    components of a Gaussian process (GP). Even though we used the zero mean and the
    RBF covariance function when implementing our GP, you can choose from many options
    when it comes to these two components.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 2 章中，我们了解到均值和协方差函数是高斯过程 (GP) 的两个核心组成部分。即使在实现我们的 GP 时使用了零均值和 RBF 协方差函数，你在这两个组成部分上可以选择很多选项。
- en: By going with a specific choice for either the mean or the covariance function,
    we are effectively specifying prior knowledge for our GP. Incorporating prior
    knowledge into prediction is something we need to do with any Bayesian model,
    including GPs. Although I say we need to do it, being able to incorporate prior
    knowledge into a model is always a good thing, especially under settings in which
    data acquisition is expensive, like BayesOpt.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 通过选择均值或协方差函数的特定选择，我们实际上在为我们的 GP 模型指定先验知识。将先验知识纳入预测是我们在任何贝叶斯模型中都需要做的事情，包括 GP。虽然我说我们需要这样做，但将先验知识纳入模型中总是一件好事，特别是在数据获取昂贵的情况下，比如贝叶斯优化。
- en: For example, in weather forecasting, if we’d like to estimate the temperature
    of a typical day in January in Missouri, we won’t have to do any complex calculations
    to be able to guess that the temperature will be fairly low. At the other end
    of the spectrum, we can make a good guess that a summer day in California will
    be relatively hot. These rough estimates may be used as initial first guesses
    in a Bayesian model, which are, in essence, the model’s prior knowledge. If we
    didn’t have these first guesses, we would have to do more involved modeling to
    produce predictions.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在天气预报中，如果我们想要估计一月份在密苏里的典型天气的温度，我们不必进行任何复杂的计算就能猜到温度会相当低。在温度较高的加州的夏天，我们也可以猜到天气会相对炎热。这些粗略的估计可以用作贝叶斯模型中的初始猜测，实质上就是模型的先验知识。如果我们没有这些初始猜测，我们将需要进行更复杂的建模来进行预测。
- en: As we learn in this chapter, incorporating prior knowledge into a GP can drastically
    change the model’s behavior, which can lead to better predictive performance (and,
    eventually, more effective decision-making). We should only use no prior knowledge
    when we have absolutely no good guess about the behavior of the function; otherwise,
    this equates to wasting information.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章中学到的，将先验知识纳入 GP 中可以大大改变模型的行为，从而带来更好的预测性能（最终带来更有效的决策）。只有当我们对函数的行为没有任何好的猜测时，才应该不使用先验知识；否则，这等同于浪费信息。
- en: In this chapter, we discuss the different options when it comes to the mean
    and covariance functions and how they affect the resulting GP model. Unlike in
    chapter 2, we take a hands-on approach here and revolve our discussions around
    code implementation in Python. By the end of this chapter, we develop a pipeline
    for selecting appropriate mean and covariance functions as well as optimizing
    their hyperparameters.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论均值和协方差函数的不同选项，以及它们如何影响生成的 GP 模型。与第 2 章不同，我们在这里采用了一种实践的方法，并围绕 Python
    中的代码实现展开讨论。到本章结束时，我们将开发一种选择适当的均值和协方差函数以及优化它们的超参数的流程。
- en: 3.1 The importance of priors in Bayesian models
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 先验概率对于贝叶斯模型的重要性
- en: 'Question: Why can’t you seem to change some people’s mind? Answer: because
    of their priors. To show how important priors are in a Bayesian model, consider
    the following scenario.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：为什么你似乎无法改变一些人的想法？答案：因为他们的先验概率。为了说明先验概率在贝叶斯模型中有多么重要，请考虑以下情景。
- en: 'Let’s say you are hanging out with your friends, Bob and Alice, at a carnival,
    and you are talking to someone who claims they are a psychic. The way they allow
    you to test this claim is via the following procedure: you and your friends each
    think of a number between 0 and 9, and the “psychic” will tell you what number
    each of you is thinking of. You can repeat this process however many times you’d
    like.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你和你的朋友鲍勃和爱丽丝在嘉年华上闲逛，你正在和一个声称自己是灵媒的人交谈。他们允许你通过以下程序来测试这一说法：你和你的朋友们每个人都想一个 0
    到 9 之间的数字，而“灵媒”将告诉你们每个人在想什么数字。你可以重复这个过程任意次数。
- en: Now, all three of you are curious about this supposed psychic, and you decide
    to conduct this test 100 times. Amazingly, after these 100 tests, the supposed
    psychic at the carnival correctly guesses the numbers you each think of. However,
    after you are done, you each have different reactions, as seen in figure 3.1.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你们三个人都对这个所谓的灵媒感到好奇，你们决定进行这个测试 100 次。令人惊讶的是，在这 100 次测试之后，嘉年华上的这位自称的灵媒准确地猜出了你们每个人心里想的数字。然而，测试结束后，你们每个人的反应却各不相同，如图
    3.1 所示。
- en: '![](../../OEBPS/Images/03-01.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/03-01.png)'
- en: Figure 3.1 Reaction among your group of friends after seeing a person correctly
    guess a secret number 100 times. Each person reached a different conclusion due
    to their prior belief.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1 显示了你们朋友中看到一个人连续猜对一个秘密数字 100 次后的反应。由于他们的先验信念不同，每个人得出了不同的结论。
- en: Further reading on Bayes’ theorem
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 关于贝叶斯定理的进一步阅读
- en: If you need to refresh your memory, feel free to return to figure 2.2, in which
    we studied Bayes’ theorem. We only sketch this out on a high level in this book,
    but I recommend chapters 1 and 2 of Will Kurt’s *Bayesian Statistics the Fun Way*
    (No Starch Press, 2019) if you’d like to examine this process in greater depth.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要恢复记忆，请随时返回图 2.2，我们在那里研究了贝叶斯定理。在本书中，我们只是大致概述了这一过程，但我建议你阅读威尔·库尔特（Will Kurt）的《贝叶斯统计学的有趣方法》（No
    Starch Press，2019）的第 1 和第 2 章，如果你想深入研究这个过程。
- en: 'How can all three of you observe the same event (the person at the carnival
    guessing correctly 100 times) but arrive at different conclusions? To answer this
    question, consider the process of updating one’s belief using Bayes’ theorem:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 你们三个人怎么可能观察同样的事件（嘉年华上的人连续猜对 100 次）却得出不同的结论呢？要回答这个问题，考虑使用贝叶斯定理更新信念的过程：
- en: Each person starts out with a specific prior probability that the person is
    a psychic.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个人都从特定的先验概率开始，认为这个人是个灵媒。
- en: Then, you each observe the event that they guess your number correctly once.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你们每个人都观察到他们猜对你的数字一次的事件。
- en: 'You each then compute the likelihood terms. First, the likelihood their guess
    is correct, given that they’re indeed a psychic, is exactly 1, since a true psychic
    can always pass this test. Second, the likelihood their guess is correct, given
    that they’re *not* a psychic, is 1 out of 10\. This is because each time, you
    are randomly choosing a number between 0 and 9, so any guess among these 10 options
    has an equal chance of being correct: 1 out of 10.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你们每个人都计算可能性项。首先，鉴于他们确实是灵媒，他们的猜测正确的可能性是完全的 1，因为真正的灵媒总是能通过这个测试。其次，鉴于他们 *不是*
    灵媒，他们的猜测正确的可能性是 10 个中的 1，因为每次，你们都是在 0 到 9 之间随机选择一个数字，所以这 10 个选项中的任何一个猜测都有相等的可能性：10
    分之 1。
- en: Finally, you update your belief by computing the posterior probability that
    this person is not a psychic by combining your prior with these likelihood terms.
    Specifically, this posterior will be proportional to the prior and the first likelihood
    term *multiplied* together.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，通过将先验与这些可能性项相结合来计算这个人不是灵媒的后验概率，你们更新了自己的信念。具体来说，这个后验概率将与先验和第一个可能性项 *相乘* 成比例。
- en: You then repeat this process 100 times, each time using the posterior probability
    of the previous iteration as your prior for the current iteration.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你们然后重复这个过程 100 次，每次使用前一次迭代的后验概率作为当前迭代的先验概率。
- en: What is important on a high level here is that after each test, you and your
    friends’ posterior belief that this person is a psychic *never decreases*, since
    that statement doesn’t agree with the data you observe. Specifically, figure 3.2
    shows the progressive posterior probability of each person in your group as a
    function of how many tests the “psychic” at the carnival has passed.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这里高层次的重要性在于，经过每次测试，你和你的朋友对这个人是灵媒的后验信念 *从未减少*，因为这个陈述与你观察到的数据不符。具体来说，图 3.2 显示了你们小组中每个人的渐进后验概率，作为“嘉年华中的灵媒”通过了多少次测试的函数。
- en: '![](../../OEBPS/Images/03-02.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/03-02.png)'
- en: Figure 3.2 Progressive posterior probability that the woman at the carnival
    is a psychic as a function of the number of successful guesses. This posterior
    never decreases but behaves differently depending on the initial prior.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2 进展后验概率，即在成功猜测次数的函数中，卡尼瓦尔上的女士是一个灵媒的概率。这个后验概率从不下降，但根据初始先验的不同而行为也不同。
- en: As we can see, each of the three curves either increases or stays flat—none
    of them actually decreases, since a decreasing probability of being a psychic
    doesn’t match the 100 successful guesses in a row. But why do the three curves
    look so different? As you might have already guessed, the starting position of
    the curve—that is, each person’s prior probability that the woman is a psychic—is
    the cause.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，三条曲线中的每一条要么增加，要么保持不变——没有一条曲线实际下降，因为一个减少的成为灵媒的概率不符合100次连续成功猜测。但为什么这三条曲线看起来如此不同呢？你可能已经猜到了，曲线的起始位置——即每个人认为女士是灵媒的先验概率——是原因。
- en: 'In Bob’s case, in the left panel, he started out with a relatively high prior
    that the person is a psychic: 1%. Bob is a believer. As he observes more and more
    data that agrees with this belief, his posterior probability increases more and
    more.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在鲍勃的案例中，在左侧面板上，他最初的先验相对较高，认为那个人是灵媒的概率为1%。鲍勃是一个信徒。随着他观察到越来越多与这一信念一致的数据，他的后验概率也越来越高。
- en: 'In your own case, in the middle, being a skeptic, you started with a much lower
    prior: 1 in 10 raised to the 14th power. However, since your observations do suggest
    the woman is a psychic, your posterior probability also increases as more data
    comes in, reaching 1% at the end.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在你自己的情况下，在中间，作为怀疑论者，你的先验要低得多：10的14次方中的1。然而，由于你的观察确实表明女士是灵媒，随着更多数据的输入，你的后验概率也增加到最后的1%。
- en: Alice’s case, on the right, on the other hand, is different. From the start,
    she didn’t believe psychics are real, so she assigned exactly zero probability
    to her prior. Now, remember that according to Bayes’ theorem, the posterior probability
    is proportional to the prior multiplied by the likelihood. Since Alice’s prior
    is exactly zero, this multiplication in the Bayesian update will always produce
    another zero.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，艾丽斯的情况则不同。从一开始，她不相信灵媒是真实存在的，因此她给了她的先验概率精确的零。现在，请记住，根据贝叶斯定理，后验概率与先验概率乘以似然性成比例。由于艾丽斯的先验概率恰好为零，贝叶斯更新中的这种乘法将总是产生另一个零。
- en: Since Alice started out with zero probability, even after a successful test,
    this probability stays the same. After one correct guess, Alice’s posterior is
    zero. After two guesses, it’s still zero. After all 100 correct guesses, this
    number is still zero. Everything is consistent with the Bayesian update rule,
    but because Alice’s prior doesn’t allow for the possibility that psychics exist,
    no amount of data could convince her otherwise.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 由于艾丽斯最初的概率为零，在一次成功测试后，这个概率保持不变。一次正确的猜测后，艾丽斯的后验概率为零。两次猜测后，仍为零。所有100次正确的猜测后，这个数字仍然是零。所有的一切都符合贝叶斯更新规则，但由于艾丽斯的先验不允许灵媒存在的可能性，任何数据都无法说服她相反。
- en: 'This highlights an important aspect of Bayesian learning—our prior determines
    how learning is done (see figure 3.3):'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这突显了贝叶斯学习的一个重要方面——我们的先验确定了学习的方式（见图3.3）：
- en: Bob’s prior is fairly high, so by the end of the 100 tests, he’s completely
    convinced the person is psychic.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鲍勃的先验相当高，因此在100次测试结束时，他完全相信那个人是灵媒。
- en: You, on the other hand, are more skeptical, in that your initial prior is much
    lower than Bob’s. This means it would take more evidence for you to arrive at
    a high posterior.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '另一方面，你更加怀疑，你的初始先验比鲍勃低得多。这意味着你需要更多的证据才能得出高后验。 '
- en: Alice’s complete disregard of the possibility, denoted by her zero prior, prevents
    her posterior probability from changing from zero.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 艾丽斯对可能性的完全忽视，用她的零先验表示，使她的后验概率保持在零。
- en: '![](../../OEBPS/Images/03-03.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/03-03.png)'
- en: Figure 3.3 How each person’s prior belief is updated by the same data. Compared
    to Bob’s, your prior is much lower and increases more slowly. Alice’s prior is
    0 and stays at 0 throughout.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3 各人的先验信念如何被相同的数据更新。与鲍勃相比，你的先验更低，增长速度更慢。艾丽斯的先验为0，始终保持为0。
- en: 'Although the claim in our example is about the event that someone is a psychic,
    the same Bayesian update procedure applies to all situations in which we have
    a probabilistic belief about some event and frequently update it in light of data.
    In fact, this is actually the reason we can’t seem to be able to change someone’s
    mind sometimes, even in the face of overwhelming evidence: because they start
    out with zero prior probability, and nothing will update the posterior to anything
    other than zero.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们的例子中的主张是关于某个人是否是通灵者的事件，但是同样的贝叶斯更新过程适用于所有情形，其中我们对某个事件有概率信念，并经常根据数据进行更新。事实上，这正是我们有时似乎无法改变某个人的想法的原因：因为他们的先验概率为零，没有任何东西可以将后验概率更新为非零。
- en: This discussion is philosophically interesting, in that it shows that to be
    able to convince someone of something, they need to at least entertain the idea
    by assigning non-zero prior probability to that event. More relevant to our topic,
    the example shows the importance of having good prior knowledge for a Bayesian
    model. As we have said, we specify a GP’s prior knowledge with the mean and covariance
    functions. Each choice leads to a different behavior in the GP’s prediction.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 从哲学角度来看，这个讨论非常有趣，因为它表明，为了能够说服某个人做某事，他们需要至少想到一种可能性，即指定事件的非零先验概率。更具体地说，这个例子说明了对贝叶斯模型拥有良好的先前知识的重要性。正如我们所说，我们通过均值和协方差函数来指定高斯过程的先验知识。每个选择都会在高斯过程的预测中产生不同的行为。
- en: 3.2 Incorporating what you already know into a GP
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 将先前已知内容并入高斯过程
- en: In this section, we identify situations in which specifying prior knowledge
    in a GP is important. This discussion motivates our discussions in the remaining
    portion of this chapter.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将确定在高斯过程中指定先前知识的重要性。这个讨论为我们在本章剩余部分的讨论下了动力。
- en: A prior GP may start out having a constant mean and CI everywhere. This GP then
    gets updated to smoothly interpolate the observed data points, as figure 3.4 denotes.
    That is, the mean prediction exactly goes through the data points, and the 95%
    CI vanishes in those regions.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 先验高斯过程可能一开始具有恒定的平均值和CI。如图3.4所示，该高斯过程然后被更新为平稳地内插观测到的数据点。也就是说，均值预测恰好穿过数据点，并且95%的CI在那些区域消失。
- en: '![](../../OEBPS/Images/03-04.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/03-04.png)'
- en: Figure 3.4 Comparison between a prior and a posterior GP. The prior GP contains
    prior information about the objective function, while the posterior GP combines
    that information with actual observations.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4 先验高斯过程和后验高斯过程的比较。先验高斯过程包含有关目标函数的先前信息，而后验高斯过程将该信息与实际观测值相结合。
- en: The prior GP in figure 3.4 assumes nothing about the objective function we are
    modeling. That’s why this GP’s mean prediction is zero everywhere. In many cases,
    however, even though we don’t know the exact form of our objective function, there
    are aspects of the objective that we *do* know.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4中的先验高斯过程不对我们正在建模的目标函数做出任何假设。这就是为什么这个高斯过程的平均预测值在任何地方都是零。但是，在许多情况下，即使我们不知道目标函数的确切形式，我们也了解目标的某些方面。
- en: 'Take the following, for example:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 以以下为例：
- en: When modeling the accuracy of a model in a hyperparameter tuning application,
    we know that the range of the objective function is between 0 and 1.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在超参数调整应用程序中模拟模型准确性时，我们知道目标函数的范围在0到1之间。
- en: In our housing price example from section 2.1, the function values (the prices)
    are strictly positive and should increase when a desirable property of the house,
    such as living area, increases.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在我们从第2.1节中的住房价格示例中，函数值（价格）严格为正，当房屋的理想属性（例如生活区）增加时，应该增加。
- en: Again, in the housing example, the function value is more sensitive to some
    features than others. For instance, the price of a house increases more quickly
    as a function of the number of stories than as a function of living area—one extra
    story increases the price of a house more than one extra square foot of living
    area.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在住房示例中，函数值对某些特征更加敏感。例如，与生活区面积的函数相比，房屋价格随着层数的增加更快——多一层楼会增加房屋价格，而多一个平方英尺的生活区则不会。
- en: This kind of information is exactly the prior knowledge that we’d like to represent
    with a GP, and one of the biggest advantages of using a GP is that we have many
    ways to incorporate prior knowledge. Doing so helps close the gap between the
    GP surrogate and the actual objective function it models, which will also more
    effectively guide optimization down the line.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这种信息正是我们希望用 GP 表示的先验知识，使用 GP 的最大优势之一是我们有许多方法来融入先验知识。这样做有助于缩小 GP 代理与其建模的实际目标函数之间的差距，这也将更有效地引导后续的优化。
- en: Incorporating prior knowledge with a GP
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 用 GP 结合先验知识
- en: We incorporate prior knowledge by selecting appropriate mean and covariance
    functions for our GP and setting their parameters’ values. Specifically
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过选择适当的平均和协方差函数以及设置它们的参数值来融入先验知识。特别是
- en: The mean function defines the expected behavior of the objective function.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均函数定义了目标函数的预期行为。
- en: The covariance function defines the structure of the objective, or, more specifically,
    the relationship between any pair of data points and how quickly and smoothly
    the objective function changes across its domain.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 协方差函数定义了目标的结构，或者更具体地说，定义了任意一对数据点之间的关系，以及目标函数在其定义域内变化的速度和平滑程度。
- en: Each of the preceding choices leads to drastically different behavior in the
    resulting GP. For example, a linear mean function will lead to a linear behavior
    in the GP’s predictions, while a quadratic mean function will lead to a quadratic
    behavior. By using different parameters in the covariance function, we can also
    control for the variability of our GP.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的每个选择都会导致生成的 GP 的行为发生 drastical 不同。例如，线性平均函数将导致 GP 预测中的线性行为，而二次平均函数将导致二次行为。通过在协方差函数中使用不同的参数，我们还可以控制
    GP 的变异性。
- en: 3.3 Defining the functional behavior with the mean function
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 使用平均函数定义函数行为
- en: First, we cover the mean function of a GP, which defines the expected behavior
    of the GP, or what we believe the function looks like on average, across all possible
    scenarios of the function. This, as we will see, helps us specify any prior knowledge
    related to the function’s general behavior and shape. The code we use throughout
    this section is included in CH03/01 - Mean functions.ipynb. To make our discussions
    concrete, we use a housing price dataset with five data points in table 3.1.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们介绍了 GP 的平均函数，它定义了 GP 的预期行为，或者说我们相信函数在所有可能的情况下平均情况下的样子。正如我们将要看到的，这有助于我们指定与函数的一般行为和形状相关的任何先验知识。本节中使用的代码包含在
    CH03/01 - Mean functions.ipynb 中。为了使我们的讨论具体化，我们使用了一个房价数据集，其中表 3.1 中有五个数据点。
- en: Table 3.1 Example training dataset. The prediction target (price) increases
    as a function of the feature (living area).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3.1 示例训练数据集。预测目标（价格）随特征（居住面积）的增加而增加。
- en: '| Living area (in squared feet times 1000) | Price (in dollars times 100,000)
    |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 居住面积（以1000平方英尺为单位） | 价格（以10万美元为单位） |'
- en: '| --- | --- |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 0.5 | 0.0625 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 0.5 | 0.0625 |'
- en: '| 1 | 0.25 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.25 |'
- en: '| 1.5 | 0.375 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 1.5 | 0.375 |'
- en: '| 3 | 2.25 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 2.25 |'
- en: '| 4 | 4 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 4 |'
- en: In this dataset, the function values we model are the housing prices, which
    are strictly positive and increase with larger values of living area. These properties
    make intuitive sense, and even without knowing the prices of unobserved houses,
    we know for sure that those unseen prices also have these properties.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个数据集中，我们建模的函数值是房价，它们是严格正的，并且随着居住面积的增加而增加。这些性质是直观的，即使不知道未观察到的房屋的价格，我们也确信这些未见价格也具有这些性质。
- en: 'Our goal here is to incorporate these properties into our mean function, as
    they describe how we expect the function to behave. Before we jump right to modeling,
    we first write a helper function that takes in a GP model (along with its likelihood
    function) and visualizes its predictions in the range from 0 to 10 (that is, a
    10,000-square-foot living area). This is implemented as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是将这些性质纳入我们的平均函数中，因为它们描述了我们对函数行为的预期。在我们开始建模之前，我们首先编写一个辅助函数，该函数接受一个 GP 模型（以及其似然函数），并在范围从
    0 到 10（即，一个 10,000 平方英尺的居住面积）内可视化其预测。实现如下：
- en: '[PRE0]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Computes predictions
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 计算预测
- en: ❷ Plots the mean line
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 绘制平均线
- en: ❸ Plots the 95% CI region
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 绘制 95% CI 区域
- en: We saw how this code works in section 2.4.3, and now we are putting it into
    a convenient function. And with that, we are ready to implement our GP models
    and see how our choices affect the predictions being produced.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第 2.4.3 节中看到了这段代码是如何工作的，现在我们将其放入一个方便的函数中。有了这个，我们就准备好实现我们的 GP 模型，并看看我们的选择如何影响所产生的预测。
- en: 3.3.1 Using the zero mean function as the base strategy
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.1 使用零均值函数作为基本策略
- en: The simplest form of the mean is a constant function at zero. In the absence
    of data, this function will produce zero as its default prediction. The zero mean
    function is used when there is no extra information about the objective function
    that we may incorporate into the GP as prior knowledge.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 均值的最简单形式是一个在零处的常数函数。在没有数据的情况下，此函数将产生零作为其默认预测。当没有关于我们可能将其作为先验知识合并到 GP 中的目标函数的额外信息时，将使用零均值函数。
- en: 'A GP with a zero mean function is implemented as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 使用零均值函数实现的 GP 如下所示：
- en: '[PRE1]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Constant mean function with a default value of zero
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 默认值为零的常数均值函数
- en: Remember from section 2.4.2 that to build a GP model with GPyTorch, we implement
    the `__init__()` and `forward()` methods. In the first method, we initialize our
    mean and covariance functions; in the second, we push the input `x` through these
    functions and return the corresponding multivariate Gaussian distribution.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，根据第 2.4.2 节的内容，要构建一个使用 GPyTorch 的 GP 模型，我们实现 `__init__()` 和 `forward()`
    方法。在第一个方法中，我们初始化我们的均值和协方差函数；在第二个方法中，我们将输入 `x` 通过这些函数，并返回相应的多元高斯分布。
- en: Note Instead of the `gpytorch.means.ZeroMean` class in our implementation from
    section 2.4.2, we are using the `gpytorch.means.ConstantMean` class to initialize
    our mean function. However, this constant mean function has a default value of
    zero, so effectively, we are still implementing the same GP model. While these
    two choices lead to identical models for now, in this chapter, we show how `gpytorch.means.ConstantMean`
    allows us to adjust the constant mean value to obtain a better model shortly.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在我们从第 2.4.2 节中的实现中，我们使用 `gpytorch.means.ZeroMean` 类，而在这里，我们使用 `gpytorch.means.ConstantMean`
    类来初始化我们的均值函数。然而，这个常数均值函数的默认值是零，因此实际上，我们仍然在实现相同的 GP 模型。尽管这两种选择目前导致相同的模型，但在本章中，我们将展示如何使用
    `gpytorch.means.ConstantMean` 来调整常数均值值，以获得更好的模型。
- en: 'Let’s now initialize an object of this class, train it on our training data,
    and visualize its predictions. We do this with the following code:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们初始化该类的一个对象，在我们的训练数据上对其进行训练，并可视化其预测。我们用以下代码来实现这个：
- en: '[PRE2]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Declares the GP
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 声明 GP
- en: ❷ Fixes the hyperparameters
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 修复超参数
- en: Here, we initialize the GP model and set its hyperparameters—the length scale
    and noise variance—to 1 and 0.0001, respectively. We will see how to set the values
    of these hyperparameters appropriately later in this chapter; for now, let’s just
    stick with these values. Finally, we call the helper function we just wrote, `visualize_gp_belief()`,
    on our GP model, which produces figure 3.5.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们初始化 GP 模型并设置其超参数——长度尺度和噪声方差分别为 1 和 0.0001。我们将在本章后面看到如何适当设置这些超参数的值；现在，让我们只使用这些值。最后，我们在我们的
    GP 模型上调用我们刚刚编写的辅助函数 `visualize_gp_belief()`，它将生成图 3.5。
- en: 'All the nice properties of the GP that we pointed out in section 2.4.4 are
    still here:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 所有我们在第 2.4.4 节中指出的 GP 的良好性质仍然存在：
- en: The posterior mean function smoothly interpolates the xs that are our training
    data points.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 后验均值函数平滑地插值出我们的训练数据点。
- en: The 95% CI vanishes around these data points, denoting a well-calibrated quantification
    of uncertainty.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 95% CI 在这些数据点周围消失，表示了一个良好校准的不确定性量化。
- en: '![](../../OEBPS/Images/03-05.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/03-05.png)'
- en: Figure 3.5 Predictions by a GP with a zero mean function. The posterior mean
    function interpolates the observed data points and reverts back to zero in regions
    that are far away from these observations.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5 使用零均值函数的 GP 的预测。后验均值函数插值了观察到的数据点，并在远离这些观测点的区域回归到零。
- en: 'We also notice from this plot that once we have gone sufficiently far away
    from our training data points (the right side of the plot), our posterior mean
    function *reverts* back to the prior mean, which is zero. This is, in fact, an
    important feature of a GP: in the absence of data (in regions without observations),
    the prior mean function is the main driving force of the inference procedure.
    This makes intuitive sense, as without actual observations, the best thing that
    a predictive model can do is simply appeal to the prior knowledge encoded in its
    mean function.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个图表中我们还注意到，一旦我们足够远离我们的训练数据点（图表的右侧），我们的后验均值函数就会*恢复*到先验均值，即零。这实际上是高斯过程的一个重要特征：在没有数据的情况下（在没有观测到的区域），先验均值函数是推断过程的主要驱动力。这在直觉上是有道理的，因为没有实际观察，预测模型能做的最好的事情就是简单地依赖于其均值函数中编码的先验知识。
- en: 'Note At this point, we see why having well-defined prior knowledge encoded
    into the prior GP is so important: in the absence of data, the only thing that
    drives predictions is the prior GP.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在这一点上，我们看到为什么将先验知识明确定义地编码到先验高斯过程中是如此重要：在没有数据的情况下，预测的唯一驱动因素就是先验高斯过程。
- en: 'A natural question then arises: Is it possible to use a nonzero mean function
    to induce a different behavior for our GP in these unexplored regions, and if
    so, what are our options? The remaining portion of this section aims to answer
    this question. We start by using a constant mean function that is not zero.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然后自然地提出了一个问题：是否可以使用非零均值函数来诱导我们的高斯过程在这些未探索的区域中产生不同的行为，如果可以，我们有哪些选择？本节的剩余部分旨在回答这个问题。我们首先使用一个不为零的常数均值函数。
- en: 3.3.2 Using the constant function with gradient descent
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用梯度下降法使用常数函数3.3.2
- en: A constant mean function that is not zero is appropriate when we expect the
    objective function we are modeling to take on some range of values that we know
    *a priori*. As we are modeling housing prices, using a constant mean function
    with a constant greater than zero makes sense, as we indeed expect the prices
    to be positive.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个常数均值函数不为零，那么当我们期望我们正在建模的目标函数具有我们*先验*知道的一些值范围时，这是合适的。由于我们正在建模房价，使用一个常数均值函数，其常数大于零，是有意义的，因为我们确实期望价格是正的。
- en: 'Of course, it’s not possible to know what value the objective function takes,
    on average, in many cases. How, then, should we find an appropriate value for
    our mean function? The strategy we use is to appeal to a specific quantity: how
    likely the training dataset is, given the value for our mean function. Roughly
    speaking, this quantity measures how well our model explains its training data.
    We show how to use this quantity to select the best mean function for our GP in
    this subsection.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在许多情况下，我们不可能知道目标函数的平均值是多少。那么，我们应该如何为我们的均值函数找到一个合适的值呢？我们使用的策略是依赖于一个特定的数量：给定我们均值函数的值时，训练数据集有多大可能性。粗略地说，这个数量衡量了我们的模型解释其训练数据的能力。我们展示了如何在这一小节中使用这个数量来选择我们的高斯过程的最佳均值函数。
- en: '![](../../OEBPS/Images/03-05-unnumb-1.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/03-05-unnumb-1.png)'
- en: 'If the likelihood of the training data given some value *c*[1] is higher than
    that given another value *c*[2], then we prefer using *c*[1] to using *c*[2].
    This quantifies our previous intuition about using nonzero mean functions to model
    positive functions: a constant mean function whose value is positive explains
    observations from an exclusively positive function better than a function whose
    value is zero (or negative) does.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果给定一些值*c*[1]时训练数据的似然性高于给定另一个值*c*[2]时的情况，那么我们更喜欢使用*c*[1]而不是使用*c*[2]。这量化了我们先前关于使用非零均值函数来建模正函数的直觉：一个常数均值函数，其值为正，比值为零（或负值）的函数更好地解释了来自完全正函数的观测。
- en: How can we compute this likelihood? GPyTorch offers a convenient class, `gpytorch.mlls.ExactMarginalLogLikelihood`,
    that takes in a GP model and computes the marginal log likelihood of its training
    data, given the hyperparameters of the model.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何计算这个似然性呢？GPyTorch提供了一个方便的类，`gpytorch.mlls.ExactMarginalLogLikelihood`，它接受一个高斯过程模型并计算其训练数据的边际对数似然性，给定模型的超参数。
- en: 'To see that this likelihood quantity is effective at quantifying data fit,
    consider figure 3.6\. This figure visualizes the predictions made by two separate
    GP models: a zero mean GP we saw in the previous subsection, on the left, and
    the GP with a mean function whose value is 2, on the right. Notice how in the
    second panel, the mean function reverts to 2 instead of 0 on the right side of
    the plot. Here, the second GP has a higher (log) likelihood than the first, which
    means the value 2 explains our training data better than the value 0.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 要看到这个似然数量在量化数据拟合方面的有效性，考虑图3.6。这个图可视化了两个不同GP模型的预测：一个我们在前面子段中看到的零均值GP，在左边，以及均值函数值为2的GP，在右边。注意在第二个面板中，均值函数在图的右侧恢复到2而不是0。在这里，第二个GP的（对数）似然性比第一个高，这意味着值2比值0更好地解释了我们的训练数据。
- en: '![](../../OEBPS/Images/03-06.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/03-06.png)'
- en: Figure 3.6 GP predictions, given two different constant mean functions. The
    value 2 gives a higher likelihood value than the value 0, indicating the former
    mean function is a better fit than the latter.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6 GP预测，给出两个不同常数均值函数。值2比值0给出了更高的似然值，表明前者的均值函数比后者更合适。
- en: '![](../../OEBPS/Images/03-06-unnumb-2.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/03-06-unnumb-2.png)'
- en: With this log likelihood computation in hand, our last step is to simply find
    the value for our mean function such that the log likelihood is maximized. In
    other words, we aim to seek the mean value that explains our training data the
    best. Since we have access to the log likelihood computation, we can use gradient-based
    optimization algorithms, such as gradient descent, to iteratively refine the mean
    value we have. Upon convergence, we will have arrived at a good mean value that
    gives high data likelihood. If you need a refresher on how gradient descent works,
    I recommend appendix B of Luis Serrano’s *Grokking Machine Learning* (Manning,
    2021), which does a good job of explaining the concept.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个对数似然计算，我们的最后一步就是简单地找到我们的均值函数的值，使得对数似然被最大化。换句话说，我们的目标是寻找最能解释我们训练数据的均值。由于我们可以访问对数似然计算，我们可以使用基于梯度的优化算法，例如梯度下降，来迭代地优化我们的均值。当收敛时，我们将得到一个很好的均值，它给出了高数据似然度。如果您需要恢复一下梯度下降的工作原理，我推荐Luis
    Serrano的《Grokking Machine Learning》（Manning，2021）的附录B，它很好地解释了这个概念。
- en: 'Now, let’s see how we can implement this process with code. Since we implemented
    our GP model with the `gpytorch.means.ConstantMean` class for our mean function,
    we don’t need to change anything here. So, for now, let’s initialize our GP model
    again:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何用代码实现这个过程。由于我们用`gpytorch.means.ConstantMean`类为我们的均值函数实现了GP模型，所以我们这里不需要做任何更改。所以，现在，让我们再次初始化我们的GP模型：
- en: '[PRE3]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The central step of this procedure is to define the log likelihood function
    as well as a gradient descent algorithm. As mentioned previously, the former is
    an instance of the `gpytorch.mlls.ExactMarginalLogLikelihood` class, implemented
    like so:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程的核心步骤是定义对数似然函数以及梯度下降算法。如前所述，前者是`gpytorch.mlls.ExactMarginalLogLikelihood`类的一个实例，实现如下：
- en: '[PRE4]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'For the gradient descent algorithm, we use Adam, which is a state-of-the-art
    algorithm that has enjoyed a lot of success in many ML tasks, especially DL. We
    declare it using PyTorch as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 对于梯度下降算法，我们使用Adam，这是一种在许多ML任务中取得了很大成功的最先进算法，尤其是DL。我们用PyTorch声明如下：
- en: '[PRE5]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note that we are passing to the `torch.optim.Adam` class `model.mean_module.constant`,
    which is the mean value we seek to optimize. When we run the gradient descent
    procedure, the Adam algorithm iteratively updates the value of `model.mean_module.constant`
    to improve the likelihood function.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们正在传递给`torch.optim.Adam`类的是`model.mean_module.constant`，这是我们希望优化的均值。当我们运行梯度下降过程时，Adam算法会迭代地更新`model.mean_module.constant`的值，以改善似然函数。
- en: 'The last thing we need to do now is to run gradient descent, which is implemented
    as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要做的最后一件事是运行梯度下降，其实现如下：
- en: '[PRE6]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Enables the training mode
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 启用训练模式
- en: ❷ Loss as the negative marginal log likelihood
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 损失作为负边缘对数似然
- en: ❸ Gradient descent on the loss
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 损失上的梯度下降
- en: ❹ Enables the prediction mode
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 启用预测模式
- en: The calls to `train()` at the beginning and `eval()` at the end are the bookkeeping
    steps we always need to take, enabling the training mode and prediction mode of
    our GP model, respectively. Resetting the gradients at each step with `optimizer.zero_grad()`
    is another bookkeeping task to make sure we don’t incorrectly compute the gradients.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 开始时的`train()`调用和最后的`eval()`调用是我们总是需要进行的记录步骤，分别启用我们GP模型的训练模式和预测模式。每一步都使用`optimizer.zero_grad()`重置梯度是另一个记录任务，以确保我们不会错误地计算梯度。
- en: In the middle, we have a 500-step gradient descent procedure in which we iteratively
    compute the loss (which is the negative of our log likelihood) and descend on
    this loss based on its gradients. During this `for` loop, we keep track of the
    negative log likelihood values that we obtain as well as the mean value, adjusted
    in each step. This is so that after training, we can visually inspect these values
    to determine whether they have converged.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在中间，我们有一个500步的梯度下降过程，通过迭代计算损失（即我们的负对数似然）并根据梯度下降来降低这个损失。在这个` for`循环过程中，我们跟踪所获得的负对数似然值以及在每一步中调整的平均值。这是为了在训练之后，我们可以可视化检查这些值以确定它们是否收敛。
- en: Figure 3.7 includes this visualization, showing the running values for the negative
    log likelihood (which we aim to minimize) and the value for the mean function
    of the GP. Our loss consistently decreases (which is a good thing!) with increasing
    values of the mean constant, showing that a positive constant does give a higher
    likelihood than zero. Both curves plateau after 500 iterations, indicating we
    have converged at an optimal value for the mean constant.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7展示了这种可视化，显示了我们希望最小化的负对数似然的运行值以及GP的均值函数值。我们的损失一直降低（这是件好事！）随着均值常数的增加，显示出正常数比零更有可能性。两条曲线在500次迭代后都趋于平稳，表明我们已经收敛到均值常数的最优值。
- en: '![](../../OEBPS/Images/03-07.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/03-07.png)'
- en: Figure 3.7 Running negative log likelihood (lower is better) and mean value
    during gradient descent. In both panels, the values have converged, indicating
    we have arrived at an optimum.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7展示了梯度下降过程中的负对数似然（较低的值为更好）和均值值。在这两个面板中，这些值已经收敛，表明我们已经到达了最优解。
- en: Note I recommend that you *always* plot out the progressive loss, like we just
    did here, when using gradient descent to see whether you have converged at an
    optimal value. Stopping before convergence might lead to poor performance for
    your model. While we won’t be showing these progressive loss plots again throughout
    this chapter, the accompanying code does include them.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在使用梯度下降时，我建议您始终绘制出渐进损失的图像，就像我们刚才在这里所做的那样，以查看您是否已经收敛到最优值。在收敛之前停止可能会导致模型性能不佳。虽然在本章节中我们不会再展示这些渐进损失图，但附带的代码中包含了它们。
- en: So far, we have learned how to use a zero mean function as the default for our
    GP model as well as optimize the mean constant value with respect to the data
    likelihood. However, in many use cases, you might have prior knowledge about how
    the objective function is expected to behave and, thus, prefer to incorporate
    more structure into your mean function.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学会了在我们的GP模型中使用零均值函数作为默认值，并优化相对于数据似然的平均常数值。然而，在许多使用情况下，您可能对目标函数的行为有先验知识，因此更喜欢将更多结构纳入您的均值函数中。
- en: For example, how can we implement the idea that the price of a house increases
    with a larger living area? Moving forward, we learn to do this with a GP by using
    a linear or quadratic mean function.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们如何实现房屋价格随着更大的居住面积而增加的想法？向前迈进，我们学会了使用线性或二次均值函数来实现这一点。
- en: 3.3.3 Using the linear function with gradient descent
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.3 使用带梯度下降的线性函数
- en: We proceed with the linear mean function, which is in the form of *μ* = *w^T**x*
    + *b*. Here, *μ* is the predictive mean value at the test point *x*, while *w*
    is the weight vector that concatenates the coefficients for each of the features
    in *x*, and *b* is a constant bias term.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续使用线性均值函数，其形式为*μ* = *w^T**x* + *b*。这里，*μ*是测试点*x*上的预测均值，而*w*是连接*x*中每个特征的系数的权重向量，*b*是一个常数偏置项。
- en: By using the linear mean function, we are encoding the assumption that the expected
    behavior of our objective function is equal to a linear combination of the features
    of the data point *x*. For our housing price example, we only have one feature,
    the living area, and we expect it to have a positive weight, so by increasing
    the living area, our model will predict a higher price.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用线性平均函数，我们对我们的目标函数的预期行为进行编码，即它等于数据点*x*的特征的线性组合。对于我们的住房价格示例，我们只有一个特征，即生活区域，我们期望它有一个正权重，因此通过增加生活区域，我们的模型将预测出更高的价格。
- en: Another way to think about this linear mean model is that we have a linear regression
    model (which also assumes the target label to be a linear combination of the features)
    and we put a probabilistic belief, a GP model, on top of our predictions. This
    gives us the power offered by a linear regression model, while maintaining all
    the benefits of modeling with a GP—namely, uncertainty quantification.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种思考线性平均模型的方式是，我们有一个线性回归模型（也假设目标标签为特征的线性组合），然后我们在预测上加上一个概率信念，一个GP模型。这给予我们线性回归模型的能力，同时保持使用GP建模的所有好处，即不确定性量化。
- en: Note Under a constant mean function, the weight vector *w* is fixed at the zero
    vector, and the bias *b* is the mean value that we learned to optimize in the
    previous subsection. In other words, the linear function is a more general model
    than the constant mean function.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 在一个常数均值函数下，权重向量*w*固定为零向量，偏差*b*是我们学会在上一小节优化的平均值。换句话说，线性函数比常数均值函数是一个更一般的模型。
- en: 'Regarding implementation, building a GP model with a linear mean function is
    quite straightforward. We simply swap out our constant mean and replace it with
    a `gpytorch.means.LinearMean` instance, as follows (our `forward()` method remains
    unchanged):'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 关于实施，使用具有线性平均函数的GP模型相当简单。我们只需用`gpytorch.means.LinearMean`实例取代我们的常数均值，这样（我们的`forward()`方法保持不变）：
- en: '[PRE7]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Linear mean
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 线性平均
- en: Here, we use `1` to initialize our mean module, indicating we are working with
    a one-dimensional objective function. When working with a higher-dimensional function,
    you can simply pass in the dimensionality of that function here. Aside from this,
    everything else about our model is similar to what we had before. Fitting and
    training this new model on our three-point dataset, we obtain the predictions
    in figure 3.8.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用`1`来初始化我们的均值模块，表示我们正在处理一维目标函数。当处理高维函数时，你可以简单地在这里传递该函数的维度。除此之外，我们模型的其它部分与之前的相似。在我们的三个点数据集上拟合和训练这个新模型，我们得到图3.8中的预测。
- en: '![](../../OEBPS/Images/03-08.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/03-08.png)'
- en: Figure 3.8 Predictions by a GP with a linear mean function. The GP has an upward
    trend, which is a direct result of the positive slope of the linear mean function.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8 GP带有线性平均函数的预测。GP有一个上升趋势，这直接是线性平均函数的正斜率的结果。
- en: Unlike what we have seen so far with the constant mean, the linear mean function
    we’re using here drives the entire GP model to have an upward trend. This is because
    the best-fit line of the five data points in our training data is one with a positive
    slope, which is exactly what we wanted to model as a relationship between living
    area and price.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们到目前为止看到的恒定平均值不同，我们在这里使用的线性平均函数驱使整个GP模型具有上升趋势。这是因为我们在训练数据中的五个数据点的最佳拟合线是具有正斜率的线，这正是我们希望作为生活区域和价格之间关系的建模。
- en: 3.3.4 Using the quadratic function by implementing a custom mean function
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.4 通过实施自定义均值函数使用二次函数
- en: Our linear mean function here successfully captures the increasing trend of
    the price, but it assumes the rate of price increase is constant. That is, adding
    an extra square foot to the living area leads, in expectation, to a constant increase
    in price.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这里的线性平均函数成功捕捉到了价格的上涨趋势，但它假定价格增长率是恒定的。也就是说，对生活区域增加一个额外的平方英尺，预期上会导致价格的恒定增加。
- en: In many cases, however, we might have prior knowledge that our objective function
    increases at a non-constant rate, which a linear mean cannot model. In fact, the
    data points we’ve been using were generated so that the price is a quadratic function
    of the living area. This is why we see the larger houses become more expensive
    faster than the smaller houses. In this subsection, we implement our GP mean as
    a quadratic function.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在许多情况下，我们可能有先验知识，即我们的目标函数以非恒定的速率增长，而线性均值无法建模。事实上，我们使用的数据点是这样生成的，即价格是生活区域的二次函数。这就是为什么我们看到较大的房子比较小的房子更快地变得更贵。在本小节中，我们将将我们的
    GP 均值实现为一个二次函数。
- en: 'At the time of this writing, GPyTorch only provides implementations for the
    constant and linear mean functions. But the beauty of this package, as we will
    see again and again in the book, is its modularity: all components of a GP model,
    such as the mean function, the covariance function, the prediction strategy, and
    even the marginal log likelihood function, are implemented as modules, and therefore,
    they can be modified, reimplemented, and extended in an object-oriented manner.
    We see this first-hand when implementing our own quadratic mean function.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，GPyTorch 仅提供了常数和线性均值函数的实现。但是，正如我们将在本书中一次又一次地看到的那样，这个软件包的美妙之处在于其模块化：GP
    模型的所有组件，如均值函数、协方差函数、预测策略，甚至边缘对数似然函数，都被实现为模块，因此，它们可以以面向对象的方式进行修改、重新实现和扩展。当我们实现自己的二次均值函数时，我们首先亲自体会到这一点。
- en: 'The first thing for us to do is define a mean function class:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要做的第一件事是定义一个均值函数类：
- en: '[PRE8]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This class extends the `gpytorch.means.Mean` class, which is the base for all
    GPyTorch mean function implementations. To implement our custom logic, we need
    to rewrite two methods: `__init__()` and `forward()`, which are exactly the same
    as when we implement a GP model!'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类扩展了 `gpytorch.means.Mean` 类，它是所有 GPyTorch 均值函数实现的基类。为了实现我们的自定义逻辑，我们需要重新编写两个方法：`__init__()`
    和 `forward()`，这与我们实现 GP 模型时完全相同！
- en: In `__init__()`, we need to declare what parameters our mean function contains.
    This process is called *parameter registration*.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `__init__()` 中，我们需要声明我们的均值函数包含哪些参数。这个过程称为 *参数注册*。
- en: 'While a linear function has two parameters, a slope and an intercept, a quadratic
    function has three: a coefficient for the second-order term *x*[2]; a coefficient
    for the first-order term *x*; and a coefficient for the zeroth-order term, which
    is typically called the bias. This is illustrated in figure 3.9.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然线性函数有两个参数，斜率和截距，而二次函数有三个参数：一个二阶项 *x*[2] 的系数；一个一阶项 *x* 的系数；和一个零阶项的系数，通常称为偏差。这在图
    3.9 中有所说明。
- en: '![](../../OEBPS/Images/03-09.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/03-09.png)'
- en: Figure 3.9 The functional forms of a linear function and a quadratic function.
    The linear function has two parameters, while the quadratic function has three.
    When these functions are used as a GP’s mean function, the corresponding parameters
    are the GP’s hyperparameters.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.9 线性函数和二次函数的函数形式。线性函数有两个参数，而二次函数有三个。当这些函数用作 GP 的均值函数时，相应的参数是 GP 的超参数。
- en: 'With that in mind, we implement the `__init__()` method like so:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个基础上，我们这样实现 `__init__()` 方法：
- en: '[PRE9]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Second-order coefficient
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 二阶系数
- en: ❷ First-order coefficient
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 一阶系数
- en: ❸ Bias
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 偏差
- en: We sequentially call `register_parameter()` to register the second-order coefficient,
    the first-order coefficient, and the bias. As we don’t have a good idea of what
    values these coefficients should take, we simply initialize them randomly using
    `torch.randn()`.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按顺序调用 `register_parameter()` 来注册二阶系数、一阶系数和偏差。由于我们不清楚这些系数应该取什么值，我们只需使用 `torch.randn()`
    随机初始化它们。
- en: Note We need to register these parameters as instances of the `torch.nn` `.Parameter`
    class, which allows their values to be adjusted (trained) during gradient descent.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 我们需要将这些参数注册为 `torch.nn` `.Parameter` 类的实例，这允许它们的值在梯度下降期间进行调整（训练）。
- en: 'For the `forward()` method, we need to define how our mean function should
    process an input. As we have said, a quadratic function is in the form of *ax²
    + bx + c*, where *a*, *b*, and *c* are the second-order coefficient, first-order
    coefficient, and the bias, respectively. So we only need to implement that logic,
    as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `forward()` 方法，我们需要定义我们的均值函数如何处理输入。正如我们所说的，二次函数的形式为 *ax² + bx + c*，其中 *a*、*b*
    和 *c* 分别是二阶系数、一阶系数和偏差。所以我们只需要实现这个逻辑，如下所示：
- en: '[PRE10]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Omitted
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 已省略
- en: ❷ The formula of a quadratic function
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 二次函数的公式
- en: 'With this quadratic mean function in hand, we can now write a GP model that
    initializes its mean module using the custom `QuadraticMean` class we just implemented:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Omitted
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Rerunning our entire training procedure with gradient descent gives us the predictions
    in figure 3.10.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/03-10.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
- en: Figure 3.10 Predictions by a GP with a quadratic mean function. This GP predicts
    that with a larger living area, the price increases at a faster rate.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Here, we successfully model the non-constant rate of increase in housing prices
    with respect to living area. Our predictions increase much more quickly on the
    right side of the plot than on the left.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: We can do this for any functional form we want to assume about the objective,
    such as a higher-degree polynomial or a sublinear function. All we need to do
    is implement a mean function class with appropriate parameters and use gradient
    descent to assign values to these parameters that will give us a good fit for
    our training data.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Our discussion so far demonstrates the mathematical flexibility of GP models,
    in that they can make use of a mean function of any structure and still produce
    probabilistic predictions. This flexibility motivates and drives the design of
    GPyTorch, whose emphasis on modularity helps us extend and implement our own custom
    mean function effortlessly. We see the same flexibility and modularity in GPyTorch’s
    covariance functions, which we discuss next.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Defining variability and smoothness with the covariance function
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While the mean function defines our expectation of the overall behavior of
    the objective function, the covariance function, or the kernel, of a GP plays
    a more complex role: defining the relationship between data points within the
    domain and controlling for the structure and smoothness of the GP. In this section,
    we compare how the predictions by a GP change as we change different components
    of our model. From there, we gain practical insights into how to select an appropriate
    covariance function for a GP model. The code that we use is in CH03/02 - Covariance
    functions.ipynb.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Throughout these examples, we use the Forrester function, which we saw in section
    2.4.1, as our objective. We, once again, randomly sample three data points between
    -3 and 3 and use them as our training dataset. All predictions visualized in this
    section are from a GP trained on these three points.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.1 Setting the scales of the covariance function
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first way of controlling the behavior of a GP via its covariance function
    is to set the length scale and the output scale. These scales, just like the constant
    or the coefficients in the mean function, are the hyperparameters of the covariance
    function:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: A length scale controls the scale of the GP’s input and, thus, how fast the
    GP can change along an axis—that is, how much we believe the objective function
    varies with respect to an input dimension.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output scale defines the range of the GP’s output or, in other words, the
    range of its predictions.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出尺度定义了 GP 的输出范围或者说是其预测范围。
- en: 'By setting these scales to different values, we can either increase or decrease
    the uncertainty in our GP’s predictions as well as scale the range of our predictions.
    We use the following implementation:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 通过设置这些尺度的不同值，我们可以增加或减少 GP 预测的不确定性，以及缩放我们的预测范围。我们使用以下代码实现：
- en: '[PRE12]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ gpytorch.kernels.ScaleKernel implements the output scale.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ gpytorch.kernels.ScaleKernel 实现了输出尺度。
- en: ❷ Omitted
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 忽略
- en: 'Notice that our code for the `covar_module` attribute here is different from
    before: we are placing a `gpytorch.kernels.ScaleKernel` object outside of our
    usual RBF kernel. This effectively implements the output scale that scales the
    output of the RBF kernel by some constant factor. The length scale, on the other
    hand, is already included within `gpytorch.kernels.RBFKernel`.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这里的 `covar_module` 属性代码与之前不同：我们将一个 `gpytorch.kernels.ScaleKernel` 对象放在了通常的
    RBF 内核之外。这实际上实现了输出尺度，它通过某个定值因子对 RBF 内核的输出进行了缩放。而长度尺度则已经包含在了 `gpytorch.kernels.RBFKernel`
    内核中。
- en: '![](../../OEBPS/Images/03-10-unnumb-3.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/03-10-unnumb-3.png)'
- en: In the code we have been using thus far, there is one line where we set the
    length scale of our kernel with `model.covar_module.base_kernel.lengthscale` `=`
    `lengthscale`. This is where the value of the length scale is stored. Using a
    similar API, we can set the output scale of our kernel with `model.covar_module.outputscale`
    `=` `outputscale`. Now, to see that the length scale effectively controls how
    fast the function varies, we compare the predictions made by two GPs—one with
    a length scale of 1 and the other with a length scale of 0.3, shown in figure
    3.11.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们迄今为止使用的代码中，我们有一行代码用于设置我们内核的长度尺度，即 `model.covar_module.base_kernel.lengthscale`
    `=` `lengthscale`。这就是长度尺度的值所保存的位置。使用类似的 API，我们可以使用 `model.covar_module.outputscale`
    `=` `outputscale` 来设置内核的输出尺度。现在，为了验证长度尺度确实可以控制函数变化的速度，我们将比较两个 GP 的预测，一个长度尺度为 1，另一个为
    0.3，如图 3.11 所示。
- en: '![](../../OEBPS/Images/03-11.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/03-11.png)'
- en: Figure 3.11 The GP’s predictions with the length scale set to 1 (left) and to
    0.3 (right). With a small length scale, the GP predictions have more variability,
    leading to more uncertainty.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.11 显示了 GP 以长度尺度为 1（左边）和 0.3（右边）进行预测的结果。长度尺度越小，GP 的预测就越不确定，变异性越高。
- en: 'The stark difference between the two panels makes clear the effect of our length
    scale:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个面板的巨大差异清晰地表明了长度尺度的效果：
- en: A shorter length scale corresponds with more variability in the objective function,
    given a constant change in the input.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个较短的长度尺度相当于给定输入常量变化情况下客观函数更多的变异性。
- en: A longer length scale, on the other hand, forces the function to be smoother,
    in the sense that it varies less, given the same input change.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相反，一个较长的长度尺度强制函数更加光滑，也就是说给定输入更少的变化会使其变异性减少。
- en: For instance, going one unit along the *x*-axis, the samples in the left panel
    in figure 3.11 vary less than those in the right panel.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，沿着 *x* 轴移动一个单位，在图 3.11 左侧面板中的样本的变化幅度小于右侧面板的样本。
- en: '![](../../OEBPS/Images/03-11-unnumb-4.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/03-11-unnumb-4.png)'
- en: What about the output scale, then? We said earlier that this parameter scales
    the output of the covariance function to a different range. This is done by simply
    multiplying the covariance output with this parameter. Hence, a large output scale
    leads to the range of the GP’s predictions being wider, while a small output scale
    shrinks this prediction range. To see that this is true, let’s once again run
    our code and regenerate the predictions, this time setting the output scale to
    3\. The produced output is shown in figure 3.12.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 那么输出尺度呢？我们之前说这个参数将协方差函数的输出值缩放到一个不同的范围。这是通过将协方差输出结果乘以这个参数来实现的。因此，较大的输出尺度会使 GP
    的预测范围更宽，而较小的输出尺度则会使预测范围缩小。为了验证这一点，让我们再次运行代码并重新生成预测，这次将输出尺度设置为 3。生成的输出结果如图 3.12
    所示。
- en: '![](../../OEBPS/Images/03-12.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/03-12.png)'
- en: Figure 3.12 The GP’s predictions with the output scale set to 3\. With a large
    output scale, the GP models the function to have a wider range, also allowing
    more uncertainty in the predictions.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.12 显示了 GP 的输出尺度为 3 的预测结果。较大的输出尺度使 GP 模拟的函数范围更宽，也容许更多的预测不确定性。
- en: While the left panel of figures 3.11 and 3.12 may look the same in that the
    GP and its samples have the same shape across the two plots, we notice that figure
    3.12 has a larger *y*-axis, as both its predictions and its samples take on larger
    values (both negative and positive). This is the direct result of scaling the
    covariance values from the RBF kernel with a large output scale.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管图 3.11 和图 3.12 的左侧面板看起来相同，因为 GP 及其样本在两个图中具有相同的形状，但我们注意到图 3.12 的 *y*-轴更大，因为它的预测值和样本值都取得了较大的值（负值和正值都有）。这是使用具有较大输出尺度的
    RBF 核对协方差值进行缩放的直接结果。
- en: With just two hyperparameters for our covariance function, we have seen that
    we can account for a wide range of functional behaviors that are modeled by our
    GP, which we summarize in table 3.2\. I invite you to rerun this code with different
    values for the length and output scales to see their effects and verify the table
    for yourself!
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 通过我们的协方差函数的两个超参数，我们已经看到我们可以解释由我们的 GP 模型建模的各种功能行为，我们在表 3.2 中总结了这些行为。我邀请您使用不同的长度和输出尺度重新运行此代码，以查看其效果并验证表格！
- en: Table 3.2  Summary of the roles that the length and output scales of a GP play
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3.2 GP 的长度和输出尺度的角色总结
- en: '| Parameter | With a large value | With a small value |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 大值 | 小值 |'
- en: '| --- | --- | --- |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Length scale | Smoother predictions, less uncertainty | More variability,
    more uncertainty |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 长度尺度 | 更平滑的预测，较少的不确定性 | 更多的变异性，更多的不确定性 |'
- en: '| Output scale | Larger output values, more uncertainty | Narrower output range,
    less uncertainty |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 输出尺度 | 较大的输出值，更多的不确定性 | 较窄的输出范围，较少的不确定性 |'
- en: 'Note This flexibility in modeling gives rise to a natural question: How should
    you appropriately set the values for these hyperparameters? Luckily, we already
    know a good way of setting the hyperparameters of a GP model. We can do this by
    choosing the values that explain our data the best or, in other words, maximizing
    the marginal log likelihood, specifically via gradient descent.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 这种建模的灵活性引发了一个自然的问题：你应该如何适当地设置这些超参数的值？幸运的是，我们已经知道了一种设置 GP 模型超参数的好方法。我们可以通过选择最好地解释我们的数据的值，或者换句话说，通过梯度下降来最大化边缘对数似然，从而实现这一目标。
- en: 'Just like when we wanted to optimize the hyperparameters of the mean function,
    we now do this by simply passing the variables we’d like to optimize—the parameters
    of the covariance function—to Adam:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们希望优化均值函数的超参数一样，我们现在只需将我们希望优化的变量 - 协方差函数的参数 - 传递给 Adam 即可实现：
- en: '[PRE13]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: By running gradient descent, we can arrive at good values for these parameters.
    Specifically, I obtained a length scale of roughly 1.3 and an output scale of
    roughly 2.1\. That is, to fit the three-point training dataset well, we want the
    GP to be slightly smoother (with a length scale greater than 1), and we also want
    the range of our predictions to be larger (with a larger output scale). This is
    certainly a reassuring result, as our objective function does have a wide range
    of values—at input 3, it takes on a value of –2, which is well outside of the
    CI with an output scale of 1.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行梯度下降，我们可以获得这些参数的良好值。具体来说，我获得了大约 1.3 的长度尺度和大约 2.1 的输出尺度。也就是说，为了很好地拟合这三点训练数据集，我们希望
    GP 稍微更平滑一些（具有大于 1 的长度尺度），并且还希望我们的预测范围更大一些（具有较大的输出尺度）。这无疑是一个让人放心的结果，因为我们的目标函数具有广泛的数值范围
    - 在输入 3 处，它的值将达到 -2，这远超出了具有输出尺度 1 的 CI。
- en: 3.4.2 Controlling smoothness with different covariance functions
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.2 使用不同的协方差函数控制平滑度
- en: Thus far, we have exclusively used the RBF kernel as our covariance function.
    It is, however, entirely possible to use a different kernel for our GP if RBF
    is not appropriate. In this subsection, we learn to use another family of kernels,
    the Matérn kernel, and see what effect this kernel would have on our GP.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们仅仅使用了 RBF 核作为我们的协方差函数。然而，如果 RBF 不合适，完全可以使用不同的核函数用于我们的 GP。在本小节中，我们将学习使用另一种核函数家族，即马特恩核，并看看这种核函数对我们的
    GP 会产生什么影响。
- en: Note By using a Matérn kernel, we are specifying the smoothness of the function
    of our GP models. *Smoothness* is a technical term here that refers to the differentiability
    of the function; the more times a function is differentiable, the smoother it
    is. We can roughly think of this as how much the function values “jump” up and
    down in a jagged manner.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 通过使用马特恩核，我们正在指定 GP 模型函数的平滑度。这里的 "平滑度" 是一个技术术语，指的是函数的可微性；函数可微性越多次，它就越平滑。我们可以大致将其视为函数值以曲折方式
    "跳动" 的程度。
- en: The RBF kernel models functions that are *infinitely* differentiable, which
    is a property that not many functions in the real world have. Meanwhile, a Matérn
    kernel produces functions that are finitely differentiable, and exactly how many
    times these functions may be differentiated (that is, how smooth these functions
    are) is controlled by a settable parameter, as we discuss shortly.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: RBF 核模拟的函数具有*无限*可微性，这是现实世界中很少有的函数特性。与此同时，Matérn 核生成的函数是有限可微的，这些函数可以被微分的次数（即这些函数的平滑度）由可设置的参数控制，我们马上就会讨论到。
- en: 'To see the Matérn kernel in action, we first reimplement our GP model class:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 要看到 Matérn 核的实际效果，我们首先重新实现我们的 GP 模型类：
- en: '[PRE14]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Omitted
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 省略
- en: Here, our `covar_module` attribute is initialized as an instance of the `gpytorch
    .kernels.MaternKernel` class. This initialization takes in a parameter `nu` that
    defines the level of smoothness our GP will have, which is also a parameter of
    our `__init__()` method.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们的 `covar_module` 属性被初始化为 `gpytorch .kernels.MaternKernel` 类的实例。这个初始化接受一个参数
    `nu`，定义了我们的 GP 将具有的平滑程度，这也是我们 `__init__()` 方法的一个参数。
- en: Important At the time of this writing, three values for `nu` are supported by
    GPyTorch, 1/2, 3/2, and 5/2, corresponding to functions being non-, once-, and
    twice-differentiable. In other words, the larger this `nu` parameter, the smoother
    our GP.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示：在撰写本文时，GPyTorch 支持三个 `nu` 值，1/2、3/2 和 5/2，对应函数分别是不可微分、一次可微分和两次可微分的。换句话说，这个
    `nu` 参数越大，我们的 GP 越平滑。
- en: 'Let’s try `nu` `=` `0.5` first by setting that value when we initialize the
    GP:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先尝试 `nu` `=` `0.5`，通过在初始化 GP 时设置该值来实现：
- en: '[PRE15]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Fixes hyperparameters and enables the prediction mode
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 修正超参数并启用预测模式
- en: This code produces figure 3.13.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码生成图 3.13。
- en: '![](../../OEBPS/Images/03-13.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/03-13.png)'
- en: Figure 3.13 The GP’s predictions with the Matérn 1/2 kernel, which induces the
    belief that the objective function is not differentiable, corresponding to very
    rough samples
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.13 中 Matérn 1/2 核的 GP 预测，这表明目标函数不可微分，对应非常粗糙的样本
- en: Unlike what we have seen before with RBF, the samples from this Matérn kernel
    are all very jagged. In fact, none of them are differentiable. `nu` `=` `0.5`
    is a good value for the Matérn kernel when modeling time-series data, such as
    stock prices.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前在 RBF 中看到的情况不同，这个 Matérn 核的样本都非常参差不齐。事实上，它们都不可微分。当建模时间序列数据时，比如股票价格，`nu`
    `=` `0.5` 是 Matérn 核的一个好值。
- en: However, this value is typically not used in BayesOpt, as jagged functions like
    those in figure 3.13 are highly volatile (they can jump up and down in an unpredictable
    way) and are usually not the target for automated optimization techniques. We
    need a certain level of smoothness from our objective function that is to be optimized;
    otherwise, effective optimization is an unrealistic goal.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在 BayesOpt 中通常不使用这个值，因为像图 3.13 中那样的参差不齐的函数非常不稳定（它们可以以不可预测的方式上下跳动），通常不是自动优化技术的目标。我们需要目标函数具有一定的平滑度，以便进行优化；否则，有效的优化是不切实际的目标。
- en: The Matérn 5/2 kernel is commonly preferred. Its predictions, along with those
    generated by Matérn 3/2, are visualized in figure 3.14.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: Matérn 5/2 核通常是首选。它的预测结果与 Matérn 3/2 生成的结果在图 3.14 中可视化。
- en: '![](../../OEBPS/Images/03-14.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/03-14.png)'
- en: Figure 3.14 The GP’s predictions with the Matérn 5/2 (left) and Matérn 3/2 (right)
    kernel. The samples here are smooth enough for the GP to effectively learn from
    data but are also jagged enough to realistically model real-life processes.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.14 中 Matérn 5/2（左）和 Matérn 3/2（右）核的 GP 预测。这里的样本足够平滑，以便 GP 有效地从数据中学习，但也足够参差不齐，以真实地模拟现实生活中的过程。
- en: We see that the samples from this 5/2 kernel are much smoother, which leads
    to more effective learning by the GP. However, these samples are also rough enough
    that they resemble functions we might see in the real world. For this reason,
    most efforts, both research and applied, in BayesOpt utilize this Matérn 5/2 kernel.
    In future chapters, when we discuss decision-making for BayesOpt, we default to
    this kernel, accordingly.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到这个 5/2 核的样本要平滑得多，这导致 GP 更有效地学习。然而，这些样本也足够粗糙，以至于它们类似于我们在现实世界中可能看到的函数。因此，BayesOpt
    中的大多数工作，无论是研究还是应用，都使用这个 Matérn 5/2 核。在未来的章节中，当我们讨论 BayesOpt 的决策时，我们将相应地默认使用这个核。
- en: Note While we do not include the corresponding details here, a Matérn kernel
    has its own length scale and output scale, which may be specified to further customize
    the behavior of the resulting GP in the same manner as in the previous subsection.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: By pairing a mean function with a kernel, we can induce complex behavior in
    the predictions of the GP. Just like our prior affects the conclusion each person
    in our group of friends reaches after seeing someone correctly guess a secret
    number 100 times, our choice of the mean function and the kernel determines the
    predictions made by a GP. Figure 3.15 shows three examples in which each combination
    of a mean function and a kernel leads to a drastically different behavior.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/03-15.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
- en: Figure 3.15 Three different choices for the mean function and kernel and the
    predictions made by their respective posterior GPs when trained on the same dataset.
    Each choice leads to a different behavior in prediction.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.3 Modeling different levels of variability with multiple length scales
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we have only been considering one-dimensional objective functions (functions
    whose inputs have one feature), there is only one length scale that we need to
    consider. However, we can imagine scenarios in which a high-dimensional objective
    function (whose inputs have more than one feature) has more variability in some
    dimensions and is smoother in others. That is, some dimensions have small-length
    scales, while others have large-length scales. Remember our motivating example
    at the beginning of this chapter: the price prediction of a house increases by
    a larger amount with an extra story than with an extra square foot of living area.
    We explore how to maintain multiple length scales in a GP to model these functions
    in this subsection.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: If we were to only use a single length scale for all dimensions, we wouldn’t
    be able to faithfully model the objective function. This situation calls for the
    GP model to maintain a separate length scale for each dimension to fully capture
    the variability in each of them. In this final section of the chapter, we learn
    how to do this with GPyTorch.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'To aid our discussion, we use a concrete two-dimensional objective function
    called Ackley, which can be modified to have various levels of variability in
    different dimensions. We implement the function as follows:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We specifically restrict the domain of this function to the square region between
    –3 and 3 in both dimensions, which is typically denoted as [–3, 3]². To visualize
    this objective function, we use the heat map in figure 3.16.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/03-16.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
- en: Figure 3.16 The two-dimensional Ackley function to be used as our objective.
    Here, the *x*-axis has less variability (it changes less) than the *y*-axis, requiring
    different length scales.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Each dark blob in the heat map can be thought of as a valley that has a low
    value across the surface of the objective function. Here, there are many more
    valleys across the *y*-axis than across the *x*-axis, indicating that the second
    dimension has more variability than the first—that is, the objective function
    goes up and down many times across the *y*-axis, more often than across the *x*-axis.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Once again, this means that using only one length scale for both dimensions
    is not a good choice. Instead, we should have a length scale for *each* dimension
    (two, in this case). Each length scale can then be independently optimized using
    gradient descent.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Important Using a kernel with a length scale for each dimension is called *automatic
    relevance determination* (ARD). This term denotes the fact that after using gradient
    descent to optimize these length scales, we can infer how relevant each dimension
    of the objective function is with respect to the function values. A dimension
    with a large length scale has low variability and is, therefore, less relevant
    in modeling the objective function values than a dimension with a small length
    scale.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'Implementing ARD is very easy with GPyTorch: we simply specify the `ard_num_dims`
    parameter to be equal to the number of dimensions our objective function has when
    initializing our covariance function. This is done with the RBF kernel like so:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Omitted
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see whether, when trained on our Ackley function, this model gives us
    different length scales for the two dimensions. To do this, we first construct
    a randomly sampled training dataset consisting of 100 points:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: After training the model using gradient descent, like we have been doing, we
    could inspect the optimized values for the length scales by printing out
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This is, indeed, the result we expected: a larger length scale for the first
    dimension, where there is less variability in the function values, and a smaller
    length scale for the second dimension, where there is more variability.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/03-16-0-unnumb-5.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
- en: More reading on kernels
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Kernels, in and of themselves, have enjoyed a lot of interest from the ML community.
    One thing to note, in addition to what we have covered so far, is that kernels
    can also encode complex structures, such as periodicity, linearity, and noise.
    For a more thorough and technical discussion on kernels and their roles in ML,
    the interested reader may refer to David Duvenaud’s *The Kernel Cookbook* ([https://www.cs.toronto.edu/~duvenaud/cookbook/](https://www.cs.toronto.edu/~duvenaud/cookbook/)).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: This discussion marks the end of chapter 3\. Throughout this chapter, we have
    extensively investigated how our GP model is influenced by the mean and covariance
    functions, specifically by their various parameters. We use this as a way to incorporate
    what we know about the objective function—that is, prior information—into our
    GP models. We have also learned to use gradient descent to estimate the values
    for these parameters to obtain the GP model that explains our data the best.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这个讨论标志着第3章的结束。在本章中，我们广泛地研究了我们的GP模型是如何受到均值和协方差函数的影响，特别是它们的各个参数。我们将这视为将我们对目标函数的了解（即先验信息）融入到我们的GP模型中的一种方式。我们还学会了使用梯度下降来估计这些参数的值，以获得最佳解释我们数据的GP模型。
- en: 'This also marks the end of the first part of the book, where we focus on GPs.
    Starting from the next chapter, we begin learning about the second component of
    the BayesOpt framework: decision-making. We begin with two of the most commonly
    used BayesOpt policies that seek to improve from the best point seen: Probability
    of Improvement and Expected Improvement.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这也标志着本书的第一部分的结束，我们将重点放在了GP上。从下一章开始，我们开始学习BayesOpt框架的第二个组成部分：决策制定。我们从两种最常用的BayesOpt策略开始，这两种策略旨在改善已见到的最佳点：概率改进和期望改进。
- en: 3.5 Exercise
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.5 练习
- en: This exercise provides practice for implementing a GP model with ARD. To do
    this, we create an objective function that varies along one axis more than it
    does along another axis. We then train a GP model, with and without ARD, on data
    points from this function and compare the learned length scale values. The solution
    is included in CH03/03 - Exercise.ipynb.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习是为了练习使用ARD实现GP模型。为此，我们创建一个目标函数，沿一个轴变化的程度比沿另一个轴变化的程度更大。然后，我们使用来自该函数的数据点训练一个有或没有ARD的GP模型，并比较学习到的长度尺度值。解决方案包含在CH03/03
    - Exercise.ipynb中。
- en: 'There are multiple steps to this process:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程有多个步骤：
- en: 'Implement the following two-dimensional function in Python using PyTorch:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用PyTorch在Python中实现以下二维函数：
- en: '![](../../OEBPS/Images/03-16-Equations_ch-3.png)'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/03-16-Equations_ch-3.png)'
- en: This function simulates the accuracy surface of a support-vector machine (SVM)
    model in a hyperparameter tuning task. The *x*-axis denotes the value of the penalty
    parameter *c*, while the *y*-axis denotes the value for the RBF kernel parameter
    *γ*. (We use this function as our objective in future chapters as well.)
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个函数模拟了一个支持向量机（SVM）模型在超参数调整任务中的准确性曲面。*x*轴表示惩罚参数*c*的值，*y*轴表示RBF核参数*γ*的值。（我们在未来的章节中也将使用该函数作为我们的目标函数。）
- en: Visualize the function over the domain [`0,` `2`]². The heat map should look
    like figure 3.17.
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在区间[`0,` `2`]²上可视化该函数。热图应该看起来像图3.17。
- en: '![](../../OEBPS/Images/03-17.png)'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/03-17.png)'
- en: Figure 3.17 The accuracy of an SVM model on a test dataset as a function of
    the penalty parameter *c* and the RBF kernel parameter *γ*. The function changes
    more quickly with respect to *γ* than to *c*.
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.17 SVM模型在测试数据集上的准确率作为惩罚参数*c*和RBF核参数*γ*的函数。函数对*γ*的变化比对*c*的变化更快。
- en: Randomly draw 100 data points from the domain [`0,` `2`]². This will be used
    as our training data.
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在区间[`0,` `2`]²中随机选择100个数据点，作为我们的训练数据。
- en: Implement a GP model with a constant mean function and a Matérn 5/2 kernel with
    an output scale implemented as a `gpytorch.kernels.ScaleKernel` object.
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用常数均值函数和Matérn 5/2卷积核来实现一个GP模型，其中输出规模作为`gpytorch.kernels.ScaleKernel`对象实现。
- en: Don’t specify the `ard_num_dims` parameter when initializing the kernel object
    or set the parameter to `None`. This will create a GP model without ARD.
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在初始化核对象时不要指定`ard_num_dims`参数，或将该参数设置为`None`。这将创建一个没有ARD的GP模型。
- en: Train the hyperparameters of the GP model using gradient descent, and inspect
    the length scale after training.
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用梯度下降训练GP模型的超参数，并在训练后检查长度尺度。
- en: Redefine the GP model class, this time setting `ard_num_dims` `=` `2`. Retrain
    the GP model with gradient descent, and verify that the two length scales have
    significantly different values.
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新定义GP模型类，这次设置`ard_num_dims` `=` `2`。使用梯度下降重新训练GP模型，并验证两个长度尺度具有显著不同的值。
- en: Summary
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: Prior knowledge plays an important role in a Bayesian model and can heavily
    influence the posterior predictions of the model.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 先验知识在贝叶斯模型中起着重要作用，可以极大地影响模型的后验预测结果。
- en: With a GP, prior knowledge can be specified using the mean and the covariance
    functions.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用GP模型可以通过均值和协方差函数来指定先验知识。
- en: The mean function describes the expected behavior of the GP model. In the absence
    of data, a GP’s posterior mean prediction reverts to the prior mean.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A GP’s mean function can take on any functional form, including a constant,
    a linear function, and a quadratic function, which can be implemented with GPyTorch.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A GP’s covariance function controls the smoothness of the GP model.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The length scale specifies the level of variability of the output with respect
    to the function input. A large length scale leads to more smoothness and, thus,
    less uncertainty in predictions.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each dimension in a GP can have its own length scale. This is called automatic
    relevance determination (ARD) and is used to model objective functions that have
    different levels of variability in different dimensions.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output scale specifies the range of the function output. A large output
    scale leads to a larger output range and, thus, more uncertainty in the predictions.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Matérn kernel class is a generalization of the RBF kernel class. By specifying
    its parameter `nu`, we can model various levels of smoothness in the GP’s predictions.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The hyperparameters of a GP can be optimized by maximizing the marginal likelihood
    of the data using gradient descent.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
