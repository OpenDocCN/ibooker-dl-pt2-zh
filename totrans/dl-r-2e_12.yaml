- en: 9 Advanced deep learning for computer vision
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9 高级计算机视觉深度学习
- en: '*This chapter covers*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: 'The different branches of computer vision: image classification, image segmentation,
    and object detection'
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机视觉的不同分支：图像分类、图像分割和目标检测
- en: 'Modern convnet architecture patterns: residual connections, batch normalization,
    and depthwise separable convolutions'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现代卷积神经网络架构模式：残差连接、批量归一化和深度可分离卷积
- en: Techniques for visualizing and interpreting what convnets learn
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化和解释卷积神经网络学到的技术
- en: The previous chapter gave you a first introduction to deep learning for computer
    vision via simple models (stacks of layer_conv_2d() and layer_max_pooling_2d()
    layers) and a simple use case (binary image classification). But there’s more
    to computer vision than image classification! This chapter dives deeper into more
    diverse applications and advanced best practices.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 上一章为您介绍了计算机视觉的深度学习，通过简单模型（layer_conv_2d() 和 layer_max_pooling_2d() 层的堆叠）和一个简单的用例（二进制图像分类）。但计算机视觉不仅仅是图像分类！本章深入探讨了更多不同应用和高级最佳实践。
- en: 9.1 Three essential computer vision tasks
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 三个基本的计算机视觉任务
- en: 'So far, we’ve focused on image classification models: an image goes in, a label
    comes out: “This image likely contains a cat; this other one likely contains a
    dog.” But image classification is only one of several possible applications of
    deep learning in computer vision. In general, there are three essential computer
    vision tasks you need to know about:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们专注于图像分类模型：输入一张图像，输出一个标签：“这张图像可能包含一只猫；那张可能包含一只狗。”但图像分类只是计算机视觉中深度学习的几种可能应用之一。一般来说，你需要了解以下三个基本的计算机视觉任务：
- en: '*Image classification*—Where the goal is to assign one or more labels to an
    image. It may be either single-label classification (an image can only be in one
    category, excluding the others), or multilabel classification (tagging all categories
    that an image belongs to, as seen in [figure 9.1](#fig9-1)). For example, when
    you search for a keyword on the Google Photos app, behind the scenes you’re querying
    a very large multilabel classification model—one with over 20,000 different classes,
    trained on millions of images.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图像分类*—其目标是为图像分配一个或多个标签。它可以是单标签分类（图像只能属于一个类别，排除其他类别），也可以是多标签分类（标记图像所属的所有类别，如[图9.1](#fig9-1)中所示）。例如，当您在
    Google Photos 应用上搜索关键字时，背后实际上在查询一个非常庞大的多标签分类模型——一个包含超过 20,000 个不同类别、在数百万张图像上训练的模型。'
- en: '![Image](../images/f0259-01.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0259-01.jpg)'
- en: '**Figure 9.1 The three main computer vision tasks: Classification, segmentation,
    detection**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**图9.1 三个主要的计算机视觉任务：分类、分割、检测**'
- en: '*Image segmentation*—Where the goal is to “segment” or “partition” an image
    into different areas, with each area usually representing a category (as seen
    in [figure 9.1](#fig9-1)). For instance, when Zoom or Google Meet diplays a custom
    background behind you in a video call, it’s using an image segmentation model
    to tell your face apart from what’s behind it, at pixel precision.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图像分割*—其目标是将图像“分割”或“划分”为不同的区域，每个区域通常代表一个类别（如在[图9.1](#fig9-1)中所示）。例如，当 Zoom
    或 Google Meet 在视频通话中为您显示自定义背景时，它正在使用图像分割模型将您的脸与背景分开，以像素级的精度。'
- en: '*Object detection*—Where the goal is to draw rectangles (called *bounding boxes*)
    around objects of interest in an image and associate each rectangle with a class.
    A self-driving car could use an object-detection model to monitor cars, pedestrians,
    and signs in view of its cameras, for instance.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*目标检测*—其目标是在图像中绘制矩形（称为*边界框*）并将每个矩形与一个类别关联起来，围绕感兴趣的对象。例如，自动驾驶汽车可以使用目标检测模型监视其摄像头视野中的汽车、行人和标志。'
- en: Deep learning for computer vision also encompasses a number of somewhat more
    niche tasks besides these three, such as image similarity scoring (estimating
    how visually similar two images are), keypoint detection (pinpointing attributes
    of interest in an image, such as facial features), pose estimation, 3D mesh estimation,
    and so on. But to start with, image classification, image segmentation, and object
    detection form the foundation that every machine learning engineer should be familiar
    with. Most computer vision applications boil down to one of these three.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉的深度学习还包括一些更为专业化的任务，例如图像相似度评分（估算两个图像在视觉上的相似度）、关键点检测（定位图像中感兴趣的属性，例如面部特征）、姿态估计、3D
    网格估计等等。但是，要入门计算机视觉应用，图像分类、图像分割和对象检测是每个机器学习工程师应该熟悉的基础。大多数计算机视觉应用都可归结为这三种技术之一。
- en: You’ve seen image classification in action in the previous chapter. Next, let’s
    dive into image segmentation. It’s a very useful and versatile technique, and
    you can straightforwardly approach it with what you’ve already learned so far.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，你已经看到了图像分类的应用。接下来，让我们深入了解图片分割技术。这是一种非常有用且多样化的技术，你可以利用已经学到的知识很容易地处理。
- en: Note that we won’t cover object detection, because it would be too specialized
    and too complicated for an introductory book. However, you can check out the RetinaNet
    example on [keras.rstudio.com/examples](http://www.keras.rstudio.com/examples),
    which shows how to build and train an object detection model from scratch in R
    using Keras.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们不会涉及对象检测，因为这对于入门书来说过于专业化和复杂化。但是，你可以在 [keras.rstudio.com/examples](http://www.keras.rstudio.com/examples)
    上查看 RetinaNet 的示例，它展示了如何使用 Keras 在 R 中从头构建和训练对象检测模型。
- en: 9.2 An image segmentation example
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2 图像分割示例
- en: 'Image segmentation with deep learning is about using a model to assign a class
    to each pixel in an image, thus *segmenting* the image into different zones (such
    as “background” and “foreground,” or “road,” “car,” and “sidewalk”). This general
    category of techniques can be used to power a considerable variety of valuable
    applications in image and video editing, autonomous driving, robotics, medical
    imaging, and so on. There are two different flavors of image segmentation that
    you should know about:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 使用深度学习进行图像分割是指使用模型为图像中的每个像素分配一个类别，从而将图像分割成不同的区域（例如“背景”和“前景”，或“道路”、“汽车”和“人行道”）。这个广泛的技术类别可以用于开发各种有价值的应用，如图像和视频编辑、自动驾驶、机器人、医学成像等等。关于图像分割，还有两种不同的分类应该知道：
- en: '*Semantic segmentation*, where each pixel is independently classified into
    a semantic category, like “cat.” If there are two cats in the image, the corresponding
    pixels are all mapped to the same generic “cat” category (see [figure 9.2](#fig9-2)).'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*语义分割*，其中每个像素被独立归类为语义类别，例如“猫”。如果图像中有两只猫，相应的像素则全部映射到同一个通用的“猫”类别（参见[图9.2](#fig9-2)）。'
- en: '*Instance segmentation*, which seeks not only to classify image pixels by category
    but also to parse out individual object instances. In an image with two cats in
    it, instance segmentation would treat “cat 1” and “cat 2” as two separate classes
    of pixels (see [figure 9.2](#fig9-2)).'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*实例分割*，不仅对图像像素按类别分类，还将解析出各个对象实例。在包含两只猫的图像中，实例分割会将“猫1”和“猫2”视为两个不同的像素类别（参见[图9.2](#fig9-2)）。'
- en: 'In this example, we’ll focus on semantic segmentation: we’ll be looking once
    again at images of cats and dogs, and this time we’ll learn how to tell apart
    the main subject and its background.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将专注于语义分割：再次观察猫和狗的图像，这次我们将学习如何将主体和背景分离开来。
- en: 'We’ll work with the Oxford-IIIT Pets dataset ([http://www.robots.ox.ac.uk/~vgg/data/pets/](http://www.robots.ox.ac.uk/~vgg/data/pets/)),
    which contains 7,390 pictures of various breeds of cats and dogs, together with
    foreground-background segmentation masks for each picture. A *segmentation mask*
    is the image-segmentation equivalent of a label: it’s an image the same size as
    the input image, with a single color channel where each integer value corresponds
    to the class of the corresponding pixel in the input image. In our case, the pixels
    of our segmentation masks can take one of three integer values:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用[Oxford-IIIT宠物数据集](http://www.robots.ox.ac.uk/~vgg/data/pets/)，该数据集包含7,390张不同品种的猫和狗的图片，以及每张图片的前景-背景分割蒙版。*分割蒙版*是图像分割标签的等效形式：它是与输入图像大小相同的图像，其中的每个整数值对应于输入图像中相应像素的类别。在我们的情况下，我们的分割蒙版的像素可以有三个整数值：
- en: '![Image](../images/f0261-01.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0261-01.jpg)'
- en: '**Figure 9.2 Semantic segmentation vs. instance segmentation**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**图9.2 语义分割与实例分割**'
- en: 1 (foreground)
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1（前景）
- en: 2 (background)
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2（背景）
- en: 3 (contour
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3（轮廓）
- en: 'Let’s start by downloading and uncompressing our dataset, using the the download
    .file() and untar() utilities provided by R. Just like in chapter 8, we’ll use
    the fs package for filesystem operations:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始下载和解压我们的数据集，使用R提供的download.file()和untar()实用程序。就像第8章一样，我们将使用fs包进行文件系统操作：
- en: library(fs)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: library(fs)
- en: data_dir <- path("pets_dataset")
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: data_dir <- path("pets_dataset")
- en: dir_create(data_dir)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: dir_create(data_dir)
- en: data_url <- path("http://www.robots.ox.ac.uk/~vgg/data/pets/data")
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: data_url <- path("http://www.robots.ox.ac.uk/~vgg/data/pets/data")
- en: for (filename in c("images.tar.gz", "annotations.tar.gz")) {
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: for (filename in c("images.tar.gz", "annotations.tar.gz")) {
- en: download.file(url = data_url / filename,
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: download.file(url = data_url / filename,
- en: destfile = data_dir / filename)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: destfile = data_dir / filename)
- en: untar(data_dir / filename, exdir = data_dir)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: untar(data_dir / filename, exdir = data_dir)
- en: '}'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: The input pictures are stored as JPG files in the images/ folder (such as images/
    Abyssinian_1.jpg), and the corresponding segmentation mask is stored as a PNG
    file with the same name in the annotations/trimaps/ folder (such as annotations/
    trimaps/Abyssinian_1.png).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 输入图片存储在images/文件夹中的JPG文件中（例如images/Abyssinian_1.jpg），相应的分割蒙版存储在annotations/trimaps/文件夹中具有相同名称的PNG文件中（例如annotations/trimaps/Abyssinian_1.png）。
- en: 'Let’s prepare a data.frame (technically, a tibble) with columns for our input
    file paths, as well as the list of the corresponding mask file paths:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们准备一个包含输入文件路径列和相应蒙版文件路径列表的数据框（技术上来说，是一个tibble）：
- en: input_dir <- data_dir / "images"
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: input_dir <- data_dir / "images"
- en: target_dir <- data_dir / "annotations/trimaps/"
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: target_dir <- data_dir / "annotations/trimaps/"
- en: image_paths <- tibble::tibble(
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: image_paths <- tibble::tibble(
- en: input = sort(dir_ls(input_dir, glob = "*.jpg")),
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: input = sort(dir_ls(input_dir, glob = "*.jpg")),
- en: target = sort(dir_ls(target_dir, glob = "*.png")))
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: target = sort(dir_ls(target_dir, glob = "*.png")))
- en: 'To make sure we match up the image with the correct target, we sort the two
    lists. The path vectors sort the same because targets and image paths share the
    same base filename. Then, to help us keep track of the paths and make sure that
    our input and target vectors stay in sync, we combine them into a two-column data
    frame (we use tibble() to make the data.frame):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保我们将图像与正确的目标匹配，我们对这两个列表进行排序。路径向量进行排序是因为目标和图像路径共享相同的基本文件名。然后，为了帮助我们跟踪路径，并确保我们的输入和目标向量保持同步，我们将它们组合成一个两列的数据框（我们使用tibble()创建数据框）：
- en: tibble::glimpse(image_paths)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: tibble::glimpse(image_paths)
- en: 'Rows: 7,390'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 'Rows: 7,390'
- en: 'Columns: 2'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 'Columns: 2'
- en: $ input <fs::path> "pets_dataset/images/Abyssinian_1.jpg", "pets_dataset/…
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: $ input <fs::path> "pets_dataset/images/Abyssinian_1.jpg", "pets_dataset/...
- en: $ target <fs::path> "pets_dataset/annotations/trimaps/Abyssinian_1.png", "…
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: $ target <fs::path> "pets_dataset/annotations/trimaps/Abyssinian_1.png", "...
- en: 'What does one of these inputs and its mask look like? Let’s take a quick look.
    We’ll use TensorFlow utilities for reading the image, so we can get familiar with
    the API. First, we define a helper function that will plot a TensorFlow Tensor
    containing an image using R’s plot() function:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这些输入及其蒙版是什么样子的？让我们快速看一下。我们将使用TensorFlow的工具来读取图像，这样我们就可以熟悉API了。首先，我们定义一个辅助函数，使用R的plot()函数绘制包含图像的TensorFlow
    Tensor：
- en: display_image_tensor <- function(x, …, max = 255,
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: display_image_tensor <- function(x, …, max = 255,
- en: plot_margins = c(0, 0, 0, 0)) {
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: plot_margins = c(0, 0, 0, 0)) {
- en: if(!is.null(plot_margins))
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: if(!is.null(plot_margins))
- en: par(mar = plot_margins)➊
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: par(mar = plot_margins)➊
- en: x %>%
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: x %>%
- en: as.array() %>%➋
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: as.array() %>%➋
- en: drop() %>%➌
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: drop() %>%➌
- en: as.raster(max = max) %>%➍
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: as.raster(max = max) %>%➍
- en: plot(…, interpolate = FALSE)➎
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: plot(…, interpolate = FALSE)➎
- en: '}'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ **Default to no margins when plotting images.**
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **绘制图像时默认不留白边。**
- en: ➋ **Convert the Tensor to an R array.**
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **将张量转换为 R 数组。**
- en: ➌ **drop() removes axes that are size 1. For example, if x is a grayscale image
    with one color channel, it would squeeze the Tensor shape from (height, width,
    1) to (height, width).**
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **drop() 移除大小为 1 的轴。例如，如果 x 是一个带有一个颜色通道的灰度图像，它会将张量形状从 (height, width, 1) 挤压成
    (height, width)。**
- en: ➍ **Convert the R array to a 'raster' object.**
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **将 R 数组转换为 'raster' 对象。**
- en: ➎ **interpolate = FALSE tells the R graphics device to draw pixels with sharp
    edges, with no blending or interpolation of colors between pixels.**
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **interpolate = FALSE 告诉 R 图形设备绘制具有锐利边缘的像素，不会在像素之间进行混合或插值。**
- en: In the as.raster() call we set max = 255 because, just like with MNIST, the
    images are encoded as uint8. Unsigned 8-bit integers can encode values only in
    the range of [0, 255]. By setting max = 255, we tell the R graphics device to
    plot pixel values of 255 as white and 0 as black and interpolate linearly for
    values in between to different shades of grey.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在 as.raster() 调用中，我们设置 max = 255，因为，就像 MNIST 一样，图像被编码为 uint8。无符号 8 位整数只能在 [0,
    255] 范围内编码值。通过设置 max = 255，我们告诉 R 图形设备将像素值 255 绘制为白色，0 绘制为黑色，并对介于两者之间的值进行线性插值以生成不同灰度的色阶。
- en: 'Now we can read an image into a Tensor, and view it using our helper display_
    image_tensor() (see [figure 9.3](#fig9-3)):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将图像读入张量，并使用我们的辅助函数 display_image_tensor() 查看它（参见 [图 9.3](#fig9-3)）：
- en: library(tensorflow)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: library(tensorflow)
- en: image_tensor <- image_paths$input[10] %>%
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: image_tensor <- image_paths$input[10] %>%
- en: tf$io$read_file() %>%
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: tf$io$read_file() %>%
- en: tf$io$decode_jpeg()
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: tf$io$decode_jpeg()
- en: str(image_tensor)
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: str(image_tensor)
- en: '<tf.Tensor: shape=(448, 500, 3), dtype=uint8, numpy=…>'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '<tf.Tensor: shape=(448, 500, 3), dtype=uint8, numpy=…>'
- en: display_image_tensor(image_tensor)➊
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: display_image_tensor(image_tensor)➊
- en: ➊ **Display input image Abyssinian_107.jpg.**
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **显示输入图像 Abyssinian_107.jpg。**
- en: '![Image](../images/f0263-01.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0263-01.jpg)'
- en: '**Figure 9.3 An example image**'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 9.3 一个示例图像**'
- en: We’ll also define a helper to display a target image. The target image is also
    read in as a uint8, but this time only values of (1, 2, 3) are found in the target
    image tensor. To plot it, we subtract 1 so that the labels range from 0 to 2,
    and then set max = 2 so that the labels become 0 (black), 1 (gray), and 2 (white).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将定义一个辅助函数来显示目标图像。目标图像也被读取为 uint8，但这次目标图像张量中只有 (1, 2, 3) 的值。为了绘制它，我们减去 1 使标签范围从
    0 到 2，并设置 max = 2，使标签变为 0（黑色）、1（灰色）和 2（白色）。
- en: 'And here is its corresponding target (see [figure 9.4](#fig9-4)):'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 并且这是其相应的目标（参见 [图 9.4](#fig9-4)）：
- en: display_target_tensor <- function(target)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: display_target_tensor <- function(target)
- en: display_image_tensor(target - 1, max = 2)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: display_image_tensor(target - 1, max = 2)
- en: target <- image_paths$target[10] %>%
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: target <- image_paths$target[10] %>%
- en: tf$io$read_file() %>%
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: tf$io$read_file() %>%
- en: tf$io$decode_png()
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: tf$io$decode_png()
- en: str(target)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: str(target)
- en: '<tf.Tensor: shape=(448, 500, 1), dtype=uint8, numpy=…>'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '<tf.Tensor: shape=(448, 500, 1), dtype=uint8, numpy=…>'
- en: display_target_tensor(target)
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: display_target_tensor(target)
- en: '![Image](../images/f0263-02.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0263-02.jpg)'
- en: '**Figure 9.4 The corresponding target mask**'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 9.4 相应的目标掩码**'
- en: 'Next, let’s load our inputs and targets into two TF Datasets, and let’s split
    the files into training and validation sets. Because the dataset is very small,
    we can just load everything into memory:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们将输入和目标加载到两个 TF 数据集中，并将文件拆分为训练集和验证集。由于数据集非常小，我们可以将所有内容加载到内存中：
- en: library(tfdatasets)
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: library(tfdatasets)
- en: tf_read_image <➊
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: tf_read_image <➊
- en: function(path, format = "image", resize = NULL, …) {
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: function(path, format = "image", resize = NULL, …) {
- en: img <- path %>%
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: img <- path %>%
- en: tf$io$read_file() %>%
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: tf$io$read_file() %>%
- en: tf$io[[paste0("decode_", format)]](…)➋
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: tf$io[[paste0("decode_", format)]](…)➋
- en: if (!is.null(resize))
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: if (!is.null(resize))
- en: img <- img %>%
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: img <- img %>%
- en: tf$image$resize(as.integer(resize))➌
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: tf$image$resize(as.integer(resize))➌
- en: img
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: img
- en: '}'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: img_size <- c(200, 200)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: img_size <- c(200, 200)
- en: tf_read_image_and_resize <- function(…, resize = img_size)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: tf_read_image_and_resize <- function(…, resize = img_size)
- en: tf_read_image(…, resize = resize)➍
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: tf_read_image(…, resize = resize)➍
- en: make_dataset <- function(paths_df) {
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: make_dataset <- function(paths_df) {
- en: tensor_slices_dataset(paths_df) %>%
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: tensor_slices_dataset(paths_df) %>%
- en: dataset_map(function(path) {➎
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: dataset_map(function(path) {➎
- en: image <- path$input %>%
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: image <- path$input %>%
- en: tf_read_image_and_resize("jpeg", channels = 3L)➏
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: tf_read_image_and_resize("jpeg", channels = 3L)➏
- en: target <- path$target %>%
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: target <- path$target %>%
- en: tf_read_image_and_resize("png", channels = 1L)➐
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: tf_read_image_and_resize("png", channels = 1L)➐
- en: target <- target - 1➑
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: target <- target - 1➑
- en: list(image, target)
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: list(image, target)
- en: '}) %>%'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '}) %>%'
- en: dataset_cache() %>%➒
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: dataset_cache() %>%➒
- en: dataset_shuffle(buffer_size = nrow(paths_df)) %>%➓
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: dataset_shuffle(buffer_size = nrow(paths_df)) %>%➓
- en: dataset_batch(32)
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: dataset_batch(32)
- en: '}'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: num_val_samples <- 1000⓫
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: num_val_samples <- 1000⓫
- en: val_idx <- sample.int(nrow(image_paths), num_val_samples)
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: val_idx <- sample.int(nrow(image_paths), num_val_samples)
- en: val_paths <- image_paths[val_idx, ]⓬
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: val_paths <- image_paths[val_idx, ]⓬
- en: train_paths <- image_paths[-val_idx, ]
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: train_paths <- image_paths[-val_idx, ]
- en: validation_dataset <- make_dataset(val_paths)
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: validation_dataset <- make_dataset(val_paths)
- en: train_dataset <- make_dataset(train_paths)
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: train_dataset <- make_dataset(train_paths)
- en: ➊ **Here we define a helper to read in and resize the image using TensorFlow
    operations.**
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **在此，我们定义一个辅助函数，使用 TensorFlow 操作读取并调整图像的大小。**
- en: ➋ **Look up decode_image(), decode_jpeg(), or decode_png() from the tf$io submodule.**
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **查找 tf$io 子模块中的 decode_image()、decode_jpeg() 或 decode_png()。**
- en: ➌ **We make sure to call the tf module function with integers using as.integer().**
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **确保使用 as.integer() 将 tf 模块函数调用为整数。**
- en: ➍ **We resize everything to 200 × 200.**
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **我们将所有内容调整为 200 × 200。**
- en: ➎ **The R function passed to dataset_map() is called with symbolic tensors and
    must return symbolic tensors. dataset_map() receives a single argument here, a
    named list of two scalar string tensors, containing file paths to the input and
    target images.**
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **传递给 dataset_map() 的 R 函数使用符号张量进行调用，并且必须返回符号张量。在这里，dataset_map() 接收一个参数，即包含输入和目标图像文件路径的两个标量字符串张量的命名列表。**
- en: '➏ **Each input image has three channels: RGB values.**'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ➏ **每个输入图像都有三个通道：RGB 值。**
- en: '➐ **Each target image has a single channel: integer labels for each pixel.**'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ➐ **每个目标图像都有一个通道：每个像素的整数标签。**
- en: ➑ **Subtract 1 so that our labels become 0, 1, and 2.**
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ➑ **减去 1 以使我们的标签变为 0、1 和 2。**
- en: ➒ **Caching the dataset will store the full dataset in memory after the first
    run. If your computer doesn't have enough RAM, remove this call, and the image
    files will be loaded dynamically as needed throughout training.**
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ➒ **缓存数据集会在第一次运行后将完整数据集存储在内存中。如果您的计算机内存不足，请删除此调用，图像文件将在训练过程中根据需要动态加载。**
- en: ➓ **Shuffle the images, using the total number of samples in the data as a buffer_size.
    We make sure to call shuffle after cache.**
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ➓ **使用数据中的总样本数作为 buffer_size 进行图像洗牌。请确保在缓存后调用 shuffle。**
- en: ⓫ **Reserve 1,000 samples for validation.**
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ⓫ **保留 1,000 个样本用于验证。**
- en: ⓬ **Split the data into training and validation sets.**
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ⓬ **将数据拆分为训练集和验证集。**
- en: 'Now it’s time to define our model:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候定义我们的模型了：
- en: get_model <- function(img_size, num_classes) {
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: get_model <- function(img_size, num_classes) {
- en: conv <- function(…, padding = "same", activation = "relu")➊➋
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: conv <- function(…, padding = "same", activation = "relu")➊➋
- en: layer_conv_2d(…, padding = padding, activation = activation)➊
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: layer_conv_2d(…, padding = padding, activation = activation)➊
- en: conv_transpose <- function(…, padding = "same", activation = "relu")➊➋
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: conv_transpose <- function(…, padding = "same", activation = "relu")➊➋
- en: layer_conv_2d_transpose(…, padding = padding, activation = activation)➊
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: layer_conv_2d_transpose(…, padding = padding, activation = activation)➊
- en: input <- layer_input(shape = c(img_size, 3))
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: input <- layer_input(shape = c(img_size, 3))
- en: output <- input %>%
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: output <- input %>%
- en: layer_rescaling(scale = 1/255) %>%➌
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: layer_rescaling(scale = 1/255) %>%➌
- en: conv(64, 3, strides = 2) %>%
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: conv(64, 3, strides = 2) %>%
- en: conv(64, 3) %>%
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: conv(64, 3) %>%
- en: conv(128, 3, strides = 2) %>%
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: conv(128, 3, strides = 2) %>%
- en: conv(128, 3) %>%
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: conv(128, 3) %>%
- en: conv(256, 3, strides = 2) %>%
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: conv(256, 3, strides = 2) %>%
- en: conv(256, 3) %>%
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: conv(256, 3) %>%
- en: conv_transpose(256, 3) %>%
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: conv_transpose(256, 3) %>%
- en: conv_transpose(256, 3, strides = 2) %>%
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: conv_transpose(256, 3, strides = 2) %>%
- en: conv_transpose(128, 3) %>%
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: conv_transpose(128, 3) %>%
- en: conv_transpose(128, 3, strides = 2) %>%
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: conv_transpose(128, 3, strides = 2) %>%
- en: conv_transpose(64, 3) %>%
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: conv_transpose(64, 3) %>%
- en: conv_transpose(64, 3, strides = 2) %>%
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: conv_transpose(64, 3, strides = 2) %>%
- en: conv(num_classes, 3, activation = "softmax")➍
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: conv(num_classes, 3, activation = "softmax")➍
- en: keras_model(input, output)
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: keras_model(input, output)
- en: '}'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: model <- get_model(img_size = img_size, num_classes = 3)
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: model <- get_model(img_size = img_size, num_classes = 3)
- en: '➊ **Define local functions conv() and conv_transpose(), so we can avoid passing
    the same arguments to each call: padding = "same", activation = "relu".**'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **定义本地函数 conv() 和 conv_transpose()，以便我们可以避免在每次调用时传递相同的参数：padding = "same"、activation
    = "relu"。**
- en: ➋ **We use padding = "same" everywhere to avoid the influence of border padding
    on feature map size.**
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **我们在所有地方都使用 padding = "same" 以避免边缘填充对特征图大小的影响。**
- en: ➌ **Don't forget to rescale input images to the [0–1] range.**
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **不要忘记将输入图像重新缩放到 [0–1] 范围内。**
- en: ➍ **We end the model with a per-pixel three-way softmax to classify each output
    pixel into one of our three categories.**
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **我们在模型末尾使用每像素的三路 softmax 将每个输出像素分类到我们的三个类别之一。**
- en: 'Here is the model summary:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这是模型摘要：
- en: model
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: model
- en: '![Image](../images/f0265-01.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0265-01.jpg)'
- en: 'The first half of the model closely resembles the kind of convnet you’d use
    for image classification: a stack of Conv2D layers, with gradually increasing
    filter sizes. We down-sample our images three times by a factor of two each, ending
    up with activations of size (25, 25, 256). The purpose of this first half is to
    encode the images into smaller feature maps, where each spatial location (or pixel)
    contains information about a large spatial chunk of the original image. You can
    understand it as a kind of compression.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的前半部分紧密地类似于用于图像分类的卷积神经网络：一堆Conv2D层，逐渐增加的过滤器大小。我们通过每次减少两倍来三次降采样我们的图像，最终得到大小为(25,
    25, 256)的激活值。这个前半部分的目的是将图像编码成更小的特征图，其中每个空间位置（或像素）包含有关原始图像的大空间块的信息。你可以将其理解为一种压缩。
- en: 'One important difference between the first half of this model and the classification
    models you’ve seen before is the way we do downsampling: in the classification
    convnets from the last chapter, we used MaxPooling2D layers to downsample feature
    maps. Here, we downsample by adding strides to every other convolution layer (if
    you don’t remember the details of how convolution strides work, see “Understanding
    convolution strides” in section 8.1.1). We do this because, in the case of image
    segmentation, we care a lot about the *spatial location* of information in the
    image, because we need to produce per-pixel target masks as output of the model.
    When you do 2 × 2 max pooling, you are completely destroying location information
    within each pooling window: you return one scalar value per window, with zero
    knowledge of which of the four locations in the windows the value came from. So
    although max pooling layers perform well for classification tasks, they would
    hurt us quite a bit for a segmentation task. Meanwhile, strided convolutions do
    a better job at downsampling feature maps while retaining location information.
    Throughout this book, you’ll notice that we tend to use strides instead of max
    pooling in any model that cares about feature location, such as the generative
    models in chapter 12.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型的前半部分和你之前见过的分类模型之间的一个重要区别是我们进行降采样的方式：在上一章的分类卷积网络中，我们使用MaxPooling2D层来降采样特征图。在这里，我们通过在每个其他卷积层中添加步幅来进行降采样（如果你不记得卷积步幅是如何工作的细节，请参见8.1.1节中的“理解卷积步幅”）。我们这样做是因为，在图像分割的情况下，我们非常关心图像中信息的*空间位置*，因为我们需要将像素级的目标掩码作为模型的输出。当你进行2×2最大池化时，你完全破坏了每个池化窗口内的位置信息：你返回每个窗口的一个标量值，而对于窗口中的四个位置之一，你完全不知道值是从哪个位置来的。因此，虽然最大池化层在分类任务中表现良好，但在分割任务中，它们会对我们造成相当大的伤害。与此同时，步幅卷积在降采样特征图的同时保留了位置信息方面做得更好。在本书中，你会注意到，我们倾向于在任何关心特征位置的模型中使用步幅而不是最大池化，比如第12章中的生成模型。
- en: 'The second half of the model is a stack of Conv2DTranspose layers. What are
    those? Well, the output of the first half of the model is a feature map of shape
    (25, 25, 256), but we want our final output to have the same shape as the target
    masks, (200, 200, 3). Therefore, we need to apply a kind of *inverse* of the transformations
    we’ve applied so far—something that will *upsample* the feature maps instead of
    downsampling them. That’s the purpose of the Conv2DTranspose layer: you can think
    of it as a kind of convolution layer that *learns to upsample*. If you have an
    input of shape (100, 100, 64), and you run it through a layer layer_conv_2d(128,
    3, strides = 2, padding = “same”), you get an output of shape (50, 50, 128). If
    you run this output through a layer layer_conv_2d_transpose(64, 3, strides = 2,
    padding = “same”), you get back an output of shape (100, 100, 64), the same as
    the original. So after compressing our inputs into feature maps of shape (25,
    25, 256) via a stack of Conv2D layers, we can simply apply the corresponding sequence
    of Conv2DTranspose layers to get back to images of shape (200, 200, 3).'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的后半部分是一堆Conv2DTranspose层。那些是什么？好吧，模型的前半部分的输出是形状为(25, 25, 256)的特征图，但我们希望我们的最终输出具有与目标掩模相同的形状，(200,
    200, 3)。因此，我们需要应用一种*反向*我们到目前为止所应用的转换的方法——一种将特征图*上采样*而不是下采样的方法。这就是Conv2DTranspose层的目的：你可以将其视为一种学习上采样的卷积层。如果你有一个形状为(100,
    100, 64)的输入，并且你通过一个层layer_conv_2d(128, 3, strides = 2, padding = “same”)，你会得到一个形状为(50,
    50, 128)的输出。如果你将这个输出通过一个层layer_conv_2d_transpose(64, 3, strides = 2, padding =
    “same”)，你会得到一个形状为(100, 100, 64)的输出，与原始的相同。因此，通过一堆Conv2D层将我们的输入压缩成形状为(25, 25, 256)的特征图后，我们可以简单地应用相应的Conv2DTranspose层序列，以得到形状为(200,
    200, 3)的图像。
- en: 'We can now compile and fit our model:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以编译并拟合我们的模型：
- en: model %>%
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: model %>%
- en: compile(optimizer = "rmsprop",
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: compile(optimizer = "rmsprop",
- en: loss = "sparse_categorical_crossentropy")
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: loss = "sparse_categorical_crossentropy")
- en: callbacks <- list(
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: callbacks <- list(
- en: callback_model_checkpoint("oxford_segmentation.keras",
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: callback_model_checkpoint("oxford_segmentation.keras",
- en: save_best_only = TRUE))
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: save_best_only = TRUE))
- en: history <- model %>% fit(
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: history <- model %>% fit(
- en: train_dataset,
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: train_dataset,
- en: epochs = 50,
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 50,
- en: callbacks = callbacks,
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: callbacks = callbacks,
- en: validation_data = validation_dataset
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: validation_data = validation_dataset
- en: )
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: 'NOTE During training, you may see a warning like Corrupt JPEG data: premature
    end of data segment. The image dataset is not perfect, but the tf$io module functions
    can recover gracefully.'
  id: totrans-185
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意 在训练过程中，你可能会看到类似 Corrupt JPEG  premature end of data segment 的警告。图像数据集并不完美，但
    tf$io 模块函数可以优雅地恢复。
- en: 'Let’s display our training and validation loss (see [figure 9.5](#fig9-5)):'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们显示我们的训练和验证损失（参见[图 9.5](#fig9-5)）：
- en: plot(history)
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制(history)
- en: '![Image](../images/f0267-01.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0267-01.jpg)'
- en: '**Figure 9.5 Displaying training and validation loss curves**'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 9.5 显示训练和验证损失曲线**'
- en: 'You can see that we start overfitting midway, around epoch 25\. Let’s reload
    our best-performing model according to the validation loss and demonstrate how
    to use it to predict a segmentation mask (see [figure 9.6](#fig9-6)):'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到我们在中途开始过拟合，大约在第 25 个周期。让我们根据验证损失重新加载我们表现最佳的模型，并演示如何使用它来预测一个分割掩码（参见[图 9.6](#fig9-6)）：
- en: model <- load_model_tf("oxford_segmentation.keras")
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: model <- load_model_tf("oxford_segmentation.keras")
- en: test_image <- val_paths$input[309] %>%
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: test_image <- val_paths$input[309] %>%
- en: tf_read_image_and_resize("jpeg", channels = 3L)
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: tf_read_image_and_resize("jpeg", channels = 3L)
- en: predicted_mask_probs <
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: predicted_mask_probs <
- en: model(test_image[tf$newaxis, , , ])➊
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: model(test_image[tf$newaxis, , , ])➊
- en: predicted_mask <➋
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: predicted_mask <➋
- en: tf$argmax(predicted_mask_probs, axis = -1L)➌
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: tf$argmax(predicted_mask_probs, axis = -1L)➌
- en: predicted_target <- predicted_mask + 1
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: predicted_target <- predicted_mask + 1
- en: par(mfrow = c(1, 2))
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: par(mfrow = c(1, 2))
- en: display_image_tensor(test_image)
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: display_image_tensor(test_image)
- en: display_target_tensor(predicted_target)
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: display_target_tensor(predicted_target)
- en: ➊ **tf$newaxis adds a batch dimension, because our model expects batches of
    images. model() returns a Tensor with shape=(1, 200, 200, 3), dtype=float32.**
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **tf$newaxis 添加了一个批次维度，因为我们的模型期望批次的图像。 model() 返回的 Tensor 形状为 (1, 200, 200,
    3)，dtype=float32。**
- en: ➋ **predicted_mask is a Tensor with shape=(1, 200, 200), dtype=int64.**
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **predicted_mask 是形状为 (1, 200, 200) 的 Tensor，dtype=int64。**
- en: ➌ **tf$argmax() is similar to which.max() in R. A key difference is that tf$argmax()
    returns 0-based values. The base R equivalent of tf$argmax(x, axis = -1L) is apply(x,
    c(1, 2, 3), which.max) - 1L.**
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **tf$argmax() 类似于 R 中的 which.max()。一个关键区别是 tf$argmax() 返回基于 0 的值。tf$argmax(x,
    axis = -1L) 的 R 基本等价是 apply(x, c(1, 2, 3), which.max) - 1L。**
- en: '![Image](../images/f0268-01.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0268-01.jpg)'
- en: '**Figure 9.6 A test image and its predicted segmentation mask**'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 9.6 测试图像及其预测的分割掩码**'
- en: There are a couple of small artifacts in our predicted mask. Nevertheless, our
    model appears to work nicely.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们预测的掩码中有一些小的伪像。尽管如此，我们的模型似乎工作得很好。
- en: 'By this point, throughout chapter 8 and the beginning of chapter 9, you’ve
    learned the basics of how to perform image classification and image segmentation:
    you can already accomplish a lot with what you know. However, the convnets that
    experienced engineers develop to solve real-world problems aren’t quite as simple
    as those we’ve been using in our demonstrations so far. You’re still lacking the
    essential mental models and thought processes that enable experts to make quick
    and accurate decisions about how to put together state-of-the-art models. To bridge
    that gap, you need to learn about *architecture patterns*. Let’s dive in.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在第8章和第9章的开头，你已经学会了如何进行图像分类和图像分割的基础知识：你已经可以用你所知道的知识做很多事情了。然而，有经验的工程师开发用于解决现实世界问题的卷积网络并不像我们迄今为止在演示中使用的那么简单。你仍然缺少使专家能够迅速准确地决定如何组合最先进模型的基本思维模式和思考过程。为了弥补这一差距，你需要学习*架构模式*。让我们深入了解一下。
- en: 9.3 Modern convnet architecture patterns
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3 现代卷积网络架构模式
- en: 'A model’s “architecture” is the sum of the choices that went into creating
    it: which layers to use, how to configure them, and in what arrangement to connect
    them. These choices define the *hypothesis space* of your model: the space of
    possible functions that gradient descent can search over, parameterized by the
    model’s weights. Like feature engineering, a good hypothesis space encodes *prior
    knowledge* that you have about the problem at hand and its solution. For instance,
    using convolution layers means that you know in advance that the relevant patterns
    present in your input images are translation invariant. To effectively learn from
    data, you need to make assumptions about what you’re looking for.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 一个模型的“架构”是创建它所做选择的总和：使用哪些层，如何配置它们，以及以何种排列方式连接它们。这些选择定义了你的模型的*假设空间*：由模型的权重参数化的梯度下降可以搜索的可能函数空间。就像特征工程一样，一个良好的假设空间编码了你对手头问题及其解决方案的*先验知识*。例如，使用卷积层意味着你事先知道你的输入图像中存在的相关模式是平移不变的。为了有效地从数据中学习，你需要对你要寻找的内容做出假设。
- en: Model architecture is often the difference between success and failure. If you
    make inappropriate architecture choices, your model may be stuck with suboptimal
    metrics, and no amount of training data will save it. Inversely, a good model
    architecture will accelerate learning and will enable your model to make efficient
    use of the training data available, reducing the need for large datasets. A good
    model architecture is one that *reduces the size of the search space* or otherwise
    *makes it easier to converge to a good point of the search space*. Just like feature
    engineering and data curation, model architecture is all about *making the problem
    simpler* for gradient descent to solve. And remember that gradient descent is
    a pretty stupid search process, so it needs all the help it can get.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 模型架构通常是成功与失败之间的区别。如果你做出不合适的架构选择，你的模型可能会陷入次优的度量指标中，任何数量的训练数据都无法挽救它。相反，一个良好的模型架构将加速学习，并使你的模型能够有效地利用可用的训练数据，减少对大型数据集的需求。一个好的模型架构是一个*减少搜索空间大小*或以其他方式*使其更容易收敛到搜索空间的好点*的架构。就像特征工程和数据策划一样，模型架构的目的是为了使梯度下降更容易解决问题。记住，梯度下降是一个相当愚蠢的搜索过程，所以它需要尽可能多的帮助。
- en: 'Model architecture is more an art than a science. Experienced machine learning
    engineers are able to intuitively cobble together high-performing models on their
    first try, while beginners often struggle to create a model that trains at all.
    The keyword here is *intuitively*: no one can give you a clear explanation of
    what works and what doesn’t. Experts rely on pattern-matching, an ability that
    they acquire through extensive practical experience. You’ll develop your own intuition
    throughout this book. However, it’s not *all* about intuition, either—there isn’t
    much in the way of actual science, but as in any engineering discipline, there
    are best practices.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 模型架构更像是一门艺术而不是科学。经验丰富的机器学习工程师能够直观地在第一次尝试时拼凑出高性能的模型，而初学者往往很难创建一个能够训练的模型。这里的关键词是*直觉*：没有人能够清楚地解释什么有效，什么无效。专家依赖于模式匹配，这是他们通过广泛的实践经验获得的能力。你将在本书中培养自己的直觉。然而，这并不全是关于直觉——实际上并没有太多的科学内容，但和任何工程学科一样，都有最佳实践。
- en: 'In the following sections, we’ll review a few essential convnet architecture
    best practices: in particular, *residual connections, batch normalization*, and
    *separable convolutions*. Once you master how to use them, you will be able to
    build highly effective image models. We will apply them to our cat vs. dog classification
    problem.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将回顾一些重要的 convnet 架构最佳实践：特别是*残差连接、批量归一化*和*可分离卷积*。一旦你掌握了如何使用它们，你就能构建高效的图像模型。我们将把它们应用到我们的猫
    vs. 狗分类问题上。
- en: 'Let’s start from the bird’s-eye view: the modularity-hierarchy-reuse (MHR)
    formula for system architecture.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从鸟瞰图开始：系统架构的模块化-层次结构-重用（MHR）公式。
- en: 9.3.1 Modularity, hierarchy, and reuse
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.1 模块化、层次结构和重用
- en: 'If you want to make a complex system simpler, you can apply a universal recipe:
    just structure your amorphous soup of complexity into *modules*, organize the
    modules into a *hierarchy*, and start *reusing* the same modules in multiple places
    as appropriate (“reuse” is another word for *abstraction* in this context). That’s
    the MHR formula (modularity-hierarchy-reuse), and it underlies system architecture
    across pretty much every domain where the term “architecture” is used. It’s at
    the heart of the organization of any system of meaningful complexity, whether
    it’s a cathedral, your own body, the US Navy, or the Keras codebase (see [figure
    9.7](#fig9-7)).'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想让一个复杂系统变得简单，您可以应用一个通用的方法：只需将您的复杂无序的复杂性结构化为*模块*，将模块组织成*层次结构*，并开始在适当的地方*重复使用*相同的模块（在这种情况下，“重用”是“抽象”的另一个词）。这就是MHR公式（模块化-层次化-重用），它是几乎每个领域中“架构”一词被使用的系统架构的基础。它是任何有意义的复杂系统的组织核心，无论是大教堂、您自己的身体、美国海军还是Keras代码库（见[图9.7](#fig9-7)）。
- en: '![Image](../images/f0270-01.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0270-01.jpg)'
- en: '**Figure 9.7 Complex systems follow a hierarchical structure and are organized
    into distinct modules, which are reused multiple times (such as your four limbs,
    which are all variants of the same blueprint, or your 20 “fingers”).**'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '**图9.7 复杂系统遵循分层结构，并被组织成不同的模块，这些模块被多次重用（例如，您的四肢，它们都是同一个蓝图的变体，或者您的20个“手指”）。**'
- en: 'If you’re a software engineer, you’re already keenly familiar with these principles:
    an effective codebase is one that is modular and hierarchical and where you don’t
    reimplement the same thing twice, but instead rely on reusable classes and functions.
    If you factor your code by following these principles, you could say you’re doing
    “software architecture.”'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您是一名软件工程师，您已经非常熟悉这些原则：一个有效的代码库是模块化和层次化的，您不会两次实现相同的事物，而是依靠可重用的类和函数。如果您按照这些原则对代码进行分解，您可以说您正在进行“软件架构”。
- en: 'Deep learning itself is simply the application of this recipe to continuous
    optimization via gradient descent: you take a classic optimization technique (gradient
    descent over a continuous function space), and you structure the search space
    into modules (layers), organized into a deep hierarchy (often just a stack, the
    simplest kind of hierarchy), where you reuse whatever you can (e.g., convolutions
    are all about reusing the same information in different spatial locations).'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习本身只是通过梯度下降对连续优化应用此方法：您采用经典的优化技术（在连续函数空间上的梯度下降），并将搜索空间结构化为模块（层），组织成深层次结构（通常只是一个堆栈，最简单的层次结构），在其中尽可能地重复使用（例如，卷积是关于在不同空间位置重用相同信息）。
- en: Likewise, deep learning model architecture is primarily about making clever
    use of modularity, hierarchy, and reuse. You’ll notice that all popular convnet
    architectures are not only structured into layers, they’re structured into repeated
    groups of layers (called “blocks” or “modules”). For instance, the popular VGG16
    architecture we used in the previous chapter is structured into repeated “conv,
    conv, max pooling” blocks (see [figure 9.8](#fig9-8)).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，深度学习模型架构主要是关于巧妙地利用模块化、层级化和重用。您会注意到，所有流行的卷积神经网络架构不仅被组织成层，它们还被组织成重复的层组（称为“块”或“模块”）。例如，我们在上一章中使用的流行的VGG16架构被组织成重复的“卷积，卷积，最大池化”块（见[图9.8](#fig9-8)）。
- en: 'Further, most convnets often feature pyramid-like structures (*feature hierarchies*).
    Recall, for example, the progression in the number of convolution filters we used
    in the first convnet we built in the previous chapter: 32, 64, 128\. The number
    of filters grows with layer depth, whereas the size of the feature maps shrinks
    accordingly. You’ll notice the same pattern in the blocks of the VGG16 model (see
    [figure 9.8](#fig9-8)).'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，大多数卷积神经网络通常具有金字塔状结构（*特征层次结构*）。例如，回想一下我们在上一章中构建的第一个卷积神经网络中使用的卷积滤波器数量的递增：32，64，128。随着层深度的增加，滤波器的数量也会增加，而特征图的大小相应缩小。您会在VGG16模型的块中注意到相同的模式（见[图9.8](#fig9-8)）。
- en: '![Image](../images/f0271-01.jpg)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0271-01.jpg)'
- en: '**Figure 9.8 The VGG16 architecture: Note the repeated layer blocks and the
    pyramid-like structure of the feature maps.**'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '**图9.8 VGG16架构：请注意重复的层块和特征图的金字塔状结构。**'
- en: 'Deeper hierarchies are intrinsically good because they encourage feature reuse
    and, therefore, abstraction. In general, a deep stack of narrow layers performs
    better than a shallow stack of large layers. However, there’s a limit to how deep
    you can stack layers, due to the problem of *vanishing gradients*. This leads
    us to our first essential model architecture pattern: residual connections.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 更深的层次结构本质上是有好处的，因为它们鼓励特征的重复利用，因此也鼓励抽象化。一般来说，窄层次的深度堆栈比大层次的浅堆栈表现更好。然而，你可以堆叠多深的层次有一个限制，这是由于*消失的梯度*问题所致。这导致我们的第一个基本模型架构模式：残差连接。
- en: '**On the importance of ablation studies in deep learning research**'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '**关于深度学习研究中消融研究的重要性**'
- en: Deep learning architectures are often more *evolved* than designed—they were
    developed by repeatedly trying things and selecting what seemed to work. Much
    like in biological systems, if you take any complicated experimental deep learning
    setup, chances are you can remove a few modules (or replace some trained features
    with random ones) with no loss of performance.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习架构通常比设计更*进化*——它们是通过反复尝试并选择看起来有效的方法而发展起来的。就像在生物系统中一样，如果你拿任何一个复杂的实验性深度学习设置，很可能你可以移除几个模块（或用随机的训练特征替换一些），而不会损失性能。
- en: 'This is made worse by the incentives that deep learning researchers face: by
    making a system more complex than necessary, they can make it appear more interesting
    or more novel and thus increase their chances of getting a paper through the peer-review
    process. If you read lots of deep learning papers, you will notice that they’re
    often optimized for peer review in both style and content in ways that actively
    hurt clarity of explanation and reliability of results. For instance, mathematics
    in deep learning papers is rarely used for clearly formalizing concepts or deriving
    non-obvious results—rather, it gets leveraged as a *signal of seriousness*, like
    an expensive suit on a salesman.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这一情况被深度学习研究人员面临的激励所加剧：通过使系统比必要复杂，他们可以使其看起来更有趣或更新颖，从而增加他们通过同行评审流程的机会。如果你阅读了大量的深度学习论文，你会注意到它们通常在风格和内容上都被优化，以在主动损害解释的清晰度和结果的可靠性方面通过同行评审。例如，在深度学习论文中，数学很少被用来清晰地形式化概念或推导非显而易见的结果——相反，它被利用作为*严肃性的信号*，就像一个销售员身上的昂贵西装一样。
- en: 'The goal of research shouldn’t be merely to publish but to generate reliable
    knowledge. Crucially, understanding *causality* in your system is the most straightforward
    way to generate reliable knowledge. And there’s a very low-effort way to look
    into causality: *ablation studies*. Ablation studies consist of systematically
    trying to remove parts of a system—making it simpler—to identify where its performance
    actually comes from. If you find that X + Y + Z gives you good results, also try
    X, Y, Z, X + Y, X + Z, and Y + Z, and see what happens.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 研究的目标不应仅仅是发表论文，而应是产生可靠的知识。至关重要的是，理解系统中的*因果关系*是产生可靠知识的最直接方式。而且有一种非常低成本的方法来探究因果关系：*消融研究*。消融研究包括系统地尝试去除系统的一部分——使其更简单——以确定其性能实际来自何处。如果你发现
    X + Y + Z 给你良好的结果，也试试 X、Y、Z、X + Y、X + Z 和 Y + Z，看看会发生什么。
- en: 'If you become a deep learning researcher, cut through the noise in the research
    process: do ablation studies for your models. Always ask, “Could there be a simpler
    explanation? Is this added complexity really necessary? Why?”'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你成为了一名深度学习研究人员，请在研究过程中抛弃噪声：为你的模型做消融研究。始终问自己，“可能存在更简单的解释吗？这种增加的复杂性真的是必要的吗？为什么？”
- en: 9.3.2 Residual connections
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.2 残差连接
- en: You probably know about the game of Telephone, also called Chinese Whispers
    in the UK and *téléphone arabe* in France, where an initial message is whispered
    in the ear of a player, who then whispers it in the ear of the next player, and
    so on. The final message ends up bearing little resemblance to its original version.
    It’s a fun metaphor for the cumulative errors that occur in sequential transmission
    over a noisy channel.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能知道电话游戏，也称为英国的Chinese Whispers和法国的*téléphone arabe*，在这个游戏中，一个初始消息被耳语给一名玩家，然后他再耳语给下一个玩家，依此类推。最终的消息与其原始版本几乎没有什么相似之处。这是一个有趣的比喻，说明了在嘈杂的信道上的顺序传输中产生的累积误差。
- en: 'As it happens, backpropagation in a sequential deep learning model is pretty
    similar to the game of Telephone. You’ve got a chain of functions, like this one:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 恰好，顺序深度学习模型中的反向传播与电话游戏非常相似。你有一系列的函数，就像这个：
- en: y = f4(f3(f2(f1(x))))
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: y = f4(f3(f2(f1(x))))
- en: The name of the game is to adjust the parameters of each function in the chain
    based on the error recorded on the output of f4 (the loss of the model). To adjust
    f1, you’ll need to percolate error information through f2, f3, and f4. However,
    each successive function in the chain introduces some amount of noise. If your
    function chain is too deep, this noise starts overwhelming gradient information,
    and backpropagation stops working. Your model won’t train at all. This is the
    *vanishing gradients* problem.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: x <- layer_add(c(x, residual))
- en: 'The fix is simple: just force each function in the chain to be nondestructive—to
    retain a noiseless version of the information contained in the previous input.
    The easiest way to implement this is to use a *residual connection*.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 保存指向原始输入的指针。这称为残差。
- en: 'It’s dead easy: just add the input of a layer or block of layers back to its
    output (see [figure 9.9](#fig9-9)). The residual connection acts as an *information
    shortcut* around destructive or noisy blocks (such as blocks that contain relu
    activations or dropout layers), enabling error gradient information from early
    layers to propagate noiselessly through a deep network. This technique was introduced
    in 2015 with the ResNet family of models (developed by He et al. at Microsoft).^([1](#Rendnote1))'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这很容易：只需将图层或图层块的输入添加回其输出即可（参见[图9.9](#fig9-9)）。残差连接充当*信息快捷方式*，绕过破坏性或嘈杂的块（例如包含relu激活或dropout层的块），使早期层的错误梯度信息能够无噪声地传播到深层网络。这项技术是在2015年由ResNet系列模型（由微软的何等人开发）介绍的。
- en: '![Image](../images/f0273-01.jpg)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0273-01.jpg)'
- en: '**Figure 9.9 A residual connection around a processing block**'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: residual <- x
- en: In practice, you’d implement a residual connection as follows.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，您将如下实现残差连接。
- en: Listing 9.1 A residual connection in pseudocode
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.9：处理块周围的残差连接
- en: x <- …➊
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: x <- ...
- en: residual <- x➋
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方法很简单：只需强制链中的每个函数为非破坏性-保留前一输入中包含的信息的无噪声版本。实现这一点的最简单方法是使用残差连接。
- en: x <- block(x)➌
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: x <- x %>% layer_conv_2d(64, 3, activation = "relu", padding = "same")
- en: x <- layer_add(c(x, residual))➍
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 具有不同数量滤波器的残差块
- en: ➊ **Some input tensor**
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 一些输入张量
- en: ➋ **Save a pointer to the original input. This is called the residual.**
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: x <- inputs %>% layer_conv_2d(32, 3, activation = "relu")
- en: ➌ **This computation block can potentially be destructive or noisy, and that's
    fine.**
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 此计算块可能是破坏性或嘈杂的，这没问题。
- en: '➍ **Add the original input to the layer''s output: the final output will thus
    always preserve full information about the original input.**'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '[将原始输入添加到层的输出中：因此，最终输出将始终保留有关原始输入的完整信息。](https://wiki.example.org/feynmans_learning_method)'
- en: Note that adding the input back to the output of a block implies that the output
    should have the same shape as the input. However, this is not the case if your
    block includes convolutional layers with an increased number of filters or a max-pooling
    layer. In such cases, use a 1 × 1 layer_conv_2d() with no activation to linearly
    project the residual to the desired output shape (see listing 9.2). You’d typically
    use padding = “same” in the convolution layers in your target block so as to avoid
    spatial downsampling due to padding, and you’d use strides in the residual projection
    to match any downsampling caused by a max pooling layer (see listing 9.3).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏的名字是根据在f4的输出上记录的错误来调整链中每个函数的参数（模型的损失）。要调整f1，您需要通过f2，f3和f4渗透错误信息。但是，链中的每个连续函数都引入了一定量的噪音。如果您的函数链太深，则此噪音开始淹没梯度信息，并且反向传播停止工作。您的模型根本无法训练。这就是梯度消失的问题。
- en: Listing 9.2 Residual block where the number of filters changes
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 伪代码中的残差连接
- en: inputs <- layer_input(shape = c(32, 32, 3))
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: inputs <- layer_input(shape = c(32, 32, 3))
- en: x <- inputs %>% layer_conv_2d(32, 3, activation = "relu")
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，将输入添加回块的输出意味着输出应具有与输入相同的形状。但是，如果您的块包含具有增加的滤波器数量或最大池化层的卷积层，则情况并非如此。在这种情况下，请使用1×1的layer_conv_2d（）层，不带激活函数将残差线性投影到所需的输出形状（请参阅清单9.2）。您通常会在目标块中的卷积层中使用padding
    =“same”，以避免由于填充而导致的空间降采样，并且您会在残差投影中使用步幅以匹配由最大池化层引起的任何降采样（请参阅清单9.3）。
- en: residual <- x➊
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: x <- block(x)
- en: x <- x %>% layer_conv_2d(64, 3, activation = "relu", padding = "same")➋
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: residual <- x
- en: residual <- residual %>% layer_conv_2d(64, 1)➌
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: residual <- residual %>% layer_conv_2d(64, 1)
- en: x <- layer_add(c(x, residual))➍
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: x <- layer_add(c(x, residual))
- en: ➊ **Set aside the residual.**
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 设置残差。
- en: '➋ **This is the layer around which we create a residual connection: it increases
    the number of output filers from 32 to 64\. Note that we use padding = "same"
    to avoid downsampling due to padding.**'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **这是我们创建残差连接的层：它将输出过滤器数量从 32 增加到 64。请注意，我们使用填充 = "same" 来避免由于填充而造成的降采样。**
- en: ➌ **The residual had only 32 filters, so we use a 1 × 1 layer_conv_2d to project
    it to the correct shape.**
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **残差仅有 32 个过滤器，因此我们使用 1 × 1 的 layer_conv_2d 将其投影到正确的形状。**
- en: ➍ **Now the block output and the residual have the same shape and can be added.**
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **现在区块输出和残差具有相同的形状，可以相加。**
- en: Listing 9.3 Case where the target block includes a max-pooling layer
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 第 9.3 节 目标区块包括最大池化层的情况
- en: inputs <- layer_input(shape = c(32, 32, 3))
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: inputs <- layer_input(shape = c(32, 32, 3))
- en: x <- inputs %>% layer_conv_2d(32, 3, activation = "relu")
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: x <- inputs %>% layer_conv_2d(32, 3, 激活 = "relu")
- en: residual <- x➊
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: residual <- x➊
- en: x <- x %>%➋
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: x <- x %>%➋
- en: layer_conv_2d(64, 3, activation = "relu", padding = "same") %>%
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: layer_conv_2d(64, 3, 激活 = "relu", 填充 = "same") %>%
- en: layer_max_pooling_2d(2, padding = "same")
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: layer_max_pooling_2d(2, 填充 = "same")
- en: residual <- residual %>%
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: residual <- residual %>%
- en: layer_conv_2d(64, 1, strides = 2)➌
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: layer_conv_2d(64, 1, 步幅 = 2)➌
- en: x <- layer_add(list(x, residual))➍
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: x <- layer_add(list(x, residual))➍
- en: ➊ **Set aside the residual.**
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **将残差放在一边。**
- en: '➋ **This is the block of two layers around which we create a residual connection:
    it includes a 2 × 2 max pooling layer. Note that we use padding = "same" in both
    the convolution layer and the max-pooling layer to avoid downsampling due to padding.**'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **这是我们创建残差连接的两层区块：它包括一个 2 × 2 的最大池化层。请注意，我们在卷积层和最大池化层中都使用填充 = "same"，以避免由于填充而造成的降采样。**
- en: ➌ **We use strides = 2 in the residual projection to match the downsampling
    created by the max-pooling layer.**
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **我们在残差投影中使用 strides = 2，以匹配由最大池化层创建的降采样。**
- en: ➍ **Now the block output and the residual have the same shape and can be added.**
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **现在区块输出和残差具有相同的形状，可以相加。**
- en: 'To make these ideas more concrete, here’s an example of a simple convnet structured
    into a series of blocks, each made of two convolution layers and one optional
    max-pooling layer, with a residual connection around each block:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这些想法更加具体，这里有一个简单的卷积网络示例，由一系列区块组成，每个区块由两个卷积层和一个可选的最大池化层组成，并且每个区块周围都有一个残差连接：
- en: inputs <- layer_input(shape = c(32, 32, 3))
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: inputs <- layer_input(shape = c(32, 32, 3))
- en: x <- layer_rescaling(inputs, scale = 1/255)
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: x <- layer_rescaling(inputs, 缩放 = 1/255)
- en: residual_block <- function(x, filters, pooling = FALSE) {➊
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: residual_block <- function(x, 过滤器, 池化 = FALSE) {➊
- en: residual <- x
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: residual <- x
- en: x <- x %>%
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: x <- x %>%
- en: layer_conv_2d(filters, 3, activation = "relu", padding = "same") %>%
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: layer_conv_2d(过滤器, 3, 激活 = "relu", 填充 = "same") %>%
- en: layer_conv_2d(filters, 3, activation = "relu", padding = "same")
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: layer_conv_2d(过滤器, 3, 激活 = "relu", 填充 = "same")
- en: if (pooling) {➋
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: if (池化) {➋
- en: x <- x %>% layer_max_pooling_2d(pool_size = 2, padding = "same")
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: x <- x %>% layer_max_pooling_2d(池化大小 = 2, 填充 = "same")
- en: residual <- residual %>% layer_conv_2d(filters, 1, strides = 2)
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: residual <- residual %>% layer_conv_2d(过滤器, 1, 步幅 = 2)
- en: '} else if (filters != dim(residual)[4]) {➌'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '} else if (过滤器 != dim(residual)[4]) {➌'
- en: residual <- residual %>% layer_conv_2d(filters, 1)
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: residual <- residual %>% layer_conv_2d(过滤器, 1)
- en: '}'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: layer_add(list(x, residual))
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: layer_add(list(x, residual))
- en: '}'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: outputs <- x %>%
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: outputs <- x %>%
- en: residual_block(filters = 32, pooling = TRUE) %>%➍
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: residual_block(过滤器 = 32, 池化 = TRUE) %>%➍
- en: residual_block(filters = 64, pooling = TRUE) %>%➎
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: residual_block(过滤器 = 64, 池化 = TRUE) %>%➎
- en: residual_block(filters = 128, pooling = FALSE) %>%➏
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: residual_block(过滤器 = 128, 池化 = FALSE) %>%➏
- en: layer_global_average_pooling_2d() %>%
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: layer_global_average_pooling_2d() %>%
- en: layer_dense(units = 1, activation = "sigmoid")
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(units = 1, 激活 = "sigmoid")
- en: model <- keras_model(inputs = inputs, outputs = outputs)
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model(inputs = inputs, outputs = outputs)
- en: ➊ **Utility function to apply a convolutional block with a residual connection,
    with an option to add max pooling**
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **应用带有残差连接的卷积区块的实用函数，可以选择添加最大池化**
- en: ➋ **If we use max pooling, we add a strided convolution to project the residual
    to the expected shape.**
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **如果我们使用最大池化，我们添加一个步幅卷积来将残差投影到预期形状。**
- en: ➌ **If we don't use max pooling, we project the residual only if the number
    of channels has changed.**
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **如果我们不使用最大池化，只有当通道数量发生变化时，我们才投影残差。**
- en: ➍ **First block**
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **第一个区块**
- en: ➎ **Second block; note the increasing filter count in each block.**
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **第二个区块；请注意每个区块中过滤器数量的增加。**
- en: ➏ **The last block doesn't need a max-pooling layer, because we will apply global
    average pooling right after it.**
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: ➏ **最后一个区块不需要最大池化层，因为我们将在其后立即应用全局平均池化。**
- en: 'This is the model summary we get:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们得到的模型摘要：
- en: model
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: model
- en: '![Image](../images/f0275-01.jpg)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0275-01.jpg)'
- en: With residual connections, you can build networks of arbitrary depth, without
    having to worry about vanishing gradients.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 有了残差连接，可以构建任意深度的网络，而不必担心梯度消失的问题。
- en: 'Now let’s move on to the next essential convnet architecture pattern: *batch
    normalization*.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们进入下一个重要的卷积神经网络架构模式：*批归一化*。
- en: 9.3.3 Batch normalization
  id: totrans-310
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.3 批归一化
- en: '*Normalization* is a broad category of methods that seek to make different
    samples seen by a machine learning model more similar to each other, which helps
    the model learn and generalize well to new data. The most common form of data
    normalization is one you’ve already seen several times in this book: centering
    the data on zero by subtracting the mean from the data and giving the data a unit
    standard deviation by dividing the data by its standard deviation. In effect,
    this makes the assumption that the data follows a normal (or Gaussian) distribution
    and makes sure this distribution is centered and scaled to unit variance:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '*归一化*是一类方法，旨在使机器学习模型看到的不同样本之间更相似，这有助于模型学习和推广到新数据。最常见的数据归一化形式是本书中已经多次使用的形式：通过从数据中减去均值来将数据基准中心化，并通过使用其标准差将数据分配为单位标准差。实际上，这假设数据遵循正态（高斯）分布，并确保此分布居中并缩放到单位方差:'
- en: normalize_data <- apply(data, <axis>, function(x) (x - mean(x)) / sd(x))
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: normalize_data <- apply(data, <axis>, function(x) (x - mean(x)) / sd(x))
- en: 'Previous examples in this book normalized data before feeding it into models.
    But data normalization may be of interest after every transformation operated
    by the network: even if the data entering a Dense or Conv2D network has a 0 mean
    and unit variance, there’s no reason to expect a priori that this will be the
    case for the data coming out. Could normalizing intermediate activations help?'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中之前的示例在将数据馈送到模型之前对数据进行了归一化。但是，每个网络操作之后，数据归一化可能会引起兴趣：即使进入Dense或Conv2D网络的数据具有0的均值和单位方差，也没有理由预期出来结果仍然是如此。对中间激活进行规范化有用吗？
- en: Batch normalization does just that. It’s a type of layer (layer_batch_normalization()
    in Keras) introduced in 2015 by Ioffe and Szegedy^([2](#Rendnote2)) ; it can adaptively
    normalize data even as the mean and variance change over time during training.
    During training, it uses the mean and variance of the current batch of data to
    normalize samples, and during inference (when a big enough batch of representative
    data may not be available), it uses an exponential moving average of the batchwise
    mean and variance of the data seen during training.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 批归一化就是做到这一点的方法。它是一种层类型（在Keras中称为layer_batch_normalization()），由Ioffe和Szegedy^([2](#Rendnote2))于2015年推出，可以自适应地对数据进行归一化，即使均值和方差在训练过程中随时间变化。在训练过程中，它使用当前批次数据的均值和方差来规范化样本，在推理（当大量具有代表性的数据批次可能不可用时）中，它使用在训练期间看到的数据的批次均值和方差的指数移动平均值。
- en: 'Although the original paper stated that batch normalization operates by “reducing
    internal covariate shift,” no one really knows for sure why batch normalization
    helps. Various hypotheses exist, but no certitudes. You’ll find that this is true
    of many things in deep learning—it is not an exact science but a set of ever-changing,
    empirically derived engineering best practices, woven together by unreliable narratives.
    You will sometimes feel like the book you have in hand tells you *how* to do something
    but doesn’t quite satisfactorily say *why* it works: that’s because we know the
    how but we don’t know the why. Whenever a reliable explanation is available, I
    make sure to mention it. Batch normalization isn’t one of those cases.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管原论文声称批归一化通过“减少内部协变量转移”起作用，但其真正的机理仍然不为人所知。存在着各种假设，但没有确定的结论。在深度学习中，此类问题是普遍存在的——它不是一门确切的学科，而是一系列不断变革、经验性的最佳工程实践，由于信源不可靠，融合在一起显得有些牵强。
    你会发现有时这些书会告诉你如何做某事，但无法令你完全满意地了解为什么会奏效：这是由于我们知道如何做，但我们并不知道为什么会奏效。每当有一个可靠的解释时，我都会提到它。但是批归一化不属于这种情况。
- en: In practice, the main effect of batch normalization appears to be that it helps
    with gradient propagation—much like residual connections—and thus allows for deeper
    networks. Some very deep networks can be trained only if they include multiple
    BatchNormalization layers. For instance, batch normalization is used liberally
    in many of the advanced convnet architectures that come packaged with Keras, such
    as ResNet50, EfficientNet, and Xception.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，批标准化的主要效果似乎是帮助梯度传播——就像残差连接一样——从而允许更深的网络。一些非常深的网络只有包含多个BatchNormalization层才能训练。例如，在许多与Keras捆绑在一起的高级卷积神经网络架构中，如ResNet50，EfficientNet和Xception中都大量使用了批标准化。
- en: 'layer_batch_normalization() can be used after any layer—layer_dense(), layer_conv_2d(),
    and so on:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在任何层之后使用layer_batch_normalization()——如layer_dense()，layer_conv_2d()等：
- en: x <- …➊
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: x <- …➊
- en: x <- x %>%
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: x <- x %>%
- en: layer_conv_2d(32, 3, use_bias = FALSE) %>%➋
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: layer_conv_2d(32, 3, use_bias = FALSE) %>%➋
- en: layer_batch_normalization()
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: layer_batch_normalization()
- en: ➊ **For example, a layer_input(), keras_model_sequential(), or output from another
    layer**
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **例如，layer_input()，keras_model_sequential()，或来自另一层的输出**
- en: ➋ **Because the output of the layer_conv_2d() is normalized, the layer doesn't
    need its own bias vector.**
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **因为layer_conv_2d()的输出已经标准化，所以该层不需要自己的偏置向量。**
- en: Both layer_dense() and layer_conv_2d() involve a *bias vector*, a learned variable
    whose purpose is to make the layer *affine* rather than purely linear. For instance,
    layer_ conv_2d() returns, schematically, y = conv(x, kernel) + bias, and layer_dense()
    returns y = dot(x, kernel) + bias. Because the normalization step will take care
    of centering the layer’s output on zero, the bias vector is no longer needed when
    using layer_ batch_normalization(), and the layer can be created without it via
    the option use_bias = FALSE. This makes the layer slightly leaner.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense()和layer_conv_2d()都涉及到一个*bias vector*，这是一个学习到的变量，其目的是使层*仿射*而不仅仅是线性的。例如，layer_conv_2d()返回，概略地说，y
    = conv(x, kernel) + bias，而layer_dense()返回y = dot(x, kernel) + bias。因为归一化步骤将负责将层的输出居中于零，所以在使用layer_batch_normalization()时不再需要偏置向量，并且可以通过选项use_bias
    = FALSE创建该层。这使得层稍微瘦了一些。
- en: Importantly, I would generally recommend placing the previous layer’s activation
    *after* the batch normalization layer (although this is still a subject of debate).
    So instead of doing what is shown in listing 9.4, you would do what’s shown in
    listing 9.5.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，我通常建议将前一层的激活*放在*批标准化层之后（尽管这仍然是一个争论的话题）。因此，与列表9.4中所示的做法相反，您应该执行列表9.5中所示的操作。
- en: Listing 9.4 How not to use batch normalization
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.4 如何不使用批标准化
- en: x %>%
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: x %>%
- en: layer_conv_2d(32, 3, activation = "relu") %>%
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: layer_conv_2d(32, 3, activation = "relu") %>%
- en: layer_batch_normalization()
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: layer_batch_normalization()
- en: 'Listing 9.5 How to use batch normalization: The activation comes last'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.5 如何使用批标准化：激活最后出现
- en: x %>%
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: x %>%
- en: layer_conv_2d(32, 3, use_bias = FALSE) %>%➊
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: layer_conv_2d(32, 3, use_bias = FALSE) %>%➊
- en: layer_batch_normalization() %>%
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: layer_batch_normalization() %>%
- en: layer_activation("relu")➋
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: layer_activation("relu")➋
- en: ➊ **Note the lack of activation here.**
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **注意这里缺少激活。**
- en: ➋ **We place the activation after layer_batch_normalization().**
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **我们在layer_batch_normalization()之后放置激活。**
- en: 'The intuitive reason for this approach is that batch normalization will center
    your inputs on zero, whereas your relu activation uses zero as a pivot for keeping
    or dropping activated channels: doing normalization before the activation maximizes
    the utilization of the relu. That said, this ordering best practice is not exactly
    critical, so if you do convolution, then activation, and then batch normalization,
    your model will still train, and you won’t necessarily see worse results.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的直觉原因是批标准化将使您的输入居中于零，而您的relu激活使用零作为保留或丢弃激活通道的枢轴：在激活之前进行归一化可以最大限度地利用relu的利用。话虽如此，这种排序最佳实践并不是绝对关键的，因此，如果您先进行卷积，然后激活，然后进行批标准化，您的模型仍将训练，并且您不一定会看到更差的结果。
- en: '**On batch normalization and fine-tuning**'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '**关于批标准化和微调**'
- en: 'Batch normalization has many quirks. One of the main ones relates to fine-tuning:
    when fine-tuning a model that includes BatchNormalization layers, I recommend
    leaving these layers frozen (call freeze_weights() to set their trainable attribute
    to FALSE). Otherwise, they will keep updating their internal mean and variance,
    which can interfere with the very small updates applied to the surrounding Conv2D
    layers:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: Batch Normalization 有很多小窍门。其中一个主要问题与微调相关：当微调包含 BatchNormalization 层的模型时，我建议将这些层冻结（使用
    freeze_weights() 将它们的 trainable 属性设置为 FALSE）。否则，它们将不断更新其内部的均值和方差，这可能会干扰周围 Conv2D
    层的微小更新：
- en: batch_norm_layer_s3_classname <- class(layer_batch_normalization())[1]
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: batch_norm_layer_s3_classname <- class(layer_batch_normalization())[1]
- en: batch_norm_layer_s3_classname
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: batch_norm_layer_s3_classname
- en: '[1] "keras.layers.normalization.batch_normalization.BatchNormalization"'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] "keras.layers.normalization.batch_normalization.BatchNormalization"'
- en: is_batch_norm_layer <- function(x)
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: is_batch_norm_layer <- function(x)
- en: inherits(x, batch_norm_layer_s3_classname)
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: inherits(x, batch_norm_layer_s3_classname)
- en: model <- application_efficientnet_b0()
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: model <- application_efficientnet_b0()
- en: for(layer in model$layers)
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: for(layer in model$layers)
- en: if(is_batch_norm_layer(layer))
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: if(is_batch_norm_layer(layer))
- en: layer$trainable <- FALSE➊
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: layer$trainable <- FALSE➊
- en: '➊ **Example of how to set trainable <- FALSE to freeze only BatchNormalization
    layers. Note: you can also call freeze_ weights(model, which = is_batch_norm_layer)
    to achieve the same outcome.**'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **示例，如何将 trainable <- FALSE 设置为仅冻结 BatchNormalization 层。注意：您也可以调用 freeze_weights(model,
    which = is_batch_norm_layer) 来实现相同的结果。**
- en: 'Now let’s take a look at the last architecture pattern in our series: depthwise
    separable convolutions.'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下我们系列中的最后一种架构模式：深度可分离卷积。
- en: 9.3.4 Depthwise separable convolutions
  id: totrans-351
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.4 深度可分离卷积
- en: What if I told you that there’s a layer you can use as a drop-in replacement
    for layer_ conv_2d() that will make your model smaller (fewer trainable weight
    parameters) and leaner (fewer floating-point operations) and cause it to perform
    a few percentage points better on its task? That is precisely what the *depthwise
    separable convolution* layer does (layer_separable_conv_2d() in Keras). This layer
    performs a spatial convolution on each channel of its input, independently, before
    mixing output channels via a pointwise convolution (a 1 × 1 convolution), as shown
    in [figure 9.10.](#fig9-10)
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我告诉你，有一种可以直接替换 layer_ conv_2d() 的层，可以让你的模型更小（可训练的权重参数更少）、更轻巧（浮点运算更少），而且在任务上的表现会好几个百分点，你会怎么样？这正是*深度可分离卷积*层所做的（在
    Keras 中为 layer_separable_conv_2d()）。这个层会对其输入的每个通道进行独立的空间卷积，然后通过一个逐点卷积（1 × 1 卷积）混合输出通道，如[图9.10](#fig9-10)所示。
- en: '![Image](../images/f0278-01.jpg)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0278-01.jpg)'
- en: '**Figure 9.10 Depthwise separable convolution: A depthwise convolution followed
    by a pointwise convolution**'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '**图9.10 深度可分离卷积：深度卷积后跟一个逐点卷积**'
- en: This is equivalent to separating the learning of spatial features and the learning
    of channel-wise features. In much the same way that convolution relies on the
    assumption that the patterns in images are not tied to specific locations, depthwise
    separable convolution relies on the assumption that *spatial locations* in intermediate
    activations are *highly correlated*, but *different channels* are *highly independent*
    . Because this assumption is generally true for the image representations learned
    by deep neural networks, it serves as a useful prior that helps the model make
    more efficient use of its training data. A model with stronger priors about the
    structure of the information it will have to process is a better model—as long
    as the priors are accurate.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 这等价于分离空间特征的学习和通道特征的学习。就像卷积假设图像中的模式与特定位置无关一样，深度可分离卷积假设中间激活的空间位置高度相关，但不同通道高度独立。因为这个假设通常对于深度神经网络学到的图像表示来说是正确的，所以它作为一种有用的先验知识，帮助模型更有效地利用其训练数据。一个具有更强有力的关于它需要处理的信息结构的先验知识的模型是一个更好的模型，只要这些先验知识是准确的。
- en: Depthwise separable convolution requires significantly fewer parameters and
    involves fewer computations compared to regular convolution, while having comparable
    representational power. It results in smaller models that converge faster and
    are less prone to overfitting. These advantages become especially important when
    you’re training small models from scratch on limited data.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 与普通卷积相比，深度可分离卷积需要较少的参数并且涉及较少的计算，同时具备相当的表征能力。它产生的模型较小，收敛速度更快，且更不容易过拟合。当你使用有限数据从头开始训练小型模型时，这些优势尤为重要。
- en: 'When it comes to larger-scale models, depthwise separable convolutions are
    the basis of the Xception architecture, a high-performing convnet that comes packaged
    with Keras. You can read more about the theoretical grounding for depthwise separable
    convolutions and Xception in the paper “Xception: Deep Learning with Depthwise
    Separable Convolutions.”^([3](#Rendnote3))'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '当涉及到大规模模型时，深度可分离卷积是Xception架构的基础，Xception是一个高性能的卷积神经网络模型，已经内置在Keras中。你可以在论文《Xception:
    Deep Learning with Depthwise Separable Convolutions》中了解更多关于深度可分离卷积和Xception的理论基础。^([3](#Rendnote3))'
- en: '**The co-evolution of hardware, software, and algorithms**'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '**硬件，软件和算法的协同演化**'
- en: 'Consider a regular convolution operation with a 3 × 3 window, 64 input channels,
    and 64 output channels. It uses 3 * 3 * 64 * 64 = 36,864 trainable parameters,
    and when you apply it to an image, it runs a number of floating-point operations
    that is proportional to this parameter count. Meanwhile, consider an equivalent
    depthwise separable convolution: it involves only 3 * 3 * 64 + 64 * 64 = 4,672
    trainable parameters and proportionally fewer floating-point operations. This
    efficiency improvement only increases as the number of filters or the size of
    the convolution windows gets larger.'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个具有3 × 3窗口，64个输入通道和64个输出通道的普通卷积操作。它使用了3 * 3 * 64 * 64 = 36,864个可训练参数，并且在应用到图像时，运行的浮点操作次数与这个参数数量成比例。此外，考虑一个等效的深度可分离卷积：它只需要3
    * 3 * 64 + 64 * 64 = 4,672个可训练参数，以及相对较少的浮点操作。当过滤器的数量或卷积窗口的大小变大时，这种效率改进只会增加。
- en: 'As a result, you would expect depthwise separable convolutions to be dramatically
    faster, right? Hold on. This would be true if you were writing simple CUDA or
    C implementations of these algorithms—in fact, you do see a meaningful speedup
    when running on CPU, where the underlying implementation is parallelized C. But
    in practice, you’re probably using a GPU, and what you’re executing on it is far
    from a “simple” CUDA implementation: it’s a *cuDNN kernel*, a piece of code that
    has been extraordinarily optimized, down to each machine instruction. It certainly
    makes sense to spend a lot of effort optimizing this code, since cuDNN convolutions
    on NVIDIA hardware are responsible for many exaFLOPS of computation every day.
    But a side effect of this extreme micro-optimization is that alternative approaches
    have little chance to compete on performance—even approaches that have significant
    intrinsic advantages, like depthwise separable convolutions.'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你可能会期望深度可分离卷积的速度明显更快，对吗？等等。如果你是在编写这些算法的简单CUDA或C实现，这当然是正确的——实际上，当在CPU上运行时，你确实能够看到有意义的加速效果，因为底层实现是并行化的C。但在实践中，你可能正在使用GPU，并且你在其上执行的代码远非“简单”的CUDA实现：它是一个*cuDNN
    kernel*，这是一段被极端优化的代码，甚至进行到每个机器指令。投入大量精力来优化这段代码是有道理的，因为基于NVIDIA硬件的cuDNN卷积每天负责计算出许多exaFLOPS。但这种极端微观优化的一个副作用是，即使是具有明显内在优势的替代方法（如深度可分离卷积）也很难在性能上与之竞争。
- en: 'Despite repeated requests to NVIDIA, depthwise separable convolutions have
    not benefited from nearly the same level of software and hardware optimization
    as regular convolutions, and as a result they remain only about as fast as regular
    convolutions, even though they’re using quadratically fewer parameters and floating-point
    operations. Note, though, that using depthwise separable convolutions remains
    a good idea even if it does not result in a speedup: their lower parameter count
    means that you are less at risk of overfitting, and their assumption that channels
    should be uncorrelated leads to faster model convergence and more robust representations.'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管反复要求 NVIDIA，深度可分离卷积并没有得到几乎与常规卷积相同水平的软件和硬件优化，并且因此它们仍然只是与常规卷积一样快，即使它们使用的参数和浮点运算量减少了平方倍。但请注意，即使深度可分离卷积没有加速，使用它们仍然是一个好主意：它们较低的参数数量意味着你不太容易过拟合，而它们的假设是通道应该是不相关的，这导致模型收敛更快并且表示更加健壮。
- en: 'What is a slight inconvenience in this case can become an impassable wall in
    other situations: because the entire hardware and software ecosystem of deep learning
    has been micro-optimized for a very specific set of algorithms (in particular,
    convnets trained via backpropagation), there’s an extremely high cost to steering
    away from the beaten path.'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下的轻微不便可能会在其他情况下成为一道不可逾越的障碍：因为整个深度学习的硬件和软件生态系统都已经被微调以适应一组非常特定的算法（特别是通过反向传播训练的卷积网络），所以偏离传统道路的成本极高。
- en: If you were to experiment with alternative algorithms, such as gradient-free
    optimization or spiking neural networks, the first few parallel C++ or CUDA implementations
    you’d come up with would be orders of magnitude slower than a good old convnet,
    no matter how clever and efficient your ideas were. Convincing other researchers
    to adopt your method would be a tough sell, even if it were just plain better.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您尝试使用替代算法，例如无梯度优化或脉冲神经网络，那么您设计的头几个并行的 C++ 或 CUDA 实现将比一个老式的卷积网络慢几个数量级，无论您的想法多么聪明和高效。即使它明显更好，说服其他研究人员采用您的方法也将是一项艰巨的任务。
- en: 'You could say that modern deep learning is the product of a co-evolution process
    between hardware, software, and algorithms: the availability of NVIDIA GPUs and
    CUDA led to the early success of backpropagation-trained convnets, which led NVIDIA
    to optimize its hardware and software for these algorithms, which in turn led
    to consolidation of the research community behind these methods. At this point,
    figuring out a different path would require a multiyear re-engineering of the
    entire ecosystem.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以说现代深度学习是硬件、软件和算法之间的共同演进过程的产物：NVIDIA GPU 和 CUDA 的可用性导致了反向传播训练卷积网络的早期成功，这导致了
    NVIDIA 优化其硬件和软件以适应这些算法，进而导致了研究社区围绕这些方法的巩固。在这一点上，要想找到一条不同的道路将需要对整个生态系统进行多年的重新设计。
- en: '9.3.5 Putting it together: A mini Xception-like model'
  id: totrans-365
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.5 把它放在一起：一个迷你的 Xception 类似的模型
- en: 'As a reminder, here are the convnet architecture principles you’ve learned
    so far:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，这是您迄今为止学到的卷积网络架构原则：
- en: Your model should be organized into repeated *blocks* of layers, usually made
    of multiple convolution layers and a max-pooling layer.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的模型应该被组织成重复的*块*层，通常由多个卷积层和一个最大池化层组成。
- en: The number of filters in your layers should increase as the size of the spatial
    feature maps decreases.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的层中过滤器的数量应随着空间特征图的大小减少而增加。
- en: Deep and narrow is better than broad and shallow.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深而窄比宽而浅更好。
- en: Introducing residual connections around blocks of layers helps you train deeper
    networks.
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在层块周围引入残差连接有助于您训练更深的网络。
- en: It can be beneficial to introduce batch normalization layers after your convolution
    layers.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在您的卷积层之后引入批量归一化层可能是有益的。
- en: It can be beneficial to replace layer_conv_2d() with layer_separable_ conv_2d(),
    which are more parameter efficient
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 layer_conv_2d() 替换为 layer_separable_conv_2d() 可能是更加参数有效的。
- en: 'Let’s bring these ideas together into a single model. Its architecture will
    resemble a smaller version of Xception, and we’ll apply it to the dogs vs. cats
    task from the previous chapter. For data loading and model training, we’ll simply
    reuse the setup we used in section 8.2.5, but we’ll replace the model definition
    with the following convnet:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把这些想法结合到一个单一的模型中。它的架构将类似于 Xception 的一个较小版本，我们将把它应用到上一章中的狗与猫任务中。对于数据加载和模型训练，我们将简单地重用我们在第
    8.2.5 节中使用的设置，但我们将用以下卷积网络替换模型定义：
- en: data_augmentation <- keras_model_sequential() %>%➊
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: data_augmentation <- keras_model_sequential() %>%➊
- en: layer_random_flip("horizontal") %>%
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: layer_random_flip("horizontal") %>%
- en: layer_random_rotation(0.1) %>%
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: layer_random_rotation(0.1) %>%
- en: layer_random_zoom(0.2)
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: layer_random_zoom(0.2)
- en: inputs <- layer_input(shape = c(180, 180, 3))
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: inputs <- layer_input(shape = c(180, 180, 3))
- en: x <- inputs %>%
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: x <- inputs %>%
- en: data_augmentation() %>%
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: data_augmentation() %>%
- en: layer_rescaling(scale = 1 / 255)➋
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: layer_rescaling(scale = 1 / 255)➋
- en: x <- x %>%
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: x <- x %>%
- en: layer_conv_2d(32, 5, use_bias = FALSE)➌
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: layer_conv_2d(32, 5, use_bias = FALSE)➌
- en: for (size in c(32, 64, 128, 256, 512)) {➍
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: for (size in c(32, 64, 128, 256, 512)) {➍
- en: residual <- x
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: residual <- x
- en: x <- x %>%
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: x <- x %>%
- en: layer_batch_normalization() %>%
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: layer_batch_normalization() %>%
- en: layer_activation("relu") %>%
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: layer_activation("relu") %>%
- en: layer_separable_conv_2d(size, 3, padding = "same", use_bias = FALSE) %>%
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: layer_separable_conv_2d(size, 3, padding = "same", use_bias = FALSE) %>%
- en: layer_batch_normalization() %>%
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: layer_batch_normalization() %>%
- en: layer_activation("relu") %>%
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: layer_activation("relu") %>%
- en: layer_separable_conv_2d(size, 3, padding = "same", use_bias = FALSE) %>%
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: layer_separable_conv_2d(size, 3, padding = "same", use_bias = FALSE) %>%
- en: layer_max_pooling_2d(pool_size = 3, strides = 2, padding = "same")
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: layer_max_pooling_2d(pool_size = 3, strides = 2, padding = "same")
- en: residual <- residual %>%
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: residual <- residual %>%
- en: layer_conv_2d(size, 1, strides = 2, padding = "same", use_bias = FALSE)
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: layer_conv_2d(size, 1, strides = 2, padding = "same", use_bias = FALSE)
- en: x <- layer_add(list(x, residual))
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: x <- layer_add(list(x, residual))
- en: '}'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: outputs <- x %>%
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: outputs <- x %>%
- en: layer_global_average_pooling_2d() %>%➎
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: layer_global_average_pooling_2d() %>%➎
- en: layer_dropout(0.5) %>%➏
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dropout(0.5) %>%➏
- en: layer_dense(1, activation = "sigmoid")
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(1, activation = "sigmoid")
- en: model <- keras_model(inputs, outputs)
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model(inputs, outputs)
- en: train_dataset <- image_dataset_from_directory(
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: train_dataset <- image_dataset_from_directory(
- en: '"cats_vs_dogs_small/train",'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '"cats_vs_dogs_small/train",'
- en: image_size = c(180, 180),
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: image_size = c(180, 180),
- en: batch_size = 32
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: batch_size = 32
- en: )
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: validation_dataset <- image_dataset_from_directory(
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: validation_dataset <- image_dataset_from_directory(
- en: '"cats_vs_dogs_small/validation",'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '"cats_vs_dogs_small/validation",'
- en: image_size = c(180, 180),
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: image_size = c(180, 180),
- en: batch_size = 32
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: batch_size = 32
- en: )
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: model %>%
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: model %>%
- en: compile(
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: compile(
- en: loss = "binary_crossentropy",
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: loss = "binary_crossentropy",
- en: optimizer = "rmsprop",
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: optimizer = "rmsprop",
- en: metrics = "accuracy"
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = "accuracy"
- en: )
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: history <- model %>%
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: history <- model %>%
- en: fit(
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: fit(
- en: train_dataset,
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: train_dataset,
- en: epochs = 100,
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 100,
- en: validation_data = validation_dataset)
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: validation_data = validation_dataset)
- en: ➊ **We use the same data augmentation configuration as before.**
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **我们使用与以前相同的数据增强配置。**
- en: ➋ **Don't forget input rescaling!**
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **不要忘记输入重新缩放！**
- en: ➌**Note that the assumption that underlies separable convolution, "feature channels
    are largely independent," does not hold for RGB images! Red, green, and blue color
    channels are actually highly correlated in natural images. As such, the first
    layer in our model is a regular layer_conv_2d() layer. We'll start using layer_separable_conv_2d()
    afterward.**
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: ➌**请注意，可分离卷积背后的假设“特征通道在很大程度上是独立的”并不适用于RGB图像！红色、绿色和蓝色通道在自然图像中实际上高度相关。因此，我们模型中的第一层是一个常规的layer_conv_2d()层。之后我们将开始使用layer_separable_conv_2d()。**
- en: ➍**We apply a series of convolutional blocks with increasing feature depth.
    Each block consists of two batch-normalized depthwise separable convolution layers
    and a max-pooling layer, with a residual connection around the entire block.**
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: ➍**我们应用一系列逐渐增加特征深度的卷积块。每个块包含两个批归一化的深度可分离卷积层和一个最大池化层，并在整个块周围具有残差连接。**
- en: ➎**In the original model, we used a layer_flatten() before the layer_dense().
    Here, we go with a layer_global_average_pooling_2d().**
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: ➎**在原始模型中，我们在layer_dense()之前使用了layer_flatten()。这里，我们使用了layer_global_average_pooling_2d()。**
- en: ➐**Like in the original model, we add a dropout layer for regularization.**
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: ➐**就像在原始模型中一样，我们为正则化添加了一个dropout层。**
- en: This convnet has a total parameter count of 721,857, slightly lower than the
    991,041 parameters of the original model we defined in chapter 8 (Listing 8.7),
    but still in the same ballpark. [Figure 9.11](#fig9-11) shows its training and
    validation curves.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 这个卷积网络的总参数数量为721,857，略低于我们在第8章中定义的原始模型的991,041个参数（清单8.7），但仍处于同一水平区间。[图9.11](#fig9-11)显示了其训练和验证曲线。
- en: '![Image](../images/f0282-01.jpg)'
  id: totrans-431
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0282-01.jpg)'
- en: '**Figure 9.11 Training and validation metrics with an Xception-like architecture**'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '**图9.11 带有Xception类似架构的训练和验证指标**'
- en: You’ll find that our new model achieves a test accuracy of 90.8%, compared to
    81.4% for the naive model in the previous chapter. As you can see, following architecture
    best practices does have an immediate, sizable impact on model performance!
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现我们的新模型的测试准确率为90.8%，而上一章中的简单模型为81.4%。正如你所看到的，遵循最佳实践架构确实会立即对模型性能产生巨大影响！
- en: At this point, if you want to further improve performance, you should start
    systematically tuning the hyperparameters of your architecture—a topic we’ll cover
    in detail in chapter 13\. We haven’t gone through this step here, so the configuration
    of the preceding model is purely based on the best practices we discussed, plus,
    when it comes to gauging model size, a small amount of intuition.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，如果您想进一步提高性能，您应该开始系统地调整架构的超参数——这是我们将在第13章中详细介绍的一个主题。我们没有在这里执行这一步，因此前述模型的配置纯粹基于我们讨论的最佳实践，再加上，在评估模型大小时，一点直觉。
- en: Note that these architecture best practices are relevant to computer vision
    in general, not just image classification. For example, Xception is used as the
    standard convolutional base in DeepLabV3, a popular state-of-the-art image segmentation
    solution.^([4](#Rendnote4))
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这些架构最佳实践与计算机视觉一般相关，不仅限于图像分类。例如，Xception被用作DeepLabV3的标准卷积基础，DeepLabV3是一种流行的最先进的图像分割解决方案。^([4](#Rendnote4))
- en: 'This concludes our introduction to essential convnet architecture best practices.
    With these principles in hand, you’ll be able to develop higher-performing models
    across a wide range of computer vision tasks. You’re now well on your way to becoming
    a proficient computer vision practitioner. To further deepen your expertise, there’s
    one last important topic we need to cover: interpreting how a model arrives at
    its predictions.'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们对卷积神经网络（convnet）关键架构最佳实践的介绍。有了这些原则，您将能够在各种计算机视觉任务中开发性能更高的模型。您现在已经在成为熟练的计算机视觉从业者的道路上取得了良好的进展。为了进一步加深您的专业知识，我们还需要涵盖最后一个重要主题：解释模型如何得出其预测结果。
- en: 9.4 Interpreting what convnets learn
  id: totrans-437
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.4 解释卷积神经网络学到的内容
- en: 'A fundamental problem when building a computer vision application is that of
    *interpretability*: *why* did your classifier think a particular image contained
    a fridge, when all you can see is a truck? This is especially relevant to use
    cases where deep learning is used to complement human expertise, such as in medical
    imaging use cases. We will end this chapter by getting you familiar with a range
    of different techniques for visualizing what convnets learn and understanding
    the decisions they make.'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建计算机视觉应用程序时的一个基本问题是*可解释性*：当您所能看到的只是一辆卡车时，为什么您的分类器会认为特定的图像包含冰箱呢？这尤其与使用深度学习来补充人类专业知识的用例相关，例如在医学成像用例中。我们将通过让您熟悉一系列不同的技术来结束本章，以便可视化卷积神经网络学习的内容并理解它们所做出的决定。
- en: 'It’s often said that deep learning models are “black boxes”: they learn representations
    that are difficult to extract and present in a human-readable form. Although this
    is partially true for certain types of deep learning models, it’s definitely not
    true for convnets. The representations learned by convnets are highly amenable
    to visualization, in large part because they’re *representations of visual concepts*.
    Since 2013, a wide array of techniques has been developed for visualizing and
    interpreting these representations. We won’t survey all of them, but we’ll cover
    three of the most accessible and useful ones:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 人们经常说深度学习模型是“黑盒子”：它们学习的表示很难以人类可读的形式提取和呈现。尽管对于某些类型的深度学习模型来说这在一定程度上是正确的，但对于卷积神经网络来说绝对不是。卷积神经网络学到的表示非常适合可视化，这在很大程度上是因为它们是*视觉概念的表示*。自2013年以来，已经开发出了各种各样的技术来可视化和解释这些表示。我们不会概述所有这些技术，但我们将涵盖三种最易于访问和有用的技术：
- en: '*Visualizing intermediate convnet outputs (intermediate activations)*—Useful
    for understanding how successive convnet layers transform their input, and for
    getting a first idea of the meaning of individual convnet filters'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可视化中间卷积神经网络输出（中间激活）*——有助于理解连续卷积神经网络层如何转换其输入，并对单个卷积神经网络滤波器的含义有初步了解'
- en: '*Visualizing convnet filters*—Useful for understanding precisely what visual
    pattern or concept each filter in a convnet is receptive to'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可视化卷积神经网络滤波器*——有助于准确了解卷积神经网络中每个滤波器对应的视觉模式或概念'
- en: '*Visualizing heatmaps of class activation in an image*—Useful for understanding
    which parts of an image were identified as belonging to a given class, thus allowing
    you to localize objects in images'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可视化图像中类别激活的热图*——有助于理解图像的哪些部分被识别为属于给定类别，从而使您能够在图像中定位对象'
- en: For the first method—activation visualization—we’ll use the small convnet that
    we trained from scratch on the dogs-versus-cats classification problem in section
    8.2\. For the next two methods, we’ll use a pretrained Xception model.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一种方法——激活可视化——我们将使用在第8.2节中针对狗与猫分类问题从头开始训练的小型卷积神经网络。对于接下来的两种方法，我们将使用一个预训练的Xception模型。
- en: 9.4.1 Visualizing intermediate activations
  id: totrans-444
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.1 可视化中间激活
- en: 'Visualizing intermediate activations consists of displaying the values returned
    by various convolution and pooling layers in a model, given a certain input (the
    output of a layer is often called its *activation*, the output of the activation
    function). This gives a view into how an input is decomposed into the different
    filters learned by the network. We want to visualize feature maps with three dimensions:
    width, height, and depth (channels). Each channel encodes relatively independent
    features, so the proper way to visualize these feature maps is by independently
    plotting the contents of every channel as a 2D image. Let’s start by loading the
    model that you saved in section 8.2:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化中间激活包括显示模型中各种卷积和池化层返回的值，给定某个输入（层的输出通常称为其*激活*，激活函数的输出）。这提供了一种查看输入如何被网络学习到的不同滤波器分解的视图。我们希望可视化具有三维特征图，宽度、高度和深度（通道）。每个通道编码相对独立的特征，因此正确的方式是将这些特征图的内容独立地绘制为2D图像。让我们从加载你在第8.2节中保存的模型开始：
- en: model <- load_model_tf("convnet_from_scratch_with_augmentation.keras")
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: model <- load_model_tf("convnet_from_scratch_with_augmentation.keras")
- en: model
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: model
- en: '![Image](../images/f0284-01.jpg)'
  id: totrans-448
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0284-01.jpg)'
- en: Next, we’ll get an input image—a picture of a cat, not part of the images the
    network was trained on.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将获取一个输入图片——一张猫的照片，不属于网络训练的图片部分。
- en: Listing 9.6 Preprocessing a single image
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.6 单个图像的预处理
- en: img_path <- get_file(➊
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: img_path <- get_file(➊
- en: fname = "cat.jpg",
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: fname = "猫.jpg"
- en: origin = "https://img-datasets.s3.amazonaws.com/cat.jpg")
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: origin = "https://img-datasets.s3.amazonaws.com/cat.jpg")
- en: img_tensor <- img_path %>%➋
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: img_tensor <- img_path %>%➋
- en: tf_read_image(resize = c(180, 180))
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: tf_read_image(resize = c(180, 180))
- en: ➊**Download a test image.**
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **下载一个测试图片。**
- en: ➋ **Read and resize the image to a float32 Tensor of shape (180, 180, 3).**
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **读取并将图像调整为形状为（180, 180, 3）的float32张量。**
- en: Let’s display the picture (see [figure 9.12](#fig9-12)).
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们显示图片（见[图9.12](#fig9-12)）。
- en: Listing 9.7 Displaying the test picture
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.7 显示测试图片
- en: display_image_tensor(img_tensor)
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: display_image_tensor(img_tensor)
- en: '![Image](../images/f0285-01.jpg)'
  id: totrans-461
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0285-01.jpg)'
- en: '**Figure 9.12 The test cat picture**'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: '**图9.12 测试猫图片**'
- en: To extract the feature maps we want to look at, we’ll create a Keras model that
    takes batches of images as input and that outputs the activations of all convolution
    and pooling layers.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提取我们想要查看的特征图，我们将创建一个Keras模型，该模型以图像批作为输入，并输出所有卷积和池化层的激活。
- en: Listing 9.8 Instantiating a model that returns layer activations
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.8 实例化返回层激活的模型
- en: conv_layer_s3_classname <-➊
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: conv_layer_s3_classname <-➊
- en: class(layer_conv_2d(NULL, 1, 1))[1]
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: class(layer_conv_2d(NULL, 1, 1))[1]
- en: pooling_layer_s3_classname <-
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: pooling_layer_s3_classname <-
- en: class(layer_max_pooling_2d(NULL))[1]
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: class(layer_max_pooling_2d(NULL))[1]
- en: is_conv_layer <- function(x) inherits(x, conv_layer_s3_classname)
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: is_conv_layer <- function(x) inherits(x, conv_layer_s3_classname)
- en: is_pooling_layer <- function(x) inherits(x, pooling_layer_s3_classname)
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: is_pooling_layer <- function(x) inherits(x, pooling_layer_s3_classname)
- en: layer_outputs <- list()
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: layer_outputs <- list()
- en: for (layer in model$layers)
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: for (layer in model$layers)
- en: if (is_conv_layer(layer) || is_pooling_layer(layer))
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: if (is_conv_layer(layer) || is_pooling_layer(layer))
- en: layer_outputs[[layer$name]] <- layer$output➋
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: layer_outputs[[layer$name]] <- layer$output➋
- en: activation_model <- keras_model(inputs = model$input,➌
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: activation_model <- keras_model(inputs = model$input,➌
- en: outputs = layer_outputs)
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: outputs = layer_outputs)
- en: ➊ **Make dummy conv and pooling layers to determine what the S3 classname is.
    This is generally a long string like "keras.layers.convolutional.Conv2D", but
    because it can change between Tensorflow versions, it's better to not hardcode
    it.**
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **创建虚拟的卷积和池化层来确定S3类名是什么。这通常是一个很长的字符串，比如“keras.layers.convolutional.Conv2D”，但由于它可能因Tensorflow版本而改变，最好不要硬编码它。**
- en: ➋ **Extract the outputs of all Conv2D and MaxPooling2D layers and put them in
    a named list.**
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **提取所有Conv2D和MaxPooling2D层的输出，并将它们放入一个命名列表中。**
- en: ➌ **Create a model that will return these outputs, given the model input.**
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **创建一个模型，将返回这些输出，给定模型输入。**
- en: 'When fed an image input, this model returns the values of the layer activations
    in the original model, as a list. This is the first time you’ve encountered a
    multi-output model in this book in practice since you learned about them in chapter
    7; until now, the models you’ve seen have had exactly one input and one output.
    This one has one input and nine outputs: one output per layer activation.'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 当输入图像时，该模型以原始模型中的层激活值作为列表返回。这是你在本书中第一次遇到实际中的多输出模型，自第7章学习以来，到目前为止，你所看到的模型都只有一个输入和一个输出。这个模型有一个输入和九个输出：每个层激活一个输出。
- en: Listing 9.9 Using the model to compute layer activations
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.9 使用模型计算层激活
- en: activations <- activation_model %>%
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: activations <- activation_model %>%
- en: predict(img_tensor[tf$newaxis, , , ])➊➋
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: predict(img_tensor[tf$newaxis, , , ])➊➋
- en: '➊ **predict() returns a list of nine R arrays: one array per layer activation.**'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **predict() 返回九个 R 数组的列表：每个层激活一个数组。**
- en: ➋ **Call [tf$newaxis, , , ] to change img_tensor shape from (180, 180, 3) to
    (1, 180, 180, 3). In other words, adds a batch dimension, because the model expects
    input to be a batch of images, not a single image.**
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **调用 [tf$newaxis, , , ] 来将 img_tensor 的形状从 (180, 180, 3) 改变为 (1, 180, 180,
    3)。换句话说，添加了一个批量维度，因为模型期望输入是一批图像，而不是单个图像。**
- en: 'Because we passed a named list for outputs when we built the model, we get
    back a named list of R arrays when we call predict() on the model:'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们在构建模型时传递了一个带有输出名称的命名列表，所以当我们在模型上调用 predict() 时，我们会得到一个带有 R 数组名称的命名列表：
- en: str(activations)
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: str(activations)
- en: List of 9
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 9 项列表
- en: '$ conv2d_15      : num [1, 1:178, 1:178, 1:32] 0.00418 0.0016 0.00453 0 …'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: '$ conv2d_15      : num [1, 1:178, 1:178, 1:32] 0.00418 0.0016 0.00453 0 …'
- en: '$ max_pooling2d_9: num [1, 1:89, 1:89, 1:32] 0.01217 0.00453 0.00742 0.00514'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '$ max_pooling2d_9: num [1, 1:89, 1:89, 1:32] 0.01217 0.00453 0.00742 0.00514'
- en: '$ conv2d_14      : num [1, 1:87, 1:87, 1:64] 0 0 0 0 0.00531 …'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: '$ conv2d_14      : num [1, 1:87, 1:87, 1:64] 0 0 0 0 0.00531 …'
- en: '$ max_pooling2d_8: num [1, 1:43, 1:43, 1:64] 0 0 0.00531 0 0 …'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: '$ max_pooling2d_8: num [1, 1:43, 1:43, 1:64] 0 0 0.00531 0 0 …'
- en: '$ conv2d_13      : num [1, 1:41, 1:41, 1:128] 0 0 0.0288 0 0.0342 …'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: '$ conv2d_13      : num [1, 1:41, 1:41, 1:128] 0 0 0.0288 0 0.0342 …'
- en: '$ max_pooling2d_7: num [1, 1:20, 1:20, 1:128] 0.0313 0.0288 0.0342 0.4004 0.'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: '$ max_pooling2d_7: num [1, 1:20, 1:20, 1:128] 0.0313 0.0288 0.0342 0.4004 0.'
- en: '$ conv2d_12      : num [1, 1:18, 1:18, 1:256] 0 0 0 0 0 0 0 0 0 0 …'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: '$ conv2d_12      : num [1, 1:18, 1:18, 1:256] 0 0 0 0 0 0 0 0 0 0 …'
- en: '$ max_pooling2d_6: num [1, 1:9, 1:9, 1:256] 0 0 0 0 0 0 0 0 0 0 …'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: '$ max_pooling2d_6: num [1, 1:9, 1:9, 1:256] 0 0 0 0 0 0 0 0 0 0 …'
- en: '[list output truncated]'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表输出已截断]'
- en: 'Lets take a closer look at the first layer activations:'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们仔细看看第一层的激活：
- en: first_layer_activation <- activations[[ names(layer_outputs)[1] ]]
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: first_layer_activation <- activations[[ names(layer_outputs)[1] ]]
- en: dim(first_layer_activation)
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: dim(first_layer_activation)
- en: '[1] 1 178 178 32'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 1 178 178 32'
- en: It’s a 178 × 178 feature map with 32 channels. Let’s try plotting the fifth
    channel of the activation of the first layer of the original model (see [figure
    9.13](#fig9-13)).
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个 178 × 178 的特征图，有 32 个通道。让我们尝试绘制原始模型的第一层激活的第五个通道（参见 [图 9.13](#fig9-13)）。
- en: Listing 9.10 Visualizing the fifth channel
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.10 可视化第五个通道
- en: plot_activations <- function(x, …) {
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: plot_activations <- function(x, …) {
- en: x <- as.array(x)➊
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: x <- as.array(x)➊
- en: if(sum(x) == 0)➋
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: if(sum(x) == 0)➋
- en: return(plot(as.raster("gray")))
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: return(plot(as.raster("gray")))
- en: rotate <- function(x) t(apply(x, 2, rev))➌
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: rotate <- function(x) t(apply(x, 2, rev))➌
- en: image(rotate(x), asp = 1, axes = FALSE, useRaster = TRUE,
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: image(rotate(x), asp = 1, axes = FALSE, useRaster = TRUE,
- en: col = terrain.colors(256), …)
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: col = terrain.colors(256), …)
- en: '}'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: plot_activations(first_layer_activation[, , , 5])
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: plot_activations(first_layer_activation[, , , 5])
- en: ➊ **Convert Tensors to arrays.**
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **将张量转换为数组。**
- en: ➋ **All-zero channels (i.e., no activations) are plotted as a gray rectangle,
    so they're easy to distinguish.**
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **所有零通道（即无激活）都被绘制成灰色矩形，因此它们很容易区分。**
- en: ➌ **Rotate the image clockwise for easier viewing.**
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **将图像顺时针旋转以便更容易查看。**
- en: '![Image](../images/f0287-01.jpg)'
  id: totrans-516
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0287-01.jpg)'
- en: '**Figure 9.13 Fifth channel of the activation of the first layer on the test
    cat picture**'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 9.13 测试猫图片上第一层激活的第五个通道**'
- en: This channel appears to encode a diagonal edge detector—but note that your own
    channels may vary, because the specific filters learned by convolution layers
    aren’t deterministic.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 这个通道似乎编码了一个对角边缘检测器——但请注意，你自己的通道可能会有所不同，因为卷积层学习的特定过滤器并不确定。
- en: Now let’s plot a complete visualization of all the activations in the network
    (see [figure 9.14](#fig9-14)). We’ll extract and plot every channel in each of
    the layer activations, and we’ll stack the results in one big grid, with channels
    stacked side by side.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们绘制网络中所有激活的完整可视化（见[图9.14](#fig9-14)）。我们将提取并绘制每个层激活中的每个通道，并将结果堆叠在一个大网格中，通道并排堆叠。
- en: Listing 9.11 Visualizing every channel in every intermediate activation
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.11 可视化每个中间激活中的每个通道
- en: for (layer_name in names(layer_outputs)) {➊
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: for (layer_name in names(layer_outputs)) {➊
- en: layer_output <- activations[[layer_name]]
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: layer_output <- activations[[layer_name]]
- en: n_features <- dim(layer_output) %>% tail(1) ➋
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: n_features <- dim(layer_output) %>% tail(1) ➋
- en: par(mfrow = n2mfrow(n_features, asp = 1.75),➌
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: par(mfrow = n2mfrow(n_features, asp = 1.75),➌
- en: mar = rep(.1, 4), oma = c(0, 0, 1.5, 0))
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: mar = rep(.1, 4), oma = c(0, 0, 1.5, 0))
- en: for (j in 1:n_features)
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: for (j in 1:n_features)
- en: plot_activations(layer_output[, , , j])➍
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: plot_activations(layer_output[, , , j])➍
- en: title(main = layer_name, outer = TRUE)
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: title(main = layer_name, outer = TRUE)
- en: '}'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ **Iterate over the activations (and the names of the corresponding layers).**
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **迭代激活（和相应层的名称）。**
- en: ➋ **The layer activation has shape (1, height, width, n_features).**
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **层激活的形状为(1, 高度, 宽度, 特征数)。**
- en: ➌ **Prepare to display all the channels in this activation in one plot.**
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **准备在一个图中显示该激活中的所有通道。**
- en: ➍ **This is a single channel (or feature).**
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **这是一个单一的通道（或特征）。**
- en: 'There are a few things to note here:'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有几件事情要注意：
- en: The first layer acts as a collection of various edge detectors. At that stage,
    the activations retain almost all of the information present in the initial picture
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一层充当各种边缘检测器的集合。在这个阶段，激活几乎保留了初始图片中的所有信息。
- en: As you go deeper, the activations become increasingly abstract and less visually
    interpretable. They begin to encode higher-level concepts such as “cat ear” and
    “cat eye.” Deeper presentations carry increasingly less information about the
    visual contents of the image and increasingly more information related to the
    class of the image
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着深入，激活变得越来越抽象，越来越难以直观解释。它们开始编码更高级别的概念，如“猫耳朵”和“猫眼睛”。更深层的表述携带的关于图像视觉内容的信息越来越少，而与图像类别相关的信息越来越多。
- en: '![Image](../images/f0288-01.jpg)'
  id: totrans-537
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0288-01.jpg)'
- en: '**Figure 9.14 Every channel of every layer activation on the test cat picture**'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: '**图9.14 测试猫图片中每一层激活的每个通道**'
- en: 'The sparsity of the activations increases with the depth of the layer: in the
    first layer, almost all filters are activated by the input image, but in the following
    layers, more and more filters are blank. This means the pattern encoded by the
    filter isn’t found in the input image'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着层的深度，激活的稀疏性增加：在第一层中，几乎所有滤波器都被输入图像激活，但在后续层中，越来越多的滤波器是空白的。这意味着滤波器编码的模式在输入图像中找不到。
- en: 'We have just evidenced an important universal characteristic of the representations
    learned by deep neural networks: the features extracted by a layer become increasingly
    abstract with the depth of the layer. The activations of higher layers carry less
    and less information about the specific input being seen and more and more information
    about the target (in this case, the class of the image: cat or dog). A deep neural
    network effectively acts as an *information distillation pipeline*, with raw data
    going in (in this case, RGB pictures) and being repeatedly transformed so that
    irrelevant information is filtered out (e.g., the specific visual appearance of
    the image) and useful information is magnified and refined (e.g., the class of
    the image).'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚证明了深度神经网络学习的表示的一个重要的普遍特性：随着层的深度，由层提取的特征变得越来越抽象。更高层的激活携带的关于正在查看的特定输入的信息越来越少，而携带的关于目标的信息越来越多（在本例中，图像的类别：猫或狗）。深度神经网络有效地充当一种*信息蒸馏管道*，原始数据（在本例中是RGB图片）被反复转换，以过滤掉不相关的信息（例如，图像的特定视觉外观），并放大和精炼有用的信息（例如，图像的类别）。
- en: 'This is analogous to the way humans and animals perceive the world: after observing
    a scene for a few seconds, a human can remember which abstract objects were present
    in it (bicycle, tree) but can’t remember the specific appearance of these objects.
    In fact, if you tried to draw a generic bicycle from memory, chances are you couldn’t
    get it even remotely right, even though you’ve seen thousands of bicycles in your
    lifetime (see, for example, [figure 9.15](#fig9-15)). Try it right now: this effect
    is absolutely real. Your brain has learned to completely abstract its visual input—to
    transform it into high-level visual concepts while filtering out irrelevant visual
    details—making it tremendously difficult to remember how things around you look.'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 这类似于人类和动物感知世界的方式：在观察一个场景几秒钟后，人类可以记住其中存在的抽象对象（自行车，树），但不能记住这些对象的具体外观。事实上，如果你试图从记忆中画一个通用自行车，很可能你甚至不能做到远远正确，尽管你一生中见过成千上万辆自行车（参见，例如，[图
    9.15](#fig9-15)）。现在就试试吧：这种效应绝对是真实的。你的大脑已经学会完全抽象化它的视觉输入——将其转换为高级视觉概念，同时过滤掉无关的视觉细节——使得记住你周围的事物的外观变得极其困难。
- en: '![Image](../images/f0289-01.jpg)'
  id: totrans-542
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0289-01.jpg)'
- en: '**Figure 9.15 Left: Attempts to draw a bicycle from memory; right: What a schematic
    bicycle should look like**'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 9.15 左图：试图从记忆中画出的自行车；右图：原理图中自行车的样子**'
- en: 9.4.2 Visualizing convnet filters
  id: totrans-544
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.2 可视化卷积神经网络滤波器
- en: 'Another easy way to inspect the filters learned by convnets is to display the
    visual pattern that each filter is meant to respond to. This can be done with
    *gradient ascent in input space*: applying *gradient descent* to the value of
    the input image of a convnet so as to *maximize* the response of a specific filter,
    starting from a blank input image. The resulting input image will be one that
    the chosen filter is maximally responsive to.'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 通过*梯度上升在输入空间*，可以轻松地检查卷积神经网络学习到的滤波器对应的视觉模式：对卷积神经网络输入图像的值应用*梯度下降*，以*最大化*特定滤波器的响应，从一个空白输入图像开始。得到的输入图像将是所选择的滤波器响应最大的图像。
- en: 'Let’s try this with the filters of the Xception model, pretrained on ImageNet.
    The process is simple: we’ll build a loss function that maximizes the value of
    a given filter in a given convolution layer, and then we’ll use stochastic gradient
    descent to adjust the values of the input image so as to maximize this activation
    value. This will be our second example of a low-level gradient descent loop leveraging
    the GradientTape() object (the first one was in chapter 2). First, let’s instantiate
    the Xception model, loaded with weights pretrained on the ImageNet dataset.'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用在ImageNet上预训练的Xception模型的滤波器。这个过程很简单：我们将构建一个损失函数，以最大化给定卷积层中给定滤波器的值，然后我们将使用随机梯度下降来调整输入图像的值，以最大化这个激活值。这将是我们的第二个例子，涉及利用GradientTape()对象的低级梯度下降循环（第一个例子在第2章中）。首先，让我们实例化Xception模型，加载在ImageNet数据集上预训练的权重。
- en: Listing 9.12 Instantiating the Xception convolutional base
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.12 实例化Xception卷积基础
- en: model <- application_xception(
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: model <- application_xception(
- en: weights = "imagenet",
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: weights = "imagenet",
- en: include_top = FALSE➊
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: include_top = FALSE➊
- en: )
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: ➊ **The classification layers are irrelevant for this use case, so we don't
    include the top stage of the model.**
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **对于这种用例，分类层不相关，因此我们不包括模型的顶层。**
- en: We’re interested in the convolutional layers of the model—the Conv2D and SeparableConv2D
    layers. We’ll need to know their names so we can retrieve their outputs. Let’s
    print their names, in order of depth.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对模型的卷积层——Conv2D和SeparableConv2D层感兴趣。我们需要知道它们的名称，以便可以检索它们的输出。让我们按深度顺序打印它们的名称。
- en: Listing 9.13 Printing the names of all convolutional layers in Xception
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.13 打印Xception中所有卷积层的名称
- en: for (layer in model$layers)
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: for (layer in model$layers)
- en: if(any(grepl("Conv2D", class(layer))))
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: if(any(grepl("Conv2D", class(layer))))
- en: print(layer$name)
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: print(layer$name)
- en: '[1] "block1_conv1"'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] "block1_conv1"'
- en: '[1] "block1_conv2"'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] "block1_conv2"'
- en: '[1] "block2_sepconv1"'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] "block2_sepconv1"'
- en: '[1] "block2_sepconv2"'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] "block2_sepconv2"'
- en: '[1] "conv2d_29"'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] "conv2d_29"'
- en: …
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: …
- en: '[1] "block14_sepconv1"'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] "block14_sepconv1"'
- en: '[1] "block14_sepconv2"'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] "block14_sepconv2"'
- en: You’ll notice that the separable conv 2D layers here are all named something
    like block6_sepconv1, block7_sepconv2, and so forth. Xception is structured into
    blocks, each containing several convolutional layers.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到这里的可分离卷积2D层都被命名为类似block6_sepconv1、block7_sepconv2等。Xception被结构化为块，每个块包含多个卷积层。
- en: 'Now let’s create a second model that returns the output of a specific layer—a
    *feature extractor* model. Because our model is a Functional API model, it is
    inspectable: we can query the output of one of its layers and reuse it in a new
    model. No need to copy the entire Xception code.'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们创建一个第二个模型，它返回特定层的输出 - 一个 *特征提取器* 模型。因为我们的模型是一个 Functional API 模型，它是可检查的：我们可以查询其层的输出，并在新模型中重用它。不需要复制整个
    Xception 代码。
- en: Listing 9.14 Creating a feature extractor model
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: Listing 9.14 创建特征提取器模型
- en: layer_name <- "block3_sepconv1"➊
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: layer_name <- "block3_sepconv1"
- en: layer <- model %>% get_layer(name = layer_name)➋
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: layer <- model %>% get_layer(name = layer_name)
- en: feature_extractor <- keras_model(inputs = model$input,➌
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: feature_extractor <- keras_model(inputs = model$input,
- en: outputs = layer$output)
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: outputs = layer$output)
- en: ➊ **You could replace this with the name of any layer in the Xception convolutional
    base.**
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: '**您可以将此替换为 Xception 卷积基中的任何层的名称。**'
- en: ➋ **This is the layer object we're interested in.**
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: '**这是我们感兴趣的层对象。**'
- en: ➌ **We use model$input and layer$output to create a model that, given an input
    image, returns the output of our target layer.**
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们使用 model$input 和 layer$output 来创建一个模型，给定一个输入图像，返回我们目标层的输出。**'
- en: To use this model, simply call it on some input data (note that Xception requires
    inputs to be preprocessed via the xception_preprocess_input() function).
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用此模型，只需在一些输入数据上调用它（请注意，Xception 需要通过 xception_preprocess_input() 函数对输入进行预处理）。
- en: Listing 9.15 Using the feature extractor
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: Listing 9.15 使用特征提取器
- en: activation <- img_tensor %>%
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: activation <- img_tensor %>%
- en: .[tf$newaxis, , , ] %>%
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: .[tf$newaxis, , , ] %>%
- en: xception_preprocess_input() %>%
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: xception_preprocess_input() %>%
- en: feature_extractor()➊
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: feature_extractor()
- en: str(activation)
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: str(activation)
- en: '<tf.Tensor: shape=(1, 44, 44, 256), dtype=float32, numpy=…>'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: '<tf.Tensor: shape=(1, 44, 44, 256), dtype=float32, numpy=…>'
- en: ➊ **Note that this time we're calling the model directly, instead of using predict(),
    and that activation is a tf.Tensor, not an R array. (More on this soon.)**
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: '**请注意，这次我们直接调用模型，而不是使用 predict()，并且 activation 是一个 tf.Tensor，而不是一个 R 数组。（很快会详细介绍。）**'
- en: 'Let’s use our feature extractor model to define a function that returns a scalar
    value quantifying how much a given input image “activates” a given filter in the
    layer. This is the “loss function” that we’ll maximize during the gradient ascent
    process:'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用我们的特征提取模型定义一个函数，它返回一个标量值，量化给定输入图像在给定层中“激活”了多少给定滤波器。这就是我们在梯度上升过程中要最大化的“损失函数”：
- en: compute_loss <- function(image, filter_index) {➊
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: compute_loss <- function(image, filter_index) {
- en: activation <- feature_extractor(image)
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: activation <- feature_extractor(image)
- en: filter_index <- as_tensor(filter_index, "int32")➋
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: filter_index <- as_tensor(filter_index, "int32")
- en: filter_activation <-
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: filter_activation <-
- en: activation[, , , filter_index, style = "python"]➌
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: activation[, , , filter_index, style = "python"]
- en: mean(filter_activation[, 3:-3, 3:-3])➎
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: mean(filter_activation[, 3:-3, 3:-3])
- en: '}'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ **The loss function takes an image tensor and the index of the filter we are
    considering (an integer).**
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: '**损失函数接受一个图像张量和我们正在考虑的滤波器的索引（一个整数）。**'
- en: ➋ **We cast filter_index to an tensor integer here to make sure we have consistent
    behavior when we're running this function eagerly (i.e., not through tf_function()).**
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: '**在这里，我们将 filter_index 转换为整数张量，以确保在运行此函数时（即通过 tf_function()），我们有一致的行为。**'
- en: ➌ **Tell [ that filter_index is zero-based with style="python".**
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用 style="python" 告诉 [filter_index 是基于零的。**'
- en: ➍ **Note that we avoid border artifacts by only involving nonborder pixels in
    the loss; we discard the first two pixels along the sides of the activation.**
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: '**请注意，我们通过仅涉及损失中的非边界像素来避免边界伪影；我们丢弃激活边缘的前两个像素。**'
- en: ➎ **Return the mean of the activation values for the filter.**
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: '**返回滤波器激活值的平均值。**'
- en: NOTE We’ll be tracing compute_loss() with tf_function() later, with filter_
    index as a tracing tensors. Python style (0-based) indexing is currently the only
    supported style when the index is itself a tensor (this may change in the future).
    We inform [ that filter_index is zero-based with style = “python”.
  id: totrans-598
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意 我们稍后将使用 tf_function() 跟踪 compute_loss()，并将 filter_index 作为跟踪张量。当前仅支持 Python
    风格（基于 0 的）索引，当索引本身是张量时（这可能会在将来更改）。我们通知 [filter_index 使用 style = “python” 是基于零的。
- en: '**The difference between predict(model, x) and model(x)**'
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: '**predict(model, x) 和 model(x) 之间的区别**'
- en: In the previous chapter, we used predict(x) for feature extraction. Here, we’re
    using model(x). What gives?
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们使用 predict(x) 进行特征提取。在这里，我们使用 model(x)。这是为什么？
- en: Both y <- predict(model, x) and y <- model(x) (where x is an array of input
    data) mean “run the model on x and retrieve the output y.” Yet they aren’t exactly
    the same thing.
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: Both y <- predict(model, x) 和 y <- model(x)（其中 x 是输入数据的数组）意味着“在 x 上运行模型并检索输出
    y”。然而，它们并不完全相同。
- en: 'predict() loops over the data in batches (in fact, you can specify the batch
    size via predict(x, batch_size = 64)), and it extracts the R array value of the
    outputs. It’s schematically equivalent to this:'
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: predict() 循环遍历数据批次（实际上，您可以通过 predict(x, batch_size = 64) 指定批次大小），并提取输出的 R 数组值。它在原理上等同于这样：
- en: predict <- function(model, x) {
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: predict <- function(model, x) {
- en: y <- list()
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: y <- list()
- en: for(x_batch in split_into_batches(x)) {
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: for(x_batch in split_into_batches(x)) {
- en: y_batch <- as.array(model(x_batch))
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: y_batch <- as.array(model(x_batch))
- en: y[[length(y)+1]] <- y_batch
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: y[[length(y)+1]] <- y_batch
- en: '}'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: unsplit_batches(y)
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: unsplit_batches(y)
- en: '}'
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: 'This means that predict() calls can scale to very large arrays. Meanwhile,
    model(x) happens in-memory and doesn’t scale. On the other hand, predict() is
    not differentiable: you cannot retrieve its gradient if you call it in a GradientTape()
    scope.'
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着 predict() 调用可以扩展到非常大的数组。同时，model(x) 在内存中进行，不具有扩展性。另一方面，predict() 不可微分：如果你在
    GradientTape() 范围内调用它，就无法检索到其梯度。
- en: You should use model(x) when you need to retrieve the gradients of the model
    call, and you should use predict() if you just need the output value. In other
    words, always use predict() unless you’re in the middle of writing a low-level
    gradient descent loop (as we are now).
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 当你需要检索模型调用的梯度时，应该使用 model(x)，而当你只需要输出值时应该使用 predict()。换句话说，除非你正在编写低级梯度下降循环（就像我们现在这样），否则始终使用
    predict()。
- en: Let’s set up the gradient ascent step function, using the GradientTape(). A
    non-obvious trick to help the gradient descent process go smoothly is to normalize
    the gradient tensor by dividing it by its L2 norm (the square root of the average
    of the square of the values in the tensor). This ensures that the magnitude of
    the updates done to the input image is always within the same range.
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设置梯度上升步骤函数，使用 GradientTape()。一个不明显的技巧来帮助梯度下降过程顺利进行是通过将梯度张量归一化，即通过将其除以其 L2
    范数（张量中值的平方的平均值的平方根）。这确保了对输入图像所做的更新的大小始终在相同的范围内。
- en: Listing 9.16 Loss maximization via stochastic gradient ascent
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: 第 9.16 节 损失最大化通过随机梯度上升
- en: gradient_ascent_step <-
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: gradient_ascent_step <-
- en: function(image, filter_index, learning_rate) {
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: function(image, filter_index, learning_rate) {
- en: with(tf$GradientTape() %as% tape, {
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: with(tf$GradientTape() %as% tape, {
- en: tape$watch(image)➊
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: tape$watch(image)➊
- en: loss <- compute_loss(image, filter_index)➋
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: loss <- compute_loss(image, filter_index)➋
- en: '})'
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: '})'
- en: grads <- tape$gradient(loss, image)➌
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: grads <- tape$gradient(loss, image)➌
- en: grads <- tf$math$l2_normalize(grads)➍
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: grads <- tf$math$l2_normalize(grads)➍
- en: image + (learning_rate * grads)➎
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: image + (learning_rate * grads)➎
- en: '}'
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ **Explicitly watch the image tensor, because it isn't a TensorFlow Variable
    (only Variables are automatically watched in a gradient tape).**
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **显式观察图像张量，因为它不是 TensorFlow 变量（只有变量在梯度磁带中自动观察）。**
- en: ➋ **Compute the loss scalar, indicating how much the current image activates
    the filter.**
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **计算损失标量，指示当前图像激活过滤器的程度。**
- en: ➌ **Compute the gradients of the loss with respect to the image.**
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **计算损失相对于图像的梯度。**
- en: ➍ **Apply the "gradient normalization trick."**
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **应用“梯度归一化技巧”。**
- en: ➎ **Move the image a little bit in a direction that activates our target filter
    more strongly. Return the updated image so we can run the step function in a loop.**
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **将图像向更有效激活我们目标过滤器的方向稍微移动一点。返回更新后的图像，以便我们可以在循环中运行步骤函数。**
- en: Now we have all the pieces. Let’s put them together into an R function that
    takes as input a layer name and a filter index and returns a tensor representing
    the pattern that maximizes the activation of the specified filter. Note that we’ll
    use tf_function() to speed it up.
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了所有的要素。让我们将它们组合成一个 R 函数，该函数以图层名称和过滤器索引作为输入，并返回表示最大化指定过滤器激活的模式的张量。请注意，我们将使用
    tf_function() 来加速它。
- en: Listing 9.17 Function to generate filter visualizations
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 第 9.17 节 生成滤波器可视化的函数
- en: c(img_width, img_height) %<-% c(200, 200)
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: c(img_width, img_height) %<-% c(200, 200)
- en: generate_filter_pattern <- tf_function(function(filter_index) {
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: generate_filter_pattern <- tf_function(function(filter_index) {
- en: iterations <- 30➊
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: iterations <- 30➊
- en: learning_rate <- 10➋
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: learning_rate <- 10➋
- en: image <- tf$random$uniform(➌
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: image <- tf$random$uniform(➌
- en: minval = 0.4, maxval = 0.6,
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: minval = 0.4, maxval = 0.6,
- en: shape = shape(1, img_width, img_height, 3)
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: shape = shape(1, img_width, img_height, 3)
- en: )
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: for (i in seq(iterations))➍
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: for (i in seq(iterations))➍
- en: image <- gradient_ascent_step(image, filter_index, learning_rate)
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: image <- gradient_ascent_step(image, filter_index, learning_rate)
- en: image[1, , , ]➎
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 图像[1, , , ]➎
- en: '})'
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: '})'
- en: ➊ **Initialize an image tensor with random values. (The Xception model expects
    input values in the [0, 1] range, so here we pick a range centered on 0.5.)**
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **用随机值初始化图像张量。（Xception 模型期望输入值在 [0, 1] 范围内，所以这里选择以 0.5 为中心的范围。）**
- en: ➋ **Number of gradient ascent steps to apply**
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **应用的梯度上升步骤数**
- en: ➌ **Amplitude of a single step**
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **单步幅的振幅**
- en: ➍ **Repeatedly update the values of the image tensor so as to maximize our loss
    function.**
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **重复更新图像张量的值，以最大化我们的损失函数。**
- en: ➎ **Drop the batch dim and return the image.**
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **去掉批次维度并返回图像。**
- en: The resulting image tensor is a floating-point array of shape (200, 200, 3),
    with values that may not be integers within [0, 255]. Hence, we need to postprocess
    this tensor to turn it into a displayable image. We do so with the following straightforward
    utility function. We’ll do this with tensor operations and wrap in a tf_function()
    to speed it up as well.
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图像张量是一个形状为 (200, 200, 3) 的浮点数组，其值可能不是在 [0, 255] 范围内的整数。因此，我们需要对这个张量进行后处理，将其转换为可显示的图像。我们使用以下简单的实用函数来做到这一点。我们将使用张量操作并将其包装在
    tf_function() 中以加快速度。
- en: Listing 9.18 Utility function to convert a tensor into a valid image
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: 将张量转换为有效图像的实用函数清单 9.18
- en: deprocess_image <- tf_function(function(image, crop = TRUE) {
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: deprocess_image <- tf_function(function(image, crop = TRUE) {
- en: image <- image - mean(image)➊
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: 图像 <- 图像 - mean(图像)➊
- en: image <- image / tf$math$reduce_std(image)➋
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: 图像 <- 图像 / tf$math$reduce_std(图像)➋
- en: image <- (image * 64) + 128
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 图像 <- (图像 * 64) + 128
- en: image <- tf$clip_by_value(image, 0, 255)
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 图像 <- tf$clip_by_value(图像, 0, 255)
- en: if(crop)
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: if(crop)
- en: image <- image[26:-26, 26:-26, ]➌
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 图像 <- 图像[26:-26, 26:-26, ]➌
- en: image
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: 图像
- en: '})'
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: '})'
- en: ➊ **mean() invokes tf$math$reduce_mean().**
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **mean() 调用了 tf$math$reduce_mean()。**
- en: ➋ **Normalize image values within the [0, 255] range.**
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **将图像值标准化到 [0, 255] 范围内。**
- en: ➌ **Center-crop to avoid border artifacts.**
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **中心裁剪以避免边缘伪影。**
- en: 'Let’s try it (see [figure 9.16](#fig9-16)):'
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试一试（参见 [图 9.16](#fig9-16)）：
- en: generate_filter_pattern(filter_index = as_tensor(2L)) %>%
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: generate_filter_pattern(filter_index = as_tensor(2L)) %>%
- en: deprocess_image() %>%
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: deprocess_image() %>%
- en: display_image_tensor()
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: display_image_tensor()
- en: '![Image](../images/f0293-01.jpg)'
  id: totrans-667
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0293-01.jpg)'
- en: '**Figure 9.16 Pattern that the second channel in layer block3_sepconv1 responds
    to maximally**'
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 9.16：layer block3_sepconv1 中第二通道响应最大的模式**'
- en: Note that we cast filter_index with as_tensor() here. We do this because a tf_
    function() compiles a separate optimized function for each unique way it’s called,
    and a different constant literal counts as a unique call signature. If we didn’t
    call as_tensor() here, then in the coming loop where we plot the first 64 activations,
    tf_function() would trace and compile generate_filter_pattern() 64 times! Calling
    the tf_function() decorated function with a tensor, however, even a constant tensor,
    doesn’t count as a unique function signature for tf_function(), and generate_
    filter_pattern() is traced only once.
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，此处将 filter_index 转换为 as_tensor()。我们这样做是因为 tf_ function() 会为其调用的每种唯一方式编译一个单独的优化函数，并且不同的常量文字会计为唯一的调用签名。如果我们在这里没有调用
    as_tensor()，那么在接下来的循环中绘制前 64 个激活时，tf_function() 将追踪并编译 generate_filter_pattern()
    64 次！然而，使用张量调用 tf_function() 装饰的函数，即使是一个常量张量，对于 tf_function() 来说并不算是一个唯一的函数签名，而
    generate_filter_pattern() 只会被追踪一次。
- en: It seems that the third filter in layer block3_sepconv1 is responsive to a horizontal
    lines pattern, somewhat water-like or fur-like.
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来在 layer block3_sepconv1 的第三个过滤器对水平线条模式有响应，有点像水或毛皮。
- en: 'Now the fun part: you can start visualizing every filter in the layer, and
    even every filter in every layer in the model .'
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有趣的部分开始了：你可以开始可视化图层中的每个滤波器，甚至是模型中每个图层中的每个滤波器。
- en: Listing 9.19 Generating a grid of all filter response patterns in a layer
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: 在一层生成所有滤波器响应模式的网格清单 9.19
- en: par(mfrow = c(8, 8))
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: par(mfrow = c(8, 8))
- en: for (i in seq(0, 63)) {➊
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: for (i in seq(0, 63)) {➊
- en: generate_filter_pattern(filter_index = as_tensor(i)) %>%
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: generate_filter_pattern(filter_index = as_tensor(i)) %>%
- en: deprocess_image() %>%
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: deprocess_image() %>%
- en: display_image_tensor(plot_margins = rep(.1, 4))
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: display_image_tensor(plot_margins = rep(.1, 4))
- en: '}'
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ **Generate and plot visualizations for the first 64 filters in the layer.**
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **生成并绘制图层中前 64 个滤波器的可视化。**
- en: 'These filter visualizations (see [figure 9.17](#fig9-17)) tell you a lot about
    how convnet layers see the world: each layer in a convnet learns a collection
    of filters such that their inputs can be expressed as a combination of the filters.
    This is similar to how the Fourier transform decomposes signals onto a bank of
    cosine functions. The filters in these convnet filter banks get increasingly complex
    and refined as you go deeper in the model:'
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: 这些滤波器可视化（见[图9.17](#fig9-17)）告诉你一些关于CNN层次如何看待世界的方法：在CNN中的每个层次学习了一些这样的滤波器，它们的输入可以表示为这些滤波器的组合。这与傅里叶变换将信号分解为一堆余弦函数的方式类似。在这些卷积网络过滤器中，你越深入模型，它们就变得越来越复杂和精细：
- en: '![Image](../images/f0294-01.jpg)'
  id: totrans-681
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0294-01.jpg)'
- en: '**Figure 9.17 Some filter patterns for layers block2_sepconv1, block4_sepconv1,
    and block8_sepconv1**'
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: '**图9.17 层次block2_sepconv1，block4_sepconv1和block8_sepconv1的一些滤波器模式。**'
- en: The filters from the first layers in the model encode simple directional edges
    and colors (or colored edges, in some cases).
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型中第一层的滤波器编码了简单的方向边缘和颜色（在某些情况下是彩色边缘）。
- en: The filters from layers a bit further up the stack, such as block4_sepconv1,
    encode simple textures made from combinations of edges and colors
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络层次更高的滤波器（如block4_sepconv1）编码了由边缘和颜色组合而成的简单纹理。
- en: 'The filters in higher layers begin to resemble textures found in natural images:
    feathers, eyes, leaves, and so on'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更高层次的滤波器开始类似于自然图像中的纹理：羽毛、眼睛，树叶等等。
- en: 9.4.3 Visualizing heatmaps of class activation
  id: totrans-686
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.3 可视化类别激活热图
- en: We’ll introduce one last visualization technique—one that is useful for understanding
    which parts of a given image led a convnet to its final classification decision.
    This is helpful for “debugging” the decision process of a convnet, particularly
    in the case of a classification mistake (a problem domain called *model interpretability*).
    It can also allow you to locate specific objects in an image.
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: 最后我们介绍一种可视化技术--这对于理解给定图像的哪些部分导致了卷积网络的最终分类决策非常有用。尤其是在分类错误（称为*模型可解释性*问题域）的情况下，这对“调试”卷积网络的决策过程非常有帮助。它还可以帮助你在图像中定位特定的对象。
- en: This general category of techniques is called *class activation map* (CAM) visualization,
    and it consists of producing heatmaps of class activation over input images. A
    class activation heatmap is a 2D grid of scores associated with a specific output
    class, computed for every location in any input image, indicating how important
    each location is with respect to the class under consideration. For instance,
    given an image fed into a dogs-versus-cats convnet, CAM visualization would allow
    you to generate a heat-map for the class “cat,” indicating how catlike different
    parts of the image are, and also a heatmap for the class “dog,” indicating how
    doglike parts of the image are.
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术的通用类别被称为*类别激活地图*（CAM）可视化，它由产生输入图像上的类别激活热图组成。类别激活热图是与特定输出类别相关联的分数字逐渐计算每个输入图像中的所有位置，表示每个位置相对于考虑的类别有多重要。例如，给定输入到狗猫卷积网络的图像，CAM可视化将允许生成一个关于“猫”的热图，指示图像的不同部分有多像猫，以及一个关于“狗”的热图，指示图像的不同部分有多像狗。
- en: 'The specific implementation we’ll use is the one described in an article titled,
    “Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization.”^([5](#Rendnote5))'
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将使用的具体实现是一个名为“Grad-CAM: Visual Explanations from Deep Networks via Gradient-based
    Localization”的文章中所描述的。'
- en: Grad-CAM consists of taking the output feature map of a convolution layer, given
    an input image, and weighing every channel in that feature map by the gradient
    of the class with respect to the channel. Intuitively, one way to understand this
    trick is to imagine that you’re weighting a spatial map of “how intensely the
    input image activates different channels” by “how important each channel is with
    regard to the class,” resulting in a spatial map of “how intensely the input image
    activates the class.” Let’s demonstrate this technique using the pretrained Xception
    model.
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: Grad-CAM由卷积层的输出特征映射、一个输入图像和将每个通道在特征映射中加权的类别相对于通道的梯度组成。直观来说，理解这个技巧的一种方式是，在将“输入图片激发不同通道的强度”空间图加权为“每个类别对于通道的重要性”，形成一个“输入图片激发类别的强度”空间图。我们使用预训练的Xception模型来演示这个技巧。
- en: Listing 9.20 Loading the Xception network with pretrained weights
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: 代码清单9.20：加载预训练权重的Xception网络。
- en: model <- application_xception(weights = "imagenet")➊
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: model <-application_xception(weights = "imagenet")➊
- en: ➊ **Note that we include the densely connected classifier on top; in all previous
    cases, we discarded it.**
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **请注意，我们在顶部包括了密集连接的分类器；在所有先前的情况下，我们都将其丢弃。**
- en: 'Consider the image of two African elephants shown in [figure 9.18](#fig9-18),
    possibly a mother and her calf, strolling on the savanna. Let’s convert this image
    into something the Xception model can read: the model was trained on images of
    size 299 × 299, preprocessed according to a few rules that are packaged in the
    xception_preprocess_input() utility function.'
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一下图9.18中显示的两只非洲象的图像，可能是一只母象和一只小象，在大草原上漫步。让我们将这个图像转换为 Xception 模型可以读取的内容：该模型是根据几条规则进行训练的，这些规则封装在
    xception_preprocess_input() 实用函数中。
- en: '![Image](../images/f0296-01.jpg)'
  id: totrans-695
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0296-01.jpg)'
- en: '**Figure 9.18 Test picture of African elephants**'
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: '**图9.18 非洲象测试图片**'
- en: So we need to load the image, resize it to 299 × 299, convert it to a float32
    tensor, and apply these preprocessing rules.
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要加载图像，将其调整大小为 299 × 299，将其转换为 float32 张量，并应用这些预处理规则。
- en: Listing 9.21 Preprocessing an input image for Xception
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: Listing 9.21 预处理 Xception 输入图像
- en: img_path <- get_file(➊
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: img_path <- get_file(➊
- en: fname = "elephant.jpg",
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: fname = "elephant.jpg",
- en: origin = "https://img-datasets.s3.amazonaws.com/elephant.jpg")
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: origin = "https://img-datasets.s3.amazonaws.com/elephant.jpg")
- en: img_tensor <- tf_read_image(img_path, resize = c(299, 299))➋
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: img_tensor <- tf_read_image(img_path, resize = c(299, 299))➋
- en: preprocessed_img <- img_tensor[tf$newaxis, , , ] %>%➌
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: preprocessed_img <- img_tensor[tf$newaxis, , , ] %>%➌
- en: xception_preprocess_input()➍
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: xception_preprocess_input()➍
- en: ➊ **Download the image and store it locally under the path img_path.**
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **下载图像并在路径 img_path 下存储到本地。**
- en: ➋ **Read the image as a tensor and resize it to 299 × 299\. img_tensor is float32
    with shape (299, 299, 3).**
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **将图像读取为张量并将其调整大小为 299 × 299。img_tensor 是形状为 (299, 299, 3) 的 float32 类型。**
- en: ➌ **Add a dimension to transform the array into a batch of size (1, 299, 299,
    3).**
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **添加一个维度，将数组转换为大小为 (1, 299, 299, 3) 的批次。**
- en: ➍ **Preprocess the batch (this does channel-wise color normalization).**
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **对批处理进行预处理（这样可以进行通道颜色归一化）。**
- en: 'You can now run the pretrained network on the image and decode its prediction
    vector back to a human-readable format:'
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以在图像上运行预训练网络，并将其预测向量解码回可读的人类格式：
- en: preds <- predict(model, preprocessed_img)
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: preds <- predict(model, preprocessed_img)
- en: str(preds)
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: str(preds)
- en: '![Image](../images/f0297-01.jpg)'
  id: totrans-712
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0297-01.jpg)'
- en: imagenet_decode_predictions(preds, top=3)[[1]]
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: imagenet_decode_predictions(preds, top=3)[[1]]
- en: '![Image](../images/f0297-02.jpg)'
  id: totrans-714
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0297-02.jpg)'
- en: 'The top three classes predicted for this image are as follows:'
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: 该图像的前三个预测类别如下：
- en: African elephant (with 90% probability)
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非洲象（概率为 90%）
- en: Tusker (with 5% probability)
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带象牙的象（概率为 5%）
- en: Indian elephant (with 2% probability
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 印度象（概率为 2%）
- en: 'The network has recognized the image as containing an undetermined quantity
    of African elephants. The entry in the prediction vector that was maximally activated
    is the one corresponding to the “African elephant” class, at index 387:'
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: 网络已经识别出图像中包含未确定数量的非洲象。预测向量中最大激活的条目是对应于“非洲象”类的条目，索引为 387：
- en: which.max(preds[1, ])
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: which.max(preds[1, ])
- en: '[1] 387'
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 387'
- en: To visualize which parts of the image are the most African-elephant-like, let’s
    set up the Grad-CAM process. First, we create a model that maps the input image
    to the activations of the last convolutional layer.
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: 要可视化图像的哪些部分最像非洲象，让我们设置 Grad-CAM 过程。首先，我们创建一个模型，将输入图像映射到最后一个卷积层的激活。
- en: Listing 9.22 Setting up a model that returns the last convolutional output
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: Listing 9.22 设置返回最后一个卷积输出的模型
- en: last_conv_layer_name <- "block14_sepconv2_act"
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: last_conv_layer_name <- "block14_sepconv2_act"
- en: classifier_layer_names <- c("avg_pool", "predictions")➊
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: classifier_layer_names <- c("avg_pool", "predictions")➊
- en: last_conv_layer <- model %>% get_layer(last_conv_layer_name)
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: last_conv_layer <- model %>% get_layer(last_conv_layer_name)
- en: last_conv_layer_model <- keras_model(model$inputs,
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: last_conv_layer_model <- keras_model(model$inputs,
- en: last_conv_layer$output)
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: last_conv_layer$output)
- en: ➊ **Names of last two layers in the Xception model**
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **Xception 模型中的最后两个层的名称**
- en: Second, we create a model that maps the activations of the last convolutional
    layer to the final class predictions.
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们创建一个模型，将最后一个卷积层的激活映射到最终的类预测。
- en: Listing 9.23 Reapplying the classifier on top of the last convolutional output
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: Listing 9.23 在最后一个卷积输出的顶部重新应用分类器
- en: classifier_input <- layer_input(batch_shape = last_conv_layer$output$shape)
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
  zh: classifier_input <- layer_input(batch_shape = last_conv_layer$output$shape)
- en: x <- classifier_input
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: x <- classifier_input
- en: for (layer_name in classifier_layer_names)
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: for (layer_name in classifier_layer_names)
- en: x <- get_layer(model, layer_name)(x)
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: x <- get_layer(model, layer_name)(x)
- en: classifier_model <- keras_model(classifier_input, x)
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: classifier_model <- keras_model(classifier_input, x)
- en: Then we compute the gradient of the top predicted class for our input image
    with respect to the activations of the last convolution layer.
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算关于最后一个卷积层的激活与输入图片之间的梯度。
- en: Listing 9.24 Retrieving the gradients of the top predicted class
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.24 检索最高预测类别的梯度
- en: with (tf$GradientTape() %as% tape, {
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: with (tf$GradientTape() %as% tape, {
- en: last_conv_layer_output <- last_conv_layer_model(preprocessed_img)
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: last_conv_layer_output <- last_conv_layer_model(preprocessed_img)
- en: tape$watch(last_conv_layer_output)➊
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: tape$watch(last_conv_layer_output)➊
- en: preds <- classifier_model(last_conv_layer_output)
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
  zh: preds <- classifier_model(last_conv_layer_output)
- en: top_pred_index <- tf$argmax(preds[1, ])
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: top_pred_index <- tf$argmax(preds[1, ])
- en: top_class_channel <- preds[, top_pred_index, style = "python"]➋
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
  zh: top_class_channel <- preds[, top_pred_index, style = "python"]➋
- en: '})'
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: '})'
- en: grads <- tape$gradient(top_class_channel, last_conv_layer_output)➌
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: grads <- tape$gradient(top_class_channel, last_conv_layer_output)➌
- en: ➊ **Compute activations of the last conv layer and make the tape watch it.**
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **计算最后一个卷积层的激活并使其纳入到计算图中。**
- en: ➋ **Retrieve the activation channel corresponding to the top predicted class.**
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **检索与最高预测类别相对应的激活通道。**
- en: ➌ **This is the gradient of the top predicted class with regard to the output
    feature map of the last convolutional layer.**
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **这是最高预测类别对于最后一个卷积层的输出特征图的梯度。**
- en: Now we apply pooling and importance weighting to the gradient tensor to obtain
    our heatmap of class activation.
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将应用池化操作和重要性加权来获得类别激活的热图。
- en: Listing 9.25 Gradient pooling and channel-importance weighting
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.25 梯度池化和通道重要性加权
- en: pooled_grads <- mean(grads, axis = c(1, 2, 3), keepdims = TRUE)➊
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: pooled_grads <- mean(grads, axis = c(1, 2, 3), keepdims = TRUE)➊
- en: ➋
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
  zh: ➋
- en: heatmap <
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: heatmap <
- en: (last_conv_layer_output * pooled_grads) %>%➌➍➎➏
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
  zh: (last_conv_layer_output * pooled_grads) %>%➌➍➎➏
- en: mean(axis = -1) %>%➐➑
  id: totrans-756
  prefs: []
  type: TYPE_NORMAL
  zh: mean(axis = -1) %>%➐➑
- en: .[1, , ]➒
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: .[1, , ]➒
- en: ➊ **pooled_grads is a vector where each entry is the mean intensity of the gradient
    for a given channel. It quantifies the importance of each channel with regard
    to the top predicted class.**
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **pooled_grads是一个向量，每个条目是给定通道的梯度的均值强度，它量化了每个通道相对于最高预测类别的重要性。**
- en: ➋ **We take advantage of Tensor broadcasting rules here to avoid writing a for
    loop. The size-1 axes of pooled_grads are automatically broadcast to match the
    corresponding axes of last_conv_layer_output.**
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **我们利用张量广播规则，避免使用for循环。大小为1的轴会自动广播以匹配last_conv_layer_output的相应轴。**
- en: ➌ **Multiply each channel in the output of the last convolutional layer by "how
    important this channel is."**
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **将上一个卷积层的每个通道的输出乘以“此通道的重要性”。**
- en: ➍ **grads and last_conv_layer_output have the same shape, (1, 10, 10, 2048).**
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **grads和last_conv_layer_output具有相同的形状（1，10，10，2048）。**
- en: ➎ **pooled_grads has shape (1, 1, 1, 2048).**
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **pooled_grads的形状为（1，1，1，2048）。**
- en: '➏ **Shape: (1, 10, 10, 2048)**'
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: ➏ **形状：（1，10，10，2048）**
- en: ➐ **The channel-wise mean of the resulting feature map is our heatmap of class
    activation.**
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
  zh: ➐ **结果特征图的逐通道平均值是我们的类别激活热图。**
- en: '➑ **Shape: (1, 10, 10)**'
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: ➑ **形状：（1，10，10）**
- en: '➒ **Drop batch dim; output shape: (10, 10).**'
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: ➒ **去掉批次维度；输出形状：（10，10）。**
- en: The result is shown in [figure 9.19.](#fig9-19)
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示在[图 9.19](#fig9-19)中。
- en: Listing 9.26 Heatmap postprocessing
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.26 热图后处理
- en: par(mar = c(0, 0, 0, 0))
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: par(mar = c(0, 0, 0, 0))
- en: plot_activations(heatmap)
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制激活图(heatmap)
- en: Finally, let’s superimpose the activations heatmap over the original image.
    We cut() the heatmap values to a sequential color palette, and then convert to
    an R raster object. Note we make sure to pass alpha = .4 to the palette, so that
    we can still see the original image when we superimpose the heatmap over it. (See
    [figure 9.20](#fig9-20).)
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们将激活的
- en: '![Image](../images/f0299-01.jpg)'
  id: totrans-772
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0299-01.jpg)'
- en: '**Figure 9.19 Standalone class activation heatmap**'
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 9.19 独立的类别激活热图**'
- en: Listing 9.27 Superimposing the heatmap on the original picture
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.27 在原始图像上叠加热图
- en: pal <- hcl.colors(256, palette = "Spectral", alpha = .4, rev = TRUE)
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: pal <- hcl.colors(256, palette = "Spectral", alpha = .4, rev = TRUE)
- en: heatmap <- as.array(heatmap)
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: heatmap <- as.array(heatmap)
- en: heatmap[] <- pal[cut(heatmap, 256)]
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: heatmap[] <- pal[cut(heatmap, 256)]
- en: heatmap <- as.raster(heatmap)
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: heatmap <- as.raster(heatmap)
- en: img <- tf_read_image(img_path, resize = NULL)➊
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
  zh: img <- tf_read_image(img_path, resize = NULL)➊
- en: display_image_tensor(img)
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: display_image_tensor(img)
- en: rasterImage(heatmap, 0, 0, ncol(img), nrow(img), interpolate = FALSE)➋
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
  zh: rasterImage(heatmap, 0, 0, ncol(img), nrow(img), interpolate = FALSE)➋
- en: ➊ **Load the original image, without resizing this time.**
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **加载原始图像，这次不调整大小。**
- en: ➋ **Superimpose the heatmap over the original image, with the heatmap at 40%
    opacity. We pass ncol(img) and nrow(img) so that the heatmap, which has fewer
    pixels, is drawn to match the size of the original image. We pass interpolate
    = FALSE so we can clearly see where the activation map pixel boundaries are.**
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **将热图叠加到原始图像上，热图的不透明度为40%。我们传递 ncol(img) 和 nrow(img)，以使热图（像素较少）的绘制大小与原始图像匹配。我们传递
    interpolate = FALSE，这样我们就可以清楚地看到激活地图像素边界的位置。**
- en: 'This visualization technique answers two important questions:'
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
  zh: 这种可视化技术回答了两个重要问题：
- en: Why did the network think this image contained an African elephant
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络为什么认为这张图片包含非洲象
- en: Where is the African elephant located in the picture
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图中非洲象位于哪里
- en: 'In particular, it’s interesting to note that the ears of the elephant calf
    are strongly activated: this is probably how the network can tell the difference
    between African and Indian elephants.'
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
  zh: 特别值得注意的是，非洲象幼崽的耳朵被强烈激活：这可能是网络区分非洲象和印度象的方式。
- en: '![Image](../images/f0300-01.jpg)'
  id: totrans-788
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0300-01.jpg)'
- en: '**Figure 9.20 African elephant class activation heatmap over the test picture**'
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: '**图9.20 测试图片上的非洲象类激活热图**'
- en: Summary
  id: totrans-790
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'You can do three essential computer vision tasks with deep learning: image
    classification, image segmentation, and object detection.'
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以使用深度学习执行三个基本的计算机视觉任务：图像分类、图像分割和目标检测。
- en: Following modern convnet architecture best practices will help you get the most
    out of your models. Some of these best practices include using residual connections,
    batch normalization, and depthwise separable convolutions.
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 遵循现代卷积神经网络架构的最佳实践将帮助您充分利用模型。其中一些最佳实践包括使用残差连接、批量归一化和深度可分离卷积。
- en: The representations that convnets learn are easy to inspect—convnets are the
    opposite of black boxes!
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络学习的表示很容易检查——卷积神经网络与黑匣子相反！
- en: You can generate visualizations of the filters learned by your convnets, as
    well as heatmaps of class activity.
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以生成卷积神经网络学习的滤波器的可视化，以及类活动的热图。
- en: ^([1](#endnote1)) Kaiming He et al., “Deep Residual Learning for Image Recognition,”
    Conference on Computer Vision and Pattern Recognition (2015), [https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385).
  id: totrans-795
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ^([1](#endnote1)) Kaiming He等人，“用于图像识别的深度残差学习”，计算机视觉与模式识别会议（2015），[https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)。
- en: '^([2](#endnote2)) Sergey Ioffe and Christian Szegedy, “Batch Normalization:
    Accelerating Deep Network Training by Reducing Internal Covariate Shift,” *Proceedings
    of the 32nd International Conference on Machine Learning* (2015), [https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167).'
  id: totrans-796
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ^([2](#endnote2)) Sergey Ioffe和Christian Szegedy，“批量归一化：通过减少内部协变量转移加速深度网络训练”，*第32届国际机器学习会议论文集*（2015），[https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167)。
- en: '^([3](#endnote3)) François Chollet, “Xception: Deep Learning with Depthwise
    Separable Convolutions,” Conference on Computer Vision and Pattern Recognition
    (2017), [https://arxiv.org/abs/1610.02357](https://arxiv.org/abs/1610.02357).'
  id: totrans-797
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ^([3](#endnote3)) François Chollet，“Xception：具有深度可分离卷积的深度学习”，计算机视觉与模式识别会议（2017），[https://arxiv.org/abs/1610.02357](https://arxiv.org/abs/1610.02357)。
- en: ^([4](#endnote4)) Liang-Chieh Chen et al., “Encoder-Decoder with Atrous Separable
    Convolution for Semantic Image Segmentation,” ECCV (2018), [https://arxiv.org/abs/1802.02611](https://arxiv.org/abs/1802.02611).
  id: totrans-798
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ^([4](#endnote4)) Liang-Chieh Chen等人，“具有空洞可分离卷积的编码器-解码器用于语义图像分割”，ECCV（2018），[https://arxiv.org/abs/1802.02611](https://arxiv.org/abs/1802.02611)。
- en: ^([5](#endnote5)) Ramprasaath R. Selvaraju et al., arXiv (2017), [https://arxiv.org/abs/1610.02391](https://arxiv.org/abs/1610.02391).
  id: totrans-799
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ^([5](#endnote5)) Ramprasaath R. Selvaraju等人，arXiv（2017），[https://arxiv.org/abs/1610.02391](https://arxiv.org/abs/1610.02391)。
