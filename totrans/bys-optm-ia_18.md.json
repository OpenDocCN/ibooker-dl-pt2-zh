["```py\n    import torch\n    import gpytorch\n    import matplotlib.pyplot as plt\n    ```", "```py\n    train_x = torch.tensor(\n        [\n            [1 / 2, 1 / 2, 0, 0],\n            [1 / 3, 1 / 3, 1 / 3, 0],\n            [0, 1 / 2, 1 / 2, 0],\n            [0, 1 / 3, 1 / 3, 1 / 3],\n        ]\n    )\n\n    train_y = torch.tensor([192.08, 258.30, 187.24, 188.54])\n    ```", "```py\n    # normalize the labels\n    train_y = (train_y - train_y.mean()) / train_y.std()\n    ```", "```py\n    class BaseGPModel(gpytorch.models.ExactGP):\n        def __init__(self, train_x, train_y, likelihood):\n            super().__init__(train_x, train_y, likelihood)\n            self.mean_module = gpytorch.means.ZeroMean()\n            self.covar_module = gpytorch.kernels.RBFKernel()\n\n        def forward(self, x):\n            mean_x = self.mean_module(x)\n            covar_x = self.covar_module(x)\n            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n    ```", "```py\n    lengthscale = 1\n    noise = 1e-4\n\n    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n    model = BaseGPModel(train_x, train_y, likelihood)\n\n    model.covar_module.lengthscale = lengthscale\n    model.likelihood.noise = noise\n\n    model.eval()\n    likelihood.eval()\n    ```", "```py\n    grid_x = torch.linspace(0, 1, 101)\n\n    grid_x1, grid_x2 = torch.meshgrid(grid_x, grid_x, indexing=\"ij\")\n    ```", "```py\n    xs = torch.vstack(\n        [\n            grid_x1.flatten(),      ❶\n            grid_x2.flatten(),      ❷\n            torch.zeros(101 ** 2),  ❸\n            torch.zeros(101 ** 2),  ❹\n        ]\n    ).transpose(-1, -2)\n    ```", "```py\n    with torch.no_grad():\n        predictive_distribution = likelihood(model(xs))\n        predictive_mean = predictive_distribution.mean\n        predictive_stddev = predictive_distribution.stddev\n    ```", "```py\n    fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n    ```", "```py\n    c = ax[0].imshow(\n        predictive_mean.detach().reshape(101, 101).transpose(-1, -2),\n        origin=\"lower\",\n        extent=[0, 1, 0, 1],\n    )                            ❶\n\n    c = ax[1].imshow(\n        predictive_stddev.detach().reshape(101, 101).transpose(-1, -2),\n        origin=\"lower\",\n        extent=[0, 1, 0, 1],\n    )                            ❷\n    plt.colorbar(c, ax=ax[1])\n    ```", "```py\n    import torch\n    import gpytorch\n    import matplotlib.pyplot as plt\n    ```", "```py\n    def f(x):\n        return (\n            torch.sin(5 * x[..., 0] / 2 - 2.5) * torch.cos(2.5 - 5 * x[..., 1])\n            + (5 * x[..., 1] / 2 + 0.5) ** 2 / 10\n        ) / 5 + 0.2\n    ```", "```py\n    lb = 0\n    ub = 2\n    xs = torch.linspace(lb, ub, 101)                                   ❶\n    x1, x2 = torch.meshgrid(xs, xs)\n    xs = torch.vstack((x1.flatten(), x2.flatten())).transpose(-1, -2)  ❷\n    ```", "```py\n    ys = f(xs)\n    ```", "```py\n    plt.imshow(ys.reshape(101, 101).T, origin=\"lower\", extent=[lb, ub, lb, ub])\n    ```", "```py\n    torch.manual_seed(0)\n    train_x = torch.rand(size=(100, 2)) * 2\n    ```", "```py\n    train_y = f(train_x)\n    ```", "```py\n    class GPModel(gpytorch.models.ExactGP):\n        def __init__(self, train_x, train_y, likelihood):\n            super().__init__(train_x, train_y, likelihood)\n            self.mean_module = gpytorch.means.ConstantMean()\n            self.covar_module = gpytorch.kernels.ScaleKernel(\n                gpytorch.kernels.MaternKernel(\n                    nu=2.5,\n                    ard_num_dims=None    ❶\n                )\n            )\n\n        def forward(self, x):\n            mean_x = self.mean_module(x)\n            covar_x = self.covar_module(x)\n            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n    ```", "```py\n    noise = 1e-4\n\n    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n    model = GPModel(train_x, train_y, likelihood)\n\n    model.likelihood.noise = noise\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n\n    model.train()                    ❶\n    likelihood.train()               ❶\n\n    losses = []\n    for i in tqdm(range(500)):       ❷\n        optimizer.zero_grad()        ❷\n\n        output = model(train_x)      ❷\n        loss = -mll(output, train_y) ❷\n\n        loss.backward()              ❷\n        losses.append(loss.item())   ❷\n\n        optimizer.step()             ❷\n\n    model.eval()                     ❸\n    likelihood.eval()                ❸\n    ```", "```py\n    >>> model.covar_module.base_kernel.lengthscale\n    tensor([[1.1535]])\n    ```", "```py\n    >>> model.covar_module.base_kernel.lengthscale\n    tensor([[1.6960, 0.8739]])\n    ```", "```py\n    epsilon = 0.1\n    ```", "```py\n    policy = botorch.acquisition.analytic.ProbabilityOfImprovement(\n        model, best_f=train_y.max() + epsilon\n    )\n    ```", "```py\n    epsilon_pct = 0.1\n\n    for i in range(num_queries):\n        ...                                                    ❶\n\n        policy = botorch.acquisition.analytic.ProbabilityOfImprovement(\n            model, best_f=train_y.max() * (1 + epsilon_pct)    ❷\n        )\n    ```", "```py\n    def f(x):\n        return (\n            torch.sin(5 * x[..., 0] / 2 - 2.5)\n            * torch.cos(2.5 - 5 * x[..., 1])\n            + (5 * x[..., 1] / 2 + 0.5) ** 2 / 10\n        ) / 5 + 0.2\n    ```", "```py\n    lb = 0\n    ub = 2\n    num_queries = 20\n\n    bounds = torch.tensor([[lb, lb], [ub, ub]], dtype=torch.float)\n\n    xs = torch.linspace(lb, ub, 101)\n    x1, x2 = torch.meshgrid(xs, xs)\n    xs = torch.vstack((x1.flatten(), x2.flatten())).transpose(-1, -2)\n    ys = f(xs)\n    ```", "```py\n    def visualize_progress_and_policy(policy, next_x=None):\n        with torch.no_grad():\n            acquisition_score = policy(xs.unsqueeze(1))\n\n        ...    ❶\n    ```", "```py\n    c = ax[0].imshow(                                                    ❶\n        ys.reshape(101, 101).T, origin=\"lower\", extent=[lb, ub, lb, ub]  ❶\n    )                                                                    ❶\n    ax[0].set_xlabel(r\"$C$\", fontsize=20)\n    ax[0].set_ylabel(r\"$\\gamma$\", fontsize=20)\n    plt.colorbar(c, ax=ax[0])\n\n    ax[0].scatter(train_x[..., 0], train_x[..., 1], marker=\"x\", c=\"k\")   ❷\n    ```", "```py\n    c = ax[1].imshow(                           ❶\n        acquisition_score.reshape(101, 101).T,  ❶\n        origin=\"lower\",                         ❶\n        extent=[lb, ub, lb, ub]                 ❶\n    )                                           ❶\n    ax[1].set_xlabel(r\"$C$\", fontsize=20)\n    plt.colorbar(c, ax=ax[1])\n    ```", "```py\n    if next_x is not None:\n        ax[1].scatter(\n            next_x[..., 0],\n            next_x[..., 1],\n            c=\"r\",\n            marker=\"*\",\n            s=500,\n            label=\"next query\"\n        )\n    ```", "```py\n    class GPModel(gpytorch.models.ExactGP,         ❶\n      botorch.models.gpytorch.GPyTorchModel):      ❶\n        _num_outputs = 1                           ❶\n\n        def __init__(self, train_x, train_y, likelihood):\n            super().__init__(train_x, train_y, likelihood)\n            self.mean_module = gpytorch.means.ConstantMean()\n            self.covar_module = gpytorch.kernels.  ❷\n            ➥ScaleKernel(                         ❷\n                gpytorch.kernels.MaternKernel(     ❷\n                    nu=2.5,                        ❷\n                    ard_num_dims=2                 ❷\n                )                                  ❷\n            )                                      ❷\n\n        def forward(self, x):\n            ...                                    ❸\n    ```", "```py\n    train_x = torch.tensor([\n        [1., 1.],\n    ])\n    train_y = f(train_x)\n    ```", "```py\n    num_queries = 20\n\n    for i in range(num_queries):\n        print(\"iteration\", i)\n        print(\"incumbent\", train_x[train_y.argmax()], train_y.max())\n\n        model, likelihood = fit_gp_model(train_x, train_y)\n\n        policy = ...              ❶\n\n        next_x, acq_val = botorch.optim.optimize_acqf(\n            policy,\n            bounds=bounds,\n            q=1,\n            num_restarts=40,      ❷\n            raw_samples=100,      ❷\n        )\n\n        visualize_progress_and_policy(policy,\n        ➥next_x=next_x)          ❸\n\n        next_y = f(next_x)\n\n        train_x = torch.cat([train_x, next_x])\n        train_y = torch.cat([train_y, next_y])\n    ```", "```py\n    policy = botorch.acquisition.analytic.ProbabilityOfImprovement(\n        model, best_f=train_y.max()\n    )\n    ```", "```py\n    policy = botorch.acquisition.analytic.ProbabilityOfImprovement(\n        model, best_f=train_y.max() + 0.1\n    )\n    ```", "```py\n    policy = botorch.acquisition.analytic.ExpectedImprovement(\n        model, best_f=train_y.max()\n    )\n    ```", "```py\n    num_repeats = 10\n\n    incumbents = torch.zeros((num_repeats, num_queries))\n\n    for trial in range(num_repeats):\n        print(\"trial\", trial)\n\n        torch.manual_seed(trial)                      ❶\n        train_x = bounds[0] + (bounds[1] - bounds[0]) ❶\n        ➥* torch.rand(1, 2)                          ❶\n        train_y = f(train_x)                          ❶\n\n        for i in tqdm(range(num_queries)):\n            incumbents[trial, i] = train_y.max()      ❷\n\n            ...                                       ❸\n\n    torch.save(incumbents, [path to file])            ❹\n    ```", "```py\n    def show_agg_progress(path, name):\n        def ci(y):                                             ❶\n            return 2 * y.std(axis=0) / np.sqrt(num_repeats)    ❶\n\n        incumbents = torch.load(path)                          ❷\n\n        avg_incumbent = incumbents.mean(axis=0)                ❸\n        ci_incumbent = ci(incumbents)                          ❸\n\n        plt.plot(avg_incumbent, label=name)                    ❹\n        plt.fill_between(                                      ❹\n            np.arange(num_queries),                            ❹\n            avg_incumbent + ci_incumbent,                      ❹\n            avg_incumbent - ci_incumbent,                      ❹\n            alpha=0.3,                                         ❹\n        )                                                      ❹\n    ```", "```py\n    plt.figure(figsize=(8, 6))\n\n    show_agg_progress([path to EI data], \"EI\")\n    show_agg_progress([path to PoI data], \"PoI\")\n    show_agg_progress([path to modified PoI data], \"PoI\" + r\"$(\\epsilon = 0.1)$\")\n    plt.xlabel(\"# queries\")\n    plt.ylabel(\"accuracy\")\n\n    plt.legend()\n\n    plt.show()\n    ```", "```py\n    num_queries = 10\n\n    start_beta = 1\n    end_beta = 10\n\n    multiplier = (end_beta / start_beta) ** (1 / num_queries)\n    ```", "```py\n    num_queries = 10\n\n    start_beta = 1\n    end_beta = 10\n\n    multiplier = (end_beta / start_beta) ** (1 / num_queries)\n\n    beta = start_beta\n\n    for i in range(num_queries):\n        ...                          ❶\n\n        policy = botorch.acquisition.analytic.UpperConfidenceBound(\n            model, beta=beta\n        )\n\n        ...                          ❷\n\n        beta *= multiplier\n    ```", "```py\n    policy = botorch.acquisition.analytic.UpperConfidenceBound(\n        model, beta=[some value]\n    )\n    ```", "```py\n    num_repeats = 10\n\n    start_beta = 3\n    end_beta = 10\n\n    multiplier = (end_beta / start_beta) ** (1 / num_queries)\n\n    incumbents = torch.zeros((num_repeats, num_queries))\n\n    for trial in range(num_repeats):\n        ...                                ❶\n\n        beta = start_beta\n\n        for i in tqdm(range(num_queries)):\n            ...                            ❷\n\n            policy = botorch.acquisition.analytic.UpperConfidenceBound(\n                model, beta=beta\n            )\n\n            ...                            ❸\n\n            beta *= multiplier\n    ```", "```py\n    num_candidates = 2000\n    num_repeats = 10\n\n    incumbents = torch.zeros((num_repeats, num_queries))\n\n    for trial in range(num_repeats):\n      ...                               ❶\n\n      for i in tqdm(range(num_queries)):\n        ...                             ❷\n\n        sobol = torch.quasirandom.SobolEngine(1, scramble=True)\n        candidate_x = sobol.draw(num_candidates)\n        candidate_x = bounds[0] + (bounds[1] - bounds[0]) * candidate_x\n\n        ts = botorch.generation.MaxPosteriorSampling(model, \n        ➥replacement=False)\n        next_x = ts(candidate_x, num_samples=1)\n\n        ...                             ❸\n    ```", "```py\n    def compute_entropy(first, last):\n        entropy = 0\n        for i in range(first, last + 1):\n            p = marginal_probability(i, first, last)   ❶\n            entropy += -p * np.log2(p)                 ❷\n\n        return entropy\n    ```", "```py\n    def marginal_probability(floor, first, last):\n        if floor == last:                   ❶\n            return 2 ** -(last - first)     ❶\n\n        return 2 ** -(floor - first + 1)\n    ```", "```py\n    def cumulative_density(floor, first, last):\n        return sum(                                  ❶\n            [                                        ❶\n                marginal_probability(i, first, last) ❶\n                for i in range(first, floor)         ❶\n            ]                                        ❶\n        )                                            ❶\n    ```", "```py\n    >>> compute_entropy(1, 4)\n    Output: 1.75\n    >>> compute_entropy(5, 10)\n    Output: 1.9375\n    ```", "```py\n    def compute_expected_posterior_entropy(floor, first, last):\n        break_probability = cumulative_density\n        ➥(floor, first, last)                            ❶\n\n        return (                                          ❷\n            break_probability * compute_entropy           ❷\n            ➥(first, floor - 1)                          ❷\n            + (1 - break_probability) * compute_entropy   ❷\n            ➥(floor, last)                               ❷\n        )                                                 ❷\n    ```", "```py\n    num_candidates = 2000\n    num_repeats = 10\n\n    incumbents = torch.zeros((num_repeats, num_queries))\n\n    for trial in range(num_repeats):\n      ...                                 ❶\n\n      for i in tqdm(range(num_queries)):\n        ...                                ❷\n\n        sobol = torch.quasirandom.SobolEngine(1, scramble=True)\n        candidate_x = sobol.draw(num_candidates)\n        candidate_x = bounds[0] + (bounds[1] - bounds[0]) * candidate_x\n\n        policy = botorch.acquisition.max_value_entropy_search.qMaxValueEntropy(\n            model, candidate_x\n        )\n\n        ...                               ❸\n    ```", "```py\n    num_candidates = 2000                                   ❶\n\n    ...                                                     ❷\n\n    for i in tqdm(range(num_iters)):\n        ...                                                 ❸\n\n        sobol = torch.quasirandom.SobolEngine\n        ➥(2, scramble=True)                                ❹\n        candidate_x = sobol.draw(num_candidates)            ❹\n        candidate_x = (bounds[1] - bounds[0]) *             ❹\n        ➥candidate_x + bounds[0]                           ❹\n\n        ts = botorch.generation.MaxPosteriorSampling(model, \n        ➥replacement=False)\n        next_x = ts(candidate_x, num_samples=batch_size)    ❺\n\n        ...                                                 ❻\n    ```", "```py\n    def flight_utility(X):\n      X_copy = X.detach().clone()\n      X_copy[:, [2, 3]] = 1 - X_copy[:, [2, 3]]\n      X_copy = X_copy * 10 - 5\n\n      return -0.005 * (X_copy**4 - 16 * X_copy**2 + 5 * X_copy).sum(dim=-1) + 3\n    ```", "```py\n    class GPModel(\n        gpytorch.models.ExactGP,\n        botorch.models.gpytorch.GPyTorchModel,\n        botorch.models.model.FantasizeMixin\n    ):\n        _num_outputs = 1\n\n        def __init__(self, train_x, train_y, likelihood):\n            super().__init__(train_x, train_y, likelihood)\n            self.mean_module = gpytorch.means.ConstantMean()\n            self.covar_module = gpytorch.kernels.ScaleKernel(\n                gpytorch.kernels.MaternKernel(     ❶\n                    nu=2.5,                        ❶\n                    ard_num_dims=4                 ❶\n                )                                  ❶\n            )\n\n        def forward(self, x):\n            ...\n    ```", "```py\n    lb = 0\n    ub = 1\n\n    bounds = torch.tensor([[lb] * 4, [ub] * 4], dtype=torch.float)\n    ```", "```py\n    num_experiments = 5\n\n    num_queries = 100\n    batch_size = 5\n    num_iters = num_queries // batch_size\n    ```", "```py\n    incumbents = torch.zeros((num_experiments, num_iters))\n\n    pbar = tqdm(total=num_experiments * num_iters)\n    for exp in range(num_experiments):\n        torch.manual_seed(exp)                       ❶\n        train_x = bounds[0] + (bounds[1] -           ❶\n        ➥bounds[0]) * torch.rand(1, 4)              ❶\n        train_y = flight_utility(train_x)            ❶\n\n        for i in range(num_iters):\n            incumbents[exp, i] = train_y.max()       ❷\n\n            model, likelihood = fit_gp_model\n            ➥(train_x, train_y)                     ❷\n\n            ...                                      ❸\n\n            next_y = flight_utility(next_x)          ❹\n\n            train_x = torch.cat([train_x, next_x])   ❹\n            train_y = torch.cat([train_y, next_y])   ❹\n\n            pbar.update()\n    ```", "```py\n    policy = botorch.acquisition.monte_carlo.qProbabilityOfImprovement(\n        model, best_f=train_y.max()\n    )\n    ```", "```py\n    policy = botorch.acquisition.monte_carlo.qExpectedImprovement(\n        model, best_f=train_y.max()\n    )\n    ```", "```py\n    policy = botorch.acquisition.monte_carlo.qUpperConfidenceBound(\n        model, beta=2\n    )\n    ```", "```py\n    next_x, acq_val = botorch.optim.optimize_acqf(\n        policy,\n        bounds=bounds,\n        q=batch_size,\n        num_restarts=100,\n        raw_samples=200,\n    )\n    ```", "```py\n    sobol = torch.quasirandom.SobolEngine(4, scramble=True)    ❶\n    candidate_x = sobol.draw(5000)\n    candidate_x = (bounds[1] - bounds[0]) * candidate_x + bounds[0]\n    ```", "```py\n    ts = botorch.generation.MaxPosteriorSampling(model, replacement=False)\n    next_x = ts(candidate_x, num_samples=batch_size)\n    ```", "```py\n    policy = botorch.acquisition.max_value_entropy_search.qMaxValueEntropy(\n        model, candidate_x\n    )\n\n    next_x, acq_val = botorch.optim.optimize_acqf_cyclic(\n        policy,\n        bounds=bounds,\n        q=batch_size,\n        num_restarts=40,\n        raw_samples=100,\n        cyclic_options={\"maxiter\": 5}    ❶\n    )\n    ```", "```py\n    lb = -5\n    ub = 5\n\n    xs = torch.linspace(lb, ub, 201)\n    ```", "```py\n    n = 3\n    torch.manual_seed(0)                                           ❶\n    train_x = bounds[0] + (bounds[1] - bounds[0]) * torch.rand(n)  ❷\n\n    train_utility = objective(train_x)\n    train_cost = cost(train_x)\n    ```", "```py\n    utility_model, utility_likelihood = fit_gp_model(   ❶\n        train_x.unsqueeze(-1), train_utility            ❶\n    )                                                   ❶\n\n    cost_model, cost_likelihood = fit_gp_model(         ❷\n        train_x.unsqueeze(-1), train_cost               ❷\n    )                                                   ❷\n    ```", "```py\n    with torch.no_grad():\n        cost_pred_dist = cost_likelihood(cost_model(xs))\n        cost_pred_mean = cost_pred_dist.mean\n        cost_pred_lower, cost_pred_upper = \\\n            cost_pred_dist.confidence_region()\n    ```", "```py\n    normal = torch.distributions.Normal(cost_pred_mean, cost_pred_dist.stddev)\n    ```", "```py\n    feasible_prob = normal.cdf(torch.zeros(1))\n    ```", "```py\n    ei = botorch.acquisition.analytic.ExpectedImprovement(\n        model=utility_model,\n        best_f=train_utility[train_cost <= 0].max(),\n    )\n    ```", "```py\n    with torch.no_grad():\n        ei_score = ei(xs[:, None, None])\n    ```", "```py\n    constrained_ei = botorch.acquisition.analytic.ConstrainedExpectedImprovement(\n        model=botorch.models.model_list_gp_regression.ModelListGP(\n            utility_model, cost_model\n        ),\n        best_f=train_utility[train_cost <= 0].max(),\n        objective_index=0,\n        constraints={1: [None, 0]}\n    )\n    ```", "```py\n    with torch.no_grad():\n        constrained_ei_score = constrained_ei(xs[:, None, None])\n    ```", "```py\n    assert torch.isclose(\n        ei_score * feasible_prob, constrained_ei_score, atol=1e-3\n    ).all()\n    ```", "```py\n    plt.plot(xs, ei_score, label=\"EI\")\n    plt.plot(xs, constrained_ei_score, label=\"BoTorch constrained EI\")\n    ```", "```py\n    def flight_cost(X):\n      X = X * 20 - 10\n\n      part1 = (X[..., 0] - 1) ** 2\n\n      i = X.new(range(2, 5))\n      part2 = torch.sum(i * (2.0 * X[..., 1:] ** 2 - X[..., :-1]) ** 2, \n      ➥dim=-1)\n\n      return -(part1 + part2) / 100_000 + 2\n    ```", "```py\n    num_queries = 50\n    num_repeats = 10\n    ```", "```py\n    default_value = -2\n    feasible_incumbents = torch.ones((num_repeats, num_queries)) * default_value\n    ```", "```py\n    if (train_cost <= 0).any():                                      ❶\n        best_f = train_utility[train_cost <= 0].max()                ❶\n    else:                                                            ❶\n        best_f = torch.tensor(default_value)                         ❶\n\n    policy = botorch.acquisition.analytic.ConstrainedExpectedImprovement(\n        model=botorch.models.model_list_gp_regression.ModelListGP(   ❷\n            utility_model, cost_model                                ❷\n        ),                                                           ❷\n        best_f=best_f,\n        objective_index=0,                                           ❸\n        constraints={1: [None, 0]}                                   ❹\n    )\n    ```", "```py\n    policy = botorch.acquisition.analytic.ExpectedImprovement(\n       model=utility_model,\n       best_f=train_utility.max(),\n    )\n    ```", "```py\n    num_repeats = 10                                   ❶\n\n    for trial in range(num_repeats):\n        torch.manual_seed(trial)                       ❷\n        train_x = bounds[0] + (bounds[1] - bounds[0])  ❷\n        ➥* torch.rand(1, 1)                           ❷\n        train_x = torch.cat(                           ❷\n            [train_x, torch.ones_like(train_x)         ❷\n            ➥* fidelities[0]], dim=1                  ❷\n        )                                              ❷\n        train_y = evaluate_all_functions(train_x)      ❷\n\n        current_budget = 0                             ❸\n        while current_budget < budget_limit:           ❸\n            ...                                        ❸\n    ```", "```py\n    num_repeats = 10\n    recommendations = []                               ❶\n    spent_budget = []                                  ❶\n\n    for trial in range(num_repeats):\n        torch.manual_seed(trial)\n        train_x = bounds[0] + (bounds[1] - bounds[0]) * torch.rand(1, 1)\n        train_x = torch.cat(\n            [train_x, torch.ones_like(train_x) * fidelities[0]], dim=1\n        )\n        train_y = evaluate_all_functions(train_x)\n\n        current_budget = 0\n        recommendations.append([])                     ❷\n        spent_budget.append([])                        ❷\n\n        while current_budget < budget_limit:\n            ...\n\n            rec_x = get_final_recommendation(model)    ❸\n            recommendations[-1].append                 ❸\n            ➥(evaluate_all_functions(rec_x).item())   ❸\n            spent_budget[-1].append(current_budget)    ❸\n\n            ...\n    ```", "```py\n    xs = np.arange(budget_limit)\n    interp_incumbents = np.empty((num_repeats, budget_limit))\n    ```", "```py\n    for i, (tmp_incumbents, tmp_budget) in enumerate(\n        zip(incumbents, spent_budget)\n    ):\n        interp_incumbents[i, :] = np.interp(xs, tmp_budget, tmp_incumbents)\n    ```", "```py\n    bounds = torch.tensor([[0.0] * 2, [1.0] * 2])\n    ```", "```py\n    fidelities = torch.tensor([0.1, 0.3, 1.0])\n    bounds_full = torch.cat(\n        [\n            bounds,\n            torch.tensor([fidelities.min(), fidelities.max()]).unsqueeze(-1)\n        ],\n        dim=1\n    )\n    ```", "```py\n    from botorch.models.cost import AffineFidelityCostModel\n\n    cost_model = AffineFidelityCostModel(fixed_cost=0.2)\n    ```", "```py\n    budget_limit = 10\n    num_repeats = 10\n    ```", "```py\n    num_samples = 5000\n\n    num_restarts = 100\n    raw_samples = 500\n    ```", "```py\n    from botorch.acquisition.fixed_feature import \n    ➥FixedFeatureAcquisitionFunction\n    from botorch.acquisition import PosteriorMean\n    from botorch.optim.optimize import optimize_acqf, optimize_acqf_mixed\n\n    def get_final_recommendation(model):\n        post_mean_policy = FixedFeatureAcquisitionFunction(\n            acq_function=PosteriorMean(model),\n            d=3,                ❶\n            columns=[2],        ❶\n            values=[1],\n        )\n\n        final_x, _ = optimize_acqf(\n            post_mean_policy,\n            bounds=bounds,\n            q=1,\n            num_restarts=num_restarts,\n            raw_samples=raw_samples,\n        )\n\n        return torch.cat([final_x, torch.ones(1, 1)], dim=1)\n    ```", "```py\n    def objective1(X):                                                  ❶\n      X_copy = X.detach().clone()                                       ❶\n      X_copy[:, [2, 3]] = 1 - X_copy[:, [2, 3]]                         ❶\n      X_copy = X_copy * 10 - 5                                          ❶\n      return (                                                          ❶\n        -0.005                                                          ❶\n        * (X_copy ** 4 - 16 * X_copy ** 2 + 5 * X_copy)                 ❶\n        ➥.sum(dim=-1)                                                  ❶\n          + 3                                                           ❶\n      )                                                                 ❶\n\n    def objective2(X):                                                  ❷\n      X = X * 20 - 10                                                   ❷\n      part1 = (X[..., 0] - 1) ** 2                                      ❷\n      i = X.new(range(2, 5))                                            ❷\n      part2 = torch.sum(i * (2.0 * X[..., 1:] ** 2 - X[..., :-1]) ** 2, ❷\n      ➥ dim=-1)                                                        ❷\n      return (part1 + part2) / 100_000 - 2                              ❷\n    ```", "```py\n    def joint_objective(X):\n        return torch.vstack(\n            [\n                objective1(X).flatten(),\n                objective2(X).flatten(),\n            ]\n        ).transpose(-1, -2)\n    ```", "```py\n    bounds = torch.tensor([[0.0] * 4, [1.0] * 4])\n    ```", "```py\n    ref_point = torch.tensor([-1.5, -2.0])\n    ```", "```py\n    num_queries = 50\n    num_repeats = 10\n    ```", "```py\n    import pandas as pd\n\n    df = pd.read_csv(\"../data/housing.csv\")\n    ```", "```py\n    plt.figure(figsize=(8, 6))\n    plt.scatter(df.longitude, df.latitude, c=np.log(df.median_house_value))\n    plt.colorbar();\n    ```", "```py\n    train_x = torch.from_numpy(df.drop([\"median_house_value\"], axis=1).values)\n    ```", "```py\n    train_y = torch.from_numpy(\n        df.median_house_value.values\n    ).log().to(train_x.dtype)\n    ```", "```py\n    train_y = (train_y - train_y.mean()) / train_y.std()\n    ```", "```py\n    class GPModel(gpytorch.models.ExactGP):\n        def __init__(self, train_x, train_y, likelihood):\n            super().__init__(train_x, train_y, likelihood)\n            self.mean_module = gpytorch.means.ConstantMean()   ❶\n            self.covar_module = gpytorch.kernels.ScaleKernel(  ❷\n                gpytorch.kernels.MaternKernel(\n                    nu=2.5,                                    ❷\n                    ard_num_dims=train_x.shape[1]              ❷\n                )                                              ❷\n            )                                                  ❷\n\n        def forward(self, x):\n            mean_x = self.mean_module(x)\n            covar_x = self.covar_module(x)\n            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n    ```", "```py\n    likelihood = gpytorch.likelihoods.GaussianLikelihood(\n        noise_constraint=gpytorch.constraints\n        ➥.GreaterThan(1e-1)                    ❶\n    )\n    ```", "```py\n    model = GPModel(train_x, train_y, likelihood)\n\n    optimizer = torch.optim.Adam(model.parameters(),\n    ➥lr=0.01)                                        ❶\n    mll = gpytorch.mlls.ExactMarginalLogLikelihood\n    ➥(likelihood, model)                             ❷\n\n    model.train()                                     ❸\n    likelihood.train()                                ❸\n\n    for i in tqdm(range(10)):\n        optimizer.zero_grad()\n\n        output = model(train_x)\n        loss = -mll(output, train_y)\n\n        loss.backward()\n        optimizer.step()\n    ```", "```py\n    class ApproximateGPModel(gpytorch.models.ApproximateGP):\n      def __init__(self, inducing_points):\n        variational_distribution =                    ❶\n        ➥gpytorch.variational                        ❶\n        ➥.NaturalVariationalDistribution(            ❶\n            inducing_points.size(0)                   ❶\n        )                                             ❶\n        variational_strategy = gpytorch.variational   ❶\n        ➥.VariationalStrategy(                       ❶\n            self,                                     ❶\n            inducing_points,                          ❶\n            variational_distribution,                 ❶\n            learn_inducing_locations=True,            ❶\n        )                                             ❶\n        super().__init__(variational_strategy)\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(\n            gpytorch.kernels.MaternKernel(\n                nu=2.5,\n                ard_num_dims=inducing_points.shape[1]\n            )\n        )\n\n      def forward(self, x):\n        ...                                           ❷\n    ```", "```py\n    num_datapoints = 100                             ❶\n    torch.manual_seed(0)                             ❶\n    model = ApproximateGPModel(                      ❶\n      train_x[torch.randint(train_x.shape[0],        ❶\n      ➥(num_datapoints,)), :]                       ❶\n    )                                                ❶\n\n    likelihood = gpytorch.likelihoods.GaussianLikelihood(\n      noise_constraint=gpytorch.constraints.GreaterThan(1e-1)\n    )\n\n    train_dataset = torch.utils.data.Tensordataset\n    ➥(train_x, train_y)                             ❷\n    train_loader = torch.utils.data.DataLoader(      ❷\n      train_data set,                                ❷\n      batch_size=100,                                ❷\n      shuffle=True                                   ❷\n    )                                                ❷\n\n    ngd_optimizer = gpytorch.optim.NGD(              ❸\n      model.variational_parameters(),                ❸\n      ➥num_data=train_y.size(0), lr=0.1             ❸\n    )                                                ❸\n    hyperparam_optimizer = torch.optim.Adam(         ❹\n      [{\"params\": model.parameters()}, {\"params\":    ❹\n      ➥likelihood.parameters()}],                   ❹\n      lr=0.01                                        ❹\n    )                                                ❹\n\n    mll = gpytorch.mlls.VariationalELBO(\n      likelihood, model, num_data=train_y.size(0)\n    )\n\n    model.train()\n    likelihood.train()\n\n    for i in tqdm(range(10)):\n      for x_batch, y_batch in train_loader:\n        ngd_optimizer.zero_grad()\n\n        output = model(x_batch)\n        loss = -mll(output, y_batch)\n\n        loss.backward()\n\n        ngd_optimizer.step()\n        hyperparam_optimizer.step()\n    ```"]