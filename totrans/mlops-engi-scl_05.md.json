["```py\n!pip install fsspec s3fs        ❶\n\nimport s3fs\nimport pandas as pd\n\ndf = pd.read_csv(f\"s3://dc-taxi-{os.environ['BUCKET_ID']}-\n➥ {os.environ['AWS_DEFAULT_REGION']}/parquet/\n➥ vacuum/.meta/stats/*\")      ❷\n\nprint(df.info())\n```", "```py\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 5 entries, 0 to 4\nData columns (total 10 columns):\n #   Column                              Non-Null Count  Dtype\n---  ------                              --------------  -----\n 0   summary                             5 non-null      object\n 1   fareamount_double                   5 non-null      float64\n 2   origin_block_latitude_double        5 non-null      float64\n 3   origin_block_longitude_double       5 non-null      float64\n 4   destination_block_latitude_double   5 non-null      float64\n 5   destination_block_longitude_double  5 non-null      float64\n 6   year_integer                        5 non-null      float64\n 7   month_integer                       5 non-null      float64\n 8   dow_integer                         5 non-null      float64\n 9   hour_integer                        5 non-null      float64\ndtypes: float64(9), object(1)\nmemory usage: 528.0+ bytes\nNone\n```", "```py\nsummary_df = df.set_index('summary')\nsummary_df\n```", "```py\nds_size = summary_df.loc['count'].astype(int).max()\nprint(ds_size)\n```", "```py\nmu = summary_df.loc['mean']\nprint(mu)\n```", "```py\nfareamount_double                        9.74\norigin_block_latitude_double            38.90\norigin_block_longitude_double          -77.03\ndestination_block_latitude_double       38.91\ndestination_block_longitude_double     -77.03\nyear_integer                         2,016.62\nmonth_integer                            6.57\ndow_integer                              3.99\nhour_integer                            14.00\nName: mean, dtype: float64\n```", "```py\nsigma = summary_df.loc['stddev']\nprint(sigma)\n```", "```py\nfareamount_double                     4.539085\norigin_block_latitude_double          0.014978\norigin_block_longitude_double         0.019229\ndestination_block_latitude_double     0.017263\ndestination_block_longitude_double    0.022372\nyear_integer                          1.280343\nmonth_integer                         3.454275\ndow_integer                           2.005323\nhour_integer                          6.145545\nName: stddev, dtype: float64\n```", "```py\nfractions = [.3, .15, .1, .01, .005]\nprint([ds_size * fraction for fraction in fractions])\n```", "```py\n[4278658.8, 2139329.4, 1426219.6, 142621.96, 71310.98]\n```", "```py\nfrom math import log, floor\nranges = [floor(log(ds_size * fraction, 2)) for fraction in fractions]\nprint(ranges)\n```", "```py\n[22, 21, 20, 17, 16]\n```", "```py\nsample_size_upper, sample_size_lower = max(ranges) + 1, min(ranges) - 1\nprint(sample_size_upper, sample_size_lower)\n```", "```py\n(23, 15)\n```", "```py\nsizes = [2 ** i for i in range(sample_size_lower, sample_size_upper)]\noriginal_sizes = sizes\nfracs = [ size / ds_size for size in sizes]\nprint(*[(idx, sample_size_lower + idx, frac, size) \\\n  for idx, (frac, size) in enumerate(zip(fracs, sizes))], sep='\\n')\n```", "```py\n(0, 15, 0.0022975423980991427, 32768)\n(1, 16, 0.004595084796198285, 65536)\n(2, 17, 0.00919016959239657, 131072)\n(3, 18, 0.01838033918479314, 262144)\n(4, 19, 0.03676067836958628, 524288)\n(5, 20, 0.07352135673917257, 1048576)\n(6, 21, 0.14704271347834513, 2097152)\n(7, 22, 0.29408542695669027, 4194304)\n```", "```py\nimport numpy as np\ndef sem_over_range(lower, upper, mu, sigma):    ❶\n  sizes_series = pd.Series([2 ** i \\            ❷\n    for i in range(lower, upper + 1)])\n  est_sem_df = \\                                ❸\n    pd.DataFrame( np.outer( (1 / np.sqrt(sizes_series)), sigma.values ),\n                        columns = sigma.index,\n                        index = sizes_series.values)\n  return est_sem_df\n\nsem_df = sem_over_range(sample_size_lower, sample_size_upper, mu, sigma)\nsem_df\n```", "```py\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.figure(figsize = (12, 9))\nplt.plot(sem_df.index, sem_df.mean(axis = 1))\nplt.xticks(sem_df.index,\n           labels = list(map(lambda i: f\"2^{i}\",\n                              np.log2(sem_df.index.values).astype(int))),\n           rotation = 90);\n```", "```py\nagg_change = sem_df.cumsum().mean(axis = 1)\nagg_change\n```", "```py\n32768     0.01\n65536     0.02\n131072    0.02\n262144    0.03\n524288    0.03\n1048576   0.03\n2097152   0.03\n4194304   0.03\n8388608   0.04\ndtype: float64\n```", "```py\nimport numpy as np\n\ndef marginal(x):\n  coor = np.vstack([x.index.values,\n            x.values]).transpose()          ❶\n\n  return pd.Series(index = x.index,         ❷\n    data = np.cross(coor[-1] - coor[0], coor[-1] - coor) \\\n             / np.linalg.norm(coor[-1] - coor[0])).idxmin()\n\nSAMPLE_SIZE = marginal(agg_change).astype(int)\nSAMPLE_SIZE, SAMPLE_SIZE / ds_size\n```", "```py\n(1048576, 0.07352135673917257)\n```", "```py\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nargs = getResolvedOptions(sys.argv, ['JOB_NAME',\n                                     'BUCKET_SRC_PATH',\n                                     'BUCKET_DST_PATH',\n                                     'SAMPLE_SIZE',\n                                     'SAMPLE_COUNT',\n                                     'SEED'\n                                     ])\n\nsc = SparkContext()\nglueContext = GlueContext(sc)\nlogger = glueContext.get_logger()\nspark = glueContext.spark_session\n\njob = Job(glueContext)\njob.init(args['JOB_NAME'], args)\n\nBUCKET_SRC_PATH = args['BUCKET_SRC_PATH']\ndf = ( spark.read.format(\"parquet\")\n        .load( f\"{BUCKET_SRC_PATH}\" ))      ❶\n```", "```py\nSAMPLE_SIZE = float( args['SAMPLE_SIZE'] )   \ndataset_size = float( df.count() )\nsample_frac = SAMPLE_SIZE / dataset_size            ❶\n\nfrom kaen.spark import spark_df_to_stats_pandas_df, \\\n                      pandas_df_to_spark_df, \\\n                      spark_df_to_shards_df         ❷\n\nsummary_df = spark_df_to_stats_pandas_df(df)        ❸\nmu = summary_df.loc['mean']                         ❹\nsigma = summary_df.loc['stddev']                    ❺\n```", "```py\nSEED = int(args['SEED'])                                ❶\nSAMPLE_COUNT = int(args['SAMPLE_COUNT'])                ❷\nBUCKET_DST_PATH = args['BUCKET_DST_PATH']\n\nfor idx in range(SAMPLE_COUNT):\n  dev_df, test_df = ( df                                ❸\n                      .cache()\n                      .randomSplit([1.0 - sample_frac,\n                                      sample_frac],     ❹\n                                    seed = SEED) )\n\n  test_df = test_df.limit( int(SAMPLE_SIZE) )           ❺\n\n  test_stats_df = \\                                     ❻\n    spark_df_to_stats_pandas_df(test_df, summary_df,\n                                  pvalues = True, zscores = True)\n\n  pvalues_series = test_stats_df.loc['pvalues']\n  if pvalues_series.min() < 0.05:\n    SEED = SEED + idx                                   ❼\n  else:\n    break\n```", "```py\nfor df, desc in [(dev_df, \"dev\"), (test_df, \"test\")]:\n    ( df\n    .write\n    .option('header', 'true')\n    .mode('overwrite')\n    .csv(f\"{BUCKET_DST_PATH}/{desc}\") )\n\n    stats_pandas_df = \\\n    spark_df_to_stats_pandas_df(df,\n                                summary_df,\n                                pvalues = True,\n                                zscores = True)\n    ( pandas_df_to_spark_df(spark,  stats_pandas_df)\n    .coalesce(1)\n    .write\n    .option('header', 'true')\n    .mode('overwrite')\n    .csv(f\"{BUCKET_DST_PATH}/{desc}/.meta/stats\") )\n\n    ( spark_df_to_shards_df(spark, df)\n    .coalesce(1)\n    .write\n    .option('header', True)\n    .mode('overwrite')\n    .csv(f\"{BUCKET_DST_PATH}/{desc}/.meta/shards\") )\n\njob.commit()\n```", "```py\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nargs = getResolvedOptions(sys.argv, ['JOB_NAME',\n                                     'BUCKET_SRC_PATH',\n                                     'BUCKET_DST_PATH',\n                                     'SAMPLE_SIZE',\n                                     'SAMPLE_COUNT',\n                                     'SEED'\n                                     ])\n\nsc = SparkContext()\nglueContext = GlueContext(sc)\nlogger = glueContext.get_logger()\nspark = glueContext.spark_session\n\njob = Job(glueContext)\njob.init(args['JOB_NAME'], args)\n\nBUCKET_SRC_PATH = args['BUCKET_SRC_PATH']\ndf = ( spark.read.format(\"parquet\")\n        .load( f\"{BUCKET_SRC_PATH}\" ))\n\nSAMPLE_SIZE = float( args['SAMPLE_SIZE'] )\ndataset_size = float( df.count() )\nsample_frac = SAMPLE_SIZE / dataset_size\n\nfrom kaen.spark import spark_df_to_stats_pandas_df, \\\n                      pandas_df_to_spark_df, \\\n                      spark_df_to_shards_df\n\nsummary_df = spark_df_to_stats_pandas_df(df)\nmu = summary_df.loc['mean']\nsigma = summary_df.loc['stddev']\n\nSEED = int(args['SEED'])\nSAMPLE_COUNT = int(args['SAMPLE_COUNT'])\nBUCKET_DST_PATH = args['BUCKET_DST_PATH']\n\nfor idx in range(SAMPLE_COUNT):\n  dev_df, test_df = ( df\n                      .cache()\n                      .randomSplit( [1.0 - sample_frac, sample_frac],\n                                    seed = SEED) )\n  test_df = test_df.limit( int(SAMPLE_SIZE) )\n\n  test_stats_df = \\\n    spark_df_to_stats_pandas_df(test_df, summary_df,\n                                  pvalues = True, zscores = True)\n\n  pvalues_series = test_stats_df.loc['pvalues']\n  if pvalues_series.min() < 0.05:\n    SEED = SEED + idx\n  else:\n    break\n\nfor df, desc in [(dev_df, \"dev\"), (test_df, \"test\")]:\n    ( df\n    .write\n    .option('header', 'true')\n    .mode('overwrite')\n    .csv(f\"{BUCKET_DST_PATH}/{desc}\") )\n\n    stats_pandas_df = \\\n    spark_df_to_stats_pandas_df(df,\n                                summary_df,\n                                pvalues = True,\n                                zscores = True)\n\n    ( pandas_df_to_spark_df(spark,  stats_pandas_df)\n    .coalesce(1)\n    .write\n    .option('header', 'true')\n    .mode('overwrite')\n    .csv(f\"{BUCKET_DST_PATH}/{desc}/.meta/stats\") )\n\n    ( spark_df_to_shards_df(spark, df)\n    .coalesce(1)\n    .write\n    .option('header', True)\n    .mode('overwrite')\n    .csv(f\"{BUCKET_DST_PATH}/{desc}/.meta/shards\") )\n\njob.commit()\n```", "```py\nos.environ['SAMPLE_SIZE'] = str(SAMPLE_SIZE)\nos.environ['SAMPLE_COUNT'] = str(1)\n```", "```py\nwget -q --no-cache https://raw.githubusercontent.com/\n➥ osipov/smlbook/master/utils.sh\n```", "```py\nsource utils.sh\n\nPYSPARK_SRC_NAME=dctaxi_dev_test.py \\\nPYSPARK_JOB_NAME=dc-taxi-dev-test-job \\\nADDITIONAL_PYTHON_MODULES=\"kaen[spark]\" \\\nBUCKET_SRC_PATH=s3://dc-taxi-$BUCKET_ID-$AWS_DEFAULT_REGION/parquet/vacuum \\\nBUCKET_DST_PATH=s3://dc-taxi-$BUCKET_ID-$AWS_DEFAULT_REGION/csv \\\nSAMPLE_SIZE=$SAMPLE_SIZE \\\nSAMPLE_COUNT=$SAMPLE_COUNT \\\nSEED=30 \\\nrun_job\n```", "```py\nAttempting to run a job using:\n  PYSPARK_SRC_NAME=dctaxi_dev_test.py\n  PYSPARK_JOB_NAME=dc-taxi-dev-test-job\n  AWS_DEFAULT_REGION=us-west-2\n  BUCKET_ID=c6e91f06095c3d7c61bcc0af33d68382\n  BUCKET_SRC_PATH=s3://dc-taxi-c6e91f06095c3d7c61bcc0af33d68382-\n➥   us-west-2/parquet/vacuum\n  BUCKET_DST_PATH=s3://dc-taxi-c6e91f06095c3d7c61bcc0af33d68382-\n➥   us-west-2/csv\n  SAMPLE_SIZE=1048576\n  SAMPLE_COUNT=1\n  BINS=\n  SEED=30\nupload: ./dctaxi_dev_test.py to s3://dc-taxi-\n➥   c6e91f06095c3d7c61bcc0af33d68382-us-west-2/glue/dctaxi_dev_test.py\n2021-08-15 17:19:37       2456 dctaxi_dev_test.py\n{\n    \"JobName\": \"dc-taxi-dev-test-job\"\n}\n{\n    \"Name\": \"dc-taxi-dev-test-job\"\n}\n{\n    \"JobRunId\": [CA\n    \"jr_05e395544e86b1534c824fa1559ac395683f3e7db35d1bb5d591590d237954f2\"\n}\nWaiting for the job to finish......................................SUCCEEDED\n```", "```py\npd.options.display.float_format = '{:,.2f}'.format\n\ntest_stats_df = pd.read_csv(f\"s3://dc-taxi-{os.environ['BUCKET_ID']}-\n➥ {os.environ['AWS_DEFAULT_REGION']}/csv/test/.meta/stats/*.csv\")\n\ntest_stats_df = test_stats_df.set_index('summary')\ntest_stats_df\n```", "```py\nimport pandas as pd\ndev_shards_df = pd.read_csv(f\"s3://dc-taxi-{os.environ['BUCKET_ID']}-\n➥ {os.environ['AWS_DEFAULT_REGION']}/csv/dev/.meta/shards/*\")\n\ndev_shards_df.sort_values(by = 'id')\n```"]